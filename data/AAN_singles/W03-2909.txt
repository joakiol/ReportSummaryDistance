A reconfigurable stochastic tagger for languages with complex tagstructure?ukasz De?bowskiInstitute of Computer SciencePolish Academy of Sciencesldebowsk@ipipan.waw.plAbstractWe present a case study of a complexstochastic disambiguator of alternativesof morphosyntactic tags which allowsfor using incomplete disambiguation,shorthand tag notation, external tagsetdefinition and external definition of mul-tivalued context features.
The taggerbases on Naive Bayes modeling and al-lows for using almost as general contextfeatures as in classical trigram taggers aswell as more specific ones.
Its prelimi-nary results for Polish still do not meetour expectations.
Possible sources of thetagger?s failures can be: inhomogene-ity of the training corpus in preparation,lack of the automatic search of probabil-ity models, too general conditional in-dependence assumptions in defining theclass of interpretable models.Automatization of high-quality morphosyntac-tic tagging for strongly inflective languages, suchas Slavic languages, seems to be a much hardertask than so called part-of-speech (POS) taggingfor weaker inflective languages.
An important fac-tor increasing the complexity is the very design ofthe tagset.
Usually, the tags assigned to word-like segments in the former task are long listsof subsequent attribute values, e.g.
POS, num-ber, case, gender, person etc.
(Hajic?
and Hladk?,1998; Wolin?ski and Przepi?rkowski, 2001), sothey provide much more information than almostatomic labels used for POS tagging (Manning andSch?tze, 1999).
To make the matter harder, manyformal descriptions become easier when the tagattribute values are allowed to form RSRL-liketype hierarchies (Przepi?rkowski et al, 2002).
Al-lowing the values to be partially ambiguous de-pending on a context raises questions what is theaccurate level of disambiguation (Wolin?ski andPrzepi?rkowski, 2001) and how to model it prob-abilistically in terms of random variables takingdisjunctive values (Brew, 1995).Working for a project aiming at building a largemorphosyntactically tagged corpus of written Pol-ish (information site http://dach.ipipan.waw.pl/CORPUS/), we have tried implement-ing a highly reconfigurable stochastic tagger ad-dressing some of these problems.
The main fea-tures of our software are as follows:?
The tagger is a contextual disambiguator: Itonly prunes the lists of tags admissible forsuccessive word-segments, given by a sepa-rate morphological analyzer.
Superiority ofthis approach over simulating a stochasticmorphological analyzer by a tagger has beendiscussed in Hajic?
(2000).?
The tags processed by the tagger have formof short human-readable lists of attribute val-ues.
Especially, non-applicable attributes areomitted and multiple atomic values can begiven for the same attribute.?
The tagger?s internal representation of dis-ambiguation alternatives (tagger?s decisions)is different than the list of all admissibleatomic tags.
In this approach, some kindof contextually-dependent incompleteness ofdisambiguation could be learned.?
Special configuration files inform the tag-ger which tag attributes are to be disam-biguated and what multivalued context at-tribute are relevant for that.
The tagger?s in-ference uses a series of Naive-Bayes-like as-sumptions founded on joint distributions ofdisambiguation decisions for one tag attributeand values of one context attribute.?
The values of the context attributes are au-tomatically instantiated and smoothed stringswhose templates are given in a hand-madeconfiguration file.
This approach allows tocombine strengths of generality of context at-tributes as in n-gram models (Brants, 2000;Megyesi, 2001) with their specificity as forbinary features in MaxEnt taggers (Ratna-parkhi, 1996; Hajic?
and Hladk?, 1998).
Pos-sibility of using alternative files defining thetemplates of context attributes eases con-structive critiques of the particular definitionof them.?
The tagger processes XML-formatted textswhere input and output files have the samestructure.
Especially, it can be run on its ownoutput to disambiguate some attributes in cas-cade rather than simultaneously.?
The tagger can be used for any other languagesupplied with morphological analyzer, train-ing data and tagset-dependent configurationfiles.In the following bulk of paper, we shall presentthe features of our tagger in more detail, we shalldiscuss its preliminary results for our Polish tag-ging project, as well as, we shall share remarks onpossible extensions/improvements of the softwarebehavior.Our general feeling is that the tagger as we haveit implemented and configured now for Polish isnot a very practical program: It works very slowand it makes much more mistakes than state-of-the-art taggers.
In fact, the tagger gives its users somuch freedom of manual configuration and feed-back information in the error reports that they getlost.
We hope that much accuracy can be earnedwhen some automatic search for the optimal con-figuration files is implemented.
On the hand, weare still afraid if we have not underfitted with vari-ous conditional independence assumptions, whichrestrain the tagger from seeing sequences of veryspecific tags as something systematic: Especially,each tag attribute is disambiguated probabilisti-cally independently and the program does not al-low for treating tags as atomic entities in proba-bility modeling.
This cannot be overcome with-out a major change in the program and, worse, inthe already complex structure of its configurationfiles.Despite all these drawbacks, we present somestudy of how one can think creatively about tag-ging and how one cannot find a quick break-evenwhen trying to implement too many good guide-lines in one piece.
We report on lots of apparentlytechnical details, hoping that their exposition canhelp see the tagger as something more than a blackbox and identify the sources of its errors.1 Human-readable tags and tagger?sdecisionsBefore we can define the probabilistic model ofour tagger and the scheme of its training, we needto define some pretty abstract entities.
These en-tities are tagger?s disambiguating decisions whoseconditional probability is maximized given the ini-tial state of the text annotation.
The decisions dif-fer from human-readable tags in the corpus.
Toexplain what they are and why they are there, weneed to clarify how the tagger interprets the struc-ture of text annotation in a way which allows forrunning the tagger on its earlier outputs.Figure 1 presents a general scheme of usingthe tagger.
Any plain text (format 0?possiblywith some general XML tags) is first fed throughscript analize.plain.text.plwhich iden-tifies word-like segments and gives them their fullmorphological analysis, i.e.
the list of all possi-ble tags (Morfeusz.pm is a library serving thePolish morphological analyzer, written by our col-league).
After the analysis, the text obtains for-mat 2 which is conserved through multiple calls ofthe tagger (named Cypher.pl).
The only thingwhich happens during these calls is reducing theCorpusio.pmMorfeusz.pmMorfeuszki.pmattribute.counts.A(attribute.weigths.A)conx.attributes.Asurvey.Aanalize.plain.text.pltest.corpus(format 3)(format 0)tag.attributes.fulltag.vtcs.fullCorpusio.pm(format 2)Cypher.pl eCypher.pl ltest.corpus.A.v(format 3)training.corpus(format 3)Cypher.pl otraining.corpus.A.v(format 3)Cypher.pl oCorpusioMorfeuszki.pm(format 2)Figure 1: Structure of tagger running.number of alternative tags initially given by themorphological analysis.
Each tagger call can beused to disambiguate different sets of tag attributesdepending on the given configuration files (such asconx.attributes.
*).In fact, in format 2, each tagged segment(roughly orthographic word) is given two identi-cally formatted lists of potentially ambiguous lin-guistic annotation: the full morphological analysisand the reduced morphological analysis as prunedby the tagger (initially equal to the full morpho-logical analysis).
In format 3, used for trainingand test corpora, there is a third list containing themorphological analysis as pruned by the humanannotators (it shares the same format and it can beonly partially disambiguated).
During any kind ofthe tagger call (training, testing, or tagging newtexts), it is only the tagger-pruned analyses thatare pruned against values of the call-dependenttag attributes.
Full morphological analyses andhuman-pruned analyses remain untouched.
Unfor-tunately, tagger-pruned analyses must be prunedalso in the original training and test data to obtainthe relevant test and training data for the next tag-ger call run in a pipe.Each morphological analysis, full or reduced,appearing in the annotated texts can be ambigu-ous and it has a form of shorthand human-readabletags grouped by lemmas.
Here is an example ofthe full morphological analysis for the Polish wordkurze (dust, cock, hen, hen:Adj):<l>kurz<t>subst:pl:nom.acc:m3<l>kur<t>subst:sg:loc.voc:m2<l>kura<t>subst:sg:dat.loc:f<l>kurzy<t>adj:sg:nom.acc:n1.n2:pos<t>adj:pl:nom.acc:m2.m3.f.n1.n2.p2.p3:pos\nThe tags are lists of values separated by colons (:)and dots (.).
Colons separate tag attributes anddots separate alternative values for the same at-tribute.
When the dot is used for more than oneattribute, the tag means the full Cartesian productof alternatives.
Values of non-applicable attributesare omitted.The omission of attributes is not used in the tag-ger?s internal representation of the reduced mor-phological analysis (called tag list).
Here, eachattribute has its fixed position, and our example offull morphological analysis is transformed one-to-one into list:[[subst,pl,nom.acc,m3,-,-,...,kurz],[subst,sg,loc.voc,m2,-,-,...,kur],[subst,sg,dat.loc,f,-,-,...,kura],[adj,sg,nom.acc,n1.n2,-,pos,...,kurzy],[adj,pl,nom.acc,m2.m3.f.n1.n2.p2.p3,-,pos,...,kurzy]]This transformation is controlled by filetag.attributes.
* which specifies namesof consequent tag attributes and enumerations oftheir values<POS> conj prep subst fin ...<number> sg pl<case> nom gen dat acc inst loc voc...(The last tag attribute is always lemma, which isnot enumerative but formally useful as a kind oftag attribute.
)Human-reduced analyses in the training data areallowed to be ambiguous, so the tagger could learnnot to disambiguate in some contexts.
In this case,one needs to differentiate tagger?s lists of disjunc-tive disambiguation decisions (decision lists) fromthe tagger-pruned tag lists.
In fact, each deci-sion list is a very simple functions of the tagger-reduced tag list.
E.g., if it is specified that onlynumber and case can be disambiguated in a giventagger call, the list of decisions for our continuedexample is:[[LEAVE,LEAVE,LEAVE,LEAVE,...,LEAVE],[LEAVE,sg,nom,LEAVE,...,LEAVE],[LEAVE,sg,dat,LEAVE,...,LEAVE],[LEAVE,sg,acc,LEAVE,...,LEAVE],[LEAVE,sg,loc,LEAVE,...,LEAVE],[LEAVE,sg,voc,LEAVE,...,LEAVE],[LEAVE,pl,nom,LEAVE,...,LEAVE],[LEAVE,pl,acc,LEAVE,...,LEAVE]]Decision list contains all choices of atomic valuesof disambiguated attributes plus special decision[LEAVE,LEAVE,...,LEAVE].When a decision is eventually selected from thelist, the new reduced tag list becomes former re-duced tag list unified with the decision against alltag attributes.
LEAVE and any value X unifiesdownto X.
Atomic value Y and any value X uni-fies downto Y if X contains Y and downto emptyset otherwise.
For learning and testing purposes,the right decision to be expected from the taggeris computed as follows: When any manually dis-ambiguated tag attribute has just one value, thesame attribute value is chosen for the right deci-sion.
When the attribute is ambiguous, same at-tribute for the right decision equals to LEAVE.The differentiation between all the tag attributesand the disambiguated tag attributes was moti-vated by the occurrence of tag lists close to largeCartesian products: Some Polish participle formscontain alternative values for so many tag at-tributes, that one would get > 90 disjunctive de-cisions for one word if disambiguating all at-tributes simultaneously.
Sequential disambigua-tion reduces polynomial complexity downto linearone and may enable better probability estimation.Additionally, disambiguation of only 4 out of 14identified morphosyntactic tag attributes usuallysuffices in Polish for the full tag disambiguation.2 Probability modelingThe general problem of probabilistic modeling ofdecision processes is simple to phrase: If we wanta particular decision T to depend on all its con-text, we find that pairs (decision, context state) areunique events and we cannot generalize beyondthem.
If we group the contexts into too few equiv-alence classes and make the same decision for allcontexts in a class, then we lose sensitivity of thedecisions.
Finding the optimal equivalence classescan need large amounts of domain knowledge orextensive search.
Probability modeling gives addi-tional hint: Define several not so sophisticated par-titions of contexts into equivalence classes.
Findthe joint probabilities of pairs (decision, equiva-lence class of i-th context partition).
From these,compute the joint probability of tuples (decision,equivalence classes for all context partitions) as-suming some regularity of this distribution.
De-pending on the size of available training data, itcan give better results than direct estimation of thetuple distribution.In stochastic text tagging by our tagger, there isa string of decisions for successive word segments(text positions) rather than one decision.
LetTi be the tagger?s decision at text position i,i ?
I .
The admissible disjunctive choices forTi are tij , j ?
Ji.
Both Ti and tij are vectorsof values Tit and tijt for disambiguated tagattributes t, t ?
T (tijt equals to an atomictag attribute value or LEAVE).
On the otherhand, there is a set of random variables, calledcontext attributes Citc, c ?
Ct, which take valuesidentifying disjunctive equivalence classes ofsuccessive context partitions.
During tagging,context attribute values are computed usingscheme: Citc = ftc(?, i, {Ti?t?}(i?,t?
)?P(i,t)),where ?
represents the whole input text(list of word-segments and their input taglists).
P(i, t) = ({i ?
k, ..., i ?
1} ?
T ) ?
({i} ?
({0, ..., t ?
1} ?
T )).
ftc are fast-computable functions taking repeatable values wedescribe later.For a formulated probability modelP(Ti = ?
| {Citc = ?
}t?T ,c?Ct), the taggeruses Viterbi search to find the optimal string ofdecisions tiji , i ?
I , maximizing product?i?IP (Ti = tiji|{Citc = ftc(?, i,{ti?ji?
t?}(i?,t?
)?P(i,t))}t?T ,c?Ct).
(Viterbi search is done as for a non-stationary hid-den Markov model of order k).
Direct estimationof P(Ti = ?
| {Citc = ?
}t?T ,c?Ct)is assumed un-feasible.
The tagger approximates it asP(Ti = tij| {Citc = ctc}t?T ,c?Ct)=?t?TP (T?t = tijt)?c?CtP (C?tc = ctc|T?t = tijt)?j?
?Ji?t?TP (T?t = tij?t)?c?CtP (C?tc = ctc|T?t = tij?t),(1)where numerical values of P (T?t = ?
), P (C?tc =?
|T?t = ? )
do not depend explicitly on i.If the decision lists were Cartesian products:?j?Ji ?t?T tijt = ?t?T?j?Ji tijt, then approx-imation (1) would be equivalent to assuming that(i) variables {Citc}c?C are independent given Tit(naive Bayes), and (ii) given all {Citc}t?T ,c?C ,variables {Tit}t?T are also independent (compareHajic?
and Hladk?
(1998)).Estimation of probabilities P (T?t = ?
),P (C?tc = ?
|T?t = ? )
is done using formulaP (T?t = tijt|C?tc = ctc) =# of positions i such that Tit = tijt i Citc = ctc# of positions i such that Citc = ctc,where the counts of positions are faked by smooth-ing procedure described in the next section.3 Definition of context partitionsFunctions ftc, defining the context partitions, arespecified in file conx.attributes.*.
The fileis merely read by the tagger and it can be pre-pared by hand.
Table 1 shows an example of thisfile.
First column is the name of some tag at-tribute, say t-th one.
Then, the second column isa string defining ftc.
The string defining ftc re-sembles definition of a decision tree with identi-cal structure of all branches.
It represents a suc-cession of tests and variable instantiations evalu-ated at each text position with decision ambigu-ous for t-th tag attribute.
Larger and larger pre-fixes of the string are taken into account as longas no false condition is demanded or no instanti-ation results in disallowed prefixes.
(Smoothingprocedure provides a list of allowed instantiatedprefixes, which is substantially smaller than thelist of all syntactically correct instantiations).
Thelongest achieved prefix with instantiated variablesis returned as ftc(?, i, {Ti?t?}(i?,t?
)?P(i,t)).The meaning of currently recognized com-mands in the string defining ftc is such:?
rp:INTEGER_NUMBER: sets an auxiliarytext position as current text position plusINTEGER_NUMBER.?
wd:SOME_STRING: checks if thesegment at auxiliary position equals toSOME_STRING.?
ac:, ex:, al:, my: followedby NAME_OF_A_TAG_ATTR: andSOME_STRING: check if SOME_STRINGfor tag attribute NAME_OF_A_TAG_ATTRat auxiliary position is: the alternative ofavailable values (ac:), one of availablevalues (ex:), the only available value (al:),the value of decision chosen by the tagger(my:).?
<>: and <0>: are variables replaced for de-manded kind of entity with its value at theauxiliary text position (<>:) or at the currentposition (<0>:).For any definition of ftc, such asrp:-1:wd:nie:rp:0:ac:<POS>:<>:ac:<lemma>:<>, and each of its maximally longvalues, such as C:rp:-1:wd:nie:rp:0:ac:<POS>:inf.impt:ac:<lemma>:is?c?.is?cic?, there is a list prefixes representing lessand less specific information on the context:C:rp:-1:wd:nie:rp:0:ac:<POS>:inf.impt:ac:<lemma>:is?c?.is?cic?, C:rp:-1:wd:nie:rp:0:ac:<POS>:inf.impt,C:rp:-1:wd:nie, C:.
For each of theseprefixes, except empty one?C, there is exactlyone prefix immediately shorter.
Basing on thisproperty, we have implemented the followingsmoothing of ftc values:1.
During learning, the alphabet of allowed in-stantiated prefixes is not closed, so for eachftc, counts of all its maximally long valuesare collected.2.
Each ftc value passes all its counts to its im-mediate prefix if it was seen less than thresh-old (3 times).3.
The alphabet of allowed values of ftc is fixedas the set of all prefixes of ftc values currentlycounted positively.4.
For each ftc value in the allowed alphabet, itis faked it was also seen (once) with specialdecision value Tit =SMOOTH.5.
During tagging, ftc yields the longest al-lowed and matching instantiated value ctc.If ctc has no positive count with particularasked decision value Tit = tijt, tijt is treatedas it were SMOOTH.In the adopted disambiguation scheme, theorder k of nonstationary Markov model to beused in Viterbi search is the negative of min-imal argument of rp: commands followed bymy: command.
Thanks to that, given fileconx.attributes.
*, our tagger identifies kautomatically and accepts any value of k.4 First resultsIn this section, we would like to present sev-eral scores of our tagger on processing Polishtexts.
The scores should be considered prelimi-nary due to several causes: (i) we believe that ourconx.attributes.
* file is not optimal yet,(ii) improving the tagger?s code continually, wehave had too little time to test its behavior on largetraining data sufficiently, (iii) the training corpusis still in a mix of two different degrees of dis-ambiguation of gender values, (iv) morphologi-cal analyzer contains no guesser and unrecognizedstrings are not considered ambiguous by the tag-ger.In our current Polish annotation scheme, tagsconsist of the following attributes: POS, num-ber, case, gender, person, degree, aspect, nega-tion, depreciation, accommodability, accentabil-ity, post-prepositionality, vocalicity, punctuation,lemma (Wolin?ski and Przepi?rkowski, 2001).
Wehave observed that for given morphological anal-ysis, any atomic tag is almost always identifiedgiven its values for just 4 attributes: POS, number,case, gender, lemma, and decided to disambiguatedirectly only these attributes.Testing several simple versions ofconx.attributes.
* file, we have ob-served that smaller error on POS is obtainedwhen POS is disambiguated simultaneouslywith number, case, and gender while lemmais to be disambiguated afterwards.1 The bestconx.attributes.
* file we have found sofar is presented in table 1.
The general structure ofthis model of ftcs resembles a product of trigrammodels for each of the disambiguated attributes.Some modification of conx.attributes.
*file might be adding POS values at positions-1,0,+1 to ftc values for number, case, and gender.Theoretically, this modification could be moresensitive to a regular grammar syntax of simplephrase structures.
Strangely, this modificationyields twice as high error rate as the originalconx.attributes.
* file.Observed error rates for ftcs as in table 1 are:?
Training data 01k (784 word segments)Error rate: (all tokens) (ambiguous)Overall 0.25 0.45on POS 0.06 0.10on number 0.06 0.10on case 0.16 0.28on gender 0.13 0.22?
Training data 10k (8118 word segments)1Some major ambiguity left for lemma disambiguation inthis case are homonymous present tense forms for musiec?
?
?to have?
and music?
?
?to compel?.<POS> ac:<POS>:<>:ac:<lemma>:<><POS> ac:<POS>:<>:rp:-1:my:<POS>:ac:<lemma>:<><POS> my:<POS>:<>:rp:+1:ac:<POS>:ac:<lemma>:<>#<number> ac:<number>:<>:rp:-1:my:<number>:<><number> ac:<number>:<>:rp:+1:ac:<number>:<>#<case> rp:-1:my:<case>:<><case> rp:+1:ac:<case>:<>#<gender> ac:<gender>:<>:rp:-1:my:<gender>:<><gender> ac:<gender>:<>:rp:+1:ac:<gender>:<>Table 1: Preliminary conx.attributes.
* file used for Polish.Error rate: (all tokens) (ambiguous)Overall 0.23 0.41on POS 0.06 0.11on number 0.06 0.10on case 0.14 0.24on gender 0.11 0.19?
Training data 50k (41208 word segments)Error rate: (all tokens) (ambiguous)Overall 0.20 0.34on POS 0.05 0.08on number 0.04 0.08on case 0.12 0.20on gender 0.07 0.13?
Full training data (558224 word segments)Error rate: (all tokens) (ambiguous)Overall 0.22 0.35on POS 0.05 0.09on number 0.05 0.08on case 0.12 0.21on gender 0.10 0.17All trained models were tested on test data 5k(4117 word segments).The error rate for full training data is larger thanfor 50k training data, especially on gender.
This isprobably due to the inhomogenous manual anno-tation of gender values we had already mentioned.Comparing with publications on similar tasks, ourminimal overall error rate is twice as big as forSlovene (D?eroski et al, 2000) and 3 times as bigas for Czech (Hajic?
and Hladk?, 1998).5 Comments and possible extensionsIt has been remarked that HMM trigram tag-gers using single multivalued context features canperform better and run faster than MaxEnt tag-gers trying to combine conditional probabilitiesfor a multitude of binary features (Brants, 2000;Megyesi, 2001), even for large structured tagsetsand Slavonic free word-order (Hajic?
et al, 2001).Our intention was to try out a hybrid approach inwhich a small number of multivalued context fea-tures is used.
We have thought it can help solv-ing data sparseness problems.
Now we do notknow exactly what is the most important causeof our present high error rate: inhomogenous Pol-ish corpus annotation, deficiencies of morpholog-ical analysis, low efficiency of the manual modelsearch, or the assumption of probabilistic indepen-dence of different tag attributes (used succesfullyby Hajic?
and Hladk?
(1998)).
Due to the last thing,we cannot either simulate a simple trigram taggerinterpreting tags as single entities.So as to overcome the manual model search, wehave thought of writing another program whichcould read the error reports of our tagger andsearch for the optimal conx.attributes.
*file in a fixed space of context functions ftcs.For binary context functions, Hajic?
and Hladk?
(1998) truncated the length of their definitions andsearched extensively through a very large finitespace of them.
We think that optimal ftc defini-tions may be so long that genetic algorithm searchthrough the infinite space of ftcs may be a betterapproach.One might also search for more powerful se-mantics of context functions ftc.
As long as ftcsdo not depend on too distant previous tagger de-cisions (increase the order k of Markov model),or do not need too much computation on theirown, one can freely decide what functions of mor-phologically analysed text ftcs are.
Better disam-biguation of Slavic nominative/accusative ambi-guity may need testing if there is a possible ver-bal form in the left/right context of a word (Hajic?and Hladk?, 1998).
Disambiguation of complexgender ambiguities may need testing unifiabilityof the attribute values.Our idea of smoothing ftc values resemblesHMM reconstruction algorithm proposed by Shal-izi, Shalizi and Crutchfield (2003), which uses ad-ditionally Kolmogorov-Smirnov test to check ifextending the context depth is necessary for ad-equate prediction.
Similarly, we could replace ftcvalue ctc by its prefix c?tc not only when ctc israre but also when P (T?t = ?
|C?tc = ctc) does notdiffer significantly from P (T?t = ?
|C?tc = c?tc).Such technique could decrease memory usage andslightly speed up the tagger.Gra?a, Alonso and Vilares (2002) proposed amodification of Viterbi search for unknown textsegmentation as a common solution for disam-biguation of segmentation and tagging.
Our Pol-ish tagset introduces some very rare ambiguitiesof segmentation but we have consciously decidednot to touch the segmentation since extending themodification of Viterbi search to multiattributetags with independent tag attributes needs a verydifferent structure of training corpus and proba-bility estimation.
Such tool would be formallycapable of performing functionality of a cascadephrase parser and would be a good subject of an-other project.The research reported here was partly supportedby the KBN grant 8 T11C 043 20.ReferencesThorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the Sixth AppliedNatural Language Processing Conference (ANLP-2000).
Seattle, WA, USA.Chris Brew.
1995.
Stochastic HPSG.
In Proceedingsof the 7th Conference of the European Chapter ofthe Association for Computational Linguistics.Sa?o D?eroski, Toma?
Erjavec, and Jakub Zavrel.2000.
Morphosyntactic Tagging of Slovene:Evaluating PoS Taggers and Tagsets.
In Sec-ond International Conference on Language Re-sources and Evaluation, LREC?00, pages 1099?1104, Paris.
ELRA.
http://nl.ijs.si/et/Bib/LREC00/lrec-tag-www/.Jorge Gra?a, Miguel A. Alonso, and Manuel Vilares.2002.
A common solution for tokenization and part-of-speech tagging.
In Proceedings of Text, Speechand Dialogue 2002.
Lecture Notes in Artificial Intel-ligence 2448.
Springer Verlag.Jan Hajic?
and Barbora Hladk?.
1998.
Tagging inflec-tive languages: Prediction of morphological cate-gories for a rich, structured tagset.
In Proceedingsof COLING-ACL Conference.
Montr?al.Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, andVladim?r Petkevic?.
2001.
Serial combination ofrules and statistics: A case study in czech tagging.In Proceedings of ACL?01.
Toulouse, France.Jan Hajic?.
2000.
Morphological tagging: Data vs. dic-tionaries.
In Proceedings of ANLP-NAACL Confer-ence.
Seattle.Christopher D. Manning and Hinrich Sch?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press.Be?ta Megyesi.
2001.
Comparing data-driven learningalgorithms for PoS tagging of Swedish.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2001).
CarnegieMellon University, Pittsburgh, PA, USA.Adam Przepi?rkowski, Anna Kups?c?, Ma?gorzataMarciniak, and Agnieszka Mykowiecka.
2002.
For-malny opis je?zyka polskiego: Teoria i implemen-tacja.
Akademicka Oficyna Wydawnicza EXIT,Warszawa.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedings ofthe First Conference on Empirical Methods in Nat-ural Language Processing (EMNLP-1996).
Univer-sity of Pennsylvania, PA, USA.Cosma Rohilla Shalizi, Kristina Lisa Shalizi, andJames P. Crutchfield.
2003.
An algorithm forpattern discovery in time series.
http://www.arxiv.org/abs/cs.LG/0210025.Marcin Wolin?ski and Adam Przepi?rkowski.
2001.Projekt anotacji morfosynktaktycznej korpusuje?zyka polskiego.
Technical Report 938, Institute ofComputer Science, Polish Academy of Sciences.
