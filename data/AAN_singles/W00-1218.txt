A Clustering Algorithm-for Chinese Adjectives and Nouns 1Yang Wen ~, Chunfa Yuan ~, Changning Huang 2~State Key Laboratory of Intelligent Technology and SystemDeptartment ofComputer Science & Technology, Tsinghua University,Beijing 100084, P.R.C.2Microsoft Research, ChinaEmail: ycf@~1000e c~ t~in~hua edu cnKey Words:?
bidirectional hierarchical clustering,collocations, minimum description length,collocational degree, revisional distanceAbstractThis paper proposes a bidirctionalhierarchical clustering algorithm forsimultaneously clustering words of differentparts of speech based on collocations.
Thealgorithm is composed of cycles of twokinds of alternate clustering processes.
Weconstruct an objective function based onMinimum Description Length.
To.
partlysolve the problem caused by sparse data twoconcepts of collocational degree andrevisional distance are presented.1 IntroductionRecently research on the compositionalframes (classification and collocafionalrelationship of words) for Chinese words hasbeen described in Ji et al (1996)\[1\], Ji(1997)\[2\].
The objective of their work is toobtain the clusters of words of different partsof speech and to derive the coUocationalrelationship between different clusters fromthe collocafional relationship between wordsof different categories.There are two ways to construct theclusters: One is to get clusters fromthesaurus classified manually by linguists.But the fact is that words with the samemeanings do not always have the sameability of collocating with other words.
Themethod isn't fit for the NLP problems underour consideration.
Another way is to getclusters automatically by computing on thedistribution environments of words based onstatistical method.
The distributionenvironment of a word is the set of words ofother parts of speech that can be collocatedwith it.
We employ the second method in ourwork.Previous research usually gets theclusters of Words of a certain part of speechbased on their distribution environments.
ButSupported by National Natural Science Foundation of China (69'7?3031 ) and "973" Project (G1998030507).124we accept he assumption that the clusteringprocesses of words of different parts ofspeech are inherently related.
For example,having collocations between Chineseadjectives and nouns and if we take on nounsas entities and adjectives as features ofnouns' distribution environments, we canobtain clusters of nouns and vice versa.
Thekey of the relationship of the two clusteringprocesses is that they use the samecollocations.
Therefore we consider thequestion of clustering the nouns andadjectives imultaneously.
Li's work showsthat they optimize the clustering resultsbased on this viewpoint (Li et al, 1997)\[3\].But they don't explain how to get initialclusters and their scale of problem is toosmall.In this paper, we propose an algorithmnamed bidirectional hierarchical c ustering toattempt answering the question.2 Concepts2.1 Problem DescriptionOur problem can be described as follows:given the set of adjectives A, the set of nounsN and the collocation instances, our systemwill construct a partition P~ over N and apartition Pa over A that respectively containsets of nouns and sets of adjectives.
Andboth partitions meet he condition that wordsin the same set (called cluster) have similarsemantic distribution environment.2.2 Partitions and ClustersLet S be a set, S~ c S(i = 1,2,...,n).
IfYs ={ S~ } satisfies thatn(1)US, =si=1(2) S, NSj = O, Vi, j = 1,2,...n,i ~ jThen Ps is a partition over S.In this paper, we call A i ~Pa an"adjective cluster" and Ni ~P~v a "nouncluster".
And we want to obtain thecomposition of partitions < PA, PN > as theclustering remit.2.3 Distance between ClustersIn order to measure the distance betweenclusters of the same part of speech, we usethe following equations:1 \[~'\["1~/I (1) disa(Ai,Aj)andlie, U%l (2)where O~ is the distributionenvironment of ~ and is make up of nounswhich can be collocated withdistribution environmentcomposed of adjectivescollocated with N i .
~ iA,.
~ is theof N~ and iswhich can beandW s followsimilar definitions.
This distance is a kind ofEuchdean distance.1252.4 Colloeational DegreeSince redundant collocations might becreated during clustering, the concept"collocational'degree" is used to measure ~ecollocational relationship between a clusterand its distribution environment.
ThecoUocational degree is defined as the ratio ofthe existing collocation instances betweenthe cluster and its distribution envffonmentto all possible collocations generated bythem.
Thus,deg~ = I(a?
I a 4,?
?,,a?
c}l1411-,I (3)anddegN~ - I ( -v  I- N,,v  ,nV, sC}l (4)IN, tillwhere C is the set of all existing instances.2.5 Redundant RatioAfter we get the collocational degree ofa cluster, redundant ratio (marked as r) iscalculated to measure the whole performanceof the clustering result.
We define theredundant ratio as 1 minus the ratio of allexisting instances to all possible colloca-tions generated by all clusters (includingnouns and adjectives) and their distributionenvironments.
Sor is calculated asr : 1- 21clEIA, II*,I+EIN, II'e,I (5)i i3 A Bidirectional HierarchicalClustering AlgorithmUsually a hierarchical clusteringalgorithm \[7\] constructs a clustering "tree"by combining small clusters into large onesor (lividing large clusters into small ones.The bidirectional hierarchical clusteringalgorithm proposed by us is composed oftwo kinds of alternate clustering processes.The algorithm flow is described asfollows:1)Initially, regard every noun andadjective ach as a cluster.
Calculatethe distances between clusters of thesame part of speech.2) Suppose without loss of generalitythat we choose to cluster nouns first.Select two noun clusters N, & N sof the minimum distance andintegrate them into a new one N~'.J) Calculate the collocational degree ofthe new cluster.
Adjust the sequencenumbers of the original clusters andthe relational information of adjectiveclusters.4) Calculate the distances between thenew cluster and other clusters.5) Repeat from step 2) to 4) until thesatisfaction of certain condition.
Forexample, the number of the clustershaas decreased to certain amount.
26) Similarly, we can follow the samesteps from 2) to 5) for constructingadjective clusters, completing onecycle, of clustering processes of nounsand adjectives.7) Repeat from step 2) to 6) until the2 In this paper, we set he proportion is 20%.126objective function 3 reaches theminimum value.One advantage of this algorithm is that:when two clusters of nouns have similardistribution environments, "they might beclassified into one cluster.
This informationcan be delivered to the clusters of adjectivesthat respectively collocate with them by theclustering process of nouns.
Thus theseclusters of adjectives have great possibilityto be combined into one cluster, while theordinary hierarchical clustering algorithmcan not do it.4 An Objective Function Based onMDLThe objective function is designed tocontrol the processes of clustering wordsbased on the Minimum Description Length(MDL) principle.
According to MDL, thebest probability model for a given set of datais a model that uses the shortest code lengthfor encoding the model itself and the givendata relative to it \[4\] \[5\].
We regard theclusters as the model for the collocations ofadjectives and nouns.
The objective functionis defined as the sum of the code length forthe model ("model description length") andthat for the data ("data description length").When the clustering result minimises theobjective function, the bidirectionalprocesses hould be stopped and the result isthe best probable one.
The objective functionbased on MDL trade-offs between thesimplicity of a model and its accuracy infitting to the data, which are respectivelyquantified by the model description lengthand the data description length.3 Described later in section 4.The following are the formulas tocalculate the objective function L:L = L,,,od + L,~ t (6)Lad is the model description lengthcalculated asL,~kA1 1 *~'1 1= -X-H--log2 -~-- ~.~--  log 2 k~r+l (7)i=1 ~A tt'A i=1 ~/q"= log2(kAk~) + 1Where k A and k N respectively denotethe number of clusters of adjectives andnouns.
"+1" means that the algorithm needsone bit to indicate whether the collocationalrelationship between the two clusters exists.L,~ t is composed of the data descriptionlength of adjectives and that of nouns,namely(8)And the two types of data descriptionlength are calculated as follows\[?,1LeQt (A) - - -  - -~- - - '~ log2 1.__.
j=.
1411Nkl(9)Vj,~j ~ ~, and ~ j~N,L o,(N) 1,__, k k.(10)~~log  2 1Vj, ~ j ~.W, and v i ~ A~5 Our ExperimentWe take the words and collocations1276 Discussions6.1 Rivisional DistanceWhen we combine clusters into a newcluster, their distribution environments willbe combined as well.
The combination ofclusters and their distribution environmentsmight very likely generate redundantcollocations that are not listed in thethesaurus.
With the word clusteringprocesses going on, there might be more andmore redundant collocations.
They willobviously affect the accuracy of thedistances between clusters.
When calculatingthe distances, the redundant collocationsmust be considered.
So the question is howto revise the distance quation.
Notice thatthe collocational degree defined in the abovemeasures the collocational relationshipgathered in Ni's Thesaurus \[6\] to test ouralgorithm.
From Ni's thesaurus, we obtain2,569 adjectives, 4,536 nouns and 37,346collocations between adjectives and nouns.Table 1 shows results of using 5different revisional distance formulasdiscussed in the next section.
Because thelength of this paper is limited, we only givesome examples (10 clusters for each part ofspeech) of clusters in section 8.
We can seethat the redundant ratio obviously decreasesby using the revisional distance, and theresult that has the lowest redundant ratiocorresponds of the minimum value of theobjective function.
By human evaluation,most clusters contain the words that havesimilar meanings and distributionenvironments.
So our algorithm proves to beeffective for word clustering based oncollocations.Table 1: Results of different revisional distancesRevisional distance k~ k u L rNot used 409 550 20.
067 99.
01%dis' = -deg lnd is  397 610 20.
082 86.
96%dis' = d is /deg 383 595 20.
002 78.
78%dis' = d is /~-~ 373 586 20.
017 80.
39%dis" = -d!s ln  deg 395 557 20.
007 80.
08%However, the redundant ratio is still verylarge.
The main cause is that existingbetween a cluster and its distributionenvironment.
Obviously under the sameinstances are too sparse, covering only0.32% of all possible collocations.
Soanother advantage of our algorithm is thatwe can acquire many new reasonablecollocations not gathered in the thesaurus, ffwe add the new collocations into initialthesaurus and execute the algorithm on newdata set, the performance will have greatpotential to improve.
It is further work thatcan be carried out in the future.distance, clusters having higher coUocationaldegree have more higher similarity betweeneach other (because they have more actualcollocations) than those having lowercollocational degree.
So the collocationaldegree can be used to revise the distanceequations.There are two problems that should-beconsidered when we design the revisionaldistance quations.
The first one is to convert128the collocational degrees of two clusters intoone collocational degree as the revisionalfactor for distance equations.
It is theaverage collocational degree, marked asdeg, calculated bydeg A -anddeg I ,I141 + deg A,I ,114ua>,l (II)deg N, Iv, llN, l + deg N, Iv, IN IIn fact it is the collocational degree ofthe new cluster into which if we assumecombining the two original clusters.The second problem is that the revjsonaldistance quations hould keep coherent ofmonotonicity with the original distance.
Itmeans that under the same average collo-cational degree, the revional distance shouldkeep the same (or opposite) monotonicitywith the original distance, and under thesame original distance, the revional distanceshould keep the same (or opposite)monotonicity with the average collocationaldegree.In this paper, four simple revisionaldistance equations are presented based onconsideration of the upper two problems.They are:a)d is '=-deg lnd isdis b) dis' -deg?
dis c) dis' -/degd) dis' = -d i s  In degWhere dis'  denotes the revionaldistance and dis denotes the originaldistance.From the comparison of the upperdifferent results (shown in Table 1), we candraw the conclusion that using revisonaldistance equations can increase theclustering accuracy remarkably.6.2 Determinant of Objective Function'sMinimum ValueThe clustering algorithm terminateswhen the objective function is minimized.As a result it is very important to find out thefunction's minimum value.
After analyzingthe objective function, we find that itnormally monotonically declines withclustering processes going on until it getsminimized.
At the beginning, there are alarge number of clusters with only oneelement in each of them.
So the modeldescription length is quite large while thedata description length is quite small.Because the clustering process is hierarchical,every time when the combination occurs thenumber of clusters will decrease by one withthe model description length's decreasing aswell.
At the same time the number of acertain cluster's elements will increase byone with the data description length'sincrement as well.
However, the decrementis larger than the increment and it is gettingsmaller while the increment is getting larger.In this way, the objective function declinesuntil the objective function reach its129Figure 1: Values of the Objective Functionsaddition, the clustering algorithm may helpto find new collocations that are not in thethesaurus.
This algodthm can also beextended to other collocation models, suchas verb-noun collocations.272523211917minimum value.
I f  we continue to executethe algorithm, we will see that the value ofthe objective function rises very fast like asis shown in Figure 1.150 1000 2000 3000 4000 5000 6000 7000 8000quantity of c lustersTherefore we choose a fairly simpleway to avoid the appearance of the localoptimum: When there are two consecutiveincreases in the objective function duringone clustering process, stop the process andstart another one.
When two consecutiveclustering processes are stopped due to thesame reason, we assume that we have got theminimum value and stop the wholeclusterilag process.
In our future work wewill try to fred a better way to determine theminimum value of the objective function.7 Conclusion & Future WorkIn this paper we have presented abidirctional hierarchical clustering algorithmof simultaneously clustering Chineseadjectives and nouns based on theircollocations.
Our preliminary experimentsshow that it can distinguish different wordsby their distribution environments.
InOur future work includes:1) Because the sparsity of collocationsis a main factor of affecting theword clustering accuracy, we canuse the clustering results to discovernew data and enrich the thesaurus.2) As there are yet no adjustments othe hierarchical clustering results,we are considering using someiterative algorithm, such as K-means algofithm~ to  optimise theclustering results.8 Attachment (Examples)We give 10 clusters of each part ofspeech clustered by our algorithm (usingrevisional distance formula b) as follows:8.1 Chinese Adjective dusters (10 of 383)130A3 ~ ~ ~f~ ~ ~ ~A4 ~ ~ i~ ~ ~ ~A5 ~ ~O ~ -F~ ~ ~A~ ~ '~ ~, ~ ,~ ~ ~~ ~'~A9 ~ ~ ~.
~A~0 ~,~ E~ ~ ~ ~ ~ ~8.
2 Chinese noun clusters (I0 of 595)N3 J~,~ .~_,i,E,)~, ~/~/~ ~IE~:::~T,/~ " f -~:~N6 ~.
:~T?
:/~ - }t~:t.IL "~i:~ ~..'\]:~ - 1~ ~:~:N7 ~ ~,~ ~ ~ ~ ~N8 ~'1~ ~ r~ ~l,~ ~ 'l~r~N9 ~.,k, ~i~:~::.
~:--~: ~:~:Communications of COCIPS 6(1): P25-P33, 1996\[2\] Donghong Ji, Computational Researchon Issues of Lexical Semantics, Post-doctoral Research Report, TsinghuaUniversity, P14-P26, 1997\[3\] Juanzi Li et al, Two-dimensionalClustering Based on CompositionalExamples, Language Engineering,P164-P169, Tsinghua University Press,1997\[4\] Hang Li & Naoki Abe, ClusteringWords with the MDL Principle, cmp-lg/9605014 v2, 1996\[5\] Wei Xu, The Study of Syntax-Semantics Integrated Chinese Parsing,Thesis for the Degree of Master inComputer Science, Tsinghua University,1997\[6\] Wenjie Ni et al, Modem ChineseThesaurus, China People Press,.
1984\[7\] Zhaoqi Bian et al, Pattern gecbgnition,Tsinghua University Press, 1997References\[1\] Donghong Ji & Changning Huang, ASemantic Composition Model forChinese Noun and Adjective,131
