Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 138?148,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCreating Causal Embeddings for Question Answeringwith Minimal SupervisionRebecca Sharp, Mihai Surdeanu, Peter Jansen, Peter Clark, and Michael HammondUniversity of Arizona, Allen Institute for Artificial Intelligence{bsharp, msurdeanu, pajansen, hammond}@email.arizona.edu, PeterC@allenai.orgAbstractA common model for question answering(QA) is that a good answer is one that isclosely related to the question, where re-latedness is often determined using general-purpose lexical models such as word embed-dings.
We argue that a better approach is tolook for answers that are related to the ques-tion in a relevant way, according to the infor-mation need of the question, which may bedetermined through task-specific embeddings.With causality as a use case, we implementthis insight in three steps.
First, we generatecausal embeddings cost-effectively by boot-strapping cause-effect pairs extracted fromfree text using a small set of seed patterns.Second, we train dedicated embeddings overthis data, by using task-specific contexts, i.e.,the context of a cause is its effect.
Finally, weextend a state-of-the-art reranking approachfor QA to incorporate these causal embed-dings.
We evaluate the causal embeddingmodels both directly with a casual implicationtask, and indirectly, in a downstream causalQA task using data from Yahoo!
Answers.
Weshow that explicitly modeling causality im-proves performance in both tasks.
In the QAtask our best model achieves 37.3% P@1, sig-nificantly outperforming a strong baseline by7.7% (relative).1 IntroductionQuestion answering (QA), i.e., finding short answersto natural language questions, is one of the most im-portant but challenging tasks on the road towardsnatural language understanding (Etzioni, 2011).
Acommon approach for QA is to prefer answers thatare closely related to the question, where relatednessis often determined using lexical semantic modelssuch as word embeddings (Yih et al, 2013; Jansenet al, 2014; Fried et al, 2015).
While appealing forits robustness to natural language variation, this one-size-fits-all approach does not take into account thewide range of distinct question types that can appearin any given question set, and that are best addressedindividually (Chu-Carroll et al, 2004; Ferrucci etal., 2010; Clark et al, 2013).Given the variety of question types, we suggestthat a better approach is to look for answers that arerelated to the question through the appropriate re-lation, e.g., a causal question should have a cause-effect relation with its answer.
If we adopt thisview, and continue to work with embeddings as amechanism for assessing relationship, this raises akey question: how do we train and use task-specificembeddings cost-effectively?
Using causality as ause case, we answer this question with a frameworkfor producing causal word embeddings with mini-mal supervision, and a demonstration that such task-specific embeddings significantly benefit causal QA.In particular, the contributions of this work are:(1) A methodology for generating causal embed-dings cost-effectively by bootstrapping cause-effectpairs extracted from free text using a small set ofseed patterns, e.g., X causes Y.
We then train dedi-cated embedding (as well as two other distributionalsimilarity) models over this data.
Levy and Gold-berg (2014) have modified the algorithm of Mikolovet al (2013) to use an arbitrary, rather than linear,context.
Here we make this context task-specific,138i.e., the context of a cause is its effect.
Further, tomitigate sparsity and noise, our models are bidirec-tional, and noise aware (by incorporating the likeli-hood of noise in the training process).
(2) The insight that QA benefits from task-specificembeddings.
We implement a QA system that usesthe above causal embeddings to answer questionsand demonstrate that they significantly improve per-formance over a strong baseline.
Further, we showthat causal embeddings encode complementary in-formation to vanilla embeddings, even when trainedfrom the same knowledge resources.
(3) An analysis of direct vs. indirect evaluationsfor task-specific word embeddings.
We evaluate ourcausal models both directly, in terms of measuringtheir capacity to rank causally-related word pairsover word pairs of other relations, as well as indi-rectly in the downstream causal QA task.
In bothtasks, our analysis indicates that including causalmodels significantly improves performance.
How-ever, from the direct evaluation, it is difficult toestimate which models will perform best in real-world tasks.
Our analysis re-enforces recent obser-vations about the limitations of word similarity eval-uations (Faruqui et al, 2016): we show that theyhave limited coverage and may align poorly withreal-world tasks.2 Related WorkAddressing the need for specialized solving meth-ods in QA, Oh et.
al (2013) incorporate a dedicatedcausal component into their system, and note that itimproves the overall performance.
However, theirmodel is limited by the need for lexical overlap be-tween a causal construction found in their knowl-edge base and the question itself.
Here, we develop acausal QA component that exploits specialized wordembeddings to gain robustness to lexical variation.There has been a vast body of work whichdemonstrates that word embeddings derivedfrom distributional similarity are useful in manytasks, including question answering ?
see interalia (Fried et al, 2015; Yih et al, 2013).
However,Levy and Goldberg (2015) note that there are lim-itations on the type of semantic knowledge whichis encoded in these general-purpose similarityembeddings.
Therefore, here we build customizedtask-specific embeddings for causal QA.Customized embeddings have been created fora variety of tasks, including semantic role la-beling (FitzGerald et al, 2015; Woodsend andLapata, 2015), and binary relation extraction(Riedel et al, 2013).
Similar to Riedel et al, wetrain embeddings customized for specific relations,but we bootstrap training data using minimal super-vision (i.e., a small set of patterns) rather than rely-ing on distant supervision and large existing knowl-edge bases.
Additionally, while Riedel et al repre-sent all relations in a general embedding space, herewe train a dedicated embedding space for just thecausal relations.In QA, embeddings have been customized to havequestion words that are close to either their answerwords (Bordes et al, 2014), or to structured knowl-edge base entries (Yang et al, 2014).
While thesemethods are useful for QA, they do not distinguishbetween different types of questions, and as suchtheir embeddings are not specific to a given questiontype.Additionally, embeddings have been customizedto distinguish functional similarity from relatedness(Levy and Goldberg, 2014; Kiela et al, 2015).
Inparticular, Levy and Goldberg train their embed-dings by replacing the standard linear context of thetarget word with context derived from the syntac-tic dependency graph of the sentence.
In this work,we make use of this extension to arbitrary context inorder to train our embeddings with contexts derivedfrom binary causal relations.
We extract cause-effecttext pairs such that the cause text becomes the targettext and the effect text serves as the context.Recently, Faruqui et al(2016) discussed issuessurrounding the evaluation of similarity word em-beddings, including the lack of correlation be-tween their performance on word-similarity tasksand ?downstream?
or real-world tasks like QA, textclassification, etc.
As they advocate, in addition to adirect evaluation of our causal embeddings, we alsoevaluate them independently in a downstream QAtask.
We provide the same comparison for two alter-native approaches (an alignment model and a con-volutional neural network model), confirming thatthe direct evaluation performance can be misleadingwithout the task-specific, downstream evaluation.139With respect to extracting causal relations fromtext, Girju et al (2002) use modified Hearst pat-terns (Hearst, 1992) to extract a large number ofpotential cause-effect tuples, where both causes andeffects must be nouns.
However, Cole et al (2005)show that these nominal-based causal relations ac-count for a relatively small percentage of all causalrelations, and for this reason, (Yang and Mao, 2014)allow for more elaborate argument structures in theircausal extraction by identifying verbs, and then fol-lowing the syntactic subtree of the verbal argumentsto construct their candidate causes and effects.
Ad-ditionally, Do et al (2011) observe that nouns aswell as verbs can signal causality.
We follow theseintuitions in developing our causal patterns by usingboth nouns and verbs to signal potential participantsin causal relations, and then allowing for the entiredominated structures to serve as the cause and/or ef-fect arguments.3 ApproachOur focus is on reranking answers to causal ques-tions using using task-specific distributional similar-ity methods.
Our approach operates in three steps:(1) We start by bootstrapping a large number ofcause-effect pairs from free text using a small num-ber of syntactic and surface patterns (Section 4).
(2) We then use these bootstrapped pairs to buildseveral task-specific embedding (and other distribu-tional similarity) models (Section 5).
We evaluatethese models directly on a causal-relation identifica-tion task (Section 6).
(3) Finally, we incorporate these models into areranking framework for causal QA and demonstratethat the resulting approach performs better than thereranker without these task-specific models, even iftrained on the same data (Section 7).4 Extracting Cause-Effect TuplesBecause the success of embedding models dependson large training datasets (Sharp et al, 2015), andsuch datasets do not exist for open-domain causality,we opted to bootstrap a large number of cause-effectpairs from a small set of patterns.
We wrote thesepatterns using Odin (Valenzuela-Esca?rcega et al,2016), a rule-based information extraction frame-work which has the distinct advantage of being ableto operate over multiple representations of content(i.e., surface and syntax).
For this work, we makeuse of rules that operate over both surface sequencesas well as dependency syntax in the grammars intro-duced in steps (2) and (3) below.Odin operates as a cascade, allowing us to imple-ment a two-stage approach.
First, we identify poten-tial participants in causal relations, i.e., the poten-tial causes and effects, which we term causal men-tions (CM).
A second grammar then identifies ac-tual causal relations that take these CMs as argu-ments.We consider both noun phrases (NP) as well asentire clauses to be potential CMs, since causal pat-terns form around participants that are syntacticallymore complex than flat NPs.
For example, in thesentence The collapse of the housing bubble causedstock prices to fall, both the cause (the collapse ofthe housing bubble) and effect (stock prices to fall)are more complicated nested structures.
Reducingthese arguments to non-recursive NPs (e.g., The col-lapse and stock prices) is clearly insufficient to cap-ture the relevant context.Formally, we extract our causal relations using thefollowing algorithm:(1) Pre-processing: Much of the text we use to ex-tract causal relation tuples comes from the Anno-tated Gigaword (Napoles et al, 2012).
This textis already fully annotated and no further process-ing is necessary.
We additionally use text from theSimple English Wikipedia1, which we processed us-ing the Stanford CoreNLP toolkit (Manning et al,2014) and the dependency parser of Chen and Man-ning (2014).
(2) CM identification: We extract causal mentions(which are able to serve as arguments in our causalpatterns) using a set of rules designed to be robust tothe variety that exists in natural language.
Namely,to find CMs that are noun phrases, we first findwords that are tagged as nouns, then follow outgoingdependency links for modifiers and attached prepo-1https://simple.wikipedia.org/wiki/Main_Page.The Simple English version was preferred over the full versiondue to its simpler sentence structures, which make extractingcause-effect tuples more straightforward.140Corpus Extracted TuplesAnnotated Gigaword 798,808Simple English Wikipedia 16,425Total 815,233Table 1: Number of causal tuples extracted from each corpus.sitional phrases2, to a maximum depth of two links.To find CMs that are clauses, we first find words thatare tagged as verbs (excluding verbs which them-selves were considered to signal causation3), thenagain follow outgoing dependency links for modi-fiers and arguments.
We used a total of four rules tolabel CMs.
(3) Causal tuple extraction: After CMs are iden-tified, a grammar scans the text for causal relationsthat have CMs as arguments.
Different patterns havevarying probabilities of signaling causation (Khoo etal., 1998).
To minimize the noise in the extractedpairs, we restrict ourselves to a set of 13 rules de-signed to find unambiguously causal patterns, suchas CAUSE led to EFFECT, where CAUSE and EF-FECT are CMs.
The rules operate by looking for atrigger phrase, e.g., led, and then following the de-pendency paths to and/or from the trigger phrase tosee if all required CM arguments exist.Applying this causal grammar over Gigaword andSimple English Wikipedia produced 815,233 causaltuples, as summarized in Table 1.
As bootstrappingmethods are typically noisy, we manually evaluatedthe quality of approximately 250 of these pairs se-lected at random.
Of the tuples evaluated, approxi-mately 44% contained some amount of noise.
Forexample, from the sentence Except for Springer?sshow, which still relies heavily on confrontationaltopics that lead to fistfights virtually every day...,while ideally we would only extract (confrontationaltopics ?
fistfights), instead we extract the tuple(show which still relies heavily on confrontationaltopics ?
fistfights virtually every day), which con-tains a large amount of noise: show, relies, heavily,etc.
This finding prompted our noise-aware modeldescribed at the end of Section 5.2The outgoing dependency links from the nouns which wefollowed were: nn, amod, advmod, ccmod, dobj,prep of, prep with, prep for, prep into,prep on, prep to, and prep in.3The verbs we excluded were: cause, result, lead, create.5 ModelsWe use the extracted causal tuples to train three dis-tinct distributional similarity models that explicitlycapture causality.Causal Embedding Model (cEmbed): The firstdistributional similarity model we use is basedon the skip-gram word-embedding algorithm ofMikolov et al (2013), which has been shown to im-prove a variety of language processing tasks includ-ing QA (Yih et al, 2013; Fried et al, 2015).
In par-ticular, we use the variant implemented by Levy andGoldberg (2014) which modifies the original algo-rithm to use an arbitrary, rather than linear, context.Our novel contribution is to make this context task-specific: intuitively, the context of a cause is its ef-fect.
Further, these contexts are generated from tu-ples that are themselves bootstrapped, which mini-mizes the amount of supervision necessary.The Levy and Goldberg model trains using single-word pairs, while our CMs could be composed ofmultiple words.
For this reason, we decomposeeach cause?effect tuple, (CMc, CMe), such thateach word wc ?
CMc is paired with each wordwe ?
CMe.After filtering the extracted cause-effect tuples forstop words and retaining only nouns, verbs, and ad-jectives, we generated over 3.6M (wc, we) word-pairs4 from the approximately 800K causal tuples.The model learns two embedding vectors for eachword, one for when the word serves as a target wordand another for when the word serves as a contextword.
Here, since the relation of interest is inher-ently directional, both sets of embeddings are mean-ingful, and so we make use of both ?
the target vec-tors encode the effects of given causes, whereas thecontext vectors capture the causes of the correspond-ing effects.Causal Alignment Model (cAlign): Monolingualalignment (or translation) models have been shownto be successful in QA (Berger et al, 2000; Echi-habi and Marcu, 2003; Soricut and Brill, 2006; Rie-zler et al, 2007; Surdeanu et al, 2011; Yao et al,2013), and recent work has shown that they can besuccessfully trained with less data than embeddingmodels (Sharp et al, 2015).4For all models proposed in this section we used lemmasrather than words.141Figure 1: Architecture of the causal convolutional network.To verify these observations in our context, wetrain an alignment model that ?translates?
causes(i.e., the ?source language?)
into effects (i.e., the?destination language?
), using our cause?effect tu-ples.
This is done using IBM Model 1 (Brown et al,1993) and GIZA++ (Och and Ney, 2003).Causal Convolutional Neural Network Model(cCNN): Each of the previous models have at theirroot a bag-of-words representation, which is a sim-plification of the causality task.
To address this po-tential limitation, we additionally trained a convo-lutional neural network (CNN) which operates overvariable-length texts, and maintains distinct embed-dings for causes and effects.
The architecture ofthis approach is shown in Figure 1, and consistsof two sub-networks (one for cause text and onefor effect text), each of which begins by convertingthe corresponding text into 50-dimensional embed-dings.
These are then fed to a convolutional layer,5which is followed by a max-pooling layer of equallength.
Then, these top sub-network layers, whichcan be thought of as a type of phrasal embedding,are merged by taking their cosine similarity.
Finally,this cosine similarity is normalized by feeding it intoa dense layer with a single node which has a soft-plus activation.
In designing our CNN, we attemptedto minimize architectural and hyperparameter tun-ing by taking inspiration from Iyyer et al (2015),preferring simpler architectures.
We train the net-work using a binary cross entropy objective functionand the Adam optimizer (Kingma and Ba, 2014), us-ing the Keras library (Chollet, 2015) operating overTheano (Theano Development Team, 2016), a pop-ular deep-learning framework.65The convolutional layer contained 100 filters, had a filterlength of 2 (i.e., capturing bigram information), and an innerReLU activation.6We also experimented with an equivalent architecturewhere the sub-networks are implemented using long short-Noise-aware Causal Embedding Model (cEm-bedNoise): We designed a variant of our cEmbedapproach to address the potential impact of the noiseintroduced by our bootstrapping method.
Whiletraining, we weigh the causal tuples by the likeli-hood that they are truly causal, which we approxi-mate with pointwise mutual information (PMI).
Forthis, we first score the tuples by their causal PMIand then scale these scores by the overall frequencyof the tuple (Riloff, 1996), to account for the PMIbias toward low-frequency items.
That is, the scoreS of a tuple, t, is computed as:S(t) = log p(t|causal)p(t) ?
log(freq(t)) (1)We then discretize these scores into five quantiles,ascribing a linearly decreasing weight during train-ing to datums in lower scoring quantiles.6 Direct Evaluation: Ranking Word PairsWe begin the assessment of our models with a directevaluation to determine whether or not the proposedapproaches capture causality better than general-purpose word embeddings and whether their robust-ness improves upon a simple database look-up.
Forthis evaluation, we follow the protocol of Levy andGoldberg (2014).
In particular, we create a collec-tion of word pairs, half of which are causally re-lated, with the other half consisting of other rela-tions.
These pairs are then ranked by our models andseveral baselines, with the goal of ranking the causalpairs above the others.
The embedding models rankthe pairs using the cosine similarity between the tar-get vector for the causal word and the context vectorof the effect word.
The alignment model ranks pairsusing the probability P (Effect|Cause) given by IBMModel 1, and the CNN ranks pairs by the value of theoutput returned by the network.6.1 DataIn order to avoid bias towards our extraction meth-ods, we evaluate our models on an external set ofterm memory (LSTM) networks (Hochreiter and Schmidhuber,1997), and found that they consistently under-perform this CNNarchitecture.
Our conjecture is that CNNs perform better be-cause LSTMs are more sensitive to overall word order thanCNNs, which capture only local contexts, and we have rela-tively little training data, which prevents the LSTMs from gen-eralizing well.1420.10.20.30.40.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallRandomLook-upvEmbedcEmbedcEmbedBicEmbedBiNoisecCNNcAlignFigure 2: Precision-recall curve showing the ability of each model to rank causal pairs above non-causal pairs.
For clarity, we donot plot cEmbedNoise, which performs worse than cEmbedBiNoise.
The Look-up model has no data points beyond the 35% recallpoint.word pairs drawn from the SemEval 2010 Task 8(Hendrickx et al, 2009), originally a multi-way clas-sification of semantic relations between nominals.We used a total of 1730 nominal pairs, 865 of whichwere from the Cause-Effect relation (e.g., (dancing?
happiness)) and an equal number which wererandomly selected from the other eight relations(e.g., (juice ?
grapefruit), from the Entity-Originrelation).
This set was then randomly divided intoequally-sized development and test partitions.6.2 BaselinesWe compared our distributional similarity modelsagainst three baselines:Vanilla Embeddings Model (vEmbed): a standardword2vec model trained with the skip-gram algo-rithm and a sliding window of 5, using the originaltexts from which our causal pairs were extracted.7As with the cEmbed model, SemEval pairs wereranked using the cosine similarity between the vec-tor representations of their arguments.Look-up Baseline: a given SemEval pair wasranked by the number of times it appeared in ourextracted cause-effect tuples.Random: pairs were randomly shuffled.6.3 ResultsFigure 2 shows the precision-recall (PR) curve foreach of the models and baselines.
As expected,the causal models are better able to rank causal7All embedding models analyzed here, including this base-line and our causal variants, produced embedding vectors of 200dimensions.pairs than the vanilla embedding baseline (vEmbed),which, in turn, outperforms the random baseline.Our look-up baseline, which ranks pairs by their fre-quency in our causal database, shows a high preci-sion for this task, but has coverage for only 35% ofthe causal SemEval pairs.Some models perform better on the low-recallportion of the curve (e.g., the look-up baseline andcCNN), while the embedding and alignment mod-els have a higher and more consistent performanceacross the PR curve.
We hypothesize that modelsthat better balance precision and recall will performbetter in a real-world QA task, which may need toaccess a given causal relation through a variety oflexical patterns or variations.
We empirically vali-date this observation in Section 7.The PR curve for the causal embeddings showsan atypical dip at low-recall.
To examine this, weanalyzed its top-ranked 15% of SemEval pairs, andfound that incorrectly ranked pairs were not foundin the database of causal tuples.
Instead, these incor-rect rankings were largely driven by low frequencywords whose embeddings could not be robustly es-timated due to lack of direct evidence.
Because thissparsity is partially driven by directionality, we im-plemented a bidirectional embedding model (cEm-bedBi) that (a) trains a second embedding modelby reversing the input (effects as targets, causes ascontexts), and (b) ranks pairs by the average of thescores returned by these two unidirectional causalembedding models.
Specifically, the final bidirec-tional score of the pair, (e1, e2), where e1 is the can-143didate cause and e2 is the candidate effect, is:sbi(e1, e2) = 12(sc?e(e1, e2) + se?c(e2, e1)) (2)where sc?e is the score given by the original causalembeddings, i.e., from cause to effect, and se?c isthe score given by the reversed-input causal embed-dings, i.e., from effect to cause.As Figure 2 shows, the bidirectional embeddingvariants consistently outperform their unidirectionalcounterparts.
All in all, the best overall model iscEmbedBiNoise, which is both bidirectional and in-corporates the noise handling approach from Sec-tion 5.
This model substantially improves perfor-mance in the low-recall portion of the curve, whilealso showing strong performance across the curve.7 Indirect Evaluation: QA TaskThe main objective of our work is to investigate theimpact of a customized causal embedding model forQA.
Following our direct evaluation, which solelyevaluated the degree to which our models directlyencode causality, here we evaluate each of our pro-posed causal models in terms of their contribution toa downstream real-world QA task.Our QA system uses a standard reranking ap-proach (Jansen et al, 2014).
In this architecture, thecandidate answers are initially extracted and rankedusing a shallow candidate retrieval (CR) componentthat uses solely information retrieval techniques,then they are re-ranked using a ?learning to rank?approach.
In particular, we used SVM rank8, a Sup-port Vector Machines classifier adapted for ranking,and re-ranked the candidate answers with a set offeatures derived from both the initial CR score andthe models we have introduced.
For our model com-binations (see Table 2), the feature set includes theCR score and the features from each of the modelsin the combination.7.1 DataWe evaluate on a set of causal questions extractedfrom the Yahoo!
Answers corpus9 with simple sur-face patterns such as What causes ... and What8 http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html9Freely available through Yahoo!
?s Webscope program(research-data-requests@yahoo-inc.com)is the result of ...10.
We extracted a total of 3031questions, each with at least four candidate answers,and we evaluated performance using five-fold cross-validation, with three folds for training, one for de-velopment, and one for testing.7.2 Models and FeaturesWe evaluate the contribution of the bidirectional andnoise-aware causal embedding models (cEmbedBi,and cEmbedBiNoise) as well as the causal alignmentmodel (cAlign) and the causal CNN (cCNN).
Thesemodels are compared against three baselines: thevanilla embeddings (vEmbed), the lookup baseline(LU), and additionally a vanilla alignment model(vAlign) which is trained over 65k question-answerpairs from Yahoo!
Answers.The features11 we use for the various models are:Embedding model features: For both our vanillaand causal embedding models, we use the same setof features as Fried et al (2015): the maximum,minimum, and average pairwise cosine similaritybetween question and answer words, as well as theoverall similarity between the composite questionand answer vectors.
When using the causal embed-dings, since the relation is directed, we first deter-mine whether the question text is the cause or the ef-fect12, which in turn determines which embeddingsto use for the question text and which to use for thecandidate answer texts.
For example, in a questionsuch as ?What causes X?
?, since X is the effect, allcosine similarities would be found using the effectvectors for the question words and the cause vectorsfor the answer candidate words.Alignment model features: We use the sameglobal alignment probability, p(Q|A) of Surdeanuet al (2011).
In our causal alignment model, weadapt this to causality as p(Effect|Cause), and againwe first determine the direction of the causal re-lation implied in the question.
We include theadditional undirected alignment features based on10We lightly filtered these with stop words to remove non-causal questions, such as those based on math problems and theresults of sporting events.
Our dataset will be freely available,conditioned on users having obtained the Webscope license.11Due to the variety of features used, each feature describedhere is independently normalized to lie between 0.0 and 1.0.12We do this through the use of simple regular expressions,e.g., ??
[Ww]hat ([a-z]+ ){0,3}cause.+?144# Model P@1Baselines1 Random 16.432 CR 24.313 CR + vEmbed 34.614 CR + vAlign 19.245 CR + Look-up (LU) 29.56Single Causal Models6 CR + cEmbedBi 31.327 CR + cEmbedBiNoise 30.158 CR + cAlign 23.499 CR + cCNN 24.66Model Combinations10 CR + vEmbed + cEmbedBi 37.08?11 CR + vEmbed + cEmbedBiNoise 35.50?12 CR + vEmbed + cEmbedBi + LU 36.75?13 CR + vEmbed + cAlign 34.3114 CR + vEmbed + cCNN 33.45Model Stacking15 CR + vEmbed + cEmbedBi + cEmbedBiNoise 37.28?Table 2: Performance in the QA evaluation, measured byprecision-at-one (P@1).
The ?Bi?
suffix indicates a bidirec-tional model; the ?Noise?
suffix indicates a model that is noiseaware.
?
indicates that the difference between the correspond-ing model and the CR + vEmbed baseline is statistically sig-nificant (p < 0.05), determined through a one-tailed bootstrapresampling test with 10,000 iterations.Jensen-Shannon distance, proposed more recentlyby Fried et al (2015), in our vanilla alignmentmodel.
However, due to the directionality inherentin causality, they do not apply to our causal modelso there we omit them.Look-up feature: For the look-up baseline wecount the number of times words from the questionand answer appear together in our database of ex-tracted causal pairs, once again after determining thedirectionality of the questions.
If the total number ofmatches is over a threshold13, we consider the causalrelation to be established and give the candidate an-swer a score of 1; or a score of 0, otherwise.7.3 ResultsThe overall results are summarized in Table 2.
Lines1?5 in the table show that each of our baselines per-formed better than CR by itself, except for vAlign,suggesting that the vanilla alignment model does notgenerate accurate predictions for causal questions.13Empirically determined to be 100 matches.
Note that us-ing this threshold performed better than simply using the totalnumber of matches.The strongest baseline was CR + vEmbed (line 3),the vanilla embeddings trained over Gigaword, at34.6% P@1.
For this reason, we consider this tobe the baseline to ?beat?, and perform statistical sig-nificance of all proposed models with respect to it.Individually, the cEmbedBi model is the best per-forming of the causal models.
While the perfor-mance of cAlign in the direct evaluation was com-parable to that of cEmbedBi, here it performs farworse (line 6 vs 8), suggesting that the robustness ofembeddings is helpful in QA.
Notably, despite thestrong performance of the cCNN in the low-recallportion of the PR curve in the direct evaluation, herethe model performs poorly (line 9).No individual causal model outperforms thestrong vanilla embedding baseline (line 3), likelyowing to the reduction in generality inherent tobuilding task-specific QA models.
However, com-paring lines 6?9 vs. 10?14 shows that the vanilla andcausal models are capturing different and comple-mentary kinds of knowledge (i.e., causality vs. as-sociation through distributional similarity), and areable to be combined to increase overall task perfor-mance (lines 10?12).
These results highlight thatQA is a complex task, where solving methods needto address the many distinct information needs inquestion sets, including both causal and direct as-sociation relations.
This contrasts with the directevaluation, which focuses strictly on causality, andwhere the vanilla embedding baseline performs nearchance.
This observation highlights one weaknessof word similarity tasks: their narrow focus may notdirectly translate to estimating their utility in real-world NLP applications.Adding in the lookup baseline (LU) to the best-performing causal model does not improve perfor-mance (compare lines 10 and 12), suggesting thatthe bidirectional causal embeddings subsume thecontribution of the LU model.
cEmbedBi (line 10)also performs better than cEmbedBiNoise (line 11).We conjecture that the ?noise?
filtered out by cEm-bedBiNoise contains distributional similarity infor-mation, which is useful for the QA task.
cEmbedBivastly outperforms cCNN (line 14), suggesting thatstrong overall performance across the precision-recall curve better translates to the QA task.
We hy-pothesize that the low cCNN performance is causedby insufficient training data, preventing the CNN ar-145Error/observation % QBoth chosen and gold are equally good answers 45%Causal max similarity of chosen is higher 35%Vanilla overall similarity of chosen is higher 35%Chosen answer is better than the gold answer 25%The question is very short / lacks content words 15%Other 10%Table 3: Results of an error analysis performed on a randomsample of 20 incorrectly answered questions showing the sourceof the error and the percentage of questions that were affected.Note that questions can belong to multiple categories.chitecture from generalizing well.Our best performing overall model combines bothvariants of the causal embedding model (cEmbedBiand cEmbedBiNoise), reaching a P@1 of 37.3%,which shows a 7.7% relative improvement over thestrong CR + vEmbed baseline.7.4 Error AnalysisWe performed an error analysis to gain more insightinto our model as well as the source of the remain-ing errors.
For simplicity, we used the combina-tion model CR + vEmbed + cEmbedBi.
Examiningthe model?s learned feature weights, we found thatthe vanilla overall similarity feature had the high-est weight, followed by the causal overall similarityand causal maximum similarity features.
This in-dicates that even in causal question answering, theoverall topical similarity between question and an-swer is still useful and complementary to the causalsimilarity features.To determine sources of error, we randomly se-lected 20 questions that were incorrectly answeredand analyzed them according to the categoriesshown in Table 3.
We found that for 70% of thequestions, the answer chosen by our system was asgood as or better than the gold answer, often the casewith community question answering datasets.Additionally, while the maximum causal similar-ity feature is useful, it can be misleading due to em-bedding noise, low-frequency words, and even thebag-of-words nature of the model (35% of the incor-rect questions).
For example, in the question Whatare the effects of growing up with an older siblingwho is better than you at everything?, the modelchose the answer ...You are you and they are them- you will be better and different at other things...largely because of the high causal similarity between(grow?
better).
While this could arguably be help-ful in another context, here it is irrelevant, suggest-ing that in the future improvement could come frommodels that better incorporate textual dependencies.8 ConclusionWe presented a framework for creating customizedembeddings tailored to the information need ofcausal questions.
We trained three popular mod-els (embedding, alignment, and CNN) using causaltuples extracted with minimal supervision by boot-strapping cause-effect pairs from free text, and eval-uated their performance both directly (i.e., the de-gree to which they capture causality), and indirectly(i.e., their real-world utility on a high-level questionanswering task).We showed that models that incorporate a knowl-edge of causality perform best for both tasks.
Ouranalysis suggests that the models that perform bestin the real-world QA task are those that have consis-tent performance across the precision-recall curve inthe direct evaluation.
In QA, where the vocabulary ismuch larger, precision must be balanced with high-recall, and this is best achieved by our causal embed-ding model.
Additionally, we showed that vanillaand causal embedding models address different in-formation needs of questions, and can be combinedto improve performance.Extending this work beyond causality, we hypoth-esize that additional embedding spaces customizedto the different information needs of questionswould allow for robust performance over a largervariety of questions, and that these customized em-bedding models should be evaluated both directlyand indirectly to accurately characterize their per-formance.ResourcesAll code and resources needed to reproduce thiswork are available at http://clulab.cs.arizona.edu/data/emnlp2016-causal/.AcknowledgmentsWe thank the Allen Institute for Artificial Intelli-gence for funding this work.
Additionally, this workwas partially funded by the Defense Advanced Re-search Projects Agency (DARPA) Big Mechanismprogram under ARO contract W911NF-14-1-0395.146References[Berger et al2000] A. Berger, R. Caruana, D. Cohn,D.
Freytag, and V. Mittal.
2000.
Bridging the lex-ical chasm: Statistical approaches to answer finding.In Proc.
of the 23rd Annual International ACM SIGIRConference on Research & Development on Informa-tion Retrieval.
[Bordes et al2014] A. Bordes, S. Chopra, and J. Weston.2014.
Question answering with subgraph embeddings.arXiv preprint arXiv:1406.3676.
[Brown et al1993] P. F. Brown, S. A. Della Pietra,V.
J. Della Pietra, and R. L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
Computational Linguistics, 19(2):263?311.
[Chen and Manning2014] D. Chen and C. D. Manning.2014.
A fast and accurate dependency parser us-ing neural networks.
In Proc.
of the Conferenc onEmpirical Methods for Natural Language Processing(EMNLP).
[Chollet2015] F. Chollet.
2015.
Keras.
https://github.com/fchollet/keras.
[Chu-Carroll et al2004] J. Chu-Carroll, K. Czuba, J. M.Prager, A. Ittycheriah, and S. Blair-Goldensohn.
2004.IBM?s PIQUANT II in TREC 2004.
In Text RetrievalConference (TREC).
[Clark et al2013] P. Clark, P. Harrison, and N. Balasub-ramanian.
2013.
A study of the knowledge base re-quirements for passing an elementary science test.
InProc.
of the 2013 workshop on Automated KnowledgeBase Construction (AKBC), pages 37?42.
[Cole et al2005] S. V. Cole, M. D. Royal, M. G. Valtorta,M.
N. Huhns, and J.
B. Bowles.
2005.
A lightweighttool for automatically extracting causal relationshipsfrom text.
In SoutheastCon, 2006.
Proc.
of the IEEE,pages 125?129.
[Do et al2011] Q. X.
Do, Y. S. Chan, and D. Roth.
2011.Minimally supervised event causality identification.In Proc.
of the Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 294?303.
[Echihabi and Marcu2003] A. Echihabi and D. Marcu.2003.
A noisy-channel approach to question answer-ing.
In Proc.
of the 41st Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages16?23.
[Etzioni2011] O. Etzioni.
2011.
Search needs a shake-up.Nature, 476(7358):25?26.
[Faruqui et al2016] M. Faruqui, Y. Tsvetkov, R. Rastogi,and C. Dyer.
2016.
Problems with evaluation ofword embeddings using word similarity tasks.
arXivpreprint arXiv:1605.02276.
[Ferrucci et al2010] D. Ferrucci, E. Brown, J. Chu-Carroll, J.
Fan, D. Gondek, A.
A. Kalyanpur, A. Lally,J.
W. Murdock, E. Nyberg, J. Prager, et al 2010.Building Watson: An overview of the DeepQAproject.
AI magazine, 31(3):59?79.
[FitzGerald et al2015] N. FitzGerald, O. Ta?ckstro?m,K.
Ganchev, and D. Das.
2015.
Semantic role labelingwith neural network factors.
In Proc.
of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 960?970.
[Fried et al2015] D. Fried, P. Jansen, G. Hahn-Powell,M.
Surdeanu, and P. Clark.
2015.
Higher-order lex-ical semantic models for non-factoid answer rerank-ing.
Transactions of the Association for Computa-tional Linguistics, 3:197?210.
[Girju and Moldovan2002] R. Girju and D. I. Moldovan.2002.
Text mining for causal relations.
In FLAIRSConference, pages 360?364.
[Hearst1992] M. A. Hearst.
1992.
Automatic acquisitionof hyponyms from large text corpora.
In Proc.
of the14th conference on Computational linguistics (COL-ING), pages 539?545.
[Hendrickx et al2009] I. Hendrickx, S. N. Kim,Z.
Kozareva, P. Nakov, D. O?
Se?aghdha, S. Pado?,M.
Pennacchiotti, L. Romano, and S. Szpakowicz.2009.
Semeval-2010 task 8: Multi-way classificationof semantic relations between pairs of nominals.
InProc.
of the Workshop on Semantic Evaluations:Recent Achievements and Future Directions, pages94?99.
[Hochreiter and Schmidhuber1997] S. Hochreiter andJ.
Schmidhuber.
1997.
Long short-term memory.Neural computation, 9(8):1735?1780.
[Iyyer et al2015] Mohit Iyyer, Varun Manjunatha, JordanBoyd-Graber, and Hal Daume?
III.
2015.
Deep un-ordered composition rivals syntactic methods for textclassification.
In Proceedings of the Association forComputational Linguistics.
[Jansen et al2014] P. Jansen, M. Surdeanu, and P. Clark.2014.
Discourse complements lexical semantics fornon-factoid answer reranking.
In Proc.
of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (ACL).
[Khoo et al1998] C. S.G. Khoo, J. Kornfilt, R. N. Oddy,and S. H. Myaeng.
1998.
Automatic extraction ofcause-effect information from newspaper text withoutknowledge-based inferencing.
Literary and LinguisticComputing, 13(4):177?186.
[Kiela et al2015] D. Kiela, F. Hill, and S. Clark.
2015.Specializing word embeddings for similarity or relat-edness.
In Proc.
of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).
[Kingma and Ba2014] D. Kingma and J. Ba.
2014.Adam: A method for stochastic optimization.
arXivpreprint arXiv:1412.6980.147[Levy and Goldberg2014] O.
Levy and Y. Goldberg.2014.
Dependency-based word embeddings.
In Proc.of the 52nd Annual Meeting of the Association forComputational Linguistics (ACL), pages 302?308.
[Levy et al2015] O.
Levy, S. Remus, C. Biemann, I. Da-gan, and I. Ramat-Gan.
2015.
Do supervised dis-tributional methods really learn lexical inference rela-tions.
In Proc.
of the Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics (NAACL).
[Manning et al2014] C. D. Manning, M. Surdeanu,J.
Bauer, J. Finkel, S. J. Bethard, and D. McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proc.
of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL).
[Mikolov et al2013] T. Mikolov, I. Sutskever, K. Chen,G.
S. Corrado, and J.
Dean.
2013.
Distributed repre-sentations of words and phrases and their composition-ality.
In Advances in neural information processingsystems, pages 3111?3119.
[Napoles et al2012] C. Napoles, M. Gormley, andB.
Van Durme.
2012.
Annotated gigaword.
In Proc.of the Joint Workshop on Automatic Knowledge BaseConstruction and Web-scale Knowledge Extraction,pages 95?100.
[Och and Ney2003] F. J. Och and H. Ney.
2003.
Asystematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.
[Oh et al2013] J.-H. Oh, K. Torisawa, C. Hashimoto,M.
Sano, S. De Saeger, and K. Ohtake.
2013.
Why-question answering using intra-and inter-sententialcausal relations.
In The 51st Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages1733?1743.
[Riedel et al2013] S. Riedel, L. Yao, A. McCallum, andB.
M. Marlin.
2013.
Relation extraction with matrixfactorization and universal schemas.
In Proc.
of An-nual Meeting of the North American Chapter of theAssociation for Computational Linguistics (NAACL).
[Riezler et al2007] S. Riezler, A. Vasserman, I. Tsochan-taridis, V. Mittal, and Y. Liu.
2007.
Statistical ma-chine translation for query expansion in answer re-trieval.
In Proc.
of the 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages464?471.
[Riloff1996] E. Riloff.
1996.
Automatically generatingextraction patterns from untagged text.
In Proc.
of theNational Conference on Artificial Intelligence (AAAI),pages 1044?1049.
[Sharp et al2015] R. Sharp, P. Jansen, M. Surdeanu, andP.
Clark.
2015.
Spinning straw into gold.
In Proc.of the Conference of the North American Chapter ofthe Association for Computational Linguistics - Hu-man Language Technologies.
[Soricut and Brill2006] R. Soricut and E. Brill.
2006.Automatic question answering using the web: Beyondthe factoid.
Journal of Information Retrieval - SpecialIssue on Web Information Retrieval, 9(2):191?206.
[Surdeanu et al2011] M. Surdeanu, M. Ciaramita, andH.
Zaragoza.
2011.
Learning to rank answers tonon-factoid questions from web collections.
Compu-tational Linguistics, 37(2):351?383.
[Theano Development Team2016] Theano DevelopmentTeam.
2016.
Theano: A Python framework for fastcomputation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May.
[Valenzuela-Esca?rcega et al2016] M. A. Valenzuela-Esca?rcega, G. Hahn-Powell, and M. Surdeanu.
2016.Odin?s runes: A rule language for information extrac-tion.
In Proc.
of the 10th International Conference onLanguage Resources and Evaluation (LREC).
[Woodsend and Lapata2015] K. Woodsend and M. Lap-ata.
2015.
Distributed representations for unsuper-vised semantic role labeling.
In Proc.
of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP).
[Yang and Mao2014] X. Yang and K. Mao.
2014.
Multilevel causal relation identification using extended fea-tures.
Expert Systems with Applications, 41(16):7171?7181.
[Yang et al2014] M.-C. Yang, N. Duan, M. Zhou, andH.-C. Rim.
2014.
Joint relational embeddings forknowledge-based question answering.
In Proc.
of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 645?650.
[Yao et al2013] X. Yao, B.
Van Durme, C. Callison-Burch, and P. Clark.
2013.
Semi-markov phrase-based monolingual alignment.
In Proc.
of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).
[Yih et al2013] W. Yih, M. Chang, C. Meek, and A. Pas-tusiak.
2013.
Question answering using enhancedlexical semantic models.
In Proc.
of the 51st AnnualMeeting of the Association for Computational Linguis-tics (ACL).148
