Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 367?370,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsKSU KDD: Word Sense Induction by Clustering in Topic SpaceWesam Elshamy, Doina Caragea, William H. HsuKansas State University{welshamy, dcaragea, bhsu}@ksu.eduAbstractWe describe our language-independent un-supervised word sense induction system.This system only uses topic features tocluster different word senses in their globalcontext topic space.
Using unlabeled data,this system trains a latent Dirichlet alo-cation (LDA) topic model then uses it toinfer the topics distribution of the test in-stances.
By clustering these topics dis-tributions in their topic space we clusterthem into different senses.
Our hypothesisis that closeness in topic space reflects sim-ilarity between different word senses.
Thissystem participated in SemEval-2 wordsense induction and disambiguation taskand achieved the second highest V-measurescore among all other systems.1 IntroductionAmbiguity of meaning is inherent in natural lan-guage because the deliverer of words tries to mini-mize the size of the vocabulary set he uses.
There-fore, a sizable portion of this vocabulary is polyse-mous and the intended meaning of such words canbe encoded in their context.Due to the knowledge acquisition bottleneckproblem and scarcity in training data (Cai etal., 2007), unsupervised corpus based approachescould be favored over supervised ones in word sensedisambiguation (WSD) tasks.Similar efforts in this area include work by Caiet al (Cai et al, 2007) in which they use latentDirichlet alocation (LDA) topic models to extractthe global context topic and use it as a featurealong other baseline features.
Another techniqueuses clustering based approach with WordNet as anexternal resource for disambiguation without rely-ing on training data (Anaya-Sa?nchez et al, 2007).To disambiguate a polysemous word in a textdocument, we use the document topic distributionto represent its context.
A document topic distri-bution is the probabilistic distribution of a docu-ment over a set of topics.
The assumption is that:given two word senses and the topic distribution?
?wz?NMFigure 1: A graphical model for LDAof their context, the closeness between these twotopic distributions in their topic space is an indi-cation of the similarity between those two senses.Our motivation behind building this system wasthe observation that the context of a polysemousword helps determining its sense to some degree.In our word sense induction (WSI) system, we useLDA to create a topic model for the given corpusand use it to infer the topic distribution of thedocuments containing the ambiguous words.This paper describes our WSI system which par-ticipated in SemEval-2 word sense induction anddisambiguation task (Manandhar et al, 2010).2 Latent Dirichlet alocationLDA is a probabilistic model for a collection of dis-crete data (Blei et al, 2003).
It can be graphicallyrepresented as shown in Figure 1 as a three levelhierarchical Bayesian model.
In this model, thecorpus consists of M documents, each is a multino-mial distribution over K topics, which are in turnmultinomial distributions over words.To generate a document d using this probabilis-tic model, a distribution over topics ?dis generatedusing a Dirichlet prior with parameter ?.
Then,for each of the Ndwords wdnin the document,a topic zdnis drawn from a multinomial distribu-tion with the parameter ?d.
Then, a word wdnisdrawn from that topic?s distribution over words,given ?ij= p(w = i?z = j).
Where ?ijis the proba-bility of choosing word i given topic j.3 System descriptionWe wanted to examine the trade-off between sim-plicity, cost and performance by building a simple367language-independent, totally unsupervised, com-putationally cheap system and compare its perfor-mance to other WSI systems participating in theSemEval-2 WSI task (Manandhar et al, 2010).
Weexpect a degradation in precision of our simple ap-proach as the granularity of senses becomes finer;This is due to the degrading sensitivity in mappingbetween the topics space and the senses space.
Wenote that our simple approach will fail if multiplesenses of the same word appear in the same docu-ment; Since these senses will be represented by thesame topic distribution of the document, they willbe clustered in the same cluster.Our system is a language-independent system.The used LDA topic model has no knowledge ofthe training or testing corpus language.
Unlikemost other WSI and WSD systems, it doesn?t makeuse of part of speech (POS) features which are lan-guage dependent and require POS annotated train-ing data.
The only features used are the topics dis-tribution of bag-of-words containing the ambigu-ous word.First, for each target polysemous word wp (nounor verb), we train a MALLET1parallel topic modelimplementation of LDA on all the training in-stances of that word.
Then we use the trained topicmodel to infer the topics distribution ?lfor each ofthe test instances of that word.
For a K-topicstopic model, each topics distribution can be repre-sented as a point in a K-dimensional topic space.These points can be clustered into C different clus-ters, each representing a word sense.
We usedMALLET?s K-means clustering algorithm with co-sine similarity to measure the distance between dif-ferent topic distributions in the topic space.4 Evaluation measuresWe use the same unsupervised evaluation mea-sures used in SemEval-2 (Manandhar and Kla-paftis, 2009).
These measures do not require de-scriptiveThe V-measure is used for unsupervised evalu-ation.
It is the harmonic mean of the homogene-ity and completeness.
Homogeneity is a measureof the degree that each formed cluster consists ofdata points that belong to a single gold standard(GS) class as defined below.homogeneity = 1 ?H(GS?C)H(GS)(1)H(GS) = ??GS??i=1?
?C?j=1 aijNlog?
?C?j=1 aijN(2)H(GS?C) = ??C??j=1?GS??i=1aijNlogaij?
?GS?k=1 akj(3)1http://mallet.cs.umass.eduTable 1: Effect of varying the number of topics Kon performanceK 10 50 200 400 500V-measure 5.1 5.8 7.2 8.4 8.1F-score 8.6 32.0 53.9 63.9 64.2Where H() is an entropy function, ?C ?
and ?GS?refer to cluster and class sizes, respectively.
N isthe number of data points, aijare data points ofclass GSithat belong to cluster Cj.On the other hand, completeness measures thedegree that each class consists of data points thatbelong to a single cluster.
It is defined as follows.completeness = 1 ?H(C ?GS)H(C)(4)H(C) = ??C??j=1?
?GS?i=1 aijNlog?
?GS?i=1 aijN(5)H(C ?GS) = ??GS??i=1?C??j=1aijNlogaij?
?C?k=1 aik(6)Homogeneity and completeness can be seen asentropy based measures of precision and recall, re-spectively.
The V-measure has a range of 0 (worstperformance) to 1, inclusive.The other evaluation measure is the F-score,which is the harmonic mean of precision and re-call.
It has a range of 0 to 1 (best performance),inclusive.5 Experiments and resultsThe WSI system described earlier was tested onSemEval-1 WSI task (task 2) data (65 verbs,35 nouns), and participated in the same task inSemEval-2 (task 14) (50 verbs, 50 nouns).
Thesense induction process was the same in both cases.Before running our main experiments, wewanted to see how the number of topics K used inthe topic model could affect the performance of oursystem.
We tested our WSI system on SemEval-1data using different K values as shown in Table 1.We found that the V-measure and F-score valuesincrease with increasing K, as more dimensions areadded to the topic space, the different senses in thisK-dimensional space unfold.
This trend stops at avalue of K = 400 in a sign to the limited vocabu-lary of the training data.
This K value is used inall other experiments.Next, we evaluated the performance of our sys-tem on SemEval-1 WSI task data.
Since no train-ing data was provided for this task, we used an un-annotated version of the test instances to create theLDA topic model.
For each target word (verb ornoun), we trained the topic model on its given test368Table 2: V-measure and F-score on SemEval-1All Verbs NounsV-measure 8.4 8.0 8.7F-score 63.9 56.8 69.0Table 3: V-measure and F-score on SemEval-2All Verbs NounsV-measure 15.7 12.4 18.0F-score 36.9 54.7 24.6instances.
Then we used the generated model?s in-ferencer to find the topics distribution of each oneof them.
These distributions are then clustered inthe topic space using the K-means algorithm andthe cosine similarity measure was used to evalu-ate the distances between these distributions.
Theresults of this experiment are shown in Table 2.Our WSI system took part in the main SemEval-2 WSI task (task 14).
In the unsupervised evalua-tion, our system had the second highest V-measurevalue of 15.7 for all words2.
A break down of theobtained V-measure and F-scores is shown in Table3.To analyze the performance of the system, weexamined the clustering of the target noun word?promotion?
to different senses by our system.
Wecompared it to the GS classes of this word in theanswer key provided by the task organizers.
Fora more objective comparison, we ran the K-meansclustering algorithm with K equal to the numberof GS classes.
Even though the number of formedclusters affects the performance of the system, weassume that the number of senses is known in thisanalysis.
We focus on the ability of the algorithmto cluster similar senses together.
A graphical com-parison is given in Figure 2.The target noun word ?promotion?
has 27 in-stances and four senses.
The lower four rectanglesin Figure 2 represent the four different GS classes,and the upper four rectangles represent the fourclusters created by our system.
Three of the fourinstances representing a job ?promotion?
(?)
wereclustered together, but the fourth one was clus-tered in a different class due to terms like ?driv-ing,?
?troops,?
and ?hostile?
in its context.
Theoffer sense of ?promotion?
(?)
was mainly splitbetween two clusters, cluster 2 which most of itsinstances has mentions of numbers and monetaryunits, and cluster 4 which describes business andlabor from an employee?s eye.The 13 instances of the third class which carrythe sense encourage of the word promotion (?)
aredistributed among the four different clusters de-2A complete evaluation of all partic-ipating systems is available online at:http://www.cs.york.ac.uk/semeval2010 WSI/task 14 ranking.htmlClassCluster1 2 3 4EncourageOfferJob PressFigure 2: Analysis of sense clusteringpending on other topic words that classified themas either belonging to cluster 4 (encouragement inbusiness), cluster 3 (encouragement in conflict orwar context), cluster 2 (numbers and money con-text), or cluster 1 (otherwise).
We can see that thetopic model is unable to detect and extract topicwords for the ?encourage?
sense of the word.
Fi-nally, due to the lack of enough training instancesof the sense of a promotional issue of a newspaper(?
), the topic model inferencer clustered it in thenumbers and monetary cluster because it was richin numbers.6 ConclusionClustering the topics distributions of the globalcontext of polysemous words in the topic space toinduce their sense is cheap as it does not requireany annotated data and is language-independent.Even though the clustering produced by our sys-tem did not fully conform with the set of sensesgiven by the GS classes, it can be seen from theanalyzed example given earlier that our cluster-ing carried some different senses.
In one case, aGS sense was not captured by the topic model,and instead, other cues from its instances contextwere used to cluster them accordingly.
The in-duced clustering had some noise though.This simple WSI approach can be used for cheapsense induction or for languages for which noPOS tagger has been created yet.
This systemwhich had the second highest V-measure score inSemEval-2 WSI task achieves a good trade-off be-tween performance and cost.ReferencesHenry Anaya-Sa?nchez, Aurora Pons-Porrata, andRafael Berlanga-Llavori.
2007.
Tkb-uo: Us-ing sense clustering for wsd.
In Proceedings ofthe Fourth International Workshop on Seman-tic Evaluations (SemEval-2007), pages 322?325,369Prague, Czech Republic, June.
Association forComputational Linguistics.David M. Blei, Andrew Y. Ng, and Michael I. Jor-dan.
2003.
Latent dirichlet alocation.
J. Mach.Learn.
Res., 3:993?1022.Junfu Cai, Wee Sun Lee, and Yee Whye Teh.2007.
Improving word sense disambiguation us-ing topic features.
In Proceedings of the 2007Joint Conference on Empirical Methods in Nat-ural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL),pages 1015?1023, Prague, Czech Republic, June.Association for Computational Linguistics.Suresh Manandhar and Ioannis P. Klapaftis.
2009.Semeval-2010 task 14: evaluation setting forword sense induction & disambiguation sys-tems.
In DEW ?09: Proceedings of the Workshopon Semantic Evaluations: Recent Achievementsand Future Directions, pages 117?122, Morris-town, NJ, USA.
Association for ComputationalLinguistics.Suresh Manandhar, Ioannis P. Klapaftis, DmitriyDligach, and Sameer S. Pradhan.
2010.Semeval-2010 task 14: Word sense induction &disambiguation.
In Proceedings of SemEval-2,Uppsala, Sweden.
ACL.370
