Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485?494,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEfficient Staggered Decoding for Sequence LabelingNobuhiro Kaji Yasuhiro Fujiwara Naoki Yoshinaga Masaru KitsuregawaInstitute of Industrial Science,The University of Tokyo,4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan{kaji,fujiwara,ynaga,kisture}@tkl.iis.u-tokyo.ac.jpAbstractThe Viterbi algorithm is the conventionaldecoding algorithm most widely adoptedfor sequence labeling.
Viterbi decodingis, however, prohibitively slow when thelabel set is large, because its time com-plexity is quadratic in the number of la-bels.
This paper proposes an exact decod-ing algorithm that overcomes this prob-lem.
A novel property of our algorithm isthat it efficiently reduces the labels to bedecoded, while still allowing us to checkthe optimality of the solution.
Experi-ments on three tasks (POS tagging, jointPOS tagging and chunking, and supertag-ging) show that the new algorithm is sev-eral orders of magnitude faster than thebasic Viterbi and a state-of-the-art algo-rithm, CARPEDIEM (Esposito and Radi-cioni, 2009).1 IntroductionIn the past decade, sequence labeling algorithmssuch as HMMs, CRFs, and Collins?
perceptronshave been extensively studied in the field of NLP(Rabiner, 1989; Lafferty et al, 2001; Collins,2002).
Now they are indispensable in a wide rangeof NLP tasks including chunking, POS tagging,NER and so on (Sha and Pereira, 2003; Tsuruokaand Tsujii, 2005; Lin and Wu, 2009).One important task in sequence labeling is howto find the most probable label sequence fromamong all possible ones.
This task, referred to asdecoding, is usually carried out using the Viterbialgorithm (Viterbi, 1967).
The Viterbi algorithmhas O(NL2) time complexity,1 where N is theinput size and L is the number of labels.
Al-though the Viterbi algorithm is generally efficient,1The first-order Markov assumption is made throughoutthis paper, although our algorithm is applicable to higher-order Markov models as well.it becomes prohibitively slow when dealing witha large number of labels, since its computationalcost is quadratic in L (Dietterich et al, 2008).Unfortunately, several sequence-labeling prob-lems in NLP involve a large number of labels.
Forexample, there are more than 40 and 2000 labelsin POS tagging and supertagging, respectively(Brants, 2000; Matsuzaki et al, 2007).
Thesetasks incur much higher computational costs thansimpler tasks like NP chunking.
What is worse,the number of labels grows drastically if we jointlyperform multiple tasks.
As we shall see later,we need over 300 labels to reduce joint POS tag-ging and chunking into the single sequence label-ing problem.
Although joint learning has attractedmuch attention in recent years, how to perform de-coding efficiently still remains an open problem.In this paper, we present a new decoding algo-rithm that overcomes this problem.
The proposedalgorithm has three distinguishing properties: (1)It is much more efficient than the Viterbi algorithmwhen dealing with a large number of labels.
(2) Itis an exact algorithm, that is, the optimality of thesolution is always guaranteed unlike approximatealgorithms.
(3) It is automatic, requiring no task-dependent hyperparameters that have to be manu-ally adjusted.Experiments evaluate our algorithm on threetasks: POS tagging, joint POS tagging and chunk-ing, and supertagging2.
The results demonstratethat our algorithm is up to several orders of mag-nitude faster than the basic Viterbi algorithm and astate-of-the-art algorithm (Esposito and Radicioni,2009); it makes exact decoding practical even inlabeling problems with a large label set.2 PreliminariesWe first provide a brief overview of sequence la-beling and introduce related work.2Our implementation is available at http://www.tkl.iis.u-tokyo.ac.jp/?kaji/staggered4852.1 ModelsSequence labeling is the problem of predicting la-bel sequence y = {yn}Nn=1 for given token se-quence x = {xn}Nn=1.
This is typically done bydefining a score function f(x,y) and locating thebest label sequence: ymax = argmaxyf(x,y).The form of f(x,y) is dependent on the learn-ing model used.
Here, we introduce two modelswidely used in the literature.Generative models HMM is the most famousgenerative model for labeling token sequences(Rabiner, 1989).
In HMMs, the score functionf(x,y) is the joint probability distribution over(x,y).
If we assume a one-to-one correspondencebetween the hidden states and the labels, the scorefunction can be written as:f(x,y) = log p(x,y)= log p(x|y) + log p(y)=N?n=1log p(xn|yn)+N?n=1log p(yn|yn?1).The parameters log p(xn|yn) and log p(yn|yn?1)are usually estimated using maximum likelihoodor the EM algorithm.
Since parameter estimationlies outside the scope of this paper, a detailed de-scription is omitted.Discriminative models Recent years have seenthe emergence of discriminative training methodsfor sequence labeling (Lafferty et al, 2001; Taskeret al, 2003; Collins, 2002; Tsochantaridis et al,2005).
Among them, we focus on the perceptronalgorithm (Collins, 2002).
Although we do notdiscuss the other discriminative models, our algo-rithm is equivalently applicable to them.
The ma-jor difference between those models lies in param-eter estimation; the decoding process is virtuallythe same.In the perceptron, the score function f(x,y) isgiven as f(x,y) = w ?
?
(x,y) where w is theweight vector, and ?
(x,y) is the feature vectorrepresentation of the pair (x,y).
By making thefirst-order Markov assumption, we havef(x,y) = w ?
?
(x,y)=N?n=1K?k=1wk?k(x, yn?1, yn),where K = |?
(x,y)| is the number of features, ?kis the k-th feature function, and wk is the weightcorresponding to it.
Parameter w can be estimatedin the same way as in the conventional perceptronalgorithm.
See (Collins, 2002) for details.2.2 Viterbi decodingGiven the score function f(x,y), we have to lo-cate the best label sequence.
This is usually per-formed by applying the Viterbi algorithm.
Let?
(yn) be the best score of the partial label se-quence ending with yn.
The idea of the Viterbialgorithm is to use dynamic programming to com-pute ?(yn).
In HMMs, ?
(yn) can be can be de-fined asmaxyn?1{?
(yn?1) + log p(yn|yn?1)} + log p(xn|yn).Using this recursive definition, we can evaluate?
(yn) for all yn.
This results in the identificationof the best label sequence.Although the Viterbi algorithm is commonlyadopted in past studies, it is not always efficient.The computational cost of the Viterbi algorithm isO(NL2), where N is the input length and L isthe number of labels; it is efficient enough if Lis small.
However, if there are many labels, theViterbi algorithm becomes prohibitively slow be-cause of its quadratic dependence on L.2.3 Related workTo the best of our knowledge, the Viterbi algo-rithm is the only algorithm widely adopted in theNLP field that offers exact decoding.
In othercommunities, several exact algorithms have al-ready been proposed for handling large label sets.While they are successful to some extent, they de-mand strong assumptions that are unusual in NLP.Moreover, none were challenged with standardNLP tasks.Felzenszwalb et al (2003) presented a fastinference algorithm for HMMs based on the as-sumption that the hidden states can be embed-ded in a grid space, and the transition probabil-ity corresponds to the distance on that space.
Thistype of probability distribution is not common inNLP tasks.
Lifshits et al (2007) proposed acompression-based approach to speed up HMMdecoding.
It assumes that the input sequence ishighly repetitive.
Amongst others, CARPEDIEM(Esposito and Radicioni, 2009) is the algorithmclosest to our work.
It accelerates decoding byassuming that the adjacent labels are not stronglycorrelated.
This assumption is appropriate for486some NLP tasks.
For example, as suggested in(Liang et al, 2008), adjacent labels do not providestrong information in POS tagging.
However, theapplicability of this idea to other NLP tasks is stillunclear.Approximate algorithms, such as beam searchor island-driven search, have been proposed forspeeding up decoding.
Tsuruoka and Tsujii (2005)proposed easiest-first deterministic decoding.
Sid-diqi and Moore (2005) presented the parameter ty-ing approach for fast inference in HMMs.
A simi-lar idea was applied to CRFs as well (Cohn, 2006;Jeong et al, 2009).In general, approximate algorithms have the ad-vantage of speed over exact algorithms.
However,both types of algorithms are still widely adoptedby practitioners, since exact algorithms have mer-its other than speed.
First, the optimality of the so-lution is always guaranteed.
It is hard for most ofthe approximate algorithms to even bound the er-ror rate.
Second, approximate algorithms usuallyrequire hyperparameters, which control the trade-off between accuracy and efficiency (e.g., beamwidth), and these have to be manually adjusted.On the other hand, most of the exact algorithms,including ours, do not require such a manual ef-fort.Despite these advantages, exact algorithms arerarely used when dealing with a large number oflabels.
This is because exact algorithms becomeconsiderably slower than approximate algorithmsin such situations.
The paper presents an exact al-gorithm that avoids this problem; it provides theresearch community with another option for han-dling a lot of labels.3 AlgorithmThis section presents the new decoding algorithm.The key is to reduce the number of labels ex-amined.
Our algorithm locates the best label se-quence by iteratively solving labeling problemswith a reduced label set.
This results in signifi-cant time savings in practice, because each itera-tion becomes much more efficient than solving theoriginal labeling problem.
More importantly, ouralgorithm always obtains the exact solution.
Thisis because the algorithm allows us to check the op-timality of the solution achieved by using only thereduced label set.In the following discussions, we restrict our fo-cus to HMMs for presentation clarity.
Extension toA A A AB B B BCDCDCDCDE E E EF F F FG G G GH H H H(a)ABCDA ABABCD(b)Figure 1: (a) An example of a lattice, where theletters {A, B, C, D, E, F, G, H} represent labelsassociated with nodes.
(b) The degenerate lattice.the perceptron algorithm is presented in Section 4.3.1 Degenerate latticeWe begin by introducing the degenerate lattice,which plays a central role in our algorithm.
Con-sider the lattice in Figure 1(a).
Following conven-tion, we regard each path on the lattice as a labelsequence.
Note that the label set is {A, B, C, D,E, F, G, H}.
By aggregating several nodes in thesame column of the lattice, we can transform theoriginal lattice into a simpler form, which we callthe degenerate lattice (Figure 1(b)).Let us examine the intuition behind the degen-erate lattice.
Aggregating nodes can be viewed asgrouping several labels into a new one.
Here, alabel is referred to as an active label if it is not ag-gregated (e.g., A, B, C, and D in the first columnof Figure 1(b)), and otherwise as an inactive label(i.e., dotted nodes).
The new label, which is madeby grouping the inactive labels, is referred to asa degenerate label (i.e., large nodes covering thedotted ones).
Two degenerate labels can be seenas equivalent if their corresponding inactive labelsets are the same (e.g., degenerate labels in the firstand the last column).
In this approach, each pathof the degenerate lattice can also be interpreted asa label sequence.
In this case, however, the label tobe assigned is either an active label or a degeneratelabel.We then define the parameters associated withdegenerate label z.
For reasons that will becomeclear later, they are set to the maxima among theparameters of the inactive labels:log p(x|z) = maxy?
?I(z)log p(x|y?
), (1)log p(z|y) = maxy?
?I(z)log p(y?|y), (2)log p(y|z) = maxy?
?I(z)log p(y|y?
), (3)log p(z|z?)
= maxy??I(z),y???I(z?
)log p(y?|y??
), (4)487A A A AB B B BCDCDCDCDE E E EF F F FG G G GH H H H(a)ABCDA ABABCD(b)Figure 2: (a) The path y = {A, E, G, C} of theoriginal lattice.
(b) The path z of the degeneratelattice that corresponds to y.where y is an active label, z and z?
are degeneratelabels, and I(z) denotes one-to-one mapping fromz to its corresponding inactive label set.The degenerate lattice has an important prop-erty which is the key to our algorithm:Lemma 1.
If the best path of the degenerate lat-tice does not include any degenerate label, it isequivalent to the best path of the original lattice.Proof.
Let zmax be the best path of the degeneratelattice.
Our goal is to prove that if zmax does notinclude any degenerate label, then?y ?
Y, log p(x,y) ?
log p(x,zmax) (5)where Y is the set of all paths on the original lat-tice.
We prove this by partitioning Y into two dis-joint sets: Y0and Y1, where Y0is the subset ofY appearing in the degenerate lattice.
Notice thatzmax ?
Y0.
Since zmax is the best path of thedegenerate lattice, we have?y ?
Y0, log p(x,y) ?
log p(x,zmax).The equation holds when y = zmax.
We next ex-amine the label sequence y such that y ?
Y1.
Foreach path y ?
Y1, there exists a unique path z onthe degenerate lattice that corresponds to y (Fig-ure 2).
Therefore, we have?y ?
Y1, ?z ?
Z, log p(x,y) ?
log p(x,z)< log p(x,zmax)where Z is the set of all paths of the degeneratelattice.
The inequality log p(x,y) ?
log p(x,z)can be proved by using Equations (1)-(4).
Usingthese results, we can complete (5).A A A A(a)A ABABABB(b)A ABCDABABCDBCDCD(c)Figure 3: (a) The best path of the initial degeneratelattice, which is denoted by the line, is located.
(b)The active labels are expanded and the best path issearched again.
(c) The best path without degen-erate labels is obtained.3.2 Staggered decodingNowwe can describe our algorithm, which we callstaggered decoding.
The algorithm successivelyconstructs degenerate lattices and checks whetherthe best path includes degenerate labels.
In build-ing each degenerate lattice, labels with high prob-ability p(y), estimated from training data, are pref-erentially selected as the active label; the expecta-tion is that such labels are likely to belong to thebest path.
The algorithm is detailed as follows:Initialization step The algorithm starts by build-ing a degenerate lattice in which there is onlyone active label in each column.
We select la-bel y with the highest p(y) as the active label.Search step The best path of the degenerate lat-tice is located (Figure 3(a)).
This is doneby using the Viterbi algorithm (and pruningtechnique, as we describe in Section 3.3).
Ifthe best path does not include any degeneratelabel, we can terminate the algorithm since itis identical with the best path of the originallattice according to Lemma 1.
Otherwise, weproceed to the next step.Expansion step We double the number of the ac-tive labels in the degenerate lattice.
The newactive labels are selected from the current in-active label set in descending order of p(y).If the inactive label set becomes empty, wesimply reconstructed the original lattice.
Af-ter expanding the active labels, we go back tothe previous step (Figure 3(b)).
This proce-dure is repeated until the termination condi-tion in the search step is satisfied, i.e., the bestpath has no degenerate label (Figure 3(c)).Compared to the Viterbi algorithm, staggereddecoding requires two additional computations for488training.
First, we have to estimate p(y) so as toselect active labels in the initialization and expan-sion step.
Second, we have to compute the pa-rameters regarding degenerate labels according toEquations (1)-(4).
Both impose trivial computa-tion costs.3.3 PruningTo achieve speed-up, it is crucial that staggereddecoding efficiently performs the search step.
Forthis purpose, we can basically use the Viterbi algo-rithm.
In earlier iterations, the Viterbi algorithm isindeed efficient because the label set to be han-dled is much smaller than the original one.
In lateriterations, however, our algorithm drastically in-creases the number of labels, making Viterbi de-coding quite expensive.To handle this problem, we propose a method ofpruning the lattice nodes.
This technique is moti-vated by the observation that the degenerate latticeshares many active labels with the previous itera-tion.
In the remainder of Section3.3, we explainthe technique by taking the following steps:?
Section 3.3.1 examines a lower bound l suchthat l ?
maxylog p(x,y).?
Section 3.3.2 examines the maximum scoreMAX(yn) in case token xn takes label yn:MAX(yn) = maxy?n=ynlog p(x,y?).?
Section 3.3.3 presents our pruning procedure.The idea is that if MAX(yn) < l, then thenode corresponding to yn can be removedfrom consideration.3.3.1 Lower boundLower bound l can be trivially calculated in thesearch step.
This can be done by retaining thebest path among those consisting of only activelabels.
The score of that path is obviously thelower bound.
Since the search step is repeated un-til the termination criteria is met, we can updatethe lower bound at every search step.
As the it-eration proceeds, the degenerate lattice becomescloser to the original one, so the lower bound be-comes tighter.3.3.2 Maximum scoreThe maximum score MAX(yn) can be computedfrom the original lattice.
Let ?
(yn) be the bestscore of the partial label sequence ending with yn.Presuming that we traverse the lattice from left toright, ?
(yn) can be defined asmaxyn?1{?
(yn?1) + log p(yn|yn?1)} + log p(xn|yn).If we traverse the lattice from right to left, an anal-ogous score ??
(yn) can be defined aslog p(xn|yn) + maxyn+1{??
(yn+1) + log p(yn|yn+1)}.Using these two scores, we haveMAX(yn) = ?
(yn) + ??
(yn) ?
log p(xn|yn).Notice that updating ?
(yn) or ??
(yn) is equivalentto the forward or backward Viterbi algorithm, re-spectively.Although it is expensive to compute ?
(yn) and??
(yn), we can efficiently estimate their upperbounds.
Let ?
(yn) and ??
(yn) be scores analogousto ?
(yn) and ??
(yn) that are computed using thedegenerate lattice.
We have ?
(yn) ?
?
(yn) and??
(yn) ?
??
(yn), by following similar discussionsas raised in the proof of Lemma 1.
Therefore, wecan still check whether MAX(yn) is smaller than lby using ?
(yn) and ??
(yn):MAX(yn) = ?
(yn) + ??(yn)?
log p(xn|yn)?
?
(yn) + ??
(yn) ?
log p(xn|yn)< l.For the sake of simplicity, we assume that yn is anactive label.
Although we do not discuss the othercases, our pruning technique is also applicable tothem.
We just point out that, if yn is an inactivelabel, then there exists a degenerate label zn in then-th column such that yn ?
I(zn), and we can use?
(zn) and ??
(zn) instead of ?
(yn) and ??
(yn).We compute ?
(yn) and ??
(yn) by using theforward and backward Viterbi algorithm, respec-tively.
In the search step immediately followinginitialization, we perform the forward Viterbi al-gorithm to find the best path, that is, ?
(yn) isupdated for all yn.
In the next search step, thebackward Viterbi algorithm is carried out, and??
(yn) is updated.
In the succeeding search steps,these updates are alternated.
As the algorithm pro-gresses, ?
(yn) and ??
(yn) become closer to ?
(yn)and ??
(yn).3.3.3 Pruning procedureWe make use of the bounds in pruning the latticenodes.
To do this, we keep the values of l, ?
(yn)489and ??(yn).
They are set as l = ??
and ?
(yn) =??
(yn) = ?
in the initialization step, and are up-dated in the search step.
The lower bound l is up-dated at the end of the search step, while ?
(yn)and ??
(yn) can be updated during the running ofthe Viterbi algorithm.
When ?
(yn) or ??
(yn) ischanged, we check whether MAX(yn) < l holdsand the node is pruned if the condition is met.3.4 AnalysisWe provide here a theoretical analysis of staggereddecoding.
In the following proofs, L, V , and Nrepresent the number of original labels, the num-ber of distinct tokens, and the length of input tokensequence, respectively.
To simplify the discussion,we assume that log2L is an integer (e.g., L = 64).We first introduce three lemmas:Lemma 2.
Staggered decoding requires at most(log2L + 1) iterations to terminate.Proof.
We have 2m?1 active labels in the m-thsearch step (m = 1, 2 .
.
.
), which means we haveL active labels and no degenerate labels in the(log2L + 1)-th search step.
Therefore, the algo-rithm always terminates within (log2L + 1) itera-tions.Lemma 3.
The number of degenerate labels islog2L.Proof.
Since we create one new degenerate labelin all but the last expansion step, we have log2Ldegenerate labels.Lemma 4.
The Viterbi algorithm requires O(L2+LV ) memory space and has O(NL2) time com-plexity.Proof.
Since we need O(L2) and O(LV ) space tokeep the transition and emission probability ma-trices, we need O(L2 + LV ) space to performthe Viterbi algorithm.
The time complexity of theViterbi algorithm is O(NL2) since there are NLnodes in the lattice and it takes O(L) time to eval-uate the score of each node.The above statements allow us to establish ourmain results:Theorem 1.
Staggered decoding requires O(L2+LV ) memory space.Proof.
Since we have L original labels and log2Ldegenerate labels, staggered decoding requiresO((L+log2L)2+(L+log2L)V ) = O(L2+LV )A A A A(a)A ABABAB(b)A ABCDABABCD(c)Figure 4: Staggered decoding with column-wiseexpansion: (a) The best path of the initial degen-erate lattice, which does not pass through the de-generate label in the first column.
(b) Column-wise expansion is performed and the best path issearched again.
Notice that the active label in thefirst column is not expanded.
(c) The final result.memory space to perform Viterbi decoding in thesearch step.Theorem 2.
Staggered decoding has O(N) bestcase time complexity and O(NL2)worst case timecomplexity.Proof.
To perform the m-th search step, staggereddecoding requires the order of O(N4m?1) timebecause we have 2m?1 active labels.
Therefore, ithas O(?Mm=1 N4m?1) time complexity if it termi-nates after the M -th search step.
In the best case,M = 1, the time complexity is O(N).
In the worstcase, M = log2L + 1, the time complexity is theorder of O(NL2) because?log2L+1m=1 N4m?1 <43NL2.Theorem 1 shows that staggered decodingasymptotically requires the same order of mem-ory space as the Viterbi algorithm.
Theorem 2 re-veals that staggered decoding has the same orderof time complexity as the Viterbi algorithm evenin the worst case.3.5 Heuristic techniquesWe present two heuristic techniques for furtherspeeding up our algorithm.First, we can initialize the value of lower boundl by selecting a path from the original lattice insome way, and then computing the score of thatpath.
In our experiments, we use the path lo-cated by the left-to-right deterministic decoding(i.e., beam search with a beam width of 1).
Al-though this method requires an additional cost tolocate the path, it is very effective in practice.
Ifl is initialized in this manner, the best case timecomplexity of our algorithm becomes O(NL).490The second technique is for the expansion step.Instead of the expansion technique described inSection 3.2, we can expand the active labels in aheuristic manner to keep the number of active la-bels small:Column-wise expansion step We double thenumber of the active labels in the columnonly if the best path of the degenerate latticepasses through the degenerate label of thatcolumn (Figure 4).A drawback of this strategy is that the algorithmrequires N(log2L+1) iterations in the worst case.As the result, we can no longer derive a reasonableupper bound for the time complexity.
Neverthe-less, column-wise expansion is highly effective inpractice as we will demonstrate in the experiment.Note that Theorem 1 still holds true even if we usecolumn-wise expansion.4 Extension to the PerceptronThe discussion we have made so far can be appliedto perceptrons.
This can be clarified by comparingthe score functions f(x,y).
In HMMs, the scorefunction can be written asN?n=1{log(xn|yn) + log(yn|yn?1)}.In perceptrons, on the other hand, it is given asN?n=1{?kw1k?1k(x, yn) +?kw2k?2k(x, yn?1, yn)}where we explicitly distinguish the unigram fea-ture function ?1k and bigram feature function ?2k.Comparing the form of the two functions, we cansee that our discussion on HMMs can be extendedto perceptrons by substituting?k w1k?1k(x, yn)and?k w2k?2k(x, yn?1, yn) for log p(xn|yn) andlog p(yn|yn?1).However, implementing the perceptron algo-rithm is not straightforward.
The problem isthat it is difficult, if not impossible, to compute?k w1k?1k(x, y) and?k w2k?2k(x, y, y?)
offline be-cause they are dependent on the entire token se-quence x, unlike log p(x|y) and log p(y|y?).
Con-sequently, we cannot evaluate the maxima analo-gous to Equations (1)-(4) offline either.For unigram features, we compute the maxi-mum, maxy?k w1k?1k(x, y), as a preprocess inthe initialization step (cf.
Equation (1)).
This pre-process requires O(NL) time, which is negligiblecompared with the cost required by the Viterbi al-gorithm.Unfortunately, we cannot use the same tech-nique for computing maxy,y?
?k w2k?2k(x, y, y?
)because a similar computation would takeO(NL2) time (cf.
Equation (4)).
For bigram fea-tures, we compute its upper bound offline.
For ex-ample, the following bound was proposed by Es-posito and Radicioni (2009):maxy,y?
?kw2k?2k(x, y, y?)
?
maxy,y??kw2k?
(0 < w2k)where ?(?)
is the delta function and the summa-tions are taken over all feature functions associatedwith both y and y?.
Intuitively, the upper boundcorresponds to an ideal case in which all featureswith positive weight are activated.3 It can be com-puted without any task-specific knowledge.In practice, however, we can compute betterbounds based on task-specific knowledge.
Thesimplest case is that the bigram features are inde-pendent of the token sequence x.
In such a situ-ation, we can trivially compute the exact maximaoffline, as we did in the case of HMMs.
Fortu-nately, such a feature set is quite common in NLPproblems and we could use this technique in ourexperiments.
Even if bigram features are depen-dent on x, it is still possible to compute betterbounds if several features are mutually exclusive,as discussed in (Esposito and Radicioni, 2009).Finally, it is worth noting that we can use stag-gered decoding in training perceptrons as well, al-though such application lies outside the scope ofthis paper.
The algorithm does not support train-ing acceleration for other discriminative models.5 Experiments and Discussion5.1 SettingThe proposed algorithm was evaluated with threetasks: POS tagging, joint POS tagging and chunk-ing (called joint tagging for short), and supertag-ging.
To reduce joint tagging into a single se-quence labeling problem, we produced the labelsby concatenating the POS tag and the chunk tag(BIO format), e.g., NN/B-NP.
In the two tasksother than supertagging, the input token is theword.
In supertagging, the token is the pair of theword and its oracle POS tag.3We assume binary feature functions.491Table 1: Decoding speed (sent./sec).POS tagging Joint tagging SupertaggingVITERBI 4000 77 1.1CARPEDIEM 8600 51 0.26SD 8800 850 121SD+C-EXP.
14,000 1600 300The data sets we used for the three experimentsare the Penn TreeBank (PTB) corpus, CoNLL2000 corpus, and an HPSG treebank built from thePTB corpus (Matsuzaki et al, 2007).
We used sec-tions 02-21 of PTB for training, and section 23 fortesting.
The number of labels in the three tasks is45, 319 and 2602, respectively.We used the perceptron algorithm for train-ing.
The models were averaged over 10 itera-tions (Collins, 2002).
For features, we basicallyfollowed previous studies (Tsuruoka and Tsujii,2005; Sha and Pereira, 2003; Ninomiya et al,2006).
In POS tagging, we used unigrams of thecurrent and its neighboring words, word bigrams,prefixes and suffixes of the current word, capital-ization, and tag bigrams.
In joint tagging, we alsoused the same features.
In supertagging, we usedPOS unigrams and bigrams in addition to the samefeatures other than capitalization.As the evaluation measure, we used the averagedecoding speed (sentences/sec) to two significantdigits over five trials.
To strictly measure the timespent for decoding, we ignored the preprocessingtime, that is, the time for loading the model fileand converting the features (i.e., strings) into inte-gers.
We note that the accuracy was comparable tothe state-of-the-art in the three tasks: 97.08, 93.21,and 91.20% respectively.5.2 Results and discussionsTable 1 presents the performance of our algo-rithm.
SD represents the proposed algorithm with-out column-wise expansion, while SD+C-EXP.uses column-wise expansion.
For comparison, wepresent the results of two baseline algorithms aswell: VITERBI and CARPEDIEM (Esposito andRadicioni, 2009).
In almost all settings, we seethat both of our algorithms outperformed the othertwo.
We also find that SD+C-EXP.
performed con-sistently better than SD.
This indicates the effec-tiveness of column-wise expansion.Following VITERBI, CARPEDIEM is the mostrelevant algorithm, for sequence labeling in NLP,as discussed in Section 2.3.
However, our resultsTable 2: The average number of iterations.POS tagging Joint tagging SupertaggingSD 6.02 8.15 10.0SD+C-EXP.
6.12 8.62 10.6Table 3: Training time.POS tagging Joint tagging SupertaggingVITERBI 100 sec.
20 min.
100 hourSD+C-EXP.
37 sec.
1.5 min.
5.3 hourdemonstrated that CARPEDIEM worked poorly intwo of the three tasks.
We consider this is becausethe transition information is crucial for the twotasks, and the assumption behind CARPEDIEM isviolated.
In contrast, the proposed algorithms per-formed reasonably well for all three tasks, demon-strating the wide applicability of our algorithm.Table 2 presents the average iteration num-bers of SD and SD+C-EXP.
We can observethat the two algorithms required almost the samenumber of iterations on average, although theiteration number is not tightly bounded if weuse column-wise expansion.
This indicates thatSD+C-EXP.
virtually avoided performing extra it-erations, while heuristically restricting active labelexpansion.Table 3 compares the training time spent byVITERBI and SD+C-EXP.
Although speeding upperceptron training is a by-product, it is interest-ing to see that our algorithm is in fact effective atreducing the training time as well.
The result alsoindicates that the speed-up is more significant attest time.
This is probably because the model isnot predictive enough at the beginning of training,and the pruning is not that effective.5.3 Comparison with approximate algorithmTable 4 compares two exact algorithms (VITERBIand SD+E-XP.)
with beam search, which is the ap-proximate algorithm widely adopted for sequencelabeling in NLP.
For this experiment, the beamwidth, B, was exhaustively calibrated: we tried B= {1, 2, 4, 8, ...} until the beam search achievedcomparable accuracy to the exact algorithms, i.e.,the difference fell below 0.1 in our case.We see that there is a substantial difference inthe performance between VITERBI and BEAM.On the other hand, SD+C-EXP.
reached speedsvery close to those of BEAM.
In fact, theyachieved comparable performance in our exper-iment.
These results demonstrate that we couldsuccessfully bridge the gap in the performance be-492Table 4: Comparison with beam search (sent./sec).POS tagging Joint tagging SupertaggingVITERBI 4000 77 1.1SD+C-EXP.
14,000 1600 300BEAM 18,000 2400 180tween exact and approximate algorithms, while re-taining the advantages of exact algorithms.6 Relation to coarse-to-fine approachBefore concluding remarks, we briefly examinethe relationship between staggered decoding andcoarse-to-fine PCFG parsing (2006).
In coarse-to-fine parsing, the candidate parse trees are prunedby using the parse forest produced by a coarse-grained PCFG.
Since the degenerate label can beinterpreted as a coarse-level label, one may con-sider that staggered decoding is an instance ofcoarse-to-fine approach.
While there is some re-semblance, there are at least two essential differ-ences.
First, coarse-to-fine approach is a heuristicpruning, that is, it is not an exact algorithm.
Sec-ond, our algorithm does not always perform de-coding at the fine-grained level.
It is designed tobe able to stop decoding at the coarse-level.7 ConclusionsThe sequence labeling algorithm is indispensableto modern statistical NLP.
However, the Viterbialgorithm, which is the standard decoding algo-rithm in NLP, is not efficient when we have todeal with a large number of labels.
In this paperwe presented staggered decoding, which providesa principled way of resolving this problem.
Weconsider that it is a real alternative to the Viterbialgorithm in various NLP tasks.An interesting future direction is to extend theproposed technique to handle more complex struc-tures than the Markov chains, including semi-Markov models and factorial HMMs (Sarawagiand Cohen, 2004; Sutton et al, 2004).
We hopethis work opens a new perspective on decoding al-gorithms for a wide range of NLP problems, notjust sequence labeling.AcknowledgementWe wish to thank the anonymous reviewers fortheir helpful comments, especially on the com-putational complexity of our algorithm.
We alsothank Yusuke Miyao for providing us with theHPSG Treebank data.ReferencesThorsten Brants.
2000.
TnT - a statistical part-of-speech tagger.
In Proceedings of ANLP, pages 224?231.Eugene Charniak, Mark Johnson, Micha Elsner, JosephAusterweil, David Ellis, Isaac Haxton, CatherineHill, R. Shrivaths, Jeremy Moore, Michael Pozar,and Theresa Vu.
2006.
Multi-level coarse-to-finePCFG parsing.
In Proceedings of NAACL, pages168?175.Trevor Cohn.
2006.
Efficient inference in large con-ditional random fields.
In Proceedings of ECML,pages 606?613.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1?8.Thomas G. Dietterich, Pedro Domingos, Lise Getoor,Stephen Muggleton, and Prasad Tadepalli.
2008.Structured machine learning: the next ten years.Machine Learning, 73(1):3?23.Roberto Esposito and Daniele P. Radicioni.
2009.CARPEDIEM: Optimizing the Viterbi algorithmand applications to supervised sequential learning.Jorunal of Machine Learning Research, 10:1851?1880.Pedro F. Felzenszwalb, Daniel P. Huttenlocher, andJon M. Kleinberg.
2003.
Fast algorithms for large-state-space HMMs with applications to Web usageanalysis.
In Proceedings of NIPS, pages 409?416.Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Efficient inference of CRFs for large-scalenatural language data.
In Proceedings of ACL-IJCNLP Short Papers, pages 281?284.John Lafferty, Andrew McCallum, and FernandPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML, pages 282?289.Percy Liang, Hal Daume?
III, and Dan Klein.
2008.Structure compilation: Trading structure for fea-tures.
In Proceedings of ICML, pages 592?599.Yury Lifshits, ShayMozes, OrenWeimann, andMichalZiv-Ukelson.
2007.
Speeding up HMM decod-ing and training by exploiting sequence repetitions.Computational Pattern Matching, pages 4?15.Dekang Lin and Xiaoyun Wu.
2009.
Phrae clusteringfor discriminative training.
In Proceedings of ACL-IJCNLP, pages 1030?1038.493Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-jii.
2007.
Efficient HPSG parsing with supertaggingand CFG-filtering.
In Proceedings of IJCAI, pages1671?1676.Takashi Ninomiya, TakuyaMatsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.Extremely lexicalized models for accurate and fastHPSG parsing.
In Proceedings of EMNLP, pages155?163.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
In Proceedings of The IEEE, pages257?286.Sunita Sarawagi and Willian W. Cohen.
2004.
Semi-Markov conditional random fields for informationextraction.
In Proceedings of NIPS, pages 1185?1192.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings ofHLT-NAACL, pages 134?141.Sajid M. Siddiqi and Andrew W. Moore.
2005.
Fastinference and learning in large-state-space HMMs.In Proceedings of ICML, pages 800?807.Charles Sutton, Khashayar Rohanimanesh, and An-drew McCallum.
2004.
Dynamic conditional ran-dom fields: Factorized probabilistic models for la-beling and segmenting sequence data.
In Proceed-ings of ICML.Ben Tasker, Carlos Guestrin, and Daphe Koller.
2003.Max-margin Markov networks.
In Proceedings ofNIPS, pages 25?32.Ioannis Tsochantaridis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent outputvariables.
Journal of Machine Learning Research,6:1453?1484.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategyfor tagging sequence data.
In Proceedings ofHLT/EMNLP, pages 467?474.Andrew J. Viterbi.
1967.
Error bounds for convo-lutional codes and an asymeptotically optimum de-coding algorithm.
IEEE Transactios on InformationTheory, 13(2):260?267.494
