Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 121?124,Prague, June 2007. c?2007 Association for Computational LinguisticsCMU-AT: Semantic Distance and Background Knowledge for Identify-ing Semantic RelationsAlicia TribbleLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, USAatribble@cs.cmu.eduScott E. FahlmanLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, USAsef@cs.cmu.eduAbstractThis system uses a background knowledgebase to identify semantic relations betweenbase noun phrases in English text, as eva-luated in SemEval 2007, Task 4.
Trainingdata for each relation is converted to state-ments in the Scone Knowledge Representa-tion Language.
At testing time a newScone statement is created for the sentenceunder scrutiny, and presence or absence ofa relation is calculated by comparing thetotal semantic distance between the newstatement and all positive examples to thetotal distance between the new statementand all negative examples.1 IntroductionThis paper introduces a knowledge-based approachto the task of semantic relation classification, asevaluated in SemEval 2007, Task 4: ?ClassifyingRelations Between Nominals?.
In Task 4, a fullsentence is presented to the system, along with theWordNet sense keys for two noun phrases whichappear there and the name of a semantic relation(e.g.
?cause-effect?).
The system should return?true?
if a person reading the sentence would con-clude that the relation holds between the two la-beled noun phrases.Our system represents a test sentence with a se-mantic graph, including the relation being testedand both of its proposed arguments.
Semantic dis-tance is calculated between this graph and a set ofgraphs representing the training examples relevantto the test sentence.
A near-match between a testsentence and a positive training example is evi-dence that the same relation which holds in theexample also holds in the test.
We compute se-mantic distances to negative training examples aswell, comparing the total positive and negativescores in order to decide whether a relation is trueor false in the test sentence.2 MotivationMany systems which perform well on related tasksuse syntactic features of the input sentence,coupled with classification by machine learning.This approach has been applied to problems likecompound noun interpretation (Rosario and Hearst2001) and semantic role labeling (Gildea and Ju-rafsky 2002).In preparing our system for Task 4, we startedby applying a similar syntax-based feature analysisto the trial data: 140 labeled examples of the rela-tion ?content-container?.
In 10-fold cross-validation  with this data we achieved an average f-score of 70.6, based on features similar to the sub-set trees used for semantic role labeling in (Mo-schitti 2004).
For classification we applied the up-dated tree-kernel package (Moschitti 2006), distri-buted with the svm-light tool (Joachims 1999) forlearning Support Vector Machines (SVMs).Training data for Task 4 is small, compared toother tasks where machine learning is commonlyapplied.
We had difficulty finding a combinationof features which gave good performance in cross-validation, but which did not result in a separatesupport vector being stored for every training sen-tence ?
a possible indicator of overfitting.
As anexample, the ratio of support vectors to training121examples for the experiment described above was.97, nearly 1-to-1.As a result of this analysis we started work onour knowledge-based system, with the goal of us-ing the two approaches together.
We were alsomotivated by an interest in using relation defini-tions and background knowledge from WordNet togreater advantage.
The algorithm we used in ourfinal submission is similar to recent systems whichdiscover textual entailment relationships (Haghig-hi, Ng et al 2005; Zanzotto and Moschitti 2006).It gives us a way to encode information from therelation definitions directly, in the form of state-ments in a knowledge representation language.The inference rules that are learned by this systemfrom training examples are also easier to interpretthan the models generated by an SVM.
In small-data applications this can be an advantage.3 System Description: A Walk-ThroughThe example sentence below is taken (in abbre-viated form) from the training data for Task 4, Re-lation 7 ?Content-Container?
(Girju, Hearst et al2007):The kitchen holds a cooker.We convert this positive example into a semanticgraph by creating a new instance of the relationContains and linking that instance to the WordNetterm for each labeled argument ("kitch-en%1:06:00::", "cooker%1:06:00::").
The result isshown in Figure 1.
WordNet sense keys (Fellbaum1998) have been mapped to a term, a part ofspeech (pos), and a sense number.Contains{relation}kitchen_n_1cont iner contentcooker_n_1Figure 1.
Semantic graph for the training example"The kitchen holds a cooker".
Arguments arerepresented by a WordNet term, part of speech,and sense number.This graph is instantiated as a statement usingthe Scone Knowledge Representation System, or(new-statement {kitchen_n_1} {contains} {cooker_n_1})(new-statement {artifact_n_1} {contains} {artifact_n_1})(new-statement  {whole_n_1}   {contains}  {whole_n_1})Figure 2.
Statements in Scone KR syntax, basedon generalizing the training example "The kitchenholds a cooker".?Scone?
(Fahlman 2005).
Scone gives us a way tostore, search, and perform inference on graphs likethe one shown above.
After instantiating the graphwe generalize it using hypernym information fromWordNet.
This generates additional Scone state-ments which are stored in a knowledge base (KB),shown in Figure 2.
The first statement in the fig-ure was generated verbatim from our training sen-tence.
The remaining statements contain hyper-nyms of the original arguments.For each argument seen in training, we also ex-tract hypernyms and siblings from WordNet.
Forthe argument kitchen, we extract 101 ancestors(artifact, whole, object, etc.)
and siblings (struc-ture, excavation, facility, etc.).
A similar set ofWordNet entities is extracted for the argumentcooker.
These entities, with repetitions removed,are encoded in a second Scone knowledge base,preserving the hierarchical (IS-A) links that comefrom WordNet.
The hierarchy is manually linkedat the top level into an existing background SconeKB where entities like animate, inanimate, person,location, and quantity are already defined.After using the training data to create these twoKBs, the system is  ready for a test sentence.
Thefollowing example is also adapted from SemEvalTask 4 training data:Equipment was carried in a box.First we convert the sentence to a semanticgraph, using the same technique as the one de-scribed above.
The graph is implemented as a newScone statement which includes the WordNet posand sense number for each of the arguments:?box_n_1 contains equipment_n_1?.Next, using inference operations in Scone, thesystem verifies that the statement conforms tohigh-level constraints imposed by the relation defi-nition.
If it does, we calculate semantic distancesbetween the argument nodes of our test statementand the analogous nodes in relevant training state-ments.
A training statement is relevant if both ofits arguments are ancestors of the appropriate ar-122guments of the test sentence.
In our example, onlytwo of the three KB statements from Figure 2 arerelevant to the test statement ?box contains equip-ment?
: ?whole contains whole?
and ?artifact con-tains artifact?.
The first statement, ?kitchen con-tains cooker?
fails to apply because kitchen is notan ancestor of box, and also because cooker is notan ancestor of equipment.Figure 3 illustrates the distance from ?box con-tains equipment?
to ?whole contains whole?, calcu-lated as the sum of the distances between box-whole and equipment-whole.Contains{relation}box equipmentcontainer contentrtifact artifactContains{relation}whole wholecontainer contentDistance = 2Support = 1/2Distance = 2Support = 1/2Figure 3.
Calculating the distance through theknowledge base between "equipment contains box"and ?whole contains whole?.
Dashed lines indicateIS-A links in the knowledge base.The total number of these relevant, positivetraining statements is an indicator of ?support?
forthe test sentence throughout the training data.
Thedistance between one such statement and the testsentence is a measure of the strength of support.To reach a verdict, we sum over the inverse dis-tances to all arguments from positive relevant ex-amples: in Figure 3, the test statement ?box con-tains equipment?
receives a support score of  (?
+?
+ 1 + 1), or 3.Counter-evidence for a test sentence can be cal-culated in the same way, using relevant negativestatements.
In our example there are no negativetraining statements, so the total positive supportscore (3) is greater than the counter-evidence score(0), and the system verdict is ?true?.4 System Components in DetailAs the detailed example above shows, this systemis designed around its knowledge bases.
The KBsprovide a consistent framework for representingknowledge from a variety of sources as well as forcalculating semantic distance.4.1 Background knowledgeWordNet-extracted knowledge bases of the typedescribed in Section 3 are generated separately foreach relation.
Average depth of these hierarchiesis 4; we store only hypernyms of WordNet depth 7and above, based on experiments in the literatureby Nastase, et al (2003; 2006).Relation-specific and task-specific knowledge isencoded by hand.
For each relation, we examinethe relation definition and create a set of con-straints in Scone formalism.
For example, the de-finition of ?container-contains?
includes the fol-lowing restriction (taken from training data forTask 4): There is strong preference against treat-ing legal entities (people and institutions) as con-tent.In Scone, we encode this preference as a typerestriction on the container role of any Containsrelation: (new-is-not-a {container} {potentialagent})During testing, before calculating semantic dis-tances, the system checks whether the test state-ment conforms to all such constraints.4.2 Calculating semantic distanceSemantic distances are calculated between con-cepts in the knowledge base, rather than throughWordNet directly.
Distance between two KB en-tites is calculated by counting the edges along theshortest path between them, as illustrated in Figure3.
In the current implementation, only ancestors inthe IS-A hierarchy are considered relevant, so thiscalculation amounts to counting the number of an-cestors between an argument from the test sentenceand an argument from a training example.
Quicktype-checking features which are built into Sconeallow us to skip the distance calculation for non-relevant training examples.5 Results & ConclusionsThis system performed reasonably well for relation3, Product-Producer, outperforming the baseline(baseline guesses ?true?
for every test sentence).Performance for this relation was also higher thanthe average F-score for all comparable groups inTask 4 (all groups in class ?B4?).
Average recallfor this system over all relations was mid-range,123compared to other participating groups.
Averageprecision and average f-score fell below the base-line and below the average for all comparablegroups.
These scores are given in Table 1.Relation  R P F1.
Cause-Effect 73.2 54.5 62.52.
Instrument-Agency 76.3 50.9 61.13.
Product-Producer 79.0 71.0 74.84.
Origin-Entity 63.9 54.8 59.05.
Theme-Tool 48.3 53.8 50.96.
Part-Whole 57.7 45.5 50.87.
Content-Container 68.4 59.1 63.4Whole test set, notdivided by relation57.1 68.9 62.4Average for CMU-AT 66.7 55.7 60.4Average for all B4systems64.4 65.3 63.6Baseline: ?alltrue?
100.0   48.5 64.8Table 1.
Recall, Precision, and F-scores, separatedby relation type.
Baseline score is calculated byguessing "true" for all test setences.Analysis of the training data reveals that relation3 is the class where target nouns occur most oftentogether in nominal compounds and base NPs, withlittle additional syntax to connect them.
Whileother relations included sentences where the targetswere covered by a single VP, Product-Producer didnot.
It seems that background knowledge plays alarger role in identifying the Producer-Producesrelationship than it does for other relations.
How-ever this conclusion is softened by the fact that wealso spent more time in development and cross-evaluation for relations 3 and 7, our two best per-forming relations.This system demonstrates a knowledge-basedframework  that performs very well for certain re-lations.
Importantly, the system we submitted forevaluation did not make use of syntactic features,which are almost certainly relevant to this task.We are already exploring methods for combiningthe knowledge-based decision process with onethat uses syntactic evidence as well as corpus sta-tistics, described in Section 2.AcknowledgementThis work was supported by a generous researchgrant from Cisco Systems, and by the Defense Ad-vanced Research Projects Agency (DARPA) undercontract number NBCHD030010.ReferencesFahlman, S. E. (2005).
Scone User's Manual.Fellbaum, C. (1998).
WordNet An Electronic LexicalDatabase, Bradford Books.Gildea, D. and D. Jurafsky (2002).
"Automatic labelingof semantic roles."
Computational Linguistics 28(3):245-288.Girju, R., M. Hearst, et al (2007).
Classification of Se-mantic Relations between Nominals: Dataset forTask 4.
SemEval 2007, 4th International Workshopon Semantic Evaluations, Prague, Czech Republic.Haghighi, A., A. Ng, et al (2005).
Robust Textual Infe-rence via Graph Matching.
Human Language Tech-nology Conference and Conference on EmpiricalMethods in Natural Language Processing, Vancou-ver, British Columbia, Canada.Joachims, T. (1999).
Making large-scale SVM learningpractical.
Advances in Kernel Methods - SupportVector Learning.
B. Sch?lkopf, C. Burges and A.Smola.Moschitti, A.
(2004).
A study on Convolution Kernelfor Shallow Semantic Parsing.
proceedings of the42nd Conference of the Association for Computa-tional   Linguistics (ACL-2004).
Barcelona, Spain.Moschitti, A.
(2006).
Making tree kernels practical fornatural language learning.
Eleventh InternationalConference on European Association for Computa-tional Linguistics, Trento, Italy.Nastase, V., J. S. Shirabad, et al (2006).
Learning noun-modifier semantic relations with corpus-based andWordnet-based features.
21st National Conference onArtificial Intelligence (AAAI-06), Boston, Massa-chusetts.Nastase, V. and S. Szpakowicz (2003).
Exploring noun-modifier semantic relations.
IWCS 2003.Rosario, B. and M. Hearst (2001).
Classifying the se-mantic relations in Noun Compounds.
2001 Confe-rence on Empirical Methods in Natural LanguageProcessing.Zanzotto, F. M. and A. Moschitti (2006).
AutomaticLearning of Textual Entailments with Cross-Pair Si-milarities.
the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the Association for Computational Linguistics(ACL), Sydney, Austrailia.124
