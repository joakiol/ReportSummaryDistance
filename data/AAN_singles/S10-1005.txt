Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 27?32,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 7: Argument Selection and CoercionJames Pustejovsky and Anna Rumshisky and Alex PlotnickDept.
of Computer ScienceBrandeis UniversityWaltham, MA, USAElisabetta JezekDept.
of LinguisticsUniversity of PaviaPavia, ItalyOlga BatiukovaDept.
of HumanitiesCarlos III University of MadridMadrid, SpainValeria QuochiILC-CNRPisa, ItalyAbstractWe describe the Argument Selection andCoercion task for the SemEval-2010 eval-uation exercise.
This task involves char-acterizing the type of compositional oper-ation that exists between a predicate andthe arguments it selects.
Specifically, thegoal is to identify whether the type thata verb selects is satisfied directly by theargument, or whether the argument mustchange type to satisfy the verb typing.
Wediscuss the problem in detail, describe thedata preparation for the task, and analyzethe results of the submissions.1 IntroductionIn recent years, a number of annotation schemesthat encode semantic information have been de-veloped and used to produce data sets for trainingmachine learning algorithms.
Semantic markupschemes that have focused on annotating entitytypes and, more generally, word senses, havebeen extended to include semantic relationshipsbetween sentence elements, such as the seman-tic role (or label) assigned to the argument by thepredicate (Palmer et al, 2005; Ruppenhofer et al,2006; Kipper, 2005; Burchardt et al, 2006; Subi-rats, 2004).In this task, we take this one step further andattempt to capture the ?compositional history?
ofthe argument selection relative to the predicate.
Inparticular, this task attempts to identify the oper-ations of type adjustment induced by a predicateover its arguments when they do not match its se-lectional properties.
The task is defined as fol-lows: for each argument of a predicate, identifywhether the entity in that argument position satis-fies the type expected by the predicate.
If not, thenidentify how the entity in that position satisfies thetyping expected by the predicate; that is, identifythe source and target types in a type-shifting or co-ercion operation.Consider the example below, where the verb re-port normally selects for a human in subject po-sition, as in (1a).
Notice, however, that througha metonymic interpretation, this constraint can beviolated, as demonstrated in (1b).
(1) a. John reported in late from Washington.b.
Washington reported in late.Neither the surface annotation of entity extentsand types nor assigning semantic roles associatedwith the predicate would reflect in this case a cru-cial point: namely, that in order for the typingrequirements of the predicate to be satisfied, atype coercion or a metonymy (Hobbs et al, 1993;Pustejovsky, 1991; Nunberg, 1979; Egg, 2005)has taken place.The SemEval Metonymy task (Markert and Nis-sim, 2007) was a good attempt to annotate suchmetonymic relations over a larger data set.
Thistask involved two types with their metonymicvariants: categories-for-locations (e.g., place-for-people) and categories-for-organizations (e.g.,organization-for-members).
One of the limitationsof this approach, however, is that while appropri-ate for these specialized metonymy relations, theannotation specification and resulting corpus arenot an informative guide for extending the annota-tion of argument selection more broadly.In fact, the metonymy example in (1) is an in-stance of a much more pervasive phenomenon oftype shifting and coercion in argument selection.For example, in (2) below, the sense annotationfor the verb enjoy should arguably assign similarvalues to both (2a) and (2b).27Figure 1: The MATTER Methodology(2) a. Mary enjoyed drinking her beer.b.
Mary enjoyed her beer.The consequence of this is that under current senseand role annotation strategies, the mapping to asyntactic realization for a given sense is mademore complex, and is in fact perplexing for a clus-tering or learning algorithm operating over subcat-egorization types for the verb.2 Methodology of AnnotationBefore introducing the specifics of the argumentselection and coercion task, we will briefly reviewour assumptions regarding the role of annotationin computational linguistic systems.We assume that the features we use for encodinga specific linguistic phenomenon are rich enoughto capture the desired behavior.
These linguisticdescriptions are typically distilled from extensivetheoretical modeling of the phenomenon.
The de-scriptions in turn form the basis for the annota-tion values of the specification language, whichare themselves the features used in a developmentcycle for training and testing a labeling algorithmover a text.
Finally, based on an analysis and eval-uation of the performance of a system, the modelof the phenomenon may be revised.We call this cycle of development the MATTERmethodology (Fig.
1):Model: Structural descriptions provide theoretically in-formed attributes derived from empirical observationsover the data;Annotate: Annotation scheme assumes a feature set that en-codes specific structural descriptions and properties ofthe input data;Train: Algorithm is trained over a corpus annotated with thetarget feature set;Test: Algorithm is tested against held-out data;Evaluate: Standardized evaluation of results;Revise: Revisit the model, annotation specification, or algo-rithm, in order to make the annotation more robust andreliable.Some of the current and completed annotation ef-forts that have undergone such a development cy-cle include PropBank (Palmer et al, 2005), Nom-Bank (Meyers et al, 2004), and TimeBank (Puste-jovsky et al, 2005).3 Task DescriptionThe argument selection and coercion (ASC) taskinvolves identifying the selectional mechanismused by the predicate over a particular argument.1For the purposes of this task, the possible relationsbetween the predicate and a given argument are re-stricted to selection and coercion.
In selection, theargument NP satisfies the typing requirements ofthe predicate, as in (3):(3) a.
The spokesman denied the statement (PROPOSI-TION).b.
The child threw the stone (PHYSICAL OBJECT).c.
The audience didn?t believe the rumor (PROPOSI-TION).Coercion occurs when a type-shifting operationmust be performed on the complement NP in orderto satisfy selectional requirements of the predicate,as in (4).
Note that coercion operations may applyto any argument position in a sentence, includingthe subject, as seen in (4b).
Coercion can also beseen as an object of a proposition, as in (4c).
(4) a.
The president denied the attack (EVENT?
PROPO-SITION).b.
The White House (LOCATION ?
HUMAN) deniedthis statement.c.
The Boston office called with an update (EVENT?INFO).In order to determine whether type-shifting hastaken place, the classification task must then in-volve (1) identifying the verb sense and the asso-ciated syntactic frame, (2) identifying selectionalrequirements imposed by that verb sense on thetarget argument, and (3) identifying the semantictype of the target argument.4 Resources and Corpus DevelopmentWe prepared the data for this task in two phases:the data set construction phase and the annotationphase (see Fig.
2).
The first phase consisted of(1) selecting the target verbs to be annotated andcompiling a sense inventory for each target, and(2) data extraction and preprocessing.
The pre-pared data was then loaded into the annotation in-terface.
During the annotation phase, the annota-tion judgments were entered into the database, andan adjudicator resolved disagreements.
The result-ing database was then exported in an XML format.1This task is part of a larger effort to annotate text withcompositional operations (Pustejovsky et al, 2009).28Figure 2: Corpus Development Architecture4.1 Data Set Construction Phase: EnglishFor the English data set, the data constructionphase was combined with the annotation phase.The data for the task was created using the fol-lowing steps:1.
The verbs were selected by examining the datafrom the BNC, using the Sketch Engine (Kilgar-riff et al, 2004) as described in (Rumshisky andBatiukova, 2008).
Verbs that consistently im-pose semantic typing on one of their argumentsin at least one of their senses (strongly coerciveverbs) were included into the final data set: ar-rive (at), cancel, deny, finish, and hear.2.
Sense inventories were compiled for each verb,with the senses mapped to OntoNotes (Pradhanet al, 2007) whenever possible.
For each sense,a set of type templates was compiled using amodification of the CPA technique (Hanks andPustejovsky, 2005; Pustejovsky et al, 2004):every argument in the syntactic pattern asso-ciated with a given sense was assigned a typespecification.
Although a particular sense isoften compatible with more than one semantictype for a given argument, this was never thecase in our data set, where no disjoint typeswere tested.
The coercive senses of the chosenverbs were associated with the following typetemplates:a.
Arrive (at), sense reach a destination or goal : HU-MAN arrive at LOCATIONb.
Cancel, sense call off : HUMAN cancel EVENTc.
Deny, sense state or maintain that something is un-true: HUMAN deny PROPOSITIONd.
Finish, sense complete an activity: HUMAN finishEVENTe.
Hear, sense perceive physical sound : HUMAN hearSOUNDWe used a subset of semantic types from theBrandeis Shallow Ontology (BSO), which is ashallow hierarchy of types developed as a partof the CPA effort (Hanks, 2009; Pustejovskyet al, 2004; Rumshisky et al, 2006).
Typeswere selected for their prevalence in manuallyidentified selection context patterns developedfor several hundred English verbs.
That is,they capture common semantic distinctions as-sociated with the selectional properties of manyverbs.
The types used for annotation were:ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,DOCUMENT,DRINK,EMOTION,ENTITY,EVENT, FOOD,HUMAN,HUMAN GROUP, IDEA, INFORMATION, LOCA-TION,OBLIGATION,ORGANIZATION, PATH, PHYSICALOBJECT, PROPERTY, PROPOSITION,RULE, SENSATION,SOUND, SUBSTANCE, TIME PERIOD, VEHICLEThis set of types is purposefully shallow andnon-hierarchical.
For example, HUMAN is asubtype of both ANIMATE and PHYSICAL OB-JECT, but annotators and system developerswere instructed to choose the most relevant type(e.g., HUMAN) and to ignore inheritance.3.
A set of sentences was randomly extracted foreach target verb from the BNC (Burnard, 1995).The extracted sentences were parsed automati-cally, and the sentences organized according tothe grammatical relation the target verb was in-volved in.
Sentences were excluded from the setif the target argument was expressed as anaphor,or was not present in the sentence.
The seman-tic head for the target grammatical relation wasidentified in each case.4.
Word sense disambiguation of the target predi-cate was performed manually on each extractedsentence, matching the target against the senseinventory and the corresponding type templatesas described above.
The appropriate senseswere then saved into the database along with theassociated type template.5.
The sentences containing coercive senses of thetarget verbs were loaded into the Brandeis An-notation Tool (Verhagen, 2010).
Annotatorswere presented with a list of sentences andasked to determine whether the argument inthe specified grammatical relation to the targetbelongs to the type associated with that sensein the corresponding template.
Disagreementswere resolved by adjudication.29Coerion Type Verb Train TestEVENT?LOCATION arrive at 38 37ARTIFACT?EVENT cancel 35 35finish 91 92EVENT?PROPOSITION deny 56 54ARTIFACT?SOUND hear 28 30EVENT?SOUND hear 24 26DOCUMENT?EVENT finish 39 40Table 1: Coercions in the English data set6.
To guarantee robustness of the data, two addi-tional steps were taken.
First, only the six mostrecurrent coercion types were selected; theseare given in table 1.
Preference was given tocross-domain coercions, where the source andthe target types are not related ontologically.Second, the distribution of selection and co-ercion instances were skewed to increase thenumber of coercions.
The final English data setcontains about 30% coercions.7.
Finally, the data set was randomly split in halfinto a training set and a test set.
The trainingdata has 1032 instances, 311 of which are co-ercions, and the test data has 1039 instances,314 of which are coercions.4.2 Data Set Construction Phase: ItalianIn constructing the Italian data set, we adopted thesame methodology used for the English data set,with the following differences:1.
The list of coercive verbs was selected by exam-ining data from the ItWaC (Baroni and Kilgar-riff, 2006) using the Sketch Engine (Kilgarriffet al, 2004):accusare ?accuse?, annunciare ?announce?, arrivare ?ar-rive?, ascoltare ?listen?, avvisare ?inform?, chiamare?call?, cominciare ?begin?, completare ?complete?, con-cludere ?conclude?, contattare ?contact?, divorare ?de-vour?, echeggiare ?echo?, finire ?finish?, informare ?in-form?, interrompere ?interrupt?, leggere ?read?, raggiun-gere ?reach?, recar(si) ?go to?, rimbombare ?resound?,sentire ?hear?, udire ?hear?, visitare ?visit?.2.
The coercive senses of the chosen verbs wereassociated with type templates, some of whichare listed listed below.
Whenever possible,senses and type templates were adapted fromthe Italian Pattern Dictionary (Hanks and Jezek,2007) and mapped to their SIMPLE equiva-lents (Lenci et al, 2000).a.
arrivare, sense reach a location: HUMAN arriva[prep] LOCATIONb.
cominciare, sense initiate an undertaking: HUMANcomincia EVENTc.
completare, sense finish an activity: HUMAN com-pleta EVENTd.
udire, sense perceive a sound : HUMAN ode SOUNDe.
visitare, sense visit a place: HUMAN visita LOCA-TIONThe following types were used to annotatethe Italian dataset:ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY,EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, IN-FORMATION, LIQUID, LOCATION, ORGANIZATION,PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND,TIME PERIOD, VEHICLEThe annotators were provided with a set of def-initions and examples of each type.3.
A set of sentences for each target verb was ex-tracted and parsed from the PAROLE sottoin-sieme corpus (Bindi et al, 2000).
They wereskimmed to ensure that the final data set con-tained a sufficient number of coercions, withproportionally more selections than coercions.Sentences were preselected to include instancesrepresenting one of the chosen senses.4.
In order to exclude instances that may have beenwrongly selected, a judge performed word sensedisambiguation of the target predicate in the ex-tracted sentences.5.
Annotators were presented with a list of sen-tences and asked to determine the usual seman-tic type associated with the argument in thespecified grammatical relation.
Every sentencewas annotated by two annotators and one judge,who resolved disagreements.6.
Some of the coercion types selected for Italianwere:a.
LOCATION?
HUMAN (accusare, annunciare)b. ARTIFACT?
HUMAN (annunciare, avvisare)c. EVENT?
LOCATION (arrivare, raggiungere)d. ARTIFACT?
EVENT (cominciare, completare)e. EVENT?
DOCUMENT (leggere, divorare)f. HUMAN?
DOCUMENT (leggere, divorare)g. EVENT?
SOUND (ascoltare, echeggiare)h. ARTIFACT?
SOUND (ascoltare, echeggiare)7.
The Italian training data contained 1466 in-stances, 381 of which are coercions; the testdata had 1463 instances, with 384 coercions.5 Data FormatThe test and training data were provided in XML.The relation between the predicate (viewed asa function) and its argument were representedby composition link elements (CompLink), as30shown below.
The test data differed from the train-ing data in the omission of CompLink elements.In case of coercion, there is a mismatch betweenthe source and the target types, and both typesneed to be identified; e.g., The State Departmentrepeatedly denied the attack:The State Department repeatedly<SELECTOR sid="s1">denied</SELECTOR>the <TARGET id="t1">attack</TARGET>.<CompLink cid="cid1"compType="COERCION"selector_id="s1"relatedToTarget="t1"sourceType="EVENT"targetType="PROPOSITION"/>When the compositional operation is selection,the source and target types must match; e.g., TheState Department repeatedly denied the statement:The State Department repeatedly<SELECTOR sid="s2">denied</SELECTOR>the <TARGET id="t2">statement</TARGET>.<CompLink cid="cid2"compType="SELECTION"selector_id="s2"relatedToTarget="t2"sourceType="PROPOSITION"targetType="PROPOSITION"/>6 Results & AnalysisWe received only a single submission for theASC task.
The UTDMet system was an SVM-based system with features derived from two mainsources: a PageRank-style algorithm over Word-Net hypernyms used to define semantic classes,and statistics from a PropBank-style parse of some8 million documents from the English Gigawordcorpus.
The results, shown in Table 2, werecomputed from confusion matrices constructed foreach of four classification tasks for the 1039 linkinstances in the English test data: determinationof argument selection or coercion, identification ofthe argument source type, identification of the ar-gument target type, and the joint identification ofthe source/target type pair.Clearly, the UTDMet system did quite well atthis task.
The one immediately noticeable outlieris the macro-averaged precision for the joint type,which reflects a small number of miscategoriza-tions of rare types.
For example, eliminating thesingle miscategorized ARTIFACT-LOCATION linkin the submitted test data bumps this score up toa respectable 94%.
This large discrepancy can ex-plained by the lack of any coercions with thosetypes in the gold-standard data.Prec.
Recall AveragingSelection vs. 95 96 (macro)Coercion: 96 96 (micro)Source Type: 96 96 (macro)96 96 (micro)Target Type: 100 100 (both)Joint Type: 86 95 (macro)96 96 (micro)Table 2: Results for the UTDMet submission.In the absence of any other submissions, it isdifficult to provide a point of comparison for thisperformance.
However, we can provide a base-line by taking each link to be a selection whosesource and target types are the most common type(EVENT for the gold-standard English data).
Thisyields micro-averaged precision scores of 69% forselection vs. coercion, 33% for source type iden-tification, 37% for the target type identification,and 22% for the joint type.The performance of the UTDMet system sug-gests that most of the type coercions were identifi-able based largely on examination of lexical cluesassociated with selection contexts.
This is in factto be expected for the type coercions that were thefocus of the English data set.
It will be interestingto see how systems perform on the Italian data setand an expanded corpus for English and Italian,where more subtle and complex type exploitationsand manipulations are at play.
These will hope-fully be explored in future competitions.7 ConclusionIn this paper, we have described the Argument Se-lection and Coercion task for SemEval-2010.
Thistask involves identifying the relation between apredicate and its argument as one that encodesthe compositional history of the selection process.This allows us to distinguish surface forms that di-rectly satisfy the selectional (type) requirements ofa predicate from those that are coerced in context.We described some details of a specification lan-guage for selection, the annotation task using thisspecification to identify argument selection behav-ior, and the preparation of the data for the task.Finally, we analyzed the results of the task sub-missions.31ReferencesM.
Baroni and A. Kilgarriff.
2006.
Largelinguistically-processed web corpora for multiplelanguages.
In Proceedings of European ACL.R.
Bindi, P. Baroni, M. Monachini, and E. Gola.
2000.PAROLE-Sottoinsieme.
ILC-CNR Internal Report.Aljoscha Burchardt, Katrin Erk, Anette Frank, An-drea Kowalski, Sebastian Pado, and Manfred Pinkal.2006.
The salsa corpus: a german corpus resourcefor lexical semantics.
In Proceedings of LREC,Genoa, Italy.L.
Burnard, 1995.
Users?
Reference Guide, British Na-tional Corpus.
British National Corpus Consortium,Oxford, England.Marcus Egg.
2005.
Flexible semantics for reinterpre-tation phenomena.
CSLI, Stanford.P.
Hanks and E. Jezek.
2007.
Building Pattern Dictio-naries with Corpus Analysis.
In International Col-loquium on Possible Dictionaries, Rome, June, 6-7.Oral Presentation.P.
Hanks and J. Pustejovsky.
2005.
A pattern dic-tionary for natural language processing.
RevueFranc?aise de Linguistique Appliqu?ee.P.
Hanks.
2009.
Corpus pattern analysis.
CPAProject Page.
Retrieved April 11, 2009, fromhttp://nlp.fi.muni.cz/projekty/cpa/.J.
R. Hobbs, M. Stickel, and P. Martin.
1993.
Interpre-tation as abduction.
Artificial Intelligence, 63:69?142.A.
Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.2004.
The Sketch Engine.
Proceedings of Euralex,Lorient, France, pages 105?116.Karin Kipper.
2005.
VerbNet: A broad-coverage, com-prehensive verb lexicon.
Phd dissertation, Univer-sity of Pennsylvania, PA.A.
Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,M.
Monachini, A. Ogonowski, I. Peters, W. Peters,N.
Ruimy, et al 2000.
SIMPLE: A general frame-work for the development of multilingual lexicons.International Journal of Lexicography, 13(4):249.K.
Markert and M. Nissim.
2007.
SemEval-2007task 8: Metonymy resolution.
In Eneko Agirre,Llu?
?s M`arquez, and Richard Wicentowski, editors,Proceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), Prague,Czech Republic, June.
Association for Computa-tional Linguistics.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.The NomBank project: An interim report.
In HLT-NAACL 2004 Workshop: Frontiers in Corpus Anno-tation, pages 24?31.Geoffrey Nunberg.
1979.
The non-uniqueness of se-mantic solutions: Polysemy.
Linguistics and Phi-losophy, 3:143?184.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.S.
Pradhan, E. Hovy, MS Marcus, M. Palmer,L.
Ramshaw, and R. Weischedel.
2007.
Ontonotes:A unified relational semantic representation.
InInternational Conference on Semantic Computing,2007, pages 517?526.J.
Pustejovsky, P. Hanks, and A. Rumshisky.
2004.Automated Induction of Sense in Context.
In COL-ING 2004, Geneva, Switzerland, pages 924?931.J.
Pustejovsky, R. Knippen, J. Littman, and R. Sauri.2005.
Temporal and event information in naturallanguage text.
Language Resources and Evaluation,39(2):123?164.J.
Pustejovsky, A. Rumshisky, J. Moszkowicz, andO.
Batiukova.
2009.
GLML: Annotating argumentselection and coercion.
IWCS-8: Eighth Interna-tional Conference on Computational Semantics.J.
Pustejovsky.
1991.
The generative lexicon.
Compu-tational Linguistics, 17(4).A.
Rumshisky and O. Batiukova.
2008.
Polysemyin verbs: systematic relations between senses andtheir effect on annotation.
In COLING Workshopon Human Judgement in Computational Linguistics(HJCL-2008), Manchester, England.A.
Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.2006.
Constructing a corpus-based ontology usingmodel bias.
In The 19th International FLAIRS Con-ference, FLAIRS 2006, Melbourne Beach, Florida,USA.J.
Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,and J. Scheffczyk.
2006.
FrameNet II: ExtendedTheory and Practice.Carlos Subirats.
2004.
FrameNet Espa?nol.
Una redsem?antica de marcos conceptuales.
In VI Interna-tional Congress of Hispanic Linguistics, Leipzig.Marc Verhagen.
2010.
The Brandeis Annotation Tool.In Language Resources and Evaluation Conference,LREC 2010, Malta.32
