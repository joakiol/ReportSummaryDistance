Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402?408,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsDiscoTK: Using Discourse Structure for Machine Translation EvaluationShafiq Joty Francisco Guzm?an Llu?
?s M`arquez and Preslav NakovALT Research GroupQatar Computing Research Institute ?
Qatar Foundation{sjoty,fguzman,lmarquez,pnakov}@qf.org.qaAbstractWe present novel automatic metrics formachine translation evaluation that usediscourse structure and convolution ker-nels to compare the discourse tree of anautomatic translation with that of the hu-man reference.
We experiment with fivetransformations and augmentations of abase discourse tree representation basedon the rhetorical structure theory, and wecombine the kernel scores for each of theminto a single score.
Finally, we add othermetrics from the ASIYA MT evaluationtoolkit, and we tune the weights of thecombination on actual human judgments.Experiments on the WMT12 and WMT13metrics shared task datasets show corre-lation with human judgments that outper-forms what the best systems that partici-pated in these years achieved, both at thesegment and at the system level.1 IntroductionThe rapid development of statistical machinetranslation (SMT) that we have seen in recentyears would not have been possible without au-tomatic metrics for measuring SMT quality.
Inparticular, the development of BLEU (Papineniet al., 2002) revolutionized the SMT field, al-lowing not only to compare two systems in away that strongly correlates with human judg-ments, but it also enabled the rise of discrimina-tive log-linear models, which use optimizers suchas MERT (Och, 2003), and later MIRA (Watanabeet al., 2007; Chiang et al., 2008) and PRO (Hop-kins and May, 2011), to optimize BLEU, or an ap-proximation thereof, directly.
While over the yearsother strong metrics such as TER (Snover et al.,2006) and Meteor (Lavie and Denkowski, 2009)have emerged, BLEU remains the de-facto stan-dard, despite its simplicity.Recently, there has been steady increase inBLEU scores for well-resourced language pairssuch as Spanish-English and Arabic-English.However, it was also observed that BLEU-like n-gram matching metrics are unreliable for high-quality translation output (Doddington, 2002;Lavie and Agarwal, 2007).
In fact, researchers al-ready worry that BLEU will soon be unable to dis-tinguish automatic from human translations.1Thisis a problem for most present-day metrics, whichcannot tell apart raw machine translation outputfrom a fully fluent professionally post-edited ver-sion thereof (Denkowski and Lavie, 2012).Another concern is that BLEU-like n-grammatching metrics tend to favor phrase-based SMTsystems over rule-based systems and other SMTparadigms.
In particular, they are unable to cap-ture the syntactic and semantic structure of sen-tences, and are thus insensitive to improvementin these aspects.
Furthermore, it has been shownthat lexical similarity is both insufficient and notstrictly necessary for two sentences to conveythe same meaning (Culy and Riehemann, 2003;Coughlin, 2003; Callison-Burch et al., 2006).The above issues have motivated a large amountof work dedicated to design better evaluation met-rics.
The Metrics task at the Workshop on Ma-chine Translation (WMT) has been instrumental inthis quest.
Below we present QCRI?s submissionto the Metrics task of WMT14, which consists ofthe DiscoTK family of discourse-based metrics.In particular, we experiment with five differenttransformations and augmentations of a discoursetree representation, and we combine the kernelscores for each of them into a single score whichwe call DISCOTKlight.
Next, we add to the com-bination other metrics from the ASIYA MT eval-uation toolkit (Gim?enez and M`arquez, 2010), toproduce the DISCOTKpartymetric.1This would not mean that computers have achieved hu-man proficiency; it would rather show BLEU?s inadequacy.402Finally, we tune the relative weights of the met-rics in the combination using human judgmentsin a learning-to-rank framework.
This provedto be quite beneficial: the tuned version of theDISCOTKpartymetric was the best performingmetric in the WMT14 Metrics shared task.The rest of the paper is organized as follows:Section 2 introduces our basic discourse metricsand the tree representations they are based on.Section 3 describes our metric combinations.
Sec-tion 4 presents our experiments and results ondatasets from previous years.
Finally, Section 5concludes and suggests directions for future work.2 Discourse-Based MetricsIn our recent work (Guzm?an et al., 2014), we usedthe information embedded in the discourse-trees(DTs) to compare the output of an MT system toa human reference.
More specifically, we useda state-of-the-art sentence-level discourse parser(Joty et al., 2012) to generate discourse trees forthe sentences in accordance with the RhetoricalStructure Theory (RST) of discourse (Mann andThompson, 1988).
Then, we computed the simi-larity between DTs of the human references andthe system translations using a convolution treekernel (Collins and Duffy, 2001), which efficientlycomputes the number of common subtrees.
Notethat this kernel was originally designed for syntac-tic parsing, and the subtrees are subject to the con-straint that their nodes are taken with all or noneof their children, i.e., if we take a direct descen-dant of a given node, we must also take all siblingsof that descendant.
This imposes some limitationson the type of substructures that can be compared,and motivates the enriched tree representations ex-plained in subsections 2.1?2.4.The motivation to compare discourse trees, isthat translations should preserve the coherence re-lations.
For example, consider the three discoursetrees (DTs) shown in Figure 1.
Notice that theAttribution relation in the reference translation isalso realized in the system translation in (b) but notin (c), which makes (b) a better translation com-pared to (c), according to our hypothesis.In (Guzm?an et al., 2014), we have shown thatdiscourse structure provides additional informa-tion for MT evaluation, which is not captured byexisting metrics that use lexical, syntactic and se-mantic information; thus, discourse should be con-sidered when developing new rich metrics.Here, we extend our previous work by devel-oping metrics that are based on new representa-tions of the DTs.
In the remainder of this section,we will focus on the individual DT representationsthat we will experiment with; then, the followingsection will describe the metric combinations andtuning used to produce the DiscoTK metrics.2.1 DR-LEX1Figure 2a shows our first representation of the DT.The lexical items, i.e., words, constitute the leavesof the tree.
The words in an Elementary DiscourseUnit (EDU) are grouped under a predefined tagEDU, to which the nuclearity status of the EDUis attached: nucleus vs. satellite.
Coherence re-lations, such as Attribution, Elaboration, and En-ablement, between adjacent text spans constitutethe internal nodes of the tree.
Like the EDUs, thenuclearity statuses of the larger discourse units areattached to the relation labels.
Notice that withthis representation the tree kernel can easily be ex-tended to find subtree matches at the word level,i.e., by including an additional layer of dummyleaves as was done in (Moschitti et al., 2007).
Weapplied the same solution in our representations.2.2 DR-NOLEXOur second representation DR-NOLEX (Figure 2b)is a simple variation of DR-LEX1, where we ex-clude the lexical items.
This allows us to measurethe similarity between two translations in terms oftheir discourse structures alone.2.3 DR-LEX2One limitation of DR-LEX1and DR-NOLEX is thatthey do not separate the structure, i.e., the skele-ton, of the tree from its labels.
Therefore, whenmeasuring the similarity between two DTs, theydo not allow the tree kernel to give partial creditto subtrees that differ in labels but match in theirstructures.
DR-LEX2, a variation of DR-LEX1, ad-dresses this limitation as shown in Figure 2c.
Ituses predefined tags SPAN and EDU to build theskeleton of the tree, and considers the nuclearityand/or relation labels as properties (added as chil-dren) of these tags.
For example, a SPAN has twoproperties, namely its nuclearity and its relation,and an EDU has one property, namely its nucle-arity.
The words of an EDU are placed under thepredefined tag NGRAM.403Elaboration ROOTSPANNucleus AttributionSatelliteVoices are coming from Germany , SPANSatellite SPANNucleussuggesting that ECB be the last resort creditor .
(a) A reference (human-written) translation.AttributionROOTSPANSatellite SPANNucleusIn Germany voices , the ECB should be the lender of last resort .
(b) A higher quality (system-generated) translation.SPANROOTIn Germany the ECB should be for the creditors of last resort .
(c) A lower quality (system-generated) translation.Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higherquality automatic translation, and (c) a lower quality automatic translation.2.4 DR-LEX1.1and DR-LEX2.1Although both DR-LEX1and DR-LEX2allow thetree kernel to find matches at the word level, thewords are compared in a bag-of-words fashion,i.e., if the trees share a common word, the ker-nel will find a match regardless of its position inthe tree.
Therefore, a word that has occurred inan EDU with status Nucleus in one tree could bematched with the same word under a Satellite inthe other tree.
In other words, the kernel basedon these representations is insensitive to the nu-clearity status and the relation labels under whichthe words are matched.
DR-LEX1.1, an exten-sion of DR-LEX1, and DR-LEX2.1, an extensionof DR-LEX2, are sensitive to these variations atthe lexical level.
DR-LEX1.1(Figure 2d) and DR-LEX2.1(Figure 2e) propagate the nuclearity sta-tuses and/or the relation labels to the lexical itemsby including three more subtrees at the EDU level.3 Metric Combination and TuningIn this section, we describe our Discourse TreeKernel (DiscoTK) metrics.
We have two mainversions: DISCOTKlight, which combines the fiveDR-based metrics, and DISCOTKparty, which fur-ther adds the Asiya metrics.3.1 DISCOTKlightIn the previous section, we have presented severaldiscourse tree representations that can be used tocompare the output of a machine translation sys-tem to a human reference.
Each representationstresses a different aspect of the discourse tree.In order to make our estimations more robust,we propose DISCOTKlight, a metric that takes ad-vantage of all the previous discourse representa-tions by linearly interpolating their scores.
Hereare the processing steps needed to compute thismetric:(i) Parsing: We parsed each sentence in order toproduce discourse trees for the human referencesand for the outputs of the systems.
(ii) Tree enrichment/simplification: For eachsentence-level discourse tree, we generated thefive different tree representations: DR-NOLEX,DR-LEX1, DR-LEX1.1, DR-LEX2, DR-LEX2.1.
(iii) Estimation: We calculated the per-sentencesimilarity scores between tree representations ofthe system hypothesis and the human referenceusing the extended convolution tree kernel as de-scribed in the previous section.
To compute thesystem-level similarity scores, we calculated theaverage sentence-level similarity; note that this en-sures that our metric is ?the same?
at the systemand at the segment level.
(iv) Normalization: In order to make the scores ofthe different representations comparable, we per-formed a min?max normalization2for each met-ric and for each language pair.
(v) Combination: Finally, for each sentence, wecomputed DISCOTKlightas the average of thenormalized similarity scores of the different repre-sentations.
For system-level experiments, we per-formed linear interpolation of system-level scores.2Where x?= (x?min)/(max?min).404      !"# !#" $ #$%$& $%$'(a) DT for DR-LEX1.ELABORATION-NUCLEUSEDU-NUCLEUS EDU-SATELLITE(b) DT for DR-NOLEX.SPANNUC REL EDU EDUNUCLEUS ELABORATION NUC NGRAM NUC NGRAMNUCLEUS to better .. titles SATELLITE published(c) DT for DR-LEX2.ELABORATION-NUCLEUSEDU-NUCLEUS ..LEX LEX:NUC LEX:REL LEX:NUC:RELto better .. to:N better:N .. to:ELAB better:ELAB .. to:N:ELAB better:N:ELAB ..(d) DT for DR-LEX1.1.SPANNUC REL EDU EDUNUCLEUS ELABORATION NUC LEX LEX:NUC LEX:REL LEX:NUC:REL NUC LEX LEX:NUC ..NUCLEUS to better .. to:N better:N .. to:ELAB better:ELAB .. to:N:ELAB better:N:ELAB .. SATELLITE published published:S(e) DT for DR-LEX2.1.Figure 2: Five different representations of the discourse tree (DT) for the sentence ?The new organisa-tional structure will also allow us to enter the market with a joint offer of advertising products, to betterlink the creation of content for all the titles published and, last but not least, to continue to streamlinesignificantly the business management of the company,?
added Cermak.
Note that to avoid visual clutter,(b)?
(e) show alternative representations only for the highlighted subtree in (a).3.2 DISCOTKpartyOne of the weaknesses of the above discourse-based metrics is that they use unigram lexicalinformation, which does not capture reordering.Thus, in order to make more informed and ro-bust estimations, we extended DISCOTKlightwiththe composing metrics of the ASIYA?s ULC met-ric (Gim?enez and M`arquez, 2010), which is a uni-form linear combination of twelve individual met-rics and was the best-performing metric at the sys-tem and at the segment levels at the WMT08 andWMT09 metrics tasks.In order to compute the individual metrics fromULC, we used the ASIYA toolkit,3but we de-parted from ASIYA?s ULC by replacing TERand Meteor with newer versions thereof that takeinto account synonymy lookup and paraphras-ing (?TERp-A?
and ?Meteor-pa?
in ASIYA?s ter-minology).
We then combined the five compo-nents in DISCOTKlightand the twelve individ-ual metrics from ULC; we call this combinationDISCOTKparty.3http://nlp.lsi.upc.edu/asiya/We combined the scores using linear interpola-tion in two different ways:(i) Uniform combination of min-max normalizedscores at the segment level.
We obtained system-level scores by computing the average over thesegment scores.
(ii) Trained interpolation at the sentence level.We determined the interpolation weights for theabove-described combination of 5+12 = 17 met-rics using a pairwise learning-to-rank frameworkand classification with logistic regression, as wehad done in (Guzm?an et al., 2014).
We obtainedthe final test-time sentence-level scores by pass-ing the interpolated raw scores through a sigmoidfunction.
In contrast, for the final system-levelscores, we averaged the per-sentence interpolatedraw scores.We also tried to learn the interpolation weightsat the system level, experimenting with both re-gression and classification.
However, the amountof data available for this type of training wassmall, and the learned weights did not perform sig-nificantly better than the uniform combination.4053.3 Post-processingDiscourse-based metrics, especially DR-NOLEX,tend to produce many ties when there is notenough information to do complete discourseanalysis.
This contributes to lower ?
scores forDISCOTKlight.
To alleviate this issue, we used asimple tie-breaking strategy, in which ties betweensegment scores for different systems are resolvedby using perturbations proportional to the globalsystem-level scores produced by the same metric,i.e., x?segsys= xsegsys+ ??sxssys.
Here,  is automati-cally chosen to avoid collisions with scores not in-volved in the tie.
This post-processing is not partof the metric; it is only applied to our segment-level submission to the WMT?14 metrics task.4 Experimental EvaluationIn this section, we present some of our experi-ments to decide on the best DiscoTK metric vari-ant and tuning set.
For tuning, testing and compar-ison, we worked with some of the datasets avail-able from previous WMT metrics shared tasks,i.e., 2011, 2012 and 2013.
From previous ex-periments (Guzm?an et al., 2014), we know thatthe tuned metrics perform very well on cross-validation for the same-year dataset.
We furtherknow that tuning can be performed by concatenat-ing data from all the into-English language pairs,which yields better results than training separatelyby language pair.
For the WMT14 metrics task,we investigated in more depth whether the tunedmetrics generalize well to new datasets.
Addition-ally, we tested the effect of concatenating datasetsfrom different years.Table 1 shows the main results of our experi-ments with the DiscoTK metrics.
We evaluatedthe performance of the metrics on the WMT12and WMT13 datasets both at the segment and thesystem level, and we used WMT11 as an addi-tional tuning dataset.
We measured the perfor-mance of the metrics in terms of correlation withhuman judgements.
At the segment level, we eval-uated using Kendall?s Tau (?
), recalculated follow-ing the WMT14 official Kendall?s Tau implemen-tation.
At the system level, we used Spearman?srank correlation (?)
and Pearson?s correlation co-efficient (r).
In all cases, we averaged the resultsover all into-English language pairs.
The symbol???
represents the untuned versions of our metrics,i.e., applying a uniform linear combination of theindividual metrics.We trained the tuned versions of the DiscoTKmeasures using different datasets (WMT11,WMT12 and WMT13) in order to study across-corpora generalization and the effect of trainingdataset size.
The symbol ?+?
stands for concatena-tion of datasets.
We trained the tuned versions atthe segment level using Maximum Entropy clas-sifiers for pairwise ranking (cf.
Section 3).
Forthe sake of comparison, the first group of rowscontains the results of the best-performing met-rics at the WMT12 and WMT13 metrics sharedtasks and the last group of rows contains the re-sults of the ASIYA combination of metrics, i.e.,DISCOTKpartywithout the discourse components.Several conclusions can be drawn from Table 1.First, DISCOTKpartyis better than DISCOTKlightin all settings, indicating that the discourse-basedmetrics are very well complemented by the hetero-geneous metric set from ASIYA.
DISCOTKlightachieves competitive scores at the system level(which would put the metric among the best par-ticipants in WMT12 and WMT13); however, asexpected, it is not robust enough at the segmentlevel.
On the other hand, the tuned versions ofDISCOTKpartyare very competitive and improveover the already strong ASIYA in each configu-ration both at the segment- and the system-level.The improvements are small but consistent, show-ing that using discourse increases the correlationwith human judgments.Focusing on the results at the segment level, itis clear that the tuned versions offer an advantageover the simple uniform linear combinations.
In-terestingly, for the tuned variants, given a test set,the results are consistent across tuning sets, rulingout over-fitting; this shows that the generalizationis very good.
This result aligns well with whatwe observed in our previous studies (Guzm?an etal., 2014).
Learning with more data (WMT11+12or WMT12+13) does not seem to help much,but it does not hurt performance either.
Overall,the ?
correlation results obtained with the tunedDISCOTKpartymetric are much better than thebest results of any participant metrics at WMT12and WMT13 (20.1% and 9.5% relative improve-ment, respectively).At the system level, we observe that tuning overthe DISCOTKlightmetric is not helpful (resultsare actually slightly lower), while tuning the morecomplex DISCOTKpartymetric yields slightly bet-ter results.406Segment Level System LevelWMT12 WMT13 WMT12 WMT13Metric Tuning ?
?
?
r ?
rSEMPOS na ?
?
0.902 0.922 ?
?SPEDE07PP na 0.254 ?
?
?
?
?METEOR-WMT13 na ?
0.264 ?
?
0.935 0.950?
0.171 0.162 0.884 0.922 0.880 0.911WMT11 0.207 0.201 0.860 0.872 0.890 0.909DISCOTKlightWMT12 ?
0.200 ?
?
0.889 0.910WMT13 0.206 ?
0.865 0.871 ?
?WMT11+12 ?
0.197 ?
?
0.890 0.910WMT11+13 0.207 ?
0.865 0.871 ?
??
0.257 0.231 0.907 0.915 0.941 0.928WMT11 0.302 0.282 0.915 0.940 0.934 0.946DISCOTKpartyWMT12 ?
0.284 ?
?
0.936 0.940WMT13 0.305 ?
0.912 0.935 ?
?WMT11+12 ?
0.289 ?
?
0.936 0.943WMT11+13 0.304 ?
0.912 0.934 ?
??
0.273 0.252 0.899 0.909 0.932 0.922WMT11 0.301 0.279 0.913 0.935 0.934 0.944ASIYA WMT12 ?
0.277 ?
?
0.932 0.938WMT13 0.303 ?
0.908 0.932 ?
?WMT11+12 ?
0.277 ?
?
0.934 0.940WMT11+13 0.303 ?
0.908 0.933 ?
?Table 1: Evaluation results on WMT12 and WMT13 datasets at segment and system level for the maincombined DiscoTK measures proposed in this paper.The scores of our best metric are higher thanthose of the best participants in WMT12 andWMT13, according to Spearman?s ?, which wasthe official metric in those years.
Overall, our met-rics are comparable to the state-of-the-art at thesystem level.
The differences between Spearman?s?
and Pearson?s r coefficients are not dramatic,with r values being always higher than ?.Given the above results, we submitted the fol-lowing runs to the WMT14 Metrics shared task:(i) DISCOTKpartytuned on the concatenationof datasets WMT11+12+13, as our primary run;(ii) Untuned DISCOTKparty, to verify that we arenot over-fitting the training set; and (iii) UntunedDISCOTKlight, to see the performance of a metricusing discourse structures and word unigrams.The results for the WMT14 Metrics shared taskhave shown that our primary run, DISCOTKpartytuned, was the best-performing metric both at thesegment- and at the system-level (Mach?a?cek andBojar, 2014).
This metric yielded significantlybetter results than its untuned counterpart, con-firming the importance of weight tuning and theabsence of over-fitting during tuning.
Finally, theuntuned DISCOTKlightachieved relatively com-petitive, albeit slightly worse results for all lan-guage pairs, except for Hindi-English, where sys-tem translations resembled a ?word salad?, andwere very hard to discourse-parse accurately.5 ConclusionWe have presented experiments with novel auto-matic metrics for machine translation evaluationthat take discourse structure into account.
In par-ticular, we used RST-style discourse parse trees,which we compared using convolution kernels.We further combined these kernels with metricsfrom ASIYA, also tuning the weights.
The re-sulting DISCOTKpartytuned metric was the best-performing at the segment- and system-level at theWMT14 metrics task.In an internal evaluation on the WMT12 andWMT13 metrics datasets, this tuned combina-tion showed correlation with human judgmentsthat outperforms the best systems that participatedin these shared tasks.
The discourse-only met-ric ranked near the top at the system-level forWMT12 and WMT13; however, it is weak at thesegment-level since it is sensitive to parsing errors,and most sentences have very little internal dis-course structure.In the future, we plan to work on an inte-grated representation of syntactic, semantic anddiscourse-based tree structures, which would al-low us to design evaluation metrics based on morefine-grained features, and would also allow us totrain such metrics using kernel methods.
Further-more, we want to make use of discourse parse in-formation beyond the sentence level.407ReferencesChris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEUin machine translation research.
In Proceedings ofthe Eleventh Conference of the European Chapterof the Association for Computational Linguistics,EACL?06, pages 249?256, Trento, Italy.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP?08, pages 224?233, Honolulu,Hawaii.Michael Collins and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Neural Informa-tion Processing Systems, NIPS?01, pages 625?632,Vancouver, Canada.Deborah Coughlin.
2003.
Correlating automated andhuman assessments of machine translation quality.In Proceedings of the Machine Translation SummitIX, MT Summit?03, pages 23?27, New Orleans, LA,USA.Christopher Culy and Susanne Riehemann.
2003.
Thelimits of n-gram translation evaluation metrics.
InProceedings of the Machine Translation Summit IX,MT Summit?03, pages 1?8, New Orleans, LA, USA.Michael Denkowski and Alon Lavie.
2012.
Chal-lenges in predicting machine translation utility forhuman post-editors.
In Proceedings of the TenthConference of the Association for Machine Trans-lation in the Americas, AMTA?12, pages 40?49, SanDiego, CA, USA.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT?02, pages 138?145, SanFrancisco, CA, USA.Jes?us Gim?enez and Llu?
?s M`arquez.
2010.
LinguisticMeasures for Automatic Machine Translation Eval-uation.
Machine Translation, 24(3?4):77?86.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez, andPreslav Nakov.
2014.
Using discourse structureimproves machine translation evaluation.
In Pro-ceedings of 52nd Annual Meeting of the Associationfor Computational Linguistics, ACL?14, Baltimore,MD, USA.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP?11, pages 1352?1362, Edinburgh, Scot-land, UK.Shafiq Joty, Giuseppe Carenini, and Raymond Ng.2012.
A Novel Discriminative Framework forSentence-Level Discourse Analysis.
In Proceed-ings of the Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL?12,pages 904?915, Jeju Island, Korea.Alon Lavie and Abhaya Agarwal.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
InProceedings of the Second Workshop on Statisti-cal Machine Translation, WMT?07, pages 228?231,Prague, Czech Republic.Alon Lavie and Michael Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2-3):105?115.Matou?s Mach?a?cek and Ond?rej Bojar.
2014.
Results ofthe WMT14 Metrics Shared Task.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, WMT?14, Baltimore, MD, USA.William Mann and Sandra Thompson.
1988.
Rhetor-ical Structure Theory: Toward a Functional Theoryof Text Organization.
Text, 8(3):243?281.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for questionanswer classification.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, ACL?07, pages 776?783, Prague, CzechRepublic.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, ACL?03, pages 160?167, Sap-poro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of 40th Annual Meeting of the Associationfor Computational Linguistics, ACL?02, pages 311?318, Philadelphia, PA, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In Proceedings of the Seventh Biennial Con-ference of the Association for Machine Translationin the Americas, AMTA?06, pages 223?231, Cam-bridge, MA, USA.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin trainingfor statistical machine translation.
In Proceedings ofthe Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natu-ral Language Learning, EMNLP-CoNLL?07, pages764?773, Prague, Czech Republic.408
