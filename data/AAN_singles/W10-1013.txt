Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 92?95,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOff-topic essay detection using short prompt textsAnnie LouisUniversity of PennsylvaniaPhiladelphia, PA 19104, USAlannie@seas.upenn.eduDerrick HigginsEducational Testing ServicePrinceton, NJ 08541, USAdhiggins@ets.orgAbstractOur work addresses the problem of predict-ing whether an essay is off-topic to a givenprompt or question without any previously-seen essays as training data.
Prior work hasused similarity between essay vocabulary andprompt words to estimate the degree of on-topic content.
In our corpus of opinion es-says, prompts are very short, and using sim-ilarity with such prompts to detect off-topicessays yields error rates of about 10%.
Wepropose two methods to enable better compar-ison of prompt and essay text.
We automat-ically expand short prompts before compari-son, with words likely to appear in an essayto that prompt.
We also apply spelling correc-tion to the essay texts.
Both methods reducethe error rates during off-topic essay detectionand turn out to be complementary, leading toeven better performance when used in unison.1 IntroductionIt is important to limit the opportunity to sub-mit uncooperative responses to educational software(Baker et al, 2009).
We address the task of detect-ing essays that are irrelevant to a given prompt (es-say question) when training data is not available andthe prompt text is very short.When example essays for a prompt are available,they can be used to learn word patterns to distin-guish on-topic from off-topic essays.
Alternatively,prior work (Higgins et al, 2006) has motivated us-ing similarity between essay and prompt vocabular-ies to detect off-topic essays.
In Section 2, we exam-ine the performance of prompt-essay comparison forfour different essay types.
We show that in the caseof prompts with 9 or 13 content words on average,the error rates are higher compared to those with 60or more content words.
In addition, more errors areobserved when the method is used on essays writtenby English language learners compared to more ad-vanced test takers.
An example short prompt fromour opinion essays?
corpus is shown below.
Test-takers provided arguments for/or against the opinionexpressed by the prompt.
[1] ?In the past, people were more friendly thanthey are today.
?To address this problem, we propose two en-hancements.
We use unsupervised methods to ex-pand the prompt text with words likely to appearin essays to that prompt.
Our approach is basedon the intuition that regularities exist in the wordswhich appear in essays, beyond the prevalence ofactual prompt words.
In a similar vein, misspellingsin the essays, particulary of the prompt words, arealso problematic for prompt-based methods.
There-fore we apply spelling correction to the essay textbefore comparison.
Our results show that both meth-ods lower the error rates.
The relative performanceof the two methods varies depending on the essaytype; however, their combination gives the overallbest results regardless of essay type.2 Effect of prompt and essay propertiesIn this section, we analyze the off-topic essay pre-diction accuracies resulting from direct comparisonof original prompt and essay texts.
We use four dif-ferent corpora of essays collected and scored duringhigh stakes tests with an English writing component.They differ in task type and average prompt length,as well as the skill level expected from the test taker.92In one of the tasks, the test taker reads a passageand listens to a lecture and then writes a summaryof the main points.
For such essays, the prompttext (reading passage plus lecture transcript) avail-able for comparison is quite long (about 276 con-tent words).
In the other 3 tasks, the test taker hasto provide an argument for or against some opin-ion expressed in the prompt.
One of these has longprompts (60 content words).
The other two involveonly single sentence prompts as in example [1] andhave 13 and 9 content words on average.
Two ofthese tasks focused on English language learners andthe other two involved advanced users (applicants tograduate study programs in the U.S.).
See Table 1for a summary of the essay types.12.1 DataFor each of the task types described above, our cor-pus contains essays written to 10 different prompts.We used essays to 3 prompts as development data.To build an evaluation test set, we randomly sam-pled 350 essays for each of the 7 remaining promptsto use as positive examples.
It is difficult to as-semble a sufficient number of naturally-occurringoff-topic essays for testing.
However, an essay on-topic to a particular prompt can be considered aspseudo off-topic to a different prompt.
Hence, tocomplement the positive examples for each prompt,an equal number of negative examples were chosenat random from essays to the remaining 6 prompts.2.2 Experimental setupWe use the approach for off-topic essay detectionsuggested in prior work by Higgins et al (2006).The method uses cosine overlap between tf*idf vec-tors of prompt and essay content words to measurethe similarity between a prompt-essay pair.sim(prompt, essay) = vessay .vprompt?vessay??vprompt?
(1)An essay is compared with the target prompt(prompt with which topicality must be checked) to-gether with a set of reference prompts, different fromthe target.
The reference prompts are also chosento be different from the actual prompts of the neg-ative examples in our dataset.
If the target prompt1Essay sources: Type 1-TOEFL integrated writing task,Type 4-TOEFL independent writing task, Types 2 & 3-argument and issue tasks in Analytical Writing section of GREType Skill Prompt len.
Avg FP Avg FN1 Learners 276 0.73 11.792 Advanced 60 0.20 6.203 Advanced 13 2.94 8.904 Learners 9 9.73 11.07Table 1: Effect of essay types: average prompt length,false positive and false negative ratesis ranked as most similar2 in the list of comparedprompts, the essay is classified as on-topic.
9 refer-ence prompts were used in our experiments.We compute two error rates.FALSE POSITIVE - percentage of on-topic essays in-correctly flagged as off-topic.FALSE NEGATIVE - percentage of off-topic essayswhich the system failed to flag.In this task, it is of utmost importance to maintainvery low false positive rates, as incorrect labeling ofan on-topic essay as off-topic is undesirable.2.3 ObservationsIn Table 1, we report the average false positive andfalse negative rates for the 7 prompts in the test setfor each essay type.
For long prompts, both Types 1and 2, the false positive rates are very low.
The clas-sification of Type 2 essays which were also writtenby advanced test takers is the most accurate.However, for essays with shorter prompts (Types3 and 4), the false positive rates are higher.
In fact,in the case of Type 4 essays written by English lan-guage learners, the false positive rates are as high as10%.
Therefore we focus on improving the resultsin these two cases which involve short prompts.Both prompt length and the English proficiencyof the test taker seem to influence the prediction ac-curacies for off-topic essay detection.
In our work,we address these two challenges by: a) automaticexpansion of short prompts (Section 3) and b) cor-rection of spelling errors in essay texts (Section 4).3 Prompt expansionWe designed four automatic methods to add relevantwords to the prompt text.2Less strict cutoffs may be used, for example, on-topic iftarget prompt is within rank 3 or 5, etc.
However even a cutoffof 2 incorrectly classifies 25% of off-topic essays as on-topic.933.1 Unsupervised methodsInflected forms: Given a prompt word, ?friendly?,its morphological variants?
?friend?, ?friendlier?,?friendliness?
?are also likely to be used in essaysto that prompt.
Inflected forms are the simplest andmost restrictive class in our set of expansions.
Theywere obtained by a rule-based approach (Leacockand Chodorow, 2003) which adds/modifies prefixesand suffixes of words to obtain inflected forms.These rules were adapted from WordNet rules de-signed to get the base forms of inflected words.Synonyms: Words with the same meaning asprompt words might also be mentioned over thecourse of an essay.
For example, ?favorable?and ?well-disposed?
are synonyms for the word?friendly?
and likely to be good expansions.
Weused an in-house tool to obtain synonyms fromWordNet for each of the prompt words.
The lookupinvolves a word sense disambiguation step to choosethe most relevant sense for polysemous words.
Allthe synonyms for the chosen sense of the promptword are added as expansions.Distributionally similar words: We also consideras expansions words that appear in similar contextsas the prompt words.
For example, ?cordial?, ?po-lite?, ?cheerful?, ?hostile?, ?calm?, ?lively?
and?affable?
often appear in the same contexts as theword ?friendly?.
Such related words form part ofa concept like ?behavioral characteristics of people?and are likely to appear in a discussion of any oneaspect.
These expansions could comprise antonymsand other related words too.
This idea of word simi-larity was implemented in work by Lin (1998).
Sim-ilarity between two words is estimated by examin-ing the degree of overlap of their contexts in a largecorpus.
We access Lin?s similarity estimates usinga tool from Leacock and Chodorow (2003) that re-turns words with similarity values above a cutoff.Word association norms: Word associations havebeen of great interest in psycholinguistic research.Participants are given a target word and asked tomention words that readily come to mind.
The mostfrequent among these are recorded as free associa-tions for that target.
They form another interestingcategory of expansions for our purpose because theyare known to be frequently recalled by human sub-jects for a particular stimulus word.
We added theassociations for prompt words from a collection of5000 target words with their associations producedby about 6000 participants (Nelson et al, 1998).Sample associations for the word ?friendly?
include?smile?, ?amiable?, ?greet?
and ?mean?.3.2 Weighting of prompt words and expansionsAfter expansion, the prompt lengths vary between87 (word associations) and 229 (distributionallysimilar words) content words, considerably higherthan the original average length of 9 and 13 contentwords.
We use a simple weighting scheme3 to mit-igate the influence of noisy expansions.
We assigna weight of 20 to original prompt words and 1 to allthe expansions.
While computing similarity, we usethese weight values as the assumed frequency of theword in the prompt.
In this case, the term frequencyof original words is set as 20 and all expansion termsare considered to appear once in the new prompt.4 Spelling correction of essay textEssays written by learners of a language are prone tospelling errors.
When such errors occur in the use ofthe prompt words, prompt-based techniques will failto identify the essay as on-topic even if it actually is.The usefulness of expansion could also be limitedif there are several spelling errors in the essay text.Hence we explored the correction of spelling errorsin the essay before off-topic detection.We use a tool from Leacock and Chodorow(2003) to perform directed spelling correction, ie.,focusing on correcting the spellings of words mostlikely to match a given target list.
We use the promptwords as the targets.
We also explore the simultane-ous use of spelling correction and expansion.
Wefirst obtain expansion words from one of our unsu-pervised methods.
We then use these along withthe prompt words for spelling correction followedby matching of the expanded prompt and essay text.5 Results and discussionWe used our proposed methods on the two essay col-lections with very short prompts, Type 3 written by3Without any weighting there was an increase in error ratesduring development tests.
We also experimented with a graph-based approach to term weighting which gave similar results.94advanced test takers and Type 4 written by learn-ers of English.
Table 2 compares the suggested en-hancements with the previously proposed method byHiggins et al (2006).
As discussed in Section 2.3,using only the original prompt words, error rates arearound 10% for both essay types.
For advanced testtakers, the false positive rates are lower, around 3%.Usefulness of expanded prompts All the expansionmethods lower the false positive error rates on es-says written by learners with almost no increase inthe rate of false negatives.
On average, the falsepositive errors are reduced by about 3%.
Inflectedforms constitute the best individual expansion cat-egory.
The overall best performance on this typeof essays is obtained by combining inflected formswith word associations.In contrast, for essays written by advanced testtakers, inflected forms is the worst expansion cate-gory.
Here word associations give the best resultsreducing both false positive and false negative er-rors; the reduction in false positives is almost 50%.These results suggest that advanced users of Englishuse more diverse vocabulary in their essays whichare best matched by word associations.Effect of spelling correction For essays written bylearners, spell-correcting the essay text before com-parison (Spell) leads to huge reductions in errorrates.
Using only the original prompt, the false pos-itive rate is 4% lower with spelling correction thanwithout.
Note that this result is even better than thebest expansion technique?inflected forms.
However,for essays written by advanced users, spelling cor-rection does not provide any benefits.
This resultis expected since these test-takers are less likely toproduce many spelling errors.Combination of methods The benefits of the twomethods appear to be population dependent.
Forlearners of English, a spelling correction moduleis necessary while for advanced users, the benefitsare minimal.
On the other hand, prompt expansionworks extremely well for essays written by advancedusers.
The expansions are also useful for essayswritten by learners but the benefits are lower com-pared to spelling correction.
However, for both es-say types, the combination of spelling correction andbest prompt expansion method (Spell + best expn.
)is better compared to either of them individually.Learners AdvancedMethod FP FN FP FNPrompt only 9.73 11.07 2.94 9.06Synonyms 7.03 12.01 1.39 9.76Dist.
6.45 11.77 1.63 8.98WAN 6.33 11.97 1.59 8.74Infl.
forms 6.25 11.65 2.53 9.06Infl.
forms + WAN 6.04 11.48 - -Spell 5.43 12.71 2.53 9.27Spell + best expn.
4.66 11.97 1.47 9.02Table 2: Average error rates after prompt expansion andspelling correctionTherefore the best policy would be to use both en-hancements together for prompt-based methods.6 ConclusionWe have described methods for improving the accu-racy of off-topic essay detection for short prompts.We showed that it is possible to predict words thatare likely to be used in an essay based on words thatappear in its prompt.
By adding such words to theprompt automatically, we built a better representa-tion of prompt content to compare with the essaytext.
The best combination included inflected formsand word associations, reducing the false positivesby almost 4%.
We also showed that spelling correc-tion is a very useful preprocessing step before off-topic essay detection.ReferencesR.S.J.d.
Baker, A.M.J.B.
de Carvalho, J. Raspat,V.
Aleven, A.T. Corbett, and K.R.
Koedinger.
2009.Educational software features that encourage and dis-courage ?gaming the system?.
In Proceedings of theInternational Conference on Artificial Intelligence inEducation.D.
Higgins, J. Burstein, and Y. Attali.
2006.
Identifyingoff-topic student essays without topic-specific trainingdata.
Natural Language Engineering, 12(2):145?159.C.
Leacock and M. Chodorow.
2003.
C-rater: Auto-mated scoring of short-answer questions.
Computersand the Humanities, 37(4):389?405.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In COLING-ACL, pages 768?774.D.
L Nelson, C. L. McEvoy, and T. A. Schreiber.1998.
The University of South Florida wordassociation, rhyme, and word fragment norms,http://www.usf.edu/FreeAssociation/.95
