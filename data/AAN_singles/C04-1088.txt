Linguistic correlates of style: authorship classification with deep linguisticanalysis featuresMichael GamonMicrosoft ResearchMicrosoft Corp.One Microsoft WayRedmond, WA 98052mgamon@microsoft.comAbstractThe identification of authorship falls into thecategory of style classification, an interestingsub-field of text categorization that deals withproperties of the form of linguistic expressionas opposed to the content of a text.
Various fea-ture sets and classification methods have beenproposed in the literature, geared towards ab-stracting away from the content of a text, andfocusing on its stylistic properties.
We demon-strate that in a realistically difficult authorshipattribution scenario, deep linguistic analysisfeatures such as context free production fre-quencies and semantic relationship frequenciesachieve significant error reduction over morecommonly used ?shallow?
features such asfunction word frequencies and part of speechtrigrams.
Modern machine learning techniqueslike support vector machines allow us to ex-plore large feature vectors, combining these dif-ferent feature sets to achieve high classificationaccuracy in style-based tasks.1 IntroductionAuthorship identification has been a long stand-ing topic in the field of stylometry, the analysis ofliterary style (Holmes 1998).
From a broader per-spective, issues of style, genre, and authorship arean interesting sub-area of text categorization.Typically, text categorization concerns itself withclassifying texts according to topics.
For that ob-jective, it is crucial to extract information about thecontent of a text.
In contrast, issues of style, genreand authorship are about the ?form?
of a text.
Theanalysis of style needs to abstract away from thecontent and focus on content-independent formproperties of the linguistic expressions in a text.This makes style analysis a prime candidate for theuse of linguistic processing to extract structuralfeatures.
Viewed from a different angle, the ab-stractness of style assessment features makes themhighly domain-independent and reusable, as longas they are used with a classification technique thattolerates large feature vectors.Previously suggested methods of style categori-zation and authorship identification have made useof a number of content independent features:?
frequencies of function words (Mosteller etal.
1964)?
word length and sentence length statistics(dating back to 1851 according to Holmes1998)?
word tags and tag n-grams (Argamon et al1998, Koppel et al 2003, Santini 2004)?
?stability features?
(Koppel et al 2003) cap-turing the extent to which an item can be re-placed by a semantically similar item?
rewrite rules in an automatic parse (Baayenet al 1996)In this paper, we demonstrate that a combinationof features based on shallow linguistic analysis(function word frequencies, part of speech tri-grams) and a set of deep linguistic analysis features(context free grammar production frequencies andfeatures derived from semantic graphs) yields veryhigh accuracy in attributing a short random textsample to one of the three Bront?
sisters as its au-thor.
Through feature ablation experiments weshow that both the syntactic information capturedin syntactic rewrite rules and the semantic informa-tion from semantic graphs contribute to the finalclassification accuracy.
We also argue that by us-ing support vector machines as a machine learningtechnique, we can leverage a very large number offeatures, effectively combining the different fea-ture sets into large feature vectors, eliminating theneed for laborious manual search for features thatmay be correlated with style.2 DataTo test our approach to authorship identifica-tion, we used texts from Anne, Charlotte andEmily Bront?.
This decision was motivated by thefact that we could keep gender, education and his-toric style differences to a minimum in order tofocus on authorship identification, and by the easyavailability of electronic versions of severallengthy texts from these authors.
The texts we usedwere:Charlotte Bront?
: Jane Eyre, The ProfessorAnne Bront?
: The Tenant of Wildfell Hall,Agnes GreyEmily Bront?
: Wuthering HeightsFor each of the three authors we collected allsentences from those titles and randomized theirorder.
The total number of sentences for each au-thor is: 13220 sentences for Charlotte, 9263 forAnne and 6410 for Emily.
We produced artificialdocuments of 20 sentences in length from thesesets of sentences.
We split the resulting 1441documents 80/20 for training and test.
This splityields 288 documents for test, and 1153 documentsfor training.
All numbers reported in this paper arebased on 5-fold cross validation.3 FeaturesAll linguistic features have been automaticallyextracted using the NLPWin system (for an over-view see Heidorn 2000).
Note that this system pro-duces partial constituent analyses for sentenceseven if no spanning parse can be found.
The onlyexception are sentences of more than 50 wordswhich do not result in any assignment of linguisticstructure.3.1 Length featuresWe measure average length of sentences, noun-phrases, adjectival/adverbial phrases, and subordi-nate clauses per document.3.2 Function word frequenciesWe measure the frequencies of function wordlemmas as identified by the NLPWin system.
Inorder to be maximally ?content-independent?, wenormalized all personal pronouns to an artificialform ?perspro?
in order to not pick up on ?she?
or?he?
frequencies which would be linked to the gen-der of characters in the works of fiction rather thanauthor style.
The number of observed functionword lemmas is 474.3.3 Part-of-speech trigramsWe extract part-of-speech (POS) trigrams fromthe documents and use the frequencies of thesetrigrams as features.
The NLPWin system uses aset of 8 POS tags.
819 different POS trigrams areobserved in the data.3.4 Syntactic productionsThe parses provided by the NLPWin system al-low us to extract context-free grammar productionsfor each sentence, similar to the features in Baayenet al (1996).
Examples of common productionsare:PP ?
PP DETP NOUNINFCL ?
INFTO VERB NPDECL ?
VP CONJ VP CHARFor each observed production, we measure theper-document frequency of the productions.
15.443individual productions (types) occurred in our data,the total number of production tokens is 618.500.3.5 Semantic informationWe extract two kinds of information from thesemantic dependency graphs produced by theNLPWin system: binary semantic features and se-mantic modification relations.
Examples of seman-tic features are number and person features onnouns and pronouns, tense and aspectual featureson verbs, and subcategorization features (indicat-ing realized as opposed to potential subcategoriza-tion) on verbs.
There is a total of 80 such semanticfeatures.Semantic modification relations are representedin a form where for each node A in a semanticgraph the POS of A, the POS of all its n daughtersB1..n, and the semantic relations SR1..n of all itsdaughters B1..n are given.
Some common modifica-tion structures are illustrated below:Noun Possr Pron (a nominal node with a pro-nominal possessor)Verb Tsub Pron Tobj Noun (a verbal node witha pronominal deep subject and a nominal deepobject)Noun Locn Noun (a nominal node with a nomi-nal modifier indicating location)As with the previously discussed features, wemeasure per-document frequency of the observedmodification structures.
There are a total of 9377such structures.3.6 n-gram frequency featuresThe use of word n-gram frequencies is not ap-propriate for style classification tasks since thesefeatures are not sufficiently content-independent.In our experiments, for example, they could pickup on nouns referring to events or locations thatare part of the story told in the work of fiction athand.
We included these features in our experi-ments only as a point of comparison for the purely?form-based?
features.
In order to prevent the mostobvious content-dependency in the word n-gramfrequency features, we normalized proper nouns to?NAME?
and singular personal pronouns to ?Per-spro?.3.7 Feature selectionWhile the total number of observed syntacticand semantic patterns is very high, most of the pat-terns occur only very few times, or even only once.In order to eliminate irrelevant features, we em-ployed a simple frequency cutoff, where the fre-quency of a pattern that occurs less than n times isnot included as a feature.4 The machine learning technique: Sup-port vector machinesFor our experiments we have used support vec-tor machines (SVMs), a machine learning algo-rithm that constructs a plane through a multi-dimensional hyperspace, separating the trainingcases into the target classes.
SVMs have been usedsuccessfully in text categorization and in otherclassification tasks involving highly dimensionalfeature vectors (e.g.
Joachims 1998, Dumais et al1998).
Diederich et al (2003) have applied supportvector machines to the problem of authorship attri-bution.
For our experiments we have used JohnPlatt?s Sequential Minimal Optimization (SMO)tool (Platt 1999).
In the absence of evidence for theusefulness of more complicated kernel functions insimilar experiments (Diederich et al 2003), weused linear SVMs exclusively.5 ResultsAll results discussed in this section should be in-terpreted against a simple baseline accuracyachieved by guessing the most frequent author(Charlotte).
That baseline accuracy is 45.8%.
Allaccuracy differences have been determined to bestatistically significant at the .99 confidence level.5.1 Feature sets in isolationClassification accuracy using the different fea-ture sets (POS trigram frequencies, function wordfrequencies, syntactic features, semantic features)are shown in Figure 1.
The four length featuresdiscussed in section 3.1 yielded a classificationaccuracy of only 54.85% and are not shown inFigure 1.Feature sets in isolation8284868890929496981005 10 20 50 75 100 200 500frequency thresholdaccuracyfunction word frequency pos trigram frequenciessyntactic features semantic featuresFigure 1: Classification accuracy using the feature sets in isolation5.2 Feature sets combinedThe combination of all feature sets yields amuch increased classification accuracy across fre-quency thresholds as shown in Figure 2.
Combin-ing all features, including length features,consistently outperforms all other scenarios.
Re-stricting features to those that only utilize shallowlinguistic analysis, such as the POS trigram fea-tures and the function word frequency features re-duces accuracy by about one percent.
Interestingly,the use of syntactic and semantic features aloneyields classification accuracy below the other fea-ture combinations.
In combination, though, thesefeatures contribute strongly to the overall accuracy.Semantic features which constitute the most ab-stract and linguistically sophisticated class, add tothe accuracy of the classifier.
This is evidenced bycomparing the top two lines in Figure 2 whichshow the accuracy using all features, and the accu-racy using all features except the semantic features.Also included in Figure 2 is the accuracy ob-tainable by using ?content-dependent?
bigram andtrigram frequency features.
As stated above, thesefeatures are not adequate for style assessment pur-poses since they pick up on content, whereas styleassessment needs to abstract away from contentand measure the form of linguistic expression.
It isnoteworthy, however, that the true stylistic and?content-independent?
features produce a classifi-cation accuracy that outperforms the ngram fea-tures by a wide margin.Precision and recall numbers using all featureswith a frequency threshold of 75 (which yields thehighest accuracy at 97.57%) are shown in Table 1.Target Precision Recall F-measureAnne 97.20 98.08 97.64Charlotte 98.18 98.20 98.19Emily 96.81 95.52 96.16Table 1: precision, recall and F-measure for the bestmodel series with all features at frequency cutoff 75.Feature combinations909192939495969798991005 10 20 50 75 100 200 500frequency thresholdaccuracyall features function words and POS trigramssyntactic and LF features no semantic featuresngram featuresFigure 2: Classification accuracy based on combinations of feature setsTable 2 shows error reduction rates for the addi-tion of deep linguistic analysis features to the?shallow?
baseline of function word frequenciesand POS trigrams.Frequencycutoff+ syntacticfeatures+ syntactic andsemantic features5 15.70% 21.60%10 3.80% 11.50%20 14.50% 32.70%50 11.30% 30.20%75 20.40% 28.60%100 14.50% 35.50%200 26.20% 32.80%500 16.40% 20.90%Table 2: Error reduction rates achieved by adding deeplinguistic analysis features to a baseline of POS trigramfeatures and function word frequencies5.3 Number of features and frequencythresholdTable 3 shows the number of features at eachfrequency cutoff.
The total number of style-relatedfeatures ranges from 6018 at a frequency cutoff ofat least 5 observed instances to 546 at a frequencycutoff of 500.
The size of these feature vectors is atthe high end of what has typically been reported inthe literature for similar experiments: For example,Argamon-Engelson et al (1998) use feature vec-tors of size 1185 for newspaper style detection,Finn and Kushmerick (2003) have 36 POS featuresand 152 text statistics features for detection of ?ob-jective?
and ?subjective?
genre, Koppel et al(2004) use 130 features for authorship verification.FrequencycutoffAll featuresFunctionwordsPOStrigramsSyntacticfeaturessemanticfeaturesNgrams5 6018 315 695 3107 1896 2882010 3947 238 650 1885 1170 1231220 2714 186 613 1176 735 543750 1730 140 542 623 421 178975 1421 125 505 442 345 1102100 1233 116 466 355 292 781200 870 88 385 201 192 357500 546 62 257 101 122 114Table 3: The number of features at different frequency cutoffs6 DiscussionWe believe that the results presented in the pre-vious section allow a number of interesting conclu-sions for research into automatic style andauthorship assessment.
First, in our experimentsthe addition of deep linguistic analysis featuresincreases classification accuracy.From a linguistic perspective this is no surprise:it is clear that matters of linguistic form are thosethat can be captured by a syntactic and to someextent by a semantic analysis (as long as the se-mantic analysis is not so abstract that it completelyabstracts away from any form properties of thesentence).
It was less clear, though, whether anautomatic language analysis system can be reliableenough to provide the necessary feature functions.This has been categorically denied in some of theliterature (e.g.
Stamatos et al 2000).
These state-ments, however, did not take into account that aslong as a language analysis system is consistent inthe errors it makes, machine learning techniquescan pick up on correlations between linguistic fea-tures and style even though the label of a linguisticfeature (the ?quality?
it measures) is mislabeled.Secondly, we would like to emphasize that theresults we have achieved are not based on deliber-ate selection of a small set of features as likelycandidates for correlation with style.
We have se-lected sets of features to be included in our ex-periments, but whether or not an individual featureplays a role was left to the machine learning tech-nique to decide.
Ideally, then, we would pass anynumber of features to the classifier algorithm andexpect it to select relevant features during the train-ing process.
While this is possible with a largenumber of training cases, a smaller number oftraining cases poses a limit to the number of fea-tures that should be used to achieve optimal classi-fication accuracy and prevent overfitting.
In orderto prevent overfitting it is desirable to reduce thevector size to a number that does not exceed thenumber of training cases.
Support vector machinesare very robust to overfitting, and in our experi-ments we find that classification results were quiterobust to feature vectors with up to 4 times the sizeof the training set.
However, it is still the case thatoptimal accuracy is achieved where the size of thefeature vector comes close to the training sample(at a frequency cutoff of 75 for the vector contain-ing all sets of features).We also examined the features that carried highweights in the SVMs.
Among the most highlyweighted features we found a mix of different fea-ture types.
Below is a very small sample from thetop-weighted features (recall that all featuresmeasure frequency):?
punctuation character starting a sentence(quote, double dash etc)?
but?
NOUN CONJ NOUN sequence?
on?
prepositional phrases consisting of preposi-tion and pronoun?
VERB ADVERB CHAR sequences?
progressive verbs?
verbal predicates with a pronominal subjectand a clausal objectIn order to determine whether our results holdon sample documents of smaller size, we con-ducted a second round of experiments wheredocument length was scaled down to five sen-tences per document.
This yielded a total of 5767documents, which we subjected to the same 80/20split and 5fold cross-validation as in the previousexperiments.
Results as shown in Table 4 are veryencouraging: using all features, we achieve amaximum classification accuracy of 85%.
As inour previous experiments, removing deep linguisticanalysis features degrades the results.FrequencythresholdNumber ofall featuresNumber ofshallow featuresAccuracy usingall featuresAccuracy usingshallow features5 6018 1011 85.00 81.6510 3947 889 84.96 81.5620 2714 800 84.84 81.2575 1421 631 84.53 80.59Table 4: results on documents of a length of 5 sentencesIt should also be clear that simple frequencycutoffs are a crude way of reducing the number offeatures.
Not every frequent feature is likely to bediscriminative (in our example, it is unlikely that aperiod at the end of a sentence is discriminative),and not every infrequent feature is likely to be non-discriminative.
In fact, hapax legomena, the singleoccurrence of a certain lexeme has been used todiscriminate authors.
Baayen et al (1996) alsohave pointed out the discriminatory role of infre-quent syntactic patterns.
What we need, then, is amore sophisticated thresholding technique to re-strict the feature vector size.
We have begun ex-perimenting with log likelihood ratio (Dunning1993) as a thresholding technique.To assess at least anecdotally whether our re-sults hold in a different domain, we also tested onsentences from speeches of George Bush Jr. andBill Clinton (2231 sentences from the former, 2433sentences from the latter).
Using document sam-ples with 5 sentences each, 10-fold cross-validation and a frequency cutoff of 5, we achieved87.63% classification accuracy using all features,and 83.00% accuracy using only shallow features(function word frequencies and POS trigrams).Additional experiments with similar methodologyare under way for a stylistic classification taskbased on unedited versus highly edited documentswithin the technical domain.7 ConclusionWe have shown that the use of deep linguisticanalysis features in authorship attribution can yielda significant reduction in error rate over the use ofshallow linguistic features such as function wordfrequencies and part of speech trigrams.
We havefurthermore argued that by using a modern ma-chine learning technique that is robust to large fea-ture vectors, combining different feature sets yieldsoptimal results.
Reducing the number of features(i.e.
the number of parameters to be estimated bythe learning algorithm) by frequency cutoffs to bein the range of the number of training cases pro-duced good results, although it is to be expectedthat more intelligent thresholding techniques suchas log likelihood ratio will further increase per-formance.
These results hold up even if documentsize is reduced to only five sentences.We believe that these results show that thecommon argument of the ?unreliability?
of auto-matic linguistic processing used for feature extrac-tion for style assessment is not as strong as itseems.
As long as the errors introduced by a parserare systematic, a machine learning system pre-sented with a large number of features can stilllearn relevant correlations.Areas for further research in this area includeexperimentation with additional authorship andstyle classification tasks/scenarios, experimentswith different thresholding techniques and possiblywith additional linguistic feature sets.Additionally, we plan to investigate the possibil-ity of training different classifiers, each of whichcontains features from one of the four major fea-ture sets (function word frequencies, POS trigramfrequencies, syntactic production frequencies, se-mantic feature frequencies), and maximally n suchfeatures where n is the number of training cases.The votes from the ensemble of four classifierscould then be combined with a number of differentmethods, including simple voting, weighted vot-ing, or ?stacking?
(Dietterich 1998).AcknowledgementsWe thank Anthony Aue, Eric Ringger (MicrosoftResearch) and James Lyle (Microsoft Natural Lan-guage Group) for many helpful discussions.ReferencesShlomo Argamon-Engelson, Moshe Koppel, andGalit Avneri.
1998.
Style-Based Text Categori-zation: What Newspaper am I Reading?
Pro-ceedings of AAAI Workshop on Learning forText Categorization, 1-4.Harald Baayen, Hans van Halteren, and FionaTweedie.
1996.
Outside the Cave of Shadows:Using Syntactic Annotation to Enhance Author-ship Attribution.
Literary and Linguistic Com-puting 11(3): 121-131.Joachim Diederich, J?rg Kindermann, Edda Leo-pold, and Gerhard Paass.2003.
Authorship At-tribution with Support Vector Machines.Applied Intelligence 19(1):109-123.Thomas G. Dietterich.
1998.
Machine LearningResearch: Four Current Directions.
The AIMagazine 18(4): 97-136.Susan Dumais, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive Learning Al-gorithms and Representations for Text Categori-zation.
Proceedings of the 7th InternationalConference on Information and KnowledgeManagement: 148-155.Ted Dunning.
1993.
Accurate Methods for the Sta-tistics of Surprise and Coincidence.
Computa-tional Linguistics 19: 61-74.Aidan Finn and Nicholas Kushmerick.
2003.Learning to Classify Documents According toGenre.
IJCAI-2003 Workshop on Computa-tional Approaches to Text Style and Synthesis,Acapulco, Mexico.George Heidorn.
2000.
Intelligent Writing Assis-tance.
In R. Dale, H. Moisl and H. Somers, eds.,Handbook of Natural Language Processing.Marcel Dekker.David I. Holmes.
1998.
The Evolution of Stylome-try in Humanities Scholarship.
Literary andLinguistic Computing 13(3):111-117.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with manyRelevant Features.
Proceedings of the tenthEuropean Conference on Machine Learn-ing:137-142.Moshe Koppel, Navot Akiva and Ido Dagan.
2003.A Corpus-Independent Feature Set for Style-Based Text Categorization.
IJCAI-2003 Work-shop on Computational Approaches to TextStyle and Synthesis, Acapulco, Mexico.Moshe Koppel, Jonathan Schler and Droz Mughaz.2004.
Text Categorization for Authorship Veri-fication.
Paper presented at the 8th Symposiumon Artifical Intelligence and Mathematics, FortLauderdale, Florida.Moshe Koppel, Shlomo Argamon, and Anat R.Shimoni.
2003.
Automatically CategorizingWritten Texts by Author Gender.
Literary andLinguistic Computing 17(4): 401-412.F.
Mosteller.
and D. L. Wallace.
1964.
AppliedBayesian and Classical Inference: The Case ofthe Federalist Papers.
Addison-Wesley, Read-ing, MA.John Platt.
1999.
Fast Training of SVMs UsingSequential Minimal Optimization.
In:B.Sch?lkopf, C. Burges and A. Smola (eds.)
Ad-vances in kernel methods: support vector learn-ing.
MIT Press, Cambridge, MA, 185-208.Marina Santini.
2004.
A Shallow Approach to Syn-tactic Feature Extraction for Genre Classifica-tion.
Proceedings of the 7th Annual Colloquiumfor the UK Special Interest Group for Computa-tional Linguistics.Efstathios Stamatos, Nikos Fakotakis and GeorgeKokkinakis.
2000.
Automatic Text Categoriza-tion in Terms of Genre and Author.
Computa-tional Linguistics 26(4): 471-495.
