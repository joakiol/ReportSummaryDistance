Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1285?1295,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning Crosslingual Word Embeddings without Bilingual CorporaLong Duong,12 Hiroshi Kanayama,3 Tengfei Ma,3 Steven Bird14 and Trevor Cohn11Department of Computing and Information Systems, University of Melbourne2National ICT Australia, Victoria Research Laboratory3IBM Research ?
Tokyo4International Computer Science Institute, University of California BerkeleyAbstractCrosslingual word embeddings represent lexi-cal items from different languages in the samevector space, enabling transfer of NLP tools.However, previous attempts had expensive re-source requirements, difficulty incorporatingmonolingual data or were unable to handlepolysemy.
We address these drawbacks inour method which takes advantage of a highcoverage dictionary in an EM style trainingalgorithm over monolingual corpora in twolanguages.
Our model achieves state-of-the-art performance on bilingual lexicon inductiontask exceeding models using large bilingualcorpora, and competitive results on the mono-lingual word similarity and cross-lingual doc-ument classification task.1 IntroductionMonolingual word embeddings have hadwidespread success in many NLP tasks includ-ing sentiment analysis (Socher et al, 2013),dependency parsing (Dyer et al, 2015), machinetranslation (Bahdanau et al, 2014).
Crosslingualword embeddings are a natural extension facilitatingvarious crosslingual tasks, e.g.
through transferlearning.
A model built in a source resource-richlanguage can then applied to the target resourcepoor languages (Yarowsky and Ngai, 2001; Dasand Petrov, 2011; Ta?ckstro?m et al, 2012; Duong etal., 2015).
A key barrier for crosslingual transferis lexical matching between the source and thetarget language.
Crosslingual word embeddingsare a natural remedy where both source and targetlanguage lexicon are presented as dense vectors inthe same vector space (Klementiev et al, 2012).Most previous work has focused on down-streamcrosslingual applications such as document classi-fication and dependency parsing.
We argue thatgood crosslingual embeddings should preserve bothmonolingual and crosslingual quality which we willuse as the main evaluation criterion through mono-lingual word similarity and bilingual lexicon induc-tion tasks.
Moreover, many prior work (Chandar A Pet al, 2014; Koc?isky?
et al, 2014) used bilingual orcomparable corpus which is also expensive for manylow-resource languages.
S?gaard et al (2015) im-pose a less onerous data condition in the form oflinked Wikipedia entries across several languages,however this approach tends to underperform othermethods.
To capture the monolingual distributionalproperties of words it is crucial to train on largemonolingual corpora (Luong et al, 2015).
How-ever, many previous approaches are not capable ofscaling up either because of the complicated objec-tive functions or the nature of the algorithm.
Othermethods use a dictionary as the bridge between lan-guages (Mikolov et al, 2013a; Xiao and Guo, 2014),however they do not adequately handle translationambiguity.Our model uses a bilingual dictionary from Pan-lex (Kamholz et al, 2014) as the source of bilin-gual signal.
Panlex covers more than a thousand lan-guages and therefore our approach applies to manylanguages, including low-resource languages.
Ourmethod selects the translation based on the contextin an Expectation-Maximization style training algo-rithm which explicitly handles polysemy through in-corporating multiple dictionary translations (wordsense and translation are closely linked (Resnik andYarowsky, 1999)).
In addition to the dictionary,1285our method only requires monolingual data.
Ourapproach is an extension of the continuous bag-of-words (CBOW) model (Mikolov et al, 2013b) toinject multilingual training signal based on dictio-nary translations.
We experiment with several vari-ations of our model, whereby we predict only thetranslation or both word and its translation and con-sider different ways of using the different learnedcenter-word versus context embeddings in applica-tion tasks.
We also propose a regularisation methodto combine the two embedding matrices duringtraining.
Together, these modifications substantiallyimprove the performance across several tasks.
Ourfinal model achieves state-of-the-art performance onbilingual lexicon induction task, large improvementover word similarity task compared with previouspublished crosslingual word embeddings, and com-petitive result on cross-lingual document classifica-tion task.
Notably, our embedding combining tech-niques are general, yielding improvements also formonolingual word embedding.This paper makes the following contributions:?
Proposing a new crosslingual training methodfor learning vector embeddings, based only onmonolingual corpora and a bilingual dictio-nary;?
Evaluating several methods for combining em-beddings, which are shown to help in bothcrosslingual and monolingual evaluations; and?
Achieving consistent results which are compet-itive in monolingual, bilingual and crosslingualtransfer settings.2 Related workThere is a wealth of prior work on crosslingualword embeddings, which all exploit some kind ofbilingual resource.
This is often in the form of aparallel bilingual text, using word alignments as abridge between tokens in the source and target lan-guages, such that translations are assigned similarembedding vectors (Luong et al, 2015; Klemen-tiev et al, 2012).
These approaches are affectedby errors from automatic word alignments, motivat-ing other approaches which operate at the sentencelevel (Chandar A P et al, 2014; Hermann and Blun-som, 2014; Gouws et al, 2015) through learningcompositional vector representations of sentences,in order that sentences and their translations rep-resentations closely match.
The word embeddingslearned this way capture translational equivalence,despite not using explicit word alignments.
Nev-ertheless, these approaches demand large parallelcorpora, which are not available for many languagepairs.Vulic?
and Moens (2015) use bilingual compara-ble text, sourced from Wikipedia.
Their approachcreates a psuedo-document by forming a bag-of-words from the lemmatized nouns in each compa-rable document concatenated over both languages.These pseudo-documents are then used for learningvector representations using Word2Vec.
Their sys-tem, despite its simplicity, performed surprisinglywell on a bilingual lexicon induction task (we com-pare our method with theirs on this task.)
Their ap-proach is compelling due to its lesser resource re-quirements, although comparable bilingual data isscarce for many languages.
Related, S?gaard et al(2015) exploit the comparable part of Wikipedia.They represent word using Wikipedia entries whichare shared for many languages.A bilingual dictionary is an alternative source ofbilingual information.
Gouws and S?gaard (2015)randomly replace the text in a monolingual cor-pus with a random translation, using this corpus forlearning word embeddings.
Their approach doesn?thandle polysemy, as very few of the translations foreach word will be valid in context.
For this reason ahigh coverage or noisy dictionary with many trans-lations might lead to poor outcomes.
Mikolov et al(2013a), Xiao and Guo (2014) and Faruqui and Dyer(2014) filter a bilingual dictionary for one-to-onetranslations, thus side-stepping the problem, how-ever discarding much of the information in the dic-tionary.
Our approach also uses a dictionary, how-ever we use all the translations and explicitly disam-biguate translations during training.Another distinguishing feature on the above-citedresearch is the method for training embeddings.Mikolov et al (2013a) and Faruqui and Dyer (2014)use a cascade style of training where the word em-beddings in both source and target language aretrained separately and then combined later using thedictionary.
Most of the other works train multlingualmodels jointly, which appears to have better perfor-mance over cascade training (Gouws et al, 2015).1286For this reason we also use a form of joint trainingin our work.3 Word2VecOur model is an extension of the contextual bag ofwords (CBOW) model of Mikolov et al (2013b), amethod for learning vector representations of wordsbased on their distributional contexts.
Specifically,their model describes the probability of a token wiat position i using logistic regression with a factoredparameterisation,p(wi|wi?k\i) =exp(u>wihi)?w?W exp(u>whi), (1)where hi = 12k?kj=?k;j 6=0 vwi+j is a vector en-coding the context over a window of size k centredaround position i, W is the vocabulary and the pa-rameters V and U ?
R|W |?d are matrices referredto as the context and word embeddings.
The modelis trained to maximise the log-pseudo likelihood ofa training corpus, however due to the high complex-ity of computing the denominator of equation (1),Mikolov et al (2013b) propose negative sampling asan approximation, by instead learning to differenti-ate data from noise (negative examples).
This givesrise to the following optimisation objective?i?D(log ?
(u>wihi)+p?j=1Ewj?Pn(w) log ?
(?u>wjhi)),(2)where D is the training data and p is the numberof negative examples randomly drawn from a noisedistribution Pn(w).4 Our ApproachOur approach extends CBOW to model bilingualtext, using two monolingual corpora and a bilin-gual dictionary.
We believe this data condition tobe less stringent than requiring parallel or compa-rable texts as the source of the bilingual signal.
Itis common for field linguists to construct a bilin-gual dictionary when studying a new language, asone of the first steps in the language documentationprocess.
Translation dictionaries are a rich informa-tion source, capturing much of the lexical ambigu-ity in a language through translation.
For example,the word bank in English might mean the river bankAlgorithm 1 EM algorithm for selecting translationduring training, where ?
= (U,V) are the modelparameters and ?
is the learning rate.1: randomly initialize V, U2: for i < Iter do3: for i ?
De ?Df do4: s?
vwi + hi5: w?i = argmaxw?dict(wi) cos(s,vw)6: ?
?
?
+ ?
?O(w?i,wi,hi)??
{see (3) or (5)}7: end for8: end foror financial bank which corresponds to two differ-ent translations sponda and banca in Italian.
If weare able to learn to select good translations, then thisimplicitly resolves much of the semantic ambiguityin the language, and accordingly we seek to use thisidea to learn better semantic vector representationsof words.4.1 Dictionary replacementTo learn bilingual relations, we use the context inone language to predict the translation of the centreword in another language.
This is motivated by thefact that the context is an excellent means of disam-biguating the translation for a word.
Our method isclosely related to Gouws and S?gaard (2015), how-ever we only replace the middle word wi with atranslation w?i while keeping the context fixed.
Wereplace each centre word with a translation on thefly during training, predicting instead p(w?i|wi?k\i)but using the same formulation as equation (1) albeitwith an augmented U matrix to cover word types inboth languages.The translation w?i is selected from the possibletranslations of wi listed in the dictionary.
The prob-lem of selecting the correct translation from themany options is reminiscent of the problem facedin expectation maximisation (EM), in that cross-lingual word embeddings will allow for accuratetranslation, however to learn these embeddings weneed to know the translations.
We propose an EM-inspired algorithm, as shown in Algorithm 1, whichoperates over both monolingual corpora, De andDf .
The vector s is the semantic representationcombining both the centre word, wi, and the con-1287text,1 which is used to choose the best translationinto the other language from the bilingual dictionarydict(wi).2 After selecting the translation, we use w?itogether with the context vector h to make a stochas-tic gradient update of the CBOW log-likelihood.4.2 Joint TrainingWords and their translations should appear in verysimilar contexts.
One way to enforce this is to jointlylearn to predict both the word and its translationfrom its monolingual context.
This gives rise to thefollowing joint objective function,O =?i?De?Df(?
log ?(u>wihi)+(1??)
log ?
(u>w?ihi)+p?j=1Ewj?Pn(w) log ?
(?u>wjhi)), (3)where ?
controls the contribution of the two terms.For our experiments, we set ?
= 0.5.
The nega-tive examples are drawn from combined vocabularyunigram distribution calculated from combined dataDe ?Df .4.3 Combining EmbeddingsMany vector learning methods learn two embeddingspaces V and U.
Usually only V is used in appli-cation.
The use of U, on the other hand, is under-studied (Levy and Goldberg, 2014) with the excep-tion of Pennington et al (2014) who use a linearcombination U + V, with minor improvement overV alone.We argue that with our model, V is better at cap-turing the monolingual regularities and U is better atcapturing bilingual signal.
The intuition for this is asfollows.
Assuming that we are predicting the wordfinance and its Italian translation finanze from thecontext (money, loan, bank, debt, credit) as shownin figure 1.
In V only the context word representa-tions are updated and in U only the representationsof finance, finanze and negative samples such as treeand dog are updated.
CBOW learns good embed-dings because each time it updates the parameters,the words in the contexts are pushed closer to each1Using both embeddings gives a small improvement com-pared to just using context vector h alone.2We also experimented with using expectations over trans-lations, as per standard EM, with slight degredation in results.moneyloancreditdebtbankfinancefinanzetreedogVUFigure 1: Example of V and U space during train-ing.other in the V space.
Similarly, the target word wiand the translation w?i are also pushed closer in theU space.
This is directly related to poitwise mutualinformation values of each pair of word and contextexplained in Levy and Goldberg (2014).
Thus, Uis bound to better at bilingual lexicon induction taskand V is better at monolingual word similarity task.The simple question is, how to combine both Vand U to produce a better representation.
We exper-iment with several ways to combine V and U. First,we can follow Pennington et al (2014) to interpolateV and U in the post-processing step.
i.e.
?V + (1?
?
)U (4)where ?
controls the contribution of each embed-ding space.
Second, we can also concatenate V andU instead of interpolation such that C = [V : U]where C ?
R|W |?2d and W is the combined vocab-ulary from De ?Df .Moreover, we can also fuse V and U duringtraining.
For each word in the combined dictionaryVe ?
Vf , we encourage the model to learn similarrepresentation in both V and U by adding a regular-ization term to the objective function in equation (3)during training.O?
= O + ?
?w?Ve?Vf?uw ?
vw?22 (5)where ?
controls to what degree we should bind twospaces together.35 Experimental SetupOur experimental evaluation seeks to determine howwell lexical distances in the learned embedding3In the stochastic gradient update for a given word in con-text, we only compute the gradient of the regularisation term in(5) with respect to the words in the set of positive and negativeexamples.1288spaces match with known lexical similarity judge-ments from bilingual and monolingual lexical re-sources.
To this end, in ?6 we test crosslingualdistances using a bilingual lexicon induction taskin which we evaluate the embeddings in terms ofhow well nearby pairs of words from two lan-guages in the embedding space match with humanjudgements.
Next, to evaluate the monolingual em-beddings we evaluate word similarities in a singlelanguage against standard similarity datasets (?7).Lastly, to demonstrate the usefulness of our em-beddings in a task-based setting, we evaluate oncrosslingual document classification (?9).Monolingual Data The monolingual data is takenfrom the pre-processed Wikipedia dump from Al-Rfou et al (2013).
The data is already cleaned andtokenized.
We additionally lower-case all words.Normally monolingual word embeddings are trainedon billions of words.
However, obtaining that muchmonolingual data for a low-resource language is in-feasible.
Therefore, we only select the first 5 millionsentences (around 100 million words) for each lan-guage.Dictionary A bilingual dictionary is the onlysource of bilingual correspondence in our tech-nique.
We prefer a dictionary that covers manylanguages, such that our approach can be appliedwidely to many low-resource languages.
We usePanlex, a dictionary which currently covers around1300 language varieties with about 12 million ex-pressions.
The translations in PanLex come fromvarious sources such as glossaries, dictionaries, au-tomatic inference from other languages, etc.
Ac-cordingly, Panlex has high language coverage butoften noisy translations.4 Table 1 summarizes thesizes of monolingual corpora and dictionaries foreach pair of language in our experiments.4We also experimented with a crowd-sourced dictionaryfrom Wiktionary.
Our initial observation was that the transla-tion quality was better but with a lower-coverage.
For example,for en-it dictionary, Panlex and Wiktionary have a coverageof 42.1% and 16.8% respectively for the top 100k most frequentEnglish words from Wikipedia.
The average number of trans-lations are 5.2 and 1.9 respectively.
We observed similar trendusing Panlex and Wiktionary dictionary in our model.
How-ever, using Panlex results in much better performance.
We canrun the model on the combined dictionary from both Panlex andWiktionary but we leave it for future work.Source (M) Target (M) Dict (k)en-es 120.1 (73.9%) 126.8 (74.4%) 712.0en-it 120.1 (74.7%) 114.6 (67.4%) 560.1en-nl 120.1 (69.1%) 80.2 (63.4%) 406.6en-de 120.1 (77.8%) 90.8 (68.3%) 964.4en-sr 120.1 (28.0%) 7.5 (17.5%) 35.1Table 1: Number of tokens in millions for the sourceand target languages in each language pair.
Alsoshown is the number of entries in the bilingual dic-tionary in thousands.
The number in the parenthesisshows the token coverage in the dictionary on eachmonolingual corpus.6 Bilingual Lexicon InductionGiven a word in a source language, the bilinguallexicon induction (BLI) task is to predict its transla-tion in the target language.
Vulic?
and Moens (2015)proposed this task to test crosslingual word embed-dings.
The difficulty of this is that it is evaluatedusing the recall of the top ranked word.
The modelmust be very discriminative in order to score well.We build the CLWE for 3 language pairs: it-en,es-en and nl-en, using similar parameters set-ting with Vulic?
and Moens (2015).5 The remainingtunable parameters in our system are ?
from Equa-tion (5), and the choice of algorithm for combiningembeddings.
We use the regularization techniquefrom ?4.3 for combining context and word embed-dings with ?
= 0.01, and word embeddings U areused as the output for all experiments (but see com-parative experiments in ?8.
)Qualitative evaluation We jointly train the modelto predict both wi and the translation w?i, combineV and U during training for each language pair.
Ta-ble 2 shows the top 10 closest words in both sourceand target languages according to cosine similarity.Note that the model correctly identifies the transla-tion in en as the top candidate, and the top 10 wordsin both source and target languages are highly re-lated.
This qualitative evaluation initially demon-strates the ability of our CLWE to capture both thebilingual and monolingual relationship.Quantitative evaluation Table 3 shows our re-sults compared with prior work.
We reimple-5Default learning rate of 0.025, negative sampling with 25samples, subsampling rate of value 1e?4, embedding dimen-sion d = 200, window size cs = 48 and run for 15 epochs.1289gravedades tassazioneites en it engravitacional gravity?
tasse taxation?gravitatoria gravitation?
fiscale taxesaceleracin acceleration tassa tax?gravitacin non-gravitational imposte leviedinercia inertia imposta fiscalgravity centrifugal fiscali low-taxmsugra free-falling l?imposta revenuecentr?
?fuga gravitational tonnage levycurvatura free-fall tax annatesmasa newton accise evasionTable 2: Top 10 closest words in both source andtarget language corresponding to es word gravedad(left) and it word tassazione (right).
They have 15and 4 dictionary translations respectively.
The enwords in the dictionary translations are marked with(?).
The correct translation is in bold.ment Gouws and S?gaard (2015) using Panlex andWiktionary dictionaries.
The result with Panlex issubstantially worse than with Wiktionary.
This con-firms our hypothesis in ?2.
That is the context mightbe corrupted if we just randomly replace the trainingdata with the translation from noisy dictionary suchas Panlex.Our model when randomly picking the translationis similar to Gouws and S?gaard (2015), using thePanlex dictionary.
The biggest difference is that theyreplace the training data (both context and middleword) while we fix the context and only replace themiddle word.
For a high coverage yet noisy dictio-nary such as Panlex, our approach gives better av-erage score.
Comparing our two most basic mod-els (EM selection and random selection), it is clearthat the model using EM to select the translation out-performs random selection by a significant margin.Our joint model, as described in equation (3)which predicts both target word and the transla-tion, further improves the performance, especiallyfor nl-en.
We use equation (5) to combine bothcontext embeddings V and word embeddings Ufor all three language pairs.
This modification dur-ing training substantially improves the performance.More importantly, all our improvements are consis-tent for all three language pairs and both evaluationmetrics, showing the robustness of our models.Our combined model out-performed previous ap-proaches by a large margin.
Vulic?
and Moens (2015)used bilingual comparable data, but this might behard to obtain for some language pairs.
Their perfor-mance on nl-en is poor because their comparabledata between en and nl is small.
Besides, they alsouse POS tagger and lemmatizer to filter only Nounand reduce the morphology complexity during train-ing.
These tools might not be available for manylanguages.
For a fairer comparison to their work,we also use the same Treetagger (Schmid, 1995) tolemmatize the output of our combined model beforeevaluation.
Table 3 (+lemmatization) shows someimprovements but minor.
It demonstrates that ourmodel is already good at disambiguating morphol-ogy.
For example, the top 2 translations for es wordlenguas in en are languages and language whichcorrectly prefer the plural translation.7 Monolingual Word SimilarityNow we consider the efficacy of our CLWE onmonolingual word similarity.
We evaluate on En-glish monolingual similarity on WordSim353 (WS-en), RareWord (RW-en) and German version ofWordSim353 (WS-de) (Finkelstein et al, 2001; Lu-ong et al, 2013; Luong et al, 2015).
Each of thosedatasets contain many tuples (w1, w2,s) where sis a scalar denoting the semantic similarity betweenw1 and w2 given by human annotators.
Good sys-tem should produce the score correlated with humanjudgement.We train the model as described in ?4, which isthe combine embeddings setting from Table 3.
Sincethe evaluation involves de and en word similar-ity, we train the CLWE for en-de pair.
Table 4shows the performance of our combined model com-pared with several baselines.
Our combined modelout-performed both Luong et al (2015) and Gouwsand S?gaard (2015)6 which represent the best pub-lished crosslingual embeddings trained on bitext andmonolingual data respectively.We also compare our system with the monolin-gual CBOW model trained on the monolingual datafor each language, using the same parameter settingsfrom earlier (?6).
Surprisingly, our combined modelperforms better than the monolingual CBOW base-line which makes our result close to the monolin-gual state-of-the-art on each different dataset.
How-ever, the best monolingual methods use much larger6trained using the Panlex dictionary1290Model es-en it-en nl-en Averagerec1 rec5 rec1 rec5 rec1 rec5 rec1 rec5Gouws and S?gaard (2015) + Panlex 37.6 63.6 26.6 56.3 49.8 76.0 38.0 65.3Gouws and S?gaard (2015) + Wikt 61.6 78.9 62.6 81.1 65.6 79.7 63.3 79.9BilBOWA: Gouws et al (2015) 51.6 - 55.7 - 57.5 - 54.9 -Vulic?
and Moens (2015) 68.9 - 68.3 - 39.2 - 58.8 -Our model (random selection) 41.1 62.0 57.4 75.4 34.3 55.5 44.3 64.3Our model (EM selection) 67.3 79.5 66.8 82.3 64.7 82.4 66.3 81.4+ Joint model 68.0 80.5 70.5 83.3 68.8 84.0 69.1 82.6+ combine embeddings (?
= 0.01) 74.7 85.4 80.8 90.4 79.1 90.5 78.2 88.8+ lemmatization 74.9 86.0 81.3 91.3 79.8 91.3 78.7 89.5Table 3: Bilingual Lexicon Induction performance from es, it, nl to en.
Gouws and S?gaard (2015)+ Panlex/Wikt is our reimplementation using Panlex/Wiktionary dictionary.
All our models use Panlex asthe dictionary.
We reported the recall at 1 and 5.
The best performance is bold.Model WS-de WS-en RW-enBaselines Klementiev et al (2012) 23.8 13.2 7.3Chandar A P et al (2014) 34.6 39.8 20.5Hermann and Blunsom (2014) 28.3 19.8 13.6Luong et al (2015) 47.4 49.3 25.3Gouws and S?gaard (2015) 67.4 71.8 31.0Mono CBOW 62.2 70.3 42.7+ combine 65.8 74.1 43.1Yih and Qazvinian (2012) - 81.0 -Shazeer et al (2016) - 74.8 48.3Ours Our joint-model 59.3 68.6 38.1+ combine 71.1 76.2 44.0Table 4: Spearman?s rank correlation for monolin-gual similarity measurement on 3 datasets WS-de(353 pairs), WS-en (353 pairs) and RW-en (2034pairs).
We compare against 5 baseline crosslingualword embeddings.
The best CLWE performance isbold.
For reference, we add the monolingual CBOWwith and without embeddings combination, Yih andQazvinian (2012) and Shazeer et al (2016) whichrepresents the monolingual state-of-the-art resultsfor WS-en and RW-en.monolingual corpora (Shazeer et al, 2016), Word-Net or the output of commercial search engines (Yihand Qazvinian, 2012).Next we explain the gain of our combined modelcompared with the monolingual CBOW model.First, we compare the combined model with thejoint-model with respect to monolingual CBOWmodel (Table 4).
It shows that the improvementseems mostly come from combining V and U. Ifwe apply the combining algorithm to the monolin-gual CBOW model (CBOW + combine), we also ob-serve an improvement.
Clearly most of the improve-ment is from combining V and U, however our Vand U are more complementary as the gain is moremarked.
Other improvements can be explained bythe observation that a dictionary can improve mono-lingual accuracy through linking synonyms (Faruquiand Dyer, 2014).
For example, since plane, airplaneand aircraft have the same Italian translation aereo,the model will encourage those words to be closer inthe embedding space.8 Model selectionCombining context embeddings and word embed-dings results in an improvement in both monolin-gual similarity and bilingual lexicon induction.
In?4.3, we introduce several combination methods in-cluding post-processing (interpolation and concate-nation) and during training (regularization).
In thissection, we justify our parameter and model choices.We use en-it pair for tuning purposes, consid-ering the value of ?
in equation 4.
Figure 2 showsthe performances using different values of ?.
Thetwo extremes where ?
= 0 and ?
= 1 correspondsto no interpolation where we just use U or V re-spectively.
As ?
increases, the performance on WS-en increases yet BLI decreases.
These results con-firm our hypothesis in ?4.3 that U is better at cap-turing bilingual relations and V is better at captur-ing monolingual relations.
As a compromise, wechoose ?
= 0.5 in our experiments.
Similarly, wetune the regularization sensitivity ?
in equation (5)which combines embeddings space during training.We test ?
= 10?n with n = {0, 1, 2, 3, 4} and us-12910 0.3 0.5 0.7 1Gamma4050607080ScoreBLI (recall@1)BLI (recall@5) Mono (WS?En)Figure 2: Performance of word embeddings inter-polated using different values of ?
evaluated usingBLI (Recall@1, Recall@5) and English monolin-gual WordSim353 (WS-en).Model BLI Monorec1 rec5 WS-enAlone Joint-model + V 67.6 82.8 70.5Joint-model + U 76.2 84.7 48.4Combine Interpolation[V+U2] 75.0 85.9 72.7Concatenation 72.7 85.2 71.2Regularization + V 80.3 89.8 45.9Regularization + U 80.8 90.4 74.8Regularization + V+U2 80.9 91.1 72.3Table 5: Performance on en-it BLI and en mono-lingual similarity WordSim353 (WS-en) for variouscombining algorithms mentioned in ?4.3 w.r.t justusing U or V alone (after joint-training).
We use?
= 0.5 for interpolation and ?
= 0.01 for regular-ization with the choice of V, U or interpolation ofbothV+U2 for the output.
The best scores are bold.ing V, U or the interpolation of both V+U2 as thelearned embeddings, evaluated on the same BLI andWS-en.
We select ?
= 0.01.Table 5 shows the performance with and with-out using combining algorithms mentioned in ?4.3.As the compromise between both monolingual andcrosslingual tasks, we choose regularization + U asthe combination algorithm.
All in all, we apply theregularization algorithm for combining V and Uwith ?
= 0.01 and U as the output for all languagepairs without further tuning.9 Crosslingual Document ClassificationIn this section, we evaluate our CLWE on a down-stream crosslingual document classification (CLDC)Model en?
de de?
enMT baseline 68.1 67.4Klementiev et al (2012) 77.6 71.1Gouws et al (2015) 86.5 75.0Koc?isky?
et al (2014) 83.1 75.4Chandar A P et al (2014) 91.8 74.2Hermann and Blunsom (2014) 86.4 74.7Luong et al (2015) 88.4 80.3Our model 86.3 76.8Table 6: CLDC performance for both en?
de andde?
en direction for many CLWE.
The MT base-line uses phrase-based statistical machine translationto translate the source language to target language(Klementiev et al, 2012).
The best scores are bold.task.
In this task, the document classifier is trainedon a source language and then applied directly toclassify a document in the target language.
This isconvenient for a target low-resource language wherewe do not have document annotations.
The experi-mental setup is the same as Klementiev et al (2012)7with the training and testing data sourced fromReuter RCV1/RCV2 corpus (Lewis et al, 2004).The documents are represented as the bag of wordembeddings weighted by tf.idf.
A multi-classclassifier is trained using the average perceptron al-gorithm on 1000 documents in the source languageand tested on 5000 documents in the target language.We use the CLWE, such that the document repre-sentation in the target language embeddings is in thesame space with the source language.We build the en-de CLWE using combinedmodels as described in section ?4.
Followingprior work, we also use monolingual data8 fromthe RCV1/RCV2 corpus (Klementiev et al, 2012;Gouws et al, 2015; Chandar A P et al, 2014).Table 6 shows the CLDC results for variousCLWE.
Despite its simplicity, our model achievescompetitive performance.
Note that aside from ourmodel, all other models in Table 6 use a large bi-text (Europarl) which may not exist for many low-resource languages, limiting their applicability.7The data split and code are kindly provided by the authors.8We randomly sample documents in RCV1 and RCV2 cor-pora and selected around 85k documents to form 400k mono-lingual sentences for both en and de.
For each document, weperform basic pre-processing including: lower-casing, removehtml tags and tokenization.
These monolingual data are thenconcatenated with the monolingual data from Wikipedia to formthe final training data.129210 20 40 80 160 320 AllDict Size (k)20406080Scorerec @ 1rec @ 5(a) BLI10 20 40 80 160 320 640 AllDict Size (k)20406080ScoreWS ?
En RW ?
En WS ?
De(b) Mono Similarity10 20 40 80 160 320 640 AllDict Size (k)20406080ScoreEn ?> DeDe ?> En(c) CLDCFigure 3: Learning curve showing how task scores increase with increasing dictionary size; showing bilin-gual lexicon induction (BLI) task (left), monolingual similarity (center) and crosslingual document classifi-cation (right).
BLI is trained on en-it, and monolingual similarity and CLDC are trained on en-de.10 Low-resource languagesOur model exploits dictionaries, which are morewidely available than parallel corpora.
However thequestion remains as to how well this performs of areal low-resource language, rather than a simulatedcondition like above, whereupon the quality of thedictionary is likely to be worse.
To test this, we eval-uation on Serbian, a language with few annotatedlanguage resources.
Table 1 shows the relative sizeof monolingual data and dictionary for en-sr com-pared with other language pairs.
Both the Serbianmonolingual data and the dictionary size is morethan 10 times smaller than other language pairs.
Webuild the en-sr CLWE using our best model (joint+ combine) and evaluate on the bilingual word in-duction task using 939 gold translation pairs.9 Weachieved recall score of 35.8% and 45.5% at 1 and 5respectively.
Although worse than the earlier results,these numbers are still well above chance.We can also simulate low-resource setting usingour earlier datasets.
For estimating the performanceloss on all three tasks, we down sample the dictio-nary for en-it and en-de based on en word fre-quency.
Figure 3 shows the performance with dif-ferent dictionary sizes for all three tasks.
The mono-lingual similarity performance is very similar acrossvarious sizes.
For BLI and CLDC, dictionary size ismore important, although performance levels off ataround 80k dictionary pairs.
We conclude that thissize is sufficient for decent performance.9The sr?en translations are sourced from Google Trans-late by translating one word at a time, followed by manuallyverification, after which 61 translation pairs were ruled out asbeing bad or questionable.11 ConclusionPrevious CLWE methods often impose high re-source requirements yet have low accuracy.
We in-troduce a simple framework based on a large noisydictionary.
We model polysemy using EM transla-tion selection during training to learn bilingual cor-respondences from monolingual corpora.
Our algo-rithm allows to train on massive amount of mono-lingual data efficiently, representing monolingualand bilingual properties of language.
This allowsus to achieve state-of-the-art performance on bilin-gual lexicon induction task, competitive result onmonolingual word similarity and crosslingual doc-ument classification task.
Our combination tech-niques during training, especially using regulariza-tion, are highly effective and could be used to im-prove monolingual word embeddings.AcknowledgmentsThis work was conducted during Duong?s internshipat IBM Research ?
Tokyo and partially supportedby the University of Melbourne and National ICTAustralia (NICTA).
We are grateful for support fromNSF Award 1464553 and the DARPA/I2O, ContractNo.
HR0011-15-C-0114.
We thank Yuta Tsuboiand Alvin Grissom II for helpful discussions, JanS?najder for helping with sr-en evaluation.ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013.Polyglot: Distributed word representations for multi-lingual nlp.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning,pages 183?192, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.1293Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
CoRR, abs/1409.0473.Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In Z. Ghahramani, M. Welling, C. Cortes, N. D.Lawrence, and K. Q. Weinberger, editors, Advancesin Neural Information Processing Systems 27, pages1853?1861.
Curran Associates, Inc.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, pages 600?609.Long Duong, Trevor Cohn, Steven Bird, and Paul Cook.2015.
Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser.In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing (Volume 2: Short Papers), pages 845?850,Beijing, China.
Association for Computational Lin-guistics.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on NaturalLanguage Processing (Volume 1: Long Papers), pages334?343, Beijing, China, July.
Association for Com-putational Linguistics.Manaal Faruqui and Chris Dyer.
2014.
Improving vec-tor space word representations using multilingual cor-relation.
In Proceedings of the 14th Conference ofthe European Chapter of the Association for Computa-tional Linguistics, pages 462?471, Gothenburg, Swe-den, April.
Association for Computational Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The con-cept revisited.
In Proceedings of the 10th Interna-tional Conference on World Wide Web, WWW ?01,pages 406?414, New York, NY, USA.
ACM.Stephan Gouws and Anders S?gaard.
2015.
Simple task-specific bilingual word embeddings.
In Proceedings ofthe 2015 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 1386?1390, Den-ver, Colorado, May?June.
Association for Computa-tional Linguistics.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
Bilbowa: Fast bilingual distributed represen-tations without word alignments.
In David Blei andFrancis Bach, editors, Proceedings of the 32nd Inter-national Conference on Machine Learning (ICML-15),pages 748?756.
JMLR Workshop and Conference Pro-ceedings.Karl Moritz Hermann and Phil Blunsom.
2014.
Mul-tilingual models for compositional distributed seman-tics.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 58?68, Baltimore, Mary-land, June.
Association for Computational Linguistics.David Kamholz, Jonathan Pool, and Susan Colowick.2014.
Panlex: Building a resource for panlingual lex-ical translation.
In Proceedings of the Ninth Interna-tional Conference on Language Resources and Evalu-ation (LREC?14), pages 3145?50, Reykjavik, Iceland.European Language Resources Association (ELRA).Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representa-tions of words.
In Proceedings of COLING 2012,pages 1459?1474, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Toma?s?
Koc?isky?, Karl Moritz Hermann, and Phil Blun-som.
2014.
Learning bilingual word representationsby marginalizing alignments.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages224?229, Baltimore, Maryland, June.
Association forComputational Linguistics.Omer Levy and Yoav Goldberg.
2014.
Neural word em-bedding as a factorization.
In Advances in Neural In-formation Processing Systems 27: Annual Conferenceon Neural Information Processing Systems 2014, De-cember 8-13 2014, Montreal, Quebec, Canada, pages2177?2185.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
Rcv1: A new benchmark collection for textcategorization research.
J. Mach.
Learn.
Res., 5:361?397, December.Thang Luong, Richard Socher, and Christopher D. Man-ning.
2013.
Better word representations with recur-sive neural networks for morphology.
In Proceed-ings of the Seventeenth Conference on ComputationalNatural Language Learning, CoNLL 2013, Sofia, Bul-garia, August 8-9, 2013, pages 104?113.Minh-Thang Luong, Hieu Pham, and Christopher D.Manning.
2015.
Bilingual word representations withmonolingual quality in mind.
In NAACL Workshopon Vector Space Modeling for NLP, Denver, UnitedStates.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a.1294Exploiting similarities among languages for machinetranslation.
CoRR, abs/1309.4168.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 746?751, Atlanta, Georgia.
Asso-ciation for Computational Linguistics.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1532?1543.Philip Resnik and David Yarowsky.
1999.
Distinguish-ing systems and distinguishing senses: New evaluationmethods for word sense disambiguation.
Nat.
Lang.Eng., 5(2):113?133, June.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to german.
In In Proceed-ings of the ACL SIGDAT-Workshop, pages 47?50.Noam Shazeer, Ryan Doherty, Colin Evans, and ChrisWaterson.
2016.
Swivel: Improving embeddings bynoticing what?s missing.
CoRR, abs/1602.02215.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1631?1642, Seattle, Washington, USA, October.
Associationfor Computational Linguistics.Anders S?gaard, Z?eljko Agic?, He?ctor Mart?
?nez Alonso,Barbara Plank, Bernd Bohnet, and Anders Johannsen.2015.
Inverted indexing for cross-lingual nlp.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 1713?1722,Beijing, China, July.
Association for ComputationalLinguistics.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of the 2012 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, NAACL HLT ?12, pages 477?487.
As-sociation for Computational Linguistics.Ivan Vulic?
and Marie-Francine Moens.
2015.
Bilin-gual word embeddings from non-parallel document-aligned data applied to bilingual lexicon induction.
InProceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing (Volume 2: Short Papers), pages 719?725,Beijing, China, July.
Association for ComputationalLinguistics.Min Xiao and Yuhong Guo, 2014.
Proceedings ofthe Eighteenth Conference on Computational NaturalLanguage Learning, chapter Distributed Word Rep-resentation Learning for Cross-Lingual DependencyParsing, pages 119?129.
Association for Computa-tional Linguistics.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofthe Second Meeting of the North American Chapterof the Association for Computational Linguistics onLanguage technologies, NAACL ?01, pages 1?8, Pitts-burgh, Pennsylvania.Wen-tau Yih and Vahed Qazvinian.
2012.
Measur-ing word relatedness using heterogeneous vector spacemodels.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, NAACL HLT ?12, pages 616?620, Stroudsburg,PA, USA.
Association for Computational Linguistics.1295
