Proceedings of the 43rd Annual Meeting of the ACL, pages 173?180,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsCoarse-to-fine n-best parsing and MaxEnt discriminative rerankingEugene Charniak and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{mj|ec}@cs.brown.eduAbstractDiscriminative reranking is one methodfor constructing high-performance statis-tical parsers (Collins, 2000).
A discrim-inative reranker requires a source of can-didate parses for each sentence.
This pa-per describes a simple yet novel methodfor constructing sets of 50-best parsesbased on a coarse-to-fine generative parser(Charniak, 2000).
This method gener-ates 50-best lists that are of substantiallyhigher quality than previously obtainable.We used these parses as the input to aMaxEnt reranker (Johnson et al, 1999;Riezler et al, 2002) that selects the bestparse from the set of parses for each sen-tence, obtaining an f-score of 91.0% onsentences of length 100 or less.1 IntroductionWe describe a reranking parser which uses a reg-ularized MaxEnt reranker to select the best parsefrom the 50-best parses returned by a generativeparsing model.
The 50-best parser is a probabilisticparser that on its own produces high quality parses;the maximum probability parse trees (according tothe parser?s model) have an f -score of 0.897 onsection 23 of the Penn Treebank (Charniak, 2000),which is still state-of-the-art.
However, the 50 best(i.e., the 50 highest probability) parses of a sentenceoften contain considerably better parses (in terms off -score); this paper describes a 50-best parsing al-gorithm with an oracle f -score of 96.8 on the samedata.The reranker attempts to select the best parse fora sentence from the 50-best list of possible parsesfor the sentence.
Because the reranker only hasto consider a relatively small number of parses persentences, it is not necessary to use dynamic pro-gramming, which permits the features to be essen-tially arbitrary functions of the parse trees.
Whileour reranker does not achieve anything like the ora-cle f -score, the parses it selects do have an f -scoreof 91.0, which is considerably better than the maxi-mum probability parses of the n-best parser.In more detail, for each string s the n-best parsingalgorithm described in section 2 returns the n high-est probability parses Y(s) = {y1(s), .
.
.
, yn(s)}together with the probability p(y) of each parse y ac-cording to the parser?s probability model.
The num-ber n of parses was set to 50 for the experimentsdescribed here, but some simple sentences actuallyreceived fewer than 50 parses (so n is actually afunction of s).
Each yield or terminal string in thetraining, development and test data sets is mappedto such an n-best list of parse/probability pairs; thecross-validation scheme described in Collins (2000)was used to avoid training the n-best parser on thesentence it was being used to parse.A feature extractor, described in section 3, is avector of m functions f = (f1, .
.
.
, fm), where eachfj maps a parse y to a real number fj(y), whichis the value of the jth feature on y.
So a featureextractor maps each y to a vector of feature valuesf(y) = (f1(y), .
.
.
, fm(y)).Our reranking parser associates a parse with a173score v?
(y), which is a linear function of the featurevalues f(y).
That is, each feature fj is associatedwith a weight ?j , and the feature values and weightsdefine the score v?
(y) of each parse y as follows:v?
(y) = ?
?
f(y) =m?j=1?jfj(y).Given a string s, the reranking parser?s output y?
(s)on string s is the highest scoring parse in the n-bestparses Y(s) for s, i.e.,y?
(s) = arg maxy?Y(s)v?
(y).The feature weight vector ?
is estimated from thelabelled training corpus as described in section 4.Because we use labelled training data we know thecorrect parse y?
(s) for each sentence s in the trainingdata.
The correct parse y?
(s) is not always a mem-ber of the n-best parser?s output Y(s), but we canidentify the parses Y+(s) in Y(s) with the highestf -scores.
Informally, the estimation procedure findsa weight vector ?
that maximizes the score v?
(y) ofthe parses y ?
Y+(s) relative to the scores of theother parses in Y(s), for each s in the training data.2 Recovering the n-best parses usingcoarse-to-fine parsingThe major difficulty in n-best parsing, compared to1-best parsing, is dynamic programming.
For exam-ple, n-best parsing is straight-forward in best-firstsearch or beam search approaches that do not usedynamic programming: to generate more than oneparse, one simply allows the search mechanism tocreate successive versions to one?s heart?s content.A good example of this is the Roark parser(Roark, 2001) which works left-to right through thesentence, and abjures dynamic programming in fa-vor of a beam search, keeping some large number ofpossibilities to extend by adding the next word, andthen re-pruning.
At the end one has a beam-width?snumber of best parses (Roark, 2001).The Collins parser (Collins, 1997) does use dy-namic programming in its search.
That is, whenevera constituent with the same history is generated asecond time, it is discarded if its probability is lowerthan the original version.
If the opposite is true, thenthe original is discarded.
This is fine if one onlywants the first-best, but obviously it does not directlyenumerate the n-best parses.However, Collins (Collins, 2000; Collinsand Koo, in submission) has created an n-best version of his parser by turning off dy-namic programming (see the user?s guide toBikel?s re-implementation of Collins?
parser,http://www.cis.upenn.edu/ dbikel/software.html#stat-parser).
As with Roark?s parser, it is necessary toadd a beam-width constraint to make the searchtractable.
With a beam width of 1000 the parserreturns something like a 50-best list (Collins,personal communication), but the actual number ofparses returned for each sentences varies.
However,turning off dynamic programming results in a loss inefficiency.
Indeed, Collins?s n-best list of parses forsection 24 of the Penn tree-bank has some sentenceswith only a single parse, because the n-best parsercould not find any parses.Now there are two known ways to produce n-bestparses while retaining the use of dynamic program-ming: the obvious way and the clever way.The clever way is based upon an algorithm devel-oped by Schwartz and Chow (1990).
Recall the keyinsight in the Viterbi algorithm: in the optimal parsethe parsing decisions at each of the choice points thatdetermine a parse must be optimal, since otherwiseone could find a better parse.
This insight extendsto n-best parsing as follows.
Consider the second-best parse: if it is to differ from the best parse, thenat least one of its parsing decisions must be subop-timal.
In fact, all but one of the parsing decisionsin second-best parse must be optimal, and the onesuboptimal decision must be the second-best choiceat that choice point.
Further, the nth-best parse canonly involve at most n suboptimal parsing decisions,and all but one of these must be involved in one ofthe second through the n?1th-best parses.
Thus thebasic idea behind this approach to n-best parsing isto first find the best parse, then find the second-bestparse, then the third-best, and so on.
The algorithmwas originally described for hidden Markov models.Since this first draft of this paper we have be-come aware of two PCFG implementations of thisalgorithm (Jimenez and Marzal, 2000; Huang andChang, 2005).
The first was tried on relatively smallgrammars, while the second was implemented ontop of the Bikel re-implementation of the Collins174parser (Bikel, 2004) and achieved oracle results for50-best parses similar to those we report below.Here, however, we describe how to find n-bestparses in a more straight-forward fashion.
Ratherthan storing a single best parse of each edge, onestores n of them.
That is, when using dynamic pro-gramming, rather than throwing away a candidate ifit scores less than the best, one keeps it if it is oneof the top n analyses for this edge discovered so far.This is really very straight-forward.
The problemis space.
Dynamic programming parsing algorithmsfor PCFGs require O(m2) dynamic programmingstates, where m is the length of the sentence, so ann-best parsing algorithm requires O(nm2).
How-ever things get much worse when the grammar is bi-lexicalized.
As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi-lexicalized PCFGs require O(m3) states, so a n-bestparser would require O(nm3) states.
Things be-come worse still in a parser like the one described inCharniak (2000) because it conditions on (and hencesplits the dynamic programming states according to)features of the grandparent node in addition to theparent, thus multiplying the number of possible dy-namic programming states even more.
Thus nobodyhas implemented this version.There is, however, one particular feature of theCharniak parser that mitigates the space problem: itis a ?coarse-to-fine?
parser.
By ?coarse-to-fine?
wemean that it first produces a crude version of theparse using coarse-grained dynamic programmingstates, and then builds fine-grained analyses by split-ting the most promising of coarse-grained states.A prime example of this idea is from Goodman(1997), who describes a method for producing a sim-ple but crude approximate grammar of a standardcontext-free grammar.
He parses a sentence usingthe approximate grammar, and the results are usedto constrain the search for a parse with the full CFG.He finds that total parsing time is greatly reduced.A somewhat different take on this paradigm isseen in the parser we use in this paper.
Here theparser first creates a parse forest based upon a muchless complex version of the complete grammar.
Inparticular, it only looks at standard CFG features,the parent and neighbor labels.
Because this gram-mar encodes relatively little state information, its dy-namic programming states are relatively coarse andhence there are comparatively few of them, so it canbe efficiently parsed using a standard dynamic pro-gramming bottom-up CFG parser.
However, pre-cisely because this first stage uses a grammar thatignores many important contextual features, the bestparse it finds will not, in general, be the best parseaccording to the finer-grained second-stage gram-mar, so clearly we do not want to perform best-firstparsing with this grammar.
Instead, the output ofthe first stage is a polynomial-sized packed parseforest which records the left and right string posi-tions for each local tree in the parses generated bythis grammar.
The edges in the packed parse for-est are then pruned, to focus attention on the coarse-grained states that are likely to correspond to high-probability fine-grained states.
The edges are thenpruned according to their marginal probability con-ditioned on the string s being parsed as follows:p(nij,k | s) =?(nij,k)?
(nij,k)p(s) (1)Here nij,k is a constituent of type i spanning thewords from j to k, ?
(nij,k) is the outside probabilityof this constituent, and ?
(nij,k) is its inside proba-bility.
From parse forest both ?
and ?
can be com-puted in time proportional to the size of the compactforest.
The parser then removes all constituents nij,kwhose probability falls below some preset threshold.In the version of this parser available on the web, thisthreshold is on the order of 10?4.The unpruned edges are then exhaustively eval-uated according to the fine-grained probabilisticmodel; in effect, each coarse-grained dynamic pro-gramming state is split into one or more fine-graineddynamic programming states.
As noted above, thefine-grained model conditions on information that isnot available in the coarse-grained model.
This in-cludes the lexical head of one?s parents, the part ofspeech of this head, the parent?s and grandparent?scategory labels, etc.
The fine-grained states inves-tigated by the parser are constrained to be refine-ments of the coarse-grained states, which drasticallyreduces the number of fine-grained states that needto be investigated.It is certainly possible to do dynamic program-ming parsing directly with the fine-grained gram-mar, but precisely because the fine-grained grammar175conditions on a wide variety of non-local contex-tual information there would be a very large numberof different dynamic programming states, so directdynamic programming parsing with the fine-grainedgrammar would be very expensive in terms of timeand memory.As the second stage parse evaluates all the re-maining constituents in all of the contexts in whichthey appear (e.g., what are the possible grand-parentlabels) it keeps track of the most probable expansionof the constituent in that context, and at the end isable to start at the root and piece together the overallbest parse.Now comes the easy part.
To create a 50-bestparser we simply change the fine-grained version of1-best algorithm in accordance with the ?obvious?scheme outlined earlier in this section.
The first,coarse-grained, pass is not changed, but the second,fine-grained, pass keeps the n-best possibilities ateach dynamic programming state, rather than keep-ing just first best.
When combining two constituentsto form a larger constituent, we keep the best 50 ofthe 2500 possibilities they offer.
Naturally, if wekeep each 50-best list sorted, we do nothing like2500 operations.The experimental question is whether, in practice,the coarse-to-fine architecture keeps the number ofdynamic programming states sufficiently low thatspace considerations do not defeat us.The answer seems to be yes.
We ran the algorithmon section 24 of the Penn WSJ tree-bank using thedefault pruning settings mentioned above.
Table 1shows how the number of fine-grained dynamic pro-gramming states increases as a function of sentencelength for the sentences in section 24 of the Tree-bank.
There are no sentences of length greater than69 in this section.
Columns two to four show thenumber of sentences in each bucket, their averagelength, and the average number of fine-grained dy-namic programming structures per sentence.
The fi-nal column gives the value of the function 100?L1.5where L is the average length of sentences in thebucket.
Except for bucket 6, which is abnormallylow, it seems that this add-hoc function tracks thenumber of structures quite well.
Thus the number ofdynamic programming states does not grow as L2,much less as L3.To put the number of these structures per sen-Len Num Av sen Av strs 100 ?
L1.5sents length per sent0?9 225 6.04 1167 148410?19 725 15.0 4246 580820?29 795 24.2 9357 1197430?39 465 33.8 15893 1965440?49 162 43.2 21015 2844050?59 35 52.8 30670 3836660?69 9 62.8 23405 49740Table 1: Number of structures created as a functionof sentence lengthn 1 2 10 25 50f -score 0.897 0.914 0.948 0.960 0.968Table 2: Oracle f -score as a function of number nof n-best parsestence in perspective, consider the size of such struc-tures.
Each one must contain a probability, the non-terminal label of the structure, and a vector of point-ers to it?s children (an average parent has slightlymore than two children).
If one were concernedabout every byte this could be made quite small.
Inour implementation probably the biggest factor isthe STL overhead on vectors.
If we figure we areusing, say, 25 bytes per structure, the total space re-quired is only 1.25Mb even for 50,000 dynamic pro-gramming states, so it is clearly not worth worryingabout the memory required.The resulting n-bests are quite good, as shown inTable 2.
(The results are for all sentences of sec-tion 23 of the WSJ tree-bank of length ?
100.)
Fromthe 1-best result we see that the base accuracy of theparser is 89.7%.1 2-best and 10-best show dramaticoracle-rate improvements.
After that things start toslow down, and we achieve an oracle rate of 0.968at 50-best.
To put this in perspective, Roark (Roark,2001) reports oracle results of 0.941 (with the sameexperimental setup) using his parser to return a vari-able number of parses.
For the case cited his parserreturns, on average, 70 parses per sentence.Finally, we note that 50-best parsing is only a fac-1Charniak in (Charniak, 2000) cites an accuracy of 89.5%.Fixing a few very small bugs discovered by users of the parseraccounts for the difference.176tor of two or three slower than 1-best.3 Features for reranking parsesThis section describes how each parse y is mappedto a feature vector f(y) = (f1(y), .
.
.
, fm(y)).
Eachfeature fj is a function that maps a parse to a realnumber.
The first feature f1(y) = log p(y) is thelogarithm of the parse probability p according tothe n-best parser model.
The other features areinteger valued; informally, each feature is associ-ated with a particular configuration, and the feature?svalue fj(y) is the number of times that the config-uration that fj indicates.
For example, the featurefeat pizza(y) counts the number of times that a phrasein y headed by eat has a complement phrase headedby pizza.Features belong to feature schema, which are ab-stract schema from which specific features are in-stantiated.
For example, the feature feat pizza is aninstance of the ?Heads?
schema.
Feature schema areoften parameterized in various ways.
For example,the ?Heads?
schema is parameterized by the type ofheads that the feature schema identifies.
FollowingGrimshaw (1997), we associate each phrase with alexical head and a function head.
For example, thelexical head of an NP is a noun while the functionalhead of an NP is a determiner, and the lexical headof a VP is a main verb while the functional head ofVP is an auxiliary verb.We experimented with various kinds of featureselection, and found that a simple count thresholdperforms as well as any of the methods we tried.Specifically, we ignored all features that did not varyon the parses of at least t sentences, where t is thecount threshold.
In the experiments described belowt = 5, though we also experimented with t = 2.The rest of this section outlines the featureschemata used in the experiments below.
These fea-ture schemata used here were developed using then-best parses provided to us by Michael Collinsapproximately a year before the n-best parser de-scribed here was developed.
We used the divisioninto preliminary training and preliminary develop-ment data sets described in Collins (2000) whileexperimenting with feature schemata; i.e., the first36,000 sentences of sections 2?20 were used as pre-liminary training data, and the remaining sentencesof sections 20 and 21 were used as preliminary de-velopment data.
It is worth noting that develop-ing feature schemata is much more of an art thana science, as adding or deleting a single schemausually does not have a significant effect on perfor-mance, yet the overall impact of many well-chosenschemata can be dramatic.Using the 50-best parser output described here,there are 1,148,697 features that meet the countthreshold of at least 5 on the main training data(i.e., Penn treebank sections 2?21).
We list eachfeature schema?s name, followed by the number offeatures in that schema with a count of at least 5, to-gether with a brief description of the instances of theschema and the schema?s parameters.CoPar (10) The instances of this schema indicateconjunct parallelism at various different depths.For example, conjuncts which have the samelabel are parallel at depth 0, conjuncts with thesame label and whose children have the samelabel are parallel at depth 1, etc.CoLenPar (22) The instances of this schema indi-cate the binned difference in length (in termsof number of preterminals dominated) in adja-cent conjuncts in the same coordinated struc-tures, conjoined with a boolean flag that indi-cates whether the pair is final in the coordinatedphrase.RightBranch (2) This schema enables the rerankerto prefer right-branching trees.
One instance ofthis schema returns the number of nonterminalnodes that lie on the path from the root nodeto the right-most non-punctuation preterminalnode, and the other instance of this schemacounts the number of the other nonterminalnodes in the parse tree.Heavy (1049) This schema classifies nodes by theircategory, their binned length (i.e., the numberof preterminals they dominate), whether theyare at the end of the sentence and whether theyare followed by punctuation.Neighbours (38,245) This schema classifies nodesby their category, their binned length, and thepart of speech categories of the `1 preterminalsto the node?s left and the `2 preterminals to the177node?s right.
`1 and `2 are parameters of thisschema; here `1 = 1 or `1 = 2 and `2 = 1.Rule (271,655) The instances of this schema arelocal trees, annotated with varying amountsof contextual information controlled by theschema?s parameters.
This schema was in-spired by a similar schema in Collins and Koo(in submission).
The parameters to this schemacontrol whether nodes are annotated with theirpreterminal heads, their terminal heads andtheir ancestors?
categories.
An additional pa-rameter controls whether the feature is special-ized to embedded or non-embedded clauses,which roughly corresponds to Emonds?
?non-root?
and ?root?
contexts (Emonds, 1976).NGram (54,567) The instances of this schema are`-tuples of adjacent children nodes of the sameparent.
This schema was inspired by a simi-lar schema in Collins and Koo (in submission).This schema has the same parameters as theRule schema, plus the length ` of the tuples ofchildren (` = 2 here).Heads (208,599) The instances of this schema aretuples of head-to-head dependencies, as men-tioned above.
The category of the node thatis the least common ancestor of the head andthe dependent is included in the instance (thisprovides a crude distinction between differentclasses of arguments).
The parameters of thisschema are whether the heads involved are lex-ical or functional heads, the number of headsin an instance, and whether the lexical item orjust the head?s part of speech are included in theinstance.LexFunHeads (2,299) The instances of this featureare the pairs of parts of speech of the lexicalhead and the functional head of nodes in parsetrees.WProj (158,771) The instances of this schema arepreterminals together with the categories of ` oftheir closest maximal projection ancestors.
Theparameters of this schema control the number `of maximal projections, and whether the preter-minals and the ancestors are lexicalized.Word (49,097) The instances of this schema arelexical items together with the categories of `of their immediate ancestor nodes, where ` isa schema parameter (` = 2 or ` = 3 here).This feature was inspired by a similar featurein Klein and Manning (2003).HeadTree (72,171) The instances of this schemaare tree fragments consisting of the local treesconsisting of the projections of a preterminalnode and the siblings of such projections.
Thisschema is parameterized by the head type (lex-ical or functional) used to determine the pro-jections of a preterminal, and whether the headpreterminal is lexicalized.NGramTree (291,909) The instances of thisschema are subtrees rooted in the least com-mon ancestor of ` contiguous preterminalnodes.
This schema is parameterized by thenumber ` of contiguous preterminals (` = 2 or` = 3 here) and whether these preterminals arelexicalized.4 Estimating feature weightsThis section explains how we estimate the featureweights ?
= (?1, .
.
.
, ?m) for the feature functionsf = (f1, .
.
.
, fm).
We use a MaxEnt estimator tofind the feature weights ?
?, where L is the loss func-tion and R is a regularization penalty term:??
= argmin?LD(?)
+ R(?
).The training data D = (s1, .
.
.
, sn?)
is a se-quence of sentences and their correct parsesy?
(s1), .
.
.
, y?(sn).
We used the 20-fold cross-validation technique described in Collins (2000)to compute the n-best parses Y(s) for each sen-tence s in D. In general the correct parse y?
(s)is not a member of Y(s), so instead we train thereranker to identify one of the best parses Y+(s) =argmaxy?Y(s) Fy?
(s)(y) in the n-best parser?s out-put, where Fy?
(y) is the Parseval f -score of y eval-uated with respect to y?.Because there may not be a unique best parse foreach sentence (i.e., |Y+(s)| > 1 for some sentencess) we used the variant of MaxEnt described in Rie-zler et al (2002) for partially labelled training data.178Recall the standard MaxEnt conditional probabilitymodel for a parse y ?
Y:P?
(y|Y) =exp v?(y)?y?
?Y exp v?(y?),wherev?
(y) = ?
?
f(y) =m?j=1?jfj(y).The loss function LD proposed in Riezler et al(2002) is just the negative log conditional likelihoodof the best parses Y+(s) relative to the n-best parseroutput Y(s):LD(?)
= ?n?
?i=1log P?(Y+(si)|Y(si)),whereP?
(Y+|Y) =?y?Y+P?
(y|Y)The partial derivatives of this loss function, whichare required by the numerical estimation procedure,are:?LD?j=n??i=1E?
[fj |Y(si)] ?
E?
[fj |Y+(si)]E?
[f |Y] =?y?Yf(y)P?
(y|Y)In the experiments reported here, we used a Gaus-sian or quadratic regularizer R(w) = c?mj=1 w2j ,where c is an adjustable parameter that controlsthe amount of regularization, chosen to optimizethe reranker?s f -score on the development set (sec-tion 24 of the treebank).We used the Limited Memory Variable Metric op-timization algorithm from the PETSc/TAO optimiza-tion toolkit (Benson et al, 2004) to find the optimalfeature weights ??
because this method seems sub-stantially faster than comparable methods (Malouf,2002).
The PETSc/TAO toolkit provides a variety ofother optimization algorithms and flags for control-ling convergence, but preliminary experiments onthe Collins?
trees with different algorithms and earlystopping did not show any performance improve-ments, so we used the default PETSc/TAO setting forour experiments here.5 Experimental resultsWe evaluated the performance of our rerankingparser using the standard PARSEVAL metrics.
Wen-best trees f -scoreNew 0.9102Collins 0.9037Table 3: Results on new n-best trees and Collins n-best trees, with weights estimated from sections 2?21 and the regularizer constant c adjusted for op-timal f -score on section 24 and evaluated on sen-tences of length less than 100 in section 23.trained the n-best parser on sections 2?21 of thePenn Treebank, and used section 24 as developmentdata to tune the mixing parameters of the smooth-ing model.
Similarly, we trained the feature weights?
with the MaxEnt reranker on sections 2?21, andadjusted the regularizer constant c to maximize thef -score on section 24 of the treebank.
We did thisboth on the trees supplied to us by Michael Collins,and on the output of the n-best parser described inthis paper.
The results are presented in Table 3.
Then-best parser?s most probable parses are already ofstate-of-the-art quality, but the reranker further im-proves the f -score.6 ConclusionThis paper has described a dynamic programmingn-best parsing algorithm that utilizes a heuristiccoarse-to-fine refinement of parses.
Because thecoarse-to-fine approach prunes the set of possibleparse edges beforehand, a simple approach whichenumerates the n-best analyses of each parse edgeis not only practical but quite efficient.We use the 50-best parses produced by this algo-rithm as input to a MaxEnt discriminative reranker.The reranker selects the best parse from this set ofparses using a wide variety of features.
The sys-tem we described here has an f -score of 0.91 whentrained and tested using the standard PARSEVALframework.This result is only slightly higher than the highestreported result for this test-set, Bod?s (.907) (Bod,2003).
More to the point, however, is that the sys-tem we describe is reasonably efficient so it canbe used for the kind of routine parsing currentlybeing handled by the Charniak or Collins parsers.A 91.0 f-score represents a 13% reduction in f-179measure error over the best of these parsers.2 Boththe 50-best parser, and the reranking parser can befound at ftp://ftp.cs.brown.edu/pub/nlparser/, namedparser and reranker respectively.Acknowledgements We would like to thanksMichael Collins for the use of his data and manyhelpful comments, and Liang Huang for providingan early draft of his paper and very useful commentson our paper.
Finally thanks to the National ScienceFoundation for its support (NSF IIS-0112432, NSF9721276, and NSF DMS-0074276).ReferencesSteve Benson, Lois Curfman McInnes, Jorge J. Mor, andJason Sarich.
2004.
Tao users manual.
Technical Re-port ANL/MCS-TM-242-Revision 1.6, Argonne Na-tional Laboratory.Daniel M. Bikel.
2004.
Intricacies of collins parsingmodel.
Computational Linguistics, 30(4).Rens Bod.
2003.
An efficient implementation of an newdop model.
In Proceedings of the European Chapterof the Association for Computational Linguists.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In The Proceedings of the North AmericanChapter of the Association for Computational Linguis-tics, pages 132?139.Michael Collins and Terry Koo.
in submission.
Discrim-inative reranking for natural language parsing.
Tech-nical report, Computer Science and Artificial Intelli-gence Laboratory, Massachusetts Institute of Technol-ogy.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In The Proceedings ofthe 35th Annual Meeting of the Association for Com-putational Linguistics, San Francisco.
Morgan Kauf-mann.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Machine Learning: Pro-ceedings of the Seventeenth International Conference(ICML 2000), pages 175?182, Stanford, California.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In Proceedings of the 37th Annual2This probably underestimates the actual improvement.There are no currently accepted figures for inter-annotateragreement on Penn WSJ, but it is no doubt well short of 100%.If we take 97% as a reasonable estimate of the the upper boundon tree-bank accuracy, we are instead talking about an 18% er-ror reduction.Meeting of the Association for Computational Linguis-tics, pages 457?464.Joseph Emonds.
1976.
A Transformational Approach toEnglish Syntax: Root, Structure-Preserving and LocalTransformations.
Academic Press, New York, NY.Joshua Goodman.
1997.
Global thresholding andmultiple-pass parsing.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 1997).Jane Grimshaw.
1997.
Projection, heads, and optimality.Linguistic Inquiry, 28(3):373?422.Liang Huang and David Chang.
2005.
Better k-best pars-ing.
Technical Report MS-CIS-05-08, Department ofComputer Science, University of Pennsylvania.Victor M. Jimenez and Andres Marzal.
2000.
Computa-tion of the n best parse trees for weighted and stochas-tic context-free grammars.
In Proceedings of the JointIAPR International Workshops on Advances in PatternRecognition.
Springer LNCS 1876.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochas-tic ?unification-based?
grammars.
In The Proceedingsof the 37th Annual Conference of the Association forComputational Linguistics, pages 535?541, San Fran-cisco.
Morgan Kaufmann.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proceed-ings of the Sixth Conference on Natural LanguageLearning (CoNLL-2002), pages 49?55.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. III Maxwell, and Mark John-son.
2002.
Parsing the wall street journal using alexical-functional grammar and discriminative estima-tion techniques.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics, pages 271?278.
Morgan Kaufmann.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.R.
Schwartz and Y.L.
Chow.
1990.
The n-best algo-rithm: An efficient and exact procedure for findingthe n most likely sentence hypotheses.
In Proceed-ings of the IEEE International Conference on Acous-tic, Speech, and Signal, Processing, pages 81?84.180
