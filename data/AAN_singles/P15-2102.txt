Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 616?622,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Hierarchical Knowledge Representation for Expert Findingon Social MediaYanran Li1, Wenjie Li1, and Sujian Li21Computing Department, Hong Kong Polytechnic University, Hong Kong2Key Laboratory of Computational Linguistics, Peking University, MOE, China{csyli, cswjli}@comp.polyu.edu.hklisujian@pku.edu.cnAbstractExpert finding on social media benefitsboth individuals and commercial services.In this paper, we exploit a 5-level tree rep-resentation to model the posts on socialmedia and cast the expert finding prob-lem to the matching problem between thelearned user tree and domain tree.
Weenhance the traditional approximate treematching algorithm and incorporate wordembeddings to improve the matching re-sult.
The experiments conducted on SinaMicroblog demonstrate the effectivenessof our work.1 IntroductionExpert finding has been arousing great interestsamong social media researchers after its success-ful applications on traditional media like academicpublications.
As already observed, social mediausers tend to follow others for professional inter-ests and knowledge (Ramage et al, 2010).
Thisbuilds the basis for mining expertise and find-ing experts on social media, which facilitates theservices of user recommendation and question-answering, etc.Despite the demand to access expertise, thechallenges of identifying domain experts on socialmedia exist.
Social media often contains plenty ofnoises such as the tags with which users describethemselves.
Noises impose the inherent drawbackon the feature-based learning methods (Krishna-murthy et al, 2008).
Data imbalance and sparse-ness also limits the performance of the promis-ing latent semantic analysis methods such as theLDA-like topic models (Blei et al, 2003; Ram-age et al, 2009).
When some topics co-occurmore frequently than others, the strict assump-tion of these topic models cannot be met and con-sequently many nonsensical topics will be gen-erated (Zhao and Jiang, 2011; Pal et al, 2011;Quercia et al, 2012).
Furthermore, not as simpleas celebrities, the definition of experts introducesadditional difficulties.
Experts cannot be simplyjudged by the number of followers.
The knowl-edge conveyed in what they say is essential.
Thisleads to the failures of the network-based meth-ods (Java et al, 2007; Weng et al, 2010; Pal et al,2011).The challenges mentioned above inherentlycome from insufficient representations.
They mo-tivate us to propose a more flexible domain expertfinding framework to explore effective representa-tions that are able to tackle the complexity lies inthe social media data.
The basic idea is as follows.Experts talk about the professional knowledge intheir posts and these posts are supposed to containmore domain knowledge than the posts from theother ordinary users.
We determine whether or notusers are experts on specific domains by matchingtheir professional knowledge and domain knowl-edge.
The key is how to capture such informationfor both users and domains with the appropriaterepresentation, which is, in our view, the reasonwhy most of previous work fails.To go beyond the feature-based classificationmethods and the vector representation inference inexpert finding, a potential solution is to incorpo-rate the semantic information for knowledge mod-eling.
We achieve this goal by representing userposts using a hierarchical tree structure to capturecorrelations among words and topics.
To tacklethe data sparseness problem, we apply word em-beddings to tree-nodes to further enhance seman-tic representation and to support semantic match-ing.
Expert finding is then cast to the problem ofdetermining the edit distance between the user treeand the domain tree, which is computed with anapproximate tree matching algorithm.The main contribution of this work is to inte-grate the hierarchical tree representation and struc-ture matching together to profile users?
and do-616mains?
knowledge.
Using such trees allows us toflexibly incorporate more information into the datarepresentation, such as the relations between la-tent topics and the semantic similarities betweenwords.
The experiments conducted on Sina Mi-croblog demonstrate the effectiveness of the pro-posed framework and the corresponding methods.2 Knowledge Representationwith Hierarchical TreeTo capture correlations between topics, PachinkoAllocation Model (PAM) (Li and McCallum,2006) uses a directed acyclic graph (DAG) withleaves representing individual words in the vocab-ulary and each interior node representing a corre-lation among its children.
In particular, multi-levelPAM is capable of revealing interconnection be-tween sub-level nodes by inferencing correspond-ing super-level nodes.
It is a desired property thatenables us to capture hierarchical relations amongboth inner-level and inter-level nodes and therebyenhance the representation of users?
posts.
Moreimportant, the inter-level hierarchy benefits to dis-tribute words from super-level generic topics tosub-level specific topics.In this work, we exploit a 5-level PAM to learnthe hierarchical knowledge representation for eachindividual user and domain.
As shown in Figure 1,the 5-level hierarchy consists of one root topic r, Itopics at the second level X = {x1, x2, .
.
.
, xI},J topics at the third level Y = {y1, y2, .
.
.
, yJ},K topics at the fourth level Z = {z1, z2, .
.
.
, zK}and words at the bottom.
The whole hierarchy isfully connected..?
?
??
?
?.
.
.. .
.wordz-topicy-topicx-topicrootFigure 1: 5-level PAMEach topic in 5-level PAM is associated witha distribution g(?)
over its children.
In general,g(?)
can be any distribution over discrete vari-ables.
Here, we use a set of Dirichlet com-pound multinomial distributions associated withthe root, the second-level and the third-level top-ics.
These distributions are {gr(?
)}, {gi(?i)}Ii=1and {gi(?j)}Jj=1.
They are used to sample themultinomial distributions ?x, ?yand ?zover thecorresponding sub-level topics.
As to the fourth-level topics, we use a fixed multinomial distribu-tion {?zk}Kk=1sampled once for the whole datafrom a single Dirichlet distribution g(?).
Figure 2illustrates the plate notation of this 5-level PAM..wzyx?z??y??x??
?NI|V |JKFigure 2: Plate Notation of 5-level PAMBy integrating out the sampled multinomial dis-tributions ?x, ?y, ?z, ?
and summing over x,y, z,we obtain the Gibbs sampling distribution forword w = wmin document d as:P (xw=xi, yw=yj, zw=zk|D,x?w,y?w, z?w, ?, ?, ?, ?
)?P (w, xw, yw, zw|D?w,x?w,y?w, z?w, ?, ?, ?, ?
)=P (D,x,y, z|?, ?, ?, ?
)P (D?w,x?w,y?w, z?w|?, ?, ?, ?
)=n(d)i+ ?in(d)r+?Ki?=1?i?
?n(d)ij+ ?ijn(d)i+?Lj?=1?ij?
?n(d)jk+ ?jkn(d)j+?Jk?=1?jk?
?n(d)km+ ?mnk+?nm?=1?m?where n(d)ris the number of occurrences of theroot r in document d, which is equivalent to thenumber of tokens in the document.
n(d)i, n(d)ijandn(d)jkare respectively the number of occurrences ofxi, yjand zksampled from their upper-level top-ics.
nkis the number of occurrences of the fourth-level topics zkin the whole dataset and nkmis thenumber of occurrences of word wmin zk.
?w617indicates all observations or topic assignments ex-cept word w.With the fixed Dirichlet parameter ?
for the rootand ?
as the prior, what?s left is to estimate (learnfrom data) ?
and ?
to capture the different corre-lations among topics.
To avoid the use of iterativemethods which are often computationally exten-sive, instead we approximate these two Dirichletparameters using the moment matching algorithm,the same as (Minka, 2000; Casella and Berger,2001; Shafiei and Milios, 2006).
With smoothingtechniques, in each iteration of Gibbs sampling weupdate:meanij=1Ni+ 1?
(?dn(d)ijn(d)i+1L)varij=1Ni+ 1?
(?d(n(d)ijn(d)i?meanij)2+ (1L?meanij)2)mij=meanij?
(1?meanij)varij?
1?ij=meanijexp(?jlog(mij)L?1)where Niis the number of documents with non-zero counts of super-level topic xi.
Parameter es-timation of ?
is the same as ?.3 Expert Findingwith Approximate Tree MatchingOnce the hierarchical representations of users anddomains have been generated, we can determinewhether or not a user is an expert on a domainbased on their matching degree, which is a prob-lem analogous to tree-to-tree correction using editdistance (Selkow, 1977; Shasha and Zhang, 1990;Wagner, 1975; Wagner and Fischer, 1974; Zhangand Shasha, 1989).
Given two trees T1and T2,a typical edit distance-based correction approachis to transform T1to T2with a sequence of edit-ing operations S =< s1, s2, .
.
.
, sk> such thatsk(sk?1(.
.
.
(s1(T1)) .
.
.))
= T2.
Each operationis assigned a cost ?
(si) that represents the diffi-culty of making that operation.
By summing upthe costs of all necessary operations, the total cost?
(S) =?ki=1?
(si) defines the matching degreeof T1and T2.We assume that an expert could only master apart of professional domain knowledge rather thanthe whole and thereby revise a traditional approxi-mate tree matching algorithm (Zhang and Shasha,1989) to calculate the matching degree.
This as-sumption especially makes sense when the domainwe are concerned with is quite general.
Let TdandTudenote the learned domain knowledge tree andthe user knowledge tree, we match Tdto the re-maining trees resulting from cutting all possiblesets of disjoint sub-trees of Tu.
We specificallypenalize no cost if some sub-trees are missing inmatching process.
We define two types of oper-ations.
The substitution operations edit the dis-similar words on tree-nodes, while the insertionand deletion operations perform on tree-structures.Expert finding is then to calculate the minimummatching cost on Tdand Tu.
If the cost is smallerthan an empirically defined threshold ?d, we iden-tify user u as an expert on domain d.To alleviate the sparseness problem caused bydirect letter-to-letter matching in tree-node map-ping, we embed word embeddings (Bengio et al,2003) into the substitution operation.
We applythe word2vec skip-gram model (Mikolov et al,2013(a); Mikolov et al, 2013(b)) to encode eachword in our vocabulary with a probability vec-tor and directly use the similarity generated byword2vec as the tree-node similarity.
The costsof insertion and deletion operations will be ex-plained in Section 4.
Actually all these three costscan be defined in accordance with applicant needs.In brief, by combining both hierarchical represen-tation of tree-structure and word embeddings oftree-nodes, we achieve our goal to enhance seman-tics.4 ExperimentsThe experiments are conducted on 5 domains (i.e.,Beauty Blogger, Beauty Doctor, Parenting, E-Commerce, and Data Science) in Sina Microblog,a Twitter-like microblog in China.
To learn PAM,we manually select 40 users in each domainfrom the official expert lists released by Sina Mi-croblog1, and crawl all of their posts.
In average,there are 113,924 posts in each domain.
Noticethat the expert lists are not of high quality.
Wehave to do manual verification to filter out noises.For evaluation, we select another 80 users in eachdomain from the expert list, with 40 verified as ex-perts and the other 40 as non-experts.Since there is no state-of-art Chinese word em-beddings publicly available, we use another Sina1http://d.weibo.com/1087030002_558_3_2014#618Table 1: Classification ResultsApproachPrecision Recall F-ScoreMacro Micro Macro Micro Macro Microunigram 0.380 0.484 0.615 0.380 0.469 0.432bigram 0.435 0.537 0.615 0.435 0.507 0.486LDA 0.430 0.473 0.540 0.430 0.474 0.451Twitter-LDA 0.675 0.763 0.680 0.430 0.675 0.451PAM 0.720 0.818 0.720 0.720 0.714 0.769Microblog dataset provided by pennyliang2,which contains 25 million posts and nearly 100million tokens in total, to learn the word embed-dings of 50-dimension.
We pre-process the datawith the Rwordseg segmentation package3anddiscard nonsensical words with the pullwordpackage4.When learning 5-level PAM, we set fixed pa-rameters ?
= 0.25, ?
= 0.25 and from top to down,I = 10, J = 20, K = 20 for the number of second,third and fourth levels of topics, respectively.
Andwe initialize ?
and ?
with 0.25.
For tree match-ing, we define the cost of tree-node substitutionoperation between word a and b as Eq (1).
Thecosts of insertion and deletion operations for tree-structure matching are MAX VALUE.
Here we setMAX VALUE as 100 experimentally.
The thresh-old ?dused to determine the expert is set to be 12times of MAX VALUE.?(a?b)=????
?0, a = bsim (a, b) , sim(a, b)>0.55MAX VALUE, otherwise(1)We compare PAM with n-gram (unigram andbigram), LDA (Blei et al, 2003) and Twitter-LDA (Zhao and Jiang, 2011).
We set ?
in LDAand Twitter-LDA to 0.01, ?
in Twiitter-LDA to 20.For ?, we adopt the commonly used 50/T heuris-tics where the number of topics T = 50.
To be fair,we all use the tokens after pullword preprocessingas the input to extract features for classification.Following Zhao and Jiang (2011), we train four?2-regularized logistic regression classifiers usingthe LIBLINEAR package (Fan et al, 2008) on thetop 200 unigrams and bigrams ranked according toChi-squared and 100-dimensional topic vectors in-duced by LDA and Twitter-LDA, respectively.
We2http://chuansong.me/account/pennyjob3http://jliblog.com/app/rwordseg4http://pullword.com/also compare our model with/without word em-beddings to demonstrate the effectiveness of thissemantic enhancement.
The results are presentedin Table 1.In general, LDA, Twitter-LDA and PAMoutperform unigram and bigram, showing thestrength of latent semantic modeling.
Within thefirst two models, Twitter-LDA yields better preci-sions than LDA because of its ability to overcomethe difficulty of modeling short posts on social me-dia.
It designs an additional background word dis-tribution to remove the noisy words and assumesthat a single post can belong to several topics.Our 5-level PAM gains observed improvementover Twitter-LDA.
We attribute this to the ad-vantages of tree representations over vector fea-ture representations, the effective approximate treematching algorithm and the complementary wordembeddings.
As mentioned in Section 1, LDAand other topic models like Twitter-LDA share thesame assumption that each topic should be inde-pendent with each other.
This assumption howeveris too strict for the real world data.
Our tree-like 5-level PAM relaxes such assumption with two addi-tional layers of super-topics modeled with Dirich-let compound multinomial distributions, which isthe key to capture topic correlations.
Furthermore,by allowing partial matching and incorporatingword embeddings, we successfully overcome thesparseness problem.While macro-averages give equal weight toeach domain, micro-averages give equal weightto each user.
The significant difference betweenthe macro- and micro- scores in Table 1 is causedby the different nature of 5 domains.
In fact, theposts of experts on the domain E-Commerce areto some extent noisy and contain lots of wordsirrelevant to the domain knowledge.
Meanwhile,the posts of experts on the domain Data Scienceare less distinguishable.
The higher micro-recallsof PAM demonstrate its generalization ability over619LDA and Twitter-LDA.5 ConclusionIn this paper, we formulate the expert finding taskas a tree matching problem with the hierarchicalknowledge representation.
The experimental re-sults demonstrate the advantage of using 5-levelPAM and semantic enhancement against n-grammodels and LDA-like models.
To further improvethe work, we will incorporate more information toenrich the hierarchical representation in the future.AcknowledgementsThe work described in this paper was supportedby the grants from the Research Grants Coun-cil of Hong Kong (PolyU 5202/12E and PolyU152094/14E) and the grants from the NationalNatural Science Foundation of China (61272291and 61273278).ReferencesEugene Agichtein, Carlos Castillo, Debora Donato, etal.
2008.
Finding high-quality content in social me-dia.
In Proc.
of WSDM.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proc.
of ACL.Yoshua Bengio, Rjean Ducharme, Pascal Vincent, et al.2003.
A neural probabilistic language model.
TheJournal of Machine Learning Research, 3: 1137-1155.Marc Bernard, Laurent Boyer, et al.
2008.
Learningprobabilistic models of tree edit distance.
PatternRecognition, 41(8): 2611-2629.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
The Journal ofMachine Learning Research, 3: 993-1022.Mohamed Bouguessa, Benot Dumoulin, and Shen-grui Wang.
2008.
Identifying authoritative actors inquestion-answering forums: the case of yahoo!
an-swers.
In Proc.
of SIGKDD.George Casella and Roger L. Berger.
2001.
StatisticalInference.
Duxbury Press.Danqi Chen and Christopher D. Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proc.
of EMNLP, pages 740750.Fei Cheng, Kevin Duh, Yuji Matsumoto.
2014.
Pars-ing Chinese Synthetic Words with a Character-basedDependency Model.
LREC.Allan M. Collins and M. Ross.
Quillian.
1969.
Re-trieval time from semantic memory.
Journal of Ver-bal Learning and Verbal Behaviour, 8: 240247.Ronan Collobert, Jason Weston, Leon Bottou, et al.2011.
Natural language processing (almost) fromscratch.
JMLR, 12.Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viacca.
In Advances in Neural Information ProcessingSystems, pages 199207.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, et al.2008.
LIBLINEAR: A library for large linear clas-sification.
The Journal of Machine Learning Re-search, 9: 1871-1874.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving WordRepresentations via Global Context and MultipleWord Prototypes.
In Proc.
of ACL.Akshay Java, Pranam Kolari, Tim Finin, et al.
2006.Modeling the spread of influence on the blogo-sphere.
In Proc.
of WWW.Akshay Java, Xiaodan Song, Tim Finin, and BelleTseng.
2007.
Why we Twitter: UnderstandingMicroblogging Usage and Communities.
In Proc.WebKDD-SNA-KDD.Jeffrey Pennington, Richard Socher, and ChristopherD.
Manning.
Glove: Global Vectors for Word Rep-resentation.
In Proc.
of EMNLP.Pawel Jurczyk and Eugene Agichtein.
2007.
Discover-ing authorities in question answer communities byusing link analysis.
In Proc.
of CIKM.David Kempe, Jon Kleinberg, and Eva Tardos.
2003.Maximizing the spread of influence through a socialnetwork.
In Proc.
of SIGKDD.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, et al.
2014.
A dependency parser fortweets.
In Proc.
of EMNLP, pages 10011012, Doha,Qatar, October.Balachander Krishnamurthy, Phillipa Gill, and MartinArlitt.
2008.
A few chirps about Twitter.
In Proc.
ofthe first workshop on Online social networks.
ACM,pages 19-24.Remi Lebret, Jo el Legrand, and Ronan Collobert.2013.
Is deep learning really necessary for word em-beddings?
In Proc.
of NIPS.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proc.
of ACL.Omer Levy, Yoav Goldberg, And Ido Dagan.
2015.Improving Distributional Similarity with LessonsLearned from Word Embeddings.
In Proc.
of TACL.620Wei Li and Andrew McCallum.
2006.
Pachinko al-location: DAG-structured mixture models of topiccorrela-tions.
In Proc.
of the 23rd international con-ference on Machine learning.
ACM, pages 577-584.Wei Li and Andrew McCallum.
2008.
Pachinko alloca-tion: Scalable mixture models of topic correlations.Journal of Machine Learning Research.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.
Arecursive recurrent neural network for statistical ma-chine translation.
In Proc.
of ACL, pages 1491 1500.Minh-Thang Luong, Richard Socher, and ChristopherD.
Manning.
2013.
Better Word Representationswith Recursive Neural Networks for Morphology.
InProc.
of CoNLL.George A. Miller.
1995.
Wordnet: A lexicaldatabase for english.
Communications of the ACM,38(11):3941.Thomas P. Minka.
2000.
Estimating a Dirichlet distri-bution.
Technical report, MIT.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013(a).
Efficient estimation of word repre-sentations in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregoryS.
Corrado, and Jeffrey Dean.
2013(b).
Distributedrepresentations of words and phrases and theircomposition-ality.
In Advances in Neural Informa-tion Processing Systems.
pages 3111-3119.Aditya Pal and Joseph A. Konstan.
2010.
Expert Iden-tification in Community Question Answering: Ex-ploring Question Selection Bias.
In Proc.
of the 19thACM international conference on Information andknowledge management.
ACM, pages 1505-1508.Aditya Pal and Scott Counts.
2011.
Identifying topi-cal authorities in microblogs.
In Proc.
of the fourthACM international conference on Web search anddata mining.
ACM, pages 45-54.Siyu Qiu, Qing Cui, Jiang Bian, and et al.
2014.
Co-learning of Word Representations and MorphemeRepresentations.
In Proc.
of COLING.Daniele Quercia, Harry Askham, and Jon Crowcroft.2012.
TweetLDA: supervised topic classificationand link prediction in Twitter.
In Proc.
of the 4thAnnual ACM Web Science Conference.
ACM, pages247-250.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: A su-pervised topic model for credit attribution in multi-label corpora.
In Proc.
of EMNLP.Daniel Ramage Susan Dumais, and Dan Liebling.2010.
Characterizing Microblogs with Topic Mod-els.
In ICWSM, 5(4): 130-137.Ana Raposo, Mafalda Mendes, and J. Frederico Mar-ques.
2012.
The hierarchical organization of seman-tic memory: Executive function in the processing ofsuperordinate concepts.
NeuroImage, 59: 18701878.Stanley M. Selkow.
1977.
The tree-to-tree editing prob-lem.
Information processing letters, 6(6): 184-186.Mahdi M. Shafiei and Evangelos E. Milios.
2006.
La-tent Dirichlet coclustering.
In Proc.
of InternationalConference on Data Mining, pages 542-551.Dennis Shasha and Kaizhong Zhang.
1990.
Fast al-gorithms for the unit cost editing distance betweentrees.
Journal of algorithms, 11(4): 581-621.Yaming Sun, Lei Lin, Duyu Tang, and et al.2014.
Radical-enhanced chinese character embed-ding.
arXiv preprint arXiv:1404.4714.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems, pages 31043112.Jie Tang, Jing Zhang, Limin Yao, et al.
2008.
Arnet-miner: Extraction and mining of academic socialnetworks.
In Proc.
of SIGKDD.Duyu Tang, Furu Wei, Nan Yang, et al.
2014.
Learningsentiment-specific word embedding for twitter sen-timent classification.
In Proc.
of ACL.Robert A. Wagner.
1975.
On the complexity of the ex-tended string-to-string correction problem.
In Proc.of seventh annual ACM symposium on Theory ofcomputing.
pages 218-223.
ACM.Robert A. Wagner and Michael J. Fischer.
1974.
Thestring-to-string correction problem.
Journal of theACM (JACM), 21(1), 168-173.Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-coso.
2015.
Two/too simple adaptations of word2vecfor syntax problems.
In Proc.
of NAACL, Denver,CO.Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He.2010.
Twitterrank: finding topic-sensitive influentialtwitterers.
In Proc.
of WSDM.Jason Weston, Antoine Bordes, Oksana Yakhnenko,and Nicolas Usunier.
2013.
Connecting languageand knowledge bases with embedding models for re-lation extraction.
In Proc.
of Computation and Lan-guage.Yi Yang and Jacob Eisenstein.
2015.
Unsupervisedmulti-domain adaptation with feature embeddings.In Proc.
of NAACL-HIT.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proc.
of ACL.Jun Zhang, Mark S. Ackerman, and Lada Adamic.2007.
Expertise networks in online communities:structure and algorithms.
In Proc.
of WWW.621Kaizhong Zhang and Dennis Shasha.
1989.
Simple fastalgorithms for the editing distance between treesand related problems.
SIAM journal on computing,18(6): 1245-1262.Meishan Zhang, Yue Zhang, Wan Xiang Che, and etal.
2013.
Chinese parsing exploiting characters.
InProc.
of ACL.Xin Zhao and Jing Jiang.
2011.
An empirical compari-son of topics in twitter and traditional media.
Singa-pore Management University School of InformationSystems Technical paper series.
Retrieved Novem-ber, 10: 2011.622
