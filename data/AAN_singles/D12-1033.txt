Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 356?367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsSyntactic surprisal affects spoken word duration in conversational contextsVera Demberg, Asad B. Sayeed, Philip J. Gorinski, and Nikolaos EngonopoulosM2CI Cluster of Excellence andDepartment of Computational Linguistics and PhoneticsSaarland University66143 Saarbru?cken, Germany{vera,asayeed,philipg,nikolaos}@coli.uni-saarland.deAbstractWe present results of a novel experiment to in-vestigate speech production in conversationaldata that links speech rate to information den-sity.
We provide the first evidence for an asso-ciation between syntactic surprisal and wordduration in recorded speech.
Using the AMIcorpus which contains transcriptions of focusgroup meetings with precise word durations,we show that word durations correlate withsyntactic surprisal estimated from the incre-mental Roark parser over and above simplermeasures, such as word duration estimatedfrom a state-of-the-art text-to-speech systemand word frequencies, and that the syntac-tic surprisal estimates are better predictors ofword durations than a simpler version of sur-prisal based on trigram probabilities.
This re-sult supports the uniform information density(UID) hypothesis and points a way to more re-alistic artificial speech generation.1 IntroductionThe uniform information density (UID) hypothesissuggests that speakers try to distribute informationuniformly across their utterances (Frank and Jaeger,2008).
Information density can be measured interms of the surprisal incurred at each word, wheresurprisal is defined as the negative log-probabilityof an event.
This paper sets out to test whether UIDholds across different linguistic levels, i.e.
whetherspeakers adapt word duration during production tosyntactic surprisal, such that words with higher sur-prisal have longer durations than words with lowersurprisal.
We investigate this question in a corpusof transcribed speech from a mix of native and non-native English speakers, a population that is a non-trivial component of the user base for language tech-nologies developed for English.
This data reflects acasual, uncontrolled conversational environment.Using linear mixed-effects modeling, we foundthat syntactic surprisal as calculated from a top-down incremental PCFG parser accounts for a sig-nificant amount of variation in spoken word dura-tion, using an HMM-trained text-to-speech systemas a baseline.
The findings of this paper provide ad-ditional support the uniform information density hy-pothesis and furthermore have implications for thedesign of text-to-speech systems, which currentlydo not take into account higher-level linguistic in-formation such as syntactic surprisal (or even wordfrequencies) for their word duration models.1.1 Related workThe use of word-level surprisal as a predictor of pro-cessing difficulty is based on the notion that pro-cessing difficulty results when a word is encounteredthat is unexpected given its preceding context.
Theamount of surprisal on a word wi can be formal-ized as the log of the inverse conditional probabil-ity of wi given the preceding words in the sentencew1 .
.
.
wi?1, or ?
logP (wi|w1...i?1).
If this proba-bility is low, then the word is unexpected, and sur-prisal is high.
Surprisal can be estimated in differentways, e.g.
from word sequences (n-grams) or withrespect to the possible syntactic structures coveringa sentence prefix (see Section 4).Hale (2001) showed that surprisal calculated froma probabilistic Earley parser correctly predicts well-356known processing phenomena that were believedto emerge from structural ambiguities (e.g., gardenpaths) and Levy (2008) further demonstrated the rel-evance of surprisal to human sentence processingdifficulty on a range of syntactic processing diffi-culty phenomena.There is existing work in correlating information-theoretic measures of linguistic redundancy to theobserved duration of speech units.
Aylett and Turk(2006) demonstrate that the contextual predictabilityof a syllable (n-gram log probability) has an inverserelationship to syllable duration in speech.
Their ex-periments were performed using a carefully articu-lated speech synthesis training corpus.This type of work fits into a larger programme ofunderstanding how speakers schedule utterances toavoid high variation in the transmission of linguis-tic information over time, also known as the Uni-form Information Density (UID) hypothesis (Flo-rian Jaeger, 2010).
Levy and Jaeger (2007) showthat the reduction of optional that-complementizersin English is related to trigram surprisal; low sur-prisal predicts a high likelihood of reduction.
Flo-rian Jaeger (2010) shows the same result of in-creased reduction when the complementizer is morepredictable according to information density calcu-lated in terms of the main verb?s subcategorizationfrequency.Frank and Jaeger (2008) provide evidence that aUID account can predict the use of reduced formsof ?be?, ?have?, and ?not?
in English.
They use thesurprisal of the candidate word itself as well as sur-prisals of the word before and after, computing bi-gram and trigram estimates directly from the corpuswithout smoothing or backoff.Jurafsky et al2001) report a corpus study sim-ilar to ours, showing that words that are more pre-dictable from context are reduced.
As measuresof word predictability, they use bigram and trigrammodels, as well as joint probabilities, but not syntac-tic surprisal.Within the same theme of utterance durationvs.
information content, Piantadosi et al2011)performed a study using Google-derived n-gramdatasets on the lexica of multiple languages, includ-ing English, Portuguese, and Czech.
For every wordin a given language?s lexicon, they calculated 2-, 3-,and 4-gram surprisal values using the Google datasetfor every occurrence of the word, and then theytook the mean surprisal for that word over all oc-currences.
The 3-gram surprisal values in particularwere a better predictor of orthographic length thanunigram frequency, providing evidence for the useof information content and contextual predictabilityas improvement over a Zipf?s Law view of commu-nicative efficiency.
This is an n-gram approach tosupporting the UID hypothesis.However, there is some counter-evidence for theUID-based view.
Kuperman et al2007) analyzedthe relationship between linguistic unit predictabil-ity and syllable duration in read-aloud speech inDutch.
Dutch makes use of interfix morphemes-s- and -e(n)- in certain contexts to make com-pound nouns, preferring a null interfix in mostcases.
For example, the Dutch noun kandidaatsex-amen (?Bachelor?s examination?)
is composed ofkandidaat-, -s-, and -examen.Kuperman et alind that the greater the pre-dictability of the interfix from the morphologicalcontext (i.e., the surrounding members of the com-pound), the longer the duration of the pronuncia-tion of the interfix.
To illustrate, if -s- is more ex-pected after kandidaat or if kandidaatsexamen is afrequent compound, we would therefore expect the-s- to be pronounced longer, given the correlationsthey found.
Their finding runs counter to a strongview of UID?s fine-grained control over speech rate,but it is focused on the morphological level.
Theyhypothesize that this counter-intuitive result may bedriven by complex paradigmatic constraints in thechoice of morpheme.Our work, however, focuses on the syntactic levelrather than the paradigmatic.
What we seek to an-swer in our work is the extent to which an infor-mation density-based analysis can not only be ap-plied to real speech data in context but also be de-rived from higher-level syntactic analyses, a com-bination hitherto little explored.
Existing broad-coverage work on syntactic surprisal has largely fo-cused on comprehension phenomena, such as Dem-berg and Keller (2008), Roark et al2009), andFrank (2010).
We provide a production study in avein similar to that of Kuperman et albut show thatfrequency effects work in the expected direction atthe syntactic level.
This in turn expands upon theview supported by n-gram-based work such as that357of Piantadosi et al2011); Levy and Jaeger (2007);Jurafsky et al2001), showing that information con-tent above the n-gram level is important in guidingspoken language production in humans.1.2 Implications for Potential ApplicationsSpoken dialogue systems are of increasing eco-nomic and technological importance in recent times,particularly as it is now feasible to include this tech-nology in everything from small consumer devicesto industrial equipment.
With this increase in impor-tance, there is also unsurprisingly growing scientificemphasis in understanding its usability and safetycharacteristics.
Recent work (Fang et al2009;Taube-Schiff and Segalowitz, 2005) has shown thatlinguistic information presentation has an effect onuser behaviour, but the overall granularity of this be-haviour is still not well-understood.Other potential applications exist in any placewhere text-to-speech technologies can be applied,such as in real-time spoken machine translation andcommunications systems for the disabled.In demonstrating that we can observe speakers be-having in the manner predicted by the UID hypoth-esis in conversational contexts, we provide evidencefor a finer-level of granularity necessary for control-ling the rate of information presentation in artificialsystems.1.3 AMI corpusThe Augmented Multi-Party Interaction (AMI) cor-pus is a collection of recorded, transcribed con-versations spanning 100 hours of simulated meet-ings.
The corpus contains a number of data streamsincluding speech, video, and whiteboard writing.Transcription of the meetings was performed man-ually, and the transcripts contain word-level timebounds that were produced by an automatic speechrecognition system.The freely-available AMI corpus is one of a verysmall number of efforts that contain orthographictranscriptions that are time-aligned at a word level.We chose it for the realism of the setting in whichit was recorded; the physical presence of multiplespeakers in an unstructured discussion reflects a po-tentially high level of noise in which we would belooking for surprisal correspondences, potentiallyincreasing the application value of the correspon-dences we find.1.4 OrganizationThe remainder of this paper proceeds as follows.
Insection 2, we describe at a high level the procedurewe used to test our hypothesis that parser-derivedsurprisal values can partly account for utterance-duration variation.
Then (section 3.2) we discuss theMARY text-to-speech system, from which we derive?canonical?
word utterance durations.
We describethe way we process and filter the AMI meeting cor-pus in section 3.1.
In section 4, we describe in detailour predictors, frequency counts, trigram surprisal,and Roark parser surprisal.
Sections 5 and 6 de-scribe how we use linear mixed effects modeling tofind significant correlations between our predictorsand the response variable, and we finally make someconcluding remarks in section 7.2 DesignThe overall design of our experiment is schemati-cally depicted in Figure 1.
We extract the wordsand the word-by-word timings from the AMI corpus,keeping track of each word?s position in the corpusby conversation ID, speaker turn, and chronologicalorder.
As we describe in the next section, we filterthe words for anomalies.After pre-processing, for each word in the cor-pus, we extract the following predictors: canoni-cal speech durations from the MARY text-to-speechsystem, logarithmic word frequencies, n-gram sur-prisal, and surprisal values produced by the Roark(2001a); Roark et al2009) parser (see Section 4).The next sections describe how and from wherethese values are obtained1.Finally, we run mixed effects regression modelanalyses (Baayen et al2008) with the observeddurations as a response variable and the predictorsmentioned above in order to detect whether syntac-tic surprisal is a significant positive predictor of spo-ken word durations above and beyond the more ba-sic effects of canonical word duration and word fre-quency.1We will make this data widely available upon publication.358AMIcorpus WordfiltrationandselectionGigaword CMUtoolkitAMIwordfreq.
AMI n-gramsurprisalRoarkparserRoarksyntacticsurprisalObservationsMARYComputedtimingsObservedtimingsRegressionanalysisRelativesignificancePennTreebankGigawordfreq.
PTBn-gramsurprisal Gigawordn-gramsurprisalFigure 1: Schematic overview of experiment.3 Experimental materials3.1 Corpus preparationThe AMI corpus is provided in the NITE XMLToolkit (NXT) format.
We developed a custom inter-preter to assemble the relevant data streams: words,meeting IDs, speaker IDs, speaker turns, and ob-served word durations.In addition to grouping and re-ordering the infor-mation found in the original XML corpus, two moresteps were taken to eliminate confounding noisefrom the data.
Non-words (e.g.
?uhm?, ?uh-hmm?,etc.)
were filtered out, as were incomplete words orincorrectly transcribed words (e.g.
?recogn?, ?some-thi?, etc); the criterion for rejection was presence inthe English Gigaword corpus with subsequent mi-nor corrections by hand, e.g., mapping unseen verbsback into the corpus and correcting obvious com-mon misspellings.2Finally, turns that did not make for complete sen-tences, e.g., utterances that were interrupted in mid-2A reviewer asks about the extent to which our Gigaword fil-tering process may remove words we might want to keep but ad-mit words we want to reject.
As Gigaword is mostly newswiretext, we do not expect the latter case to hold often.
AMI ishand-transcribed and uses consistent spellings for non-word in-terjections (easy to remove), and any spelling mistakes wouldhave to coincide exactly with a Gigaword mistake.The other way around (rejecting what should be allowed) iseasier to check, and we find that of 13K word types in AMI,about 7.2% are rejected for non-appearance in Gigaword, afterfiltering for interjections like ?mm-hmm?.
However, we man-ually checked them and returned all but 2.9% of word types tothe corpus.
These tend to be very low-frequency types.
Themanual check suggests that ultimately there would be few falserejections.359sentence, were filtered out in order to maximize theproportion of complete parses in surprisal calcula-tion.3.2 Word duration modelIn order to investigate whether there is an associationbetween high/low surprisal and increased/decreasedword duration, one needs to have a baseline mea-sure of what constitutes the ?canonical?
duration ofeach word?in other words, to account for the factthat some words have longer pronunciations thanothers.
As one reviewer notes, one way of estimat-ing word durations would be to calculate the aver-age duration of each word in the corpus.
However,this approach would be insensitive to the phonolog-ical, syllabic and phrasal context that a word oc-curs in, which can have a large effect on word du-ration.
Therefore, we use word duration estimatesfrom the state-of-the-art open-source text-to-speechsystem MARY (Schro?der et al2008, version 4.3.1),with the default voice package included in this ver-sion (cmu-slt-hsmm).The cmu-slt-hsmm voice package usesa Hidden Markov model, trained on the fe-male US English section of the CMU ARCTICdatabase (Kominek and Black, 2003), to predictprosodic attributes of each individual synthesizedphone, including duration.
Training was carriedout using a version of the HTS system (Zen et al2007), modified for using the MARY contextfeatures (Schro?der et al2008) for estimating theparameters of the model and for decoding.
Thosefeatures include3:?
phonological features of the current and neigh-boring phonemes?
syllabic and lexical features (e.g.
syllablestress, (estimated) part-of-speech, position ofsyllable in word)?
phrasal / sentential features (e.g.
sen-tence/phrase boundaries, neighboring pausesand punctuation)For each word in the AMI corpus, we ob-tained two alternative estimates of word duration:3For further information about how HMM-based voices forMARY TTS are trained, see http://mary.opendfki.de/wiki/HMMVoiceCreationone version which is independent of a word?ssentential context, and a second version whichdoes take into account the sentential context (suchas phrasal/sentential and across-word-boundariesphonological features) the word occurs in.
In otherwords, we obtain MARY word duration estimatesin the second version by running individual wholesentences through MARY, segmented by standardpunctuation marks used in the AMI corpus transcrip-tions.
For each version, we obtained phone dura-tions using MARY and calculate the total duration ofa word as the sum of the estimated phone durationsfor that word.
These durations serve as the ?canoni-cal?
baselines to which the observed durations of thewords in the AMI corpus are compared.3.3 Word frequency baselinesIn order to account for the effects of simple wordfrequency on utterance duration, we extracted twotypes of frequency counts.
One was taken di-rectly from the AMI corpus alone.
The other wastaken from a 151 million-word (4.3 million full-paragraph) sample of the English Gigaword cor-pus.
These came from the following newswiresources: Agence France Press, Associated PressWorldstream, New York Times Newswire, and theXinhua News Agency English Service.
Thesesources are organized by month-of-year.
We se-lected the subset of Gigaword by randomly select-ing month-of-year files from those sources with uni-form probability.
Punctuation was stripped from thebeginnings and ends of words before taking the fre-quency counts.4 Surprisal modelsFor predicting the surprisal of utterances in context,two different types of models were used?
n-gramprobabilities models, as well as Roark?s 2001 incre-mental top-down parser capable of calculating pre-fix probabilities.
We also estimated word frequen-cies to account for words being spoken more quicklydue to their higher frequency which is independentof structural surprisal.The n-gram probabilities models, while being fastin both training and application, inherently capturevery limited contextual influences on surprisal.
Thefull-fledged parser, on the other hand, quantifies sur-360prisal based in the prefix probability of the completesentence prefix and captures long-distance effectsby conditioning on c-commanding lexical items aswell as non-local node labels such as parents, grand-parents and siblings from the left context.CMU n-grams We used the CMU Statistical Nat-ural Language Modeling Toolkit to provide a con-venient way to calculate n-grams probabilities.
Forthe prediction of surprisal, we calculated 3-grammodels, 4-gram models and 5-gram models withWitten-Bell smoothing.
Different n-gram modelswere trained on the full Gigaword corpus, as wellas the AMI corpus.To avoid overfitting, the AMI text corpus was splitinto 10 sub-corpora of equal word counts, preserv-ing coherence of meetings.
N-gram probabilitieswere then calculated for each of the sub-corpora us-ing models trained on the 9 others.We also produced a trigram model using the textof chapter 2?21 of the Penn Treebank?s (PTB) un-derlying Wall Street Journal corpus.
This consistsof approximately one million tokens.
We generatedthis model because it is the underlying training datafor the Roark parser, described below.Syntactic Surprisal from Roark parser In orderto capture the effect of syntactically expected vs. un-expected events, we can calculate the syntactic sur-prisal of each word in a sentence.
The syntactic sur-prisal at word Swi is defined as the difference be-tween the prefix probability at word wi and the pre-fix probability at word wi?1.
The prefix probabilityat word wi is the sum of the probabilities of all treesT spanning words w1 .
.
.
wi; see also (Levy, 2008;Demberg and Keller, 2008).Swi = log?TP (T,w1..wi?1)?
log?TP (T,w1..wi)The top-down incremental Roark parser (Roark,2001a) has the characteristic that all partial left-to-right parses are rooted: they form a single tree withone root.
A set of heuristics ensures that rule appli-cation occurs only through node expansion withinthe connected structure.4 The grammar-derived pre-fix probabilities of a given sentence prefix can there-4The formulae for the calculation of the prefix probabilitiesfrom the PCFG rules can be found in Roark et al2009).SNPDTA3.989NNpuppy4.570VPAUXis3.089SVPTOto3.873PPTOto3.873NPDTa5.973Figure 2: Top-ranked partial parse of A puppy is to a dogwhat a kitten is to a cat., stopping at the second a andproviding the Roark parser surprisal values by word.
Thebranch with dashed lines and struck-out symbols repre-sents an analysis abandoned at the appearance of the a.fore be calculated directly by multiplying the prob-abilities of all rules used to generate the prefix tree.The Roark parser shares this characteristic of gener-ating fully connected structures with Earley parsers(Earley, 1970) and left corner parsers (Rosenkrantzand II, 1970).The Roark parser uses a beam search.
As theamount of probability mass lost has been shownto be small (Roark, 2001b), the surprisal estimatescan be assumed to be a good approximation.
Thebeam width of the parser search is controlled by a?base parsing threshold?, which defines the distancein terms of natural log-probability between the mostprobable parse and the least probable parse withinthe beam.
For the experiments reported here, theparsing beam was set to 21 (default setting is 12).
Awider beam also reduces the effects of pruning.The parser was trained on Wall Street Journal sec-tions 2?21 and applied to parse the full sentencesof the AMI corpus, collecting predicted surprisal ateach word (see Figure 2 for an example).The syntactic surprisal can be furthermore be de-composed into a structural and a lexical part: some-times, high surprisal might be due to a word be-ing incompatible with the high-probability syntacticstructures, other times high surprisal might just bedue to a lexical item being unexpected.
It is inter-361esting to evaluate these two aspects of syntactic sur-prisal separately, and the Roark parser convenientlyoutputs both surprisal estimates.
Structural surprisalis estimated from the occurrence counts of the appli-cation of syntactic rules during the parse discount-ing the effect of lexical probabilities, while lexicalsurprisal is calculated from the probabilities of thederivational step from the POS-tag to lexical item.5 Linear mixed effects modellingIn order to test whether surprisal estimates correlatewith speech durations, we use linear mixed effectsmodels (LME, Pinheiro and Bates (2000)).
Thistype of model can be thought of as a generalizationof linear regression that allows the inclusion of ran-dom factors as well as fixed factors.We treat speak-ers as a random factor, which means that our mod-els contain an intercept term for each speaker, rep-resenting the individual differences in speech rates.Furthermore, we include a random slope for thepredictors (e.g.
frequency, canonical duration, sur-prisal), essentially accounting for idiosyncrasies ofa participant with respect to the predictor, such thatonly the part of the variance that is common to allparticipants and is attributed to that predictor.In a first step, we fit a baseline model with all pre-dictors related to a word?s canonical duration and itsfrequency as well as their random slopes to the ob-served word durations.
Models with more than tworandom slopes generally did not converge.
We there-fore included in the baseline model only the two bestrandom slopes (in terms of model fit).
We then cal-culated the residuals of that model, the part of theobserved word durations that cannot be accountedfor through canonical word durations or word fre-quency.For each of our predictors of interest (n-gram sur-prisal, syntactic surprisal), we then fit another lin-ear mixed-effects model with random slopes to theresiduals of the baseline model.
This two-step pro-cedure allows us to make sure to avoid problemsof collinearity between e.g.
surprisal and word fre-quency or canonical duration.
A simpler (but lessconservative) method is to directly add the predic-tors of interest to the baseline model.
Results forboth modelling variants lead to the same conclusionsfor our model, so we here report the more conserva-tive two-step model.
We compare models based onthe Akaike Information Criterion (AIC).6 ResultsOur baseline model uses speech durations from theAMI corpus as the response variable and canoni-cal duration estimates from the MARY TTS systemand log word frequencies as predictors.
We excludefrom the analysis all data points with zero duration(effectively, punctuation) or a real duration longerthan 2 seconds.
Furthermore, we exclude all wordswhich were never seen in Gigaword and any wordsfor which syntactic surprisal couldn?t be estimated.This leaves us with 771,234 out of the 799,997 datapoints with positive duration.MARY duration models As mentioned in theearlier sections, we have calculated different ver-sions of the MARY estimated word durations: onemodel without the sentential context and one modelwith the sentential context.
In our regression analy-ses, we find, as expected, that the model which in-cludes sentential context achieves a much better fitwith the actually measured word durations from theAMI corpus (AIC = 32167) than the model withoutcontext (AIC = 70917).Word frequency estimates We estimated wordfrequencies from several different resources, fromthe AMI corpus to have a spoken domain frequencyand from Gigaword as a very large resource.
Wefind that both frequency estimates significantly im-prove model fit over a model that does not containfrequency estimates.
Including both frequency esti-mates improves model fit with respect to a modelthat includes just one of the predictors (all p <0.0001).Furthermore, including into the regression an in-teraction of estimated word duration and word fre-quency also significantly increases model fit (p <0.0001).
This means that words which are short andfrequent have longer duration than would be esti-mated by adding up their length and frequency ef-fects.Baseline model Fixed effects of the fitted modelare shown in Table 2.
We see a highly significant ef-fect in the expected direction for both the canonicalduration estimate and word frequency.
The positive362coefficient for MARY CONTEXT means that TTSduration estimates are positively correlated with themeasured word durations.
The negative coefficientfor WORDFREQUENCY means that more frequentwords are spoken faster than less frequent words.Finally, the negative coefficient for the interactionbetween word durations and frequencies means thatthe duration estimate for short frequent and long in-frequent words is less extreme than otherwise pre-dicted by the main effects of duration and frequency.Ami Mary Mary Giga PTB AMI AMI GigaDur Word Cntxt Freq Freq Freq 3grm 4grmMary Word .36 1Mary Cntxt .42 .72 1GigaFreq -.35 -.52 -.65 1PTBFreq -.33 -.48 -.62 .98 1AMIFreq -.33 -.61 -.57 .65 .62 1AMI3gram .21 .40 .41 -.41 -.39 -.68 1Giga4gram .24 .33 .44 -.59 -.59 -.44 .61 1Srprsl .29 .40 .48 -.71 -.73 -.50 .50 .73Table 1: Correlations (pearson) of model predictors.Note though that the predictors are also correlated(for correlations of the main predictors used in theseanalyses, see Table 1), so there is some collinearityin the below model.
Since we are less interested inthe exact coefficients and significance sizes for thesebaseline predictors, this does not have to bother ustoo much.
What is more important, is that we re-move any collinearity between the baseline predic-tors and our predictors of interest, i.e.
the surprisalestimates from the ngram models and parser.
There-fore, we run separate regression models for thesepredictors on the residuals of the baseline model.N-gram estimates We estimated 3-gram, 4-gramand 5-gram models on the AMI corpus (9-fold-Predictor Coef t-value SigINTERCEPT 0.3098 212.11 ***MARY CONTEXT 0.4987 95.48 ***AMIWORDFREQUENCY -0.0282 -32.28 ***GIGAWORDFREQUENCY -0.0275 -62.44 ***MARY CNTXT:GIGAFREQ -0.0922 -45.41 ***Table 2: Baseline linear mixed effects model of speechdurations on the AMI corpus data for MARY CONTEXT(including the sentential context), WORDFREQUENCYunder speaker with random intercept for speaker and ran-dom slopes under speaker.
Predictors are centered.Predictor Coef t-value SigINTERCEPT 0.3099 212.94 ***MARY CONTEXT 0.4970 94.60 ***AMIWORDFREQUENCY -0.0279 -31.98 ***GIGAWORDFREQUENCY -0.0254 -53.68 ***GIGA4GRAMSURPRISAL 0.0027 11.81 ***MARY CNTXT:GIGAFREQ -0.0912 -44.87 ***Table 3: Linear mixed effects model of speech durationsincluding 4-gram surprisal trained on gigaword as a pre-dictor.cross), the Penn Treebank and the Gigaword Cor-pus.
We found that coefficient estimates and signif-icance levels of the resulting models were compara-ble.
This is not surprising, given that 4-gram and 5-gram models were backing of to 3-grams or smallercontexts for more than 95% of cases on the AMI andPTB corpora (both ca.
1m words), and thus werecorrelated at p > .98.
On the Gigaword Corpus,the larger contexts were seen more often (5-grams:11%, 4-grams: 36%), but still correlation with 3-grams were high at (p > .96).N-gram model surprisal estimated on newspapertexts from PTB or Gigaword were statistically sig-nificant positive predictors of spoken word durationsbeyond simple word frequencies (but PTB ngramsurprisal did not improve fit over models containingGigaword frequency estimates).
Counter-intuitivelyhowever, ngram models estimated based on the AMIcorpus have a small negative coefficient in modelsthat already include word frequency as a predictor?
residuals of an AMI-estimated ngram model withrespect to word frequency are very noisy and do notshow a clear correlation anymore with word dura-tions.Surprisal Surprisal effects were found to have arobust significant positive coefficient, meaning thatwords with higher surprisal are spoken more slowly /clearly than expected when taking into account onlycanonical word duration and word frequency.
Sur-prisal achieves a better model fit than any of then-gram models, based on a comparsion of AICs,and Surprisal significantly improved model fit overa model including frequencies and ngram modelsbased on AMI and Gigaword.
Table 4 shows the es-timate for SURPRISAL on the residuals of the modelin Table 2.363Predictor Coef t-value SigINTERCEPT -0.0154 -23.45 ***SURPRISAL 0.0024 26.09 ***Table 4: Linear mixed effects model of surprisal (basedon Roark parser) with random intercept for speaker andrandom slope.
The response variable is residual word du-rations from the model shown in Table 3.Surprisal estimated from the Roark parser alsoremains a significant positive predictor when re-gressed against the residuals of a baseline model in-cluding both 3-gram surprisal from the AMI corpusand 4-gram surprisal from the Gigaword corpus.
Inorder to make really sure that the observed surprisaleffect has indeed to do with syntax and can not beexplained away as a frequency effect, we also cal-culated frequency estimates for the corpus based onthe Penn Treebank.
The significant positive surprisaleffect remains stable, also when run on the residualsof a model which includes PTB trigrams and PTBfrequencies.It is difficult from these regression models to in-tuitively grasp the size of the effect of a particularpredictor on reading times, since one would have toknow the exact range and distribution of each pre-dictor.
To provide some intuition, we calculate theestimated effect size of Roark surprisal on speechdurations.
Per Roark surprisal ?unit?, the model es-timates a 7 msec difference5.
The range of Roarksurprisal in our data set is roughly from 0 to 25,with most values between 2 and 15.
For a wordlike ?thing?
which in one instance in the AMI cor-pus was estimated with a surprisal of 2.179 and inanother instance as 16.277, the estimated differencein duration between these instances would thus be104msec, which is certainly an audible difference.
(Full range for Roark surprisal: 174msec, whereasfull range for gigaword 4gram surprisal is 35 msec.
)When analysing the surprisal effect in more detail,we find that both the syntactic component of sur-prisal and its lexical component are significant pos-itive predictors of word durations, as well as the in-teraction between them, which has a negative slope.A model with the separate components and their in-52.4msec for a unit of residualized Roark surprisal, but itis even less intuitive what that means, hence we calculate withnon-residualized surprisal here.Predictor Coef t-value SigINTERCEPT -0.0219 -18.77 ***STRUCTSURPRISAL 0.0009 2.71 **LEXICALSURPRISAL 0.0044 24.00 ***STRUCT:LEXICAL -0.0004 - 6.83 ***Table 5: Linear mixed effects model of residual speechdurations wrt.
baseline model from Table 3, with randomintercept for speaker and random slope for structural andlexical component of surprisal, estimated using the Roarkparser.teraction achieves a better model fit (in AIC and BICscores) than a model with only the full surprisal ef-fect.
The detailed model is shown in Table 5.To summarize, the positive coefficient of surprisalmeans that words which carry a lot of informationfrom a structural point of view are spoken moreslowly than words that carry less such information.These results thus provide good evidence for ourhypothesis that the predictability of syntactic struc-ture affects phonetic realization and that speakersuse speech rate to achieve more uniform informationdensity.Native vs. non-native speakers Finally, we alsocompared effects in our native vs. non-nativespeaker populations, see Table 6.
Both populationsshow the same effects and tell the same story (notethat significance values can?t be compared as thesample sizes are different).
It might be possible tointerpret the findings in the sense that native speak-ers are more proficient at adapting their speech rateto (syntactic) complexity to achieve more uniforminformation density, given the slightly higher coeffi-cient and significance for Surprisal for native speak-ers.
Since the effects are statistically significantfor both groups, we don?t want to make too strongclaims about differences between the groups.7 Conclusions and future workWe have shown evidence in this work that syntac-tic surprisal effects in transcribed speech data canbe detected through word utterance duration in bothnative and non-native speech, and we did so usinga meeting corpus not specifically designed to iso-late these effects.
This result is the potential foun-dation for futher work in applied, experimental, and364Native English Non-nativePredictor Coef t-value Sig Coef t-value SigINTERCEPT 0.2947 149.74 *** 0.3221 175.38 ***MARY CONTEXT 0.5304 69.27 *** 0.4699 67.77 ***AMIWORDFREQUENCY -0.0226 -18.10 *** -0.0321 -28.00 ***GIGAWORDFREQUENCY -0.0264 -41.19 *** -0.0248 -39.58 ***GIGAWORD4-GRAMS 0.0018 5.36 *** 0.0033 10.85 ***MARY CONTEXT:GIGAFREQ -0.0810 -27.20 *** -0.0993 -35.71 ***SURPRISAL 0.0033 24.21 *** 0.0018 15.09 ***no of data points 320,592 391,106*p < 0.05, **p < 0.01, ***p < 0.001Table 6: Native speakers are possibly slightly better at adapting their speech rate to syntactic surprisal than non-nativespeakers.
Surprisal value is for model with residuals of other predictors as dependent variable.theoretical psycholinguistics.
It provides additionaldirect support for approaches based on the UID hy-pothesis.From an applied perspective, the fact that fre-quency and syntactic surprisal have a significant ef-fect beyond what a HMM-trained TTS model wouldpredict for individual words is a case for furtherresearch into incorporating syntactic models intospeech production systems.
Our methodology im-mediately provides a framework for estimating theword-by-word effect on duration for increased nat-uralness in TTS output.
This is relevant to spo-ken dialogue systems because it appears that syn-thesized speech requires a greater level of attentionfrom the dialogue system users when compared tothe same words delivered in natural speech (Deloguet al1998).
Some of this effect may be attributableto peaks in information density which are caused bycurrent generation systems not compensating for ar-eas of high information density through speech rate,lexical and structural choice.Furthermore, syntax and semantics have been ob-served to interact with the mode of speech deliv-ery.
Eye-tracking experiments by Swift et al2002)showed that there was a synthetic vs. natural speechdifference in the time required to pay attention toan object referred to using definite articles, but notindefinite articles.
Our result points a way towardsa direction for explaining of this phenomenon bydemonstrating that the differences between current-technology artificial speech and natural speech canbe partially explained through higher-level syntacticfeatures.However, further experimentation is required onother measures of syntactic complexity (e.g.
DLT,Gibson (2000)) as well as other levels of representa-tion such as the semantic level.
From a theoreticaland neuroanatomical perspective, the finding that ameasure of syntactic ambiguity reduction has an ef-fect on the phonological layer of production has ad-ditional implications for the organization of the hu-man language production system.ReferencesAylett, M. and Turk, A.
(2006).
Language redun-dancy predicts syllabic duration and the spec-tral characteristics of vocalic syllable nuclei.Journal of the acoustical society of America,119(5):3048?3059.Baayen, R., Davidson, D., and Bates, D. (2008).Mixed-effects modeling with crossed random ef-fects for subjects and items.
Journal of memoryand language, 59(4):390?412.Delogu, C., Conte, S., and Sementina, C. (1998).Cognitive factors in the evaluation of syntheticspeech.
Speech Communication, 24(2):153?168.Demberg, V. and Keller, F. (2008).
Data fromeye-tracking corpora as evidence for theoriesof syntactic processing complexity.
Cognition,109:193?210.Earley, J.
(1970).
An efficient context-free parsingalgorithm.
Commun.
ACM, 13(2):94?102.Fang, R., Chai, J. Y., and Ferreira, F. (2009).
Be-365tween linguistic attention and gaze fixations in-multimodal conversational interfaces.
In Inter-national Conference on Multimodal Interfaces,pages 143?150.Florian Jaeger, T. (2010).
Redundancy and reduc-tion: Speakers manage syntactic information den-sity.
Cognitive Psychology, 61(1):23?62.Frank, A. and Jaeger, T. F. (2008).
Speaking ra-tionally: uniform information density as an opti-mal strategy for language production.
In The 30thannual meeting of the Cognitive Science Society,pages 939?944.Frank, S. (2010).
Uncertainty reduction as a mea-sure of cognitive processing effort.
In Proceed-ings of the 2010 Workshop on Cognitive Model-ing and Computational Linguistics, pages 81?89,Uppsala, Sweden.Gibson, E. (2000).
Dependency locality theory: Adistance-dased theory of linguistic complexity.
InMarantz, A., Miyashita, Y., and O?Neil, W., ed-itors, Image, Language, Brain: Papers from theFirst Mind Articulation Project Symposium, pages95?126.
MIT Press, Cambridge, MA.Hale, J.
(2001).
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chapterof the Association for Computational Linguistics,volume 2, pages 159?166, Pittsburgh, PA.Jurafsky, D., Bell, A., Gregory, M., and Raymond,W.
(2001).
Evidence from reduction in lexicalproduction.
Frequency and the emergence of lin-guistic structure, 45:229.Kominek, J. and Black, A.
(2003).
The cmuarctic speech databases for speech synthesisresearch.
Language Technologies Institute,Carnegie Mellon University, Pittsburgh, PA, Tech.Rep.
CMULTI-03-177 http://festvox.
org/cmu arc-tic.Kuperman, V., Pluymaekers, M., Ernestus, M., andBaayen, H. (2007).
Morphological predictabilityand acoustic duration of interfixes in dutch com-pounds.
The Journal of the Acoustical Society ofAmerica, 121(4):2261?2271.Levy, R. (2008).
Expectation-based syntactic com-prehension.
Cognition, 106(3):1126?1177.Levy, R. and Jaeger, T. F. (2007).
Speakers opti-mize information density through syntactic reduc-tion.
In Advances in Neural Information Process-ing Systems.Piantadosi, S., Tily, H., and Gibson, E. (2011).
Wordlengths are optimized for efficient communica-tion.
Proceedings of the National Academy of Sci-ences, 108(9).Pinheiro, J. C. and Bates, D. M. (2000).
Mixed-effects models in S and S-PLUS.
Statistics andcomputing series.
Springer-Verlag.Roark, B.
(2001a).
Probabilistic top-down parsingand language modeling.
Computational linguis-tics, 27(2):249?276.Roark, B.
(2001b).
Robust probabilistic predictivesyntactic processing: motivations, models, andapplications.
PhD thesis, Brown University.Roark, B., Bachrach, A., Cardenas, C., and Pal-lier, C. (2009).
Deriving lexical and syntacticexpectation-based measures for psycholinguisticmodeling via incremental top-down parsing.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages324?333, Singapore.
Association for Computa-tional Linguistics.Rosenkrantz, D. J. and II, P. M. L. (1970).
Deter-ministic left corner parsing (extended abstract).
InSWAT (FOCS), pages 139?152.Schro?der, M., Charfuelan, M., Pammi, S., and Tu?rk,O.
(2008).
The MARY TTS entry in the Bliz-zard Challenge 2008.
In Proc.
Blizzard Chal-lenge.
Citeseer.Swift, M. D., Campana, E., Allen, J. F., and Tanen-haus, M. K. (2002).
Monitoring eye movementsas an evaluation of synthesized speech.
In Pro-ceedings of the IEEE 2002 Workshop on SpeechSynthesis.Taube-Schiff, M. and Segalowitz, N. (2005).
Lin-guistic attention control: attention shifting gov-erned by grammaticized elements of language.Journal of experimental psychology Learningmemory and cognition, 31(3):508?519.Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko,T., Black, A., and Tokuda, K. (2007).
The HMM-based speech synthesis system (HTS) version 2.0.366In Proc.
of Sixth ISCA Workshop on Speech Syn-thesis, pages 294?299.367
