Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55?59,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsDependency Parsing domain adaptation using transductive SVMAntonio Valerio Miceli-BaroneUniversity of Pisa, Italy /Largo B. Pontecorvo, 3, Pisa, Italymiceli@di.unipi.itGiuseppe AttardiUniversity of Pisa, Italy /Largo B. Pontecorvo, 3, Pisa, Italyattardi@di.unipi.itAbstractDependency Parsing domain adaptationinvolves adapting a dependency parser,trained on an annotated corpus from a givendomain (e.g., newspaper articles), to workon a different target domain (e.g., legal doc-uments), given only an unannotated corpusfrom the target domain.We present a shift/reduce dependencyparser that can handle unlabeled sentencesin its training set using a transductive SVMas its action selection classifier.We illustrate the the experiments we per-formed with this parser on a domain adap-tation task for the Italian language.1 IntroductionDependency parsing is the task of identifying syn-tactic relationships between words of a sentenceand labeling them according to their type.
Typ-ically, the dependency relationships are not de-fined by an explicit grammar, rather implicitlythrough a human-annotated corpus which is thenprocessed by a machine learning procedure, yield-ing a parser trained on that corpus.Shift-reduce parsers (Yamada and Matsumoto,2003; Nivre and Scholz, 2004; Attardi, 2006) arean accurate and efficient (linear complexity) ap-proach to this task: They scan the words of a sen-tence while updating an internal state by means ofshift-reduce actions selected by a classifier trainedon the annotated corpus.Since the training corpora are made by human an-notators, they are expensive to produce and aretypically only available for few domains that don?tadequately cover the whole spectrum of the lan-guage.
Parsers typically lose significant accuracywhen applied on text from domains not coveredby their training corpus.
Several techniques havebeen proposed to adapt a parser to a new domain,even when only unannotated samples from it areavailable (Attardi et al, 2007a; Sagae and Tsujii,2007).In this work we present a domain adaptation basedon the semi-supervised training of the classifier ofa shift-reduce parser.
We implement the classifieras a multi-class SVM and train it with a transduc-tive SVM algorithm that handles both labeled ex-amples (generated from the source-domain anno-tated corpus) and unlabeled examples (generatedfrom the the target-domain unannotated corpus).2 Background2.1 Shift-Reduce ParsingA shift-reduce dependency parser is essentially apushdown automaton that scans the sentence onetoken at a time in a fixed direction, while updat-ing a stack of tokens and also updating a set ofdirected, labeled edges that is eventually returnedas the dependency parse graph of the sentence.Let T be the set of input token instances ofthe sentence and D be the set of dependencylabels.
The state of the parser is defined bythe tuple ?s, q, p?, where s ?
T ?
is the stack,q ?
T ?
is the current token sequence and p ?
{E|E ?
2T?T?D, E is a forest}is the currentparse graph.The parser starts in the state ?
[], q0, {}?, where q0is the input sentence, and terminates whenever itreaches a state in the form ?s, [], p?.
At each step,55it performs one of the following actions:shift :?s, [t|q], p??
[t|s], q, p?rightreduced :?
[u|s], [t|q], p?
?s, [t|q], p ?
{(u, t, d)}?leftreduced :?
[u|s], [t|q], p?
?s, [u|q], p ?
{(t, u, d)}?note that there are rightreduced andleftreduced actions for each label d ?
D.Action selection is done by the combinationof two functions f ?
c : a feature extractionfunction f : States ?
Rn that computes a(typically sparse) vector of numeric features ofthe current state and the multi-class classifierc : R ?
Actions.
Alternatively, the classifiercould score each available action, allowing asearch procedure such as best-first (Sagae andTsujii, 2007) or beam search to be used.In our experiments we used an extension ofthis approach that has an additional stack andadditional actions to handle non-projective de-pendency relationships (Attardi, 2006).
Trainingis performed by computing, for each sentencein the annotated training corpus, a sequence ofstates and actions that generates its correct parse,yielding, for each transition, a training example(x, y) ?
Rn ?Actions for the classifier.Various classification algorithms have beensuccessfully used, including maximum entropy,multi-layer perceptron, averaged perceptron,SVM, etc.
In our approach, the classifier isalways a multi-class SVM composed of multiple(one-per-parsing-action) two-class SVMs inone-versus-all configuration.2.2 Parse Graph RevisionAttardi and Ciaramita (2007b) developed amethod for improving parsing accuracy usingparse graph revision: the output of the parser isfed to a procedure that scans the parsed sentencein a fixed direction and, at each step, possibly re-vises the current node (rerouting or relabeling itsunique outgoing edge) based on the classifier?soutput.Training is performed by parsing the training cor-pus and comparing the outcome against the anno-tation: for each sentence, a sequence of actionsnecessary to transform the machine-generatedparse into the reference parse is computed and itis used to train the classifier.
(Usually, a lower-quality parser is used during training, assumingthat it will generate more errors and hence morerevision opportunities).This method tends to produce robust parsers: er-rors in the first stage have the opportunity to becorrected in the revision stage, thus, even if itdoes not learn from unlabeled data, it neverthe-less performs well in domain adaptation tasks (At-tardi et al, 2007a).
In our experiments we usedparse graph revision both as a baseline for accu-racy comparison, and in conjunction with our ap-proach (using a transductive SVM classifier in therevision stage).2.3 Transductive SVMTransductive SVM (Vapnik, 1998) is a frameworkfor the semi-supervised training of SVM classi-fiers.Consider the inductive (completely supervised)two-class SVM training problem: given a trainingset {(xi, yi) |xi ?
Rn, yi ?
{?1, 1}}Li=1, findthe maximum margin separation hypersurface w ??
(x) + b = 0 by solving the following optimiza-tion problem:arg minw, b, ?12?w?22 + CL?i=1?i (1)?i : yiw ?
?
(x) + b ?
1?
?i?i : ?i ?
0w ?
Rm, b ?
Rwhere C ?
0 is a regularization parameter and?(?)
is defined such that k (x, x?)
?
?
(x) ?
?
(x?
)is the SVM kernel function.
This is a convexquadratic programming problem that can besolved efficiently by specialized algorithms.Including an unlabeled example set{x?j |x?j ?
Rn}L?j=1we obtain the transduc-tive SVM training problem:arg minw, b, ?, y?, xi?12?w?22 + CL?i=1?i + C?L??j=1?
?j(2)56?i : yiw ?
?
(xi) + b ?
1?
?i?j : y?j w ?
?
(x?j)+ b ?
1?
?
?j?i : ?i ?
0?j : ?
?j ?
0?j : y?j ?
{?1, 1}w ?
Rm, b ?
RThis formulation essentially models the unlabeledexamples the same way the labeled examplesare modeled, with the key difference that they?j (the unknown labels of the unlabeled exam-ples) are optimization variables rather than pa-rameters.
Optimizing over these discrete variablesmakes the problem non-convex and in fact NP-hard.
Nevertheless, algorithms that feasibly finda local minimum that is typically good enoughfor practical purposes do exist.
In our exper-iments we used the iterative transductive SVMalgorithm implemented in the SvmLight library(Joachims, 1999).
This algorithm tends to be-come impractical when the number of unlabeledexamples is greater than a few thousands, hencewe were forced to use only a small portion on theavailable target domain corpus.
We also tried theconcave-convex procedure (CCCP) TSVM algo-rithm (Collobert et al, 2006) as implemented bythe the Universvm package, and the multi-switchand deterministic annealing algorithms for linearTSVM (Sindhwani and Keerthi, 2007) as imple-mented by the Svmlin package.
These methodsare considerably faster but appear to be substan-tially less accurate than SvmLight on our trainingdata.3 Proposed approachWe present a semi-supervised training procedurefor shift/reduce SVM parsers that allows to in-clude unannotated sentences in the training cor-pus.We randomly sample a small number (approx.100) of sentences from the unannotated corpus(the target domain corpus in a domain adaptationtask).
For each of these sentences, we generate asequence of states that the parser may encounterwhile scanning the sentence.
For each state weextract the features to generate an unlabeled train-ing example for the SVM classifier which is in-cluded in the training set alng with the labeledexamples generated from the annotated corpus.There is a caveat here: the parser state at any givenpoint during the parsing of a sentence generallydepends on the actions taken before, but when weare training on an unannotated sentence, we haveno way of knowing what actions the parser shouldhave taken, and thus the state we generate can begenerally incorrect.
For this reason we evaluatedpre-parsing the unannotated sentences with a non-transductively trained parser in order to generateplausible state transitions while still adding unla-beled examples.
However, it turned out that thispre-parsing does not seem to improve accuracy.We conjecture that, because the classifier does notsee actual states but only features derived fromthem, and many of these features are independentof previous states and actions (features such as thelemma and POS tag of the current token and itsneighbors have this property), these features con-tain enough information to perform parsing.The classifier is trained using the SvmLight trans-ductive algorithm.
Since SvmLight supports onlytwo-class SVMs while our classifier is multi-class(one class for each possible parsing action), weimplement it in terms of two-class classifiers.
Wechose the one-versus-all strategy:We train a number of sub-classifiers equal to thenumber of original classes.
Each labeled trainingexample (x, y) is converted to the example (x, 1)for the sub-classifier number y and to the example(x, ?1) for the rest of sub-classifiers.
Unlabeledexamples are just replicated to all sub-classifiers.During classification the input example is eval-uated by all the sub-classifiers and the one re-turning the maximum SVM score determines theclass.Our approach has been also applied to the secondstage of the revision parser, by presenting the fea-tures of the unannotated sentences to the revisionclassifier as unlabeled training examples.4 Experiments4.1 Experimental setupWe performed our experiments using the DeSRparser (Attardi, 2006) on the data sets for theEvalita 2011 dependency parsing domain adapta-tion task for the Italian language (Evalita, 2011).The data set consists in an annotated source-domain corpus (newspaper articles) and an unan-notated target-domain corpus (legal documents),57plus a small annotated development corpus alsofrom the target domain, which we used to evalu-ate the performance.We performed a number of runs of the DeSRparser in various configurations, which differedin the number and type of features extracted, thesentence scanning direction, and whether or notparse tree revision was enabled.
The SVM clas-sifiers always used a quadratic kernel.
In order tokeep the running time of transductive SVM train-ing acceptable, we limited the number of unanno-tated sentences to one hundred, which resulted inabout 3200 unlabeled training examples fed to theclassifiers.
The annotated sentences were 3275.We performed one run with 500 unannotated sen-tences and, at the cost of a greatly increased run-ning time, the accuracy improvement was about1%.
We conjecture that a faster semi-supervisedtraining algorithm could allow greater perfor-mance improvements by increasing the size of theunannotated corpus that can be processed.
Allthe experiments were performed on a machineequipped with an quad-core Intel Xeon X3440processor (8M Cache, 2.53 GHz) and 12 Giga-bytes of RAM.4.2 DiscussionAs it is evidenced from the table in figure1, our approach typically outperforms the non-transductive parser by about 1% of all the threescore measures we considered.
While the im-provement is small, it is consistent with differ-ent configurations of the parser that don?t useparse tree revision.
Accuracy remained essen-tially equal or became slightly worse in the twoconfigurations that use parse tree revision.
This ispossibly due to the fact that the first stage parser ofthe revision configurations uses a maximum en-tropy classifier during training that does not learnfrom the unlabeled examples.These results suggest that unlabeled examplescontain information that can exploited to improvethe parser accuracy on a domain different than thelabeled set domain.
However, the computationalcost of transductive learning algorithm we usedlimits the amount of unlabeled data we can ex-ploit.This is consistent with the results obtained bythe self-training approaches, where a first parseris trained on a the labeled set, which is used toparse the unlabeled set which is then included intothe training set of a second parser.
(In fact, self-training is performed in the first step of the Svm-Light TSVM algorithm).Despite earlier negative results, (Sagae, 2010)showed that even naive self-training can provideaccuracy benefits (about 2%) in domain adapta-tion, although these results are not directly com-parable to ours because they refer to constituencyparsing rather than dependency parsing.
(Mc-Closky et al, 2006) obtain even better results (5%f-score gain) using a more sophisticated form ofself-training, involving n-best generative parsingand discriminative reranking.
(Sagae and Tsujii,2007) obtain similar gains (about 3 %) for de-pendency parsing domain adaptation, using self-training on a subset of the target-domain instancesselected on the basis of agreement between twodifferent parsers.
(the results are not directly com-parable to ours because they were obtained on adifferent corpus in a different language).5 Conclusions and future workWe presented a semi-supervised training ap-proach for shift/reduce SVM parsers and we illus-trated an application to domain adaptation, withsmall but mostly consistent accuracy gains.
Whilethese gains may not be worthy enough to justifythe extra computational cost of the transductiveSVM algorithm (at least in the SvmLight imple-mentation), they do point out that there exist asignificant amount of information in an unanno-tated corpus that can be exploited for increasingparser accuracy and performing domain adapta-tion.
We plan to further investigate this method byexploring classifier algorithms other than trans-ductive SVM and combinations with other semi-supervised parsing approaches.
We also plan totest our method on standardized English-languagecorpora to obtain results that are directly compa-rable to those in the literature.ReferencesH.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.Proceedings of the 9th International Workshop onParsing Technologies.J.
Nivre and M. Scholz.
2004.
Deterministic De-pendency Parsing of English Text.
Proceedings ofCOLING 2004.G.
Attardi.
2006.
Experiments with a Multilanguage58Figure 1: Experimental resultsAccuracy (-R: right-to-left, -rev: left-to-right with revision, -rev2: right-to-left with revision):Transductive NormalParser configuration LAS UAS Label only LAS UAS Label only6 74.3 77.0 87.5 73.1 75.5 86.76-R 75.7 78.6 88.7 74.6 77.6 87.86-rev 75.2 78.2 88.6 75.1 78.0 88.36-rev2 75.0 77.8 88.7 75.8 78.6 88.78 74.3 77.0 87.3 73.4 76.0 85.98-R 75.7 78.6 88.7 75.3 78.3 88.12 74.7 77.4 87.4 73.1 75.8 86.5Figure 2: Typical features (configuration 6).Numbers denote offsets.?FEATS?
denotes rich morphological features (grammatical number, gender, etc).LEMMA -2 -1 0 1 2 3 prev(0) leftChild(-1) leftChild(0) rightChild(-1) rightChild(0)POSTAG -2 -1 0 1 2 3 next(-1) leftChild(-1) leftChild(0) rightChild(-1) rightChild(0)CPOSTAG -1 0 1FEATS -1 0 1DEPREL leftChild(-1) leftChild(0) rightChild(-1)Non-Projective Dependency Parser.
Proceedings ofCoNNL-X 2006.G.
Attardi, A. Chanev, M. Ciaramita, F. Dell?Orlettaand M. Simi.
2007.
Multilingual Dependency Pars-ing and domain adaptation using DeSR.
Proceed-ings the CoNLL Shared Task Session of EMNLP-CoNLL 2007, Prague, 2007.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
CoNLL Shared Task.G.
Attardi, M. Ciaramita.
2007.
Tree Revision Learn-ing for Dependency Parsing.
Proc.
of the HumanLanguage Technology Conference 2007.V.
Vapnik.
1998.
Statistical Learning Theory.
Wiley.Ronan Collobert and Fabian Sinz and Jason Westonand Lon Bottou and Thorsten Joachims.
2006.Large Scale Transductive SVMs.
Journal of Ma-chine Learning ResearchThorsten Joachims.
1999.
Transductive Infer-ence for Text Classification using Support VectorMachines.
International Conference on MachineLearning (ICML), 1999.Vikas Sindhwani and S. Sathiya Keerthi 2007.
New-ton Methods for Fast Solution of SemisupervisedLinear SVMs.
Large Scale Kernel Machines.
MITPress (Book Chapter), 2007Kenji Sagae 2010.
Self-Training without Rerankingfor Parser Domain Adaptation and Its Impact onSemantic Role Labeling.
Proceedings of the 2010Workshop on Domain Adaptation for Natural Lan-guage Processing.
Uppsala, Sweden: Associationfor Computational Linguistics.
p. 37-44David McClosky, Eugene Charniak and Mark John-son 2006.
Reranking and self-training for parseradaptation.
Proceeding ACL-44.
Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the As-sociation for Computational LinguisticsEvalita.
2011.
Domain Adaptation for DependencyParsing.
.59
