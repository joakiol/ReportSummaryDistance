Chinese Word Sense Induction with Basic Clustering AlgorithmsYuxiang Jia1,2, Shiwen Yu1, Zhengyan Chen31Key Laboratory of Computational Linguistics, Ministry of Education, China2College of Information and Engineering, Zhengzhou University, Zhengzhou, China3Department of Information Technology, Henan Institute of Education, Zhengzhou, China{yxjia,yusw}@pku.edu.cn  chenzhengyan1981@163.comAbstractWord Sense Induction (WSI) is animportant topic in natural langageprocessing area.
For the bakeoff taskChinese Word Sense Induction (CWSI),this paper proposes two systems usingbasic clustering algorithms, k-means andagglomerative clustering.
Experimentalresults show that k-means achieves abetter performance.
Based only on thedata provided by the task organizers, thetwo systems get FScores of 0.7812 and0.7651 respectively.1 IntroductionWord Sense Induction (WSI) or Word SenseDiscrimination is a task of automatically discov-ering word senses from un-annotated text.
It isdistinct from Word Sense Disambiguation(WSD) where the senses are assumed to beknown and the aim is to decide the right mean-ing of the target word in context.
WSD generallyrequires the use of large-scale manually anno-tated lexical resources, while WSI can overcomethis limitation.
Furthermore, automatically in-duced word senses can improve performance onmany natural language processing tasks such asinformation retrieval (Uzuner et al, 1999), in-formation extraction (Chai and Biermann, 1999)and machine translation (Vickrey et al, 2005).WSI is typically treated as a clustering prob-lem.
The input is instances of the ambiguousword with their accompanying contexts and theoutput is a grouping of these instances intoclasses corresponding to the induced senses.
Inother words, contexts that are grouped togetherin the same class represent a specific word sense.The task can be formally defined as a twostage process, feature selection and word cluster-ing.
The first stage determines which contextfeatures to consider when comparing similaritybetween words, while the second stage applysome process that clusters similar words usingthe selected features.
So the simplest approachesto WSI involve the use of basic word co-occurrence features and application of classicalclustering algorithms, more sophisticated tech-niques improve performance by introducing newcontext features, novel clustering algorithms, orboth.
(Denkowski, 2009) makes a comprehen-sive survey of techniques for unsupervised wordsense induction.Two tasks on English Word Sense Inductionwere held on SemEval2007 (Agirre and Soroa,2007) and SemEval2010 (Manandhar and Kla-paftis, 2010) respectively, which greatly pro-mote the research of English WSI.However, the study on Chinese Word SenseInduction (CWSI) is inadequate (Zhu, 2009),and Chinese word senses have their own charac-teristics.
The methods that work well in Englishmay not work well in Chinese.
So, as an explo-ration, this paper proposes simple approachesutilizing basic features and basic clustering algo-rithms, such as partitional method k-means andhierarchical agglomerative method.The rest of this paper is organized as follows.Section 2 briefly introduces the basic clusteringalgorithms.
Section 3 describes the feature set.Section 4 gives experimental details and analysis.Conclusions and future work are given in Sec-tion 5.2 Clustering AlgorithmsPartitional clustering and hierarchical clusteringare the two basic types of clustering algorithms.Partitional clustering partitions a given datasetinto a set of clusters without any explicitstructure, while hierarchical clustering creates ahierarchy of clusters.The k-means algorithm is the most notablepartitional clustering method.
It takes a simpletwo step iterative process, data assignment andrelocation of means, to divide the dataset into aspecified number of clusters, k.Hierarchical clustering algorithms are eithertop-down or bottom-up.
Bottom-up algorithmstreat each instance as a singleton cluster at thebeginning and then successively merge pairs ofclusters until all clusters have been merged intoa single cluster.
Bottom-up clustering is alsocalled hierarchical agglomerative clustering,which is more popular than top-down clustering.We use k-means and agglomerative algo-rithms for the CWSI task, and compare the per-formances of the two algorithms.Estimating the number of the induced clusters,k, is difficult for general clustering problems.But in CWSI, it is simplified because the sensenumber of the target word is given beforehand.CLUTO (Karypis, 2003), a clustering toolkit,is used for implementation.
The similarity be-tween objects is computed using cosine function.The criterion functions for k-means and agglom-erative algorithms are I2 and UPGMA respec-tively.
Biased agglomerative approach is chosenin stead of the traditional agglomerative ap-proach.3 Feature SetFor each target word, instances are extractedfrom the XML data file.
Then the encoding ofthe instance file is transformed from UTF-8 toGB2312.
Word segmentation and part-of-speechtagging is finished with the tool ICTCLAS 1 .Then the following three types of features areextracted:1.
The part-of-speech of the target word2.
Words before and after the target wordwithin window of size 3 with position informa-tion3.
Unordered single words in all the contex-tual sentences without the target word, punctua-tions and symbols of the part-of-speech ?nx?
(Each word is only counted once, which is dif-1http://ictclas.org/ferent from the word frequency in the bag-of-words model)The target word is not necessarily a seg-mented word.
Their relations are as follows:1.
The target word is a segmented word.E.g.
?/d  ?/v  ?/r  ?
?/nDon?t dial my phone.The target word is ???
(dial) and the seg-mented word is also ???
(dial).
So they match.2.
The target word is inside of a segmentedword.E.g.
?/p  ?
?/n  ??
?/vdeal with mediaThe target word is ???
(deal), but the seg-mented word is ?????
(deal with).
Then wesplit the segmented word and specify the part-of-speech of the target word as ?1?.3.
The target word is the combination of twosegmented words.E.g.
?/v  ?/v  ?/w  ????
?/nz  ?/wlaunching the ?Culture Revolution?The target word is ????
(launching), but itis split into two segmented words ???
(start)and ???
(move).
Then we combine the twosegmented words and specify the part-of-speechof the target word as ?2?.4.
The target word is split into two segmentedwords.E.g.
?/v  ?/v  ?/u  ?/j  ?
?/nblow up northeast windThe target word is ???
?, but it is segmentedinto two words ???
(east) and ????
(northwind).
In this case, we specify the postion offirst segmented word as the position of the targetword and the part-of-speech of the target wordas ?3?.If the target word occurs more than once in aninstance, we consider the first occurrence.4 Experiments4.1 Data SetsTwo data sets are provided.
The trial set contains50 target words and 50 examples for each targetword.
The test set consists of 100 new targetword and 50 examples for each target word.Both data sets are collected from the internet.Table 1 shows the distribution of sense num-bers of the target words in the two data sets.
Wecan see that two sense words dominate and threesense words are the second majority.
The word???
(beat) in the trial set has 21 senses.Table 1.
Distribution of sense numberssense number 2 3 4 6 7 8 21trial set 39 9 1 0 0 0 1test set 77 10 7 4 1 1 0Table 2.
Distribution of relations between targetwords and segmented wordsrelation type 1 2 3 4 Totaltrial set 2314 105 68 12 2499test set 4031 710 212 47 5000As is shown in table 2, the total instancenumber in the trial set is 2499 because there is atarget word has only 49 instances.
About 7.4%of the instances in the trial set and 19.38% of theinstances in the test set have mismatched targetwords and segmented words (with relation types2, 3 and 4).4.2 Evaluation MetricsThe official performance metric for the CWSItask is FScore (Zhao and Karypis, 2005).
Givena particular class Ci of size ni and a cluster Sr ofsize nr, suppose irn  examples in the class Ci be-long to Sr.
The F value of this class and cluster isdefined to be:),(),(),(*),(*2),(ririririri SCRSCPSCRSCPSCF+= ,whererirri nnSCP =),( is the precision valueandiirri nnSCR =),( is the recall value definedfor class Ci and cluster Sr.
The FScore of class Ciis the maximum F value attained at any cluster,that is),(max)( riSiSCFCFScorer=and the FScore of the entire clustering solutionis?==ciii CFScorennFScore1)(where c is the number of classes and n is the sizeof the clustering solution.Another two metrics, Entropy and Purity(Zhao and Karypis, 2001), are also employed inthis paper to measure our system performance.Entropy measures how the various classes ofword senses are distributed within each cluster,while Purity measures the extent to which eachcluster contained word senses from primarilyone class.
The entropy of cluster Sr is defined asrirci rirr nnnncSE ?=?=1loglog1)(The entropy of the entire clustering solution isthen defined to be the sum of the individual clus-ter entropies weighted according to the clustersize.
That is?==krrr SEnnEntropy1)(The purity of a cluster is defined to be)(max1)( irirr nnSP = ,which is the fraction of the overall cluster sizethat the largest class of examples assigned to thatcluster represents.
The overall purity of the clus-tering solution is obtained as a weighted sum ofthe individual cluster purities and is given by?==krrr SPnnPurity1)(In general, the larger the values of FScore andPurity, the better the clustering solution is.
Thesmaller the Entropy values, the better the cluster-ing solution is.The above three metrics are defined to evalu-ate the result of a single target word.
Macro av-erage metrics are used to evaluate the overallperformance of all the target words.4.3 ResultsThe overall performance on the trial data isshown in table 3.
From the Macro Average En-tropy and Macro Average Purity, we can see thatk-means works better than agglomerativemethod.
The detailed results of the k-means sys-tem are shown in table 4.Table 3.
Result comparison on the trial dataEntropy Purityk-means 0.4858 0.8288agglomerative 0.5328 0.8020Table 4.
Detailed results of k-means systemTargetWord SenseNum Entropy Purity??
2 0.855 0.72??
2 0.692 0.78??
2 0.377 0.92??
3 0.207 0.94??
2 0.833 0.7??
2 0 1??
2 0.592 0.82??
2 0.245 0.959??
2 0.116 0.98??
3 0.396 0.82??
2 0.201 0.96??
2 0.201 0.96??
3 0.181 0.9??
2 0.122 0.98??
2 0.327 0.92??
2 0.653 0.82??
2 0 1??
2 0.855 0.72??
2 0.5 0.8??
2 0.312 0.92??
2 0.519 0.86??
3 0.534 0.72??
2 0.846 0.7?
21 0.264 0.48??
2 0.521 0.88??
3 0 1??
2 0.76 0.78??
3 0.205 0.92??
2 0.854 0.72??
2 0.449 0.9??
2 0.467 0.9??
2 0.881 0.7??
2 0.402 0.92??
2 0.39 0.92??
2 0.793 0.76??
2 0.904 0.68??
2 0.943 0.64??
3 0.548 0.74??
2 0.583 0.86??
2 0.999 0.52??
2 0.242 0.96??
2 0.75 0.74??
3 0.464 0.84??
2 0.181 0.96??
2 0.672 0.78??
2 0.471 0.82??
3 0.543 0.7??
2 0.347 0.9??
4 0.508 0.66??
2 0.583 0.86The official results on the test set are shown intable 5.
Our k-means system and agglomerativesystem rank 5 and 8 respectively among all the18 systems.Table 5.
System rankingRank FScore Rank FScore1 0.7933 6 0.77882 0.7895 7 0.77293 0.7855 8* 0.76514 0.7849 9 0.75985* 0.7812 18 0.57895 Conclusions and Future WorkThis paper tries to build basic systems for Chi-nese Word Sense Induction (CWSI) task.
Basicclustering algorithms including k-means andagglomerative methods are studied.
No extralanguage resources are used except the datagiven by the task organizers.To improve the performance of CWSI sys-tems, we will introduce new features and studynovel clustering algorithms.
We will also inves-tigate the bakeoff data sets to find some morecharacteristics of Chinese word senses.AcknowledgementsThe authors are grateful to the organizers of theWord Sense Induction task for their hard work toprovide such a good research platform.
Thework in this paper is supported by grants fromthe National Natural Science Foundation ofChina (No.60773173, No.60970083).ReferencesD.
Vickrey, L. Biewald, M. Teyssler, and D. Koller.2005.
Word sense disambiguation for machinetranslation.
In Proceedings of HLT/EMNLP2005,pp.
771-778.E.
Agirre and A. Soroa.
2007.
Semeval-2007 task 02:Evaluating word sense induction and discrimina-tion systems.
In Proceedings of SemEval2007, pp.7-12.G.
Karypis.
2002.
CLUTO - a clustering toolkit.Technical Report 02-017, Dept.
of Computer Sci-ence, University of Minnesota.
Available athttp://www.cs.umn.edu?cluto.H.
Zhu.
2009.
Research into Automatic Word SenseDiscrimination on Chinese.
PhD Dissertation ofPeking University.J.
Y. Chai and A. W. Biermann.
1999.
The use ofword sense disambiguation in an information ex-traction system.
In Proceedings of AAAI/IAAI1999,pp.
850-855.M.
Denkowski.
2009.
A Survey of Techniques forUnsupervised Word Sense Induction.
Language &Statistics II Literature Review.O.
Uzuner, B. Katz, and D. Yuret.
1999.
Word sensedisambiguation for information retrieval.
In Pro-ceedings of AAAI/IAAI1999, pp.985.S.
Manandhar and I. P. Klapaftis.
2010.
SemEval-2010 Task 14: Evaluation Setting forWord SenseInduction &Disambiguation Systems.
In Proceed-ings of SemEval2010, pp.
117-122.Y.
Zhao and G. Karypis.
2005.
Hierarchical cluster-ing algorithms for document datasets.
Data Miningand Knowledge Discovery, 10(2):141?168.Y.
Zhao and G. Karypis.
2001.
Criterion functions fordocument clustering: Experiments and analysis.Technical Report 01?40, Dept.
of Computer Sci-ence, University of Minnesota.
Available athttp://cs.umn.edu/?karypis/publications.
