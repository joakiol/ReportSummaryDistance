PhraseNet:Towards Context Sensitive Lexical Semantics?Xin Li?, Dan Roth?, Yuancheng Tu?Dept.
of Computer Science?Dept.
of Linguistics?University of Illinois at Urbana-Champaign{xli1,danr,ytu}@uiuc.eduAbstractThis paper introduces PhraseNet, a context-sensitive lexical semantic knowledge base sys-tem.
Based on the supposition that seman-tic proximity is not simply a relation betweentwo words in isolation, but rather a relationbetween them in their context, English nounsand verbs, along with contexts they appear in,are organized in PhraseNet into Consets; Con-sets capture the underlying lexical concept, andare connected with several semantic relationsthat respect contextually sensitive lexical infor-mation.
PhraseNet makes use of WordNet asan important knowledge source.
It enhancesa WordNet synset with its contextual informa-tion and refines its relational structure by main-taining only those relations that respect con-textual constraints.
The contextual informa-tion allows for supporting more functionali-ties compared with those of WordNet.
Nat-ural language researchers as well as linguistsand language learners can gain from accessingPhraseNet with a word token and its context, toretrieve relevant semantic information.We describe the design and construction ofPhraseNet and give preliminary experimentalevidence to its usefulness for NLP researches.1 IntroductionProgress in natural language understanding research ne-cessitates significant progress in lexical semantics andthe development of lexical semantics resources.
Ina broad range of natural language applications, from?Research supported by NSF grants IIS-99-84168,ITR-IIS-00-85836 and an ONR MURI award.Names of authors are listed alphabetically.prepositional phrase attachment (Pantel and Lin, 2000;Stetina and Nagao, 1997), co-reference resolution (Ngand Cardie, 2002) to text summarization (Saggion andLapalme, 2002), semantic information is a necessarycomponent in the inference, by providing a level of ab-straction that is necessary for robust decisions.Inducing that the prepositional phrase in ?They atea cake with a fork?
has the same grammaticalfunction as that in ?They ate a cake with aspoon?, for example, depends on the knowledge that?cutlery?
and ?tableware?
are the hypernyms of both?fork?
and ?spoon?.
However, the noun ?fork?
has fivesenses listed in WordNet and each of them has severaldifferent hypernyms.
Choosing the correct one is a con-text sensitive decision.WordNet (Fellbaum, 1998), a manually constructedlexical reference system provides a lexical database alongwith semantic relations among the lexemes of Englishand is widely used in NLP tasks today.
However, Word-Net is organized at the word level, and at this level, En-glish suffers ambiguities.
Stand-alone words may haveseveral meanings and take on relations (e.g., hypernyms,hyponyms) that depend on their meanings.
Consequently,there are very few success stories of automatically us-ing WordNet in natural language applications.
In manycases, reported (and unreported) problems are due to thefact that WordNet enumerates all the senses of polyse-mous words; attempts to use this resource automaticallyoften result in noisy and non-uniform information (Brilland Resnik, 1994; Krymolowski and Roth, 1998).PhraseNet is designed based on the assumption that,by and large, semantic ambiguity in English disappearswhen local context of words is taken into account.
Itmakes use of WordNet as an important knowledge sourceand is generated automatically using WordNet and ma-chine learning based processing of large English corpora.It enhances a WordNet synset with its contextual informa-tion and refines its relational structure, including relationssuch as hypernym, hyponym, antonym and synonym, bymaintaining only those links that respect contextual con-straints.
However, PhraseNet is not just a functional ex-tension of WordNet.
It is an independent lexical semanticsystem allied with proper user interfaces and access func-tions that will allow researchers and practitioners to useit in applications.As stated before, PhraseNet, is built on the assumptionthat linguistic context is an indispensable factor affectingthe perception of a semantic proximity between words.In its current design, PhraseNet defines ?context?
hierar-chically with three abstraction levels: abstract syntacticskeletons, such as[(S)?
(V )?
(DO)?
(IO)?
(P )?
(N)]which stands for Subject, Verb, Direct Object, Indi-rect Object, Preposition and Noun(Object) of the Prepo-sition, respectively; syntactic skeletons whose compo-nents are enhanced by semantic abstraction, such as[Peop ?
send ?
Peop ?
gift ?
on ?
Day] and fi-nally concrete syntactic skeletons from real sentences as[they ?
send?mom?
gift?
on?
Christmas].Intuitively, while ?candle?
and ?cigarette?
would scorepoorly on semantic similarity without any contextual in-formation, their occurrence in sentences such as ?Johntried to light a candle/cigarette?
mayhighlight their connection with the process of burning.PhraseNet captures such constraints from the contextualstructures extracted automatically from natural languagecorpora and enumerates word lists with their hierarchicalcontextual information.
Several abstractions are made inthe process of extracting the context in order to preventsuperfluous information and support generalization.The basic unit in PhraseNet is a conset, a word in itscontext, together with all relations associated with it.
Inthe lexical database, consets are chained together via theirsimilar or hierarchical contexts.
By listing every contextextracted from large corpora and all the generalized con-texts based on those attested sentences, PhraseNet willhave much more consets than synsets in WordNet.
How-ever, the organization of PhraseNet respects the syntacticstructure together with the distinction of senses of eachword in its corresponding contexts.For example, rather than linking all hypernyms of apolysemous word to a single word token, PhraseNet con-nects the hypernym of each sense to the target word inevery context that instantiates that sense.
While in Word-Net every word has an average of 5.4 hypernyms, inPhraseNet, the average number of hypernyms of a wordin a conset is 1.51.In addition to querying WordNet semantic relationsto disambiguate consets, PhraseNet alo maintains fre-1The statistics is taken over 200, 000 words from a mixedcorpus of American English.quency records of each word in its context to help dif-ferentiate consets and makes use of defined similarity be-tween contexts in this process 2.Several access functions are built into PhraseNet thatallow retrieving information relevant to a word and itscontext.
When accessed with words and their contextualinformation, the system tends to output more relevant se-mantic information due to the constraint set by their syn-tactic contexts.While still in preliminary stages of development andexperimentation and with a lot of functionalities stillmissing, we believe that PhraseNet is an important efforttowards building a contextually sensitive lexical semanticresource, that will be of much value to NLP researchersas well as linguists and language learners.The rest of this paper is organized as follows.
Sec.
2presents the design principles of PhraseNet.
Sec.
3 de-scribes the construction of PhraseNet and the currentstage of the implementation.
An application that pro-vides a preliminary experimental evaluation is describedin Sec.
4.
Sec.
5 discuses some related work on lexical se-mantics resources and Sec.
6 discusses future directionswithin PhraseNet.2 The Design of PhraseNetContext is one important notion in PhraseNet.
While thecontext may mean different things in natural language,many previous work in statistically natural language pro-cessing defined ?context?
as an n-word window aroundthe target word (Gale et al, 1992; Brown et al, 1991;Roth, 1998).
In PhraseNet, ?context?
has a more precisedefinition that depends on the grammatical structure of asentence rather than simply counting surrounding words.We define ?context?
to be the syntactic structure of thesentence in which the word of interest occurs.
Specif-ically, we define this notion at three abstraction levels.The highest level is the abstract syntactic skeleton of thesentence.
That is, it is in the form of the different combi-nations of six syntactic components.
Some componentsmay be missing as long as the structure is from a legit-imate English sentence.
The most complete form of theabstract syntactic skeleton is:[(S)?
(V )?
(DO)?
(IO)?
(P )?
(N)] (1)which captures all of the six syntactic components suchas Subject, Verb, Direct Object, Indirect Object, Prepo-sition and Noun(Object) of Preposition, respectively, inthe sentence.
And all components are assumed to bearranged to obey the word order in English.
The low-est level of contexts is the concrete instantiation of thestated syntactic skeleton, such as [Mary(S)?give(V )?John(DO) ?
gift(IO) ?
on(P ) ?
birthday(N)] and2See details in Sec.
3[I(S)?
eat(V )?
bread(DO)?
with(P )?
hand(N)]which are extracted directly from corpora with grammat-ical lemmatization done during the process.
Therefore,all word tokens are in their lemma format.
The middlelayer(s) consists of generalized formats of the syntacticskeleton.
For example, the first example given above canbe generalized as [Peop(S)?give(V )?Peop(DO)?Possession(IO) ?
on(P ) ?Day(N)] by replacingsome of its components with more abstract semantic con-cepts.PhraseNet organizes nouns and verbs into ?consets?and a ?conset?
is defined as a context with all itscorresponding pointers (edges) to other consets.
Thecontext that forms a conset can be either directly ex-tracted from the corpus, or at a certain level of ab-straction.
For example, both [Mary(S) ?
eat(V ) ?cake(DO) ?
on(P ) ?
birthday(N), {p1, p2, .
.
.
, pn}]and [Peop(S) ?
eat(V ) ?
Food(DO) ?
on(P ) ?Day(N), {p1, p2, .
.
.
, pn}] are consets.Two types of relational pointers are defined currentlyin PhraseNet: Equal and Hyper.
Both of these two re-lations are based on the context of each conset.
Equalis defined among consets with same number of compo-nents and same syntactic ordering, i.e, some contextsunder the same abstract syntactic structure (the highestlevel of context as defined in this paper).
It is definedthat the Equal relation exists among consets whose con-texts are with same abstract syntactic skeleton, if there isonly one component at the same position that is differ-ent.
For example, [Mary(S)?give(V )?John(DO)?gift(IO)?on(P )?birthday(N), {p1, p2, .
.
.
, pn}] and[Mary(S) ?
send(V ) ?
John(DO) ?
gift(IO) ?on(P ) ?
birthday(N), {p1, p2, .
.
.
, pk}] are equal be-cause the syntactic skeleton each of them has is thesame, i.e., [(S) ?
(V ) ?
(DO) ?
(IO) ?
(P ) ?
(N)]and except one word in the verb position that is differ-ent, i.e., ?give?
and ?send?, all other five componentsat the corresponding same position are the same.
TheEqual relation is transitive only with regard to a spe-cific component in the same position.
For example,to be transitive to the above two example consets, theEqual conset should be also different from them onlyby its verb.
The Hyper relation is also defined for con-sets with same abstract syntactic structure.
For consetA and conset B, if they have the same syntactic struc-ture, and if there is at least one component of the con-text in A that is the hypernym of the component in thatof B at the corresponding same position, and all othercomponents are the same respectively, A is the Hyperconset of B.
For example, both [Molly(S) ?
hit(V ) ?Body(DO), {p1, p2, .
.
.
, pj}] and [Peop(S)?hit(V )?Body(DO), {p1, p2, .
.
.
, pn}] are Hyper consets of[Molly(S)?hit(V )?nose(DO), {p1, p2, .
.
.
, pk}].
Theintuition behind these two relations is that the Equal rela-Figure 1: The basic organization of PhraseNet: The upwardarrow denotes the Hyper relation and the dotted two-way arrowwith a V above denotes the Equal relation that is transitive withregard to the V component.tion can cluster a list of words which occur in exactly thesame contextual structure and if the extreme case occurs,namely when the same context in all these equal consetswith regard to a specific syntactic component groups vir-tually any nouns or verbs, the Hyper relation can be usedhere for further disambiguation.To summarize, PhraseNet can be thought of as a graphon consets.
Each node is a context and edges betweennodes are relations defined by the context of each node.They are either Equal or Hyper.
Equal relation can bederived by matching consets and it is easy to implementwhile building the Hyper relation requires the assistanceof WordNet and the defined Equal relation.
Semantic re-lations among words can be generated using the two typesof defined edges.
For example, it is likely that the targetwords in all equal consets with transitivity have similarmeaning.
If this is not true at the lowest lower of contexts,it is more likely to be true at higher, i.e., more generalizedlevel.
Figure 1 shows a simple example reflecting the pre-liminary design of PhraseNet.After we get the similar meaning lists based on theircontexts, we can build interaction from this word list toWordNet and inherit other semantic relations from Word-Net.
However, each member of a word list can help to dis-ambiguate other members in this list.
Therefore, it is ex-pected that with the pruning assisted by list members, i.e.,the disambiguation by truncating semantic relations asso-ciated with each synset in WordNet, the extract meaningin the context together with all other semantic relationssuch as hypernyms, holonyms, troponyms, antonyms canbe derived from WordNet.In the next two sections we describe our current im-plementation of these operations and preliminary experi-ments we have done with them.2.1 Accessing PhraseNetRetrieval of information from PhraseNet is done via sev-eral access functions that we describe below.
PhraseNetis designed to be accessed via multiple functions withflexible input modes set by the user.
These functionsmay allow users to exploit several different functionali-ties of PhraseNet, depending on their goal and amount ofresources they have.An access function in PhraseNet has two components.The first component is the input, which can vary froma single word token to a word with its complete con-text.
The second component is the functionality, whichranges over simple retrieval and several relational func-tions, modelled after WordNet relations.The most basic and simplest way to query PhraseNetis with a single word.
In this case, the system outputs allcontexts the word can occur in, and its related words ineach context.PhraseNet can also be accessed with input that consistsof a single word token along with its context information.Context information refers to any of the elements in thesyntactic skeleton defined in Eq.
1, namely, Subject(S),Verb(V), Direct Object(DO), Indirect Object(IO), Prepo-sition(P) and Noun(Object) of the Preposition(N).
Thecontextual roles S, V, DO, IO, P or N or any subset ofthem, can be specified by the user or derived by an appli-cation making use of a shallow or full parser.
The moreinformation the user provides, the more specific the re-trieved information is.To ease the requirements from the user, say, in caseno information of this form is available to the user,PhraseNet will, in the future, have functions that allow auser to supply a word token and some context, where thefunctionality of the word in the context is not specified.See Sec.
6 for a discussion.Function Name Input Variables OutputPN WL Word [, Context] Word ListPN RL Word [, Context] WordNet relationsPN SN Word [, Context] SensePN ST Context SentenceTable 1: PhraseNet Access Functions: PhraseNet accessfunctions along with their input and output.
[i] denotes optionalinput.
PN RL is a family of functions, modelled after WordNetrelations.Table 1 lists the functionality of the access functions inPhraseNet.
If the user only input a word token withoutany context, all those designed functions will return eachcontext the input word occurs together with the wordlistin these contexts.
Otherwise, the output is constrained bythe input context.
The functions are described below:PN WL takes the optional contextual skeleton and onespecified word in that context as inputs and returnsthe corresponding wordlist occurring in that contextor a higher level of context.
A parameter to thisfunction specifies if we want to get the completewordlist or those words in the list that satisfy a spe-cific pruning criterion.
(This is the function used inthe experiments in Sec.
4.
)PN RL is modelled after the WordNet access functions.It will return all words in those contexts that arelinked in PhraseNet by their Equal or Hyper rela-tion.
Those words can help to access WordNet toderive all lexical relations stored there.PN SN is modelled after the semantic concordancein (Landes et al, 1998).
It takes a word token andan optional context as input, and returns the senseof the word in that context.
Similarly to PN RL thisfunction is implemented by appealing to WordNetsenses and pruning the possible sense based on thewordlist determined for the given context.PN ST is not implemented at this point, but is designedto output a sentence that has same structure as theinput context, but use different words.
It is inspiredby the work on reformulation, e.g., (Barzilay andMcKeown, 2001).We can envision many ways users of PhraseNet canmake use of the retrieved information.
At this point in thelife of PhraseNet we focus mostly on using PhraseNet asa way to acquire semantic features to aid learning basednatural language applications.
This determines our prior-ities in the implementation that we describe next.3 Constructing PhraseNetConstructing PhraseNet involves three main stages: (1)extracting syntactic skeletons from corpora, (2) con-structing the core element in PhraseNet: consets, and (3)developing access functions.The first stage makes use of fully parsed data.
Inconstructing the current version of PhraseNet we usedtwo corpora.
The first, relatively small corpus of the1.1 million-word Penn-State Treebank which consistsof American English news articles (WSJ), and is fullyparsed.
The second corpus has about 5 million sentencesof the TREC-11 (Voorhees, 2002), also containing mostlyAmerican English news articles (NYT, 1998) and parsedwith Dekang Lin?s minipar parser (Lin, 1998a).In the near future we are planning to construct a muchlarger version of PhraseNet, using Trec-10 and Trec-11data sets, which cover about 8 GB of text.
We believe thatthe size is very important here, and will add significantrobustness to our results.To reduce ostensibly different contexts, two importantabstractions take place at this stage.
(1) Syntactic lemma-tization to get the lemma for both nouns and verbs inthe context defined in Eq.
1.
For data parsed via Lin?sminipar, the lexeme of each word is already includedin the parser.
(2) Sematic categorization to unify pro-nouns, proper names of people, locations and organiza-tion as well as numbers.
This semantic abstraction cap-tures the underlying semantic proximity by categorizingmultitudinous surface-form proper names into one repre-senting symbol.While the first abstraction is simple the second is not.At this point we use an NE tagger we developed our-selves based on the approach to phrase identification de-veloped in (Punyakanok and Roth, 2001).
Note that thisabstraction handles multiword phrases.
While the accu-racy of the NE tagger is around 90%, we have yet to ex-periment with the implication of this additional noise onPhraseNet.At the end of this stage, each sentence in the originalcorpora is transformed into a single context either atthe lowest level or a more generalized instantiation(with name entity tagged).
For example, ?For sixyears, T. Marshall Hahn Jr. has madecorporate acquisitions in the GeorgeBush mode: kind and gentle.
?, changes to:[Peop?make?
acquisition?
in?mode].The second stage of constructing PhraseNet concen-trates on constructing the core element in PhraseNet:consets.To do that, for each context, we collect wordlists thatcontain those words that we determine to be admissible inthe context(or contexts share the equal relation).
The firststep in constructing the wordlists in PhraseNet is to fol-low the most strict definition ?
include those words thatactually occur in the same context in the corpus.
This in-volves all Equal consets with the transitive property toa specific syntactic component.
We then apply to thewordlists three types of pruning operations that are basedon (1) frequency of word occurrences in identical or simi-lar contexts; (2) categorization of words in wordlist basedon clustering all contexts they occur in, and (3) pruningvia the relational structure inherited from WordNet - weprune from the wordlist outliers in terms of this relationalstructure.
Some of these operations are parameterizedand determining the optimal setting is an experimentalissue.1.
Every word in a conset wordlist has a frequencyrecord associated with it, which records the fre-quency of the word in its exact context.
We prunewords with a frequency below k (with the currentcorpus we choose k = 3).
A disadvantage ofthis pruning method is that it might filter out someappropriate words with a low frequency in reality.For example, for the partial context [strategy ?involve?
?
?
?
?
?
], we have:[strategy - involve - * - * - *, < DO : advertisement4, abuse 1, campaign 2, compromise 1, everything 1,fumigation 1, item 1, membership 1, option 3, stock-option 1> ]In this case,?strategy?
is the subject and ?involve?is the predicate and all words in the list serve as thedirect object.
The number in the parentheses is thefrequency of the token.
With k = 3 we actually getas a wordlist only: < advertisment, option >.2.
There are several ways to prune wordlists based onthe different contexts words may occur in.
This in-volves a definition of similar contexts and threshold-ing based on the number of such contexts a word oc-curs in.
At this point, we implement the constructionof PhraseNet using a clustering of contexts, as donein (Pantel and Lin, 2002).
An exhaustive PhraseNetlist is intersected with word lists generated based onclustered contexts given by (Pantel and Lin, 2002).3.
We prune from the wordlist outliers in terms of therelational structure inherited from WordNet.
Cur-rently, this is implemented only using the hypernymrelation.
The hypernym shared by the highest num-ber of words in the wordlist is kept in the database.For example, by searching ?option?
in WordNet, weget its three senses.
Then we collect the hypernymsof ?option?
from all the senses as follows:05319492(a financial instrument whose value isbased on another security)04869064(the cognitive process of reaching a deci-sion)00026065(something done)We do this for every word in the original list and findout the hypernym(s) shared by the highest number ofwords in the original wordlist.
The final pick in thiscase is the synset 05319492 which is shared by both?option?
and ?stock option?
as their hypernym.The third stage is to develop the access functions.
Asmentioned before, while we envision many ways usersof PhraseNet can use the retrieved information, at thispreliminary stage of PhraseNet we focus mostly on us-ing PhraseNet as a way to supply abstract semantic fea-tures that learning based natural language applicationscan benefit from.For this purpose, so far we have only used and evalu-ated the function PN WL.
PN WL takes as input asspecific word and (optionally) its context and returns alists of words which are semantically related to the targetword in the given context.
For example,PN WL ( V= protest, [peop - legislation - * - * - * ])=[protest, resist, dissent, veto, blackball, negative, for-bid, prohibit, interdict, proscribe, disallow ].This function can be implemented via any of the threepruning methods discussed earlier (see Sec.
4).
Thiswordlists that this function outputs, can be used to aug-ment feature based representations for other, learningbased, NLP tasks.
Other access functions of PhraseNetcan serve in other ways, e.g., expansions in informationretrieval, but we have not experimented with it yet.With the experiments we are doing right now,PhraseNet only takes inputs with the context informationin the format of Eq.
1.
Semantic categorization and syn-tactic lemmatization of the context is required in order toget matched in the database.
However, PhraseNet will,in the future, have functions that allow a user to supply aword token and more flexible contexts.4 Evaluation and ApplicationIn this section we provide a first evaluation of PhraseNet.We do that in the context of a learning task.Learning tasks in NLP are typically modelled as clas-sification tasks, where one seeks a mapping g : X ?c1, ..., ck, that maps an instance x ?
X (e.g., a sentence)to one of c1, ..., ck ?
representing some properties of theinstance (e.g., a part-of-speech tag of a word in the con-text of the sentence).
Typically, the raw representation?
sentence or document ?
are first mapped to some fea-ture based representation, and then a learning algorithmis applied to learn a mapping from this representation tothe desired property (Roth, 1998).
It is clear that in mostcases representing the mapping g in terms of the raw rep-resentation of the input instance ?
words and their order?
is very complex.
Functionally simple representationsof this mapping can only be formed if we augment theinformation that is readily available in the input instancewith additional, more abstract information.
For exam-ple, it is common to augment sentence representationswith syntactic categories ?
part-of-speech (POS), underthe assumption that the sought-after property, for whichwe seek the classifier, depends on the syntactic role of aword in the sentence rather than the specific word.
Sim-ilar logic can be applied to semantic categories.
In manycases, the property seems not to depend on the specificword used in the sentence ?
that could be replaced with-out affecting this property ?
but rather on its ?meaning?.In this section we show the benefit of using PhraseNetin doing that in the context of Question Classification.Question classification (QC) is the task of determiningthe semantic class of the answer of a given question.For example, given the question: ?What Cubandictator did Fidel Castro force outof power in 1958??
we would like to determinethat its answer should be a name of a person.
Ourapproach to QC follows that of (Li and Roth, 2002).The question classifier used is a multi-class classifierwhich can classify a question into one of 50 fine-grainedclasses.The baseline classifier makes use of syntactic featureslike the standard POS information and information ex-tracted by a shallow parser in addition to the words inthe sentence.
The classifier is then augmented with stan-dard WordNet or with PhraseNet information as follows.In all cases, words in the sentence are augmented withadditional words that are supposed to be semantically re-lated to them.
The intuition, as described above, is thatthis provides a level of abstract ?
we could have poten-tially seen an equivalent question, where other ?equiva-lent?
words occur.For WordNet, for each word in a question, all its hyper-nyms are added to its feature based representation (in ad-dition to the syntactic features).
For PhraseNet, for eachword in a question, all the words in the correspondingconset wordlist are added (where the context is suppliedby the question).Our experiments compare the three pruning operationsdescribed above.
Training is done on a data set of 21,500questions.
Performance is evaluated by the precision ofclassifying 1,000 test questions, defined as follows:Precison = # of correct predictions# of predictions (2)Table 2 presents the classification precision before andafter incorporating WordNet and PhraseNet informationinto the classifier.
By augmenting the question classi-fier with PhraseNet information, even in this preliminarystage, the error rate of the classifier can be reduced by12%, while an equivalent use of WordNet information re-duces the error by only 5.7%.Information Used Precision Err ReductionBaseline 84.2% 0%WordNet 85.1% 5.7%PN: Freq.
based Pruning 84.4% 1.3%PN: Categ.
based Pruning 85% 5.1%PN: Relation based Pruning 86.1% 12%Table 2: Question Classification with PhraseNet Informa-tion Question classification precision and error rate reductioncompared with the baseline error rate(15.8%) by incorporat-ing WordNet and PhraseNet(PN) information.
?Baseline?
isthe classifier that uses only syntactic features.
The classifieris trained over 21,500 questions and tested over 1000 TREC 10and 11 questions.5 Related WorkIn this section we point to some of the related workon syntax, semantics interaction and lexical semantic re-sources in computational linguistics and natural languageprocessing.
Many current syntactic theories make thecommon assumption that various aspects of syntactic al-ternation are predicable via the meaning of the predi-cate in the sentence (Fillmore, 1968; Jackendoff, 1990;Levin, 1993).
With the resurgence of lexical seman-tics and corpus linguistics during the past two decades,this so-called linking regularity triggers a broad interestof using syntactic representations illustrated in corporato classify lexical meaning (Baker et al, 1998; Levin,1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin,1998b; Pantel and Lin, 2002).FrameNet (Baker et al, 1998) produces a seman-tic dictionary that documents combinatorial propertiesof English lexical items in semantic and syntactic termsbased on attestations in a very large corpus.
In FrameNet,a frame is an intuitive structure that formalizes the linksbetween semantics and syntax in the results of lexicalanalysis.
(Fillmore et al, 2001) However, instead of de-rived via attested sentences from corpora automatically,each conceptual frame together with all its frame ele-ments has to be constructed via slow and labor-intensivemanual work.
FrameNet is not constructed automaticallybased on observed syntactic alternations.
Though deepsemantic analysis is built for each frame, lack of auto-matic derivation of the semantic roles from large corpora3confines the usage of this network drastically.Levin?s classes (Levin, 1993) of verbs are based on theassumption that the semantics of a verb and its syntacticbehavior are predictably related.
She defines 191 verbclasses by grouping 4183 verbs which pattern togetherwith respect to their diathesis alternations, namely alter-nations in the expressions of arguments.
In Levin?s clas-sification, it is the syntactic skeletons (such as np-v-np-pp)to classify verbs directly.
Levin?s classification is val-idated via experiments done by (Dorr and Jones, 1996)and some counter-arguments are in (Baker and Ruppen-hofer, 2002).
Her work provides a a small knowledgesource that needs further expansion.Lin?s work (Lin, 1998b; Pantel and Lin, 2002) makesuse of distributional syntactic contextual information todefine semantic proximity.
Dekang Lin?s grouping ofsimilar words is a combination of the abstract syntacticskeleton and concrete word tokens.
Lin uses syntactic de-pendencies such as ?Subj-people?, ?Modifier-red?, whichcombine both abstract syntactic notations and their con-crete word token representations.
He applies this methodto classifying not only verbs, but also nouns and adjec-tives.
While no evaluation has ever been done to deter-mine if concrete word tokens are necessary when the syn-tactic phrase types are already presented, Lin?s work in-directly shows that the concrete lexical representation iseffective.WordNet (Fellbaum, 1998) by far is the most widelyused semantic database.
However, this database does not3The attempt to label these semantic roles automatically in(Gildea and Jurafsky, 2002) assumes knowledge of the frameand covers only 20% of them.always work as successfully as researchers have expected(Krymolowski and Roth, 1998; Montemagni and Pirelli,1998).
This seems to be due to lack of topical context(Harabagiu et al, 1999; Agirre et al, 2001) as well aslocal context (Fellbaum, 1998).
By adding contextual in-formation, many researchers, (e.g., (Green et al, 2001;Lapata and Brew, 1999; Landes et al, 1998)), have al-ready made some improvements over it.The work on the importance of connecting syntax andsemantics in developing lexical semantic resources showsthe importance of contextual information as a step to-wards deeper level of processing.
With hierarchical sen-tential local contexts embedded and used to categorizeword classes automatically, we believe that PhraseNetprovides the right direction for building useful lexical se-mantic database.6 Discussion and Further WorkWe believe that progress in semantics and in develop-ing lexical resources is a prerequisite to any signifi-cant progress in natural language understanding.
Thiswork makes a step in this direction by introducing acontext-sensitive lexical semantic knowledge base sys-tem, PhraseNet.
We have argued that while cur-rent lexical resources like WordNet are invaluable, weshould move towards contextually sensitive resources.PhraseNet is designed to fill this gap, and our preliminaryexperiments with it are promising.PhraseNet is an ongoing project and is still in its pre-liminary stage.
There are several key issues that we arecurrently exploring.
First, given that PhraseNet drawspart of it power from corpora, we are planning to en-large the corpus used.
We believe that the data sizeis very important and will add significant robustness toour current results.
At the same time, since construct-ing PhraseNet relies on machine learning techniques, weneed to study extensively the effect of tuning these onthe reliability of PhraseNet.
Second, there are severalfunctionalities and access functions that we are planningto augment PhraseNet with.
Among those is the abilityof allowing a user to query PhraseNet even without ex-plicitly specifying the role of words in the context.
Thiswould reduce the requirement for users and applicationsusing PhraseNet.
Finally, current PhraseNet has no lexi-cal information about adjectives and adverbs, which maycontain important distributional information about theirmodified nouns or verbs.
We would like to take this in-formation into consideration in the near future.ReferencesE.
Agirre, O. Ansa, D. Martinez, and E. Hovy.
2001.
Enrichingwordnet concepts with topic signatures.C.
Baker and J. Ruppenhofer.
2002.
Framenet?s frames vs.levin?s verb classes.
In Proceedings of the 28th Annual Meet-ing of the Berkeley Linguistics Society.C.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFrameNet project.
In Christian Boitet and Pete Whitelock,editors, Proceedings of the Thirty-Sixth Annual Meeting ofthe Association for Computational Linguistics and Seven-teenth International Conference on Computational Linguis-tics, pages 86?90, San Francisco, California.
Association forComputational Linguistics, Morgan Kaufmann Publishers.R.
Barzilay and K. R. McKeown.
2001.
Extracting paraphrasesfrom a parallel corpus.
In Proceeding of the 10th Conferenceof the European Chapter of ACL.E.
Brill and P. Resnik.
1994.
A rule-based approach to prepo-sitional phrase attachment disambiguation.
In Proc.
of COL-ING.P.
F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer.1991.
Word sense disambiguation using statistical methods.In Proceedings of ACL-1991.B.
Dorr and D. Jones.
1996.
Role of word-sense disambigua-tion in lexical acquisition.C.
Fellbaum.
1998.
In C. Fellbaum, editor, WordNet: An Elec-tronic Lexical Database.
The MIT Press.C.
J. Fillmore, C. Wooters, and C. F. Baker.
2001.
Buildinga large lexical databank which provides deep semantics.
InProceedings of the Pacific Asian Conference on Language,Information and Computation, HongKong.C.
J. Fillmore.
1968.
The case for case.
In Bach and Harms,editors, Universals in Linguistic Theory, pages 1?88.
Holt,Rinehart, and Winston, New York.W.
A. Gale, K. W. Church, and D. Jarowsky.
1992.
A methodfor disambiguation word senses in large corpora.
Computersand the Humanities, 26(5-6):415?439.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of se-mantic roles.
Computational Linguistics, 28(3):245?288,September.R.
Green, L. Pearl, B. J. Dorr, and P. Resnik.
2001.
Lexical re-source integration across the syntax-semantics interface.
InProceedings of WordNet and Other Lexical Resources Work-shop, NAACL, Pittsburg, June.S.
M. Harabagiu, G. A. Miller, and D. I. Moldovan.
1999.Wordnet2 - a morphologically and semantically enhanced re-sources.
In Proceedings of ACL-SIGLEX99: StandardizingLexical Resources, pages 1?8, Maryland.R.
Jackendoff.
1990.
Semantic Structures.
MIT Press, Cam-bridge, MA.Y.
Krymolowski and D. Roth.
1998.
Incorporating knowledgein natural language learning: A case study.
In COLING-ACL?98 workshop on the Usage of WordNet in Natural Lan-guage Processing Systems.S.
Landes, C. Leacock, and R. I. Tengi.
1998.
Building seman-tic concordances.
In C. Fellbaum, editor, WordNet: an Elec-toronic Lexical Database, pages 199?216.
The MIT Press.M.
Lapata and C. Brew.
1999.
Using subcategorization to re-solve verb class ambiguity.
In Proceedings of EMNLP, pages266?274.B.
Levin.
1993.
English Verb Classes and Alternations:A Preliminary Investigation.
University of Chicago Press,Chicago, IL.X.
Li and D. Roth.
2002.
Learning question classifiers.
InProceedings of COLING.D.
Lin.
1998a.
Dependency-based evaluation of minipar.
InIn Workshop on the Evaluation of Parsing Systems GranadaSpain.D.
Lin.
1998b.
An information-theoretic definition of similar-ity.
In Proc.
15th International Conf.
on Machine Learning,pages 296?304.
Morgan Kaufmann, San Francisco, CA.S.
Montemagni and V. Pirelli.
1998.
Augmenting WordNet-like lexical resources with distributional evidence.
anapplication-oriented perspective.
In S. Harabagiu, editor,Use of WordNet in Natural Language Processing Systems:Proceedings of the Conference, pages 87?93.
Association forComputational Linguistics.V.
Ng and C. Cardie.
2002.
Improving machine learning ap-proaches to coreference resolution.
In Proceedings of 40thAnnual Meeting of the ACL, TaiPei.P.
Pantel and D. Lin.
2000.
An unsupervised approach toprepositional phrase attachment using contextually similarwords.
In Proceedings of Association for ComputationalLinguistics, Hongkong.P.
Pantel and D. Lin.
2002.
Discovering word senses from text.In The Eighth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining.V.
Punyakanok and D. Roth.
2001.
The use of classifiers insequential inference.
In NIPS-13; The 2000 Conference onAdvances in Neural Information Processing Systems, pages995?1001.
MIT Press.D.
Roth.
1998.
Learning to resolve natural language ambigu-ities: A unified approach.
In Proc.
National Conference onArtificial Intelligence, pages 806?813.H.
Saggion and G. Lapalme.
2002.
Generating indicative-informative summaries with sumum.
Computational Lin-guistics, 28(4):497?526.J.
Stetina and M. Nagao.
1997.
Corpus based pp attachmentambiguity rosolution with a semantic dictionary.
In Proceed-ings of the 5th Workshop on Very Large Corpora, Beijing andHongkong.E.
Voorhees.
2002.
Overview of the TREC-2002 question an-swering track.
In The Eleventh TREC Conference, pages115?123.
