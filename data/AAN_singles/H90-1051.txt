Using Explanation-Based Learning to IncreasePerformance in a Large-Scale NL Query SystemManny Rayner, Christer SamuelssonSwedish Institute of Computer ScienceBox 1263S-164 28 KISTA, Sweden1.
IntroductionExplanation-based learning (EBL) is a machine-learningtechnique, closely connected to other techniques like macro-operator learning, chunking, and partial evaluation; a phrasewe have found useful for describing the method to logicprogrammers is example-guided partial evaluation.
The basicideas of the method are well-described in an overview articlewhich recently appeared in Artificial Intelligence \[1\], towhich we refer the reader who wants to understand thetheoretical principles; here, we will only summarize brieflywhat EBL means in the context of natural-languageprocessing.
A detailed presentation can be found in \[3\] and\[4\].What EBL tries to do in the context of NLP is exploitthe well-known observation that users of an NL interfacetend to ask the same types of question most of the time;lacking exact figures, it seems reasonable to guess that atleast 80% of all questions posed to a given specific NLapplication will be accounted for by the 100 most commonquestion-types.
If one had some simple way ofautomatically identifying these "common" question-types, itwould be possible to win a great deal of efficiency by by-passing the normal parsing mechanism in all but the hardcases,Unfortunately, it is not feasible simply to add 100 extrarules to the grammar, since the common question-types varydepending on the application: a construction which occursconstantly in one domain may hardly exist in another.Something more sophisticated is required, which is capableof taking examples of common types of query andsynthesizing the corresponding special rules.
This is exactlywhat the EBL method offers.
The normal route through theparser is extended with an EBL bypass, which containsspecial rules for efficient processing of common queries;these rules are not coded by the programmer, but rather areproduced automatically by inspecting the solutions topreviously posed queries of the same type.
EBL can thusbest be thought of as a way of automatically tuning an NLsystem to produce increased performance in a particulardomain.The EBL module can consequently be divided into itscompile-time and run-time parts.
The compile-time partextracts the learned rules from sample queries: the centralcomponent is the generalizer, which in our version isessentially a type of Prolog interpreter.
The run-time partthen applies the rules to input queries, the compile-timesystem having previously indexed them so as to make themreadily accessible to some kind of table look-up facility.
In\[4\], we demonstrated, using examples taken from anapplication of the EBL method to CHAT-80 \[2\], that tablelook-up methods of this kind can be implemented quitesimply in Prolog with a minimal overhead.In the current paper, we describe the results ofexperiments carried out at IBM Nordic Laboratories, wherethe EBL method was used on a large-scale NL queryinterface prototype.
The EBL module learns a "two-lever'set of special grammar rules; the top-level rules for S's treatNP's as primitive, and these are supplemented by a secondset of rules for common NP's.
Both types of rules arelearned automatically in the way described above.
In theremainder of the paper, we first give a brief overview of theIBM system, concentrating on the features that presentedproblems for the implementation f the EBL process; wethen describe the architecture of the EBL module's compile-time and run-time components.
In section 4, we present ourexperimental results, which indicate fairly unambiguouslythat the EBL method gives a real, and quite substantial,speed-up of the system as a whole; the final section containsour conclusions together with suggested irections for?
further research.2.
Relevant characteristics of thetarget NL systemThe system used for our experiments was a large-scaleNL query prototype, implemented in Prolog, which isintended to provide good coverage of a fairly large portion ofEnglish.
The main components perform the tasks ofparsing, semantic interpretation, paraphrasing and databasequery generation; since the first of these is both the"cleanest" and by far the most time-consuming, we decidedonly to attempt to apply EBL to this phase of the process.We will thus concentrate xclusively in the followingdescription on the grammar formalism, grammar and parser.As explained in \[4\], the main difficulties derive from the factthat our implementation f the EBL method requires thegrammar to be reduced to a set of Horn-clauses: in ourearlier experiments with CHAT-80, this was fairly simple,and only involved some minor editing of the code.
Here,however, the gap between the grammar and an equivalent"clean" version was non-trivial.
This was much moreimportant than the mere increase in its size (~1000 rules, asopposed to 150 for CHAT-80), which in fact caused noproblems at all.251The two major hurdles with regard to the grammarformalism were its non-standard treatment of features andmovement.
The basic feature operation is not unification,but priority merge: movement is handled not by gapfeatures, but rather by "non-restrictive" rules, in which morethan one non-terminal can occur on the left-hand side of therule as well as the right.
Partly due to this, an unusualparsing mechanism is used, in which extra-logical predicates(especially "assert") play an integral part.
To give theflavour of the formalism, the following is a slightlymodified version of a typical non-restrictive rule, in thiscase intended to cover free relatives like the one in "Johnmentioned a book yesterday which you should read":s (2,prm=l, fpe (2)) &temp_advp (i, trm=l, fpe (i)) ->temp_advp (dng=0) & s (rel=l)The rule reverses the sequence of temporal adverbial andrelative clause, in effect transforming the sentence into"John mentioned a book which you should read yesterday".The "2" in the first argument position in the left-hand "s"indicates that its features are to be inherited from those inthe second constituent on the right-hand side; "pr ra=l"means that the "13 rra" feature in the inherited set will ifnecessary be overridden and set to 1.As we shall see in section 3.3, the parsing mechanismturns out to be irrelevant for our purposes; all that issignificant is the grammar, viewed as a declarativedescription.
We shall accordingly conclude our description ofthe target system at this point.3.
Design of the EBL module3.1 Overall architectureAs explained above, the EBL module can naturally bedivided into its compile-time and run-time components,which we will further describe in the following sections.For convenience, we will sub-divide the compile-timesystem into three smaller components.
These are thegrammar pre-processor, which converts the grammar into asuitable pure Horn-clause representation; the generalizer,which performs the actual extraction of learned rules; and thesimplifier, which attempts to reduce them in size byremoving unnecessary calls.
We now examine ach of thesein turn.3.2 The grammar pre-processorThis component performs the job of converting theoriginal grammar into a pure DCG form, in which the firstargument of each non-terminal contains a term encoding itsderivation history; the motivation for this additionalcondition will be apparent in the next section.
The onlynon-trivial part of the process, from our viewpoint, wasdealing with unrestricted rules, since the other problems hadalready been taken care of by the normal grammar compiler.However, it turned out that this problem could also besolved simply, by first representing the unrestricted rules inPereira's Extraposition Grammar (XG) format; using the XGcompiler from \[2\], it is then straight-forward to turn thegrammar into pure Horn-clauses.
Conceptually, the XGcompiler turns the ~mrestricted grammar into a DCG, whereeach non-terminal is given an extra pair of arguments (the"extraposition list"), to pass around the additional left-handconstituents.
To give an example, the rule quoted at the endof section 2 is represented (again in a slightly edited form)as follows:s(s(rulell2,S,T),Feats l,Sem i,X_in, x (nogap, nonterm~nal,t emp_advp (T, Feat s_2, Sere_2 ),X out) ) ->temp_advp (T, Feat s_3, X in, Xnext ) ,{get_feature (Feats_3, dng, 0) },s (S, Feats_4, X_next, X out),{get_feature (Feat s_4, rel, 1 ),put_feature (Feat s_3, prm, i, Feat s_l ),put feature (Feat s_4, trm, I, Feat s_2) }.The DCG produced can potentially contain left-recursiverules.
However, we shall see in the next section that thiscauses no problems, since it is not used for normal,unrestricted parsing; the non-terminating branches in thesearch space can thus never be entered.3.3 The generalizerSince a detailed escription of the generalizer can be foundin \[4\], we will restrict ourselves here to an example and abrief overview.
The basic idea is first to define the class ofoperational goals; by this, we mean the goals which will beallowed to appear on the right-hand-side of learned rules.Having done this, a successfully processed example isgeneralized by (notionally) constructing a derivation tree forit, and then chopping off all the branches rooted inoperational goals; the leaves in the new, "generalized"derivation will be the conditions in the learned rule (and thusby construction operational), and the root will be a moregeneral version of the goal corresponding to that in theexample.
In the simplest (one-level) version of the scheme,operational goals will coincide with lexical ones: thusgeneralization will be at the word level.
An illustrativeexample is shown in diagram 1.A slight refinement is to allow non-lexical operationalgoals, in particular ones corresponding to NP's.
The basicmethod can now be applied recursively, first to the prooftree corresponding to the entire example, and then to eachtree rooted in an operational NP goal; in the latter case, theoperationality criterion is once again lexical.
This results inthe acquisition of two sets of rules, corresponding to thetwo different operationality criteria: the top-level rulesconstruct S's from NP's and lexical items, and the second-level ones construct NP's from lexical items alone.252s (S) --> np(Agr,VP^S), vp(Agr,VP).
(i)np (3-s, NP) --> pn(NP) .
(2)np (Agr,NP) --> det (Agr,Ni^NP), n (Agr, Nl) .
(3)vp (Agr,X^S) --> tv(Agr, X^VP), np (_,VP^S).
(4)pn((PN^VP)^VP) --> \[PN\], {lex(PN, pn)}.
(5)det (Agr, (X^Si) ^ (X^S2) ^quant (Det,X, Si, S2) ) -->\[Det\], {lex(Det,det(Agr))}.
(6)n(Agr,X^\[N,X\]) --> \[N\], {lex(N,n(Agr))}.
(7)tv(Agr,X^yA\[TV, X,Y\]) --> \[TV\], {lex(TV, tv(Agr))}.
(8)lex (john, pn) .lex (cat, n (3-s)) .lex (a, det) .lex (loves, tv (3-s)) .Grammar and lexiconlex ( john, pn)(5)pn ( (john^VP) ^VP,\[ johnlR\],R)lex (loves, tv (3-s))(8)tv (3-S,X^Y ^  \[loves, X, Y\],lex (a, det (3-s)) lex (cat, n (3-s))det(3-s, (Y^Sl) ^(Y^S2) ^ n(3-s,Y ^\[cat,Y\],quant (a, Y, Sl, S2), \[cat }R\] ,R)\[loves I R\], R) \[al R\], R ~ ~np (3-s, (Y^S) ^quant (a,Y, \[cat,Y\],S),(2) ~ \[a, cat I R\] ,R)np (3-s, (john^VP) ^VP, ~ /\[ johnl R\], R)vp (3-s,X^quant (a,Y, \[cat,Y\], \[loves,X, Y\] ),\[ loves, a, cat I R\], R)s (quant (a,Y, \[cat, Y\], \[loves, john, Y\] ),\[ john, loves, a, cat\], \[ \] )Derivation of"John loves a cat"lex (A, pn) lex (B, tv (3-s))I(5) 1(8)tv (3-S, pn ((A^VP) ^VP,\[AIR\] ,R) X^Y^ \[B'X' Y\] '\[BIR\] ,R)np (Agr, (Y^S) ^quant (C,Y, \[D,Y\],S),(2) ~ \[C,DIR\] ,R) np (3-s, (A^VP) ^VP, ~ /\[AIR\] ,R)vp (3-s, X^quant (C,Y, \[D,Y\], , \[B,X,Y\] )\[B, C,D IR\], R)s (quant (C,Y, \[D,Y\], \[B,A,Y\] ),\[A,B,C,D\], \[\])Generalized derivation trees (quant (C,Y, \[D,Y\], \[B,A,Y\] ), \[A,B,C,D\], \[\] ) "-lex(A, pn), lex(B,tv(3-s)), lex(C, det), lex(D,n(Agr)) .Generalized erived ruleDiagram 1.
Example application of EBL to a toy logic grammar.lex (C, det (Agr)) lex (D, n (Agr))I (6) I(7)det (Agr, (X^SI) ^ (X^S2) ^ n (Agr,Y^ \[D,Y\],quant (C, X, SI, $2), \[DIR\],R)\[CIR\] ,R)253The generalizer is basically a Prolog meta-interpreter,which means that generalization is from a computationalperspective essentially the parsing of a query with a DCG;this means that care has to be taken to ensure that parsingefficiency is acceptably high, and even more importantlythat infinite recursions are not caused by left-recursivegrammar rules.
Luckily, there is a simple and uniform wayto solve this problem, by exploiting the fact that the firstargument in each rule has been set up to hold the derivationhistory.
The query is first run through the normal, "dirty"grammar, to find the intended instantiation of the derivationargument; his is then used to guide DCG parser used by thegeneralizer, effectively making the "parsing" deterministic.The top-level is thus schematically:extract  ru le  (Query,Rule) "-d i r ty_parse  (s (Tree, , ) ,Query),genera l i ze  (c lean_parse (s (Tree, , ),Query) ,Rule) .where the predicate names have their obvious meanings.3.4 The  s impl i f ie rThe purpose of this module is to attempt to reduce thesize of learned rules, in particular calls to feature-manipulation primitives; these make up most of the body oftypical rules with on average about 50 calls per rule.
Thebasic mechanism is to take each feature-value, and trace itsupdate history backwards through successive updates.Dividing feature-manipulation nto "gets" and "puts", wecan optimize in at least the following ways:- Removing "gets" which can already be seen at compile-time to succeed.
Since learned rules are compositions ofnormal ones, this case occurs when one component rule"gets" a feature that an earlier component has "put".- Removing duplicate copies, when the same "get" occursmore than once in the rule.- Reordering the rule body so that all structure-buildingtakes place at the end: this ensures that structure willonly be built if the rule succeeds.If features were only used for syntax, it would also bepossible to perform a further kind of optimization for S-level rules; having traced each "get" back through the chainof "puts" ending in the feature set it accesses, we could thenremove the "puts" altogether.
This would represent a veryconsiderable reduction in average rule-size.
Semanticprocessing in the target system is unfortunately notstructured so as to allow this, but we think it likely that themethod could be applicable in other, similar, contexts.The following pseudo-code characterizes the simplifica-tion algorithm:Phase 11.
Combine "gets" and "puts" accessing the same featureset into groups.
Replace ach group with acorresponding call to get_group or put_group.2.
Collect all calls to structure-building routines.Phase 2Go through the body of the rule, passing an alist ofannotations; this is used to replace or simplify calls to"get_group".
The alist associates with each feature set ahistory of its derivation.
This is one of?
p r imi t ive  (Const i tuent)  - the feature setis theone associated with Const i tuent .?
update_ f rom (Old_features,  Update_set)  -the feature set was derived from 01 d_  f e at u r e s bythe chain of updates Update_set .For each literal L in the rule body, do one of the following.i) If L is of the form put  group  (Old, Updates ,New ), then add a suitable entry to the alist, constructedfrom L and the derivation history of Old.ii) IlL is of the form getgroup (Feature_set ,Ac  c e s s i i st ), replace it with a literal of the formget_gro'up (Ori gi nal, Acce  s s_l i st_ l  ),where:a) Or ig ina l  is the base of the update chain thatFeature  set  belongs to.b) Access  l i s t_ l  is derived from Access_ l i s tas follows: for each element F=V, if F=VI  is in thelist of updates, unify V with V l  and throw awayF=V.iii) If L is of any other form, keep it unaltered.Phase 31.
Remove duplicate calls.2.
Re-expand calls to get_group and put_group.3.
Add structure-building calls to the end of the rule body.3.5.
The  pat tern -matcherSince the learned rules acquired by the generalizer ineffectcomprise a specialized grammar, it would be possible toapply the normal parsing mechanism to them.
However,this fails to exploit the grammar's unusually simplestructure: the depth of a derivation-tree cannot exceed two,and NP is the only non-lexical category.
Thinking about heproblem in this way should make the pattern-matcher'sconstruction easy to understand.
The rules are compiled intoa trie-structure, indexed by constituent category; this caneither be "NP", or some lexical category.
The pattern-matcher then locates potentially suitable rules by a kind ofnon-deterministic LR parsing method, driven by the trie-structure and otherwise optimized to exploit he peculiaritiesof the situation; a well-formed substring table is used toremember previously located NP's.
Our tests indicate thatthis method is at least five times faster than the targetsystem's normal parser.The following pseudo-code characterizes the algorithm.Positions in the input string are marked from 0 to *end* ;254* t r i e -  root*  denotes the root-node of the trie-structure;po in ter  marks the place we have reached in the inputstring, t r i e_no  de  the current position in the rule trie,and nps  the sequence of NP's so far located between 0 andp o i  n te  r.  We assume that lexical analysis has already beenperformed, so that we can discover by a suitable look-upoperation whether or not there is an item of a given lexicalcategory at a given location in the input string.Pattern-matching algorithm1.
Set po inter  to 0.
Set t r ie -node  to *tr ie-root*.2.
Set category  to the lexical category of the item atpointer.3.
Non-determinisfically do one of:a) If there is a tde arc from t r ie -node  to next-node triggering on category  then set t r ie-node to next-node.
Bump po inter  andgoback to 2.b) If there is atde arc from t r ie -node  to next-node triggering on "NP", and there is an NP  frompo inter  to next-pointer,  set t r ie-nodeto next-node, set po in ter  to next-pointer,  push the found NP  onto nps, and goback to 2.c) If po inter  = *end*, and t r ie -node is a leafof the trie marked with a rule, then try to apply it tothe whole input string, if necessary looking up NP'sin sequence from rips.The subroutine for finding NP's is similar, thoughslightly simpler; the variable and constant names correspondin the obvious way to those in the first algorithm.To find an NP from po in ter  to next -po in ter :1.
If the well-formed substring table records that NP's havebeen searched for at po in ter ,  pick one non-deterministically and return, else2.
Set NP-po in ter  to pointer.
Set NP-t r ie -node to *NP-tr ie-root*.3.
Set NP-category  to the lexical category of the itemat NP-pointer.4.
Non-deterministically do one of:a) Find a trie arc from NP- t r ie -node  to NP-next -node  UJggering on category.
Set NP-t r ie -node to NP-next-node.
Bump NP-po in ter  and go back to 3.b) If there is a reduction rule at NP-tr ie-node,attempt to apply it to the segment of the inputstring joining po in ter  to NP-po in ter ,  andrecord the result in the well-formed substring table.Then return.c) If NP-po in ter  = po in ter  and there are noalternatives left, record in the well-formed substringtable that NP's have been searched for at po in ter ,and return with failure.4.
ResultsA proper evaluation of performance gain due to the EBLbypass is impossible without a large statistical sample oftypical user interactions with the target system; at this stageof the project, such data is unfortunately not available.
Ourpreliminary performance measurements have been based on acorpus of 31 queries of distinct syntactic type, in lengthvarying between 3 and 14 words; the histogram in diagram 2summarizes the distribution of the speed-up factor over thisset.
The speed-up factor was defined as the ratio of EBLlook-up to parsing for sentences where an applicable ruleexisted.
It averaged slightly over 30, and as shown in thediagram exceeded 10 on all queries.
The average look-upoverhead on sentences for which no applicable rule existedwas less than 3%.
One of the few disappointments of theproject was however the poor performance of the simplifier,which was unable to achieve better than an average 20%reduction in rule size; this appeared mainly to be due to thenecessity to keep all feature sets for possible later use insemantic interpretation.Distribution of Speed-ups64200-10 10-2020-3030-4040-5050-6060-70Diagram 2.
Distribution of speed-ups due to EBLbypassing.The following transcript of a short session with thesystem illustrates the EBL module in action.
Inputsentences are shown in bold-face, and comments in italics.Note that the glosses for acquired rules are only veryapproximate, and omit nearly all features.EBL bypass initial ized, no rules.Does I ce land  export  f i sh?Bypassing.No match.Adding a top level rule.'~->doesNPTVNP?
"Adding 2 second level rules.
"NP-> Name"and"NP->N:\[mass=y\]"Is the V ip  C luborgan izat ion?Bypassing.a smal l255No match.Adding a top level rule.
"S -> is NP NP?
"Adding 2 second level rules.
"NP -> the Name" and"NP -> DET ADJ N"Who is a member  of  the VipBypassing.No match.Adding a top level rule.
"S -> NP:\[wh=y\] isNP?
"Adding 2 second level rules.
'TVP -> PRO" and'TVP -> DET N P DET NAME"Club?Is John a c i t i zen  of the Un i tedStates?Bypassing.EBL look-up succeeded.The top-level rule used is "S -> is NP NP?
", from thesecond example; the second-level rules are "NP -> Name"from the first example, and "NP -> DET N P DETNAME"from the third.5.
Conclusions and furtherdirectionsOn the basis of the experiments reported here, we thinkthere are good reasons to take EBL seriously as a practicaland generally applicable way of optimizing NL querysystems; the speed-ups achieved were very considerable at alow overhead.
Even more importantly, it was possible toapply the EBL method espite the target's having severalcharacteristics undesirable from this point of view; our apriori guess at the beginning of the project was that, if itworked here, it would work on most systems.
We plan soonto begin implementation f a similar module for a largeunification grammar for Swedish, where it should be easy tocover both syntactic and semantic processing.One thing that ought o be studied more is the dependenceof access time on the number of learned rules when thisnumber becomes large (over 500, say).
It certainly seemsreasonable to hope that the pattern-matching al orithmpresented here will give approximately logarithmicbehaviour, but this is really an empirical question, since itdepends on the distribution of the common query-types interms of their lexical categories.
Another important questionis the extent to which it is possible to compress thegenerated rules.
Since we are essentially trading space fortime, this is likely to define the limits of the method, sincewe will eventually simply run out of space to store morelearned rules, even if we can index them efficiently.In conclusion, it seems to us that application of the EBLmethod to Natural Language offers a fruitful field forcontinued investigation of both a practical and theoreticalnature.?
AcknowledgementsThis project would have been impossible without heassistance of many people at SICS and IBM NordicLaboratories.
We would in particular like to thank IvanBretan, Carl Brown, Jane Brown, Mats Carlsson, P~Dahlin, Mikael Eriksson, Per Kristiansson, Sten Orsvarnand Mohammad Sanarnrad for their help and support.References1.
Minton, S., Carbonell, J.G., Knoblock, C.A., Kuokka,D.R., Etzioni, O.
& Gil, Y., "Explanation-Based Learning:A Problem-Solving Perspective", Artificial Intelligence 40pp.
11-62, 19892.
Pereira, F.N.C.
Logic for Natural Language Analysis,SRI Technical Note No 275, 19833.
Rayner, M. "Applying Explanation-Based Generalizationto Natural-Language Processing".
Proc.
Intl.
Conference onFifth Generation Computer Systems, Tokyo, 19884.
Rayner, M. and Samuelsson, C., "ApplyingExplanation-Based Generalization to Natural-LanguageProcessing (Part 2)".
SICS Research Report 89015, 1989256
