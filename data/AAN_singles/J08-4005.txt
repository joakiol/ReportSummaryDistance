Constructing Corpora for the Developmentand Evaluation of Paraphrase SystemsTrevor Cohn?University of EdinburghChris Callison-Burch?
?Johns Hopkins UniversityMirella Lapata?University of EdinburghAutomatic paraphrasing is an important component in many natural language processing tasks.In this article we present a new parallel corpus with paraphrase annotations.
We adopt a defini-tion of paraphrase based on word alignments and show that it yields high inter-annotator agree-ment.
As Kappa is suited to nominal data, we employ an alternative agreement statistic which isappropriate for structured alignment tasks.
We discuss how the corpus can be usefully employedin evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1)and also in developing linguistically rich paraphrase models based on syntactic structure.1.
IntroductionThe ability to paraphrase text automatically carries much practical import for manyNLP applications ranging from summarization (Barzilay 2003; Zhou et al 2006) toquestion answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machinetranslation (Callison-Burch, Koehn, and Osborne 2006).
It is therefore not surprisingthat recent years have witnessed increasing interest in the acquisition of paraphrasesfrom real world corpora.
These are most often monolingual corpora containing paralleltranslations of the same source text (Barzilay and McKeown 2001; Pang, Knight, andMarcu 2003).
Truly bilingual corpora consisting of documents and their translations havealso been used to acquire paraphrases (Bannard and Callison-Burch 2005; Callison-Burch 2007) as well as comparable corpora such as collections of articles producedby two different newswire agencies about the same events (Barzilay and Elhadad2003).Although paraphrase induction algorithms differ in many respects?for example,the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle)or structural (last week?s fighting, the battle last week), and are represented as words or?
School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK.
E-mail: tcohn@inf.ed.ac.uk.??
Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218.E-mail: ccb@cs.jhu.edu.?
School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK.
E-mail: mlap@inf.ed.ac.uk.Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted forpublication: 26 March 2008.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 4syntax trees?they all rely on some form of alignment for extracting paraphrase pairs.In its simplest form, the alignment can range over individual words, as is often donein machine translation (Quirk, Brockett, and Dolan 2004).
In other cases, the alignmentsrange over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilayand Lee 2003).The obtained paraphrases are typically evaluated via human judgments.
Para-phrase pairs are presented to judges who are asked to decide whether they are seman-tically equivalent, that is, whether they can be generally substituted for one another inthe same context without great information loss (Barzilay and Lee 2003; Barzilay andMcKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005).
Insome cases the automatically acquired paraphrases are compared against manually gen-erated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performanceincrease for a specific application, such as machine translation (Callison-Burch, Koehn,and Osborne 2006).Unfortunately, manually evaluating paraphrases in this way has at least three draw-backs.
First, it is infeasible to perform frequent evaluations when assessing incrementalsystem changes or tuning system parameters.
Second, it is difficult to replicate resultspresented in previous work because there is no standard corpus, and no standard evalu-ation methodology.
Consequently comparisons across systems are few and far between.The third drawback concerns the evaluation studies themselves, which primarily focuson precision.
Recall is almost never evaluated directly in the literature.
And this isfor a good reason: There is no guarantee that participants will identify the same setof paraphrases as each other or with a computational model.
The problem relates tothe nature of the paraphrasing task, which has so far eluded formal definition (seethe discussion in Barzilay [2003]).
Such a definition is not so crucial when assessingprecision, because subjects are asked to rate the paraphrases without actually having toidentify them.
However, recall might be measured with respect to some set of ?gold-standard?
paraphrases which will have to be collected according to some concretedefinition.In this article we present a resource that could potentially be used to addressthese problems.
Specifically, we create a monolingual parallel corpus with humanparaphrase annotations.
Our working definition of paraphrase is based on word andphrase1 alignments between semantically equivalent sentences.
Other definitions arepossible, for instance we could have asked our annotators to identify all constituentsthat are more or less meaning preserving in our parallel corpus.
We chose to workwith alignments for two reasons.
First, the notion of alignment appears to be centralin paraphrasing?most existing paraphrase induction algorithms rely on alignmentseither implicitly or explicitly for identifying paraphrase units.
Secondly, research inmachine translation, where several gold-standard alignment corpora have been created,shows that word alignments can be identified reliably by annotators (Melamed 1998;Och andNey 2000b;Mihalcea and Pedersen 2003;Martin,Mihalcea, and Pedersen 2005).We therefore create word alignments similar to those observed in machine transla-tion, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many linksbetween words.
Alignment blocks larger than one-to-one are used to specify phrasecorrespondences.1 Our definition of the term phrase follows the SMT literature.
It refers to any contiguous sequence ofwords, whether it is a syntactic constituent or not.
See Section 2 for details.598Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsIn the following section we explain how our corpus was created and summarize ourannotation guidelines.
Section 3 gives the details of an agreement study, demonstratingthat our annotators can identify and align paraphrases reliably.
We measure agreementusing alignment overlap measures from the SMT literature, and also introduce a novelagreement statistic for non-enumerable labeling spaces.
Section 4 illustrates how thecorpus can be used in paraphrase research, for example, as a test set for evaluatingthe output of automatic systems or as a training set for the development of paraphrasesystems.
Discussion of our results concludes the article.2.
Corpus Creation and AnnotationOur corpus was compiled from three data sources that have been previously used forparaphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003;Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus,Jules Verne?s Twenty Thousand Leagues Under the Sea novel (Leagues), and the MicrosoftResearch (MSR) paraphrase corpus.
These are monolingual parallel corpora, aligned atthe sentence level.
Both source and target sentences are in English, and express the samecontent using different surface forms.The MTC corpus contains news stories from three sources of journalistic MandarinChinese text.2 These stories were translated into English by 11 translation agencies.Because the majority of the translators were non-native English speakers, occasionallytranslations contain syntactic or grammatical errors and are not entirely fluent.
Afterinspection, we identified four translators with consistently fluent English, and usedtheir sentences for our corpus.
The Leagues corpus contains two English translationsof the French novel Twenty Thousand Leagues Under the Sea.
The corpus was createdby Tagyoung Chung and manually aligned at the paragraph level.3 In order to obtainsentence level paraphrase pairs, we sampled from the subset of one-to-one sentencealignments.
The MSR corpus was harvested automatically from online news sources.4The obtained sentence pairs were further submitted to judges who rated them as beingsemantically equivalent or not (Dolan, Quirk, and Brockett 2004).
We only used seman-tically equivalent pairs.
The sentence pairs were filtered for length (?
50) and lengthratio (?
1 : 9 between the shorter and longer sentence).
This was necessary to prune outincorrectly aligned sentences.We randomly sampled 300 sentence pairs from each corpus (900 in total).
Of these,300 pairs (100 per corpus) were first annotated by two coders to assess inter-annotatoragreement.
The remaining 600 sentence pairs were split into two distinct sets, eachconsisting of 300 sentences (100 per corpus), and were annotated by a single coder.Each coder annotated the same amount of data.
In addition, we obtained a trial setof 50 sentences from the MTC corpus which was used for familiarizing our annotatorswith the paraphrase alignment task (this set does not form part of the corpus).
In sum,we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doublyannotated.To speed up the annotation process, the data sources were first aligned automati-cally and then hand-corrected.We usedGiza++ (Och andNey 2003), a publicly available2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1.3 The corpus can be downloaded from http://www.isi.edu/?knight/.4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9-20CD-47E3-85BC-A2F65CD28042/Details.aspx.599Computational Linguistics Volume 34, Number 4implementation of the IBM word alignment models (Brown et al 1993).
Giza++ wastrained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair-ings of English translations as training instances.
This resulted in 55 =11?
(11?1)2 trainingpairs per sentence and a total of 54, 615 training pairs.
In addition, we augmented thetraining data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan(2004).
This follows standard practice in SMT where entries from a bilingual dictionaryare added to the training set (Och and Ney 2000a), except in our case the ?dictionary?is monolingual and specifies that each word type can be paraphrased as itself.
This isnecessary in order to inform Giza++ about word identity.A common problem with automatic word alignments is that they are asymmetric:one source word can only be aligned to one target word, whereas one target word canbe aligned to multiple source words.
In SMT, word alignments are typically predictedin both directions: source-to-target and target-to-source.
These two alignments are thenmerged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).Symmetrization improves the alignment quality compared to that of a single directionalmodel, while also allowing a greater range of alignment types (i.e., some many-to-one, one-to-many, and many-to-many alignments can be produced).
Analogously, weobtained word alignments in both directions6 which we subsequently merged by takingtheir intersection.
This resulted in a high precision and low recall alignment.Our annotators (two linguistics graduates) were given pairs of sentences and askedto show which parts of these were in correspondence by aligning them on a word-by-word basis.7 Our definition of alignment was fairly general (Och andNey 2003): Given asource string X = x1, .
.
.
, xN and a target string Y = y1, .
.
.
, yM, an alignmentA betweentwo word strings is the subset of the Cartesian product of the word positions:A ?
{(n,m) : n = 1, .
.
.
,N;m = 1, .
.
.
,M} (1)We did not provide a formal definition of what constitutes a correspondence.
As arule of thumb, annotators were told to align words or phrases x ?
y in two sentences(X,Y) whenever the words x could be substituted for y in Y, or vice versa.
This relation-ship should hold within the context of the sentence pair in question: the relation x ?
yneed not hold in general contexts.
Trivially this definition allowed for identical wordpairs.Following common practice (Och, Tillmann, and Ney 1999; Och and Ney 2003;Daume?
III and Marcu 2004), we distinguished between sure (S) and possible (P) align-ments, where S ?
P. The intuition here is that sure alignments are clear-cut decisionsand typical of genuinely substitutable words or phrases, whereas possible alignmentsflag a correspondence that has slightly divergent syntax or semantics.
Annotators wereencouraged to produce sure alignments.
They were also instructed to prefer smalleralignments whenever possible, but were allowed to create larger block alignments.Smaller alignments were generally used to indicate lexical correspondences, whereasblock alignments were reserved for non-compositional phrase pairs (e.g., idiomaticexpressions) or simply expressions with radically different syntax or vocabulary.
In5 The IBM alignment models require a large amount of parallel data to yield reliable alignments.
Wetherefore selected the MTC for training purposes as it was the largest of our parallel corpora.6 We used five iterations for each of Model 1, Model 2, and the HMMmodel.7 The annotation was conducted using a Web-based alignment tool available athttp://demo.linearb.co.uk/paraphrases/.600Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsFigure 1Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid.
Blacksquares represent sure alignment, gray squares represent possible alignment.cases where information in one sentence was not present in the other, the annotatorswere asked to leave this information unaligned.Finally, annotators were given a list of heuristics to help them decide how tomake alignments in cases of ambiguity.
These heuristics handled the alignment ofnamed entities (e.g., George Bush) and definite descriptions (e.g., the president), tenses(e.g., had been and shall be), noun phrases with mismatching determiners (e.g., a manand the man), verb complexes (e.g., was developed and had been developed), phrasalverbs (e.g., take up and accept), genitives (e.g., Bush?s infrequent speeches and the infre-quent speeches by Bush), pronouns, repetitions, typographic errors, and approximatecorrespondences.
For more details, we refer the interested reader to our annotationguidelines.8Figure 1 shows the alignment for two sentence pairs from the MTC corpus.
Thefirst pair (Australia is concerned with the issue of carbon dioxide emissions.
?
The problemof greenhouse gases has attracted Australia?s attention.)
contains examples of word-to-word (the ?
The; issue ?
problem; of ?
of ; Australia ?
Australia) and many-to-manyalignments (carbon dioxide emissions ?
greenhouse gases).
Importantly, we do not usea large many-to-many block for Australia is concerned with and has attracted Australia?sattention because it is possible to decompose the two phrases into smaller alignments.The second sentence pair illustrates a possible alignment (could have very long termeffects?
was of profound significance) indicated by the gray squares.
Possible alignmentsare used here because the two phrases only loosely correspond to each other.
Possiblealignments are also used to mark significant changes in syntax where the words denotea similar concept: for example, in cases where two words have the same stem but are8 Both the corpus and the annotation guidelines can be found at: http://homepages.inf.ed.ac.uk/tcohn/paraphrase corpus.html.601Computational Linguistics Volume 34, Number 4expressed with different parts of speech, (e.g., co-operative ?
cooperation) or when twoverbs are used that are not synonyms (e.g., this is also?
this also marks).3.
Human AgreementAs mentioned in the previous section, 300 sentence pairs (100 pairs from each sub-corpus) were doubly annotated, in order to measure inter-annotator agreement.
Here,we treat one annotator as gold-standard (reference) andmeasure the extent to which theother annotator deviates from this reference.Word-Based Measures.
The standard technique for evaluating word alignments is torepresent them as a set of links (i.e., pairs of words) and compare them against gold-standard alignments.
The quality of an alignmentA (defined in Equation (1)) comparedto reference alignment B can be then computed using standard recall, precision, andF1 measures (Och and Ney 2003):Precision =|AS ?
BP||AS|Recall =|AP ?
BS||BS|F1 = 2 ?
Precision ?
RecallPrecision+ Recall(2)where the subscripts S and P denote sure and possible word alignments, respectively.Note that both precision and recall are asymmetric in that they compare sets of possibleand sure alignments.
This is designed to be maximally generous: sure predictionswhich are present in the reference as possibles are not penalized in precision, and theconverse applies for recall.
We adopt Fraser and Marcu (2007)?s definition of F1, anF-measure between precision and recall over the sure and possibles.
They argue thatit is a better alternative to the commonly used Alignment Error Rate (AER), whichdoes not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-lingual, in order to avoid artificial score inflation, we limit the precision and recallcalculations to consider only pairs of non-identical words (and phrases, as discussedsubsequently).To give an example, consider the sentence pairs in Figure 2, whose alignments havebeen produced by the two annotators A (left) and B (right).
Table 1 shows the individualword alignments for each annotator and their type (sure or possible).
In order to mea-sure F1, we must first estimate Precision and Recall (see Equation (2)).
Treating annota-tor B as the gold standard, |AS| = 4, |BS| = 5, |AS ?
BP| = 4, and |AP ?
BS| = 4.
Thisresults in a precision of 44 = 1, a recall of45 , and F1 of2?1?0.81+0.8 = 0.89.
Note that we ignorealignments over identical words (i.e., discussed ?
discussed, the ?
the, and ?
and,.
?
.
).Phrase-Based Measures.
The given definitions are all word-based; however, our annota-tors, and several paraphrasing models, create correspondences not only between wordsbut also between phrases.
To take this into account, we also evaluate these measuresover larger blocks (similar to Ayan and Dorr [2006]).
Specifically, we extract phrasepairs from the alignments produced by our annotators using a modified version of thestandard SMT phrase extraction heuristic (Och, Tillmann, and Ney 1999).
The heuristic9 Fraser and Marcu (2007) also argue for an unbalanced F-measure to bias towards recall.
This is shown tocorrelate better with translation quality.
For paraphrasing it is not clear if such a bias would be beneficial.602Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsFigure 2Sample sentence pair showing the word alignments from two annotators.extracts all phrase pairs consistent with the word alignment.
These include phrase pairswhose words are aligned to each other or nothing, but not to words outside the phraseboundaries.10 The phrase extraction heuristic creates masses of phrase pairs, many ofwhich are of dubious quality.
This is often due to the inclusion of unaligned words orsimply to the extraction of overly-large phrase pairs which might be better decomposedinto smaller units.
For our purposes we wish to be maximally conservative in how weprocess the data, and therefore we do not extract phrase pairs with unaligned words ontheir boundaries.Figure 3 illustrates the types of phrase pairs our extraction heuristic permits.
Here,the pair and reached ?
and arrived at is consistent with the word alignment.
In contrast,the pair and reached ?
and arrived isn?t; there is an alignment outside the hypotheticalphrase boundary which is not accounted for (reached is also aligned to at).
The phrasepair and reached an ?
and arrived at is consistent with the word alignment; however ithas an unaligned word (i.e., an) on the phrase boundary, which we disallow.Our phrase extraction procedure distinguishes between two types of phrase pairs:atomic, that is, the smallest possible phrase pairs, and composite, which can be createdby combining smaller phrase pairs.
For example, the phrase pair and reached ?
andarrived at in Figure 3 is composite, as it can be decomposed into and ?
and andreached ?
arrived at.
Table 2 shows the atomic and composite phrase pairs extractedfrom the possible alignments produced by annotators A and B for the sentence pair inFigure 2.We compute recall, precision, and F1 over the phrase pairs extracted from the wordalignments as follows:Precision =|Apatom ?
Bp||Apatom|Recall =|Ap ?
Bpatom||Bpatom|F1 = 2 ?
Precision ?
RecallPrecision+ Recall(3)10 The term phrase is not used here in the linguistic sense; many extracted phrases will not be constituents.603Computational Linguistics Volume 34, Number 4Table 1Single word pairs specified by the word alignments from Figure 2, for two annotators, A and B.The column entries specify the alignment type for each annotator, either sure (S) or possible (P).Dashes indicate that the word pair was not predicted by the annotator.
Italics denote lexicallyidentical word pairs.Word alignments A Bthey ?
both ?
Pthey ?
parties P Pdiscussed ?
discussed S Sthe ?
the S Saspects ?
specific P ?in ?
specific P Pdetail ?
specific P Paspects ?
issues P Sin ?
issues P ?detail ?
issues P ?and ?
and S Sreached ?
arrived S Sreached ?
at ?
San ?
a S Sextensive ?
general S Pagreement ?
consensus S S. ?
.
S SFigure 3Validity of phrase pairs according to the phrase extraction heuristic.
Only the leftmost phrasepair is valid.
The others are inconsistent with the alignment or have an unaligned word on aboundary, respectively, indicated by a cross.where Ap and Bp are the predicted and reference phrase pairs, respectively, andthe atom subscript denotes the subset of atomic phrase pairs, Apatom ?
Ap.
As shownin Equation (3) we measure precision and recall between atomic phrase pairs andthe full space of atomic and composite phrase pairs.
This ensures that we do notmultiply reward composite phrase pair combinations,11 while also not unduly pe-nalizing non-matching phrase pairs which are composed of atomic phrase pairs in11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and thereforemight multiply count phrase pairs.604Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsTable 2Phrase pairs are specified by the word alignments from Figure 2, using the possible alignments.The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of theremaining 57 composite phrase pairs are shown.
The italics denote lexically identical phrase pairs.
?This phrase pair is atomic in A but composite in B.Atomic phrase pairs A Bthey ?
parties P ?they ?
both parties ?
Pdiscussed ?
discussed S Sthe ?
the S Saspects in detail ?
specific issues P ?in detail ?
specific ?
Paspects ?
issues ?
Sand ?
and S Sreached ?
arrived S Sreached ?
arrived at ?
Sreached an ?
arrived at a S P?an ?
a S Sextensive ?
general S Pagreement ?
consensus S S. ?
.
S SComposite phrase pairs A Bthey discussed ?
both parties discussed ?
Pthey discussed ?
parties discussed P ?they discussed the ?
both parties discussed the ?
Pthey discussed the ?
parties discussed the P ?they ... reached an ?
both parties ... arrived at a P ?the aspects in detail ?
the specific issues P Preached an extensive ?
arrived at a general S Sextensive agreement .
?
general consensus .
S S...the reference.
Returning to the example in Table 2, with annotator B as the goldstandard, |Apatom| = 7, |Bpatom| = 8, |Apatom ?
Bp| = 5, and |Ap ?
Bpatom| = 4.
Consequently,precision= 57 = 0.71, recall=48 = 0.50, and F1=2?0.71?0.500.71+0.50 = 0.59.
Again we ignoreidentical phrase pairs.A potential caveat here concerns the quality of the atomic phrase pairs, which areautomatically induced and may not correspond to linguistic intuition.
To evaluate this,we had two annotators review a random sample of 166 atomic phrase pairs drawn fromthe MTC corpus (sure), classifying each phrase pair as correct, incorrect, or uncertaingiven the sentence pair as context.
From this set, 73% were deemed correct, 22% un-certain, and 5% incorrect.12 Annotators agreed in their decisions 75% of the time (usingthe Kappa13 statistic, their agreement is 0.61).
This confirms that the phrase-extractionprocess produces reliable phrase pairs from our word-aligned data (although we cannotclaim that it is exhaustive).12 Taking a more conservative position by limiting the proportion of unaligned words within the phrasepair improves these figures monotonically to 90% correct and 0% incorrect (fully aligned phrase pairs).13 This Kappa is computed over three nominal categories (correct, incorrect, and uncertain) and should notbe confused with the agreement measure we develop in the following section for phrase pairs.605Computational Linguistics Volume 34, Number 4Chance-Corrected Agreement.
Besides precision and recall, inter-annotator agreement iscommonly measured using the Kappa statistic (Cohen 1960).
Thus is a desirable mea-sure because it is adjusted for agreement due purely to chance:?
=Pr(A)?
Pr(E)1?
Pr(E)(4)where Pr(A) is the proportion of times two coders14 agree, corrected by Pr(E), theproportion of times we would expect them to agree by chance.Kappa is a suitable agreement measure for nominal data.
An example would be aclassification task, where two coders must assign n linguistic instances (e.g., sentencesor words) into one of m categories.
Given this situation, it would be possible for eachcoder to assign each instance to the same category.
Kappa allows us to quantify whetherthe coders agree with each other about the category membership of each instance.
It isrelatively straightforward to estimate Pr(A)?it is the proportion of instances on whichthe two coders agree.
Pr(E) requires a model of what would happen if the coders wereto assign categories randomly.
Under the assumption that coders r1 and r2 are indepen-dent, the chance of them agreeing on the jth category is the product of each of themassigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2).
Chance agreement is then thesum of this product across all categories: Pr(E) =m?j=1Pr(Cj|r1) Pr(Cj|r2).
The literaturedescribes two different methods for estimating Pr(Cj|ri).
Either a separate distributionis estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott1955; Fleiss 1971; Siegel and Castellan 1988).We refer the interested reader to Di Eugenioand Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion.Unfortunately, Kappa is not universally suited to every categorization task.
A primeexample is structured labeling problems that allow a wide variety of output categories.Importantly, the number and type of categories is not fixed in advance and can varyfrom instance to instance.
In parsing, annotators are given a sentence for which theymust specify a tree, of which there is an exponential number in the sentence length.
Sim-ilarly, in our case the space of possible alignments for a sentence pair is also exponentialin the input sentence lengths.
Considering these annotations as nominal variables isinappropriate.Besides, alignments are only an intermediate representation that we have used tofacilitate the annotation of paraphrases.
Ideally, we would like to measure agreementover the set of phrase pairs which are specified by our annotators (via the word align-ments), not the alignment matrices themselves.Kupper and Hafner (1989) present an alternative measure similar to Kappa that isespecially designed for sets of variables:C?
=???
?01?
?0, (5)where ??
=I?i=1|Ai ?
Bi|min(|Ai|, |Bi|), and ?0 =1Ik?imin(|Ai|, |Bi|)14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976).
Forsimplicity?s sake our discussion and subsequent examples involve two coders.
Also note that we use theterm coder instead of the more common rater.
This is because in our task the annotators must identify(a.k.a.
code) the paraphrases rather than rate them.606Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsHere, Ai and Bi are the coders?
predictions on sentence pair i from our corpus of Isentence pairs.
Each prediction is a subset of the full space of k items.
Expression (5)measures the agreement (or concordance) between coders A and B and follows thegeneral form of Kappa from Equation (4), which is defined analogously with Pr(A) andPr(E) taking the roles of ??
and ?0, but with different definitions.Kupper and Hafner (1989) developed their agreement measure with medical diag-nostic tasks in mind.
For example, two physicians classify subjects into k = 3 diagnosticcategories and wish to find out whether they agree in their diagnoses.
Here, each codermust decidewhich (possibly empty) subset from k categories best describes each subject.The size of k is thus invariant with the instance under consideration.
This is not truein our case, where k will vary across sentence pairs as sentences of different lengthslicense different numbers of phrase pairs.
More critically, the formulation in Equa-tion (5) assumes that items in the set are independent: All subsets of the same car-dinality as k are equally likely, and no combination is impossible.
This independenceassumption is inappropriate for the paraphrase annotation task.
The phrase extractionheuristic allows each contiguous span in a sentence to be aligned to either zero or onespan in the other sentence; that is, nominating a phrase pair precludes the choice ofmany other possible phrase pairs.
Consequently relatively few of the subsets of thefull set of possible phrase pairs are valid.
Formally, an alignment can specify onlyO(N2) phrase pairs from a total set of k = O(N4) possible phrase pairs.
This disparity inmagnitudes leads to increasingly underestimated ??
for larger N, namely, limN??
?0 =limN?
?O(N2)/O(N4) = 0.
The end result is an overestimate of C?
on longer sentences.For these reasons, we adapt the method of Kupper and Hafner (1989) to account forour highly interdependent item sets.
We use C?
from Equation (5) as our agreement sta-tistic defined over sets of atomic phrase pairs, that is, A = Apatom,B = Bpatom.
We redefine?0 as follows:?0 =1II?i=1?Apatom?BpatomPr(Apatom) Pr(Bpatom)|Apatom ?
Bpatom|min(|Apatom|, |Bpatom|)(6)where Apatom and Bpatom range over the sets of atomic phrase pairs licensed by sentencepair i, and Pr(Apatom) and Pr(Bpatom) are priors over these sets for each annotator.
A conse-quence of dropping the independence assumptions is that calculating ?0 is considerablymore difficult.While it may be possible to calculate ?0 analytically, this gets increasingly compli-cated for larger phrase pairs or with an expressive prior.
For the sake of flexibility weestimate ?0 using Monte Carlo sampling.
Specifically, we approximate the full sum bydrawing samples from a prior distribution over sets of phrase pairs for each of ourannotators (Pr(Apatom) and Pr(Bpatom) in Equation (6)).
These samples are then comparedusing the intersection metric.
This is repeated many times and the results are thenaveraged.
More formally:?
?0 =1II?i=11JJ?j=1|Apatom( j)?
Bpatom( j)|min(|Apatom( j)|, |Bpatom( j)|)(7)where for each sentence pair, i, we draw J samples of pairs of sets of phrase pairs,(Apatom,Bpatom).
We use J = 1, 000, which is ample to give reliable estimates.
So far we have607Computational Linguistics Volume 34, Number 4not defined how we sample valid sets of phrase pairs.
This is done via the word align-ments.
Recall that the annotators start out with alignments from an automatic word-aligner.
Firstly, we develop a distribution to predict how often an annotator changes acell from the initial alignment matrix.
We model the number of changes made with abinomial distribution, that is, each local change is assumed independent and has a fixedprobability, Pr(edit|r,Ni,Mi) where r is the coder andNi andMi are the sentence lengths.This distribution is fit to each annotator?s predictions using a linear function over thecombined length of two sentences.
Next we sample word alignments.
Each samplestarts with the automatic alignment, and each cell is changed with probability Pr(edit).These changes are binary, swapping alignments for non-alignments and vice versa.Finally, the phrase-extraction heuristic is run over the alignment matrix to produce aset of phrase pairs.
This is done for each annotator, A and B, after which we have asample, (Apatom,Bpatom).
Each sample is then fed into Equation (7).
Admittedly, this is notthemost accurate prior, as annotators are not just randomly changing the alignment, butinstead are influenced by the content expressed by the sentence pair and other factorssuch as syntactic complexity.
However, this prior produces estimates for ?
?0 which areseveral orders of magnitude larger than those using Kupper and Hafner?s model of ?0in Equation (5).We now illustrate the process of measuring chance-corrected agreement, C?, withrespect to the example in Figure 2.
Here, |Apatom| = 7, |Bpatom| = 8, |Apatom ?
Bpatom| = 4, andtherefore ??
= 47 = 0.571.
For this sentence our annotators edited eight and nine align-ment cells, respectively, of the initial alignment matrix.
This translates into Pr(edit|r =A) = 812?13 = 5.13% and Pr(edit|r = B) = 5.77%.
Given these priors, we run the MonteCarlo sampling process from Equation (7), which results in ?
?0 = 0.147.
Combining theagreement estimate, ?
?, and chance correction estimate, ?
?0, using Equation (6) results inC?
= 0.571?0.1471?0.147 = 0.497.Now, imagine a hypothetical case where ??
= 47 = 0.571 (i.e., the agreement is thesame as before), annotator B edits nine alignment cells, but annotator A chooses notto make any edits.
This leads to an increased estimate of ?
?0 = 0.259 and a decreasedC?
= 0.442.
If both annotators were not to make any edits, ?
?0 = 1 and C?
= ??.
Interest-ingly, at the other extreme when Pr(edit|r = A) = Pr(edit|r = B) = 1, agreement is alsoperfect, ?
?0 = 1 and C?
= ??.
This is because only one phrase pair can be extractedwhich consists of the two full sentences.Results.
Tables 3 and 4 display agreement statistics on our three corpora using precision,recall, F1, and C?.
Specifically, we estimate C?
by aggregating ??
and ?
?0 into corpus-level estimates.
Table 3 shows agreement scores for individual words, whereas Table 4shows agreement for phrase pairs.
In both cases the agreement is computed over non-identical word and phrase pairs which are more likely to correspond to paraphrases.The agreement figures are broken down into possible (Poss) and sure alignments (Sure)for precision and recall.When agreement is measured over words, our annotators obtain high F1 on allthree corpora (MTC, Leagues, and News).
Recall on Possibles seems worse on theNews corpus when compared to MTC or Leagues.
This is to be expected because thiscorpus was automatically harvested from the Web, and some of its instances may notbe representative examples of paraphrases.
For example, it is common for one sentenceto provide considerably more details than the other, despite the fact that both describethe same event.
The annotators in turn have difficulty deciding whether such instancesare valid paraphrases.
The C?
scores for the three corpora are in the same ballpark.608Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsTable 3Inter-annotator agreement using precision, recall, F1, and C?
; the agreement is measured overwords.MTC Leagues NewsMeasure Poss Sure Measure Poss Sure Measure Poss SurePrec 0.79 0.59 Prec 0.85 0.73 Prec 0.78 0.55Rec 0.77 0.73 Rec 0.74 0.75 Rec 0.57 0.70F1 0.76 F1 0.79 F1 0.74C?
0.85 C?
0.87 C?
0.89Table 4Inter-annotator agreement using precision, recall, F1, and C?
; the agreement is measured overatomic phrase pairs.MTC Leagues NewsMeasure Poss Sure Measure Poss Sure Measure Poss SurePrec 0.77 0.67 Prec 0.74 0.72 Prec 0.72 0.68Rec 0.77 0.66 Rec 0.77 0.73 Rec 0.69 0.81F1 0.71 F1 0.74 F1 0.76C?
0.63 C?
0.62 C?
0.53Interestingly, C?
is highest on the News corpus, whereas F1 is lowest.
Whereas precisionand recall are normalized by the number of predictions from annotators A and B,respectively, C?
is normalized by the minimum number of predictions between the two.Therefore, when the predictions are highly divergent, C?
will paint a rosier picture thanF1 (which is the combination of precision and recall).
This indeed seems to be the casefor the News corpus, where precision and recall have a higher spread in comparison tothe other two corpora (see the Poss column in Table 3).Agreement scores tend to be lower when taking phrases into account (see Table 4).This is expected because annotators are faced with a more complex task; they mustgenerally make more decisions: for example, determining the phrase boundaries andhow to align their constituent words.
An exception to this trend is the News corpuswhere the F1 is higher for phrase pairs than for individual word pairs.
This is due to thefact that there are many similar sentence pairs in this data.
These have many identicalwords and a few different words.
The differences are often in a clump (e.g., personnames, verb phrases), rather than distributed throughout the sentence.
The annotatorstend to block align these and there is a large scope for disagreement.Whereas estimatingagreement over words heavily penalizes block differences, when phrases are takeninto account in the F1 measure, these are treated more leniently.
Note that C?
is notso lenient, as it measures agreement over the sets of atomic phrase pairs rather thanbetween atomic and composite phrase pairs in the F1 measure.
This means that underC?, choosing different granularities of phrases will be penalized, but would not havebeen under the F1 measure.In Figure 4we show how C?
varies with sentence length for our three corpora.
Specif-ically, we plot observed agreement ?
?, chance agreement ?0, and C?
against sentence pairs609Computational Linguistics Volume 34, Number 4Figure 4Agreement statistics plotted against sentence length for the three sub-corpora.
Each group ofthree columns correspond to ?
?, ?
?0, and C?, respectively.
The statistics were measured overnon-identical phrase pairs using all phrase pairs, atomic and composite.Table 5Agreement between automatic Giza++ predicted word alignments and our manually correctedalignments, measured over atomic phrase pairs.MTC Leagues NewsMeasure Poss Sure Measure Poss Sure Measure Poss SurePrec 0.58 0.55 Prec 0.63 0.60 Prec 0.63 0.65Rec 0.42 0.49 Rec 0.39 0.47 Rec 0.50 0.64F1 0.53 F1 0.54 F1 0.63binned by (the shorter) sentence length.
In all cases we observe that chance agreementis substantially lower than observed agreement for all sentence lengths.
We also see thatC?
tends to be higher for shorter sentences.
Differences in C?
across sentence lengths aremostly of small magnitude across all three corpora.
This indicates that disagreementsmay be due to other factors, besides sentence length.Unfortunately, there are no comparable annotation studies that would allow usto gauge the quality of the obtained agreements.
The use of precision, recall, and F1is widespread in SMT, but these measures evaluate automatic alignments against agold standard, rather than the agreement between two or more annotators (but seeMelamed [1998] for an exception).
Nevertheless, we would expect the humans to agreemore with each other than with Giza++, given that the latter produces many erroneousword alignments and is not specifically tuned to the paraphrasing task.
Table 5 showsagreement between one annotator and Giza++ for atomic phrase pairs.15 We obtainedsimilar results for the other annotator and with the word-based measures.
As can beseen, human?Giza++ agreement is much lower than human?human agreement on allthree corpora (compare Tables 5 and 4).
Taken together the results in Tables 3?5 showa substantial level of agreement, thus indicating that our definition of paraphrases viaword alignments can yield reliable annotations.
In the following section we discuss howour corpus can be usefully employed in the study of paraphrasing.15 Note that we cannot meaningfully measure C?
for this data because the Giza++ predictions are alreadybeing used to estimate ?0 in our formulation.
Consequently, P(A) = P(B) and C?
is zero.610Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems4.
ExperimentsOur annotated corpus can be used in a number of ways to help paraphrase research:for example, to inform the linguistic analysis of paraphrases, as a training set for thedevelopment of discriminative paraphrase systems, and as a test set for the automaticevaluation of computational models.
Here, we briefly demonstrate some of these uses.Paraphrase Modeling.Much previous research has focused on lexical paraphrases (but seeLin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions).
We arguethat our corpus should support a richer range of structural (syntactic) paraphrases.To demonstrate this we have extracted paraphrase rules from our annotations usingthe grammar induction algorithm from Cohn and Lapata (2007).
Briefly, the algorithmextracts tree pairs from word-aligned text by choosing aligned constituents in a pair ofequivalent sentences.
These pairs are then generalized by factoring out aligned subtrees,thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variablenodes.We parsed the MTC corpus with Bikel?s (2002) parser and extracted synchronousrules from the gold-standard alignments.
A sample of these rules are shown in Figure 5.Here we see three lexical paraphrases, followed by five structural paraphrases.
Inexample 4, also is replaced with moreover and is moved to the start of the sentence fromthe pre-verbal position.
Examples 5?8 show various reordering operations, where theboxed numbers indicate correspondences between non-terminals in the two sides of therules.The synchronous rules in Figure 5 provide insight into the process of paraphrasingat the syntactic level, and also a practical means for developing algorithms for para-phrase generation?a task which has received little attention to date.
For instance, wecould envisage a paraphrase model that transforms parse trees of an input sentenceinto parse trees that represent a sentential paraphrase of that sentence.
Our corpus canbe used to learn this mapping using discriminative methods (Cowan, Kuc?erova?, andCollins 2006; Cohn and Lapata 2007).Evaluation Set.
As mentioned in Section 1, it is currently difficult to compare competingapproaches due to the effort involved in eliciting manual judgments of paraphraseoutput.
Our corpus could fill the role of a gold-standard test set, allowing for automaticevaluation techniques.Developing measures for automatic paraphrase evaluation is outside the scope ofthis article.
Nevertheless, we illustrate how the corpus can be used for this purpose.For example we could easily measure the precision and recall of an automatic systemFigure 5Synchronous grammar rules extracted from the MTC corpus.611Computational Linguistics Volume 34, Number 4against our annotations.
Computing precision and recall for an individual system is notperhaps the most meaningful test, considering the large potential for paraphrasing ina given sentence pair.
A better evaluation strategy would include a comparison acrossmany systems on the same corpus.
We could then rank these systems without, however,paying so much attention to the absolute precision and recall values.
We expect thesecomparisons to yield relatively low numbers for many reasons.
First and foremost thetask is hard, as shown by our inter-annotator agreement figures in Tables 3 and 4.Secondly, there may be valid paraphrases that the systems identify but are not listedin our gold standard.
Thirdly, systems may have different biases, for example, towardsproducing more lexical or syntactic paraphrases, but our comparison would not takethis into account.
Despite all these considerations, we believe that comparison againstour corpus would treat these systems on an equal footing against the same materialswhile factoring out nonessential degrees of freedom inherent in human elicitation stud-ies (e.g., attention span, task familiarity, background).We evaluated the performance of two systems against our corpus.
Our first systemis simply Giza++ trained on the 55, 615 sentence pairs described in Section 4.
Thesecond system uses a co-training-based paraphrase extraction algorithm (Barzilay andMcKeown 2001).
It was also trained on the MTC part 1 corpus, on the same data setused for Giza++, with its default parameters.
For each system, we filtered the predictedparaphrases to just those which match part of a sentence pair in the test set.
Theseparaphrases were then compared to the sure phrase pairs extracted from our manuallyaligned corpus.
Giza++?s precision is 55% and recall 49% (see Table 5).
The co-trainingsystem obtained a precision of 30% and recall of 16%.
To confirm the accuracy ofthe precision estimate, we performed a human evaluation on a sample of 48 of thepredicted paraphrases which were treated as errors.
Of these, 63% were confirmed asbeing incorrect and only 20%were acceptable (the remaining were uncertain).
The inter-annotator agreement in Table 4 can be used as an upper bound for precision and recall(precision for Sure phrase pairs is 67% and recall 66%).
These results seem to suggestthat a hypothetical paraphrase extractor based on automatic word alignments wouldobtain performance superior to the co-training approach.
However, we must bear inmind that the co-training system is highly parametrized and was not specifically tunedto our data set.5.
ConclusionsIn this article we have presented a human-annotated paraphrase corpus and arguedthat it can be usefully employed for the evaluation and modeling of paraphrases.
Wehave defined paraphrases as word alignments in a corpus containing pairs of equivalentsentences and shown that these can be reliably identified by annotators.
In measur-ing agreement, we used the standard measures of precision, recall, and F1, but alsoproposed a novel formulation of chance-corrected agreement for word (and phrase)alignments.
Beyond alignment, our formulation could be applied to other structuredtasks including parsing and sequence labeling.The uses of the corpus are many and varied.
It can serve as a test set for eval-uating the precision and recall of paraphrase induction systems trained on parallelmonolingual corpora.
The corpus could be further used to develop new evaluationmetrics for paraphrase acquisition or novel paraphrasing models.
An exciting avenuefor future research concerns paraphrase prediction, that is, determiningwhen and how toparaphrase single sentence input.
Because our corpus contains paraphrase annotationsat the sentence level, it could provide a natural test-bed for prediction algorithms.612Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase SystemsAcknowledgmentsThe authors acknowledge the support of theEPSRC (Cohn, grant GR/T04557/01;Lapata, grant GR/T04540/01), the NationalScience Foundation (Callison-Burch, grantIIS-071344), and the EuroMatrix project(Callison-Burch) funded by the EuropeanCommission (6th Framework Programme).We are grateful to our annotators VasilisKaraiskos and Tom Segler.
Thanks to ReginaBarzilay for providing us the output of hersystem on our data and to the anonymousreferees whose feedback helped tosubstantially improve the present article.ReferencesAho, A. V. and J. D. Ullman.
1969.
Syntaxdirected translations and the pushdownassembler.
Journal of Compute SystemSciences, 3(1):37?56.Artstein, Ron and Massimo Poesio.
2008.Inter-coder agreement for ComputationalLinguistics.
Computational Linguistics.Ayan, Necip Fazil and Bonnie J. Dorr.
2006.Going beyond AER: An extensive analysisof word alignments and their impact onMT.
In Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics, pages 9?16,Sydney.Bannard, Colin and Chris Callison-Burch.2005.
Paraphrasing with bilingual parallelcorpora.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 597?604, Ann Arbor, MI.Bartko, John J. and William T. Carpenter.1976.
On the methods and theory ofreliability.
Journal of Nervous and MentalDisease, 163(5):307?317.Barzilay, Regina.
2003.
Information Fusion forMulti-Document Summarization:Paraphrasing and Generation.
Ph.D. thesis,Columbia University, New York, NY.Barzilay, Regina and Noemie Elhadad.2003.
Sentence alignment for monolingualcomparable corpora.
In Proceedingsof the Conference on Empirical Methods inNatural Language Processing, pages 25?32,Sapporo.Barzilay, Regina and Lillian Lee.
2003.Learning to paraphrase: An unsupervisedapproach using multiple-sequencealignment.
In Proceedings of the HumanLanguage Technology Conference and theAnnual Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 16?23, Edmonton.Barzilay, Regina and Kathy McKeown.
2001.Extracting paraphrases from a parallelcorpus.
In Proceedings of the 39th AnnualMeeting of the Association for ComputationalLinguistics, pages 50?57, Toulouse.Bikel, Daniel.
2002.
Design of a multi-lingual,parallel-processing statistical parsingengine.
In Proceedings of the HumanLanguage Technology Conference,pages 24?27, San Diego, CA.Brown, Peter F., Stephen A. Della-Pietra,Vincent J. Della-Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Callison-Burch, Chris.
2007.
Paraphrasing andTranslation.
Ph.D. thesis, University ofEdinburgh, Edinburgh, Scotland.Callison-Burch, Chris, Philipp Koehn, andMiles Osborne.
2006.
Improved statisticalmachine translation using paraphrases.
InProceedings of the Human LanguageTechnology Conference and Annual Meeting ofthe North American Chapter of the Associationfor Computational Linguistics, pages 17?24,New York, NY.Cohen, J.
1960.
A coefficient of agreement fornominal scales.
Educational andPsychological Measurement, 20:37?46.Cohn, Trevor and Mirella Lapata.
2007.
Largemargin synchronous generation and itsapplication to sentence compression.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing andon Computational Natural LanguageLearning, pages 73?82, Prague.Cowan, Brooke, Ivona Kuc?erova?, andMichael Collins.
2006.
A discriminativemodel for tree-to-tree translation.
InProceedings of the 2006 Conference onEmpirical Methods in Natural LanguageProcessing, pages 232?241, Sydney.Daume?
III, Hal and Daniel Marcu.2004.
A phrase-based HMM approachto document/abstract alignment.In Proceedings of the 2004 Conferenceon Empirical Methods in NaturalLanguage Processing, pages 119?126,Barcelona.Di Eugenio, Barbara and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Dolan, William, Chris Quirk, and ChrisBrockett.
2004.
Unsupervised constructionof large paraphrase corpora: Exploitingmassively parallel news sources.
InProceedings of the 20th InternationalConference on Computational Linguistics,pages 350?356, Geneva.613Computational Linguistics Volume 34, Number 4Duboue, Pablo and Jennifer Chu-Carroll.2006.
Answering the question you wishthey had asked: The impact ofparaphrasing for question answering.In Proceedings of the Human LanguageTechnology Conference of the NAACL,Companion Volume: Short Papers,pages 33?36, New York, NY.Fleiss, Joseph L. 1971.
Measuring nominalscale agreement among many raters.Psychological Bulletin, 76(5):378?382.Fraser, Alexander and Daniel Marcu.
2007.Measuring word alignment quality forstatistical machine translation.Computational Linguistics, 33(3):293?303.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Human Language TechnologyConference and Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 48?54,Edmonton.Kupper, Lawrence L. and Kerry B. Hafner.1989.
On assessing interrater agreementfor multiple attribute responses.
Biometrics,45(3):957?967.Lin, Dekang and Patrick Pantel.
2001.Discovery of inference rules for questionanswering.
Natural Language Engineering,7(4):342?360.Martin, Joel, Rada Mihalcea, and TedPedersen.
2005.
Word alignment forlanguages with scarce resources.
InProceedings of the ACL Workshop on Buildingand Using Parallel Texts, pages 67?74,Ann Arbor, MI.Melamed, I. Dan.
1998.
Manual annotationof translational equivalence: The Blinkerproject.
IRCS Technical Report #98-07,University of Pennsylvania,Philadelphia, PA.Mihalcea, Rada and Ted Pedersen.
2003.
Anevaluation exercise for word alignment.
InProceedings of the HLT-NAACL Workshop onBuilding and Using Parallel Texts: DataDriven Machine Translation and Beyond,pages 1?6, Edmonton.Och, Franz Josef and Hermann Ney.
2000a.
Acomparison of alignment models forstatistical machine translation.
InProceedings of the 18th InternationalConference on Computational Linguistics,pages 1086?1090, Saarbru?cken.Och, Franz Josef and Hermann Ney.2000b.
Improved statistical alignmentmodels.
In Proceedings of the 38thAnnual Meeting of the Association forComputational Linguistics, pages 440?447,Hong Kong.Och, Franz Josef and Hermann Ney.
2003.
Asystematic comparison of variousstatistical alignment models.
ComputationalLinguistics, 29(1):19?52.Och, Franz Josef, Christoph Tillmann, andHermann Ney.
1999.
Improved alignmentmodels for statistical machine translation.In Proceedings of the Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,pages 20?28, College Park, MD.Pang, Bo, Kevin Knight, and Daniel Marcu.2003.
Syntax-based alignment of multipletranslations: Extracting paraphrases andgenerating new sentences.
In Proceedings ofthe Human Language Technology Conferenceand the Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 181?188,Edmonton.Quirk, Chris, Chris Brockett, and WilliamDolan.
2004.
Monolingual machinetranslation for paraphrase generation.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing,pages 142?149, Barcelona.Scott, William A.
1955.
Reliability of contentanalysis: The case of nominal scale.
PublicOpinion Quarterly, 19:127?141.Siegel, Sidney and N. John Castellan.
1988.Non Parametric Statistics for the BehavioralSciences.
McGraw-Hill, New York.Zhou, Liang, Chin-Yew Lin, Dragos StefanMunteanu, and Eduard Hovy.
2006.Paraeval: Using paraphrases toevaluate summaries automatically.
InProceedings of the Human LanguageTechnology Conference, pages 447?454,New York, NY.614
