Proceedings of NAACL HLT 2007, pages 508?515,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsStatistical Phrase-based Post-editingMichel Simard Cyril GoutteInteractive Language TechnologiesNational Research Council of CanadaGatineau, Canada, K1A 0R6FirstName.LastName@nrc.gc.caPierre IsabelleAbstractWe propose to use a statistical phrase-based machine translation system in apost-editing task: the system takes as in-put raw machine translation output (froma commercial rule-based MT system), andproduces post-edited target-language text.We report on experiments that were per-formed on data collected in precisely sucha setting: pairs of raw MT output andtheir manually post-edited versions.
In ourevaluation, the output of our automaticpost-editing (APE) system is not only bet-ter quality than the rule-based MT (bothin terms of the BLEU and TER metrics),it is also better than the output of a state-of-the-art phrase-based MT system usedin standalone translation mode.
These re-sults indicate that automatic post-editingconstitutes a simple and efficient way ofcombining rule-based and statistical MTtechnologies.1 IntroductionThe quality of machine translation (MT) is gener-ally considered insufficient for use in the field with-out a significant amount of human correction.
In thetranslation world, the term post-editing is often usedto refer to the process of manually correcting MToutput.
While the conventional wisdom is that post-editing MT is usually not cost-efficient compared tofull human translation, there appear to be situationswhere it is appropriate and even profitable.
Unfortu-nately, there are few reports in the literature aboutsuch experiences (but see Allen (2004) for exam-ples).One of the characteristics of the post-editing task,as opposed to the revision of human translation forexample, is its partly repetitive nature.
Most MTsystems invariably produce the same output whenconfronted with the same input; in particular, thismeans that they tend to make the same mistakes overand over again, which the post-editors must correctrepeatedly.
Batch corrections are sometimes pos-sible when multiple occurrences of the same mis-take appear in the same document, but when it isrepeated over several documents, or equivalently,when the output of the same machine translationsystem is handled by multiple post-editors, then theopportunities for factoring corrections become muchmore complex.
MT users typically try to reducethe post-editing load by customizing their MT sys-tems.
However, in Rule-based Machine Translation(RBMT), which still constitutes the bulk of the cur-rent commercial offering, customization is usuallyrestricted to the development of ?user dictionaries?.Not only is this time-consuming and expensive, itcan only fix a subset of the MT system?s problems.The advent of Statistical Machine Translation,and most recently phrase-based approaches (PBMT,see Marcu and Wong (2002), Koehn et al (2003))into the commercial arena seems to hold the promiseof a solution to this problem: because the MT sys-tem learns directly from existing translations, it canbe automatically customized to new domains andtasks.
However, the success of this operation cru-508cially depends on the amount of training data avail-able.
Moreover, the current state of the technologyis still insufficient for consistently producing humanreadable translations.This state of affairs has prompted some to ex-amine the possibility of automating the post-editingprocess itself, at least as far as ?repetitive errors?
areconcerned.
Allen and Hogan (2000) sketch the out-line of such an automated post-editing (APE) sys-tem, which would automatically learn post-editingrules from a tri-parallel corpus of source, raw MTand post-edited text.
Elming (2006) suggests usingtranformation-based learning to automatically ac-quire error-correcting rules from such data; however,the proposed method only applies to lexical choiceerrors.
Knight and Chander (1994) also argue in fa-vor of using a separate APE module, which is thenportable across multiple MT systems and languagepairs, and suggest that the post-editing task could beperformed using statistical machine translation tech-niques.
To the best of our knowledge, however, thisidea has never been implemented.In this paper, we explore the idea of using aPBMT system as an automated post-editor.
The un-derlying intuition is simple: if we collect a paral-lel corpus of raw machine-translation output, alongwith its human-post-edited counterpart, we can trainthe system to translate from the former into the lat-ter.
In section 2, we present the case study that mo-tivates our work and the associated data.
In section3, we describe the phrase-based post-editing modelthat we use for improving the output of the auto-matic translation system.
In section 4, we illus-trate this on a dataset of moderate size containingjob ads and their translation.
With less than 500kwords of training material, the phrase-based MTsystem already outperforms the rule-based MT base-line.
However, a phrase-based post-editing modeltrained on the output of that baseline outperformsboth by a fairly consistent margin.
The resultingBLEU score increases by up to 50% (relative) andthe TER is cut by one third.2 Background2.1 ContextThe Canadian government?s department of HumanResources and Social Development (HRSDC) main-tains a web site called Job Bank,1 where poten-tial employers can post ads for open positions inCanada.
Over one million ads are posted on JobBank every year, totalling more than 180 millionwords.
By virtue of Canada?s Official Language Act,HRSDC is under legal obligation to post all ads inboth French and English.
In practice, this meansthat ads submitted in English must be translated intoFrench, and vice-versa.To address this task, the department has put to-gether a complex setup, involving text databases,translation memories, machine translation and hu-man post-editing.
Employers submit ads to the JobBank website by means of HTML forms containing?free text?
data fields.
Some employers do period-ical postings of identical ads; the department there-fore maintains a database of previously posted ads,along with their translations, and new ads are sys-tematically checked against this database.
The trans-lation of one third of all ads posted on the Job Bankis actually recuperated this way.
Also, employerswill often post ads which, while not entirely identi-cal, still contain identical sentences.
The departmenttherefore also maintains a translation memory of in-dividual sentence pairs from previously posted ads;another third of all text is typically found verbatimin this way.The remaining text is submitted to machine trans-lation, and the output is post-edited by human ex-perts.
Overall, only a third of all submitted text re-quires human intervention.
This is nevertheless verylabour-intensive, as the department tries to ensurethat ads are posted at most 24 hours after submis-sion.
The Job Bank currently employs as many as20 post-editors working full-time, most of whom arejunior translators.2.2 The DataHRSDC kindly provided us with a sample of datafrom the Job Bank.
This corpus consists in a collec-tion of parallel ?blocks?
of textual data.
Each blockcontains three parts: the source language text, assubmitted by the employer, its machine-translation,produced by a commercial rule-based MT system,and its final post-edited version, as posted on thewebsite.1http://www.jobbank.gc.ca509The entire corpus contains less than one millionwords in each language.
This corresponds to thedata processed in less than a week by the Job Bank.Basic statistics are given in Table 1 (see Section 4.1).Most blocks contain only one sentence, but someblocks may contain many sentences.
The longestblock contains 401 tokens over several sentences.Overall, blocks are quite short: the median numberof tokens per source block is only 9 for French-to-English and 7 for English-to-French.
As a conse-quence, no effort was made to segment the blocksfurther for processing.We evaluated the quality of the Machine Transla-tion contained in the corpus using the TranslationEdit Rate (TER, cf.
Snover et al (2006)).
TheTER counts the number of edit operations, includingphrasal shifts, needed to change a hypothesis trans-lation into an adequate and fluent sentence, and nor-malised by the length of the final sentence.
Notethat this closely corresponds to the post-editing op-eration performed on the Job Bank application.
Thismotivates the choice of TER as the main metric inour case, although we also report BLEU scores inour experiments.
Note that the emphasis of our workis on reducing the post-edition effort, which is wellestimated by TER.
It is not directly on quality so thequestion of which metric better estimates translationquality is not so relevant here.The global TER (over all blocks) are 58.77%for French-to-English and 53.33% for English-to-French.
This means that more than half the wordshave to be post-edited in some way (delete / substi-tute / insert / shift).
This apparently harsh result issomewhat mitigated by two factors.First, the distribution of the block-based TER2shows a large disparity in performance, cf.
Figure 1.About 12% of blocks have a TER higher than 100%:this is because the TER normalises on the length ofthe references, and if the raw MT output is longerthan its post-edited counterpart, then the number ofedit operations may be larger than that length.3 Atthe other end of the spectrum, it is also clear thatmany blocks have low TER.
In fact more than 10%2Contrary to BLEU or NIST, the TER naturally decomposesinto block-based scores.3A side effect of the normalisation is that larger TER aremeasured on small sentences, e.g.
3 errors for 2 referencewords.Histogram of TER for rule?based MTTER for rule?based MTFrequency0 50 100 15001000200030004000Figure 1: Distribution of TER on 39005 blocks fromthe French-English corpus (thresholded at 150%).have a TER of 0.
The global score therefore hides alarge range of performance.The second factor is that the TER measures thedistance to an adequate and uent result.
A highTER does not mean that the raw MT output is notunderstandable.
However, many edit operations maybe needed to make it fluent.3 Phrase-based Post-editingTranslation post-editing can be viewed as a simpletransformation process, which takes as input rawtarget-language text coming from a MT system, andproduces as output target-language text in which ?er-rors?
have been corrected.
While the automationof this process can be envisaged in many differ-ent ways, the task is not conceptually very differ-ent from the translation task itself.
Therefore, theredoesn?t seem to be any good reason why a machinetranslation system could not handle the post-editingtask.
In particular, given such data as described inSection 2.2, the idea of using a statistical MT systemfor post-editing is appealing.
Portage is preciselysuch a system, which we describe here.Portage is a phrase-based, statistical machinetranslation system, developed at the National Re-search Council of Canada (NRC) (Sadat et al,5102005).
A version of the Portage system is madeavailable by the NRC to Canadian universities forresearch and education purposes.
Like other SMTsystems, it learns to translate from existing parallelcorpora.The system translates text in three main phases:preprocessing of raw data into tokens; decoding toproduce one or more translation hypotheses; anderror-driven rescoring to choose the best final hy-pothesis.
For languages such as French and English,the first of these phases (tokenization) is mostly astraightforward process; we do not describe it anyfurther here.Decoding is the central phase in SMT, involv-ing a search for the hypotheses t that have high-est probabilities of being translations of the cur-rent source sentence s according to a model forP (t|s).
Portage implements a dynamic program-ming beam search decoding algorithm similar to thatof Koehn (2004), in which translation hypothesesare constructed by combining in various ways thetarget-language part of phrase pairs whose source-language part matches the input.
These phrase pairscome from large phrase tables constructed by col-lecting matching pairs of contiguous text segmentsfrom word-aligned bilingual corpora.Portage?s model for P (t|s) is a log-linear com-bination of four main components: one or more n-gram target-language models, one or more phrasetranslation models, a distortion (word-reordering)model, and a sentence-length feature.
The phrase-based translation model is similar to that of Koehn,with the exception that phrase probability estimatesP (s?|t?)
are smoothed using the Good-Turing tech-nique (Foster et al, 2006).
The distortion model isalso very similar to Koehn?s, with the exception of afinal cost to account for sentence endings.Feature function weights in the loglinear modelare set using Och?s minium error rate algorithm(Och, 2003).
This is essentially an iterative two-stepprocess: for a given set of source sentences, generaten-best translation hypotheses, that are representativeof the entire decoding search space; then, apply avariant of Powell?s algorithm to find weights that op-timize the BLEU score over these hypotheses, com-pared to reference translations.
This process is re-peated until the set of translations stabilizes, i.e.
nonew translations are produced at the decoding step.To improve raw output from decoding, Portage re-lies on a rescoring strategy: given a list of n-besttranslations from the decoder, the system reordersthis list, this time using a more elaborate loglinearmodel, incorporating more feature functions, in ad-dition to those of the decoding model: these typ-ically include IBM-1 and IBM-2 model probabili-ties (Brown et al, 1993) and an IBM-1-based fea-ture function designed to detect whether any wordin one language appears to have been left withoutsatisfactory translation in the other language; all ofthese feature functions can be used in both languagedirections, i.e.
source-to-target and target-to-source.In the experiments reported in the next section,the Portage system is used both as a translation andas an APE system.
While we can think of a numberof modifications to such a system to better adapt itto the post-editing task (some of which are discussedlater on), we have done no such modifications to thesystem.
In fact, whether the system is used for trans-lation or post-editing, we have used exactly the sametranslation model configuration and training proce-dure.4 Evaluation4.1 Data and experimental settingThe corpus described in section 2.2 is available fortwo language pairs: English-to-French and French-to-English.4 In each direction, each block is avail-able in three versions (or slices): the original text(or source), the output of the commercial rule-basedMT system (or baseline) and the final, post-editedversion (or reference).In each direction (French-to-English and English-to-French), we held out two subsets of approxi-mately 1000 randomly picked blocks.
The valida-tion set is used for testing the impact of various high-level choices such as pre-processing, or for obtain-ing preliminary results based on which we setup newexperiments.
The test set is used only once, in orderto obtain the final experimental results reported here.The rest of the data constitutes the training set,which is split in two.
We sampled a subset of1000 blocks as train-2, which is used for optimiz-4Note that, in a post-editing context, translation direction iscrucially important.
It is not possible to use the same corpus inboth directions.511English-to-French French-to-EnglishCorpus words: words:blocks source baseline reference blocks source baseline referencetrain-1 28577 310k 382k 410k 36005 485k 501k 456ktrain-2 1000 11k 14k 14k 1000 13k 14k 12kvalidation 881 10k 13k 13k 966 13k 14k 12ktest 899 10k 12k 13k 953 13k 13k 12kTable 1: Data and split used in our experiments, (in thousand words).
?baseline?
is the output of the com-mercial rule-based MT system and ?reference?
is the final, post-edited text.ing the log-linear model parameters used for decod-ing and rescoring.
The rest is the train-1 set, usedfor estimating IBM translation models, constructingphrasetables and estimating a target language model.The composition of the various sets is detailed inTable 1.
All data was tokenized and lowercased;all evaluations were performed independent of case.Note that the validation and test sets were originallymade out of 1000 blocks sampled randomly fromthe data.
These sets turned out to contain blocksidentical to blocks from the training sets.
Consider-ing that these would normally have been handled bythe translation memory component (see the HRSDCworkflow description in Section 2.1), we removedthose blocks for which the source part was alreadyfound in the training set (in either train-1 or train-2),hence their smaller sizes.In order to check the sensitivity of experimentalresults to the choice of the train-2 set, we did arun of preliminary experiments using different sub-sets of 1000 blocks.
The experimental results werenearly identical and highly consistent, showing thatthe choice of a particular train-2 subset has no in-fluence on our conclusions.
In the experiments re-ported below, we therefore use a single identicaltrain-2 set.We initially performed two sets of experimentson this data.
The first was intended to compare theperformance of the Portage PBMT system as an al-ternative to the commercial rule-based MT systemon this type of data.
In these experiments, English-to-French and French-to-English translation systemswere trained on the source and reference (manuallypost-edited target language) slices of the training set.In addition to the target language model estimatedon the train-1 data, we used an external contribution,Language TER BLEUEnglish-to-FrenchBaseline 53.5 32.9Portage translation 53.7 36.0Baseline + Portage APE 47.3 41.6French-to-EnglishBaseline 59.3 31.2Portage translation 43.9 41.0Baseline + Portage APE 41.0 44.9Table 2: Experimental Results: For TER, lower (er-ror) is better, while for BLEU, higher (score) is bet-ter.
Best results are in bold.a trigram target language model trained on a fairlylarge quantity of data from the Canadian Hansard.The goal of the second set of experiments was toassess the potential of the Portage technology in au-tomatic post-editing mode.
Again, we built systemsfor both language directions, but this time using theexisting rule-based MT output as source and the ref-erence as target.
Apart from the use of differentsource data, the training procedure and system con-figurations of the translation and post-editing sys-tems were in all points identical.4.2 Experimental resultsThe results of both experiments are presented in Ta-ble 2.
Results are reported both in terms of the TERand BLEU metrics; Baseline refers to the commer-cial rule-based MT output.The first observation from these results is that,while the performance of Portage in translationmode is approximately equivalent to that of the base-line system when translating into French, its perfor-mance is much better than the baseline when trans-lating into English.
Two factors possibly contribute512to this result: first, the fact that the baseline systemitself performs better when translating into French;second, and possibly more importantly, the fact thatwe had access to less training data for English-to-French translation.The second observation is that when Portage isused in automatic post-editing mode, on top of thebaseline MT system, it achieves better quality thaneither of the two translation systems used on its own.This appears to be true regardless of the translationdirection or metric.
This is an extremely interestingresult, especially in light of how little data was actu-ally available to train the post-editing system.One aspect of statistical MT systems is that, con-trary to rule-based systems, their performance (usu-ally) increases as more training data is available.
Inorder to quantify this effect in our setting, we havecomputed learning curves by training the Portagetranslation and Portage APE systems on subsets ofthe training data of increasing sizes.
We start withas little as 1000 blocks, which corresponds to around10-15k words.Figure 2 (next page) compares the learning ratesof the two competing approaches (Portage transla-tion vs. Portage APE).
Both approaches display verysteady learning rates (note the logarithmic scale fortraining data size).
These graphs strongly suggestthat both systems would continue to improve givenmore training data.
The most impressive aspect ishow little data is necessary to improve upon thebaseline, especially when translating into English:as little as 8000 blocks (around 100k words) for di-rect translation and 2000 blocks (around 25k words)for automatic post-editing.
This suggests that sucha post-editing setup might be worth implementingeven for specialized domains with very small vol-umes of data.4.3 ExtensionsGiven the encouraging results of the Portage APEapproach in the above experiments, we were curi-ous to see whether a Portage+Portage combinationmight be as successful: after all, if Portage was goodat correcting some other system?s output, could itnot manage to correct the output of another Portagetranslator?We tested this in two settings.
First, we actu-ally use the output of the Portage translation sys-Language TER BLEUEnglish-to-FrenchPortage Job Bank 53.7 36.0+ Portage APE 53.7 36.2Portage Hansard 76.9 13.0+ Portage APE 64.6 26.2French-to-EnglishPortage Job Bank 43.9 41.0+ Portage APE 43.9 41.4Portage Hansard 80.1 14.0+ Portage APE 57.7 28.6Table 3: Portage translation - Portage APE systemcombination experimental results.tem obtained above, i.e.
trained on the same data.In our second experiment, we use the output ofa Portage translator trained on different domaindata (the Canadian Hansard), but with much largeramounts of training material (over 85 million wordsper language).
In both sets of experiments, thePortage APE system was trained as previously, butusing Portage translations of the Job Bank data asinput text.The results of both experiments are presented inTable 3.
The first observation in these results is thatthere is nothing to be gained from post-editing whenboth the translation and APE systems are trained onthe same data sets (Portage Job Bank + Portage APEexperiments).
In other words, the translation systemis apparently already making the best possible use ofthe training data, and additional layers do not help(but nor do they hurt, interestingly).However, when the translation system has beentrained using distinct data (Portage Hansard +Portage APE experiments), post-editing makes alarge difference, comparable to that observed withthe rule-based MT output provided with the JobBank data.
In this case, however, the Portage trans-lation system behaves very poorly in spite of the im-portant size of the training set for this system, muchworse in fact than the ?baseline?
system.
This high-lights the fact that both the Job Bank and Hansarddata are very much domain-specific, and that accessto appropriate training material is crucial for phrase-based translation technology.In this context, combining two phrase-based sys-5131000 2000 5000 10000 200004045505560TER learning curvesTraining set sizeTERto Englishto FrenchPost?editionTranslation1000 2000 5000 10000 200000.300.350.400.45BLEU learning curvesTraining set sizeBLEUto Englishto FrenchPost?editionTranslationFigure 2: TER and BLEU scores of the phrase-based post-editing models as the amount of training dataincreases (log scale).
The horizontal lines correspond to the performance of the baseline system (rule-basedtranslation).tems as done here can be seen as a way of adaptingan existing MT system to a new text domain; theAPE system then acts as an ?adapter?, so to speak.Note however that, in our experiments, this setupdoesn?t perform as well as a single Portage transla-tion system, trained directly and exclusively on theJob Bank data.Such an adaptation strategy should be contrastedwith one in which the translation models of theold and new domains are ?merged?
to create a newtranslation system.
As mentioned earlier, Portageallows using multiple phrase translation tables andlanguage models concurrently.
For example, in thecurrent context, we can extract phrase tables and lan-guage models from the Job Bank data, as when train-ing the ?Portage Job Bank?
translation system, andthen build a Portage translation model using both theHansard and Job Bank model components.
Loglin-ear model parameters are then optimized on the JobBank data, so as to find the model weights that bestfit the new domain.In a straightforward implementation of this idea,we obtained performances almost identical to thoseof the Portage translation system trained solely onJob Bank data.
Upon closer examination of themodel parameters, we observed that Hansard modelcomponents (language model, phrase tables, IBMtranslation models) were systematically attributednegligeable weights.
Again, the amount of trainingmaterial for the new domain may be critical in chos-ing between alternative adaptation mechanisms.5 Conclusions and Future WorkWe have proposed using a phrase-based MT sys-tem to automatically post-edit the output of an-other MT system, and have tested this idea withthe Portage MT system on the Job Bank data set, acorpus of manually post-edited French-English ma-chine translations.
In our experiments, not only doesphrase-based APE significantly improve the qualityof the output translations, this approach outperformsa standalone phrase-based translation system.While these results are very encouraging, thelearning curves of Figure 2 suggest that the outputquality of the PBMT systems increases faster thanthat of the APE systems as more data is used fortraining.
So while the combination strategy clearlyperforms better with limited amounts of trainingdata, there is reason to believe that, given sufficienttraining data, it would eventually be outperformed514by a direct phrase-based translation strategy.
Ofcourse, this remains to be verified empirically, some-thing which will obviously require more data than iscurrently available to us.
But this sort of behavioris expectable: while both types of system improveas more training data is used, inevitably some de-tails of the source text will be lost by the front-endMT system, which the APE system will never beable to retrieve.5 Ultimately, the APE system willbe weighted down by the inherent limitations of thefront-end MT system.One way around this problem would be to modifythe APE system so that it not only uses the base-line MT output, but also the source-language input.In the Portage system, this could be achieved, forexample, by introducing feature functions into thelog-linear model that relate target-language phraseswith the source-language text.
This is one researchavenue that we are currently exploring.Alternatively, we could combine these two in-puts differently within Portage: for example, usethe source-language text as the primary input, anduse the raw MT output as a secondary source.
Inthis perspective, if we have multiple MT systemsavailable, nothing precludes using all of them asproviders of secondary inputs.
In such a setting, thephrase-based system becomes a sort of combinationMT system.
We intend to explore such alternativesin the near future as well.AcknowledgementsThe work reported here was part of a collaborationbetween the National Research Council of Canadaand the department of Human Resources and SocialDevelopment Canada.
Special thanks go to SouadBenayyoub, Jean-Fre?de?ric Hu?bsch and the rest ofthe Job Bank team at HRSDC for preparing data thatwas essential to this project.ReferencesJeffrey Allen and Christofer Hogan.
2000.
Towardthe development of a post-editing module for Ma-chine Translation raw output: a new productivity toolfor processing controlled language.
In Third Inter-5As a trivial example, imagine an MT system that ?deletes?out-of-vocabulary words.national Controlled Language Applications Workshop(CLAW2000), Washington, USA.Jeffrey Allen.
2004.
Case study: Implementing MT forthe translation of pre-sales marketing and post-salessoftware deployment documentation.
In Proceedingsof AMTA-2004, pages 1?6, Washington, USA.Peter F Brown, Stephen A Della Pietra, Vincent J DellaPietra, and Robert L Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?311.Jakob Elming.
2006.
Transformation-based correctionsof rule-based MT.
In Proceedings of the EAMT 11thAnnual Conference, Oslo, Norway.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical MachineTranslation.
In Proceedings of EMNLP 2006, pages53?61, Sydney, Australia.Kevin Knight and Ishwar Chander.
1994.
AutomatedPostediting of Documents.
In Proceedings of NationalConference on Artificial Intelligence, pages 779?784,Seattle, USA.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceed-ings of HLT-NAACL 2003, pages 127?133, Edmonton,Canada.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In Proceedings of AMTA 2004, pages115?124, Washington, USA.Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical Ma-chine Translation.
In Proceedings of EMNLP 2002,Philadelphia, USA.Franz Josef Och.
2003.
Minimum error rate trainingin Statistical Machine Translation.
In Proceedings ofACL-2003, pages 160?167, Sapporo, Japan.Fatiha Sadat, Howard Johnson, Akakpo Agbago, GeorgeFoster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.2005.
PORTAGE: A Phrase-Based Machine Trans-lation System.
In Proceedings of the ACL Workshopon Building and Using Parallel Texts, pages 129?132,Ann Arbor, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of AMTA-2006, Cambridge,USA.515
