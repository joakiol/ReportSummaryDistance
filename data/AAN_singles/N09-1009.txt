Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74?82,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsShared Logistic Normal Distributions for Soft Parameter Tyingin Unsupervised Grammar InductionShay B. Cohen and Noah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{scohen,nasmith}@cs.cmu.eduAbstractWe present a family of priors over probabilis-tic grammar weights, called the shared logisticnormal distribution.
This family extends thepartitioned logistic normal distribution, en-abling factored covariance between the prob-abilities of different derivation events in theprobabilistic grammar, providing a new wayto encode prior knowledge about an unknowngrammar.
We describe a variational EM al-gorithm for learning a probabilistic grammarbased on this family of priors.
We then experi-ment with unsupervised dependency grammarinduction and show significant improvementsusing our model for both monolingual learn-ing and bilingual learning with a non-parallel,multilingual corpus.1 IntroductionProbabilistic grammars have become an importanttool in natural language processing.
They are mostcommonly used for parsing and linguistic analy-sis (Charniak and Johnson, 2005; Collins, 2003),but are now commonly seen in applications like ma-chine translation (Wu, 1997) and question answer-ing (Wang et al, 2007).
An attractive property ofprobabilistic grammars is that they permit the useof well-understood parameter estimation methodsfor learning?both from labeled and unlabeled data.Here we tackle the unsupervised grammar learningproblem, specifically for unlexicalized context-freedependency grammars, using an empirical Bayesianapproach with a novel family of priors.There has been an increased interest recentlyin employing Bayesian modeling for probabilisticgrammars in different settings, ranging from puttingpriors over grammar probabilities (Johnson et al,2007) to putting non-parametric priors over deriva-tions (Johnson et al, 2006) to learning the set ofstates in a grammar (Finkel et al, 2007; Liang et al,2007).
Bayesian methods offer an elegant frame-work for combining prior knowledge with data.The main challenge in Bayesian grammar learningis efficiently approximating probabilistic inference,which is generally intractable.
Most commonly vari-ational (Johnson, 2007; Kurihara and Sato, 2006)or sampling techniques are applied (Johnson et al,2006).Because probabilistic grammars are built out ofmultinomial distributions, the Dirichlet family (or,more precisely, a collection of Dirichlets) is a naturalcandidate for probabilistic grammars because of itsconjugacy to the multinomial family.
Conjugacy im-plies a clean form for the posterior distribution overgrammar probabilities (given the data and the prior),bestowing computational tractability.Following work by Blei and Lafferty (2006) fortopic models, Cohen et al (2008) proposed an alter-native to Dirichlet priors for probabilistic grammars,based on the logistic normal (LN) distribution overthe probability simplex.
Cohen et al used this priorto softly tie grammar weights through the covarianceparameters of the LN.
The prior encodes informa-tion about which grammar rules?
weights are likelyto covary, a more intuitive and expressive represen-tation of knowledge than offered by Dirichlet distri-butions.1The contribution of this paper is two-fold.
First,from the modeling perspective, we present a gen-eralization of the LN prior of Cohen et al (2008),showing how to extend the use of the LN prior to1Although the task, underlying model, and weights beingtied were different, Eisner (2002) also showed evidence for theefficacy of parameter tying in grammar learning.74tie between any grammar weights in a probabilisticgrammar (instead of only allowing weights withinthe same multinomial distribution to covary).
Sec-ond, from the experimental perspective, we showhow such flexibility in parameter tying can help inunsupervised grammar learning in the well-knownmonolingual setting and in a new bilingual settingwhere grammars for two languages are learned atonce (without parallel corpora).Our method is based on a distribution which wecall the shared logistic normal distribution, whichis a distribution over a collection of multinomialsfrom different probability simplexes.
We provide avariational EM algorithm for inference.The rest of this paper is organized as follows.
In?2, we give a brief explanation of probabilistic gram-mars and introduce some notation for the specifictype of dependency grammar used in this paper, dueto Klein and Manning (2004).
In ?3, we present ourmodel and a variational inference algorithm for it.
In?4, we report on experiments for both monolingualsettings and a bilingual setting and discuss them.
Wediscuss future work (?5) and conclude in ?6.2 Probabilistic Grammars andDependency Grammar InductionA probabilistic grammar defines a probability dis-tribution over grammatical derivations generatedthrough a step-by-step process.
HMMs, for exam-ple, can be understood as a random walk througha probabilistic finite-state network, with an outputsymbol sampled at each state.
Each ?step?
of thewalk and each symbol emission corresponds to onederivation step.
PCFGs generate phrase-structuretrees by recursively rewriting nonterminal symbolsas sequences of ?child?
symbols (each itself eithera nonterminal symbol or a terminal symbol analo-gous to the emissions of an HMM).
Each step oremission of an HMM and each rewriting operationof a PCFG is conditionally independent of the otherrewriting operations given a single structural ele-ment (one HMM or PCFG state); this Markov prop-erty permits efficient inference for the probabilitydistribution defined by the probabilistic grammar.In general, a probabilistic grammar defines thejoint probability of a string x and a grammaticalderivation y:p(x,y | ?)
=K?k=1Nk?i=1?fk,i(x,y)k,i (1)= expK?k=1Nk?i=1fk,i(x,y) log ?k,iwhere fk,i is a function that ?counts?
the numberof times the kth distribution?s ith event occurs inthe derivation.
The ?
are a collection of K multi-nomials ?
?1, ...,?K?, the kth of which includes Nkevents.
Note that there may be many derivations yfor a given string x?perhaps even infinitely manyin some kinds of grammars.2.1 Dependency Model with ValenceHMMs and PCFGs are the best-known probabilis-tic grammars, but there are many others.
In thispaper, we use the ?dependency model with va-lence?
(DMV), due to Klein and Manning (2004).DMV defines a probabilistic grammar for unla-beled, projective dependency structures.
Klein andManning (2004) achieved their best results with acombination of DMV with a model known as the?constituent-context model?
(CCM).
We do not ex-periment with CCM in this paper, because it doesnot fit directly in a Bayesian setting (it is highly defi-cient) and because state-of-the-art unsupervised de-pendency parsing results have been achieved withDMV alone (Smith, 2006).Using the notation above, DMV defines x =?x1, x2, ..., xn?
to be a sentence.
x0 is a special?wall?
symbol, $, on the left of every sentence.
Atree y is defined by a pair of functions yleft andyright (both {0, 1, 2, ..., n} ?
2{1,2,...,n}) that mapeach word to its sets of left and right dependents,respectively.
Here, the graph is constrained to be aprojective tree rooted at x0 = $: each word except $has a single parent, and there are no cycles or cross-ing dependencies.
yleft(0) is taken to be empty, andyright(0) contains the sentence?s single head.
Lety(i) denote the subtree rooted at position i. Theprobability P (y(i) | xi,?)
of generating this sub-tree, given its head word xi, is defined recursively,as described in Fig.
1 (Eq.
2).The probability of the entire tree is given byp(x,y | ?)
= P (y(0) | $,?).
The ?
are the multi-nomial distributions ?s(?
| ?, ?, ?)
and ?c(?
| ?, ?).
To75P (y(i) | xi,?)
= ?D?
{left ,right} ?s(stop | xi,D , [yD(i) = ?])
(2)?
?j?yD (i) ?s(?stop | xi,D ,firsty(j))?
?c(xj | xi,D)?
P (y(j) | xj ,?
)Figure 1: The ?dependency model with valence?
recursive equation.
firsty(j) is a predicate defined to be true iff xj isthe closest child (on either side) to its parent xi.
The probability of the tree p(x,y | ?)
= P (y(0) | $,?
).follow the general setting of Eq.
1, we index thesedistributions as ?1, ...,?K .Headden et al (2009) extended DMV so that thedistributions ?c condition on the valence as well,with smoothing, and showed significant improve-ments for short sentences.
Our experiments foundthat these improvements do not hold on longer sen-tences.
Here we experiment only with DMV, butnote that our techniques are also applicable to richerprobabilistic grammars like that of Headden et al2.2 Learning DMVKlein and Manning (2004) learned the DMV prob-abilities ?
from a corpus of part-of-speech-taggedsentences using the EM algorithm.
EM manipulates?
to locally optimize the likelihood of the observedportion of the data (here, x), marginalizing out thehidden portions (here, y).
The likelihood surfaceis not globally concave, so EM only locally opti-mizes the surface.
Klein and Manning?s initializa-tion, though reasonable and language-independent,was an important factor in performance.Various alternatives to EM were explored bySmith (2006), achieving substantially more accu-rate parsing models by altering the objective func-tion.
Smith?s methods did require substantial hyper-parameter tuning, and the best results were obtainedusing small annotated development sets to choosehyperparameters.
In this paper, we consider onlyfully unsupervised methods, though we the Bayesianideas explored here might be merged with the bias-ing approaches of Smith (2006) for further benefit.3 Parameter Tying in the Bayesian SettingAs stated above, ?
comprises a collection of multi-nomials that weights the grammar.
Taking theBayesian approach, we wish to place a prior on thosemultinomials, and the Dirichlet family is a naturalcandidate for such a prior because of its conjugacy,which makes inference algorithms easier to derive.For example, if we make a ?mean-field assumption,?with respect to hidden structure and weights, thevariational algorithm for approximately inferring thedistribution over ?
and trees y resembles the tradi-tional EM algorithm very closely (Johnson, 2007).In fact, variational inference in this case takes an ac-tion similar to smoothing the counts using the exp-?function during the E-step.
Variational inference canbe embedded in an empirical Bayes setting, in whichwe optimize the variational bound with respect to thehyperparameters as well, repeating the process untilconvergence.3.1 Logistic Normal DistributionsWhile Dirichlet priors over grammar probabilitiesmake learning algorithms easy, they are limiting.In particular, as noted by Blei and Lafferty (2006),there is no explicit flexible way for the Dirichlet?sparameters to encode beliefs about covariance be-tween the probabilities of two events.
To illustratethis point, we describe how a multinomial ?
of di-mension d is generated from a Dirichlet distributionwith parameters ?
= ?
?1, ..., ?d?:1.
Generate ?j ?
?
(?j , 1) independently for j ?
{1, ..., d}.2.
?j ?
?j/?i ?i.where ?
(?, 1) is a Gamma distribution with shape ?and scale 1.Correlation among ?i and ?j , i 6= j, cannot bemodeled directly, only through the normalizationin step 2.
In contrast, LN distributions (Aitchison,1986) provide a natural way to model such correla-tion.
The LN draws a multinomial ?
as follows:1.
Generate ?
?
Normal(?,?).2.
?j ?
exp(?j)/?i exp(?i).76I1 = {1:2, 3:6, 7:9} = { I1,1, I1,2, I1,L1 }I2 = {1:2, 3:6} = { I2,1, I2,L2 }I3 = {1:4, 5:7} = { I3,1, I3,L3 }IN = {1:2} = { I4,L4 }J1 J2 JK??????????
?partition struct.
S?1 = ?
?1,1, ?1,2, ?1,3, ?1,4, ?1,5, ?1,6, ?1,7, ?1,8, ?1,`1?
?
Normal(?1,?1)?2 = ?
?2,1, ?2,2, ?2,3, ?2,4, ?2,5, ?2,`2?
?
Normal(?2,?2)?3 = ?
?3,1, ?3,2, ?3,3, ?3,4, ?3,5, ?3,6, ?3,`3?
?
Normal(?3,?3)?4 = ?
?4,1, ?4,`4?
?
Normal(?4,?4)??????
?sample ??
?1 = 13 ?
?1,1 + ?2,1 + ?4,1, ?1,2 + ?2,2 + ?4,2??
?2 = 13 ?
?1,3 + ?2,3 + ?3,1, ?1,4 + ?2,4 + ?3,2, ?1,5 + ?2,5 + ?3,3, ?1,6 + ?2,6 + ?3,4??
?3 = 12 ?
?1,7 + ?3,5, ?1,8 + ?3,6, ?1,9 + ?3,7????
combine ?
?1 = (exp ?
?1)/?N1i?=1 exp ??1,i?
?2 = (exp ?
?2)/?N2i?=1 exp ??2,i?
?3 = (exp ?
?3)/?N3i?=1 exp ??3,i?????????
?softmaxFigure 2: An example of a shared logistic normal distribution, illustrating Def.
1.
N = 4 experts are used to sampleK = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, andN3 = 3.
This figure is best viewed in color.Blei and Lafferty (2006) defined correlated topicmodels by replacing the Dirichlet in latent Dirich-let alocation models (Blei et al, 2003) with a LNdistribution.
Cohen et al (2008) compared Dirichletand LN distributions for learning DMV using em-pirical Bayes, finding substantial improvements forEnglish using the latter.In that work, we obtained improvements evenwithout specifying exactly which grammar proba-bilities covaried.
While empirical Bayes learningpermits these covariances to be discovered withoutsupervision, we found that by initializing the covari-ance to encode beliefs about which grammar prob-abilities should covary, further improvements werepossible.
Specifically, we grouped the Penn Tree-bank part-of-speech tags into coarse groups basedon the treebank annotation guidelines and biasedthe initial covariance matrix for each child distri-bution ?c(?
| ?, ?)
so that the probabilities of childtags from the same coarse group covaried.
For ex-ample, the probability that a past-tense verb (VBD)has a singular noun (NN) as a right child may becorrelated with the probability that it has a plu-ral noun (NNS) as a right child.
Hence linguisticknowledge?specifically, a coarse grouping of wordclasses?can be encoded in the prior.A per-distribution LN distribution only permitsprobabilities within a multinomial to covary.
Wewill generalize the LN to permit covariance amongany probabilities in ?, throughout the model.
Forexample, the probability of a past-tense verb (VBD)having a noun as a right child might correlate withthe probability that other kinds of verbs (VBZ, VBN,etc.)
have a noun as a right child.The partitioned logistic normal distribution(PLN) is a generalization of the LN distributionthat takes the first step towards our goal (Aitchison,1986).
Generating from PLN involves drawing arandom vector from a multivariate normal distribu-tion, but the logistic transformation is applied to dif-ferent parts of the vector, leading to sampled multi-nomial distributions of the required lengths fromdifferent probability simplices.
This is in principlewhat is required for arbitrary covariance betweengrammar probabilities, except that DMV has O(t2)weights for a part-of-speech vocabulary of size t, re-quiring a very large multivariate normal distributionwith O(t4) covariance parameters.773.2 Shared Logistic Normal DistributionsTo solve this problem, we suggest a refinement ofthe class of PLN distributions.
Instead of using asingle normal vector for all of the multinomials, weuse several normal vectors, partition each one andthen recombine parts which correspond to the samemultinomial, as a mixture.
Next, we apply the lo-gisitic transformation on the mixed vectors (each ofwhich is normally distributed as well).
Fig.
2 givesan example of a non-trivial case of using a SLNdistribution, where three multinomials are generatedfrom four normal experts.We now formalize this notion.
For a natural num-ber N , we denote by 1:N the set {1, ..., N}.
For avector in v ?
RN and a set I ?
1:N , we denoteby vI to be the vector created from v by using thecoordinates in I .
Recall that K is the number ofmultinomials in the probabilistic grammar, and Nkis the number of events in the kth multinomial.Definition 1.
We define a shared logistic nor-mal distribution with N ?experts?
over a collec-tion of K multinomial distributions.
Let ?n ?Normal(?n,?n) be a set of multivariate normalvariables for n ?
1:N , where the length of ?nis denoted `n. Let In = {In,j}Lnj=1 be a parti-tion of 1:`n into Ln sets, such that ?Lnj=1In,j =1:`n and In,j ?
In,j?
= ?
for j 6= j?.
Let Jkfor k ?
1:K be a collection of (disjoint) sub-sets of {In,j | n ?
1:N, j ?
1:`n, |In,j | =Nk}, such that all sets in Jk are of the same size,Nk.
Let ?
?k = 1|Jk|?In,j?Jk ?n,In,j , and ?k,i =exp(??k,i)/?i?
exp(??k,i?)
.
We then say ?
distributesaccording to the shared logistic normal distributionwith partition structure S = ({In}Nn=1, {Jk}Kk=1)and normal experts {(?n,?n)}Nn=1 and denote it by?
?
SLN(?,?, S).The partitioned LN distribution in Aitchison(1986) can be formulated as a shared LN distributionwhere N = 1.
The LN collection used by Cohen etal.
(2008) is the special case where N = K, eachLn = 1, each `k = Nk, and each Jk = {Ik,1}.The covariance among arbitrary ?k,i is not defineddirectly; it is implied by the definition of the nor-mal experts ?n,In,j , for each In,j ?
Jk.
We notethat a SLN can be represented as a PLN by relyingon the distributivity of the covariance operator, andmerging all the partition structure into one (perhapssparse) covariance matrix.
However, if we are inter-ested in keeping a factored structure on the covari-ance matrices which generate the grammar weights,we cannot represent every SLN as a PLN.It is convenient to think of each ?i,j as a weightassociated with a unique event?s probability, a cer-tain outcome of a certain multinomial in the prob-abilistic grammar.
By letting different ?i,j covarywith each other, we loosen the relationships among?k,j and permit the model?at least in principle?to learn patterns from the data.
Def.
1 also impliesthat we multiply several multinomials together in aproduct-of-experts style (Hinton, 1999), because theexponential of a mixture of normals becomes a prod-uct of (unnormalized) probabilities.Our extension to the model in Cohen et al (2008)follows naturally after we have defined the sharedLN distribution.
The generative story for this modelis as follows:1.
Generate ?
?
SLN(?,?, S), where ?
is a col-lection of vectors ?k, k = 1, ...,K.2.
Generate x and y from p(x,y | ?)
(i.e., samplefrom the probabilistic grammar).3.3 InferenceIn this work, the partition structure S is known, thesentences x are observed, the trees y and the gram-mar weights ?
are hidden, and the parameters of theshared LN distribution ?
and ?
are learned.2Our inference algorithm aims to find the poste-rior over the grammar probabilities ?
and the hiddenstructures (grammar trees y).
To do that, we usevariational approximation techniques (Jordan et al,1999), which treat the problem of finding the pos-terior as an optimization problem aimed to find thebest approximation q(?,y) of the posterior p(?,y |x,?,?, S).
The posterior q needs to be constrainedto be within a family of tractable and manageabledistributions, yet rich enough to represent good ap-proximations of the true posterior.
?Best approx-imation?
is defined as the KL divergence betweenq(?,y) and p(?,y | x,?,?, S).Our variational inference algorithm uses a mean-field assumption: q(?,y) = q(?)q(y).
The distri-bution q(?)
is assumed to be a LN distribution with2In future work, we might aim to learn S.78log p(x | ?,?, S) ?
(?Nn=1 Eq [log p(?k | ?k,?k)])+(?Kk=1?Nki=1 f?k,i??k,i)+H(q)?
??
?B(3)f?k,i , ?y q(y)fk,i(x,y) (4)?
?k,i , ?
?Ck,i ?
log ?
?k + 1?
1?
?k?Nki?=1 exp(?
?Ck,i +(??Ck,i)22)(5)?
?Ck , 1|Jk|?Ir,j?Jk ?
?r,Ir,j (6)(?
?Ck )2 , 1|Jk|2?Ir,j?Jk ?
?2r,Ir,j (7)Figure 3: Variational inference bound.
Eq.
3 is the bound itself, using notation defined in Eqs.
4?7 for clarity.
Eq.
4defines expected counts of the grammar events under the variational distribution q(y), calculated using dynamic pro-gramming.
Eq.
5 describes the weights for the weighted grammar defined by q(y).
Eq.
6 and Eq.
7 describe the meanand the variance, respectively, for the multivariate normal eventually used with the weighted grammar.
These valuesare based on the parameterization of q(?)
by ?
?i,j and ?
?2i,j .
An additional set of variational parameters is ?
?k, whichhelps resolve the non-conjugacy of the LN distribution through a first order Taylor approximation.all off-diagonal covariances fixed at zero (i.e., thevariational parameters consist of a single mean ?
?k,iand a single variance ?
?2k,i for each ?k,i).
There isan additional variational parameter, ?
?k per multino-mial, which is the result of an additional variationalapproximation because of the lack of conjugacy ofthe LN distribution to the multinomial distribution.The distribution q(y) is assumed to be defined by aDMV with unnormalized probabilities ?
?.Inference optimizes the bound B given in Fig.
3(Eq.
3) with respect to the variational parameters.Our variational inference algorithm is derived simi-larly to that of Cohen et al (2008).
Because we wishto learn the values of?
and?, we embed variationalinference as the E step within a variational EM algo-rithm, shown schematically in Fig.
4.
In our exper-iments, we use this variational EM algorithm on atraining set, and then use the normal experts?
meansto get a point estimate for ?, the grammar weights.This is called empirical Bayesian estimation.
Ourapproach differs from maximum a posteriori (MAP)estimation, since we re-estimate the parameters ofthe normal experts.
Exact MAP estimation is prob-ably not feasible; a variational algorithm like oursmight be applied, though better performance is ex-pected from adjusting the SLN to fit the data.4 ExperimentsOur experiments involve data from two treebanks:the Wall Street Journal Penn treebank (Marcus etal., 1993) and the Chinese treebank (Xue et al,2004).
In both cases, following standard practice,sentences were stripped of words and punctuation,leaving part-of-speech tags for the unsupervised in-duction of dependency structure.
For English, wetrain on ?2?21, tune on ?22 (without using annotateddata), and report final results on ?23.
For Chinese,we train on ?1?270, use ?301?1151 for developmentand report testing results on ?271?300.3To evaluate performance, we report the fractionof words whose predicted parent matches the goldstandard corpus.
This performance measure is alsoknown as attachment accuracy.
We considered twoparsing methods after extracting a point estimatefor the grammar: the most probable ?Viterbi?
parse(argmaxy p(y | x,?))
and the minimum Bayes risk(MBR) parse (argminy Ep(y?|x,?)[`(y;x,y?)])
withdependency attachment error as the loss function(Goodman, 1996).
Performance with MBR parsingis consistently higher than its Viterbi counterpart, sowe report only performance with MBR parsing.4.1 Nouns, Verbs, and AdjectivesIn this paper, we use a few simple heuristics to de-cide which partition structure S to use.
Our heuris-3Unsupervised training for these datasets can be costly,and requires iteratively running a cubic-time inside-outside dy-namic programming algorithm, so we follow Klein and Man-ning (2004) in restricting the training set to sentences of ten orfewer words in length.
Short sentences are also less structurallyambiguous and may therefore be easier to learn from.79Input: initial parameters ?
(0), ?
(0), partitionstructure S, observed data x, number ofiterations TOutput: learned parameters ?, ?t?
1 ;while t ?
T doE-step (for ` = 1, ...,M ) do: repeatoptimize B w.r.t.
?
?`,(t)r , r = 1, ..., N ;optimize B w.r.t.
?
?`,(t)r , r = 1, ..., N ;update ?
?`,(t)r , r = 1, ..., N ;update ?
?`,(t)r , r = 1, ..., N ;compute counts f?
`,(t)r , r = 1, ..., N ;until convergence of B ;M-step: optimize B w.r.t.
?
(t) and ?(t);t?
t+ 1;endreturn ?
(T ), ?
(T )Figure 4: Main details of the variational inference EMalgorithm with empirical Bayes estimation of ?
and ?.B is the bound defined in Fig.
3 (Eq.
3).
N is the numberof normal experts for the SLN distribution defining theprior.
M is the number of training examples.
The fullalgorithm is given in Cohen and Smith (2009).tics rely mainly on the centrality of content words:nouns, verbs, and adjectives.
For example, in the En-glish treebank, the most common attachment errors(with the LN prior from Cohen et al, 2008) happenwith a noun (25.9%) or a verb (16.9%) parent.
Inthe Chinese treebank, the most common attachmenterrors happen with noun (36.0%) and verb (21.2%)parents as well.
The errors being governed by suchattachments are the direct result of nouns and verbsbeing the most common parents in these data sets.Following this observation, we compare four dif-ferent settings in our experiments (all SLN settingsinclude one normal expert for each multinomial onits own, equivalent to the regular LN setting fromCohen et al):?
TIEV: We add normal experts that tie all proba-bilities corresponding to a verbal parent (any par-ent, using the coarse tags of Cohen et al, 2008).Let V be the set of part-of-speech tags which be-long to the verb category.
For each direction D(left or right), the set of multinomials of the form?c(?
| v,D), for v ?
V , all share a normal expert.For each direction D and each boolean value Bof the predicate firsty(?
), the set of multinomials?s(?
| x,D , v), for v ?
V share a normal expert.?
TIEN: This is the same as TIEV, only for nominalparents.?
TIEV&N: Tie both verbs and nouns (in separatepartitions).
This is equivalent to taking the unionof the partition structures of the above two set-tings.?
TIEA: This is the same as TIEV, only for adjecti-val parents.Since inference for a model with parameter tyingcan be computationally intensive, we first run the in-ference algorithm without parameter tying, and thenadd parameter tying to the rest of the inference algo-rithm?s execution until convergence.Initialization is important for the inference al-gorithm, because the variational bound is a non-concave function.
For the expected values of thenormal experts, we use the initializer from Klein andManning (2004).
For the covariance matrices, wefollow the setting in Cohen et al (2008) in our ex-periments also described in ?3.1.
For each treebank,we divide the tags into twelve disjoint tag families.4The covariance matrices for all dependency distri-butions were initialized with 1 on the diagonal, 0.5between tags which belong to the same family, and0 otherwise.
This initializer has been shown to bemore successful than an identity covariance matrix.4.2 Monolingual ExperimentsWe begin our experiments with a monolingual set-ting, where we learn grammars for English and Chi-nese (separately) using the settings described above.The attachment accuracy for this set of experi-ments is described in Table 1.
The baselines includeright attachment (where each word is attached to theword to its right), MLE via EM (Klein and Man-ning, 2004), and empirical Bayes with Dirichlet andLN priors (Cohen et al, 2008).
We also include a?ceiling?
(DMV trained using supervised MLE fromthe training sentences?
trees).
For English, we seethat tying nouns, verbs or adjectives improves per-formance compared to the LN baseline.
Tying bothnouns and verbs improves performance a bit more.4These are simply coarser tags: adjective, adverb, conjunc-tion, foreign word, interjection, noun, number, particle, prepo-sition, pronoun, proper noun, verb.80attachment acc.
(%)?
10 ?
20 allEnglishAttach-Right 38.4 33.4 31.7EM (K&M, 2004) 46.1 39.9 35.9Dirichlet 46.1 40.6 36.9LN (CG&S, 2008) 59.4 45.9 40.5SLN, TIEV 60.2 46.2 40.0SLN, TIEN 60.2 46.7 40.9SLN, TIEV&N 61.3 47.4 41.4SLN, TIEA 59.9 45.8 40.9Biling.
SLN, TIEV ?61.6 47.6 41.7Biling.
SLN, TIEN ?61.8 48.1 ?42.1Biling.
SLN, TIEV&N 62.0 ?48.0 42.2Biling.
SLN, TIEA 61.3 47.6 41.7Supervised MLE 84.5 74.9 68.8ChineseAttach-Right 34.9 34.6 34.6EM (K&M, 2004) 38.3 36.1 32.7Dirichlet 38.3 35.9 32.4LN 50.1 40.5 35.8SLN, TIEV ?51.9 42.0 35.8SLN, TIEN 43.0 38.4 33.7SLN, TIEV&N 45.0 39.2 34.2SLN, TIEA 47.4 40.4 35.2Biling.
SLN, TIEV ?51.9 42.0 35.8Biling.
SLN, TIEN 48.0 38.9 33.8Biling.
SLN, TIEV&N ?51.5 ?41.7 35.3Biling.
SLN, TIEA 52.0 41.3 35.2Supervised MLE 84.3 66.1 57.6Table 1: Attachment accuracy of different models, on testdata from the Penn Treebank and the Chinese Treebankof varying levels of difficulty imposed through a lengthfilter.
Attach-Right attaches each word to the word onits right and the last word to $.
Bold marks best overallaccuracy per length bound, and ?
marks figures that arenot significantly worse (binomial sign test, p < 0.05).4.3 Bilingual ExperimentsLeveraging information from one language for thetask of disambiguating another language has re-ceived considerable attention (Dagan, 1991; Smithand Smith, 2004; Snyder and Barzilay, 2008; Bur-kett and Klein, 2008).
Usually such a setting re-quires a parallel corpus or other annotated data thatties between those two languages.5Our bilingual experiments use the English andChinese treebanks, which are not parallel corpora,to train parsers for both languages jointly.
Shar-5Haghighi et al (2008) presented a technique to learn bilin-gual lexicons from two non-parallel monolingual corpora.ing information between those two models is doneby softly tying grammar weights in the two hiddengrammars.We first merge the models for English and Chi-nese by taking a union of the multinomial fami-lies of each and the corresponding prior parame-ters.
We then add a normal expert that ties be-tween the parts of speech in the respective parti-tion structures for both grammars together.
Partsof speech are matched through the single coarsetagset (footnote 4).
For example, with TIEV, letV = V Eng ?V Chi be the set of part-of-speech tagswhich belong to the verb category for either tree-bank.
Then, we tie parameters for all part-of-speechtags in V .
We tested this joint model for each ofTIEV, TIEN, TIEV&N, and TIEA.
After runningthe inference algorithm which learns the two mod-els jointly, we use unseen data to test each learnedmodel separately.Table 1 includes the results for these experiments.The performance on English improved significantlyin the bilingual setting, achieving highest perfor-mance with TIEV&N.
Performance with Chinese isalso the highest in the bilingual setting, with TIEAand TIEV&N.5 Future WorkIn future work we plan to lexicalize the model, in-cluding a Bayesian grammar prior that accounts forthe syntactic patterns ofwords.
Nonparametric mod-els (Teh, 2006) may be appropriate.
We also believethat Bayesian discovery of cross-linguistic patternsis an exciting topic worthy of further exploration.6 ConclusionWe described a Bayesian model that allows soft pa-rameter tying among any weights in a probabilisticgrammar.
We used this model to improve unsuper-vised parsing accuracy on two different languages,English and Chinese, achieving state-of-the-art re-sults.
We also showed how our model can be effec-tively used to simultaneously learn grammars in twolanguages from non-parallel multilingual data.AcknowledgmentsThis research was supported by NSF IIS-0836431.
Theauthors thank the anonymous reviewers and Sylvia Reb-holz for helpful comments.81ReferencesJ.
Aitchison.
1986.
The Statistical Analysis of Composi-tional Data.
Chapman and Hall, London.D.
M. Blei and J. D. Lafferty.
2006.
Correlated topicmodels.
In Proc.
of NIPS.D.
M. Blei, A. Ng, and M. Jordan.
2003.
Latent Dirich-let alocation.
Journal of Machine Learning Research,3:993?1022.D.
Burkett and D. Klein.
2008.
Two languages are betterthan one (for syntactic parsing).
In Proc.
of EMNLP.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProc.
of ACL.S.
B. Cohen and N. A. Smith.
2009.
Inference for proba-bilistic grammars with shared logistic normal distribu-tions.
Technical report, Carnegie Mellon University.S.
B. Cohen, K. Gimpel, and N. A. Smith.
2008.
Logisticnormal priors for unsupervised probabilistic grammarinduction.
In NIPS.M.
Collins.
2003.
Head-driven statistical models for nat-ural language processing.
Computational Linguistics,29:589?637.I.
Dagan.
1991.
Two languages are more informativethan one.
In Proc.
of ACL.J.
Eisner.
2002.
Transformational priors over grammars.In Proc.
of EMNLP.J.
R. Finkel, T. Grenager, and C. D. Manning.
2007.
Theinfinite tree.
In Proc.
of ACL.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProc.
of ACL.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.2008.
Learning bilingual lexicons from monolingualcorpora.
In Proc.
of ACL.W.
P. Headden, M. Johnson, and D. McClosky.
2009.Improving unsupervised dependency parsing withricher contexts and smoothing.
In Proc.
of NAACL-HLT.G.
E. Hinton.
1999.
Products of experts.
In Proc.
ofICANN.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2006.Adaptor grammars: A framework for specifying com-positional nonparameteric Bayesian models.
In NIPS.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2007.Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Proc.
of NAACL.M.
Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proc.
EMNLP-CoNLL.M.
I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.Saul.
1999.
An introduction to variational methodsfor graphical models.
Machine Learning, 37(2):183?233.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In Proc.
of ACL.K.
Kurihara and T. Sato.
2006.
Variational Bayesiangrammar induction for natural language.
In Proc.
ofICGI.P.
Liang, S. Petrov, M. Jordan, and D. Klein.
2007.
Theinfinite PCFG using hierarchical Dirichlet processes.In Proc.
of EMNLP.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn treebank.
Computational Linguistics,19:313?330.D.
A. Smith and N. A. Smith.
2004.
Bilingual parsingwith factored estimation: Using English to parse Ko-rean.
In Proc.
of EMNLP, pages 49?56.N.
A. Smith.
2006.
Novel Estimation Methods for Unsu-pervised Discovery of Latent Structure in Natural Lan-guage Text.
Ph.D. thesis, Johns Hopkins University.B.
Snyder and R. Barzilay.
2008.
Unsupervised multi-lingual learning for morphological segmentation.
InProc.
of ACL.Y.
W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proc.
ofCOLING-ACL.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
a quasi-synchronous grammarfor question answering.
In Proc.
of EMNLP.D.
Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Comp.
Ling., 23(3):377?404.N.
Xue, F. Xia, F.-D. Chiou, and M. Palmer.
2004.
ThePenn Chinese Treebank: Phrase structure annotationof a large corpus.
Natural Language Engineering,10(4):1?30.82
