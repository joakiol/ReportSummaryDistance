Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 25?30,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsJoint memory-based learning of syntactic and semantic dependenciesin multiple languagesRoser Morante, Vincent Van AschCNTS - Language Technology GroupUniversity of AntwerpPrinsstraat 13B-2000 Antwerpen, Belgium{Roser.Morante,Vincent.VanAsch}@ua.ac.beAntal van den BoschTilburg UniversityTilburg centre for Creative ComputingP.O.
Box 90153NL-5000 LE Tilburg, The NetherlandsAntal.vdnBosch@uvt.nlAbstractIn this paper we present a system submitted to theCoNLL Shared Task 2009 performing the identifi-cation and labeling of syntactic and semantic depen-dencies in multiple languages.
Dependencies aretruly jointly learned, i.e.
as if they were a singletask.
The system works in two phases: a classifica-tion phase in which three classifiers predict differenttypes of information, and a ranking phase in whichthe output of the classifiers is combined.1 IntroductionIn this paper we present the machine learning systemsubmitted to the CoNLL Shared Task 2009 (Hajic?et al, 2009).
The task is an extension to multi-ple languages (Burchardt et al, 2006; Hajic?
et al,2006; Kawahara et al, 2002; Palmer and Xue, 2009;Surdeanu et al, 2008; Taule?
et al, 2008) of theCoNLL Shared Task 2008, combining the identifica-tion and labeling of syntactic dependencies and se-mantic roles.
Our system is a joint-learning systemtested in the ?closed?
challenge, i.e.
without makinguse of external resources.Our system operates in two phases: a classifica-tion phase in which three memory-based classifierspredict different types of information, and a rank-ing phase in which the output of the classifiers iscombined by ranking the predictions.
Semantic andsyntactic dependencies are jointly learned and pro-cessed.
In the task description no precise defini-tion is given of joint learning.
We consider that ajoint-learning system is one in which semantic andsyntactic dependencies are learned and processedjointly as a single task.
In our system this is achievedby fully merging semantic and syntactic dependen-cies at the word level as the first step.One direct consequence of merging the two tasks,is that the class space becomes more complex;the number of classes increases.
Many machine-learning approaches do not scale well to larger classspaces in terms of efficiency and computer resourcerequirements.
Memory-based learning is a noted ex-ception, as it is largely insensitive to the number ofclasses in terms of efficiency.
This is the primaryreason for using memory-based learning.
Memory-based language processing (Daelemans and van denBosch, 2005) is based on the idea that NLP prob-lems can be solved by storing solved examples of theproblem in their literal form in memory, and apply-ing similarity-based reasoning on these examples inorder to solve new ones.
Memory-based algorithmshave been previously applied to semantic role la-beling and parsing separately (Morante et al, 2008;Canisius and Tjong Kim Sang, 2007).We briefly discuss the issue of true joint learningof two tasks in Section 2.
The system is describedin Section 3, Section 4 presents and discusses theresults, and in Section 5 we put forward some con-clusions and future research.2 Joint learningWhen two tasks share the same feature space, thereis the natural option to merge them and consider themerge as a single task.
The merging of two taskswill typically lead to an increase in the number ofclasses, and generally a more complex class space.In practice, if two combined tasks are to some ex-25tent related, the increase will tend to be less than theproduct of the number of classes in the two originaltasks, as classes from both tasks will tend to cor-relate.
Yet, even a mild increase of the number ofclasses leads to a further fragmentation of the classspace, and thus to less training examples per classlabel.
Joint learning can therefore only lead to posi-tive results if the data sparsity effect of the fragmen-tation of the class space is counter-balanced by animproved learnability.Here, we treat the syntactic and semantic tasks asone and the same task.
At the word level, we mergethe class labels of the two tasks into single labels,and present the classifiers with these labels.
Furtheron in our system, as we describe in the next sec-tion, we do make use of the compositionality of thelabels, as the semantic and syntactic output spacesrepresented two different types of structure.3 System descriptionThe joint system that we submitted works intwo phases: a classification phase in which threememory-based classifiers predict different aspects ofjoint syntactic and semantic labeling, and a rankingphase in which the output of the classifiers is com-bined.
Additionally, a memory-based classifier isused for predicate sense disambiguation.
As a firststep, before generating the instances of the classi-fiers we merge the semantic and syntactic dependen-cies into single labels.
The merged version of thedependencies from an example sentence is shownin Table 1, where column MERGED DEPs containsall the dependencies of a token separated by a blankspace expressed in labels with the following format:PHEAD::PDEPREL:APRED.3.1 Phase 1: ClassificationIn the classification phase, three classifiers predictdifferent local aspects of the global output structure.The classifiers have been optimized for English, bytraining on the full training set and testing on thedevelopment set; these optimized settings were thenused for the other six languages.
We experimentedwith manually selected parameters and with param-eters selected by a genetic algorithm, but the param-eters found by the genetic algorithm did not yieldbetter results than the manually selected parameters.N Token Merged Dependencies1 Housing 2::NMOD:A12 starts 2:: :A2 3::SBJ: 4:: :A1 6:: :A1 13:: :A03 are 0::ROOT:4 expected 3::VC:5 to 4::OPRD:C-A16 quicken 5::IM:7 a 8::NMOD:8 bit 6::OBJ:A29 from 6::ADV:A310 August 13::NMOD:AM-TMP11 ?s 10::SUFFIX:12 annual 13::NMOD:AM-TMP13 pace 9::PMOD:14 of 13::NMOD:A215 1,350,000 16::NMOD:16 units 14::PMOD:17 .
3::P:Table 1: Example sentence with merged depen-dency labels.3.1.1 Classifier 1: Pairwise semantic andsyntact dependenciesClassifier 1 predicts the merged semantic and syn-tactic dependencies that hold between two tokens.Instances represent combinations of pairs of tokenswithin a sentence.
Each token is combined with allother tokens in the sentence.
The class predicted isthe PDEPREL:APRED label.
The amount of classesper language is shown in Table 2 (?Classifier 1?
).Number of classesLang.
Classifier 1 Classifier 2Cat 111 111Chi 309 1209Cze 395 1221Eng 351 1957Ger 152 300Jap 103 505Spa 124 124Table 2: Number of classes per language predictedby Classifiers 1 and 2.We use an IB1 memory?based algorithm as im-plemented in TiMBL (version 6.1.2) 1, a memory-based classifier based on the k-nearest neighbor1TiMBL: http://ilk.uvt.nl/timbl26rule.
The IB1 algorithm was parameterised by us-ing modified value difference as the similarity met-ric, gain ratio for feature weighting, using 11 k-nearest neighbors, and weighting the class vote ofneighbors as a function of their inverse linear dis-tance.
Because of time limitations we used TRIBLfor Czech and Chinese to produce the official results,although we also provide postevaluation results pro-duced with IB1.
TRIBL is a hybrid combinationof IB1 and IGTREE, a fast decision-tree approxi-mation of k-NN (Daelemans and van den Bosch,2005), trading off fast decision-tree lookup on themost important features (in our experiments, five)with slower k-NN classification on the remainingfeatures.The features2 used by this classifier are:?
The word, lemma, POS and FILLPRED3 of the to-ken, the combined token and of two tokens beforeand after token and combined token.?
POS and FILLPRED of the third token before andafter token and combined token.?
Distance between token and combined token, loca-tion of token in relation to combined token.Because data are skewed towards the NONEclass, we downsampled the training instances so thatthere would be a negative instance for every positiveinstance.
Instances with the NONE class to be keptwere randomly selected.3.1.2 Classifier 2: Per-token relationsClassifier 2 predicts the labels of the dependencyrelations of a token with its syntactic and/or seman-tic head(s).
Instances represent a token.
As an ex-ample, the instance that represents token 2 in Table 1would have as class: :A2-SBJ: - :A1- :A1- :A0.The amount of classes per language is shown in Ta-ble 2 under ?Classifier 2?.
The number of classesexceeds 1,000 for Chinese, Czech, and English.The features used by the classifier are the word,lemma, POS and FILLPRED of the token and twotokens before and after the token.
We use the IB1memory?based algorithm parameterised in the sameway as Classifier 1.2POS refers to predicted part-of-speech and lemma to pre-dicted lemma in the description of features for all classifiers.3The FILLPRED column has value Y if a token is a predi-cate.3.1.3 Classifier 3: Pairwise detection of arelationClassifier 3 is a binary classifier that predictswhether two tokens have a dependency relation.
In-stance representation follows the same scheme aswith Classifier 1.
We use the IGTREE algorithm asimplemented in TiMBL.
The data are also skewedtowards the NONE class, so we downsampled thetraining instances so that there would be a negativeinstance for every four positive instances.The features used by this classifier are:?
The word, lemma, POS and FILLPRED of the to-ken, of the combined token, and of two tokens be-fore and after the token.?
Word and lemma of two tokens before and aftercombined token.?
Distance between token and combined token.3.1.4 ResultsThe results of the Classifiers are presented in Ta-ble 3.
The performance of Classifiers 1 and 3 is sim-ilar across languages, whereas the scores for Clas-sifier 2 are lower for Chinese, Czech and English.This can be explained by the fact that the number ofclasses that Classifier 2 predicts for these languagesis significantly higher.Lang.
C1 C2 C3Cat 94.77 86.30 97.96Chi 92.10 70.11 95.47Cze 87.33 67.87 93.88Eng 94.17 76.16 95.37Ger 92.76 83.23 93.77Jap 91.55 81.22 96.75Spa 94.76 84.40 96.39Table 3: Micro F1 scores per classifier (C) and perlanguage.Training times for the three classifiers were rea-sonably short, as is to be expected with memory-based classification.
With English, C2 takes justover two minutes to train, and C3 half a minute.
C1takes 8 hours and 18 minutes, due to the much largeramount of examples and features.3.2 Phase 2: RankingThe classifier that is at the root of generating thedesired output (dependency graphs and semantic27role assignments) is Classifier 1, which predicts themerged semantic and syntactic dependencies thathold between two tokens (PDEPREL:APRED la-bels).
If this classifier would be able to predict thedependencies with 100% accuracy, no further pro-cessing would be necessary.
Naturally, however, theclassifier predicts incorrect dependencies to a certaindegree, and does not provide a graph in wich all to-kens have at least a syntactic head.
It achieves 51.3%labeled macro F1.
The ranking phase improves thisperformance.
This is done in three steps: (i) rankingthe predictions of Classifier 1; (ii) constructing anintermediate dependency tree, and (iii) adding extrasemantic dependencies to the tree.3.2.1 Ranking predictions of Classifier 1In order to disambiguate between all possible de-pendencies predicted by this classifier, the systemapplies ranking rules.
It analyses the dependencyrelations that have been predicted for a token withits potential parents in the sentence and ranks them.For example, for a sentence with 10 tokens, the sys-tem would make 10 predictions per token.
The pre-dictions are first ranked by entropy of the class dis-tribution for that prediction, then using the output ofClassifier 2, and next using the output of Classifier 3.Ranking by entropy In order to compute entropywe use the (inverse-linear) distance-weighted classlabel distributions among the nearest neighbors thatClassifier 1 was able to find.
For example, the pre-diction for an instance can be: { NONE (2.74),NMOD: (0.48) }.
We can compute the entropy forthis instance using the formula in (1):?n?i=1P (labeli)log2(P (labeli)) (1)with- n: the total number of different labels in the distri-bution, and- P (labeli): the weight of label ithe total sum of the weights in the distributionThe system ranks the prediction with the lowestentropy in position 1, while the prediction with thehighest entropy is ranked in the last position.
Therationale behind this is that the lower the entropy,the more certain the classifier is about the predicteddependency.
Table 4 lists the first six heads for thepredicate word ?starts?
ranked by entropy (cf.
Ta-ble 1).Head Predicted label Distribution EntropyHousing NONE { NONE (8.51) } 0.0expected :A1 { :A1 (5.64) } 0.0to NONE { NONE (4.74) } 0.0quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56are NONE { NONE (2.56), SBJ: (0.52) } 0.65starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93Table 4: Output of Classifier 1 for the first six headsof ?starts?, ranked by entropy.On the development data for English, applyingthis rule causes a marked error reduction of 26.5%on labeled macro F1: from 51.3% to 64.2%.Ranking by Classifier 2 The next ranking step isperformed by using the predictions of Classifier 2,i.e.
the estimated labels of the dependency rela-tions of a token with its syntactic and/or semantichead(s).
The system ranks the predictions that arenot in the set of possible dependencies predicted byClassifier 2 at the bottom of the ranked list.Head Predicted label Distribution Entropyexpected :A1 { :A1 (5.64) } 0.0Housing NONE { NONE (8.51) } 0.0to NONE { NONE (4.74) } 0.0quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56are NONE { NONE (2.56), SBJ: (0.52) } 0.65starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93Table 5: Output of Classifier 1 for the first six headsof ?starts?.
Ranked by entropy and Classifier 2.Because this is done after ranking by entropy, theinstances with the lowest entropy are still at the topof the list.
Table 5 displays the re-ranked six headsof ?starts?, given that Classifier 2 has predicted thatpossible relations to heads are SBJ:A1 and :A1, andgiven that only ?expected?
is associated with one ofthese two relations.On the development data for English, applyingthis rule induces a 9.0% error reduction on labeledmacro F1: from 64.2% to 67.4%.Ranking by Classifier 3 The final ranking stepmakes use of Classifier 3, which predicts the rela-tion that holds between two tokens.
The dependencyrelations predicted by Classifier 1 that are not con-firmed by Classifier 3 predicting that a relation existsare moved to the end of the ranked list.
Table 6 liststhe resulting ranked list.
On the development datafor English, applying this rule yields another 5.2%28error reduction on labeled macro F1: from 67.4% to69.1%.Head Predicted label Distribution Entropyexpected :A1 { :A1 (5.64) } 0.0quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93Housing NONE { NONE (8.51) } 0.0to NONE { NONE (4.74) } 0.0are NONE { NONE (2.56), SBJ: (0.52) } 0.65Table 6: Output of Classifier 1 for the first six headsof ?starts?.
Ranked by entropy, Classifier 2, andClassifier 3.3.2.2 Construction of the intermediatedependency treeAfter ranking the predictions of Classifier 1, thesystem selects a syntactic head for every token.
Thisis motivated by the fact that every token has one andonly one syntactic head.
The system selects the pre-diction with the best ranking that has in the PDE-PREL part a value different than ?
?.The intermediate tree can have more than one rootor no root at all.
To make sure that every sentencehas one and only one root we apply some extra rules.If the sentence does not have a token with a root la-bel, the system checks the distributions of Classi-fier 1.
The token with the rootlabel in its distributionthat is the head of the biggest number of tokens istaken as root.
If the intermediate tree has more thanone root, the last root is taken as root.
The other roottokens get the label with a syntax part (PDEPREL)that has the highest score in the distribution of Clas-sifier 1.The product of this step is a tree in which ev-ery token is uniquely linked to a syntactic head.Because syntactic and semantic dependencies havebeen linked, the tree contains also semantic depen-dencies.
However, the tree is missing the purely se-mantic dependencies.
The next step adds these rela-tions to the dependency tree.3.2.3 Adding extra semantic dependenciesIn order to find the tokens that have only a seman-tic relation with a predicate, the system analyses foreach predicate (i.e.
tokens marked with Y in FILL-PRED) the list of predictions made by Classifier 1and selects the predictions in which the PDEPRELpart of the label is ?
?
and the APRED part of thelabel is different than ?
?.
On the development datafor English, applying this rule produces a 6.7% er-ror reduction on labeled macro F1: from 69.1% to71.1%.3.3 Predicate sense disambiguationPredicate sense disambiguation is performed by aclassifier per language that predicts the sense of thepredicate, except for Japanese, as with that languagethe lemma is taken as the sense.
We use the IGTREEalgorithm.
Instances represent predicates and thefeatures used are the word, lemma and POS of thepredicate, and the lemma and POS of two tokens be-fore and after the predicate.
The results per languageare presented in Table 7.Lang.
Cat Chi Cze Eng Ger SpaF1 82.40 94.85 87.84 93.64 73.57 81.13Table 7: Micro F1 for the predicate sense disam-biguation.4 Overall resultsThe system was developed by training on the train-ing set provided by the task organisers and testingon the development set.
The final results were ob-tained by testing on the testing set.
Table 8 showsthe global results of the system for syntactic and se-mantic dependencies.Lang.
F1 Precision RecallCat 73.75 74.91 72.63Chi 67.16 68.09 66.26Chi* 67.79 68.70 66.89Cze 60.50 62.55 58.58Cze* 68.68 70.38 67.07Eng 78.19 79.69 76.74Ger 67.51 69.52 65.62Jap 77.75 81.91 73.98Spa 70.78 71.34 70.22Av.
70.81 72.57 69.15Table 8: Macro F1, precision and recall for all de-pendencies per language.
Postevaluation results aremarked with *.Table 9 shows the scores of syntactic and seman-tic dependencies in isolation.29Syntax SemanticsLang.
LA F1 Precision RecallCat 77.33 70.14 72.49 67.94Chi 67.58 66.71 68.59 64.93Chi* 67.92 67.63 69.48 65.86Cze 49.41 71.49 75.68 67.75Cze* 60.03 77.28 80.73 74.11Eng 80.35 75.97 79.04 73.13Ger 73.88 61.01 65.15 57.36Jap 86.17 68.82 77.66 61.80Spa 73.07 68.48 69.62 67.38Av.
72.54 68.95 72.60 65.76Table 9: Labeled attachment (LA) score for syntac-tic dependencies and Macro F1, precision and recallof semantic dependencies per language.
Postevalua-tion results are marked with *.5 ConclusionsIn this paper we presented the system that we sub-mitted to the ?closed?
challenge of the CoNLLShared Task 2009.
We observe fairly low scores,which can be possibly improved for all languages bymaking use of the available morpho-syntactic fea-tures, which we did not use in the present system,by optimising the classifiers per language, and byimproving the reranking algorithm.
We also ob-serve a relatively low recall on the semantic task ascompared to overall recall, indicating that syntacticdependencies are identified with a better precision-recall balance.
A logical continuation of this studyis to compare joint learning to learning syntactic andsemantic dependencies in isolation, using the samearchitecture.
Only then will we be able to put for-ward conclusions about the performance of a jointlearning system versus the performance of a systemthat learns syntax and semantics independently.AcknowledgmentsThis study was made possible through financial sup-port from the University of Antwerp (GOA projectBIOGRAPH), and from the Netherlands Organisa-tion for Scientific Research.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.S.
Canisius and E. Tjong Kim Sang.
2007.
A con-straint satisfaction approach to dependency parsing.In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 1124?1128.W.
Daelemans and A. van den Bosch.
2005.
Memory-based language processing.
Cambridge UniversityPress, Cambridge, UK.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k Z?abokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.R.
Morante, W. Daelemans, and V. Van Asch.
2008.
Acombined memory-based semantic role labeler of en-glish.
In Proc.
of the CoNLL 2008, pages 208?212,Manchester, UK.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008).Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.30
