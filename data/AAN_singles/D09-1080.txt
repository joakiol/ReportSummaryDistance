Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 765?774,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPIntegrating sentence- and word-level error identificationfor disfluency correctionErin FitzgeraldJohns Hopkins UniversityBaltimore, MD, USAerinf@jhu.eduFrederick JelinekJohns Hopkins UniversityBaltimore, MD, USAjelinek@jhu.eduKeith HallGoogle Inc.Z?urich, Switzerlandkbhall@google.comAbstractWhile speaking spontaneously, speakersoften make errors such as self-correctionor false starts which interfere with thesuccessful application of natural languageprocessing techniques like summarizationand machine translation to this data.
Thereis active work on reconstructing this error-ful data into a clean and fluent transcriptby identifying and removing these simpleerrors.Previous research has approximated thepotential benefit of conducting word-levelreconstruction of simple errors only onthose sentences known to have errors.
Inthis work, we explore new approachesfor automatically identifying speaker con-struction errors on the utterance level, andquantify the impact that this initial step hason word- and sentence-level reconstruc-tion accuracy.1 IntroductionA system would accomplish reconstruction of itsspontaneous speech input if its output were to rep-resent, in flawless, fluent, and content-preservingtext, the message that the speaker intended to con-vey.
While full speech reconstruction would likelyrequire a range of string transformations and po-tentially deep syntactic and semantic analysis ofthe errorful text (Fitzgerald, 2009), in this work wewill attempt only to resolve less complex errors,correctable by deletion alone, in a given manually-transcribed utterance.The benefit of conducting word-level recon-struction of simple errors only on those sen-tences known to have errors was approximated in(Fitzgerald et al, 2009).
In the current work, weexplore approaches for automatically identifyingspeaker-generated errors on the utterance level,and calculate the gain in accuracy that this initialstep has on word- and sentence-level accuracy.1.1 Error classes in spontaneous speechCommon simple disfluencies in sentence-like ut-terances (SUs) include filler words (i.e., ?um?, ?ah?,and discourse markers like ?you know?
), as well asspeaker edits consisting of a reparandum, an inter-ruption point (IP), an optional interregnum (like ?Imean?
), and a repair region (Shriberg, 1994), asseen in Figure 1.[that?s]?
??
?reparandumIP???
?+ {uh}????interregnumthat?s?
??
?repaira reliefFigure 1: Typical edit region structure.These reparanda, or edit regions, can be classifiedinto three main groups:1.
In a repetition (above), the repair phrase isapproximately identical to the reparandum.2.
In a revision, the repair phrase alters reparan-dum words to correct the previously statedthought.EX1: but [when he] + {i mean} when she put itthat wayEX2: it helps people [that are going to quit] + thatwould be quitting anyway3.
In a restart fragment an utterance is abortedand then restarted with a new train of thought.EX3: and [i think he?s] + he tells me he?s glad hehas one of thoseEX4: [amazon was incorporated by] {uh} well ionly knew two people thereIn simple cleanup (a precursor to full speech re-construction), all detected filler words are deleted,and the reparanda and interregna are deleted whilethe repair region is left intact.
This is a strong ini-tial step for speech reconstruction, though more7651 he that ?s uh that ?s a relief2 E E E FL - - - -3 NC RC RC FL - - - -Figure 2: Example of word class and refined wordclass labels, where - denotes a non-error, FL de-notes a filler, E generally denotes reparanda, andRC and NC indicate rough copy and non-copyspeaker errors, respectively.
Line 3 refines the la-bels of Line 2.complex and less deterministic changes may berequired for generating fluent and grammaticalspeech text in all cases.1.2 Related WorkStochastic approaches for simple disfluency de-tection use features such as lexical form, acous-tic cues, and rule-based knowledge.
State-of-the-art methods for edit region detection such as(Johnson and Charniak, 2004; Zhang and Weng,2005; Kahn et al, 2005; Honal and Schultz, 2005)model speech disfluencies as a noisy channelmodel, though direct classification models havealso shown promise (Fitzgerald et al, 2009; Liuet al, 2004).
The final output is a word-level tag-ging of the error condition of each word in the se-quence, as seen in line 2 of Figure 2.The Johnson and Charniak (2004) approach,referred to in this document as JC04, combinesthe noisy channel paradigm with a tree-adjoininggrammar (TAG) to capture approximately re-peated elements.
The TAG approach models thecrossed word dependencies observed when thereparandum incorporates the same or very simi-lar words in roughly the same word order, whichJC04 refer to as a rough copy.
Line 3 of Figure2 refines ?edits?
(E) into rough copies (RC) andnon-copies (NC).As expected given the assumptions of theTAG approach, JC04 identifies repetitions andmost revisions in spontaneous data, but is lesssuccessful in labeling false starts and otherspeaker self-interruptions without cross-serial cor-relations.
These non-copy errors hurt the edit de-tection recall and overall accuracy.Fitzgerald et al (2009) (referred here as FHJ)used conditional random fields (CRFs) and theSpontaneous Speech Reconstruction (SSR) corpus(Fitzgerald and Jelinek, 2008) corpus for word-level error identification, especially targeting im-provement of these non-copy errors.
The CRF wastrained using features based on lexical, languagemodel, and syntactic observations along with fea-tures based on JC04 system output.Alternate experimental setup showed that train-ing and testing only on SUs known from the la-beled corpus to contain word-level errors yieldeda notable improvement in accuracy, indicating thatthe described system was falsely identifying manynon-error words as errors.Improved sentence-level identification of error-ful utterances was shown to help improve word-level error identification and overall reconstructionaccuracy.
This paper describes attempts to extendthese efforts.2 Approach2.1 DataWe conducted our experiments on the recently re-leased Spontaneous Speech Reconstruction (SSR)corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisherconversational telephone speech data (Cieri et al,2004)1.
Advantages of the SSR data include?
aligned parallel original and cleaned sen-tences?
several levels of error annotations, allowingfor a coarse-to-fine reconstruction approach?
multiple annotations per sentence reflectingthe occasional ambiguity of correctionsAs reconstructions are sometimes non-deterministic, the SSR provides two manualreconstructions for each utterance in the data.
Weuse these dual annotations to learn complemen-tary approaches in training and to allow for moreaccurate evaluation.The Spontaneous Speech Reconstruction cor-pus is partitioned into three subcorpora: 17,162training sentences (119,693 words), 2,191 sen-tences (14,861 words) in the development set, and2,288 sentences (15,382 words) in the test set.
Ap-proximately 17% of the total utterances contain areparandum-type error.
In constructing the data,two approaches were combined to filter out theutterances considered most likely to be errorful(6,384 in total) and only those SUs were manuallyreconstructed.
However the entire data set was in-cluded in the distribution ?
and used in training forthis work ?
to maintain data balance.1The Spontaneous Speech Reconstruction corpus can bedownloaded from http://www.clsp.jhu.edu/PIRE/ssr.766The training of the TAG model for JC04, usedas a feature in this work, requires a very specificdata format, and thus is trained not with SSR butwith Switchboard (SWBD) data (Godfrey et al,1992).
Key differences in these corpora, besidesthe granularity and form of their annotations, in-clude:?
SSR aims to correct speech output, whileSWBD edit annotation aims to identifyreparandum structures specifically.
SSR onlymarks those reparanda which annotators be-lieve must be deleted to generate a grammat-ical and content-preserving reconstruction.?
SSR includes more complex error identifi-cation and correction, not considered in thiswork.While the SWBD corpus has been used insome previous simple disfluency labeling work(e.g., Johnson and Charniak, 2004; Kahn et al,2005), we consider the SSR for its fine-grained er-ror annotations.3 Identifying poor constructionsPrior to reconstruction, it is to our advantage to au-tomatically identify poorly constructed sentences,defined as being ungrammatical, incomplete, ormissing necessary sentence boundaries.
Accu-rately extracting ill-formed sentences prior to sub-sentential error correction helps to minimize therisk of information loss posed by unnecessarilyand incorrectly reconstructing well-formed text.To evaluate the efforts described below, wemanually label each SU s in the SSR test set S(including those not originally annotated with re-constructions but still included in the SSR distri-bution) as well-formed or poorly-formed, form-ing the set of poorly constructed SUs P ?
S,|P | = 531 and |S| = 2288 utterances.To identify speaker errors on the sentence level,we consider and combine a collection of featuresinto a single framework using a maximum entropymodel (implemented with the Daum?e III (2004)MEGA Model toolkit).3.1 SU-level error featuresSix feature types are presented in this section.?
Features #1 and #2 are the two methods in-cluded in a similar though less exhaustive ef-fort by (Fitzgerald and Jelinek, 2008) in errorfiltering for the creation of the SSR corpus it-self.?
Feature types #3 and #4 extract features fromautomatic parses assigned to the given sen-tence.
It is expected that these parses willcontain some errors and the usefulness ofthese features may be parser-specific.
Thevalue of these features though is the con-sistent, if not always accurate, treatment ofsimilar construction errores given a particu-lar state-of-the-art parser.?
Feature type #5 investigates the relationshipbetween the probability of a SU-internal errorand the number of words it contains.?
Feature type #6 serves to bias the probabil-ity against assigning a backchannel acknowl-edgement SU as an error instance.Feature #1 (JC04): Consider only sentences withJC04 detected edit regions.
This approach takesadvantage of the high precision, low recall JC04disfluency detection approach described in Section1.2.
We apply the out-of-box JC04 system andconsider any sentence with one or more labeledreparanda as a ?poor?
indicator.
Since speakers re-pairing their speech once are often under a highercognitive load and thus more likely to make moreserious speech errors (in other words, there is ahigher probability of making an error given that anerror has already been made (Bard et al, 2001)).This is a reasonable first order approach for find-ing deeper problems.Feature #2 (HPSG): Use deep linguistic parsersto confirm well-formedness.
Statistical context-free parsers are highly robust and, due to smooth-ing, can assign a non-zero probability syntac-tic structure even for text and part-of-speech se-quences never seen during training.
However,sometimes no output is preferable to highly er-rorful output.
Hand-built rule-based parsers canproduce extremely accurate and context-sensitivesyntactic structures, but are also brittle and do notadapt well to never before seen input.
We use thisinflexibility to our advantage.Head-driven Phrase Structure Grammar(HPSG) is a deep-syntax phrase structure gram-mar which produces rich, non-context-freesyntactic analyses of input sentences based ona collection of carefully constructed rules andlexical item structures (Pollard and Sag, 1994;Wahlster, 2000).
Each utterance is parsed using767the PET deep parser produced by the inter-institutional DELPH-IN group2.
The manuallycompiled English Resource Grammar (ERG)(Flickinger, 2002) rules have previously beenextended for the Verbmobil (Wahlster, 2000)project to allow for the parsing of basic conversa-tional elements such as SUs with no verb or basicbackchannel acknowledgements like ?last thursday?or ?sure?, but still produce strict HPSG parsesbased on these rules.
We use the binary result ofwhether or not each SU is parsable by the HPSGERG as binary indicator functions in our models.There has been some work on producing partialparses for utterances for which a full HPSG analy-sis is not deemed possible by the grammar (Zhanget al, 2007).
This work has shown early promisefor identifying coherent substrings within error-ful SUs given subjective analysis; as this technol-ogy progresses, HPSG may offer informative sub-sentential features for word-level error analysis aswell.Feature #3 (Rules): Mark unseen phrase rule ex-pansions.
Phrase-based parses are composed ofa recursive sequence of non-terminal (NT) rule ex-pansions, such as those detailed for the exampleparse shown in Figure 3.
These rules are learnedfrom training data such as the Switchboard tree-bank, where telephone conversation transcriptswere manually parsed.
In many statistical parsers,new structures are generated based on the relativefrequencies of such rules in the training treebank,conditioned on the terminal words and some localcontext, and the most probable parse (roughly thejoint probability of its rule expansions) is selected.Because parsers are often required to produceoutput for words and contexts never seen in thetraining corpus, smoothing is required.
TheCharniak (1999) parser accomplishes this in partthrough a Markov grammar which works top-down, expanding rules to the left and right of anexpansion head M of a given rule.
The non-terminal (NT) M is first predicted from the parentP , then ?
in order ?L1throughLm(stopping sym-bol ?#?)
and R1through Rn(again ?#?
), as shownin Equation 1.parent P ?
#Lm.
.
.
L1MR1.
.
.
Rn# (1)In this manner, it is possible to produce rulesnever before seen in the training treebank.
While2The DEep Linguistic Processing with HPSG INitiative(see http://www.delph-in.net/)this may be required for parsing grammatical sen-tences with rare elements, this SU-level error pre-diction feature indicates whether the automaticparse for a given SU includes an expansion neverseen in the training treebank.
If an expansion rulein the one-best parse was not seen in training (heremeaning in the SWBD treebank after EDITEDnodes have been removed), the implication is thatnew rule generation is an indicator of a speakererror within a SU.Feature #4 (C-comm): Mark unseen rule c-commanding NTs.
In X?
theory (Chomsky,1970), lexical categories such as nouns and verbsare often modified by a specifier (such as the DT ?a?modifying the NN ?lot?
in the NP3phrase in Figure3 or an auxiliary verb for a verb in a verb phrase(VBZ for VP3) and a complement (such as the ob-ject of a verb NP3for VBG in the phrase VP3).In each of these cases, an NT tree node A hasthe following relationship with a second NT P :?
Neither does node A dominate P nor node PdominateA, (i.e., neither is directly above theother in the parse tree), and?
Node A immediately precedes P in the tree(precedence is represented graphically in left-to-right order in the tree).Given these relationships, we say that A locallyc-commands P and its descendants.
We furtherextend this definition to say that, if node?A is theonly child of nodeA (a unary expansion) andA lo-cally c-commands P , then?A locally c-commandsP (so both [SBAR ?
S] and [S ?
NP2VP2] arec-commanded by VBP).
See Figure 3 for other ex-amples of non-terminal nodes in c-commandingrelationships, and the phrase expansion rule theyc-command.The c-command relationship is fundamental insyntactic theory, and has uses such as predictingthe scope of pronoun antecedents.
In this case,however, we use it to describe two nodes which arein a specifier?category relationship or a category?complement relationship (e.g., subject?verb andverb?object, respectively).
This is valuable to usbecause it takes advantage of a weakness of sta-tistical parsers: the context used to condition theprobability of a given rule expansion generallydoes not reach beyond dominance relationships,and thus parsers rarely penalize for the juxtapo-sition of A c-commanding P and its children as768a) SNP1PRPtheyVP1VBPareSBARSNP2DTthatVP2VBZisVP3VBGsayingNP3DTaNNlotb) Rules expansions:S?
NP VPNP1?
PRPVP1?
VBP SBARSBAR?
SS?
NP2VP2NP2?
DTVP2?
VBZ VPVP3?
VBG NPNP3?
DT NNc) Rule expansions + c-commanding NT:S?
NP VP no local c-commandNP1?
PRP no local c-commandVP1?
V SBAR NP1SBAR?
S VBPS?
NP2VP2VBPNP2?
DT no local c-commandVP2?
VBZ VP NP2VP3?
VBG NP VBZNP3?
DT NN VBGFigure 3: The automatically generated parse (a) for an errorful sentence-like unit (SU), with accompa-nying rule expansions (b) and local c-commands (c).
Non-terminal indices such as NP2are for readerclarification only and are not considered in the feature extraction process.long as they have previously seen NT type A pre-ceding NT type P .
Thus, we can use the childrenof a parent node P as a way to enrich a NT type Pand make it more informative.For example, in Figure 3, the rule [S ?
NP2VP2] is routinely seen in the manual parses ofthe SWBD treebank, as is [VP1?
VBP SBAR].However, it is highly unusual for VBP to immedi-ately precede SBAR or S when this rule expandsto NP2VP2.
So, not only does SBAR/S comple-ment VBP, but a very specific type of [SBAR/S?
NP VP] is the complement of VBP.
This con-ditional infrequency serves as an indication ofdeeper structural errors.Given these category relationship observations,we include in our maximum entropy model a fea-ture indicating whether a given parse includes ac-command relationship not seen in training data.Feature #5 (Length): Threshold sentences basedon length.
Empirical observation indicates thatlong sentences are more likely to contain speakererrors, while very short sentences tend to bebackchannel acknowledgments like ?yeah?
or ?Iknow?
which are not considered errorful.
Oviatt(1995) quantifies this, determining that the dis-fluency rate in human-computer dialog increasesroughly linearly with the number of words in anutterance.The length-based feature value for each sen-tence therefore is defined to be the number of wordtokens in that sentence.Feature #6 (Backchannel): Bias backchannelacknowledgements as non-errors A backchan-nel acknowledgement is a short sentence-like unit(SU) which is produced to indicate that the speakeris still paying attention to the other speaker, with-out requesting attention or adding new content tothe dialog.
These SUs include ?uh-huh?, ?sure?,or any combination of backchannel acknowledge-ments with fillers (ex.
?sure uh uh-huh?
).To assign this feature, fifty-two commonbackchannel acknowledgement tokens are consid-ered.
The indicator feature is one (1) if the SU inquestion is some combination of these backchan-nel acknowledgements, and zero (0) otherwise.3.2 SU-level error identification resultsWe first observe the performance of each featuretype in isolation in our maximum entropy frame-work (Table 1(a)).
The top-performing individual769Features includedSetup JC04 HPSG Rules C-comm Length Backchannel F1-scorea) Individual features1??
?
?
?
?
79.92 ???
?
?
?
77.15 ?
?
?
???
59.74 ?
?
???
?
42.23 ?
???
?
?
23.26 ?
?
?
?
?
?0.0b) All features combined7?
?
?
?
?
?83.3c) All-but-one8 ??
?
?
?
?78.4 (-4.9)9?
?
???
?81.2 (-2.1)10???
?
?
?81.3 (-2.0)11?
???
?
?82.1 (-1.2)12?
?
?
?
??
82.9 (-0.4)13?
?
?
??
?83.2 (-0.1)Table 1: Comparison of poor construction identification features, tested on the SSR test corpus.feature is the JC04 edit indicator, which is not sur-prising as this is the one feature whose existencewas designed specifically to predict speaker errors.Following JC04 in individual performance are theHPSG parsability feature, length feature, and un-seen c-command rule presence feature.
Backchan-nel acknowledgements had no predictive power ontheir own.
This was itself unsurprising as the fea-ture was primarily meant to reduce the probabilityof selecting these SUs as errorful.Combining all rules together (Table 1(b)), wenote an F1-score gain of 3.4 as compared to the topindividual feature JC04.
(JC04 has a precision of97.6, recall of 67.6, and F of 79.9; the combinedfeature model has a precision of 93.0, a recall of75.3, and an F of 83.3, so unsurprisingly our gainprimarily comes from increased error recall).In order to understand the contribution of an in-dividual feature, it helps not only to see the pre-diction results conditioned only on that feature,but the loss in accuracy seen when only that fea-ture is removed from the set.
We see in Table 1(c)that, though the c-command prediction feature wasonly moderately accurate in predicting SU errorson its own, it has the second largest impact afterJC04 (an F-score loss of 2.1) when removed fromthe set of features.
Such a change indicates theorthogonality of the information within this fea-ture to the other features studied.
Length, on theother hand, while moderately powerful as a sin-gle indicator, had negligible impact on classifica-tion accuracy when removed from the feature set.This indicates that the relationship between error-ful sentences and length can be explained away bythe other features in our set.We also note that the combination of all featuresexcluding JC04 is competitive with JC04 itself.Additional complementary features seem likely tofurther compete with the JC04 prediction feature.4 Combining effortsThe FHJ work shows that the predictive power ofa CRF model could greatly improve (given a re-striction on only altering SUs suspected to containerrors) from an F-score of 84.7 to as high as 88.7for rough copy (RC) errors and from an F-score of47.5 to as high as 73.8 for non-copy (NC) errors.Now that we have built a model to predict con-struction errors on the utterance level, we combinethe two approaches to analyze the improvementpossible for word-level identification (measuredagain by precision, recall, and F-score) and forSU-level correction (measured by the SU Matchmetric defined in Section 4.2).4.1 Word-level evaluation of erroridentification, post SU filteringWe first evaluate edit detection accuracy on thosetest SUs predicted to be errorful on a per-word ba-sis.
To evaluate our progress identifying word-770level error classes, we calculate precision, recalland F-scores for each labeled class c in each exper-imental scenario.
As usual, these metrics are cal-culated as ratios of correct, false, and missed pre-dictions.
However, to take advantage of the doublereconstruction annotations provided in SSR (andmore importantly, in recognition of the occasionalambiguities of reconstruction) we modified thesecalculations slightly to account for all references.Analysis of word-level label evaluation, post SUfiltering.
Word-level F1-score results for errorregion identification are shown in Table 2.By first automatically selecting testing as de-scribed in Section 3 (with a sentence-level F-scoreof 83.3, Table 1(b)), we see in Table 2 some gain inF-score for all three error classes, though much po-tential improvement remains based on the oraclegain (rows indicated as having ?Gold errors?
test-ing data).
Note that there are no results from train-ing only on errorful data but testing on all data, asthis was shown to yield dramatically worse resultsdue to data mismatch issues.Unlike in the experiments where all data wasused for testing and training, the best NC and RCdetection performance given the automatically se-lected testing data was achieved when training aCRF model to detect each class separately (RCor NC alone) and not in conjunction with fillerword detection FL.
As in FHJ, training RC and NCmodels separately instead of in a joint FL+RC+NCmodel yielded higher accuracy.We notice also that the F-score for RC identi-fication is lower when automatically filtering thetest data.
There are two likely causes.
The mostlikely issue is that the automatic SU-error clas-sifier filtered out some SUs with true RC errorswhich had previously been correctly identified, re-ducing the overall precision ratio as well as re-call (i.e., we no longer receive accuracy credit forsome easier errors once caught).
A second, relatedpossibility is that the errorful SUs identified bythe Section 3 method had a higher density of er-rors that the current CRF word-level classificationmodel is unable to identify (i.e.
the more difficulterrors are now a higher relative percentage of theerrors we need to catch).
While the former pos-sibility seems more likely, both causes should beinvestigated in future work.The F-score gain in NC identification from 42.5to 54.6 came primarily from a gain in precision (inthe original model, many non-errorful SUs weremistakenly determined to include errors).
Thoughcapturing approximately 55% of the non-copy NCerrors (for SUs likely to have errors) is an im-provement, this remains a challenging and un-solved task which should be investigated furtherin the future.4.2 Sentence-level evaluation of erroridentification and region deletion, postSU identificationDepending on the downstream task of speech re-construction, it may be imperative not only toidentify many of the errors in a given spoken ut-terance, but indeed to identify all errors (and onlythose errors), yielding the exact cleaned sentencethat a human annotator might provide.In these experiments we apply simple cleanup(as described in Section 1.1) to both JC04 out-put and the predicted output for each experimentalsetup, deleting words when their error class is afiller, rough copy or non-copy.Taking advantage of the dual annotations pro-vided for each sentence in the SSR corpus, wecan report double-reference evaluation.
Thus, wejudge that if a hypothesized cleaned sentence ex-actly matches either reference sentence cleaned inthe same manner we count the cleaned utterance ascorrect, and otherwise we assign no credit.
We re-port double-reference exact match evaluation be-tween a given SU s and references r ?
R, as de-fined below.SU match =1S?s?Smaxr?R?
(s, r) (2)Analysis of sentence level evaluation, post SUidentification.
Results from this second evalua-tion of rough copy and non-copy error reconstruc-tion can be seen in Table 3.As seen in word-level identification results (Ta-ble 2), automatically selecting a subset of testingdata upon which to apply simple cleanup recon-struction does not perform at the accuracy shownto be possible given an oracle filtering.
Whilemeasuring improvement is difficult (here, non-filtered data is incomparable to filtered test dataresults since a majority of these sentences requireno major deletions at all), we note again that ourmethods (MaxEnt/FHJ-x) outperform the baselineof deleting nothing but filled pauses like ?eh?
and?um?, as well as the state-of-the-art baseline JC04.771Class labeled Training SUs for Testing FL RC NCAll data All SU data 71.0 80.3 47.4FL+RC+NC Errorful only Auto ID?d SU errors 87.9 79.9 49.0Errorful only Gold SU errors 91.6 84.1 52.2All data All SU data - - 42.5NC Errorful only Auto ID?d SU errors - - 54.6Errorful only Gold SU errors - - 73.8All data All SU data 70.8 - 47.5NC+FL Errorful only Auto ID?d SU errors 88.8 - 53.3Errorful only Gold SU errors 90.7 - 69.8All data All SU data - /84.2/ -RC Errorful only Auto ID?d SU errors - 81.3 -Errorful only Gold SU errors - 88.7 -All data All SU data 67.8 /84.7/ -RC+FL Errorful only Auto ID?d SU errors 88.1 80.5 -Errorful only Gold SU errors 92.3 87.4 -Table 2: Error predictions, post-SU identification: F1-score results.
Automatically identified ?SUs fortesting?
were determined via the maximum entropy classification model described earlier in this paper,and feature set #7 from Table 1.
Filler (FL), rough copy error (RC) and non-copy error (NC) results aregiven in terms of word-level F1-score.
Bold numbers indicate the highest performance post-automaticfilter for each of the three classes.
Italicized values indicate experiments where no filtering outperformedautomatic filtering (for RC errors).# SUs # SUs that %Setup Classed deleted Testing (filt/unfilt) match ref accuracyBaseline-1 only filled pauses All data 2288 1800 78.7%JC04-1 E+FL All data 2288 1858 81.2%MaxEnt/FHJ-1 FL+RC+NC All data 2288 1922 84.0%Baseline-2 only filled pauses Auto ID?d 430 84 19.5%JC04-2 E+FL Auto ID?d 430 187 43.5%MaxEnt/FHJ-2 FL+RC+NC Auto ID?d 430 223 51.9%Baseline-3 only filled pauses Gold errors 281 5 1.8%JC04-3 E+FL Gold errors 281 126 44.8%MaxEnt/FHJ-3 FL+RC+NC Gold errors 281 156 55.5%Table 3: Error predictions, post-SU identification: Exact Sentence Match Results.For the baseline, we delete only filled pause filler words like ?eh?
and ?um?.
For JC04 output, we deletedany word assigned the class E or FL.
Finally, for the MaxEnt/FHJ models, we used the jointly trainedFL+RC+NC CRF model and deleted all words assigned any of the three classes.5 Future WorkWhile some success and improvements for theautomatic detection and deletion of fillers andreparanda (i.e., ?simple cleanup?)
have beendemonstrated in this work, much remains to bedone to adequately address the issues and criteriaconsidered here for full reconstruction of sponta-neous speech.Included features for both the word level andSU-level error detection have only skimmed thesurface of potentially powerful features for spon-taneous speech reconstruction.
There should becontinued development of complementary parser-based features (such as those from dependencyparsers or even deep syntax parsers such as im-plementations of HPSG as well as additional syn-tactic features based on automatic constituent orcontext-free grammar based parsers).
Prosodic772features, though demonstrated to be unnecessaryfor at least moderately successful detection of sim-ple errors, also hold promise for additional gains.Future investigators should evaluate the gains pos-sible by integrating this information into the fea-tures and ideas presented here.6 Summary and conclusionsThis work was an extension of the results in FHJ,which showed that automatically determiningwhich utterances contain errors before attemptingto identify and delete fillers and reparanda has thepotential to increase accuracy significantly.In Section 3, we built a maximum entropy clas-sification model to assign binary error classes tospontaneous speech utterances.
Six features ?JC04, HPSG, unseen rules, unseen c-command re-lationships, utterance length, and backchannel ac-knowledgement composition ?
were considered.The combined model achieved a precision of 93.0,a recall of 75.3, and an F1-score of 83.3.We then, in Section 4, cascaded the sentence-level error identification system output into theFHJ word-level error identification and simplecleanup system.
This combination lead to non-copy error identification with an F1-score of 54.6,up from 47.5 in the experiments conducted on alldata instead of data identified to be errorful, whilemaintaining accuracy for rough copy errors and in-creasing filler detection accuracy as well.
Thoughthe data setup is slightly different, the true errorsare common across both sets of SUs and thus theresults are comparable.This work demonstrates that automatically se-lecting a subset of SUs upon which to imple-ment reconstruction improves the accuracy of non-copy (restart fragment) reparanda identificationand cleaning, though less improvement resultsfrom doing the same for rough copy identification.AcknowledgmentsThe authors thank our anonymous reviewers fortheir valuable comments.
Support for this workwas provided by NSF PIRE Grant No.
OISE-0530118.
Any opinions, findings, conclusions,or recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the supporting agency.ReferencesEllen G. Bard, Robin J. Lickley, and Matthew P. Aylett.2001.
Is disfluency just difficult?
In Disfluencies inSpontaneous Speech Workshop, pages 97?100.Eugene Charniak.
1999.
A maximum-entropy-inspired parser.
In Proceedings of the Annual Meet-ing of the North American Association for Compu-tational Linguistics.Noam Chomsky, 1970.
Remarks on nominalization,pages 184?221.
Waltham: Ginn.Christopher Cieri, Stephanie Strassel, MohamedMaamouri, Shudong Huang, James Fiumara, DavidGraff, Kevin Walker, and Mark Liberman.
2004.Linguistic resource creation and distribution forEARS.
In Rich Transcription Fall Workshop.Hal Daum?e III.
2004.
Notes on CG andLM-BFGS optimization of logistic regression.Paper available at http://pub.hal3.name\#daume04cg-bfgs, implementation available athttp://hal3.name/megam/, August.Erin Fitzgerald and Frederick Jelinek.
2008.
Linguis-tic resources for reconstructing spontaneous speechtext.
In Proceedings of the Language Resources andEvaluation Conference.Erin Fitzgerald, Keith Hall, and Frederick Jelinek.2009.
Reconstructing false start errors in sponta-neous speech text.
In Proceedings of the AnnualMeeting of the European Association for Computa-tional Linguistics.Erin Fitzgerald.
2009.
Reconstructing SpontaneousSpeech.
Ph.D. thesis, The Johns Hopkins University.Dan Flickinger.
2002.
On building a more efficientgrammar by exploiting types.
In Stephan Oepen,Dan Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit,editors, Collaborative Language Engineering, pages1?17.
CSLI Publications, Stanford.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
SWITCHBOARD: Telephone speechcorpus for research and development.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech, and Signal Processing, pages 517?520,San Francisco.Matthias Honal and Tanja Schultz.
2005.
Au-tomatic disfluency removal on recognized spon-taneous speech ?
rapid adaptation to speaker-dependent disfluenices.
In Proceedings of the IEEEInternational Conference on Acoustics, Speech, andSignal Processing.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy channel model of speech repairs.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics.773Jeremy Kahn, Matthew Lease, Eugene Charniak, MarkJohnson, and Mari Ostendorf.
2005.
Effective useof prosody in parsing conversational speech.
In Pro-ceedings of the Conference on Human LanguageTechnology, pages 561?568.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-bara Peskin, and Mary Harper.
2004.
The ICSI/UWRT04 structural metadata extraction system.
In RichTranscription Fall Workshop.Sharon L. Oviatt.
1995.
Predicting and managingspoken disfluencies during human-computer interac-tion.
Computer Speech and Language, 9:19?35.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChiacgoPress and CSLI Publications, Chicago and Stanford.Elizabeth Shriberg.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. thesis, University ofCalifornia, Berkeley.Wolfgang Wahlster, editor.
2000.
Verbmobil: Foun-dations of Speech-to-Speech Translation.
Springer,Berlin.Qi Zhang and Fuliang Weng.
2005.
Exploring fea-tures for identifying edited regions in disfluent sen-tences.
In Proceedings of the International Work-shop on Parsing Techniques, pages 179?185.Yi Zhang, Valia Kordoni, and Erin Fitzgerald.
2007.Partial parse selection for robust deep processing.
InProceedings of ACL Workshop on Deep LinguisticProcessing, pages 128?135.774
