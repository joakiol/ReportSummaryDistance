Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 240?250,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsEvaluating Models of Latent Document Semanticsin the Presence of OCR ErrorsDaniel D. Walker, William B. Lund, and Eric K. RinggerBrigham Young UniversityProvo, Utah, USAdanl4@cs.byu.edu, bill lund@byu.edu, ringger@cs.byu.eduAbstractModels of latent document semantics such asthe mixture of multinomials model and La-tent Dirichlet Allocation have received sub-stantial attention for their ability to discovertopical semantics in large collections of text.In an effort to apply such models to noisyoptical character recognition (OCR) text out-put, we endeavor to understand the effectthat character-level noise can have on unsu-pervised topic modeling.
We show the ef-fects both with document-level topic analy-sis (document clustering) and with word-leveltopic analysis (LDA) on both synthetic andreal-world OCR data.
As expected, experi-mental results show that performance declinesas word error rates increase.
Common tech-niques for alleviating these problems, such asfiltering low-frequency words, are successfulin enhancing model quality, but exhibit fail-ure trends similar to models trained on unpro-cessed OCR output in the case of LDA.
To ourknowledge, this study is the first of its kind.1 IntroductionAs text data becomes available in massive quanti-ties, it becomes increasingly difficult for human cu-rators to manually catalog and index modern docu-ment collections.
To aid in the automation of suchtasks, algorithms can be used to create models ofthe latent semantics present in a given corpus.
Oneexample of this type of analysis is document cluster-ing, in which documents are grouped into clustersby topic.
Another type of topic analysis attemptsto discover finer-grained topics?labeling individualwords in a document as belonging to a particulartopic.
This type of analysis has grown in popular-ity recently as inference on models containing largenumbers of latent variables has become feasible.The modern explosion of data includes vastamounts of historical documents, made availableby means of Optical Character Recognition (OCR),which can introduce significant numbers of er-rors.
Undertakings to produce such data includethe Google Books, Internet Archive, and HathiTrustprojects.
In addition, researchers are having increas-ing levels of success in digitizing hand-written man-uscripts (Bunke, 2003), though error rates remainmuch higher than for OCR.
Due to their nature, thesecollections often lack helpful meta-data or labels.
Inthe absence of such labels, unsupervised machinelearning methods can reveal patterns in the data.Finding good estimates for the parameters ofmodels such as the mixture of multinomials docu-ment model (Walker and Ringger, 2008) and the La-tent Dirichlet Allocation (LDA) model (Blei et al,2003) requires accurate counts of the occurrencesand co-occurrences of words.
Depending on theage of a document and the way in which it wascreated, the OCR process results in text containingmany types of noise, including character-level er-rors, which distort these counts.
It is obvious, there-fore, that model quality must suffer, especially sinceunsupervised methods are typically much more sen-sitive to noise than supervised methods.
Good su-pervised learning algorithms are substantially moreimmune to spurious patterns in the data created bynoise for the following reason: under the mostlyreasonable assumption that the process contributingthe noise operates independently from the class la-bels, the noise in the features will not correlate wellwith the class labels, and the algorithm will learn240to ignore those features arising from noise.
Unsu-pervised models, in contrast, have no grounding inlabels to prevent them from confusing patterns thatemerge by chance in the noise with the ?true?
pat-terns of potential interest.
For example, even onclean data, LDA will often do poorly if the very sim-ple feature selection step of removing stop-words isnot performed first.
Though we expect model qual-ity to decrease, it is not well understood how sensi-tive these models are to OCR errors, or how qualitydeteriorates as the level of OCR noise increases.In this work we show how the performance of un-supervised topic modeling algorithms degrades ascharacter-level noise is introduced.
We demonstratethe effect using both artificially corrupted data andan existing real-world OCR corpus.
The results arepromising, especially in the case of relatively lowword error rates (e.g.
less than 20%).
Though modelquality declines as errors increase, simple feature se-lection techniques enable the learning of relativelyhigh quality models even as word error rates ap-proach 50%.
This result is particularly interestingin that even humans find it difficult to make senseof documents with error rates of that magnitude(Munteanu et al, 2006).Because of the difficulties in evaluating topicmodels, even on clean data, these results should notbe interpreted as definitive answers, but they do offerinsight into prominent trends.
For example, proper-ties of the OCR data suggest measures that can betaken to improve performance in future work.
It isour hope that this work will lead to an increase inthe usefulness of collections of OCRed texts, as doc-ument clustering and topic modeling expose usefulpatterns to historians and other interested parties.The remainder of the paper is outlined as follows.After an overview of related work in Section 2, Sec-tion 3 introduces the data used in our experiments,including an explanation of how the synthetic datawere created and of some of their properties.
Sec-tion 4 describes how the experiments were designedand carried out, and gives an analysis of the resultsboth empirically and qualitatively.
Finally, conclu-sions and future work are presented in Section 5.2 Related WorkTopic models have been used previously to processdocuments digitized by OCR, including eighteenth-century American newspapers (Newmann andBlock, 2006), OCRed editions of Science (Blei andLafferty, 2006), OCRed NIPS papers (Wang andMcCallum, 2006), and books digitized by the OpenContent Alliance (Mimno and Mccallum, 2007).Most of this previous work ignores the presence ofOCR errors or attempts to remove corrupted tokenswith special pre-processing such as stop-word re-moval and frequency cutoffs.
Also, there are at leasttwo instances of using topic modeling to improvethe results of an OCR algorithm (Wick et al, 2007;Farooq et al, 2009).Similar evaluations to ours have been conductedto assess the effect of OCR errors on supervised doc-ument classification (Taghva et al, 2001; Agarwal etal., 2007), information retrieval (Taghva et al, 1994;Beitzel et al, 2003), and a more general set of natu-ral language processing tasks (Lopresti, 2008).
Re-sults suggest that in these supervised tasks OCR er-rors have a minimal impact on the performance ofthe methods employed, though it has remained un-clear how well these results transfer to unsupervisedmethods.3 DataWe conducted experiments on synthetic and realOCR data.
As a real-world dataset, we used a cor-pus consisting of 604 of the Eisenhower World WarII communique?s (Jordan, 1945; Lund and Ringger,2009).
These documents relate the daily progressof the Allied campaign from D-Day until the Ger-man surrender.
They were originally produced astelegrams and were distributed as mimeographedcopies.
The quality of the originals is often quitepoor, making them a challenging case for OCR en-gines.
The communique?s have been OCRed usingthree popular OCR engines: ABBYY FineReader(ABBYY, 2010), OmniPage Pro (Nuance Commu-nications, Inc., 2010), and Tesseract (Google, Inc.,2010).
In addition, the curator of the collection hascreated a ?gold standard?
transcription, from whichit is possible to obtain accurate measures of averagedocument word error rates (WER) for each engine,which are: 19.9%, 30.4%, and 50.1% respectively.241While the real-world data is attractive as an ex-ample of just the sort of data that the questions ad-dressed here apply to, it is limited in size and scope.All of the documents in the Eisenhower corpus dis-cuss the fairly narrow topic of troop movements andbattle developments taking place at the end of WorldWar II.
Neither the subject matter nor the means ofconveyance allowed for a large or diverse vocabu-lary of discourse.In an attempt to generalize our results to largerand more diverse data, we also ran experimentsusing synthetic OCR data.
This synthetic datawas created by corrupting ?clean?
datasets, addingcharacter-level noise.
The synthetic data was cre-ated by building a noise model based on mistakesmade by the worst performing OCR engine on theEisenhower dataset, Tesseract.To construct the noise model, a character-levelalignment between the human transcribed Eisen-hower documents and the OCR output was first com-puted.
From this alignment, the contingency tableMd was generated such that Mdx,y was the count ofthe instances in which a character x in the transcriptwas aligned with a y in the OCR output.
The rowsin this matrix were then normalized so that each rep-resented the parameters of a categorical distribution,conditioned on x.
To parameterize the amount ofnoise being generated, the Md matrix was interpo-lated with an identity matrix I using a parameter ?
sothat the final interpolated parameters Mi were cal-culated with the formula Mi = ?Md + (1 ?
?
)I.So that at ?
= 0, Mi = I and no errors were in-troduced.
At ?
= 1.0, Mi = Md, and we wouldexpect to see characters corrupted at the same rateas in the output of the OCR engine.We then iterated over each document, choosing anew (possibly the same) character yl for each orig-inal character xl according to the probability distri-bution p(yl = w?|xl = w) = M iw,w?
.
Our pro-cess was a one-substitution algorithm, as we did notinclude instances of insertions or deletions, conse-quently words were changed but not split or deleted.This allowed for a more straightforward calculationof word error rate.
Segmentation errors can stilloccur in the learning stage, however, as the noisemodel sometimes replaced alphabet characters withpunctuation characters, which were treated as delim-iters by our tokenizer.Dataset |D| K # Types # Tokens20 News 19997 20 107211 2261805Reuters 11367 81 29034 747458Enron 4935 32 60495 2063667Eisenhower 604 N/A 8039 76674Table 1: Summary of test dataset characteristics.
|D| isthe number of documents in the dataset.
K is the numberof human-labeled classes provided with the dataset.We chose three datasets to corrupt: 20 News-groups (Lang, 1995), Reuters 21578 (Lewis, 1997),and the LDC-annotated portion of the Enron e-mailarchive (Berry et al, 2007).
Each of these datasetswere corrupted at values ?
= i?0.01 for i ?
(0, 13).At this point, the word error rate of the corrupteddata was near 50% and, since this was approxi-mately the WER observed for the worst OCR engineon the real-world data, we chose to stop there.
Theword error rate was calculated during the corruptionprocess.
Here is an example sentence corrupted attwo ?
values:?
= 0.000 I am also attaching the RFP itself.?
= 0.02 I am also attachEng the RFP itself.?
= 0.10 I Jm alAo attaching the RFP itself.Table 3 shows some basic statistics for thedatasets.
The values shown are for the ?clean?
ver-sions of the data.
For an example of how noiseand pre-processing techniques affect these countssee Section 4.1.It is interesting to note that the word error ratesproduced by the noise model appear to be signif-icantly higher than first expected.
One might as-sume that the WER should increase fairly steadilyfrom 0% at ?
= 0 to about 50% (the error rateachieved by the Tesseract OCR engine on the Eisen-hower dataset) at ?
= 1.
There are at least twosources for the discrepancy.
First, the vocabularyof the Eisenhower dataset does not match well withthat of any of the source datasets from which thesynthetic data were generated.
This means that theword and character distributions are different and sothe error rates will be as well.
Secondly, whereas ourtechnique gives the same probability of corruption toall instances of a given character, errors in true OCRoutput are bursty and more likely to be concentratedin specific tokens, or regions, of a document.
This242is because most sources of noise do not affect docu-ment images uniformly.
Also, modern OCR enginesdo not operate at just the character level.
They in-corporate dictionaries and language models to pre-vent them from positing words that are highly un-likely.
As a consequence, an OCR engine is muchmore likely to either get a whole word correct, or tomiss it completely, concentrating multiple errors ina single word.
This is the difference between 10 er-rors in a single word, which only contributes 1 to thenumerator of the WER formula and 10 errors spreadacross 10 different words, which contributes 10 tothe numerator.
Furthermore, because content bear-ing words tend to be relatively rare, language mod-els are poorer for them than for frequent functionwords, meaning that the words most correlated withsemantics are also the most likely to be corrupted byan OCR engine.An example of this phenomenon is easy to find.In the Enron corpus, there are 165,871 instances ofthe word ?the?
and 102 instances of the string ?thc?.Since ?c?
has a high rate of confusion with ?e?, wewould expect at least some instances of ?the?
to becorrupted to ?thc?
by the error model.
At ?
= 0.03,there are 156,663 instances of the word ?the?
and513 instances of ?thc?.
So, the noise model converts?the?
to ?thc?
roughly 0.3% of the time.
In con-trast, there are no instances of ?thc?
in the TesseractOCR output even though there are 5186 instancesof ?the?
in the transcription text, and so we wouldexpect approximately 16 occurrences of ?thc?
if theerrors introduced by the noise model were truly rep-resentative of the errors in the actual OCR output.Another interesting property of the noise intro-duced by actual OCR engines and our syntheticnoise model is the way in which this noise affectswords distributions.
This is very important, sinceword occurrence and co-occurrence counts are thebasis for model inference in both clustering andtopic modeling.
As mentioned previously, one com-mon way of lessening the impact of OCR noisewhen training topic models over OCRed data is toapply a frequency cutoff filter to cull words that oc-cur fewer than a certain number of times.
Figures 1and 2 show the number of word types that are culledfrom the synthetic 20 Newsgroups OCR data and theEisenhower OCR data, respectively, at various levelsof noise.
Note that the cutoff filters use a strict ?less0 10 20 30 40 50WER02000004000006000008000001000000NumberofFeatures Culled2510Figure 1: The number of word types culled with fre-quency cutoff filters applied to the 20 Newsgroups datawith various levels of errors introduced.than?, so a frequency cutoff of 2 eliminates onlywords that occur once in the entire dataset.
Also,these series are additive, as the words culled witha frequency cutoff of 2 are a subset of those culledwith a frequency cutoff of j > 2.In both cases, it is apparent that by far the largestimpact that noise has is in the creation of single-tons.
It seems that the most common corruptions inthese scenarios is the creation of one-off word typesthrough a unique corruption of a (most likely rare)word.
This means that it is unlikely that enough evi-dence will be available to associate, through similarcontexts, the original word and its corrupted forms.Due to the fact that most clustering and topicmodels ignore the forms of word tokens (the charac-ters that make them up), and only take into accountword identities, we believe that the similarity in theway in which real OCR engines and our syntheticOCR noise model distort word distributions is suf-ficient evidence to support the use of the syntheticdata until larger and better real-world OCR datasetscan be made available.
Though the actual errors willtake a different form, the character-level details ofthe errors are less relevant than the word distributionalterations for the models in question.4 Experimental ResultsWe ran experiments on both the real and syntheticOCR data.
In this section we explain our experi-2430 10 20 30 40 50 60WER50001000015000200002500030000NumberofWordsCulled2510Figure 2: The number of word types culled with fre-quency cutoff filters applied to the transcript and threeOCR engine outputs for the Eisenhower data.mental methodology and present both empirical andqualitative analyses of the results.4.1 MethodologyFor the synthetic OCR datasets, we ran clusteringexperiments using EM on a mixture of multinomials(c.f.
(Walker and Ringger, 2008)).
We specifiedthe number of clusters to be the same as the num-ber of classes provided with the data.
Clusters wereevaluated using several external cluster quality met-rics which compare ?gold standard?
labels to thosecreated through clustering.
The metrics used wereVariation of Information (VI) (Meila?, 2007), andthe Adjusted Rand Index (ARI) (Hubert and Arabie,1985).
Other metrics were also calculated (e.g.
theV-Measure (Rosenberg and Hirschberg, 2007), andAverage Entropy (Liu et al, 2003)), but these resultswere excluded due to space constraints and the factthat their plots are similar to those shown.
We didnot cluster the Eisenhower data because of the ab-sence of the class labels necessary for evaluation.For both the synthetic and non-synthetic data wealso trained LDA topic models (Blei et al, 2003) us-ing Gibbs sampling.
We used the implementationfound in the MALLET software package (McCal-lum, 2002) with the option enabled to learn the pri-ors during sampling as discussed by Wallach et al(2009a).
Each LDA model was trained on 90% ofthe documents in each dataset.
The trained modelwas used to calculate an estimate of the marginallog-likelihood of the remaining 10% of the docu-ments using the left-to-right algorithm (Wallach etal., 2009b).
The number of topics used for eachdataset was adjusted a priori according to the num-ber of documents it contained.
We used 100 topicsfor Enron and Eisenhower, 150 for Reuters, and 200for 20 Newsgroups.In addition to running experiments on the ?raw?synthetic data, we also applied simple unsupervisedfeature selectors before training in order to evalu-ate the effectiveness of such measures in mitigat-ing problems caused by OCR errors.
For the topicmodeling (LDA) experiments three feature selectorswere used.
The first method employed was a simpleterm frequency cutoff filter (TFCF), with a cutoffof 5 as in (Wang and McCallum, 2006).
The nextmethod employed was Term Contribution (TC), afeature selection algorithm developed for documentclustering (Liu et al, 2003).
Term contribution isparameterized by the number of word types that areto remain after selection.
We attempted three val-ues for this parameter, 10,000, 20,000, and 50,000.The final method we employed was a method calledTop-N per Document (TNPD) (Walker and Ring-ger, 2010), which is a simple feature selection al-gorithm that first assigns each type in every doc-ument a document-specific score (e.g.
its TF-IDFweight), and then selects words to include in the fi-nal vocabulary by choosing the N words with thehighest score from each document in the corpus.
Wefound that N = 1 gave the best results at the great-est reduction in word types.
After the vocabulary isbuilt, all words not in the vocabulary are culled fromthe documents.
This does not mean that all docu-ments contain only one word after feature selection,as the top word in one document may occur in manyother documents, even if it is not the top word inthose documents.
Likewise, if two documents wouldboth contribute the same word, then the second doc-ument makes no contribution to the vocabulary.
Thisprocess can result in vocabularies with thousands ofwords, leaving sufficient words in each documentfor analysis.
For the clustering experiments, initialtests showed little difference in the performance ofthe feature selectors, so only the TNPD selector wasused.
Figures 3(a) and 3(b) show how the variouspre-processing methods affect word type and token2440 5 10 15 20 25 30 35 40 45Word Error Rate0100002000030000400005000060000Typestc.10000tc.20000tc.50000tfcf.5tnpd.1(a) The number of word types remaining after pre-processing.0 5 10 15 20 25 30 35 40 45Word Error Rate6000008000001000000120000014000001600000180000020000002200000Tokenstc.10000tc.20000tc.50000tfcf.5tnpd.1(b) The number of word tokens remaining after pre-processing.Figure 3: The effect of pre-processing on token and type counts for the 20 Newsgroups dataset at various error rates.counts, respectively, for the 20 Newsgroups dataset.In contrast, without pre-processing the number oftypes scales from 107,211 to 892,983 and the num-ber of tokens from 2,261,805 to 3,073,208.Because all of these procedures alter the numberof words and tokens in the final data, log-likelihoodmeasured on a held-out set cannot be used to accu-rately compare the quality of topic models trainedon pre-processed data, as the held-out data will con-tain many unknown words.
If the held-out data isalso pre-processed to only include known words,then the likelihood will be greater for those proce-dures that remove the most tokens, as the productthat dominates the calculation will have fewer terms.If unknown words are allowed to remain, even asmoothed model will assign them very little prob-ability and so models will be heavily penalized.We use an alternative method for evaluating thetopic models, discussed in (Griffiths et al, 2005), toavoid the aforementioned problems with an evalu-ation based on log-likelihood.
Since the syntheticdata is derived from datasets that have topical docu-ment labels, we are able to use the output from LDAin a classification problem with the word vectorsfor each document being replaced by the assignedtopic vectors.
This is equivalent to using LDA as adimensionality reduction pre-process for documentclassification.
A naive Bayes learner is trained on aportion of the topic vectors, labeled with the origi-nal document label, and then the classification accu-racy on a held-out portion of the data is computed.Ten-fold cross-validation is used to control for sam-pling issues.
The rationale behind this evaluation isthat, even though the topics discovered by LDA willnot correspond directly to the labels, there should atleast be a high degree of correlation.
Models thatdiscover topical semantics that correlate well withthe labels applied by humans will yield higher clas-sification accuracies and be considered better mod-els according to this metric.To compensate for the randomness inherent inthe algorithms, each experiment was replicated tentimes.
The results of these runs were averaged toproduce the values reported here.4.2 Empirical AnalysisBoth the mixture of multinomials document modeland LDA appear to be fairly resilient to character-level noise.
Figures 4 and 5 show the results of thedocument clustering experiments with and withoutfeature selection, respectively.
Memory issues pre-vented the collection of results for the highest errorrates on the Enron and Reuters data without featureselection.With no pre-processing, the results are somewhatmixed.
The Enron dataset experiences almost noquality degradation as the WER increases, yieldingremarkably constant results according to the metrics.However, this may be an artifact of the relativelypoor starting performance for this dataset, a resultof the fact that the gold standard labels do not align2450 5 10 15 20 25 30 35 40 45Word Error Rate0.00.10.20.30.40.5ari20_newsgroupsreuters21578enron(a) Adjusted Rand Index results0 5 10 15 20 25 30 35 40 45Word Error Rate2.02.53.03.54.04.5vi20_newsgroupsreuters21578enron(b) Variation of Information results (lower is better)Figure 4: Results for the clustering experiments on the three synthetic datasets without feature selection.well with automatically discovered patterns becausethey correspond to external events.
In contrast, theReuters data appears to experience drastic degrada-tion in performance.
Once feature selection occurs,however, performance remains much more stable aserror rates increase.Figure 6(a) shows the results of running LDA onthe transcript and digitized versions of the Eisen-hower dataset.
Log-likelihoods of the held-out setare plotted with respect to the WER observed foreach OCR engine.
The results support the find-ing that the WER of the OCR engine that producedthe data has a significant negative correlation withmodel quality.
Unfortunately, it was not possibleto compare the performance of the pre-processingmethods on this dataset, due to a lack of documenttopic labels and the deficiencies of log-likelihoodmentioned previously.Figure 6(b) shows the results of the LDA topic-modeling experiments on the three ?raw?
syntheticdatasets.
Similar trends are observed in this graph.LDA experiences a marked degree of performancedegradation, with all of the trend lines indicating alinear decrease in log-likelihood.Figures 7(a) through 7(c) show the results of eval-uating the various proposed pre-processing proce-dures in the context of topic modeling.
In the graph?noop.0?
represents no pre-processing, ?tc.N?
arethe Term Contribution method parameterized to se-lectN word types, ?tfcf.5?
is the term frequency cut-off filter with a cutoff of 5 and ?tnpd.1?
is the TopN per Document method with N = 1.
The y-axis isthe average of the results for 10 distinct trials, wherethe output for each trial is the average of the ten ac-curacies achieved using ten-fold cross-validation asdescribed in Section 4.1.Here, the cross-validation accuracy metric revealsa slightly different story.
These results show thattopic quality on both the raw and pre-processednoisy data degrades at a rate relative to the amountof errors in the data.
That is, the difference in perfor-mance between two relatively low word error rates(e.g.
5% and 7% on the Reuters data) is small,whereas the differences between two high error rates(e.g.
30% and 32% on the Reuters data) can be rela-tively quite large.While pre-processing does improve model qual-ity, in the case of LDA this improvement amountsto a nearly constant boost; at high error rates qualityis improved the same amount as at low error rates.Degradations in model quality, therefore, follow thesame trends, occurring at mostly the same rate inpre-processed data as in the raw noisy data.
This isin contrast to the clustering experiments where pre-processing virtually eliminates failure trends.4.3 Qualitative AnalysisHigher values measured with automated metricssuch as log-likelihood on a held-out set and thecross-validation classification metric discussed here2460 10 20 30 40 50Word Error Rate0.00.10.20.30.40.5ari20_newsgroupsreuters21578enron(a) Adjusted Rand Index results0 10 20 30 40 50Word Error Rate2.02.53.03.54.04.5vi 20_newsgroupsreuters21578enron(b) Variation of Information results (lower is better)Figure 5: Results for the clustering experiments on the three synthetic OCR datasets with TNPD feature selection.do not necessarily indicate superior topics accordingto human judgement (Chang et al, 2009).
In orderto provide a more thorough discussion of the relativequality of the topic models induced on the OCR dataversus those induced on clean data, we sampled theresults of several of the runs of the LDA algorithm.In Tables 2 and 3 we show the top words for thefive topics with the highest learned topic prior (?
inthe LDA literature) learned during Gibbs sampling.This information is shown for the Reuters data firstwith no corruption and then at the highest error ratefor which we have results for that data of 45% WER.In general, there appears to be a surprisingly goodcorrelation between the topics learned on the cleandata and those learned on the corrupted data, giventhe high level of noise involved.
The topics aregenerally cohesive, containing mostly terms relat-ing to financial market news.
However, the topicstrained on the clean data, though all related to finan-cial markets, are fairly distinctive.
Topic 3, for ex-ample seems to be about fluctuations in stock prices,and Topics 106 and 34 about business acquisitionsand mergers.
The topics trained on the noisy dataare fairly homogeneous and the differences betweenthem are more difficult to identify.In addition, it appears as though the first topic(topic 93) is not very coherent at all.
This topic issignificantly larger, in terms of the number of tokensassigned to it than the other topics shown in eithertable.
In addition, the most probable words listed forthe topic seem less cohesive than for the other top-ics.
It contains many two-letter words that are likelya mixture of valid terms (e.g., stock exchange andticker symbols, and parts of place names like ?Riode Janeiro?)
and corruptions of real words.
For ex-ample, even though there are no instances of ?ts?
asa distinct token in the clean Reuters data, it is in thelist of the top 19 words for topic 93.
This is perhapsdue to the fact that ?is?
can easily be converted to?ts?
by mistaking t for i.It is also the case that, for most topics learnedon the corrupted data, the most probable words forthose topics tend to be shorter, on average, than fortopics learned on clean data.
We believe this is dueto the fact that the processes used to add noise to thedata (both real OCR engines and our synthetic noisemodel) are more likely to corrupt long words, es-pecially in the case of the synthetic data which wascreated using a character-level noise model.Examination of the data tends to corroborate thishypothesis, though even long words usually containonly a few errors.
For example, in the 20 News-groups data there are 379 instances of the word ?yes-terday?, a long word that is not close to other Englishwords in edit distance.
When the data has been cor-rupted to a WER of 47.9%, however, there are only109 instances of ?yesterday?
and 132 tokens that arewithin an edit distance of 1 from ?yesterday?.To some extent, we would expect to observe sim-ilar trends in real-world data.
However, most OCR24710 0 10 20 30 40 50 60Word Error Rate950009000085000800007500070000Log-likelihood of Held-outabbyyomnipagetesseracttranscript(a) Eisenhower Communique?s0 10 20 30 40 50Word Error Rate70000006000000500000040000003000000200000010000000Log-likelihood of Held-out20_newsgroupsreuters21578enron(b) Synthetic DataFigure 6: Log-likelihood of heldout data for the LDA experiments.0 5 10 15 20 25 30 35 40 45Word Error Rate52545658606264666870Avereage CV Accuracynoop.0tc.10000tc.20000tc.50000tfcf.5tnpd.1(a) 20 Newsgroups Data0 5 10 15 20 25 30 35 40 45Word Error Rate67686970717273Avereage CV Accuracynoop.0tc.10000tc.20000tc.50000tfcf.5tnpd.1(b) Reuters Data0 5 10 15 20 25 30 35 40 45Word Error Rate34353637383940414243Avereage CV Accuracynoop.0tc.10000tc.20000tc.50000tfcf.5tnpd.1(c) Enron DataFigure 7: Average ten-fold cross-validation accuracy for the LDA pre-processing experiments on the synthetic OCRdata.248Topic Words Tokens64 told, market, reuters, reuter, added, time,year, major, years, president, make, made,march, world, today, officials, industry,government, move671593 year, pct, prices, expected, rise, lower,higher, demand, increase, due, fall, de-cline, current, high, end, added, level,drop, market32907106 reuter, corp, company, unit, sale, march,dlrs, mln, sell, subsidiary, acquisition,terms, group, april, purchase, acquired,products, division, business2216734 shares, dlrs, company, mln, stock, pct,share, common, reuter, corp, agreement,march, shareholders, buy, cash, outstand-ing, merger, acquire, acquisition226687 mln, cts, net, shr, qtr, revs, reuter, avg, shrs,march, mths, dlrs, sales, st, corp, oct, note,year, april18511Table 2: Top words for the five topics with the highest ?prior values found using MALLET for one run of LDAon the uncorrupted Reuters data.engines employ language models and dictionaries toattempt to mitigate OCR errors.
As a result, giventhat a word recognition error has occurred in trueOCR output, it is more likely to be an error that liesat an edit distance greater than one from the trueword, or else it would have been corrected inter-nally.
For example, there are 349 instances of theword ?yesterday?
in the Eisenhower transcripts, and284 instances in the Tesserect OCR output and only5 tokens within an edit distance of one, meaning that60 corruptions of this word contained more than oneerror, making up 90% of the errors for that word.However, many of these errors still contain most ofthe letters from the original word (e.g.
?yesterdj.
?,and ?yestjkday?).
In all cases, the corrupted versionsof a given word are very rare, occurring usually onlyonce or twice in the noisy output, making them use-less features for informing a model.5 Conclusions and Future WorkThe primary outcome of these experiments is anunderstanding regarding when clustering and LDAtopic models can be expected to function well onnoisy OCR data.
Our results imply that clusteringmethods should perform almost as well on OCR dataas they do on clean data, provided that a reasonablefeature selection algorithm is employed.
The LDAtopic model degraded less gracefully in performanceTopic Words Tokens93 reuter, march, pct, year, april, ed, market,er, told, es, st, end, ts, al, de, ng, id, sa,added25893299 company, pct, corp, shares, stock, dlrs,share, offer, group, reuter, mln, march,unit, stake, buy, cash, bid, sale, board5037796 mln, cts, net, shr, qtr, dlrs, revs, reuter,note, oper, avg, march, shrs, year, mths, st,sales, corp, oct54659141 mln, dlrs, year, net, quarter, share, com-pany, billion, tax, sales, earnings, dlr,profit, march, income, ln, results, sale, corp4047553 pct, year, rose, rise, january, february, fell,march, index, december, month, figures,compared, reuter, rate, earlier, show, ago,base22556Table 3: Top words for the five topics with the highest ?prior values found using MALLET for one run of LDAon the Reuters data corrupted with the data-derived noisemodel to a WER of 45%.with the addition of character level errors to its in-put, with higher error rates impacting model qualityin a way that was apparent empirically in the log-likelihood and ten-fold cross-validation metrics aswell as through human inspection of the producedtopics.
Pre-processing the data also helps modelquality for LDA, yet still yields failure trends sim-ilar to those observed on unprocessed data.We found it to be the case that even in data withhigh word error rates, corrupted words often sharemany characters in common with their uncorruptedform.
This suggests an approach in which word sim-ilarities are used to cluster the unique corrupted ver-sions of a word in order to increase the evidenceavailable to the topic model during training time andimprove model quality.
As the quality of models in-creases on these noisy datasets, we anticipate a con-sequent rise in their usefulness to researchers andhistorians as browsing the data and mining it for use-ful patterns becomes more efficient and profitable.AcknowledgmentsWe would like to thank the Fulton Supercomput-ing Center at BYU for providing the computing re-sources required for experiments reported here.ReferencesABBYY.
2010.
ABBYY finereader.http://finereader.abbyy.com.249S.
Agarwal, S. Godbole, D. Punjani, and Shourya Roy.
2007.How much noise is too much: A study in automatic text clas-sification.
In Proceedings of the Seventh IEEE Intl.
Conf.
onData Mining (ICDM 2007), pages 3?12.Steven M. Beitzel, Eric C. Jensen, and David A. Grossman.2003.
A survey of retrieval strategies for ocr text collec-tions.
In In Proceedings of the Symposium on DocumentImage Understanding Technologies.Michael W. Berry, Murray Brown, and Ben Signer.
2007.
2001topic annotated Enron email data set.David M. Blei and John D. Lafferty.
2006.
Dynamic topicmodels.
In Proceedings of the 23rd Intl.
Conf.
on MachineLearning (ICML 2006).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003.Latent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Horst Bunke.
2003.
Recognition of cursive romanhandwriting- past, present and future.
In 7th InternationalConference on Document Analysis and Recognition (ICDAR2003), volume 1, pages 448?459.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, ChongWang, and David Blei.
2009.
Reading tea leaves: Howhumans interpret topic models.
In Advances in Neural Infor-mation Processing Systems 22, pages 288?296.Faisal Farooq, Anurag Bhardwaj, and Venu Govindaraju.
2009.Using topic models for OCR correction.
Intl.
Journalon Document Analysis and Recognition (IJDAR), 12(3),September.Google, Inc. 2010.
Tesseract.http://code.google.com/p/tesseract-ocr.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics and syntax.In In Advances in Neural Information Processing Systems17, pages 537?544.
MIT Press.Lawrence Hubert and Phipps Arabie.
1985.
Comparing parti-tions.
Journal of Classification, 2(1):193?218, December.David Reed Jordan.
1945.
Daily battle communiques, 1944-1945.
Harold B. Lee Library, L. Tom Perry Special Collec-tions, MSS 2766.Ken Lang.
1995.
NewsWeeder: Learning to filter netnews.In Proceedings of the Twelfth International Conference onMachine Learning, pages 331?339.D.
Lewis.
1997.
Reuters-21578 text categorization test collec-tion.
http://www.research.att.com/?lewis.Tao Liu, Shengping Liu, Zheng Chen, and Wei-Ying Ma.
2003.An evaluation on feature selection for text clustering.
In Pro-ceedings of the Twentieth Intl.
Conf.
on Machine Learning(ICML 2003), August.Daniel Lopresti.
2008.
Optical character recognition errors andtheir effects on natural language processing.
In Proceedingsof the second workshop on Analytics for noisy unstructuredtext data (AND 2008), pages 9?16.William B. Lund and Eric.
K Ringger.
2009.
Improving op-tical character recognition through efficient multiple systemalignment.
In Proceedings of the Joint Conf.
on Digital Li-braries (JCDL?09), June.Andrew Kachites McCallum.
2002.
MALLET: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.Marina Meila?.
2007.
Comparing clusterings?an informa-tion based distance.
Journal of Multivariate Analysis,98(5):873?895.David Mimno and Andrew Mccallum.
2007.
Organizing theOCA: learning faceted subjects from a library of digitalbooks.
In Proceedings of the Joint Conf.
on Digital Libraries(JCDL?07), pages 376?385.Cosmin Munteanu, Ronald Baecker, Gerald Penn, Elaine Toms,and David James.
2006.
The effect of speech recognitionaccuracy rates on the usefulness and usability of webcastarchives.
In Proceedings of the SIGCHI conference on Hu-man Factors in computing systems, pages 493?502.David J. Newmann and Sharon Block.
2006.
Probabilistic topicdecomposition of an eighteenth-century american newspa-per.
J.
Am.
Soc.
Inf.
Sci.
Technol., 57(6):753?767, February.Nuance Communications, Inc. 2010.
OmniPage Pro.http://www.nuance.com/imaging/products/omnipage.asp.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: Aconditional entropy-based external cluster evaluation mea-sure.
In Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Language Learn-ing (EMNLP-CoNLL 2007).Kazem Taghva, Julie Borsack, and Allen Condit.
1994.
Resultsof applying probabilistic ir to ocr text.
In in Proc.
17th Intl.ACM/SIGIR Conf.
on Research and Development in Infor-mation Retrieval, pages 202?211.Kazem Taghva, Tom Nartker, Julie Borsack, Steve Lumos,Allen Condit, and Ron Young.
2001.
Evaluating text catego-rization in the presence of ocr errors.
In In Proc.
IS&T/SPIE2001 Intl.
Symp.
on Electronic Imaging Science and Tech-nology, pages 68?74.
SPIE.Daniel Walker and Eric Ringger.
2008.
Model-based documentclustering with a collapsed gibbs sampler.
In Proceedings ofthe 14th ACM SIGKDD Intl.
Conf.
on Knowledge Discoveryand Data Mining (KDD 2008).Daniel Walker and Erik K. Ringger.
2010.
Top N per docu-ment: Fast and effective unsupervised feature selection fordocument clustering.
Technical Report 6, Brigham YoungUniversity.
http://nlp.cs.byu.edu/techreports/BYUNLP-TR6.pdf.Hanna Wallach, David Mimno, and Andrew McCallum.
2009a.Rethinking LDA: Why priors matter.
In Advances in NeuralInformation Processing Systems 22, pages 1973?1981.Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, andDavid Mimno.
2009b.
Evaluation methods for topic models.In Proceedings of the 26th Annual Intl.
Conf.
on MachineLearning (ICML 2009), pages 1105?1112.Xuerui Wang and Andrew McCallum.
2006.
Topics over time:A non-markov continuous-time model of topical trends.
InProceedings of the 12th ACM SIGKDD Intl.
Conf.
on Knowl-edge Discovery and Data Mining (KDD 2006).Michael L. Wick, Michael G. Ross, and Erik G. Learned-Miller.
2007.
Context-sensitive error correction: Usingtopic models to improve OCR.
In Proceedings of the NinthIntl.
Conf.
on Document Analysis and Recognition (ICDAR2007), pages 1168?1172.250
