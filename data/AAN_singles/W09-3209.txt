Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 58?65,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPRanking and Semi-supervised Classificationon Large Scale Graphs Using Map-ReduceDelip RaoDept.
of Computer ScienceJohns Hopkins Universitydelip@cs.jhu.eduDavid YarowskyDept.
of Computer ScienceJohns Hopkins Universityyarowsky@cs.jhu.eduAbstractLabel Propagation, a standard algorithmfor semi-supervised classification, suffersfrom scalability issues involving memoryand computation when used with large-scale graphs from real-world datasets.
Inthis paper we approach Label Propagationas solution to a system of linear equationswhich can be implemented as a scalableparallel algorithm using the map-reduceframework.
In addition to semi-supervisedclassification, this approach to Label Prop-agation allows us to adapt the algorithm tomake it usable for ranking on graphs andderive the theoretical connection betweenLabel Propagation and PageRank.
We pro-vide empirical evidence to that effect usingtwo natural language tasks ?
lexical relat-edness and polarity induction.
The versionof the Label Propagation algorithm pre-sented here scales linearly in the size ofthe data with a constant main memory re-quirement, in contrast to the quadratic costof both in traditional approaches.1 IntroductionNatural language data often lend themselves to agraph-based representation.
Words can be linkedby explicit relations as in WordNet (Fellbaum,1989), and documents can be linked to one an-other via hyperlinks.
Even in the absence of such astraightforward representation it is possible to de-rive meaningful graphs such as the nearest neigh-bor graphs, as done in certain manifold learningmethods, e.g.
Roweis and Saul (2000); Belkin andNiyogi (2001).
Typically, these graphs share thefollowing properties:?
They are edge-weighted.?
The edge weight encodes some notion of re-latedness between the vertices.?
The relation represented by edges is at leastweakly transitive.
Examples of such rela-tions include, ?is similar to?, ?is more gen-eral than?, and so on.
It is important that therelations selected are transitive for the graph-based learning methods using random walks.Such graphs present several possibilities forsolving natural language problems involving rank-ing, classification, and clustering.
Graphs havebeen successfully employed in machine learningin a variety of supervised, unsupervised, and semi-supervised tasks.
Graph based algorithms performbetter than their counterparts as they capture thelatent structure of the problem.
Further, their ele-gant mathematical framework allows simpler anal-ysis to gain a deeper understanding of the prob-lem.
Despite these advantages, implementationsof most graph-based learning algorithms do notscale well on large datasets from real world prob-lems in natural language processing.
With largeamounts of unlabeled data available, the graphscan easily grow to millions of nodes and most ex-isting non-parallel methods either fail to work dueto resource constraints or find the computation in-tractable.In this paper we describe a scalable implemen-tation of Label Propagation, a popular randomwalk based semi-supervised classification method.We show that our framework can also be used forranking on graphs.
Our parallel formulation showsa theoretical connection between Label Propaga-tion and PageRank.
We also confirm this em-pirically using the lexical relatedness task.
The58proposed Parallel Label Propagation scales up lin-early in the data and the number of processing ele-ments available.
Also, the main memory requiredby the method does not grow with the size of thegraph.The outline of this paper is as follows: Section 2introduces the manifold assumption and explainswhy graph-based learning algorithms perform bet-ter than their counterparts.
Section 3 motivatesthe random walk based approach for learning ongraphs.
Section 4 introduces the Label Propaga-tion method by Zhu et al (2003).
In Section 5 wedescribe a method to scale up Label Propagationusing Map-Reduce.
Section 6 shows how LabelPropagation could be used for ranking on graphsand derives the relation between Label Propaga-tion and PageRank.
Parallel Label Propagation isevaluated on ranking and semi-supervised classifi-cation problems in natural language processing inSection 8.
We study scalability of this algorithm inSection 9 and describe related work in the area ofparallel algorithms and machine learning in Sec-tion 10.2 Manifold AssumptionThe training data D can be considered as a collec-tion of tuples D = (X ,Y) where Y are the labelsand X are the features, and the learned modelMis a surrogate for an underlying physical processwhich generates the data D. The data D can beconsidered as a sampling from a smooth surface ora manifold which represents the physical process.This is known as the manifold assumption (Belkinet al, 2005).
Observe that even in the simple caseof Euclidean data (X = {x : x ?
Rd}) as shownin Figure 1, points that lie close in the Euclideanspace might actually be far off on the manifold.A graph, as shown in Figure 1c, approximates thestructure of the manifold which was lost in vector-ized algorithms operating in the Euclidean space.This explains the better performance of graph al-gorithms for learning as seen in the literature.3 Distance measures on graphsMost learning tasks on graphs require some notionof distance or similarity to be defined between thevertices of a graph.
The most obvious measure ofdistance in a graph is the shortest path between thevertices, which is defined as the minimum numberof intervening edges between two vertices.
This isalso known as the geodesic distance.
To convertthis distance measure to a similarity measure, wetake the reciprocal of the shortest-path length.
Werefer to this as the geodesic similarity.Figure 2: Shortest path distances on graphs ignorethe connectivity structure of the graph.While shortest-path distances are useful inmany applications, it fails to capture the followingobservation.
Consider the subgraph of WordNetshown in Figure 2.
The term moon is con-nected to the terms religious leaderand satellite.1Observe that bothreligious leader and satellite areat the same shortest path distance from moon.However, the connectivity structure of the graphwould suggest satellite to be more similarthan religious leader as there are multiplesenses, and hence multiple paths, connectingsatellite and moon.Thus it is desirable to have a measure that cap-tures not only path lengths but also the connectiv-ity structure of the graph.
This notion is elegantlycaptured using random walks on graphs.4 Label Propagation: Random Walk onManifold GraphsAn efficient way to combine labeled and unla-beled data involves construction of a graph fromthe data and performing a Markov random walkon the graph.
This has been utilized in Szummerand Jaakkola (2001), Zhu et.
al.
(2003), and Azran(2007).
The general idea of Label Propagation in-volves defining a probability distribution F overthe labels for each node in the graph.
For labelednodes, this distribution reflects the true labels andthe aim is to recover this distribution for the unla-beled nodes in the graph.Consider a graph G(V,E,W ) with vertices V ,edges E, and an n ?
n edge weight matrix W =1The religious leader sense of moon is due to SunMyung Moon, a US religious leader.59(a) (b) (c)Figure 1: Manifold Assumption [Belkin et al, 2005]: Data lies on a manifold (a) and points along themanifold are locally similar (b).
[wij], where n = |V |.
The Label Propagation al-gorithm minimizes a quadratic energy functionE =12?
(i, j) ?
Ewij(Fi?
Fj)2(1)The general recipe for using random walksfor classification involves constructing the graphLaplacian and using the pseudo-inverse of theLaplacian as a kernel (Xiao and Gutman, 2003).Given a weighted undirected graph, G(V,E,W ),the Laplacian is defined as follows:Lij=??
?diif i = j?wijif i is adjacent to j0 otherwise(2)where di=?jwij.It has been shown that the pseudo-inverse of theLaplacian L is a kernel (Xiao and Gutman, 2003),i.e., it satisfies the Mercer conditions.
However,there is a practical limitation to this approach.
Forvery large graphs, even if the graph Laplacians aresparse, their pseudo-inverses are dense matricesrequiring O(n2) space.
This can be prohibitive inmost computing environments.5 Parallel Label PropagationIn developing a parallel algorithm for LabelPropagation we instead take an alternate approachand completely avoid the use of inverse Lapla-cians for the reasons stated above.
Our approachfollows from the observation made from Zhu etal.
?s (2003) Label Propagation algorithm:Observation: In a weighted graph G(V,E,W )with n = |V | vertices, minimization of Equation(1) is equivalent to solving the following systemof linear equations.?
(i, j) ?
EwijFi=?
(i, j) ?
EwijFj(3)?c ?
classes(i)Fi(c) = 1 ?i, j ?
V.We use this observation to derive an iterativeLabel Propagation algorithm that we will later par-allelize.
Consider a weighted undirected graphG(V,E,W ) with the vertex set partitioned into VLand VU(i.e., V = VL?VU) such that all vertices inVLare labeled and all vertices in VUare unlabeled.Typically only a small set of vertices are labeled,i.e., |VU|  |VL|.
Let Fudenote the probabilitydistribution over the labels associated with vertexu ?
V .
For v ?
VL, Fvis known, and we alsoadd a ?dummy vertex?
v?to the graph G such thatwvv?= 1 and Fv?= Fv.
This is equivalent to the?clamping?
done in (Zhu et al, 2003).
Let VDbethe set of dummy vertices.Algorithm 1: Iterative Label Propogationrepeatforall v ?
(V ?
VD) doFv=?
(v,u)?EwuvFvRow normalize Fv.enduntil convergence or maxIterationsObserve that every iteration of Algorithm 1 per-forms certain operations on each vertex of thegraph.
Further, these operations only rely onlocal information (from neighboring vertices ofthe graph).
This leads to the parallel algorithm(Algorithm 2) implemented using the map-reducemodel.
Map-Reduce (Dean and Ghemawat, 2004)is a paradigm for implementing distributed algo-rithms with two user supplied functions ?map?
and?reduce?.
The map function processes the inputkey/value pairs with the key being a unique iden-60tifier for a node in the graph and the value corre-sponds to the data associated with the node.
Themappers run on different machines operating ondifferent parts of the data and the reduce functionaggregates results from various mappers.Algorithm 2: Parallel Label Propagationmap(key, value):begind = 0neighbors = getNeighbors(value);foreach n ?
neighbors dow = n.weight();d += w ?
n.getDistribution();endnormalize(d);value.setDistribution(d);Emit(key, value);endreduce(key, values): Identity ReducerAlgorithm 2 represents one iteration of Algo-rithm 1.
This is run repeatedly until convergenceor for a specified number of iterations.
The al-gorithm is considered to have converged if the la-bel distributions associated with each node do notchange significantly, i.e.,????F(i+1)?
F(i)???
?2< for a fixed  > 0.6 Label Propagation for RankingGraph ranking is applicable in a variety of prob-lems in natural language processing and informa-tion retrieval.
Given a graph, we would like torank the vertices of a graph with respect to a node,called the pivot node or query node.
Label Prop-agation and its variants (Szummer and Jaakkola,2001; Zhu et al, 2003; Azran, 2007) have beentraditionally used for semi-supervised classifica-tion.
Our view of Label Propagation (via Algo-rithm 1) suggests a way to perform ranking ongraphs.Ranking on graphs can be performed in the Par-allel Label Propagation framework by associatinga single point distribution with all vertices.
Thepivot node has a mass fixed to the value 1 at all it-erations.
In addition, the normalization step in Al-gorithm 2 is omitted.
At the end of the algorithm,the mass associated with each node determines itsrank.6.1 Connection to PageRankIt is interesting to note that Algorithm 1 bringsout a connection between Label Propagation andPageRank (Page et al, 1998).
PageRank is a ran-dom walk model that allows the random walk to?jump?
to its initial state with a nonzero proba-bility (?).
Given the probability transition matrixP = [Prs], where Prsis the probability of jumpingfrom node r to node s, the weight update for anyvertex (say v) is derived as followsvt+1= ?vtP + (1?
?
)v0(4)Notice that when ?
= 0.5, PageRank is reducedto Algorithm 1, by a constant factor, with the ad-ditional (1?
?
)v0term corresponding to the con-tribution from the ?dummy vertices?
VDin Algo-rithm 1.We can in fact show that Algorithm 1 reduces toPageRank as follows:vt+1= ?vtP + (1?
?)v0?
vtP +(1?
?
)?v0= vtP + ?v0(5)where ?
=(1??)?.
Thus by setting the edgeweights to the dummy vertices to ?, i.e., ?
(z, z?)
?E and z??
VD, wzz?= ?, Algorithm 1, and henceAlgorithm 2, reduces to PageRank.
Observe thatwhen ?
= 1 we get the original Algorithm 1.We?ll refer to this as the ?
?-correction?.7 Graph RepresentationSince Parallel Label Propagation algorithm usesonly local information, we use the adjacency listrepresentation (which is same as the sparse adja-cency matrix representation) for the graph.
Thisrepresentation is important for the algorithm tohave a constant main memory requirement as nofurther lookups need to be done while comput-ing the label distribution at a node.
The interfacedefinition for the graph is listed in Appendix A.Often graph data is available in an edge format,as <source, destination, weight> triples.
We useanother map-reduce step (Algorithm 3) to convertthat data to the form shown in Appendix A.8 EvaluationWe evaluate the Parallel Label Propagation algo-rithm for both ranking and semi-supervised clas-sification.
In ranking our goal is to rank the ver-tices of a graph with respect to a given node calledthe pivot/query node.
In semi-supervised classi-fication, we are given a graph with some vertices61Algorithm 3: Graph Constructionmap(key, value):beginedgeEntry = value;Node n(edgeEntry);Emit(n.id, n);endreduce(key, values):beginEmit(key, serialize(values));endlabeled and would like to predict labels for the re-maining vertices.8.1 RankingTo evaluate ranking, we consider the problemof deriving lexical relatedness between terms.This has been a topic of interest with applica-tions in word sense disambiguation (Patwardhanet al, 2005), paraphrasing (Kauchak and Barzilay,2006), question answering (Prager et al, 2001),and machine translation (Blatz et al, 2004), toname a few.
Following the tradition in pre-vious literature we evaluate on the Miller andCharles (1991) dataset.
We compare our rankingswith the human judegments using the Spearmanrank correlation coefficient.
The graph for thistask is derived from WordNet, an electronic lex-ical database.
We compare Algorithm 2 with re-sults from using geodesic similarity as a baseline.As observed in Table 1, the parallel implemen-tation in Algorithm 2 performs better than rank-ing using geodesic similarity derived from short-est path lengths.
This reinforces the motivation ofusing random walks as described in Section 3.Method SpearmanCorrelationGeodesic (baseline) 0.28Parallel Label 0.36PropagationTable 1: Lexical-relatedness results: Comparisonwith geodesic similarity.We now empirically verify the equivalence ofthe ?-corrected Parallel Label Propagation andPageRank established in Equation 4.
To do this,we use ?
= 0.1 in the PageRank algorithm andset ?
=(1??
)?= 9 in the ?-corrected Parallel La-bel Propagation algorithm.
The results are seen inTable 2.Method SpearmanCorrelationPageRank (?
= 0.1) 0.39Parallel Label 0.39Propagation (?
= 9)Table 2: Lexical-relatedness results: Comparisionof PageRank and ?-corrected Parallel Label Prop-agation8.2 Semi-supervised ClassificationLabel Propagation was originally developed as asemi-supervised classification method.
Hence Al-gorithm 2 can be applied without modification.After execution of Algorithm 2, every node v inthe graph will have a distribution over the labelsFv.
The predicted label is set to argmaxc?classes(v)Fv(c).To evaluate semi-supervised classification weconsider the problem of learning sentiment polar-ity lexicons.
We consider the polarity of a word tobe either positive or negative.
For example, wordssuch as good, beautiful , and wonderful are consid-ered as positive sentiment words; whereas wordssuch as bad, ugly, and sad are considered negativesentiment words.
Learning such lexicons has ap-plications in sentiment detection and opinion min-ing.
We treat sentiment polarity detection as asemi-supervised Label Propagation problem in agraph.
In the graph, each node represents a wordwhose polarity is to be determined.
Each weightededge encodes a relation that exists between twowords.
Each node (word) can have two labels:positive or negative.
It is important to note that La-bel Propagation, and hence Algorithms 1&2, sup-port multi-class classification but for the purposeof this task we have two labels.
The graph for thetask is derived from WordNet.
We use the Gen-eral Inquirer (GI)2data for evaluation.
GeneralInquirer is lexicon of English words hand-labeledwith categorical information along several dimen-sions.
One such dimension is called valence, with1915 words labeled ?Positiv?
(sic) and 2291 wordslabeled ?Negativ?
for words with positive and neg-ative sentiments respectively.
We used a random20% of the data as our seed labels and the restas our unlabeled data.
We compare our results2http://www.wjh.harvard.edu/?inquirer/62(a) (b)Figure 3: Scalability results: (a) Scaleup (b) Speedup(F-scores) with another scalable previous work byKim and Hovy (Kim and Hovy, 2006) in Table 2for the same seed set.
Their approach starts with afew seeds of positive and negative terms and boot-straps the list by considering all synonyms of pos-itive word as positive and antonyms of positivewords as negative.
This procedure is repeated mu-tatis mutandis for negative words in the seed listuntil there are no more words to add.Method Nouns Verbs AdjectivesKim & Hovy 34.80 53.36 47.28Parallel Label 58.53 83.40 72.95PropagationTable 3: Polarity induction results (F-scores)The performance gains seen in Table 3 shouldbe attributed to the Label Propagation in generalas the previous work (Kim and Hovy, 2006) didnot utilize a graph based method.9 Scalability experimentsWe present some experiments to study the scala-bility of the algorithm presented.
All our experi-ments were performed on an experimental clusterof four machines to test the concept.
The machineswere Intel Xeon 2.4 GHz with 1Gb main memory.All performance measures were averaged over 20runs.Figure 3a shows scaleup of the algorithm whichmeasures how well the algorithm handles increas-ing data sizes.
For this experiment, we used allnodes in the cluster.
As observed, the increase intime is at most linear in the size of the data.
Fig-ure 3b shows speedup of the algorithm.
Speedupshows how well the algorithm performs with in-crease in resources for a fixed input size.
Inthis case, we progressively increase the number ofnodes in the cluster.
Again, the speedup achievedis linear in the number of processing elements(CPUs).
An appealing factor of Algorithm 2 is thatthe memory used by each mapper process is fixedregardless of the size of the graph.
This makes thealgorithm feasible for use with large-scale graphs.10 Related WorkHistorically, there is an abundance of work in par-allel and distributed algorithms for graphs.
SeeGrama et al (2003) for survey chapters on thetopic.
In addition, the emergence of open-sourceimplementations of Google?s map-reduce (Deanand Ghemawat, 2004) such as Hadoop3has madeparallel implementations more accessible.Recent literature shows tremendous interest inapplication of distributed computing to scale upmachine learning algorithms.
Chu et al (2006)describe a family of learning algorithms that fitthe Statistical Query Model (Kearns, 1993).
Thesealgorithms can be written in a special summationform that is amenable to parallel speed-up.
Exam-ples of such algorithms include Naive Bayes, Lo-gistic Regression, backpropagation in Neural Net-works, Expectation Maximization (EM), Princi-pal Component Analysis, and Support Vector Ma-chines to name a few.
The summation form can beeasily decomposed so that the mapper can com-pute the partial sums that are then aggregated by areducer.
Wolfe et al (2008) describe an approachto estimate parameters via the EM algorithm in asetup aimed to minimize communication latency.The k-means clustering algorithm has been anarchetype of the map-reduce framework with sev-eral implementations available on the web.
In3http://hadoop.apache.org/core63addition, the Netflix Million Dollar Challenge4generated sufficient interest in large scale cluster-ing algorithms.
(McCallum et al, 2000), describealgorithmic improvements to the k-means algo-rithm, called canopy clustering, to enable efficientparallel clustering of data.While there is earlier work on scalable map-reduce implementations of PageRank (E.g., Gle-ich and Zhukov (2005)) there is no existing liter-ature on parallel algorithms for graph-based semi-supervised learning or the relationship betweenPageRank and Label Propagation.11 ConclusionIn this paper, we have described a parallel algo-rithm for graph ranking and semi-supervised clas-sification.
We derived this by first observing thatthe Label Propagation algorithm can be expressedas a solution to a set of linear equations.
This iseasily expressed as an iterative algorithm that canbe cast into the map-reduce framework.
This al-gorithm uses fixed main memory regardless of thesize of the graph.
Further, our scalability study re-veals that the algorithm scales linearly in the sizeof the data and the number of processing elementsin the cluster.
We also showed how Label Prop-agation can be used for ranking on graphs andthe conditions under which it reduces to PageR-ank.
We evaluated our implementation on twolearning tasks ?
ranking and semi-supervised clas-sification ?
using examples from natural languageprocessing including lexical-relatedness and senti-ment polarity lexicon induction with a substantialgain in performance.A Appendix A: Interface definition forUndirected GraphsIn order to guarantee the constant main memoryrequirement of Algorithm 2, the graph represen-tation should encode for each node, the completeinformation about it?s neighbors.
We representour undirected graphs in the Google?s ProtocolBuffer format.5Protocol Buffers allow a compact,portable on-disk representation that is easily ex-tensible.
This definition can be compiled into effi-cient Java/C++ classes.The interface definition for undirected graphs islisted below:4http://www.netflixprize.com5Implementation available athttp://code.google.com/p/protobuf/package graph;message NodeNeighbor {required string id = 1;required double edgeWeight = 2;repeated double labelDistribution = 3;}message UndirectedGraphNode {required string id = 1;repeated NodeNeighbor neighbors = 2;repeated double labelDistribution = 3;}message UndirectedGraph {repeated UndirectedGraphNode nodes = 1;}ReferencesArik Azran.
2007.
The rendezvous algorithm: Multi-class semi-supervised learning with markov randomwalks.
In Proceedings of the International Confer-ence on Machine Learning (ICML).Micheal.
Belkin, Partha Niyogi, and Vikas Sindhwani.2005.
On manifold regularization.
In Proceedingsof AISTATS.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
In Proceeding ofCOLING.Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-tun.
2006.
Map-reduce for machine learning onmulticore.
In Proceedings of Neural InformationProcessing Systems.Jeffrey Dean and Sanjay Ghemawat.
2004.
Map-reduce: Simplified data processing on large clusters.In Proceedings of the symposium on Operating sys-tems design and implementation (OSDI).Christaine Fellbaum, editor.
1989.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.D.
Gleich and L. Zhukov.
2005.
Scalable comput-ing for power law graphs: Experience with parallelpagerank.
In Proceedings of SuperComputing.Ananth Grama, George Karypis, Vipin Kumar, and An-shul Gupta.
2003.
Introduction to Parallel Comput-ing (2nd Edition).
Addison-Wesley, January.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedingsof HLT-NAACL.Michael Kearns.
1993.
Efficient noise-tolerant learn-ing from statistical queries.
In Proceedings of theTwenty-Fifth Annual ACM Symposium on Theory ofComputing (STOC).64Soo-Min Kim and Eduard H. Hovy.
2006.
Identifyingand analyzing judgment opinions.
In Proceedings ofHLT-NAACL.Andrew McCallum, Kamal Nigam, and Lyle H. Un-gar.
2000.
Efficient clustering of high-dimensionaldata sets with application to reference matching.In Knowledge Discovery and Data Mining (KDD),pages 169?178.G.
Miller and W. Charles.
1991.
Contextual correlatesof semantic similarity.
In Language and CognitiveProcess.Larry Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1998.
The pagerank citation ranking:Bringing order to the web.
Technical report, Stan-ford University, Stanford, CA.Siddharth Patwardhan, Satanjeev Banerjee, and TedPedersen.
2005.
Senserelate::targetword - A gen-eralized framework for word sense disambiguation.In Proceedings of ACL.John M. Prager, Jennifer Chu-Carroll, and KrzysztofCzuba.
2001.
Use of wordnet hypernyms for an-swering what-is questions.
In Proceedings of theText REtrieval Conference.M.
Szummer and T. Jaakkola.
2001.
Clustering andefficient use of unlabeled examples.
In Proceedingsof Neural Information Processing Systems (NIPS).Jason Wolfe, Aria Haghighi, and Dan Klein.
2008.Fully distributed EM for very large datasets.
In Pro-ceedings of the International Conference in MachineLearning.W.
Xiao and I. Gutman.
2003.
Resistance distance andlaplacian spectrum.
Theoretical Chemistry Associa-tion, 110:284?289.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using Gaussianfields and harmonic functions.
In Proceedings ofthe International Conference on Machine Learning(ICML).65
