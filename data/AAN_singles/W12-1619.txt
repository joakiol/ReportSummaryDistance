Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 134?136,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsIntegrating Location, Visibility, and Question-Answering in a SpokenDialogue System for Pedestrian City ExplorationSrinivasan Janarthanam1, Oliver Lemon1, Xingkun Liu1, Phil Bartie2,William Mackaness2, Tiphaine Dalmas3 and Jana Goetze41Interaction Lab, Heriot-Watt University, Edinburgh2 School of GeoSciences, University of Edinburgh3School of Informatics, University of Edinburgh4KTH Royal Institute of Technology, Stockholm, Swedensc445,o.lemon,x.liu@hw.ac.uk, philbartie@gmail.com,william.mackaness@ed.ac.uk,tiphaine.dalmas@aethys.com, jagoetze@kth.seAbstractWe demonstrate a spoken dialogue-based in-formation system for pedestrians.
The systemis novel in combining geographic informationsystem (GIS) modules such as a visibility en-gine with a question-answering (QA) system,integrated within a dialogue system architec-ture.
Users of the demonstration system canuse a web-based version (simulating pedes-trian movement using StreetView) to engagein a variety of interleaved conversations suchas navigating from A to B, using the QA func-tionality to learn more about points of interest(PoI) nearby, and searching for amenities andtourist attractions.
This system explores a va-riety of research questions involving the inte-gration of multiple information sources withinconversational interaction.1 MotivationAlthough navigation and local information are avail-able to users through smartphone apps, there are stillimportant problems such as how such information isdelivered safely and proactively, and without cogni-tively overloading the user.
(Kray et al, 2003) sug-gested that cognitive load of information presentedin textual and speech-based interfaces is mediumand low respectively when compared to more com-plicated visual interfaces.
Our objective, therefore,is to build a hands-free and eyes-free system that en-gages the pedestrian user by presenting all informa-tion and receiving user requests through speech only.In addition, and in contrast to other mobile ap-plications, this system is conversational ?
meaningthat it accumulates information over time, and plansits utterances to achieve long-term goals.
It inte-grates with a city model and a visibility engine (Bar-tie and Mackaness, 2012) to identify points of inter-ests and visibile landmarks for presentation, a pedes-trian tracker to improve the GPS positioning of theuser and a question-answering (QA) system to en-able users to explore information about the city morefreely than with a graphical interface.Table 1 presents an example dialogue interactionwith the system showing the use of visibility infor-mation and Question-Answering.User: Take me to Princes Street.System: Turn left on to South Bridge andwalk towards the tower in front of you....System: Near you is the famous statue of David Hume.User: Tell me more about David Hume.System: David Hume is a Scottish philosopher....Table 1: An example interaction with the system2 Related workThere are several mobile apps such as Triposo, Trip-wolf, and Guidepal that provide point of interestinformation, and apps such as Google Navigationthat provide navigation instructions to users.
How-ever, they demand the user?s visual attention becausethey predominantly present information on a mobilescreen.
In contrast, ours is a speech only interfacein order to keep the user?s cognitive load low andavoid users from being distracted (perhaps danger-134ously so) from their primary task.Generating navigation instructions in the realworld for pedestrians is an interesting researchproblem in both computational linguistics and geo-informatics (Dale et al, 2003; Richter and Duck-ham, 2008).
CORAL is an NLG system that gener-ates navigation instructions incrementally upon userrequests based on the user?s location (Dale et al,2003).
DeepMap is a system that interacts withthe user to improve positioning using GUI controls(Malaka and Zipf, 2000).
SmartKom is a dialoguesystem that presents navigation information multi-modally (Reithinger et al, 2003).
There are alsoseveral mobile apps developed to help low-visionusers with navigation instructions (see (Stent et al,2010) for example).
In contrast to these earlier sys-tems we present navigational, point-of-interest andamenity information in an integrated way with usersinteracting eyes-free and hands-free through a head-set connected to a smartphone.3 ArchitectureThe architecture of the current system is shown infigure 1.
The server side consists of a dialogue in-terface (parser, interaction manager, and generator),a City Model, a Visibility Engine, a QA server and aPedestrian tracker.
On the user?s side is a web-basedclient that consists of the simulated real-world andthe interaction panel.Figure 1: System Architecture3.1 Dialogue interfaceThe dialogue interface consists of an utteranceparser, an interaction manager and an utterance gen-erator.
The interaction manager is the central com-ponent of this architecture, which provides the usernavigational instructions and interesting PoI infor-mation.
It receives the user?s input in the form of adialogue act and the user?s location in the form oflatitude and longitude information.
Based on theseinputs and the dialogue context, it responds with sys-tem output dialogue act (DA), based on a dialoguepolicy.
The utterance generator is a natural languagegeneration module that translates the system DA intosurface text, using the Open CCG toolkit (White etal., 2007).3.2 Pedestrian trackerGlobal Navigation Satellite Systems (GNSS) (e.g.GPS, GLONASS) provide a useful positioning so-lution with minimal user side setup costs, for loca-tion aware applications.
However urban environ-ments can be challenging with limited sky views,and hence limited line of sight to the satellites, indeep urban corridors.
There is therefore signifi-cant uncertainty about the user?s true location re-ported by GNSS sensors on smartphones (Zandber-gen and Barbeau, 2011).
This module improves onthe reported user position by combining smartphonesensor data (e.g.
accelerometer) with map matchingtechniques, to determine the most likely location ofthe pedestrian (Bartie and Mackaness, 2012).3.3 City ModelThe city model is a spatial database containing in-formation about thousands of entities in the city ofEdinburgh.
These data have been collected from avariety of existing resources such as Ordnance Sur-vey, OpenStreetMap and the Gazetteer for Scotland.It includes the location, use class, name, street ad-dress, and where relevant other properties such asbuild date.
The model also includes a pedestrian net-work (streets, pavements, tracks, steps, open spaces)which can be used to calculate minimal cost routes,such as the shortest path.3.4 Visibility EngineThis module identifies the entities that are in theuser?s vista space (Montello, 1993).
To do this itaccesses a digital surface model, sourced from Li-DAR, which is a 2.5D representation of the city in-cluding buildings, vegetation, and land surface ele-vation.
The visibility engine uses this dataset to offera number of services, such as determining the line135of sight from the observer to nominated points (e.g.which junctions are visible), and determining whichentities within the city model are visible.
These met-rics can be then used by the interaction managerto generate effective navigation instructions.
E.g.
?Walk towards the castle?, ?Can you see the towerin front of you?
?, ?Turn left after the large buildingon your left after the junction?
and so on.3.5 Question-Answering serverThe QA server currently answers a range of defini-tion questions.
E.g., ?Tell me more about the Scot-tish Parliament?, ?Who was David Hume?
?, etc.
QAidentifies the entity focused on in the question us-ing machine-learning techniques (Mikhailian et al,2009), and then proceeds to a textual search on textsfrom the Gazetteer of Scotland and Wikipedia, anddefinitions from WordNet glosses.
Candidates arereranked using a trained confidence score with thetop candidate used as the final answer.
This answeris provided as a flow of sentence chunks that the usercan interrupt.
This information can also be pushedby the system when a salient entity appears in theuser?s viewshed.4 Web-based User interfaceFor the purposes of this (necessarily non-mobile)demonstration, we present a web-based interfacethat simulates users walking in a 3D city environ-ment.
Users will be able to provide speech or textinput (if the demonstration environment is too noisyfor usable speech recognition as is often the case atconference demonstration sessions).The web-based client is a JavaScript/HTML pro-gram running on the user?s web browser.
For adetailed description of this component, please re-fer to (Janarthanam et al, 2012).
It consists of twoparts: the Streetview panel and the Interaction panel.The Streetview panel presents a simulated real worldvisually to the user.
A Google Streetview client(Google Maps API) is created with an initial usercoordinate which then allows the web user to geta panoramic view of the streets around the user?svirtual location.
The user can walk around usingthe arrow keys on his keyboard or the mouse.
Thesystem?s utterances are synthesized using Cereproctext-to-speech engine and presented to the user.AcknowledgmentsThe research has received funding from the Eu-ropean Community?s 7th Framework Programme(FP7/2007-2013) under grant agreement no.
270019(SPACEBOOK project http://www.spacebook-project.eu/).ReferencesP.
Bartie and W. Mackaness.
2012.
D3.4 Pedestrian Po-sition Tracker.
Technical report, The SPACEBOOKProject (FP7/2011-2014 grant agreement no.
270019).R.
Dale, S. Geldof, and J. Prost.
2003.
CORAL : UsingNatural Language Generation for Navigational Assis-tance.
In Proceedings of ACSC2003, South Australia.S.
Janarthanam, O.
Lemon, and X. Liu.
2012.
A web-based evaluation framework for spatial instruction-giving systems.
In Proc.
of ACL 2012, South Korea.C.
Kray, K. Laakso, C. Elting, and V. Coors.
2003.
Pre-senting route instructions on mobile devices.
In Pro-ceedings of IUI 03, Florida.R.
Malaka and A. Zipf.
2000.
Deep Map - challenging ITresearch in the framework of a tourist information sys-tem.
In Information and Communication Technologiesin Tourism 2000, pages 15?27.
Springer.A.
Mikhailian, T. Dalmas, and R. Pinchuk.
2009.
Learn-ing foci for question answering over topic maps.
InProceedings of ACL 2009.D.
Montello.
1993.
Scale and multiple psychologies ofspace.
In A. U. Frank and I. Campari, editors, Spatialinformation theory: A theoretical basis for GIS.N.
Reithinger, J. Alexandersson, T. Becker, A. Blocher,R.
Engel, M. Lckelt, J. Mller, N. Pfleger, P. Poller,M.
Streit, and V. Tschernomas.
2003.
SmartKom -Adaptive and Flexible Multimodal Access to MultipleApplications.
In Proceedings of ICMI 2003, Vancou-ver, B.C.K.
Richter and M. Duckham.
2008.
Simplest instruc-tions: Finding easy-to-describe routes for navigation.In Proceedings of the 5th Intl.
Conference on Geo-graphic Information Science.A.
J. Stent, S. Azenkot, and B. Stern.
2010.
Iwalk: alightweight navigation system for low-vision users.
InProc.
of the ASSETS 2010.M.
White, R. Rajkumar, and S. Martin.
2007.
TowardsBroad Coverage Surface Realization with CCG.
InProc.
of the UCNLG+MT workshop.P.
A. Zandbergen and S. J. Barbeau.
2011.
PositionalAccuracy of Assisted GPS Data from High-SensitivityGPS-enabled Mobile Phones.
Journal of Navigation,64(3):381?399.136
