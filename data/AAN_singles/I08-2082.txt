A Web-based English Proofing System for English as a Second LanguageUsersXing Yi1, Jianfeng Gao2 and William B. Dolan21Center for Intelligent Information Retrieval, Department of Computer ScienceUniversity of Massachusetts, Amherst, MA 01003-4610, USAyixing@cs.umass.edu2Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA{jfgao,billdol}@microsoft.comAbstractWe describe an algorithm that relies onweb frequency counts to identify and correctwriting errors made by non-native writers ofEnglish.
Evaluation of the system on a real-world ESL corpus showed very promisingperformance on the very difficult problem ofcritiquing English determiner use: 62% pre-cision and 41% recall, with a false flag rateof only 2% (compared to a random-guessingbaseline of 5% precision, 7% recall, andmore than 80% false flag rate).
Performanceon collocation errors was less good, sug-gesting that a web-based approach should becombined with local linguistic resources toachieve both effectiveness and efficiency.1 IntroductionProofing technology for native speakers of Englishhas been a focus of work for decades, and sometools like spell checkers and grammar checkers havebecome standard features of document processingsoftware products.
However, designing an Englishproofing system for English as a Second Language(ESL) users presents a major challenge: ESL writ-ing errors vary greatly among users with differentlanguage backgrounds and proficiency levels.
Re-cent work by Brockett et al (2006) utilized phrasalStatistical Machine Translation (SMT) techniques tocorrect ESL writing errors and demonstrated thatthis data-intensive SMT approach is very promising,but they also pointed out SMT approach relies on theavailability of large amount of training data.
The ex-pense and difficulty of collecting large quantities ofSearch Phrase Google.com Live.com Yahoo.comEnglish asSecond Language 306,000 52,407 386,000English as aSecond Language 1,490,000 38,336,308 4,250,000Table 1: Web Hits for Phrasal Usagesraw and edited ESL prose pose an obstacle to thisapproach.In this work we consider the prospect of usingthe Web, with its billions of web pages, as a datasource with the potential to aid ESL writers.
Ourresearch is motivated by the observation that ESLusers already use the Web as a corpus of good En-glish, often using search engines to decide whethera particular spelling, phrase, or syntactic construc-tion is consistent with usage found on the Web.
Forexample, unsure whether the native-sounding phraseincludes the determiner ?a?, a user might search forboth quoted strings ?English as Second Language?and ?English as a Second Language?.
The countsobtained for each of these phrases on three differentsearch engines are shown in Table 1.
Note the cor-rect version, ?English as a Second Language?, has amuch higher number of web hits.In order to determine whether this approach holdspromise, we implemented a web-based system forESL writing error proofing.
This pilot study was in-tended to:1. identify different types of ESL writing errors andhow often they occur in ESL users?
writing samples,so that the challenges and difficulties of ESL errorproofing can be understood better;2. explore the advantages and drawbacks of a web-619based approach, discover useful web data features,and identify which types of ESL errors can be reli-ably proofed using this technique.We first catalog some major categories of ESLwriting errors, then review related work.
Section 3describes our Web-based English Proofing Systemfor ESL users (called ESL-WEPS later).
Section 4presents experimental results.
Section 5 concludes.1.1 ESL Writing ErrorsIn order to get ESL writing samples, we employeda third party to identify large volumes of ESL webpages (mostly from Japanese, Korean and ChineseESL users?
blogs), and cull 1K non-native sen-tences.
A native speaker then rewrote these ESLsentences ?
when possible ?
to produce a native-sounding version.
353 (34.9%) of the original 1012ESL sentences were labeled ?native-like?, another347 (34.3%) were rewritten, and the remaining 312(30.8%) were classified as simply unintelligible.Table 2 shows some examples from the corpus il-lustrating some typical types of ESL writing errorsinvolving: (1) Verb-Noun Collocations (VNC) and(4) Adjective-Noun Collocations (ANC); (2) incor-rect use of the transitive verb ?attend?
; (3) deter-miner (article) usage problems; and (5) more com-plex lexical and style problems.
We analyzed allthe pre- and post-edited ESL samples and found 441ESL errors: about 20% are determiner usage prob-lems(missing/extra/misused); 15% are VNC errors,1% are ANC errors; others represent complex syn-tactic, lexical or style problems.
Multiple errors canco-occur in one sentence.
These show that real-world ESL error proofing is very challenging.Our findings are consistent with previous researchresults on ESL writing errors in two respects:1.
ESL users have significantly more problemswith determiner usage than native speakers be-cause the use and omission of definite andindefinite articles varies across different lan-guages (Schneider and McCoy, 1998)(Lons-dale and Strong-Krause, 2003).2.
Collocation errors are common among ESLusers, and collocational knowledge contributesto the difference between native speakers andESL learners (Shei and Pain, 2000): in CLEC,a real-world Chinese English Learner Corpus(Gui and Yang, 2003), about 30% of ESL writ-ing errors involve different types of collocationerrors.In the remainder of the paper, we focus on proofingdeterminer usage and VNC errors.2 Related WorkResearchers have recently proposed some success-ful learning-based approaches for the determiner se-lection task (Minnen et al, 2000), but most of thiswork has aimed only at helping native English userscorrect typographical errors.
Gamon et al(2008)recently addressed the challenging task of proofingwriting errors for ESL users: they propose combin-ing contextual speller techniques and language mod-eling for proofing several types of ESL errors, anddemonstrate some promising results.
In a departurefrom this work, our system directly uses web datafor the ESL error proofing task.There is a small body of previous work on theuse of online systems aimed at helping ESL learnerscorrect collocation errors.
In Shei and Pain?s sys-tem (2000), for instance, the British National Cor-pus (BNC) is used to extract English collocations,and an ESL learner writing corpus is then used tobuild a collocation Error Library.
In Jian et al?s sys-tem (2004), the BNC is also used as a source of col-locations, with collocation instances and translationcounterparts from the bilingual corpus identified andshown to ESL users.
In contrast to this earlier work,our system uses the web as a corpus, with string fre-quency counts from a search engine index used to in-dicate whether a particular collocation is being usedcorrectly.3 Web-based English Proofing System forESL Users (ESL-WEPS)The architecture of ESL-WEPS, which consists offour main components, is shown in Fig.1.Parse ESL Sentence and Identify Check PointsESL-WEPS first tags and chunks (Sang and Buck-holz, 2000) the input ESL sentence1, and identi-fies the elements of the structures in the sentenceto be checked according to certain heuristics: when1One in-house HMM chunker trained on English Penn Tree-bank was used.620ID Pre-editing version Post-editing version1 Which team can take the champion?
Which team will win the championship?2 I attend to Pyoung Taek University.
I attend Pyoung Taek University.3 I?m a Japanese and studying Info and I?m Japanese and studying InfoComputer Science at Keio University.
Computer Science at Keio University.4 Her works are kinda erotic but they will Her works are kind of erotic, but they willnever arouse any obscene, devil thoughts which might never arouse any obscene, evil thoughts which mightdestroy the soul of the designer.
destroy the soul of the designer.5 I think it is so beautiful to go the way of theology I think it is so beautiful to get into theology,and very attractive too, especially in the area of Christianity.
especially Christianity, which attracts me.Table 2: Some pre- and post-editing ESL writing samples, Bold Italic characters show where the ESL errorsare and how they are corrected/rewritten by native English speaker.ESLSentencesPre-processing(POS Tagger and Chunk Parser)IdentifyCheck PointI amlearning economicsat university.
[NP I/PRP] [VP am/VBPlearning/VBGeconomics/NNS] [PP at/IN] [NPuniversity/NN] ./.
[VP am/VBPlearning/VBGeconomics/NNS]Generate a set of queries, in order tosearch correct English usages from WebQueries:1.
[economics at university]  AND  [learning]2.
[economics] AND  [at university] AND[learning]3.
[economics]  AND  [university]  AND[learning]SearchEngineUse Web statistics to identify plausible errors, Collect Summaries, Mine collocations ordeterminer usages, Generate good suggestions and provide Web example sentencesN-best suggestions:1. studying 1942. doing 123. visiting 11Web Examples:Why Study Economics?
- For LecturersThe design of open days, conferences and other events for schoolstudentsstudying economicsand/or thinking ofstudying economics atuniversity.
These could be held in a university, in a conference  http://whystudyeconomics.ac.uk/lecturers/Figure 1: System Architecturechecking VNC errors, the system searches for astructure of the form (VP)(NP) or (VP)(PP)(NP) inthe chunked sentence; when checking determinerusage, the system searches for (NP).
Table 3 showssome examples.
For efficiency and effectiveness, theuser can specify that only one specific error type becritiqued; otherwise it will check both error types:first determiner usage, then collocations.Generate Queries In order to find appropriate webexamples, ESL-WEPS generates at each check pointa set of queries.
These queries involve three differ-ent granularity levels, according to sentence?s syntaxstructure:1.
Reduced Sentence Level.
In order to usemore contextual information, our system pref-erentially generates a maximal-length queryhereafter called S-Queries, by using the origi-nal sentence.
For the check point chunk, theverb/adj.
to be checked is found and extractedbased on POS tags; other chunks are simplyconcatenated and used to formulate the query.For example, for the first example in Table 3,the S-Query is [?I have?
AND ?this person foryears?
AND ?recognized?].2.
Chunk Level.
The system segments each ESLsentence according to chunk tags and utilizeschunk pairs to generate a query, hereafter re-ferred to as a C-Query, e.g.
the C-Query for thesecond example in Table 3 is [?I?
AND ?went?AND ?to climb?
AND ?a tall mountain?
AND?last week?]3.
Word Level.
The system generates queries byusing keywords from the original string, in theprocessing eliminating stopwords used in typ-ical IR engines, hereafter referred to as a W-Query, e.g.
W-Query for the first example inTable 3 is [?I?
AND ?have?
AND ?person?
AND?years?
AND ?recognized?
]As queries get longer, web search engines tend to re-turn fewer and fewer results.
Therefore, ESL-WEPSfirst segments the original ESL sentence by usingpunctuation characters like commas and semicolons,then generates a query from only the part which con-tains the given check point.
When checking deter-miner usage, three different cases (a or an/the/none)621Parsed ESL sentence Error Type Check Points(NP I/PRP) (VP have/VBP recognized/VBN) (NP this/DT person/NN) (PP for/IN) (NP years/NNS) ./.
VNC recognized this person(NP I/PRP) (VP went/VBD) (VP to/TO climb/VB) (NP a/DT tall/JJ mountain/NN) (NP last/JJ week/NN) ./.
ANC tall mountain, last week(NP I/PRP) (VP went/VBD) (PP to/TO) (NP coffee/NN) (NP shop/NN) (NP yesterday/NN) ./.
Determiner usage coffee, shop, yesterday(NP Someone/NN) (ADVP once/RB) (VP said/VBD) (SBAR that/IN) Determiner usage meet a right person(ADVP when/WRB) (NP you/PRP) (VP meet/VBP) (NP a/DT right/JJ person/NN) at the wrong time(PP at/IN) (NP the/DT wrong/JJ time/NN),/, (NP it/PRP) (VP ?s/VBZ) (NP a/DT pity/NN)./.
?s a pityTable 3: Parsed ESL sentences and Check Points.are considered for each check point.
For instance,given the last example in Table 3, three C-Querieswill be generated: [meet a right person],[meet theright person] and [meet right person].
Note that aterm which has been POS-tagged as NNP (propernoun) will be skipped and not used for generatingqueries in order to obtain more web hits.Retreive Web Statistics, Collect Snippets To col-lect enough web examples, three levels of query setsare submitted to the search engine in the followingorder: S-Query, C-Query, and finally W-Query.
Foreach query, the web hits df returned by search en-gine is recorded, and the snippets from the top 1000hits are collected.
For efficiency reasons, we followDumais (2002)?s approach: the system relies onlyon snippets rather than full-text of pages returnedfor each hit; and does not rely on parsing or POS-tagging for this step.
However, a lexicon is used inorder to determine the possible parts-of-speech of aword as well as its morphological variants.
For ex-ample, to find the correct VNC for a given noun ?tea?in the returned snippets, the verb drank in the sameclause will be matched before ?tea?.Identify Errors and Mine Correct Usages To de-tect determiner usage errors, both the web hit dfq andthe length lq of a given query q are utilized, sincelonger query phrases usually lead to fewer web hits.DFLq, DFLMAX , qmax and Rq are defined as:DFLq = dfq ?
lq, for a given query q;DFLMAX = max(DFLq),qmax = argmaxq(DFLq),q ?
{queries for a given check point};Rq = DFLq/DFLMAX, given query q and check point.If DFLMAX is less than a given threshold t1, thischeck point will be skipped; otherwise the qmax in-dicates the best usage.
We also calculate the relativeratio Rq for three usages (a or an/the/none).
If Rq islarger than a threshold t2 for a query q, the systemwill not report that usage as an error because it issufficiently supported by web data.For collocation check points, ESL-WEPS may in-teract twice with the search engine: first, it issuesquery sets to collect web examples and identify plau-sible collocation errors; then, if errors are detected,new query sets will be issued in the second step inorder to mine correct collocations from new web ex-amples.
For example, for the first sentence in Ta-ble 3, the S-Query will be [?I have?
AND ?this per-son for years?
AND ?recognized?
]; the system an-alyzes returned snippets and identifies ?recognized?as a possible error.
The system then issues a newS-Query [?I have?
AND ?this person for years?
], andfinally mines the new set of snippets to discover that?known?
is the preferred lexical option.
In contrastto proofing determiner usages errors, mfreq:mfreq = frequency of matched collocational verb/adj.in the snippets for a given noun,is utilized to both identify errors and suggest correctVNCs/ANCs.
If mfreq is larger than a thresholdt3, the system will conclude that the collocation isplausible and skip the suggestion step.4 ExperimentsIn order to evaluate the proofing algorithm describedabove, we utilized the MSN search engine API andthe ESL writing sample set described in Section1.1 to evaluate the algorithm?s performance on twotasks: determiner usage and VNC proofing.
Froma practical standpoint, we consider precision on theproofing task to be considerably more importantthan recall: false flags are annoying and highly vis-ible to the user, while recall failures are much lessproblematic.Given the complicated nature of the ESL errorproofing task, about 60% of ESL sentences in our setthat contained determiner errors also contained othertypes of ESL errors.
As a result, we were forcedto slightly revise the typical precision/recall mea-surement in order to evaluate performance.
First,622Good Proofing ExamplesError sentence 1 In my opinion, therefore, when we describe terrorism, its crucially important thatwe consider the degree of the influence (i.e., power) on the other countries.proofing suggestion consider the degree of influenceError sentence 2 Someone once said that when you meet a right person at the wrong time, it?s a pity.proofing suggestion meet the right person at the wrong timePlausible Useful Proofing ExamplesError sentence 3 The most powerful place in Beijing, and in the whole China.native speaker suggestion in the whole of Chinasystem suggestion in whole ChinaError sentence 4 Me, I wanna keep in touch with old friends and wanna talk with anyone who has different thought, etc.native speaker suggestion has different ideassystem suggestion has a different thoughtTable 4: ESL Determiner Usage Proofing by Native Speaker and ESL-WEPS.Good Proofing ExamplesError sentence 1 I had great time there and got many friends.proofing suggestion made many friendsError sentence 2 Which team can take the champion?proofing suggestion win the championPlausible Useful Proofing ExamplesError sentence 3 It may sounds fun if I say my firm resolution of this year is to get a girl friend.native speaker suggestion sound funnysystem suggestion make * fun or get * funTable 5: ESL VNC Proofing by Native Speaker and ESL-WEPS.we considered three cases: (1) the system correctlyidentifies an error and proposes a suggestion that ex-actly matches the native speaker?s rewrite; (2) thesystem correctly identifies an error but makes a sug-gestion that differs from the native speaker?s edit;and (3) the system incorrectly identifies an error.
Inthe first case, we consider the proofing good, in thesecond, plausibly useful, and in the third case it issimply wrong.
Correspondingly, we introduce thecategories Good Precision (GP), Plausibly UsefulPrecision (PUP) and Error Suggestion Rate (ESR),which were calculated by:GP = # of Good Proofings# of System?s Proofings ;PUP = # of Plausibly Useful Proofings# of System?s Proofings ;ESR = # of Wrong Proofings# of System?s Proofings ;GP + PUP + ESR = 1Furthermore, assuming that there are overall NA er-rors for a given type A of ESL error , the typicalrecall and false alarm were calculated by:recall = # of Good ProofingsNA ;false alarm = # of Wrong Proofings# of Check points for ESL error ATable 4 and Table 5 show examples of Good orPlausibly Useful proofing for determiner usage andcollocation errors, respectively.
It can be seen thesystem makes plausibly useful proofing suggestionsbecause some errors types are out of current sys-tem?s checking range.The system achieved very promising performancedespite the fact that many of the test sentences con-tained other, complex ESL errors: using appro-priate system parameters, ESL-WEPS showed re-call 40.7% on determiner usage errors, with 62.5%of these proofing suggestions exactly matching therewrites provided by native speakers.
Crucially, thefalse flag rate was only 2%.
Note that a random-guessing baseline was about 5% precision, 7% re-call, but more than 80% false flag rate.For collocation errors, we focused on the mostcommon VNC proofing task.
mfreq and thresholdt3 described in Section 3 are used to control falsealarm, GP and recall.
A smaller t3 can reduce recall,but can increase GP.
Table 7 shows how performancechanges with different settings for t3, and Fig.
2(b)plots the GP/recall curve.
Results are not very good:as recall increases, GP decreases too quickly, so thatat 30.7% recall, precision is only 37.3%.
We at-tribute this to the fact that most search engines onlyreturn the top 1000 web snippets for each query andour current system relies on this limited number ofsnippets to generate and rank candidates.623Recall 16.3% 30.2% 40.7% 44.2% 47.7% 50.0%GP 73.7% 70.3% 62.5% 56.7% 53.3% 52.4%PUP 15.8% 16.2% 25.0% 29.9% 29.9% 29.3%false alarm 0.4% 1.4% 2.0% 2.6% 3.7% 4.3%Table 6: Proofing performance of determiner usagechanges when setting different system parameters.Recall 11.3% 12.9% 17.8% 25.8% 29.0% 30.7%GP 77.8% 53.3% 52.4% 43.2% 40.9% 37.3%PUP 11.11% 33.33% 33.33% 45.10% 48.65% 50.00%false alarm 0.28% 0.57% 0.85% 0.85% 1.13% 2.55%Table 7: VNC Proofing performance changes whensetting different system parameters.5 ConclusionThis paper introduced an approach to the challeng-ing real-world ESL writing error proofing task thatuses the index of a web search engine for cor-pus statistics.
We validated ESL-WEPS on a web-crawled ESL writing corpus and compared the sys-tem?s proofing suggestions to those produced by na-tive English speakers.
Promising performance wasachieved for proofing determiner errors, but lessgood results for VNC proofing, possibly because thecurrent system uses web snippets to rank and gener-ate collocation candidates.
We are currently investi-gating a modified strategy that exploits high qualitylocal collocation/synonym lists to limit the numberof proposed Verb/Adj.
candidates.We are also collecting more ESL data to validateour system and are extending our system to moreESL error types.
Recent experiments on new datashowed that ESL-WEPS can also effectively proofincorrect choices of prepositions.
Later research willcompare the web-based approach to conventionalcorpus-based approaches like Gamon et al (2008),and explore their combination to address complexESL errors.Good Precision vs. Recall20.0%30.0%40.0%50.0%60.0%70.0%80.0%10.0%20.0%30.0%40.0%50.0%60.0%70.0%80.0%Good Precision vs. Recall20.0%30.0%40.0%50.0%60.0%70.0%80.0%90.0%5.0%10.0%15.0%20.0%25.0%30.0%35.0%(a)Determiner Usage Error Proofing(b)VNC Error ProofingFigure 2: GP/recall curves.
X and Y axis denotesGP and Recall respectively.Acknowledgement The authors have benefitedextensively from discussions with Michael Gamonand Chris Brockett.
We also thank the Butler HillGroup for collecting the ESL examples.ReferencesC.
Brockett, W. B. Dolan, and M. Gamon.
2006.
Cor-recting ESL errors using phrasal smt techniques.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the ACL, pages 249?256, Sydney, Australia.S.
Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002.Web question answering: is more always better?
InProceedings of the 25th Annual International ACM SI-GIR, pages 291?298, Tampere, Finland.M.
Gamon, J.F.
Gao, C. Brockett, A. Klementiev, W.B.Dolan, and L. Vanderwende.
2008.
Using contextualspeller techniques and language modeling for ESL er-ror correction.
In Proceedings of IJCNLP 2008, Hy-derabad, India, January.S.
Gui and H. Yang, 2003.
Zhongguo Xuexizhe YingyuYuliaoku.
(Chinese Learner English Corpus).
Shang-hai Waiyu Jiaoyu Chubanshe, Shanghai.
(In Chinese).Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang.2004.
TANGO: bilingual collocational concordancer.In Proceedings of the ACL 2004, pages 19?23,Barcelona, Spain.D.
Lonsdale and D. Strong-Krause.
2003.
Automatedrating of ESL essays.
In Proceedings of the NAACL-HLT 03 workshop on Building educational applica-tions using natural language processing, pages 61?67,Edmonton, Canada.G.
Minnen, F. Bond, and A. Copestake.
2000.
Memory-based learning for article generation.
In Proceedingsof the Fourth Conference on Computational NaturalLanguage Learning and of the Second Learning Lan-guage in Logic Workshop, pages 43?48.E.
Tjong Kim Sang and S. Buckholz.
2000.
Introductionto the conll-2000 shared task: Chunking.
In Proceed-ings of CoNLL-2000 and LLL-2000, pages 127?132,Lisbon, Portugal.D.
Schneider and K. F. McCoy.
1998.
Recognizing syn-tactic errors in the writing of second language learn-ers.
In Proceedings of the 17th international confer-ence on Computational linguistics, pages 1198?1204,Montreal, Quebec, Canada.C.-C. Shei and H. Pain.
2000.
An esl writer?s collo-cational aid.
Computer Assisted Language Learning,13(2):167?182.624
