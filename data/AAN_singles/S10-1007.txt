Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 9: The Interpretation of Noun CompoundsUsing Paraphrasing Verbs and PrepositionsCristina ButnariuUniversity College Dublinioana.butnariu@ucd.ieSu Nam KimUniversity of Melbournenkim@csse.unimelb.edu.auPreslav NakovNational University of Singaporenakov@comp.nus.edu.sgDiarmuid?O S?eaghdhaUniversity of Cambridgedo242@cam.ac.ukStan SzpakowiczUniversity of OttawaPolish Academy of Sciencesszpak@site.uottawa.caTony VealeUniversity College Dublintony.veale@ucd.ieAbstractPrevious research has shown that the mean-ing of many noun-noun compounds N1N2can be approximated reasonably well byparaphrasing clauses of the form ?N2that.
.
.
N1?, where ?.
.
.
?
stands for a verbwith or without a preposition.
For exam-ple, malaria mosquito is a ?mosquito thatcarries malaria?.
Evaluating the quality ofsuch paraphrases is the theme of Task 9 atSemEval-2010.
This paper describes somebackground, the task definition, the processof data collection and the task results.
Wealso venture a few general conclusions be-fore the participating teams present theirsystems at the SemEval-2010 workshop.There were 5 teams who submitted 7 sys-tems.1 IntroductionNoun compounds (NCs) are sequences of two ormore nouns that act as a single noun,1 e.g., stemcell, stem cell research, stem cell research organi-zation, etc.
Lapata and Lascarides (2003) observethat NCs pose syntactic and semantic challenges forthree basic reasons: (1) the compounding processis extremely productive in English; (2) the seman-tic relation between the head and the modifier isimplicit; (3) the interpretation can be influenced bycontextual and pragmatic factors.
Corpus studieshave shown that while NCs are very common inEnglish, their frequency distribution follows a Zip-fian or power-law distribution and the majority ofNCs encountered will be rare types (Tanaka andBaldwin, 2003; Lapata and Lascarides, 2003; Bald-win and Tanaka, 2004; ?O S?eaghdha, 2008).
As aconsequence, Natural Language Processing (NLP)1We follow the definition in (Downing, 1977).applications cannot afford either to ignore NCs orto assume that they can be handled by relying on adictionary or other static resource.Trouble with lexical resources for NCs notwith-standing, NC semantics plays a central role in com-plex knowledge discovery and applications, includ-ing but not limited to Question Answering (QA),Machine Translation (MT), and Information Re-trieval (IR).
For example, knowing the (implicit)semantic relation between the NC components canhelp rank and refine queries in QA and IR, or selectpromising translation pairs in MT (Nakov, 2008a).Thus, robust semantic interpretation of NCs shouldbe of much help in broad-coverage semantic pro-cessing.Proposed approaches to modelling NC seman-tics have used semantic similarity (Nastase and Sz-pakowicz, 2003; Moldovan et al, 2004; Kim andBaldwin, 2005; Nastase and Szpakowicz, 2006;Girju, 2007; ?O S?eaghdha and Copestake, 2007)and paraphrasing (Vanderwende, 1994; Kim andBaldwin, 2006; Butnariu and Veale, 2008; Nakovand Hearst, 2008).
The former body of work seeksto measure the similarity between known and un-seen NCs by considering various features, usuallycontext-related.
In contrast, the latter group usesverb semantics to interpret NCs directly, e.g., oliveoil as ?oil that is extracted from olive(s)?, drugdeath as ?death that is caused by drug(s)?, flu shotas a ?shot that prevents flu?.The growing popularity ?
and expected directutility ?
of paraphrase-based NC semantics hasencouraged us to propose an evaluation exercisefor the 2010 edition of SemEval.
This paper givesa bird?s-eye view of the task.
Section 2 presentsits objective, data, data collection, and evaluationmethod.
Section 3 lists the participating teams.Section 4 shows the results and our analysis.
InSection 5, we sum up our experience so far.392 Task Description2.1 The ObjectiveFor the purpose of the task, we focused on two-word NCs which are modifier-head pairs of nouns,such as apple pie or malaria mosquito.
There areseveral ways to ?attack?
the paraphrase-based se-mantics of such NCs.We have proposed a rather simple problem: as-sume that many paraphrases can be found ?
perhapsvia clever Web search ?
but their relevance is up inthe air.
Given sufficient training data, we seek to es-timate the quality of candidate paraphrases in a testset.
Each NC in the training set comes with a longlist of verbs in the infinitive (often with a prepo-sition) which may paraphrase the NC adequately.Examples of apt paraphrasing verbs: olive oil ?be extracted from, drug death ?
be caused by, flushot ?
prevent.
These lists have been constructedfrom human-proposed paraphrases.
For the train-ing data, we also provide the participants with aquality score for each paraphrase, which is a simplecount of the number of human subjects who pro-posed that paraphrase.
At test time, given a nouncompound and a list of paraphrasing verbs, a partic-ipating system needs to produce aptness scores thatcorrelate well (in terms of relative ranking) withthe held out human judgments.
There may be adiverse range of paraphrases for a given compound,some of them in fact might be inappropriate, butit can be expected that the distribution over para-phrases estimated from a large number of subjectswill indeed be representative of the compound?smeaning.2.2 The DatasetsFollowing Nakov (2008b), we took advantage ofthe Amazon Mechanical Turk2 (MTurk) to acquireparaphrasing verbs from human annotators.
Theservice offers inexpensive access to subjects fortasks which require human intelligence.
Its APIallows a computer program to run tasks easily andcollate the subjects?
responses.
MTurk is becominga popular means of eliciting and collecting linguis-tic intuitions for NLP research; see Snow et al(2008) for an overview and a further discussion.Even though we recruited human subjects,whom we required to take a qualification test,32www.mturk.com3We soon realized that we also had to offer a version ofour assignments without a qualification test (at a lower payrate) since very few people were willing to take a test.
Overall,data collection was time-consuming since manyannotators did not follow the instructions.
We hadto monitor their progress and to send them timelymessages, pointing out mistakes.
Although theMTurk service allows task owners to accept or re-ject individual submissions, rejection was the lastresort since it has the triply unpleasant effect of(1) denying the worker her fee, (2) negatively af-fecting her rating, and (3) lowering our rating asa requester.
We thus chose to try and educate ourworkers ?on the fly?.
Even so, we ended up withmany examples which we had to correct manu-ally by labor-intensive post-processing.
The flawswere not different from those already described byNakov (2008b).
Post-editing was also necessary tolemmatize the paraphrasing verbs systematically.Trial Data.
At the end of August 2009, wereleased as trial data the previously collected para-phrase sets (Nakov, 2008b) for the Levi-250 dataset(after further review and cleaning).
This datasetconsisted of 250 noun-noun compounds form (Levi,1978), each paraphrased by 25-30 MTurk workers(without a qualification test).Training Data.
The training dataset was an ex-tension of the trial dataset.
It consisted of the same250 noun-noun compounds, but the number of an-notators per compound increased significantly.
Weaimed to recruit at least 30 additional MTurk work-ers per compound; for some compounds we man-aged to get many more.
For example, when weadded the paraphrasing verbs from the trial datasetto the newly collected verbs, we had 131 differentworkers for neighborhood bars, compared to just50 for tear gas.
On the average, we had 72.7 work-ers per compound.
Each worker was instructedto try to produce at least three paraphrasing verbs,so we ended up with 191.8 paraphrasing verbs percompound, 84.6 of them being unique.
See Table 1for more details.Test Data.
The test dataset consisted of 388noun compounds collected from two data sources:(1) the Nastase and Szpakowicz (2003) dataset;and (2) the Lauer (1995) dataset.
The formercontains 328 noun-noun compounds (there arealso a number of adjective-noun and adverb-nounpairs), while the latter contains 266 noun-nouncompounds.
Since these datasets overlap betweenthemselves and with the training dataset, we hadto exclude some examples.
In the end, we had 388we found little difference in the quality of work of subjectsrecruited with and without the test.40Training: 250 NCs Testing: 388 NCs All: 638 NCsTotal Min/Max/Avg Total Min/Max/Avg Total Min/Max/AvgMTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1Table 1: Statistics about the the training/test datasets.
Shown are the total number of verbs proposed aswell as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.unique noun-noun compounds for testing, distinctfrom those used for training.
We aimed for 100human workers per testing NC, but we could onlyget 68.3, with a minimum of 57 and a maximum of96; there were 185.0 paraphrasing verbs per com-pound, 70.9 of them being unique, which is closeto what we had for the training data.Data format.
We distribute the training data asa raw text file.
Each line has the following tab-separated format:NC paraphrase frequencywhere NC is a noun-noun compound (e.g., ap-ple cake, flu virus), paraphrase is a human-proposed paraphrasing verb optionally followedby a preposition, and frequency is the numberof annotators who proposed that paraphrase.
Hereis an illustrative extract from the training dataset:flu virus cause 38flu virus spread 13flu virus create 6flu virus give 5flu virus produce 5...flu virus be made up of 1flu virus be observed in 1flu virus exacerbate 1The test file has a similar format, except that thefrequency is not included and the paraphrases foreach noun compound appear in random order:...chest pain originatechest pain start inchest pain descend inchest pain be in...License.
All datasets are released under the Cre-ative Commons Attribution 3.0 Unported license.44creativecommons.org/licenses/by/3.02.3 EvaluationAll evaluation was performed by computing an ap-propriate measure of similarity/correlation betweensystem predictions and the compiled judgements ofthe human annotators.
We did it on a compound-by-compound basis and averaged over all compoundsin the test dataset.
Section 4 shows results for threemeasures: Spearman rank correlation, Pearson cor-relation, and cosine similarity.Spearman Rank Correlation (?)
was adoptedas the official evaluation measure for the competi-tion.
As a rank correlation statistic, it does not usethe numerical values of the predictions or humanjudgements, only their relative ordering encodedas integer ranks.
For a sample of n items rankedby two methods x and y, the rank correlation ?
iscalculated as follows:?
=n?xiyi?
(?xi)(?yi)?n?x2i?
(?xi)2?n?y2i?
(?yi)2(1)where xi, yiare the ranks given by x and y to theith item, respectively.
The value of ?
ranges be-tween -1.0 (total negative correlation) and 1.0 (totalpositive correlation).Pearson Correlation (r) is a standard measureof correlation strength between real-valued vari-ables.
The formula is the same as (1), but withxi, yitaking real values rather than rank values;just like ?, r?s values fall between -1.0 and 1.0.Cosine similarity is frequently used in NLP tocompare numerical vectors:cos =?nixiyi?
?nix2i?niy2i(2)For non-negative data, the cosine similarity takesvalues between 0.0 and 1.0.
Pearson?s r can beviewed as a version of the cosine similarity whichperforms centering on x and y.Baseline: To help interpret these evaluation mea-sures, we implemented a simple baseline.
A dis-tribution over the paraphrases was estimated by41System Institution Team DescriptionNC-INTERP International Institute ofInformation Technology,HyderabadPrashantMathurUnsupervised model using verb-argument frequen-cies from parsed Web snippets and WordNetsmoothingUCAM University of Cambridge Clemens Hepp-nerUnsupervised model using verb-argument frequen-cies from the British National CorpusUCD-GOGGLE-I University CollegeDublinGuofu Li Unsupervised probabilistic model using pattern fre-quencies estimated from the Google N-Gram corpusUCD-GOGGLE-II Paraphrase ranking model learned from trainingdataUCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-GOGGLE-IIUCD-PN University CollegeDublinPaul Nulty Scoring according to the probability of a paraphraseappearing in the same set as other paraphrases pro-videdUVT-MEPHISTO Tilburg University SanderWubbenSupervised memory-based ranker using featuresfrom Google N-Gram Corpus and WordNetTable 2: Teams participating in SemEval-2010 Task 9summing the frequencies for all compounds in thetraining dataset, and the paraphrases for the test ex-amples were scored according to this distribution.Note that this baseline entirely ignores the identityof the nouns in the compound.3 ParticipantsThe task attracted five teams, one of which (UCD-GOGGLE) submitted three runs.
The participantsare listed in Table 2 along with brief system de-scriptions; for more details please see the teams?own description papers.4 Results and DiscussionThe task results appear in Table 3.
In an evaluationby Spearman?s ?
(the official ranking measure),the winning system was UVT-MEPHISTO, whichscored 0.450.
UVT also achieved the top Pear-son?s r score.
UCD-PN is the top-scoring systemaccording to the cosine measure.
One participantsubmitted part of his results after the official dead-line, which is marked by an asterisk.The participants used a variety of informationsources and estimation methods.
UVT-MEPHISTOis a supervised system that uses frequency informa-tion from the Google N-Gram Corpus and featuresfrom WordNet (Fellbaum, 1998) to rank candidateparaphrases.
On the other hand, UCD-PN usesno external resources and no supervised training,yet came within 0.009 of UVT-MEPHISTO in theofficial evaluation.
The basic idea of UCD-PN ?that one can predict the plausibility of a paraphrasesimply by knowing which other paraphrases havebeen given for that compound regardless of theirfrequency ?
is clearly a powerful one.
Unlike theother systems, UCD-PN used information about thetest examples (not their ranks, of course) for modelestimation; this has similarities to ?transductive?methods for semi-supervised learning.
However,post-hoc analysis shows that UCD-PN would havepreserved its rank if it had estimated its model onthe training data only.
On the other hand, if the taskhad been designed differently ?
by asking systemsto propose paraphrases from the set of all possi-ble verb/preposition combinations ?
then we wouldnot expect UCD-PN?s approach to work as well asmodels that use corpus information.The other systems are comparable to UVT-MEPHISTO in that they use corpus frequenciesto evaluate paraphrases and apply some kind ofsemantic smoothing to handle sparsity.
How-ever, UCD-GOGGLE-I, UCAM and NC-INTERPare unsupervised systems.
UCAM uses the 100-million word BNC corpus, while the other systemsuse Web-scale resources; this has presumably ex-acerbated sparsity issues and contributed to a rela-tively poor performance.The hybrid approach exemplified by UCD-GOGGLE-III combines the predictions of a sys-tem that models paraphrase correlations and onethat learns from corpus frequencies and thus at-tains better performance.
Given that the two top-scoring systems can also be characterized as usingthese two distinct information sources, it is natu-ral to consider combining these systems.
Simplynormalizing (to unit sum) and averaging the twosets of prediction values for each compound does42Rank System Supervised?
Hybrid?
Spearman ?
Pearson r Cosine1 UVT-MEPHISTO yes no 0.450 0.411 0.6352 UCD-PN no no 0.441 0.361 0.6693 UCD-GOGGLE-III yes yes 0.432 0.395 0.6524 UCD-GOGGLE-II yes no 0.418 0.375 0.6605 UCD-GOGGLE-I no no 0.380 0.252 0.6296 UCAM no no 0.267 0.219 0.3747 NC-INTERP* no no 0.186 0.070 0.466Baseline yes no 0.425 0.344 0.524Combining UVT and UCD-PN yes yes 0.472 0.431 0.685Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).indeed give better scores: Spearman ?
= 0.472,r = 0.431, Cosine = 0.685.The baseline from Section 2.3 turns out to bevery strong.
Evaluating with Spearman?s ?, onlythree systems outperform it.
It is less competitiveon the other evaluation measures though.
Thissuggests that global paraphrase frequencies maybe useful for telling sensible paraphrases from badones, but will not do for quantifying the plausibilityof a paraphrase for a given noun compound.5 ConclusionGiven that it is a newly-proposed task, this initialexperiment in paraphrasing noun compounds hasbeen a moderate success.
The participation ratehas been sufficient for the purposes of comparingand contrasting different approaches to the roleof paraphrases in the interpretation of noun-nouncompounds.
We have seen a variety of approachesapplied to the same dataset, and we have been ableto compare the performance of pure approaches tohybrid approaches, and of supervised approachesto unsupervised approaches.
The results reportedhere are also encouraging, though clearly there isconsiderable room for improvement.This task has established a high baseline for sys-tems to beat.
We can take heart from the fact thatthe best performance is apparently obtained from acombination of corpus-derived usage features anddictionary-derived linguistic knowledge.
Althoughclever but simple approaches can do quite well onsuch a task, it is encouraging to note that the bestresults await those who employ the most robustand the most informed treatments of NCs and theirparaphrases.
Despite a good start, this is a chal-lenge that remains resolutely open.
We expect thatthe dataset created for the task will be a valuableresource for future research.AcknowledgementsThis work is partially supported by grants fromAmazon and from the Bulgarian National ScienceFoundation (D002-111/15.12.2008 ?
SmartBook).ReferencesTimothy Baldwin and Takaaki Tanaka.
2004.
Trans-lation by Machine of Compound Nominals: Gettingit Right.
In Proceedings of the ACL-04 Workshopon Multiword Expressions: Integrating Processing,pages 24?31, Barcelona, Spain.Cristina Butnariu and Tony Veale.
2008.
A Concept-Centered Approach to Noun-Compound Interpreta-tion.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (COLING-08), pages 81?88, Manchester, UK.Pamela Downing.
1977.
On the creation and use of En-glish compound nouns.
Language, 53(4):810?842.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press.Roxana Girju.
2007.
Improving the Interpretationof Noun Phrases with Cross-linguistic Information.In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics (ACL-07),pages 568?575, Prague, Czech Republic.Su Nam Kim and Timothy Baldwin.
2005.
Automaticinterpretation of noun compounds using WordNetsimilarity.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing(IJCNLP-05), pages 945?956, Jeju Island, South Ko-rea.Su Nam Kim and Timothy Baldwin.
2006.
Inter-preting Semantic Relations in Noun Compounds viaVerb Semantics.
In Proceedings of the COLING-ACL-06 Main Conference Poster Sessions, pages491?498, Sydney, Australia.Mirella Lapata and Alex Lascarides.
2003.
Detect-ing novel compounds: The role of distributional evi-dence.
In Proceedings of the 10th Conference of the43European Chapter of the Association for Computa-tional Linguistics (EACL-03), pages 235?242, Bu-dapest, Hungary.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Noun Compounds.
Ph.D.thesis, Macquarie University.Judith Levi.
1978.
The Syntax and Semantics of Com-plex Nominals.
Academic Press, New York, NY.DanMoldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the Se-mantic Classification of Noun Phrases.
In Proceed-ings of the HLT-NAACL-04 Workshop on Computa-tional Lexical Semantics, pages 60?67, Boston, MA.Preslav Nakov and Marti A. Hearst.
2008.
SolvingRelational Similarity Problems Using the Web as aCorpus.
In Proceedings of the 46th Annual Meet-ing of the Association of Computational Linguistics(ACL-08), pages 452?460, Columbus, OH.Preslav Nakov.
2008a.
Improved Statistical MachineTranslation Using Monolingual Paraphrases.
In Pro-ceedings of the 18th European Conference on Artifi-cial Intelligence (ECAI-08), pages 338?342, Patras,Greece.Preslav Nakov.
2008b.
Noun Compound Interpreta-tion Using Paraphrasing Verbs: Feasibility Study.In Proceedings of the 13th International Confer-ence on Artificial Intelligence: Methodology, Sys-tems and Applications (AIMSA-08), pages 103?117,Varna, Bulgaria.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Proceedingsof the 5th International Workshop on ComputationalSemantics (IWCS-03), pages 285?301, Tilburg, TheNetherlands.Vivi Nastase and Stan Szpakowicz.
2006.
Matchingsyntactic-semantic graphs for semantic relation as-signment.
In Proceedings of the 1st Workshop onGraph Based Methods for Natural Language Pro-cessing (TextGraphs-06), pages 81?88, New York,NY.Diarmuid?O S?eaghdha and Ann Copestake.
2007.
Co-occurrence Contexts for Noun Compound Interpre-tation.
In Proceedings of the ACL-07 Workshopon A Broader Perspective on Multiword Expressions(MWE-07), pages 57?64, Prague, Czech Republic.Diarmuid?O S?eaghdha.
2008.
Learning CompoundNoun Semantics.
Ph.D. thesis, University of Cam-bridge.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and Fast ?
But is it Good?Evaluating Non-Expert Annotations for Natural Lan-guage Tasks.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-08), pages 254?263, Honolulu, HI.Takaaki Tanaka and Timothy Baldwin.
2003.
Noun-noun compound machine translation: A feasibilitystudy on shallow processing.
In Proceedings of theACL-03 Workshop on Multiword Expressions (MWE-03), pages 17?24, Sapporo, Japan.Lucy Vanderwende.
1994.
Algorithm for AutomaticInterpretation of Noun Sequences.
In Proceedingsof the 15th International Conference on Compu-tational Linguistics (COLING-94), pages 782?788,Kyoto, Japan.44
