Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 340?350,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Dataset and Evaluation Metrics for Abstractive Compression of Sentencesand Short ParagraphsKristina ToutanovaMicrosoft ResearchRedmond, WA, USAChris BrockettMicrosoft ResearchRedmond, WA, USAKe M. Tran?University of AmsterdamAmsterdam, The NetherlandsSaleema AmershiMicrosoft ResearchRedmond, WA, USAAbstractWe introduce a manually-created, multi-reference dataset for abstractive sentence andshort paragraph compression.
First, we exam-ine the impact of single- and multi-sentencelevel editing operations on human compres-sion quality as found in this corpus.
We ob-serve that substitution and rephrasing opera-tions are more meaning preserving than otheroperations, and that compressing in contextimproves quality.
Second, we systematicallyexplore the correlations between automaticevaluation metrics and human judgments ofmeaning preservation and grammaticality inthe compression task, and analyze the impactof the linguistic units used and precision ver-sus recall measures on the quality of the met-rics.
Multi-reference evaluation metrics areshown to offer significant advantage over sin-gle reference-based metrics.1 IntroductionAutomated sentence compression condenses a sen-tence or paragraph to its most important content inorder to enhance writing quality, meet documentlength constraints, and build more accurate docu-ment summarization systems (Berg-Kirkpatrick etal., 2011; Vanderwende et al, 2007).
Though worddeletion is extensively used (e.g., (Clarke and La-pata, 2008)), state-of-the-art compression models(Cohn and Lapata, 2008; Rush et al, 2015) bene-fit crucially from data that can represent complexabstractive compression operations, including sub-stitution of words and phrases and reordering.
?This research was conducted during the author?s intern-ship at Microsoft Research.This paper has two parts.
In the first half, we in-troduce a manually-created multi-reference datasetfor abstractive compression of sentences and shortparagraphs, with the following features:?
It contains approximately 6,000 source textswith multiple compressions (about 26,000 pairsof source and compressed texts), representingbusiness letters, newswire, journals, and tech-nical documents sampled from the Open Amer-ican National Corpus (OANC1).?
Each source text is accompanied by up to fivecrowd-sourced rewrites constrained to a presetcompression ratio and annotated with qualityjudgments.
Multiple rewrites permit study ofthe impact of operations on human compres-sion quality and facilitate automatic evaluation.?
This dataset is the first to provide compressionsat the multi-sentence (two-sentence paragraph)level, which may present a stepping stone towhole document summarization.
Many ofthese two-sentence paragraphs are compressedboth as paragraphs and separately sentence-by-sentence, offering data that may yield insightsinto the impact of multi-sentence operations onhuman compression quality.?
A detailed edit history is provided that mayallow fine-grained alignment of original andcompressed texts and measurement of the cog-nitive load of different rewrite operations.Our analysis of this dataset reveals that abstrac-tion has a significant positive impact on meaningpreservation, and that application of trans-sentential1http://www.anc.org/data/oanc340context has a significant positive impact on bothmeaning preservation and grammaticality.In the second part, we provide a systematic em-pirical study of eighty automatic evaluation met-rics for text compression using this dataset, corre-lating them with human judgments of meaning andgrammar.
Our study shows strong correlation of thebest metrics with human judgments of meaning, butweaker correlations with judgments of grammar.
Wedemonstrate significant gains from multiple refer-ences.
We also provide analyses of the impact ofthe linguistics units used (surface n-grams of differ-ent sizes versus parse-based triples), and the use ofprecision versus recall-based measures.2 Related WorkPrior studies of human compression: Clarke(2008) studied the properties of manually-collecteddeletion-based compressions in the news genre,comparing them with automatically-mined datafrom the Ziff-Davis corpus in terms of compressionrate, length of deleted spans, and deletion probabil-ity by syntactic constituent type.
Jing and McKeown(1999) identified abstractive operations (other thanword deletion) employed by professional writers, in-cluding paraphrasing and re-ordering of phrases, andmerging and reordering sentences, but did not quan-tify their impact on compression quality.Deletion-based compression corpora: Currentlyavailable automatically-mined deletion corpora aresingle-reference and have varying (uncontrolled)compression rates.
Knight and Marcu (2002) auto-matically mined a small parallel corpus (1,035 train-ing and 32 test sentences) by aligning abstracts tosentences in articles.
Filippova and Altun (2013)extracted deletion-based compressions by aligningnews headlines to first sentences, yielding a corpusof 250,000 parallel sentences.
The same approachwas used by Filippova et al (2015) to create a set of2M sentence pairs.
Only a subset of 10,000 parallelsentences from the latter has been publicly released.Clarke and Lapata (2006) and Clarke and Lapata(2008) provide two manually-created two-referencecorpora for deletion-based compression:2 their sizesare 1,370 and 1,433 sentences, respectively.2http://jamesclarke.net/research/resourcesAbstractive compression corpora: Rush et al(2015) have mined 4 million compression pairs fromnews articles and released their code to extract datafrom the Annotated Gigaword (Napoles et al, 2012).A news-domain parallel sentence corpus containing1,496 parallel examples has been culled from multi-reference Chinese-English translations by Ganitke-vitch et al (2011).
The only publicly-availablemanually-created abstractive compression corpus isthat described by Cohn and Lapata (2008), whichcomprises 575 single-reference sentence pairs.Automatic metrics: Early automatic metrics forevaluation of compressions include success rate(Jing, 2000), defined as accuracy of individual wordor constituent deletion decisions; Simple String Ac-curacy (string edit distance), introduced by Banga-lore et al (2000) for natural language generationtasks; and Word Accuracy (Chiori and Furui, 2004),which generalizes Bangalore et al (2000) to multi-ple references.
Riezler et al (2003) introduced theuse of F-measure over grammatical relations.
Wordunigram and word-bigram F-measure have also beenused (Unno et al, 2006; Filippova et al, 2015).
Vari-ants of ROUGE (Lin, 2004), used for summarizationevaluation, have also been applied to sentence com-pressions (Rush et al, 2015).Riezler et al (2003) show that F-measure overgrammatical relations agrees with human ratingson the relative ranking of three systems at thecorpus level.
Clarke and Lapata (2006) evaluatetwo deletion-based automatic compression systemsagainst a deletion-based gold-standard on sets of 20sentences.
Parse-based F-1 was shown to have highsentence-level Pearson?s ?
correlation with humanjudgments of overall quality, and to have higher ?than Simple String Accuracy.Napoles et al (2011) have pointed to the need ofmultiple references and studies of evaluation met-rics.
For the related tasks of document and multi-document summarization, Graham (2015) providesa fine-grained comparison of automated evaluationmethods.
However, to the best of our knowledge,no studies of automatic evaluation metrics exist forabstractive compression of shorter texts.341Length Text Operations1-Sent Source Think of all the ways everyone in your household will benefit from your membership N/Ain Audubon.Ref-1 Imagine how your household will benefit from your Audubon membership.
paraphrase + deletion+ transformationRef-2 Everyone in your household will benefit from membership in Audubon.
deletion2-Sent Source Will the administration live up to its environmental promises?
Can we save the last of N/Aour ancient forests from the chainsaw?Ref-1 Can the administration keep its promises?
Can we save the last of our forests from loss?
two-sentences + deletion+ paraphraseRef-2 Will the administration live up to its environmental promises to save our ancient forests?
merge + deletionTable 1: Examples of 1- and 2-sentence crowd-sourced compressions, illustrating different rewrite types.Newswire Letters Journal Non-fiction#texts 695 1,591 1,871 2,012Table 2: Overview of the dataset by genre.3 Dataset: Annotation and PropertiesWe sampled single sentences and two-sentence para-graphs from several genres in the written text sectionof the Manually Annotated Sub-Corpus (MASC)(Ide et al, 2008; Ide et al, 2010) of the Open Amer-ican National Corpus (OANC), supplemented by ad-ditional data from the written section of OANC.Two-sentence paragraphs account for approximately23% of multi-sentence paragraphs in the OANC.The two-sentence paragraphs we sampled contain atleast 25 words.
Table 2 breaks the sampled textsdown by genre.
Non-news genres are better repre-sented in our sample than the newswire typicallyused in compression tasks.
The Letters examplesare expected to be useful for learning to compressemails.
The Journal texts are likely to be challeng-ing as their purpose is often more than to convey in-formation.
The Non-Fiction collection includes ma-terial from technical academic publications, such asPLoS Medicine, an open access journal.33.1 AnnotationCompressions were created using UHRS, an in-house crowd-sourcing system similar to Amazon?sMechanical Turk, in two annotation rounds, one forshortening and a second to rate compression quality.Generating compressions: In the first round, weasked five workers (editors) to abridge each sourcetext by at least 25%, while remaining grammaticaland fluent, and retaining the meaning of the orig-inal.
This requirement was enforced programmat-3http://journals.plos.org/plosmedicine/ically on the basis of character count.
The 25%rate is intended to reflect practical editing scenarios(e.g., shrink 8 pages to 6).
To facilitate meeting thisrequirement, the minimum source text length pre-sented to editors was 15 words.
For a subset of para-graphs, we collected compressions both as indepen-dent rewrites of their component sentences, and ofthe paragraph as a whole.
Table 1 show compres-sion examples and strategies.Evaluating compression quality: In the secondround, we asked 3-5 judges (raters) to evaluate thegrammaticality of each compression on a scale from1 (major errors, disfluent) through 3 (fluent), andagain analogously for meaning preservation on ascale from 1 (orthogonal) through 3 (most impor-tant meaning-preserving).4 We later used the sameprocess to evaluate compressions produced by auto-matic systems.
The full guidelines for the editorsand raters are available with the data release.Quality controls: All editors and raters werebased in the US, and the raters were required topass a qualification test which asked them to rate themeaning and grammaticality for a set of exampleswith known answers.
To further improve the qual-ity of the data, we removed low-quality compres-sions.
We computed the quality of each compressionas the average of the grammar and meaning qualityas judged by the raters.
We then computed the meanquality for each editor, and removed compressionsauthored by the bottom 10% of editors.
We did thesame for the bottom 10% of the raters.54Pilot studies suggested that a scale of 1-3 offered betterinter-annotator agreement than the standard 5-point Likert-typescale, at the cost of granularity.5This was motivated by the observation that the quality ofwork produced by judges is relatively constant (Gao et al,2015).342Description Texts QualitySource Target Avg CPS Meaning GrammarAll 6,169 26,423 4.28 2.78 2.82Per Source Length1-Sent 3,764 15,523 4.12 2.78 2.812-Sent 2,405 10,900 4.53 2.78 2.83Table 3: Overview of the dataset, presenting the overall numberof source and target texts, the average quality of the compressedtexts, and breakdown by length of source (number of sentences).Table 3 shows the number of compressions in thecleaned dataset, as well as the average number ofcompressions per source text (CPS) and the averagemeaning and grammar scores.
Meaning quality andgrammaticality scores are relatively good, averag-ing 2.78 and 2.82 respectively.
The filtered crowd-sourced compressions were most frequently judgedto retain the most important meaning (80% of thetime), or much of the meaning (17% of the time),with the lowest rating of 1 appearing only 3% of thetime.
This distribution is quite different from that ofautomatic compression systems in Section 4.We provide a standard split of the data into train-ing, development and test sets.6 There are 4,936source texts in the training, 448 in the development,and 785 in the test set.3.2 Inter-Annotator AgreementCrowd Workers: Since a different set of judgesperforms each task, large sets of inputs judged bythe same two raters are unavailable.
To simulatetwo raters, we follow Pavlick and Tetrault (2016):for each sentence, we randomly choose one anno-tator?s output as the category for annotator A, andselect the rounded average ranking for the remain-ing annotators as the category for annotator B. Wethen compute quadratic weighted ?
(Cohen, 1968)for this pair over the whole corpus.
We repeat theprocess 1000 times to compute the mean and vari-ance of ?.
The first row of the Table 4 reports theabsolute agreement and ?, where the absolute agree-ment measures the fraction of times that A is equalto B.
The 95% confidence intervals for ?
are narrow,with width at most .01.6The dataset can be downloaded from the project?s websitehttps://www.microsoft.com/en-us/research/project/intelligent-editing/.Description Meaning GrammarAgreement ?
Agreement ?worker versus worker .721 .306 .784 .381expert versus expert .888 .518 .890 .514expert versus worker .946 .549 .930 .344Table 4: Agreement on meaning preservation and grammati-cally between crowd workers and experts.Expert Raters: A small sample of 116 sentencepairs was rated by two expert judges.
We usedquadratic weighted ?
directly, without sampling.
Toassess agreement between experts and non-experts,we computed weighted ?
between the (rounded) av-erage of the expert judgments and the (rounded) av-erage of the crowd judgments, using 25,000 boot-strap replications each.
The results are shown in thelast two rows of Table 4.
The confidence intervalsfor ?
are wide due to the small sample size, andspan values up to .17 away from the mean.
Over-all, agreement of experts with the average crowd-sourced ratings is moderate (approaching substan-tial) for meaning, and fair for grammar.3.3 Analysis of Editing OperationsFrequency analysis: To analyze the editing oper-ations used, we applied the state-of-the-art monolin-gual aligner Jacana (Yao et al, 2013) to align inputto compressed texts.
Out of the 26,423 compres-sions collected, 25.2% contained only token dele-tions.
Those containing deletion and reorderingamounted to a mere 9.1%, while those that also con-tain substitution or rephrasing (abstractive compres-sions) is 65.6%.
Although abstraction is present inthe large majority of compressions, these statisticsdo not indicate that paraphrasing is more prevalentthan copying at the token level.
The word align-ments for target compression words indicate that7.1% of target tokens were inserted, 75.4% werecopied and 17.3% were paraphrased.
From thealignments for source text words, we see that 31% ofsource words were deleted.
The fraction of insertedand deleted words is probably overestimated by thisapproach, as it is likely that sequences of sourcewords were abstracted as shorter sequences of targetwords in many-to-one or many-to-many alignmentpatterns that are difficult to detect automatically.For the subset of examples where the input text343Meaning GrammarOperation Present Absent Present AbsentSubstitute 2.81** 2.70 2.79 2.85**Reorder 2.80 2.82 2.80 2.82**Merge 2.63 2.82** 2.84** 2.82Sentence Delete 2.57 2.82* 2.84 2.75Table 5: Meaning and grammaticality judgments by compres-sion operation.
*p = 0.002.
**p < 0.0001.Source Type Meaning Grammar2-Sentence 2.86** 2.87**1-Sentence 2.78 2.82Table 6: Meaning and grammaticality judgments for compress-ing two sentences jointly versus individually.
**p < 0.0001.contained more than one sentence, we computed thefrequency of sentence-merging and sentence dele-tion when compressing.
Of the compressions fortwo-sentence paragraphs, 72.4% had two sentencesin the output, 0.4% had one sentence deleted, and27.3% had the two source sentences merged.Impact of operations: Because the dataset con-tains multiple compressions of the same sources, weare able to estimate the impact of different editingoperations.
These were classified using the Jacanaword alignment tool.
Table 5 presents the averagejudgment scores for meaning preservation and gram-maticality for four operations.
The upper two rowsapply to all texts, the lower two to two-sentenceparagraphs only.
The statistical significance of theirimpact was tested using the Wilcoxon signed-ranktest on paired observations.
It appears that ratersview compressions that involve substitutions as sig-nificantly more meaning-preserving than those thatdo not (p < 0.0001), but judge their grammatical-ity to be lower than that of deletion-based compres-sions.
Note that the reduced grammaticality maybe due to typographical errors that have been in-troduced during rephrasing, which could have beenavoided had a more powerful word processor beenused as an editing platform.
Reordering has no sig-nificant impact on meaning, but leads to substantialdegradation in grammatically.
Conversely, abridg-ments that merge or delete sentences are rated as sig-nificantly less meaning preserving, but score higherfor grammaticality, possibly reflecting greater skillon the part of those editors..Impact of sentence context: Table 6 shows thatthe context provided by 2-sentence sources yieldssignificantly improved scores for both meaning andgrammaticality.
Here we used the matched pairs de-sign to compare the average quality of two-sentenceparagraph compressions with the average quality ofthe compressions of the same paragraphs producedby separately compressing the two sentences.4 Evaluating Evaluation MetricsProgress in automated text compression is stan-dardly measured by comparing model outputs at thecorpus level.
To train models discriminatively andto perform fine-grained system comparisons, how-ever, it is also necessary to have evaluation of sys-tem outputs at the individual input level.
Below, weexamine automated metric correlation with humanjudgments at both levels of granularity.4.1 Automatic MetricsThe goal of this analysis is to develop an under-standing of the performance of automatic evalua-tion metrics for text compression, and the factorscontributing to their performance.
To this end, wegroup automatic metrics according to three crite-ria.
The first is the linguistic units used to comparesystem and reference compressions.
Prior work oncompression evaluation has indicated that a parse-based metric is superior to one based on surface sub-strings (Clarke and Lapata, 2006), but the contribu-tion of the linguistic units has not been isolated, andsurface n-gram units have otherwise been success-fully used for evaluation in related tasks (Graham,2015).
Accordingly, we empirically compare met-rics based on surface uni-grams (LR-1), bi-grams(LR-2), tri-grams (LR-3), and four-grams (LR-4), aswell skip bi-grams (with a maximum of four inter-vening words as in ROUGE-S4) (SKIP-2), and de-pendency tree triples obtained from collapsed de-pendencies output from the Stanford parser (PARSE-2).7 The second criterion is the scoring measureused to evaluate the match between two sets of lin-guistic units corresponding to a system output and areference compression.
We compare Precision, Re-call, F-measure, and Precision+Brevity penalty (as7Clarke and Lapata (2006) used the RASP parser (Briscoeand Carroll, 2002), but we expect that the Stanford parser issimilarly robust and would lead to similar correlations.344in BLEU).
The third criterion is whether multiplereferences or a single reference is used, and in thecase of multiple references, the method used to ag-gregate information from multiple references.
Weinvestigate two previously applied methods and in-troduce a novel approach that often outperforms thestandard methods.To illustrate, we introduce some notation and usea simple example.
Consider a sub-phrase of one ofthe sentences in Table 1, think about your household,as an input text to compress.
Let us assume that wehave two reference compressions, R1: imagine yourhousehold, and R2: your household.
Each metric mis a function from a pair of a system output o and alist of references r1, r2, .
.
.
, rk to the reals.
To com-pute most metrics, we first compute a linguistic unitfeature vector for each reference ?
(rj), as well asfor the set of references ?
(r1, r2, .
.
.
, rk).
Similarly,we compute a linguistic unit vector for the output?
(o) and measure the overlap between the systemand reference vectors.
The vectors of the examplereferences, if we use surface bigram units, would be,for R1, {imagine your:1, your household:1},and for R2, {your household:1}.
The weightsof all n-grams in individual references and systemoutputs are equal to 1.8 If we use dependency-parse triples instead, the vector of R2 would be{nmod:poss(household, your):1}.The precision of a system output against a refer-ence is defined as the match ?(r)T?
(o) divided bythe number of units in the vector of o; the latter canbe expressed as the L1 norm of ?
(o) because allweights are positive: Precision(o, r) = ?
(r)T ?(o)|?
(o)|1 .The recall against a single reference can be similarlydefined as the match divided by the number of unitsin the reference: Recall(o, r) = ?
(r)T ?(o)|?
(r)|1 .We distinguish three methods for aggregating in-formation from multiple references: MULT-MAXwhich uses the single reference out of a set thatresults in the highest single-reference score, andtwo further methods, MULT-ALL and MULT-PROB,that construct an aggregate linguistic unit vector?
(r1, .
.
.
, rk) before matching.
MULT-ALL is thestandard method used in multi-reference BLEU,8We handle repeating n-grams by assigning each subsequentn-gram of the same type a distinct type, so that the i-th the of asystem output can match the i-th the of a reference.where the vector for a set of references is definedas the union of the features of the set.
For our ex-ample, the combined vector of R1 and R2 is equal tothe vector of R1, because R2 adds no new bigrams.MULT-PROB, a new method that we propose here,is motivated by the observation that although judg-ments of importance of content are subjective, themore annotators assert some information is impor-tant, the more this information should contribute tothe matching score.9 In MULT-PROB we define theweight of a linguistic unit in the combined referencevector as the proportion of references that includethe unit.
For our example, ?MULT-PROB(R1, R2) is{imagine your:.5, your household:1}.4.2 Models for Text CompressionFor the purpose of analysis, we trained and eval-uated four compression systems.
These includeboth deletion-based and abstractive models: (1) ILP,an integer linear programing approach for deletion-based compression (Clarke and Lapata, 2008), (2)T3, a tree transducer-based model for abstractivecompression (Cohn and Lapata, 2008), (3) Seq2seq,a neural network model for deletion-based compres-sion (Filippova et al, 2015), and (4) NAMAS, aneural model for abstractive compression and sum-marization (Rush et al, 2015).
We are not con-cerned with the relative performance of these mod-els so much as we are concerned with evaluating theautomatic evaluation metrics themselves.
We havesought to make the models competitive, but have notrequired that all systems use identical training data.All of the models are evaluated on the test setportion of our dataset.
All models use the train-ing portion of the data for training, and two models(Seq2Seq and NAMAS10) additionally use externaltraining data.
The external data is summarized inTable 7.
The Gigaword set was extracted from theAnnotated Gigaword (Napoles et al, 2012), usingthe implementation provided by Rush et al (2015).The Headline data was extracted in similar fashionusing an in-house news collection.9A similar insight was used in one of the component met-rics of the SARI evaluation metric used for text simplificationevaluation (Xu et al, 2016).10The original works introducing these models employedmuch larger training corpora, believed to be key to improvingthe accuracy of neutral network models with large parameterspaces.345Data #src tokens #trg tokens #sentsAbstractive Gigaword 114.1M 30.0M 3.6MHeadline 6.0M 1.4M 0.2MDeletion-based Gigaword 1,353K 329K 47KHeadline 59K 11K 2KTable 7: External data statistics.ILP: We use an open-source implementation11 ofthe semi-supervised ILP model described in (Clarkeand Lapata, 2008).
The model uses a trigram lan-guage model trained on a 9 million token subsetof the OANC corpus.
The ILP model requiresparsed sentences coupled with deletion-based com-pressions for training, so we filtered and prepro-cessed our dataset to satisfy these constraints.
Weused all single sentence inputs with their corre-sponding deletion-based compressions, and addi-tionally used two-sentence paragraph input/outputpairs split into sentences by heuristically aligningsource to target sentences in the paragraphs.T3: We use the authors?
implementation of thetree transducer system described in Cohn and La-pata (2008).
T3 similarly requires sentence-level in-put/output pairs, but can also learn from abstractivecompressions.
We thus used a larger set of approx-imately 28,000 examples (single sentences with ab-stractive compressions taken directly from the dataor as a result of heuristic sentence-level alignmentof two-sentence paragraphs).
We obtained parsetrees using the Stanford parser (Klein and Manning,2003), and used Jacana (Yao et al, 2013) for wordalignment.
The performance obtained by T3 in ourexperiments is substantially weaker (relative to ILP)than that reported in prior work (Cohn and Lapata,2008).
We therefore interpret this system outputsolely as data for evaluating automatic metrics.NAMAS: We run the publicly available implemen-tation of NAMAS12 with the settings described byRush et al (2015).
We modified the beam search al-gorithm to produce output with a compression ratiosimilar to that of the human references, since this ra-tio is a large factor in compression quality (Napoleset al, 2011), and systems generally perform betterif allowed to produce longer output, up to the max-imum length limit.
We enforced output length be-11https://github.com/cnap/sentence-compression12https://github.com/facebook/NAMAStween 50% and 75% of input length, which resultedin improved performance.Seq2seq: We implemented the sequence-to-sequence model13 described in Filippova et al(2015).
A deletion-based model, it uses the deletion-based subset of our training dataset and the deletion-based subset from the external data in Table 7.
Theencoder and decoder have three stacked LSTM lay-ers, the hidden dimension size is 512, and the vocab-ulary size is 30,000.
The compression rate was con-trolled in the same range as for the NAMAS model.All models produce output on all inputs in the testset.
For all models, we generated outputs for multi-sentence inputs by concatenating outputs for eachindividual sentence.144.3 ResultsOverall, we consider 80 metric variants, consistingof combinations of six types of linguistic units, com-bined with three scoring methods (Precision, Recall,and F-measure) and four settings of single referenceSINGLE-REF or three ways of scoring against multi-ple references MULT-ALL,MULT-MAX,MULT-PROB.Additionally, we include the standard single andmulti-reference versions of BLEU-2,BLEU-3,BLEU-4, and ROUGE-L.We compare automatic metrics to human judge-ments at the level of individual outputs or groupsof outputs (the whole corpus).
For a single output o,the human quality judgment is defined as the averageassigned by up to five human raters.
We denote themeaning, grammar, and combined quality values byM(o), G(o), and C(o) = .5M(o) + .5G(o), respec-tively.
We define the quality for a group of outputs asthe arithmetic mean of judgments over the outputs inthe group.
We use the arithmetic mean of automat-ing metrics at the individual output level to defineautomatic corpus quality metrics as well.15 To com-pare different metrics and establish statistical signif-icance of the difference between two metrics, we useWilliams test of the significance of the difference13https://github.com/ketranm/tardis14In small scale preliminary manual evaluation, we foundthat, although some models are theoretically able to make useof context beyond the sentence boundary, they performed betterif they compressed each sentence in a sequence independently.15This method has been standard for ROUGE, but has not forBLEU.
We find that averaging sentence-level metrics is also ad-vantageous for BLEU .346System Meaning Grammar CombinedT3 1.14 1.40 1.26NAMAS 1.56 1.30 1.43Seq2Seq 1.64 1.51 1.57ILP 2.28 2.22 2.25Table 8: Average human ratings of system outputs for meaningand grammar separately and in combination.between dependent Pearson correlations with hu-man judgments (Williams, 1959) as recommendedfor summarization evaluation (Graham, 2015) andother NLP tasks (e.g.
(Yannakoudakis et al, 2011)).4.3.1 Corpus-level metricsTable 8 shows the average human ratings of thefour systems, separately in meaning and grammar,as well as the combined measure (an arithmeticmean of meaning and grammar judgments).
Eventhough the performance of some systems is simi-lar, the differences between all pairs of systems inmeaning and grammar are significant p < 0.0001according to a paired t-test.
It is interesting tonote that ILP outperforms the more recently devel-oped neural network systems Seq2Seq and NAMAS.This might seem to contradict recent results show-ing that the new models are superior to traditionalbaselines, such as ILP.
We note however that per-formance on the test corpus in our study might notsubstantially improve through the use of large au-tomatically mined data-sets of headlines and corre-sponding news article sentences, due to differencesin genre and domain.
Using such data-sets for ef-fective training of neural network models for non-newswire domains remains an open problem.For each of the 80 metrics, we compared the rank-ing of the four systems with the ranking accordingto average human quality.
Fifty three of the metricsachieved perfect Spearman ?
and Kendall ?B cor-relation with human judgments of combined mean-ing and grammar quality.
Due to the small samplesize (four systems), we are unable to find statisti-cally significant differences among metrics at thecorpus level.
We only note that precision-based met-rics involving large linguistic units (four-grams) hadnegative correlations with human judgments.
Wecan conclude, however, that evaluation at the corpuslevel is robust for a wide variety of standard metricsusing linguistic units of size three or smaller.4.3.2 Single input-level pairwise systemcomparisonsWe can garner greater insight into the differenceof metric performance when we compare metrics atthe single input level.
To gauge the ability of met-rics to comparatively evaluate the quality of two sys-tems, we compute single input-level correlations ofautomatic metrics with human judgments followingthe protocol of Galley et al (2015).
Each system Aproduces a sequence of outputs o1A, .
.
.
, onA, corre-sponding to inputs x1, .
.
.
, xn.
For each system out-put, we use Q(a) to denote a generic human qualitymetric, varying over meaning, grammar, and theircombination.
For each pair of systems A and B, andeach metric m, we compute the difference in qual-ity for corresponding system outputs for each inputxi: m(oiA) ?m(oiB) and the difference in qualityaccording to human judgments: Q(oiA) ?
Q(oiB),and compute the correlation between these two se-quences.
We can thus compute the single input-levelcorrelation between m and Q for each pair of sys-tems A and B, resulting in a total of six correlationvalues (for the six pairs of systems) for each metric.For each pair of metrics m1 and m2, and for eachpair of systems A and B, we compute the statisti-cal significance of the difference between the Pear-son correlations of these metrics with human judge-ments.
We say that m1 is significantly better thanm2 on the A vs. B comparison if its Pearson cor-relation with human quality Q is significantly bet-ter (according to the Williams test of the differencein dependent correlations) than that of m2 with a p-value less than .05.
We say that m1 dominates m2overall if it is significantly better than m2 on at least80% of the pair-wise system comparisons.Table 9 shows the main correlation results at thelevel of individual inputs.
We report correlationswith meaning, grammar, and combined quality sep-arately.
For each human quality metric, we see thetop automatic metrics in the first group of rows.
Thetop metrics are ones that, for at least 80% of the sys-tem comparisons, are not significantly dominated byany other metric.
In addition, we show the impact ofeach of the three criteria: linguistic units, scoringmeasure, and multiple references, in correspondinggroups of rows.
For each linguistic unit type, weshow the best-performing metric that uses units of347SKIP-2+Recall+MULT-PROB .59PARSE-2+Recall+MULT-PROB .57SKIP-2+Recall+MULT-MAX .58LR-1+Recall+MULT-PROB .54LR-2+Recall+MULT-PROB .56?LR-3+Recall+MULT-ALL .55?LR-4+Recall+MULT-ALL .52-SKIP-2+Recall+MULT-PROB .59?PARSE-2+Recall+MULT-PROB .57?SKIP-2+Recall+MULT-PROB .59?SKIP-2+Precision+MULT-ALL .36-SKIP-2+F-1+MULT-ALL .58?SKIP-2+Recall+SINGLE-REF .49-SKIP-2+Recall+MULT-MAX .58?SKIP-2+Recall+MULT-PROB .59?SKIP-2+Recall+MULT-ALL .58?BLEU-3+PrecBrev+MULT-ALL .50-ROUGE-L+Recall+MULT-MAX .49-Top metricsPARSE-2+Recall+MULT-MAX .35LR-3+F-1+MULT-ALL .35PARSE-2+F-1+MULT-ALL .35PARSE-2+Recall+MULT-PROB .35LR-2+F-1+MULT-ALL .34LR-3+Recall+MULT-ALL .34Best per linguistic unitLR-1+Recall+MULT-MAX .25-LR-2+F-1+MULT-ALL .34?LR-3+F-1+MULT-ALL .35?LR-4+F-1+MULT-ALL .34?SKIP-2+F-1+MULT-PROB .33PARSE-2+Recall+MULT-MAX .35?Best per scoring typePARSE-2+Recall+MULT-MAX .35LR-2+Precision+MULT-ALL .31LR-3+F-1+MULT-ALL .35Best per reference aggregationPARSE-2+F-1+SINGLE-REF .29PARSE-2+Recall+MULT-MAX .35PARSE-2+Recall+MULT-PROB .35LR-3+F-1+MULT-ALL .35Other standard setting combinationsBLEU-4+PrecBrev+MULT-ALL .30ROUGE-L+Recall+MULT-MAX .27PARSE-2+Recall+MULT-PROB .52PARSE-2+Recall+MULT-MAX .52SKIP-2+Recall+MULT-MAX .51LR-2+Recall+MULT-PROB .51LR-2+F-1+MULT-ALL .50LR-1+Recall+MULT-PROB .44-LR-2+Recall+MULT-PROB .51?LR-3+F-1+MULT-ALL .50?LR-4+Recall+MULT-ALL .47SKIP-2+Recall+MULT-MAX .51?PARSE-2+Recall+MULT-PROB .52?PARSE-2+Recall+MULT-PROB .52SKIP-2+Precision+MULT-ALL .37-LR-2+F-1+MULT-ALL .50SKIP-2+Recall+SINGLE-REF .44-PARSE-2+Recall+MULT-MAX .52PARSE-2+Recall+MULT-PROB .52LR-2+F-1+MULT-ALL .50BLEU-3+PrecBrev+MULT-ALL .45-ROUGE-L+Recall+MULT-MAX .43Table 9: Left to right: Pearson correlation of automatic metrics with human ratings for meaning, grammar, and combined quality.this type.
Similarly, for the other criteria, we showthe best performing metric for each value of the cri-terion.
Metrics with a ?
suffix in each group signif-icantly dominate metrics with a ?
suffix.
Metricswith a ?
suffix in a group are dominated by at leastone other metric, possibly outside of the group.
Thelowest group of rows in each main column presentsthe performance of other metrics that cannot be clas-sified directly based on the three criteria.A high-level observation that can be made is thatthe correlations with meaning are much higher thanthe correlations with grammar.
The best corre-lations in meaning can be classified as ?strong?,whereas the best correlations in grammar are inthe ?medium?
range.
Unigrams are heavily domi-nated by higher order n-grams in all settings.
Four-grams are also weaker that other units in measuringmeaning preservation.
Dependency triple (parse-based) metrics are strong, in particular in measuringgrammaticality, but do not significantly dominateskip bi-grams or contiguous bi-grams.
The scor-ing measure used has a strong impact.
We see thatprecision-based metrics are substantially dominatedby metrics that incorporate recall, except for gram-mar evaluation.
Importantly, we see that multiplereferences contribute substantially to metric qual-ity, as all methods that use multiple references out-perform single-reference metrics.
In both meaningand combined evaluation, this difference was statis-tically significant.
Finally, we observe that standardBLEU metrics and ROUGE-L were not competitive.5 ConclusionWe have introduced a large manually collectedmulti-reference abstractive dataset and quantifiedthe impact of editing operations and context on hu-man compression quality, showing that substitutionand rephrasing operations are more meaning pre-serving than other operations, and that compressionin context improves quality.
Further, in the first sys-tematic study of automatic evaluation metrics fortext compression, we have demonstrated the impor-tance of utilizing multiple references and suitablelinguistic units, and incorporating recall.AcknowledgmentsWe are grateful to Jaime Teevan, Shamsi Iqbal, DanLiebling, Bill Dolan, Michel Galley, and Wei Xu, to-gether with the three anonymous reviewers for theirhelpful advice and suggestions.348ReferencesSrinivas Bangalore, Owen Rambow, and Steve Whittaker.2000.
Evaluation metrics for generation.
In Proceed-ings of INLG.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL-HLT.Ted Briscoe and John A Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof LREC.Hori Chiori and Sadaoki Furui.
2004.
Speech summa-rization: an approach through word extraction and amethod for evaluation.
IEICE Transactions on Infor-mation and Systems, 87(1):15?25.James Clarke and Mirella Lapata.
2006.
Models forsentence compression: A comparison across domains,training requirements and evaluation measures.
InProceedings of ACL-COLING.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, pages 399?429.James Clarke.
2008.
Global Inference for Sentence Com-pression: An Integer Linear Programming Approach.Ph.D.
thesis, Univeristy of Edinburgh.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement provision for scaled disagreement or partialcredit.
Psychological bulletin, 70(4):213.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofCOLING.Katja Filippova and Yasemin Altun.
2013.
Overcomingthe lack of parallel data in sentence compression.
InProceedings of EMNLP.Katja Filippova, Enrique Alfonseca, Carlos A Col-menares, Lukasz Kaiser, and Oriol Vinyals.
2015.Sentence compression by deletion with LSTMs.
InProceedings of EMNLP.Michel Galley, Chris Brockett, Alessandro Sordoni,Yangfeng Ji, Michael Auli, Chris Quirk, MargaretMitchell, Jianfeng Gao, and Bill Dolan.
2015.deltableu: A discriminative metric for generation taskswith intrinsically diverse targets.
In Proceedings ofACL-IJCNLP (Volume 2: Short Papers).Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
In Proceedings of EMNLP.Mingkun Gao, Wei Xu, and Chris Callison-Burch.
2015.Cost optimization in crowdsourcing translation: Lowcost translations made even cheaper.
In Proceedingsof NAACL-HLT.Yvette Graham.
2015.
Re-evaluating automatic summa-rization with BLEU and 192 shades of ROUGE.
InProceedings of EMNLP.Nancy Ide, Collin F. Baker, Christiane Fellbaum,Charles J. Fillmore, and Rebecca J. Passonneau.
2008.MASC: the manually annotated sub-corpus of ameri-can english.
In Proceedings of LREC.Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-becca Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by the peo-ple.
In Proceedings of ACL (Volume 2: Short Papers).Hongyan Jing and Kathleen R McKeown.
1999.
Thedecomposition of human-written summary sentences.In Proceedings of SIGIR.Hongyan Jing.
2000.
Sentence reduction for automatictext summarization.
In Proceedings of ANLP.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Text summarizationbranches out: Proceedings of the ACL-04 workshop,volume 8.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011.
Evaluating sentence compres-sion: Pitfalls and suggested remedies.
In Proceedingsof the Workshop on Monolingual Text-To-Text Genera-tion.Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In Proceed-ings of the Joint Workshop on Automatic KnowledgeBase Construction and Web-scale Knowledge Extrac-tion.Ellie Pavlick and Joel Tetrault.
2016.
An empirical anal-ysis of formality in online communication.
Transac-tions of the Association for Computational Linguistics,pages 61?74.Stefan Riezler, Tracy H King, Richard Crouch, and AnnieZaenen.
2003.
Statistical sentence condensation us-ing ambiguity packing and stochastic disambiguationmethods for lexical-functional grammar.
In Proceed-ings of NAACL-HLT.Alexander M. Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of EMNLP.Yuya Unno, Takashi Ninomiya, Yusuke Miyao, andJun?ichi Tsujii.
2006.
Trimming CFG parse treesfor sentence compression using machine learning ap-proaches.
In Proceedings of COLING-ACL.349Lucy Vanderwende, Hisami Suzuki, Chris Brockett, andAni Nenkova.
2007.
Beyond sumbasic: Task-focusedsummarization with sentence simplification and lex-ical expansion.
Information Processing & Manage-ment, 43(6):1606?1618.Evan James Williams.
1959.
Regression analysis.
Wiley,New York.Wei Xu, Courtney Napoles, Quanze Chen, and ChrisCallison-Burch.
2016.
Optimizing statistical machinetranslation for text simplification.
Transactions of theAssociation for Computational Linguistics, 4.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of ACL-HLT.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark.
2013.
A lightweight and highperformance monolingual word aligner.
In Proceed-ings of ACL (Volume 2: Short Papers).350
