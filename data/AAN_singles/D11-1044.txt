Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474?485,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsQuasi-Synchronous Phrase Dependency Grammarsfor Machine TranslationKevin Gimpel Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniveristyPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractWe present a quasi-synchronous dependencygrammar (Smith and Eisner, 2006) for ma-chine translation in which the leaves of thetree are phrases rather than words as in pre-vious work (Gimpel and Smith, 2009).
Thisformulation allows us to combine structuralcomponents of phrase-based and syntax-basedMT in a single model.
We describe a methodof extracting phrase dependencies from paral-lel text using a target-side dependency parser.For decoding, we describe a coarse-to-fine ap-proach based on lattice dependency parsing ofphrase lattices.
We demonstrate performanceimprovements for Chinese-English and Urdu-English translation over a phrase-based base-line.
We also investigate the use of unsuper-vised dependency parsers, reporting encourag-ing preliminary results.1 IntroductionTwo approaches currently dominate statistical ma-chine translation (MT) research.
Phrase-based mod-els (Koehn et al, 2003) excel at capturing localreordering phenomena and memorizing multi-wordtranslations.
Models that employ syntax or syntax-like representations (Chiang, 2005; Galley et al,2006; Zollmann and Venugopal, 2006; Huang et al,2006) handle long-distance reordering better thanphrase-based systems (Auli et al, 2009) but often re-quire constraints on the formalism or rule extractionmethod in order to achieve computational tractabil-ity.
As a result, certain instances of syntactic diver-gence are more naturally handled by phrase-basedsystems (DeNeefe et al, 2007).In this paper we present a new way of combin-ing the advantages of phrase-based and syntax-basedMT.
We propose a model in which phrases are orga-nized into a tree structure inspired by dependencysyntax.
Instead of standard dependency trees inwhich words are vertices, our trees have phrases asvertices.
We describe a simple heuristic to extractphrase dependencies from an aligned parallel cor-pus parsed on the target side, and use them to com-pute target-side tree features.
We define additionalstring-to-tree features and, if a source-side depen-dency parser is available, tree-to-tree features to cap-ture properties of how phrase dependencies interactwith reordering.To leverage standard phrase-based features along-side our novel features, we require a formalismthat supports flexible feature combination and effi-cient decoding.
Quasi-synchronous grammar (QG)provides this backbone (Smith and Eisner, 2006);we describe a coarse-to-fine approach for decod-ing within this framework, advancing substantiallyover earlier QG machine translation systems (Gim-pel and Smith, 2009).
The decoder involves generat-ing a phrase lattice (Ueffing et al, 2002) in a coarsepass using a phrase-based model, followed by lat-tice dependency parsing of the phrase lattice.
Thisapproach allows us to feasibly explore the combinedsearch space of segmentations, phrase alignments,and target phrase dependency trees.Our experiments demonstrate an average im-provement of +0.65 BLEU in Chinese-Englishtranslation across three test sets and an improvementof +0.75 BLEU in Urdu-English translation overa phrase-based baseline.
We also describe experi-ments in which we replace supervised dependencyparsers with unsupervised parsers, reporting promis-ing results: using a supervised Chinese parser anda state-of-the-art unsupervised English parser pro-vides our best results, giving an averaged gain of+0.79 BLEU over the baseline.
We also discuss howour model improves translation quality and discussfuture possibilities for combining approaches to ma-474chine translation using our framework.2 Related WorkWe previously applied quasi-synchronous grammarto machine translation (Gimpel and Smith, 2009),but that system performed translation fundamentallyat the word level.
Here we generalize that model tofunction on phrases, enabling a tighter coupling be-tween the phrase segmentation and syntactic struc-tures.
We also present a decoder efficient enough toscale to large data sets and present performance im-provements in large-scale experiments over a state-of-the-art phrase-based baseline.Aside from QG, there have been many effortsto use dependency syntax in machine translation.Quirk et al (2005) used a source-side dependencyparser and projected automatic parses across wordalignments in order to model dependency syntax onphrase pairs.
Shen et al (2008) presented an exten-sion to Hiero (Chiang, 2005) in which rules havetarget-side dependency syntax and therefore enablethe use of a dependency language model.More recently, researchers have sought the bene-fits of dependency syntax while preserving the ad-vantages of phrase-based models, such as efficiencyand coverage.
Galley and Manning (2009) loos-ened standard assumptions about dependency pars-ing so that the efficient left-to-right decoding pro-cedure of phrase-based translation could be retainedwhile a dependency language model is incorporated.Carreras and Collins (2009) presented a string-to-dependency system that permits non-projective de-pendency trees (thereby allowing a larger space oftranslations) and use a rule extraction procedure thatincludes rules for every phrase in the phrase table.We take an additional step in this direction byworking with dependency grammars on the phrasesthemselves, thereby bringing together the structuralcomponents of phrase-based and dependency-basedMT in a single model.
While others have workedon combining rules from multiple syntax-based sys-tems (Liu et al, 2009) or using posteriors from mul-tiple models to score translations (DeNero et al,2010), we are not aware of any other work that seeksto directly integrate phrase-based and syntax-basedmachine translation at the modeling level.11Dymetman and Cancedda (2010) present a formal analy-3 ModelGiven a sentence s and its dependency tree ?s,we formulate the translation problem as finding thetarget sentence t?, the segmentation ??
of s intophrases, the segmentation ??
of t?
into phrases, thedependency tree ???
on the target phrases ?
?, and theone-to-one phrase alignment a?
such that?t?,??,?
?, ???,a?
?= argmax?t,?,?,?
?,a?p(t,?,?, ?
?,a |s, ?s)We use a linear model (Och and Ney, 2002):p(t,?,?, ?
?,a | s, ?s) ?exp{?>g(s, ?s, t,?,?, ?
?,a)}where g is a vector of arbitrary feature functions onthe full set of structures and ?
holds correspondingfeature weights.
Table 1 summarizes our notation.In modeling p(t,?,?, ?
?,a | s, ?s), we makeuse of quasi-synchronous grammar (QG; Smithand Eisner, 2006).
Given a source sentence andits parse, a QG induces a probabilistic monolingualgrammar over sentences ?inspired?
by the sourcesentence and tree.
We denote this grammar byGs,?s ;its (weighted) language is the set of translations of s.Quasi-synchronous grammar makes no restric-tions on the form of the target monolingual gram-mar, though dependency grammars have been usedin most previous applications of QG (Wang et al,2007; Das and Smith, 2009; Smith and Eisner,2009), including previous work in MT (Smith andEisner, 2006; Gimpel and Smith, 2009).
We pre-viously presented a word-based machine translationmodel based on a quasi-synchronous dependencygrammar.
However, it is well-known in the MT com-munity that translation quality is improved whenlarger units are modeled.
Therefore, we use a de-pendency grammar in which the leaves are phrasesrather than words.We define a phrase dependency grammar as amodel p(?, ?
?|t) over the joint space of segmen-tations of a sentence into phrases and dependencytrees on the phrases.2 Phrase dependency grammarssis of the problem of intersecting phrase-based and hierarchicaltranslation models, but do not provide experimental results.2We restrict our attention to projective trees in this paper,but the generalization to non-projective trees is easily made.475s = ?s1, .
.
.
, sn?
source language sentencet = ?t1, .
.
.
, tm?
target language sentence, translation of s?
= ?
?1, .
.
.
, ?n??
segmentation of s into phrases?i, ?i = ?sj , .
.
.
, sk?
s.t.
?1 ?
.
.
.
?
?n?
= s?
= ?
?1, .
.
.
, ?m??
segmentation of t into phrases?i, ?i = ?tj , .
.
.
, tk?
s.t.
?1 ?
.
.
.
?
?m?
= t?s : {1, .
.
.
, n} ?
{0, .
.
.
, n} dependency tree on source words s, where ?s(i) is the index ofthe parent of word si (0 is the root, $)??
: {1, .
.
.
,m?}
?
{0, .
.
.
,m?}
dependency tree on target phrases ?, where ??
(i) is the index ofthe parent of phrase ?ia : {1, .
.
.
,m?}
?
{1, .
.
.
, n?}
one-to-one alignment from phrases in ?
to phrases in ??
= ??,??
parameters of the full model (?
= phrase-based, ?
= QPDG)Table 1: Key notation.have recently been used by Wu et al (2009) for fea-ture extraction for opinion mining.
When used fortranslation modeling, they allow us to capture phe-nomena like local reordering and idiomatic transla-tions within each phrase as well as long-distance re-lationships among the phrases in a sentence.We then define a quasi-synchronous phrasedependency grammar (QPDG) as a conditionalmodel p(t,?,?, ?
?,a | s, ?s) that induces a prob-abilistic monolingual phrase dependency grammarover sentences inspired by the source sentence and(lexical) dependency tree.
The source and tar-get sentences are segmented into phrases and thephrases are aligned in a one-to-one alignment.We note that we actually depart here slightly fromthe original definition of QG.
The alignment variablein QG links target tree nodes to source tree nodes.However, we never commit to a source phrase de-pendency tree, instead using a source lexical depen-dency tree output by a dependency parser, so ouralignment variable a is a function from target treenodes (phrases in ?)
to source phrases in ?, whichmight not be source tree nodes.
The features in ourmodel may consider a large number of source phrasedependency trees as long as they are consistent with?s.4 FeaturesOur model contains all of the standard phrase-basedfeatures found in systems like Moses (Koehn et al,2007), including four phrase table probability fea-tures, a phrase penalty feature, an n-gram languagemodel, a distortion cost, six lexicalized reorderingfeatures, and a word penalty feature.We now describe in detail the additional features$?
said : $?
we should$?
said that $?
has been$?
is a - us?
relations$?
will be $?
he said$?
it is cross - strait?
relations$?
this is $?
pointed out that$?
we must , and?
isthe?
united states the chinese?
governmentthe?
development of $?
is thethe two?
countries $?
said ,he?
said : one - china?
principle$?
he said : sino - us?
relationsTable 2: Most frequent phrase dependencies with at least2 words in one of the phrases (dependencies in which onephrase is entirely punctuation are not shown).
$ indicatesthe root of the tree.in our model that are used to score phrase depen-dency trees.
We shall refer to these as QPDGfeatures and will find it useful later to notation-ally distinguish their feature weights from those ofthe phrase-based model.
We use ?
for weights ofthe standard phrase-based model features and ?
forweights of the QPDG features.
We include three cat-egories of features, differentiated by what pieces ofstructure they consider.4.1 Target Tree FeaturesWe first include features that only consider t, ?,and ??.
These features can be categorized as ?syn-tactic language model?
features (Shen et al, 2008;Galley and Manning, 2009), though unlike previouswork our features model both the phrase segmenta-tion and dependency structure.
Typically, these sortsof features are probabilities estimated from a corpusparsed using a supervised parser.
However, there donot currently exist treebanks with annotated phrase476,?
made up 0.057he?
made up 0.021supreme court?
made up 0.014court?
made up 0.014in september 2000?
made up 0.014in september 2000 ,?
made up 0.014made up?
of 0.065made up?
.
0.029made up?
, 0.016made up?
mind to 0.01Table 3: Most probable child phrases for the parentphrase ?made up?
for each direction, sorted by the con-ditional probability of the child phrase given the parentphrase and direction.dependency trees.Our solution is to use a standard supervised de-pendency parser and extract phrase dependencies us-ing bilingual information.3 We begin by obtainingsymmetrized word alignments and extracting phrasepairs using the standard heuristic from phrase-basedMT (Koehn et al, 2003).
Given the set of extractedphrase pairs for a sentence, denote by W the set ofunique target-side phrases among them.
We parsethe target sentence with a dependency parser and, foreach pair of phrases u, v ?
W , we extract a phrasedependency (along with its direction) if u and v donot overlap and there is at least one lexical depen-dency between a word in u and a word in v. If thereare lexical dependencies in both directions, we ex-tract a phrase dependency only for the single longestone.
Since we use a projective dependency parser,the longest lexical dependency between two phrasesis guaranteed to be unique.
Table 2 shows a listingof the most frequent phrase dependencies extracted(lexical dependencies are omitted).We note that during training we never explicitlycommit to any single phrase dependency tree for atarget sentence.
Rather, we extract phrase depen-dencies from all phrase dependency trees consis-tent with the word alignments and the lexical de-pendency tree.
Thus we treat phrase dependencytrees analogously to phrase segmentations in stan-dard phrase extraction.We perform this procedure on all sentence pairsin the parallel corpus.
Given a set of extracted3For a monolingual task, Wu et al (2009) used a shal-low parser to convert lexical dependencies from a dependencyparser into phrase dependencies.phrase dependencies of the form ?u, v, d?, whereu is the head phrase, v is the child phrase, andd ?
{left , right} is the direction, we then estimateconditional probabilities p(v|u, d) using relative fre-quency estimation.
Table 3 shows the most probablechild phrases for an example parent phrase.
To com-bat data sparseness, we perform the same procedurewith each word replaced by its word cluster ID ob-tained from Brown clustering (Brown et al, 1992).We include a feature in the model for the sum ofthe scaled log-probabilities of each attachment:m?
?i=1max(0, C + log p(?i|???
(i), d(i))(1)where d(i) = I[??(i)?
i > 0] is the direction of thedependency arc.Although we use log-probabilities in this featurefunction, we first add a constant C to each to ensurethey are all positive.4 The max expression protectsunseen parent-child phrase dependencies from caus-ing the score to be negative infinity.
Our motivationis a desire for the features to be used to prefer onederivation over another but not to rule out a deriva-tion completely if it merely happens to contain a de-pendency unobserved in the training data.We also include lexical weighting features simi-lar to those used in phrase-based MT (Koehn et al,2003).
Whenever we extract a phrase dependency,we extract the longest lexical dependency containedwithin it.
For all ?parent, child, direction?
lexi-cal dependency tuples ?x, y, d?, we estimate condi-tional probabilities plex (y|x, d) from the parsed cor-pus using relative frequency estimation.
Then, for aphrase dependency with longest lexical dependency?x, y, d?, we add a feature for plex (y|x, d) to themodel, using a formula similar to Eq.
1.
Differentinstances of a phrase dependency may have differentlexical dependencies extracted with them.
We addthe lexical weight for the most frequent, breakingties by choosing the lexical dependency that maxi-mizes p(y|x, d), as was also done by Koehn et al(2003).In all, we include 4 target tree features: one forphrase dependencies, one for lexical dependencies,4The reasoning here is that whenever we use a phrase de-pendency that we have observed in the training data, we want toboost the score of the translation.
If we used log-probabilities,each observed dependency would incur a penalty.477k?nnen:cank?nnen:maysie:youes:it...vorbei:by$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can you deliver it by tomorrow morning ?can     you     deliver  it    by     tomorrow morning ?CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?...
... .........k?nnen:canliefern:deliversie:yousie:ites:it k?nnen:cank?nnen:canliefern:deliversie:youes:itvorbei:bymorgen:tomorrowmorgen:tomorrowliefern:deliveres:itvorbei:byfr?h:morning...es:itmorgen:tomorrowliefern:delivervorbei:byfr?h:morningfr?h:early?:?morgen:morningk?nnen:cank?nnen:maysie:youes:it...vorbei:by...
... .........k?nnen:canliefern:deliversie:yousie:ites:it k?nnen:cank?nnen:canliefern:deliversie:youes:itvorbei:bymorgen:tomorrowmorgen:tomorrowliefern:deliveres:itvorbei:byfr?h:morningfr?h:early?
:?morgen:morning...fr?h:morningmorgen:tomorrowmorgen:morningliefern:delivervorbei:by$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can     you     deliver  it    by     tomorrow morning ?$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can     you     deliver  it    by     tomorrow morning ?x y      za    b   c d ex y      za    b   c d ex y      za    b   c d ex y      za    b   c d eFigure 1: String-to-tree configurations; each is associatedwith a feature that counts its occurrences in a derivation.and the same features computed from a transformedversion of the corpus in which each word is replacedby its Brown cluster.4.2 String-to-Tree ConfigurationsWe consider features that count instances of reorder-ing configurations involving phrase dependencies.In addition to the target-side structures, these fea-tures consider ?
and a, though not s or ?s.
For ex-ample, when building a parent-child phrase depen-dency with the child to the left, one feature value isincremented if their aligned source-side phrases arein the same order.
This configuration is the leftmostin Fig.
1; we include features for the other three con-figurations there as well, for a total of 4 features inthis category.4.3 Tree-to-Tree ConfigurationsWe include features that consider s, ?, and ?s in ad-dition to t, ?, and ??.
We begin with features foreach of the quasi-synchronous configurations fromSmith and Eisner (2006), adapted to phrase depen-dency grammars.
That is, for a parent-child pair???
(i), i?
in ?
?, we consider the relationship be-tween a(??
(i)) and a(i), the source-side phrasesto which ??
(i) and i align.
We use the follow-ing named configurations from Smith and Eisner:root-root, parent-child, child-parent, grandparent-grandchild, sibling, and c-command.5 We define afeature to count instances of each of these configu-rations, including an additional feature for ?other?configurations that do not fit into these categories.6When using a QPDG, there are multiple waysto compute tree-to-tree configuration features, since5See Fig.
3 in Smith and Eisner (2006) for illustrations.6We actually include two versions of each configuration fea-ture other than ?root-root?
: one for the source phrases being inthe same order as the target phrases and one for them beingswapped.Input: sentence s, dependency parse ?s, coarseparameters ?M , fine parameters ??,?
?Output: translation tLMERT ?
GenerateLattices (s, ?M );LFB ?
FBPrune (LMERT, ?M );?t,?,?, ??,a?
?
QGDEPPARSE(LFB, ??,??
);return t;Algorithm 1: CoarseToFineDecodewe use a phrase dependency tree for the target side,a lexical dependency tree for the source side, anda phrase alignment.
We use the following heuristicapproach.
Given a pair of source words, one withindex j in source phrase a(??
(i)) and the other withindex k in source phrase a(i), we have a parent-child configuration if ?s(k) = j; if ?s(j) = k, achild-parent configuration is present.
In order for thegrandparent-grandchild configuration to be present,the intervening parent word must be outside bothphrases.
For sibling and other c-command config-urations, the shared parent or ancestor must also beoutside both phrases.After obtaining a list of all configurations presentfor each pair of words ?j, k?, we fire the feature forthe single configuration corresponding to the max-imum distance |j ?
k|.
If no configurations arepresent between any pair of words, the ?other?
fea-ture fires.
Therefore, only one configuration featurefires for each phrase dependency attachment.Finally, we include features that consider thedependency path distance between phrases in thesource-side dependency tree that are aligned toparent-child pairs in ??.
We include a feature thatsums, for each target phrase i, the inverse of theminimum undirected path length between each wordin a(i) and each word in ??(a(i)).
The minimumundirected path length is defined as the number ofdependency arcs that must be crossed to travel fromone word to the other in ?s.
We use one featurefor undirected path length and one other for directedpath length.
If there is no (un)directed path from aword in a(i) to a word in ??
(a(i)), we use?
as theminimum length.There are 15 features in this category, for a totalof 23 QPDG features.4785 DecodingFor a QPDG model, decoding consists of findingthe highest-scoring tuple ?t,?,?, ??,a?
for an in-put sentence s and its parse ?s, i.e., finding the mostprobable derivation under the s/?s-specific grammarGs,?s .
We follow Gimpel and Smith (2009) in con-structing a lattice to representGs,?s and using latticeparsing to search for the best derivation, but we con-struct the lattice differently and employ a coarse-to-fine strategy (Petrov, 2009) to speed up decoding.It has become common in recent years for MT re-searchers to exploit efficient data structures for en-coding concise representations of the pruned searchspace of the model, such as phrase lattices forphrase-based MT (Ueffing et al, 2002; Machereyet al, 2008; Tromble et al, 2008).
Each edge ina phrase lattice corresponds to a phrase pair andeach path through the lattice corresponds to a tuple?t,?,?,a?
for the input s. Decoding for a phraselattice consists of finding the highest-scoring path,which is done using dynamic programming.
To alsomaximize over ?
?, we perform lattice dependencyparsing, which allows us to search over the space oftuples ?t,?,?,a, ???.
Given the lattice and Gs,?s ,lattice parsing is a straightforward generalization ofthe standard arc-factored dynamic programming al-gorithm from Eisner (1996).The lattice parsing algorithm requires O(E2V )time and O(E2 + V E) space, where E is the num-ber of edges in the lattice and V is the number ofnodes.7 Typical phrase lattices might easily containtens of thousands of nodes and edges, making exactsearch prohibitively expensive for all but the small-est lattices.
So, we use approximate search based oncoarse-to-fine decoding.
We now discuss each stepof this procedure; an outline is shown as Alg.
1.Pass 1: Lattice Pruning After generating phraselattices using a phrase-based MT system, we prunelattice edges using forward-backward pruning (Six-tus and Ortmanns, 1999), which has also been usedin previous work using phrase lattices (Tromble etal., 2008).
This pruning method computes the max-marginal for each lattice edge, which is the score ofthe best full path that uses that edge.
Max-marginals7To prevent confusion, we use the term edge to refer to aphrase lattice edge and arc to refer to a parent-child dependencyin the phrase dependency tree.offer the advantage that the best path in the lattice ispreserved during pruning.
For each lattice, we usea grid search to find the most liberal threshold thatleaves fewer than 1000 edges in the resulting lattice.As complexity is quadratic in E, forcing E to beless than 1000 improves runtime substantially.
Af-ter pruning, the lattices still contain more than 1016paths on average and oracle BLEU scores are typi-cally 12-15 points higher than the model-best paths.Pass 2: Parent Ranking Given a pruned lattice,we then remove some candidate dependency arcsfrom consideration.
It is common in dependencyparsing to use a coarse model to rank the top k par-ents for each word, and to only consider these duringparsing (Martins et al, 2009; Bergsma and Cherry,2010).
Unlike string parsing, our phrase lattices im-pose several types of constraints on allowable arcs.For example, each node in the phrase lattice is an-notated with a coverage vector?a bit vector indicat-ing which words in the source sentence have beentranslated?which implies a topological ordering ofthe nodes.
To handle constraints like these, we firstuse the Floyd-Warshall algorithm (Floyd, 1962) tofind the best score between every pair of nodes inthe lattice.
This algorithm also tells us whether eachedge is reachable from each other edge, allowingus to immediately prune dependency arcs betweenedges that are unreachable from each other.After eliminating impossible arcs, we turn topruning away unlikely ones.
In standard (string) de-pendency parsing, every word is assigned a parent.In lattice parsing, however, most lattice edges willnot be assigned any parent.
Certain lattice edges aremuch more likely to be contained within paths, sowe allow some edges to have more candidate parentedges than others.
We introduce hyperparameters?, ?, and ?
to denote, respectively, the minimum,maximum, and average number of parent edges tobe considered for each lattice edge (?
?
?
?
?
).We rank the full set of E2 arcs according to theirscores (using the QPDG features and their weights?)
and choose the top ?E of these arcs while en-suring that each edge has at least ?
and at most ?potential parent edges.This step reduces the time complexity fromO(E2V ) to O(?EV ), where ?
< E. In our ex-periments, we set ?
= 300, ?
= 100, and ?
= 400.479Input: tuning set D = ?S, T ?, initial weights ?0 forcoarse model, initial weights ?0 foradditional features in fine modelOutput: coarse model learned weights: ?M , finemodel learned weights: ???,???
?M ?
MERT (S, T , ?0, 100, MOSES);LMERT ?
GenerateLattices (S, ?M );LFB ?
FBPrune (LMERT, ?M );???,???
?MERT (LFB, T , ?
?M ,?0?, 200, QGDEPPARSE);return ?M , ???,???
;Algorithm 2: CoarseToFineTrainPass 3: Lattice Dependency Parsing After com-pleting the coarse passes, we parse using bottom-updynamic programming based on the agenda algo-rithm (Nederhof, 2003; Eisner et al, 2005).
We onlyconsider arcs that survived the filtering in Pass 2.We weight agenda items by the sum of their scoresand the Floyd-Warshall best path scores both fromthe start node of the lattice to the beginning of theitem and the end of the item to any final node.
Thisheuristic helps us to favor exploration of items thatare highly likely under the phrase-based model.If the score of the partial structure can only getworse when combining it with other structures (e.g.,in a PCFG), then the first time that we pop an itemof type GOAL from the agenda, we are guaranteedto have the best parse.
However, in our model, somefeatures are positive and others negative, making thisproperty no longer hold; as a result, GOAL itemsmay be popped out of order from the agenda.
There-fore, we use an approximation, simply popping GGOAL items from the agenda and then stopping.
Theitems are sorted by their scores and the best is re-turned by the decoder (or the k best in the case ofMERT).
In our experiments, we set G = 4000.The combined strategy yields average decodingtimes in the range of 30 seconds per sentence, whichis comparable to other syntax-based MT systems.6 TrainingFor tuning the coarse and fine parameters, we useminimum error rate training (MERT; Och, 2003) ina procedure shown as Alg.
2.
We first use MERT totrain parameters for the coarse phrase-based modelused to generate phrase lattices.
Then, after gener-ating the lattices, we prune them and run MERT asecond time to tune parameters of the fine model,which includes all phrase-based and QPDG param-eters.
The arguments to MERT are a vector of sourcesentences (or lattices), a vector of target sentences,the initial parameter values, the size of the k-bestlist, and finally the decoder.
We initialize ?
to thedefault Moses feature weights and for ?
we ini-tialize the two target phrase dependency weights to0.004, the two lexical dependency weights to 0.001,and the weights for all configuration features to 0.0.Our training procedure requires two executions ofMERT, and the second typically takes more itera-tions to converge (10 to 20 is typical) than the firstdue to the use of a larger feature set and increasedpossibility for search error due to the enlarged searchspace.7 ExperimentsFor experimental evaluation, we consider Chinese-to-English (ZH-EN) and Urdu-to-English (UR-EN) translation and compare our system toMoses (Koehn et al, 2007).
For ZH-EN, weused 303k sentence pairs from the FBIS corpus(LDC2003E14).
We segmented the Chinese datausing the Stanford Chinese segmenter in ?CTB?mode (Chang et al, 2008), giving us 7.9M Chinesewords and 9.4M English words.
For UR-EN, weused parallel data from the NIST MT08 evaluationconsisting of 1.2M Urdu words and 1.1M Englishwords.We trained a baseline Moses system using de-fault settings and features.
Word alignment wasperformed using GIZA++ (Och and Ney, 2003) inboth directions and the grow-diag-final-andheuristic was used to symmetrize the alignments.We used a max phrase length of 7 when extractingphrases.
Trigram language models were estimatedusing the SRI language modeling toolkit (Stolcke,2002) with modified Kneser-Ney smoothing (Chenand Goodman, 1998).
To estimate language modelsfor each language pair, we used the English side ofthe parallel corpus concatenated with 200M wordsof randomly-selected sentences from the Gigawordv4 corpus (excluding the NY Times and LA Times).We used this baseline Moses system to gener-ate phrase lattices for our system, so our model in-cludes all of the Moses features in addition to the480MT03 (tune) MT02 MT05 MT06 AverageMoses 33.84 33.35 31.81 28.82 31.33QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53)QPDG (TT+S2T+T2T) 34.98 (+1.14) 34.26 (+0.91) 32.34 (+0.53) 29.35 (+0.53) 31.98 (+0.65)Table 4: Chinese-English Results (% BLEU).QPDG features described in ?4.
In our experiments,we compare our QPDG system (lattice parsing oneach lattice) to the Moses baseline (finding the bestpath through each lattice).
The conventional wis-dom holds that hierarchical phrase-based transla-tion (Chiang, 2005) performs better than phrase-based translation for language pairs that requirelarge amounts of reordering, such as ZH-EN andUR-EN.
However, researchers have shown that thisperformance gap diminishes when using a larger dis-tortion limit (Zollmann et al, 2008) and may dis-appear entirely when using a lexicalized reorderingmodel (Lopez, 2008; Galley and Manning, 2010).So, we increase the Moses distortion limit from 6(the default) to 10 and use Moses?
default lexical-ized reordering model (Koehn et al, 2005).We parsed the Chinese text using the Stanfordparser (Levy and Manning, 2003) and the Englishtext using TurboParser (Martins et al, 2009).
Wenote that computing our features requires parsing thetarget (English) side of the parallel text, but not thesource side.
We only need to parse the source sideof the tuning and test sets, and the only features thatlook at the source-side parse are those from ?4.3.To obtain Brown clusters for the target tree fea-tures in ?4.1, we used code from Liang (2005).8We induced 100 clusters from the English side ofthe parallel corpus concatenated with 10M words ofrandomly-selected Gigaword sentences.
Only wordsthat appeared at least twice in this data were con-sidered during clustering.
An additional cluster wascreated for all other words; this allowed us to usephrase dependency cluster features even for out-of-vocabulary words.
We used a max phrase length of7 when extracting phrase dependencies to match themax phrase length used in phrase extraction.
Ap-proximately 87M unique phrase dependencies wereextracted from the ZH-EN data and 7M from theUR-EN data.We tuned the weights of our model using the pro-8http://www.cs.berkeley.edu/?pliang/softwareDev (tune) MT09Moses 24.21 23.56QPDG (TT+S2T) 24.94 (+0.73) 24.31 (+0.75)Table 5: Urdu-English Results (% BLEU).cedure described in ?6.
For ZH-EN we used MT03for tuning and MT02, MT05, and MT06 for test-ing.
For UR-EN we used half of the documents (882sentence pairs) from the MT08 test set for tuning(?Dev?)
and MT09 for testing.
We evaluated trans-lation output using case-insensitive IBM BLEU (Pa-pineni et al, 2001).7.1 ResultsResults for ZH-EN and UR-EN translation areshown in Tables 4 and 5.
We show results when us-ing only the target tree features from ?4.1 (TT), aswell as when adding the string-to-tree features from?4.2 (S2T) and the tree-to-tree features from ?4.3(T2T).
We note that T2T features are unavailable forUR-EN because we do not have an Urdu parser.
Wefind that we can achieve moderate but consistent im-provements over the baseline Moses system, for anaverage increase of 0.65 BLEU points for ZH-ENand 0.75 for UR-EN.Fig.
2 shows an example sentence from the MT05test set alng with its translation output and deriva-tions produced by Moses and our QPDG systemwith the full feature set.
This example shows thekind of improvements that our system makes.
InChinese, modifiers such as prepositional phrases andclauses are generally placed in front of the wordsthey modify, frequently the opposite of English.
Inaddition, Chinese occasionally uses postpositionswhere English uses prepositions.
The Chinese sen-tence in Fig.
2 exhibits both of these, as the preposi-tional phrase ?after the Palestinian election?
appearsbefore the verb ?strengthen?
in the Chinese sen-tence and ?after?
appears as a postposition.
Moses(Fig.
2(a)) does not properly reorder the preposi-tional phrase, while our system (Fig.
2(b)) properlyhandles both reorderings.9 We shall discuss these9Our system?s derivation is not perfect, in that ?in?
is incor-481k?nnen:cank?nnen:maysie:youes:it...vorbei:by$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can you deliver it by tomorrow morning ?can     you     deliver  it    by     tomorrow morning ?CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?...
... .........k?nnen:canliefern:deliversie:yousie:ites:it k?nnen:cank?nnen:canliefern:deliversie:youes:itvorbei:bymorgen:tomorrowmorgen:tomorrowliefern:deliveres:itvorbei:byfr?h:morning...es:itmorgen:tomorrowliefern:delivervorbei:byfr?h:morningfr?h:early?:?morgen:morningk?nnen:cank?nnen:maysie:youes:it...vorbei:by...
... .........k?nnen:canliefern:deliversie:yousie:ites:it k?nnen:cank?nnen:canliefern:deliversie:youes:itvorbei:bymorgen:tomorrowmorgen:tomorrowliefern:deliveres:itvorbei:byfr?h:morningfr?h:early?
:?morgen:morning...fr?h:morningmorgen:tomorrowmorgen:morningliefern:delivervorbei:by$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can     you     deliver  it    by     tomorrow morning ?$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?can     you     deliver  it    by     tomorrow morning ?bush   :   palestinian   presidential election   in   the  united states will   strengthen the peace  effortspalestine elections strengthen peace effortsafterbush will in:unitedstatesbush   : the united states will   strengthen   the   peace   efforts   after   the palestinian   election$$bush : us set to boost peace efforts after palestinian electionbush : us to step up peace efforts after palestinian electionsbush : u.s. will enhance peace efforts after palestinian electionus to boost peace efforts after palestinian elections : bushReferences(a)(b)(c)??
:    ?
?
?
????
?
?
??
??
????
:  ?
?
?
????
?
?
??
??
?
?Figure 2: (a) Moses translation output along with ?, ?, and a.
An English gloss is shown above the Chinese sentenceand above the gloss is shown the dependency parse from the Stanford parser.
(b) QPDG system output with additionalstructure ??.
(c) reference translations.types of improvements further in ?8.7.2 Unsupervised ParsingOur results thus far use supervised parsers for bothChinese and English, but parsers are only availablefor a small fraction of the languages we would liketo translate.
Fortunately, unsupervised dependencygrammar induction has improved substantially in re-cent years due to a flurry of recent research.
Whileattachment accuracies on standard treebank test setsare still relatively low, it may be the case that eventhough unsupervised parsers do not match treebankannotations very well, they may perform well whenused for extrinsic applications.
We believe thatsyntax-based MT offers a compelling platform fordevelopment and extrinsic evaluation of unsuper-vised parsers.In this paper, we use the standard dependencymodel with valence (DMV; Klein and Manning,2004).
When training is initialized using the out-put of a simpler, concave dependency model, therectly translated and reordered, but the system was nonethelessable to use it to improve the fluency of the output.DMV can approach state-of-the-art unsupervised ac-curacy (Gimpel and Smith, 2011).
For English, theresulting parser achieves 53.1% attachment accu-racy on Section 23 of the Penn Treebank (Marcus etal., 1993), which approaches the 55.7% accuracy ofa recent state-of-the-art unsupervised model (Blun-som and Cohn, 2010).
The Chinese parser, ini-tialized and trained the same way, achieves 44.4%,which is the highest reported accuracy on the Chi-nese Treebank (Xue et al, 2004) test set.Most unsupervised grammar induction modelsassume gold standard POS tags and sentencesstripped of punctuation.
We use the Stanford tag-ger (Toutanova et al, 2003) to obtain tags for bothEnglish and Chinese, parse the sentences withoutpunctuation using the DMV, and then attach punc-tuation tokens to the root word of the tree in a post-processing step.
For English, the predicted parentsagreed with those of TurboParser for 48.7% of thetokens in the corpus.We considered all four scenarios: supervised andunsupervised English parsing paired with supervisedand unsupervised Chinese parsing.
Table 6 shows482ENunsupervised supervisedZH unsupervised 31.18 (33.76) 31.86 (34.78)supervised 32.12 (34.74) 31.98 (34.98)Moses 31.33 (33.84)Table 6: Results when using unsupervised dependencyparsers.
Cells contain averaged % BLEU on the three testsets and % BLEU on tuning data (MT03) in parentheses.Feature Initial LearnedLeft child, same order 9.0 8.9Left child, swap phrases 1.1 0.0Right child, same order 7.3 7.3Right child, swap phrases 1.6 2.3Root-root 0.4 0.8Parent-child 4.2 6.1Child-parent 1.2 0.4Grandparent-grandchild 1.0 0.2Sibling 2.4 1.9C-command 6.1 6.7Other 1.5 0.9Table 7: Average feature values across best translationsof sentences in the MT03 tuning set, both before MERT(column 2) and after (column 3).
?Same?
versions of tree-to-tree configuration features are shown; the rarer ?swap?features showed a similar trend.BLEU scores averaged over the three test sets withtuning data BLEU in parentheses.
Surprisingly, weachieve our best results when using the unsupervisedEnglish parser in place of the supervised one (+0.79over Moses), while keeping the Chinese parser su-pervised.
Competitive performance is also foundby using the unsupervised Chinese parser and super-vised English parser (+0.53 over Moses).However, when using unsupervised parsers forboth languages, performance was below that ofMoses.
During tuning for this configuration, wefound that MERT struggled to find good parameterestimates, typically converging to suboptimal solu-tions after a small number of iterations.
We believethis is due to the large number of features (37), thenoise in the parse trees, and known instabilities ofMERT.
In future work we plan to experiment withtraining algorithms that are more stable and that canhandle larger numbers of features.8 AnalysisTo understand what our model learns during MERtraining, we computed the feature vectors of the bestderivation for each sentence in the tuning data atboth the start and end of tuning.
Table 7 showsthese feature values averaged across all tuning sen-tences.
The first four features are the configurationsfrom Fig.
1, in order from left to right.
From theserows, we can observe that the model learns to en-courage swapping when generating right childrenand penalize swapping for left children.
In addi-tion to objects, right children in English are oftenprepositional phrases, relative clauses, or other mod-ifiers; as we noted above, Chinese generally placesthese modifiers before their heads, requiring reorder-ing during translation.
Here the model appears to belearning this reordering behavior.From the second set of features, we see that themodel learns to favor producing dependency treesthat are mostly isomorphic to the source tree, by fa-voring root-root and parent-child configurations atthe expense of most others.9 DiscussionIn looking at BLEU score differences between thetwo systems, the unigram precisions were typicallyequal or only slightly different, while precisions forhigher-order n-grams contained the bulk of the im-provement.
This suggests that our system is notfinding substantially better translations for individ-ual words in the input, but rather is focused on re-ordering the existing translations.
This is not sur-prising given our choice of features, which focus onsyntactic language modeling and syntax-based re-ordering.
The obvious next step for our frameworkis to include bilingual rules that include source syn-tax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides.
Our framework al-lows integrating together all of these and other typesof structures, with the ultimate goal of combiningthe strengths of multiple approaches to translationin a single model.AcknowledgmentsWe thank Chris Dyer and the anonymous reviewers forhelpful comments that improved this paper.
This researchwas supported in part by the NSF through grant IIS-0844507, the U. S. Army Research Laboratory and theU.
S. Army Research Office under contract/grant numberW911NF-10-1-0533, and Sandia National Laboratories(fellowship to K. Gimpel).483ReferencesM.
Auli, A. Lopez, H. Hoang, and P. Koehn.
2009.
Asystematic analysis of translation model search spaces.In Proceedings of the Fourth Workshop on StatisticalMachine Translation.S.
Bergsma and C. Cherry.
2010.
Fast and accurate arcfiltering for dependency parsing.
In Proc.
of COLING.P.
Blunsom and T. Cohn.
2010.
Unsupervised inductionof tree substitution grammars for dependency parsing.In Proc.
of EMNLP.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-based n-gram mod-els of natural language.
Computational Linguistics,18.X.
Carreras and M. Collins.
2009.
Non-projective pars-ing for statistical machine translation.
In Proc.
ofEMNLP.P.
Chang, M. Galley, and C. Manning.
2008.
Optimiz-ing Chinese word segmentation for machine transla-tion performance.
In Proc.
of the Third Workshop onStatistical Machine Translation.S.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal report 10-98, Harvard University.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL.D.
Das and N. A. Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.
InProc.
of ACL-IJCNLP.S.
DeNeefe, K. Knight, W. Wang, and D. Marcu.
2007.What can syntax-based MT learn from phrase-basedMT?
In Proc.
of EMNLP-CoNLL.J.
DeNero, S. Kumar, C. Chelba, and F. J. Och.
2010.Model combination for machine translation.
In Proc.of NAACL.M.
Dymetman and N. Cancedda.
2010.
Intersecting hi-erarchical and phrase-based models of translation.
for-mal aspects and algorithms.
In Proc.
of SSST-4.J.
Eisner, E. Goldlust, and N. A. Smith.
2005.
Com-piling Comp Ling: Practical weighted dynamic pro-gramming and the Dyna language.
In Proc.
of HLT-EMNLP.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
of COL-ING.R.
W. Floyd.
1962.
Algorithm 97: Shortest path.
Com-munications of the ACM, 5(6).M.
Galley and C. D. Manning.
2009.
Quadratic-timedependency parsing for machine translation.
In Proc.of ACL-IJCNLP.M.
Galley and C. D. Manning.
2010.
Accurate non-hierarchical phrase-based translation.
In Proc.
ofNAACL.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference andtraining of context-rich syntactic translation models.In Proc.
of COLING-ACL.K.
Gimpel and N. A. Smith.
2009.
Feature-rich transla-tion by quasi-synchronous lattice parsing.
In Proc.
ofEMNLP.K.
Gimpel and N. A. Smith.
2011.
Concavity and initial-ization for unsupervised dependency grammar induc-tion.
Technical report, Carnegie Mellon University.L.
Huang, K. Knight, and A. Joshi.
2006.
Statisticalsyntax-directed translation with extended domain oflocality.
In Proc.
of AMTA.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In Proc.
of ACL.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL.P.
Koehn, A. Axelrod, A. Birch Mayne, C. Callison-Burch, M. Osborne, and D. Talbot.
2005.
Edinburghsystem description for the 2005 iwslt speech transla-tion evaluation.
In Proc.
of IWSLT.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL (demosession).R.
Levy and C. D. Manning.
2003.
Is it harder to parsechinese, or the chinese treebank?
In Proc.
of ACL.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.Y.
Liu, H. Mi, Y. Feng, and Q. Liu.
2009.
Joint de-coding with multiple translation models.
In Proc.
ofACL-IJCNLP.A.
Lopez.
2008.
Tera-scale translation models via pat-tern matching.
In Proc.
of COLING.W.
Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008.Lattice-based minimum error rate training for statisti-cal machine translation.
In EMNLP.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19:313?330.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009.Concise integer linear programming formulations fordependency parsing.
In Proc.
of ACL.M.-J.
Nederhof.
2003.
Weighted deductive parsing andknuth?s algorithm.
Computational Linguistics, 29(1).F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of ACL.484F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In Proc.
of ACL.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.S.
Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of California atBerkeley.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proc.
of ACL.L.
Shen, J. Xu, and R. Weischedel.
2008.
A new string-to-dependency machine translation algorithm with atarget dependency language model.
In Proc.
of ACL.A.
Sixtus and S. Ortmanns.
1999.
High quality wordgraphs using forward-backward pruning.
In Proc.
ofthe IEEE Int.
Conf.
on Acoustics, Speech and SignalProcessing.D.
A. Smith and J. Eisner.
2006.
Quasi-synchronousgrammars: Alignment by soft projection of syntacticdependencies.
In Proc.
of HLT-NAACL Workshop onStatistical Machine Translation.D.
A. Smith and J. Eisner.
2009.
Parser adaptation andprojection with quasi-synchronous features.
In Proc.of EMNLP.A.
Stolcke.
2002.
SRILM?an extensible language mod-eling toolkit.
In Proc.
of ICSLP.K.
Toutanova, D. Klein, C. D. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proc.
of HLT-NAACL.R.
Tromble, S. Kumar, F. Och, and W. Macherey.
2008.Lattice Minimum Bayes-Risk decoding for statisticalmachine translation.
In EMNLP.N.
Ueffing, F. J. Och, and H. Ney.
2002.
Generation ofword graphs in statistical machine translation.
In Proc.of EMNLP.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
a quasi-synchronous grammarfor QA.
In Proc.
of EMNLP-CoNLL.Y.
Wu, Q. Zhang, X. Huang, and L. Wu.
2009.
Phrasedependency parsing for opinion mining.
In Proc.
ofEMNLP.N.
Xue, F. Xia, F.-D. Chiou, and M. Palmer.
2004.
ThePenn Chinese Treebank: Phrase structure annotationof a large corpus.
Natural Language Engineering,10(4):1?30.A.
Zollmann and A. Venugopal.
2006.
Syntax aug-mented machine translation via chart parsing.
InProc.
of NAACL 2006 Workshop on Statistical Ma-chine Translation.A.
Zollmann, A. Venugopal, F. J. Och, and J. Ponte.2008.
A systematic comparison of phrase-based, hi-erarchical and syntax-augmented statistical MT.
InProc.
of COLING.485
