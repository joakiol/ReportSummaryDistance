Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 493?498,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCross-Lingual Word Representations via Spectral Graph EmbeddingsTakamasa Oshikiri, Kazuki Fukui, Hidetoshi ShimodairaDivision of Mathematical Science, Graduate School of Engineering ScienceOsaka University, Japan1-3 Machikaneyama-cho, Toyonaka, Osaka{oshikiri, fukui, shimo}@sigmath.es.osaka-u.ac.jpAbstractCross-lingual word embeddings are usedfor cross-lingual information retrieval ordomain adaptations.
In this paper, weextend Eigenwords, spectral monolin-gual word embeddings based on canoni-cal correlation analysis (CCA), to cross-lingual settings with sentence-alignment.For incorporating cross-lingual informa-tion, CCA is replaced with its general-ization based on the spectral graph em-beddings.
The proposed method, whichwe refer to as Cross-Lingual Eigenwords(CL-Eigenwords), is fast and scalable forcomputing distributed representations ofwords via eigenvalue decomposition.
Nu-merical experiments of English-Spanishword translation tasks show that CL-Eigenwords is competitive with state-of-the-art cross-lingual word embeddingmethods.1 IntroductionThere have been many methods proposed for wordembeddings.
Neural network based models arepopular, and one of the most major approachesis the skip-gram model (Mikolov et al, 2013b),and some extended methods have also been devel-oped (Levy and Goldberg, 2014a; Lazaridou et al,2015).
The skip-gram model has many interest-ing syntactic and semantic properties, and it canbe seen as the factorization of a word-context ma-trix whose elements represent pointwise mutualinformation (Levy and Goldberg, 2014b).
How-ever, word embeddings based on neural networks(without neat implementation) can be very slowin general, and it is sometimes difficult to under-stand how they work.
Recently, a simple spectralmethod, called Eigenwords, for word embeddings+ +++++++++++++++?10 ?5 0 5 10?6?4?202468PC1PC2italiasueciagreciaaustriafranciafinlandiaitalyswedengreeceaustriafrancefinlandromaestocolmoatenasvienapar?shelsinkiromestockholmathensviennaparishelsinkiFigure 1: PCA projections (PC1 and PC2) of CL-Eigenwords of countries (bold) and its capitals(italic) in Spanish (red) and English (blue).
Wordvectors of the two languages match quite well,although they are computed using sentence-levelalignment without knowing word-level alignment.100-dim word representations are used for PCAcomputation.is proposed (Dhillon et al, 2012; Dhillon et al,2015).
It is based on canonical correlation anal-ysis (CCA) for computing word vectors by maxi-mizing correlations between words and their con-texts.
Eigenword algorithms are fast and scalable,yet giving good performance comparable to neuralnetwork approaches for capturing the meaning ofwords from their context.The skip-gram model, originally proposed formonolingual corpora, has been extended to cross-lingual settings.
Given two vector representa-tions of two languages, a linear transformation be-tween the two spaces is trained from a set of wordpairs for translation task (Mikolov et al, 2013a),while other researchers use CCA for learning lin-ear projections to a common vector space where493translation pairs are strongly correlated (Faruquiand Dyer, 2014).
These methods require word-alignment in the training data, while some multi-lingual corpora have only coarse information suchas a set of sentence pairs or paragraph pairs.
Re-cently, extensions of the skip-gram model requir-ing only sentence-alignment have been developedby introducing cross-lingual losses in the objectiveof the original models (Gouws et al, 2015; Coul-mance et al, 2015; Shi et al, 2015).In this paper, instead of the skip-gram model,we extend Eigenwords (Dhillon et al, 2015)to cross-lingual settings with sentence-alignment.Our main idea is to replace CCA, which is applica-ble to only two different kinds of data, with a gen-eralized method (Nori et al, 2012; Shimodaira,2016) based on spectral graph embeddings (Yanet al, 2007) so that the Eigenwords can dealwith two or more languages for cross-lingual wordembeddings.
Our proposed method, referred toas Cross-Lingual Eigenwords (CL-Eigenwords),requires only sentence-alignment for capturingcross-lingual relationships.
The method is verysimple in mathematics as well as computation; itinvolves a generalized eigenvalue problem, whichcan be solved by fast and scalable algorithmssuch as the randomized eigenvalue decomposi-tion (Halko et al, 2011).Fig.
1 shows an illustrative example of cross-lingual word vectors obtained by CL-Eigenwords.Although only sentence-alignment is available inthe corpus, word-level translation is automaticallycaptured in the vector representations; the samewords (countries and capitals) in the two lan-guages are placed in close proximity to each other;greece is close to grecia and rome is close to roma.In addition, the same kinds of relationships be-tween word pairs share similar directions in thevector space; the direction from sweden to stock-holm is nearly parallel to the direction from finlandto helsinki.We evaluate the word vectors obtained byour method on the English-Spanish cross-lingualtranslation task and compare the results with thoseof state-of-the-art methods, showing that our pro-posed method is competitive with those existingmethods.
We use Europarl corpus for learning thevector representation of words.
Although the ex-periments in this paper are conducted using bilin-gual corpus, our method can be easily applied tothree or more languages.2 Eigenwords (One Step CCA)CCA (Hotelling, 1936) is a multivariate analysismethod for finding optimal projections of two setsof data vectors by maximizing the correlations.Applying CCA to pairs of raw word vector andraw context vector, Eigenword algorithms attemptto find low dimensional vector representations ofwords (Dhillon et al, 2012).
Here we explain thesimplest version of Eigenwords called One StepCCA (OSCCA).We have monolingual corpus consisting of T to-kens; (ti)i=1,...,T, and the vocabulary consistingof V word types; {vi}i=1,...,V. Each token tiisdrawn from this vocabulary.
We define word ma-trixV ?
{0, 1}T?Vwhose i-th row encodes tokentiby 1-of-V representation; the j-th element is 1if the word type of tiis vj, 0 otherwise.Let h be the size of context window.
We de-fine context matrix C ?
{0, 1}T?2hVwhose i-throw represents the surrounding context of tokentiwith concatenated 1-of-V encoded vectors of(ti?h, .
.
.
, ti?1, ti+1, .
.
.
, ti+h).We apply CCA to T pairs of row vectors of Vand C. The objective function of CCA is con-structed using V?V, V?C, C?C which rep-resent occurrence and co-occurrence counts ofwords and contexts.
In Eigenwords, however, weuse CV V?
RV?V+, CV C?
RV?2hV+, CCC?R2hV?2hV+with the following preprocessing ofthese matrices before constructing the objectivefunction.
First, centering-process of V and C isomitted, and off-diagonal elements of C?C areignored for simplifying the computation of in-verse matrices.
Second, we take the square rootof the elements of these matrices for ?squash-ing?
the heavy-tailed word count distributions.
Fi-nally, we obtain vector representations of wordsas C?1/2V V(u1, .
.
.
,uK), where u1, .
.
.
,uK?
RVare left singular vectors of C?1/2V VCV CC?1/2CCcor-responding to the K largest singular values.
Thecomputation of SVD is fast and scalable usingrecent idea of random projections (Halko et al,2011).3 Cross-Lingual EigenwordsIn this section, we introduce Cross-LingualEigenwords (CL-Eigenwords), a novel methodfor cross-lingual word embeddings.
Supposethat we have parallel corpora that contain L lan-guages.
Schematic diagrams of Eigenwords and494Figure 2: Eigenwords are CCA-based spectralmonolingual word embeddings.
CL-Eigenwordsare CDMCA-based spectral cross-lingual wordembeddings, where the two (or more) languagesare linked by sentence-alignment.CL-Eigenwords (with L = 2) are shown in Fig.
2.In the same way as the monolingual Eigen-words, we denote the word matrix and the contextmatrix for ?-th language byV(?)?
RT(?)?V(?)+andC(?)?
RT(?)?2h(?)V(?
)+respectively, where V(?
)isthe size of vocabulary, T(?
)is the number of to-kens, and h(?
)is the size of context window.
Thereare D sentences (or paragraphs) in the multilin-gual corpora, and each token is included in oneof the sentences.
The sentence-alignment is repre-sented in the matrix J(?)?
RT(?
)?D+whose (i, j)-element J(?
)i,jis set to 1 if the i-th token t(?
)iof ?-thlanguage corpus comes from the j-th sentence or0 otherwise.
We also define document matrix Dwhose j-th row encodes j-th sentence by 1-of-Drepresentation; D = ID, where IDrepresents D-dimensional identity matrix.The goal of CL-Eigenwords is to construct vec-tor representations of words of two (or more)languages from multilingual corpora at the sametime.
This problem is formulated as an exampleof Cross-Domain Matching Correlation Analysis(CDMCA) (Shimodaira, 2016), which deals withmany-to-many relationships between data vectorsfrom multiple sources.
CDMCA is based onthe spectral graph embeddings (Yan et al, 2007),and attempts to find optimal linear projections ofdata vectors so that associated transformed vec-tors are placed in close proximity to each other.The strength of association between two vectorsis specified by a nonnegative real value calledmatching weight.
Since CDMCA includes CCAand a variant of Latent Semantic Indexing (LSI)(Deerwester et al, 1990) as special cases, CL-Eigenwords can be interpreted as LSI-equippedEigenwords (See Appendix).In CL-Eigenwords, the data vectors are givenas v(?
)i, c(?
)i,di, namely, the i-th row vectors ofV(?),C(?
),D, respectively.
The matching weightsbetween row vectors of V(?
)and C(?
)are speci-fied by the identity matrix IT(?
)because the datavectors are in one-to-one correspondence.
On theother hand, the matching weights between rowvectors of V(?
)and D as well as those betweenC(?
)and D are specified by?J(?
)= b(?)J(?
), thesentence-alignment matrix multiplied by a con-stant b(?).
Then we will find linear transformationmatrices A(?)V,A(?
)C,AD, (?
= 1, 2, .
.
.
, L) to K-dimensional vector space by minimizing the ob-jective functionL??=1T(?)?i=1?v(?)iA(?)V?
c(?)iA(?)C?22+L??=1T(?)?i=1D?j=1?J(?)i,j?v(?)iA(?)V?
djAD?22+L??=1T(?)?i=1D?j=1?J(?)i,j?c(?)iA(?)C?
djAD?22(1)with a scale constraint for projection matrices.Note that the first term in (1) is equivalent to thatof CCA between words and contexts, namely theobjective of monolingual Eigenwords, and there-fore word vectors of two languages are obtainedas row vectors ofA(?)V(?
= 1, 2, .
.
.
, L).Hereafter, we assume L = 2 for notational sim-plicity.
A generalization to the case L > 2 isstraightforward; redefine X, W, A below by re-peating the submatrices, such asV(?)andC(?
), forL times.
For solving the optimization problem, wedefineX =???
?V(1)O O O OO C(1)O O OO O V(2)O OO O O C(2)OO O O O D???
?,W =?????
?O IT(1)O O?J(1)IT(1)O O O?J(1)O O O IT(2)?J(2)O O IT(2)O?J(2)?J(1)??J(1)??J(2)??J(2)?O?????
?,A?= (A(1)?V,A(1)?C,A(2)?V,A(2)?C,A?D).4951 ?
1000 1 ?
1000 5001 ?
6000 5001 ?
6000es?
en en?
es es?
en en?
esMethod Time [min] P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5Edit distance - 29.1 37.8 20.6 34.4 28.5 40.0 26.4 33.5BilBOWA (40 dim.
)?4.6 46.7 59.6 43.6 56.4 44.6 53.6 49.4 58.7BilBOWA (100 dim.
)?7.5 43.3 55.9 36.8 49.0 43.6 53.3 48.6 57.9BilBOWA (200 dim.
)?11.6 38.8 52.2 29.7 43.2 43.3 52.0 47.3 57.2CL-LSI (40 dim.)
1.4 45.9 54.8 46.9 55.8 31.6 38.5 40.7 45.1CL-LSI (100 dim.)
2.4 51.7 62.9 48.5 61.8 41.6 49.8 42.8 49.1CL-LSI (200 dim.)
5.1 55.2 66.5 50.7 65.5 45.5 54.7 45.6 51.9CL-Eigenwords (40 dim.)
9.5 54.7 66.2 53.3 65.7 40.3 49.2 44.7 50.0CL-Eigenwords (100 dim.)
19.6 57.7 71.3 54.9 70.3 47.9 59.0 49.3 54.6CL-Eigenwords (200 dim.)
37.5 58.7 72.4 56.2 72.2 51.6 62.4 50.6 55.7Table 1: Computational times (in minutes) and word translation accuracies (in percent, higher is better)evaluated by Precision@n using the 1,000 test words (the 1st to 1,000th most frequent words or the5,001st to 6,000th most frequent words).
Shown are for Spanish (es) to English (en) translation andfor English (en) to Spanish (es) translation.
?BilBOWA is executed on 3 threads, while CL-LSI andCL-Eigenwords are executed on a single thread.Also define H = X?WX, G = X?MX, M =diag(W1).
Then the optimization problem (1)is equivalent to maximizing Tr(A?HA) with ascale constraint A?GA = IK.
Following theEigenwords implementation (Dhillon et al, 2015),we replace H,G with H,G by ignoring the non-diagonal elements ofG and taking the square rootof elements in H,G.
The optimization problemis solved as a generalized eigenvalue problem, andthe word representations, as well as those for con-texts and sentences, are obtained as row vectorsof?A = G?1/2(u1, .
.
.
,uK), where u1, .
.
.
,uKare eigenvectors of (G?1/2)?HG?1/2for the Klargest eigenvalues.
We choose K so that all theK eigenvalues are positive.
As in the case ofmonolingual Eigenwords, we can exploit fast im-plementations such as the randomized eigenvaluedecomposition (Halko et al, 2011); our compu-tation in the experiments is only approximationbased on the low-rank factorization with rank 2K.For measuring similarities between two wordvectors x,y ?
RK, we use the weighted cosinesimilaritysim(x,y) = (?x?2?
?y?2)?1K?i=1?ixiyi,where ?iis the i-th largest eigenvalue.4 ExperimentsThe implementation of our method is available onGitHub1.
Following the previous works (Mikolovet al, 2013a; Gouws et al, 2015), we use only1https://github.com/shimo-lab/kadingirthe first 500K lines of English-Spanish sentence-aligned parallel corpus of Europarl (Koehn, 2005)for numerical experiments.4.1 Word Translation TasksExperiments are performed in similar settingsas the previous works based on the skip-grammodel (Mikolov et al, 2013a; Gouws et al, 2015).We extract 1,000 test words with frequency rank1?1000 or 5001?6000 from the source language,and translate these words to the target languageusing Google Translate, assuming they are the cor-rect translations.
Then, we evaluate the transla-tion accuracies of each method with precision@nas the fraction of correct translations for the testwords being in the top-n words of the target lan-guage returned by each method.4.2 Baseline SystemsWe compare CL-Eigenwords with the followingthree methods.Edit distance Finding the nearest words mea-sured by Levenshtein distance.CL-LSI Cross-Language LSI (CL-LSI) (Littmanet al, 1998) is not originally for word embed-dings.
However, since this method can be used forcross-lingual information retrieval, we select it asone of our baselines.
For each language, we con-struct the term-document matrix of size V(?)?
Dwhose (i, j)-element represents the frequency ofi-th word in j-th sentence.
Then LSI is applied tothe concatenated matrix of size (V(1)+V(2))?D.BilBOWA BilBOWA (Gouws et al, 2015) is oneof the state-of-the-art methods for cross-lingual496word embeddings based on the skip-gram model.We obtain vector representations of words usingpublicly available implementation.24.3 ResultsIn CL-Eigenwords, vocabulary size V(1)=V(2)= 104, window size h(1)= h(2)= 2, theconstant b(1)= b(2)= 103.
The dimensional-ity of vector representations is K = 40, 100, or200.
Similarities of two vector representationsare measured by the unweighted cosine similar-ity in CL-LSI and BilBOWA.
Our experimentswere performed on a CentOS 7.2 server with In-tel Xeon E5-2680 v3 CPU, 256GB of RAM andgcc 4.8.5.
The computation times and the resultaccuracies of word translation tasks are shown inTable 1.
We observe that CL-Eigenwords is com-petitive with BilBOWA and CL-LSI.
In particu-lar, CL-Eigenwords performed very well for themost frequent words (ranks 1?1000) in this par-ticular parameter setting.
Furthermore, the com-putation times of CL-Eigenwords are as short asthose of BilBOWA for achieving similar accura-cies.
Preliminary experiments also suggest thatCL-Eigenwords works well for semi-supervisedlearning where sentence-alignment is specifiedonly partially; the word translation accuracies aremaintained well with aligned 240K lines and un-aligned 260K lines.5 ConclusionWe proposed CL-Eigenwords for incorporatingcross-lingual information into the monolingualEigenwords.
Although our method is simple, ex-perimental results of English-Spanish word trans-lation tasks show that the proposed method is com-petitive with other state-of-the-art cross-lingualmethods.AcknowledgmentsThis work was partially supported by grants fromJapan Society for the Promotion of Science KAK-ENHI (24300106, 16H01547 and 16H02789) toHS.AppendixIn this Appendix, we discuss the relationships be-tween CL-LSI and CL-Eigenwords.2https://github.com/gouwsmeister/bilbowaFigure 3: Cross-Language Latent Semantic Index-ing (CL-LSI) does not use the context information.LetV(1),V(2),D,J(1),J(2)be those defined inSection 3.
In CL-LSI, we consider the truncatedsingular value decomposition of a word-documentmatrixB =(V(1)?J(1)V(2)?J(2))?
AV?KA?Dusing the largestK singular values.
Then row vec-tors ofAVare the vector representations of wordsof CL-LSI.CL-LSI can also be interpreted as an eigenvaluedecomposition ofH = X?WX whereX =(V(1)O OO V(2)OO O D),W =(O O J(1)O O J(2)J(1)?J(2)?O)are redefined from those in Section 3 by remov-ing submatrices related to contexts.
The structureof X and W is illustrated in Fig.
3.
Similarly toCL-Eigenwords of Section 3, but ignoring G, wedefine A = (u1, .
.
.
,uK) with the eigenvectorsofH for the largest K eigenvalues ?1, .
.
.
, ?K.
Itthen follows fromH =(O BB?O)that A?= 2?1/2(A?V,A?D) with the same AVand ADobtained by the truncated singular valuedecomposition.
The eigenvalues are the same asthe singular values: diag(?1, .
.
.
, ?K) = ?K.Therefore CL-LSI is interpreted as a variant ofCL-Eigenwords without the context information.ReferencesJocelyn Coulmance, Jean-Marc Marty, GuillaumeWenzek, and Amine Benhalloum.
2015.
Trans-gram, fast cross-lingual word-embeddings.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1109?1113, Lisbon, Portugal, September.
Association forComputational Linguistics.497Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American society for information science,41(6):391.Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,and Lyle H. Ungar.
2012.
Two step cca: Anew spectral method for estimating vector modelsof words.
In John Langford and Joelle Pineau, ed-itors, Proceedings of the 29th International Confer-ence on Machine Learning (ICML-12), ICML ?12,pages 1551?1558, New York, NY, USA, July.
Om-nipress.Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-gar.
2015.
Eigenwords: Spectral word embeddings.Journal of Machine Learning Research, 16:3035?3078.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proceedings of the 14th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 462?471, Gothenburg,Sweden, April.
Association for Computational Lin-guistics.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
Bilbowa: Fast bilingual distributed represen-tations without word alignments.
In Proceedingsof the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015,pages 748?756.Nathan Halko, Per-Gunnar Martinsson, and Joel A.Tropp.
2011.
Finding structure with random-ness: Probabilistic algorithms for constructing ap-proximate matrix decompositions.
SIAM review,53(2):217?288.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit, pages 79?86, Phuket, Thailand.
AAMT.Angeliki Lazaridou, The Nghia Pham, and Marco Ba-roni.
2015.
Combining language and vision with amultimodal skip-gram model.
In Proceedings of the2015 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 153?163.
Asso-ciation for Computational Linguistics.Omer Levy and Yoav Goldberg.
2014a.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), pages302?308.
Association for Computational Linguis-tics.Omer Levy and Yoav Goldberg.
2014b.
Neural wordembedding as implicit matrix factorization.
In Ad-vances in Neural Information Processing Systems,pages 2177?2185.Michael L. Littman, Susan T. Dumais, and Thomas K.Landauer.
1998.
Automatic cross-language infor-mation retrieval using latent semantic indexing.
InCross-language information retrieval, pages 51?62.Springer.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.2013a.
Exploiting similarities among lan-guages for machine translation.
arXiv preprintarXiv:1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeff Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in neural information processingsystems, pages 3111?3119.Nozomi Nori, Danushka Bollegala, and HisashiKashima.
2012.
Multinomial relation predictionin social data: A dimension reduction approach.
InAAAI, volume 12, pages 115?121.Tianze Shi, Zhiyuan Liu, Yang Liu, and MaosongSun.
2015.
Learning cross-lingual word embed-dings via matrix co-factorization.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Volume 2: Short Papers), pages 567?572, Beijing,China, July.
Association for Computational Linguis-tics.Hidetoshi Shimodaira.
2016.
Cross-validation ofmatching correlation analysis by resampling match-ing weights.
Neural Networks, 75:126?140.Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-JiangZhang, Qiang Yang, and Stephen Lin.
2007.
Graphembedding and extensions: A general framework fordimensionality reduction.
Pattern Analysis and Ma-chine Intelligence, IEEE Transactions on, 29(1):40?51, Jan.498
