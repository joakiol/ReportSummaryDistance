POSBIOTM-NER in the shared task of BioNLP/NLPBA 2004Yu Song, Eunju Kim, Gary Geunbae Lee, Byoung-kee YiDepartment of CSE,Pohang University of Science and Technology (POSTECH)Pohang, Korea 790-784{songyu, hosuabi, gblee, bkyi} @postech.ac.krAbstractTwo classifiers -- Support Vector Machine(SVM) and Conditional Random Fields (CRFs) areapplied here for the recognition of biomedicalnamed entities.
According to their differentcharacteristics, the results of two classifiers aremerged to achieve better performance.
We proposean automatic corpus expansion method for SVMand CRF to overcome the shortage of the annotatedtraining data.
In addition, we incorporate akeyword-based post-processing step to deal withthe remaining problems such as assigning anappropriate named entity tag to the word/phrasecontaining parentheses.1 IntroductionRecently, with the rapid growth in the number ofpublished papers in biomedical domain, many NLP(Natural Language Processing) researchers havebeen interested in a task of automatic extraction offacts from biomedical articles.
The first andfundamental step is to extract the named entities.And recently several SVM-based named entityrecognition models have been proposed.
Lee et.
al.
([Lee et.
al., 2003]) proposed a two-phrase SVMrecognition model.
Yamamoto et.
al.
([Yamamotoet.
al., 2003]) proposed a SVM-based recognitionmethod which uses various morphologicalinformation and input features such as base nounphrase information, stemmed forms of a word, etc.However, notable limitation of SVM is its lowspeed both for training and recognition.On the other hand, conditional random fields(CRFs) ([Lafferty, 2001]) is a probabilisticframework for labelling and segmenting sequentialdata, which is much faster  comparing with SVM.The conditional probability of the label sequencecan depend on arbitrary, non-independent featuresof the observation sequence without forcing themodel to account for the distribution of thosedependencies.
Named entity recognition problemcan be taken as assigning the named entity classtag sequences to the input sentences.
We adoptCRF to be the complementary scheme of SVM.In natural language processing, supervisedmachine-learning based approach is a kind ofstandard and its efficiency is proven in various taskfields.
However, the most problematic point ofsupervised learning methods is that the size oftraining data is essential to achieve goodperformance, but building a training corpus byhuman labeling is time consuming, labor intensive,and expensive.
To overcome this problem, variousattempts have been proposed to acquire a trainingdata set in an easy and fast way.
Some approachesfocus on minimally-supervised style learning andsome approaches try to expand or acquire thetraining data automatically or semi-automatically.Using virtual examples, i.e., artificially createdexamples, is a type of method to expand thetraining data in an automatic way ([Niyogi et al1998] [Sasano, 2003] [Scholkopf et.
al., 1996].
Inthis paper, we propose an automatic corpusexpansion method both for SVM and CRF basedbiological named entity recognition using virtualexample idea.The remainder of this paper is organized asfollows: Section 2 introduces named entityrecognition (NER) part: two machine learningapproaches with some justification, feature setused in NER and virtual examples generation.
Insection 3, we present some keyword-based post-processing methods.
The experiment results andanalysis will be presented in section 4.
Finally,conclusion is provided in section 5.2 Named Entity RecognitionThe training corpus is provided in IOB notion.The IOB notation is used where named entities arenot nested and therefore do not overlap.
Wordsoutside of named entities are tagged with ?O?,while the first word in a named entity is taggedwith B-[entity class], and further named entitywords receive tag I-[entity class] for inside.
Wedefine the named entity recognition problem as aclassification problem, assigning an appropriateclassification tag for each token in the inputsentences.To simplify the classification problem, we assigneach token only with I-[entity class]/O.
Then weconvert the tag of the initial token of a consecutive100sequence of predicted named entity tokens to B-[entity class].2.1 SVMSupport Vector Machine (SVM) is a well-knownmachine learning technique showing a goodperformance in several classification problems.However, SVM has suffered from low speed andunbalanced distributed data.Named entity token is a compound token thatconsists of the constituents of some other namedentities, and all other un-related tokens areconsidered as outside tokens.
Due to thecharacteristics of SVM, this unbalanceddistribution of training data can cause a drop-off inclassification coverage.In order to resolve this low coverage and lowspeed problem together, we filter out possibleoutside tokens in the training data through twosteps.
First, we eliminate tokens that are notconstituents of a base noun phrase, assuming thatevery named entity token should be inside of abase noun phrase boundary.
Second, we excludesome tokens according to their part-of-speech tags.We build a stop-part-of-speech tag list bycollecting tags which have a small chance of beinga named entity token, such as predeterminer,determiner, etc.2.2 CRFConditional random fields (CRFs) ([Wallach,2004] is a probabilistic framework for labellingand segmenting a sequential data.
Let ),( EVG bea graph such that VvYvY ?= )( , and there is anode Vv?
corresponding to each of the randomvariable representing an element Yv of Y .
Then),( YX  is a conditional random field, and whenconditioned on X  , the random variables Yv  obeythe Markov property with respect to the graph:),~,,|(),,||( vwYwXYvpvwYwXYvp =?where vw ~  means that w and v are neighbours inG.Let X  and Y  be jointly distributed randomvariables respectively representing observationsequences and corresponding label sequences.
ACRF is an undirected graphical model, globallyconditioned on X (the observation sequence).We try to use this CRF model to our NER as acomplementary method for both speed andcoverage.
SVM predicts the named entities basedon feature information of words collected in apredefined window size while CRF predicts thembased on the information of the whole sentence.So, CRF can handle the named entities withoutside tokens which SVM always tags as ?O?.2.3 Feature setAs an input to the classifier, we use a bit-vectorrepresentation, each dimension of which indicateswhether the input matches with the correspondingfeature.The followings are the basic input features:z Surface word - only in the case that theprevious/current/next words are in thesurface word dictionary.z word feature - orthographical feature ofthe previous/current/next words.z prefix/suffix - prefixes/suffixes which arecontained in the current word among theentries in the prefix/suffix dictionary.z part-of-speech tag - POS tag of theprevious/current/next words.z Base noun phrase tag - base noun tag ofthe previous/current/next words.z previous named entity tag - named entitytag which is assigned for previous word.This feature is only for SVM.The surface word dictionary is constructed fromthe words that occur more than one time in thetraining part of the corpus.2.4 Automatic Corpus Expansion usingVirtual ExamplesTo achieve good results in machine learningbased classification, it is important to use trainingdata which is sufficient not only in the quality butalso in the quantity.
But making the training databy hand requires considerable man-power andtakes a long time.
Expanding the training datausing virtual examples is an attempt for corpusexpansion in the biomedical domain.We expand the training data by augmentingthe set of virtual examples generated using someprior knowledge on the training data.
We use thefact that the syntactic role of a named entity is anoun and the basic syntactic structure of a sentenceis preserved if we replace a noun with anothernoun in the sentence.
Based on this linguisticparadigmatic relation, we can generate a newsentence by replacing each named entity byanother named entity which is in the named entitydictionary of the corresponding class.
Then weaugment the sentence into the original training data.If we apply this replacement processes n times foreach sentence in the original corpus, then we canobtain a virtual corpus about n+1 times bigger thanthe original one.
Since the virtual corpusstrengthens the right information which may not beobserved in the original corpus, it is helpful toextend the coverage of a recognition model and101also helpful to improve the recognitionperformance.3 Keyword based post-processingWe notice that some words occur morefrequently in the specific entity class.
For example,the word ?genes?
appears in class DNA 590 timeswhile in other entity class appears less than 10times.
The information provided by these keywords not only impacts the named entity predictionpart but also shows great power in post-processingpart.
Once keywords appear at specific position ina named entity, we can surely decide the entityclass of this named entity.3.1 Words containing parentheses or ?and?It is difficult but significant to decide whetherparentheses or ?and?
are part of named entity ornot.
Parentheses occur in the named entity morethan 700 times in the training data.
Both SVM andCRF cannot work well while dealing with thisproblem.Once a specific keyword appears at the right sideof ?
)?, we can tell that the parentheses belong to anamed entity.
The named entity tag informationcan also be determined by the keyword.
Forexample, in Table 1, the left column is the result ofthe NER module.
At post-processing stage, theword ?genes?
is detected on the right side of ?
)?,then this pair of parentheses and keyword ?genes?are included in the current named entity.Before Aftertext tag text tag(        O (  I-DNAVH      I-DNA VH       I-DNA)        O   )         I-DNAgenes    O genes      I-DNATable 1: An example for the usage ofkeywords.A keyword list for parentheses is collected fromthe training corpus, including the named entitytag information.
It not only solves theparentheses named entity tag problem but alsohelps to correct the wrong named entity assignedto the words between parentheses by the previousstep.
The word ?and?
can be treated similarly asthe parenthesis case.3.2 Correcting the wrong named entity tagSome keywords occur in one specific type ofnamed entities with high frequency.
We employthe information provided by those keywords incorrecting the wrongly assigned named entity tag.First a list of high frequency keywords withclass information is collected.
Once a keyword ispredicted as another type of named entity, all thewords in the current named entity boundary will becorrected as the corresponding named entity typeas the keyword.
For example, the keywords?protein?
and ?proteins?, in a very rare case,belong to other named entity class rather than theclass ?PROTEIN?.4 Experiment Result and analysis4.1 CorpusThe shared task BioNLP/NLPBA 2004 provides2000 MEDLINE abstracts from the GENIA ([Ohtaet.
al., 2002]) bio-named entity corpus version 3.02.There are total 5 entity classes: DNA, RNA,protein, cell_line and cell_type.4.2 Experiment results and analysisCLASS Recall/Precision/F-scoreFull R64.80   P67.82   F66.28Left R69.99   P73.25   F71.58ALLRight R73.25   P76.67   F74.92Full R65.50   P73.04   F69.07Left R71.26   P79.46   F75.13ProteinRight R72.23   P80.54   F76.16Full R53.77   P61.40   F57.33Left R56.39   P64.40   F60.13Cell_LineRight R63.57   P72.60   F67.79Full R58.60   P61.65   F60.08Left R64.27   P67.61   F65.90DNARight R66.79   P70.27   F68.48Full R65.49   P62.71   F64.07Left R67.26   P64.41   F65.80RNARight R75.22   P72.03   F73.59Full R70.45   P59.45   F64.48Left R74.46   P62.83   F68.15Cell_TypeRight R84.52   P71.32   F77.36Table 2: Final result of POSBIOTM-NER (withno abstract boundary information).Method Full:   Recall/Precision/F-scoreSVM.base R62.01   P65.80   F63.85SVM+V R63.91   P66.89   F65.37CRF.base R64.90   P61.33   F63.06CRF+V R65.78   P61.06   F63.34Final R64.80   P67.82   F66.28Table 3: Step by step result102From table 3, we can see that after using virtualsamples, both the precision and recall increased,especially for SVM.
In CRF, even though the fullf-score did not increase the full F-score much, butfor RNA class, after using virtual samples, the f-score has increased 3%.A CRF has different characteristics from SVM,and is good at handling different kinds of data.
So,we simply merge the results of two machinelearning approaches, by using the CRF results toextend the boundaries of named entities predictedby SVM.
After merging the results of the baselineof SVM and CRF (without using virtual samples)the f-score reaches to 64.58, while the f-score ofSVM alone is 63.85.
The final score in Table 3 isthe merged results with the virtual samples.Although we have improved our system byusing virtual samples, CRF and SVM ascomplementary means and post-processing, westill have some problems to solve, such as correctnamed entity boundary detection.
It is moredifficult to correctly predict the left boundary ofnamed entities than the right boundary.
From theanalysis of the results, we usually predict ?human?and ?factor?
as the beginning and end of a namedentity, but, it is even difficult for human to decidecorrectly whether it is a part of a named entity ornot.5 Conclusion and Future WorksIn this paper, we propose a general method fornamed entity recognition in the biomedical domain.Various morphological, part-of-speech and basenoun phrase features are incorporated to recognisethe named entities.
We use two different kinds ofmachine learning techniques, SVM and CRF, withmerged results.
We also developed a virtual sampletechnique to overcome the training data shortageproblem.
Finally, we present a   keyword-basedheuristic post-processing which increases bothprecision and recall.As shown in the experiment results, morecorrect detection of the named entity boundary isrequired, especially the detection of left boundary.6 AcknowledgementsThis research is supported by BIT Fusion project(by MOST).ReferencesJ.Lafferty, A.McCallum, and F.Pereira.Conditional random fields: probabilistic modelsfor segmenting and labelling sequence data.
InInternational Conference on Machine Learning,2001.Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang Rim.
Two-phase biomedical NErecognition based on SVMs.
Proceedings ofACL 2003 Workshop on Natural LanguageProcessing in Biomedicine,2003.P.Niyogi, F.Girosi, and T.Poggio.
Incorporatingprior information in machine learning bycreating virtual examples.
Proceedings ofIEEE volume 86, pages 2196-2207, 1998Manabu Sasano.
Virtual examples for textclassification with support vector machines.Proceedings of 2003 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP 2003), 2003.Bernhard Scholkopf, Chris Burges, and VladimirVapnik.
Incorporating invariances in supportvector learning machines.
Artificial NeuralNetworks- ICANN96,1112:47-52,1996.Hanna M.Wallach.
Conditional Random Fields:An Introduction.
2004T.Ohta, Y. Tateisi, J.Kim, H. Mima andJ.Tsujiii.2002.
The GENIA corpus: Anannotated research abstract corpus inmolecular biology domain.
In Proceedings ofHuman Language Technology Conference,2002.Kaoru Yamamoto, Taku Kudo, Akihiko Konagaya,and Yuji Matusmoto.
Protein name taggingfor biomedical annotation in text.
Proceedingsof ACL 2003 Workshop on Natural LanguageProcessing in Biomedicine, 2003.103
