SESS ION 8 &: 9: STAT IST ICAL  AND LEARNING METHODSFrederick Jelinek, ChairCenter for Speech ProcessingThe Johns Hopkins UniversityBaltimore, MD 21218The statistical and learning methods of these two ses-sions are related to text processing.
The presented tech-niques should facilitate achieving the desired goal of textunderstanding with all of its potential exploitation: speechunderstanding, gisting, information retrieval, indexing, ma-chine translation, etc.For many years, the conventional pproach consisted ofan attempt o design systems by experts who gained theirknowledge through personal experience, introspection, andinformation exchange with other experts.
As time went on,and claims of promise failed to be redeemed, the realizationbegan to take hold that facts about language are too com-plex to be either listed or incorporated in rules specified byhumans.
Something always eemed to have been forgotten,and surprisingly frequent cases remained untreated.Influenced in no small measure by the success of self-organizing methods in speech recognition, a growing circleof researchers have concluded that decisive progress in thefield of language can only be achieved when the necessaryknowledge will be extracted irectly from data, lots and lotsof diverse data.
The proper work of experts is to securedata, create facilities for the reliable, plentiful, and appro-priate annotation of the data, and help design systems thatcan extract he appropriate information from this data.The results presented in the sessions on statistical andlearning methods provide help in carrying out the last task.The papers by Miller, Chodorow, Landes, Leacock, andThomas (Using a Semantic Concordance for Sense Identi-fication) and by Bruce and Wiebe (A New Approach toSense Disambiguation) present methods that can be usedto annotate in context he sense in which a word is used.E.g., when we say BANK, do we mean the institution forthe deposit of money, or the shore of a river?The training data for the work of the next four papersare treebanks.The paper by Miller and Fox (Automatic Grammar Ac-quisition) attempts to learn statistical rules of a context-dependent grammar appropriate for shift-reduced parsers.Assuming that text understanding will require parsing(constituent analysis), and realizing that grammar isonly atool that (for this application) isnot needed for its own sake,three papers are dedicated to direct parsing without an ex-plicit specification of any grammar.
Two of these papers arestatistical.
They are related not just by authorship, but byuse of techniques extracting the needed probabilities.
Thearticle by Ratnaparkhi and Roukos (A Maximum EntropyModel for Prepositional Phrase Attachment) deals with thedecision whether in a sequence consisting of verb, noun, andprepositional phrases, the last two form a compound nounphrase or all three form a unit (e.g.
does =ate the banana onthe plate" specify a manner of eating or the location of thebanana?).
The article by Jelinek, Lafferty, Magerman, Mer-cer, Ratnaparkhi, and Roukos (Decision Tree Parsing Usinga Hidden Derivation Model) presents anew method of con-structing a statistical, direct, grammar-less parser attach-ing annotated trees to given word strings.
Finally, Brill (AReport of Recent Progress in 'I~ansformation-Based Error-Driven Learning) discusses a deterministic method of directparsing and tagging (part-of-speech annotation) in which anoriginal standard parse or tagging is modified in a step bystep fashion until the final form is attMned.Of course, parsing itself is only a means to an end.What one is really after is text understanding which theparse should facilitate.
Miller, Bobrow, Schwartz, and In-gria (Statistical Language Processing Using Hidden Un-derstanding Models) attempt o extract meaning directlyand develop a co~esponding methodology applicable to re-stricted domains uch as the ATIS task.
Given a sentence,their goal is to fill a template.Meng, Seneff, and Zue (Phonological Parsing for Bi-directional Letter-to-Sound / Sound-to-Letter Generation)deal with a problem interesting to speech recognition andsynthesis: given a spelled word, what should be its pho-netic realization, and vice versa: given a phonetic stringwhat is its likely spelling?
The latter problem may arisewhen a person pronounces a word whose phonetic structurethe system can recognize but which is new to its vocabulary.The processing of Japanese text is complicated by the factthat there are no word delimiters.
Japanese text consistsof sequences of kanji followed by kana characters signalinginflection, politeness, and other information.
The segmen-tation of such text is conventionally accomplished by deter-ministic rules.
Papageorgiu (Japanese Word Segmentationby Hidden Markov Model) uses statistics.Finally, Pereira, Riley, and Sproat (Weighted RationalTransductions and their Application to Human LanguageProcessing) present a new algebraic uniform representationwhich they claim to be applicable to varied informationsources, such as pronunciation dictionaries, language mod-eels, and lattices.
If practically successful, this automatatheory approach will surely be considerably elaborated.239
