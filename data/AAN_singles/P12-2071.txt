Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 363?367,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFast and Robust Part-of-Speech Tagging Using Dynamic Model SelectionJinho D. ChoiDepartment of Computer ScienceUniversity of Colorado Boulderchoijd@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado Bouldermpalmer@colorado.eduAbstractThis paper presents a novel way of improv-ing POS tagging on heterogeneous data.
First,two separate models are trained (generalizedand domain-specific) from the same data setby controlling lexical items with different doc-ument frequencies.
During decoding, one ofthe models is selected dynamically given thecosine similarity between each sentence andthe training data.
This dynamic model selec-tion approach, coupled with a one-pass, left-to-right POS tagging algorithm, is evaluatedon corpora from seven different genres.
Evenwith this simple tagging algorithm, our sys-tem shows comparable results against otherstate-of-the-art systems, and gives higher ac-curacies when evaluated on a mixture of thedata.
Furthermore, our system is able to tagabout 32K tokens per second.
We believe thatthis model selection approach can be appliedto more sophisticated tagging algorithms andimprove their robustness even further.1 IntroductionWhen it comes to POS tagging, two things must bechecked.
First, a POS tagger needs to be tested forits robustness in handling heterogeneous data.1 Sta-tistical POS taggers perform very well when theirtraining and testing data are from the same source,achieving over 97% tagging accuracy (Toutanova etal., 2003; Gime?nez and Ma`rquez, 2004; Shen etal., 2007).
However, the performance degrades in-creasingly as the discrepancy between the training1We use the term ?heterogeneous data?
as a mixture of datacollected from several different sources.and testing data gets larger.
Thus, to ensure robust-ness, a tagger needs to be evaluated on several dif-ferent kinds of data.
Second, a POS tagger should betested for its speed.
POS tagging is often performedas a pre-processing step to other tasks (e.g., pars-ing, chunking) and it should not be a bottleneck forthose tasks.
Moreover, recent NLP tasks deal withvery large-scale data where tagging speed is critical.To improve robustness, we first train two separatemodels; one is optimized for a general domain andthe other is optimized for a domain specific to thetraining data.
During decoding, we dynamically se-lect one of the models by measuring similarities be-tween input sentences and the training data.
Our hy-pothesis is that the domain-specific and generalizedmodels perform better for sentences similar and notsimilar to the training data, respectively.
In this pa-per, we describe how to build both models using thesame training data and select an appropriate modelgiven input sentences during decoding.
Each modeluses a one-pass, left-to-right POS tagging algorithm.Even with the simple tagging algorithm, our systemgives results that are comparable to two other state-of-the-art systems when coupled with this dynamicmodel selection approach.
Furthermore, our systemshows noticeably faster tagging speed compared tothe other two systems.For our experiments, we use corpora from sevendifferent genres (Weischedel et al, 2011; Nielsen etal., 2010).
This allows us to check the performanceof each system on different kinds of data when runindividually or selectively.
To the best of our knowl-edge, this is the first time that a POS tagger has beenevaluated on such a wide variety of data in English.3632 Approach2.1 Training generalized and domain-specificmodels using document frequencyConsider training data as a collection of documentswhere each document contains sentences focusingon a similar topic.
For instance, in the Wall StreetJournal corpus, a document can be an individual fileor all files within each section.2 To build a gener-alized model, lexical features (e.g., n-gram word-forms) that are too specific to individual documentsshould be avoided so that a classifier can place moreweight on features common to all documents.To filter out these document-specific features, athreshold is set for the document frequency of eachlowercase simplified word-form (LSW) in the train-ing data.
A simplified word-form (SW) is derived byapplying the following regular expressions sequen-tially to the original word-form, w. ?replaceAll?
is afunction that replaces all matches of the regular ex-pression in w (the 1st parameter) with the specificstring (the 2nd parameter).
In a simplified word, allnumerical expressions are replaced with 0.1. w.replaceAll(\d%, 0) (e.g., 1% ?
0)2. w.replaceAll(\$\d, 0) (e.g., $1 ?
0)3. w.replaceAll(?\.\d, 0) (e.g., .1 ?
0)4.
w.replaceAll(\d(,|:|-|\/|\.
)\d, 0)(e.g., 1,2|1:2|1-2|1/2|1.2 ?
0)5. w.replaceAll(\d+, 0) (e.g., 1234 ?
0)A LSW is a decapitalized SW.
Given a set of LSW?swhose document frequencies are greater than a cer-tain threshold, a model is trained by using only lexi-cal features associated with these LSW?s.
For a gen-eralized model, we use a threshold of 2, meaningthat only lexical features whose LSW?s occur in atleast 3 documents of the training data are used.
Fora domain-specific model, we use a threshold of 1.The generalized and domain-specific models aretrained separately; their learning parameters are op-timized by running n-fold cross-validation where nis the total number of documents in the training dataand grid search on Liblinear parameters c and B (seeSection 2.4 for more details about the parameters).2For our experiments, we treat each section of the WallStreet Journal as one document.2.2 Dynamic model selection during decodingOnce both generalized and domain-specific modelsare trained, alternative approaches can be adaptedfor decoding.
One is to run both models and mergetheir outputs.
This approach can produce output thatis potentially more accurate than output from eithermodel, but takes longer to decode because the merg-ing cannot be processed until both models are fin-ished.
Instead, we take an alternative approach, thatis to select one of the models dynamically given theinput sentence.
If the model selection is done ef-ficiently, this approach runs as fast as running justone model, yet can give more robust performance.The premise of this dynamic model selection isthat the domain-specific model performs better forinput sentences similar to its training space, whereasthe generalized model performs better for ones thatare dissimilar.
To measure similarity, a set of SW?s,say T , used for training the domain-specific modelis collected.
During decoding, a set of SW?s in eachsentence, say S, is collected.
If the cosine similaritybetween T and S is greater than a certain threshold,the domain-specific model is selected for decoding;otherwise, the generalized model is selected.0.0710 0.02 0.0419004080120160Cosine SimilarityOccurrence5%Figure 1: Cosine similarity distribution: the y-axis showsthe number of occurrences for each cosine similarity dur-ing cross-validation.The threshold is derived automatically by runningcross-validation; for each fold, both models are runsimultaneously and cosine similarities of sentenceson which the domain-specific model performs bet-ter are extracted.
Figure 1 shows the distributionof cosine similarities extracted during our cross-validation.
Given the cosine similarity distribution,the similarity at the first 5% area (in this case, 0.025)is taken as the threshold.3642.3 Tagging algorithm and featuresEach model uses a one-pass, left-to-right POS tag-ging algorithm.
The motivation is to analyze howdynamic model selection works with a simple algo-rithm first and then apply it to more sophisticatedones later (e.g., bidirectional tagging algorithm).Our feature set (Table 1) is inspired by Gime?nezand Ma`rquez (2004) although ambiguity classes arederived selectively for our case.
Given a word-form,we count how often each POS tag is used with theform and keep only ones above a certain threshold.For both generalized and domain-specific models, athreshold of 0.7 is used, which keeps only POS tagsused with their forms over 70% of the time.
Fromour experiments, we find this to be more useful thanexpanding ambiguity classes with lower thresholds.Lexicalfi?
{0,1,2,3}, (mi?2,i?1), (mi?1,i), (mi?1,i+1),(mi,i+1), (mi+1,i+2), (mi?2,i?1,i), (mi?1,i,i+1),(mi,i+1,i+2), (mi?2,i?1,i+1), (mi?1,i+1,i+2)POSpi?
{3,2,1}, ai+{0,1,2,3}, (pi?2,i?1), (ai+1,i+2),(pi?1, ai+1), (pi?2, pi?1, ai), (pi?2, pi?1, ai+1),(pi?1, ai, ai+1), (pi?1, ai+1, ai+2)Affix c:1, c:2, c:3, cn:, cn?1:, cn?2:, cn?3:Binaryinitial uppercase, all uppercase/lowercase,contains 1/2+ capital(s) not at the beginning,contains a (period/number/hyphen)Table 1: Feature templates.
i: the index of the currentword, f : SW, m: LSW, p: POS, a: ambiguity class, c?
:character sequence in wi (e.g., c:2: the 1st and 2nd char-acters of wi, cn?1:: the n-1?th and n?th characters of wi).See Gime?nez and Ma`rquez (2004) for more details.2.4 Machine learningLiblinear L2-regularization, L1-loss support vectorclassification is used for our experiments (Hsieh etal., 2008).
From several rounds of cross-validation,learning parameters of (c = 0.2, e = 0.1, B = 0.4) and(c = 0.1, e = 0.1, B = 0.9) are found for the gener-alized and domain-specific models, respectively (c:cost, e: termination criterion, B: bias).3 Related workToutanova et al (2003) introduced a POS taggingalgorithm using bidirectional dependency networks,and showed the best contemporary results.
Gime?nezand Ma`rquez (2004) used one-pass, left-to-rightand right-to-left combined tagging algorithm andachieved near state-of-the-art results.
Shen et al(2007) presented a tagging approach using guidedlearning for bidirectional sequence classification andshowed current state-of-the-art results.3Our individual models (generalized and domain-specific) are similar to Gime?nez and Ma`rquez (2004)in that we use a subset of their features and take one-pass, left-to-right tagging approach, which is a sim-pler version of theirs.
However, we use Liblinear forlearning, which trains much faster than their classi-fier, Support Vector Machines.4 Experiments4.1 CorporaFor training, sections 2-21 of the Wall Street Jour-nal (WSJ) from OntoNotes v4.0 (Weischedel et al,2011) are used.
The entire training data consists of30,060 sentences with 731,677 tokens.
For evalua-tion, corpora from seven different genres are used:the MSNBC broadcasting conversation (BC), theCNN broadcasting news (BN), the Sinorama newsmagazine (MZ), the WSJ newswire (NW), and theGALE web-text (WB), all from OntoNotes v4.0.
Ad-ditionally, the Mipacq clinical notes (CN) and theMedpedia articles (MD) are used for evaluation ofmedical domains (Nielsen et al, 2010).
Table 2shows distributions of these evaluation sets.4.2 Accuracy comparisonsOur models are compared with two other state-of-the-art systems, the Stanford tagger (Toutanova etal., 2003) and the SVMTool (Gime?nez and Ma`rquez,2004).
Both systems are trained with the same train-ing data and use configurations optimized for theirbest reported results.
Tables 3 and 4 show taggingaccuracies of all tokens and unknown tokens, re-spectively.
Our individual models (Models D andG) give comparable results to the other systems.Model G performs better than Model D for BC, CN,and MD, which are very different from the WSJ.This implies that the generalized model shows itsstrength in tagging data that differs from the train-ing data.
The dynamic model selection approach(Model S) shows the most robust results across gen-res, although Models D and G still can perform3Some semi-supervised and domain-adaptation approachesusing external data had shown better performance (Daume III,2007; Spoustova?
et al, 2009; S?gaard, 2011).365BC BN CN MD MZ NW WB TotalSource MSNBC CNN Mipacq Medpedia Sinorama WSJ ENG -Sentences 2,076 1,969 3,170 1,850 1,409 1,640 1,738 13,852All tokens 31,704 31,328 35,721 34,022 32,120 39,590 34,707 239,192Unknown tokens 3,077 1,284 6,077 4,755 2,663 983 2,609 21,448Table 2: Distributions of evaluation sets.
The Total column indicates a mixture of data from all genres.BC BN CN MD MZ NW WB TotalModel D 91.81 95.27 87.36 90.74 93.91 97.45 93.93 92.97Model G 92.65 94.82 88.24 91.46 93.24 97.11 93.51 93.05Model S 92.26 95.13 88.18 91.34 93.88 97.46 93.90 93.21G over D 50.63 36.67 68.80 40.22 21.43 9.51 36.02 41.74Stanford 87.71 95.50 88.49 90.86 92.80 97.42 94.01 92.50SVMTool 87.82 95.13 87.86 90.54 92.94 97.31 93.99 92.32Table 3: Tagging accuracies of all tokens (in %).
Models D and G indicate domain-specific and generalized models,respectively and Model S indicates the dynamic model selection approach.
?G over D?
shows how often Model G isselected over Model D using the dynamic selection (in %).BC BN CN MD MZ NW WB TotalModel S 60.97 77.73 68.69 67.30 75.97 88.40 76.27 70.54Stanford 19.24 87.31 71.20 64.82 66.28 88.40 78.15 64.32SVMTool 19.08 78.35 66.51 62.94 65.23 86.88 76.47 47.65Table 4: Tagging accuracies of unknown tokens (in %).better for individual genres (except for NW, whereModel S performs better than any other model).For both all and unknown token experiments,Model S performs better than the other systemswhen evaluated on a mixture of the data (the Totalcolumn).
The differences are statistically significantfor both experiments (McNemar?s test, p < .0001).The Stanford tagger gives significantly better resultsfor unknown tokens in BN; we suspect that this iswhere their bidirectional tagging algorithm has anadvantage over our simple left-to-right algorithm.4.3 Speed comparisonsTagging speeds are measured by running each sys-tem on the mixture of all data.
Our system and theStanford system are both written in Java; the Stan-ford tagger provides APIs that allow us to make faircomparisons between the two systems.
The SVM-Tool is written in Perl, so there is a systematic dif-ference between the SVMTool and our system.Table 5 shows speed comparisons between thesesystems.
All experiments are evaluated on an In-tel Xeon 2.57GHz machine.
Our system tags about32K tokens per second (0.03 milliseconds per to-ken), which includes run-time for both POS taggingand model selection.Stanford SVMTool Model Stokens / sec.
421 1,163 31,914Table 5: Tagging speeds.5 ConclusionWe present a dynamic model selection approach thatimproves the robustness of POS tagging on hetero-geneous data.
We believe that this approach canbe applied to more sophisticated algorithms and im-prove their robustness even further.
Our system alsoshows noticeably faster tagging speed against twoother state-of-the-art systems.
For future work, wewill experiment with more diverse training and test-ing data and also more sophisticated algorithms.AcknowledgmentsThis work was supported by the SHARP programfunded by ONC: 90TR0002/01.
The content issolely the responsibility of the authors and does notnecessarily represent the official views of the ONC.366ReferencesHal Daume III.
2007.
Frustratingly Easy Domain Adap-tation.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,ACL?07, pages 256?263.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool: Ageneral POS tagger generator based on Support Vec-tor Machines.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation,LREC?04.Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. SathiyaKeerthi, and S. Sundararajan.
2008.
A Dual Coordi-nate Descent Method for Large-scale Linear SVM.
InProceedings of the 25th international conference onMachine learning, ICML?08, pages 408?415.Rodney D. Nielsen, James Masanz, Philip Ogren, WayneWard, James H. Martin, Guergana Savova, and MarthaPalmer.
2010.
An architecture for complex clinicalquestion answering.
In Proceedings of the 1st ACMInternational Health Informatics Symposium, IHI?10,pages 395?399.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided Learning for Bidirectional Sequence Classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,ACL?07, pages 760?767.Anders S?gaard.
2011.
Semi-supervised condensednearest neighbor for part-of-speech tagging.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, ACL?11, pages 48?52.Drahom?
?ra ?johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervised Train-ing for the Averaged Perceptron POS Tagger.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, EACL?09, pages 763?771.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of the Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,NAACL?03, pages 173?180.Ralph Weischedel, Eduard Hovy, Martha Palmer, MitchMarcus, Robert Belvin, Sameer Pradhan, LanceRamshaw, and Nianwen Xue.
2011.
OntoNotes: ALarge Training Corpus for Enhanced Processing.
InJoseph Olive, Caitlin Christianson, and John McCary,editors, Handbook of Natural Language Processingand Machine Translation.
Springer.367
