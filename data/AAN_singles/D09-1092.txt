Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880?889,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPPolylingual Topic ModelsDavid Mimno Hanna M. Wallach Jason NaradowskyUniversity of Massachusetts, AmherstAmherst, MA 01003{mimno, wallach, narad, dasmith, mccallum}cs.umass.eduDavid A. Smith Andrew McCallumAbstractTopic models are a useful tool for analyz-ing large text collections, but have previ-ously been applied in only monolingual,or at most bilingual, contexts.
Mean-while, massive collections of interlinkeddocuments in dozens of languages, suchas Wikipedia, are now widely available,calling for tools that can characterize con-tent in many languages.
We introduce apolylingual topic model that discovers top-ics aligned across multiple languages.
Weexplore the model?s characteristics usingtwo large corpora, each with over ten dif-ferent languages, and demonstrate its use-fulness in supporting machine translationand tracking topic trends across languages.1 IntroductionStatistical topic models have emerged as an in-creasingly useful analysis tool for large text col-lections.
Topic models have been used for analyz-ing topic trends in research literature (Mann et al,2006; Hall et al, 2008), inferring captions for im-ages (Blei and Jordan, 2003), social network anal-ysis in email (McCallum et al, 2005), and expand-ing queries with topically related words in infor-mation retrieval (Wei and Croft, 2006).
Much ofthis work, however, has occurred in monolingualcontexts.
In an increasingly connected world, theability to access documents in many languages hasbecome both a strategic asset and a personally en-riching experience.
In this paper, we present thepolylingual topic model (PLTM).
We demonstrateits utility and explore its characteristics using twopolylingual corpora: proceedings of the Europeanparliament (in eleven languages) and a collectionof Wikipedia articles (in twelve languages).There are many potential applications forpolylingual topic models.
Although research liter-ature is typically written in English, bibliographicdatabases often contain substantial quantities ofwork in other languages.
To perform topic-basedbibliometric analysis on these collections, it isnecessary to have topic models that are alignedacross languages.
Such analysis could be sig-nificant in tracking international research trends,where language barriers slow the transfer of ideas.Previous work on bilingual topic modelinghas focused on machine translation applications,which rely on sentence-aligned parallel transla-tions.
However, the growth of the internet, andin particular Wikipedia, has made vast corporaof topically comparable texts?documents that aretopically similar but are not direct translations ofone another?considerably more abundant thanever before.
We argue that topic modeling isboth a useful and appropriate tool for leveragingcorrespondences between semantically compara-ble documents in multiple different languages.In this paper, we use two polylingual corporato answer various critical questions related topolylingual topic models.
We employ a set of di-rect translations, the EuroParl corpus, to evaluatewhether PLTM can accurately infer topics whendocuments genuinely contain the same content.We also explore how the characteristics of dif-ferent languages affect topic model performance.The second corpus, Wikipedia articles in twelvelanguages, contains sets of documents that are nottranslations of one another, but are very likely tobe about similar concepts.
We use this corpusto explore the ability of the model both to infersimilarities between vocabularies in different lan-guages, and to detect differences in topic emphasisbetween languages.
The internet makes it possiblefor people all over the world to access documentsfrom different cultures, but readers will not be flu-ent in this wide variety of languages.
By linkingtopics across languages, polylingual topic mod-els can increase cross-cultural understanding byproviding readers with the ability to characterize880the contents of collections in unfamiliar languagesand identify trends in topic prevalence.2 Related WorkBilingual topic models for parallel texts withword-to-word alignments have been studied pre-viously using the HM-bitam model (Zhao andXing, 2007).
Tam, Lane and Schultz (Tam etal., 2007) also show improvements in machinetranslation using bilingual topic models.
Bothof these translation-focused topic models inferword-to-word alignments as part of their inferenceprocedures, which would become exponentiallymore complex if additional languages were added.We take a simpler approach that is more suit-able for topically similar document tuples (wheredocuments are not direct translations of one an-other) in more than two languages.
A recent ex-tended abstract, developed concurrently by Ni etal.
(Ni et al, 2009), discusses a multilingual topicmodel similar to the one presented here.
How-ever, they evaluate their model on only two lan-guages (English and Chinese), and do not use themodel to detect differences between languages.They also provide little analysis of the differ-ences between polylingual and single-languagetopic models.
Outside of the field of topic mod-eling, Kawaba et al (Kawaba et al, 2008) usea Wikipedia-based model to perform sentimentanalysis of blog posts.
They find, for example,that English blog posts about the Nintendo Wii of-ten relate to a hack, which cannot be mentioned inJapanese posts due to Japanese intellectual prop-erty law.
Similarly, posts about whaling oftenuse (positive) nationalist language in Japanese and(negative) environmentalist language in English.3 Polylingual Topic ModelThe polylingual topic model (PLTM) is an exten-sion of latent Dirichlet alocation (LDA) (Blei etal., 2003) for modeling polylingual document tu-ples.
Each tuple is a set of documents that areloosely equivalent to each other, but written in dif-ferent languages, e.g., corresponding Wikipediaarticles in French, English and German.
PLTM as-sumes that the documents in a tuple share the sametuple-specific distribution over topics.
This is un-like LDA, in which each document is assumed tohave its own document-specific distribution overtopics.
Additionally, PLTM assumes that each?topic?
consists of a set of discrete distributionsDN1TNL...w?
?wzz...?1?L?1?LFigure 1: Graphical model for PLTM.over words?one for each language l = 1, .
.
.
, L.In other words, rather than using a single set oftopics ?
= {?1, .
.
.
,?T}, as in LDA, there are Lsets of language-specific topics, ?1, .
.
.
,?L, eachof which is drawn from a language-specific sym-metric Dirichlet with concentration parameter ?l.3.1 Generative ProcessA new document tuplew = (w1, .
.
.
,wL) is gen-erated by first drawing a tuple-specific topic dis-tribution from an asymmetric Dirichlet prior withconcentration parameter ?
and base measurem:?
?
Dir (?, ?m).
(1)Then, for each language l, a latent topic assign-ment is drawn for each token in that language:zl?
P (zl|?)
=?n?zln.
(2)Finally, the observed tokens are themselves drawnusing the language-specific topic parameters:wl?
P (wl| zl,?l) =?n?lwln|zln.
(3)The graphical model is shown in figure 1.3.2 InferenceGiven a corpus of training and test documenttuples?W and W?, respectively?two possibleinference tasks of interest are: computing theprobability of the test tuples given the trainingtuples and inferring latent topic assignments fortest documents.
These tasks can either be accom-plished by averaging over samples of ?1, .
.
.
,?Land ?m from P (?1, .
.
.
,?L, ?m |W?, ?)
or byevaluating a point estimate.
We take the lat-ter approach, and use the MAP estimate for ?mand the predictive distributions over words for?1, .
.
.
,?L.
The probability of held-out docu-ment tuples W?given training tuples W is thenapproximated by P (W?|?1, .
.
.
,?L, ?m).Topic assignments for a test document tuplew = (w1, .
.
.
,wL) can be inferred using Gibbs881sampling.
Gibbs sampling involves sequentiallyresampling each zlnfrom its conditional posterior:P (zln= t |w, z\l,n,?1, .
.
.
,?L, ?m)?
?lwln|t(Nt)\l,n+ ?mt?tNt?
1 + ?, (4)where z\l,nis the current set of topic assignmentsfor all other tokens in the tuple, while (Nt)\l,nisthe number of occurrences of topic t in the tuple,excluding zln, the variable being resampled.4 Results on Parallel TextOur first set of experiments focuses on documenttuples that are known to consist of direct transla-tions.
In this case, we can be confident that thetopic distribution is genuinely shared across alllanguages.
Although direct translations in multi-ple languages are relatively rare (in contrast withcomparable documents), we use direct translationsto explore the characteristics of the model.4.1 Data SetThe EuroParl corpus consists of parallel texts ineleven western European languages: Danish, Ger-man, Greek, English, Spanish, Finnish, French,Italian, Dutch, Portuguese and Swedish.
Thesetexts consist of roughly a decade of proceedingsof the European parliament.
For our purposes weuse alignments at the speech level rather than thesentence level, as in many translation tasks usingthis corpus.
We also remove the twenty-five mostfrequent word types for efficiency reasons.
Theremaining collection consists of over 121 millionwords.
Details by language are shown in Table 1.Table 1: Average document length, # documents, andunique word types per 10,000 tokens in the EuroParl corpus.Lang.
Avg.
leng.
# docs types/10kDA 160.153 65245 121.4DE 178.689 66497 124.5EL 171.289 46317 124.2EN 176.450 69522 43.1ES 170.536 65929 59.5FI 161.293 60822 336.2FR 186.742 67430 54.8IT 187.451 66035 69.5NL 176.114 66952 80.8PT 183.410 65718 68.2SV 154.605 58011 136.1Models are trained using 1000 iterations ofGibbs sampling.
Each language-specific topic?word concentration parameter ?lis set to 0.01.centralbank europ?iske ecb s l?n centralbankszentralbank ezb bank europ?ischen investitionsbank darlehen???????
????????
????????
???
?????????
???????
?bank central ecb banks european monetarybanco central europeo bce bancos centraleskeskuspankin ekp n euroopan keskuspankki eipbanque centrale bce europ?enne banques mon?tairebanca centrale bce europea banche prestitibank centrale ecb europese banken leningenbanco central europeu bce bancos empr?stimoscentralbanken europeiska ecb centralbankens s l?nb?rn familie udnyttelse b?rns b?rnene seksuelkinder kindern familie ausbeutung familien eltern??????
???????
??????????
???????????
??????
???????
?children family child sexual families exploitationni?os familia hijos sexual infantil menoreslasten lapsia lapset perheen lapsen lapsiinenfants famille enfant parents exploitation famillesbambini famiglia figli minori sessuale sfruttamentokinderen kind gezin seksuele ouders familiecrian?as fam?lia filhos sexual crian?a infantilbarn barnen familjen sexuellt familj utnyttjandem?l n?
m?ls?tninger m?let m?ls?tning opn?ziel ziele erreichen zielen erreicht zielsetzungen???????
?????
??????
??????
??????
???????
?objective objectives achieve aim ambitious setobjetivo objetivos alcanzar conseguir lograr estostavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteenobjectif objectifs atteindre but cet ambitieuxobiettivo obiettivi raggiungere degli scopo quellodoelstellingen doel doelstelling bereiken bereikt doelenobjectivo objectivos alcan?ar atingir ambicioso conseguirm?l m?let uppn?
m?len m?ls?ttningar m?ls?ttningandre anden side ene andet ?vrigeanderen andere einen wie andererseits anderer?????
????
????
?????
??????
???
?other one hand others another thereotros otras otro otra parte dem?smuiden toisaalta muita muut muihin muunautres autre part c?t?
ailleurs m?mealtri altre altro altra dall parteandere anderzijds anderen ander als kantoutros outras outro lado outra noutrosandra sidan ?
annat ena annanDADEELENESFIFRITNLPTSVDADEELENESFIFRITNLPTSVDADEELENESFIFRITNLPTSVDADEELENESFIFRITNLPTSVFigure 2: EuroParl topics (T=400)The concentration parameter ?
for the prior overdocument-specific topic distributions is initializedto 0.01T , while the base measure m is initializedto the uniform distribution.
Hyperparameters ?mare re-estimated every 10 Gibbs iterations.4.2 Analysis of Trained ModelsFigure 2 shows the most probable words in all lan-guages for four example topics, from PLTM with400 topics.
The first topic contains words relatingto the European Central Bank.
This topic providesan illustration of the variation in technical ter-minology captured by PLTM, including the widearray of acronyms used by different languages.The second topic, concerning children, demon-strates the variability of everyday terminology: al-though the four Romance languages are closely882related, they use etymologically unrelated wordsfor children.
(Interestingly, all languages exceptGreek and Finnish use closely related words for?youth?
or ?young?
in a separate topic.)
The thirdtopic demonstrates differences in inflectional vari-ation.
English and the Romance languages useonly singular and plural versions of ?objective.
?The other Germanic languages include compoundwords, while Greek and Finnish are dominated byinflected variants of the same lexical item.
The fi-nal topic demonstrates that PLTM effectively clus-ters ?syntactic?
words, as well as more semanti-cally specific nouns, adjectives and verbs.Although the topics in figure 2 seem highly fo-cused, it is interesting to ask whether the modelis genuinely learning mixtures of topics or simplyassigning entire document tuples to single topics.To answer this question, we compute the posteriorprobability of each topic in each tuple under thetrained model.
If the model assigns all tokens ina tuple to a single topic, the maximum posteriortopic probability for that tuple will be near to 1.0.If the model assigns topics uniformly, the maxi-mum topic probability will be near 1/T .
We com-pute histograms of these maximum topic prob-abilities for T ?
{50, 100, 200, 400, 800}.
Forclarity, rather than overlaying five histograms, fig-ure 3 shows the histograms converted into smoothcurves using a kernel density estimator.1Althoughthere is a small bump around 1.0 (for extremelyshort documents, e.g., ?Applause?
), values aregenerally closer to, but greater than, 1/T .0.0 0.2 0.4 0.6 0.8 1.0024681012Smoothed histograms of max(P(t))Maximum topic probability in documentDensity 800 topics400 topics200 topics100 topics50 topicsFigure 3: Smoothed histograms of the probability of themost probable topic in a document tuple.Although the posterior distribution over topicsfor each tuple is not concentrated on one topic,it is worth checking that this is not simply be-cause the model is assigning a single topic to the1We use the R density function.tokens in each of the languages.
Although themodel does not distinguish between topic assign-ment variables within a given document tuple (soit is technically incorrect to speak of different pos-terior distributions over topics for different docu-ments in a given tuple), we can nevertheless dividetopic assignment variables between languages anduse them to estimate a Dirichlet-multinomial pos-terior distribution for each language in each tuple.For each tuple we can then calculate the Jensen-Shannon divergence (the average of the KL di-vergences between each distribution and a meandistribution) between these distributions.
Figure 4shows the density of these divergences for differ-ent numbers of topics.
As with the previous fig-ure, there are a small number of documents thatcontain only one topic in all languages, and thushave zero divergence.
These tend to be very short,formulaic parliamentary responses, however.
Thevast majority of divergences are relatively low (1.0indicates no overlap in topics between languagesin a given document tuple) indicating that, for eachtuple, the model is not simply assigning all tokensin a particular language to a single topic.
As thenumber of topics increases, greater variability intopic distributions causes divergence to increase.0.0 0.1 0.2 0.3 0.4 0.505101520Smoothed histograms of inter?language JS divergenceJensen?Shannon DivergenceDensity800 topics400 topics200 topics100 topics50 topicsFigure 4: Smoothed histograms of the Jensen-Shannondivergences between the posterior probability of topics be-tween languages.4.3 Language Model EvaluationA topic model specifies a probability distributionover documents, or in the case of PLTM, docu-ment tuples.
Given a set of training document tu-ples, PLTM can be used to obtain posterior esti-mates of ?1, .
.
.
,?Land ?m.
The probability ofpreviously unseen held-out document tuples giventhese estimates can then be computed.
The higherthe probability of the held-out document tuples,the better the generalization ability of the model.883Analytically calculating the probability of a setof held-out document tuples given ?1, .
.
.
,?Land?m is intractable, due to the summation over anexponential number of topic assignments for theseheld-out documents.
However, recently developedmethods provide efficient, accurate estimates ofthis probability.
We use the ?left-to-right?
methodof (Wallach et al, 2009).
We perform five esti-mation runs for each document and then calculatestandard errors using a bootstrap method.Table 2 shows the log probability of held-outdata in nats per word for PLTM and LDA, bothtrained with 200 topics.
There is substantial varia-tion between languages.
Additionally, the predic-tive ability of PLTM is consistently slightly worsethan that of (monolingual) LDA.
It is important tonote, however, that these results do not imply thatLDA should be preferred over PLTM?that choicedepends upon the needs of the modeler.
Rather,these results are intended as a quantitative analy-sis of the difference between the two models.Table 2: Held-out log probability in nats/word.
(Smallermagnitude implies better language modeling performance.
)PLTM does slightly worse than monolingual LDA models,but the variation between languages is much larger.Lang PLTM sd LDA sdDA -8.11 0.00067 -8.02 0.00066DE -8.17 0.00057 -8.08 0.00072EL -8.44 0.00079 -8.36 0.00087EN -7.51 0.00064 -7.42 0.00069ES -7.98 0.00073 -7.87 0.00070FI -9.25 0.00089 -9.21 0.00065FR -8.26 0.00072 -8.19 0.00058IT -8.11 0.00071 -8.02 0.00058NL -7.84 0.00067 -7.75 0.00099PT -7.87 0.00085 -7.80 0.00060SV -8.25 0.00091 -8.16 0.00086As the number of topics is increased, the wordcounts per topic become very sparse in mono-lingual LDA models, proportional to the size ofthe vocabulary.
Figure 5 shows the proportionof all tokens in English and Finnish assigned toeach topic under LDA and PLTM with 800 topics.More than 350 topics in the Finnish LDA modelhave zero tokens assigned to them, and almost alltokens are assigned to the largest 200 topics.
En-glish has a larger tail, with non-zero counts in allbut 16 topics.
In contrast, PLTM assigns a sig-nificant number of tokens to almost all 800 top-ics, in very similar proportions in both languages.PLTM topics therefore have a higher granularity ?i.e., they are more specific.
This result is impor-tant: informally, we have found that increasing thegranularity of topics correlates strongly with userperceptions of the utility of a topic model.0 200 400 600 8000.000.010.020.030.04Sorted topic rankPercentage of tokensFigure 5: Topics sorted by number of words assigned.Finnish is in black, English is in red; LDA is solid, PLTM isdashed.
LDA in Finnish essentially learns a 200 topic modelwhen given 800 topics, while PLTM uses all 800 topics.4.4 Partly Comparable CorporaAn important application for polylingual topicmodeling is to use small numbers of comparabledocument tuples to link topics in larger collectionsof distinct, non-comparable documents in multiplelanguages.
For example, a journal might publishpapers in English, French, German and Italian.
Nopaper is exactly comparable to any other paper, butthey are all roughly topically similar.
If we wishto perform topic-based bibliometric analysis, it isvital to be able to track the same topics across alllanguages.
One simple way to achieve this topicalignment is to add a small set of comparable doc-ument tuples that provide sufficient ?glue?
to bindthe topics together.
Continuing with the exam-ple above, one might extract a set of connectedWikipedia articles related to the focus of the jour-nal and then train PLTM on a joint corpus consist-ing of journal papers and Wikipedia articles.In order to simulate this scenario we create aset of variations of the EuroParl corpus by treat-ing some documents as if they have no paral-lel/comparable texts ?
i.e., we put each of thesedocuments in a single-document tuple.
To do this,we divide the corpusW into two sets of documenttuples: a ?glue?
set G and a ?separate?
set S suchthat |G| / |W| = p. In other words, the proportionof tuples in the corpus that are treated as ?glue?
(i.e., placed in G) is p. For every tuple in S, weassign each document in that tuple to a new single-document tuple.
By doing this, every document inS has its own distribution over topics, independentof any other documents.
Ideally, the ?glue?
doc-884uments in G will be sufficient to align the topicsacross languages, and will cause comparable doc-uments in S to have similar distributions over top-ics even though they are modeled independently.Table 3: The effect of the proportion p of ?glue?
tuples onmean Jensen-Shannon divergence in estimated topic distribu-tions for pairs of documents in S that were originally part ofa document tuple.
Lower divergence means the topic distri-butions distributions are more similar to each other.p Mean JS # of pairs Std.
Err.0.01 0.83755 487670 0.000180.05 0.79144 467288 0.000210.1 0.70228 443753 0.000260.25 0.38480 369608 0.000290.5 0.29712 246380 0.00030Table 4: Topics are meaningful within languages but di-verge between languages when only 1% of tuples are treatedas ?glue?
tuples.
With 25% ?glue?
tuples, topics are aligned.lang Topics at p = 0.01DE ru?land russland russischen tschetschenien sicherheitEN china rights human country s burmaFR russie tch?etch?enie union avec russe r?egionIT ho presidente mi perch?e relazione votatolang Topics at p = 0.25DE ru?land russland russischen tschetschenien ukraineEN russia russian chechnya cooperation region belarusFR russie tch?etch?enie avec russe russes situationIT russia unione cooperazione cecenia regione russaWe train PLTM with 100 topics on corpora withp ?
{0.01, 0.05, 0.1, 0.25, 0.5}.
We use 1000 it-erations of Gibbs sampling with ?
= 0.01.
Hy-perparameters ?m are re-estimated every 10 it-erations.
We calculate the Jensen-Shannon diver-gence between the topic distributions for each pairof individual documents in S that were originallypart of the same tuple prior to separation.
Thelower the divergence, the more similar the distri-butions are to each other.
From the results in fig-ure 4, we know that leaving all document tuplesintact should result in a mean JS divergence ofless than 0.1.
Table 3 shows mean JS divergencesfor each value of p. As expected, JS divergence isgreater than that obtained when all tuples are leftintact.
Divergence drops significantly when theproportion of ?glue?
tuples increases from 0.01 to0.25.
Example topics for p = 0.01 and p = 0.25are shown in table 4.
At p = 0.01 (1% ?glue?
doc-uments), German and French both include wordsrelating to Russia, while the English and Italianword distributions appear locally consistent butunrelated to Russia.
At p = 0.25, the top wordsfor all four languages are related to Russia.These results demonstrate that PLTM is appro-priate for aligning topics in corpora that have onlya small subset of comparable documents.
One areafor future work is to explore whether initializa-tion techniques or better representations of topicco-occurrence might result in alignment of topicswith a smaller proportion of comparable texts.4.5 Machine TranslationAlthough the PLTM is clearly not a substitute fora machine translation system?it has no way torepresent syntax or even multi-word phrases?it isclear from the examples in figure 2 that the sets ofhigh probability words in different languages for agiven topic are likely to include translations.
Wetherefore evaluate the ability of the PLTM to gen-erate bilingual lexica, similar to other work in un-supervised translation modeling (Haghighi et al,2008).
In the early statistical translation modelwork at IBM, these representations were called?cepts,?
short for concepts (Brown et al, 1993).We evaluate sets of high-probability words ineach topic and multilingual ?synsets?
by compar-ing them to entries in human-constructed bilingualdictionaries, as done by Haghighi et al (2008).Unlike previous work (Koehn and Knight, 2002),we evaluate all words, not just nouns.
We col-lected bilingual lexica mapping English words toGerman, Greek, Spanish, French, Italian, Dutchand Swedish.
Each lexicon is a set of pairs con-sisting of an English word and a translated word,{we, w`}.
We do not consider multi-word terms.We expect that simple analysis of topic assign-ments for sequential words would yield such col-locations, but we leave this for future work.For every topic t we select a small number Kof the most probable words in English (e) andin each ?translation?
language (`): Wteand Wt`,respectively.
We then add the Cartesian productof these sets for every topic to a set of candidatetranslations C. We report the number of elementsof C that appear in the reference lexica.
Resultsfor K = 1, that is, considering only the singlemost probable word for each language, are shownin figure 6.
Precision at this level is relativelyhigh, above 50% for Spanish, French and Italianwith T = 400 and 800.
Many of the candidatepairs that were not in the bilingual lexica werevalid translations (e.g.
EN ?comitology?
and IT885?comitalogia?)
that simply were not in the lexica.We also do not count morphological variants: themodel finds EN ?rules?
and DE ?vorschriften,?
butthe lexicon contains only ?rule?
and ?vorschrift.
?Results remain strong as we increase K. WithK = 3, T = 800, 1349 of the 7200 candidatepairs for Spanish appeared in the lexicon.l llll200 400 600 8000100200300400500Translation pairs at K=1TopicsCorrect translationslllllllllllllllllESFRITDESVELFigure 6: Are the single most probable words for a giventopic in different languages translations of each other?
Thenumber of such pairs that appear in bilingual lexica is shownon the y-axis.
For T = 800, the top English and Spanishwords in 448 topics were exact translations of one another.4.6 Finding TranslationsIn addition to enhancing lexicons by aligningtopic-specific vocabulary, PLTM may also be use-ful for adapting machine translation systems tonew domains by finding translations or near trans-lations in an unstructured corpus.
These aligneddocument pairs could then be fed into standardmachine translation systems as training data.
Toevaluate this scenario, we train PLTM on a set ofdocument tuples from EuroParl, infer topic distri-butions for a set of held-out documents, and thenmeasure our ability to align documents in one lan-guage with their translations in another language.It is not necessarily clear that PLTM will be ef-fective at identifying translations.
In finding a low-dimensional semantic representation, topic mod-els deliberately smooth over much of the varia-tion present in language.
We are therefore inter-ested in determining whether the information inthe document-specific topic distributions is suffi-cient to identify semantically identical documents.We begin by dividing the data into a trainingset of 69,550 document tuples and a test set of17,435 document tuples.
In order to make the taskmore difficult, we train a relatively coarse-grainedPLTM with 50 topics on the training set.
We thenuse this model to infer topic distributions for each405060708090100Min query doc length% of translat rank0 50 100 200Rank 1Rank 5Rank 10Rank 20Figure 7: Percent of query language documents for whichthe target language translation is ranked at or above 1, 5, 10or 20 by JS divergence, averaged over all language pairs.of the 11 documents in each of the held-out doc-ument tuples using a method similar to that usedto calculate held-out probabilities (Wallach et al,2009).
Finally, for each pair of languages (?query?and ?target?)
we calculate the difference betweenthe topic distribution for each held-out documentin the query language and the topic distribution foreach held-out document in the target language.
Weuse both Jensen-Shannon divergence and cosinedistance.
For each document in the query languagewe rank all documents in the target language andrecord the rank of the actual translation.Results averaged over all query/target languagepairs are shown in figure 7 for Jensen-Shannondivergence.
Cosine-based rankings are signifi-cantly worse.
It is important to note that thelength of documents matters.
As noted before,many of the documents in the EuroParl collectionconsist of short, formulaic sentences.
Restrict-ing the query/target pairs to only those with queryand target documents that are both longer than 50words results in significant improvement and re-duced variance: the average proportion of querydocuments for which the true translation is rankedhighest goes from 53.9% to 72.7%.
Performancecontinues to improve with longer documents, mostlikely due to better topic inference.
Results varyby language.
Table 5 shows results for all tar-get languages with English as a query language.Again, English generally performs better with Ro-mance languages than Germanic languages.5 Results on Comparable TextsDirectly parallel translations are rare in many lan-guages and can be extremely expensive to pro-duce.
However, the growth of the web, and in par-ticular Wikipedia, has made comparable text cor-886CYDEELENFAFIFRHEITPLRUTRCYDEELENFAFIFRHEITPLRUTRCYDEELENFAFIFRHEITPLRUTRFigure 8: Squares represent the proportion of tokens in each language assigned to a topic.
The left topic, world ski km won,centers around Nordic counties.
The center topic, actor role television actress, is relatively uniform.
The right topic, ottomanempire khan byzantine, is popular in all languages but especially in regions near Istanbul.Table 5: Percent of English query documents for which thetranslation was in the top n ?
{1, 5, 10, 20} documents by JSdivergence between topic distributions.
To reduce the effectof short documents we consider only document pairs wherethe query and target documents are longer than 100 words.Lang 1 5 10 20DA 78.0 90.7 93.8 95.8DE 76.6 90.0 93.4 95.5EL 77.1 90.4 93.3 95.2ES 81.2 92.3 94.8 96.7FI 76.7 91.0 94.0 96.3FR 80.1 91.7 94.3 96.2IT 79.1 91.2 94.1 96.2NL 76.6 90.1 93.4 95.5PT 80.8 92.0 94.7 96.5SV 80.4 92.1 94.9 96.5pora ?
documents that are topically similar but arenot direct translations of one another ?
consider-ably more abundant than true parallel corpora.In this section, we explore two questions re-lating to comparable text corpora and polylingualtopic modeling.
First, we explore whether com-parable document tuples support the alignment offine-grained topics, as demonstrated earlier usingparallel documents.
This property is useful forbuilding machine translation systems as well asfor human readers who are either learning newlanguages or analyzing texts in languages they donot know.
Second, because comparable texts maynot use exactly the same topics, it becomes cru-cially important to be able to characterize differ-ences in topic prevalence at the document level (dodifferent languages have different perspectives onthe same article?)
and at the language-wide level(which topics do particular languages focus on?
).5.1 Data SetWe downloaded XML copies of all Wikipedia ar-ticles in twelve different languages: Welsh, Ger-man, Greek, English, Farsi, Finnish, French, He-brew, Italian, Polish, Russian and Turkish.
Theseversions of Wikipedia were selected to provide adiverse range of language families, geographic ar-eas, and quantities of text.
We preprocessed thedata by removing tables, references, images andinfo-boxes.
We dropped all articles in non-Englishlanguages that did not link to an English article.
Inthe English version of Wikipedia we dropped allarticles that were not linked to by any other lan-guage in our set.
For efficiency, we truncated eacharticle to the nearest word after 1000 charactersand dropped the 50 most common word types ineach language.
Even with these restrictions, thesize of the corpus is 148.5 million words.We present results for a PLTM with 400 topics.1000 Gibbs sampling iterations took roughly fourdays on one CPU with current hardware.5.2 Which Languages Have High TopicDivergence?As with EuroParl, we can calculate the Jensen-Shannon divergence between pairs of documentswithin a comparable document tuple.
We can thenaverage over all such document-document diver-gences for each pair of languages to get an over-all ?disagreement?
score between languages.
In-terestingly, we find that almost all languages inour corpus, including several pairs that have his-torically been in conflict, show average JS diver-gences of between approximately 0.08 and 0.12for T = 400, consistent with our findings forEuroParl translations.
Subtle differences of sen-timent may be below the granularity of the model.887sadwrn blaned gallair at lloeren mytholegspace nasa sojus flug mission??????????
sts nasa ????
smallspace mission launch satellite nasa spacecraft???????
???????
????
????
???????
????
?sojuz nasa apollo ensimm?inen space lentospatiale mission orbite mars satellite spatial??????
?
????
???
????
???
?spaziale missione programma space sojuz stazionemisja kosmicznej stacji misji space nasa???????????
????
????????????
???????
??????
?uzay soyuz ay uzaya salyut sovyetlersbaen madrid el la jos?
sbaenegde spanischer spanischen spanien madrid la????????
???????
de ???????
???
??????
?de spanish spain la madrid y??????
????
?????????
???????
de ???
?espanja de espanjan madrid la realespagnol espagne madrid espagnole juan y????
???????
?????
??
??????
???
?de spagna spagnolo spagnola madrid elde hiszpa?ski hiszpanii la juan y??
??????
???????
???????
?????????
deispanya ispanyol madrid la k?ba realbardd gerddi iaith beirdd fardd gymraegdichter schriftsteller literatur gedichte gedicht werk???????
??????
??????
????
???????
???????
?poet poetry literature literary poems poem????
????
?????
??????
???
???
?runoilija kirjailija kirjallisuuden kirjoitti runo julkaisipo?te ?crivain litt?rature po?sie litt?raire ses??????
?????
????
????
?????
????
?poeta letteratura poesia opere versi poemapoeta literatury poezji pisarz in jego????
???
????????
??????????
??????
?????????
?air edebiyat ?iir yazar edebiyat?
adl?CYDEELENFAFIFRHEITPLRUTRCYDEELENFAFIFRHEITPLRUTRCYDEELENFAFIFRHEITPLRUTRFigure 9: Wikipedia topics (T=400).Overall, these scores indicate that although indi-vidual pages may show disagreement, Wikipediais on average consistent between languages.5.3 Are Topics Emphasized DifferentlyBetween Languages?Although we find that if Wikipedia contains an ar-ticle on a particular subject in some language, thearticle will tend to be topically similar to the arti-cles about that subject in other languages, we alsofind that across the whole collection different lan-guages emphasize topics to different extents.
Todemonstrate the wide variation in topics, we cal-culated the proportion of tokens in each languageassigned to each topic.
Figure 8 represents the es-timated probabilities of topics given a specific lan-guage.
Competitive cross-country skiing (left) ac-counts for a significant proportion of the text inFinnish, but barely exists in Welsh and the lan-guages in the Southeastern region.
Meanwhile,interest in actors and actresses (center) is consis-tent across all languages.
Finally, historical topics,such as the Byzantine and Ottoman empires (right)are strong in all languages, but show geographicalvariation: interest centers around the empires.6 ConclusionsWe introduced a polylingual topic model (PLTM)that discovers topics aligned across multiple lan-guages.
We analyzed the characteristics of PLTMin comparison to monolingual LDA, and demon-strated that it is possible to discover aligned top-ics.
We also demonstrated that relatively smallnumbers of topically comparable document tu-ples are sufficient to align topics between lan-guages in non-comparable corpora.
Additionally,PLTM can support the creation of bilingual lexicafor low resource language pairs, providing candi-date translations for more computationally intensealignment processes without the sentence-alignedtranslations typically used in such tasks.
Whenapplied to comparable document collections suchas Wikipedia, PLTM supports data-driven analysisof differences and similarities across all languagesfor readers who understand any one language.7 AcknowledgmentsThe authors thank Limin Yao, who was involvedin early stages of this project.
This work wassupported in part by the Center for Intelligent In-formation Retrieval, in part by The Central In-telligence Agency, the National Security Agencyand National Science Foundation under NSF grantnumber IIS-0326249, and in part by Army primecontract number W911NF-07-1-0216 and Uni-versity of Pennsylvania subaward number 103-548106, and in part by National Science Founda-tion under NSF grant #CNS-0619337.
Any opin-ions, findings and conclusions or recommenda-tions expressed in this material are the authors?and do not necessarily reflect those of the sponsor.ReferencesDavid Blei and Michael Jordan.
2003.
Modeling an-notated data.
In SIGIR.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
JMLR.Peter F Brown, Stephen A Della Pietra, Vincent J DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
CL, 19(2):263?311.888Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In ACL, pages 771?779.David Hall, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Studying the history of ideas usingtopic models.
In EMNLP.Mariko Kawaba, Hiroyuki Nakasaki, Takehito Utsuro,and Tomohiro Fukuhara.
2008.
Cross-lingual bloganalysis based on multilingual blog distillation frommultilingual Wikipedia entries.
In ICWSM.Philipp Koehn and Kevin Knight.
2002.
Learn-ing a translation lexicon from monolingual corpora.In Proceedings of ACL Workshop on UnsupervisedLexical Acquisition.Gideon Mann, David Mimno, and Andrew McCal-lum.
2006.
Bibliometric impact measures leverag-ing topic analysis.
In JCDL.Andrew McCallum, Andr?es Corrada-Emmanuel, andXuerui Wang.
2005.
Topic and role discovery insocial networks.
In IJCAI.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining multilingual topics from Wikipedia.In WWW.Yik-Cheung Tam, Ian Lane, and Tanja Schultz.
2007.Bilingual LSA-based adaptation for statistical ma-chine translation.
Machine Translation, 28:187?207.Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In ICML.Xing Wei and Bruce Croft.
2006.
LDA-based docu-ment models for ad-hoc retrieval.
In SIGIR.Bing Zhao and Eric P. Xing.
2007.
HM-BiTAM: Bilin-gual topic exploration, word alignment, and transla-tion.
In NIPS.889
