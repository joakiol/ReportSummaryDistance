Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99?107,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsIdentifying the Information Structure of Scientific Abstracts: AnInvestigation of Three Different SchemesYufan GuoUniversity of Cambridge, UKyg244@cam.ac.ukAnna KorhonenUniversity of Cambridge, UKalk23@cam.ac.ukMaria LiakataAberystwyth University, UKmal@aber.ac.ukIlona SilinsKarolinska Institutet, SWEDENIlona.Silins@ki.seLin SunUniversity of Cambridge, UKls418@cam.ac.ukUlla SteniusKarolinska Institutet, SWEDENUlla.Stenius@ki.seAbstractMany practical tasks require accessingspecific types of information in scientificliterature; e.g.
information about the ob-jective, methods, results or conclusions ofthe study in question.
Several schemeshave been developed to characterize suchinformation in full journal papers.
Yetmany tasks focus on abstracts instead.
Wetake three schemes of different type andgranularity (those based on section names,argumentative zones and conceptual struc-ture of documents) and investigate theirapplicability to biomedical abstracts.
Weshow that even for the finest-grained ofthese schemes, the majority of categoriesappear in abstracts and can be identifiedrelatively reliably using machine learning.We discuss the impact of our results andthe need for subsequent task-based evalu-ation of the schemes.1 IntroductionScientific abstracts tend to be very similar in termsof their information structure.
For example, manyabstracts provide some background informationbefore defining the precise objective of the study,and the conclusions are typically preceded by thedescription of the results obtained.Many readers of scientific abstracts are inter-ested in specific types of information only, e.g.the general background of the study, the methodsused in the study, or the results obtained.
Accord-ingly, many text mining tasks focus on the ex-traction of information from certain parts of ab-stracts only.
Therefore classification of abstracts(or full articles) according to the categories of in-formation structure can support both the manualstudy of scientific literature as well as its auto-matic analysis, e.g.
information extraction, sum-marization and information retrieval (Teufel andMoens, 2002; Mizuta et al, 2005; Tbahriti et al,2006; Ruch et al, 2007).To date, a number of different schemes andtechniques have been proposed for sentence-basedclassification of scientific literature according toinformation structure, e.g.
(Teufel and Moens,2002; Mizuta et al, 2005; Lin et al, 2006; Hi-rohata et al, 2008; Teufel et al, 2009; Shatkayet al, 2008; Liakata et al, 2010).
Some of theschemes are coarse-grained and merely classifysentences according to typical section names seenin scientific documents (Lin et al, 2006; Hirohataet al, 2008).
Others are finer-grained and basede.g.
on argumentative zones (Teufel and Moens,2002; Mizuta et al, 2005; Teufel et al, 2009),qualitative dimensions (Shatkay et al, 2008) orconceptual structure (Liakata et al, 2010) of doc-uments.The majority of such schemes have been de-veloped for full scientific journal articles whichare richer in information and also considered tobe more in need of the definition of informationstructure (Lin, 2009).
However, many practicaltasks currently focus on abstracts.
As a distilledsummary of key information in full articles, ab-stracts may exhibit an entirely different distribu-tion of scheme categories than full articles.
Fortasks involving abstracts, it would be useful toknow which schemes are applicable to abstractsand which can be automatically identified in themwith reasonable accuracy.In this paper, we will compare the applicabil-ity of three different schemes ?
those based onsection names, argumentative zones and concep-tual structure of documents ?
to a collection ofbiomedical abstracts used for cancer risk assess-ment (CRA).
CRA is an example of a real-worldtask which could greatly benefit from knowledgeabout the information structure of abstracts sincecancer risk assessors look for a variety of infor-mation in them ranging from specific methods to99results concerning different chemicals (Korhonenet al, 2009).
We report work on the annotationof CRA abstracts according to each scheme andinvestigate the schemes in terms of their distri-bution, mutual overlap, and the success of iden-tifying them automatically using machine learn-ing.
Our investigation provides an initial idea ofthe practical usefulness of the schemes for tasksinvolving abstracts.
We discuss the impact of ourresults and the further task-based evaluation whichwe intend to conduct in the context of CRA.2 The three schemesWe investigate three different schemes ?
thosebased on Section Names (S1), ArgumentativeZones (S2) and Core Scientific Concepts (S3):S1: The first scheme differs from the others in thesense that it is actually designed for abstracts.
Itis based on section names found in some scientificabstracts.
We use the 4-way classification from(Hirohata et al, 2008) where abstracts are dividedinto objective, method, results and conclusions.Table 1 provides a short description of each cate-gory for this and other schemes (see also this tablefor any category abbreviations used in this paper).S2: The second scheme is based on Argumenta-tive Zoning (AZ) of documents.
The idea of AZis to follow the knowledge claims made by au-thors.
Teufel and Moens (2002) introduced AZand applied it to computational linguistics papers.Mizuta et al (2005) modified the scheme for biol-ogy papers.
More recently, Teufel et al (2009) in-troduced a refined version of AZ and applied it tochemistry papers.
As these schemes are too fine-grained for abstracts (some of the categories donot appear in abstracts at all), we adopt a reducedversion of AZ which integrates seven categoriesfrom (Teufel and Moens, 2002) and (Mizuta et al,2005) - those which actually appear in abstracts.S3: The third scheme is concept-driven andontology-motivated (Liakata et al, 2010).
It treatsscientific papers as humanly-readable representa-tions of scientific investigations and seeks to re-trieve the structure of the investigation from thepaper as generic high-level Core Scientific Con-cepts (CoreSC).
The CoreSC is a 3-layer annota-tion scheme but we only consider the first layerin the current work.
The second layer pertains toproperties of the categories (e.g.
?advantage?
vs.?disadvantage?
of METH, ?new?
vs. ?old?
METHor OBJT).
Such level of granularity is rare in ab-stracts.
The 3rd layer involves coreference iden-tification between the same instances of each cat-egory, which is also not of concern in abstracts.With eleven categories, S3 is the most fine-grainedof our schemes.
CoreSC has been previously ap-plied to chemistry papers (Liakata et al, 2010,2009).3 Data: cancer risk assessment abstractsWe used as our data the corpus of CRA ab-stracts described in (Korhonen et al, 2009) whichcontains MedLine abstracts from different sub-domains of biomedicine.
The abstracts were se-lected so that they provide rich information aboutvarious scientific data (human, animal and cellu-lar) used for CRA.
We selected 1000 abstracts (inrandom) from this corpus.
The resulting data in-cludes 7,985 sentences and 225,785 words in total.4 Annotation of abstractsAnnotation guidelines.
We used the guidelines ofLiakata for S3 (Liakata and Soldatova, 2008), anddeveloped the guidelines for S1 and S2 (15 pageseach).
The guidelines define the unit (a sentence)and the categories of annotation and provide ad-vice for conflict resolution (e.g.
which categoriesto prefer when two or several are possible withinthe same sentence), as well as examples of anno-tated abstracts.Annotation tool.
We modified the annotation toolof Korhonen et al (2009) so that it could be used toannotate abstracts according to the schemes.
Thistool was originally developed for the annotation ofCRA abstracts according to the scientific evidencethey contain.
The tool works as a Firefox plug-in.Figure 1 shows an example of an abstract anno-tated according to the three schemes.Description of annotation.
Using the guidelinesand the tool, the CRA corpus was annotated ac-cording to each of the schemes.
The annotationproceeded scheme by scheme, independently, sothat annotations of one scheme were not based onany of the other two.
One annotator (a computa-tional linguist) annotated all the abstracts accord-ing to the three schemes, starting from the coarse-grained S1, then proceeding to S2 and finally tothe finest-grained S3.
It took 45, 50 and 90 hoursin total for S1, S2 and S3, respectively.The resulting corpus.
Table 2 shows the distri-bution of sentences per scheme category in the re-sulting corpus.100Table 1: The Three SchemesS1 Objective OBJ The background and the aim of the researchMethod METH The way to achieve the goalResult RES The principle findingsConclusion CON Analysis, discussion and the main conclusionsS2 Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.Objective OBJ A thing aimed at or sought, a target or goalMethod METH A way of doing research, esp.
according to a defined and regular plan; a special form of proce-dure or characteristic set of procedures employed in a field of study as a mode of investigationand inquiryResult RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc.
obtainedby calculationConclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction, induc-tion; a proposition deduced by reasoning from other propositions; the result of a discussion,or examination of a question, final determination, decision, resolution, final arrangement oragreementRelated work REL A comparison between the current work and the related workFuture work FUT The work that needs to be done in the futureS3 Hypothesis HYP A statement that has not been yet confirmed rather than a factual statementMotivation MOT The reason for carrying out the investigationBackground BKG Description of generally accepted background knowledge and previous workGoal GOAL The target state of the investigation where intended discoveries are madeObject OBJT An entity which is a product or main theme of the investigationExperiment EXP Experiment detailsModel MOD A statement about a theoretical model or frameworkMethod METH The means by which the authors seek to achieve a goal of the investigationObservation OBS The data/phenomena recorded within an investigationResult RES Factual statements about the outputs of an investigationConclusion CON Statements inferred from observations and results, relating to research hypothesisInter-annotator agreement.
We measured theinter-annotator agreement on 300 abstracts (i.e.
athird of the corpus) using three annotators (one lin-guist, one expert in CRA, and the computationallinguist who annotated all the corpus).
Accord-ing to Cohen?s Kappa (Cohen, 1960), the inter-annotator agreement for S1, S2, and S3 was ?
=0.84, ?
= 0.85, and ?
= 0.50, respectively.
Ac-cording to (Landis and Koch, 1977), the agree-ment 0.81-1.00 is perfect and 0.41-0.60 is mod-erate.
Our results indicate that S1 and S2 arethe easiest schemes for the annotators and S3 themost challenging.
This is not surprising as S3 isthe scheme with the finest granularity.
Its reliableidentification may require a longer period of train-ing and possibly improved guidelines.
Moreover,previous annotation efforts using S3 have used do-main experts for annotation (Liakata et al, 2009,2010).
In our case the domain expert and the lin-guist agreed the most on S3 (?
= 0.60).
For S1and S2 the best agreement was between the lin-guist and the computational linguist (?
= 0.87 and?
= 0.88, respectively).Table 2: Distribution of sentences in the scheme-annotated CRA corpusS1 OBJ METH RES CON61483 39163 89575 35564 Words2145 1396 3203 1241 Sentences27% 17% 40% 16% SentencesS2 BKG OBJ METH RES CON REL FUT36828 23493 41544 89538 30752 2456 1174 Words1429 674 1473 3185 1082 95 47 Sentences18% 8% 18% 40% 14% 1% 1% SentencesS3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON2676 4277 28028 10612 15894 22444 1157 17982 17402 75951 29362 Words99 172 1088 294 474 805 41 637 744 2582 1049 Sentences1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences5 Comparison of the schemes in terms ofannotationsThe three schemes we have used to annotate ab-stracts were developed independently and haveseparate guidelines.
Thus, even though they seemto have some categories in common (e.g.
METH,RES, CON) this does not necessarily guarantee thatthe latter cover the same information across allthree schemes.
We therefore wanted to investigatethe relation between the schemes and the extent ofoverlap or complementarity between them.We used the annotations obtained with eachscheme to create three contingency matrices forpairwise comparison.
We calculated the chi-squared Pearson statistic, the chi-squared like-101Figure 1: An example of an abstract annotated ac-cording to the three schemesS1S2S3lihood ratio, the contingency coefficient andCramer?s V (Table 3)1, all of which showed a def-inite correlation between rows and columns for thepairwise comparison of all three schemes.However, none of the above measures give anindication of the differential association betweenschemes, i.e.
whether it goes both directions andto what extent.
For this reason we calculated theGoodman-Kruskal lambda L statistic (Siegel andCastellan, 1988), which gives us the reduction inerror for predicting the categories of one annota-tion scheme, if we know the categories assignedaccording to the other.
When using the categoriesof S1 as the independent variables, we obtained alambda of over 0.72 which suggests a 72% reduc-tion in error in predicting S2 categories and 47% in1These are association measures for r x c tables.
We usedthe implementation in the vcd package of R (http://www.r-project.org/).predicting S3 categories.
With S2 categories beingthe independent variables, we obtained a reductionin error of 88% when predicting S1 and 55% whenpredicting S3 categories.
The lower lambdas forpredicting S3 are hardly surprising as S3 has 11categories as opposed to 4 and 7 for S1 and S2 re-spectively.
S3 on the other hand has strong predic-tive power in predicting the categories of S1 andS2 with lambdas of 0.86 and 0.84 respectively.
Interms of association, S1 and S2 seem to be morestrongly associated, followed by S1 and S3 andthen S2 and S3.We were then interested in the correspondencebetween the actual categories of the three schemes,which is visualized in Figure 2.
Looking at thecategories of S1, OBJ maps mostly to BKG and OBJin S2 (with a small percentage in METH and REL).S1 OBJ maps to BKG, GOAL, HYP, MOT and OBJTin S3 (with a small percentage in METH and MOD).S1 METH maps to METH in S2 (with a small per-centage in S2 OBJ) while it maps to EXP, METHand MOD in S3 (with a small percentage in GOALand OBJT).
S1 RES covers S2 RES and 40% REL,whereas in S3 it covers RES, OBS and 20% MOD.S1 CON covers S2 CON, FUT, 45% REL and a smallpercentage of RES.
In terms of the S2 vs S3 com-parison, S2 BKG maps to S3 BKG, HYP, MOT and asmall percentage of OBJT and MOD.
S2 CON mapsto S3 CON, with a small percentage in RES, OBSand HYP.
S2 FUT maps entirely to S3 CON.
S2METH maps to S3 METH, EXP, MOD, 20% OBJTand a small percentage of GOAL.
S2 OBJ mapsto S3 GOAL and OBJT, with 15% HYP, MOD andMOT and a small percentage in METH.
S2 RELspans across S3 CON, RES, MOT and OBJT, albeitin very small percentages.
Finally, S2 RES maps toS3 RES and OBS, with 25% in MOD and small per-centages in METH, CON, OBJT.
Thus, it appearsthat each category in S1 maps to a couple of cate-gories in S2 and several in S3, which in turn seemto elaborate on the S2 categories.Based on the above analysis of the categories,it is reasonable to assume a subsumption relationbetween the categories of the type S1 > S2 >S3, with REL cutting across several of the S3 cat-egories and FUT branching off S3 CON.
This isan interesting and exciting outcome given that thethree different schemes have such a different ori-gin.102Table 3: Association measures between schemes S1, S2, S3S1 vs S2 S1 vs S3 S2 vs S3X2 df P X2 df P X2 df PLikelihood Ratio 5577.1 18 0 5363.6 30 0 6293.4 60 0Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0Contingency Coeff 0.842 0.837 0.871Cramer?s V 0.901 0.885 0.725Figure 2: Pairwise interpretation of categories ofone scheme in terms of the categories of the other.6 Automatic identification of informationstructure6.1 FeaturesThe first step in automatic identification of infor-mation structure is feature extraction.
We chosea number of general purpose features suitable forall the three schemes.
With the exception of ournovel verb class feature, the features are similar tothose employed in related works, e.g.
(Teufel andMoens, 2002; Mullen et al, 2005; Hirohata et al,2008):History.
There are typical patterns in the infor-mation structure, e.g.
RES tends to be followedby CON rather than by BKG.
Therefore, we usedthe category assigned to the previous sentence asa feature.Location.
Categories tend to appear in typical po-sitions in a document, e.g.
BKG occurs often in thebeginning and CON at the end of the abstract.
Wedivided each abstract into ten equal parts (1-10),measured by the number of words, and defined thelocation (of a sentence) feature by the parts wherethe sentence begins and ends.Word.
Like many text classification tasks, we em-ployed all the words in the corpus as features.Bi-gram.
We considered each bi-gram (combina-tion of two word features) as a feature.Verb.
Verbs are central to the meaning of sen-tences, and can vary from one category to another.For example, experiment is frequent in METH andconclude in CON.
Previous works have used thematrix verb of each sentence as a feature.
Becausethe matrix verb is not the only meaningful verb,we used all the verbs instead.Verb Class.
Because individual verbs can result insparse data problems, we also experimented with anovel feature: verb class (e.g.
the class of EXPERI-MENT verbs for verbs such as measure and inject).We obtained 60 classes by clustering verbs appear-ing in full cancer risk assessment articles using theapproach of Sun and Korhonen (2009).POS.
Tense tends to vary from one category to an-other, e.g.
past is common in RES and past partici-103ple in CON.
We used the part-of-speech (POS) tagof each verb assigned by the C&C tagger (Curranet al, 2007) as a feature.GR.
Structural information about heads and de-pendents has proved useful in text classification.We used grammatical relations (GRs) returned bythe C&C parser as features.
They consist of anamed relation, a head and a dependent, and pos-sibly extra parameters depending on the relationinvolved, e.g.
(dobj investigate mouse).
We cre-ated features for each subject (ncsubj), direct ob-ject (dobj), indirect object (iobj) and second object(obj2) relation in the corpus.Subj and Obj.
As some GR features may suf-fer from data sparsity, we collected all the subjectsand objects (appearing with any verbs) from GRsand used them as features.Voice.
There may be a correspondence betweenthe active and passive voice and categories (e.g.passive is frequent in METH).
We therefore usedvoice as a feature.6.2 MethodsWe used Naive Bayes (NB) and Support VectorMachines (SVM) for classification.
NB is a sim-ple and fast method while SVM has yielded highperformance in many text classification tasks.NB applies Bayes?
rule and Maximum Like-lihood estimation with strong independence as-sumptions.
It aims to select the class c with maxi-mum probability given the feature set F :argmaxc P (c|F )=argmaxcP (c)?P (F |c)P (F )=argmaxc P (c)?P (F |c)=argmaxc P (c)?
?f?F P (f |c)SVM constructs hyperplanes in a multidimen-sional space that separates data points of differentclasses.
Good separation is achieved by the hyper-plane that has the largest distance from the nearestdata points of any class.
The hyperplane has theform w ?
x?
b = 0, where w is the normal vectorto the hyperplane.
We want to maximize the dis-tance from the hyperplane to the data points, or thedistance between two parallel hyperplanes each ofwhich separates the data.
The parallel hyperplanescan be written as:w?x?b = 1 andw?x?b = ?1, and the distancebetween the two is 2|w| .
The problem reduces to:Minimize |w|Subject to w ?
xi ?
b ?
1 for xi of one class,and w ?
xi ?
b ?
?1 for xi of the other.7 Experimental evaluation7.1 PreprocessingWe developed a tokenizer to detect the bound-aries of sentences and to perform basic tokenisa-tion, such as separating punctuation from adjacentwords e.g.
in tricky biomedical terms such as 2-amino-3,8-diethylimidazo[4,5-f]quinoxaline.
Weused the C&C tools (Curran et al, 2007) for POStagging, lemmatization and parsing.
The lemmaoutput was used for extracting Word, Bi-gram andVerb features.
The parser produced GRs for eachsentence from which we extracted the GR, Subj,Obj and Voice features.
We only considered theGRs relating to verbs.
The ?obj?
marker in a sub-ject relation indicates a verb in passive voice (e.g.
(ncsubj observed 14 difference 5 obj)).
To controlthe number of features we removed the words andGRs with fewer than 2 occurrences and bi-gramswith fewer than 5 occurrences, and lemmatized thelexical items for all the features.7.2 Evaluation methodsWe used Weka (Witten, 2008) for the classifica-tion, employing its NB and SVM linear kernel.
Theresults were measured in terms of accuracy (thepercentage of correctly classified sentences), pre-cision, recall, and F-Measure.
We used 10-foldcross validation to avoid the possible bias intro-duced by relying on any one particular split of thedata.
The data were randomly divided into tenparts of approximately the same size.
Each indi-vidual part was retained as test data and the re-maining nine parts were used as training data.
Theprocess was repeated ten times with each part usedonce as the test data.
The resulting ten estimateswere then combined to give a final score.
Wecompare our classifiers against a baseline methodbased on random sampling of category labels fromtraining data and their assignment to sentences onthe basis of their observed distribution.7.3 ResultsTable 4 shows F-measure results when using eachindividual feature alone, and Table 5 when usingall the features but the individual feature in ques-tion.
In these two tables, we only report the resultsfor SVM which performed considerably better thanNB.
Although we have results for most schemecategories, the results for some are missing due tothe lack of sufficient training data (see Table 2), ordue to a small feature set (e.g.
History alone).104Table 4: F-Measure results when using each in-dividual feature alonea b c d e f g h i j kS1 OBJ .39 .83 .71 .69 .52 .45 .45 .45 .54 .39 -METH - .47 .81 .74 .63 .49 - .46 .03 .42 .51RES - .76 .85 .86 .76 .70 .72 .69 .70 .68 .54CON - .72 .70 .65 .63 .53 .49 .57 .68 .20 -S2 BKG .26 .73 .69 .67 .45 .38 .56 .33 .33 .29 -OBJ - .13 .72 .68 .54 .63 - .49 .48 .20 -METH - .50 .81 .72 .64 .47 - .47 .03 .42 .51RES - .76 .85 .87 .76 .72 .72 .70 .69 .68 .54CON - .70 .73 .71 .62 .51 .40 .61 .67 .23 -REL - - - - - - - - - - -FUT - - - - - - - - - - -S3 HYP - - - - .67 - - - - - -MOT .18 .57 .70 .49 .39 .13 .36 .33 .30 .40 -BKG - - .54 .40 .21 - - .11 .06 .06 -GOAL - - .53 .33 .22 - .19 .31 - .25 -OBJT - - .73 .63 .60 .10 - .26 .32 - -EXP - .22 .63 .46 .33 .30 - .31 .07 .44 .25MOD - - - - - - - - - - -METH - - .82 .61 .39 .39 - .50 - .37 -OBS - .59 .75 .71 .63 .56 .56 .54 .48 .52 .47RES - - .87 .73 .41 .34 - .38 .24 .35 -CON - .74 .68 .65 .65 .50 .48 .49 .55 .21 -a-k: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR,Subj, Obj, VoiceLooking at individual features alone, Word,Bi-gram and Verb perform the best for all theschemes, and History and Voice perform the worst.In fact History performs very well on the trainingdata, but for the test data we can only use esti-mates rather than the actual labels.
The Voice fea-ture works only for RES and METH for S1 and S2,and for OBS for S3.
This feature is probably onlymeaningful for some of the categories.When using all but one of the features, S1 andS2 suffer the most from the absence of Location,while S3 from the absence of Word/POS.
VerbClass on its own performs worse than Verb, how-ever when combined with other features it per-forms better: leave-Verb-out outperforms leave-Verb Class-out.After comparing the various combinations offeatures, we found that the best selection of fea-tures was all but the Verb for all the schemes.
Ta-ble 6 shows the results for the baseline (BL), andthe best results for NB and SVM.
NB and SVM per-form clearly better than BL for all the schemes.The results for SVM are the best.
NB yields thehighest performance with S1.
Being sensitive tosparse data, it does not perform equally well on S2and S3 which have a higher number of categories,some of which are low in frequency (see Table 2).For S1, SVM finds all the four scheme categorieswith the accuracy of 89%.
F-measure is 90 forOBJ, RES and CON and 81 for METH.
For S2,the classifier finds six of the seven categories, withthe accuracy of 90% and the average F-measure ofTable 5: F-Measure results using all the features andall but one of the featuresALL A B C D E F G H I J KS1 OBJ .90 .89 .87 .92 .90 .90 .91 .91 .91 .92 .91 .88METH .80 .81 .80 .80 .79 .81 .79 .80 .80 .80 .81 .81RES .88 .90 .88 .90 .88 .90 .88 .88 .88 .89 .89 .90CON .86 .85 .82 .87 .88 .90 .90 .88 .89 .88 .88 .90S2 BKG .91 .94 .90 .90 .93 .94 .94 .91 .93 .94 .92 .94OBJ .72 .78 .84 .78 .83 .88 .84 .81 .83 .84 .78 .83METH .81 .83 .80 .81 .80 .85 .80 .78 .81 .81 .82 .83RES .88 .90 .88 .89 .88 .91 .89 .89 .90 .90 .90 .89CON .84 .83 .77 .83 .86 .88 .86 .87 .88 .89 .88 .81REL - - - - - - - - - - - -FUT - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0S3 HYP - - - - - - - - - - - -MOT .82 .84 .80 .76 .82 .82 .83 .78 .83 .83 .83 .83BKG .59 .60 .60 .54 .67 .62 .62 .59 .61 .61 .62 .61GOAL .62 .67 .67 .62 .71 .62 .67 .43 .67 .67 .67 .62OBJT .88 .85 .83 .74 .83 .85 .83 .74 .83 .83 .83 .85EXP .72 .68 .72 .53 .65 .70 .72 .73 .74 .74 .72 .68MOD - - - - - - - - - - - -METH .87 .86 .87 .66 .85 .89 .87 .88 .86 .86 .87 .86OBS .82 .81 .84 .72 .80 .82 .81 .80 .82 .82 .81 .81RES .87 .87 .88 .74 .87 .86 .87 .86 .87 .87 .87 .88CON .88 .88 .82 .88 .83 .87 .87 .84 .87 .88 .87 .86A-K: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR, Subj,Obj, VoiceWe have 1.0 for FUT in S2 probably because the size of the training data isjust right, and the model doesn?t overfit the data.
We make this assumptionbecause we have 1.0 for almost all the categories in the training data, but onlyfor FUT on the test data.Table 6: Baseline and best NB and SVM resultsAcc.
F-MeasureS1 OBJ METH RES CONBL .29 .23 .23 .39 .18NB .82 .85 .75 .85 .71SVM .89 .90 .81 .90 .90Acc.
F-MeasureS2 BKG OBJ METH RES CON REL FUTBL .25 .13 .08 .22 .40 .13 - -NB .76 .79 .25 .70 .83 .66 - -SVM .90 .94 .88 .85 .91 .88 - 1.0Acc.
F-MeasureS3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CONBL .15 - .10 .06 .04 .06 .11 - .13 .24 .15 .17NB .53 - .56 - - - .30 - .32 .61 .59 .62SVM .81 - .82 .62 .62 .85 .70 - .89 .82 .86 .8791 for the six categories.
As with S2, METH hasthe lowest performance (at 85 F-measure).
Theone missing category (REL) appears in our abstractdata with very low frequency (see Table 2).For S3, SVM uncovers as many as nine of the11 categories with accuracy of 81%.
Six cate-gories perform well, with F-measure higher than80.
EXP, BKG and GOAL have F-measure of 70,62 and 62, respectively.
Like the missing cate-gories HYP and MOD, GOAL is very low in fre-quency.
The lower performance of the higher fre-quency EXP and BKG is probably due to low pre-cision in distinguishing between EXP and METH,and BKG and other categories, respectively.1058 Discussion and conclusionsThe results from our corpus annotation (see Ta-ble 2) show that for the coarse-grained S1, all thefour categories appear frequently in biomedicalabstracts (this is not surprising because S1 was ac-tually designed for abstracts).
All of them can beidentified using machine learning.
For S2 and S3,the majority of categories appear in abstracts withhigh enough frequency that we can conclude thatalso these two schemes are applicable to abstracts.For S2 we identified six categories using machinelearning, and for S3 as many as nine, indicatingthat automatic identification of the schemes in ab-stracts is realistic.Our analysis in section 5 showed that there isa subsumption relation between the categories ofthe schemes.
S2 and S3 provide finer-grained in-formation about the information structure of ab-stracts than S1, even with their 2-3 low frequency(or missing) categories.
They can be useful forpractical tasks requiring such information.
For ex-ample, considering S3, there may be tasks whereone needs to distinguish between EXP, MOD andMETH, between HYP, MOT and GOAL, or betweenOBS and RES.Ultimately, the optimal scheme will depend onthe level of detail required by the application athand.
Therefore, in the future, we plan to conducttask-based evaluation of the schemes in the con-text of CRA and to evaluate the usefulness of S1-S3 for tasks cancer risk assessors perform on ab-stracts (Korhonen et al, 2009).
Now that we haveannotated the CRA corpus for S1-S3 and have amachine learning approach available, we are in anexcellent position to conduct this evaluation.A key question for real-world tasks is the levelof machine learning performance required.
Weplan to investigate this in the context of our task-based evaluation.
Although we employed fairlystandard text classification methodology in our ex-periments, we obtained high performance for S1and S2.
Due to the higher number of categories(and less training data for each of them), the over-all performance was not equally impressive for S3(although still quite high at 81% accuracy).Hirohata et al (2008) have showed that theamount of training data can have a big impacton our task.
They used c. 50,000 Medline ab-stracts annotated (by the authors of the Medlineabstracts) as training data for S1.
When using asmall set of standard text classification featuresand Conditional Random Fields (CRF) (Laffertyet al, 2001) for classification, they obtained 95.5%per-sentence accuracy on 1000 abstracts.
How-ever, when only 1000 abstracts were used for train-ing the accuracy was considerably worse; their re-ported per-abstract accuracy dropped from 68.8%to less than 50%.
Although it would be difficult toobtain similarly huge training data for S2 and S3,this result suggests that one key to improved per-formance is larger training data, and this is whatwe plan to explore especially for S3.In addition we plan to improve our method.
Weshowed that our schemes partly overlap and thatsimilar features and methods tend to perform thebest / worst for each of the schemes.
It is thereforeunlikely that considerable scheme specific tuningwill be necessary.
However, we plan to developour features further and to make better use of thesequential nature of information structure.
Cur-rently this is only represented as the History fea-ture, which provides a narrow window view to thecategory of the previous sentence.
Also we plan tocompare SVM against methods such as CRF andMaximum Entropy which have proved successfulin recent related works (Hirohata et al, 2008; Mer-ity et al, 2009).
The resulting models will be eval-uated both directly and in the context of CRA toprovide an indication of their practical usefulnessfor real-world tasks.AcknowledgmentsThe work reported in this paper was funded by theRoyal Society (UK), the Swedish Research Coun-cil, FAS (Sweden), and JISC (UK) which is fund-ing the SAPIENT Automation project.
YG wasfunded by the Cambridge International Scholar-ship.106ReferencesJ.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Mea-surement, 20:37?46.J.
R. Curran, S. Clark, and J. Bos.
2007.
Linguisticallymotivated large-scale nlp with c&c and boxer.
InProceedings of the ACL 2007 Demonstrations Ses-sion, pages 33?36.K.
Hirohata, N. Okazaki, S. Ananiadou, andM.
Ishizuka.
2008.
Identifying sections in scien-tific abstracts using conditional random fields.
InProc.
of 3rd International Joint Conference on Nat-ural Language Processing.A.
Korhonen, L. Sun, I. Silins, and U. Stenius.
2009.The first step in the development of text mining tech-nology for cancer risk assessment: Identifying andorganizing scientific evidence in risk assessment lit-erature.
BMC Bioinformatics, 10:303.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditionl random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ofICML.J.
R. Landis and G. G. Koch.
1977.
The measurementof observer agreement for categorical data.
Biomet-rics, 33:159?174.M.
Liakata and L.N.
Soldatova.
2008.
Guide-lines for the annotation of general scientific con-cepts.
Aberystwyth University, JISC Project Reporthttp://ie-repository.jisc.ac.uk/88/.M.
Liakata, Claire Q, and L.N.
Soldatova.
2009.
Se-mantic annotation of papers: Interface & enrichmenttool (sapient).
In Proceedings of BioNLP-09, pages193?200, Boulder, Colorado.M.
Liakata, S. Teufel, A. Siddharthan, and C. Batch-elor.
2010.
Corpora for the conceptualisation andzoning of scientific papers.
To appear in the 7th In-ternational Conference on Language Resources andEvaluation.J.
Lin, D. Karakos, D. Demner-Fushman, and S. Khu-danpur.
2006.
Generative content models for struc-tural analysis of medical abstracts.
In Proceedingsof BioNLP-06, pages 65?72, New York, USA.J.
Lin.
2009.
Is searching full text more effective thansearching abstracts?
BMC Bioinformatics, 10:46.S.
Merity, T. Murphy, and J. R. Curran.
2009.
Ac-curate argumentative zoning with maximum entropymodels.
In Proceedings of the 2009 Workshopon Text and Citation Analysis for Scholarly DigitalLibraries, pages 19?26.
Association for Computa-tional Linguistics.Y.
Mizuta, A. Korhonen, T. Mullen, and N. Collier.2005.
Zone analysis in biology articles as a basisfor information extraction.
International Journal ofMedical Informatics on Natural Language Process-ing in Biomedicine and Its Applications.T.
Mullen, Y. Mizuta, and N. Collier.
2005.
A baselinefeature set for learning rhetorical zones using full ar-ticles in the biomedical domain.
Natural languageprocessing and text mining, 7:52?58.P.
Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-Schuhmann, C. Lovis, and A. L. Veuthey.
2007.Using argumentation to extract key sentences frombiomedical abstracts.
Int J Med Inform, 76:195?200.H.
Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.2008.
Multi-dimensional classification of biomed-ical text: Toward automated, practical provision ofhigh-utility text to diverse users.
Bioinformatics,18:2086?2093.S.
Siegel and N. J. Jr. Castellan.
1988.
Nonparamet-ric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.L.
Sun and A. Korhonen.
2009.
Improving verb clus-tering with automatically acquired selectional pref-erence.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing.I.
Tbahriti, C. Chichester, Frederique Lisacek, andP.
Ruch.
2006.
Using argumentation to retrievearticles with similar citations.
Int J Med Inform,75:488?495.S.
Teufel and M. Moens.
2002.
Summarizing scientificarticles: Experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28:409?445.S.
Teufel, A. Siddharthan, and C. Batchelor.
2009.
To-wards domain-independent argumentative zoning:Evidence from chemistry and computational linguis-tics.
In Proc.
of EMNLP.I.
H. Witten, 2008.
Data mining: practical machinelearning tools and techniques with Java Implemen-tations.
http://www.cs.waikato.ac.nz/ml/weka/.107
