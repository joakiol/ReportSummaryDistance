Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88?92,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsFBK at WMT 2010: Word Lattices forMorphological Reduction and Chunk-based ReorderingChristian Hardmeier, Arianna Bisazza and Marcello FedericoFondazione Bruno KesslerHuman Language TechnologiesTrento, Italy{hardmeier,bisazza,federico}@fbk.euAbstractFBK participated in the WMT 2010Machine Translation shared task withphrase-based Statistical Machine Transla-tion systems based on the Moses decoderfor English-German and German-Englishtranslation.
Our work concentrates on ex-ploiting the available language modellingresources by using linear mixtures of large6-gram language models and on address-ing linguistic differences between Englishand German with methods based on wordlattices.
In particular, we use lattices to in-tegrate a morphological analyser for Ger-man into our system, and we present someinitial work on rule-based word reorder-ing.1 System overviewThe Human Language Technologies group at Fon-dazione Bruno Kessler (FBK) participated in theWMT 2010 Machine Translation (MT) evaluationwith systems for English-German and German-English translation.
While the English-Germansystem we submitted was relatively simple, weput some more effort into the inverse translationdirection to make better use of the abundanceof language modelling data available for Englishand to address the richness of German morphol-ogy, which makes it hard for a Statistical MachineTranslation (SMT) system to achieve good vocab-ulary coverage.
In the remainder of this section,an overview of the common features of our sys-tems will be given.
The next two sections providea more detailed description of our approaches tolanguage modelling, morphological preprocessingand word reordering.Both of our systems were based on the Mosesdecoder (Koehn et al, 2007).
They were simi-lar to the WMT 2010 Moses baseline system.
In-stead of lowercasing the training data and addinga recasing step, we retained the data in documentcase throughout our system, except for the mor-phologically normalised word forms described insection 3.
Our phrase tables were trained with thestandard Moses training script, then filtered basedon statistical significance according to the methoddescribed by Johnson et al (2007).
Finally, weused Minimum Bayes Risk decoding (Kumar andByrne, 2004) based on the BLEU score (Papineniet al, 2002).2 Language modellingAt the 2009 NIST MT evaluation, our system ob-tained good results using a mixture of linearly in-terpolated language models (LMs) combining datafrom different sources.
As the training data pro-vided for the present evaluation campaign againincluded a large set of language modelling corporafrom different sources, especially for English asa target language, we decided to adopt the samestrategy.
The partial corpora for English and theirsizes can be found in table 1.
Our base mod-els of the English Gigaword texts were trainedon version 3 of the corpus (LDC2007T07).
Wetrained separate language models for the new datafrom the years 2007 and 2008 included in ver-sion 4 (LDC2009T13).
Apart from the mono-lingual English data, we also included languagemodels trained on the English part of the addi-tional parallel datasets supplied for the French-English and Czech-English tasks.
All the mod-els were estimated as 6-gram models with Kneser-Ney smoothing using the IRSTLM language mod-elling toolkit (Federico et al, 2008).For technical reasons, we were unable to use allthe language models during decoding.
We there-fore selected a subset of the models with the fol-lowing data selection procedure:1.
For a linear mixture of the complete set of24 language models, we estimated a set of88Corpus n-gramsEuroparl v5 115,702,157News 1,437,562,740News commentary 10 10,381,511Gigaword v3: 6 models 7,990,828,834Gigaword 2007/08: 6 models 1,418,281,597109 fr-en 1,190,593,051UNDOC fr-en 333,120,732CzEng: 7 models 153,355,518Total: 24 models 12,649,826,140Table 1: Language modelling corpora for EnglishLMs PerplexityDEV EVAL2 188.57 181.385 163.68 158.9910 156.43 151.7315 154.71 144.9820 154.39 144.9124 154.42 144.92Table 2: Perplexities of LM mixturesoptimal interpolation weights to minimisethe perplexity of the mixture model on thenews-test2008 development set.2.
By sorting the mixture coefficients in de-scending order, we obtained an ordering ofthe language models by their importance withrespect to the development set.
We createdpartial mixtures by selecting the top n mod-els according to this order and retraining themixture weights with the same algorithm.Computing the perplexities of these partialmixtures on the news-test2008 (DEV) andnewstest2009 (EVAL) corpora shows that signif-icant improvements can be obtained up to a mix-tures size of about 15 elements.
As this size stillturned out to be too large to be managed by oursystems, we used a 5-element mixture in our finalsubmission (see table 3 for details about the mix-ture and table 4 for the evaluation results of thesubmitted systems).For the English-German system, the only cor-pora available for the target language were Eu-roparl v5, News commentary v10 and the mono-lingual News corpus.
Similar experiments showedthat the News corpus was by far the most impor-tant for the text genre to be translated and thatincluding language models trained on the otherWeight Language model0.368023 News0.188156 109 fr-en0.174802 Gigaword v3: NYT0.144465 Gigaword v3: AFP0.124553 Gigaword v3: APWTable 3: 5-element LM mixture used for decodingBLEU-cased BLEUen-deprimary 15.5 15.8secondary 15.3 15.6primary: only News language modelsecondary: linear mixture of 3 LMsde-enprimary 20.9 21.9secondary 20.3 21.3primary: morph.
reduction, linear mixture of 5 LMssecondary: reordering, only News LMTable 4: Evaluation results of submitted systemscorpora could even degrade system performance.We therefore decided not to use Europarl or Newscommentary for language modelling in our pri-mary submission.
However, we submitted a sec-ondary system using a mixture of language modelsbased on all three corpora.3 Morphological reduction anddecompounding of GermanCompounding is a highly productive part of Ger-man noun morphology.
Unlike in English, Ger-man compound nouns are usually spelt as sin-gle words, which greatly increases the vocabulary.For a Machine Translation system, this propertyof the language causes a high number of out-of-vocabulary (OOV) words.
It is likely that manycompounds in an input text have not been seen inthe training corpus.
We addressed this problem bysplitting compounds in the German source text.Compound splitting was done using the Gert-wol morphological analyser (Koskenniemi andHaapalainen, 1996), a linguistically informed sys-tem based on two-level finite state morphology.Since Gertwol outputs all possible analyses of aword form without taking into account the context,the output has to be disambiguated.
For this pur-pose, we used part-of-speech (POS) tags obtainedfrom the TreeTagger (Schmid, 1994) along witha set of POS-based heuristic disambiguation rules89provided to us by the Institute of ComputationalLinguistics of the University of Zurich.As a side effect, Gertwol outputs the base formsof all words that it processes: Nominative singu-lar of nouns, infinitive of verbs etc.
We decided tocombine the tokens analysed by Gertwol, whetheror not they had been decompounded and lower-cased, in a further attempt to reduce data sparse-ness, with their original form in a word lattice(see fig.
1) and to let the decoder make the choicebetween the two according to the translations thephrase table can provide for each.Our word lattices are similar to those used byDyer et al (2008) for handling word segmentationin Chinese and Arabic.
For each word that wassegmented by Gertwol, we provide exactly one al-ternative edge labelled with the component wordsand base forms as identified by Gertwol, after re-moving linking morphemes.
The edge transitionprobabilities are used to identify the source of anedge: their values are e?1 = 0.36788 for edges de-riving from Gertwol analysis and e0 = 1 for edgescarrying unprocessed words.
Tokens whose de-compounded base form according to Gertwol isidentical to the surface form in the input are rep-resented by a single edge with transition proba-bility e?0.5 = 0.606531.
These transition proba-bilities translate into a binary feature with values?1, ?0.5 and 0 after taking logarithms in the de-coder.
The feature weight is determined by Min-imum Error-Rate Training (Och, 2003), togetherwith the weights of the other feature functionsused in the decoder.
During system training, theprocessed version of the training corpus was con-catenated with the unprocessed text.Experiments show that decompounding andmorphological analysis have a significant impacton the performance of the MT system.
Afterthese steps, the OOV rate of the newstest2009test set decreases from 5.88% to 3.21%.
Us-ing only the News language model, the BLEUscore of our development system (measured onthe newstest2009 corpus) increases from 18.77to 19.31.
There is an interesting interaction withthe language models.
While using a linear mixtureof 15 language models instead of just the NewsLM does not improve the performance of the base-line system (BLEU score 18.78 instead of 18.77),the BLEU score of the 15-LM system increases to20.08 when adding morphological reduction.
Inthe baseline system, the additional language mod-els did not have a noticeable effect on translationquality; however, their impact was realised in thedecompounding system.4 Word reorderingCurrent SMT systems are based on the assump-tion that the word order of the source and the tar-get languages are fundamentally similar.
Whilethe models permit some local reordering, system-atic differences in word order involving move-ments of more than a few words pose major prob-lems.
In particular, Statistical Machine Transla-tion between German and English is notoriouslyimpacted by the different fundamental word orderin subordinate clauses, where German Subject?Object?Verb (SOV) order contrasts with EnglishSubject?Verb?Object (SVO) order.In our English-German system, we made theobservation that the verb in an SVO subordi-nate clause following a punctuation mark fre-quently gets moved before the preceding punctu-ation.
This movement is triggered by the Ger-man language model, which prefers verbs pre-ceding punctuation as consistent with SOV or-der, and it is facilitated by the fact that the dis-tance from the verb to the end of the precedingclause is often smaller than the distance to the endof the current phrase, so moving the verb back-wards results in a better score from the distance-based reordering model.
This tendency can becounteracted effectively by enabling the Mosesdecoder?s monotone-at-punctuation feature,which makes sure that words are not reorderedacross punctuation marks.
The result is a mod-est gain from 14.28 to 14.38 BLEU points(newstest2009).In the German-English system, we applied achunk-based technique to produce lattices repre-senting multiple permutations of the test sentencesin order to enable long-range reorderings of verbphrases.
This approach is similar to the reorder-ing technique based on part-of-speech tags pre-sented by Niehues and Kolss (2009), which re-sults in the addition of a large number of reorder-ing paths to the lattices.
By contrast, we assumethat verb reorderings only occur between shallowsyntax chunks, and not within them.
This makes itpossible to limit the number of long-range reorder-ing options in an effective way.We used the TreeTagger to perform shallowsyntax chunking of the German text.
By man-90Figure 1: Word lattice for morphological reductionSonst [drohe]VC , dass auch [weitere L?nder]NC [vom Einbruch]PC [betroffen sein w?rden]VC .Figure 2: Chunk reordering latticeBLEUtest-09 test-10Baseline 18.77 20.1+ chunk-based reordering 18.94 20.3Morphological reduction 19.31 20.6+ chunk-based reordering 19.79 21.1note: only News LM, case-sensitive evaluationTable 5: Results with morphological reduction andchunk reordering on newstest 2009/2010ual inspection of a data sample, we then identi-fied a few recurrent patterns of long reorderingsinvolving the verbs.
In particular, we focused onclause-final verbs in German SOV clauses, whichwe move to the left in order to approximate the En-glish SVO word order.
For each sentence a chunk-based lattice is created, which is then expandedinto a word lattice like the one shown in fig.
2.
Thelattice representation provides the decoder with upto three possible reorderings for a particular verbchunk.
It always retains the original word order asan alternative input.For technical reasons, we were unable to pre-pare a system with reordering, morphological re-duction and all language models in time for theshared task.
Our secondary submission with re-ordering is therefore not comparable with our bestsystem, which includes more language modelsand morphological reduction.
In subsequent ex-periments, we combined morphological reductionwith chunk-based reordering (table 5).
When mor-phological reduction is used, the reordering ap-proach yields an improvement of about 0.5 BLEUpercentage points.5 ConclusionsThere are three important features specific to theFBK systems at WMT 2010: mixtures of largelanguage models, German morphological reduc-tion and decompounding and word reordering.Our approach to using large language modelsproved successful at the 2009 NIST MT evalua-tion.
In the present evaluation, its effectivenesswas reduced by a number of technical problems,which were mostly due to the limitations of diskaccess throughput in our parallel computing en-vironment.
We are working on methods to re-duce and distribute disk accesses to large lan-guage models, which will be implemented in theIRSTLM language modelling toolkit (Federico etal., 2008).
By doing so, we hope to overcome thecurrent limitations and exploit the power of lan-guage model mixtures more fully.The Gertwol-based morphological reductionand decompounding component we used is aworking solution that results in a significant im-provement in translation quality.
It is an alterna-tive to the popular statistical compound splittingmethods, such as the one by Koehn and Knight(2003), incorporating a greater amount of linguis-tic knowledge and offering morphological reduc-tion even of simplex words to their base form inaddition.
It would be interesting to compare therelative performance of the two approaches sys-tematically.Word reordering between German and Englishis a complex problem.
Encouraged by the successof chunk-based verb reordering lattices on Arabic-English (Bisazza and Federico, 2010), we tried toadapt the same approach to the German-Englishlanguage pair.
It turned out that there is a largervariety of long reordering patterns in this case.Nevertheless, some experiments performed after91the official evaluation showed promising results.We plan to pursue this work in several directions:Defining a lattice weighting scheme that distin-guishes between original word order and reorder-ing paths could help the decoder select the morepromising path through the lattice.
Applying sim-ilar reordering rules to the training corpus wouldreduce the mismatch between the training data andthe reordered input sentences.
Finally, it would beuseful to explore the impact of different distortionlimits on the decoding of reordering lattices in or-der to find an optimal trade-off between decoder-driven short-range and lattice-driven long-rangereordering.AcknowledgementsThis work was supported by the EuroMatrixPlusproject (IST-231720), which is funded by the Eu-ropean Commission under the Seventh FrameworkProgramme for Research and Technological De-velopment.ReferencesArianna Bisazza andMarcello Federico.
2010.
Chunk-based verb reordering in VSO sentences for Arabic-English statistical machine translation.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and Metrics MATR, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proceedings of ACL-08: HLT, pages 1012?1020, Columbus, Ohio, June.
Association for Com-putational Linguistics.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Inter-speech 2008, pages 1618?1621.
ISCA.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qual-ity by discarding most of the phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic,June.
Association for Computational Linguistics.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proceedings ofEACL, pages 187?193.Philipp Koehn, Hieu Hoang, Alexandra Birch, et al2007.
Moses: open source toolkit for statistical ma-chine translation.
In Annual meeting of the Associa-tion for Computational Linguistics: Demonstrationsession, pages 177?180, Prague.Kimmo Koskenniemi and Mariikka Haapalainen.1996.
GERTWOL ?
Lingsoft Oy.
In RolandHausser, editor, Linguistische Verifikation.
Doku-mentation zur Ersten Morpholympics 1994, chap-ter 11, pages 121?140.
Niemeyer, T?bingen.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 169?176, Boston, Massachusetts, USA,May 2 - May 7.
Association for Computational Lin-guistics.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 206?214, Athens, Greece,March.
Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st annual meeting of the Association for Computa-tional Linguistics, pages 160?167, Sapporo (Japan).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia.ACL.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49.92
