Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 65?72,Prague, 28 June 2007. c?2007 Association for Computational LinguisticsCross Lingual and Semantic Retrieval for Cultural Heritage AppreciationIdan Szpektor, Ido DaganDept.
of Computer ScienceBar Ilan Universityszpekti@cs.biu.ac.ilAlon LavieLanguage Technologies Inst.Carnegie Mellon Universityalavie+@cs.cmu.eduDanny Shacham, Shuly WintnerDept.
of Computer ScienceUniversity of Haifashuly@cs.haifa.ac.ilAbstractWe describe a system which enhances theexperience of museum visits by providingusers with language-technology-based in-formation retrieval capabilities.
The sys-tem consists of a cross-lingual search en-gine, augmented by state of the art semanticexpansion technology, specifically designedfor the domain of the museum (history andarchaeology of Israel).
We discuss the tech-nology incorporated in the system, its adap-tation to the specific domain and its contri-bution to cultural heritage appreciation.1 IntroductionMuseum visits are enriching experiences: they pro-vide stimulation to the senses, and through them tothe mind.
But the experience does not have to endwhen the visit ends: further exploration of the ar-tifacts and their influence on the visitor is possibleafter the visit, either on location or elsewhere.
Onecommon means of exploration is Information Re-trieval (IR) via a Search Engine.
For example, a mu-seum could implement a search engine over a col-lection of documents relating to the topics exhibitedin the museum.However, such document collections are usuallymuch smaller than general collections, in particularthe World Wide Web.
Consequently, phenomena in-herent to natural languages may severely hamper theperformance of human language technology whenapplied to small collections.
One such phenomenonis the semantic variability of natural languages, theability to express a specific meaning in many dif-ferent ways.
For example, the expression ?Archae-ologists found a new tomb?
can be expressed alsoby ?Archaeologists discovered a tomb?
or ?A sar-cophagus was dug up by Egyptian Researchers?.
Ontop of monolingual variability, the same informationcan also be expressed in different languages.
Ignor-ing natural language variability may result in lowerrecall of relevant documents for a given query, espe-cially in small document collections.This paper describes a system that attempts tocope with semantic variability through the use ofstate of the art human language technology.
Thesystem provides both semantic expansion and crosslingual IR (and presentation of information) in thedomain of archaeology and history of Israel.
Itwas specifically developed for the Hecht Museumin Haifa, Israel, which contains a small but uniquecollection of artifacts in this domain.
The systemprovides different users with different capabilities,bridging over language divides; it addresses seman-tic variation in novel ways; and it thereby comple-ments the visit to the museum with long-lasting in-stillation of information.The main component of the system is a domain-specific search engine that enables users to specifyqueries and retrieve information pertaining to the do-main of the museum.
The engine is enriched by lin-guistic capabilities which embody an array of meansfor addressing semantic variation.
Queries are ex-panded using two main techniques: semantic expan-sion based on textual entailment; and cross-lingualexpansion based on translation of Hebrew queriesto English and vice versa.
Retrieved documents arepresented as links with associated snippets; the sys-tem also translates snippets from Hebrew to English.The main contribution of this work is, of course,the system itself, which was recently demonstrated65successfully at the museum and which we believecould be useful to a variety of museum visitor types,from children to experts.
For example, the systemprovides Hebrew speakers access to English doc-uments pertaining to the domain of the museum,and vice versa, thereby expanding the availabilityof multilingual material to museum visitors.
Moregenerally, it is an instance of adaptation of state ofthe art human language technology to the domainof cultural heritage appreciation, demonstrating howgeneral resources and tools are adapted to a specificdomain, thereby improving their accuracy and us-ability.
Finally, it provides a test-bed for evaluatingthe contribution of language technology in general,as well as specific components and resources, to alarge-scale natural language processing system.2 Background and MotivationInternet search is hampered by the complexity ofnatural languages.
The two main characteristics ofthis complexity are ambiguity and variability: theformer refers to the fact that a given text can beinterpreted in more than one way; the latter indi-cates that the same meaning can be linguistically ex-pressed in several ways.
The two phenomena makesimple search techniques too weak for unsophisti-cated users, as existing search engines perform onlydirect keyword matching, with very limited linguis-tic processing of the texts they retrieve.Specifically, IR systems that do not address thevariability in languages may suffer from lower re-call, especially in restricted domains and small doc-ument locations.
We next describe two prominenttypes of variability that we think should be ad-dressed in IR systems.2.1 Textual Entailment and Entailment RulesIn many NLP applications, such as Question An-swering (QA), Information Extraction (IE) and In-formation Retrieval (IR), it is crucial to recognizethat a specific target meaning can be inferred fromdifferent text variants.
For example, a QA systemneeds to induce that ?Mendelssohn wrote inciden-tal music?
can be inferred from ?Mendelssohn com-posed incidental music?
in order to answer the ques-tion ?Who wrote incidental music??.
This type ofreasoning has been identified as a core semantic in-ference task by the generic textual entailment frame-work (Dagan et al, 2006; Bar-Haim et al, 2006).The typical way to address variability in IR is touse lexical query expansion (Lytinen et al, 2000;Zukerman and Raskutti, 2002).
However, there arevariability patterns that cannot be described usingjust constant phrase to phrase entailment.
Anotherimportant type of knowledge representation is en-tailment rules and paraphrases.
An entailment ruleis a directional relation between two templates, textpatterns with variables, e.g., ?X compose Y ?X write Y ?.
The left hand side is assumed to en-tail the right hand side in certain contexts, underthe same variable instantiation.
Paraphrases can beviewed as bidirectional entailment rules.
Such rulescapture basic inferences in the language, and areused as building blocks for more complex entail-ment inference.
For example, given the above en-tailment rule, a QA system can identify the answer?Mendelssohn?
in the above example.
This needsparked intensive research on automatic acquisitionof paraphrase and entailment rules.Although knowledge-bases of entailment-rulesand paraphrases learned by acquisition algorithmswere used in other NLP applications, such as QA(Lin and Pantel, 2001; Ravichandran and Hovy,2002) and IE (Sudo et al, 2003; Romano et al,2006), to the best of our knowledge the output ofsuch algorithms was never applied to IR before.2.2 Cross Lingual Information RetrievalThe difficulties caused by variability are amplifiedwhen the user is not a native speaker of the languagein which the retrieved texts are written.
For exam-ple, while most Israelis can read English documents,fewer are comfortable with the specification of Eng-lish queries.
In a museum setting, some visitors maybe able to read Hebrew documents but still be rel-atively poor at searching for them.
Other visitorsmay be unable to read Hebrew texts, but still benefitfrom non-textual information that are contained inHebrew documents (e.g., pictures, maps, audio andvideo files, external links, etc.
)This problem is addressed by the paradigm ofCross-Lingual Information Retrieval (CLIR).
Thisparadigm has become a very active research areain recent years, addressing the needs of multilingualand non-English speaking communities, such as the66European Union, East-Asian nations and Spanishspeaking communities in the US (Hull and Grefen-stette, 1996; Ballesteros and Croft, 1997; Carbonellet al, 1997).
The common approach for CLIR isto translate a query in a source language to anothertarget language and then issue the translated queryto retrieve target language documents.
As explainedabove, CLIR research has to address various genericproblems caused by the variability and ambiguity ofnatural languages, as well as specific problems re-lated to the particular languages being addressed.3 Coping with Semantic Variability in IRWe describe a search engine that is capable of per-forming: (a) semantic English information retrieval;and (b) cross-lingual (Hebrew-English and English-Hebrew) information retrieval, allowing users topose queries in either of the two languages and re-trieve documents in both.
This is achieved by twosub-processes of the search engine: first, the en-gine performs shallow semantic linguistic inferenceand supports the retrieval of documents which con-tain phrases that imply the meaning of the translatedquery, even when no exact match of the translatedkeywords is found.
This is enabled by automatic ac-quisition of semantic variability patterns that are fre-quent in the language, which extend traditional lexi-cal query expansion techniques.
Second, the enginetranslates the original or expanded query to the tar-get language, based on several linguistic processesand a machine readable bilingual dictionary.
The re-sult is a semantic expansion of a given query to a va-riety of alternative wordings in which an answer tothis query may be expressed in the target languageof the retrieved documents.These enhancements are facilitated via a speci-fication of the domain.
As our system is specifi-cally designed to work in the domain of the historyand archaeology, we could focus our attention on re-sources and tools that are dedicated to this domain.Thus, for example, lexicons and dictionaries, whosepreparation is always costly and time consuming,were developed with the specific domain in mind;and textual entailment and paraphrase patterns wereextracted for the specific domain.
While the result-ing system is focused on visiting the Hecht Museum,the methodology which we used and discuss herecan be adapted to other areas of cultural heritage, aswell as to other narrow domains, in the same way.3.1 Setting Up a Basic Retrieval ApplicationWe created a basic retrieval system in two steps:first, we collected relevant documents; then, we im-plemented a search engine over the collected docu-ments.In order to construct a local corpus, an archae-ology expert searched the Web for relevant sitesand pages.
We then downloaded all the documentslinked from those pages using a crawler.
The expertlooked for documents in both English and Hebrew.In total, we collected a non-comparable bilingualcorpus for Archaeology containing several thousanddocuments in English and Hebrew.We implemented our enhanced retrieval moduleson top of the basic Jakarta Lucene indexing andsearch engine1.
All documents were indexed usingLucene, but instead of inflected words, we indexedthe lemma of each word (see detailed description ofour Hebrew lemmatization in Section 3.3).
In orderto match the indexed terms, query terms (either He-brew or English) were also lemmatized before theindex was searched, in a manner similar to lemma-tizing the documents.3.2 Query Expansion Using Entailment RulesAs described in Section 2.1, entailment rules had notbeen used as a knowledge resource for expanding IRqueries, prior to our work.
In this paper we use thisresource instead of the typical lexical expansion inorder to test its benefit.
Most entailment rules cap-ture relations between different predicates.
We thusfocus on documents retrieved for queries that con-tain a predicate over one or two entities, which weterm here Relational IR.
We would like to retrieveonly documents that describe an occurrence of thatpredicate, but possibly in words different than theones used in the query.
In this section we describein detail how we learn entailment rules and how weapply them in query expansion.Automatically Learning Entailment Rules fromthe Web Many algorithms for automatically learn-ing paraphrases and entailment rules have beenexplored in recent years (Lin and Pantel, 2001;1http://jakarta.apache.org/lucene/docs/index.html67Ravichandran and Hovy, 2002; Shinyama et al,2002; Barzilay and Lee, 2003; Sudo et al, 2003;Szpektor et al, 2004; Satoshi, 2005).
In this pa-per we use TEASE (Szpektor et al, 2004), a state-of-the-art unsupervised acquisition algorithm forlexical-syntactic entailment rules.TEASE acquires entailment relations for a giveninput template from the Web.
It first retrieves fromthe Web sentences that match the input template.From these sentences it extracts the variable instan-tiations, termed anchor-sets, which are identified asbeing characteristic for the input template based onstatistical criteria.Next, TEASE retrieves from the Web sentencesthat contain the extracted anchor-sets.
The retrievedsentences are parsed and the anchors found in eachsentence are replaced with their corresponding vari-ables.
Finally, from this retrieved corpus of parsedsentences, templates that are assumed to entail orbe entailed by the input template are learned.
Thelearned templates are ranked by the number of oc-currences they were learned from.Entailment Rules for Domain Specific Query Ex-pansion Our goal is to use the knowledge-base ofentailment rules learned by TEASE in order to per-form query expansion.
The two subtasks that ariseare: (a) acquiring an appropriate knowledge-baseof rules; and (b) expanding a query given such aknowledge-base.TEASE learns entailment rules for a given inputtemplate.
As our document collection is domainspecific, a list of such relevant input templates canbe prepared.
In our case, we used an archaeologyexpert to generate a list of verbs and verb phrasesthat relate to archaeology, such as: ?excavate?, ?in-vade?, ?build?, ?reconstruct?, ?grow?
and ?be locatedin?.
We then executed TEASE on each of the tem-plates representing these verbs in order to learn fromthe Web rules in which the input templates partici-pate.
An example for such rules is presented in Ta-ble 1.
We learned approximately 3900 rules for 80input templates.Since TEASE learns lexical-syntactic rules, weneed a syntactic representation of the query.
Weparse each query using the Minipar dependencyparser (Lin, 1998).
We next try to match the lefthand side template of every rule in the learnedknowledge-base.
Since TEASE does not identifythe direction of the relation learned between twotemplates, we try both directional rules that are in-duced from a learned relation.
Whenever a matchis found, a new query is generated, in which theconstant terms of the matched left hand side tem-plate are replaced with the constant terms of the righthand side template.
For example, given the query?excavations of Jerusalem by archaeologists?
and alearned rule ?excavation of Y by X ?
X dig in Y ?,a new query is generated, containing the terms ?ar-chaeologists dig in Jerusalem?.
Finally, we retrieveall the documents that contain all the terms of at leastone of the expanded queries (including the originalquery).
The basic search engine provides a score foreach document.
We re-score each document as thesum of scores it obtained from the different queriesthat it matched.
Figure 1 shows an example of ourquery expansion, where the first retrieved documentsdo not contain the words used to describe the predi-cate in the query, but other ways to describe it.All the templates learned by TEASE contain twovariables, and thus the rules that are learned can onlybe applied to queries that contain predicates overtwo terms.
In order to broaden the coverage of thelearned rules, we automatically generate also all thepartial templates of a learned template.
These aretemplates that contain just one of variables in theoriginal template.
We then generate rules betweenthese partial templates that correspond to the origi-nal rules.
With partial templates/rules, expansion forthe query in Figure 1 becomes possible.3.3 Cross-lingual IRUntil very recently, linguistic resources for Hebrewwere few and far between (Wintner, 2004).
The lastfew years, however, have seen a proliferation of re-sources and tools for this language.
In this work weutilize a relatively large-scale lexicon of over 22,000entries (Itai et al, 2006); a finite-state based mor-phological analyzer of Hebrew that is directly linkedto the lexicon (Yona and Wintner, 2007); a medium-size bilingual dictionary of some 24,000 word pairs;and a rudimentary Hebrew to English machine trans-lation system (Lavie et al, 2004).
All these re-sources had to be adapted to the domain of the Hechtmuseum.Cross-lingual language technology is utilized in68Figure 1: Semantic expansion example.
Note that the expanded queries that were generated in the first tworetrieved texts (listed under ?matched query?)
do not contain the original query.three different components of the system: Hebrewdocuments are morphologically processed to pro-vide better indexing; query terms in English aretranslated to Hebrew and vice versa; and Hebrewsnippets are translated to English.
We discuss eachof these components in this section.Linguistically-aware indexing The correct levelof indexing for morphologically-rich language hasbeen a matter of some debate in the information re-trieval literature.
When Arabic is concerned, Dar-wish and Oard (2002) conclude that ?Character n-grams or lightly stemmed words were found totypically yield near-optimal retrieval effectiveness?.Since Hebrew is even more morphologically (andorthographically) ambiguous than Arabic, and espe-cially in light of the various prefix particles whichcan be attached to Hebrew words, we opted for fullmorphological analysis of Hebrew documents be-fore they are indexed, followed by indexing on thelexeme.We use the HAMSAH morphological analyzer(Yona and Wintner, 2007), which was recently re-written in Java and is therefore more portable andefficient (Wintner, 2007).
We processed the entiredomain specific corpus described above and usedthe resulting lexemes to index documents.
This pre-processing brought to the foreground several omis-sions of the analyzer, mostly due to domain-specificterms missing in the lexicon.
We selected the onethousand most frequent words with no morphologi-cal analysis and added their lexemes to the lexicon.While we do not have quantitative evaluation met-rics, the coverage of the system improved in a veryevident way.Query translation When users submit a query inone language they are provided with the option to re-quest a translation of the query to the other language,thereby retrieving documents in the other language.The motivation behind this capability is that userswho may be able to read documents in a languagemay find the specification of queries in that languagetoo challenging; also, retrieving documents in a for-eign language may be useful due to the non-textualinformation in the retrieved documents, especially ina museum environment.In order to support cross-lingual query specifica-tion we capitalized on a medium-size bilingual dic-tionary that was already used for Hebrew to Eng-lish machine translation.
Since the coverage of thedictionary was rather limited, and many domain-specific items were missing, we chose the one thou-sand most frequent lexemes which had no transla-69Input Template Learned TemplateX excavate Y X discover Y , X find Y ,X uncover Y , X examine Y ,X unearth Y , X explore YX construct Y X build Y , X develop Y ,X create Y , X establish YX contribute to Y X cause Y , X linked to Y ,X involve in Ydate X to Y X built in Y , X began in Y ,X go back to YX cover Y X bury Y ,X provide coverage for YX invade Y X occupy Y , X attack Y ,X raid Y , X move into YX restore Y X protect Y , X preserve Y ,X save Y , X conserve YTable 1: Examples for correct templates that werelearned by TEASE for input templates.tions and translated them manually, augmenting thelexicon with missing Hebrew lexemes where neces-sary and expanding the bilingual dictionary to coverthis domain.In order to translate query terms we use the He-brew English dictionary also as an English-Hebrewdictionary.
While this is known to be sub-optimal,our current results support such an adaptation in lieuof dedicated directional bilingual dictionaries.Translating a query from one language to anothermay introduce ambiguity where none exists.
Forexample, the query term spinh ?vessel?
is unam-biguous in Hebrew, but once translated into Englishwill result in retrieving documents on both sensesof the English word.
Usually, this problem is over-come since users tend to specify multi-term queries,and the terms disambiguate each other.
However,a more systematic solution can be offered since wehave access to semantic expansion capabilities (in asingle language).
That is, expanding the query inthe source language will result in more query termswhich, when translated, are more likely to disam-biguate the context.
We leave such an extension forfuture work.Snippet translation When Hebrew documents areretrieved, we augment the (Hebrew) snippet whichthe system produces by an English translation.
Weuse an extended, improved version of a rudimentaryHebrew to English MT system developed by Lavieet al (2004).
Extensions include an improved mor-phological analysis of the input, an extended bilin-gual dictionary and a revised set of transfer rules,as well as a more modern transfer engine and amuch larger language model for generating the tar-get (English) sentences.The MT system is transfer based: it performs lin-guistic pre-processing of the source language (in ourcase, morphological analysis) and post-processingof the target (generation of English word forms), anduses a small set of transfer rules to translate localstructures from the source to the target and createtranslation hypotheses, which are stored in a lattice.A statistical language model is used to decode thelattice and select the best hypotheses.The benefit of this architecture is that domain spe-cific adaptation of the system is relatively easy, anddoes not require a domain specific parallel corpus(which we do not have).
The system has accessto our domain-specific lexicon and bilingual dictio-nary, and we even refined some transfer rules due topeculiarities of the domain.
One advantage of thetransfer-based approach is that it enables us to treatout-of-lexicon items in a unique way.
We considersuch items proper names, and transfer rules processthem as such.
As an example, Figure 2 depicts thetranslation of a Hebrew snippet meaning A jar fromthe early bronze period with seashells from the Nile.The word nilws ?Nile?
is missing from the lexicon,but this does not prevent the system from producinga legible translation, using the transliterated formwhere an English equivalent is unavailable.4 ConclusionsWe described a system for cross-lingual andsemantically-enhanced retrieval of information inthe cultural heritage domain, obtained by adaptingexisting state-of-the-art tools and resources to thedomain.
The system enhances the experience of mu-seum visits, using language technology as a vehi-cle for long-lasting instillation of information.
Dueto the novelty of this application and the dearth ofavailable multilingual annotated resources in thisdomain, we are unable to provide a robust, quan-70Figure 2: Translation exampleQuery Without Expansion With ExpansionRelevant Total Relevant Totalin Top 10 Retrieved in Top 10 Retrieveddiscovering boats 2 2 5 86growing vineyards 0 0 6 8Persian invasions 5 5 8 22excavations of the Byzantine period 10 37 10 100restoring mosaics 0 0 3 69Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieveddocuments (up to 100) for a sample of queries.titative evaluation of the approach.
A preliminaryanalysis of a sample of queries is presented in Ta-ble 2.
It illustrates the potential of expansion fordocument collections of narrow domain.
In whatfollows we provide some qualitative impressions.We observed that the system was able to learnmany expansion rules that cannot be induced frommanually constructed lexical resources, such as the-sauri or WordNet (Fellbaum, 1998).
This is espe-cially true for rules that are specific for a narrow do-main, e.g.
?X restore Y ?
X preserve Y ?.
Fur-thermore, the system learned lexical syntactic rulesthat cannot be expressed by a mere lexical substitu-tion, but include also a syntactic transformation.
Forexample, ?date X to Y ?
X go back to Y ?.In addition, since rules are acquired by searchingthe Web, they are not necessarily restricted to learn-ing from the target domain, but can be learned fromsimilar terminology in other domains.
For example,the rule ?X discover Y ?
X find Y ?
was learnedfrom contexts such as {X=?astronomers?
;Y =?newplanets?}
and {X=?zoologists?
;Y =?new species?
}.The quality of the rules that were automaticallyacquired is mediocre.
We found that although manyrules were useful for expansion, they had to bemanually filtered in order to retain only rules thatachieved high precision.Finally, we note that applying semantic query ex-pansion (using entailment rules), followed by Eng-lish to Hebrew query translation, results in query ex-pansion for Hebrew using techniques that were sofar applicable only to resource-rich languages, suchas English.AcknowledgementsThis research was supported by the Israel InternetAssociation; by THE ISRAEL SCIENCE FOUN-DATION (grant No.
137/06 and grant No.
1095/05);by the Caesarea Rothschild Institute for Interdisci-plinary Application of Computer Science at the Uni-versity of Haifa; by the ITC-irst/University of Haifacollaboration; and by the US National Science Foun-dation (grants IIS-0121631, IIS-0534217, and theOffice of International Science and Engineering).71We wish to thank the Hebrew Knowledge Centerat the Technion for providing resources for Hebrew.We are grateful to Oliviero Stock, Martin Golumbic,Alon Itai, Dalia Bojan, Erik Peterson, Nurit Mel-nik, Yaniv Eytani and Noam Ordan for their helpand support.ReferencesLisa Ballesteros and W. Bruce Croft.
1997.
Phrasaltranslation and query expansion techniques for cross-language information retrieval.
In ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 84?91.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second pascal recognising textual entail-ment challenge.
In Second PASCAL Challenge Work-shop for Recognizing Textual Entailment.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL.Jaime G. Carbonell, Yiming Yang, Robert E. Frederk-ing, Ralf D. Brown, Yibing Geng, and Danny Lee.1997.
Translingual information retrieval: A compar-ative evaluation.
In IJCAI (1), pages 708?715.Ido Dagan, Oren Glickman, and Bernardo.
Magnini.2006.
The pascal recognising textual entailment chal-lenge.
In Lecture Notes in Computer Science, Volume3944, volume 3944, pages 177?190.Kareem Darwish and Douglas W. Oard.
2002.
Term se-lection for searching printed Arabic.
In SIGIR ?02:Proceedings of the 25th annual international ACM SI-GIR conference on Research and development in in-formation retrieval, pages 261?268, New York, NY,USA.
ACM Press.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Language, Speech and Com-munication.
MIT Press.D.
A.
Hull and G. Grefenstette.
1996.
Querying acrosslanguages.
a dictionary-based approach to multilingualinformation retrieval.
In Proceedings of the 19th ACMSIGIR Conference, pages 49?57.Alon Itai, Shuly Wintner, and Shlomo Yona.
2006.
Acomputational lexicon of contemporary Hebrew.
InProceedings of The fifth international conference onLanguage Resources and Evaluation (LREC-2006).Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson,and Katharina Probst.
2004.
Rapid prototyping of atransfer-based Hebrew-to-English machine translationsystem.
In Proceedings of TMI-2004: The 10th Inter-national Conference on Theoretical and Methodolog-ical Issues in Machine Translation, Baltimore, MD,October.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
In Natural Lan-guage Engineering, volume 7(4), pages 343?360.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on Evalu-ation of Parsing Systems at LREC.S.
Lytinen, N. Tomuro, and T. Repede.
2000.
The use ofwordnet sense tagging in faqfinder.
In Proceedings ofthe AAAI00 Workshop on AI and Web Search.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of ACL.Lorenza Romano, Milen Kouylekov, Idan Szpektor, IdoDagan, and Alberto Lavelli.
2006.
Investigating ageneric paraphrase-based approach for relation extrac-tion.
In Proceedings of EACL.Sekine Satoshi.
2005.
Automatic paraphrase discoverybased on context and keywords between ne pairs.
InProceedings of IWP.Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of HLT.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic ie pattern acquisition.
In Pro-ceedings of ACL.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisition ofentailment relations.
In Proceedings of EMNLP.Shuly Wintner.
2004.
Hebrew computational linguis-tics: Past and future.
Artificial Intelligence Review,21(2):113?138.Shuly Wintner.
2007.
Finite-state technology as a pro-gramming environment.
In Alexander Gelbukh, edi-tor, Proceedings of the Conference on ComputationalLinguistics and Intelligent Text Processing (CICLing-2007), volume 4394 of Lecture Notes in Computer Sci-ence, pages 97?106, Berlin and Heidelberg, February.Springer.Shlomo Yona and Shuly Wintner.
2007.
A finite-statemorphological grammar of Hebrew.
Natural Lan-guage Engineering.
To appear.Ingrid Zukerman and Bhavani Raskutti.
2002.
Lexicalquery paraphrasing for document retrieval.
In Pro-ceedings of ACL.72
