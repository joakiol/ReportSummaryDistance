SESSION 5a: ACOUSTIC MODEL INGHy Murveit, ChairSRI InternationalSpeech Research and Technology ProgramMenlo Park, CA, 940251.
SUMMARY OF PAPERSThe session focused on acoustic modeling for speech rec-ognition; which can be segmented into three broad sub-areas: (1) feature xtraction, (2) modeling the features forthe speech source, and (3) estimation of the model parame-ters.
The papers in this session touched on all of these areas.Huang focuses on the feature representation.
Furui et al,Austin et al, and Kimbal et al discuss new models for thespeech source.
Hon and Lee, Hwang and Huang, andGauvain and Lee focus on parameter stimation issues.In "Minimizing Speaker Variation Effects for Speaker-Independent Speech Recognition," Huang discusses a fea-ture representation forspeech recognition that is less sensi-tive to the speaker.
It is a cepstral mapping technique wherethe mapping is done with neural networks.
A codeword-dependent cepstral-mapping etwork is estimated for eachof a group of different speaker types.
This cepstral mappingimproves the speaker-independent performance of theCMU system.In "Recent Topics in Speech Recognition Research at NTTLaboratories," Furui et al discuss three topics.
This firsttopic focuses on an improved model for speech.
TypicalHMM recognition systems make flame-to-frame indepen-dence assumptions.
Furui presented a technique aimed atminimizing this effect, using bigram-constrained HMMs,and showed an improvement when using this technique.
Healso discussed two issues in language modeling, one spe-cific to Japanese, and another showing how task-indepen-dent language models can be adapted to a task at hand.Austin et al ("Improving State-of-the-Art ContinuousSpeech Recognition Systems Using the N-Best Paradigmwith Neural Networks") and Kimbal et al ("RecognitionUsing Classification and Segmentation Scoring") discusssegment-level models for the speech source.
Austin pointsout that neural networks can be combined with HMMs toautomatically derive segment-level acoustic models thatreduce the effect of frame-to-frame independence assump-tions in standard HMMs.
He shows that proper parameterestimation techniques are key for these models and presentsa technique called N-best raining which improves the per-formance of his segmental model.
Kimbal focuses on thesegmentation aspects of segmental models.
He shows thatincorporating a probabilistic segmentation model improvesthe performance of the Boston University speech recogni-tion system.The following three papers discuss the area of parameterestimation.
"Vocabulary and Environment Adaptation inVocabulary-Independent Speech Recognition" by Hun andLee revisits the area of task independence, but this timefrom an acoustic point of view.
Traditional HMM-basedspeech-recognition systems work much better if theiracoustic training data use the same task/vocabulary as thetesting data.
Hon and Lee look at techniques for making thetraining data more general.
In particular, they examinenovel techniques that improve vocabulary-independentperformance by making the parameter-estimation tech-nique focus on the testing vocabulary.Hwang and Huang, in "Subphonetic Modelling for SpeechRecognition," discuss another parameter-estimation issue-,the issue of fled models.
Most large-vocabulary speech-rec-ognition systems must tie together estimates of certainparameters that would not otherwise have sufficient train-ing data to be estimated accurately.
Often this is done in aphonetic way.
For instance, the same allophone or allo-phone-in-context in different words would share parame-ters.
Hwang and Huang describe atechnique (similar to theIBM phenome technique) for automatically deriving theunits to be tied.In "MAP Estimation of Continuous Density HMM: Theoryand Applications," Gauvain and Lee discuss a parameterestimation technique based on Bayesian learning.
Theyshow that it is useful for parameter smoothing as well as forspeaker adaptation and discriminative training.
In speakeradaptation, speaker-independent models can be moved to aspeaker using a small amount of training since the speaker-independent models are used as priors.
Adapted speaker-independent performance never performed worse thanspeaker-dependent systems given the same amount ofspeaker-dependent training data.161
