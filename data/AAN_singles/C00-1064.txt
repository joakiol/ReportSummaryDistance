St ructura l  Feature  Se lect ion  For Eng l i sh -Korean  Stat is t ica lMach ine  Trans la t ionSeonho Kim, Juntae Yoon, Mansuk Song{ pobi, j tyoon, mssong} @decemb er.yonsei, ac.kr\ ] )ept .
of  Computer  Science,Yonsci  Univers i ty ,  Seoul,  KoreaAbstractWhen aligning texts in very different languages suchas Korean and English, structural features beyondword or phrase give useful intbrmation.
In this pa-per, we present a method for selecting struetm'alfeatures of two languages, from which we constructa model that assigns the conditional probabilitiesto corresponding tag sequences in bilingual English-Korean corpora.
For tag sequence mapl)ing 1)etweentwo langauges, we first, define a structural featurefllnction which represents tatistical prol)erties ofelnpirical distribution of a set of training samples.The system, based on maximmn entrol)y coneet)t, se-le(:ts only ti;atures that pro(luee high increases in log-likelihood of training salnl)les.
These structurallymat)ped features are more informative knowledge forstatistical machine translation t)etween English andKorean.
Also, the inforum.tion can help to reduce the1)arameter sl)ace of statisti('al alignment 1)y eliminat-ing synta(:tically uiflikely alignmenls.1 IntroductionAligned texts have been used for derivation of 1)ilin-gual dictioimries and terminoh)gy databases whichare useflfl for nlachine translation and cross lan-guages infornmtion retriewfl.
Thus, a lot of align-ment techniques have been suggested at; the sen-tence (Gale et al, 1993), phrase (Shin et al, 1996),nomt t)hrase (Kupiec, 1993), word (Brown et al,1993; Berger et al, 1996; Melamed, 1997), collo-cation (Smadja et al, 1996) and terminology level.Seine work has used lexical association measuresfor word alignments.
However, the association mea-sures could be misled since a word in a source lan-guage frequently co-occurs with more titan one wordin a target language.
In other work, iterative re-estimation techniques have beets emt)loyed.
Theywere usually incorporated with the EM algorithmmid dynmnic progranmfing.
In that case, the prob-al)ilities of aligmnents usually served as 1)arametersin a model of statistical machine translation.In statistical machine translation, IBM 1~5 mod-els (Brown et al, 1993) based on the source-chmmelmodel have been widely used and revised for manylanguage donmins and applications.
It has alsoshortconfing that it needs much iteration time forparameter estimation and high decoding complex-ity, however.Much work has been done to overcome the prob-lem.
Wu (1996) adopted chammls that eliminatesyntactically unlikely alignments and Wang et al(1998) presented a model based on structures of twola.nguages.
Tilhnann et al (1997) suggested thedynanfie programming lmsed search to select thebest alignment and preprocessed bilingual texts toremove word order differences.
Sate et al (1998)and Och et al (1998) proposed a model for learn-ing translation rules with morphological informationmid word category in order to improve statisticaltranslation.Furthemlore, llla,lly researches assullle(t Olle-to-one correspondence due to the coml)lexity and com-Imtati(m time of statistical aliglunents.
Althoughthis assumption Ire'ned out t;o 1)e useful for align-ment of close lallguages uch as English and French,it is not, applicabh~ to very different languages, inparticular, Korean and English where there is rarely(:lose corresl)ondence in order at the word level.
Forsuch languages, even phrase level alignment, not tomei~tion word aligmnent, does not gives good trans-lation due to structural diflbrence.
Itence, structuralfeatures beyond word or t)hrase should t)e consid-ered to get t)etter translation 1)etween English andKoreml.
In addition, the construction of structuralbilingual texts would be more informative for ex-tracting linguistic knowledge.In this paper, we suggest a method for structuralmat)t)ing of bilingual language on the basis of themaximum entorl)y and feature induction fl'alnework.Our model based on POS tag sequence mapl)ing hastwo advantages: First;, it can reduce a lot of 1)armne-ters in statistical machilm translation by eliminatingsyntactically unlikely aligmnents.
Second, it: can beused as a t)reprocessor for lexical alignments of bilin-gual corpora although it; (:an be also exl)loited 1)y it-self tbr alignment.
In this case, it would serve as thefirst stet) of alignment for reducing the 1)arametersI)ace.4392 Mot ivat ionIn order to devise parameters for statistical model-ing of translation, we started our research from theIBM model which has bee:: widely used by :nanyresearches.
The IBM model is represented with theformula shown in (1)l 17tv(f, al ) = I I  I-I t(fJ l%)d(jlaj,m, l)i=1 j= l(1)Here, n is the fertility probability that an Englishword generates n h'end:  words, t is tim aligmnentprobability that the English word c generates theFrench word f ,  and d is the distortion probabilitythat an English word in a certain t)osition will gener-ate a lh'ench word in a certain 1)osition.
This formulais Olm of many ways in which p(f, ale ) can tie writtm.as the product of a series of conditional prot)at)ilities.In above model, the distortion probability is re--lated with positional preference(word order).
SinceKorean is a free order language, the probability isnot t~asible in English-Korean translation.Furthermore, the difference between two lan-guages leads to the discordance between words thatthe one-to-one correst)ondence b tween words gen-erally does not keel).
The n:odel (1), however, as--sumed that an English word cat: be connected withmultiple French words, but that each French wordis connected to exactly one English word inch:dingthe empty word.
hl conclusion, many-to-:nany :nap--pings are not allowed in this model.According to our ext)eri:nent, inany-to-nmnymappings exceed 40% in English and Korean lexicalaligninents.
Only 25.1% of then: can be explainedby word for word correspondences.
It means that weneed a statistical model which can lmndle phrasalmat) pings.In the case of the phrasal mappings, a lot of pa-rameters hould be searched eve:: if we restrict thelength of word strings.
Moreover, in order to prop--erly estimate t)arameters we need much larger voI--ume of bilingual aligned text than it in word-for-word modeling.
Even though such a large corporaexist sometimes, they do not come up with the lex--ical alignments.For this problem, we here consider syntactic fea-tures which are importmlt in determining structures.A structural feature means here a mapt)ing betweentag sequences in bilingual parallel sentences.If we are concerned with tag sequence alignments,it is possible to estimate statistical t)armneters ina relatively small size of corpora.
As a result, wecan remarkably reduce the problem space for possi-ble lexical alignments, a sort of t probability in (1),which improve the complexity of a statistical ma-chine translation model.If there are similarities between corresponding tagsequences in two language, tile structural featureswould be easily computed or recognized.
However,a tag sequence in English can be often translatedinto a completely different tag sequence in Koreanas follows.can/MD -+ ~-, cul/ENTR1 su/NNDE1 'iss/ AJMAda/ENTEIt nmans that similarities of tag features between twolanguages are not; kept all the time and it is neces-saw to get the most likely tag sequence mappingsthat reflect structural correspondences between twolanguages.In this paper, the tag sequence mappings are ob-taind by automatic feature selection based on themaximum entropy model.3 Prob lem Set t ingIn tiffs ctlat)ter, we describe how the features arerelated to the training data.
Let tc be an Englishtag sequence and tk be a Korean tag sequence.
LetTs be the set of all possible tag sequence niapI)ings ina aligned sentence, S. We define a feature function(or a feature) as follows:1 pair(t~,tk) C "\]-sf(t~,tk) = 0 othcrwi.s'cIt indicates co-occurrence information l)etweentags appeared in Ts.
f(t?,tk) expresses the infor-mation for predicting that te maps into ta.. A fea-ture means a sort of inforination for predicting some-thing.
In our model, co-occurrence information onthe same aligned sentence is used for a feature, whilecontext is used as a feature in Inost of systems usingmaximum entropy.
It can be less informative thancontext.
Hence, we considered an initial supervisionand feature selection.Our model starts with initial seed(active) featuresfor mapI)ing extracted by SUl)ervision.
In the nextstep, thature pool is constructed from training sam-ples fro:n filtering and oifly features with a large gainto the model are added into active feature set.
Thefinal outputs of our model are the set of active t'ea-tures, their gain values, and conditional probabilitiesof features which maximize the model.
Tim resultscan be embedded in parameters of statistical ma-chine translation and hell) to construct structuralbilingual text.Most alignment algorithm consists of two steps:(1) estimate translation probabilities.
(2) use these probabilities to search for most t)roba-ble alignment path.Our study is focused on (1), especially the part oftag string alignments.Next, we will explain the concept of the model.We are concerned with an ot)timal statistical inodelwhich can generate the traiifing samples.
Nmnely,our task is to construct a stochastic model that pro-440(1) duces outl)ut tag sequenc0, "~k, given a tag sequence ~+~,-~.To The l)roblem of interest is to use Salnt/les of ?
--J~\,What ....tagged sentences to observe the/)charier of the ran- u~, ,~(loin t)roeess.
'rile model p estinmtes tile conditional tt'2,Yprobability that tile process will outlmt t,~, given t~.. ~ ,~/~ ,o~!
!It is chosen out of a set of all allowed probability o~,~ ~e}~..?0,, me (tistributions .
.
.
.The fbllowing steps are emt)loyed for ()tit" model, v~ /Input: a set L of POS-labeled bilingual alignedsentences.I.
Make a set ~: of corresl)ondence pairs of tagsequences, (t~, tk) from a small portion of L bysupervision.2.
Set 2F into a set of active features, A.3.
Maximization of 1)arameters, A of at:tire fea-tures 1)y I IS(hnproved Iterative Sealing) algo-rithm.4.
Create a feature pool set ?9 of all possible align-nmnts a(t(,, tk) from tag seqllellces of samples.5.
Filter 7 ) using frequency and sintilarity with M.6.
Coml)ute the atit)roximate gains of tkmtm:es in"p.7.
Select new features(A/') with a large gain vahle,and add A.Outt)ut: p(tklt~,)whcrc(t(,, t~.)
C M and their Ai.We I)egan with training samples comi)osed ofEnglish-Korean aligned sentence t)airs, (e,k).
Sincethey included long sentences, w(', 1)roke them intoshorter ones.
The length of training senl;en(:es waslimited to ml(h',r 14 on the basis of English.
It isreasona,bh; \])(',(:&llSe we are interested in not lexicalalignments lint tag sequence aliglmients.
The sam-ples were tagged using brill's tagger and qVIorany'that we iml)lenmnted as a Korean tagger.
Figure \]shows the POS tags we considered.
For simplicity,we adjusted some part of Brill's tag set.In the, sut)ervision step, 700 aligned sentences wereused to construct he tag sequences mal)I)ings wlfichare referred to as an active feature set A.
As Fig-ure 2 shows, there are several ways in constructingthe corresl)ondem;es.
We chose the third mappingalthough (1) can be more useflll to explain Koreanwith I)redieate-argunmnt structure.
Since a subjectof a English sentence is always used for a subjecttbrln in Korean, we exlcuded a subject case fi'onl ar-gulnents of a l/redicate.
For examl)le, 'they' is onlyused for a subject form, whereas 'me' is used for aobject form and a dative form.II1 tile next step, training events, (t,:, It.)
are con-structed to make a feature 1)eel froln training sam-pies.
The event consists of a tag string t,, of a English(2) (31?
~ lm-~" - -  + ~ '~WhateverFigure 2: Tag sequence corresl)ondences at thephrase level1)OS-tagged sentence and a tag string tL~ of the cor-responding Korean POS-tagged sentence and it Callbe represented with indicator functions fi(t~, tk).For a given sequence, the features were drawnfl'om all adjacent i)ossible I)airs and sonic interruptedpairs.
Only features (tci, tfii ) out of the feature poolthat meet the following conditions are extracted.?
#(l, ei,t~:i) _> 3, # is count?
there exist H:.~,, where (t(,i,tt.~.)
in A and thesimilarity(sanle tag; colin|;) of lki an(1 tkx _> 0.6Table \] shows possible tL'atures, for a given alignedsentence , 'take her out - g'mdCOrcul baggcurodcrfleoflara'.Since the set of the structural ti;atm'es for align-ment modeling is vast, we constructed a maximumentrol)y model for p(tkltc) by the iterative modelgrowing method.4 Maximum Ent ropyTo explain our method, we l)riefly des(:ribe the con-(:ept of maximum entrol)y.
Recently, many al)-lnoaches l)ased on the maximum entroi)y lnodel havet)een applied to natural anguage processing (BergereL al., \]994; Berger et al, 1996; Pietra et al, 1997).Suppose a model p which assigns a probability toa random variable.
If we don't have rely knowledge,a reasonal)le solution for p is the most unifbrnl dis-tribution.
As some knowledge to estilnate the modelp are added, tile solution st)ace of p are more con-strained and the model would lie close to the ol\]timalprobability model.For the t)url/ose of getting tile optimal 1)robabilitymodel, we need to maxi\]nize the unifl)rnlity undersome constraints we have.
ltere, the constraints arerelated with features.
A feature, fi is usually rel/re -sented with a binary indicator funct, ion.
The inlpor-tahoe of a feature, fi can be identified by requiringthat the model accords with it.As a (:onstraint, the expected vahle of fi with re-spect to tile model P(fi) is supposed to be the sameas tile exl)ected value of fi with respect o empiri(:aldistril)ution in training saml)les, P(fi).441TAGcbDTPWJJJJSMDNNPPDTPRPRBRBSSYMUHVBDVBNWP$NOTBEDBEGHVDDODDESCRIPTIONcommaconjunction,coordinatingdeterminerforeign wordadjective, ordinaladjective, superlativemodal auxiliarynoun, proper, singularpre-determinerpronoun, personaladverbadverb, superlativesymbolinterjectionverb, past tenseverb, past participleWH-pronoun, possessivenotbe verb, past tensebe verb, present participlehave verb, past participledo verb, past tenseTAGCDEXINJ JRLSNNNNPSPOSPRP$RBRRPTOVBPVBGWDTWR8BEPBENHVPDOPDONDESCRIPTIONsentence terminator TAGnumeral, cardinalexistential there NNIN1preposition, subordinating NNIN2adjective, comparative NNDE1 NNDE2 list item marker PNnoun, common NUnoun, proper, plural VBMAgenitive marker AJMApronoun, possessive COadverb, comparative AXparticle ADCOto or infinitive marker APSEverb, present tense CJverb, present participle ANCOWH-determiner ANDEWH-adverb ANNUbe verb.
present tense EXbe verb, past participle LQhave verb, present tense RQdo verb, present tense SYdo verb, past participlePOSproper nouncommon nouncommon-dependent noununit-dependent nounpronounnumberverbadjectivecopulaauxiliary verbconstituent adverbsentential adverbconjunctive adverbconfigurative adnominaldemonstrative adnominalnumeral adnominalexclaminationleft quotation markright quotation marksymbolsTAGPPCA1PPCA2PPCA3PPCA4PPADPPCJPPAUENTEENCO1ENCO2ENCO3ENTRtENTR2ENTR3ENCMPESFPFCMSOFigure 1: English Tags (left) and Korean Tags (right)POSnominative postpositionaccusative postpositionpossessive postpositionvocative postpositionadverbial postpositionconjunctive postpositionauxiliary postpositionfinal endingcoordinate endingsubordinate endingauxiliary endingadnominal endingnominal endingadverbial endingending+postpositionpre-endingsuffixprefixcommatermination~P~II'~ l lq  l l l l  l l~l '~l~t I l l i i LU i I f ( i |  I I  IL'II|II~ ;'~ ;~ l i l | ' t l l l  I I I l l  I LI~M\[VBI'+IN\] [take+out\] \[1+3\]\[wp\] \[tak(q \[t\]\[VBP+PI~P\] \[take+her\] \[1+2\]\[W3P+PRP+IN\] \[take+her+out\] \[1+2+3\]\[PRP\] [ho,-\] [2\]\[IN\] \[out\] \[3\]\[I'PCA2+P1)AD-FVBMA\] \[rcul+euro+deryeoga\] \[2+4+5\]\[PN\] b.... :/~o\] i l l\[PI)AI)+VBIvlA-FENTE\] \[reu/+cure-I- dcrycoga+ra\] \[4+5+6\]\[NNIN2\] [bagg \] \[3\]\[NNIN2+PPAD\] \[bagg+euro\] \[3+41\[ENTE\] \[ra\] [6\]\[P1)AD+VBMA\] \[curo+deryeoga\] \[4+5\]\[PPAD+VBMA+ENTE\] [euro+deryeoga+ra\] \[4+5+6\]\[PPCA2+NNIN2+PPAD+VBMA\] \[reul+bagg+curo+ deryeoga \] \[2+3+4+5\]\[PPCA2+NNIN2+PPAI)+VBMA+ENTE\] \[reul+bagg+euro+dcryeoga+ra \] \[2+3+4+5+6\]\[P1)CA2+NNIN2+PPAD+VBMA\] \[renl+deryeoga \] \[2+3+4+5\]\[PPCA2+NNIN2+Pt)AD+VBMA+ENTE\] \[reul+deryeoga+ra\] \[2+3+4+5+6\]Table 1: possible tag sequencesIn sun1, the maxilnunl entropy fralnework findsthe model which has highest entropy(most uniform)~given constraints.
It is related to the constrainedoptimization.
To select a model from a constrainedset, C of allowed l)rol)ability distributions, the modelp, C C with maximum entropy H(p) is chosen.In general, for the constrained optimization prob-lem, Lagrange inultipliers of the number of featurescan be used.
However, it was proved that the modelwith maximum entropy is equivalent o the modelthat maximizes the log likelihood of the trainingsamples like (2) if we can assume it as an exponentialmodel.hi (2), the left side is Lagrangian of the condi-.tional entropy and the right side is inaxilnlHn log-.likelihood.
We use the right side equation of (2) toselect I. for the best model p,.~,g,,~..~,(- ~.,,.
~(~)v(yl~)logv(vlx)+~,,(v(f,)-~(/,))) (2):a,'9,,,ax~, .,,.
~(x,v)lo.~n,(ylx)Since t ,  cannot be tbund analytically, we usethe tbllowing improved iterative scaling algorithm tocolnpute I ,  of n active features in .4 in total sam--ples.1.
Start with l i  = 0 for all i 6 {1 ,2 , .
.
.
,n}2.
Do for ca.oh i ~ { \ ] ,2 , .
.
.
,n}  :(a) Let AAi be the solution to the log likeli-hood(b) Update the value of Ai into l i  + A,h,~.
..... ~(.,,v)A(:~,v)where AAi = log ~, : ,  ~i.~)v~(?11.~)/~(.~,v)px(yl:r) = ~-A--e(~ x'f'("':')) zx (:~.)
' ,z (x) = E:, c(E ,3.
Stop if not all the Ai have converged, otherwisego to step 2Tile exponential model is represented as(3).
Here,l i  is the weight of feature f i .
In ore" model, sinceonly one feature is applied to each pair of x and y,it can be represented as (4) and fi is the featurerelated with x and y.~(ylx) = ~ i  C'f'(x'Y) (3)cAifi(x,Y)= (4)4425 Feature  select ionOnly a small subset of features will 1)e emph)yed ina model by sele(:ting useflfl feal;m'es from (;tie flmture1)ool 7 ).
Let 1).,4 lie (;tie optimal mo(lel constrainedby a set of active features M and A U J'i 1)e ,/lfi.
Le(;PAf~ be the ot)timal model in the space of l)rol)abil-ity distribution C(Af i ) .
The optimal model can betel)resented as (5).
Here, the optimal model meansa maxilmnn entropy nlodd.1v~ :, = z,,.
(:,;) p'~ (:11,)::' ("'")zo,(: .)
= ~ v.~(::l:,,)c"S'(*'"> (5)YThe imi)rovement of l;he model regarding the ad-dition of a single feature f i  can be estiumted by mea-suring the difference of maximmn log-likelihood be-tween L(pAf~) and L(pA).
We denote the gain oft~ature f i t i y  A(~lfi) an(l it can be r(!t/resented in(G).A(A .
I 'd  - .,..,:,;,,cAI~(.
)('A:,(,,,) = J~(>t:,)-- L(v,O= _ ~(:~)~, .~( : , / I , . )
:  :'('''')x y?
'~P(.fi) IS)Note that a model PA has a, set of t)arameters Awhich means weights of teatures.
The m(idel P.Aflcontains the l )a ra .
lnetc .
rs  an( I  the  new \[)a.l'a, lllCi;('~r (11with l'eSl)ect () the t'eal;ure fi.
W'hen adding a newfeature to A, the optimal values (if all parame(ers ofprobability (listril)u(,ion change.
To make th(; (:om-i)utation of feature selection tractal)le, we al)l)roxi-mate that the addition of a feature f i  affec(;s onlythe single 1)aranxeter a, as shown in (5).qShe following a.lgoritlnn is used for (;omputing thegain of the model with rest)ect o fi.
We referredto the studies of (Berger et al, 1996; Pietra e.t al.,1997).
We skip tile detailed contents and 1)root~.1.
Let1 i f  P(fi) <_ PA(J;)r = -1  oth, erwise2.
Set a0 = 03.
Repeat the following until GAf f (%, )  has con-verged :Co i l l l ) l l te  0@1,+ i frOll l  og n l lS i l lga log (1 !
-~6t:' ( ' "~)  ~ ctn+l = (xn + 7" ,.
(;~:~(~,,):Compute GaV~ (a,~+l) usingGAA (a)  = - Ea,/3(a,') log Z,,(:,:) + ctf)(fi) ,c'A:, (,~) = ~(k)  - Ex  ~(~0M( : , . )
,G"  :ct~ A:,, , = -- E.~ P(")V2i:, ((fi -- M(;,;))" la;)set description # of disjoint totalfeatures cvtultsA active feat m'es 1483 4113P feature Calldidat, es 3172 63773N new f'eaLures 97 5503Table 2: Summery of Features Selectedwhere  (:~ ~ (l~n+ 1A f~ = A u f~,M(z)  - p~f~ (fila-) ,PP4S, (fi l ':) --- E .
~'~s, (:,?l:r)k(:., ~J)d. Set ~ AL(Af i )  <-- GAS,(ct.)This algorittun is iteratively comtmted using Net-Wel l 'S  method.
\?e cmt recognize the iml)ortance of afl;ature with the gain value.
As mentioned above, itmeans how much the feature accords with the model.We viewed the feature as tile information that Q. andt, occur together.6 Exper imenta l  resu l t sThe total saml)les consists of 3,000 aligned Sellteiicepairs of English-Korean, which were extracted fromnews on the web site of 'Korea Times' and a. maga-zine fl)r English learning.In the initial step, we manually constucted (;tiecorrespondences of tag sequences with 700 POS-tagged sentence I)airs.
hi the SUl)ervision step,we extracte(t l.,d83 correct tag sequence corresl)on-it(miles its shown in Table 2, and it work as activefeatures.
As a feature I)OOI, 3,172 (lisjoint %a(;uresof tag sequence ma.I)pings were retrieved.
1% is veryimportant o make atomic thatures.We maxinfized A of active features with resl)ectto total smnples using improved the iterative scal-ing algoritlun.
Figure 3 shows Ai of each feature.f(Q31,:P+.m,ttO C A.
There a.re nlany corresl)on-dence 1)atterns with resl)ect o the Englsh tag string,'BEP+J J ' .Note that p(tt~lQ) is comtmted by the exponentialmodel of (4) mid the conditional probability is thesaine with empirical probal)ility in (7).
Since thewflue of p(ylx)  shows the maxinmm likelihood, it isproved that each A was converged correctly.# of  (.% y) occurs in sam, pieP(ylx) - n, um, ber o f  t imes  o f  a: (7)hi feature selection step, we chose useflll fea-tures with the gain threshold of 0.008.
Figure4 shows some feaures with a large gain.
Anion\then1, tag sequences mapping including 'RB'  are er-roneous.
It means that position of adverb in Ko-rean is very compl icated  to handle.
Also, propernoun in English aligned coInmon nouns in Korean443EnglishBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJBEP+JJFeature(x,y)KoreanVBMA+ENCO3+AX+ENTEVBMAAJ MAAJMA+ENTEVBMA+ENTENNIN2+CONNIN2+CO+VBMANNIN2+PPCA1 +VBMA+ENTENNIN2+CO+ENTENNIN2+PPCA2+AX+ENTENNIN2+PPCA1 +VBMA110.13698.85208.67878.26287.23797.13726.99098.84026.83086.42566.4250Figure 3: A\[PRP\] you ~/'\[PN\] cJ~ (dangsin)"-\[PPAU\] ~-(eun)\[RB\] usually \[ADCO\] EttX41_~(dachero)HVP\] have, /\[NNIN2\] ~uf~(ilbanseok)TO\] to /~ \[PPAD\] 0tl(e)\[VBP\] take ; / \ [VBMA\ ]  N(anj),\[ENCO3\] O~OIE~(ayaman)\[J J\] regular .
.
.
.
.
.
':-:- \[AX\] "$F(ha)\[NN\] seating/' \[ENTE\] L E}(nda)Figure 5: Best Lexical alignmentbecause of tagging errors.
Note that in the case of'PN+PPCA2+PPAD+VBMA' ,  it is not an adjacentstring but an interrupted string.
It means ttlat averb in English generally map to a verb taking asargument the accusative and adverbial postpositionin Korean.One way of testing usefulness of our method isto construct structured aligned bil ingual sentences.Table 3 shows lexical al ignments using tag sequenceal ignments drawn from our algorithm for a givensentence, 'you usually have to take regular seating- dangsineun dachcw ilbanscokc anjayaman handa'and Figure 5 shows the best lexical alignment of thesentence.We conducted the exi)eriment on 100 sentencescomposed of words in length 14 or less and siln-lilY chose the most likely paths.
As tim result, theaccuray was about 71.1~.
It shows that we canpartly use the tag sequence alignments for lexicalalignments.
We will extend tlle structural mappingmodel with consideration to the lexical information.The parmneters, the conditional probabilities aboutstuctural mappings will be embedded in a statisti-cal model.
Table 4 shows conditional probabilitiesof seine features according to 'DT+NN'.
In general,determiner is translated into NULL or adnominalword in Korean.7 Conclus ionWhen aligning Englist>Korean sentences, the differ-ences of word order and word unit require structuralinformation.
For tiffs reason, we tried structural tagc(x,y)162.0C45.0039.0025.009.008.007.006.006.004.004.00P(Ylx)0.42470.11800.09960.06550.02360.02100.01830.01570.01570.01050.0105ExampleEnglishare+preparedare+carefulam+healthyis+newam+suream+richis+selfishis+patrioticis+reasonableis+reprehensibleis+helpfulKorean~kllKl+0~+?~+~ LI El-0-94 8i~ J  8t+ ~ LI C\[~X\[+010191~+01+~LIEt011~;Xt+9\[+~+EF'NPJ ~ +01 +g~.~N+01 +.Elof active features in At~I)T-t-NNI)Tq-NNDT-FNNDT+NNDT-t-NNDT-FNNI)T-FNNetct~NNIN2ANI)E+NNIN2ANNUWNNDE2NNIN2+PPCA1NNIN2+NNIN2NNIN2-FPPAUADCOeI;cp(t~l*~)0.5241310.151610.0910360.0635150.0583220.057680.049622Table 4: Conditional Probabilitystring mapping using maximum entropy modelingand feature selection concept.
We devised a nlodelthat generates a English tag string given a Koreantag string.
From initial active structural features,useful features are extended by feature selection.Tile retrieved features and parameters can be em-bedded in statistical maclfine translation and reducethe complexity of searching.
We showed that theycan helpful to construct structured aligned bilingualsentences.ReferencesAdam L. Berger, Peter F. Brown, Stephen A.Della Pietra, Vincent J. Della Pietra, John R.Gillett, John D. Lafferty, Robert L. Mercer, HarryPrintz, and Lubos Ures.
1994.
The Calldie sys-tem for machine translation, hi Proceedings of theARPA Conference on Human Language Technol-ogy, Plainsborough, New Jersey.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproacll to natural anguage processing.
Compu-tational Linguistics, 22(1):39-73.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Fredrick Jelinek,John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to nlachinetranslation.
Computational Linguistics, 16(2):79-85Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Robert L. Mercer.
1993.
The math-enlatics of statistical machine translation: pa-444Feature(x,y)XVBP+PRP+TOBEP+RBR+INDT4-CDJJ+INVBG+TOBEPBEPNNPNNPTO+PRPTO+PRPMD+FIBMD+RBMD+BEPNNP+NNSVBP+TO+VBPBED+VSN+INBED+VSN+INYPNYPPCA2+PPAD+VBMAPPAD+AJMANU+NNDE2PPAD+AJMA+ENTR1PPAD+PPCA2+VBMAPPCA1 +AJMAPPCA1 +AJMA+ENTENNIN2+PPAUNNIN2+NNIN2PN+PPADPN+PPCA1ENTRI+NNDEI+COENTRI+NNDEI+CO+ENTECO+ENCO2+VBMANNIN2+SFPPCA2+VBMA+ENCO2+VBM~PPAD+VBMA+PE+ENTEPPAD+VBMA+PEr,~; P(ylx) /~ L(Afi)9.8687 0.1722 0.01949.6780 0.3265 0.01929.2799 0.2449 0.01909.6343 0.2450 0.01909.9542 0,3269 0.01899.6720 0.2941 0.01889.2724 0.1961 0.01888.7481 0.1225 0.01829.1397 0.1337 0.01809.5634 0.2307 0.01809.2604 0.1730 0.01809.2445 0.1548 0.01779.2564 0.1548 0.01778.5435 0.0934 0.01779.1597 0.1470 0,01768.9928 0.1278 0.01749.1511 0.1704 0.01739.1636 0.1706 0,0173Englishsend+Nm+to - -is+more+thanthe+twosmarter+thanserving+toisareIBMHarvardto+himto+her?
?should+beEnglish+booksrequest+to+sendwas+thrown+towere+sent+toExampleKorean21+ N+-0tF+-~ L\]I-h20+~?+N~H+~PA+N+L~OII3tI+-~+U~-0I+OA-01+~+HIBM+~-3+OIl)II~LI +)F?
?Ol+OiOI+N-N+~Ll l+Kf~+5/N bF~011?t1+c3 ~1XI+~+FAFigure d: Some fbatm'es with a large gainTag a l ignment  (km( l i t io lml  Lex ica l  aliglulw, ntl ) l{P : PN+I )PAU 0.150109 you : dangs in+cunlt\]3 : A I )CO 0.142193 usmt l ly  : dacheroII, B : NNIN2+PI 'A I )  0.038105 usua l ly  : i lbanseol?-l-eI IVP+TO : I~N( JO3-bAX+I"NTE 0,982839 have+to  : ayaman+handaVBP  : P1)A I )+VBMA 0.05022,l take  : e+an jVBP  : VBMA+F,  NCO3+AX+I , ;NTE  0.011110 take  : an jay+aman-Fha+ndaV I3P  : P1)A I )+VI~MA+ENCO3-}-AX- I -ENT\ ] ,~  0,001851 take  : e -Fan jaya imm+handaV I3P -F J J  : NNIN2+PI )A I ) - \ ] -VBMA 0.057657 take- t - regu lar  : i l bansenk+e+,a l l j. I .
J+NN : NNIN2 0.581791 regu lar+seat ing  : i l l )anseokan(, 3: l,exi(:al aligmnents using tag alignmentsrameter estimation.
Computational Linguistics,19(2):263-311.Stanley F. Chert.
1993.
Aligning sentences in bilin-gual corpora using lexical information.
In l'rocccd-ings of ACL ,71, 9-16.A.
P. Dempster, N. M. Laird and 1).
13. l{ubin.1976.
Maximum likelihood fi'om incomplet,e datavia the EM algorithm.
The Royal ,S'tatistics Soci-ety, 39(B) 205-237.Williain A. Gale, Kenneth W. Church.
1993.
A pro-gram fbr aligning senten(:es in bilingual (-orl)ora.Coml)utational Linguistics, \]9:75-102.Frederick Jelinek.
\]997.
Statistical Methods forSpeech Recognition MIT Press.Marin Kay, Martin Roscheisen.
1993.
Text-translation alignment.
Computational Linguis-tics, 19:121-142.Julian Kupiec.
1993.
An algorithm tbr finding nounphrase corresl)ondenccs in bilingual corl)ola.
InProceedings of ACL 31, 17-22.Yuji Matsmno~o, Hiroyuki Ishimoto, Takehito Ut-sure.
1993.
Structural inatching of para.llel texts.In Proceedings of ACL 3I, 23-30.I.
Dan Melame(l. 1997.
A word-to-word model oftranslation equivalence.
In PTvcccdings of ACL35/EACL 8, 1.6-23.Frmlz Josef Och mid Ilans Wel)cr.
1.998. hnt)rov-ing Statistical Natural Language Translation withCategories and Rules.
In Procccdings of ACL36/COLING, 985-989.Stephen A. Della Pietra, Vincent J. Della Pietra,John D. La.tl'erty.
1997. llnducing features of ran-dora fields.
IEEE ~IYansactions on Pattern Anal-ysis and Machine Intelligence, 19(4):380-393.Frank Smadja, Kathleen R. McKeown, and VasileiosHatziw~ssiloglou.
1996.
Translating collocationsfi)r bilingual lexicons: A statistical approa(:h.Computational Linguistics, 22 (1) :1-38.Kengo Sate 1998.
Maximum Entrol)y Model Learn-ing of the Translation Rules.
In Procccdinfls ofACL 35/COLING, 1171-1175.Jung H. Shin, Y(mng S. Han, and Key-SunChoi.
1996.
Bilingual knowledge acquisition fromKorean-English paralM cort)us using aligmnentmethod.
In Proceedings of COLING 96.C.
Tilhnann, S. Vogel, H. Ney, and A. Zubiaga.1997.
A I)P t)ased sea.rch using monotone a.lign-ments in statistical translation.
In Procccdings ofACL 35/EACL 8, 289-296.Ye-Yi Wa.ng and Alex Waibel.
1997.
Decoding algo-rithm in statistical machine translation.
In Pro-cccdinfls of ACL 35/EACL 8, 366-372.Ye-Yi Wang and Alex Waibel.
1998.
Modeling withstructures in machine translation.
In Procccdingsof ACL 36/COLINGDekai Wu 1996.
A t)olynonlial-time algorithm forstatistical machine translation.
In Proceeding ofA CL 34.445
