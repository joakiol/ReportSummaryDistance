Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 67?72,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsTwo-Step Model for Sentiment Lexicon Extraction from Twitter StreamsIlia ChetviorkinLomonosov Moscow State UniversityMoscow, Leninskiye Gory 1ilia.chetviorkin@gmail.comNatalia LoukachevitchLomonosov Moscow State UniversityMoscow, Leninskiye Gory 1louk nat@mail.ruAbstractIn this study we explore a novel techniquefor creation of polarity lexicons from theTwitter streams in Russian and English.With this aim we make preliminary fil-tering of subjective tweets using generaldomain-independent lexicons in each lan-guage.
Then the subjective tweets areused for extraction of domain-specific sen-timent words.
Relying on co-occurrencestatistics of extracted words in a large un-labeled Twitter collections we utilize theMarkov random field framework for theword polarity classification.
To evaluatethe quality of the obtained sentiment lex-icons they are used for tweet sentimentclassification and outperformed previousresults.1 IntroductionWith growing popularity of microblogging ser-vices such as Twitter, the amount of subjective in-formation containing user opinions and sentimentsis increasing dramatically.
People tend to expresstheir opinions about events in the real life and suchopinions contain valuable information for marketresearch, brand monitoring and political polls.The task of automatic processing of such in-formal resources is challenging because peopleuse a lot of slang, vulgarity and out-of-vocabularywords to state their opinions about various ob-jects and situations.
In particular, it is difficultto achieve the high quality of sentiment analy-sis on such type of short informal texts as tweetsare.
Standard domain-independent lexicon-basedmethods suffer from low coverage, and for ma-chine learning methods it is difficult to prepare arepresentative collection of labeled data becausetopics of discussion are changing rapidly.Thus, special methods for processing social me-dia data streams should be developed.
We pro-posed and evaluated our approach for Russian lan-guage, where only a limited number of naturallanguage processing tools and resources are avail-able.
Then to demonstrate the robustness of themethod and to compare the results with the otherapproaches we used it for English.The current research can be separated into twosteps.
We start with a special supervised modelbased on statistical and linguistic features of sen-timent words, which is trained and evaluated inthe movie domain.
Then this model is utilizedfor extraction of sentiment words from unlabeledTwitter datasets, which are preliminary filtered us-ing the domain-independent lexicons: Product-SentiRus (Chetviorkin and Loukachevitch, 2012)for Russian and MPQA (Wilson et al., 2005) forEnglish.In the second step an algorithm for polarity clas-sification of extracted sentiment words is intro-duced.
It is built using the Markov random fieldframework and uses only information contained intext collections.To evaluate the quality of the created lexiconsextrinsically, we conduct the experiments on thetweet subjectivity and polarity classification tasksusing various lexicons.The key advantage of the proposed two-step al-gorithm is that once trained it can be utilized todifferent domains and languages with minor mod-ifications.
To demonstrate the ability of the pro-posed algorithm to extract sentiment words in var-ious domains we took significantly different col-lections for training and testing: movie review col-lection for training and large collections of tweetsfor testing.2 Related workThere are two major approaches for creationof a sentiment lexicon in a specific language:dictionary-based methods and corpus-based meth-ods.67Dictionary-based methods for various lan-guages have received a lot of attention in the lit-erature (P?erez-Rosas et al., 2012; Mohammad etal., 2009; Clematide and Klenner, 2010), but themain problem of such approaches is that it is diffi-cult to apply them to processing social media.
Thereason is that short informal texts contain a lot ofmisspellings and out-of-vocabulary words.Corpus-based methods are more suitable forprocessing social media data.
In such approachesvarious statistical and linguistic features are usedto discriminate opinion words from all otherwords (He et al., 2008; Jijkoun et al., 2010).Another important group of approaches, whichcan be both dictionary-based and corpus-based aregraph-based methods.
In (Velikovich et al., 2010)a new method for constructing a lexical networkwas proposed, which aggregates the huge amountof unlabeled data.
Then the graph propagation al-gorithm was used.
Several other researchers uti-lized the graph or label propagation techniquesfor solving the problem of opinion word extrac-tion (Rao and Ravichandran, 2009; Speriosu et al.,2011).In (Takamura et al., 2005) authors describe aprobabilistic model for assigning polarity to eachword in a collection.
This model is based onthe Ising spin model of magnetism and is builtupon Markov random field framework, using var-ious dictionary-based and linguistic features.
Inour research, unlike (Takamura et al., 2005) weuse only information contained in a text collectionwithout any external dictionary resources (due tothe lack of necessary resources for Russian).
Ouradvantage is that we use only potential domain-specific sentiment words during the constructionof the network.A large body of research has been focused onTwitter sentiment analysis during the previous sev-eral years (Barbosa and Feng, 2010; Berminghamand Smeaton, 2010; Bifet and Frank, 2010; Davi-dov et al., 2010; Kouloumpis et al., 2011; Jianget al., 2011; Agarwal et al., 2011; Wang et al.,2011).
In (Chen et al., 2012) authors propose anoptimization framework for extraction of opinionexpressions from tweets.
Using extracted lexiconsauthors were able to improve the tweet sentimentclassification quality.
Our approach is based onsimilar assumptions (like consistency relations),but we do not use any syntactic parsers and dic-tionary resources.
In (Volkova et al., 2013) a newmultilingual bootstrapping technique for buildingtweet sentiment lexicons was introduced.
Thismethod is used as a baseline in our work.3 DataFor the experiments in this paper we use severalcollections in two domains: movie review col-lection in Russian for training and fine-tuning ofthe proposed algorithms and Twitter collectionsfor evaluation and demonstration of robustness inRussian and English languages.Movie domain.
The movie review dataset col-lected from the online service imhonet.ru.
Thereare 28, 773 movie reviews of various genres withnumeric scores specified by their authors (DOM).Additionally, special collections with low con-centration of sentiment words are utilized: thecontrast collection consists of 17, 980 movie plots(DESC) and a collection of two million news doc-uments (NEWS).
Such collections are useful forfiltering out of domain-specific and general neu-tral words, which are very frequent in news andobject descriptions.Twitter collections.
We use three datasets foreach language: 1M+ of unlabeled tweets (UNL)for extraction of sentiment lexicons, 2K labeledtweets for development data (DEV), and 2K la-beled tweets for evaluation (TEST).
DEV datasetis used to find the best combination of various lex-icons for processing Twitter data and TEST forevaluating the quality of constructed lexicons.The UNL dataset in Russian was collected dur-ing one day using Twitter API.
These tweets con-tain various topics without any filtering.
Onlystrict duplicates and retweets were removed fromthe dataset.
The similar collection for English wasdownloaded using the links from (Volkova et al.,2013).All tweets in DEV and TEST collections aremanually labeled by subjectivity and polarity us-ing the Mechanical Turk with five workers (ma-jority voting).
This data was used for developmentand evaluation in (Volkova et al., 2013).4 Method for sentiment word extractionIn this section we introduce an algorithm forsentiment lexicon extraction, which is inspiredby the method described in (Chetviorkin andLoukachevitch, 2012), but have more robust fea-tures, which allow us to apply it to any unlabeledtext collection (e.g.
tweets collection).
The pro-68posed algorithm is applied to text collections inRussian and English and obtained results are eval-uated intrinsically for Russian and extrinsically forboth languages.4.1 An extraction modelOur algorithm is based on several text collec-tions: collection with the high concentration ofsentiment words (e.g.
DOM collection), con-trast domain-specific collection (e.g.
DESC col-lection), contrast domain-independent collection(e.g.
NEWS collection).
Thus, taking into ac-count statistical distributions of words in such col-lections we are able to distinguish domain-specificsentiment words.We experimented with various features to createthe robust cross-domain feature representation ofsentiment words.
As a result the eight most valu-able features were used in further experiments:Linguistic features.
Adjective binary indica-tor, noun binary indicator, feature reflecting part-of-speech ambiguity (for lemma), binary featureof predefined list of prefixes (e.g.
un, im);Statistical features.
Frequency of capitalizedwords, frequency of co-occurrence with polarityshifters (e.g.
no, very), TFIDF feature calculatedon the basis of various collection pairs, weirdnessfeature (the ratio of relative frequencies of certainlexical items in special and general collections)calculated using several pairs of collections.To train supervised machine learning algo-rithms all words with frequency greater than threein the Russian movie review collection (DOM)were labeled manually by two assessors.
If therewas a disagreement about the sentiment of a spe-cific word, the collective judgment after the dis-cussion was used as a final ground truth.
As a re-sult of the assessment procedure the list of 4079sentiment words was obtained.The best quality of classification using labeleddata was shown by the ensemble of three classi-fiers: Logistic Regression, LogitBoost and Ran-dom Forest.
The quality according to Precision@nmeasure can be found in Table 1.
This trainedmodel was used in further experiments for extrac-tion of sentiment words both in English and inRussian.4.2 Extraction of subjective words fromTwitter dataTo verify the robustness of the model on new un-labeled data it was utilized for sentiment word ex-Lexicon P@100 P@1000MovieLex 95.0% 78.3%TwitterLex 95.0% 79.9%Table 1: Quality of subjective word extraction inRussiantraction from multi-topic tweet collection UNL ineach language.
To apply this model we preparedthree collections: domain-specific with high con-centration of sentiment words, domain-specificwith low concentration of sentiment words andone general collection with low concentration ofsentiment words.
As the general collection wecould take the same NEWS collection (see Sec-tion 3) for Russian and British National Corpus1for English.To prepare domain-specific collections we clas-sified the UNL collections by subjectivity usinggeneral purpose sentiment lexicons ProductSen-tiRus and MPQA in accordance with the language.The subjectivity classifier predicted that a tweetwas subjective if it contained at least one subjec-tive term from this lexicon.
All subjective tweetsconstituted a collection with the high concentra-tion of sentiment words and all the other tweetsconstituted the contrast collection.Finally, using all specially prepared collectionsand the trained model (in the movie domain), newlexicons of twitter-specific sentiment words wereextracted.
The quality of extraction in Russian ac-cording to manual labeling of two assessors can befound in Table 1.
The resulting quality of extractedRussian lexicon is on the same level as in the ini-tial movie domain, what confirms the robustnessof the proposed model.We took 5000 of the most probable sentimentwords from each lexicon for further work.5 Polarity classification using MRFIn the second part of current research we describean algorithm for polarity classification of extractedsentiment words.
The proposed method relies onseveral assumptions:?
Each word has the prior sentiment score cal-culated using the review scores where it ap-pears (simple averaging);?
Words with similar polarity tend to co-occurclosely to each other;1http://www.natcorp.ox.ac.uk/69?
Negation between sentiment words leads tothe opposite polarity labels.5.1 Algorithm descriptionTo formalize all these assumptions we construct anundirected graphical model using extracted sen-timent word co-occurrence statistics.
Each ex-tracted word is represented by a vertex in a graphand an edge between two vertexes is established incase if they co-occur together more than once inthe collection.
We drop all the edges where aver-age distance between words is more than 8 words.Our model by construction is similar to ap-proach based on the Ising spin model describedin (Takamura et al., 2005).
Ising model is usedto describe ferromagnetism in statistical mechan-ics.
In general, the system is composed of N bi-nary variables (spins), where each variable xi?
{?1,+1}, i = 1, 2, ..., N .
The energy function ofthe system is the following:E(x) = ??ijsijxixj?
?ihixi(1)where sijrepresents the efficacy of interaction be-tween two spins and histands for external fieldadded to xi.
The probability of each system con-figuration is provided by Boltzmann distribution:P (X) =exp?
?E(X)Z(2)where Z is a normalizing factor and ?
= (T?1>0) is inverse temperature, which is parameter ofthe model.
We calculate values of P (X) with sev-eral different values of ?
and try to find the locallypolarized state of the network.To specify the initial polarity of each word, weassume that each text from the collection has itssentiment score.
This condition is not very strict,because there are a lot of internet review serviceswhere people assign numerical scores to their re-views.
Using such scores we can calculate the de-viation from the average score for each word in thecollection:h(i) = E(c|wi)?
E(c)where c is the review score random variable, E(c)is the expectation of the score in the collection andE(c|wi) is the expectation of the score for reviewscontaining word wi.
Thus we assign the initialweight of each vertex i in the MRF to be equalto h(i).To specify the weight of each edge in the net-work we made preliminary experiments to detectthe dependency between the probability of theword pair to have similar polarity and average dis-tance between them.
The result of such experi-ment for movie reviews can be found on Figure 1.One can see that if the distance between the wordsFigure 1: The dependency between the probabilityto have similar polarity and average distanceis above four, then the probability is remain on thesame level which is slightly biased to similar po-larity.
Relying on this insight and taking into ac-count the frequency of co-occurrence of the wordswe used the following edge weights:s(i, j) = f(wi, wj)max(0.5?d(wi, wj)d(wi, wj) + 4, 0)where f(wi, wj) is the co-occurrence frequencyin the collection and d(wi, wj) is the average dis-tance between words wiand wj.Finally, we revert the sign of this equation incase of more than half of co-occurrences containsnegation (no, not, but) between opinion words.In practice we can find approximate solution us-ing such algorithms as: Loopy Belief Propagation(BP), Mean Field (MF), Gibbs Sampling (Gibbs).The performance of the methods was evalu-ated for a lexical network constructed from thefirst 3000 of the most probable extracted sentimentwords in the movie review collection (DOM).
Wetook from them 822 interconnected words withstrict polarity labeled by two assessors as a goldstandard.
Testing was performed by varying ?from 0.1 to 1.0.
The primary measure in this ex-periment was accuracy.
The best results can befound in Table 2.The best performance was demonstrated by MFalgorithm and ?
= 0.4.
This algorithm and pa-rameter value were used in further experiments onunlabeled tweet collections.70?
BP MF Gibbs0.4 83.8 85.2 83.70.5 83.6 84.5 82.00.6 85.0 83.1 79.4Table 2: Dependence between the accuracy ofclassification and ?5.2 Polarity classification of subjective wordsfrom Twitter dataUsing the general polarity lexicons we classify allsubjective tweets in large UNL collections intopositive and negative categories.
For the polarityclassifier, we predict a tweet to be positive (nega-tive) if it contains at least one positive (negative)term from the lexicon taking into account nega-tion.
If a tweet contains both positive and nega-tive terms, we take the majority label.
In case if atweet does not contain any word from the lexiconwe predict it to be positive.These labels (+1 for positive and ?1 for nega-tive) can be used to compute initial polarity h(i)for all extracted sentiment words from the UNLcollections.
The weights of the links betweenwords s(i, j) can be also computed using full un-labeled collections.Thus, we can utilize the algorithm for polarityclassification of sentiment words extracted fromTwitter.
The resulting lexicon for Russian contains2772 words and 2786 words for English (we takeonly words that are connected in the network).
Toevaluate the quality of the obtained lexicons theRussian one was labeled by two assessors.
In re-sult of such markup 1734 words with strict posi-tive or negative polarity were taken.
The accuracyof the lexicon on the basis of the markup was equalto 72%, which is 1.5 % better than the simple av-erage score baseline.6 Lexicon EvaluationsTo evaluate all newly created lexicons they wereutilized in tweet polarity and subjectivity classifi-cation tasks using the TEST collections.
The re-sults of the classification for both languages canbe found in Table 3 and Table 4.As one can see, the newly created Twitter-specific sentiment lexicon results outperform theresult of (Volkova et al., 2013) in subjectivity clas-sification for Russian but slightly worse than theresult for English.
On the other hand the re-sults of polarity classification are on par or betterLexicon P R FsubjRussianVolkova, 2013 - - 61.0TwitterLex 60.2 79.3 68.5EnglishVolkova, 2013 - - 75.0TwitterLex 58.8 95.5 73.0Table 3: Quality of tweet subjectivity classifica-tionLexicon P R FpolRussianVolkova, 2013 - - 73.0TwitterLex 65.5 82.0 72.8Combined 65.8 85.5 74.3EnglishVolkova, 2013 - - 78.0TwitterLex 72.1 88.1 79.3Combined 73.2 89.3 80.4Table 4: Quality of tweet polarity classificationthan the results of (Volkova et al., 2013) lexiconsbootstrapped from domain-independent sentimentlexicons.
Thus, to push the quality of polarityclassification forward we combined the domain-independent lexicons and our Twitter-specific lex-icons.
We experimented with various word countsfrom general lexicons and found the optimal com-bination on the DEV collection: all words fromTwitterLex and 2000 the most strong sentimentwords from ProductSentiRus in Russian and allstrong sentiment words from MPQA in English.The lexicon combination outperforms all previousresults by F-measure leading to the conclusion thatproposed method can capture valuable domain-specific sentiment words.7 ConclusionIn this paper we proposed a new method for ex-traction of domain-specific sentiment lexicons andadopted the Ising model for polarity classifica-tion of extracted words.
This two-stage methodwas applied to a large unlabeled Twitter datasetand the extracted sentiment lexicons performed onthe high level in the tweet sentiment classificationtask.
Our method can be used in a streaming modefor augmentation of sentiment lexicons and sup-porting the high quality of multilingual sentimentclassification.71Acknowledgements This work is partially sup-ported by RFBR grant 14-07-00682.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of twitter data.
In Proceedings of the Work-shop on Languages in Social Media, pages 30?38.Association for Computational Linguistics.Luciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on twitter from biased and noisydata.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 36?44.
Association for Computational Lin-guistics.Adam Bermingham and Alan F Smeaton.
2010.
Clas-sifying sentiment in microblogs: is brevity an advan-tage?
In Proceedings of the 19th ACM internationalconference on Information and knowledge manage-ment, pages 1833?1836.
ACM.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In Discov-ery Science, pages 1?15.
Springer.Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shao-jun Wang, and Amit P Sheth.
2012.
Extractingdiverse sentiment expressions with target-dependentpolarity from twitter.
In ICWSM.Ilia Chetviorkin and Natalia V Loukachevitch.
2012.Extraction of russian sentiment lexicon for productmeta-domain.
In COLING, pages 593?610.Simon Clematide and Manfred Klenner.
2010.
Eval-uation and extension of a polarity lexicon for ger-man.
In Proceedings of the First Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis, pages 7?13.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, pages 241?249.
Association for Computa-tional Linguistics.Ben He, Craig Macdonald, Jiyin He, and Iadh Ounis.2008.
An effective statistical approach to blog postopinion retrieval.
In Proceedings of the 17th ACMconference on Information and knowledge manage-ment, pages 1063?1072.
ACM.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twitter sen-timent classification.
In ACL, pages 151?160.Valentin Jijkoun, Maarten de Rijke, and WouterWeerkamp.
2010.
Generating focused topic-specific sentiment lexicons.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 585?594.
Association forComputational Linguistics.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In ICWSM.Saif Mohammad, Cody Dunne, and Bonnie Dorr.2009.
Generating high-coverage semantic orien-tation lexicons from overtly marked words and athesaurus.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Pro-cessing: Volume 2, pages 599?608.
Association forComputational Linguistics.Ver?onica P?erez-Rosas, Carmen Banea, and Rada Mi-halcea.
2012.
Learning sentiment lexicons in span-ish.
In LREC, pages 3077?3081.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 675?682.
Association for Computational Lin-guistics.Michael Speriosu, Nikita Sudan, Sid Upadhyay, andJason Baldridge.
2011.
Twitter polarity classifica-tion with label propagation over lexical links and thefollower graph.
In Proceedings of the First work-shop on Unsupervised Learning in NLP, pages 53?63.
Association for Computational Linguistics.H.
Takamura, T. Inui, and M. Okumura.
2005.
Ex-tracting semantic orientations of words using spinmodel.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages133?140.Leonid Velikovich, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The viabilityof web-derived polarity lexicons.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 777?785.
As-sociation for Computational Linguistics.Svitlana Volkova, Theresa Wilson, and DavidYarowsky.
2013.
Exploring sentiment in socialmedia: Bootstrapping subjectivity clues frommultilingual twitter streams.
In Proceedingsof the 51st Annual Meeting of the Associationfor Computational Linguistics (ACL13), pages505?510.Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou,and Ming Zhang.
2011.
Topic sentiment analysisin twitter: a graph-based hashtag sentiment classifi-cation approach.
In Proceedings of the 20th ACMinternational conference on Information and knowl-edge management, pages 1031?1040.
ACM.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on human language technology and empiri-cal methods in natural language processing, pages347?354.
Association for Computational Linguis-tics.72
