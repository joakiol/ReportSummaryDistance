Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 729?739,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsThe Inside-Outside Recursive Neural Network model forDependency ParsingPhong Le and Willem ZuidemaInstitute for Logic, Language, and ComputationUniversity of Amsterdam, the Netherlands{p.le,zuidema}@uva.nlAbstractWe propose the first implementation ofan infinite-order generative dependencymodel.
The model is based on a newrecursive neural network architecture, theInside-Outside Recursive Neural Network.This architecture allows information toflow not only bottom-up, as in traditionalrecursive neural networks, but also top-down.
This is achieved by computingcontent as well as context representationsfor any constituent, and letting these rep-resentations interact.
Experimental re-sults on the English section of the Uni-versal Dependency Treebank show thatthe infinite-order model achieves a per-plexity seven times lower than the tradi-tional third-order model using counting,and tends to choose more accurate parsesin k-best lists.
In addition, reranking withthis model achieves state-of-the-art unla-belled attachment scores and unlabelledexact match scores.1 IntroductionEstimating probability distributions is the core is-sue in modern, data-driven natural language pro-cessing methods.
Because of the traditional defi-nition of discrete probabilityPr(A) ?the number of times A occursthe size of event spacecounting has become a standard method to tacklethe problem.
When data are sparse, smoothingtechniques are needed to adjust counts for non-observed or rare events.
However, successful useof those techniques has turned out to be an art.
Forinstance, much skill and expertise is required tocreate reasonable reduction lists for back-off, andto avoid impractically large count tables, whichstore events and their counts.An alternative to counting for estimating prob-ability distributions is to use neural networks.Thanks to recent advances in deep learning, thisapproach has recently started to look very promis-ing again, with state-of-the-art results in senti-ment analysis (Socher et al., 2013), language mod-elling (Mikolov et al., 2010), and other tasks.
TheMikolov et al.
(2010) work, in particular, demon-strates the advantage of neural-network-based ap-proaches over counting-based approaches in lan-guage modelling: it shows that recurrent neu-ral networks are capable of capturing long histo-ries efficiently and surpass standard n-gram tech-niques (e.g., Kneser-Ney smoothed 5-gram).In this paper, keeping in mind the success ofthese models, we compare the two approaches.Complementing recent work that focused on sucha comparison for the case of finding appropriateword vectors (Baroni et al., 2014), we focus hereon models that involve more complex, hierarchicalstructures.
Starting with existing generative mod-els that use counting to estimate probability distri-butions over constituency and dependency parses(e.g., Eisner (1996b), Collins (2003)), we developan alternative based on recursive neural networks.This is a non-trivial task because, to our knowl-edge, no existing neural network architecture canbe used in this way.
For instance, classic recur-rent neural networks (Elman, 1990) unfold to left-branching trees, and are not able to process ar-bitrarily shaped parse trees that the counting ap-proaches are applied to.
Recursive neural net-works (Socher et al., 2010) and extensions (Socheret al., 2012; Le et al., 2013), on the other hand,do work with trees of arbitrary shape, but pro-cess them in a bottom-up manner.
The probabil-ities we need to estimate are, in contrast, definedby top-down generative models, or by models thatrequire information flows in both directions (e.g.,the probability of generating a node depends onthe whole fragment rooted at its just-generated sis-729Figure 1: Inner (ip) and outer (op) representationsat the node that covers constituent p. They are vec-torial representations of p?s content and context,respectively.ter).To tackle this problem, we propose a new ar-chitecture: the Inside-Outside Recursive NeuralNetwork (IORNN) in which information can flownot only bottom-up but also top-down, inward andoutward.
The crucial innovation in our architec-ture is that every node in a hierarchical structureis associated with two vectors: one vector, the in-ner representation, representing the content underthat node, and another vector, the outer represen-tation, representing its context (see Figure 1).
In-ner representations can be computed bottom-up;outer representations, in turn, can be computedtop-down.
This allows information to flow inany direction, depending on the application, andmakes the IORNN a natural tool for estimatingprobabilities in tree-based generative models.We demonstrate the use of the IORNN by ap-plying it to an ?-order generative dependencymodel which is impractical for counting due tothe problem of data sparsity.
Counting, instead, isused to estimate a third-order generative model asin Sangati et al.
(2009) and Hayashi et al.
(2011).Our experimental results show that our new modelnot only achieves a seven times lower perplex-ity than the third-order model, but also tends tochoose more accurate candidates in k-best lists.
Inaddition, reranking with this model achieves state-of-the-art scores on the task of supervised depen-dency parsing.The outline of the paper is following.
Firstly, wegive an introduction to Eisner?s generative modelin Section 2.
Then, we present the third-ordermodel using counting in Section 3, and proposethe IORNN in Section 4.
Finally, in Section 5 weshow our experimental results.2 Eisner?s Generative ModelEisner (1996b) proposed a generative model fordependency parsing.
The generation process istop-down: starting at the ROOT, it generatesleft dependents and then right dependents for theROOT.
After that, it generates left dependents andright dependents for each of ROOT?s dependents.The process recursively continues until there is nofurther dependent to generate.
The whole processis captured in the following formulaP (T (H)) =L?l=1P(HLl|CHLl)P(T (HLl))?R?r=1P(HRr|CHRr)P(T (HRr))(1)whereH is the current head, T (N) is the fragmentof the dependency parse rooted in N , and CNisthe context in which N is generated.
HL, HRarerespectively H?s left dependents and right depen-dents, plus EOC (End-Of-Children), a special to-ken to indicate that there are no more dependentsto generate.
Thus, P (T (ROOT )) is the proba-bility of generating the entire dependency struc-ture T .
We refer to ?HLl, CHLl?, ?HRr, CHRr?
as?events?, and ?CHLl?, ?CHRr?
as ?conditioning con-texts?.In order to avoid the problem of data sparsity,the conditioning context in which a dependent Dis generated should capture only part of the frag-ment generated so far.
Based on the amount ofinformation that contexts hold, we can define theorder of a generative model (see Hayashi et al.
(2011, Table 3) for examples)?
first-order: C1Dcontains the head H ,?
second-order: C2Dcontains H and the just-generated sibling S,?
third-order: C3Dcontains H , S, the sibling S?before S (tri-sibling); or H , S and the grand-head G (the head of H) (grandsibling) (thefragment enclosed in the blue doted contourin Figure 2),?
?-order: C?Dcontains all of D?s ancestors,theirs siblings, and its generated siblings (thefragment enclosed in the red dashed contourin Figure 2).In the original models (Eisner, 1996a), each de-pendent D is a 4-tuple ?dist, w, c, t??
dist(H,D) the distance between D and itsheadH , represented as one of the four ranges1, 2, 3-6, 7-?.730Figure 2: Example of different orders of context of ?diversified?.
The blue dotted shape correspondsto the third-order outward context, while the red dashed shape corresponds to the?-order left-to-rightcontext.
The green dot-dashed shape corresponds to the context to compute the outer representation.?
word(D) the lowercase version of the wordof D,?
cap(D) the capitalisation feature of the wordof D (all letters are lowercase, all letters areuppercase, the first letter is uppercase, thefirst letter is lowercase),?
tag(D) the POS-tag of D,Here, to make the dependency complete,deprel(D), the dependency relation of D (e.g.,SBJ, DEP), is also taken into account.3 Third-order Model with CountingThe third-order model we suggest is similar tothe grandsibling model proposed by Sangati etal.
(2009) and Hayashi et al.
(2011).
It definesthe probability of generating a dependent D =?dist, d, w, c, t?
as the product of the distance-based probability and the probabilities of gener-ating each of its components (d, t, w, c, denotingdependency relation, POS-tag, word and capitali-sation feature, respectively).
Each of these prob-abilities is smoothed using back-off according tothe given reduction lists (as explained below).P (D|CD)= P (dist(H,D), dwct(D)|H,S,G, dir)= P (d(D)|H,S,G, dir)reduction list:tw(H), tw(S), tw(G), dirtw(H), tw(S), t(G), dir{tw(H), t(S), t(G), dirt(H), tw(S), t(G), dirt(H), t(S), t(G), dir?
P (t(D)|d(D), H, S,G, dir)reduction list:d(D), dtw(H), t(S), dird(D), d(H), t(S), dird(D), d(D), dir?
P (w(D)|dt(D), H, S,G, dir)reduction list:dtw(H), t(S), dirdt(H), t(S), dir?
P (c(D)|dtw(D), H, S,G, dir)reduction list:tw(D), d(H), dirtw(D), dir?
P (dist(H,D)|dtwc(D), H, S,G, dir) (2)reduction list:dtw(D), dt(H), t(S), dirdt(D), dt(H), t(S), dirThe reason for generating the dependency rela-tion first is based on the similarity between rela-tion/dependent and role/filler: we generate a roleand then choose a filler for that role.Back-off The back-off parameters are identi-cal to Eisner (1996b).
To estimate the proba-bility P (A|context) given a reduction list L =(l1, l2, ..., ln) of context, letpi={count(A,li)+0.005count(li)+0.5if i = ncount(A,li)+3pi+1count(li)+3otherwisethen P (A|context) = p1.4 The Inside-Outside Recursive NeuralNetworkIn this section, we first describe the Recur-sive Neural Network architecture of Socher etal.
(2010) and then propose an extension wecall the Inside-Outside Recursive Neural Network(IORNN).
The IORNN is a general architecturefor trees, which works with tree-based genera-tive models including those employed by Eisner(1996b) and Collins (2003).
We then explain howto apply the IORNN to the?-order model.
Notethat for the present paper we are only concernedwith the problem of computing the probability of731Figure 3: Recursive Neural Network (RNN).a tree; we assume an independently given parser isavailable to assign a syntactic structure, or multi-ple candidate structures, to an input string.4.1 Recursive Neural NetworkThe architecture we propose can best be under-stood as an extension of the Recursive Neural Net-works (RNNs) proposed by Socher et al.
(2010),that we mentioned above.
In order to see howan RNN works, consider the following example.Assume that there is a constituent with parse tree(p2(p1x y) z) (Figure 3), and that x,y, z ?
Rnare the (inner) representations of the three wordsx, y and z, respectively.
We use a neural networkwhich consists of a weight matrix W1?
Rn?nforleft children and a weight matrix W2?
Rn?nforright children to compute the vector for a parentnode in a bottom up manner.
Thus, we computep1as followsp1= f(W1x + W2y + b)where b is a bias vector and f is an activationfunction (e.g., tanh or logistic).
Having computedp1, we can then move one level up in the hierarchyand compute p2:p2= f(W1p1+ W2z + b)This process is continued until we reach the rootnode.
The RNN thus computes a single vectorfor each node p in the tree, representing the con-tent under that node.
It has in common with log-ical semantics that representations for compounds(here xyz) are computed by recursively applying acomposition function to meaning representationsof the parts.
It is difficult to characterise the ex-pressivity of the resulting system in logical terms,but recent work suggests it is surprisingly power-ful (e.g., Kanerva (2009)).Figure 4: Inside-Outside Recursive Neural Net-work (IORNN).
Black rectangles correspond to in-ner representations, white rectangles correspondto outer representations.4.2 IORNNWe extend the RNN-architecture by adding a sec-ond vector to each node, representing the contextof the node, shown as white rectangles in figure 4.The job of this second vector, the outer represen-tation, is to summarize all information about thecontext of node p so that we can either predict itscontent (i.e., predict an inner representation), orpass on this information to the daughters of p (i.e.,compute outer representations of these daughters).Outer representations thus allow information toflow top-down.We explain the operation of the resulting Inside-Outside Recursive Neural Network in terms of thesame example parse tree (p2(p1x y) z) (see Fig-ure 4).
Each node u in the syntactic tree carriestwo vectors ouand iu, the outer representation andinner representation of the constituent that is cov-ered by the node.Computing inner representations Inner repre-sentations are computed from the bottom up.
Weassume for every word w an inner representationiw?
Rn.
The inner representation of a non-terminal node, say p1, is given byip1= f(Wi1ix+ Wi2iy+ bi)where Wi1,Wi2are n ?
n real matrices, biis abias vector, and f is an activation function, e.g.tanh.
(This is the same as the computation ofnon-terminal vectors in the RNNs.)
The inner rep-resentation of a parent node is thus a function ofthe inner representations of its children.Computing outer representations Outer repre-sentations are computed from the top down.
For anode which is not the root, say p1, the outer repre-732sentation is given byop1= g(Wo1op2+ Wo2iz+ bo)where Wo1,Wo2are n ?
n real matrices, bois abias vector, and g is an activation function.
Theouter representation of a node is thus a function ofthe outer representation of its parent and the innerrepresentation of its sisters.If there is information about the external contextof the utterance that is being processed, this infor-mation determines the outer representation of theroot node oroot.
In our first experiments reportedhere, no such information was assumed to be avail-able.
In this case, a random value o?is chosen atinitialisation and assigned to the root nodes of allutterances; this value is then adjusted by the learn-ing process discussed below.Training Training the IORNN is to minimise anobjective function J(?)
which depends on the pur-pose of usage where ?
is the set of parameters.
Todo so, we compute the gradient ?J/??
and ap-ply the gradient descent method.
The gradient iseffectively computed thanks to back-propagationthrough structure (Goller and K?uchler, 1996).
Fol-lowing Socher et al.
(2013), we use AdaGrad(Duchi et al., 2011) to update the parameters.4.3 The?-order Model with IORNNThe RNN and IORNN are defined for context-free trees.
To apply the IORNN architecture todependency parses we need to adapt the defini-tions somewhat.
In particular, in the generativedependency model, every step in the generativestory involves the decision to generate a specificword while the span of the subtree that this wordwill dominate only becomes clear when all depen-dents are generated.
We therefore introduce par-tial outer representation as a representation of thecurrent context of a word in the generative pro-cess, and compute the final outer representationonly when all its siblings have been generated.Consider an example of head h and its depen-dents x, y (we ignore directions for simplicity) inFigure 5.
Assume that we are in the state in thegenerative process where the generation of h iscomplete, i.e.
we know its inner and outer rep-resentations ihand oh.
Now, when generating h?sfirst dependent x (see Figure 5-a), we first com-pute x?s partial outer representation (representingits context at this stage in the process), which isa function of the outer representation of the head(representing the head?s context) and the inner rep-resentation of the head (representing the content ofthe head word):?o1= f(Whiih+ Whooh+ bo)where Whi,Whoare n ?
n real matrices, bois abias vector, f is an activation function.With the context of the first dependent deter-mined, we can proceed and generate its content.For this purpose, we assume a separate weight ma-trix W, trained (as explained below) to predict aspecific word given a (partial) outer representa-tion.
To compute a proper probability for wordx, we use the softmax function:softmax(x,?o1) =eu(x,?o1)?w?Veu(w,?o1)where[u(w1,?o1), ..., u(w|V |,?o1)]T= W?o1+ band V is the set of all possible dependents.Note that since oh, the outer representation ofh, represents the entire dependency structure gen-erated up to that point,?o1is a vectorial represen-tation of the ?-order context generating the firstdependent (like the fragment enclosed in the reddashed contour in Figure 2).
The softmax func-tion thus estimates the probability P (D = x|C?D).The next step, now that x is generated, is tocompute the partial outer representation for thesecond dependent (see Figure 5-b)?o2= f(Whiih+ Whooh+ Wdr(x)ix+ bo)where Wdr(x)is a n ?
n real matrix specific forthe dependency relation of x with h.Next y is generated (using the softmax functionabove), and the partial outer representation for thethird dependent (see Figure 5-c) is computed:?o3= f(Whiih+ Whooh+12(Wdr(x)ix+ Wdr(y)iy)+ bo)Since the third dependent is the End-of-Children symbol (EOC), the process of generat-ing dependents for h stops.
We can then returnto x and y to replace the partial outer represen-tations with complete outer representations1(see1According to the IORNN architecture, to compute theouter representation of a node, the inner representations ofthe whole fragments rooting at its sisters must be taken intoaccount.
Here, we replace the inner representation of a frag-ment by the inner representation of its root since the meaningof a phrase is often dominated by the meaning of its head.733Figure 5: Example of applying IORNN to dependency parsing.
Black, grey, white boxes are respectivelyinner, partial outer, and outer representations.
For simplicity, only links related to the current computationare drawn (see text).Figure 5-d,e):ox= f(Whiih+ Whooh+ Wdr(y)iy+ bo)oy= f(Whiih+ Whooh+ Wdr(x)ix+ bo)In general, if u is the first dependent of h then?ou= f(Whiih+ Whooh+ bo)otherwise?ou= f(Whiih+ Whooh+ bo+1|?S(u)|?v?
?S(u)Wdr(v)iv)where?S(u) is the set of u?s sisters generated be-fore it.
And, if u is the only dependent of h (ig-noring EOC) thenou= f(Whiih+ Whooh+ bo)otherwiseou= f(Whiih+ Whooh+ bo+1|S(u)|?v?S(u)Wdr(v)iv)where S(u) is the set of u?s sisters.We then continue this process to generate de-pendents for x and y until the process stops.Inner Representations In the calculation of theprobability of generating a word, described above,we assumed inner representations of all possiblewords to be given.
These are, in fact, themselves afunction of vector representations for the words (inour case, the word vectors are initially borrowedfrom Collobert et al.
(2011)), the POS-tags andcapitalisation features.
That is, the inner represen-tation at a node h is given by:ih= f (Wwwh+ Wpph+ Wcch)where Ww?
Rn?dw, Wp?
Rn?dp, Wc?Rn?dc, whis the word vector of h, and ph, charerespectively binary vectors representing the POS-tag and capitalisation feature of h.Training Training this IORNN is to minimisethe following objective function which is the reg-ularised cross-entropyJ(?)
=?1m?T?D?w?Tlog(P (w|?ow))+12(?W?
?W?2+ ?L?
?L?2)where D is the set of training dependency parses,m is the number of dependents; ?W, ?Larethe weight matrix set and the word embeddings(?
= (?W, ?L)); ?W, ?Lare regularisation hyper-parameters.Implementation We decompose a dependent Dinto four features: dependency relation, POS-tag,lowercase version of word, capitalisation featureof word.
We then factorise P (D|C?D) similarly toSection 3, where each component is estimated bya softmax function.5 ExperimentsIn our experiments, we convert the Penn Treebankto dependencies using the Universal dependencyannotation (McDonald et al., 2013)2; this yieldsa dependency tree corpus we label PTB-U.
In or-der to compare with other systems, we also ex-periment with an alternative conversion using thehead rules of Yamada and Matsumoto (2003)3;this yields a dependency tree corpus we label PTB-YM.
Sections 2-21 are used for training, section22 for development, and section 23 for testing.
Forthe PTB-U, the gold POS-tags are used.
For thePTB-YM, the development and test sets are taggedby the Stanford POS-tagger4trained on the whole2https://code.google.com/p/uni-dep-tb/3http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html4http://nlp.stanford.edu/software/tagger.shtml734Perplexity3rd-order model 1736.73?-order model 236.58Table 1: Perplexities of the two models on PTB-U-22.training data, whereas 10-way jackknifing is usedto generate tags for the training set.The vocabulary for both models, the third-ordermodel and the ?-order model, is taken as a listof words occurring more than two times in thetraining data.
All other words are labelled ?UN-KNOWN?
and every digit is replaced by ?0?.
Forthe IORNN used by the ?-order model, we setn = 200, and define f as the tanh activation func-tion.
We initialise it with the 50-dim word embed-dings from Collobert et al.
(2011) and train it withthe learning rate 0.1, ?W= 10?4, ?L= 10?10.5.1 PerplexityWe firstly evaluate the two models on PTB-U-22using the perplexity-per-word metricppl(P ) = 2?1N?T?Dlog2P (T )where D is a set of dependency parses, N is thetotal number of words.
It is worth noting that,the better P estimates the true distribution P?ofD, the lower its perplexity is.
Because Eisner?smodel with the dist(H,D) feature (Equation 2)is leaky (the model allocates some probability toevents that can never legally arise), this feature isdiscarded (only in this experiment).Table 1 shows results.
The perplexity of thethird-order model is more than seven times higherthan the?-order model.
This reflects the fact thatdata sparsity is more problematic for counting thanfor the IORNN.To investigate why the perplexity of the third-order model is so high, we compute the percent-ages of events extracted from the developmentset appearing more than twice in the training set.Events are grouped according to the reduction listsin Equation 2 (see Table 2).
We can see that re-ductions at level 0 (the finest) for dependency re-lations and words seriously suffer from data spar-sity: more than half of the events occur less thanthree times, or not at all, in the training data.
Wethus conclude that counting-based models heavilyrely on carefully designed reduction lists for back-off.back-off level d t w c0 47.4 61.6 43.7 87.71 69.8 98.4 77.8 97.32 76.0, 89.5 99.73 97.9total 76.1 86.6 60.7 92.5Table 2: Percentages of events extracted fromPTB-U-22 appearing more than twice in the train-ing set.
Events are grouped according to the reduc-tion lists in Equation 2. d, t, w, c stand for depen-dency relation, POS-tag, word, and capitalisationfeature.5.2 RerankingIn the second experiment, we evaluate the twomodels in the reranking framework proposed bySangati et al.
(2009) on PTB-U.
We used the MST-Parser (with the 2nd-order feature mode) (McDon-ald et al., 2005) to generate k-best lists.
Twoevaluation metrics are labelled attachment score(LAS) and unlabelled attachment score (UAS), in-cluding punctuation.Rerankers Given D(S), a k-best list of parsesof a sentence S, we define the generative rerankerT?= arg maxT?D(S)P (T (ROOT ))which is identical to Sangati et al.
(2009).Moreover, as in many mixture-model-based ap-proaches, we define the mixture reranker as a com-bination of the generative model and the MST dis-criminative model (Hayashi et al., 2011)T?= arg maxT?D(S)?
logP (T (ROOT ))+(1??
)s(S, T )where s(S, T ) is the score given by the MST-Parser, and ?
?
[0, 1].Results Figure 6 shows UASs of the generativereranker on the development set.
The MSTParserachieves 92.32% and the Oracle achieve 96.23%when k = 10.
With the third-order model, thegenerative reranker performs better than the MST-Parser when k < 6 and the maximum improve-ment is 0.17%.
Meanwhile, with the ?-ordermodel, the generative reranker always gains higherUASs than the MSTParser, and with k = 6, thedifference reaches 0.7%.
Figure 7 shows UASs ofthe mixture reranker on the same set.
?
is opti-mised by searching with the step-size 0.005.
Un-surprisingly, we observe improvements over the735Figure 6: Performance of the generative rerankeron PTB-U-22.Figure 7: Performance of the mixture reranker onPTB-U-22.
For each k, ?
was optimized with thestep-size 0.005.LAS UASMSTParser 89.97 91.99Oracle (k = 10) 93.73 96.24Generative reranker with3rd-order (k = 3) 90.27 (+0.30) 92.27 (+0.28)?-order (k = 6) 90.76 (+0.79) 92.83 (+0.84)Mixture reranker with3rd-order (k = 6) 90.62 (+0.65) 92.62 (+0.63)?-order (k = 9) 91.02 (+1.05) 93.08 (+1.09)Table 3: Comparison based on reranking on PTB-U-23.
The numbers in the brackets are improve-ments over the MSTParser.generative reranker as the mixture reranker cancombine the advantages of the two models.Table 3 shows scores of the two rerankers on thetest set with the parameters tuned on the develop-ment set.
Both the rerankers, either using third-order or ?-order models, outperform the MST-Parser.
The fact that both gain higher improve-ments with the ?-order model suggests that theIORNN surpasses counting.Figure 9: F1-scores of binned HEAD distance(PTB-U-23).5.3 Comparison with other systemsWe first compare the mixture reranker using the?-order model against the state-of-the-art depen-dency parser TurboParser (with the full mode)(Martins et al., 2013) on PTB-U-23.
Table 4 showsLASs and UASs.
When taking labels into account,the TurboParser outperforms the reranker.
Butwithout counting labels, the two systems performcomparably, and when ignoring punctuation thereranker even outperforms the TurboParser.
Thispattern is also observed when the exact match met-rics are used (see Table 4).
This is due to the factthat the TurboParser performs significantly betterthan the MSTParser, which generates k-best listsfor the reranker, in labelling: the former achieves96.03% label accuracy score whereas the latterachieves 94.92%.One remarkable point is that reranking withthe ?-order model helps to improve the exactmatch scores 4% - 6.4% (see Table 4).
Becausethe exact match scores correlate with the abilityto handle global structures, we conclude that theIORNN is able to capture?-order contexts.
Fig-ure 8 shows distributions of correct-head accuracyover CPOS-tags and Figure 9 shows F1-scores ofbinned HEAD distance.
Reranking with the ?-order model is clearly helpful for all CPOS-tagsand dependent-to-head distances, except a minordecrease on PRT.We compare the reranker against other systemson PTB-YM-23 using the UAS metric ignoringpunctuation (as the standard evaluation for En-glish) (see Table 5).
Our system performs slightlybetter than many state-of-the-art systems such asMartins et al.
(2013) (a.k.a.
TurboParser), Zhangand McDonald (2012), Koo and Collins (2010).It outperforms Hayashi et al.
(2011) which is areranker using a combination of third-order gen-erative models with a variational model learnt736LAS (w/o punc) UAS (w/o punc) LEM (w/o punc) UEM (w/o punc)MSTParser 89.97 (90.54) 91.99 (92.82) 32.37 (34.19) 42.80 (45.24)w. ?-order (k = 9) 91.02 (91.51) 93.08 (93.84) 37.58 (39.16) 49.17 (51.53)TurboParser 91.56 (92.02) 93.05 (93.70) 40.65 (41.72) 48.05 (49.83)Table 4: Comparison with the TurboParser on PTB-U-23.
LEM and UEM are respectively the labelledexact match score and unlabelled exact match score metrics.
The numbers in brackets are scores com-puted excluding punctuation.Figure 8: Distributions of correct-head accuracy over CPOS-tags (PTB-U-23).System UASHuang and Sagae (2010) 92.1Koo and Collins (2010) 93.04Zhang and McDonald (2012) 93.06Martins et al.
(2013) 93.07Bohnet and Kuhn (2012) 93.39RerankingHayashi et al.
(2011) 92.89Hayashi et al.
(2013) 93.12MST+?-order (k = 12) 93.12Table 5: Comparison with other systems on PTB-YM-23 (excluding punctuation).on the fly; performs equally with Hayashi et al.
(2013) which is a discriminative reranker using thestacked technique; and slightly worse than Bohnetand Kuhn (2012), who develop a hybrid transition-based and graphical-based approach.6 Related WorkUsing neural networks to process trees was firstproposed by Pollack (1990) in the Recursive Au-toassociative Memory model which was used forunsupervised learning.
Socher et al.
(2010) laterintroduced the Recursive Neural Network archi-tecture for supervised learning tasks such as syn-tactic parsing and sentiment analysis (Socher etal., 2013).
Our IORNN is an extension ofthe RNN: the former can process trees not onlybottom-up like the latter but also top-down.Elman (1990) invented the simple recurrentneural network (SRNN) architecture which is ca-pable of capturing very long histories.
Mikolovet al.
(2010) then applied it to language mod-elling and gained state-of-the-art results, outper-forming the the standard n-gram techniques suchas Kneser-Ney smoothed 5-gram.
Our IORNNarchitecture for dependency parsing bears a re-semblance to the SRNN in the sense that it canalso capture long ?histories?
in context represen-tations (i.e., outer representations in our terminol-ogy).
Moreover, the IORNN can be seen as a gen-eralization of the SRNN since a left-branching treeis equivalent to a chain and vice versa.The idea of letting parsing decisions dependon arbitrarily long derivation histories is also ex-plored in Borensztajn and Zuidema (2011) andis related to parsing frameworks that allow arbi-trarily large elementary trees (e.g., Scha (1990),O?Donnell et al.
(2009), Sangati and Zuidema(2011), and van Cranenburgh and Bod (2013)).Titov and Henderson (2007) were the firstproposing to use deep networks for dependencyparsing.
They introduced a transition-based gen-erative dependency model using incremental sig-moid belief networks and applied beam pruningfor searching best trees.
Differing from them,our work uses the IORNN architecture to rescorek-best candidates generated by an independent737graph-based parser, namely the MSTParser.Reranking k-best lists was introduced byCollins and Koo (2005) and Charniak and Johnson(2005).
Their rerankers are discriminative and forconstituent parsing.
Sangati et al.
(2009) proposedto use a third-order generative model for rerankingk-best lists of dependency parses.
Hayashi et al.
(2011) then followed this idea but combined gen-erative models with a variational model learnt onthe fly to rerank forests.
In this paper, we alsofollowed Sangati et al.
(2009)?s idea but used an?-order generative model, which has never beenused before.7 ConclusionIn this paper, we proposed a new neural networkarchitecture, the Inside-Outside Recursive NeuralNetwork, that can process trees both bottom-upand top-down.
The key idea is to extend the RNNsuch that every node in the tree has two vectorsassociated with it: an inner representation for itscontent, and an outer representation for its context.Inner and outer representations of any constituentcan be computed simultaneously and interact witheach other.
This way, information can flow top-down, bottom-up, inward and outward.
Thanks tothis property, by applying the IORNN to depen-dency parses, we have shown that using an ?-order generative model for dependency parsing,which has never been done before, is practical.Our experimental results on the English sectionof the Universal Dependency Treebanks show thatthe ?-order generative model approximates thetrue dependency distribution better than the tradi-tional third-order model using counting, and tendsto choose more accurate parses in k-best lists.In addition, reranking with this model even out-performs the state-of-the-art TurboParser on unla-belled score metrics.Our source code is available at: github.com/lephong/iornn-depparse.AcknowledgmentsWe thank Remko Scha and three anonymous re-viewers for helpful comments.ReferencesMarco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, volume 1.Bernd Bohnet and Jonas Kuhn.
2012.
The best ofboth worlds: a graph-based completion model fortransition-based parsers.
In Proceedings of the 13thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 77?87.Association for Computational Linguistics.Gideon Borensztajn and Willem Zuidema.
2011.Episodic grammar: a computational model of theinteraction between episodic and semantic memoryin language processing.
In Proceedings of the 33dAnnual Conference of the Cognitive Science Soci-ety (CogSci?11), pages 507?512.
Lawrence ErlbaumAssociates.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 173?180.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):25?66.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
Computational lin-guistics, 29(4):589?637.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, pages 2121?2159.Jason M. Eisner.
1996a.
An empirical comparison ofprobability models for dependency grammar.
Tech-nical report, University of Pennsylvania Institute forResearch in Cognitive Science.Jason M Eisner.
1996b.
Three new probabilistic mod-els for dependency parsing: An exploration.
In Pro-ceedings of the 16th conference on Computationallinguistics-Volume 1, pages 340?345.
Associationfor Computational Linguistics.Jeffrey L. Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Christoph Goller and Andreas K?uchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In InternationalConference on Neural Networks.
IEEE.Katsuhiko Hayashi, Taro Watanabe, Masayuki Asa-hara, and Yuji Matsumoto.
2011.
Third-ordervariational reranking on packed-shared dependency738forests.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1479?1488.
Association for ComputationalLinguistics.Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-sumoto.
2013.
Efficient stacked dependency pars-ing by forest reranking.
Transactions of the Associ-ation for Computational Linguistics, 1(1):139?150.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086.
Association for Computational Linguistics.Pentti Kanerva.
2009.
Hyperdimensional comput-ing: An introduction to computing in distributed rep-resentation with high-dimensional random vectors.Cognitive Computation, 1(2):139?159.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1?11.
Association forComputational Linguistics.Phong Le, Willem Zuidema, and Remko Scha.
2013.Learning from errors: Using vector-based composi-tional semantics for parse reranking.
In ProceedingsWorkshop on Continuous Vector Space Models andtheir Compositionality (at ACL 2013).
Associationfor Computational Linguistics.Andre Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), pages617?622, Sofia, Bulgaria, August.
Association forComputational Linguistics.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training ofdependency parsers.
In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 91?98.
Association for Computa-tional Linguistics.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, KuzmanGanchev, Keith Hall, Slav Petrov, Hao Zhang, Os-car T?ackstr?om, et al.
2013.
Universal dependencyannotation for multilingual parsing.
Proceedings ofACL, Sofia, Bulgaria.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Timothy J O?Donnell, Noah D Goodman, and Joshua BTenenbaum.
2009.
Fragment grammar: Exploringreuse in hierarchical generative processes.
Techni-cal report, Technical Report MIT-CSAIL-TR-2009-013, MIT.Jordan B. Pollack.
1990.
Recursive distributed repre-sentations.
Artificial Intelligence, 46(1):77105.Federico Sangati and Willem Zuidema.
2011.
Ac-curate parsing with compact tree-substitution gram-mars: Double-DOP.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMLNP?11), pages 84?95.
Associationfor Computational Linguistics.Federico Sangati, Willem Zuidema, and Rens Bod.2009.
A generative re-ranking model for depen-dency parsing.
In Proceedings of the 11th Inter-national Conference on Parsing Technologies, pages238?241.Remko Scha.
1990.
Taaltheorie en taaltechnolo-gie; competence en performance.
In R. de Kortand G.L.J.
Leerdam, editors, Computertoepassin-gen in de Neerlandistiek, pages 7?22.
LVVN,Almere, the Netherlands.
English translation athttp://iaaa.nl/rs/LeerdamE.html.Richard Socher, Christopher D. Manning, and An-drew Y. Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, Seattle, Washington, USA, October.Ivan Titov and James Henderson.
2007.
A latent vari-able model for generative dependency parsing.
InProceedings of the 10th International Conference onParsing Technologies, pages 144?155.Andreas van Cranenburgh and Rens Bod.
2013.
Dis-continuous parsing with an efficient and accurateDOP model.
In Proceedings of the InternationalConference on Parsing Technologies (IWPT?13).Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of International Conferenceon Parsing Technologies (IWPT), pages 195?206.Hao Zhang and Ryan McDonald.
2012.
Generalizedhigher-order dependency parsing with cube prun-ing.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 320?331.
Association for Computational Lin-guistics.739
