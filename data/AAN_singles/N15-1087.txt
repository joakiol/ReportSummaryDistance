Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 862?871,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSentence segmentation of aphasic speechKathleen C. Fraser1,3, Naama Ben-David1, Graeme Hirst1,Naida L. Graham2,3, Elizabeth Rochon2,31Department of Computer Science, University of Toronto, Toronto, Canada2Department of Speech-Language Pathology, University of Toronto, Toronto, Canada3Toronto Rehabilitation Institute, Toronto, Canada{kfraser,naama,gh}@cs.toronto.edu, {naida.graham,elizabeth.rochon}@utoronto.caAbstractAutomatic analysis of impaired speech forscreening or diagnosis is a growing researchfield; however there are still many barriers toa fully automated approach.
When automaticspeech recognition is used to obtain the speechtranscripts, sentence boundaries must be in-serted before most measures of syntactic com-plexity can be computed.
In this paper, weconsider how language impairments can affectsegmentation methods, and compare the re-sults of computing syntactic complexity met-rics on automatically and manually segmentedtranscripts.
We find that the important bound-ary indicators and the resulting segmentationaccuracy can vary depending on the type ofimpairment observed, but that results on pa-tient data are generally similar to control data.We also find that a number of syntactic com-plexity metrics are robust to the types of seg-mentation errors that are typically made.1 IntroductionThe automatic analysis of speech samples is apromising direction for the screening and diagno-sis of cognitive impairments.
For example, recentstudies have shown that machine learning classifierstrained on speech and language features can detect,with reasonably high accuracy, whether a speakerhas mild cognitive impairment (Roark et al, 2011),frontotemporal lobar degeneration (Pakhomov et al,2010b), primary progressive aphasia (Fraser et al,2014), or Alzheimer?s disease (Orimaye et al, 2014;Thomas et al, 2005).
These studies used manuallytranscribed samples of patient speech; however, it isturning to politics for al gore and george w bush anotherday of rehearsal in just over forty eight hours the two menwill face off in their first of three debates for the first timevoters will get a live unfiltered view of them togetherTurning to politics, for Al Gore and George W Bush an-other day of rehearsal.
In just over forty-eight hours thetwo men will face off in their first of three debates.
For thefirst time, voters will get a live, unfiltered view of them to-gether.Figure 1: ASR text before and after processing.clear that for such systems to be practical in the realworld they must use automatic speech recognition(ASR).
One issue that arises with ASR is the intro-duction of word recognition errors: insertions, dele-tions, and substitutions.
This problem as it relates toimpaired speech has been considered elsewhere (Jar-rold et al, 2014; Fraser et al, 2013; Rudzicz et al,2014), although more work is needed.
Another is-sue, which we address here, is how ASR transcriptsare divided into sentences.The raw output from an ASR system is generallya stream of words, as shown in Figure 1.
With someeffort, it can be transformed into a format which ismore readable by both humans and machines.
Manyalgorithms exist for the segmentation of the raw textstream into sentences.
However, there has been noprevious work on how those algorithms might be ap-plied to impaired speech.This problem must be addressed for two reasons:first, sentence boundaries are important when an-alyzing the syntactic complexity of speech, whichcan be a strong indicator of potential impairment.862Many measures of syntactic complexity are basedon properties of the syntactic parse tree (e.g.
Yng-ve depth, tree height), which first require the de-marcation of individual sentences.
Even very basicmeasures of syntactic complexity, such as the meanlength of sentence, require this information.
Sec-ondly, there are many reasons to believe that exist-ing algorithms might not perform well on impairedspeech, since assumptions about normal speech donot hold true in the impaired case.
For example, innormal speech, pausing is often used to indicate aboundary between syntactic units, whereas in sometypes of dementia or aphasia a pause may indicateword-finding difficulty instead.
Other indicators ofsentence boundaries, such as prosody, filled pauses,and discourse markers, can also be affected by cog-nitive impairments (Emmorey, 1987; Bridges andVan Lancker Sidtis, 2013).Here we explore whether we can apply standardapproaches to sentence segmentation to impairedspeech, and compare our results to the segmentationof broadcast news.
We then extract syntactic com-plexity features from the automatically segmentedtext, and compare the feature values with measure-ments taken on manually segmented text.
We assesswhich features are most robust to the noisy segmen-tation, and thus could be appropriate features for fu-ture work on automatic diagnostic interfaces.2 Background2.1 Automatic sentence segmentationMany approaches to the problem of segmenting rec-ognized speech have been proposed.
One popu-lar way of framing the problem is to treat it as asequence tagging problem, where each interwordboundary must be labelled as either a sentenceboundary (B) or not (NB) (Liu and Shriberg, 2007).Liu et al (2005) showed that using a conditionalrandom field (CRF) classifier for this problem re-sulted in a lower error rate than using a hiddenMarkov model or maximum entropy classifier.
Theystated that the CRF approach combined the benefitsof these two other popular approaches, since it is dis-criminative, can handle correlated features, and usesa globally optimal sequence decoding.The features used to train such classifiers fallbroadly into two categories: word features andprosodic features.
Word features can include wordor part-of-speech n-grams, keyword identification,and filled pauses (Stevenson and Gaizauskas, 2000;Stolcke and Shriberg, 1996; Gavalda et al, 1997).Prosodic features include measures of pitch, energy,and duration of phonemes around the boundary, aswell as the length of the silent pause between words(Shriberg et al, 2000; Wang et al, 2003).The features which are most discriminative tothe segmentation task can change depending on thenature of the speech.
One important factor canbe whether the speech is prepared or spontaneous.Cuendet et al (2007) explored three different gen-res of speech: broadcast news, broadcast conversa-tions, and meetings.
They analyzed the effective-ness of different feature sets on each type of data.They found that pause features were the most dis-criminative across all groups, although the best re-sults were achieved using a combination of lexi-cal and prosodic features.
Kol?ar et al (2009) alsolooked at genre effects on segmentation, and foundthat prosodic features were more useful for segment-ing broadcast news than broadcast conversations.2.2 Primary progressive aphasiaThere are many different forms of language impair-ment that could affect how sentence boundaries areplaced in a transcript.
Here, we focus on the syn-drome of primary progressive aphasia (PPA).
PPAis a form of frontotemporal dementia which is char-acterized by progressive language impairment with-out other notable cognitive impairment.
In partic-ular, we consider two subtypes of PPA: semanticdementia (SD) and progressive nonfluent aphasia(PNFA).
SD is typically marked by fluent but emptyspeech, obvious word finding difficulties, and sparedgrammar (Gorno-Tempini et al, 2011).
In con-trast, PNFA is characterized by halting and some-times agrammatic speech, reduced syntactic com-plexity, and relatively spared single-word compre-hension (Gorno-Tempini et al, 2011).
Because syn-tactic impairment, including reduced syntactic com-plexity, is a core feature of PNFA, we expect thatmeasures of syntactic complexity would be impor-tant for a downstream screening application.
Fraseret al (2013) presented an automatic system for clas-sifying PPA subtypes from ASR transcripts, but theywere not able to include any syntactic complexity863metrics because their transcripts did not contain sen-tence boundaries.3 Data3.1 PPA dataTwenty-eight patients with PPA (11 with SD and 17with PNFA) were recruited through three memoryclinics, and 23 age- and education-matched healthycontrols were recruited through a volunteer pool.
Allparticipants were native speakers of English, or hadcompleted some of their education in English.To elicit a sample of narrative speech, partici-pants were asked to tell the well-known story of Cin-derella.
They were given a wordless picture bookto remind them of the story; then the book was re-moved and they were asked to tell the story in theirown words.
This procedure, described in full by Saf-fran et al (1989), is commonly used in studies ofconnected speech in aphasia.The narrative samples were transcribed by trainedresearch assistants.
The transcriptions include filledpauses, repetitions, and false starts.
Sentence bound-aries were marked by a single annotator accordingto semantic, syntactic, and prosodic cues.
We re-moved capitalization and punctuation, keeping trackof original sentence boundaries for training and eval-uation, to simulate a high-quality ASR transcript.3.2 Broadcast news dataFor the broadcast news data, we use a 804,064 wordsubset of the English section of the TDT4 Multilin-gual Broadcast News Speech Corpus1.
Using theannotations in the transcripts, we extracted newsstories only (ignoring teasers, miscellaneous text,and under-transcribed segments).
The transcriptionswere generated by closed captioning services andcommercial transcription agencies (Strassel, 2005),and so they are of high but not perfect quality.Again, we remove capitalization and punctuation tosimulate the output from an ASR system.Since the TDT4 corpus is much larger than ourPPA data set, we also construct a small news dataset by randomly selecting 20 news stories from theTDT4 corpus.
This allows us to determine whicheffects are due to differences in genre and which aredue to having a smaller training set.1catalog.ldc.upenn.edu/LDC2005S114 Methods4.1 Lexical and POS featuresThe lexical features are simply the unlemmatizedword tokens.
We do not consider word n-grams dueto the small size of our PPA data set.
To extractour part-of-speech (POS) features, we first tag thetranscripts using the NLTK POS tagger (Bird et al,2009).
We use the POS of the current word, the nextword, and the previous word as features.4.2 Prosodic featuresTo calculate the prosodic features, we first performautomatic alignment of the transcripts to the audiofiles.
This provides us with a phone-level transcrip-tion, with the start and end of each phone linked toa time in the audio file.
Using this information, weare able to calculate the length of the pauses betweenwords, which we bin into three categories based onprevious work by Pakhomov et al (2010a).
Each in-terword boundary either contains no pause, a shortpause (<400 ms), or a long pause (>400 ms).We calculate the pitch (Talkin, 1995; Brookes,1997), energy, and duration of the last vowel be-fore an interword boundary.
For each measurement,we compare the value to the average value for thatspeaker, as well as to the values for the last vowelbefore the next and previous interword boundaries.We perform the automatic alignment using theHTK toolkit (Young et al, 1997).
Our pronunci-ation dictionary is based on the CMU dictionary2,augmented with estimated pronunciations of out-of-vocabulary words using the ?g2p?
grapheme-to-phoneme toolkit (Bisani and Ney, 2008).
We use ageneric acoustic model that has been trained on WallStreet Journal text (Vertanen, 2006).4.3 ClassificationWe use a conditional random field (CRF) to labeleach interword boundary as either a sentence bound-ary (B) or not (NB).
We use a CRF implementationcalled CRFsuite (Okazaki, 2007) with the passive-aggressive learning algorithm.
To avoid overfitting,we set the minimum feature frequency cut-off to 20.To evaluate the performance of our system, wecompare the hypothesized sentence boundaries with2www.speech.cs.cmu.edu/cgi-bin/cmudict864the manually annotated sentence boundaries and re-port the F score, where F is the harmonic mean ofrecall and precision.
For the PPA data and the smallnews data, we assess the system using a leave-one-out cross validation framework, in which each nar-rative is sequentially held out as test data while thesystem is trained on the remaining narratives.
Forthe large TDT4 corpus, we randomly hold out 10%of the corpus as test data, and train on the remaining90%.4.4 Assessment of syntactic complexityOnce we have segmented the transcripts, we wantto assess how the (presumably noisy) segmentationaffects our measures of syntactic complexity.
Herewe consider a number of syntactic complexity met-rics that have been previously used in the study ofPPA speech (Fraser et al, 2014).
The metrics aredefined in the first column of Table 3.
For the firstfour metrics, we generated the parse tree for eachsentence using the Stanford parser (Klein and Man-ning, 2003).
The Yngve depth is a well-known mea-sure of how left-branching a parse tree is (Sampson,1997; Yngve, 1960).
The remaining metrics in Ta-ble 3 were calculated using Lu?s Syntactic Complex-ity Analyzer (SCA) (Lu, 2010).
We follow Lu?s def-initions for the various syntactic units: a clause is astructure consisting of at least a subject and a finiteverb, a dependent clause is a clause which could notform a sentence on its own, a verb phrase is a phraseconsisting of at least a verb and its dependents, acomplex nominal is a noun phrase, clause, or gerundthat stands in for a noun, a coordinate phrase is anadjective, adverb, noun, or verb phrase immediatelydominated by a coordinating conjunction, a T-unit isa clause and all of its dependent clauses, and a com-plex T-unit is a T-unit which contains a dependentclause.5 Segmentation results5.1 Comparison between data setsTable 1 shows the performance on the different datasets when trained using different combinations offeature types.
We also report the chance baselinefor comparison.We first consider the differences in results ob-served between the two news data sets.
The best re-Feature set TDT4 TDT4(small)Con-trolsSD PNFAChance baseline 0.07 0.07 0.05 0.07 0.06All 0.61 0.57 0.51 0.43 0.47Lexical+prosody 0.57 0.50 0.44 0.30 0.33Lexical+POS 0.48 0.36 0.36 0.36 0.40POS+prosody 0.61 0.59 0.45 0.39 0.45POS 0.45 0.39 0.28 0.35 0.39Prosody 0.50 0.48 0.24 0.23 0.25Lexical 0.26 0.14 0.18 0.17 0.18Table 1: F score for the automatic segmentationmethod on each data set.
Boldface indicates best incolumn.sults are similar in both groups, although, as wouldbe expected, the larger training sample performs bet-ter.
However, the difference is small, which sug-gests that the small size of the PPA data set shouldnot greatly hurt the performance.
When we comparethe performance of these two groups with differentsets of training features, we notice that the differencein performance is greatest when training on lexicalfeatures.
In a small random sample from the TDT4corpus, it is unlikely that two stories will cover thesame topic, and so there will be little overlap in vo-cabulary.
This is reflected in the results showing thatlexical features hurt the performance in this smallnews sample.Performance on the news corpus is better than onthe PPA data (including the control group).
Com-paring the small news sample to the PPA controls,we see that this is not simply due to the size of thetraining set, so we instead attribute the effect to thefact that speech in broadcast news is often prepared,while in the PPA data sets it is spontaneous.A closer look at the effect of prosodic featuresin our training data further shows the difference weobserve between prepared and spontaneous speech.When trained on the prosodic features alone, thenews data set performs relatively well, while perfor-mance on the control data is much worse.
Theseresults are consistent with the findings of Kol?ar etal.
(2009) regarding the effect of prosodic featuresin prepared and spontaneous speech.When comparing the performance on the controlgroup and on the PPA data, we see that generally, theresults are better on the controls.
This is to be ex-pected, as the speech in the control group has more865complete sentences and fewer disfluencies.
How-ever, it is interesting to note that performance onthe PNFA and SD groups is not much worse.
Allthree data sets achieved the best results when trainedwith all feature types.
This suggests that standardmethods of sentence segmentation for spontaneousspeech can be effective on PPA speech as well.Looking at the PPA and control groups with otherfeature sets, we see that POS features are more im-portant in the PNFA and SD groups than they arefor the control data.
A closer look at the transcriptsshows us that the PPA participants tend to connectindependent clauses with a conjunction more fre-quently than control participants, and independentclauses are often separated in the manual segmenta-tion.
This means that many sentence boundaries inthe PPA data are marked by conjunctions.
This isdiscussed further in the next section.When considering the prosodic and lexical featuresets individually, we see that performance is similarin all three cases (control, SD, and PNFA).
However,when we combine prosodic and lexical features to-gether, the performance in the control case increasesby a much larger margin than in the two aphasiccases.
This suggests that control participants com-bine words and prosody in a manner that is morepredictive of sentence boundaries than in the apha-sic case.5.2 Important featuresIn Table 2, we report the 10 features in each data setwhich are most strongly associated with a boundaryor a non-boundary.
We consider only the small newscorpus, for a fair comparison with the PPA data.The POS tags shown are the output of the NLTKpart of speech tagger, which uses the Penn TreebankTag Set.
We append ?
next?
and ?
prev?
to indicatethat this is the POS tag of the next and previousword respectively.
Italicized words represent lexicalitems.We first consider the features that indicate a sen-tence boundary (see Table 2a).
In general, we ob-serve that our minimum frequency cut-off removesmany of the lexical features from the top 10.
(Inthe absence of such a cut-off, we observed that verylow frequency words can be given deceptively highweights.)
The exceptions to this are the words goand her in the control set.
When we look at the data,TDT-4 (small) Control SD PNFAPRP next long pause CC next long pauseDT next go NNS CC nextRB her RB NNNNS NNS NN RB nextlong pause CC next RB next NNSpitch<ave RB PRP next RBNN RB next energy<ave short pauseCC next PRP next RB prev PRP nextenergy<ave IN VB no pauseIN prev short pause IN prev RB prev(a) Features associated with a boundaryTDT-4 (small) Control SD PNFAVBD next TO next the TO nextthe so PRP$ next thenIN CC and theMD next NNS next then sheCC the VBD next VBP nextVBG next she VBZ next andVBN next and TO next uhCD prev VBD next ?s VB nexta of I VBD nextto uh a a(b) Features associated with a non-boundaryTable 2: The 10 features with the highest weights ineach CRF model, indicating either that the followinginterword boundary is or is not a sentence boundary.there are indeed many occurrences of go and her atthe end of sentences, for example, she was not al-lowed to go or she couldn?t go, and very mean toher or so in love with her.
While these lexical itemsare not specific to the Cinderella story, it seems un-likely that these features would generalize to otherstory-telling tasks (although we note that the Cin-derella story is very widely used in the assessmentof aphasia and some types of dementia).The POS of the given word and its neighbours aregenerally important features.
In all four cases, thenext word being a coordinating conjunction or a pro-noun is indicative of a boundary.
In the three PPAcases, but not the news case, the next word beingan adverb is also indicative.
Looking at the data, weobserve that this very often corresponds to the use ofwords like so, then, well, anyway, etc.
This wouldseem to reflect a difference between the frequentuse of discourse markers in spontaneous speech andtheir relative sparsity in prepared speech.The POS of the current word is also important.
In866all cases, a boundary is associated with the currentword being an adverb or a noun.
In the control dataonly, the tag IN, representing either a prepositionor a subordinating clause, is also associated with aboundary.
Although this seems counter-intuitive, anexamination of the data reveals that in almost everycase, this corresponds to the phrase happily ever af-ter.
The fact that this feature does not occur in theother PPA groups could indicate that the patients areless likely to use this phrase, but could also be dueto our relatively high frequency cut-off.Another anomalous result is that the tag VB (verb,base form) is associated with a sentence boundaryin the SD case only.
Again, examples from the datasuggest a probable explanation.
In many cases, sen-tences ending with VB are actually statements aboutthe difficulty of the task, rather than narrative con-tent; e.g., that?s all I can say, I can?t recall, or Idon?t know.
These statements are consistent with theword-finding difficulties that are a hallmark of SD.In the prosodic features, we see that long pausesand decreases in pitch and energy are associatedwith sentence boundaries in the news corpus.
How-ever, the results are mixed in the PPA data.
This find-ing is consistent with our results in Section 5.1, andsupports the conclusion of Cuendet et al (2007) andKol?ar et al (2009) that prosodic features are moreuseful in prepared than spontaneous speech.We now look briefly at the features which are as-sociated with a non-boundary (Table 2b).
Here wesee more lexical features in the top 10, mostly func-tion words and filled pauses.
These features reflectthe reasonable assumption that most sentences donot end with determiners, conjunctions, or subjec-tive pronouns.
One feature which occurs in the newsdata but not the PPA data is the next word being amodal verb (MD).
This seems to be a result of themore frequent use of the future tense in the newsstories (e.g.
the senator will serve another term), incontrast to the Cinderella stories, which are gener-ally told in the present or simple past tense.6 Complexity resultsWe first compare calculating the syntactic complex-ity metrics on the manually segmented transcriptsand the automatically segmented transcripts.
The re-sults are given in Table 3.
Metrics for which thereis no significant difference between the manual andautomatic segmentation are marked with ?NS?.
Ofcourse, we do not claim that there is actually no dif-ference between the values, as can be seen in thetable, but we use this as a threshold to determinewhich features are less affected by the automaticsegmentation.All the features relating to Yngve depth andheight of the parse trees are significantly different (inat least one of the three clinical groups).
However, ofthe eight primary syntactic units calculated by Lu?sSCA, six show no significant difference when mea-sured on the automatically segmented transcripts.
Toexamine this effect further, we will discuss how eachof the eight is affected by the segmentation process.Although the number of sentences (S) is differ-ent, the number of clauses (C) is not significantly af-fected by the automatic segmentation, which impliesthat the boundaries are rarely placed within clauses,but rather between clauses.
An example of this phe-nomenon is given in Example 1:Manual: And then they go off to the ball and then shecomes I dunno how she meets up with this um fairygodmother whatever.Auto: And then they go off to the ball.
And then shecomes I dunno how she meets up with this um fairygodmother whatever.Our automatic method inserts a sentence boundarybefore the second and, breaking one sentence intotwo but not altering the number of clauses.
In fact,the proposed boundary seems quite reasonable, al-though it does not agree with the human annota-tor.
The correlation between the number of clausescounted in the manual and automatic transcripts is0.99 in all three clinical groups.
The counts for de-pendent clauses (DC) are also relatively unaffectedby the automatic segmentation, for similar reasons.The T-unit count (T) is also not significantly af-fected by the automatic segmentation.
Since a T-unit only contains one independent clause as wellas any attached dependent clauses, this suggests thatthe segmentation generally does not separate depen-dent clauses from their independent clauses.
Thisalso helps explain the lack of difference on complexT-units (CT).Table 3 also indicates that the number of verbphrases (VP) and complex nominals (CN) is not sig-nificantly different in the automatically segmented867Metric Diff?
Controls SD PNFAManual Auto Manual Auto Manual AutoMax YD maximum Yngve depth 5.10 4.53 4.45 3.87 4.66 3.83Mean YD mean Yngve depth 2.97 2.72 2.68 2.44 2.77 2.41Total YD total sum of the Yngve depths 66.92 53.41 42.48 32.95 49.95 32.57Tree height average parse tree height 12.56 11.30 10.79 9.81 11.25 9.88S number of sentences 24.35 31.22 27.73 37.36 18.82 25.47T number of T-units NS 31.43 35.13 32.55 39.27 23.29 27.41C number of clauses NS 61.48 64.48 57.73 62.45 42.94 46.65DC number of dependent clauses NS 24.70 27.30 26.27 26.09 16.59 18.88CN number of complex nominals NS 41.39 43.52 38.73 39.64 27.12 27.88VP number of verb phrases NS 77.00 79.65 72.09 77.00 51.76 55.24CP number of coordinate phrases 12.39 10.30 11.55 6.91 7.82 4.18CT number of complex T-units NS 14.30 13.52 12.00 11.82 9.29 8.71MLS mean length of sentence 19.79 16.22 14.04 11.25 15.86 11.60MLT mean length of T-unit 14.92 13.72 12.19 10.46 12.78 10.66MLC mean length of clause 7.55 7.21 7.13 6.58 6.89 6.39T/S T-units per sentence 1.34 1.17 1.15 1.06 1.23 1.08C/S clauses per sentence 2.64 2.25 1.96 1.70 2.28 1.82DC/T dependent clauses per T-unit NS 0.80 0.78 0.73 0.63 0.73 0.69VP/T verb phrases per T-unit 2.47 2.34 2.11 1.92 2.23 1.98CP/T coordinate phrases per T-unit 0.40 0.33 0.35 0.17 0.35 0.15CN/T complex nominals per T-unit NS 1.32 1.26 1.18 1.10 1.17 1.01C/T clauses per T-unit 1.99 1.91 1.71 1.58 1.86 1.68CT/T complex T-units per T-unit NS 0.46 0.40 0.37 0.32 0.39 0.32DC/C dependent clauses per clause NS 0.39 0.41 0.42 0.40 0.38 0.41CP/C coordinate phrases per clause 0.20 0.17 0.20 0.10 0.19 0.09CN/C complex nominals per clause NS 0.65 0.65 0.70 0.71 0.63 0.61Table 3: Mean values of syntactic complexity metrics for the different patient groups.
Features which showno significant difference between the manual and automatic segmentation on all three clinical groups aremarked as ?NS?
(not significant).transcripts.
Since these syntactic units are typicallysub-clausal, this is not unexpected given the argu-ments above.The remaining primary syntactic unit, the coordi-nate phrase (CP), is different in the automatic tran-scripts.
This represents a weakness of our method;namely, it has a tendency to insert a boundary beforeall coordinating conjunctions, as in Example 2:Manual: So she is very upset and she?s crying and withher fairy godmother who then uh creates a carriageand horses and horsemen and and driver and beau-tiful dress and magical shoes.Auto: So she is very upset.
And she?s crying and withher.
Fairy godmother who then uh creates a car-riage.
And horses and horsemen and and driver.And beautiful dress.
And magical shoes.In this case, the manual transcript has five coordinatephrases, while the automatic transcript has only two.The mean lengths of sentence (MLS), clause(MLC), and T-unit (MLT) are all significantly differ-ent in the automatically segmented transcripts.
Weascribe this to the fact that a small change in C or Tcan lead to a large change in MLC or MLT.
The re-maining metrics in Table 3 are simply combinationsof the primary units discussed above.Our analysis so far suggests that some syntac-tic units are relatively impervious to the automaticsentence segmentation, while others are more sus-ceptible to error.
However, when we examine themean values given in Table 3, we observe that evenin cases when the complexity metrics are signifi-cantly different in the automatic transcripts, the dif-ferences appear to be systematic.
For example, weknow that our segmentation method tends to pro-duce more sentences than appear in the manual tran-scripts (i.e., S is always greater in the automatic tran-scripts).
If we look at the differences across clinicalgroups, the same pattern emerges in both the man-868Metric SD vscontrolsPNFA vscontrolsSD vsPNFAmanual auto manual auto manual autoMax YD * * * *Mean YD * * *Total YD * * * *Tree height * * * *ST * *C * *DC * *CN * *VP * *CP * *CT * *MLS * * * *MLT * * * *MLC * * *T/S *C/S * * * *DC/TVP/T * *CP/T * *CN/T *C/T *CT/T * *DC/CCP/C * *CN/CTable 4: Difference between syntactic complexitymetrics for each pair of patient groups.
A significantdifference (p < 0.05) is marked with an asterisk.ual and automatic transcripts: participants with SDproduce the most sentences, followed by controls,followed by participants with PNFA.
In most appli-cations of these syntactic complexity metrics, whatmatters most is not the absolute value of the metric,but the relative differences between groups.
So, wenow ask: which features distinguish between clin-ical groups in the manually segmented transcripts,and do they still distinguish between the groups inthe automatically segmented transcripts?Our results for this experiment are reported in Ta-ble 4.
In the case of SD vs. controls, there are 11features which are significantly different betweenthe two groups in the manual transcripts.
Sevenof these features are also significantly different be-tween groups in the automatic transcripts, while anadditional three features are significant only in theautomatic transcripts.
In the PNFA vs. controls case,there are 15 distinguishing features in the manualtranscripts, and 14 of those are also significantly dif-ferent in the automatic transcripts.
There are fourfeatures which are significant only in the automaticcase.
Finally, in the case of SD vs. PNFA, there isonly one distinguishing feature in the manual tran-scripts, and none in the automatic transcript.These findings suggest that automatically seg-mented transcripts can still be useful, even if thecomplexity metrics have different values from themanual transcripts.
Importantly, a comparison of themean feature values in Table 3 reveals that in 92% ofcases, and in every case marked as significant in Ta-ble 4 4, the direction of the trend is the same in themanual and automatic transcripts.For example, in the first column of Table 4, maxi-mum Yngve depth is significantly different betweenSD participants and controls.
In both the manualand automatic transcripts, the controls have a greatermaximum depth than SD participants.
This is truefor every metric that is significant in both the man-ual and automatic transcripts.
This indicates thatthe metrics are not only significantly different, andtherefore useful for machine learning classificationor some other downstream application, but that theyare interpretable in relation to the specific languageimpairments that we expect to observe in the patientgroups.7 DiscussionWe have introduced the issue of sentence segmenta-tion of impaired speech, and tested the effectivenessof standard segmentation methods on PPA speechsamples.
We found that, as expected, performancewas best on prepared speech from broadcast news,then on healthy controls, and worst on speech sam-ples from PPA patients.
However, the results onthe PPA data are promising, and suggest that simi-lar methods could be effective for impaired speech.Future work will look at adapting the standard algo-rithms to improve performance in the impaired case.This would include an evaluation of the forced align-ment on impaired speech data, as well as the explo-ration of new features for the boundary classifica-tion.One limitation of this study is the use of manu-ally transcribed data with capitalization and punc-tuation removed to simulate perfect ASR data.
Weexpect that real ASR data will contain recognitionerrors, and it is not clear how these errors will affectthe segmentation process.
As well, our PPA data setis relatively small from a machine learning perspec-869tive, due to the inherent difficulties associated withcollecting clinical data.
Furthermore, we assumedthat the diagnostic group is known a priori, allow-ing us to train and test on each group separately.We analyzed our results to see how the noise in-troduced by our segmentation affects various syn-tactic complexity measures.
Some measures (e.g.T-units) were robust to the noise, while others (e.g.Yngve depth) were not.
When using such automaticmethods for the analysis of speech data, researchersshould be aware of the unequal effects on differentcomplexity metrics.For the more practical goal of distinguishing be-tween different patient groups, we found that mostmeasures that were significant for this task using themanual transcripts remained so when using the au-tomatically segmented ones, and the direction of thedifference was the same in the manual and automatictranscripts.
In all cases where a significant differ-ence between the groups was detected, the directionof the difference was the same in the manual and au-tomatic transcripts.
These results indicate that im-perfect segmentation methods might still be usefulfor some applications, since they affect the data in asystematic way.Although we evaluated our methods againsthuman-annotated data, there is some uncertaintyabout whether a single gold standard for the sen-tence segmentation of speech truly exists.
Millerand Weinert (1998), among others, argue that theconcept of a sentence as defined in written languagedoes not necessarily exist in spoken language.
Infuture work, it would useful to compare the inter-annotator agreement between trained human anno-tators to determine an upper bound for the accuracy.AcknowledgmentsMany thanks to Bruna Seixas Lima for providing themanual annotations.
This work was supported bythe Canadian Institutes of Health Research (CIHR),Grant #MOP-82744, and the Natural Sciences andEngineering Research Council of Canada (NSERC).ReferencesSteven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural language processing with Python.
?
O?Reilly Me-dia, Inc.?.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451.Kelly Ann Bridges and Diana Van Lancker Sidtis.
2013.Formulaic language in Alzheimer?s disease.
Aphasiol-ogy, pages 1?12.Michael Brookes.
1997.
Voicebox: Speech process-ing toolbox for Matlab.
www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html.Sebastien Cuendet, Elizabeth Shriberg, Benoit Favre,James Fung, and Dilek Hakkani-T?ur.
2007.
An anal-ysis of sentence segmentation features for broadcastnews, broadcast conversations, and meetings.
Search-ing Spontaneous Conversational Speech, pages 43?49.Karen D. Emmorey.
1987.
The neurological substratesfor prosodic aspects of speech.
Brain and Language,30(2):305?320.Kathleen Fraser, Frank Rudzicz, Naida Graham, andElizabeth Rochon.
2013.
Automatic speech recog-nition in the diagnosis of primary progressive apha-sia.
In Proceedings of the Fourth Workshop on Speechand Language Processing for Assistive Technologies,pages 47?54.Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham,Carol Leonard, Graeme Hirst, Sandra E. Black, andElizabeth Rochon.
2014.
Automated classification ofprimary progressive aphasia subtypes from narrativespeech transcripts.
Cortex, 55:43?60.Marsal Gavalda, Klaus Zechner, et al 1997.
High perfor-mance segmentation of spontaneous speech using partof speech and trigger word information.
In Proceed-ings of the Fifth Conference on Applied Natural Lan-guage Processing, pages 12?15.
Association for Com-putational Linguistics.M.
L. Gorno-Tempini, A. E. Hillis, S. Weintraub,A.
Kertesz, M. Mendez, S. F. Cappa, J. M. Ogar,J.
D. Rohrer, S. Black, B. F. Boeve, F. Manes, N. F.Dronkers, R. Vandenberghe, K. Rascovsky, K. Patter-son, B. L. Miller, D. S. Knopman, J. R. Hodges, M. M.Mesulam, and M. Grossman.
2011.
Classification ofprimary progressive aphasia and its variants.
Neurol-ogy, 76:1006?1014.William Jarrold, Bart Peintner, David Wilkins, DimitraVergryi, Colleen Richey, Maria Luisa Gorno-Tempini,and Jennifer Ogar.
2014.
Aided diagnosis of dementiatype through computer-based analysis of spontaneousspeech.
In Proceedings of the ACL Workshop on Com-putational Linguistics and Clinical Psychology, pages27?36.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics, pages 423?430.870J?achym Kol?ar, Yang Liu, and Elizabeth Shriberg.
2009.Genre effects on automatic sentence segmentation ofspeech: A comparison of broadcast news and broad-cast conversations.
In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 4701?4704.
IEEE.Yang Liu and Elizabeth Shriberg.
2007.
Comparingevaluation metrics for sentence boundary detection.
InProceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP),volume 4, pages 182?185.
IEEE.Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and MaryHarper.
2005.
Using conditional random fields forsentence boundary detection in speech.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, ACL ?05, pages 451?458,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Xiaofei Lu.
2010.
Automatic analysis of syntacticcomplexity in second language writing.
InternationalJournal of Corpus Linguistics, 15(4):474?496.Jim Miller and Regina Weinert.
1998.
Spontaneous spo-ken language.
Clarendon Press.Naoaki Okazaki.
2007.
CRFsuite: a fast implementationof conditional random fields (CRFs).Sylvester Olubolu Orimaye, Jojo Sze-Meng Wong, andKaren Jennifer Golden.
2014.
Learning predictive lin-guistic features for Alzheimer?s disease and related de-mentias using verbal utterances.
In Proceedings of the1st Workshop on Computational Linguistics and Clin-ical Psychology (CLPsych), pages 78?87, Baltimore,Maryland.
Association for Computational Linguistics.S.
V. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano,N.
Graff-Radford, R. Caselli, and D. S. Knopman.2010a.
Computerized analysis of speech and languageto identify psycholinguistic correlates of frontotempo-ral lobar degeneration.
Cognitive and Behavioral Neu-rology, 23:165?177.Serguei V.S.
Pakhomov, Glen E. Smith, Susan Marino,Angela Birnbaum, Neill Graff-Radford, RichardCaselli, Bradley Boeve, and David D. Knopman.2010b.
A computerized technique to assess languageuse patterns in patients with frontotemporal dementia.Journal of Neurolinguistics, 23:127?144.Brian Roark, Margaret Mitchell, John-Paul Hosom,Kristy Hollingshead, and Jeffery Kaye.
2011.
Spokenlanguage derived measures for detecting mild cogni-tive impairment.
IEEE Transactions on Audio, Speech,and Language Processing, 19(7):2081?2090.Frank Rudzicz, Rosalie Wang, Momotaz Begum, andAlex Mihailidis.
2014.
Speech recognition inAlzheimer?s disease with personal assistive robots.
InProceedings of the 5th Workshop on Speech and Lan-guage Processing for Assistive Technologies (SLPAT),pages 20?28.
Association for Computational Linguis-tics.Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.Schwartz.
1989.
The quantitative analysis of agram-matic production: Procedure and data.
Brain and Lan-guage, 37(3):440?479.Geoffrey Sampson.
1997.
Depth in English grammar.Journal of Linguistics, 33:131?51.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-T?ur,and G?okhan T?ur.
2000.
Prosody-based automatic seg-mentation of speech into sentences and topics.
SpeechCommunication, 32(1):127?154.Mark Stevenson and Robert Gaizauskas.
2000.
Exper-iments on sentence boundary detection.
In Proceed-ings of the Sixth Conference on Applied Natural Lan-guage Processing, pages 84?89.
Association for Com-putational Linguistics.Andreas Stolcke and Elizabeth Shriberg.
1996.
Au-tomatic linguistic segmentation of conversationalspeech.
In Proceedings of the Fourth InternationalConference on Spoken Language Processing, vol-ume 2, pages 1005?1008.
IEEE.Stephanie Strassel, 2005.
Topic Detection and TrackingAnnotation Guidelines: Task Definition to Support theTDT2002 and TDT2003 Evaluations in English, Chi-nese and Arabic.
Linguistic Data Consortium, 1.5 edi-tion.David Talkin.
1995.
A robust algorithm for pitch track-ing (RAPT).
Speech Coding and synthesis, 495:495?518.Calvin Thomas, Vlado Keselj, Nick Cercone, KennethRockwood, and Elissa Asp.
2005.
Automatic detec-tion and rating of dementia of Alzheimer type throughlexical analysis of spontaneous speech.
In Proceed-ings of the IEEE International Conference on Mecha-tronics and Automation, pages 1569?1574.Keith Vertanen.
2006.
Baseline WSJ acoustic modelsfor HTK and Sphinx: Training recipes and recognitionexperiments.
Technical report, Cavendish Laboratory,University of Cambridge.Dong Wang, Lie Lu, and Hong-Jiang Zhang.
2003.Speech segmentation without speech recognition.
InProceedings of the IEEE International ConferenceAcoustics, Speech, and Signal Processing (ICASSP),volume 1, pages 468?471.
IEEE.Victor Yngve.
1960.
A model and hypothesis for lan-guage structure.
Proceedings of the American Physi-cal Society, 104:444?466.Steve Young, Gunnar Evermann, Dan Kershaw, GarethMoore, Julian Odell, Dave Ollason, Valtcho Valtchev,and Phil Woodland.
1997.
The HTK book, volume 2.Entropic Cambridge Research Laboratory Cambridge.871
