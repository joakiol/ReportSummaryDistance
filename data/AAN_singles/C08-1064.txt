Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 505?512Manchester, August 2008Tera-Scale Translation Models via Pattern MatchingAdam LopezSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, United Kingdomalopez@inf.ed.ac.ukAbstractTranslation model size is growing at a pacethat outstrips improvements in computingpower, and this hinders research on manyinteresting models.
We show how an al-gorithmic scaling technique can be usedto easily handle very large models.
Us-ing this technique, we explore several largemodel variants and show an improvement1.4 BLEU on the NIST 2006 Chinese-English task.
This opens the door for workon a variety of models that are much lessconstrained by computational limitations.1 IntroductionTranslation model size is growing quickly due tothe use of larger training corpora and more com-plex models.
As an example of the growth in avail-able training data, consider the curated Europarlcorpus (Koehn, 2005), which more than doubled insize from 20 to 44 million words between 2003 and2007.1 As an example of model complexity, con-sider the popular hierarchical phrase-based modelof Chiang (2007), which can translate discontigu-ous phrases.
Under the loosest interpretation ofthis capability, any subset of words in a sentenceThis research was conducted while I was at the Universityof Maryland.
I thank David Chiang, Bonnie Dorr, Doug Oard,Philip Resnik, and the anonymous reviewers for comments,and especially Chris Dyer for many helpful discussions andfor running the Moses experiments.
This research was sup-ported by the GALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-2-001 andby the EuroMatrix project funded by the European Commis-sion (6th Framework Programme).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1Statistics from http://www.statmt.org/europarl.can be a phrase.
Therefore, the number of rulesthat the model can learn is exponential in sentencelength unless strict heuristics are used, which maylimit the model?s effectiveness.
Many other mod-els translate discontiguous phrases, and the size oftheir extracted rulesets is such a pervasive problemthat it is a recurring topic in the literature (Chiang,2007; DeNeefe et al, 2007; Simard et al, 2005).Most decoder implementations assume that allmodel rules and parameters are known in advance.With very large models, computing all rules andparameters can be very slow.
This is a bottleneckin experimental settings where we wish to exploremany model variants, and therefore presents a realimpediment to full exploration of their potential.We present a solution to this problem.To fully motivate the discussion, we give a con-crete example of a very large model, which wegenerate using simple techniques that are knownto improve translation accuracy.
The model takes77 CPU days to compute and consumes nearly aterabyte of external storage (?2).
We show howto solve the problem with a previously developedalgorithmic scaling technique that we call transla-tion by pattern matching (?3).
The key idea be-hind this technique is that rules and parameters arecomputed only as needed.
Using this technique,we explore a series of large models, giving experi-mental results along a variety of scaling axes (?4).Our results extend previous findings on the use oflong phrases in translation, shed light on the sourceof improved performance in hierarchical phrase-based models, and show that our tera-scale trans-lation model outperforms a strong baseline.2 A Tera-Scale Translation ModelWe will focus on the hierarchical phrase-basedmodel of Chiang (2007).
It compares favorably505with conventional phrase-based translation (Koehnet al, 2003) on Chinese-English news translation(Chiang, 2007).
We found that a baseline systemtrained on 27 million words of news data is alreadyquite strong, but we suspect that it would be possi-ble to improve it using some simple techniques.Add additional training data.
Our baselinealready uses much of the available curated newsdata, but there is at least three times as much cu-rated data available in the United Nations proceed-ings.
Adding the UN data gives us a training cor-pus of 107 million words per language.Change the word alignments.
Our baselineuses Giza++ alignments (Och and Ney, 2003)symmetrized with the grow-diag-final-and heuris-tic (Koehn et al, 2003).
We replace these withthe maximum entropy aligments of Ayan and Dorr(2006b).
They reported improvements of 1.6BLEU in Chinese-English translation, though withmuch less training data.Change the bilingual phrase extractionheuristic.
Our baseline uses a tight heuristic, re-quiring aligned words at phrase edges.
However,Ayan and Dorr (2006a) showed that a loose heuris-tic, allowing unaligned words at the phrase edges,improved accuracy by 3.7 BLEU with some align-ments, again with much less training data.Quadrupling the amount of training data pre-dictably increases model size.
The interaction ofthe alignment and phrase extraction heuristic in-creases the model size much more.
This is becausethe maximum entropy alignments are sparse?fewer than 70% of the words are aligned.
Con-sider a contiguous phrase chosen at random from atraining sentence.
With our sparse alignments, thechance that both of its edge words are aligned isless than half.
The tight heuristic discards manypossible phrases on this basis alone.
The situa-tion worsens with discontiguous phrases.
How-ever, with the loose heuristic, we see the oppo-site effect.
Not only is a randomly chosen sourcephrase with unaligned edge words legal, but it mayhave many translations, since its minimal align-ment in the target is likely to have one or moreadjacent unaligned words, and any combination ofthese can be part of a valid target phrase.To make matters concrete, we estimated the sizeof the translation model that would be producedusing these modifications.
We did not actuallycompute the full model, for reasons that will be-come apparent.
Instead, we modified Chiang?s ex-tractor to simply report the number of rules that itwould normally extract.
We then computed the ra-tio of extracted rules to that of the baseline system.Under the rough assumption that the number ofunique rules and representation size grows linearlyin the number of extracted rules, we were then ableto estimate the size of the large model.
The resultsshow that it would be impractical to compute themodel (Table 1).
Merely counting all of the ruleoccurrences took nearly 5 days on our 17-node re-search cluster.
This does not even include the timerequired for sorting and scoring the rules, whichwe did not attempt.
The resulting model wouldbe nearly two orders of magnitude larger than thelargest one we could find in the literature (Table 2).3 Translation by Pattern MatchingClearly, substantial experimentation with modelsthis large is impossible unless we have consider-able resources at our disposal.
To get around thisproblem, we use an algorithmic scaling techniquethat we call translation by pattern matching.
In thisapproach, the training text and its word alignmentreside in memory.
We then translate as follows.for each input sentence dofor each possible phrase in the sentence doFind its occurrences in the source textfor each occurrence doExtract its aligned target phrase (if any)for each extracted phrase pair doScore using maximum likelihoodDecode as usual using the scored rulesA similar method is used in example-basedtranslation (Brown, 2004).
It was applied tophrase-based translation by Callison-Burch et al(2005) and Zhang and Vogel (2005).
The key pointis that the complete translation model is never ac-tually computed?rules and associated parametersare computed only as needed.
Obviously, the on-demand computation must be very fast.
If we canachieve this, then the model can in principle be ar-bitrarily large.
Callison-Burch et al (2005) andZhang and Vogel (2005) give very similar recipesfor application to phrase-based models.Fast lookup using pattern matching algo-rithms.
The complexity of the na?
?ve algorithm tofind all occurrences of a source phrase in a train-ing text T is linear in the length of the text, O(|T |).This is much too slow for large texts.
They solvethis using an index data structure called a suffix506Baseline LargeRules extracted (millions) 195 19,300Extract time (CPU hours) 10.8 1,840Unique rules (millions) 67 6,600*Extract file size (GB) 9.3 917*Model size (GB) 6.1 604*Table 1: Extraction time and model sizes.
Themodel size reported is the size of the files contain-ing an external prefix tree representation (Zens andNey, 2007).
*Denotes estimated quantities.Citation Millions of rulesSimard et al (2005) (filtered) 4Chiang (2007) (filtered) 6DeNeefe et al (2007) 57Zens and Ney (2007) 225this paper 6,600Table 2: Model sizes in the literature.array (Manber and Myers, 1993).
Its size is 4|T |bytes and it enables lookup of any length-m sub-string of T in O(m+ log |T |) time.Fast extraction using sampling.
The complex-ity of extracting target phrases is linear in the num-ber of source phrase occurrences.
For very fre-quent source phrases, this is expensive.
They solvethis problem by extracting only from a sample ofthe found source phrases, capping the sample sizeat 100.
For less frequent source phrases, all possi-ble targets are extracted.Fast scoring using maximum likelihood.Scoring the phrase pairs is linear in the numberof pairs if we use the maximum likelihood esti-mate p(e|f) for source phrase f and target phrasee.
Since each source phrase only has small numberof targets (up to the sample limit), this step is fast.However, notice that we cannot easily compute thetarget-to-source probability p(f |e) as is commonlydone.
We address this in ?4.1.Callison-Burch et al (2005) and Zhang and Vo-gel (2005) found that these steps added only tenthsof a second to per-sentence decoding time, whichwe independently confirmed (Lopez, 2008, Chap-ter 3).
Their techniques also apply to discontigu-ous phrases, except for the pattern matching algo-rithm, which only works for contiguous phrases.Since our model (and many others) uses discon-tiguous phrases, we must use a different algorithm.The most straightforward way to accomplish thisis to use the fast suffix array lookup for the con-tiguous subphrases of a discontiguous phrase, andthen to combine the results.
Suppose that we havesubstrings u and v, and a gap character X whichcan match any arbitrary sequence of words.
Thento look up the phrase uXv, we first find all occur-rences of u, and all occurrences of v. We can thencompute all cases where an occurrence of u pre-cedes and occurrence of v in the same sentence.The complexity of this last step is linear in thenumber of occurrences of u and v. If either u orv is very frequent, this is too slow.
Lopez (2007;2008) solves this with a series of empirically fastexact algorithms.
We briefly sketch the solutionhere; for details see Lopez (2008, Chapter 4).Lossless pruning.
For each phrase, we onlysearch if we have already successfully found bothits longest suffix and longest prefix.
For example,if a, b, c, and d are all words, then we only searchfor phrase abXcd if we have already found occur-rences of phrases abXc and bXcd.Precomputation of expensive searches.
Forphrases containing multiple very frequent sub-phrases, we precompute the list of occurrences intoan inverted index.
That is, if both u and v are fre-quent, we simply precompute all locations of uXvand vXu.Fast merge algorithm.
For phrases pairing afrequent subphrase with infrequent subphrases, weuse a merge algorithm whose upper bound com-plexity is logarithmic in the number of occurrencesof the frequent subphrase.
That is, if count(u) issmall, and count(v) is big, then we can find uXvin at most O(count(u) ?
log(count(v))) time.Our implementation is a fast extension to the Hi-ero decoder (Chiang, 2007), written in Pyrex.2 Itis an order of magnitude faster than the Python im-plementation of Lopez (2007).
Pattern matching,extraction, and scoring steps add approximately2 seconds to per-sentence decoding time, slow-ing decoding by about 50% compared with a con-ventional exact model representation using exter-nal prefix trees (Zens and Ney, 2007).
See Lopez(2008, Chapter 4) for analysis.4 ExperimentsAlthough the algorithmic issues of translation bypattern matching are largely solved, none of theprevious work has reported any improvements in2Pyrex combines Python and C code for performance.http://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/507state of the art with very large models.3 In theremainder of this work, we scratch the surface ofpossible uses.We experimented on Chinese-English newswiretranslation.
Except where noted, each system wastrained on 27 million words of newswire data,aligned with GIZA++ (Och and Ney, 2003) andsymmetrized with the grow-diag-final-and heuris-tic (Koehn et al, 2003).
In all experiments that fol-low, each system configuration was independentlyoptimized on the NIST 2003 Chinese-English testset (919 sentences) using minimum error rate train-ing (Och, 2003) and tested on the NIST 2005Chinese-English task (1082 sentences).
Optimiza-tion and measurement were done with the NISTimplementation of case-insensitive BLEU 4n4r(Papineni et al, 2002).44.1 BaselineWe compared translation by pattern matching witha conventional exact model representation usingexternal prefix trees (Zens and Ney, 2007).
Tomake model computation efficient for the lattercase, we followed the heuristic limits on phrase ex-traction used by Chiang (2007).?
Phrases were restricted to five words.
Eachgap character counts as a single word regard-less of how many actual words it spans.
Thusphrase aXb consisting of words a and b sep-arated by a gap is three words.?
Phrases were restricted to a span of ten wordsin the training data.?
Phrases were restricted to two gaps.?
Gaps were required to span at least two wordsin the training data.?
Phrases were extracted using a tight heuristic.Chiang (2007) uses eight features, so we incor-porate these into the conventional baseline.
How-ever, as discussed previously in ?3, the patternmatching architecture makes it difficult to computethe target-to-source translation probability, so thisfeature is not included in the pattern matching sys-tem.
This may not be a problem?Och and Ney3Zhang and Vogel (2005) report improvements, but all oftheir results are far below state of the art for the reported task.This may be because their system was not tuned using mini-mum error rate training (Och, 2003).4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl(2002) observed that this feature could be replacedby the source-to-target probability without loss ofaccuracy.
Preliminary experiments suggested thattwo other features in Chiang?s model based on rulecounts were not informative, so we considered amodel containing only five features.1.
Sum of logarithms of source-to-target phrasetranslation probabilities.2.
Sum of logarithms of source-to-target lexicalweighting (Koehn et al, 2003).3.
Sum of logarithms of target-to-source lexicalweighting.4.
Sum of logarithms of a trigram languagemodel.5.
A word count feature.The sample size (see ?3) for the pattern match-ing system was 300.5 Results show that both trans-lation by pattern matching and reduced feature setare harmless to translation accuracy (Table 3).4.2 Rapid Prototyping via Pattern MatchingEven in this limited experiment, translation by pat-tern matching improved experimental turnaround.For the conventional system, it took 10.8 hours tocompute the full model, which required 6.1GB ofspace.
For the pattern matching system, it tookonly 8 minutes to compute all data structures andindexes.
These required only 852MB of space.There are two tradeoffs.
First, memory use ishigher in the pattern matching system, since theconventional representation resides on disk.
Sec-ond, per-sentence decoding time with the patternmatching system is slower by about 2 seconds dueto the expense of computing rules and parameterson demand.
Even so, the experimental turnaroundtime with the pattern matching system was stillfaster.
We would need to decode nearly 20,000sentences with it to equal the computation time ofconventional model construction.
We might needto decode this many times during MERT, but onlybecause it decodes the same test set many times.However, it is straightforward to extract all rulesfor the development set using pattern matching,and use them in a conventional system for MERT.5We use deterministic sampling, which is useful for repro-ducibility and for minimum error rate training (Och, 2003).See Lopez (2008, Chapter 3) for details.508System BLEUConventional (eight features) 30.7Conventional (five features) 30.6Pattern matching (five features) 30.9Table 3: Baseline system results.Taking our five-feature pattern matching sys-tem as a starting point, we next considered severalways in which we might scale up the translationmodel.
Here the benefits of prototyping becomemore apparent.
If we were to run the followingexperiments in a conventional system, we wouldneed to compute a new model for each condition.With translation by pattern matching, nearly ev-ery variant uses the same underlying representa-tion, so it was rarely even necessary to recomputedata structures and indexes.4.3 Relaxing Length RestrictionsIncreasing the maximum phrase length in standardphrase-based translation does not improve BLEU(Koehn et al, 2003; Zens and Ney, 2007).
How-ever, this effect has not yet been evaluated in hier-archical phrase-based translation.We experimented with two analogues to themaximum phrase length.
First, we varied the limiton source phrase length (counting each gap as asingle word), the closest direct analogue.
Chiang(2007) used a limit of five.
We found that accuracyplateaus at this baseline setting (Figure 1).
Second,we varied the limit on the span of phrases extractedfrom the training text.
Suppose we are interested ina source phrase uXv.
If u and v are collocated in atraining sentence, but within a span longer than thelimit, then the model is prevented from learning arule that translates this discontiguous phrase as asingle unit.
Chiang (2007) fixes this limit at ten.Again, accuracy plateaus near the baseline setting(Figure 2).Our results are similar to those for conventionalphrase-based models (Koehn et al, 2003; Zens andNey, 2007).
Though scaling along these axes is un-helpful, there is still a large space for exploration.4.4 Interlude: Hierarchical Phrase-BasedTranslation versus Lexical ReorderingIn a related line of inquiry, we considered the ef-fect of increasing the number of gaps in phrases.Chiang (2007) limits them to two.
Although weconsider more than this, we also considered fewer,2 4 6 8 1016.9 25.4 29.730.430.930.930.830.830.930.7BLEUFigure 1: Effect of the maximum phrase length.1 3 5 7 9 11 13 1516.1 25.528.530.330.831.131.230.8BLEUFigure 2: Effect of the maximum phrase span.to gain insight into the hierarchical model.Hierarchical phrase-based translation is oftenreported to be better than conventional phrase-based translation, but the actual reason for this isunknown.
It is often argued that the ability to trans-late discontiguous phrases is important to model-ing translation (Chiang, 2007; Simard et al, 2005;Quirk and Menezes, 2006), and it may be that thisexplains the results.
However, there is another hy-pothesis.
The model can also translate phrases inthe form uX or Xu (a single contiguous unit anda gap).
If it learns that uX often translates asXu?, then in addition to learning that u translatesas u?, it has also learned that u switches places witha neighboring phrase during translation.
This issimilar to lexicalized reordering in conventionalphrase-based models (Tillman, 2004; Al-Onaizanand Papineni, 2006).6 If this is the real benefit ofthe hierarchical model, then the ability to translatediscontiguous phrases may be irrelevant.To tease apart these claims, we make the follow-ing distinction.
Rules in which both source and tar-get phrases contain a single contiguous element?that is, in the form u, Xu, uX , or XuX?encode lexicalized reordering in hierarchical form.Rules representing the translation of discontigu-ous units?minimally uXv?encode translationknowledge that is strictly outside the purview oflexical reordering.We ran experiments varying both the number ofcontiguous subphrases and the number of gaps (Ta-6This hypothesis was suggested independently in personalcommunications with several researchers, including ChrisCallison-Burch, Chris Dyer, Alex Fraser, and Franz Och.509ble 4).
For comparison, we also include resultsof the phrase-based system Moses (Koehn et al,2007) with and without lexicalized reordering.Our results are consistent with those found else-where in the literature.
The strictest setting allow-ing no gaps replicates a result in Chiang (2007, Ta-ble 7), with significantly worse accuracy than allothers.
The most striking result is that the accu-racy of Moses with lexicalized reordering is indis-tinguishable from the accuracy of the full hierar-chical system.
Both improve over non-lexicalizedMoses by about 1.4 BLEU.
The hierarchical emu-lation owes its performance only partially to lex-icalized reordering.
Additional improvement isseen when we add discontiguous phrases.
That theeffect of lexicalized reordering is weaker in the hi-erarchical model is unsurprising, since its parame-terization is much simpler than the one used by theMoses, which includes several specialized featuresfor this purpose.
This suggests that the hierarchicalmodel could be improved through better parame-terization, and still benefit from the translation ofdiscontiguous phrases.Finally, we observe that using more gaps doesnot improve the hierarchical model.4.5 The Tera-Scale ModelThese are interesting scientific findings, but wehave so far failed to show an improvement overthe baseline.
For this, we return to the tera-scalemodel of ?2.
Recall that in this model, we mod-ify the baseline by adding 80 million words ofUN data and using sparse maximum entropy align-ments with a loose phrase extraction heuristic.To avoid conflating rules learned from in-domain newswire and out-of-domain UN data, wetreat each corpus independently.
We sample fromup to 300 source phrase occurrences from each,and compute lexical weighting and the source-to-target phrase translation probabilities separatelyfor both samples.
For the UN corpus, the resultingprobabilities are incorporated into three new fea-tures.
These features receive a value of zero forany rule computed from the newswire data.
Like-wise, the baseline source-to-target phrase transla-tion probability and lexical weighting features re-ceive a value of zero for rules computed from theUN data.We make one more modification to the modelthat is quite easy with pattern matching.
We noticethat it is not always possible to extract a transla-Subphrases Gaps Example BLEU1 0 u 26.31 1 uX,Xu 30.2*1 2 XuX 30.0*2 1 uXv 30.52 2 uXvX,XuXv 30.83 2 uXvXw 30.94 3 uXvXwXy 30.95 4 uXvXwXyXz 30.8Moses without lexicalized reordering 29.4Moses with lexicalized reordering 30.7Table 4: Comparison with Moses and effect of themaximum number of subphrases and gaps.
*De-notes emulation of lexicalized reordering.tion for a source phrase occurrence, even under theloose heuristic.
This is because there may be noconsistently aligned target phrase according to thealignment.
If a phrase occurs frequently but wecan only rarely extract a translation for it, then ourconfidence that it represents a natural unit of trans-lation should diminish.
Conversely, if we usuallyextract a translation, then the phrase is probablya good unit of translation.
We call this propertycoherence.
Conventional offline extraction meth-ods usually ignore coherence.
If a phrase occursmany times but we can only extract a translationfor it a few times, then those translations tend toreceive very high probabilities, even though theymight simply be the result of noisy alignments.We can incorporate the notion of coherence di-rectly into the phrase translation probability.
In thebaseline model, the denominator of this probabil-ity is the sum of the number of rule occurrencescontaining the source phrase, following Koehn etal.
(2003).7 We replace this with the number ofattempted extractions.
This parameterization mayinteract nicely with the loose extraction heuris-tic, reducing the probability of many greedily ex-tracted but otherwise noisy phrases.We compared the baseline with our tera-scalemodel.
Since we had already performed substan-tial experimentation with the NIST 2005 set, wealso included the NIST 2006 task as a new held-out test set.
Results including variants producedby ablating a single modification on the develop-ment set are given in Table 5.We also compared our modified system withan augmented baseline using a 5-gram language7Or the sample size, whichever is less.510NIST 2005 NIST 2006System BLEU loss BLEUTera-Scale Model (all modifications) 32.6 ?
28.4with grow-diag-final-and instead of maximum entropy alignment 32.1 -0.5with tight extraction heuristic instead of loose 31.6 -1.0without UN data 31.6 -1.0without separate UN features 32.2 -0.4with standard p(f |e) instead of coherent p(f |e) 31.7 -0.9Baseline (conventional) 30.7 -1.9Baseline (pattern matching) 30.9 -1.7 27.0Table 5: Results of scaling modifications and ablation experiments.model and rule-based number translation.
The ob-jective of this experiment is to ensure that our im-provements are complementary to better languagemodeling, which often subsumes other improve-ments.
The new baseline achieves a score of 31.9on the NIST 2005 set, making it nearly the sameas the state-of-the-art results reported by Chiang(2007).
Our modifications increase this to 34.5, asubstantial improvement of 2.6 BLEU.5 Related Work and Open ProblemsThere are several other useful approaches to scal-ing translation models.
Zens and Ney (2007) re-move constraints imposed by the size of mainmemory by using an external data structure.
John-son et al (2007) substantially reduce model sizewith a filtering method.
However, neither ofthese approaches addresses the preprocessing bot-tleneck.
To our knowledge, the strand of researchinitiated by Callison-Burch et al (2005) and Zhangand Vogel (2005) and extended here is the first todo so.
Dyer et al (2008) address this bottleneckwith a promising approach based on parallel pro-cessing, showing reductions in real time that arelinear in the number of CPUs.
However, they donot reduce the overall CPU time.
Our techniquesalso benefit from parallel processing, but they re-duce overall CPU time, thus comparing favorablyeven in this scenario.8 Moreover, our methodworks even with limited parallel processing.Although we saw success with this approach,there are some interesting open problems.
As dis-cussed in ?4.2, there are tradeoffs in the form ofslower decoding and increased memory usage.
De-coding speed might be partially addressed usinga mixture of online and offline computation as inZhang and Vogel (2005), but faster algorithms are8All of our reported decoding runs were done in parallel.still needed.
Memory use is important in non-distributed systems since our data structures willcompete with the language model for memory.
Itmay be possible to address this problem with anovel data structure known as a compressed self-index (Navarro and Ma?kinen, 2007), which sup-ports fast pattern matching on a representation thatis close in size to the information-theoretic mini-mum required by the data.Our approach is currently limited by the require-ment for very fast parameter estimation.
As wesaw, this appears to prevent us from computing thetarget-to-source probabilities.
It would also appearto limit our ability to use discriminative trainingmethods, since these tend to be much slower thanthe analytical maximum likelihood estimate.
Dis-criminative methods are desirable for feature-richmodels that we would like to explore with patternmatching.
For example, Chan et al (2007) andCarpuat and Wu (2007) improve translation ac-curacy using discriminatively trained models withcontextual features of source phrases.
Their fea-tures are easy to obtain at runtime using our ap-proach, which finds source phrases in context.However, to make their experiments tractable, theytrained their discriminative models offline only forthe specific phrases of the test set.
Combining dis-criminative learning with our approach is an openproblem.6 ConclusionWe showed that very large translation modelspresent an interesting engineering challenge, andillustrated a solution to this challenge using patternmatching algorithms.
This enables practical, rapidexploration of vastly larger models than those cur-rently in use.
We believe that many other improve-ments are possible when the size of our models is511unconstrained by resource limitations.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProc.
of ACL-COLING, pages 529?536, Jul.Necip Fazil Ayan and Bonnie Dorr.
2006a.
Goingbeyond AER: An extensive analysis of word align-ments and their impact on MT.
In Proc.
of ACL-COLING, pages 9?16, Jul.Necip Fazil Ayan and Bonnie J. Dorr.
2006b.
A max-imum entropy approach to combining word align-ments.
In Proc.
of HLT-NAACL, pages 96?103, Jun.Ralf D. Brown.
2004.
A modified Burrows-Wheelertransform for highly-scalable example-based trans-lation.
In Proc.
of AMTA, number 3265 in LNCS,pages 27?36.
Springer, Sep.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proc.
of ACL, pages 255?262, Jun.Marine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proc.
of ACL, pages 61?72, Jun.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves statis-tical machine translation.
In Proc.
of ACL, pages33?40, Jun.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learnfrom phrase-based MT?
In Proc.
of EMNLP-CoNLL, pages 755?763, Jun.Chris Dyer, Aaron Cordova, Alex Mont, and JimmyLin.
2008.
Fast, easy, and cheap: Construction ofstatistical machine translation models with mapre-duce.
In Proc.
of the Workshop on Statistical Ma-chine Translation, pages 199?207, Jun.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qualityby discarding most of the phrasetable.
In Proc.
ofEMNLP-CoNLL, pages 967?975, Jun.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL, pages 127?133, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of ACL Demo and Poster Sessions, pages 177?180, Jun.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proc.
of MT Sum-mit.Adam Lopez.
2007.
Hierarchical phrase-based transla-tion with suffix arrays.
In Proc.
of EMNLP-CoNLL,pages 976?985, Jun.Adam Lopez.
2008.
Machine Translation by PatternMatching.
Ph.D. thesis, University of Maryland,Mar.Udi Manber and Gene Myers.
1993.
Suffix arrays: Anew method for on-line string searches.
SIAM Jour-nal of Computing, 22(5):935?948.Gonzalo Navarro and Veli Ma?kinen.
2007.
Com-pressed full-text indexes.
ACM Computing Surveys,39(1), Apr.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for ma-chine translation.
In Proc.
of ACL, pages 156?163,Jul.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, Mar.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL, Jul.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of ACL,pages 311?318, Jul.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
Challenging the conventional wisdom instatistical machine translation.
In Proc.
of HLT-NAACL, pages 8?16, Jun.Michel Simard, Nicola Cancedda, Bruno Cavestro,Marc Dymetman, Eric Gaussier, Cyril Goutte, KenjiYamada, Philippe Langlais, and Arne Mauser.
2005.Translating with non-contiguous phrases.
In Proc.
ofHLT-EMNLP, pages 755?762, Oct.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Proc.
of HLT-NAACL: Short Papers, pages 101?104, May.Richard Zens and Hermann Ney.
2007.
Efficientphrase-table representation for machine translationwith applications to online MT and speech transla-tion.
In Proc.
of HLT-NAACL.Ying Zhang and Stephan Vogel.
2005.
An effi-cient phrase-to-phrase alignment model for arbitrar-ily long phrase and large corpora.
In Proc.
of EAMT,May.512
