Proceedings of the SIGDIAL 2013 Conference, pages 384?393,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsContinuously Predicting and Processing Barge-inDuring a Live Spoken Dialogue TaskEthan O.
Selfridge?, Iker Arizmendi?, Peter A.
Heeman?, and Jason D. Williams1?
Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA?AT&T Labs ?
Research, Shannon Laboratory, Florham Park, NJ, USA1Microsoft Research, Redmond, WA, USAselfridg@ohsu.eduAbstractBarge-in enables the user to provide inputduring system speech, facilitating a morenatural and efficient interaction.
Stan-dard methods generally focus on single-stage barge-in detection, applying the di-alogue policy irrespective of the barge-incontext.
Unfortunately, this approach per-forms poorly when used in challengingenvironments.
We propose and evaluatea barge-in processing method that uses aprediction strategy to continuously decidewhether to pause, continue, or resume theprompt.
This model has greater task suc-cess and efficiency than the standard ap-proach when evaluated in a public spokendialogue system.Index Terms: spoken dialogue systems, barge-in1 IntroductionSpoken dialogue systems (SDS) communicatewith users with spoken natural language; the op-timal SDS being effective, efficient, and natural.Allowing input during system speech, known asbarge-in, is one approach that designers use toimprove system performance.
In the ideal usecase, the system detects user speech, switches offthe prompt, and then responds to the user?s utter-ance.
Dialogue efficiency improves, as the sys-tem receives information prior to completing itsprompt, and the interaction becomes more natu-ral, as the system demonstrates more human-liketurn-taking behavior.
However, barge-in poses anumber of new challenges; the system must nowrecognize and process input during its prompt thatmay not be well-formed system directed speech.This is a difficult task and standard barge-in ap-proaches often stop the prompt for input that willnot be understood, subsequently initiating a clari-fication sub-dialogue (?I?m sorry, I didn?t get that.You can say...etc.?).
This non-understood barge-in(NUBI) could be from environmental noise, non-system directed speech, poorly-formed system di-rected speech, legitimate speech recognition diffi-culties (such as acoustic model mismatch), or anycombination thereof.This paper proposes and evaluates a barge-inprocessing method that focuses on handling NU-BIs.
Our Prediction-based Barge-in Response(PBR) model continuously predicts interpretationsuccess by applying adaptive thresholds to incre-mental recognition results.
In our view, predictingwhether the recognition will be understood has farmore utility than detecting whether the barge-inis truly system directed speech as, for many do-mains, we feel only understandable input has morediscourse importance than system speech.
If theinput is predicted to be understood, the prompt ispaused.
If it is predicted or found to be NUBI, theprompt is resumed.
Using this method, the sys-tem may resume speaking before recognition iscomplete and will never initiate a clarifying sub-dialogue in response to a NUBI.
The PBR modelwas implemented in a public Lets Go!
statisticaldialogue system (Raux et al 2005), and we com-pare it with a system using standard barge-in meth-ods.
We find the PBR model has a significantlybetter task success rate and efficiency.Table 1 illustrates the NUBI responses producedby the standard barge-in (Baseline) and PBR mod-els.
After both prompts are paused, the standardmethod initiates a clarifying sub-dialogue whereasPBR resumes the prompt.We first provide background on IncrementalSpeech Recognition and describe the relevant re-lated work on barge-in.
We then detail thePrediction-based Barge-in Response model?s op-eration and motivation before presenting a whole-call and component-wise analysis of the PBR1Work done while at AT&T Labs - Research384Table 1: System response to Non-Understood Barge-In (NUBI)Baseline Ok, sixty one <NUBI> Sorry, say a bus route like twenty eight xPBR Ok, sixty one <NUBI> sixty one c. Where are you leaving from?model.
The paper concludes with a discussion ofour findings and implications for future SDS.2 Background and Related WorkIncremental Speech Recognition: IncrementalSpeech Recognition (ISR) provides the real-timeinformation critical to the PBR model?s continu-ous predictions.
ISR produces partial recognitionresults (?partials?)
until input ceases and the ?fi-nal?
recognition result is produced following somesilence.
As partials have a tendency to be revisedas more audio is processed, stability measures areused to predict whether a given partial hypothe-sis will be present in the final recognition result(McGraw and Gruenstein, 2012; Selfridge et al2011).
Here, we use Lattice-Aware ISR, whichproduces partials after a Voice Activity Detector(VAD) indicates speech and limits them to be acomplete language model specified phrase or haveguaranteed stability (Selfridge et al 2011).Barge-In: Using the standard barge-in model,the system stops the prompt if barge-in is detectedand applies the dialogue logic to the final recogni-tion result.
This approach assumes that the barge-in context should not influence the dialogue pol-icy, and most previous work on barge-in has fo-cused on detection: distinguishing system directedspeech from other environmental sounds.
Cur-rently, these methods are either based on a VAD(e.g.
(Stro?m and Seneff, 2000)), ISR hypothe-ses (Raux, 2008), or some combination (Rose andKim, 2003).
Both approaches can lead to detectionerrors: background speech will trigger the VAD,and partial hypotheses are unreliable (Baumann etal., 2009).
To minimize this, many systems onlyenable barge-in at certain points in the dialogue.One challenge with the standard barge-in modelis that detection errors can initiate a clarifying sub-dialogue to non-system directed input, as it is un-likely that this input will be understood (Raux,2008).
Since this false barge-in, which in mostcases is background speech (e.g.
the television), ishighly indicative of poor recognition performanceoverall, the system?s errant clarifying response canonly further degrade user experience.Strom and Seneff (2000) provide, to our knowl-edge, the only mature work that proposed deviat-ing from the dialogue policy when responding toa barge-in recognition.
Instead of initiating a clar-ifying sub-dialogue, the system produced a filled-pause disfluency (?umm?)
and resumed the promptat the phrase boundary closest to the prompt?s sus-pension point.
However, this model only operatedat the final recognition level (as opposed the incre-mental level) and, unfortunately, they provide noevaluation of their approach.
An explicit compar-ison between the approaches described here andthe PBR model is found in Section 3.5.3 Prediction-based Barge-in ResponseThe PBR model is characterized by three high-level states: State 1 (Speaking Prediction), whosegoal is to pause the prompt if stability scores pre-dict understanding; State 2 (Silent Prediction),whose goal is to resume the prompt if stabilityscores and the incremental recognition rate pre-dict non-understanding; and State 3 (Completion),which operates on the final recognition result, andresumes the prompt unless the recognition is un-derstood and the new speech act will advance thedialogue.
Here, we define ?advancing the dia-logue?
to be any speech act that does not start aclarifying sub-dialogue indicating a NUBI.
Tran-sitions between State 1 and 2 are governed byadaptive thresholds ?
repeated resumptions sug-gest the user is in a noisy environment, so eachresumption increases the threshold required to ad-vance from State 1 to State 2 and decreases thethreshold required to advance from State 2 to State1.
A high-level comparison of the standard modeland our approach is shown in Figure 1; a completePBR state diagram is provided in the Appendix.3.1 State 1: Speaking PredictionIn State 1, Speaking Prediction, the system is bothspeaking and performing ISR.
The system scoreseach partial for stability, predicting the probabilitythat it will remain ?stable?
?
i.e., will not be laterrevised ?
using a logistic regression model (Self-ridge et al 2011).
This model uses a number offeatures related to the recognizer?s generic confi-dence score, the word confusion network, and lat-tice characteristics.
Table 2 shows partial results385Table 2: Background noise and User Speech ISRBackground Noise User UtterancePartial Stab.
Scr.
Partial Stab.
Scr.one 0.134 six 0.396two 0.193 sixty 0.542six 0.127 fifty one 0.428two 0.078 sixty one a 0.491and stability scores for two example inputs: back-ground noise on the left, and the user saying ?sixtyone a?
on the right.State 1 relies on the internal threshold param-eter, T1.
If a partial?s stability score falls belowT1, control remains in State 1 and the partial re-sult is discarded.
If a stability score meets T1, theprompt is paused and control transitions to State 2.T1 is initially set to 0 and is adapted as the dialogueprogresses.
The adaptation procedure is describedbelow in Section 3.4.
If a final recognition resultis received, control transitions directly to State 3.Transitioning from State 1 to State 2 is only al-lowed during the middle 80% of the prompt; oth-erwise only transitions to State 3 are allowed.13.2 State 2: Silent PredictionUpon entering State 2, Silent Prediction, theprompt is paused and a timer is started.
State 2 re-quires continuous evidence (at least every T2 ms)that the ISR is recognizing valid speech and eachtime a partial result that meets T1 is received, thetimer is reset.
If the timer reaches the time thresh-old T2, the prompt is resumed and control returnsto State 1.
T2 is initially set at 1.0 seconds and isadapted as the dialogue progresses.
Final recogni-tion results trigger a transition to State 3.The resumption prompt is constructed using thetemporal position of the VAD specified speechstart to find the percentage of the prompt that wasplayed up to that point.
This percentage is thenreduced by 10% and used to create the resump-tion prompt by finding the word that is closest to,but not beyond, the modified percentage.
Whitespace characters and punctuation are used to deter-mine word boundaries for text-to-speech prompts,whereas automatically generated word-alignmentsare used for pre-recorded prompts.1We hypothesized that people will rarely respond to thecurrent prompt during the first 10% of prompt time as over-laps at the beginning of utterances are commonly initiativeconflicts (Yang and Heeman, 2010).
Users may produceearly-onset utterances during the last 10% that should notstop the prompt as it is not an ?intentional?
barge-in.Figure 1: The Standard Barge-in and PBR Models3.3 State 3: CompletionState 3, Completion, is entered when a final recog-nition result is received and determines whetherthe current dialogue policy will advance the dia-logue or not.
Here, the PBR model relies on theability of the dialogue manager (DM) to produce aspeculative action without transitioning to the nextdialogue state.
If the new action will not advancethe dialogue, it is discarded and the recognitionis NUBI.
However, if it will advance the dialoguethen it is classified as an Understood Barge-In(UBI).
In the NUBI case, the system either contin-ues speaking or resumes the current prompt (tran-sitioning to State 1).
In the UBI case, the systeminitiates the new speech act after playing a shortreaction sound and the DM transitions to the nextdialogue state.
This reaction sound precedes allspeech acts outside the barge-in context but is notused for resumption or timeout prompts.
Note thatby depending solely on the new speech act, ourmodel does not require access to the DM?s internalunderstanding or confidence scoring components.3.4 Threshold adjustmentsStates 1 and 2 contain parameters T1 and T2 thatare adapted to the user?s environment.
T1 is thestability threshold used in State 1 and State 2 thatcontrols how stable an utterance must be beforethe prompt should be paused.
In quiet environ-ments ?
where only the user?s speech producespartial results ?
a low threshold is desirable asit enables near-immediate pauses in the prompt.Conversely, noisy environments yield many spu-rious partials that (in general) have much lowerstability scores, so a higher threshold is advan-tageous.
T2 is the timing threshold used to re-sume the prompt during recognition in State 2.
Inquiet environments, a higher threshold reduces thechance that the system will resume its prompt dur-ing a well-formed user speech.
In noisy environ-386Figure 2: Example dialogue fragment of PBR Modelments, a lower threshold allows the system to re-sume quickly as the NUBI likelihood is greater.Both T1 and T2 are dependent on the number ofsystem resumptions, as we view the action of re-suming the prompt as an indication that the thresh-old is not correct.
With every resumption, the pa-rameter R is incremented by 1 and, to account forchanging environments, R is decremented by 0.2for every full prompt that is not paused until itreaches 0.
UsingR, T1 is computed by T1 = 0.17?R, and T2 by T2 = argmax(0.1, 1?
(0.1 ?R)).23.5 Method DiscussionThe motivation behind the PBR model is both the-oretical and practical.
According to Selfridge andHeeman (2010), turn-taking is best viewed as acollaborative process where the turn assignmentshould be determined by the importance of theutterance.
During barge-in, the system is speak-ing and so should only yield the turn if the user?sspeech is more important than its own.
For manydomains, we view non-understood input as lessimportant than the system?s prompt and so, in thiscase, the system should not release the turn bystopping the prompt and initiating a clarifying sub-dialogue.
On the practical side, there is a highlikelihood that non-advancing input is not systemdirected, to which the system should neither con-sume, in terms of belief state updating, nor re-spond to, in terms of asking for clarification.
Inthe rare case of non-understood system directedspeech, the user can easily repeat their utterance.Here, we note that in the event that the user isbackchanneling, the PBR model will behave cor-rectly and not release the turn.The PBR approach differs from standard barge-in approaches in several respects.
First, standardbarge-in stops the prompt (i.e., transitions fromState 1 to State 2) if either the VAD or the partialhypothesis suggests that there is speech; our ap-proach?
using acoustic, language model, and lat-tice features ?
predicts whether the input is likelyto contain an interpretable recognition result.
Sec-2The threshold update values were determined empiri-cally by the authors.ond, standard barge-in uses a static threshold; ourapproach uses dynamic thresholds that adapt tothe user?s acoustic environment.
Parameter adjust-ments are straightforward since our method auto-matically classifies each barge-in as NUBI or UBI.In practice, the prompt will be paused incorrectlyonly a few times in a noisy environment, afterwhich the adaptive thresholds will prevent incor-rect pauses at the expense of being less responsiveto true user speech.
If the noise level decreases,the thresholds will become more sensitive again,enabling swifter responses.
Finally, with the ex-ception of Strom and Seneff, standard approachesalways discard the prompt; our approach can re-sume the prompt if recognition is not understoodor is proceeding poorly, enabling the system toresume speaking before recognition is complete.Moreover, resumption yields a natural user expe-rience as it often creates a repetition disfluency(?Ok, sixty - sixty one c?
), which are rarely no-ticed by the listener (Martin and Strange, 1968).An example dialogue fragment is shown in Fig-ure 2, with the state transitions shown above.
Notethe transition from State 2 to State 1, which is thesystem resuming speech during recognition.
Thisrecognition stream, produced by non-system di-rected user speech, does not end until the user says?repeat?
for the last time.4 Evaluation ResultsThe PBR model was evaluated during the SpokenDialog Challenge 2012-2013 in a live Lets Go!bus information task.
In this task, the public canaccess bus schedule information during off hoursin Pittsburgh, PA via a telephonic interaction witha dialogue system (Raux et al 2005).
The taskcan be divided into five sub-tasks: route, origin,destination, date/time, and bus schedules.
The lastsub-task, bus schedules, provides information tothe user whereas the first four gather information.We entered two systems using the same POMDP-based DM (Williams, 2012).
The first system, the?Baseline?, used the standard barge-in model withVAD barge-in detection and barge-in disabled in387Figure 3: Estimated success rate for the PBR and Baseline systems.
Stars indicate p<0.018 with ?2 test.a small number of dialogue states that appearedproblematic during initial testing.
The second sys-tem used the PBR model with an Incremental In-teraction Manager (Selfridge et al 2012) to pro-duce speculative actions in State 3.
The pub-lic called both systems during the final weeks of2011 and the start of 2012.
The DM applied a lo-gistic regression based confidence measure to de-termine whether the recognition was understood.Both systems used the AT&T WATSONSM speechrecognizer (Goffin et al 2005) with the samesub-task specific rule-based language models andstandard echo cancellation techniques.
The beamwidth was set to maximize accuracy while stillrunning faster than real-time.
The PBR systemused a WATSON modification to output lattice-aware partial results.Call and barge-in statistics are shown in Table3.
Here, we define (potential) barge-in (some-what imprecisely) as a full recognition that atsome point overlaps with the system prompt, asdetermined by the call logs.
We show the callswith barge-in before the bus schedule sub-task wasreached (BI-BS) and the calls with barge-in duringany point of the call (BI All).
Since the Baselinesystem only enabled barge-in at specific points inthe dialogue, it has fewer instances of barge-in(Total Barge-In) and fewer barge-in calls.
Regret-fully, due to logging issues with the PBR system,recognition specific metrics such as Word ErrorRate and true/false barge-in rates are unavailable.4.1 Estimated Success RateWe begin by comparing the success rate andefficiency between the Baseline and PBR sys-Table 3: Baseline and PBR call/barge-in statistics.Baseline PBRTotal Calls 1027 892BI-BS 228 (23%) 345 (39%)BI All 281 (27%) 483 (54%)Total Barge-In 829 1388tems.
Since task success can be quite difficult tomeasure, we use four increasingly stringent tasksuccess definitions: Bus Times Reached (BTR),where success is achieved if the call reaches thebus schedule sub-task; List Navigation (List Nav.
),where success is achieved if the user says ?
?next?,?previous?, or ?repeat?
?
the intuition being thatif the user attempted to navigate the bus sched-ule sub-task they were somewhat satisfied withthe system?s performance so far; and ImmediateExit (BTR2Ex and ListNav2Ex), which furtherconstrains both of the previous definitions to onlycalls that finish directly after the initial visit to thebus times sub-task.
Success rate for the defini-tions were automatically computed (not manuallylabeled).
Figure 3 shows the success rate of thePBR and Baseline systems for all four definitionsof success.
It shows, from left to right, Barge-In,No Barge-In (NBI), and All calls.
Here we restrictbarge-in calls to those where barge-in occurredprior to the bus schedule task being reached.For the calls with barge-in, a ?2 test finds sig-nificant differences between the PBR and Base-line for all four task success definitions.
However,we also found significant differences in the NBIcalls.
This was surprising since, when barge-inis not triggered, both systems are ostensibly thesame.
We speculate this could be due to the Base-line?s barge-in enabling strategy: an environmentthat triggers barge-in in the Baseline would alwaystrigger barge-in in the PBR model, whereas theconverse is not true as the Baseline only enabledbarge-in in some of the states.
This means thatthere is a potential mismatch when separating thecalls based on barge-in, and so the fairest compar-ison is using All the calls.
This is shown on the farright of Figure 3.
We find that, while the effect isnot as large, there are significant differences in thesuccess rate for the PBR model for the most andleast stringent success definition, and very strongtrends for the middle two definitions (p < 0.07 forBTR2Ex and p < 0.054 for List Nav.).
Taken asa whole, we feel this offers compelling evidence388Figure 4: Seconds from beginning of dialogue toreaching the Bus Schedule Information sub-taskthat the PBR method is more effective: i.e.
yieldshigher task completion.Next, we turn our attention to task efficiency.For this, we report the amount of clock time fromthe beginning of the call to when the Bus Schedulesub-task was reached.
Calls that do not reach thissub-task are obviously excluded, and PBR timesare adjusted for the reaction sound (explained inSection 3.3).
Task efficiency is reported by cu-mulative percentage in Figure 4.
We find that,while the NBI call times are nearly identical forboth systems, the PBR barge-in calls are muchfaster than the Baseline calls.
Here, we do notfeel the previously described mismatch is partic-ularly problematic as all the calls reached the goalstate and the NBI are nearly identical.
In fact, asmore NUBI should actually reduce efficiency, thepotential mismatch only strengthens the result.Taken together, these results provide substantialevidence that the PBR model is more effective andmore efficient than the Baseline.
In order to ex-plain PBR?s performance, we explore the effect ofprediction and resumption in isolation.4.2 State 1: Speaking PredictionState 1 is responsible for pausing the prompt, thegoal being to pause the prompt for UBI input andnot to pause the prompt for NUBI input.
Theprompt is paused if a partial?s stability score meetsor exceeds the T1 threshold.
We evaluate the ef-ficacy of State 1 and T1 by analyzing the statis-tics of NUBI/UBI input and Paused/Not Paused(hereafter Continued) prompts.
Since resumingthe prompt during recognition affects the recog-nition outcome, we restrict our analysis to recog-nitions that do not transition from State 2 backto State 1.
For comparison we show the overallUBI/NUBI percentages for the Baseline and PBRsystems.
This represents the recognition distri-Table 4: Evaluation of T1, off-line PBR, and Base-line VAD.
For T1 we respectively (?-?
split) showthe UBI/NUBI % that are Paused/Continued, thePaused/Continued % that are UBI/NUBI, and thepercentage over all recognitionsT1 (%) VAD (%)Paused Continued PBR BLUBI 72-40-26 28-29-10 36 54NUBI 61-60-39 39-71-25 64 46bution for the live Baseline VAD detection andoff-line speculation for the PBR model.
RecallPBR does have VAD activation preceding partialresults and so the off-line PBR VAD shows howthe model would have behaved if it only used theVAD for detection, as the Baseline does.Table 4 provides a number of percentages, withthree micro-columns separated by dashes (?-?)
forT1.
The first micro-column shows the percent-age of UBI/NUBI that either Paused or Contin-ued the prompt (sums to 100 horizontally).
Thesecond micro-column shows the percentage ofPaused/Continued that are UBI/NUBI (sums to100 vertically).
The third micro-column showsthe percentage of each combination (e.g.
UBI andPaused) over all the barge-in recognitions.
TheVAD columns show the percentage of UBI/NUBIthat (would) pause the prompt.We first look at UBI/NUBI percentage that arePaused/Continued (first micro-column): We findthat 72% of UBI are paused and 28% are Contin-ued versus 61% of NUBI that are Paused with 39%Continued.
We now look at the Paused/Continuedpercentage that are UBI/NUBI (second micro-column): We find that 40% of Paused are UBIand 60% are NUBI, whereas 29% of Continuedare UBI and 71% are NUBI.
So, while T1 sus-pends the prompt for the majority of NUBI (notdesirable, though expected since T1 starts at 0),it has high precision when continuing the prompt.This reduces the number of times that the promptis paused erroneously for NUBI while minimizingincorrect (UBI) continues.
This is clearly shownby considering all of the recognitions (third micro-column).
We find that PBR erroneously pausedthe prompt for 39% of recognitions, as opposed to64% for the off-line PBR and 46% for the Base-line.
This came at the cost of reducing the numberof correct (UBI) pauses to 26% from 36% (off-linePBR) and 54% (Baseline VAD).The results show that the T1 threshold had389Figure 5: Secs from Speech Start to Final Resultmodest success at discriminating UBI and NUBI;while continuing the prompt had quite a highprecision for NUBI, the recall was substantiallylower.
We note that, since erroneous pauses leadto resumptions and erroneous continues still leadto a new speech act, there is minimal cost to theseerrors.
Furthermore, in our view, reducing the per-centage of recognitions that pause and resume theprompt is more critical as these needlessly disruptthe prompt.
In this, T1 is clearly effective, reduc-ing the percentage from 64% to 39%.4.3 State 2: Silent PredictionState 2 governs whether the prompt will remainpaused or be resumed during incremental recogni-tion.
This decision depends on the time parameterT2, which should trigger resumptions for NUBIs.Since the act of resuming the prompt during recog-nition changes the outcome of the recognition, itis impossible to evaluate how well T2 discrimi-nated recognition results.
However, we can evalu-ate the effect of that resumption by comparing UBIpercentages between the PBR and Baseline sys-tems.
We first present evidence that T2 is most ac-tive during longer recognitions, and then show thatlonger Baseline recognitions have a lower UBIpercentage than longer PBR recognitions specif-ically because of T2 resumptions.
?Recognitions?refer to speech recognition results, with ?longer?or ?shorter?
referring to the clock time betweenspeech detection and the final recognition result.We first report the PBR and Baseline responseand recognition time.
We separate the PBR barge-in recognitions into two groups: State 2?State 3,where the system never transitions from State 2to State 1, and State 2?State 1, where the sys-tem resumes the prompt during recognition, tran-sitioning from State 2 to State 1.
The cumulativepercentages of the time from speech detection tofinal recognition are shown in Figure 5.
We findthat the State 2?State 3 recognitions are far fasterFigure 6: UBI % by minimum recognition timethan the Baseline recognitions, which in turn arefar faster than the State 2?State 1 recognitions.The difference between PBR and Baseline recog-nitions implies that T2 has greater activation dur-ing longer recognitions.
Given this, the overallbarge-in response time for PBR should be fasterthan the Baseline (as the PBR system is resum-ing where the Baseline is silent).
Indeed this isthe case: the PBR system?s overall mean/medianresponse time is 1.58/1.53 seconds whereas Base-line has a mean/median response time of 2.61/1.8seconds.The goal of T2 is for the system to resume whenrecognition is proceeding poorly, and we haveshown that it is primarily being activated duringlonger recognitions.
If T2 is functioning properly,recognition length should be inversely related torecognition performance, and longer recognitionsshould be less likely to be understood.
Further-more, if T2 resumption improves the user?s expe-rience then longer PBR recognitions should per-form better than Baseline recognitions of compa-rable length.
Figure 6 presents the UBI percent-age by the minimum time for recognitions thatreach State 2.
We find that, when all recogni-tions are accounted for (0 second minimum), theBaseline has a higher rate of UBI.
However, asrecognition time increases the Baseline UBI per-centage decreases (suggesting successful T2 func-tioning) whereas the PBR UBI percentage actu-ally increases.
Since longer PBR recognitions aredominated by T2 resumptions, we speculate thisimprovement is driven by users repeating or initi-ating new speech that leads to understanding suc-cess, as the PBR system is responding where theBaseline system is silent.4.4 ResumptionThe PBR model relies on resumption to recoverfrom poor recognitions, either produced in State 2or State 3.
Instead of a resumption, the Baseline390Figure 7: Sub-Task Abandonment Rate.
NUBI isdifferent at p < 0.003system initiates a clarifying sub-dialogue when abarge-in recognition is not understood.
We com-pare these two behaviors using the call abandon-ment rate ?
the user hangs-up ?
of sub-taskswith and without NUBI.
Here, we exclude the BusSchedule sub-task as it is the goal state.Figure 7 shows the call abandonment rate forsub-tasks that either have or do not have NUBI.We find that there is a significant difference inabandoned calls for NUBI sub-tasks between thetwo systems (33% vs 48%, p < 0.003 using a ?2test), but that there is no difference for the callsthat do not have NUBI (7.6% vs 8.4%).
This re-sult shows that prompt resumption is viewed farmore favorably by users than initiating a clarify-ing sub-dialogue.5 Discussion and ConclusionThe above results offer strong evidence that thePBR model increases task success and efficiency,and we found that all three states contribute tothe improved performance by creating a more ro-bust, responsive, and natural interaction.
T1 pre-diction in State 1 reduced the number of spuriousprompt suspensions, T2 prediction in State 2 led toimproved understanding performance, and promptresumption (States 2 and 3) reduced the number ofabandoned calls.An important feature of the Prediction-basedBarge-in Response model is that, while it lever-ages incremental speech processing for barge-inprocessing, it does not require an incremental di-alogue manager to drive its behavior.
Since themodel is also domain independent and does notrequire access to internal dialogue manager com-ponents, it can easily be incorporated into any ex-isting dialogue system.
However, one limitation ofthe current model is that the prediction thresholdsare hand-crafted.
We also believe that substan-tial improvements can be made by explicitly at-tempting to predict eventual understanding insteadof using the stability score and partial productionrate as a proxy.
Furthermore, the PBR model doesnot distinguish between the causes of the non-understanding, specifically whether the input con-tained in-domain user speech, out-of-domain userspeech, or background noise.
This case is specifi-cally applicable in domains where system and userspeech are in the same channel, such as interact-ing via speaker phone.
In this context, the systemshould be able to initiate a clarifying sub-dialogueand release the turn, as the system must be moresensitive to the shared acoustic environment andso its current prompt may be less important thanthe user?s non-understood utterance.The results challenge a potential assumption re-garding barge-in: that barge-in indicates greateruser pro-activity and engagement with the task.One of the striking findings was that dialogueswith barge-in are slower and less successful thandialogues without barge-in.
This suggests that,for current systems, dialogues with barge-in aremore indicative of environmental difficulty thanuser pro-activity.
The superior performance ofthe PBR model, which is explicitly resistant tonon-system directed speech, implies that domi-nant barge-in models will have increasingly lim-ited utility as spoken dialogue systems becomemore prevalent and are used in increasingly dif-ficult environments.
Furthermore, within the con-text of overall dialogue systems, the PBR model?sperformance emphasizes the importance of contin-uous processing for future systems.This paper has proposed and evaluated thePrediction-based Barge-in Response model.
Thismodel?s behavior is driven by continuously pre-dicting whether a barge-in recognition will be un-derstood successfully, and combines incrementalspeech processing techniques with a prompt re-sumption procedure.
Using a live dialogue taskwith real users, we evaluated this model againstthe standard barge-in model and found that it ledto improved performance in both task success andefficiency.AcknowledgmentsMany thanks to Vincent Goffin for help with thiswork, and to the anonymous reviewers for their in-sightful comments and critique.
We acknowledgefunding from the NSF under grant IIS-0713698.391ReferencesT.
Baumann, M. Atterer, and D. Schlangen.
2009.
As-sessing and improving the performance of speechrecognition for incremental systems.
In Proc.NAACL: HLT, pages 380?388.
Association for Com-putational Linguistics.V.
Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,A.
Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,and M. Saraclar.
2005.
The AT&T WATSONspeech recognizer.
In Proceedings of ICASSP, pages1033?1036.James G Martin and Winifred Strange.
1968.
The per-ception of hesitation in spontaneous speech.
Percep-tion & Psychophysics, 3(6):427?438.Ian McGraw and Alexander Gruenstein.
2012.
Es-timating word-stability during incremental speechrecognition.
In in Proc.
of Interspeech 2012.A.
Raux, B. Langner, D. Bohus, A.W.
Black, andM.
Eskenazi.
2005.
Lets go public!
taking a spo-ken dialog system to the real world.
In in Proc.
ofInterspeech 2005.A.
Raux.
2008.
Flexible Turn-Taking for Spoken Dia-log Systems.
Ph.D. thesis, CMU.Richard C Rose and Hong Kook Kim.
2003.
Ahybrid barge-in procedure for more reliable turn-taking in human-machine dialog systems.
In Auto-matic Speech Recognition and Understanding, 2003.ASRU?03.
2003 IEEE Workshop on, pages 198?203.IEEE.E.O.
Selfridge and P.A.
Heeman.
2010.
Importance-Driven Turn-Bidding for spoken dialogue systems.In Proc.
of ACL 2010, pages 177?185.
Associationfor Computational Linguistics.E.O.
Selfridge, I. Arizmendi, P.A.
Heeman, and J.D.Williams.
2011.
Stability and accuracy in incre-mental speech recognition.
In Proceedings of theSIGdial 2011.E.O.
Selfridge, I. Arizmendi, P.A.
Heeman, and J.D.Williams.
2012.
Integrating incremental speechrecognition and pomdp-based dialogue systems.
InProceedings of the SIGdial 2012.Nikko Stro?m and Stephanie Seneff.
2000.
Intelligentbarge-in in conversational systems.
Procedings ofICSLP.Jason D Williams.
2012.
A critical analysis of two sta-tistical spoken dialog systems in public use.
In Spo-ken Language Technology Workshop (SLT), 2012IEEE, pages 55?60.
IEEE.Fan Yang and Peter A. Heeman.
2010.
Initiative con-flicts in task-oriented dialogue?.
Computer SpeechLanguage, 24(2):175 ?
189.392A AppendixThis diagram represents the possible operating positions the Prediction-based Barge-in Responsemodel can be in.
If the prompt is complete, the PBR model applies the dialogue policy to the finalrecognition result and initiates the on-policy speech act.
If the prompt was finished without being pausedit decrements R. In the latter case (barge-in), it operates using the three states as described in Section 2.When a partial is recognized the Stability Score is computed and compared to the T1 threshold parame-ter.
If the score is below T1 the partial is discarded.
Otherwise, if the model is in State 1 (the prompt ison) the prompt is paused, a timer is started, and control transitions to State 2.
If the model is in State 2the timer is restarted.
After transitioning to State 2, control only returns to State 1 if the timer exceedsT2.
At this time, the prompt is resumed and the resumption parameter R is incremented.
Control im-mediately transitions to State 3 if a final recognition result is received.
The result is evaluated by thedialogue manager, and the new speech act is returned.
If the speech act indicates the recognition was notunderstood successfully, the system either resumes (if in State 1) or continues (if in State 2).
In the caseof resumption, R is incremented.
If the new speech act indicates understanding success, the new speechis immediately produced.393
