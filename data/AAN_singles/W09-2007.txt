Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 47?54,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsQuantifying Constructional Productivity with Unseen Slot MembersAmir ZeldesInstitut f?r deutsche Sprache und LinguistikHumboldt-Universit?t zu BerlinUnter den Linden 6, 10099 Berlin, Germanyamir.zeldes@rz.hu-berlin.deAbstractThis paper is concerned with the possibility ofquantifying and comparing the productivity ofsimilar yet distinct syntactic constructions,predicting the likelihood of encountering un-seen lexemes in their unfilled slots.
Two ex-amples are explored: variants of comparativecorrelative constructions (CCs, e.g.
the fasterthe better), which are potentially very produc-tive but in practice lexically restricted; andambiguously attached prepositional phraseswith the preposition with, which can host bothlarge and restricted inventories of argumentsunder different conditions.
It will be shownthat different slots in different constructionsare not equally likely to be occupied produc-tively by unseen lexemes, and suggested thatin some cases this can help disambiguate theunderlying syntactic and semantic structure.1 IntroductionSome syntactic constructions1 are more productivethan others.
Innovative coinages like the CC: Thebubblier the Mac-ier (i.e.
the more bubbly a pro-gram looks, the more it feels at home on a Macin-tosh computer) are possible, but arguably moresurprising and marked than: I have a bubblier op-erating system with a Mac-ier look in their respec-tive construction, despite the same novel lexemes.The aim of this paper is to measure differences inthe productivity of slots in such partially-filledconstructions and also to find out if this productiv-ity can be used to disambiguate constructions.1 I use the term ?construction?
in a construction grammar sensefollowing Goldberg (1995, 2006) to mean mentally storedhierarchically organized form-meaning pairs with empty, par-tially-filled or fully specified lexical material.
In this sense,both comparative adjectives and the pattern The [COMP] the[COMP] are constructions, and the productivity of such pat-terns is the quantity being examined here.As one of the defining properties of language,productivity has received much attention in debatesabout the nature of derivational processes, thestructure of the mental lexicon and the interpreta-tion of key terms such as compositionality, gram-maticality judgments or well-formedness.
Howeverin computational linguistics it is probably fair tosay that it can be regarded most of all as a problem.Familiar items present in training data can be listedin lexical resources, the probabilities of their dif-ferent realizations can be estimated from corpusfrequency distributions etc.
Thus using lexical in-formation (statistically extracted or handcraftedresources) is the most successful strategy in resolv-ing syntactic ambiguities such as PP-attachment(Hindle and Rooth, 1993; Ratnaparkhi, 1998; Stet-ina and Nagao, 1997; Pantel and Lin, 2000; Kawa-hara and Kurohashi, 2005), basing decisions onprevious cases with identical lexemes or additionalinformation about those lexemes.
Yet because ofproductivity, even very large training data willnever cover examples for all inputs being analyzed.In morphological theory (and correspondingcomputational linguistic practice), the situation hasbeen somewhat different: a much larger part of theword formations encountered in data can be listedin a lexicon, with neologisms being the exception,whereas in syntax most sentences are novel, withrecurring combinations being the exception.2 Thefocus in morphology has therefore often been onwhich word formation processes are productiveand to what extent, with the computational coun-terpart being whether or not corresponding rulesshould be built into a morphological analyzer.
Syn-tacticians, conversely, may ask which apparentlyregular constructions are actually lexicalized orhave at least partly non-compositional properties(e.g.
collocations, see Choueka, 1988, Evert, 2005,2 Compounding represents an exception to this generalization,standing, at least for some languages, between syntax andword formation and often generating an unusually largeamount of items unlisted in lexica (cf.
Bauer, 2001:36-7).472009; multiword expressions, Sag et al, 2002;lexical bundles, Salem, 1987, Altenberg and Eeg-Olofsson, 1990, Biber et al, 1999, 2004).In morphology, the realization that productivityis a matter of degree, rather than a binary trait ofword formation processes (see e.g.
Bauer,2001:125-162), has lead to the exploration of quan-titative measures to assess and compare differentaspects of the fertility of various patterns (esp.
thework of Baayen, 2001, 2009).
Yet syntactic appli-cations of these measures have only very recentlybeen proposed, dealing with one slot of a patternmuch like the stem operated on by a morphologicalprocess (cf.
Bar?dal, 2006; Kiss, 2007).In this paper I will examine the application ofmeasures based on Baayen?s work on morphologyto different variants of syntactic constructions withmore or less variable slots.
The goal will be toshow that different constructions have inherentlydifferent productivity rates, i.e.
they are more orless liable to produce new members in their freeslots.
If this view is accepted, it may have conse-quences both theoretically (novelty in certain posi-tions will be more surprising or marked) andpractically, e.g.
for parsing ambiguous structureswith novel arguments, since one parse may imply aconstruction more apt to novelty than another.The remainder of this article is structured as fol-lows: the next section introduces concepts underly-ing morphological productivity and related corpus-based measures following Baayen (2009).
The fol-lowing two sections adapt and apply these meas-ures to different types of CCs (such as the fasterthe better) and NP/VP-attached PPs, respectively,using the BNC3 as a database.
The final sectiondiscusses the results of these studies and their im-plications for the study of syntactic productivity.2 Morphological Productivity MeasuresProductivity has probably received more attentionas a topic in morphology than in syntax, if for noother reason than that novel words are compara-tively rare and draw attention, whereas novelphrases or sentences are ubiquitous.
The exactdefinition of a novel word or ?neologism?
is how-ever less than straightforward.
For the present pur-pose we may use Bauer?s (2001:97-98) workingdefinition as a starting point:3 The British National Corpus (http://www.natcorp.ox.ac.uk/),with over 100 million tokens of British English.
[Productivity] is a feature of morphological proc-esses which allow for new coinages, [?]
coiningmust be repetitive in the speech community [?
]Various factors appear to aid productivity: typefrequency of appropriate bases, phonological andsemantic transparency, naturalness, etc., but theseare aids to productivity, not productivity itself.For Bauer, productivity is defined for a morpho-logical process, which is ideally frequently andconsistently found and coins ideally transparentnovel forms.
The word ?coining?
in this contextimplies that speakers use the process to constructthe transparent novel forms in question, which inturn means the process has a regular output.
Yetnovelty, transparency and regularity are difficult tojudge intuitively, and the definitions of ?new?
vs.?existing?
words cannot be judged reliably for anyone speaker, nor with any adequacy for a speakercommunity (cf.
Bauer, 2001:34-35).This problem has led researchers to turn to cor-pus data as a sort of ?objective?
model of languageexperience, in which the output of a process can besearched for, categorized and tagged for evalua-tion.
Baayen (e.g.
2001, 2009) proposes three cor-pus-based measures for the productivity of wordformation processes.
The first measure, which heterms extent of use, is written V(C,N) and is simplythe proportion of types produced by a process C ina corpus of size N, e.g.
the count of different nounsin -ness out of all the types in N. According to thismeasure, -ness would have a much higher realizedproductivity than the -th in warmth since it is foundin many more words.
However, this measure indis-criminately deals with all existing material ?
allwords that have already been generated ?
andhence it cannot assess how likely it is that novelwords will be created using a certain process.Baayen?s other two measures address differentaspects of this problem and rely on the use of ha-pax legomena, words appearing only once in a cor-pus.
The intuitive idea behind looking at suchwords is that productively created items are one-off unique occurrences, and therefore they mustform a subset of the hapax legomena in a corpus.Baayen uses V(1,C,N), the number of types fromcategory C occurring once in a corpus of N wordsand V(1,N), the number of all types occurring oncein a corpus of N words.
The second measure,termed hapax-conditioned degree of productivity issaid to measure expanding productivity, the rate at48which a process is currently creating neologisms.
Itis computed as V(1,C,N)/V(1,N), the proportion ofhapax legomena from the examined category Cwithin the hapax legomena from all categories inthe corpus.
Intuitively, if the amount of hapax le-gomena could be replaced by ?true?
neologismsonly, this would be the relative contribution of aprocess to productivity in the corpus, which couldthen be compared between different processes4.The third measure, category-conditioned degreeof productivity measures the potential productivityof a process, meaning how likely it is to producenew members, or how saturated a process is.
Thismeasure is the proportion of hapax legomena fromcategory C divided by N(C), the total token countfrom this category:  V(1,C,N)/N(C).
It intuitivelyrepresents the probability of the next item fromcategory C, found in further corpus data of thesame type, to be a hapax legomenon.Baayen?s measures (hence p1, p2 and p3 respec-tively) are appealing since they are rigorously de-fined, easily extractable from a corpus (providedthe process can be identified reliably in the data)and offer an essential reduction of the corpus widebehavior of a process to a number between 1 and 0,that is, an item producing no hapax legomenawould score 0 on p2 and p3, and an item with100% hapax legomena would score 1 on p3, evenif it is overall rather insignificant for productivityin the corpus as a whole (as reflected in a lowscore for p2).
The measure p3 is the most impor-tant one in the present context, since it allows us toreason conversely that, given that an item is noveland could belong to one of two processes, it ismore likely to have come from whichever processis more productive, i.e.
has a higher p3 score.Indeed the assumptions made in these measuresdo not necessarily fit syntactic productivity at afirst glance: that the process in question has aclearly defined form (e.g.
a suffix such as -ness)that it accommodates one variable slot (the stem,e.g.
good- in goodness), and that each differentstem forms a distinct type.
Applying these meas-ures to syntactic constructions requires conceptual4 This statement must be restricted somewhat: in items show-ing multiple processes, e.g.
bullishness, the processes associ-ated with the suffixes -ish and -ness are not statisticallyindependent, creating a difficulty in using such cases for thecomparison of these two processes (see Baayen, 2009).
Insyntax the extent of this problem is unclear, since even occur-rences of NPs and VPs are not independent of each other.and mathematical adaptation, which will be dis-cussed in the next section using the example ofcomparative correlative constructions.3 Measuring Productivity in CCsComparative correlatives are a complex yet typo-logically well attested form of codependent clausesexpressing a corresponding monotonous positiveor negative change in degree between two proper-ties (see den Dikken, 2005 for a cross-linguisticoverview).
For example, in the faster we go, thesooner we?ll get there, speed is monotonously cor-related with time of arrival.
A main reason for syn-tactic interest in this type of sentence is a proposed?mismatch?
(see McCawley, 1988, Culicover andJackendoff, 1999) between its syntax, which ap-pears to include two identically constructed para-tactic clauses, and its semantics, which implypossible hypotaxis of the first clause as a sort of?conditional?
(if and in so much as we go fast?
).Two other noteworthy features of this construc-tion in use (the following examples are from theBNC) are the frequent lack of a verb (the largerthe leaf the better quality the tea) and even of asubject noun (the sooner the better) 5 and a ten-dency for the (at least partial) lexicalization of cer-tain items.
The verbless variant often houses these,e.g.
the more the merrier, but also with verbs, e.g.the bigger they come the harder they fall.
A con-text-free grammar might describe a simplifiedvariant of such clauses in the following terms:Scc > the COMP (NP (VP))S > Scc Sccwhere Scc is one of the comparative correlativeclauses, COMP represents either English compara-tive allomorph (in -er like bigger or analytic withmore or less in more/less important), and NP andVP are optional subjects and corresponding predi-cates for each clause.6However like many CFG rules, these rules maybe too general, since it is clearly the case that not5 The latter form has been analyzed as a case of ellipsis of thecopula be (Culicover and Jackendoff, 1999:554; similarly forGerman: Zifonun et al, 1997:2338).
It is my position that thisis not the case, as the bare construction has distinct semanticproperties as well as different productive behavior, see below.6 These rules should be understood as agnostic with respect tothe parataxis/hypotaxis question mentioned above.
The paren-theses mean NP may appear without VP but not vice versa.49all comparatives, nouns and verbs fit in this con-struction, if only because of semantic limitations,i.e.
they must be plausibly capable of forming apair of monotonously correlated properties.
Corpusdata shows that comparatives in CC clauses selectquite different lexemes than comparatives at large,that the first and second slots (hence cc1 and cc2)have different preferences, and that the presence orabsence of a VP and possibly a subject NP alsointeract with these choices.
Table 1 shows com-paratives in the BNC sorted by frequency in gen-eral, along with their frequencies in cc1 and cc2.Some frequent comparatives do not or hardly ap-pear in CCs given their frequency7 while othersprefer a certain slot exclusively (e.g.
more likely incc2) or substantially (e.g.
higher in cc1).
Columns?1 and ?2 show bare comparatives (no subject orverb) in cc1 or 2 and the next two columns showsubsets of bare cc1 or 2 given that the other clauseis also bare.
The last columns show CCs with onlyNPs and no verb, either in one clause or both.
Inbare CCs we find that better selects cc2 exclu-sively, in fact making up some 88% of cc2s in thisconstruction (the COMP the better) in the BNC.7 Occurrences of items which cannot serve attributively, suchas more with no adjective and sooner, have been excluded,since they are not comparable to the other items.
Most occur-rences of the most frequent item, further, should arguably beexcluded too, since it is mostly used as a lexicalized adverband not a canonical comparative.
However comparative usageis also well-attested, e.g.
: he was going much further than that.A look at the list of lexemes typical to cc1 vs.cc2 shows that cc1 tends to express a dependentvariable with spatiotemporal semantics (higher,older, longer), whereas cc2 typically shows an in-dependent evaluative (better, more likely), thoughmany common lexemes appear in both.8Although the results imply varying degrees ofpreference and lexicalization in different construc-tions, they do not yet tell us whether or not, or bet-ter how likely, we can expect to see new lexemesin each slot.
This can be assessed using Baayen?smeasures, by treating each construction as a mor-phological process and the comparative slot as thelexical base forming the type (see Kiss, 2007 for asimilar procedure).9 The results in Table 2 showthat all constructions are productive to some ex-tent, though clearly some yield fewer new types.p1 and p2 show that CCs are responsible forvery little of the productive potential of compara-tives in the corpus.
This is not only a function ofthe relative rarity of CCs: if we look at their rate ofvocabulary growth (Figure 1), general compara-tives gather new types more rapidly than CCs evenfor the same sample size10.
Using a Finite ZipfMandelbrot Model (FZM, Evert, 2004), we canextrapolate from the observed data to predict thegap will grow with sample size.toks types hpx p1 p2 p3266703 5988 2616 0.00772 0.00651 0.00988 I thank Livio Gaeta and an anonymous reviewer for com-menting on this point.9 In fact, one could also address the productivity of the con-struction as a whole by regarding each argument tuple as atype, e.g.
<more ergonomic, better> could be a hapax legome-non despite better appearing quite often.
Since each slot mul-tiplies the chances a construction has to be unique, the nth rootof the value of the measure would have to be taken in order tomaintain comparability, thus the square root of pk for 2 slots, the cube root for 3 slots and so on.
Another option, if one isinterested in the chance that any particular slot will be unique,is to take the average of pk for all slots.
However for the pre-sent purpose the individual score of each slot is more relevant.10 The comparative curve is taken from 2000 occurrencesevenly distributed across the sections of the BNC, to corre-spond topically to the CCs, which cover the whole corpus.word comp cc1 cc2 ?1 ?2?1(?2)(?1)?2 n1 n2n1(n2)(n1)n2further 21371better 20727 15 143  89  51 9 22 5 15higher 15434 97 39 4 2 3  84 23 44 21greater 13883 82 171 1 1    75 92 35 80lower 10983 20 27  2    18 12 7 12older 8714 24 1 1  1  3  1?longer 3820 45 15 3 1 3  11 3 9 3bigger 3469 43 13 4 1 3  30 8 15 8morelikely 3449  28          2  1?morewholistic 1zanier 1 1          1Table 1.
Comparative frequencies independently andin cc1/cc2, with or without nominal subjects/verbs inone or both clauses.comp802 208 140 0.00026 0.00034 0.1745cc1802 181 126 0.00023 0.00031 0.1571cc258 45 37 5.80E-05 9.22E-05 0.6379bare158 7 5 9.03E-06 1.24E-05 0.0862bare2Table 2.
Productivity scores for comparatives, cc-clauses in general and specifically for bare CCs50However, p3 shows the surprising result thatCCs have more potential productivity than com-paratives in general, with the bare cc1 slot leading,both general CC slots somewhat behind, and thebare cc2 last.
This means our data does not beginto approach covering this category ?
the next CC ismuch likelier to be novel, given the data we?veseen so far.With this established, the question ariseswhether a CFG rule like the one above should takeaccount of the likelihood of each slot to containnovel vs. familiar members.
For instance, if aPCFG parser correctly identifies a novel compara-tive and the input matches the rule, should it bemore skeptical of an unseen bare cc1 than an un-seen bare cc2 (keeping in mind that the latter haveso far been better in 88% of cases)?
To illustratethis, we may consider the output of a PCFG parser(in this case the Stanford Parser, Klein and Man-ning, 2003) for an ambiguous example.Since CCs are rather rare, PCFGs will tend toprefer most other parses of a sentence, if these areavailable.
Where no other reading is available wemay get the expected two clause structure, as in theexample in Figure 2.1111 The X nodes conform to the Penn Treebank II BracketingGuidelines for CCs (Bies et al, 1995:178).The Stanford Parser fares quite well in caseslike these, since the pronoun (it, I) can hardly bemodified by the comparative (*[NP the closer it] or*[NP the more worried I]), and similarly for NPswith articles (*[NP the closer the time]).
Yet article-less NPs and bare CCs cause problems, as in thetree in Figure 3.Figure 2.
Stanford Parser tree for: The closer it gets,the more worried I become.Figure 1.
Vocabulary growth curves and FZM ex-trapolations for comparatives in cc1, cc2 and at largein the BNC.Figure 3.
Stanford Parser tree for: The less cloudy,the better views can be seen to the south.Here The less cloudy and the better views form oneNP, separate from the VP complex.
Such a readingis not entirely impossible: the sentence could mean?less cloudy, better views?
appositively.
Howeverdespite the overall greater frequency of appositionsand the fact that less cloudy has probably not beenobserved in cc1 in training data, the pattern of anovel form for cc1 and better in cc2 is actuallyconsistent with a novel CC.
With these ideas inmind, the next section examines the potential ofproductivity to disambiguate a much more preva-lent phenomenon, namely PP attachment.4 PP Attachment and ProductivityThe problem of attaching prepositional phrases assister nodes of VP or as adjuncts to its object nouns51is a classic case of syntactic ambiguity that causestrouble for parsers (see Hindle and Rooth, 1993;Manning and Sch?tze, 1999:278-287; Atterer andSch?tze, 2007), e.g.
the difference between I ate afish with a fork and I ate a fish with bones12, i.e.denoting the instrument or an attribute of the fish.There are also two further common readings of thepreposition with in this context, namely attachedeither high or low in the VP in a comitative sense:I ate a fish with Mary and I ate a fish with potatoesrespectively, though most approaches do not dis-tinguish these, rather aiming at getting the attach-ment site right.Already in early work on PP attachment (Hindleand Rooth, 1993) it was realized that the lexicalidentity of the verb, its object, the preposition andin later approaches also the prepositional objectnoun (Ratnaparkhi et al, 1994) are useful for pre-dicting the attachment site, casting the task as aclassification of tuples <v, n1, p, n2> into theclasses V (VP attachment) and N (NP attachment).Classifiers are commonly either supervised, withdisambiguated training data, or more recently un-supervised (Ratnaparkhi, 1998) using data fromunambiguous cases where no n1 or v appears.Other approaches supplement this information withhand-built or automatically acquired lexical re-sources and collocation databases to determine therelationship between the lexemes, or, for lexemesunattested in the tuples, for semantically similarones (Stetina and Nagao, 1997; Pantel and Lin,2000).Although the state of the art in lexically basedsystems actually approaches human performance,they lose their power when confronted with unfa-miliar items.
For example, what is the likeliest at-tachment for the following BNC example: I canalways eat dim-sum with my dybbuk?
It is safe toassume that the (originally Hebrew) loan-worddybbuk ?
(demonic) possession?
does not appear inmost training datasets, though dim-sum is attestedmore than once as an object of eat in the BNC.Crucially, the triple (eat, dim-sum, with) alonecannot reliably resolve the attachment site (con-sider soy-sauce vs. chopsticks as n2).
It is thusworth examining how likely a novel item is in the12 Though in some cases the distinction is not so tenable, e.g.we have not signed a settlement agreement with them (Man-ning and Sch?tze, 1999:286), where with them can arguably beattached low or high.
Incidentally, the ?fish?
examples areactually attested in the BNC in a linguistic context.relevant slot of each reading?s construction.
Therest of this section therefore examines productivityscores for the slots in eat NP with NP and theircorrelation with different readings as an example.Since these cases cannot be identified automati-cally in an unparsed text with any reliability, andsince there is not enough hand-parsed data contain-ing these constructions, a conservative proximityassumption was made (cf.
Ratnaparkhi, 1998) andall occurrences of eat and related forms within tenwords of with and with no intervening punctuationin the BNC were evaluated and tagged manuallyfor this study.
This also allowed for head-noun andanaphor resolution to identify the referent of a slotin the case of pronominal realization; thus all slottypes in the data including pronouns are evaluatedin terms of a single head noun.Results show that out of 131 hits, the largestgroup of PPs (59 tokens) were object noun modifi-ers, almost all comitatives13, justifying the preva-lent heuristic to prefer low attachment.
Howeververbal instrumentals and high comitatives (25 and23 respectively) come at a very close second.
Theremaining 24 cases were adverbial modifications(e.g.
with enthusiasm).
Looking at hapax legomenain the respective slots we can calculate the meas-ures in Table 3.The scores show that the verbal instrumental read-ing is the least likely to exhibit a novel head at then2 slot, which is semantically plausible ?
the reper-toire of eating instruments is rather conventional-ized and slow to expand.
The comitative reading isvery likely to innovate in n2, but much less so inn1, fitting e.g.
the ?dim-sum with dybbuk?-scenario.
This fits the fact that one may eat to-gether with many distinct persons etc., but whenn1 slot n2 slot totalhapax p3 hapax p3 tokensn 39 0.661 45 0.7627 59v adv 15 0.625 21 0.875 24v com 8 0.3478 20 0.8696 23v inst 15 0.6 4 0.16 25Table 3. p3 for the first and second headnoun in nominal and three types of verbal PPattachment for eat n with n in the BNC.13 Only 4 hits were truely non-comitative noun modifiers, e.g.<eat, anything, with, preservatives>, where a comitative read-ing is clearly not intended.
Since the group was so small, allnoun modifiers have been treated here together.52these are specified, the exact nature of the meal orfood is often left unspecified14.
The adverbial read-ing is likely to innovate in both slots, since manyways or circumstances of eating can be specifiedand these hardly restrict the choice of object foreat.
Interestingly, the choice of object maintains avery stable productivity in all but the high comita-tive construction.
n2 innovation in nominal modi-fiers is actually lower than for adverbials andcomitatives, meaning low attachment may not bethe preferred choice for unknown nouns.While these results imply what some reasonableexpectations may be to find a novel member ofeach slot in each reading, they do not take the iden-tity of the lexemes into account.
In order to com-bine the general information about the slot withknowledge of a known slot member, we may si-multaneously attempt to score the productivity ofthe construction?s components, namely the noun orverb in question, for PP modifiers.
This raises theproblem of what exactly should be counted.
Onemay argue that high-attached comitatives and ad-verbials should be counted separately, since theyare almost always optional regardless of the verb(one can equally well eat or do anything else withsomeone in some way), unlike instrumentals whichmay be more closely linked to the verb.
On theother hand, the exact constructional sense of suchPPs is colored by the verb, e.g.
eating a meal withsomeone has a rather particular meaning (as op-posed to coincidentally performing the act of eat-ing alongside another eater).
If the decision is onlybetween high and low attachment, then groupingall variants together may be sensible in any case.Depending on the argument and verb, it is pos-sible to make fine distinctions, provided enoughcases are found.
For dim-sum, for example, nocases of NP modifying with (novel or otherwise)are found, making the (correct) high comitativereading likely.
By contrast, for the head noun fish,which is a common object of eat, 37 hits with with-PPs are found in the BNC, forming 32 preposi-tional object noun types of which 28 are hapax le-gomena in this slot.
All high readings of with-PPswith eat (including intransitive eat) form 92 to-kens, 68 noun types and 44 hapax legomena.
Thusfish + PP scores p3=0.756 while eat + PP scores14 In fact the non-food specific nouns breakfast, lunch, dinner,dish and meal cover 16 of the high comitative n1 tokens, al-most 70%.0.478, corresponding to less productivity.
Thismeans novel prepositional objects are substantiallyless probable for the high attachment given that thedirect object is fish.5 ConclusionThe above results show that similar yet distinctconstructions, which vary slightly in either con-stituent structure (high vs. low attachment), seman-tics (comitative or instrumental PPs), number ofarguments (more and less bare CCs) or position(cc1 vs. cc2),  show very different lexical behavior,exhibiting more or less variety in different slotsand differing proportions of hapax legomena.
Theinference which should become apparent from thesharp contrasts in slot scores (especially in p3)given the size of the data, is that these differencesare not coincidental but are indicative of inherentlydifferent productivity rates for each slot in eachconstruction.
These properties need not be attrib-uted to system internal, linguistic reasons alone,but may also very well reflect world knowledgeand pragmatic considerations.15 However, from aconstruction grammar point of view, the entrench-ment of these constructions in speakers and there-fore in data is inextricably connected withinteraction in the world, thus making syntacticproductivity a plausible and relevant quantity boththeoretically and potentially for NLP practice.It remains to be seen whether or not productiv-ity scores can help automatically disambiguatestructures with unseen arguments (e.g.
PP attach-ment with unencountered n2), or even distinguishsemantic classes such as comitatives, instrumentalsetc.
for novel nouns, for which a classification intohelpful semantic categories (animate, human andso forth) is not available.
A large-scale evaluationof this question will depend on how easily and re-liably productivity scores can be extracted auto-matically from data for the relevant constructions.ReferencesBengt Altenberg and Mats Eeg-Olofsson.
1990.
Phrase-ology in Spoken English.
In: Jan Aarts and WillemMeijs, editors, Theory and Practice in Corpus Lin-guistics.
Rodopi, Amsterdam: 1-26.15 In this context it is worth mentioning that similar ongoingexaminations of German CCs reveal different lexical prefer-ences, implying that some of this behavior is language de-pendent and to some extent language internally lexicalized.53Michaela Atterer and Hinrich Sch?tze.
2007.
Preposi-tional Phrase Attachment without Oracles.
Computa-tional Linguistics, 33(4): 469-476.R.
Harald Baayen.
2001.
Word Frequency Distributions.
(Text, Speech and Language Technologies 18.)
Klu-wer Academic Publishers, Dordrecht / Boston / Lon-don.R.
Harald Baayen.
2009.
Corpus Linguistics in Mor-phology: Morphological Productivity.
In: AnkeL?deling and Merja Kyt?, editors, Corpus Linguis-tics.
An International Handbook, vol.
2.
Mouton deGruyter, Berlin: 899-919.J?hanna Bar?dal.
2006.
Predicting the Productivity ofArgument Structure Constructions.
In: The 32nd An-nual Meeting of the Berkeley Linguistics Society.Berkeley Linguistics Society, Berkeley.
Available at:http://ling.uib.no/barddal/BLS-32.barddal.pdf.Laurie Bauer.
2001.
Morphological Productivity.
(Cam-bridge Studies in Linguistics 95.)
Cambridge Univer-sity Press, Cambridge, UK.Ann Bies, Mark Ferguson, Karen Katz and Robert Mac-Intyre.
1995.
Bracketing Guidelines for Treebank IIStyle Penn Treebank Project.
Technical report, Uni-versity of Pennsylvania.Douglas Biber, Susan Conrad and Viviana Cortes.
2004.If you look at?
: Lexical Bundles in UniversityTeaching and Textbooks.
Applied Linguistics, 25(3):371-405.Douglas Biber, Stig Johansson, Geoffrey Leech, SusanConrad and Edward Finegan.
1999.
The LongmanGrammar of Spoken and Written English.
Longman,London.Yaacov Choueka.
1988.
Looking for Needles in a Hay-stack.
In: Proceedings of RIAO ?88.
Cambridge, MA,609-623.Peter W. Culicover and Ray Jackendoff.
1999.
TheView from the Periphery: The English ComparativeCorrelative.
Linguistic Inquiry 30(4): 543-571.Marcel den Dikken.
2005.
Comparative CorrelativesComparatively.
Linguistic Inquiry, 36(4): 497-532.Stefan Evert.
2004.
A simple LNRE model for randomcharacter sequences.
In: Proceedings of JADT 2004:411-422.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
PhD disserta-tion, University of Stuttgart.Stefan Evert.
2009.
Corpora and Collocations.
In: AnkeL?deling and Merja Kyt?, editors, Corpus Linguis-tics.
An International Handbook, vol.
2.
Mouton deGruyter, Berlin: 1212-1248.Adele E. Goldberg.
1995.
Constructions: A Construc-tion Grammar Approach to Argument Structure.University of Chicago Press, Chicago and London.Adele E. Goldberg.
2006.
Constructions at Work: TheNature of Generalization in Language.
Oxford Uni-versity Press, Oxford, UK.Donald Hindle and Mats Rooth.
1993.
Structural Ambi-guity and Lexical Relations.
Computational Linguis-tics, 19(1): 103-130.Daisuke Kawahara and Sadao Kurohashi.
2005.
PP-Attachment Disambiguation Boosted by a GiganticVolume of Unambiguous Examples.
In: Proceedingsof the 2nd International Joint Conference on NaturalLanguage Processing (IJCNLP-05): 188-198.Tibor Kiss.
2007.
Produktivit?t und Idiomatizit?t vonPr?position-Substantiv-Sequenzen.
Zeitschrift f?rSprachwissenschaft, 26(2): 317-345.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In: Proceedings of the 41stMeeting of the Association for Computational Lin-guistics: 423-430.Christopher D. Manning and Hinrich Sch?tze.
1999.Foundations of Statistical Natural Language Proc-essing.
MIT Press, Cambridge, MA.James D. McCawley.
1988.
The Comparative Condi-tional in English, German and Chinese.
In: Proceed-ings of the Fourteenth Annual Meeting of theMerkeley Linguistics Society.
Berkeley LinguisticsSociety, Berkeley: 176-187.Patrick Pantel and Dekang Lin.
2000.
An UnsupervisedApproach to Prepositional Phrase Attachment usingContextually Similar Words.
In: Proceedings of the38th Annual Meeting of the Association for Computa-tional Linguistics: 101-108.Adwait Ratnaparkhi.
1998.
Statistical Models for Unsu-pervised Prepositional Phrase Attachment.
In: Pro-ceedings of COLING-ACL98, Montreal Canada:1079-1085Adwait Ratnaparkhi, Jeff Reynar and Salim Roukos.1994.
A Maximum Entropy Model for PrepositionalPhrase Attachment.
In: Proceedings of the ARPAHuman Language Technology Workshop.
Plainsboro,NJ: 250-255.Ivan Sag, Timothy Baldwin, Francis Bond, AnnCopestake and Dan Flickinger.
2002.
Multiword Ex-pressions: A Pain in the Neck for NLP.
In: Proceed-ings of the Third International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLING 2002).
Mexico City, Mexico: 1-15.Andr?
Salem.
1987.
Pratique des segments r?p?t?s.Institut National de la Langue Fran?aise, Paris.Jiri Stetina and Makoto Nagao.
1997.
Corpus Based PPAttachment Ambiguity Resolution with a SemanticDictionary.
In: Jou Zhao and Kenneth Church, edi-tors, Proceedings of the Fifth Workshop on VeryLarge Corpora.
Beijing and Hong Kong: 66-80.Gisela Zifonun, Ludger Hoffmann and Bruno Strecker,editors.
1997.
Grammatik der deutschen Sprache, Bd.3.
(Schriften des Instituts f?r deutsche Sprache 7.)
DeGruyter, Berlin / New York.54
