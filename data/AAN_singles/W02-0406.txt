Manual and Automatic Evaluation of SummariesChin-Yew Lin and Eduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292+1-310-448-8711/8731{cyl,hovy}@isi.eduAbstractIn this paper we discuss manual andautomatic evaluations of summaries usingdata from the Document UnderstandingConference 2001 (DUC-2001).
We firstshow the instability of the manualevaluation.
Specifically, the low inter-human agreement indicates that morereference summaries are needed.
Toinvestigate the feasibility of automatedsummary evaluation based on the recentBLEU method from machine translation, weuse accumulative n-gram overlap scoresbetween system and human summaries.
Theinitial results provide encouragingcorrelations with human judgments, basedon the Spearman rank-order correlationcoefficient.
However, relative ranking ofsystems needs to take into account theinstability.1 IntroductionPrevious efforts in large-scale evaluation of textsummarization include TIPSTER SUMMAC(Mani et al 1998) and the DocumentUnderstanding Conference (DUC) sponsored bythe National Institute of Standards andTechnology (NIST).
DUC aims to compilestandard training and test collections that can beshared among researchers and to providecommon and large scale evaluations in singleand multiple document summarization for theirparticipants.In this paper we discuss manual and automaticevaluations of summaries using data from theDocument Understanding Conference 2001(DUC-2001).
Section 2 gives a brief overviewof the evaluation procedure used in DUC-2001and the Summary Evaluation Environment(SEE) interface used to support the DUC-2001human evaluation protocol.
Section 3 discussesevaluation metrics.
Section 4 shows theinstability of manual evaluations.
Section 5outlines a method of automatic summaryevaluation using accumulative n-gram matchingscore (NAMS) and proposes a view that castssummary evaluation as a decision makingprocess.
It shows that the NAMS method isbounded and in most cases not usable, givenonly a single reference summary to comparewith.
Section 6 discusses why this is so,illustrating various forms of mismatchingbetween human and system summaries.
Weconclude with lessons learned and futuredirections.2 Document UnderstandingConference (DUC)DUC2001 included three tasks:?
Fully automatic single-documentsummarization: given a document,participants were required to create ageneric 100-word summary.
The trainingset comprised 30 sets of approximately 10documents each, together with their 100-word human written summaries.
The testset comprised 30 unseen documents.?
Fully automatic multi-documentsummarization: given a set of documentsabout a single subject, participants wererequired to create 4 generic summaries ofthe entire set, containing 50, 100, 200, and400 words respectively.
The document setswere of four types: a single natural disasterevent; a single event; multiple instances of atype of event; and information about anindividual.
The training set comprised 30sets of approximately 10 documents, eachprovided with their 50, 100, 200, and 400-word human written summaries.
The testset comprised 30 unseen sets.Philadelphia, July 2002, pp.
45-51.
Association for Computational Linguistics.Proceedings of the Workshop on Automatic Summarization (including DUC 2002),?
Exploratory summarization: participantswere encouraged to investigate alternativeapproaches to evaluating summarization andreport their results.A total of 11 systems participated in the single-document summarization task and 12 systemsparticipated in the multi-document task.The training data were distributed in earlyMarch of 2001 and the test data were distributedin mid-June of 2001.
Results were submitted toNIST for evaluation by July 1st 2001.2.1 Evaluation MaterialsFor each document or document set, one humansummary was created as the ?ideal?
modelsummary at each specified length.
Two otherhuman summaries were also created at eachlength.
In addition, baseline summaries werecreated automatically for each length asreference points.
For the multi-documentsummarization task, one baseline, lead baseline,took the first 50, 100, 200, and 400 words in thelast document in the collection.
A secondbaseline, coverage baseline, took the firstsentence in the first document, the first sentencein the second document and so on until it had asummary of 50, 100, 200, or 400 words.
Onlyone baseline (baseline1) was created for thesingle document summarization task.2.2 Summary Evaluation EnvironmentNIST assessors who created the ?ideal?
writtensummaries did pairwise comparisons of theirsummaries to the system-generated summaries,other assessors?
summaries, and baselinesummaries.
They used the Summary EvaluationEnvironment (SEE) 2.0 developed by one of theauthors (Lin 2001) to support the process.Using SEE, the assessors compared the system?stext (the peer text) to the ideal (the model text).As shown in Figure 1, each text wasdecomposed into a list of units and displayed inseparate windows.
In DUC-2001 the sentencewas used as the smallest unit of evaluation.Figure 1.
SEE in an evaluation session.SEE 2.0 provides interfaces for assessors tojudge both the content and the quality ofsummaries.
To measure content, assessors stepthrough each model unit, mark all system unitssharing content with the current model unit(shown in green highlight in the model summarywindow), and specify that the marked systemunits express all, most, some or hardly any ofthe content of the current model unit.
Tomeasure quality, assessors rate grammaticality1,cohesion2, and coherence3 at five differentlevels: all, most, some, hardly any, or none.For example, as shown in Figure 1, an assessormarked system units 1.1 and 10.4 (shown in redunderlines) as sharing some content with thecurrent model unit 2.2 (highlighted green).3 Evaluation MetricsOne goal of DUC-2001 was to debug theevaluation procedures and identify stablemetrics that could serve as common referencepoints.
NIST did not define any officialperformance metric in DUC-2001.
It releasedthe raw evaluation results to DUC-2001participants and encouraged them to proposemetrics that would help progress the field.3.1 Recall, Coverage, Retention andWeighted RetentionRecall at different compression ratios has beenused in summarization research to measure howwell an automatic system retains importantcontent of original documents (Mani andMaybury 1999).
Assume we have a systemsummary Ss and a model summary Sm.
Thenumber of sentences occurring in Ss is Ns, thenumber of sentences in Sm is Nm, and the numberin both Ss and Sm is Na.
Recall is defined asNa/Nm.
The Compression Ratio is defined as thelength of a summary (by words or sentences)divided by the length of its original document.Applying this direct all-or-nothing recall inDUC-2001 without modification is notappropriate because:1 Does the summary observe English grammaticalrules independent of its content?2 Do sentences in the summary fit in with theirsurrounding sentences?3 Is the content of the summary expressed andorganized in an effective way?1.
Multiple system units contribute to multiplemodel units.2.
Exact overlap between Ss and Sm rarelyoccurs.3.
Overlap judgment is not binary.For example in Figure 1, an assessor judgedsystem units 1.1 and 10.4 sharing some contentwith model unit 2.2.
Unit 1.1 says ?Thousandsof people are feared dead?
and unit 2.2 says?3,000 and perhaps ?
5,000 people have beenkilled?.
Are ?thousands?
equivalent to ?3,000 to5,000?
or not?
Unit 10.4 indicates it was an?earthquake of magnitude 6.9?
and unit 2.2 saysit was ?an earthquake measuring 6.9 on theRichter scale?.
Both of them report a ?6.9?earthquake.
But the second part of system unit10.4, ?in an area so isolated?
?, seems to sharesome content with model unit 4.4 ?the quakewas centered in a remote mountainous area?.Are these two equivalent?
This examplehighlights the difficulty of judging the contentcoverage of system summaries against modelsummaries and the inadequacy of using simplerecall as defined.For this reason, NIST assessors not only markedthe segments shared between system units (SU)and model units (MU), they also indicated thedegree of match, i.e., all, most, some, hardlyany, or none.
This enables us to computeweighted recall.Different versions of weighted recall wereproposed by DUC-2001 participants.
(McKeownet al 2001) treated the completeness of coverageas a threshold: 4 for all, 3 for most and above, 2for some and above, and 1 for hardly any andabove.
They then proceeded to compare systemperformances at different threshold levels.
Theydefined recall at threshold t, Recallt, as follows:summary model in the MUs ofnumber  Totalaboveor at  marked MUs ofNumber tInstead of thresholds, we use here as coveragescore the ratio of completeness of coverage C: 1for all, 3/4 for most, 1/2 for some, 1/4 for hardlyany, and 0 for none.
To avoid confusion withthe recall used in information retrieval, we callour metric weighted retention, Retentionw, anddefine it as follows:summary model in the MUs ofnumber  Totalmarked) MUs of(Number C?If we ignore C (set it to 1), we obtain anunweighted retention, Retention1.
We usedRetention1 in our evaluation to illustrate thatrelative system performance (i.e., systemranking) changes when different evaluationmetrics are chosen.
Therefore, it is important tohave common and agreed upon metrics tofacilitate large scale evaluation efforts.4 Instability of Manual JudgmentsIn the human evaluation protocol described inSection 2, nothing prevents an assessor fromassigning different coverage scores to the samesystem units produced by different systemsagainst the same model unit.
(Since mostsystems produce extracts, the same sentencemay appear in many summaries, especially forsingle-document summaries.)
Analyzing theDUC-2001 results, we found the following:?
Single document tasko A total of 5,921 judgmentso Among them, 1,076 (18%) containmultiple judgments for the same unitso 143 (2.4%) of them have three differentcoverage scores?
Multi-document tasko A total of 6,963 judgmentso Among them 528 (7.6%) contain multiplejudgmentso 27 (0.4%) of them have three differentcoverage scoresIntuitively this is disturbing; the same phrasecompared to the same model unit should alwayshave the same score regardless of which systemproduced it.
The large percentage of multiplejudgments found in the single documentevaluation are test-retest errors that need to beaddressed in computing performance metrics.Figure 2 and Figure 3 show the retention scoresfor systems participating in the single- andmulti-document tasks respectively.
The errorbars are bounded at the top by choosing themaximum coverage score (MAX) assigned byan assessor in the case of multiple judgmentscores and at the bottom by taking the minimumassignment (MIN).
We also compute system39.7540.9533.9831.32 31.7231.1536.2927.9334.4429.90 30.2128.0230.7126.0339.5740.8830.1228.0928.26 27.7133.0124.4634.4426.0227.4025.5428.6623.9039.6740.9032.2429.89 30.21 29.6134.7826.3834.4428.2528.9927.0229.7625.1220.0025.0030.0035.0040.0045.00Human1Human2Baseline1 O P Q R S T V W X Y ZSystemsRetentionMAJORITYORIGINALMAXMINAVGFigure 2.
DUC 2001 single document retention score distribution.retentions using the majority (MAJORITY) andaverage (AVG) of assigned coverage scores.The original (ORIGINAL) does not consider theinstability in the data.Analyzing all systems?
results, we made thefollowing observations.
(1) Inter-human agreement is low in the single-document task (~40%) and even lower inmulti-documents task (~29%).
Thisindicates that using a single model asreference summary is not adequate.
(2) Despite the low inter-human agreement,human summaries are still much better thanthe best performing systems.
(3) The relative performance (rankings) ofsystems changes when the instability ofhuman judgment is considered.
However,the rerankings remain local; systems remainwithin performance groups.
For example,we have the following groups in the multi-document summarization task (Figure 3,considering 0.5% error):a.
{Human1, Human2}b.
{N, T, Y}c. {Baseline2, L, P}d. {S}e. {M, O, R}f. {Z}g. {Baseline1, U, W}The existence of stable performance regions isencouraging.
Still, given the large error bars,one can produce 162 different rankings of these16 systems.
Groups are less obvious in thesingle document summarization task due toclose performance among systems.Table 1 shows relative performance betweensystems x and y in the single documentTable 1.
Pairwise relative system performance(single document summarization task).28.55 29.037.4915.42 15.7111.5617.9211.3916.4911.5613.6018.636.61 6.9017.949.1628.5529.027.3014.0215.0010.9617.9210.9315.0610.8712.4018.296.476.7417.608.8728.5529.037.3814.7615.3811.2617.9211.1915.8011.2213.0218.476.54 6.8117.809.025.0010.0015.0020.0025.0030.00Human1Human2Baseline1Baseline2 L M N O P R S T U W Y ZSystemsRetentionMAJORITYORIGINALMAXMINAVGFigure 3.
DUC 2001 multi-document retention score distribution.H1 H2 B1 O P Q R S T V W X Y ZH1 = - + + + + + + + + + + + +H2 + = + + + + + + + + + + + +B1 - - = ~ ~ ~ ~ + - + ~ + ~ +O - - ~ = ~ ~ - + - ~ ~ + ~ +P - - ~ ~ = ~ - + - ~ ~ + ~ +Q - - ~ ~ ~ = - ~ - ~ ~ ~ ~ +R - - ~ + + + = + ~ + + + + +S - - - - - ~ - = - ~ ~ ~ - ~T - - + + + + ~ + = + + + + +V - - - ~ ~ ~ - ~ - = ~ ~ ~ ~W - - ~ ~ ~ ~ - ~ - ~ = ~ ~ +X - - - - - ~ - ~ - ~ ~ = - ~Y - - ~ ~ ~ ~ - + - ~ ~ + = +Z - - - - - - - ~ - ~ - ~ - =summarization task.
A ?+?
indicates theminimum retention score of x (row) is higherthan the maximum retention score of y(column), a ?-?
indicates the maximum retentionscore of x is lower than the minimum retentionscore of y, and a ?~?
means x and y areindistinguishable.
Table 2 shows relativesystem performance in the multi-documentsummarization task.Despite the instability of the manual evaluation,we discuss automatic summary evaluation in anattempt to approximate the human evaluationresults in the next section.5 Automatic Summary EvaluationInspired by recent progress in automaticevaluation of machine translation (BLEU;Papineni et al 2001), we would like to apply thesame idea in the evaluation of summaries.Following BLEU, we used the automaticallycomputed accumulative n-gram matching scores(NAMS) between a model unit (MU) and asystem summary (S)4 as performance indicator,considering multi-document summaries.
Onlycontent words were used in forming n-grams.NAMS is defined as follows:a1?NAM1 + a2?NAM2 + a3?NAM3 + a4?NAM4NAMn is n-gram hit ratio defined as:MUin  grams-n of # totalS and MUbetween  grams-n matched of #We tested three different configurations of ai:4 The whole system summary was used to computeNAMS against a model unit.C1: a1 = 1 and a2 = a3 = a4 = 0;C2: a1 = 1/3, a2 = 2/3, and a3 = a4 = 0;C3: a1 = 1/6, a2 = 2/6, a3 = 3/6, and a4 = 0;C1 is simply unigram matching.
C2 and C3give more credit to longer n-gram matches.
Toexamine the effect of stemmers in helping the n-gram matching, we also tested all configurationswith two different stemmers (Lovin?s andPorter?s).
Figure 4 shows the results with andwithout using stemmers and their Spearmanrank-order correlation coefficients (rho)compared against the original retention rankingfrom Figure 4.
X-nG is configuration n withoutusing any stemmer, L-nG with the Lovinstemmer, and P-nG with the Porter stemmer.The results in Figure 4 indicate that unigrammatching provides a good approximation, butthe best correlation is achieved using C2 withthe Porter stemmer.
Using stemmers didimprove correlation.
Notice that rank inversionremains within the performance groupsidentified in Section 4.
For example, theretention ranking of Baseline1, U, and W is 14,16, and 15 respectively.
The P-2G ranking ofthese three systems is 15, 14, and 16.
The onlysystem crossing performance groups is Y.  Yshould be grouped with N and T but theautomatic evaluations place it lower, in thegroup with Baseline2, L, and P.  The primaryreason for Y?s behavior may be that itssummaries consist mainly of headlines, whoseabbreviated style differs from the languagemodels derived from normal newspaper text.For comparison, we also ran IBM?s BLEUevaluation script5 over the same model andsystem summary set.
The Spearman rank-ordercorrelation coefficient (?)
for the singledocument task is 0.66 using one referencesummary and 0.82 using three referencesummaries; while Spearman ?
for the multi-document task is 0.67 using one reference and0.70 using three.6 ConclusionsWe described manual and automatic evaluationof single and multi-document summarization inDUC-2001.
We showed the instability of5 We thank Kishore Papineni for sending us BLEU1.0.Table 2.
Pairwise relative system performance(multi-document summarization task).H1 H2 B1 B2 L M N O P R S T U W Y ZH1 = - + + + + + + + + + + + + + +H2 + = + + + + + + + + + + + + + +B1 - - = - - - - - - - - - + + - -B2 - - + = ~ + - + ~ + + - + + - +L - - + ~ = + - + ~ + + - + + - +M - - + - - = - ~ - ~ - - + + - +N - - + + + + = + + + + - + + ~ +O - - + - - ~ - = - ~ - - + + - +P - - - ~ ~ + - + = + + - + + - +R - - + - - ~ - ~ - = - - + + - +S - - + - - + - + - + = - + + - +T - - + + + + + + + + + = + + + +U - - - - - - - - - - - - = - - -W - - - - - - - - - - - - + = - -Y - - + + + + ~ + + + + - + + = +Z - - + - - - - - - - - - + + - =human evaluations and the need to consider thisfactor when comparing system performances.As we factored in the instability, systems tendedto form separate performance groups.
Oneshould treat with caution any interpretation ofperformance figures that ignores this instability.Automatic evaluation of summaries usingaccumulative n-gram matching scores seemspromising.
System rankings using NAMS andretention ranking had a Spearman rank-ordercorrelation coefficient above 97%.
Usingstemmers improved the correlation.
However,satisfactory correlation is still elusive.
The mainproblem we ascribe to automated summaryevaluation is the large expressive range ofEnglish since human summarizers tend to createfresh text.
No n-gram matching evaluationprocedure can overcome the paraphrase orsynonym problem unless (many) modelsummaries are available.We conclude the following:(1) We need more than one model summaryalthough we cannot estimate how manymodel summaries are required to achievereliable automated summary evaluation.
(2) We need more than one evaluation for eachsummary against each model summary.
(3) We need to ensure a single rating for eachsystem unit.ReferencesDUC.
2001.
The Document UnderstandingConference 2001. http://www-nlpir.nist.gov/projects/duc/2001.html.Lin, C.-Y.
2001.
Summary EvaluationEnvironment.
http://www.isi.edu/~cyl/SEE.Mani, I., D. House, G. Klein, L. Hirschman, L.Obrst, T. Firmin, M. Chrzanowski, and B.Sundheim.
1998.
The TIPSTER SUMMACText Summarization Evaluation: FinalReport.
MITRE Corp. Tech.
Report.Papineni K., S. Roukos, T. Ward, W.-J.
Zhu.2001.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
IBMResearch Report RC22176(W0109-022).OriginalSYSCODE Retention X-1G X-2G X-3G L-1G L-2G L-3G P-1G P-2G P-3Granking (unigram) (unigram) (unigram)Human1 1 2 1 1 1 1 1 1 1 1Human2 2 1 2 2 2 2 2 2 2 2Baseline1 14 15 15 15 16 15 14 16 15 14Baseline2 8 8 7 6 8 8 6 8 8 6L 7 7 6 7 7 7 7 7 7 7M 10 10 10 10 10 11 11 9 10 11N 4 4 4 4 4 4 4 4 4 4O 11 12 12 12 12 12 12 11 12 12P 6 5 5 5 5 5 5 5 5 5R 11 11 11 11 11 10 10 12 11 10S 9 9 9 9 9 9 9 9 9 9T 3 3 3 3 3 3 3 3 3 3U 16 14 14 14 14 14 15 14 14 15W 15 16 16 16 15 16 16 15 16 16Y 5 6 8 8 6 6 8 6 6 8Z 13 13 13 13 13 13 13 13 13 13Spearman ?
1.00000 0.98382 0.97206 0.96912 0.98382 0.98382 0.97206 0.98235 0.98676 0.97206No stemmer Lovin stemmer Porter stemmerFigure 4.
Manual and automatic ranking comparisons.
