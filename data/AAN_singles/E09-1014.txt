Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112?120,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsHuman Evaluation of a German Surface Realisation RankerAoife CahillInstitut fu?r Maschinelle Sprachverarbeitung (IMS)University of Stuttgart70174 Stuttgart, Germanyaoife.cahill@ims.uni-stuttgart.deMartin ForstPalo Alto Research Center3333 Coyote Hill RoadPalo Alto, CA 94304, USAmforst@parc.comAbstractIn this paper we present a human-basedevaluation of surface realisation alterna-tives.
We examine the relative rankings ofnaturally occurring corpus sentences andautomatically generated strings chosen bystatistical models (language model, log-linear model), as well as the naturalness ofthe strings chosen by the log-linear model.We also investigate to what extent preced-ing context has an effect on choice.
Weshow that native speakers do accept quitesome variation in word order, but there arealso clearly factors that make certain real-isation alternatives more natural.1 IntroductionAn important component of research on surfacerealisation (the task of generating strings for agiven abstract representation) is evaluation, espe-cially if we want to be able to compare across sys-tems.
There is consensus that exact match withrespect to an actually observed corpus sentence istoo strict a metric and that BLEU score measuredagainst corpus sentences can only give a rough im-pression of the quality of the system output.
It isunclear, however, what kind of metric would bemost suitable for the evaluation of string realisa-tions, so that, as a result, there have been a range ofautomatic metrics applied including inter alia ex-act match, string edit distance, NIST SSA, BLEU,NIST, ROUGE, generation string accuracy, gener-ation tree accuracy, word accuracy (Bangalore etal., 2000; Callaway, 2003; Nakanishi et al, 2005;Velldal and Oepen, 2006; Belz and Reiter, 2006).It is not always clear how appropriate these met-rics are, especially at the level of individual sen-tences.
Using automatic evaluation metrics cannotbe avoided, but ideally, a metric for the evaluationof realisation rankers would rank alternative real-isations in the same way as native speakers of thelanguage for which the surface realisation systemis developed, and not only globally, but also at thelevel of individual sentences.Another major consideration in evaluation iswhat to take as the gold standard.
The easiest op-tion is to take the original corpus string that wasused to produce the abstract representation fromwhich we generate.
However, there may well beother realisations of the same input that are assuitable in the given context.
Reiter and Sripada(2002) argue that while we should take advantageof large corpora in NLG, we also need to take carethat we do not introduce errors by learning fromincorrect data present in corpora.In order to better understand what makes goodevaluation data (and metrics), we designed and im-plemented an experiment in which human judgesevaluated German string realisations.
The mainaims of this experiment were: (i) to establish howmuch variation in German word order is accept-able for human judges, (ii) to find an automaticevaluation metric that mirrors the findings of thehuman evaluation, (iii) to provide detailed feed-back for the designers of the surface realisationranking model and (iv) to establish what effectpreceding context has on the choice of realisation.In this paper, we concentrate on points (i) and (iv).The remainder of the paper is structured as fol-lows: In Section 2 we outline the realisation rank-ing system that provided the data for the experi-ment.
In Section 3 we outline the design of theexperiment and in Section 4 we present our find-ings.
In Section 5 we relate this to other work andfinally we conclude in Section 6.2 A Realisation Ranking System forGermanWe take the realisation ranking system for Germandescribed in Cahill et al (2007) and present theoutput to human judges.
One goal of this seriesof experiments is to examine whether the results112based on automatic evaluation metrics publishedin that paper are confirmed in an evaluation by hu-mans.
Another goal is to collect data that will al-low us and other researchers1 to explore more fine-grained and reliable automatic evaluation metricsfor realisation ranking.The system presented by Cahill et al (2007)ranks the strings generated by a hand-craftedbroad-coverage Lexical Functional Grammar(Bresnan, 2001) for German (Rohrer and Forst,2006) on the basis of a given input f-structure.In these experiments, we use f-structures fromtheir held-out and test sets, of which 96% canbe associated with surface realisations by thegrammar.
F-structures are attribute-value ma-trices representing grammatical functions andmorphosyntactic features; roughly speaking,they are predicate-argument structures.
In LFG,f-structures are assumed to be a crosslinguisticallyrelatively parallel syntactic representation level,alongside the more surface-oriented c-structures,which are context-free trees.
Figure 1 showsthe f-structure2 associated with TIGER Corpussentence 8609, glossed in (1), as well as the 4string realisations that the German LFG generatesfrom this f-structure.
The LFG is reversible,i.e.
the same grammar is used for parsing as forgeneration.
It is a hand-crafted grammar, andhas been carefully constructed to only parse (andtherefore generate) grammatical strings.3(1) WilliamsWilliamswarwasininderthebritischenBritishPolitikpoliticsa?u?erstextremelyumstritten.controversial.
?Williams was extremely controversial in Britishpolitics.
?The ranker consists of a log-linear model thatis based on linguistically informed structural fea-tures as well as a trigram language model, whose1The data is available for download fromhttp://www.ims.uni-stuttgart.de/projekte/pargram/geneval/data/2Note that only grammatical functions are displayed;morphosyntactic features are omitted due to space con-straints.
Also note that the discourse function TOPIC wasignored in generation.3A ranking mechanism based on so-called optimalitymarks can lead to a certain ?asymmetry?
between parsing andgeneration in the sense that not all sentences that can be as-sociated with a certain f-structure are necessarily generatedfrom this same f-structure.
E.g.
the sentence Williams wara?u?erst umstritten in der britischen Politik.
can be parsedinto the f-structure in Figure 1, but it is not generated becausean optimality mark penalizes the extraposition of PPs to theright of a clause.
Only few optimality marks were used in theprocess of generating the data for our experiments, so that thebias they introduce should not be too noticeable.score is integrated into the model simply as an ad-ditional feature.
The log-linear model is trained oncorpus data, in this case sentences from the TIGERCorpus (Brants et al, 2002), for which f-structuresare available; the observed corpus sentences areconsidered as references whose probability is tobe maximised during the training process.The output of the realisation ranker is evalu-ated in terms of exact match and BLEU score,both measured against the actually observed cor-pus sentences.
In addition to the figures achievedby the ranker, the corresponding figures achievedby the employed trigram language model on itsown are given as a baseline, and the exact matchfigure of the best possible string selection is givenas an upper bound.4 We summarise these figuresin Table 1.Exact Match BLEU scoreLanguage model 27% 0.7306Log-linear model 37% 0.7939Upper bound 62% ?Table 1: Results achieved by trigram LM rankerand log-linear model ranker in Cahill et al (2007)By means of these figures, Cahill et al (2007)show that a log-linear model based on structuralfeatures and a language model score performs con-siderably better realisation ranking than just a lan-guage model.
In our experiments, presented in de-tail in the following section, we examine whetherhuman judges confirm this and how natural and/oracceptable the selection performed by the realisa-tion ranker under consideration is for German na-tive speakers.3 Experiment DesignThe experiment was divided into three parts.
Eachpart took between 30 and 45 minutes to complete,and participants were asked to leave some time(e.g.
a week) between each part.
In total, 24 par-ticipants completed the experiment.
All were na-tive German speakers (mostly from South-WesternGermany) and almost all had a linguistic back-ground.
Table 2 gives a breakdown of the itemsin each part of the experiment.54The observed corpus sentence can be (re)generated fromthe corresponding f-structure for only 62% of the sentencesused, usually because of differences in punctuation.
Hencethis exact match upper bound.
An upper bound in termsof BLEU score cannot be computed because BLEU score iscomputed on entire corpora rather than individual sentences.5Experiments 3a and 3b contained the same items as ex-periments 1a and 1b.113"Williams war in der britischen Politik ?u?erst umstritten.
"'sein<[378:umstritten]>[1:Williams]'PRED'Williams'PRED1SUBJ'umstritten<[1:Williams]>'PRED[1:Williams]SUBJ'?u?erst'PRED274ADJUNCT378XCOMP-PRED'in<[115:Politik]>'PRED'Politik'PRED'britisch<[115:Politik]>'PRED[115:Politik]SUBJ171ADJUNCT'die'PREDDETSPEC115OBJ88ADJUNCT[1:Williams]TOPIC65Williams war in der britischen Politik a?u?erst umstritten.In der britischen Politik war Williams a?u?erst umstritten.
?Au?erst umstritten war Williams in der britischen Politik.
?Au?erst umstritten war in der britischen Politik Williams.Figure 1: F-structure associated with (1) and strings generated from it.Exp 1a Exp 1b Exp 2Num.
items 44 52 41Avg.
sent length 14.4 12.1 9.4Table 2: Statistics for each experiment part3.1 Part 1The aim of part 1 of the experiment was twofold.First, to identify the relative rankings of the sys-tems evaluated in Cahill et al (2007) according tothe human judges, and second to evaluate the qual-ity of the strings as chosen by the log-linear modelof Cahill et al (2007).
To these ends, part 1 wasfurther subdivided into two tasks: 1a and b.Task 1a: During the first task, participants werepresented with alternative realisations for an inputf-structure (but not shown the original f-structure)and asked to rank them in order of how naturalsounding they were, 1 being the best and 3 be-ing the worst.6 Each item contained three alter-natives, (i) the original string found in TIGER, (ii)the string chosen as most likely by the trigram lan-guage model, and (iii) the string chosen as mostlikely by the log-linear model.
Only items whereeach system chose a different alternative were cho-sen from the evaluation data of Cahill et al (2007).The three alternatives were presented in randomorder for each item, and the items were presentedin random order for each participant.
Some itemswere presented randomly to participants more than6Joint rankings were not allowed, i.e.
the participantswere forced to make strict ranking decisions, and in hindsightthis may have introduced some noise into the data.once as a sanity check, and in total for Part 1a, par-ticipants made 52 ranking judgements on 44 items.Figure 2 shows a screen shot of what the partici-pant was presented with for this task.Task 1b: In the second task of part 1, partic-ipants were presented with the string chosen bythe log-linear model as being the most likely andasked to evaluate it on a scale from 1 to 5 on hownatural sounding it was, 1 being very unnaturalor marked and 5 being completely natural.
Fig-ure 3 shows a screen shot of what the participantsaw during the experiment.
Again some randomitems were presented to the participant more thanonce, and the items themselves were presented inrandom order.
In total, the participants made 58judgements on 52 items.3.2 Part 2In the second part of the experiment, participantswere presented between 4 and 8 alternative sur-face realisations for an input f-structure, as wellas some preceding context.
This preceding con-text was automatically determined using informa-tion from the export release of the TIGER treebankand was not hand-checked for relevance.7 The par-ticipants were then asked to choose the realisationthat they felt fit best given the preceding sentences.7The export release of the TIGER treebank includes anarticle ID for each sentence.
Unfortunately, this is not com-pletely reliable for determining relevant context, since an ar-ticle can also contain several short news snippets which arecompletely unrelated.
Paragraph boundaries are not marked.This leads to some noise, which unfortunately is difficult tomeasure objectively114Figure 2: Screenshot of Part 1a of the ExperimentFigure 3: Screenshot of Part 1b of the ExperimentTotal AverageRank 1 Rank 2 Rank 3 RankOriginal String 817 366 65 1.40LL String 303 593 352 2.04LM String 128 289 831 2.56Table 3: Task 1a: Ranks for each systemThe items were presented in random order, and thelist of alternatives were presented in random orderto each participant.
Some items were randomlypresented more than once, resulting in 50 judge-ments on 41 items.
Figure 4 shows a screen shotof what the participant saw.3.3 Part 3Part 3 of the experiment was identical to Part 1,except that now, rather than the participants beingpresented with sentences in isolation, they weregiven some preceding context.
The context wasdetermined automatically, in the same way as inPart 2.
The items themselves were the same as inPart 1.
The aim of this part of the experiment wasto see what effect preceding context had on judge-ments.4 ResultsIn this section we present the result and analysisof the experiments outlined above.4.1 How good were the strings?The data collected in Experiment 1a showed theoverall human relative ranking of the three sys-tems.
We calculate the total numbers of eachrank for each system.
Table 3 summarises the re-sults.
The original string is the string found in theFigure 5: Task 1b: Naturalness scores for stringschosen by log-linear model, 1=worstTIGER Corpus, the LM String is the string cho-sen as being most likely by the trigram languagemodel and the LL String is the string chosen asbeing most likely by the log-linear model.Table 3 confirms the overall relative rankingsof the three systems as determined using BLEUscores.
The original TIGER strings are ranked best(average 1.4), the strings chosen by the log-linearmodel are ranked better than the strings chosen bythe language model (average 2.65 vs 2.04).In Experiment 1b, the aim was to find out howacceptable the strings chosen by the log-linearmodel were, although they were not the same asthe original string.
Figure 5 summarises the data.The graph shows that the majority of strings cho-sen by the log-linear model ranked very highly onthe naturalness scale.4.2 Did the human judges agree with theoriginal authors?In Experiment 2, the aim was to find out how of-ten the human judges chose the same string as theoriginal author (given alternatives generated by theLFG grammar).
Most items had between 4 and 6alternative strings.
In 70% of all items, the humanjudges chose the same string as the original au-thor.
However, the remaining 30% of the time, thehuman judges picked an alternative as being the115Figure 4: Screenshot of Part 2 of the Experimentmost fitting in the given context.8 This suggeststhat there is quite some variation in what nativeGerman speakers will accept, but that this varia-tion is by no means random, as indicated by 70%of choices being the same string as the original au-thor?s.Figure 6 shows for each bin of possible alterna-tives, the percentage of items with a given num-ber of choices made.
For example, for the itemswith 4 possible alternatives, over 70% of the time,the judges chose between only 2 of them.
For theitems with 5 possible alternatives, in 10% of thoseitems the human judges chose only 1 of those al-ternatives; in 30% of cases, the human judges allchose the same 2 solutions, and for the remain-ing 60% they chose between only 3 of the 5 pos-sible alternatives.
These figures indicate that al-though judges could not always agree on one beststring, often they were only choosing between 2 or3 of the possible alternatives.
This suggests that,on the one hand, native speakers do accept quitesome variation, but that, on the other hand, thereare clearly factors that make certain realisation al-ternatives more preferable than others.Figure 6: Exp 2: Number of Alternatives Chosen8Recall that almost all strings presented to the judges weregrammatical.The graph in Figure 6 shows that only in twocases did the human judges choose from amongall possible alternatives.
In one case, there were 4possible alternatives and in the other 6.
The origi-nal sentence that had 4 alternatives is given in (2).The four alternatives that participants were askedto choose from are given in Table 4, with the fre-quency of each choice.
The original sentence thathad 6 alternatives is given in (3).
The six alterna-tives generated by the grammar and the frequen-cies with which they were chosen is given in Table5.
(2) DieTheBrandursachecause of firebliebremainedzuna?chstinitiallyunbekannt.unknown.
?The cause of the fire remained unknown initially.
?Alternative Freq.Zuna?chst blieb die Brandursache unbekannt.
2Die Brandursache blieb zuna?chst unbekannt.
24Unbekannt blieb die Brandursache zuna?chst.
1Unbekannt blieb zuna?chst die Brandursache.
1Table 4: The 4 alternatives given by the grammarfor (2) and their frequenciesTables 4 and 5 tell different stories.
On the onehand, although each of the 4 alternatives was cho-sen at least once from Table 4, there is a clear pref-erence for one string (and this is also the origi-nal string from the TIGER Corpus).
On the otherhand, there is no clear preference9 for any one ofthe alternatives in Table 5, and, in fact, the alterna-tive that was selected most frequently by the par-ticipants is not the original string.
Interestingly,out of the 41 items presented to participants, theoriginal string was chosen by the majority of par-ticipants in 36 cases.
Again, this confirms thehypothesis that there is a certain amount of ac-ceptable variation for native speakers but there areclear preferences for certain strings over others.9Although it is clear that alternative 2 is dispreferred.116(3) DieTheUnternehmensgruppegroup of companiesTengelmannTengelmannfo?rdertassistsmitwitheinemasechsstelligen6-figureBetragsumdietheArbeitworkiminbrandenburgischenof-BrandenburgBiospha?renreservatbiosphere reserveSchorfheide.Schorfheide.
?The Tengelmann group of companies is supporting the work at the biosphere reserve in Schorfheide, Brandenburg,with a 6-figure sum.
?Alternative Freq.Mit einem sechsstelligen Betrag fo?rdert die Unternehmensgruppe Tengelmann die Arbeit im brandenburgischenBiospha?renreservat Schorfheide.
7Mit einem sechsstelligen Betrag fo?rdert die Arbeit im brandenburgischen Biospha?renreservat Schorfheidedie Unternehmensgruppe Tengelmann.
1Die Arbeit im brandenburgischen Biospha?renreservat Schorfheide fo?rdert die Unternehmensgruppe Tengelmannmit einem sechsstelligen Betrag.
4Die Arbeit im brandenburgischen Biospha?renreservat Schorfheide fo?rdert mit einem sechsstelligen Betragdie Unternehmensgruppe Tengelmann.
5Die Unternehmensgruppe Tengelmann fo?rdert die Arbeit im brandenburgischen Biospha?renreservat Schorfheidemit einem sechsstelligen Betrag.
5Die Unternehmensgruppe Tengelmann fo?rdert mit einem sechsstelligen Betrag die Arbeit im brandenburgischenBiospha?renreservat Schorfheide.
5Table 5: The 6 alternatives given by the grammar for (3) and their frequencies4.3 Effects of contextAs explained in Section 3.1, Part 3 of our exper-iment was identical to Part 1, except that the par-ticipants could see some preceding context.
Theaim of this part was to investigate to what extentdiscourse factors influence the way in which hu-man judges evaluate the output of the realisationranker.
In Task 3a, we expected the original stringsto be ranked (even) higher in context than out ofcontext; consequently, the ranks of the realisationsselected by the log-linear and the language modelwould have to go down.
With respect to Task 3b,we had no particular expectation, but were just in-terested in seeing whether some preceding contextwould affect the evaluation results for the stringsselected as most probable by the log-linear modelranker in any way.Table 6 summarises the results of Task 3a.
Itshows that, at least overall, our expectation that theoriginal corpus sentences would be ranked higherwithin context than out of context was not borneout.
Actually, they were ranked a bit lower thanthey were when presented in isolation, and theonly realisations that are ranked slightly higheroverall are the ones selected by the trigram LM.The overall results of Task 3b are presented inFigure 7.
Interestingly, although we did not ex-pect any particular effect of preceding context onthe way the participants would rate the realisa-tions selected by the log-linear model, the natu-ralness scores were higher in the condition withcontext (Task 3b) than in the one without contextTotal AverageRank 1 Rank 2 Rank 3 RankOriginal String 810 365 71 1.41(-7) (-1) (+6) (+0.01)LL String 274 615 357 2.07(-29) (+22) (+5) (+0.03)LM String 162 266 818 2.53(+34) (-23) (-13) (-0.03)Table 6: Task 3a: Ranks for each system (com-pared to ranks in Task 1a)(Task 1b).
One explanation might be that sen-tences in some sort of default order are generallyrated higher in context than out of context, simplybecause the context makes sentences less surpris-ing.Since, contrary to our expectations, we couldnot detect a clear effect of context in the overall re-sults of Task 3a, we investigated how the averageranks of the three alternatives presented for indi-vidual items differ between Task 1a and Task 3a.An example of an original corpus sentence whichmany participants ranked higher in context than inisolation is given in (4a.).
The realisations selectedby the the log-linear model and the trigram LM aregiven in (4b.)
and (4c.)
respectively, and the con-text shown to the participants is given above thesealternatives.
We believe that the context has thiseffect because it prepares the reader for the struc-ture with the sentence-initial predicative partici-ple entscheidend; usually, these elements appearrather in clause-final position.In contrast, (5a) is an example of a corpus117(4) -2 BetroffenConcernedsindaredietheAntibabypillencontraceptive pillsFemovan,Femovan,Lovelle,Lovelle,[...][...],undandDimirel.Dimirel.-1 DasTheBundesinstitutfederal instituteschlie?texcludesnichtnotaus, da?thatsich dietheThrombose-Warnungthrombosis warningalsasgrundlosunfoundederweisenturn outko?nnte.could.a.
EntscheidendDecisiveseiisdiethe[...][...]abschlie?endefinalBewertung,evaluation,sagtesaidJu?rgenJu?rgenBeckmannBeckmannvomof theInstitutinstitutedemtheZDF.ZDF.b.
Die [...] abschlie?ende Bewertung sei entscheidend, sagte Ju?rgen Beckmann vom Institut dem ZDF.c.
Die [...] abschlie?ende Bewertung sei entscheidend, sagte dem ZDF Ju?rgen Beckmann vom Institut.
(5) -2 ImIn thekonkretenconcreteFallcasedarfmaydertheKurdeKurdallerdingshowevertrotzdespitedertheEntscheidungdecisionderof theBundesrichterfederal judgesnichtnotintodietheTu?rkeiTurkeyabgeschobendeportedwerden,beweilbecauseihmhimdorttherenachaccording todentheFeststellungenconclusionsderof theVorinstanzcourt of lower instancepolitischepoliticalVerfolgungpersecutiondroht.threatens.-1 EsItbestehtexistsAbschiebeschutzdeportation protectionnachaccording todemtheAusla?ndergesetz.foreigner law.a.
DerThe9.9thSenatsenate[...][...]a?u?erteexpressedsichitselfininseineritsEntscheidungdecisionnichtnotzurto theVerfassungsgema?
?heitconstitutionalityderof theDrittstaatenregelung.third-country rule.b.
In seiner Entscheidung a?u?erte sich der 9.
Senat [...] nicht zur Verfassungsgema?
?heit der Drittstaatenregelung.c.
Der 9.
Senat [...] a?u?erte sich in seiner Entscheidung zur Verfassungsgema?
?heit der Drittstaatenregelung nicht.Figure 7: Tasks 1b and 3b: Naturalness scoresfor strings chosen by log-linear model, presentedwithout and with contextsentence which our participants tended to ranklower in context than in isolation.
Actually, thehuman judges preferred the realisation selectedby the trigram LM to the original sentence andthe realisation chosen by the log-linear model inboth conditions, but this preference was even re-inforced when context was available.
One expla-nation might be that the two preceding sentencesare precisely about the decision to which the ini-tial phrase of variant (5b) refers, which ensures asmooth flow of the discourse.4.4 Inter-Annotator AgreementWe measure two types of annotator agreement.First we measure how well each annotator agreeswith him/herself.
This is done by evaluating whatpercentage of the time an annotator made the samechoice when presented with the same item choices(recall that as described in Section 3, a number ofitems were presented randomly more than once toeach participant).
The results are given in Table 7.The results show that in between 70% and 74% ofcases, judges make the same decision when pre-sented with the same data.
We found this to be asurprisingly low number and think that it is mostlikely due to the acceptable variation in word or-der for speakers.
Another measure of agreementis how well the individual participants agree witheach other.
In order to establish this, we cal-culate an average Spearman?s correlation coeffi-cient (non-parametric Pearson?s correlation coef-ficient) between each participant for each experi-ment.
The results are summarised in Table 8.
Al-though these figures indicate a high level of inter-annotator agreement, more tests are required to es-tablish exactly what these figures mean for eachexperiment.5 Related WorkThe work that is most closely related to what ispresented in this paper is that of Velldal (2008).
In118Experiment Agreement (%)Part 1a 77.43Part 1b 71.05Part 2 74.32Part 3a 72.63Part 3b 70.89Table 7: How often did a participant make thesame choice?Experiment Spearman coefficientPart 1a 0.62Part 1b 0.60Part 2 0.58Part 3a 0.61Part 3b 0.51Table 8: Inter-Annotator Agreement for each ex-perimenthis thesis several models of realisation ranking arepresented and evaluated against the original cor-pus text.
Chapter 8 describes a small human-basedexperiment, where 7 native English speakers rankthe output of 4 systems.
One system is the orig-inal text, another is a randomly chosen baseline,another is a string chosen by a log-linear modeland the fourth is one chosen by a language model.Joint rankings were allowed.
The results presentedin Velldal (2008) mirror our findings in Exper-iments 1a and 3a, that native speakers rank theoriginal strings higher than the log-linear modelstrings which are ranked higher than the languagemodel strings.
In both cases, the log-linear mod-els include the language model score as a featurein the log-linear model.
Nakanishi et al (2005) re-port that they achieve the best BLEU scores whenthey do not include the language model score intheir log-linear model, but they also admit thattheir language model was not trained on enoughdata.Belz and Reiter (2006) carry out a comparisonof automatic evaluation metrics against human do-main experts and human non-experts in the do-main of weather forecast statements.
In their eval-uations, the NIST score correlated more closelythan BLEU or ROUGE to the human judgements.They conclude that more than 4 reference texts areneeded for automatic evaluation of NLG systems.6 Conclusion and Outlook to FutureWorkIn this paper, we have presented a human-basedexperiment to evaluate the output of a realisationranking system for German.
We evaluated theoriginal corpus text, and strings chosen by a lan-guage model and a log-linear model.
We foundthat, at a global level, the human judgements mir-rored the relative rankings of the three system ac-cording to the BLEU score.
In terms of natural-ness, the strings chosen by the log-linear modelwere generally given 4 or 5, indicating that al-though the log-linear model might not choose thesame string as the original author had written, thestrings it was choosing were mostly very naturalstrings.When presented with all alternatives generatedby the grammar for a given input f-structure, thehuman judges chose the same string as the origi-nal author 70% of the time.
In 5 out of 41 cases,the majority of judges chose a string other thanthe original string.
These figures show that nativespeakers accept some variation in word order, andso caution should be exercised when using corpus-derived reference data.
The observed acceptablevariation was often linked to information struc-tural considerations, and further experiments willbe carried out to investigate this relationship be-tween word order and information structure.In examining the effect of preceding context, wefound that overall context had very little effect.
Atthe level of individual sentences, however, cleartendencies were observed, but there were somesentences which were judged better in context andothers which were ranked lower.
This again indi-cates that corpus-derived reference data should beused with caution.An obvious next step is to examine how wellautomatic metrics correlate with the human judge-ments collected, not only at an individual sen-tence level, but also at a global level.
This can bedone using statistical techniques to correlate thehuman judgements with the scores from the auto-matic metrics.
We will also examine the sentencesthat were consistently judged to be of poor quality,so that we can provide feedback to the developersof the log-linear model in terms of possible addi-tional features for disambiguation.AcknowledgmentsWe are extremely grateful to all of our participantsfor taking part in this experiment.
This work waspartly funded by the Collaborative Research Cen-tre (SFB 732) at the University of Stuttgart.119ReferencesSrinivas Bangalore, Owen Rambow, and Steve Whit-taker.
2000.
Evaluation metrics for generation.
InProceedings of the First International Natural Lan-guage Generation Conference (INLG2000), pages1?8, Mitzpe Ramon, Israel.Anja Belz and Ehud Reiter.
2006.
Comparing auto-matic and human evaluation of NLG systems.
InProceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 313?320, Trento, Italy.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories, Sozopol, Bulgaria.Joan Bresnan.
2001.
Lexical-Functional Syntax.Blackwell, Oxford.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Stochastic Realisation Ranking for a Free Word Or-der Language.
In Proceedings of the Eleventh Eu-ropean Workshop on Natural Language Generation,pages 17?24, Saarbru?cken, Germany, June.
DFKIGmbH.
Document D-07-01.Charles Callaway.
2003.
Evaluating Coverage forLarge Symbolic NLG Grammars.
In Proceedingsof the 18th International Joint Conference on Artifi-cial Intelligence (IJCAI 2003), pages 811?817, Aca-pulco, Mexico.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsu-jii.
2005.
Probabilistic models for disambiguationof an HPSG-based chart generator.
In Proceedingsof IWPT 2005.Ehud Reiter and Somayajulu Sripada.
2002.
ShouldCorpora Texts Be Gold Standards for NLG?
In Pro-ceedings of INLG-02, pages 97?104, Harriman, NY.Christian Rohrer and Martin Forst.
2006.
Improvingcoverage and parsing quality of a large-scale LFGfor German.
In Proceedings of the Language Re-sources and Evaluation Conference (LREC-2006),Genoa, Italy.Erik Velldal and Stephan Oepen.
2006.
Statisticalranking in tactical generation.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, Sydney, Australia.Erik Velldal.
2008.
Empirical Realization Ranking.Ph.D.
thesis, University of Oslo.120
