Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 17: All-words Word Sense Disambiguationon a Specific DomainEneko Agirre, Oier Lopez de LacalleIXA NLP groupUBCDonostia, Basque Country{e.agirre,oier.lopezdelacalle}@ehu.esChristiane FellbaumDepartment of Computer SciencePrinceton UniversityPrinceton, USAfellbaum@princeton.eduShu-Kai HsiehDepartment of EnglishNational Taiwan Normal UniversityTaipei, Taiwanshukai@ntnu.edu.twMaurizio TesconiIITCNRPisa, Italymaurizio.tesconi@iit.cnr.itMonica MonachiniILCCNRPisa, Italymonica.monachini@ilc.cnr.itPiek Vossen, Roxanne SegersFaculteit der LetterenVrije Universiteit AmsterdamAmsterdam, Netherlandsp.vossen@let.vu.nl,roxane.segers@gmail.comAbstractDomain portability and adaptation of NLPcomponents and Word Sense Disambigua-tion systems present new challenges.
Thedifficulties found by supervised systems toadapt might change the way we assess thestrengths and weaknesses of supervisedand knowledge-based WSD systems.
Un-fortunately, all existing evaluation datasetsfor specific domains are lexical-samplecorpora.
This task presented all-wordsdatasets on the environment domain forWSD in four languages (Chinese, Dutch,English, Italian).
11 teams participated,with supervised and knowledge-based sys-tems, mainly in the English dataset.
Theresults show that in all languages the par-ticipants where able to beat the most fre-quent sense heuristic as estimated fromgeneral corpora.
The most successful ap-proaches used some sort of supervision inthe form of hand-tagged examples fromthe domain.1 IntroductionWord Sense Disambiguation (WSD) competitionshave focused on general domain texts, as attestedin previous Senseval and SemEval competitions(Kilgarriff, 2001; Mihalcea et al, 2004; Snyderand Palmer, 2004; Pradhan et al, 2007).
Spe-cific domains pose fresh challenges to WSD sys-tems: the context in which the senses occur mightchange, different domains involve different sensedistributions and predominant senses, some wordstend to occur in fewer senses in specific domains,the context of the senses might change, and newsenses and terms might be involved.
Both super-vised and knowledge-based systems are affectedby these issues: while the first suffer from differ-ent context and sense priors, the later suffer fromlack of coverage of domain-related words and in-formation.The main goal of this task is to provide a mul-tilingual testbed to evaluate WSD systems whenfaced with full-texts from a specific domain.
Alldatasets and related information are publicly avail-able from the task websites1.This task was designed in the context of Ky-oto (Vossen et al, 2008)2, an Asian-Europeanproject that develops a community platform formodeling knowledge and finding facts across lan-guages and cultures.
The platform operates as aWiki system with an ontological support that so-cial communities can use to agree on the mean-ing of terms in specific domains of their interest.Kyoto focuses on the environmental domain be-cause it poses interesting challenges for informa-tion sharing, but the techniques and platforms are1http://xmlgroup.iit.cnr.it/SemEval2010/and http://semeval2.fbk.eu/2http://www.kyoto-project.eu/75independent of the application domain.The paper is structured as follows.
We firstpresent the preparation of the data.
Section 3 re-views participant systems and Section 4 the re-sults.
Finally, Section 5 presents the conclusions.2 Data preparationThe data made available to the participants in-cluded the test set proper, and background texts.Participants had one week to work on the test set,but the background texts where provided monthsearlier.2.1 Test datasetsThe WSD-domain comprises comparable all-words test corpora on the environment domain.Three texts were compiled for each language bythe European Center for Nature Conservation3andWorldwide Wildlife Forum4.
They are documentswritten for a general but interested public and in-volve specific terms from the domain.
The docu-ment content is comparable across languages.
Ta-ble 1 shows the numbers for the datasets.Although the original plan was to annotate mul-tiword terms, and domain terminology, due to timeconstraints we focused on single-word nouns andverbs.
The test set clearly marked which werethe words to be annotated.
In the case of Dutch,we also marked components of single-word com-pounds.
The format of the test set followed that ofprevious all-word exercises, which we extended toaccommodate Dutch compounds.
For further de-tails check the datasets in the task website.The sense inventory was based on publiclyavailable wordnets of the respective languages(see task website for details).
The annotation pro-cedure involved double-blind annotation by ex-perts plus adjudication, which allowed us to alsoprovide Inter Annotator Agreement (IAA) figuresfor the dataset.
The procedure was carried out us-ing KAFnotator tool (Tesconi et al, 2010).
Dueto limitations in resources and time, the Englishdataset was annotated by a single expert annota-tor.
For the rest of languages, the agreement wasvery good, as reported in Table 1.Table 1 includes the results of the random base-line, as an indication of the polysemy in eachdataset.
Average polysemy is highest for English,and lowest for Dutch.3http://www.ecnc.org4http://www.wwf.orgTotal Noun Verb IAA RandomChinese 3989 754 450 0.96 0.321Dutch 8157 997 635 0.90 0.328English 5342 1032 366 n/a 0.232Italian 8560 1340 513 0.72 0.294Table 1: Dataset numbers, including number oftokens, nouns and verbs to be tagged, Inter-Annotator Agreement (IAA) and precision of ran-dom baseline.Documents WordsChinese 58 455359Dutch 98 21089English 113 2737202Italian 27 240158Table 2: Size of the background data.2.2 Background dataIn addition to the test datasets proper, we also pro-vided additional documents on related subjects,kindly provided by ECNC and WWF.
Table 2shows the number of documents and words madeavailable for each language.
The full list with theurls of the documents are available from the taskwebsite, together with the background documents.3 ParticipantsEleven participants submitted more than thirtyruns (cf.
Table 3).
The authors classified their runsinto supervised (S in the tables, three runs), weaklysupervised (WS, four runs), unsupervised (no runs)and knowledge-based (KB, the rest of runs)5.
Onlyone group used hand-tagged data from the domain,which they produced on their own.
We will brieflyreview each of the participant groups, ordered fol-lowing the rank obtained for English.
They all par-ticipated on the English task, with one exceptionas noted below, so we report their rank in the En-glish task.
Please refer to their respective paper inthese proceedings for more details.CFILT: They participated with a domain-specific knowledge-based method based on Hop-field networks (Khapra et al, 2010).
They firstidentify domain-dependant words using the back-ground texts, use a graph based on hyponyms inWordNet, and a breadth-first search to select themost representative synsets within domain.
In ad-dition they added manually disambiguated aroundone hundred examples from the domain as seeds.5Note that boundaries are slippery.
We show the classifi-cations as reported by the authors.76EnglishRank Participant System ID Type P R R nouns R verbs1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.0472 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.0433 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.0414 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.0415 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.0436 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.0457 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.0488 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.0469 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.04410 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.03911 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.03912 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.04613 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.04314 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.04315 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.04416 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.04417 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.03818 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.04919 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.04320 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.04421 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.04322 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.04723 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.03724 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.04925 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.03026 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.03727 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.04428 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.04029 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041- - Random baseline - 0.232 0.232 0.253 0.172ChineseRank Participant System ID Type P R R nouns R verbs- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.0391 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.0392 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038- - Random baseline - 0.321 0.321 0.326 0.3124 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.0383 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.0405 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031DutchRank Participant System ID Type P R R nouns R verbs1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.0342 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.0253 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033- - Random baseline - 0.328 0.328 0.350 0.293ItalianRank Participant System ID Type P R R nouns R verbs1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.0382 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.0353 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035- - Random baseline - 0.294 0.294 0.308 0.257Table 3: Overall results for the domain WSD datasets, ordered by recall.This is the only group using hand-tagged datafrom the target domain.
Their best run ranked 1st.IIITTH: They presented a personalized PageR-ank algorithm over a graph constructed fromWordNet similar to (Agirre and Soroa, 2009),with two variants.
In the first (IIITH1), the verticesof the graph are initialized following the rank-ing scores obtained from predominant senses as in(McCarthy et al, 2007).
In the second (IIITH2),the graph is initialized with keyness values as in770.3 0.35 0.4 0.45 0.5 0.55Rel.
CliquesRel.
Sem.
Trees-2Rel.
Sem.
TreesNLEL-WSDRACAI-Lexical-ChainsNLEL-WSD-PDBBLC20BGKyoto-1UCF-WS-domain.noPropersIIITH2-d.r.l.ppv.05IIITH1-d.l.ppv.05RACAI-2MFS-BOWIIITH1-d.l.baseline.05IIITH2-d.r.l.baseline.05UCF-WS-domainHIT-CIR-DMFSUCF-WSRACAI-MFSTreematch-3Kyoto-2Treematch-2TreematchCFILT-3BLC20SCBLC20SCBGIIITH2-d.l.ppr.05IIITH1-d.l.ppr.05CFILT-1CFILT-2MFSFigure 1: Plot for all the systems which participated in English domain WSD.
Each point correspondto one system (denoted in axis Y) according each recall and confidence interval (axis X ).
Systems areordered depending on their rank.
(Rayson and Garside, 2000).
Some of the runsuse sense statistics from SemCor, and have beenclassified as weakly supervised.
They submitted atotal of six runs, with the best run ranking 3rd.BLC20(SC/BG/SCBG): This system is super-vised.
A Support Vector Machine was trained us-ing the usual set of features extracted from con-text and the most frequent class of the target word.Semantic class-based classifiers were built fromSemCor (Izquierdo et al, 2009), where the classeswere automatically obtained exploiting the struc-tural properties of WordNet.
Their best run ranked5th.Treematch: This system uses a knowledge-based disambiguation method that requires a dic-tionary and untagged text as input.
A previouslydeveloped system (Chen et al, 2009) was adaptedto handle domain specific WSD.
They built adomain-specific corpus using words mined fromrelevant web sites (e.g.
WWF and ECNC) asseeds.
Once parsed the corpus, the used the de-pendency knowledge to build a nodeset that wasused for WSD.
The background documents pro-vided by the organizers were only used to test howexhaustive the initial seeds were.
Their best runranked 8th.Kyoto: This system participated in all fourlanguages, with a free reimplementation ofthe domain-specific knowledge-based method forWSD presented in (Agirre et al, 2009).
Ituses a module to construct a distributional the-saurus, which was run on the background text, anda disambiguation module based on PersonalizedPageRank over wordnet graphs.
Different Word-Net were used as the LKB depending on the lan-guage.
Their best run ranked 10th.
Note that thisteam includes some of the organizers of the task.A strict separation was kept, in order to keep thetest dataset hidden from the actual developers ofthe system.RACAI: This participant submitted three differ-ent knowledge-based systems.
In the first, they usethe mapping to domains of WordNet (version 2.0)in order to constraint the domains of the contentwords of the test text.
In the second, they chooseamong senses using lexical chains (Ion and Ste-fanescu, 2009).
The third system combines theprevious two.
Their best system ranked 12th.HIT-CIR: They presented a knowledge-basedsystem which estimates predominant sense fromraw test.
The predominant senses were calculatedwith the frequency information in the providedbackground text, and automatically constructed78thesauri from bilingual parallel corpora.
The sys-tem ranked 14.UCFWS: This knowledge-based WSD systemwas based on an algorithm originally described in(Schwartz and Gomez, 2008), in which selectorsare acquired from the Web via searching with lo-cal context of a given word.
The sense is cho-sen based on the similarity or relatedness betweenthe senses of the target word and various typesof selectors.
In some runs they include predom-inant senses(McCarthy et al, 2007).
The best runranked 13th.NLEL-WSD(-PDB): The system used for theparticipation is based on an ensemble of differentmethods using fuzzy-Borda voting.
A similar sys-tem was proposed in SemEval-2007 task-7 (Bus-caldi and Rosso, 2007).
In this case, the com-ponent method used where the following ones:1) Most Frequent Sense from SemCor; 2) Con-ceptual Density ; 3) Supervised Domain RelativeEntropy classifier based on WordNet Domains;4) Supervised Bayesian classifier based on Word-Net Domains probabilities; and 5) UnsupervisedKnownet-20 classifiers.
The best run ranked 24th.UMCC-DLSI (Relevant): The team submittedthree different runs using a knowledge-based sys-tem.
The first two runs use domain vectors andthe third is based on cliques, which measure howmuch a concept is correlated to the sentence byobtaining Relevant Semantic Trees.
Their best runranked 27th.
(G)HR: They presented a Knowledge-basedWSD system, which make use of two heuristicrules (Li et al, 1995).
The system enriched theChinese WordNet by adding semantic relations forEnglish domain specific words (e.g.
ecology, en-vironment).
When in-domain senses are not avail-able, the system relies on the first sense in the Chi-nese WordNet.
In addition, they also use sensedefinitions.
They only participated in the Chinesetask, with their best system ranking 1st.4 ResultsThe evaluation has been carried out using the stan-dard Senseval/SemEval scorer scorer2 as in-cluded in the trial dataset, which computes preci-sion and recall.
Table 3 shows the results in eachdataset.
Note that the main evaluation measure isrecall (R).
In addition we also report precision (P)and the recall for nouns and verbs.
Recall mea-sures are accompanied by a 95% confidence in-terval calculated using bootstrap resampling pro-cedure (Noreen, 1989).
The difference betweentwo systems is deemed to be statistically signifi-cant if there is no overlap between the confidenceintervals.
We show graphically the results in Fig-ure 1.
For instance, the differences between thehighest scoring system and the following four sys-tems are not statistically significant.
Note that thismethod of estimating statistical significance mightbe more strict than other pairwise methods.We also include the results of two baselines.The random baseline was calculated analytically.The first sense baseline for each language wastaken from each wordnet.
The first sense baselinein English and Chinese corresponds to the mostfrequent sense, as estimated from out-of-domaincorpora.
In Dutch and Italian, it followed the in-tuitions of the lexicographer.
Note that we don?thave the most frequent sense baseline from the do-main texts, which would surely show higher re-sults (Koeling et al, 2005).5 ConclusionsDomain portability and adaptation of NLP com-ponents and Word Sense Disambiguation systemspresent new challenges.
The difficulties found bysupervised systems to adapt might change the waywe assess the strengths and weaknesses of super-vised and knowledge-based WSD systems.
Withthis paper we have motivated the creation of anall-words test dataset for WSD on the environ-ment domain in several languages, and presentedthe overall design of this SemEval task.One of the goals of the exercise was to showthat WSD systems could make use of unannotatedbackground corpora to adapt to the domain andimprove their results.
Although it?s early to reachhard conclusions, the results show that in each ofthe datasets, knowledge-based systems are able toimprove their results using background text, andin two datasets the adaptation of knowledge-basedsystems leads to results over the MFS baseline.The evidence of domain adaptation of supervisedsystems is weaker, as only one team tried, and thedifferences with respect to MFS are very small.The best results for English are obtained by a sys-tem that combines a knowledge-based system withsome targeted hand-tagging.
Regarding the tech-niques used, graph-based methods over WordNetand distributional thesaurus acquisition methodshave been used by several teams.79All datasets and related information are publiclyavailable from the task websites6.AcknowledgmentsWe thank the collaboration of Lawrence Jones-Walters, AmorTorre-Marin (ECNC) and Karin de Boom (WWF), com-piling the test and background documents.
This worktask is partially funded by the European Commission (KY-OTO ICT-2007-211423), the Spanish Research Department(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-ment (BERBATEK IE09-262).ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizing pager-ank for word sense disambiguation.
In Proceedings of the12th Conference of the European Chapter of the Associa-tion for Computational Linguistics (EACL09), pages 33?41.
Association for Computational Linguistics.Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009.Knowledge-based wsd on specific domains: Performingbetter than generic supervised wsd.
In Proceedigns of IJ-CAI.
pp.
1501-1506.?.Davide Buscaldi and Paolo Rosso.
2007.
Upv-wsd : Com-bining different wsd methods by means of fuzzy bordavoting.
In Proceedings of the Fourth International Work-shop on Semantic Evaluations (SemEval-2007), pages434?437.P.
Chen, W. Ding, and D. Brown.
2009.
A fully unsupervisedword sense disambiguation method and its evaluation oncoarse-grained all-words task.
In Proceeding of the NorthAmerican Chapter of the Association for ComputationalLinguistics (NAACL09).Radu Ion and Dan Stefanescu.
2009.
Unsupervised wordsense disambiguation with lexical chains and graph-basedcontext formalization.
In Proceedings of the 4th Languageand Technology Conference: Human Language Technolo-gies as a Challenge for Computer Science and Linguistics,pages 190?194.Rub?en Izquierdo, Armando Su?arez, and German Rigau.2009.
An empirical study on class-based word sense dis-ambiguation.
In EACL ?09: Proceedings of the 12th Con-ference of the European Chapter of the Association forComputational Linguistics, pages 389?397, Morristown,NJ, USA.
Association for Computational Linguistics.Mitesh Khapra, Sapan Shah, Piyush Kedia, and PushpakBhattacharyya.
2010.
Domain-specific word sense dis-ambiguation combining corpus based and wordnet basedparameters.
In Proceedings of the 5th International Con-ference on Global Wordnet (GWC2010).A.
Kilgarriff.
2001.
English Lexical Sample Task Descrip-tion.
In Proceedings of the Second International Work-shop on evaluating Word Sense Disambiguation Systems,Toulouse, France.R.
Koeling, D. McCarthy, and J. Carroll.
2005.
Domain-specific sense distributions and predominant sense acqui-sition.
In Proceedings of the Human Language Technol-ogy Conference and Conference on Empirical Methods in6http://xmlgroup.iit.cnr.it/SemEval2010/and http://semeval2.fbk.eu/Natural Language Processing.
HLT/EMNLP, pages 419?426, Ann Arbor, Michigan.Xiaobin Li, Stan Szpakowicz, and Stan Matwin.
1995.
Awordnet-based algorithm for word sense disambiguation.In Proceedings of The 14th International Joint Conferenceon Artificial Intelligence (IJCAI95).Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-roll.
2007.
Unsupervised acquisition of predominantword senses.
Computational Linguistics, 33(4).R.
Mihalcea, T. Chklovski, and Adam Killgariff.
2004.
TheSenseval-3 English lexical sample task.
In Proceedings ofthe 3rd ACL workshop on the Evaluation of Systems for theSemantic Analysis of Text (SENSEVAL), Barcelona, Spain.Eric W. Noreen.
1989.
Computer-Intensive Methods for Test-ing Hypotheses.
John Wiley & Sons.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 task-17: Englishlexical sample, srl and all words.
In Proceedings of theFourth International Workshop on Semantic Evaluations(SemEval-2007), pages 87?92, Prague, Czech Republic.Paul Rayson and Roger Garside.
2000.
Comparing corporausing frequency profiling.
In Proceedings of the workshopon Comparing corpora, pages 1?6.Hansen A. Schwartz and Fernando Gomez.
2008.
Acquir-ing knowledge from the web to be used as selectors fornoun sense disambiguation.
In Proceedings of the TwelfthConference on Computational Natural Language Learn-ing (CONLL08).B.
Snyder and M. Palmer.
2004.
The English all-words task.In Proceedings of the 3rd ACL workshop on the Evalua-tion of Systems for the Semantic Analysis of Text (SENSE-VAL), Barcelona, Spain.M.
Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, andA.
Marchetti.
2010.
Kafnotator: a multilingual seman-tic text annotation tool.
In In Proceedings of the SecondInternational Conference on Global Interoperability forLanguage Resources.Piek Vossen, Eneko Agirre, Nicoletta Calzolari, ChristianeFellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-chini, Federico Neri, Remo Raffaelli, German Rigau,Maurizio Tescon, and Joop VanGent.
2008.
Kyoto: asystem for mining, structuring and distributing knowl-edge across languages and cultures.
In Proceedings of theSixth International Language Resources and Evaluation(LREC?08), Marrakech, Morocco, may.
European Lan-guage Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.80
