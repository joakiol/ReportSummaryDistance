Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192?197,Dublin, Ireland, August 23-24, 2014.CNRC-TMT:Second Language Writing Assistant System DescriptionCyril Goutte Michel SimardNational Research Council CanadaMultilingual Text Processing1200 Montreal Road, Ottawa, Ontario K1A 0R6, CanadaFirstName.LastName@nrc.caMarine CarpuatAbstractWe describe the system entered by theNational Research Council Canada inthe SemEval-2014 L2 writing assistanttask.
Our system relies on a standardPhrase-Based Statistical Machine Transla-tion trained on generic, publicly availabledata.
Translations are produced by takingthe already translated part of the sentenceas fixed context.
We show that translationsystems can address the L2 writing assis-tant task, reaching out-of-five word-basedaccuracy above 80 percent for 3 out of 4language pairs.
We also present a briefanalysis of remaining errors.1 IntroductionThe Semeval L2 writing assistant task simulatesthe situation of an L2 language learner trying totranslate a L1 fragment in a L2 context.
This isclearly motivated by a L2 language learning sce-nario.However, a very similar scenario can be en-countered in Computer-Aided Translation.
Trans-lation memories retrieve from a large corpus of al-ready translated documents the source segmentsthat best match a new sentence to be translated.If an exact source match is found, the correspond-ing target translation can be expected to be suit-able with little or no post-editing.
However, whenonly approximate matches are found, post-editingwill typically be required to adapt the target sideof the partially matching source segment to thesource sentence under consideration.
It is possibleto automate this process: standard string matchingalgorithms and word alignment techniques can beused to locate the parts of the source segment thatdo not match the sentence to translate, and fromc?2014, The Crown in Right of Canada.there the parts of the target segment that need tobe modified (Bic?ici and Dymetman, 2008; Simardand Isabelle, 2009; Koehn and Senellart, 2010).The task of translating a L1 fragment in L2 con-text therefore has much broader application thanlanguage learning.
This motivation also providesa clear link of this task to the Machine Translationsetting.
There are also connections to the code-switching and mixed language translation prob-lems (Fung et al., 1999).In our work, we therefore investigate the useof a standard Phrase-Based Statistical MachineTranslation (SMT) system to translate L1 frag-ments in L2 context.
In the next section, we de-scribe the SMT system that we used in our submis-sion.
We then describe the corpora used to trainthe SMT engine (Section 3), and our results on thetrial and test data, as well as a short error analysis(Section 4).section2 System DescriptionThe core Machine Translation engine used for allour submissions is Portage (Larkin et al., 2010),the NRC?s phrase-based SMT system.
Given asuitably trained SMT system, the Task 5 input isprocessed as follows.
For each sentence with anL1 fragment to translate, the already translatedparts are set as left and right context.
The L1 frag-ment in L2 context is sent to the decoder.
Theoutput is a full sentence translation that ensures 1)that the context is left untouched, and 2) that theL1 fragment is translated in a way that fits with theL2 context.We now describe the key components of the MTsystem (language, translation and reordering mod-els), as well as the decoding and parameter tuning.Translation Models We use a single staticphrase table including phrase pairs extracted fromthe symmetrized HMM word-alignment learned192on the entire training data.
The phrase table con-tains four features per phrase pair: lexical esti-mates of the forward and backward probabilitiesobtained either by relative frequencies or using themethod of Zens and Ney (2004).
These estimatesare derived by summing counts over all possiblealignments.
This yields four corresponding pa-rameters in the log-linear model.Reordering Models We use standard reorder-ing models: a distance-based distortion feature, aswell as a lexicalized distortion model (Tillmann,2004; Koehn et al., 2005).
For each phrase pair,the orientation counts required for the lexicalizeddistortion model are computed using HMM word-alignment on the full training corpora.
We esti-mate lexicalized probabilities for monotone, swap,and discontinuous ordering with respect to the pre-vious and following target phrase.
This results ina total of 6 feature values per phrase pair, in addi-tion to the distance-based distortion feature, henceseven parameters to tune in the log-linear model.Language Models When translating L1 frag-ments in L2 context, the L2 language model (LM)is particularly important as it is the only compo-nent of the SMT system that scores how well thetranslation of the L1 fragment fits in the existingL2 context.
We test two different LM configura-tions.
The first of these (run1) uses a single staticLM: a standard 4-gram, estimated using Kneser-Ney smoothing (Kneser and Ney, 1995) on the tar-get side of the bilingual corpora used for trainingthe translation models.
In the second configuration(run2), in order to further adapt the translations tothe test domain, a smaller LM trained on the L2contexts of the test data is combined to the train-ing corpus LM in a linear mixture model (Fosterand Kuhn, 2007).
The linear mixture weights areestimated on the L2 context of each test set in across-validation fashion.Decoding Algorithm and Parameter TuningDecoding uses the cube-pruning algorithm (Huangand Chiang, 2007) with a 7-word distortion limit.Log-linear parameter tuning is performed using alattice-based batch version of MIRA (Cherry andFoster, 2012).3 DataSMT systems require large amounts of data toestimate model parameters.
In addition, transla-tion performance largely depends on having in-Europarl News Totalen-de train 1904k 177k 2081kdev - 2000 2000en-es train 1959k 174k 2133kdev - 2000 2000fr-en train 2002k 157k 2158kdev - 2000 2000nl-en train 1974k - 1974kdev 1984 - 1984Table 1: Number of training segments for eachlanguage pair.domain data to train on.
As we had no informa-tion on the domain of the test data for Task 5, wechose to rely on general purpose publicly avail-able data.
Our main corpus is Europarl (Koehn,2005), which is available for all 4 language pairsof the evaluation.
As Europarl covers parliamen-tary proceedings, we added some news and com-mentary (henceforth ?News?)
data provided forthe 2013 workshop on Machine Translation sharedtask (Bojar et al., 2013) for language pairs otherthan nl-en.
In all cases, we extracted from the cor-pus a tuning (?dev?)
set of around 2000 sentencepairs.
Statistics for the training data are given inTable 1.The trial and test data each consist of 500 sen-tences with L1 fragments in L2 context providedby the organizers.
As the trial data came from Eu-roparl, we filtered our training corpora in order toremove close matches and avoid training on thetrial data (Table 1 takes this into account).All translation systems were trained on lower-cased data, and predictions were recased using astandard (LM-based) truecasing approach.4 Experimental Results4.1 Results on Trial and Simulated DataOur first evaluation was performed on the trial dataprovided by the Task 5 organizers.
Each examplewas translated in context by two systems:run1: Baseline, non-adapted system (marked 1below);run2: Linear LM mixture adaptation, using acontext LM (marked 2 below).Table 2 shows that our run1 system alreadyyields high performance on the trial data, while193W@1 F@1 W@5 F@5 +BLEUen-de1 78.1 77.0 95.6 94.8 12.4en-de2 79.8 79.0 95.8 95.0 12.6en-es1 81.8 80.2 97.7 97.2 12.1en-es2 84.3 83.2 97.7 97.2 12.5fr-en1 84.4 83.6 97.1 96.4 11.8fr-en2 85.9 85.0 97.4 96.6 12.0nl-en1 83.3 82.0 97.0 96.4 11.8nl-en2 86.7 86.2 97.5 97.0 12.1Table 2: Trial data performance, from official eval-uation script: (W)ord and (F)ragment accuracy at(1) and (5)-best and BLEU score gain.adapting the language model on the L2 contextsin run2 provides a clear gain in the top-1 results.That improvement all but disappears when takinginto account the best out of five translations (ex-cept maybe for nl-en).
The BLEU scores1arevery high (97-98) and the word error rates (notreported) are around 1%, suggesting that the sys-tem output almost matches the references.
Thisis no doubt due to the proximity between the trialdata and the MT training corpus.
Both are fully ormainly drawn from Europarl material.In order to get a less optimistic estimate of per-formance, we automatically constructed a num-ber of test examples from the WMT News Com-mentary development test sets.
The L1 sourcesegments and their L2 reference translations wereword aligned in both directions using the GIZA++implementation of IBM4 (Och and Ney, 2003)and the grow-diag-final-and combination heuris-tic (Koehn et al., 2005).
Test instances were cre-ated by substituting some L2 fragments with theirword-aligned L1 source within L2 reference seg-ments.
Since the goal was to select examples thatwere more ambiguous and harder to translate thanthe trial data, a subset of interesting L1 phraseswas randomly selected among phrases that oc-cured at least 4 times in the training corpus andhave a high entropy in the baseline phrase-table.We selected roughly 1000 L1 phrases per languagepair.
For each occurrence p1of these L1 phrases inthe news development sets, we identify the short-est L2 phrase p2that is consistently aligned with1+BLEU in Tables 2-4 is the difference between our sys-tem?s output and the sentence with untranslated L1 fragment.W@1 F@1 W@5 F@5 +BLEUen-de1 48.0 46.4 70.8 68.7 4.26en-de2 52.3 50.6 71.0 68.9 4.63en-es1 47.6 45.2 68.0 65.8 4.12en-es2 50.0 47.9 67.8 65.5 4.34fr-en1 50.1 49.2 73.6 71.8 5.18fr-en2 51.1 49.5 73.1 71.2 5.19Table 3: News data performance (cf Tab.
2).p1.2A new mixed language test example is con-structed by replacing p2with p1in L2 context.Results on that simulated data are given in Ta-ble 3.
Performance is markedly lower than on thetrial data.
This is due in part to the fact that theNews data is not as close to the training materialas the official trial set, and in part to the fact thatthis automatically extracted data contains imper-fect alignments with an unknown (but sizeable)amount of ?noise?.
However, it still appears run2consistently provides several points of increase inperformance for the top-1 results, over the base-line run1.
Performance on the 5-best is either un-affected or lower, and the gain in BLEU is muchlower than in Table 2 although the resulting BLEUis around 96%.4.2 Test ResultsOfficial test results provided by the organizersare presented in Table 4.
While these resultsare clearly above what we obtained on the syn-thetic news data, they fall well below the perfor-mance observed on the trial data.
This is not un-expected as the trial data is unrealistically closeto the training material, while the automaticallyextracted news data is noisy.
What we did notexpect, however, is the mediocre performance ofLM adaptation (run2): while consistently betterthan run1 on both trial and news, it is consistentlyworse on the official test data.
This may be due tothe fact that test sentences were drawn from differ-ent sources3such that it does not constitute a ho-mogeneous domain on which we can easily adapta language model.For German and Spanish, and to a lesser extent2As usual in phrase-based MT, two phrases are said to beconsistently aligned, if there is at least one link between theirwords and no external links.3According to the task description, the test set is basedon ?language learning exercises with gaps and cloze-tests, aswell as learner corpora with annotated errors?.194W@1 F@1 W@5 F@5 +BLEUen-de1 71.7 65.7 86.8 83.4 16.6en-de2 70.2 64.5 86.5 82.8 16.4en-es1 74.5 66.7 88.7 84.3 17.0en-es2 73.5 65.1 88.4 83.7 17.5fr-en1 69.4 55.6 83.9 73.9 10.2fr-en2 68.6 53.3 83.4 73.1 9.9nl-en1 61.0 45.0 72.3 60.6 5.03nl-en2 60.9 44.4 72.1 60.2 5.02Table 4: Test data performance, from official eval-uation results (cf.
Table 2).OOV?s failed alignen-de 0.002 0.058en-es 0.010 0.068fr-en 0.026 0.139nl-en 0.123 0.261Table 5: Test data error analysis: OOV?s is theproportion of all test fragments containing out-of-vocabulary tokens; failed align is the proportion offragments which our system cannot align to any ofthe reference translations by forced decoding.for French and Dutch, the BLEU and Word ErrorRate (WER) gains are much higher on the test thanon the trial data, although the resulting BLEU arearound 86-92%.
This results from the fact that theamount of L1 material to translate relative to theL2 context was significantly higher on the test datathan it was on the trial data (e.g.
17% of words onen-es test versus 7% on trial).4.3 Error AnalysisOn the French and, especially, on the Dutch data,our systems suffer from a high rate of out-of-vocabulary (OOV) source words in the L1 frag-ments, i.e.
words that simply did not appear in ourtraining data (see Table 5).
In the case of Dutch,OOV?s impose a hard ceiling of 88% on fragment-level accuracy.
These problems could possibly bealleviated by using more training data, and incor-porating language-specific mechanisms to handlemorphology and compounding into the systems.We also evaluate the proportion of reference tar-get fragments that can not be reached by forceddecoding (Table 5).
Note that to produce trialand test translations, we use standard decoding toFreq Type77 Incorrect L2 sense chosen75 Incorrect or mangled syntax26 Incomplete reference20 Non-idiomatic translation13 Out-of-vocab.
word in fragment6 Problematic source fragment3 Casing error220 TotalTable 6: Analysis of the types of error on 220French-English test sentences.predict a translation that maximizes model scoregiven the input.
Once we have the referencetranslation, we use forced decoding to try to pro-duce the exact reference given the source frag-ment and our translation model.
In some situa-tions, the correct translations are simply not reach-able by our systems, either because some targetword has not been observed in training, some partof the correspondence between source and targetfragments has not been observed, or the system?sword alignment mechanism is unable to accountfor this correspondence, in whole or in part.
Ta-ble 5 shows that this happens between 6% ans26% of cases, which gives a better upper bound onthe fragment-level accuracy that our system mayachieve.
Again, many of these problems could besolved by using more training data.To better understand the behavior of our sys-tems, we manually reviewed 220 sentences whereour baseline French-English system did not ex-actly match any of the references.
We annotatedseveral types of errors (Table 6).
The most fre-quent source of errors is incorrect sense (35%), i.e.the system produced a translation of the fragmentthat may be correct in some setting, but is not thecorrect sense in that context.
Those are presum-ably the errors of interest in a sense disambigua-tion setting.
A close second (34%) were errorsinvolving incorrect syntax in the fragment transla-tion, which points to limitations of the StatisticalMT approach, or to a limited language model.The last third combines several sources of er-rors.
Most notable in this category are non-idiomatic translations, where the system?s outputwas both syntactically correct and understandable,but clearly not fluent (e.g.
?take a siesta?
for ?havea nap?
); We also identified a number of cases195where we felt that either the source segment wasincorrect (eg ?je vais ?evanouir?
instead of ?je vaism??evanouir?
), or the references were incomplete.Table 7 gives a few examples.5 ConclusionWe described the systems used for the submissionsof the National Research Council Canada to theL2 writing assistant task.
We framed the problemas a machine translation task, and used standardstatistical machine translation systems trained onpublicly available corpora for translating L1 frag-ments in their L2 context.
This approach lever-ages the strengths of phrase-based statistical ma-chine translation, and therefore performs particu-larly well when the test examples are close to thetraining domain.
Conversely, it suffers from theinherent weaknesses of phrase-based models, in-cluding their inability to generalize beyond seenvocabulary, as well as sense and syntax errors.Overall, we showed that machine translation sys-tems can be used to address the L2 writing assis-tant task with a high level of accuracy, reachingout-of-five word-based accuracy above 80 percentfor 3 out of 4 language pairs.ReferencesErgun Bic?ici and Marc Dymetman.
2008.
DynamicTranslation Memory: Using Statistical MachineTranslation to Improve Translation Memory FuzzyMatches.
In Computational Linguistics and Intelli-gent Text Processing, pages 454?465.
Springer.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436, Montr?eal, Canada, June.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 128?135, Prague, Czech Republic, June.Pascale Fung, Xiaohu Liu, and Chi Shun Cheung.1999.
Mixed Language Query Disambiguation.
InProceedings of ACL?99, pages 333?340, Maryland,June.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 144?151, Prague, Czech Republic, June.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-off for M-gram Language Modeling.
InAcoustics, Speech, and Signal Processing, 1995.ICASSP-95., 1995 International Conference on, vol-ume 1, pages 181?184.
IEEE.Philipp Koehn and Jean Senellart.
2010.
Conver-gence of Translation Memory and Statistical Ma-chine Translation.
In Proceedings of AMTA Work-shop on MT Research and the Translation Industry,pages 21?31.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proceedings of IWSLT-2005, pages 68?75, Pitts-burgh, PA.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Machine Trans-lation Summit X, pages 79?86, Phuket, Thailand,September.Samuel Larkin, Boxing Chen, George Foster, Ull-rich Germann,?Eric Joanis, J. Howard Johnson, andRoland Kuhn.
2010.
Lessons from NRC?s PortageSystem at WMT 2010.
In 5th Workshop on Statisti-cal Machine Translation, pages 127?132.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?52.Michel Simard and Pierre Isabelle.
2009.
Phrase-based Machine Translation in a Computer-assistedTranslation Environment.
Proceedings of theTwelfth Machine Translation Summit (MT SummitXII), pages 120?127.Christoph Tillmann.
2004.
A Unigram Orienta-tion Model for Statistical Machine Translation.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Short Papers, pages 101?104, Boston, Massachusetts, USA, May 2 - May 7.Richard Zens and Hermann Ney.
2004.
Improve-ments in Phrase-Based Statistical Machine Trans-lation.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 257?264, Boston, Massachusetts, USA,May 2 - May 7.196Incorrect L2 sense:In: My dog usually barks au facteur - but look at that , for once , he is being friendly .
.
.Out: My dog usually barks to the factor - but look at that , for once , he is being friendly .
.
.Ref: My dog usually barks at the postman - but look at that , for once , he is being friendly .
.
.In: Grapes ne poussent pas in northern climates , unless one keeps them in a hot-house .Out: Grapes do not push in northern climates , unless one keeps them in a hot-house .Ref: Grapes do not grow in northern climates , unless one keeps them in a hot-house .Missing reference?In: Twenty-two other people ont?et?e bless?ees in the explosion .Out: Twenty-two other people were injured in the explosion .Ref: Twenty-two other people have been wounded in the explosion .Non-idiomatic translation:In: After patiently stalking its prey , the lion makes a rapide comme l?
?eclair charge for the kill .Out: After patiently stalking its prey , the lion makes a rapid as flash charge for the kill .Ref: After patiently stalking its prey , the lion makes a lightning-fast charge for the kill .Problem with input:In: every time I do n?t eat for a while and my blood sugar gets low I feel like je vais?evanouir .Out: every time I do n?t eat for a while and my blood sugar gets low I feel like I will evaporate .Ref: every time I do n?t eat for a while and my blood sugar gets low I feel like I ?m going to faint .Table 7: Examples errors on French-English.197
