Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 823?833,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTree-to-Sequence Attentional Neural Machine TranslationAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa TsuruokaThe University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jpAbstractMost of the existing Neural MachineTranslation (NMT) models focus on theconversion of sequential data and donot directly use syntactic information.We propose a novel end-to-end syntac-tic NMT model, extending a sequence-to-sequence model with the source-sidephrase structure.
Our model has an at-tention mechanism that enables the de-coder to generate a translated word whilesoftly aligning it with phrases as well aswords of the source sentence.
Experi-mental results on the WAT?15 English-to-Japanese dataset demonstrate that ourproposed model considerably outperformssequence-to-sequence attentional NMTmodels and compares favorably with thestate-of-the-art tree-to-string SMT system.1 IntroductionMachine Translation (MT) has traditionally beenone of the most complex language processingproblems, but recent advances of Neural MachineTranslation (NMT) make it possible to performtranslation using a simple end-to-end architecture.In the Encoder-Decoder model (Cho et al, 2014b;Sutskever et al, 2014), a Recurrent Neural Net-work (RNN) called the encoder reads the wholesequence of source words to produce a fixed-length vector, and then another RNN called thedecoder generates the target words from the vec-tor.
The Encoder-Decoder model has been ex-tended with an attention mechanism (Bahdanau etal., 2015; Luong et al, 2015a), which allows themodel to jointly learn the soft alignment betweenthe source language and the target language.
NMTmodels have achieved state-of-the-art results inEnglish-to-French and English-to-German trans-Figure 1: Alignment between an English phraseand a Japanese word.lation tasks (Luong et al, 2015b; Luong et al,2015a).
However, it is yet to be seen whetherNMT is competitive with traditional StatisticalMachine Translation (SMT) approaches in trans-lation tasks for structurally distant language pairssuch as English-to-Japanese.Figure 1 shows a pair of parallel sentences inEnglish and Japanese.
English and Japanese arelinguistically distant in many respects; they havedifferent syntactic constructions, and words andphrases are defined in different lexical units.
Inthis example, the Japanese word ????
is alignedwith the English words ?green?
and ?tea?, andthe English word sequence ?a cup of?
is alignedwith a special symbol ?null?, which is not explic-itly translated into any Japanese words.
One wayto solve this mismatch problem is to consider thephrase structure of the English sentence and alignthe phrase ?a cup of green tea?
with ????.
InSMT, it is known that incorporating syntactic con-stituents of the source language into the modelsimproves word alignment (Yamada and Knight,2001) and translation accuracy (Liu et al, 2006;Neubig and Duh, 2014).
However, the existingNMT models do not allow us to perform this kindof alignment.In this paper, we propose a novel attentionalNMT model to take advantage of syntactic infor-823mation.
Following the phrase structure of a sourcesentence, we encode the sentence recursively in abottom-up fashion to produce a vector representa-tion of the sentence and decode it while aligningthe input phrases and words with the output.
Ourexperimental results on the WAT?15 English-to-Japanese translation task show that our proposedmodel achieves state-of-the-art translation accu-racy.2 Neural Machine Translation2.1 Encoder-Decoder ModelNMT is an end-to-end approach to data-drivenmachine translation (Kalchbrenner and Blunsom,2013; Sutskever et al, 2014; Bahdanau et al,2015).
In other words, the NMT models directlyestimate the conditional probability p(y|x) givena large collection of source and target sentencepairs (x,y).
An NMT model consists of an en-coder process and a decoder process, and hencethey are often called Encoder-Decoder models.
Inthe Encoder-Decoder models, a sentence is treatedas a sequence of words.
In the encoder pro-cess, the encoder embeds each of the source wordsx = (x1, x2, ?
?
?
, xn) into a d-dimensional vectorspace.
The decoder then outputs a word sequencey = (y1, y2, ?
?
?
, ym) in the target language giventhe information on the source sentence providedby the encoder.
Here, n and m are the lengthsof the source and target sentences, respectively.RNNs allow one to effectively embed sequentialdata into the vector space.In the RNN encoder, the i-th hidden unit hi?Rd?1is calculated given the i-th input xiand theprevious hidden unit hi?1?
Rd?1,hi= fenc(xi,hi?1), (1)where fencis a non-linear function, and the initialhidden unit h0is usually set to zeros.
The encod-ing function fencis recursively applied until the n-th hidden unit hnis obtained.
The RNN Encoder-Decoder models assume that hnrepresents a vec-tor of the meaning of the input sequence up to then-th word.After encoding the whole input sentence intothe vector space, we decode it in a similar way.The initial decoder unit s1is initialized with theinput sentence vector (s1= hn).
Given the pre-vious target word and the j-th hidden unit of thedecoder, the conditional probability that the j-thtarget word is generated is calculated as follows:p(yj|y<j,x) = g(sj), (2)where g is a non-linear function.
The j-th hiddenunit of the decoder is calculated by using anothernon-linear function fdecas follows:sj= fdec(yj?1, sj?1).
(3)We employ Long Short-Term Memory (LSTM)units (Hochreiter and Schmidhuber, 1997; Gers etal., 2000) in place of vanilla RNN units.
The t-th LSTM unit consists of several gates and twodifferent types of states: a hidden unit ht?
Rd?1and a memory cell ct?
Rd?1,it= ?
(W(i)xt+U(i)ht?1+ b(i)),ft= ?
(W(f)xt+U(f)ht?1+ b(f)),ot= ?
(W(o)xt+U(o)ht?1+ b(o)),?ct= tanh(W(c?)xt+U(c?
)ht?1+ b(c?
)),ct= it?
?ct+ ft?
ct?1,ht= ot?
tanh(ct), (4)where each of it, ft, otand?ct?
Rd?1denotesan input gate, a forget gate, an output gate, and astate for updating the memory cell, respectively.W(?)?
Rd?dand U(?)?
Rd?dare weight matri-ces, b(?)?
Rd?1is a bias vector, and xt?
Rd?1is the word embedding of the t-th input word.
?(?
)is the logistic function, and the operator ?
denoteselement-wise multiplication between vectors.2.2 Attentional Encoder-Decoder ModelThe NMT models with an attention mecha-nism (Bahdanau et al, 2015; Luong et al, 2015a)have been proposed to softly align each decoderstate with the encoder states.
The attention mech-anism allows the NMT models to explicitly quan-tify how much each encoder state contributes tothe word prediction at each time step.In the attentional NMT model in Luong et al(2015a), at the j-th step of the decoder process,the attention score ?j(i) between the i-th sourcehidden unit hiand the j-th target hidden unit sjiscalculated as follows:?j(i) =exp(hi?
sj)?nk=1exp(hk?
sj), (5)where hi?
sjis the inner product of hiand sj,which is used to directly calculate the similarityscore between hiand sj.
The j-th context vector824Figure 2: Attentional Encoder-Decoder model.djis calculated as the summation vector weightedby ?j(i):dj=n?i=1?j(i)hi.
(6)To incorporate the attention mechanism into thedecoding process, the context vector is used for thethe j-th word prediction by putting an additionalhidden layer?sj:?sj= tanh(Wd[sj; dj] + bd), (7)where [sj;dj] ?
R2d?1is the concatenation of sjand dj, and Wd?
Rd?2dand bd?
Rd?1are aweight matrix and a bias vector, respectively.
Themodel predicts the j-th word by using the softmaxfunction:p(yj|y<j,x) = softmax(Ws?sj+ bs), (8)whereWs?
R|V |?dand bs?
R|V |?1are a weightmatrix and a bias vector, respectively.
|V | standsfor the size of the vocabulary of the target lan-guage.
Figure 2 shows an example of the NMTmodel with the attention mechanism.2.3 Objective Function of NMT ModelsThe objective function to train the NMT modelsis the sum of the log-likelihoods of the translationpairs in the training data:J(?)
=1|D|?
(x,y)?Dlog p(y|x), (9)where D denotes a set of parallel sentence pairs.The model parameters ?
are learned throughStochastic Gradient Descent (SGD).3 Attentional Tree-to-Sequence Model3.1 Tree-based Encoder + SequentialEncoderThe exsiting NMT models treat a sentence as asequence of words and neglect the structure ofFigure 3: Proposed model: Tree-to-sequence at-tentional NMT model.a sentence inherent in language.
We propose anovel tree-based encoder in order to explicitly takethe syntactic structure into consideration in theNMT model.
We focus on the phrase structure ofa sentence and construct a sentence vector fromphrase vectors in a bottom-up fashion.
The sen-tence vector in the tree-based encoder is there-fore composed of the structural information ratherthan the sequential data.
Figure 3 shows our pro-posed model, which we call a tree-to-sequence at-tentional NMT model.In Head-driven Phrase Structure Grammar(HPSG) (Sag et al, 2003), a sentence is composedof multiple phrase units and represented as a bi-nary tree as shown in Figure 1.
Following thestructure of the sentence, we construct a tree-basedencoder on top of the standard sequential encoder.The k-th parent hidden unit h(phr)kfor the k-thphrase is calculated using the left and right childhidden units hlkand hrkas follows:h(phr)k= ftree(hlk,hrk), (10)where ftreeis a non-linear function.We construct a tree-based encoder with LSTMunits, where each node in the binary tree is repre-sented with an LSTM unit.
When initializing theleaf units of the tree-based encoder, we employ thesequential LSTM units described in Section 2.1.Each non-leaf node is also represented with anLSTM unit, and we employ Tree-LSTM (Tai etal., 2015) to calculate the LSTM unit of the par-ent node which has two child LSTM units.
Thehidden unit h(phr)k?
Rd?1and the memory cellc(phr)k?
Rd?1for the k-th parent node are calcu-825lated as follows:ik= ?
(U(i)lhlk+U(i)rhrk+ b(i)),flk= ?
(U(fl)lhlk+U(fl)rhrk+ b(fl)),frk= ?
(U(fr)lhlk+U(fr)rhrk+ b(fr)),ok= ?
(U(o)lhlk+U(o)rhrk+ b(o)),?ck= tanh(U(c?)lhlk+U(c?
)rhrk+ b(c?
)),c(phr)k= ik?
?ck+ flk?
clk+ frk?
crk,h(phr)k= ok?
tanh(c(phr)k), (11)where ik, flk, frk, oj,?cj?
Rd?1are an inputgate, the forget gates for left and right child units,an output gate, and a state for updating the mem-ory cell, respectively.
clkand crkare the mem-ory cells for the left and right child units, respec-tively.
U(?)?
Rd?ddenotes a weight matrix, andb(?)?
Rd?1represents a bias vector.Our proposed tree-based encoder is a naturalextension of the conventional sequential encoder,since Tree-LSTM is a generalization of chain-structured LSTM (Tai et al, 2015).
Our encoderdiffers from the original Tree-LSTM in the cal-culation of the LSTM units for the leaf nodes.The motivation is to construct the phrase nodes ina context-sensitive way, which, for example, al-lows the model to compute different representa-tions for multiple occurrences of the same word ina sentence because the sequential LSTMs are cal-culated in the context of the previous units.
Thisability contrasts with the original Tree-LSTM, inwhich the leaves are composed only of the wordembeddings without any contextual information.3.2 Initial Decoder SettingWe now have two different sentence vectors: oneis from the sequence encoder and the other fromthe tree-based encoder.
As shown in Figure 3, weprovide another Tree-LSTM unit which has the fi-nal sequential encoder unit (hn) and the tree-basedencoder unit (h(phr)root) as two child units and set itas the initial decoder s1as follows:s1= gtree(hn,h(phr)root), (12)where gtreeis the same function as ftreewith an-other set of Tree-LSTM parameters.
This initial-ization allows the decoder to capture informationfrom both the sequential data and phrase struc-tures.
Zoph and Knight (2016) proposed a simi-lar method using a Tree-LSTM for initializing thedecoder, with which they translate multiple sourcelanguages to one target language.
When the syn-tactic parser fails to output a parse tree for a sen-tence, we encode the sentence with the sequentialencoder by setting h(phr)root= 0.
Our proposed tree-based encoder therefore works with any sentences.3.3 Attention Mechanism in Our ModelWe adopt the attention mechanism into our tree-to-sequence model in a novel way.
Our modelgives attention not only to sequential hidden unitsbut also to phrase hidden units.
This attentionmechanism tells us which words or phrases in thesource sentence are important when the model de-codes a target word.
The j-th context vector djis composed of the sequential and phrase vectorsweighted by the attention score ?j(i):dj=n?i=1?j(i)hi+2n?1?i=n+1?j(i)h(phr)i.
(13)Note that a binary tree has n ?
1 phrase nodes ifthe tree has n leaves.
We set a final decoder?sjinthe same way as Equation (7).In addition, we adopt the input-feedingmethod (Luong et al, 2015a) in our model, whichis a method for feeding?sj?1, the previous unitto predict the word yj?1, into the current targethidden unit sj,sj= fdec(yj?1, [sj?1;?sj?1]), (14)where [sj?1;?sj?1] is the concatenation of sj?1and?sj?1.
The input-feeding approach contributesto the enrichment in the calculation of the decoder,because?sj?1is an informative unit which can beused to predict the output word as well as to becompacted with attentional context vectors.
Lu-ong et al (2015a) showed that the input-feedingapproach improves BLEU scores.
We also ob-served the same improvement in our preliminaryexperiments.3.4 Sampling-Based Approximation to theNMT ModelsThe biggest computational bottleneck of train-ing the NMT models is in the calculation of thesoftmax layer described in Equation (8), becauseits computational cost increases linearly with thesize of the vocabulary.
The speedup techniquewith GPUs has proven useful for sequence-basedNMT models (Sutskever et al, 2014; Luong et al,8262015a) but it is not easily applicable when deal-ing with tree-structured data.
In order to reducethe training cost of the NMT models at the soft-max layer, we employ BlackOut (Ji et al, 2016), asampling-based approximation method.
BlackOuthas been shown to be effective in RNN LanguageModels (RNNLMs) and allows a model to run rea-sonably fast even with a million word vocabularywith CPUs.At each word prediction step in the training,BlackOut estimates the conditional probability inEquation (2) for the target word and K neg-ative samples using a weighted softmax func-tion.
The negative samples are drawn from theunigram distribution raised to the power ?
?
[0, 1] (Mikolov et al, 2013).
The unigram dis-tribution is estimated using the training data and?
is a hyperparameter.
BlackOut is closely re-lated to Noise Contrastive Estimation (NCE) (Gut-mann and Hyv?arinen, 2012) and achieves betterperplexity than the original softmax and NCE inRNNLMs.
The advantages of Blackout over theother methods are discussed in Ji et al (2016).Note that BlackOut can be used as the originalsoftmax once the training is finished.4 Experiments4.1 Training DataWe applied the proposed model to the English-to-Japanese translation dataset of the ASPEC corpusgiven in WAT?15.1Following Zhu (2015), we ex-tracted the first 1.5 million translation pairs fromthe training data.
To obtain the phrase structuresof the source sentences, i.e., English, we used theprobabilistic HPSG parser Enju (Miyao and Tsu-jii, 2008).
We used Enju only to obtain a binaryphrase structure for each sentence and did not useany HPSG specific information.
For the targetlanguage, i.e., Japanese, we used KyTea (Neubiget al, 2011), a Japanese segmentation tool, andperformed the pre-processing steps recommendedin WAT?15.2We then filtered out the translationpairs whose sentence lengths are longer than 50and whose source sentences are not parsed suc-cessfully.
Table 1 shows the details of the datasetsused in our experiments.
We carried out two ex-periments on a small training dataset to investigate1http://orchid.kuee.kyoto-u.ac.jp/WAT/WAT2015/index.html2http://orchid.kuee.kyoto-u.ac.jp/WAT/WAT2015/baseline/dataPreparationJE.htmlSentences Parsed successfullyTrain 1,346,946 1,346,946Development 1,790 1,789Test 1,812 1,811Table 1: Dataset in ASPEC corpus.Train (small) Train (large)sentence pairs 100,000 1,346,946|V | in English 25,478 87,796|V | in Japanese 23,532 65,680Table 2: Training dataset and the vocabulary sizes.the effectiveness of our proposed model and ona large training dataset to compare our proposedmethods with the other systems.The vocabulary consists of words observed inthe training data more than or equal to N times.We set N = 2 for the small training dataset andN = 5 for the large training dataset.
The out-of-vocabulary words are mapped to the special token?unk?.
We added another special symbol ?eos?
forboth languages and inserted it at the end of all thesentences.
Table 2 shows the details of each train-ing dataset and its corresponding vocabulary size.4.2 Training DetailsThe biases, softmax weights, and BlackOutweights are initialized with zeros.
The hyperpa-rameter ?
of BlackOut is set to 0.4 as recom-mended by Ji et al (2016).
Following J?ozefowiczet al (2015), we initialize the forget gate biases ofLSTM and Tree-LSTM with 1.0.
The remainingmodel parameters in the NMT models in our ex-periments are uniformly initialized in [?0.1, 0.1].The model parameters are optimized by plain SGDwith the mini-batch size of 128.
The initial learn-ing rate of SGD is 1.0.
We halve the learning ratewhen the development loss becomes worse.
Gra-dient norms are clipped to 3.0 to avoid explodinggradient problems (Pascanu et al, 2012).Small Training Dataset We conduct experi-ments with our proposed model and the sequentialattentional NMT model with the input-feeding ap-proach.
Each model has 256-dimensional hiddenunits and word embeddings.
The number of nega-tive samples K of BlackOut is set to 500 or 2000.827Large Training Dataset Our proposed modelhas 512-dimensional word embeddings and d-dimensional hidden units (d ?
{512, 768, 1024}).K is set to 2500.Our code3is implemented in C++ using theEigen library,4a template library for linear alge-bra, and we run all of the experiments on multi-core CPUs.5It takes about a week to train a modelon the large training dataset with d = 512.4.3 Decoding processWe use beam search to decode a target sentencefor an input sentence x and calculate the sumof the log-likelihoods of the target sentence y =(y1, ?
?
?
, ym) as the beam score:score(x,y) =m?j=1log p(yj|y<j,x).
(15)Decoding in the NMT models is a generative pro-cess and depends on the target language modelgiven a source sentence.
The score becomessmaller as the target sentence becomes longer, andthus the simple beam search does not work wellwhen decoding a long sentence (Cho et al, 2014a;Pouget-Abadie et al, 2014).
In our preliminaryexperiments, the beam search with the length nor-malization in Cho et al (2014a) was not effectivein English-to-Japanese translation.
The methodin Pouget-Abadie et al (2014) needs to estimatethe conditional probability p(x|y) using anotherNMT model and thus is not suitable for our work.In this paper, we use statistics on sentencelengths in beam search.
Assuming that the lengthof a target sentence correlates with the length ofa source sentence, we redefine the score of eachcandidate as follows:score(x,y) = Lx,y +m?j=1log p(yj|y<j,x),(16)Lx,y = log p(len(y)|len(x)), (17)where Lx,y is the penalty for the conditional prob-ability of the target sentence length len(y) giventhe source sentence length len(x).
It allowsthe model to decode a sentence by consideringthe length of the target sentence.
In our exper-iments, we computed the conditional probability3https://github.com/tempra28/tree2seq4http://eigen.tuxfamily.org/index.php516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @3.20GHzp(len(y)|len(x)) in advance following the statis-tics collected in the first one million pairs of thetraining dataset.
We allow the decoder to generateup to 100 words.4.4 EvaluationWe evaluated the models by two automatic eval-uation metrics, RIBES (Isozaki et al, 2010) andBLEU (Papineni et al, 2002) following WAT?15.We used the KyTea-based evaluation script for thetranslation results.6The RIBES score is a metricbased on rank correlation coefficients with wordprecision, and the BLEU score is based on n-gramword precision and a Brevity Penalty (BP) for out-puts shorter than the references.
RIBES is knownto have stronger correlation with human judge-ments than BLEU in translation between Englishand Japanese as discussed in Isozaki et al (2010).5 Results and Discussion5.1 Small Training DatasetTable 3 shows the perplexity, BLEU, RIBES, andthe training time on the development data with theAttentional NMT (ANMT) models trained on thesmall dataset.
We conducted the experiments withour proposed method using BlackOut and soft-max.
We decoded a translation by our proposedbeam search with a beam size of 20.As shown in Table 3, the results of our proposedmodel with BlackOut improve as the number ofnegative samples K increases.
Although the resultof softmax is better than those of BlackOut (K =500, 2000), the training time of softmax per epochis about three times longer than that of BlackOuteven with the small dataset.As to the results of the ANMT model, reversingthe word order in the input sentence decreases thescores in English-to-Japanese translation, whichcontrasts with the results of other language pairsreported in previous work (Sutskever et al, 2014;Luong et al, 2015a).
By taking syntactic infor-mation into consideration, our proposed modelimproves the scores, compared to the sequentialattention-based approach.We found that better perplexity does not alwayslead to better translation scores with BlackOut asshown in Table 3.
One of the possible reasons isthat BlackOut distorts the target word distribution6http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/automatic_evaluation_systems/automaticEvaluationJA.html828K Perplexity RIBES BLEU Time/epoch (min.
)Proposed model 500 19.6 71.8 20.0 55Proposed model 2000 21.0 72.6 20.5 70Proposed model (Softmax) ?
17.9 73.2 21.8 180ANMT (Luong et al, 2015a) 500 21.6 70.7 18.5 45+ reverse input 500 22.6 69.8 17.7 45ANMT (Luong et al, 2015a) 2000 23.1 71.5 19.4 60+ reverse input 2000 26.1 69.5 17.5 60Table 3: Evaluation results on the development data using the small training data.
The training time perepoch is also shown, and K is the number of negative samples in BlackOut.Beam size RIBES BLEU (BP)Simple BS6 72.3 20.0 (90.1)20 72.3 19.5 (85.1)Proposed BS 20 72.6 20.5 (91.7)Table 4: Effects of the Beam Search (BS) on thedevelopment data.by the modified unigram-based negative samplingwhere frequent words can be treated as the nega-tive samples multiple times at each training step.Effects of the proposed beam search Table 4shows the results on the development data of pro-posed method with BlackOut (K = 2000) bythe simple beam search and our proposed beamsearch.
The beam size is set to 6 or 20 in the sim-ple beam search, and to 20 in our proposed search.We can see that our proposed search outperformsthe simple beam search in both scores.
UnlikeRIBES, the BLEU score is sensitive to the beamsize and becomes lower as the beam size increases.We found that the BP had a relatively large im-pact on the BLEU score in the simple beam searchas the beam size increased.
Our search methodworks better than the simple beam search by keep-ing long sentences in the candidates with a largebeam size.Effects of the sequential LSTM units We alsoinvestigated the effects of the sequential LSTMsat the leaf nodes in our proposed tree-based en-coder.
Table 5 shows the result on the develop-ment data of our proposed encoder and that of anattentional tree-based encoder without sequentialLSTMs with BlackOut (K = 2000).7The resultsshow that our proposed encoder considerably out-7For this evaluation, we used the 1,789 sentences thatwere successfully parsed by Enju because the encoder with-out sequential LSTMs always requires a parse tree.RIBES BLEUWithout sequential LSTMs 69.4 19.5With sequential LSTMs 72.3 20.0Table 5: Effects of the sequential LSTMs in ourproposed tree-based encoder on the developmentdata.performs the encoder without sequential LSTMs,suggesting that the sequential LSTMs at the leafnodes contribute to the context-aware constructionof the phrase representations in the tree.5.2 Large Training DatasetTable 6 shows the experimental results of RIBESand BLEU scores achieved by the trained modelson the large dataset.
We decoded the target sen-tences by our proposed beam search with the beamsize of 20.8The results of the other systems are theones reported in Nakazawa et al (2015).All of our proposed models show similar per-formance regardless of the value of d. Our ensem-ble model is composed of the three models withd = 512, 768, and 1024, and it shows the bestRIBES score among all systems.9As for the time required for training, our im-plementation needs about one day to perform oneepoch on the large training dataset with d = 512.It would take about 11 days without using theBlackOut sampling.Comparison with the NMT models The modelof Zhu (2015) is an ANMT model (Bahdanau etal., 2015) with a bi-directional LSTM encoder,and uses 1024-dimensional hidden units and 1000-8We found two sentences which ends without eos withd = 512, and then we decoded it again with the beam size of1000 following Zhu (2015).9Our ensemble model yields a METEOR (Denkowski andLavie, 2014) score of 53.6 with language option ?-l other?.829Model RIBES BLEUProposed model (d = 512) 81.46 34.36Proposed model (d = 768) 81.89 34.78Proposed model (d = 1024) 81.58 34.87Ensemble of the above three models 82.45 36.95ANMT with LSTMs (Zhu, 2015) 79.70 32.19+ Ensemble, unk replacement 80.27 34.19+ System combination,80.91 36.213 pre-reordered ensemblesANMT with GRUs (Lee et al, 2015)81.15 35.75+ character-based decoding,Begin/Inside representationPB baseline 69.19 29.80HPB baseline 74.70 32.56T2S baseline 75.80 33.44T2S model (Neubig and Duh, 2014) 79.65 36.58+ ANMT Rerank (Neubig et al, 2015) 81.38 38.17Table 6: Evaluation results on the test data.dimensional word embeddings.
The model of Leeet al (2015) is also an ANMT model with a bi-directional Gated Recurrent Unit (GRU) encoder,and uses 1000-dimensional hidden units and 200-dimensional word embeddings.
Both models aresequential ANMT models.
Our single proposedmodel with d = 512 outperforms the best result ofZhu (2015)?s end-to-end NMT model with ensem-ble and unknown replacement by +1.19 RIBESand by +0.17 BLEU scores.
Our ensemble modelshows better performance, in both RIBES andBLEU scores, than that of Zhu (2015)?s best sys-tem which is a hybrid of the ANMT and SMTmodels by +1.54 RIBES and by +0.74 BLEUscores and Lee et al (2015)?s ANMT systemwith special character-based decoding by +1.30RIBES and +1.20 BLEU scores.Comparison with the SMT models PB, HPBand T2S are the baseline SMT systems inWAT?15: a phrase-based model, a hierarchicalphrase-based model, and a tree-to-string model,respectively (Nakazawa et al, 2015).
The bestmodel in WAT?15 is Neubig et al (2015)?s tree-to-string SMT model enhanced with reranking byANMT using a bi-directional LSTM encoder.
Ourproposed end-to-end NMT model compares favor-ably with Neubig et al (2015).5.3 Qualitative AnalysisWe illustrate the translations of test data by ourmodel with d = 512 and several attentional rela-tions when decoding a sentence.
In Figures 4 and5, an English sentence represented as a binary treeis translated into Japanese, and several attentionalrelations between English words or phrases andFigure 4: Translation example of a short sen-tence and the attentional relations by our proposedmodel.Japanese word are shown with the highest atten-tion score ?.
The additional attentional relationsare also illustrated for comparison.
We can see thetarget words softly aligned with source words andphrases.In Figure 4, the Japanese word ????
means?liquid crystal?, and it has a high attention score(?
= 0.41) with the English phrase ?liquid crystalfor active matrix?.
This is because the j-th tar-get hidden unit sjhas the contextual informationabout the previous words y<jincluding ?????????
??
(?for active matrix?
in English).The Japanese word ????
is softly aligned withthe phrase ?the cells?
with the highest attentionscore (?
= 0.35).
In Japanese, there is no defi-nite article like ?the?
in English, and it is usuallyaligned with null described as Section 1.In Figure 5, in the case of the Japanese word???
(?showed?
in English), the attention scorewith the English phrase ?showed excellent perfor-mance?
(?
= 0.25) is higher than that with theEnglish word ?showed?
(?
= 0.01).
The Japaneseword ???
(?of?
in English) is softly aligned withthe phrase ?of Si dot MOS capacitor?
with thehighest attention score (?
= 0.30).
It is becauseour attention mechanism takes each previous con-text of the Japanese phrases ???????
(?ex-cellent performance?
in English) and ?????????
??????
(?Si dot MOS capacitor?
inEnglish) into account and softly aligned the targetwords with the whole phrase when translating theEnglish verb ?showed?
and the preposition ?of?.Our proposed model can thus flexibly learn the at-tentional relations between English and Japanese.We observed that our model translated the word?active?
into ???
?, a synonym of the referenceword ???????.
We also found similar exam-ples in other sentences, where our model outputs830Figure 5: Translation example of a long sentence and the attentional relations by our proposed model.synonyms of the reference words, e.g.
???
and ????
(?female?
in English) and ?NASA?
and ???????
(?National Aeronautics and Space Ad-ministration?
in English).
These translations arepenalized in terms of BLEU scores, but they do notnecessarily mean that the translations were wrong.This point may be supported by the fact that theNMTmodels were highly evaluated in WAT?15 bycrowd sourcing (Nakazawa et al, 2015).6 Related WorkKalchbrenner and Blunsom (2013) were the firstto propose an end-to-end NMT model using Con-volutional Neural Networks (CNNs) as the sourceencoder and using RNNs as the target decoder.The Encoder-Decoder model can be seen as an ex-tension of their model, and it replaces the CNNswith RNNs using GRUs (Cho et al, 2014b) orLSTMs (Sutskever et al, 2014).Sutskever et al (2014) have shown that mak-ing the input sequences reversed is effective in aFrench-to-English translation task, and the tech-nique has also proven effective in translation tasksbetween other European language pairs (Luong etal., 2015a).
All of the NMT models mentionedabove are based on sequential encoders.
To incor-porate structural information into the NMT mod-els, Cho et al (2014a) proposed to jointly learnstructures inherent in source-side languages butdid not report improvement of translation perfor-mance.
These studies motivated us to investigatethe role of syntactic structures explicitly given byexisting syntactic parsers in the NMT models.The attention mechanism (Bahdanau et al,2015) has promoted NMT onto the next stage.
Itenables the NMT models to translate while align-ing the target with the source.
Luong et al (2015a)refined the attention model so that it can dynami-cally focus on local windows rather than the entiresentence.
They also proposed a more effective at-tentional path in the calculation of ANMT models.Subsequently, several ANMT models have beenproposed (Cheng et al, 2016; Cohn et al, 2016);however, each model is based on the existing se-quential attentional models and does not focus ona syntactic structure of languages.7 ConclusionIn this paper, we propose a novel syntactic ap-proach that extends attentional NMT models.
Wefocus on the phrase structure of the input sen-tence and build a tree-based encoder followingthe parsed tree.
Our proposed tree-based encoderis a natural extension of the sequential encodermodel, where the leaf units of the tree-LSTMin the encoder can work together with the origi-nal sequential LSTM encoder.
Moreover, the at-tention mechanism allows the tree-based encoderto align not only the input words but also inputphrases with the output words.
Experimental re-sults on the WAT?15 English-to-Japanese transla-tion dataset demonstrate that our proposed modelachieves the best RIBES score and outperformsthe sequential attentional NMT model.AcknowledgmentsWe thank the anonymous reviewers for their con-structive comments and suggestions.
This workwas supported by CREST, JST, and JSPS KAK-ENHI Grant Number 15J12597.831ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural Machine Translation by JointlyLearning to Align and Translate.
In Proceedings ofthe 3rd International Conference on Learning Rep-resentations.Yong Cheng, Shiqi Shen, Zhongjun He, Wei He,Hua Wu, Maosong Sun, and Yang Liu.
2016.Agreement-based Joint Training for BidirectionalAttention-based Neural Machine Translation.
InProceedings of the 25th International Joint Confer-ence on Artificial Intelligence.
to appear.KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On the Prop-erties of Neural Machine Translation: Encoder-Decoder Approaches.
In Proceedings of EighthWorkshop on Syntax, Semantics and Structure inStatistical Translation (SSST-8).Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014b.
Learn-ing Phrase Representations using RNN Encoder?Decoder for Statistical Machine Translation.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing, pages 1724?1734.Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-molova, Kaisheng Yao, Chris Dyer, and Gholam-reza Haffari.
2016.
Incorporating Structural Align-ment Biases into an Attentional Neural TranslationModel.
In Proceedings of the 15th Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies.
to appear.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: Language specific translation evaluationfor any target language.
In Proceedings of the 14thConference of the European Chapter of the Associ-ation for Computational Linguistics 2014 Workshopon Statistical Machine Translation.Felix A. Gers, J?urgen Schmidhuber, and Fred A.Cummins.
2000.
Learning to Forget: Contin-ual Prediction with LSTM.
Neural Computation,12(10):2451?2471.Michael U. Gutmann and Aapo Hyv?arinen.
2012.Noise-contrastive estimation of unnormalized sta-tistical models, with applications to natural imagestatistics.
Journal of Machine Learning Research,13(1):307?361.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long Short-Term Memory.
Neural Computation,9(8):1735?1780.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
AutomaticEvaluation of Translation Quality for Distant Lan-guage Pairs.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 944?952.Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish,Michael J. Anderson, and Pradeep Dubey.
2016.BlackOut: Speeding up Recurrent Neural NetworkLanguage Models With Very Large Vocabularies.
InProceedings of the 4th International Conference onLearning Representations.Rafal J?ozefowicz, Wojciech Zaremba, and IlyaSutskever.
2015.
An Empirical Exploration ofRecurrent Network Architectures.
In Proceedingsof the 32nd International Conference on MachineLearning, volume 37, pages 2342?2350.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1700?1709.Hyoung-Gyu Lee, JaeSong Lee, Jun-Seok Kim, andChang-Ki Lee.
2015.
NAVER Machine TranslationSystem for WAT 2015.
In Proceedings of the 2ndWorkshop on Asian Translation (WAT2015), pages69?73.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics, pages 609?616.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015a.
Effective Approaches to Attention-based Neural Machine Translation.
In Proceedingsof the 2015 Conference on Empirical Methods inNatural Language Processing, pages 1412?1421.Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,and Wojciech Zaremba.
2015b.
Addressing theRare Word Problem in Neural Machine Translation.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages11?19.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems 26, pages 3111?3119.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature For-est Models for Probabilistic HPSG Parsing.
Compu-tational Linguistics, 34(1):35?80.Toshiaki Nakazawa, Hideya Mino, Isao Goto, Gra-ham Neubig, Sadao Kurohashi, and Eiichiro Sumita.2015.
Overview of the 2nd Workshop on AsianTranslation.
In Proceedings of the 2nd Workshopon Asian Translation (WAT2015), pages 1?28.832Graham Neubig and Kevin Duh.
2014.
On the ele-ments of an accurate tree-to-string machine trans-lation system.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 143?149.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise Prediction for Robust, AdaptableJapanese Morphological Analysis.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 529?533.Graham Neubig, Makoto Morishita, and Satoshi Naka-mura.
2015.
Neural Reranking Improves Sub-jective Quality of Machine Translation: NAIST atWAT2015.
In Proceedings of the 2nd Workshop onAsian Translation (WAT2015), pages 35?41.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2012.
Understanding the exploding gradient prob-lem.
arXiv: 1211.5063.Jean Pouget-Abadie, Dzmitry Bahdanau, Bart vanMer-rienboer, Kyunghyun Cho, and Yoshua Bengio.2014.
Overcoming the curse of sentence length forneural machine translation using automatic segmen-tation.
In Proceedings of SSST-8, Eighth Workshopon Syntax, Semantics and Structure in StatisticalTranslation, pages 78?85.Ivan A.
Sag, Thomas Wasow, and Emily Bender.
2003.Syntactic Theory: A Formal Introduction.
Center forthe Study of Language and Information, Stanford,2nd edition.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to Sequence Learning with Neural Net-works.
In Advances in Neural Information Process-ing Systems 27, pages 3104?3112.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved Semantic Representa-tions From Tree-Structured Long Short-Term Mem-ory Networks.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 1556?1566.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof the 39th Annual Meeting on Association for Com-putational Linguistics, pages 523?530.Zhongyuan Zhu.
2015.
Evaluating Neural MachineTranslation in English-Japanese Task.
In Proceed-ings of the 2nd Workshop on Asian Translation(WAT2015), pages 61?68.Barret Zoph and Kevin Knight.
2016.
Multi-SourceNeural Translation.
In Proceedings of the 15th An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies.
to appear.833
