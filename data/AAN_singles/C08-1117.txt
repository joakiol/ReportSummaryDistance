Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 929?936Manchester, August 2008Using Three Way Data for Word Sense DiscriminationTim Van de CruysHumanities ComputingUniversity of Groningent.van.de.cruys@rug.nlAbstractIn this paper, an extension of a dimen-sionality reduction algorithm called NON-NEGATIVE MATRIX FACTORIZATION ispresented that combines both ?bag ofwords?
data and syntactic data, in orderto find semantic dimensions according towhich both words and syntactic relationscan be classified.
The use of three waydata allows one to determine which dimen-sion(s) are responsible for a certain senseof a word, and adapt the correspondingfeature vector accordingly, ?subtracting?one sense to discover another one.
Theintuition in this is that the syntactic fea-tures of the syntax-based approach can bedisambiguated by the semantic dimensionsfound by the bag of words approach.
Thenovel approach is embedded into cluster-ing algorithms, to make it fully automatic.The approach is carried out for Dutch, andevaluated against EuroWordNet.1 IntroductionAutomatically acquiring semantics from text is asubject that has gathered a lot of attention for quitesome time now.
As Manning and Schu?tze (Man-ning and Schu?tze, 2000) point out, most workon acquiring semantic properties of words has fo-cused on semantic similarity.
?Automatically ac-quiring a relative measure of how similar a wordis to known words [...] is much easier than deter-mining what the actual meaning is.?
(Manning andSchu?tze, 2000, ?8.5)c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Most work on semantic similarity relies on thedistributional hypothesis (Harris, 1985).
This hy-pothesis states that words that occur in similar con-texts tend to be similar.
With regard to the contextused, two basic approaches exist.
One approachmakes use of ?bag of words?
co-occurrence data; inthis approach, a certain window around a word isused for gathering co-occurrence information.
Thewindow may either be a fixed number of words,or the paragraph or document that a word appearsin.
Thus, words are considered similar if they ap-pear in similar windows (documents).
One of thedominant methods using this method is LATENTSEMANTIC ANALYSIS (LSA).The second approach uses a more fine graineddistributional model, focusing on the syntactic re-lations that words appear with.
Typically, a largetext corpus is parsed, and dependency triples areextracted.1 Words are considered similar if theyappear with similar syntactic relations.
Note thatthe former approach does not need any kind oflinguistic annotation, whereas for the latter, someform of syntactic annotation is needed.The results yielded by both approaches are typ-ically quite different in nature: the former ap-proach typically puts its finger on a broad, the-matic kind of similarity, while the latter approachtypically grasps a tighter, synonym-like similarity.Example (1) shows the difference between bothapproaches; for each approach, the top ten mostsimilar nouns to the Dutch noun muziek ?music?are given.
In (a), the window-based approach isused, while (b) uses the syntax-based approach.
(a)shows indeed more thematic similarity, whereas(b) shows tighter similarity.1e.g.
dependency relations that qualify apple might be?object of eat?
and ?adjective red?.
This gives us dependencytriples like < apple, obj, eat >.929(1) a. muziek ?music?
: gitaar ?guitar?, jazz ?jazz?,cd ?cd?, rock ?rock?, bas ?bass?, song ?song?,muzikant ?musician?, musicus ?musician?, drum?drum?, slagwerker ?drummer?b.
muziek ?music?
: dans ?dance?, kunst ?art?,klank ?sound?, liedje ?song?, geluid ?sound?,poe?zie ?poetry?, literatuur ?literature?, pop-muziek ?pop music?, lied ?song?, melodie?melody?Especially the syntax-based method has beenadopted by many researchers, in order to find se-mantically similar words.
There is, however, oneimportant problem with this kind of approach: themethod is not able to cope with ambiguous words.Take the examples:(2) eenaonevenoddnummernumberan odd number(3) eenasteengoedgreatnummernumber?a great song?The word nummer does not have the samemeaning in these examples.
In example (2), num-mer is used in the sense of ?designator of quantity?.In example (3), it is used in the sense of ?musi-cal performance?.
Accordingly, we would like theword nummer to be disambiguated into two senses,the first sense being similar to words like getal?number?, cijfer ?digit?
and the second to wordslike liedje ?song?, song ?song?.While it is relatively easy for a human languageuser to distinguish between the two senses, thisis a difficult task for a computer.
Even worse:the results get blurred because the attributes ofboth senses (in this example oneven and steen-goed) are grouped together into one sense.
Thisis the main drawback of the syntax-based method.On the other hand, methods that capture seman-tic dimensions are known to be useful in disam-biguating different senses of a word.
Particu-larly, PROBABILISTIC LATENT SEMANTIC ANAL-YSIS (PLSA) is known to simultaneously encodevarious senses of words according to latent seman-tic dimensions (Hofmann, 1999).
In this paper, wewant to explore an approach that tries to remedythe shortcomings of the former, syntax-based ap-proach with the benefits of the latter.
The intuitionin this is that the syntactic features of the syntax-based approach can be disambiguated by the ?la-tent semantic dimensions?
found by the window-based approach.2 Previous Work2.1 Distributional SimilarityThere have been numerous approaches for com-puting the similarity between words from distribu-tional data.
We mention some of the most impor-tant ones.With regard to the first approach ?
using a con-text window ?
we already mentioned LSA (Lan-dauer and Dumais, 1997).
In LSA, a term-document matrix is created, containing the fre-quency of each word in a specific document.
Thismatrix is then decomposed into three other matri-ces with a mathematical technique called SINGU-LAR VALUE DECOMPOSITION.
The most impor-tant dimensions that come out of the SVD allegedlyrepresent ?latent semantic dimensions?, accordingto which nouns and documents can be presentedmore efficiently.LSA has been criticized for not being the mostappropriate data reduction method for textual ap-plications.
The SVD underlying the method as-sumes normally-distributed data, whereas textualcount data (such as the term-document matrix)can be more appropriately modeled by other dis-tributional models such as Poisson (Manning andSchu?tze, 2000, ?15.4.3).
Successive methods suchas PROBABILISTIC LATENT SEMANTIC ANALY-SIS (PLSA) (Hofmann, 1999), try to remedy thisshortcoming by imposing a proper latent variablemodel, according to which the values can be es-timated.
The method we adopt in our research?
NON-NEGATIVE MATRIX FACTORIZATION ?
issimilar to PLSA, and adequately remedies thisproblem as well.The second approach ?
using syntactic relations?
has been adopted by many researchers, in orderto acquire semantically similar words.
One of themost important is Lin?s (1998).
For Dutch, the ap-proach has been applied by Van der Plas & Bouma(2005).2.2 Discriminating sensesSchu?tze (1998) uses a disambiguation algorithm ?called context-group discrimination ?
based on theclustering of the context of ambiguous words.
Theclustering is based on second-order co-occurrence:the contexts of the ambiguous word are similar ifthe words they in turn co-occur with are similar.Pantel and Lin (2002) present a clustering al-gorithm ?
coined CLUSTERING BY COMMITTEE(CBC) ?
that automatically discovers word senses930from text.
The key idea is to first discover a setof tight, unambiguous clusters, to which possiblyambiguous words can be assigned.
Once a wordhas been assigned to a cluster, the features associ-ated with that particular cluster are stripped off theword?s vector.
This way, less frequent senses ofthe word can be discovered.The former approach uses a window-basedmethod; the latter uses syntactic data.
But noneof the algorithms developed so far have combinedboth sources in order to discriminate among differ-ent senses of a word.3 Methodology3.1 Non-negative Matrix Factorization3.1.1 TheoryNon-negative matrix factorization (NMF) (Leeand Seung, 2000) is a group of algorithms in whicha matrix V is factorized into two other matrices, Wand H .Vn?m?Wn?rHr?m(1)Typically r is much smaller than n,m so thatboth instances and features are expressed in termsof a few components.Non-negative matrix factorization enforces theconstraint that all three matrices must be non-negative, so all elements must be greater than orequal to zero.
The factorization turns out to beparticularly useful when one wants to find additiveproperties.Formally, the non-negative matrix factorizationis carried out by minimizing an objective function.Two kinds of objective function exist: one thatminimizes the Euclidean distance, and one thatminimizes the Kullback-Leibler divergence.
Inthis framework, we will adopt the latter, as ?
fromour experience ?
entropy-based measures tend towork well for natural language.
Thus, we want tofind the matrices W and H for which the Kullback-Leibler divergence between V and WH (the mul-tiplication of W and H) is the smallest.Practically, the factorization is carried outthrough the iterative application of update rules.Matrices W and H are randomly initialized, andthe rules in 2 and 3 are iteratively applied ?
alter-nating between them.
In each iteration, each vec-tor is adequately normalized, so that all dimensionvalues sum to 1.Ha??
Ha??iWiaVi?(WH)i??kWka(2)Wia?Wia??Ha?Vi?(WH)i?
?vHav(3)3.1.2 ExampleWe can now straightforwardly apply NMF tocreate semantic word models.
NMF is applied toa frequency matrix, containing bag of words co-occurrence data.
The additive property of NMF en-sures that semantic dimensions emerge, accordingto which the various words can be classified.
Twosample dimensions are shown in example (4).
Foreach dimension, the words with the largest valueon that dimension are given.
Dimension (a) canbe qualified as a ?transport?
dimension, and dimen-sion (b) as a ?cooking?
dimension.
(4) a. bus ?bus?, taxi ?taxi?, trein ?train?, halte ?stop?,reiziger ?traveler?, perron ?platform?, tram?tram?, station ?station?, chauffeur ?driver?,passagier ?passenger?b.
bouillon ?broth?, slagroom ?cream?, ui ?onion?,eierdooier ?egg yolk?, laurierblad ?bay leaf?,zout ?salt?, deciliter ?decilitre?, boter ?butter?,bleekselderij ?celery?, saus ?sauce?3.2 Extending Non-negative MatrixFactorizationWe now propose an extension of NMF that com-bines both the bag of words approach and the syn-tactic approach.
The algorithm finds again latentsemantic dimensions, according to which nouns,contexts and syntactic relations are classified.Since we are interested in the classification ofnouns according to both ?bag-of-words?
contextand syntactic context, we first construct three ma-trices that capture the co-occurrence frequency in-formation for each mode.
The first matrix con-tains co-occurrence frequencies of nouns cross-classified by dependency relations, the second ma-trix contains co-occurrence frequencies of nounscross-classified by words that appear in the noun?scontext window, and the third matrix contains co-occurrence frequencies of dependency relationscross-classified by co-occurring context words.We then apply NMF to the three matrices, but weinterleave the separate factorizations: the results ofthe former factorization are used to initialize thefactorization of the next matrix.
This implies thatwe need to initialize only three matrices at random;the other three are initialized by calculations of the931previous step.
The process is represented graphi-cally in figure 1.Figure 1: A graphical representation of the ex-tended NMFIn the example in figure 1, matrix H is initial-ized at random, and the update of matrix W is cal-culated.
The result of update W is then used toinitialize matrix V , and the update of matrix G iscalculated.
This matrix is used again to initializematrix U , and the update of matrix F is calculated.This matrix can be used to initialize matrix H , andthe process is repeated until convergence.In (5), an example is given of the kind of se-mantic dimensions found.
This dimension may becoined the ?transport?
dimension, as is shown bythe top 10 nouns (a), context words (b) and syntac-tic relations (c).
(5) a. auto ?car?, wagen ?car?, tram ?tram?, motor?motorbike?, bus ?bus?, metro ?subway?, auto-mobilist ?driver?, trein ?trein?, stuur ?steeringwheel?, chauffeur ?driver?b.
auto ?car?, trein ?train?, motor ?motorbike?, bus?bus?, rij ?drive?, chauffeur ?driver?, fiets ?bike?,reiziger ?reiziger?, passagier ?passenger?, ver-voer ?transport?c.
viertrapsadj?four pedal?, verplaats metobj?move with?, toeteradj?honk?, tank in houdobj[parsing error], tanksubj?refuel?, tankobj?re-fuel?, rij voorbijsubj?pass by?, rij voorbijadj?pass by?, rij afsubj?drive off?, peperduuradj?very expensive?3.3 Sense SubtractionNext, we want to use the factorization that has beencreated in the former step for word sense discrim-ination.
The intuition is that we ?switch off?
onedimension of an ambiguous word, to reveal pos-sible other senses of the word.
From matrix H,we know the importance of each syntactic relationgiven a dimension.
With this knowledge, we can?subtract?
the syntactic relations that are responsi-ble for a certain dimension from the original nounvector:??vnew=??vorig(?
?1 ??
?hdim) (4)Equation 4 multiplies each feature (syntactic re-lation) of the original noun vector (?
?vorig) with ascaling factor, according to the load of the featureon the subtracted dimension (??hdim?
the vectorof matrix H containing the dimension we want tosubtract).
?
?1 is a vector of ones, the size of ?
?hdim.3.4 A Clustering FrameworkThe last step is to determine which dimension(s)are responsible for a certain sense of the word.
Inorder to do so, we embed our method in a cluster-ing approach.
First, a specific word is assigned toits predominant sense (i.e.
the most similar clus-ter).
Next, the dominant semantic dimension(s)for this cluster are subtracted from the word vec-tor (equation 4), and the resulting vector is fed tothe clustering algorithm again, to see if other wordsenses emerge.
The dominant semantic dimen-sion(s) can be identified by ?folding in?
the clustercentroid into our factorization (so we get a vec-tor ?
?w of dimension size r), and applying a thresh-old to the result (in our experiments a threshold of?
= .05 ?
so dimensions responsible for > 5% ofthe centroid are subtracted).We used two kinds of clustering algorithms todetermine our initial centroids.
The first algorithmis a standard K-means algorithm.
The second oneis the CBC algorithm by Pantel and Lin (2002).The initial vectors to be clustered are adapted withpointwise mutual information (Church and Hanks,1990).3.4.1 K-meansFirst, a standard K-means algorithm is appliedto the nouns we want to cluster.
This yields a hardclustering, in which each noun is assigned to ex-actly one (dominant) cluster.
In the second step,we try to determine for each noun whether it canbe assigned to other, less dominant clusters.
First,the salient dimension(s) of the centroid to whichthe noun is assigned are determined.
We com-pute the centroid of the cluster by averaging thefrequencies of all cluster elements except for thetarget element we want to reassign, and adapt thecentroid with pointwise mutual information.
After932subtracting the salient dimensions from the nounvector, we check whether the vector is reassignedto another cluster centroid (i.e.
whether it is moresimilar to a different centroid).
If this is the case,(another instance of) the noun is assigned to thecluster, and we repeat the second step.
If there isno reassignment, we continue with the next word.The target element is removed from the centroidto make sure that we only subtract the dimensionsassociated with the sense of the cluster.Note that K-means requires to set the number ofclusters beforehand, so k is a parameter to be set.3.4.2 CBCThe second clustering algorithm operates in asimilar vein, but instead of using simple K-means,we use Pantel and Lin?s CBC algorithm to find theinitial centroids (coined COMMITTEES).In order to find committees, the top k nounsfor each noun in the database are clustered withaverage-link clustering.
The clusters are scoredand sorted in such a way that preference is givento tight, representative clusters.
If the committeesdo not cover all elements sufficiently, the algorithmrecursively tries to find more committees.
An elab-orate description of the algorithm can be found in(Pantel and Lin, 2002).In the second step, we start assigning elementsto committees.
Once an element is assigned, thesalient dimensions are subtracted from the nounvector in the same way as in 3.4.1 (only do we nothave to remove any target word from the centroid;committees are supposed to represent tight, unam-biguous clusters).CBC attempts to find the number of committeesautomatically from the data, so k does not have tobe set.4 Examples4.1 Sense SubtractionIn what follows, we will talk about semantic di-mensions as, e.g., the ?music?
dimension or the?city?
dimension.
In the vast majority of the cases,the dimensions are indeed as clear-cut as the trans-port dimension shown above, so that the dimen-sions can be rightfully labeled this way.Two examples are given of how the semanticdimensions that have been found can be used forword sense discrimination.
We will consider twoambiguous nouns: pop, which can mean ?pop mu-sic?
as well as ?doll?, and Barcelona, which candesignate either the Spanish city or the Spanishfootball club.First, we look up the top dimensions for eachnoun.
Next, we successively subtract the dimen-sions dealing with a particular sense of the noun,as described in 3.3.
This gives us three vectorsfor each noun: the original vector, and two vectorswith one of the dimensions eliminated.
For each ofthese vectors, the top ten similar nouns are given,in order to compare the changes brought about.
(6) a. pop, rock, jazz, meubilair ?furniture?, pop-muziek ?pop music?, heks ?witch?, speelgoed?toy?, kast ?cupboard?, servies ?
[tea] service?,vraagteken ?question mark?b.
pop, meubilair ?furniture?, speelgoed ?toy?,kast ?cupboard?, servies ?
[tea] service?, heks?witch?, vraagteken ?question mark?
sieraad?jewel?, sculptuur ?sculpture?, schoen ?shoe?c.
pop, rock, jazz, popmuziek ?pop music?, heks?witch?, danseres ?dancer?, servies ?
[tea] ser-vice?, kopje ?cup?, house ?house music?, aap?monkey?Example (6) shows the top similar words for thethree vectors of pop.
In (a), the most similar wordsto the original vector are shown.
In (b), the topdimension (the ?music dimension?)
has been sub-tracted from (a), and in (c), the second highest di-mension (a ?domestic items?
dimension) has beensubtracted from (a).The differences between the three vectors areclear: in vector (a), both senses are mixed together,with ?pop music?
and ?doll?
items interleaved.
In(b), no more music items are present.
Only itemsrelated to the doll sense are among the top similarwords.
In (c), the music sense emerges much moreclearly, with rock, jazz and popmuziek being themost similar, and a new music term (house) show-ing up among the top ten.Admittedly, in vector (c), not all items related tothe ?doll?
sense are filtered out.
We believe thisis due to the fact that this sense cannot be ade-quately filtered out by one dimension (in this case,a dimension of ?domestic items?
alone), whereas itis much easier to filter out the ?music?
sense withonly one ?music?
dimension.
We will try to rem-edy this in our clustering framework, in which it ispossible to subtract multiple dimensions related toone sense.A second example, the ambiguous proper nameBarcelona, is given in (7).
(7) a. Barcelona, Arsenal, Inter, Juventus, Vitesse,Milaan ?Milan?, Madrid, Parijs ?Paris?, Wenen?Vienna?, Mu?nchen ?Munich?b.
Barcelona, Milaan ?Milan?, Mu?nchen ?Mu-933nich?, Wenen ?Vienna?, Madrid, Parijs ?Paris?,Bonn, Praag ?Prague?, Berlijn ?Berlin?, Londen?London?c.
Barcelona, Arsenal, Inter, Juventus, Vitesse,Parma, Anderlecht, PSV, Feyenoord, AjaxIn (a), the two senses of Barcelona are clearlymixed up, showing cities as well as football clubsamong the most similar nouns.
In (b), wherethe ?football dimension?
has been subtracted, onlycities show up.
In (c), where the ?city dimension?has been subtracted, only football clubs remain.4.2 Clustering OutputIn (8), an example of our clustering algorithm withinitial K-means clusters is given.
(8) a. werk ?work?
beeld ?image?
foto ?photo?schilderij ?painting?
tekening ?drawing?
doek?canvas?
installatie ?installation?
afbeelding?picture?
sculptuur ?sculpture?
prent ?pic-ture?
illustratie ?illustration?
handschrift?manuscript?
grafiek ?print?
aquarel ?aquarelle?maquette ?scale-model?
collage ?collage?
ets?etching?b.
werk ?work?
boek ?book?
titel ?title?
roman?novel?
boekje ?booklet?
debuut ?debut?
bi-ografie ?biography?
bundel ?collection?
toneel-stuk ?play?
bestseller ?bestseller?
kinderboek?child book?
autobiografie ?autobiography?novelle ?short story?c.
werk ?work?
voorziening ?service?
arbeid?labour?
opvoeding ?education?
kinderopvang?child care?
scholing ?education?
huisvest-ing ?housing?
faciliteit ?facility?
accommodatie?acommodation?
arbeidsomstandigheid ?work-ing condition?The example shows three different clusters towhich the noun werk ?work?
is assigned.
In (a),werk refers to a work of art.
In (b), it refers to awritten work.
In (c), the ?labour?
sense of werkemerges.5 Evaluation5.1 MethodologyThe clustering results are evaluated according toDutch EuroWordNet (Vossen and others, 1999).Precision and recall are calculated by comparingthe results to EuroWordNet synsets.
The precisionis the number of clusters found that correspond toan actual sense of the word.
Recall is the numberof word senses in EuroWordNet that are found bythe algorithm.
Our evaluation method is largely thesame as the one used by Pantel and Lin (2002).Both precision and recall are based on wordnetsimilarity.
A number of similarity measures havebeen developed to calculate semantic similarity ina hierarchical wordnet.
Among these measures,the most important are Wu & Palmer?s (Wu andPalmer, 1994), Resnik?s (Resnik, 1995) and Lin?s(Lin, 1998).
In this evaluation, Wu & Palmer?s(1994) measure will be adopted.
The similarity iscalculated according to the formula in (5), in whichN1and N2are the number of is-a links from A andB to their most specific common superclass C; N3is the number of is-a links from C to the root ofthe taxonomy.simWu&Palmer(A,B) =2N3N1+N2+ 2N3(5)ietsobjectwezenorganismedierzoogdier vishond zalmFigure 2: Extract from the Dutch EuroWordNetHierarchyFor example, the most common superclass ofhond ?dog?
en zalm ?salmon?
is dier ?animal?
(ascan be seen on the extract from Dutch EuroWord-Net in figure 2).
Consequently, N1= 2, N2= 2,N3= 4 and simWP(hond, zalm) = 0.67.To calculate precision, we apply the samemethodology as Pantel and Lin (2002).2 Let S(w)be the set of EuroWordNet senses.
simW(s, u),the similarity between a synset s and a word u isthen defined as the maximum similarity between sand a sense of u:simW(s, u) = maxt?S(u)sim(s, t) (6)Let ckbe the top k-members of a cluster c,where these are the k most similar members to thecentroid of c. simC(c, s), the similarity betweens and c, is then defined as the average similaritybetween s and the top-k members of c:simC(s, c) =?u?cksimW (s, u)k(7)2Note, however, that our similarity measure is different.Where Pantel and Lin use Lin?s (1998) measure, we use Wuand Palmer?s (1994) measure.934An assigment of a word w to a cluster c can nowbe classified as correct ifmaxs?S(w)simC(s, c) > ?
(8)and the EuroWordNet sense of w that corre-sponds to c isargmaxs?S(w)simC(s, c) (9)When multiple clusters correspond to the sameEuroWordNet sense, only one of them is countedas correct.Precision of a word w is the percentage of cor-rect clusters to which it is assigned.
Recall ofa word w is the percentage of senses from Eu-roWordnet that have a corresponding cluster.3 Pre-cision and recall of a clustering algorithm is theaverage precision and recall of all test words.5.2 Experimental DesignWe have applied the interleaved NMF presented insection 3.2 to Dutch, using the TWENTE NIEUWSCORPUS (Ordelman, 2002), containing > 500Mwords of Dutch newspaper text.
The corpus is con-sistently divided into paragraphs, which have beenused as the context window for the bag-of-wordsmode.
The corpus has been parsed by the Dutchdependency parser Alpino (van Noord, 2006), anddependency triples have been extracted.
Next, thethree matrices needed for our method have beenconstructed: one containing nouns by dependencyrelations (5K ?
80K), one containing nouns bycontext words (5K ?
2K) and one containing de-pendency relations by context words (80K ?
2K).We did 200 iterations of the algorithm, factorizingthe matrices into 50 dimensions.
The NMF algo-rithm has been implemented in Matlab.For the evaluation, we use all the words that ap-pear in our original clustering input as well as inEuroWordNet.
This yields a test set of 3683 words.5.3 ResultsTable 1 shows precision and recall figures for fourdifferent algorithms, according to two similaritythresholds ?
(equation 8).
kmeansnmfdescribesthe results of our algorithm with K-means clus-ters, as described in section 3.4.1.
CBC describes3Our notion of recall is slightly different from the one usedby Pantel and Lin, as they use ?the number of senses in whichw was used in the corpus?
as gold standard.
This information,as they acknowledge, is difficult to get at, so we prefer to usethe sense information in EuroWordNet.the results of our algorithm with the CBC commit-tees, as described in section 3.4.2.
For comparison,we have also included the results of a standard K-means clustering (kmeansorig, k = 600), and theoriginal CBC algorithm (CBCorig) as described byPantel and Lin (2002).threshold ?.40 (%) .60 (%)kmeansnmfprec.
78.97 55.16rec.
63.90 44.77CBCnmfprec.
82.70 54.87rec.
60.27 40.51kmeansorigprec.
86.13 58.97rec.
60.23 41.80CBCorigprec.
44.94 29.74rec.
69.61 48.00Table 1: Precision and recall for four different al-gorithms according to two similarity thresholdsThe results show the same tendency across allsimilarity thresholds: kmeansnmfhas a high pre-cision, but lower recall compared to CBCorig.
Stillthe recall is higher compared to standard K-means,which indicates that the algorithm is able to findmultiple senses of nouns, with high precision.
Theresults of CBCnmfare similar to the results ofkmeansorig, indicating that few words are reas-signed to multiple clusters when using CBC com-mittees with our method.Obviously, kmeansorigscores best with regardto precision, but worse with regard to recall.CBCorigfinds most senses (highest recall), but pre-cision is considerably worse.The fact that recall is already quite high withstandard K-means clustering indicates that theevaluation is skewed towards nouns with only onesense, possibly due to a lack of coverage in Eu-roWordNet.
In future work, we specifically wantto evaluate the discrimination of ambiguous words.Also, we want to make use of the new CornettoDatabase4, a successor of EuroWordNet for Dutchwhich is currently under development.Still, the evaluation shows that our method pro-vides a genuine way of finding multiple senses ofwords, while retaining high precision.
Especiallythe method using a simple K-means clustering per-4http://www.let.vu.nl/onderzoek/projectsites/cornetto/index.html935forms particularly well.
The three way data al-lows the algorithm to put its finger on the particularsense of a centroid, and adapt the feature vector ofa possibly ambiguous noun accordingly.6 Conclusion & Future WorkIn this paper, an extension of NMF has been pre-sented that combines both bag of words data andsyntactic data in order to find latent semantic di-mensions according to which both words and syn-tactic relations can be classified.
The use of threeway data allows one to determine which dimen-sion(s) are responsible for a certain sense of aword, and adapt the corresponding feature vec-tor accordingly, ?subtracting?
one sense to dis-cover another one.
When embedded in a clusteringframework, the method provides a fully automaticway to discriminate the various senses of words.The evaluation against EuroWordNet shows thatthe algorithm is genuinely able to disambiguate thefeatures of a given word, and accordingly its wordsenses.We conclude with some issues for future work.First of all, we would like to test the method thathas been explored in this paper with other evalua-tion frameworks.
We already mentioned the focuson ambiguous nouns, and the use of the new Cor-netto database for Dutch.
Next, we would like towork out a proper probabilistic framework for the?subtraction?
of dimensions.
At this moment, thesubtraction (using a cut-off) is somewhat ad hoc.
Aprobabilistic modeling of this intuition might leadto an improvement.And finally, we would like to use the results ofour method to learn selectional preferences.
Ourmethod is able to discriminate the syntactic fea-tures that are linked to a particular word sense.
Ifwe can use the results to improve a parser?s perfor-mance, this will also provide an external evaluationof the algorithm.ReferencesChurch, Kenneth Ward and Patrick Hanks.
1990.
Wordassociation norms, mutual information & lexicogra-phy.
Computational Linguistics, 16(1):22?29.Harris, Z.
1985.
Distributional structure.
In Katz, Jer-rold J., editor, The Philosophy of Linguistics, pages26?47.
Oxford University Press.Hofmann, Thomas.
1999.
Probabilistic latent semanticanalysis.
In Proc.
of Uncertainty in Artificial Intelli-gence, UAI?99, Stockholm.Landauer, Thomas and Se Dumais.
1997.
A solution toPlato?s problem: The Latent Semantic Analysis the-ory of the acquisition, induction, and representationof knowledge.
Psychology Review, 104:211?240.Lee, Daniel D. and H. Sebastian Seung.
2000.
Al-gorithms for non-negative matrix factorization.
InNIPS, pages 556?562.Lin, D. 1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING/ACL 98,Montreal, Canada.Manning, Christopher and Hinrich Schu?tze.
2000.Foundations of Statistical Natural Language Pro-cessing.
MIT Press, Cambridge, Massachussets.Ordelman, R.J.F.
2002.
Twente Nieuws Corpus(TwNC), August.
Parlevink Language TechonologyGroup.
University of Twente.Pantel, Patrick and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 613?619,New York, NY, USA.
ACM Special Interest Groupon Knowledge Discovery in Data, ACM Press.Resnik, Philip.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In IJ-CAI, pages 448?453.Schu?tze, Hinrich.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.van der Plas, Lonneke and Gosse Bouma.
2005.Syntactic contexts for finding semantically similarwords.
In van der Wouden, Ton et al, editors,Computational Linguistics in the Netherlands 2004.Selected Papers from the Fifteenth CLIN Meeting,pages 173?184, Utrecht.
LOT.van Noord, Gertjan.
2006.
At Last Parsing Is NowOperational.
In Mertens, Piet, Cedrick Fairon, AnneDister, and Patrick Watrin, editors, TALN06.
VerbumEx Machina.
Actes de la 13e conference sur le traite-ment automatique des langues naturelles, pages 20?42, Leuven.Vossen, Piek et al 1999.
The Dutch Wordnet, July.University of Amsterdam.Wu, Zhibiao and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In 32nd.
Annual Meeting ofthe Association for Computational Linguistics, pages133?138, New Mexico State University, Las Cruces,New Mexico.936
