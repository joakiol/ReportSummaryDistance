Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 446?453,Prague, June 2007. c?2007 Association for Computational LinguisticsUSYD: WSD and Lexical Substitution using the Web1T CorpusTobias HawkerSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiatoby@it.usyd.edu.auAbstractThis paper describes the University of Syd-ney?s WSD and Lexical Substitution sys-tems for SemEval-2007.
These systems areprincipally based on evaluating the substi-tutability of potential synonyms in the con-text of the target word.
Substitutability ismeasured using Pointwise Mutual Informa-tion as obtained from the Web1T corpus.The WSD systems are supervised, whilethe Lexical Substitution system is unsuper-vised.
The lexical sample sub-task also usedsyntactic category information given from aCCG-based parse to assist in verb disam-biguation, while both WSD tasks also makeuse of more traditional features.These related systems participated in theCoarse-Grained English All-Words WSDtask (task 7), the Lexical Substitution Task(task 10) and the English Lexical SampleWSD sub-task (task 17).1 IntroductionThis paper describes closely related systems thatwere applied to three tasks of the SemEval-2007workshop.
The unifying characteristic of these sys-tems is that they use the same measure of ?substi-tutability?
for a given word and a surrounding con-text to perform the tasks.
This measure is basedon frequencies involving the word and the contextfrom n-gram counts derived from one trillion wordsof Web text.These systems participated in the English Coarse-Grained All Words and English Lexical SampleWord Sense Disambiguation (WSD) tasks, and inthe Lexical Substitution task.The Lexical Substitution system relies entirely onthe substitutability measure to rank potential syn-onyms, and only uses manual sense inventories topreferentially select words which have been iden-tified by lexicographers as being synonyms for theoriginal word in some contexts.
It does not make useof any machine learning, and is thus unsupervised.The WSD systems are supervised, using a Sup-port Vector Machine (SVM) to learn from sense-tagged examples of ambiguous words and predictthe class of the test instances.
Classifiers for bothsystems use a small number of additional featuretypes beyond those derived from the n-gram counts,including Bag of Words (BOW) and local contextfeatures.
A single separate model was trained foreach ambiguous lemma.For verbs in the lexical sample, the classifier alsouses the syntactic category assigned to the targetverb by a parser as additional information for dis-ambiguation.The remainder of this paper is organised as fol-lows.
Relevant background for the ideas employedis briefly discussed, as is the nature of the Web1Tcorpus.
Descriptions of the particular systems usedfor each of the tasks are described in ascending or-der of task number.
Details of particular sources ofinformation and the methods used to capture themare introduced along with the task they are used in.A presentation of results and discussion follows thedescription of each system, and overall conclusions446are presented at the end of the paper.2 BackgroundAlgorithms making use of unannotated data forWSD and similar tasks are not particularly new.One strategy which resembles the substitutabilitytechnique employed by our systems is relatives-in-context (Martinez et al, 2006), an unsupervised ap-proach which uses a web search engine to find the?best?
match for the current context, according toheuristic criteria.
Monosemous relatives (Leacocket al, 1998) increase the amount of training datafor supervised learners by recruiting the contexts ofsynonyms in unannotated data, with the caveat thatthose synonyms are not themselves ambiguous.
Assubstantial gold-standard data sets for lexical sub-stitution have not previously been available, the Se-mEval data presents a promising opportunity to ex-amine the behaviour of our method.Gomez (2001) argues that the syntactic roles ofambiguous verbs in particular are interlinked withtheir semantic class, and thus knowledge about thesyntactic function of a verb can provide informationto help identify its sense.
Syntactic relationshipshave been used to resolve ambiguity (Lin, 1997) anda reduction of ambiguity has been shown to assistin the acquisition of verb subcategorization frames(Korhonen and Preiss, 2003).3 The Substitutability MeasureAs an example to demonstrate the basic mechanismunderlying the measure of substitutability, considerthe sentence fragments around the verb ruled in:the court ruled it was clear thatanda republic ruled by the peopleTwo possible synonyms, pertaining to differentsenses for the verb ruled, are found and governed.It is clear that in a sufficiently large quantity of text,the fragments:the court found it was clear thatanda republic governed by the peoplewould be substantially more common than the se-quences:the court governed it was clear thatora republic found by the peopleand thus found should be considered more substi-tutable in the context of the first fragment, and gov-erned in the second.Church et al (1994) show that Pointwise MutualInformation (PMI) is a suitable measure to capturethe degree to which a given word may substitutefor another; we have adopted PMI as the quantifiedmeasure of substitutability in the systems used forthese tasks.While previous WSD systems have made use ofcounts obtained from Internet search engines, forexample Martinez et al (2006), to our knowledgeWSD using corpus data at the scale of the Web1Tresource has not previously been published.
OurWSD systems combine our novel PMI-Web1T fea-tures and CCG category features with additional fea-tures described in the literature.
While the Web1Tcorpus consists only of counts, and thus is some-what similar to the direct use of counts from Internetsearch engines, it is also of a known size and thusit is straightforward to determine useful quantitiessuch as PMI, and to exhaustively catalog potentialmatches as for the lexical substitution task.3.1 Web1T CorpusThe Web1T corpus (Brants and Franz, 2006) is adataset consisting of the counts for n-grams obtainedfrom 1 trillion (1012) words of English Web text,subject to a minimum occurrence threshold (200 in-stances for unigrams, 40 for others).
The Web1Tcorpus contains counts for 1, 2, 3, 4 and 5-grams,and is large enough to present serious processing dif-ficulties: it is 25GB in compressed form.The systems presented here thus use custom high-performance software to extract only the n-gramcounts of interest from the Web1T data, includ-ing simple wildcard pattern-matching.
The scale ofthe data rules out attempting to perform arbitraryqueries ?
even though the counts are lexicographi-cally ordered, disk access times and decompressionoverheads are severe, and case-insensitive queriesare not possible.
This software will be released forcommunity use.
A limitation in the implementationis that the number of tokens that can be matched ina wildcard expression is fixed at one.
This limita-tion precluded the testing of substitutability of multi-word-expressions (MWEs) in the systems applied to447the SemEval tasks.4 Task 7: Coarse Grained-EnglishAll-Words WSDThe system for Coarse-Grained All-Words WSDwas supervised, but only attempted classification fora subset of words.
These words were chosen ac-cording to the amount of sense-tagged training dataavailable, drawn from SemCor (Miller et al, 1993)and the SenseEval-3 lexical sample (Mihalcea et al,2004) task.
Features were extracted and a classifiertrained for each ambiguous content word that waseither present in the SenseEval-3 lexical sample, oroccurred at least 100 times in SemCor.
These crite-ria yielded classifiers for 183 words.For ambiguous words without sufficient availabletraining data, the first sense baseline (determinedfrom WordNet version 2.1 (Fellbaum, 1998)) wasassigned to every instance.
No manual augmentationof the information from WordNet was performed.For those words where models were being trained,the sense clusterings provided by the task organis-ers were used to completely unify all senses belong-ing to a cluster, thus attempting disambiguation atthe level of the coarse senses.
As the system doesnot attempt to disambiguate words not selected formodeling, the exclusion of the most frequent sense(MFS) baseline would be likely to have a severe ad-verse impact on this type of supervised approach.Extension of the substitutability measure to directlyselect a sense related to good substitutes, similar tothe approach outlined in Lin (1997) would be onepossible approach to resolve this consistently.The classifier used for the system was an SVM(libsvm) (Chang and Lin, 2001).
Linear kernelswere used, as previous experiments using similarfeatures with other data sets for WSD had shownthat these kernels outperformed radial basis func-tion and polynomial kernels; this disparity becameparticularly pronounced with larger number of fea-tures compared to training instances, and with thecombination of different feature types.
The num-ber of unique features for each lemma was, on av-erage, more than an order of magnitude higher thanthe number of training instances: 4475 compared to289.The features used to train the selected lemmas in-cluded the substitutability measurement, all contentwords within 3 sentences of the target, and imme-diate local context features.
These are detailed be-low.
There is no in-principle reason why CCG cate-gory features used for the Lexical Sample task (seeSection 6.2) could not also be used for verbs in theall-words task.
Sentences containing target verbscould have been selectively parsed and redundancyamong disambiguated running text in SemCor ex-ploited.
However, the system architecture was notamenable to small modifications along these lines,and time constraints prevented implementation be-fore the close of the evaluation period.
The impactof this additional useful feature would be an inter-esting subject for future study.4.1 Features4.1.1 Substitutability: Pointwise MutualInformationTo transform the notion of substitutability into aset of features suitable for WSD, a set of poten-tial substitute words was chosen for each modeledlemma.
These words were taken from WordNet 2.1(Fellbaum, 1998).
For nouns, all synonyms, imme-diate hypernyms and immediate hyponyms for allsenses were included.
For verbs, synonyms for allsenses were used.
The selection of potential sub-stitutes was stricter for verbs as the number of syn-onyms tended to be greater than for nouns, and thesecriteria kept the number of substitutes manageable.A sliding window was used to maximise the infor-mation extracted from the Web1T corpus.
All win-dows at all sizes covered by the Web1T corpus thatincluded the target word were used to determine theoverall substitutability.The counts of interest for determining the PMI fora single substitute in a single window position in-clude the unigram frequency of the substitute itselfthe overall frequency of the context, irrespective ofthe word in the target position; and crucially, the fre-quency of the substitute in that context.
For a givensubstitute and context, an overall PMI is determinedas a single quantity, obtained by simply adding thePMI together from each window position of eachsize covered in the data:448PMI =5?n=2n?i=1log2observationn,iexpectationn,i=5?n=2n?i=1log2#(sub + contextn,i)p(sub) ?
p(contextn,i) ?NnHere n represents the window size (varying from2 to 5), i is the position within the window, andNn indicates the total number of n-grams presentin the corpus for a given value of n. FollowingChurch et al (1994) the Maximum Likelihood Es-timate (MLE) is used for both probabilities in thedenominator.
p(sub) is estimated from the unigramfrequency of the substitute word, while p(context)is derived from the counts of the context ignoring thetoken in the target location.Features were also created that harnessed the ideathat it is not only the level of substitutability for eachcandidate word that is useful, but also that it may beinformative to recognise that some words are bettersubstitutes than others.
This information was cap-tured by adding additional features consisting of thepairwise differences between PMI values for all can-didate substitute words.
To further draw the dif-fering levels of substitutability into relief, featuresrepresenting the rank of each pair?s PMI differencewere also included.Finally, each of the above feature types yieldsreal-valued features.
Before being used in clas-sification, these features were converted to binaryfeatures using supervised Entropy-Based Discretisa-tion (Fayyad and Irani, 1993).
This process char-acterises the partition selection as a message cod-ing problem: the class labels in the training dataare a message to be encoded given that the value ofthe feature is known for each instance, and the pro-cess aims to minimise the length of that message.This is achieved by recursively bifurcating each fea-ture?s values at the partition point that would resultin the shortest message.
Useful boundaries are thosewhere knowing which side of the partition the fea-ture value falls on can be used to reduce the mes-sage length beyond any increase required to specifythe partition.
The algorithm terminates when the ex-isting partitions cannot be divided further and stillsatisfy this condition.
If this occurs when attempt-ing to find the first partition, the feature is droppedaltogether.4.1.2 Bag of Words in broad contextBag of words (BOW) features were introducedto represent the presence or absence of almost allwords within a window of three sentences of thetarget word.
A small stop list (approximately 50words) was used to remove common closed-classwords such as prepositions and conjunctions.
Thewords were lemmatised before being transformedinto features, and were not weighted for their dis-tance from the target word.
No attribute subset se-lection was performed on the BOW features.4.1.3 Local Context FeaturesThe sentence containing the target word wastagged for Part of Speech (POS) using the POS tag-ger in the C&C parser tools.
For four tokens eitherside of the target lemma, features were formed fromthe displacement of the token concatenated with:?
The POS tag?
The lemmatised word?
The POS and lemma togetherAlso included were features combining the aboveinformation for pairs of tokens before, after, and ei-ther side of the target word.
Finally, a feature rep-resenting the POS tag of the target word was added,providing such information as number and tense.The portion of the context used to form these fea-tures is identical with that used to determine substi-tutability of potential synonyms using the Web1T-based features.
Combining the abstract substi-tutability features with features that use the particu-lar tokens in the local context helps to maximise theutility of information present near the target word byapproaching it from multiple perspectives.4.2 Results and DiscussionThe results of the system are shown in Table 1The first-sense baseline achieves scores of 0.788for precision, recall and F1, and thus outperformsour system for all documents.Unfortunately we are currently unable to explainthis relatively poor performance.
It is possible thatan error of a similar nature to the one which af-fected the initial results for the lexical sample system449Doc.
Attempted Precision Recall F1d001 0.986 0.625 0.617 0.621d002 0.958 0.598 0.573 0.585d003 0.948 0.610 0.578 0.593d004 0.929 0.606 0.563 0.583d005 0.965 0.471 0.455 0.463Total 0.953 0.588 0.560 0.574Table 1: Coarse-Grained WSD results(see Section 6.3) was also present in this system, al-though we have not yet been unable to identify sucha problem.
It is also possible that the current highlysupervised and lexicalised approach employed is notwell-suited to the all-words task, and may requireextension to achieve broad coverage.5 Task 10: English Lexical Substitution5.1 MethodologyAs for the WSD systems, the Lexical Substitutionsystem concentrated on words whose occurrence inlocal contexts similar to that of the target was morefrequent than expected in the Web1T corpus.Aside from preferring sets of potential syn-onyms obtained from lexical resources, the systemis entirely unsupervised.
Consequently, no sense-annotated corpus resources were used.The lexical resources used were WordNet ver-sion 2.1 (Fellbaum, 1998) and the Macquarie The-saurus (Bernard, 1985), a pre-defined, manuallyconstructed Thesaurus.
The only information usedfrom these resources was a list of potential syn-onyms for all listed senses that matched the targetword?s part-of-speech.
These synonyms were usedto preferentially choose potential substitutes ob-tained from the corpus data, as described below.
Theunion of potential synonyms from both resourceswas used, although MWEs were not included dueto limitations with the corpus.
Although these lex-ical resources were not augmented, the system wascapable of producing substitutes not present in theseresources by using high-scoring words found in thecorpus.
The ordering of synonyms in these resourceswas not used directly, nor was their association withparticular senses.The PMI for potential substitutes that occurred inthe target position of each local context window wasdetermined using the Web1T corpus, as for coarseWSD above.
The strategy differed slightly from thesupervised process employed for WSD however, inthat rather than testing a fixed set of potential substi-tutes, every word that occurred in the correct loca-tion in a matching context was considered as a sub-stitute.
This introduced an additional computationalburden which restricted the set of n-grams used to4 and 5 grams.
In particular, this is because the setof words occurring in the target position grew pro-hibitively large for 2 and 3 grams.As for WSD, the PMI for each potential substi-tute was combined by summing the individual PMIsover all locations and size of n-gram where it oc-curred.
This sum was used to rank the substitutes.After the production of the ranked list, the set of syn-onyms obtained from the lexical resources was usedfor preferential selection.
Substitutes in the rankedlist that also occurred in the synonym pool were cho-sen first.
The exact manner of the preferential se-lection differed for the two evaluation measures thesystem participated in.For the BEST measure, the highest PMI-rankedsubstitute that occurred in the synonym pool wasgiven as the only substitute.
If no substitutes fromthe synonym pool were present in the ranked list,the top three substitutes from the list were given.For the out-of-ten (OOT) measure, the ten highest-ranked substitutes that were in the synonym poolwere given.
If fewer than 10 substitutes were presentin the list, the remaining best ranked substitutes notin the synonym pool were used to make up the tenanswers.As with the Coarse-Grained All Word WSD, lim-itations in the current implementation of the Web1Tprocessing software meant that it was not possibleto examine MWEs, and there was thus no provisionto detect or handle MWEs in the system.
For thisreason, the MW measure was not produced by thesystem.5.2 Results and DiscussionThe results for the BEST and OOT measures are givenin tables 2 and 3 respectively.
While the results forthe other tasks are reported as a decimal fraction of1, the results here are percentage scores, in line withthe results provided by the task organisers.450P R Mode P Mode Rall 11.23 10.88 18.22 17.64Further AnalysisNMWT 11.68 11.34 18.46 17.90NMWS 12.48 12.10 19.25 18.63RAND 11.47 11.01 19.14 18.35MAN 10.95 10.73 17.20 16.84Table 2: BEST resultsP R Mode P Mode Rall 36.07 34.96 43.66 42.28Further AnalysisNMWT 37.62 36.17 44.71 43.35NMWS 40.13 38.89 46.25 44.77RAND 35.67 34.26 42.90 41.13MAN 36.52 35.78 44.50 43.58Table 3: OOT resultsNotably, recall is always lower than precision.
Ifno substitutes were found to have finite PMI at anyposition, no substitute was rendered by the system.This meant a small number of examples in the sub-mitted system had no answer provided.
The sys-tem?s design meant that no attempt was made toprovide any answer when counts were zero for allWeb1T queries.
This was the case for around 3%of the evaluation set.
As the query retrieval soft-ware was limited to single word substitutions, thisshould be expected to occur for MWEs more fre-quently than for single word substitutions.
The re-sults for both BEST and OOT confirm this, show-ing that the system?s performance is uniformly betterwhen MWEs are excluded.As a consequence of the properties of the Web1Tcorpus, the system chooses substitutes on the ba-sis of information that is derived from at most fourwords either side of the target word.
It is thus en-couraging that it is able to outperform the baselineson each evaluation measure.Interestingly, for the BEST evaluation the perfor-mance on the randomly selected (RAND) examplesoutperforms that on the manually selected (MAN)examples.
For the OOT evaluation the situation isreversed.
This could indicate that, depending on themotivation for the manual selections, the system isnot particularly well-suited to selecting an obvioussingular substitution, but is quite capable of rankingreasonably acceptable ones near the top of the list.6 Task 17: Coarse Grained EnglishLexical Sample sub-task6.1 ApproachThe Lexical Sample system used features identicalto those described for the Coarse-Grained All-Wordstask, with the addition of the CCG supertag feature,discussed below.
Labeled data used for training theclassifier models in this system consisted of only theinstances in the training data supplied for the task,although the Web1T corpus was of course used toprovide extensive information in the form of featuresfor those instances.
As for the All-Words system, anindividual SVM model was trained using linear ker-nels for each lemma being disambiguated.
The con-textual BOW features were not selected from withina window as for the All-Words system; instead theentire context provided in the training and test datawas used.Unlike the other systems, the Lexical Sample sys-tem produced a prediction for every instance in thetest data, as the MWE limitation of the Web1T pro-cessing software did not present an impediment.6.2 CCG Verb CategoriesThe Lexical sample data was parsed using the Clarkand Curran CCG parser (Clark and Curran, 2004).Existing tagging and parsing models, derived fromCCGBank are included with the parser package, andwere used without adjustment.
Gold-standard parsesavailable for the source data were not used.The syntactic combination category (?supertags?
)assigned to target verbs by the parser were used asfeatures.
This category label encodes informationabout the types of the other sentential componentsused when building a parse.
A forward slash indi-cates that the current token requires a component ofthe specified type to the right; a backwards slash re-quires one to the left.
The C&C parser includes asupertagger, but this supertagger assigns multiple la-bels with varying degrees of confidence, and whenthe parse is performed, the supertag labels are sub-ject to revision in determining the most likely parse.The feature used for the Lexical Sample system uses451the final, parser-determined supertag.As an example, consider the occur-rence of the verb find in the following twofragments where it has different senses:managers did not find out about questionable billingandor new revenues are found by CongressIn the first fragment find has a (simplified) supertagof (S\NP)/PP, while in the second it is playing adifferent grammatical role, and hence has a differentsupertag: S\NP.
While these supertags are gener-ally not exclusively associated with a single sensein particular, their distribution is sufficiently distinctover different senses that features derived fromthem are informative for the WSD task.
To formfeatures, the system uses the supertags obtainedfrom the parser as binary features, with a slightsimplification: by removing distinctions betweenthe argument types of the main S component,generalisation is facilitated among instances ofverbs which differ slightly on a local level butcombine with other parts of the sentence similarly.6.3 Results and DiscussionUnfortunately, the component of the lexical samplesystem responsible for assigning identifiers for eval-uation contained a systematic error, resulting in amismatch between the predictions of the system andthe correct labels as used in evaluation.
The systemassumed that for each lemma in the test set, the in-stances in the test data file would have lexicographi-cally ascending identifiers, and matched predictionsto identifiers using this assumption.
This was notthe case in the task data, and yielded a result forthe submission that severely underestimated the per-formance of the system.
We calculated a baselineof 0.788 for the Lexical Sample sub-task, using theMost Frequent Sense for each lemma in the trainingdata.
The result for the systems initial submissionwas 0.743 (precision, recall, accuracy and F1 are allidentical, as the system provides an answer for everyinstance).However, as the mismatch is systematic, and onlyoccurred after the classifier had made its predictions,it was possible to correct almost all of the alignmentby post-processing the erroneous answer file.
Byholding the order of predictions constant, but lexico-graphically sorting instance identifiers within eachlemma, predictions were re-matched with their in-tended identifiers.
Using the test labels provided bythe task organisers, the accuracy of the system afterrepairing the mismatch was 0.891.As the parser does not have 100% coverage, theparse of the test sentence did not succeed in everyinstance.
This in turn caused some supertag featuresto be misaligned with other feature types before theerror was rectified.
This meant that a small frac-tion of instances were given predictions in the sub-mitted data that differed from those produced by thecorrected system.
When the already-trained modelswere used to re-predict the classes of the correctlyaligned test instances, a further small improvementto a result of 0.893 was achieved.It is encouraging that the results (after correctingthe misaligned identifiers) for the patched system areapproaching the Inter Tagger Agreement (ITA) levelreported for OntoNotes sense tags by the task or-ganisers ?
90%.
This could be seen as an positiveoutcome of the movement towards coarser-grainedsense inventories for the WSD tasks, it is difficultfor automated systems to agree with humans moreoften than they agree with each other.7 ConclusionSubstantially similar information in the form of aPMI-based substitutability measure from the Web1Tcorpus was used in all USYD systems.
Thatthis information yielded positive results in differentsemantic-ambiguity related tasks, both supervisedand unsupervised, demonstrates the usefulness ofthe data at the scale of the Web1T corpus, eitheralone or in concert with other information sources,and there are still many more approaches to usingthis resource for semantic processing that could beexplored.The systems demonstrated outstanding perfor-mance on the Lexical Sample WSD task ?
nearlyat the level of the reported ITA.
Good unsupervisedperformance above the baseline was also achievedon the Lexical Substitution task.8 AcknowledgementsMany thanks to Jon Patrick, James Curran, andMatthew Honnibal for their invaluable assistance,insights and advice.452ReferencesJ.
R. L. Bernard, editor.
1985.
The Macquarie The-saurus.
The Macquarie Library, Sydney.Thorsten Brants and Alex Franz.
2006.
Web 1T5-gram corpus version 1.1.
Technical report,Google Research.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: A Library for Support Vector Machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Kenneth Ward Church, Willam Gale, Patrick Hanks,Donald Hindle, and Rosamund Moon.
1994.
Lex-ical substitutability.
In B. T. S. Atkins andA.
Zampolli, editors, Computational Approachesto the Lexicon, pages 153?177.
Oxford UniversityPress.Stephen Clark and James R. Curran.
2004.
Parsingthe WSJ using CCG and log-linear models.
InProceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics (ACL-04), pages 104?111.
Barcelona, Spain.Usama M. Fayyad and Keki.
B. Irani.
1993.
Multi-interval discretization of continuous-valued at-tributes for classification learning.
In Proceed-ings of the Thirteenth International Joint Confer-ence on Artificial Intelligence, pages 1022?1029.Chambery, France.Christiane Fellbaum, editor.
1998.
Wordnet: AnElectronic Lexical Database.
MIT Press.Fernando Gomez.
2001.
An algorithm for aspects ofsemantic interpretation using an enhanced word-net.
In Proceedings of NAACL-2001, pages 1?8.Anna Korhonen and Judita Preiss.
2003.
Improvingsubcategorization acquisition using word sensedisambiguation.
In ACL ?03: Proceedings of the41st Annual Meeting on Association for Compu-tational Linguistics, pages 48?55.Claudia Leacock, Martin Chodorow, and George A.Miller.
1998.
Using corpus statistics and WordNetrelations for sense identification.
ComputationalLinguistics, 24:147?165.Dekang Lin.
1997.
Using syntactic dependency aslocal context to resolve word sense ambiguity.
InProceedings of the 35th Annual Meeting of the As-sociation for Computational Linguistics.David Martinez, Eneko Agirre, and Xinglong Wang.2006.
Word relatives in context for word sensedisambiguation.
In Proceedings of the 20062006 Australasian Language Technology Work-shop (ALTW 2006), pages 42?50.Rada Mihalcea, Timothy Chklovski, and Adam Kil-garriff.
2004.
The senseval-3 english lexical sam-ple task.
In Rada Mihalcea and Phil Edmonds,editors, Senseval-3: Third International Work-shop on the Evaluation of Systems for the Seman-tic Analysis of Text, pages 25?28.
Association forComputational Linguistics.George.
A. Miller, Claudia.
Leacock, Tengi Randee,and Ross Bunker.
1993.
A semantic concordance.In Proceedings of the 3rd DARPA Workshop onHuman Language Technology, pages 303?308.453
