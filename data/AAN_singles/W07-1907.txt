Proceedings of the Workshop on Embodied Language Processing, pages 51?58,Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational LinguisticsDesign and Evaluation of an American Sign Language GeneratorMatt HuenerfauthComputer Science DepartmentCUNY Queens CollegeThe City University of New York65-30 Kissena BoulevardFlushing, NY 11375 USAmatt@cs.qc.cuny.eduLiming Zhao, Erdan Gu, Jan AllbeckCenter for Human Modeling & SimulationUniversity of Pennsylvania3401 Walnut StreetPhiladelphia, PA 19104 USA{liming,erdan,allbeck}@seas.upenn.eduAbstractWe describe the implementation andevaluation of a prototype American SignLanguage (ASL) generation componentthat produces animations of ASL classifierpredicates, some frequent and complexspatial phenomena in ASL that no previousgeneration system has produced.
We dis-cuss some challenges in evaluating ASLsystems and present the results of a user-based evaluation study of our system.1 Background and MotivationsAmerican Sign Language (ASL) is a natural lan-guage with a linguistic structure distinct from Eng-lish used as a primary means of communication forapproximately one half million people in the U.S.(Mitchell et al, 2006).
A majority of deaf 18-year-olds in the U.S. have an English reading level be-low that of an average 10-year-old hearing student(Holt, 1991), and so software to translate Englishtext into ASL animations can improve many peo-ple?s access to information, communication, andservices.
Previous English-to-ASL machine trans-lation projects (S?f?r & Marshall, 2001; Zhou etal., 2000) could not generate classifier predicates(CPs), phenomena in which signers use specialhand movements to indicate the location andmovement of invisible objects in space aroundthem (representing entities under discussion).
Be-cause CPs are frequent in ASL and necessary forconveying many concepts, we have developed aCP generator that can be incorporated into a fullEnglish-to-ASL machine translation system.During a CP, signers use their hands to position,move, trace, or re-orient imaginary objects in thespace in front of them to indicate the location,movement, shape, contour, physical dimension, orsome other property of corresponding real worldentities under discussion.
CPs consist of a seman-tically meaningful handshape and a 3D handmovement path.
A handshape is chosen from aclosed set based on characteristics of the entity de-scribed (whether it be a vehicle, human, animal,etc.)
and what aspect of the entity the signer is de-scribing (surface, position, motion, etc).
For ex-ample, the sentence ?the car parked between thecat and the house?
could be expressed in ASL us-ing 3 CPs.
First, a signer performs the ASL signHOUSE while raising her eyebrows (to introduce anew entity as a topic).
Then, she moves her handin a ?Spread C?
handshape (Figure 1) forward to apoint in space where a miniature house could beenvisioned.
Next, the signer performs the signCAT with eyebrows raised and makes a similarmotion with a ?Hooked V?
handshape to a locationwhere a cat could be imagined.
Finally, she per-forms the sign CAR (with eyebrows raised) anduses a ?Number 3?
handshape to trace a path thatstops at between the ?house?
and the ?cat.?
Herother hand makes a flat surface for the ?car?
to parkon.
(Figure 3 will show our system?s animation.
)Figure 1: ASL handshapes: Spread C (bulky object),Number 3 (vehicle), Hooked V (animal), Flat (surface).512 System Design and ImplementationWe have built a prototype ASL generation modulethat could be incorporated into an English-to-ASLmachine translation system.
When given a 3Dmodel of the arrangement of a set of objects whoselocation and movement should be described inASL, our system produces an animation of ASLsentences containing classifier predicates to de-scribe the scene.
Classifier predicates are the waysuch spatial information is typically conveyed inASL.
Since this is the first ASL generation systemto produce classifier predicate sentences (Huener-fauth, 2006b), we have also conducted an evalua-tion study in which native ASL signers comparedour system's animations to the current state of theart: Signed English animations (described later).2.1 Modeling the Use of SpaceTo produce classifier predicates and other ASLexpressions that associate locations in spacearound a signer with entities under discussion, anEnglish-to-ASL system must model what objectsare being discussed in an English text, and it mustmap placeholders for these objects to locations inspace around the signer?s body.
The input to ourASL classifier predicate generator is an explicit 3Dmodel of how a set of placeholders representingdiscourse entities are positioned in the spacearound the signing character?s body (Huenerfauth,2006b).
This 3D model is ?mapped?
onto a vol-ume of space in front of the signer?s torso, and thismodel is used to guide the motion of the ASLsigner?s hands during the performance of classifierpredicates describing the motion of these objectsThe model encodes the 3D location (center-of-mass) and orientation values of the set of objectsthat we want to our system describe using ASLanimation.
For instance, to generate the ?car park-ing between the cat and the house?
example, wewould pass our system a model with three sets oflocation (x, y, z coordinates) and orientation (x, y,z, rotation angles) values: for the cat, the car, andthe house.
Each 3D placeholder also includes a setof bits that represent the set of possible ASL classi-fier handshapes that can be used to describe it.While this 3D model is given as input to ourprototype classifier predicate generator, when partof a full generation system, virtual reality ?scenevisualization?
software can be used to produce a3D model of the arrangement and movement ofobjects discussed in an English input text (Badleret al, 2000; Coyne and Sproat, 2001).2.2 Template-Based Planning GenerationGiven the 3D model above, the system uses aplanning-based approach to determine how tomove the signer?s hands, head-tilt, and eye-gaze toproduce an animation of a classifier predicate.
Thesystem stores a library of templates representingthe various kinds of classifier predicates it mayproduce.
These templates are planning operators(they have logical pre-conditions, monitored ter-mination conditions, and effects), allowing the sys-tem to trigger other elements of ASL signing per-formance that may be required during a grammati-cally correct classifier predicate (Huenerfauth,2006b).
Each planning operator is parameterizedon an object in the 3D model (and its 3D coordi-nates); for instance, there is a templated planningoperator for generating an ASL classifier predicateto show a ?parking?
event.
The specific loca-tion/orientation of the vehicle that is parking wouldbe the parameter passed to the planning operator.There is debate in the ASL linguistics commu-nity about the underlying structure of classifierpredicates and the generation process by whichsigners produce them.
Our parameterized templateapproach mirrors one recent linguistic model (Lid-dell, 2003), and the implementation and evaluationof our prototype generator will help determinewhether this was a good choice for our system.2.3 Multi-Channel Syntax RepresentationWhile strings and syntax trees are used to representwritten languages inside of NLP software, theseencodings are difficult to adapt to a sign language.ASL lacks a standard writing system, and the mul-tichannel nature of an ASL performance makes itdifficult to encode in a linear single-channel string.This project developed a new formalism for repre-senting a linguistic signal in a multi-channel man-ner and for encoding temporal coordination andnon-coordination relationships between portions ofthe signal (Huenerfauth, 2006a).
The output of ourplanner is a tree-like structure that represents theanimation to be synthesized.
The tree has twokinds of non-terminal nodes: some indicate thattheir children should be performed in sequence(like a traditional linguistic syntax tree), and somenon-terminals indicate that their children should beperformed in parallel (e.g.
one child subtree may52specify the movement of the arms, and another, thefacial expression).
In this way, the structure canencode how multiple parts of the sign languageperformance should be coordinated over timewhile still leaving flexibility to the exact timing ofevents ?
see Figure 2.
In earlier work, we haveargued that this representation is sufficient for en-coding ASL animation (Huenerfauth, 2006a), andthe implementation and evaluation of our system(using this formalism) will help test this claim.Figure 2: A multichannel representation for the sentence?The cat is next to the house.?
This example showshandshape, hand location, and eye gaze direction ?some details omitted from the example: hand orienta-tion, head tilt, and brow-raising.
Changes in timing ofindividual animation events causes the structure tostretch in the time dimension (like an HTML table).2.4 Creating Virtual Human AnimationAfter planning, the system has a tree-structure thatspecifies activities for parts of the signer?s body.Non-terminal nodes indicate whether their childrenare performed in sequence or in parallel, and theterminal nodes (the inner rectangles in Figure 2)specify animation events for a part of the signer?sbody.
Nodes?
time durations are not yet specified(since the human animation component wouldknow the time that movements require, not the lin-guistic planner).
So, the generator queries the hu-man animation system to calculate an estimatedtime duration for each body animation event (eachterminal node), and the structure is then ?balanced?so that if several events are meant to occur in par-allel, then the shorter events are ?stretched out.?
(The linguistic system can set max/min times forsome events prior to the animation processing.
)2.5 Eye-Gaze and Brow ControlThe facial model is implemented using the Gretafacial animation engine (Pasquariello and Pela-chaud, 2001).
Our model controls the motion ofthe signer?s eye-brows, which can be placed in a?raised?
or ?flat?
position.
The eye motor controlrepertoire contains three behaviors: fixation on a3D location in space around the signer?s body,smooth pursuit of a moving 3D location, and eye-blinking.
Gaze direction is computed from the lo-cation values specified inside the 3D model, andthe velocity and time duration of the movement aredetermined by the timing values inside the tree-structure output from the planner.
The signer?shead tilt changes to accommodate horizontal orvertical gaze shifts greater than a set threshold.When performing a ?fixation?
or ?smooth pursuit?with the eye-gaze, the rate of eye blinking is de-creased.
Whenever the signer?s eye-gaze is nototherwise specified for the animation performance,the default behavior is to look at the audience.2.6 Planning Arm MovementGiven the tree-structure with animation events, theoutput of arm-planning should be a list of anima-tion frames that completely specify the rotationangles of the joints of the signer?s hands and arms.The hand is specified using 20 rotation angles forthe finger joints, and the arm is specified using 9rotation angles: 2 for the clavicle joint, 3 for theshoulder joint, 1 for the elbow joint, and 3 for thewrist.
The linguistic planner specifies the hand-shape that should be used for specific classifierpredicates; however, the tree-structure specifies thearm movements by giving a target location for thecenter of the signer?s palm and a target orientationvalue for the palm.
The system must find a set ofclavicle, shoulder, elbow, and wrist angles that getthe hand to this desired location and palm orienta-tion.
In addition to reaching this target, the armpose for each animation frame must be as naturalas possible, and the animation between framesmust be smooth.
The system uses an inversekinematics (IK) which automatically favors naturalarm poses.
Using the wrist as the end-effector, anelbow angle is selected based on the distance fromshoulder to the target, and this elbow angle isfixed.
We next compute a set of possible shoulderand wrist rotation angles in order to align thesigner?s hand with the target palm orientation.Disregarding elbow angles that force impossiblewrist joint angles, we select the arm pose that iscollision free and is the most natural, according toa shoulder strength model (Zhao et al, 2005).dominanthand shape Hook Vdominanthand locationto catlocationeye gaze audience to house location audienceto catlocationnon-dominanthand locationto houselocationnon-dominanthand shape Spread C?ASLNounSign:HOUSE?
?ASLNounSign:HOUSEASLNounSign:CATtime532.7 Synthesizing Virtual Human AnimationThis animation specification is performed by ananimated human character in the Virtual HumanTestbed (Badler et al, 2005).
Because the Gretasystem used a female head with light skin tone, afemale human body was chosen with matchingskin.
The character was dressed in a blue shirt andpants that contrasted with its skin tone.
To makethe character appear to be a conversational partner,the ?camera?
inside the virtual environment wasset at eye-level with the character and at an appro-priate distance for ASL conversation.2.8 Coverage of the Prototype SystemOur prototype system can be used to translate alimited range of English sentences (discussing thelocations and movements of a small set of peopleor objects) into animations of an onscreen human-like character performing ASL classifier predicatesto convey the locations and movements of the enti-ties in the English text.
Table 1 includes shorthandtranscripts of some ASL sentence animations pro-duced by the system; the first sentence correspondsto the classifier predicate animation in Figure 3.3 Issues in Evaluating ASL GenerationThere has been little work on developing evalua-tion methodologies for sign language generation orMT systems.
Some have shown how automaticstring-based evaluation metrics fail to identify cor-rect sign language translations (Morrisey and Way,2006), and they propose building large parallelwritten/sign corpora containing more syntactic andsemantic information (to enable more sophisticatedmetrics to be created).
Aside from the expense ofcreating such corpora, we feel that there are severalfactors that motivate user-based evaluation studiesfor sign language generation systems ?
especiallyfor those systems that produce classifier predicates.These factors include some unique linguistic prop-erties of sign languages and the lack of standardwriting systems for most sign languages, like ASL.Figure 3: Images from our system?s animation of a classifier predicate for ?the car parked between thehouse and the cat.?
(a) ASL sign HOUSE, eyes at audience, brows raised; (b) Spread C handshape andeye gaze to house location; (c) ASL sign CAT, eyes at audience, brows raised; (d) Hooked V handshapeand eye gaze to cat location; (e) ASL sign CAR, eyes at audience, brows raised; (f) Number 3 handshape(for the car) parks atop Flat handshape while the eye gaze tracks the movement path of the car.
(a) (b) (c)(d) (e) (f)54Most automatic evaluation approaches for gen-eration or MT systems compare a string producedby a system to a human-produced ?gold-standard?string.
Sign languages usually lack written formsthat are commonly used or known among signers.While we could invent an artificial ASL writingsystem for the generator to produce as output (forevaluation purposes only), it?s not clear that humanASL signers could accurately or consistently pro-duce written forms of ASL sentences to serve as?gold standards?
for such an evaluation.
Further,real users of the system would never be shown arti-ficial written ASL; they would see animation out-put.
Thus, evaluations based on strings would nottest the full process ?
including the synthesis of the?string?
into an animation ?
when errors may arise.Another reason why string-based evaluationmetrics are not well-suited to ASL is that sign lan-guages have linguistic properties that can confoundstring-edit-distance-based metrics.
ASL consistsof the coordinated movement of several parts ofthe body in parallel (i.e.
face, eyes, head, hands),and so a string listing the set of signs performed isa lossy representation of the original performance(Huenerfauth, 2006a).
The string may not encodethe non-manual parts of the sentence, and sostring-based metrics would fail to consider thoseimportant aspects.
Discourse factors (e.g.
topicali-zation) can also result in movement phenomena inASL that may scramble the sequence of signs inthe sentence without substantially changing its se-mantics; such movement would affect string-basedmetrics significantly though the sentence meaningmay change little.
The use of head-tilt and eye-gaze during the performance of ASL verb signsmay also license the dropping of entire sentenceconstituents (Neidle et.
al, 2000).
The entities dis-cussed are associated with locations in spacearound the signer at which head-tilt or eye-gaze isaimed, and thus the constituent is actually still ex-pressed although no manual signs are performedfor it.
Thus, an automatic metric may penalizesuch a sentence (for missing a constituent) whilethe information is still there.
Finally, ASL classi-fier predicates convey a lot of information in a sin-gle complex ?sign?
(handshape indicates semanticcategory, movement shows 3D path/rotation), andit is unclear how we could ?write?
the 3D data of aclassifier predicate in a string-based encoding orhow to calculate an edit-distance between a ?goldstandard?
classifier predicate and a generated one.4 Evaluation of the SystemWe used a user-based evaluation methodology inwhich human native ASL signers are shown theoutput of our generator and asked to rate each ani-mation on ten-point scales for understandability,naturalness of movement, and ASL grammaticalcorrectness.
To evaluate whether the animationconveyed the proper semantics, signers were alsoasked to complete a matching task.
After viewinga classifier predicate animation produced by thesystem, signers were shown three short animationsshowing the movement or location of the set ofobjects that were described by the classifier predi-cate.
The movement of the objects in each anima-tion was slightly different, and signers were askedto select which of the three animations depicted thescene that was described by the classifier predicate.Since this prototype is the first generator to pro-duce animations of ASL classifier predicates, thereare no other systems to compare it to in our study.To create a lower baseline for comparison, wewanted a set of animations that reflect the currentstate of the art in broad-coverage English-to-signEnglish Gloss ASL Sentence with Classifier Predicates (CPs) Signed English SentenceThe car parks between the house andthe cat.ASL sign HOUSE; CP: house location; sign CAT; CP: cat location; signCAR; CP: car path.THE CAR PARK BETWEEN THE HOUSEAND THE CATThe man walks next to the woman.
ASL sign WOMAN; CP: woman location; sign MAN; CP: man path.
THE MAN WALK NEXT-TO THEWOMANThe car turns left.
ASL sign CAR; CP: car path.
THE CAR TURN LEFTThe lamp is on the table.
ASL sign TABLE; CP: table location; sign LIGHT; CP: lamp location.
THE LIGHT IS ON THE TABLEThe tree is near the tent.
ASL sign TENT; CP: tent location; sign TREE; CP: tree location.
THE TREE IS NEAR THE TENTThe man walks between the tent andthe frog.ASL sign TENT; CP: tent location; sign FROG; CP: frog location; signMAN; CP: man path.THE MAN WALK BETWEEN THE TENTAND THE FROGThe man walks away from thewoman.ASL sign WOMAN: CP: woman location; sign MAN; CP: man path.
THE MAN WALK FROM THE WOMANThe car drives up to the house.
ASL sign HOUSE; CP: house location; sign CAR; CP: car path.
THE CAR DRIVE TO THE HOUSEThe man walks up to the woman.
ASL sign WOMAN; CP: woman location; sign MAN; CP: man path.
THE MAN WALK TO THE WOMANThe woman stands next to the table.
ASL sign TABLE; CP: table location; sign WOMAN; CP: womanlocation.THE WOMAN STAND NEXT-TO THETABLETable 1: ASL and Signed English sentences included in the evaluation study (with English glosses).55translation.
Since there are no broad-coverageEnglish-to-ASL MT systems, we used Signed Eng-lish transliterations as our lower baseline.
SignedEnglish is a form of communication in which eachword of an English sentence is replaced with a cor-responding sign, and the sentence is presented inoriginal English word order without any accompa-nying ASL linguistic features such as meaningfulfacial expressions or eye-gaze.Ten ASL animations (generated by our system)were selected for inclusion in this study based onsome desired criteria.
The ASL animations consistof classifier predicates of movement and location ?the focus of our research.
The categories of peopleand objects discussed in the sentences require avariety of ASL handshapes to be used.
Some sen-tences describe the location of objects, and othersdescribe movement.
The sentences describe fromone to three objects in a scene, and some pairs ofsentences actually discuss the same set of objects,but moving in different ways.
Since the creation ofa referring expression generator was not a focus ofour prototype, all referring expressions in the an-imations are simply an ASL noun phrase consist-ing of a single sign ?
some one-handed and sometwo-handed.
Table 1 lists the ten classifier predi-cate animations we selected (with English glosses).For the ?matching task?
portion of the study,three animated visualizations were created for eachsentence showing how the objects mentioned in thesentence move in 3D.
One animation was an accu-rate visualization of the location/movement of theobjects, and the other two animations were ?con-fusables?
?
showing orientations/movements forthe objects that did not match the classifier predi-cate animations.
Because we wanted to evaluatethe classifier predicates (and not the referring ex-pressions), the set of objects that appeared in allthree visualizations for a sentence was the same.Thus, it was the movement and orientation infor-mation conveyed by the classifier predicate (andnot the object identity conveyed by the referringexpression) that would distinguish the correct visu-alization from the confusables.
For example, thefollowing three visualizations were created for thesentence ?the car parks between the cat and thehouse?
(the cat and house remain in the same loca-tion in each): (1) a car drives on a curved path andparks at a location between a house and a cat, (2) acar drives between a house and a cat but continuesdriving past them off camera, and (3) a car starts ata location between a house and a cat and drives toa location that is not between them anymore.To create the Signed English animations foreach sentence, some additional signs were added tothe generator?s library of signs.
(ASL does nottraditionally use signs such as ?THE?
that are usedin Signed English.)
A sequence of signs for eachSigned English transliteration was concatenated,and the synthesis sub-component of our systemwas used to calculate smooth transitional move-ments for the arms and hands between each sign inthe sentence.
The glosses for the ten Signed Eng-lish transliterations are also listed in Table 1.4.1 User-Interface for Evaluation StudyAn interactive slideshow was created with oneslide for each of the 20 animations (10 from ourASL system, 10 Signed English).
On each slide,the signing animation was shown on the left of thescreen, and the three possible visualizations of thatsentence were shown to the right (see Figure 4).The slides were placed in a random order for eachof the participants in the study.
A user could re-play the animations as many times as desired be-fore going to the next signing animation.
Subjectswere asked to rate each of these animations on a 1-to-10-point scale for ASL grammatical correctness,understandability, and naturalness of movement.Subjects were also asked to select which of thethree animated visualizations (choice ?A,?
?B,?
or?C?)
matched the scene as described in the sen-tence performed by the virtual character.After these slides, 3 more slides appeared con-taining animations from our generator.
(Thesewere repeats of 3 animations used in the main partof the study.)
These three slides only showed theVideo # 1Next 1CLICK TO START MOVIEABCCLICK TO START MOVIECLICK TO START MOVIECLICK TO START MOVIEFigure 4: Screenshot from evaluation program.56?correct?
animated visualization for that sentence.For these last three slides, subjects were insteadasked to comment on the animation?s speed, col-ors/lighting, hand visibility, correctness of handmovement, facial expression, and eye-gaze.
Sign-ers were also asked to write any comments theyhad about how the animation should be improved.4.2 Recruitment and Screening of SubjectsSubjects were recruited through personal contactsin the deaf community who helped identify friends,family, and associates who met the screening crite-ria.
Participants had to be native ASL signers ?many deaf individuals are non-native signers wholearned ASL later in life (and may accept English-like signing as being grammatical ASL).
Subjectswere preferred who had learned ASL since birth,had deaf parents that used ASL at home, and/orattending a residential school for the deaf as a child(where they were immersed in an ASL-signingcommunity).
Of our 15 subjects, 8 met al threecriteria, 2 met two criteria, and 5 met one (1 grewup with ASL-signing deaf parents and 4 attended aresidential school for the deaf from an early age).During the study, instructions were given to par-ticipants in ASL, and a native signer was presentduring 13 of the 15 sessions to answer questions orto explain experimental procedures.
This signerengaged the participants in conversation in ASLbefore the session to produce an ASL-immersiveenvironment.
Participants were given instructionsin ASL about how to score each category.
Forgrammaticality, they were told that ?perfect ASLgrammar?
would be a 10, but ?mixed-up?
or ?Eng-lish-like?
grammar should be a 1.
For understand-ability, ?easy to understand?
sentences should be a10, but ?confusing?
sentences should be a 1.
Fornaturalness, animations in which the signer moved?smoothly, like a real person?
should be a 10, butanimations in which the signer moved in a?choppy?
manner ?like a robot?
should be a 1.4.3 Results of the EvaluationFigure 5 shows average scores for grammaticality,understandability, naturalness, and matching-task-success percentage for the animations from oursystem compared to the Signed English anima-tions.
Our system?s higher scores in all categoriesis significant (?
= 0.05, pairwise Mann-Whitney Utests with Bonferonni-corrected p-values).Subjects were asked to comment on the anima-tion speed, color, lighting, visibility of the hands,correctness of hand movement, correctness of fa-cial expressions, correctness of eye-gaze, and otherways of improving the animations.
Of the 15 sub-jects, eight said that some animations were a littleslow, and one felt some were very slow.
Eightsubjects wanted the animations to have more facialexpressions, and 4 of these specifically mentionednose and mouth movements.
Four subjects saidthe signer?s body should seem more loose/relaxedor that it should move more.
Two subjects wantedthe signer to show more emotion.
Two subjectsfelt that eye-brows should go higher when raised,and three felt there should be more eye-gazemovements.
Two subjects felt the blue color of thesigner?s shirt was a little too bright, and one dis-liked the black background.
Some subjects com-mented on particular ASL signs that they felt wereperformed incorrectly.
For example, three dis-cussed the sign ?FROG?
: one felt it should be per-formed a little more to the right of its current loca-tion, and another felt that the hand should be ori-ented with the fingers aimed more to the front.Some participants commented on the classifierpredicate portions of the performance.
For exam-ple, in the sentence ?the car parked between the catand the house,?
one subject felt it would be betterto use the non-dominant hand to hold the locationof the house during the car?s movement instead ofusing the non-dominant hand to create a platformfor the dominant hand (the car) to park upon.5 Conclusions and Future WorkUnlike an evaluation of a broad-coverage NLP sys-tem, during which we obtain performance statisticsAverage Scores for Survey Questions& Matching-Task-Success Percentage0102030405060708090100Grammatical Understandable Natural Matching TaskOur SystemSigned EnglishFigure 5: Grammaticality, understandability, natu-ralness, and matching-task-success scores.57for the system as it carries out a linguistic task on alarge corpus or ?test set,?
this paper has describedan evaluation of a prototype system.
We were notmeasuring the linguistic coverage of the system butrather its functionality.
Did signers agree that theanimation output: (1) is actually a grammatically-correct and understandable classifier predicate and(2) conveys the information about the movementof objects in the 3D scene being described?
Weexpected to find animation details that could beimproved in future work; however, since there arecurrently no other systems capable of generatingASL classifier predicate animations, any systemreceiving an answer of ?yes?
to questions (1) and(2) above is an improvement to the state of the art.Another contribution of this initial evaluation isthat it serves as a pilot study to help us determinehow to better evaluate sign language generationsystems in the future.
We found that subjects werecomfortable critiquing ASL animations, and mostsuggested specific (and often subtle) elements ofthe animation to be improved.
Their feedback sug-gested new modifications we can make to the sys-tem (and then evaluate again in future studies).Because subjects gave such high quality feedback,future studies will also elicit such comments.During the study, we also experimented with re-cording a native ASL signer (using a motion-capture suit and datagloves) performing classifierpredicates.
We tried to use this motion-capture datato animate a virtual human character superficiallyidentical to the one used by our system.
We hopedthat this character controlled by human movementscould serve as an upper-baseline in the evaluationstudy.
Unfortunately, the motion-capture data wecollected contained minor errors that required post-processing clean-up, and the resulting animationscontained enough movement inaccuracies that na-tive ASL signers who viewed them felt they wereactually less understandable than our system's an-imations.
In future work, we intend to explore al-ternative upper-baselines to compare our system?sanimations to: animation from alternative motion-capture techniques, hand-coded animations basedon a human?s performance, or simply a video of ahuman signer performing ASL sentences.AcknowledgementsNational Science Foundation Award #0520798?SGER: Generating Animations of American SignLanguage Classifier Predicates?
(Universal Access,2005) supported this work.
Software was donatedby UGS Tecnomatix and Autodesk.
Thank you toMitch Marcus, Martha Palmer, and Norman Badler.ReferencesN.I.
Badler, J. Allbeck, S.J.
Lee, R.J. Rabbitz, T.T.
Broderick,and K.M.
Mulkern.
2005.
New behavioral paradigms forvirtual human models.
SAE Digital Human Modeling.N.
Badler, R. Bindiganavale, J. Allbeck, W. Schuler, L. Zhao,S.
Lee, H. Shin, & M. Palmer.
2000.
Parameterized actionrepresentation & natural language instructions for dynamicbehavior modification of embodied agents.
AAAI Spring.R.
Coyne and R. Sproat.
2001.
WordsEye: an automatic text-to-scene conversion system.
ACM SIGGRAPH.J.A.
Holt.
1991.
Demographic, Stanford Achievement Test -8th Edition for Deaf and Hard of Hearing Students: Read-ing Comprehension Subgroup Results.?
.
S?f?r & I. Marshall.
2001.
The architecture of an English-text-to-Sign-Languages translation system.
Recent Ad-vances in Natural Language Processing.Matt Huenerfauth.
In Press.
Representing American Sign Lan-guage classifier predicates using spatially parameterizedplanning templates.
In M. Banich and D. Caccamise (Eds.),Generalization.
Mahwah: LEA.Matt Huenerfauth.
2006a.
Representing Coordination andNon-Coordination in American Sign Language Anima-tions.
Behaviour & Info.
Technology, 25:4.Matt Huenerfauth.
2006b.
Generating American Sign Lan-guage Classifier Predicates for English-to-ASL MachineTranslation.
Dissertation, U. Pennsylvania.Liddell, S. 2003.
Grammar, Gesture, and Meaning in Ameri-can Sign Language.
UK: Cambridge U. Press.R.E.
Mitchell, T.A.
Young, B. Bachleda, & M.A.
Karchmer.2006.
How Many People Use ASL in the United States?Why estimates need updating.
Sign Language Studies, 6:3.S.
Morrissey & A.
Way.
2006.
Lost in Translation: The prob-lems of using mainstream MT evaluation metrics for signlanguage translation.
5th SALTMIL Workshop on MinorityLanguages, LREC-200C.
Neidle, J. Kegl, D. MacLaughlin, B. Bahan, & R.G.
Lee.2000.
The Syntax of American Sign Language: FunctionalCategories and Hierarchical Structure.
Cambridge: MIT.S.
Pasquariello & C. Pelachaud.
2001.
Greta: A simple facialanimation engine.
In 6th Online World Conference on SoftComputing in Industrial Applications.L.
Zhao, K. Kipper, W. Schuler, C. Vogler, N.I.
Badler, & M.Palmer.
2000.
Machine Translation System from English toAmerican Sign Language.
Assoc.
for MT in the Americas.L.
Zhao, Y. Liu, N.I.
Badler.
2005.
Applying empirical dataon upper torso movement to real-time collision-free reachtasks.
SAE Digital Human Modeling.58
