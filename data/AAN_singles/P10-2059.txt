Proceedings of the ACL 2010 Conference Short Papers, pages 318?324,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsClassification of Feedback Expressions in Multimodal DataCostanza NavarrettaUniversity of CopenhagenCentre for Language Technology (CST)Njalsgade 140, 2300-DK Copenhagencostanza@hum.ku.dkPatrizia PaggioUniversity of CopenhagenCentre for Language Technology (CST)Njalsgade 140, 2300-DK Copenhagenpaggio@hum.ku.dkAbstractThis paper addresses the issue of how lin-guistic feedback expressions, prosody andhead gestures, i.e.
head movements andface expressions, relate to one another ina collection of eight video-recorded Dan-ish map-task dialogues.
The study showsthat in these data, prosodic features andhead gestures significantly improve auto-matic classification of dialogue act labelsfor linguistic expressions of feedback.1 IntroductionSeveral authors in communication studies havepointed out that head movements are relevant tofeedback phenomena (see McClave (2000) for anoverview).
Others have looked at the applicationof machine learning algorithms to annotated mul-timodal corpora.
For example, Jokinen and Ragni(2007) and Jokinen et al (2008) find that machinelearning algorithms can be trained to recognisesome of the functions of head movements, whileReidsma et al (2009) show that there is a depen-dence between focus of attention and assignmentof dialogue act labels.
Related are also the stud-ies by Rieks op den Akker and Schulz (2008) andMurray and Renals (2008): both achieve promis-ing results in the automatic segmentation of dia-logue acts using the annotations in a large multi-modal corpus.Work has also been done on prosody and ges-tures in the specific domain of map-task dialogues,also targeted in this paper.
Sridhar et al (2009)obtain promising results in dialogue act taggingof the Switchboard-DAMSL corpus using lexical,syntactic and prosodic cues, while Gravano andHirschberg (2009) examine the relation betweenparticular acoustic and prosodic turn-yielding cuesand turn taking in a large corpus of task-orienteddialogues.
Louwerse et al (2006) and Louwerseet al (2007) study the relation between eye gaze,facial expression, pauses and dialogue structurein annotated English map-task dialogues (Ander-son et al, 1991) and find correlations between thevarious modalities both within and across speak-ers.
Finally, feedback expressions (head nods andshakes) are successfully predicted from speech,prosody and eye gaze in interaction with Embod-ied Communication Agents as well as human com-munication (Fujie et al, 2004; Morency et al,2005; Morency et al, 2007; Morency et al, 2009).Our work is in line with these studies, all ofwhich focus on the relation between linguisticexpressions, prosody, dialogue content and ges-tures.
In this paper, we investigate how feedbackexpressions can be classified into different dia-logue act categories based on prosodic and ges-ture features.
Our data are made up by a collec-tion of eight video-recorded map-task dialogues inDanish, which were annotated with phonetic andprosodic information.
We find that prosodic fea-tures improve the classification of dialogue actsand that head gestures, where they occur, con-tribute to the semantic interpretation of feedbackexpressions.
The results, which partly confirmthose obtained on a smaller dataset in Paggio andNavarretta (2010), must be seen in light of thefact that our gesture annotation scheme comprisesmore fine-grained categories than most of the stud-ies mentioned earlier for both head movementsand face expressions.
The classification resultsimprove, however, if similar categories such ashead nods and jerks are collapsed into a more gen-eral category.In Section 2 we describe the multimodal Dan-ish corpus.
In Section 3, we describe how theprosody of feedback expressions is annotated, howtheir content is coded in terms of dialogue act, turnand agreement labels, and we provide inter-coderagreement measures.
In Section 4 we account forthe annotation of head gestures, including inter-318coder agreements results.
Section 5 contains a de-scription of the resulting datasets and a discussionof the results obtained in the classification experi-ments.
Section 6 is the conclusion.2 The multimodal corpusThe Danish map-task dialogues from the Dan-PASS corpus (Gr?nnum, 2006) are a collectionof dialogues in which 11 speaker pairs cooper-ate on a map task.
The dialogue participantsare seated in different rooms and cannot see eachother.
They talk through headsets, and one of themis recorded with a video camera.
Each pair goesthrough four different sets of maps, and changesroles each time, with one subject giving instruc-tions and the other following them.
The materialis transcribed orthographically with an indicationof stress, articulatory hesitations and pauses.
Inaddition to this, the acoustic signals are segmentedinto words, syllables and prosodic phrases, and an-notated with POS-tags, phonological and phonetictranscriptions, pitch and intonation contours.Phonetic and prosodic segmentation and anno-tation were performed independently and in paral-lel by two annotators and then an agreed upon ver-sion was produced with the supervision of an ex-pert annotator, for more information see Gr?nnum(2006).
The Praat tool was used (Boersma andWeenink, 2009).The feedback expressions we analyse here areYes and No expressions, i.e.
in Danish words likeja (yes), jo (yes in a negative context), jamen (yesbut, well), nej (no), n?h (no).
They can be singlewords or multi-word expressions.Yes and No feedback expressions representabout 9% of the approximately 47,000 runningwords in the corpus.
This is a rather high pro-portion compared to other corpora, both spokenand written, and a reason why we decided to usethe DanPASS videos in spite of the fact that thegesture behaviour is relatively limited given thefact that the two dialogue participants cannot seeeach other.
Furthermore, the restricted contextsin which feedback expressions occur in these di-alogues allow for a very fine-grained analysis ofthe relation of these expressions with prosody andgestures.
Feedback behaviour, both in speech andgestures, can be observed especially in the personwho is receiving the instructions (the follower).Therefore, we decided to focus our analysis onlyon the follower?s part of the interaction.
Becauseof time restrictions, we limited the study to fourdifferent subject pairs and two interactions perpair, for a total of about an hour of video-recordedinteraction.3 Annotation of feedback expressionsAs already mentioned, all words in DanPASS arephonetically and prosodically annotated.
In thesubset of the corpus considered here, 82% of thefeedback expressions bear stress or tone informa-tion, and 12% are unstressed; 7% of them aremarked with onset or offset hesitation, or both.For this study, we added semantic labels ?
includ-ing dialogue acts ?
and gesture annotation.
Bothkinds of annotation were carried out using ANVIL(Kipp, 2004).
To distinguish among the variousfunctions that feedback expressions have in the di-alogues, we selected a subset of the categories de-fined in the emerging ISO 24617-2 standard forsemantic annotation of language resources.
Thissubset comprises the categories Accept, Decline,RepeatRephrase and Answer.
Moreover, all feed-back expressions were annotated with an agree-ment feature (Agree, NonAgree) where relevant.Finally, the two turn management categories Turn-Take and TurnElicit were also coded.It should be noted that the same expression maybe annotated with a label for each of the three se-mantic dimensions.
For example, a yes can be anAnswer to a question, an Agree and a TurnElicit atthe same time, thus making the semantic classifi-cation very fine-grained.
Table 1 shows how thevarious types are distributed across the 466 feed-back expressions in our data.Dialogue ActAnswer 70 15%RepeatRephrase 57 12%Accept 127 27%None 212 46%AgreementAgree 166 36%NonAgree 14 3%None 286 61%Turn ManagementTurnTake 113 24%TurnElicit 85 18%None 268 58%Table 1: Distribution of semantic categories3193.1 Inter-coder agreement on feedbackexpression annotationIn general, dialogue act, agreement and turn anno-tations were coded by an expert annotator and theannotations were subsequently checked by a sec-ond expert annotator.
However, one dialogue wascoded independently and in parallel by two expertannotators to measure inter-coder agreement.
Ameasure was derived for each annotated featureusing the agreement analysis facility provided inANVIL.
Agreement between two annotation setsis calculated here in terms of Cohen?s kappa (Co-hen, 1960)1 and corrected kappa (Brennan andPrediger, 1981)2.
Anvil divides the annotations inslices and compares each slice.
We used slices of0.04 seconds.
The inter-coder agreement figuresobtained for the three types of annotation are givenin Table 2.feature Cohen?s k corrected kagreement 73.59 98.74dial act 84.53 98.87turn 73.52 99.16Table 2: Inter-coder agreement on feedback ex-pression annotationAlthough researchers do not totally agree onhow to measure agreement in various types of an-notated data and on how to interpret the resultingfigures, see Artstein and Poesio (2008), it is usu-ally assumed that Cohen?s kappa figures over 60are good while those over 75 are excellent (Fleiss,1971).
Looking at the cases of disagreement wecould see that many of these are due to the factthat the annotators had forgotten to remove someof the features automatically proposed by ANVILfrom the latest annotated element.4 Gesture annotationAll communicative head gestures in the videoswere found and annotated with ANVIL using asubset of the attributes defined in the MUMIN an-notation scheme (Allwood et al, 2007).
The MU-MIN scheme is a general framework for the studyof gestures in interpersonal communication.
Inthis study, we do not deal with functional classi-fication of the gestures in themselves, but rather1(Pa?
Pe)/(1?
Pe).2(Po ?
1/c)/(1 ?
1/c) where c is the number of cate-gories.with how gestures contribute to the semantic in-terpretations of linguistic expressions.
Therefore,only a subset of the MUMIN attributes has beenused, i.e.
Smile, Laughter, Scowl, FaceOther forfacial expressions, and Nod, Jerk, Tilt, SideTurn,Shake, Waggle, Other for head movements.A link was also established in ANVIL betweenthe gesture under consideration and the relevantspeech sequence where appropriate.
The link wasthen used to extract gesture information togetherwith the relevant linguistic annotations on whichto apply machine learning.The total number of head gestures annotated is264.
Of these, 114 (43%) co-occur with feedbackexpressions, with Nod as by far the most frequenttype (70 occurrences) followed by FaceOther asthe second most frequent (16).
The other tokensare distributed more or less evenly, with a few oc-currences (2-8) per type.
The remaining 150 ges-tures, linked to different linguistic expressions orto no expression at all, comprise many face ex-pressions and a number of tilts.
A rough prelim-inary analysis shows that their main functions arerelated to focusing or to different emotional atti-tudes.
They will be ignored in what follows.4.1 Measuring inter-coder agreement ongesture annotationThe head gestures in the DanPASS data have beencoded by non expert annotators (one annotatorper video) and subsequently controlled by a sec-ond annotator, with the exception of one videowhich was annotated independently and in parallelby two annotators.
The annotations of this videowere then used to measure inter-coder agreementin ANVIL as it was the case for the annotationson feedback expressions.
In the case of gestureswe also measured agreement on gesture segmen-tation.
The figures obtained are given in Table 3.feature Cohen?s k corrected kface segment 69.89 91.37face annotate 71.53 94.25head mov segment 71.21 91.75head mov annotate 71.65 95.14Table 3: Inter-coder agreement on head gestureannotationThese results are slightly worse than those ob-tained in previous studies using the same annota-tion scheme (Jokinen et al, 2008), but are still sat-320isfactory given the high number of categories pro-vided by the scheme.A distinction that seemed particularly difficultwas that between nods and jerks: although thedirection of the two movement types is different(down-up and up-down, respectively), the move-ment quality is very similar, and makes it difficultto see the direction clearly.
We return to this pointbelow, in connection with our data analysis.5 Analysis of the dataThe multimodal data we obtained by combiningthe linguistic annotations from DanPASS with thegesture annotation created in ANVIL, resulted intotwo different groups of data, one containing all Yesand No expressions, and the other the subset ofthose that are accompanied by a face expressionor a head movement, as shown in Table 4.Expression Count %Yes 420 90No 46 10Total 466 100Yes with gestures 102 90No with gestures 12 10Total with gestures 114 100Table 4: Yes and No datasetsThese two sets of data were used for automaticdialogue act classification, which was run in theWeka system (Witten and Frank, 2005).
We exper-imented with various Weka classifiers, compris-ing Hidden Naive Bayes, SMO, ID3, LADTreeand Decision Table.
The best results on most ofour data were obtained using Hidden Naive Bayes(HNB) (Zhang et al, 2005).
Therefore, here weshow the results of this classifier.
Ten-folds cross-validation was applied throughout.In the first group of experiments we took intoconsideration all the Yes and No expressions (420Yes and 46 No) without, however, considering ges-ture information.
The purpose was to see howprosodic information contributes to the classifica-tion of dialogue acts.
We started by totally leav-ing out prosody, i.e.
only the orthographic tran-scription (Yes and No expressions) was consid-ered; then we included information about stress(stressed or unstressed); in the third run we addedtone attributes, and in the fourth information onhesitation.
Agreement and turn attributes wereused in all experiments, while Dialogue act anno-tation was only used in the training phase.
Thebaseline for the evaluation are the results providedby Weka?s ZeroR classifier, which always selectsthe most frequent nominal class.In Table 5 we provide results in terms of preci-sion (P), recall (R) and F-measure (F).
These arecalculated in Weka as weighted averages of the re-sults obtained for each class.dataset Algor P R FYesNo ZeroR 27.8 52.8 36.5HNB 47.2 53 46.4+stress HNB 47.5 54.1 47.1+stress+tone HNB 47.8 54.3 47.4+stress+tone+hes HNB 47.7 54.5 47.3Table 5: Classification results with prosodic fea-turesThe results indicate that prosodic informationimproves the classification of dialogue acts withrespect to the baseline in all four experiments withimprovements of 10, 10.6, 10.9 and 10.8%, re-spectively.
The best results are obtained usinginformation on stress and tone, although the de-crease in accuracy when hesitations are introducedis not significant.
The confusion matrices showthat the classifier is best at identifying Accept,while it is very bad at identifying RepeatRephrase.This result if not surprising since the former typeis much more frequent in the data than the latter,and since prosodic information does not correlatewith RepeatRephrase in any systematic way.The second group of experiments was con-ducted on the dataset where feedback expressionsare accompanied by gestures (102 Yes and 12 No).The purpose this time was to see whether ges-ture information improves dialogue act classifica-tion.
We believe it makes sense to perform thetest based on this restricted dataset, rather than theentire material, because the portion of data wheregestures do accompany feedback expressions israther small (about 20%).
In a different domain,where subjects are less constrained by the techni-cal setting, we expect gestures would make for astronger and more widespread effect.The Precision, Recall and F-measure of the Ze-roR classifier on these data are 31.5, 56.1 and 40.4,respectively.
For these experiments, however, weused as a baseline the results obtained based onstress, tone and hesitation information, the com-bination that gave the best results on the larger321dataset.
Together with the prosodic information,Agreement and turn attributes were included justas earlier, while the dialogue act annotation wasonly used in the training phase.
Face expressionand head movement attributes were disregardedin the baseline.
We then added face expressionalone, head movement alone, and finally both ges-ture types together.
The results are shown in Ta-ble 6.dataset Algor P R FYesNo HNB 43.1 56.1 46.4+face HNB 43.7 56.1 46.9+headm HNB 44.7 55.3 48.2+face+headm HNB 49.9 57 50.3Table 6: Classification results with head gesturefeaturesThese results indicate that adding head ges-ture information improves the classification of di-alogue acts in this reduced dataset, although theimprovement is not impressive.
The best resultsare achieved when both face expressions and headmovements are taken into consideration.The confusion matrices show that although therecognition of both Answer and None improve, itis only the None class which is recognised quitereliably.
We already explained that in our annota-tion a large number of feedback utterances have anagreement or turn label without necessarily havingbeen assigned to one of our task-related dialogueact categories.
This means that head gestureshelp distinguishing utterances with an agreementor turn function from other kinds.
Looking closerat these utterances, we can see that nods and jerksoften occur together with TurnElicit, while tilts,side turns and smiles tend to occur with Agree.An issue that worries us is the granularity ofthe annotation categories.
To investigate this, ina third group of experiments we collapsed Nodand Jerk into a more general category: the distinc-tion had proven difficult for the annotators, and wedon?t have many jerks in the data.
The results, dis-played in Table 7, show as expected an improve-ment.
The class which is recognised best is stillNone.6 ConclusionIn this study we have experimented with the au-tomatic classification of feedback expressions intodifferent dialogue acts in a multimodal corpus ofdataset Algor P R FYesNo HNB 43.1 56.1 46.4+face HNB 43.7 56.1 46.9+headm HNB 47 57.9 51+face+headm HNB 51.6 57.9 53.9Table 7: Classification results with fewer headmovementsDanish.
We have conducted three sets of experi-ments, first looking at how prosodic features con-tribute to the classification, then testing whetherthe use of head gesture information improved theaccuracy of the classifier, finally running the clas-sification on a dataset in which the head move-ment types were slightly more general.
The re-sults indicate that prosodic features improve theclassification, and that in those cases where feed-back expressions are accompanied by head ges-tures, gesture information is also useful.
The re-sults also show that using a more coarse-graineddistinction of head movements improves classifi-cation in these data.Slightly more than half of the head gestures inour data co-occur with other linguistic utterancesthan those targeted in this study.
Extending our in-vestigation to those, as we plan to do, will provideus with a larger dataset and therefore presumablywith even more interesting and reliable results.The occurrence of gestures in the data stud-ied here is undoubtedly limited by the technicalsetup, since the two speakers do not see each other.Therefore, we want to investigate the role playedby head gestures in other types of video and largermaterials.
Extending the analysis to larger datasetswill also shed more light on whether our gestureannotation categories are too fine-grained for au-tomatic classification.AcknowledgementsThis research has been done under the projectVKK (Verbal and Bodily Communication) fundedby the Danish Council for Independent Researchin the Humanities, and the NOMCO project, acollaborative Nordic project with participating re-search groups at the universities of Gothenburg,Copenhagen and Helsinki which is funded by theNOS-HS NORDCORP programme.
We wouldalso like to thank Nina Gr?nnum for allowing us touse the DanPASS corpus, and our gesture annota-tors Josephine B?dker Arrild and Sara Andersen.322ReferencesJens Allwood, Loredana Cerrato, Kristiina Jokinen,Costanza Navarretta, and Patrizia Paggio.
2007.The MUMIN Coding Scheme for the Annotation ofFeedback, Turn Management and Sequencing.
Mul-timodal Corpora for Modelling Human MultimodalBehaviour.
Special Issue of the International Jour-nal of Language Resources and Evaluation, 41(3?4):273?287.Anne H. Anderson, Miles Bader, Ellen Gurman Bard,Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,Stephen Isard, Jacqueline Kowtko, Jan McAllister,Jim Miller, Catherine Sotillo, Henry S. Thompson,and Regina Weinert.
1991.
The HCRC Map TaskCorpus.
Language and Speech, 34:351?366.Ron Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596.Paul Boersma and David Weenink, 2009.
Praat: do-ing phonetics by computer.
Retrieved May 1, 2009,from http://www.praat.org/.Robert L. Brennan and Dale J. Prediger.
1981.
Co-efficient Kappa: Some uses, misuses, and alterna-tives.
Educational and Psychological Measurement,41:687?699.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-lettin, 76(5):378?382.Shinya Fujie, Y. Ejiri, K. Nakajima, Y Matsusaka, andTetsunor Kobayashi.
2004.
A conversation robotusing head gesture recognition as para-linguistic in-formation.
In Proceedings of the 13th IEEE Inter-national Workshop on Robot and Human InteractiveCommunication, pages 159 ?
164, september.Agustin Gravano and Julia Hirschberg.
2009.
Turn-yielding cues in task-oriented dialogue.
In Pro-ceedings of SIGDIAL 2009: the 10th Annual Meet-ing of the Special Interest Group in Discourse andDialogue, September 2009, pages 253?261, QueenMary University of London.Nina Gr?nnum.
2006.
DanPASS - a Danish pho-netically annotated spontaneous speech corpus.
InN.
Calzolari, K. Choukri, A. Gangemi, B. Maegaard,J.
Mariani, J. Odijk, and D. Tapias, editors, Pro-ceedings of the 5th LREC, pages 1578?1583, Genoa,May.Kristiina Jokinen and Anton Ragni.
2007.
Cluster-ing experiments on the communicative prop- ertiesof gaze and gestures.
In Proceeding of the 3rd.Baltic Conference on Human Language Technolo-gies, Kaunas, Lithuania, October.Kristiina Jokinen, Costanza Navarretta, and PatriziaPaggio.
2008.
Distinguishing the communica-tive functions of gestures.
In Proceedings of the5th MLMI, LNCS 5237, pages 38?49, Utrecht, TheNetherlands, September.
Springer.Michael Kipp.
2004.
Gesture Generation by Imita-tion - From Human Behavior to Computer Charac-ter Animation.
Ph.D. thesis, Saarland University,Saarbruecken, Germany, Boca Raton, Florida, dis-sertation.com.Max M. Louwerse, Patrick Jeuniaux, Mohammed E.Hoque, Jie Wu, and Gwineth Lewis.
2006.
Mul-timodal communication in computer-mediated maptask scenarios.
In R. Sun and N. Miyake, editors,Proceedings of the 28th Annual Conference of theCognitive Science Society, pages 1717?1722, Mah-wah, NJ: Erlbaum.Max M. Louwerse, Nick Benesh, Mohammed E.Hoque, Patrick Jeuniaux, Gwineth Lewis, Jie Wu,and Megan Zirnstein.
2007.
Multimodal communi-cation in face-to-face conversations.
In R. Sun andN.
Miyake, editors, Proceedings of the 29th AnnualConference of the Cognitive Science Society, pages1235?1240, Mahwah, NJ: Erlbaum.Evelyn McClave.
2000.
Linguistic functions of headmovements in the context of speech.
Journal ofPragmatics, 32:855?878.Louis-Philippe Morency, Candace Sidner, ChristopherLee, and Trevor Darrell.
2005.
Contextual Recog-nition of Head Gestures.
In Proceedings of the In-ternational Conference on Multi-modal Interfaces.Louis-Philippe Morency, Candace Sidner, ChristopherLee, and Trevor Darrell.
2007.
Head gestures forperceptual interfaces: The role of context in im-proving recognition.
Artificial Intelligence, 171(8?9):568?585.Louis-Philippe Morency, Iwan de Kok, and JonathanGratch.
2009.
A probabilistic multimodal ap-proach for predicting listener backchannels.
Au-tonomous Agents and Multi-Agent Systems, 20:70?84, Springer.Gabriel Murray and Steve Renals.
2008.
DetectingAction Meetings in Meetings.
In Proceedings ofthe 5th MLMI, LNCS 5237, pages 208?213, Utrecht,The Netherlands, September.
Springer.Harm Rieks op den Akker and Christian Schulz.
2008.Exploring features and classifiers for dialogue actsegmentation.
In Proceedings of the 5th MLMI,pages 196?207.Patrizia Paggio and Costanza Navarretta.
2010.
Feed-back in Head Gesture and Speech.
To appear in Pro-ceedings of 7th Conference on Language Resourcesand Evaluation (LREC-2010), Malta, May.323Dennis Reidsma, Dirk Heylen, and Harm Rieks op denAkker.
2009.
On the Contextual Analysis of Agree-ment Scores.
In Michael Kipp, Jean-Claude Mar-tin, Patrizia Paggio, and Dirk Heylen, editors, Multi-modal Corpora From Models of Natural Interactionto Systems and Applications, number 5509 in Lec-ture Notes in Artificial Intelligence, pages 122?137.Springer.Vivek Kumar Rangarajan Sridhar, Srinivas Bangaloreb,and Shrikanth Narayanan.
2009.
Combining lexi-cal, syntactic and prosodic cues for improved onlinedialog act tagging.
Computer Speech & Language,23(4):407?422.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.Morgan Kaufmann, San Francisco, second edition.Harry Zhang, Liangxiao Jiang, and Jiang Su.
2005.Hidden Naive Bayes.
In Proceedings of the Twen-tieth National Conference on Artificial Intelligence,pages 919?924.324
