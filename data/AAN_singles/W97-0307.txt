Tagging Grammatical FunctionsThors ten  Brants ,  Wo jc iech  Skut ,  Br ig i t te  KrennUn ivers i t~t  des Saar landesComputat iona l  L inguist icsD-66041 Saarbr / icken,  Germany{brant s, skut,krenn}@coli.uni-sb, deAbst ractThis paper addresses issues in automatedtreebank construction.
We show how stan-dard part-of-speech tagging techniques ex-tend to the more general problem of struc-tural annotation, especially for determi-ning grammatical functions and syntacticcategories.
Annotation is viewed as an in-teractive process where manual and auto-matic processing alternate.
Efficiency andaccuracy results are presented.
We also dis-cuss further automation steps.1 In t roduct ionThe aim of the work reported here is to construct acorpus of German annotated with syntactic structu-res (treebank).
The required size of the treebank andgranularity of encoded information make it neces-sary.
to ensure high annotation efficiency and accu-racy.
Annotation automation has thus become oneof the central issues of the project.In this section, we discuss the relation between au-tomatic and manual annotation.
Section 2 focuseson the annotation format employed in our treebank.The annotation software is presented in section 3.Sections 4 and 5 deal with automatic assignment ofgrammatical functions and phrasal categories.
Ex-periments on automating the annotation are presen-ted in section 6.1.1 Automat ic  vs. Manua l  Annotat ionA problem for corpus annotation is the trade-off bet-ween efficiency, accuracy and coverage.
Althoughaccuracy increases ignificantly as annotators gainexpertise, incorrect hand-parses still occur.
Theirfrequency depends on the granularity of the enco-ded information.Due to this residual error rate, automatic anno-tation of frequently occurring phenomena is likelyto yield better results than even well-trained hu-man annotators.
For infrequently occurring con-structions, however, manual annotation is more re-liable, as is manual annotation of phenomena invol-ving non-syntactic information (e.g., resolution ofattachment ambiguities based on world knowledge).As a consequence, fficiency and reliability of an-notation can be significantly increased by combiningautomatic annotation with human processing skillsand supervision, especially if this combination is im-plemented as an interactive process.2 Annotat ion  SchemeExisting treebanks of English ((Marcus et al, 1994),(Sampson, 1995), (Black et al, 1996)) contain con-ventional phrase-structure tr es augmented with an-notations for discontinuous constituents.
As this en-coding strategy is not well-suited to a free word or-der language like German, we have focussed on a lesssurface-oriented level of description, most closely re-lated to the LFG f-structure, and representationsused in dependency grammar.
To avoid confusionwith theory-specific constructs, we use the genericterm argument structure to refer to our annotationformat.
The main advantages of the model are: it isrelatively theory-independent a d closely related tosemantics.
For more details on the linguistic speci-fications of the annotation scheme see (Skut et al,1997).
A similar approach as been also successfullyapplied in the TSNLP database, cf.
(Lehmann et al,1996).In contrast to conventional phrase-structuregrammars, argument structure annotations are notinfluenced by word order.
Local and non-local de-pendencies are represented in the same way, thelatter indicated by crossing branches in the hier-archical structure, as shown in figure 1 where inthe VP the terminals of the direct object OA (denTraum yon der kleinen Gastst~tte) are not adjacentto the head HD aufgegeben 1.
For a related handling1 See appendix A for a description of tags used throng-64DenARTTheTraumNNdream+vonAPPRofder k le inen  Gastst"atte hat er nochART ADJA NN VAFIN PPER ADVthe small inn has he yet'He has not yet given up the dream of a small inn.
'nichtPTKNEGnotaufgegebenVVPPgiven upFigure 1: Example sentenceof non-projective phenomena see (Tapanainen andJ/irvinen, 1997).Such a representation permits clear separation ofword order (in the surface string) and syntactic de-pendencies (in the hierarchical structure).
Thuswe avoid explicit explanatory statements about thecomplex interrelation between word order and syn-tactic structure in free word order languages.
Suchstatements are generally theory-specific and there-fore are not appropriate for a descriptive approachto annotation.
The relation between syntactic de-pendencies and surface order can nontheless be in-ferred from the data.
This provides a promising wayof handling free word order phenomena.
2.3 Annotat ion  Too lSince syntactic annotation of corpora is time-consuming, a partially automated annotation toolhas been developed in order to increase fficiency.3.1 The User Inter faceFor optimal human-machine interaction, the toolsupports immediate graphical representation f thestructure being annotated.Since keyboard input is most efficient for assigningcategories to words and phrases, cf.
(Lehmann et al,1996; Marcus et al, 1994), and structural manipula-tions are executed most efficiently using the mouse,both an elaborate keyboard and optical interface isprovided.
As suggested by Robert MacIntyre 3, it ishout this paper.2'Free' word order is a function of several interactingparameters such as category, case and topic-focus arti-culation.
Varying the order of words in a sentence yieldsa continuum of grammaticality judgments rather than asimple right-wrong distinction.3personal communication, Oct. 1996most efficient o use one hand for structural com-mands with the mouse and the other hand for shortkeyboard input.By additionally offering online menus for com-mands and labels, the tool suits beginners as wellas experienced users.
Commands uch as "groupwords", "group phrases", "ungroup", "change la-bels", "re-attach nodes", "generate postscript out-put", etc.
are available.The three tagsets (word, phrase, and edge labels)used by the annotation tool are variable.
They arestored together with the corpus, which allows easymodification and exchange of tagsets.
In addition,appropriateness checks are performed automatically.Comments can be added to structures.Figure 2 shows a screen dump of the graphicalinterface.3.2 Automat ing  Annotat ionExisting treebank annotation tools are characterisedby a high degree of automation.
The task of theannotator is to correct the output of a parser, i.e.,to eliminate wrong readings, complete partial parses,and adjust partially incorrect ones.Since broad-coverage parsers for German, espe-cially robust parsers that assign predicate-argumentstructure and allow crossing branches, are not availa-ble, or require an annotated traing corpus (cf.
(Col-lins, 1996), (Eisner, 1996)).As a consequence, we have adopted a bootstrap-ping approach, and gradually increased the degreeof automation using already annotated sentences astraining material for a stochastic processing module.This aspect of the work has led to a new modelof human supervision.
Here automatic annotationand human supervision are combined interactivelywhereby annotators are asked to confirm the local65- G_enm'a l :Corpus: IRefCorpus Tes~,ople I J~\]Editor: IThorsten J~\]I -~- I i  _,,oa, li E.,t i i O~,o?.
i-Sentence:No.
: 4 / 1269Comment: IOdgln: refcorp.ttLast edited: Thorsten, 28/05/97, 14:08:48Es o spleit I ebe~ keine 3 Roll%PPER WFIN ADV PlAT NNI<U511KOUS ART NNgef"~llg 9 iS~oADJD VAFIN-Move:I .~r~, II _"..' I ~_o'o:' ,I -,o II +,o I D ~,,,e,I -'??
II +,oo I Mat~eo:oi r--_Dependeney: / -s?~''?n: I_Command: I| i ~-"~'?
IIIB_ our o ,o "?u?"'
;i' mu,IiT ag:Node no.
: I JZag: I IBI-""' II "-'?~ I1-~-II Switchin~ to sentence no, 4.,.
Done.Figure 2: Screen dump of the annotation toolpredictions of the parser.
The size of such 'super-vision increments' varies from local trees of depthone to larger chunks, depending on the amount oftraining data available.We distinguish six degrees of automation:0) Completely manual annotation.1) The user determines phrase boundaries andsyntactic ategories (S, NP, VP, .
.
. )
.
The pro-gram automatically assigns grammatical func-tions.
The annotator can alter the assigned tags(cf.
figure 3).2) The user only determines the components of anew phrase (local tree of depth 1), while bothcategory and function labels are assigned auto-matically.
Again, the annotator has the optionof altering the assigned tags (cf.
figure 4).3) The user selects a substring and a category,whereas the entire structure covering the sub-string is determined automatically (cf.
figure 5).4) The program performs imple bracketing, i.e.,finds 'kernel phrases' without the user havingto explicitly mark phrase boundaries.
The taskcan be performed by a chunk parser that isequipped with an appropriate finite state gram-mar (Abney, 1996).5) The program suggests partiM or complete par-ses.A set of 500 manually annotated training sent-ences (step 0) was sufficient for a statistical taggerto reliably assign grammatical functions, providedthe user determines the elements of a phrase andits category (step 1).
Approximately 700 additio-nal sentences have been annotated this way.
An-notation efficiency increased by 25 %, namely froman average annotation time of 4 minutes to 3 minu-tes per sentence (300 to 400 words per hour).
The1,200 sentences were used to train the tagger for au-tomation step 2.
Together with improvements in theuser interface, this increased the efficiency by ano-ther 33%, from approximately 3 to 2 minutes (600words per hour).
The fastest annotators cover up to66das 1993 startende Bonusprogramm for VielfliegerART CARD ADJA NN APPR NN'the bonus program for .h'equent fliers starting in 1993'Figure 3: Example for automation level 1: the userhas marked das, the AP, Bonusprogramm, and thePP as a constituent of category NP, and the tool'stask is to determine the new edge labels (markedwith question marks), which are, from left to right,NK, NK, NK, MNR.das 1993 startende Bonusprogramm ffir VielfliegerART CARD ADJA NN APPR NN'the bonus program for frequent fliers starting in 1993'Figure 4: Example for automation level 2: the userhas marked as, the AP, Bonusprogramm and the PPas a constituent, and the tool's task is to determinethe new node and edge labels (marked with questionmarks).1000 words per hour.At present, the treebank comprises 3000 sent-ences, each annotated independently b two anno-tators.
1,200 of the sentences are compared withthe corresponding second annotation and are clea-ned, 1,800 are currently cleaned.In the following sections, the automation steps 1and 2 are presented in detail.4 Tagg ing  Grammat ica l  Funct ions4.1 The TaggerIn contrast to a standard part-of-speech taggerwhich estimates lexical and contextual probabilitiesof tags from sequences of word-tag pairs in a corpus,(e.g.
(Cutting et al, 1992; Feldweg, 1995)), the tag-ger for grammatical functions works with lexical andcontextual probability measures Pq(.)
depending onthe category of the mother node (Q).
Each phrasalcategory (S, VP, NP, PP etc.)
is represented by adifferent Markov model.
The categories of the dau-+++?+ ++das 1993 startende Bonusprograrnm for VielfliegerART CARD ADJA NN APPR NN'the bonus program for frequent fliers starting in 1993'Figure 5: Example for automation level 3: the userhas marked the words as a constituent, and the tool'stask is to determine simple sub-phrases (the AP andPP) as well as the new node and edge labels (cf.previous figures ~br the resulting structure).SelbstADVhimselfl"lbesucht hat PeterVVPP VAFIN NEvisited has Peter+lSabine nieNE ADVSabine never'Peter never visited Sabine himself'Figure 6: Example sentenceghter nodes correspond to the outputs of the Mar-kov model, while grammatical functions correspondto states.The structure of a sample sentence is shown infigure 6.
Figure 7 shows those parts of the Markovmodels for sentences (S) and verb phrases (VP) thatrepresent the correct paths for the example.
4Given a sequence of word and phrase categoriesT = T1...Tk and a parent category Q, we cal-culate the sequence of grammatical functions G =G1 ... Gk that link T and Q asargmaxPQ(GIT ) (1)GPq(a).
Pq(TIC)= argmax a PQ(T)= argm xPq(a).
Pq(TJG)GAssuming the Markov property we have4cf.
appendix A for a description of tags used in theexample67VP VA FIN NE A D V&--@--?---?--@--?O ~m m ~ao ~ .2.
~ ~-ADV VVPP NEPs(ADVIMO) 1 PVp (VVPP IHD) l Pvp(N~IOA) 1N ~ oo ~ d dFigure 7: Parts of the Markov models used in Selbst besucht hat Peter Sabine hie (cf.
figure 6).
All unusedstates, transitions and outputs are omitted.andkPQ(TIG) = I I  PQ(~qlG,) (2)i=1kPq(a) = I I  P (a, lC,) (3)i=1The contexts Ci are modeled by a fixed number ofsurrounding elements.
Currently, we use two gram-matical functions, which results in a trigram model:PO(G) = H Po(GiIGi-2, Gi-1) (4)i=1The contexts are smoothed by linear interpolationof unigrams, bigrams, and trigrams.
Their weightsare calculated by deleted interpolation (Brown et al,1992).The predictions of the tagger are correct in ap-prox.
94% of Ml cases.
In section 4.3, we demons-trate how to cope with wrong predictions.4.2 Ser ia l  OrderAs the annotation format permits trees with crossingbranches, we need a convention for determining therelative position of overlapping sibling phrases in or-der to assign them a position in a Markov model.
Forinstance, in figure 6 the range of the terminal nodepositions of VP overlaps with those of the subject$B and the finite verb HD.
Thus there is no singlea-priori position for the VP node 5.The position of a phrase depends on the positionof its descendants.
We define the relative order oftwo phrases recursively as the order of their anchors,i.e., some specified daughter nodes.
If the anchorsare words, we simply take their linear order.The exact definition of the anchor is based on lin-guistic knowledge.
We choose the most intuitive al-ternative and define the anchor as the head of thephrase (or some equivalent function).
Noun phrasesdo not necessarily have a unique head; instead, weuse the last element in the noun kernel (elementsof the noun kernel are determiners, adjectives, andnouns) to mark the anchor position.
Except for NPs,we employ a default rule that takes the leftmost ele-ment as the anchor in case the phrase has no (uni-que) head.Thus the position of the VP in figure 6 is definedas equal to the string position of besucht.
The po-sition of the VP node in figure 1 is equal to that ofanfgegeben, and the position of the NP in figure 3 isequivalent to that of Bonusprograrara.4.3 Rel iab i l i tyExperience gained from the development ofthe PennTreebank (Marcus et al, 1994) has shown that au-SWithout crossing edges, the serial order of phrasesis trivial: phrase Q1 precedes phrase Q2 if and only ifall terminal nodes derived from Qa precede those of Q2.This suffices to uniquely determine the order of siblingnodes.68tomatic annotation is useful only if it is absolutelycorrect, while wrong analyses are often difficult todetect and their correction can be time-consuming.To prevent he human annotator f om missing er-rors, the tagger for grammatical functions is equip-ped with a measure for the reliability of its output.Given a sequence of categories, the tagger cal-culates the most probable sequence of grammaticalfunctions.
In addition, it computes the probabili-ties of the second-best functions of each daughternode.
If some of these probabilities are close to thatof the best sequence, the alternatives are regardedas equally suited and the most probable one is nottaken to be the sole winner, the prediction is markedas unreliable in the output of the tagger.These unreliable predictions can be further classi-fied in that we distinguish "unreliable" sequences asopposed to "almost reliable" ones.The distance between two probabilities for thebest and second-best alternative, Pbest and Pse?ond,is measured by their quotient.
The classification ofreliability is based on thresholds.
In the current im-plementation we employ three degrees of reliabilitywhich are separated by two thresholds 01 and 02.
01separating unreliable decisions from those conside-red almost reliable.
02 marks the difference betweenalmost and fully reliable predictions.Unrel iable:Pbes-----k- < 01PseeondThe probabilities of alternative assignments are wi-thin some small specified distance.
In this case, itis the annotator who has to specify the grammaticalfunction.A lmost  rel iable:01 < Pbes_____t__ < 02PsecondThe probability of an alternative is within some lar-ger distance.
In this case, the most probable func-tion is displayed, but the annotator has to confirmit.Rel iable:Pbes-----L- __> 02PsecondThe probabilitiesof all alternatives are much smallerthan that of the best assignment, thus the latter isassigned.For efficiency, an extended Viterbi algorithm isused.
Instead of keeping track of the best path only(of.
(Rabiner, 1989)), we keep track of all paths thatfall into the range marked by the probability of thebest path and 02, i.e., we keep track of all alternativepaths with probability Palt for whichPbestPart _> 02 "Suitable values for 01 and 02 were determined em-pirically (cf.
section 6).5 Tagg ing  Phrase  Categor iesThe second level of automation (cf.
section 3) au-tomates the recognition of phrasal categories, andso frees the annotator from typing phrase labels.The task is performed by an extension of the tag-ger presented in the previous ection where differentMarkov models for each category were introduced.The annotator determines the category of the cur-rent phrase, and the tool runs the appropriate modelto determine the edge labels.To assign the phrase label automatically, we runall models in parallel.
Each model assigns gramma-tical functions and, more important for this step,a probability to the phrase.
The model assigningthe highest probability is assumed to be most ade-quate, and the corresponding label is assigned to thephrase.Formally, we calculate the phrase category Q (andat the same time the sequence of grammatical func-tions G = G1 ... Gk) on the basis of the sequence ofdaughters T = T1 .
.
.
Tk withargmax maXPQ(G\]T).O GThis procedure is equivalent to a different viewon the same problem involving one large (combined)Markov model that enables a very efficient calcula-tion of the maximum.Let ~Q be the set of all grammatical functionsthat can occur within a phrase of type Q. Assumethat these sets are pairwise disjoint.
One can easilyachieve this property by indexing all used gramma-tical functions with their associated phrases and, ifnecessary, duplicating labels, e.g., instead of usingHD, MO, .
.
.
,  use the indexed labels HDs, HDvp,MONp, ...This property makes it possible to deter-mine a phrase category by inspecting the gramma-tical functions involved.When applied, the combined model assigns gram-matical functions to the elements of a phrase (notknowing its category in advance).
If transitions bet-ween states representing labels with different indicesare forced to zero probability (together with smoo-thing applied to other transitions), all labels assi-gned to a phrase get the same index.
This uniquelyidentifies a phrase category.The two additional conditionsG e GQi :=v G ?
GQ2 (Qi ?
Q2)andG1 E CO A G2 ~ GQ :::V P(G2\[G1) = 069are sufficient o calculateargmax P( G\[T)Gusing the Viterbi algorithm and to identify boththe phrase category and the respective grammaticalfunctions.Again, as described in section 4, we calculate pro-babilities for alternative candidates in order to getreliability estimates.The overall accuracy of this approach is approx.95%, and higher if we only consider the reliable ca-ses.
Details about the accuracy are reported in thenext section.6 Exper imentsTo investigate the possibility of automating anno-tation, experiments were performed with the clea-ned part of the treebank 6 (approx.
1,200 sentences,24,000 words).
The first run of experiments was car-ried out to test tagging of grammatical functions, thesecond run to test tagging of phrase categories.6.1 Grammatical  Funct ionsThis experiment tested the reliability of assigninggrammatical functions given the category of thephrase and the daughter nodes (supplied by the an-notator).Let us consider the sentence in figure 6: two se-quences of grammatical functions are to be determi-ned, namely the grammatical functions of the dau-ghter nodes of S and VP.
The information givenfor selbst besucht Sabine is its category (VP) andthe daughter categories: adverb (ADV), past parti-ciple (wee), and proper noun (NE).
The task isto assign the functions modifier (MO) to ADV, head(SO) to wee and direct (accusative) object (OA)to NE.
Similarly, function tags are assigned to thecomponents of the sentence (S).The tagger described in section 4 was used.The corpus was divided into two disjoint parts,one for training (90% of the respective corpus), andone for testing (10%).
This procedure was repeated10 times with different partitions.
Then the averageaccuracy was calculated.The thresholds for search beams were set to 61 = 5and 62 = 100, i.e., a decision is classified as reliableif there is no alternative with a probability largerthan 1~0 of the best function tag.
The predictionis classified as unreliable if the probability of an al-ternative is larger than ~ of the most probable tag.6The corpus is part of the German newspaper textprovided on the ECI CD-ROM.
It has been part-of-speech tagged and manually corrected previously, cf.
(Thielen and Schiller, 1995).Table 1: Levels of reliability and the percentage ca-ses where the tagger assigned a correct grammati-cal function (or would have assigned if a decision isforced).reliablemarkedunreliableoverallcases correct89% 96.7%7% 84.3%4% 57.3%100% 94.2%If there is an akernative between these two thres-holds, the prediction is classified as almost reliableand marked in the output (cf.
section 4.3: markedassignments are to be confirmed by the annotator,unreliable assignments are deleted, annotation is leftto the annotator).Table 1 shows tagging accuracy depending on thethree different levels of reliability.
The results con-firm the choice of reliability measures: the lower thereliability, the lower the accuracy.Table 2 shows tagging accuracy depending on thecategory of the phrase and the level of reliability.The table contains the following information: thenumber of all mother-daughter relations (i.e., num-ber of words and phrases which are immediately do-minated by a mother node of a particular category),the overall accuracy for that phrasal category andthe accuraciees for the three reliability intervals.6.2 Er ror  Analys is  for  Funct ionAss ignmentThe inspection of tagging errors reveals everal sour-ces of wrong assignments.
Table 3 shows the 10 mostfrequent errors 7 which constitute 25% of all errors(1509 errors occurred uring 10 test runs).Read the table in the following way: line 2 showsthe second-most frequent error.
It concerns NPs oc-curring in a sentence (S); this combination occurred1477 times during testing.
In 286 of these occur-rences the N P is assigned the grammatical functionOA (accusative object) manually, but of these 286cases the tagger assigned the function SB (subject)56 times.The errors fall into the following classes:1.
There is insufficient information in the node la-bels to disambiguate he grammatical function.Line 1 is an example for insufficient information.The tag NP is uninformative about its case and the-refore the tagger has to distinguish SB (subject) and7See appendix A for a description of tags used in thetable.70Table 2: Tagging accuracy for assigning rammaticalfunctions depending on the category of the mothernode.
For each category, the first row shows the per-centage of branches that occur within this categoryand the overall accuracy, the following rows show therelative percentage and accuracy for different levelsof reliability.cases correctS 26% 89.1%decision 85% 92.7%marked 8% 81.9%no decision 7% 52.9%VP 7% 90.9%decision 97% 92.2%marked 1% 57.7%no decision 2% 52.3%NP 26% 96.4%decision 86% 98.6%marked 10% 86.8%no decision 4% 73.0%PP 24% 97.9%decision 92% 99.2%marked 6% 85.8%no decision 2% 75.5%others 18% 94.7%decision 91% 98.0%marked 6% 82.8%no decision 3% 22.1%Table 3: The 10 most frequent errors in assigninggrammatical functions.
The table shows a motherand a daughter node category, the frequency of thisparticular combination (sum over 10 test runs), thegrammatical function assigned manually (and its fre-quency) and the grammatical function assigned bythe tagger (and its frequency).phrase elem f original assigned5.6.7.8.9.10.1.
S2.
S3.
NP4.
SPPVPSSSVPNP 1477NP 1477PP 470VP 613PP 252NP 286NP 1477NP 1477S 186PP 453SBOAPGPDPGDAPDMOMOSBP894 OA286 SB52 MNR47 OC30 MNR32 OA72 SB33 SB78 PD21 MO65565042302625212121OA (accusative object) on the basis of its position,which is not very reliable in German.
Missing in-formation in the labels is the main source of errors.Therefore, we currently investigate the benefits of amorphological component and percolation of selec-ted information to parent nodes.2.
Due to the n-gram approach, the tagger onlysees a local window of the sentences.Some linguistic knowledge is inherently global, e.g.,there is at most one subject in a sentence and onehead in a VP.
Errors of this type may be reduced byintroducing finite state constraints that restrict thepossible sequences of functions within each phrase.3.
The manual annotation is wrong, and a correcttagger prediction is counted as an error.At earlier stages of annotation, the main source oferrors was wrong or missing manual annotation.
Insome cases, the tagger was able to abstract fromthese errors during the training phase and subse-quently assigned the correct tag for the test data.However, when performing a comparison against hecorpus, these differences are marked as errors.
Mostof these errors were eliminated by comparing twoindependent annotations and cleaning up the data.6.3 Phrase  Categor iesIn this experiment, he reliability of assigning phrasecategories given the categories of the daughter nodes(they are supplied by the annotator) was tested.Consider the sentence in figure 6: two phrase ca-tegories are to be determined (VP and S).
The in-formation given for selbst besucM Sabine is the se-quence of categories: adverb (ADV), past participle71Table 4: Levels of reliability and the percentage ofcases in which the tagger assigned a correct phrasecategory (or would have assigned if a decision isforced).reliablemarkedunreliableoverallcases correct79% 98.5%16% 90.4%5% 65.9%100% 95.4%(VVPP), and proper noun (NE).
The task is to as-sign category VP.
Subsequently, S is to be assignedbased on the categories of the daughters VP, VAFIN,NE, and ADV.The extended tagger using a combined model asdescribed in section 5 was applied.Again, the corpus is divided into two disjointparts, one for training (90% of the corpus), andone for testing (10%).
The procedure is repeated10 times with different partitions.
Then the averageaccuracy was calculated.The same thresholds for search beams as for thefirst set of experiments were used.Table 4 shows tagging accuracy depending on thethree different levels of reliability.Table 5 shows tagging accuracy depending on thecategory of the phrase and the level of reliability.The table contains the following information: thepercentage of occurrences of the particular phrase,the overall accuracy for that phrasal category andthe accuracy for each of the three reliability inter-vals.6.4 Er ror  Analysis for CategoryAssignmentWhen forced to make a decision (even in unrelia-ble cases) 435 errors occured during the 10 testruns (4.5% error rate).
Table 6 shows the 10 most-frequent errors which constitute 50% of all errors.The most frequent error was the confusion of Sand VP.
They differ in that sentences S contain fi-nite verbs and verb phrases VP contain non-finiteverbs.
But the tagger is trained on data that con-tain incomplete sentences and therefore sometimeserroneously assumes an incomplete S instead of aVP.
To avoid this type of error, the tagger shouldbe able to take the neighborhood fphrases into ac-count.
Then, it could detect the finite verb thatcompletes the sentence.Adjective phrases AP and noun phrases NP areconfused by the tagger (line 5 in table 6), since al-most all AP's can be NP's.
This error could alsoTable 5: Tagging accuracy for assigning phrase cate-gories, depending on the manually assigned category.For each category, the first row shows the percentageof phrases belongi:lg to a specific category (accor-ding to manual ~,zsignment) and the percentage ofcorrect assignments.
The following rows show therelative percentage and accuracy for different levelsof reliability.cases correctS 20% 97.5%decision 96% 99.7%marked 2% 63.2%no decision 2% 29.0%VP 9% 93.2%decision 71% 96.4%marked 24% 91.3%no decision 5% 60.9%NP 29% 96.1%decision 81% 99.3%marked 13% 91.8%no decision 6% 64.9%PP 24% 98.7%decision 94% 99.6%marked 4% 92.5%no decision 2% 70.8%others 18% 89.0%decision 42% 91.7%marked 45% 90.6%no decision 12% 73.2%72Table 6: The 10 most frequent errors in assigningphrase categories (summed over reliability levels).The table shows the phrase category assigned manu-ally (and its frequency) and the category erroneouslyassigned by the tagger (and its frequency).I?2.3.4.5.6.7.8.9.10.phrase f assignedVP 828 SNP 2812 NMNP 2812 PPNP 2812 SAP 419 NPDL 20 CSPP 2298 NPS 1910 NPAP 419 PPMPN 293 NP46323125151515151111be fixed by inspecting the context and detecting theassociated NP.As for assigning rammatical functions, insuffi-cient information i the labels is a significant sourceof errors, cf.
the second-most frequent error.
Alarge number of cardinal-noun pairs forms a nume-rical component (NM), like 7 Millionen, 50 Prozent,etc (7 million, 50 percent).
But this combinationalso occurs in NPs like 20 Leule, 3 Monate, ... (20people, 3 months), which are mis-tagged since theyare less frequent.
This can be fixed by introducingan extra tag for nouns denoting numericals.7 ConclusionA German newspaper corpus is currently being an-notated with a new annotation scheme specially de-signed for free word order languages.Two levels of automatic annotation (level 1: assi-gning grammatical functions and level 2: assigningphrase categories) have been presented and evalua-ted in this paper.The overall accuracy for assigning rammaticalfunctions is 94.2%, ranging from 89% to 98%, de-pending on the type of phrase.
The least accuracyis achieved for sentences, the best for prepositionalphrases.
By suppressing unreliable decisions, pre-cision can be increased to range from 92% to 99%.The overall accuracy for assigning phrase catego-ries is 95.4%, ranging from 89% to 99%, dependingthe category.
By suppressing unreliable decisions,precision can also be increased to range from 92% toover 99%.In the error analysis, the following sources of mi-sinterpretation could be identified: insufficient lin-guistic information i  the nodes (e.g., missing caseinformation), and insufficient information about theglobal structure of phrases (e.g., missing valency in-formation).
Morphological information in the tag-set, for example, helps to identify the objects andthe subject of a sentence.
Using a more fine-grainedtagset, however, requires methods for adjusting thegranularity of the tagset o the size (and coverage)of the corpus, in order to cope with the sparse dataproblem.8 AcknowledgementsThis work is part of the DFG Sonderforschungs-bereich 378 Resource-Adaptive Cognitive Processes,Project C3 Concu rent Grammar Processing.We wish to tl~ank the universities of Stuttgartand Tiibingen for kindly providing us with a hand-corrected part-of-speech tagged corpus.
We alsowish to thank Jason Eisner, Robert MacIntyre andAnn Taylor for valuable discussions on dependencyparsing and the Penn Treebank annotation.
Specialthanks go to Oliver Plaehn, who implemented theannotation tool, and to our six fearless annotators.ReferencesAbney, Steven.
1996.
Partial parsing via finite-statecascades.
In Proceedings of the ESSLLI'96 RobustParsing Workshop, Prague, Czech Republic.Black, Ezra, Stephen Eubank, Hideki Kashioka,David Magerman, Roger Garside, and GeoffreyLeech.
1996.
Beyond skeleton parsing: Producinga comprehensive large-scale general-english tree-bank with full grammaticall analysis.
In Proc.
ofCOLING-96, pages 107-113, Kopenhagen, Den-mark.Brown, P. F., V. J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural anguage.
Com-putational Linguistics, 18(4):467-479.Collins, Michael.
1996.
A new statistical parser ba-sed on bigram lexical dependencies.
In Procee-dings ofACL-96, Sant Cruz, CA, USA.Cutting, Doug, Julian Kupiee, Jan Pedersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the 3rd Conference onApplied Natural Language Processing (ACL), pa-ges 133-140.Eisner, Jason M. 1996.
Three new probabilisticmodels for dependency parsing: An exploration.In Proceedings of COLING-96, Kopenhagen, Den-mark.Feldweg, Helmut.
1995.
Implementation a d eva-luation of a german hmm for pos disambiguation.73In Proceedings of EACL-SIGDAT-95 Workshop,Dublin, Ireland.Lehmann, Sabine, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein,Kirsten Falkedal, Frederik Fouvry, DominiqueEstival, Eva Dauphin, I-Ierv~ Compagnion, JudithBaur, Lorna Balkan, and Doug Arnold.
1996.
TS-NLP - -  Test Suites for Natural Language Proces-sing.
In Proceedings of COLING 1996, Kopenha-gen.Marcus, Mitchell, Grace Kim, Mary Ann Marcinkie-wicz, Robert MacIntyre, Ann Bies, Mark Fergu-son, Karen Katz, and Britta Schasberger.
1994.The penn treebank: Annotating predicate argu-ment structure.
In Proceedings ofthe Human Lan-guage Technology Workshop, San Francisco, Mor-gan Kaufmann.Rabiner, L. R. 1989.
A tutorial on hidden markovmodels and selected applications in speech reco-gnition.
In Proceedings ofthe IEEE, volume 77(2),pages 257-285.Sampson, Geoffrey.
1995.
English for the Computer.Oxford University Press, Oxford.Skut, Wojciech, Brigitte Krenn, Thorsten Brants,and Hans Uszkoreit.
1997.
An annotation schemefor free word order languages.
In Proceedings ofANLP-97, Washington, DC.Tapanainen, Pasi and Timo J~irvinen.
1997.
A non-projective dependency parser.
In Proceedings ofANLP-97, Washington, DC.Thielen, Christine and Anne Schiller.
1995.
Ein klei-nes und erweitertes Tagset ffirs Deutsche.
In Ta-gungsberichte d s Arbeitstreffens Lezikon + Text17./18.
Februar 1994, Schlofl Hohent~bingen.
Le-zicographica Series Maior, Tfibingen.
Niemeyer.Append ix  A: TagsetsThis section contains descriptions of tags used in thispaper.
These are not complete lists.ADJA attributive adjectiveADJ D adverbial adjectiveADV adverbAPPR prepositionART articleCARD cardinal numberFM foreign materialKOKOM comparing conjunctionKOUS sub-ordinating conjunctionNE proper nounN N common ounPlAT indefinite pronounPPER personal pronounPTKNEG negat".~nVAFIN finit,~ auxiliaryVMFIN finite modal verbVVPP past participle of main verbA.2 Phrasal CategoriesAP adjective phraseCS coordination of sentencesDI.
discurse levelMPN multi-word proper nounN M multi-token umericalN P noun phrasePP prepositional phraseS sentenceVP verb phraseA.3 Grammat ica l  FunctionsAC adpositional case markerCJ conjunctDA dativeHD headJU junctorMNR post-nominal modifierMO modifierNG negationNK noun kernelOA accusative objectOC clausal objectPD predicativePG pseudo genitivePNC proper noun componentSB subjectSBP passivized subjectSP subject or predicativeA.1 Part-of-Speech TagsWe use the Stuttgart-Tiibingen-Tagset.
The com-plete set is described in (Thielen and Schiller, 1995).74
