Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 971?981,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsJointly optimizing word representations for lexical and sentential taskswith the C-PHRASE modelNghia The Pham Germ?an Kruszewski Angeliki Lazaridou Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento{thenghia.pham|german.kruszewski|angeliki.lazaridou|marco.baroni}@unitn.itAbstractWe introduce C-PHRASE, a distributionalsemantic model that learns word repre-sentations by optimizing context predic-tion for phrases at all levels in a syntactictree, from single words to full sentences.C-PHRASE outperforms the state-of-the-art C-BOW model on a variety of lexicaltasks.
Moreover, since C-PHRASE wordvectors are induced through a composi-tional learning objective (modeling thecontexts of words combined into phrases),when they are summed, they produce sen-tence representations that rival those gen-erated by ad-hoc compositional models.1 IntroductionDistributional semantic models, that inducevector-based meaning representations from pat-terns of co-occurrence of words in corpora, haveproven very successful at modeling many lexicalrelations, such as synonymy, co-hyponomy andanalogy (Mikolov et al, 2013c; Turney and Pan-tel, 2010).
The recent evaluation of Baroni et al(2014b) suggests that the C-BOW model intro-duced by Mikolov et al (2013a) is, consistently,the best across many tasks.1Interestingly, C-BOW vectors are estimatedwith a simple compositional approach: Theweights of adjacent words are jointly optimized sothat their sum will predict the distribution of theircontexts.
This is reminiscent of how the parame-ters of some compositional distributional seman-1We refer here not only to the results reported inBaroni et al (2014b), but also to the more exten-sive evaluation that Baroni and colleagues present inthe companion website (http://clic.cimec.unitn.it/composes/semantic-vectors.html).
The ex-periments there suggest that only the Glove vectors of Pen-nington et al (2014) are competitive with C-BOW, and onlywhen trained on a corpus several orders of magnitude largerthan the one used for C-BOW.tic models are estimated by optimizing the pre-diction of the contexts in which phrases occur incorpora (Baroni and Zamparelli, 2010; Guevara,2010; Dinu et al, 2013).
However, these compo-sitional approaches assume that word vectors havealready been constructed, and contextual evidenceis only used to induce optimal combination rulesto derive representations of phrases and sentences.In this paper, we follow through on this observa-tion to propose the new C-PHRASE model.
Sim-ilarly to C-BOW, C-PHRASE learns word repre-sentations by optimizing their joint context pre-diction.
However, unlike in flat, window-basedC-BOW, C-PHRASE groups words according totheir syntactic structure, and it simultaneously op-timizes context-predictions at different levels ofthe syntactic hierarchy.
For example, given train-ing sentence ?A sad dog is howling in the park?,C-PHRASE will optimize context prediction fordog, sad dog, a sad dog, a sad dog is howling,etc., but not, for example, for howling in, as thesetwo words do not form a syntactic constituent bythemselves.C-PHRASE word representations outperformC-BOW on several word-level benchmarks.
In ad-dition, because they are estimated in a composi-tional way, C-PHRASE word vectors, when com-bined through simple addition, produce sentencerepresentations that are better than those obtainedwhen adding other kinds of vectors, and competi-tive against ad-hoc compositional methods on var-ious sentence meaning benchmarks.2 The C-PHRASE modelWe start with a brief overview of the models pro-posed by Mikolov et al (2013a), as C-PHRASEbuilds on them.
The Skip-gram model derivesthe vector of a target word by setting its weightsto predict the words surrounding it in the corpus.971More specifically, the objective function is:1TT?t=1?
?c?j?c,j 6=0log p(wt+j|wt) (1)where the word sequence w1, w2, ..., wTis thetraining corpus and c is the size of the windowaround the target word wt, consisting of the con-text words wt+jthat must be predicted by the in-duced vector representation for the target.While Skip-gram learns each word represen-tation separately, the C-BOW model takes theircombination into account.
More precisely, it triesto predict a context word from the combination ofthe previous and following words, where the com-bination method is vector addition.
The objectivefunction is:1TT?t=1log p(wt|wt?c..wt?1, wt+1..wt+c) (2)While other distributional models consider se-quences of words jointly as context when estimat-ing the parameters for a single word (Agirre et al,2009; Melamud et al, 2014), C-BOW is uniquein that it estimates the weights of a sequence ofwords jointly, based on their shared context.
Inthis respect, C-BOW extends the distributional hy-pothesis (Harris, 1954) that words with similarcontext distributions should have similar meaningsto longer sequences.
However, the word combi-nations of C-BOW are not natural linguistic con-stituents, but arbitrary n-grams (e.g., sequences of5 words with a gap in the middle).
Moreover, themodel does not attempt to capture the hierarchicalnature of syntactic phrasing, such that big browndog is a meaningful phrase, but so are its childrenbrown dog and dog.C-PHRASE aims at capturing the same intu-ition that word combinations with similar con-text distributions will have similar meaning, butit applies it to syntactically motivated, potentiallynested phrases.
More precisely, we estimate wordvectors such that they and their summed combi-nations are able to predict the contexts of words,phrases and sentences.
The model is formalizedas follows.
We start from a parsed text corpusT, composed of constituents C[wl, ?
?
?
, wr], wherewl, ?
?
?
, wrare the words spanned by the con-stituent, located in positions l to r in the corpus.We minimize an objective function analogous toequations (1) and (2), but instead of just using in-dividual words or bags of words to predict context,we use summed vector representations of well-formed constituents at all levels in the syntactictree to predict the context of these constituents.There are similarities with both CBOW and Skip-gram.
At the leaf nodes, C-PHRASE acts likeSkip-gram, whereas at higher node in the parsetree, it behaves like CBOW model.
Concretely, wetry to predict the words located within a windowcCfrom every constituent in the parse tree.2In or-der to do so, we learn vector representations forwords vwby maximizing the sum of the log prob-abilities of the words in the context window of thewell-formed constituents with stochastic gradientdescent:?C[wl,???
,wr]?T?1?j?cC(log p(wl?j|C[wl, ?
?
?
, wr])+ log p(wr+j|C[wl, ?
?
?
, wr]))(3)with p theoretically defined as:p(wO|C[wl, ?
?
?
, wr])=exp(v?>wO?ri=lvwir?l+1)?Ww=1exp(v?>w?ri=lvwir?l+1)where W is the size of the vocabulary, v?and vdenote output (context) and input vectors, respec-tively, and we take the input vectors to representthe words.
In practice, since the normalizationconstant for the above probability is expensive tocompute, we follow Mikolov et al (2013b) anduse negative sampling.We let the context window size cCvary as afunction of the height of the constituent in thesyntactic tree.
The height h(C) of a constituentis given by the maximum number of intermedi-ate nodes separating it from any of the words itdominates (such that h = 0 for words, h = 1 fortwo-word phrases, etc.).
Then, for a constituentof height h(C), C-PHRASE considers cC= c1+h(C)c2context words to its left and right (the non-negative integers c1and c2are hyperparameters ofthe model; with c2= 0, context becomes constant2Although here we only use single words as context,the latter can be extended to encompass any sensible lin-guistic item, e.g., frequent n-grams or, as discussed below,syntactically-mediated expressions972Figure 1: C-PHRASE context prediction objec-tive for the phrase small cat and its children.
Thephrase vector is obtained by summing the wordvectors.
The predicted window is wider for thehigher constituent (the phrase).across heights).
The intuition for enlarging thewindow proportionally to height is that, for shorterphrases, narrower contexts are likely to be most in-formative (e.g., a modifying adjective for a noun),whereas for longer phrases and sentences it mightbe better to focus on broader ?topical?
informationspread across larger windows (paragraphs contain-ing sentences about weather might also contain thewords rain and sun, but without any tendency forthese words to be perfectly adjacent to the targetsentences).Figure 1 illustrates the prediction objective fora two-word phrase and its children.
Since allconstituents (except the topmost) form parts oflarger constituents, their representations will belearned both from the objective of predicting theirown contexts, and from error propagation from thesame objective applied higher in the tree.
As a sideeffect, words, being lower in the syntactic tree,will have their vectors updated more often, andthus might have a greater impact on the learned pa-rameters.
This is another reason for varying win-dow size with height, so that the latter effect willbe counter-balanced by higher constituents havinglarger context windows to predict.For lexical tasks, we directly use the vectorsinduced by C-PHRASE as word representations.For sentential tasks, we simply add the vectors ofthe words in a sentence to obtain its representation,exploiting the fact that C-PHRASE was trained topredict phrase contexts from the additive combi-nation of their elements.Joint optimization of word and phrase vectorsThe C-PHRASE hierarchical learning objectivecan capture, in parallel, generalizations about thecontexts of words and phrases at different levelsof complexity.
This results, as we will see, in bet-ter word vectors, presumably because C-PHRASEis trained to predict how the contexts of a wordchange based on its phrasal collocates (cup willhave very different contexts in world cup vs. cof-fee cup ).
At the same time, because the vectors areoptimized based on their occurrence in phrases ofdifferent syntactic complexity, they produce goodsentence representations when they are combined.To the best of our knowledge, C-PHRASE is thefirst model that is jointly optimized for lexical andcompositional tasks.
C-BOW uses shallow com-position information to learn word vectors.
Con-versely, some compositional models ?e.g., Kalch-brenner et al (2014), Socher et al (2013)?
in-duce word representations, that are only optimizedfor a compositional task and are not tested at thelexical level.
Somewhat relatedly to what wedo, Hill et al (2014) evaluated representationslearned in a sentence translation task on word-level benchmarks.
Some a priori justification fortreating word and sentence learning as joint prob-lems comes from human language acquisition, asit is obvious that children learn word and phrasemeanings in parallel and interactively, not sequen-tially (Tomasello, 2003).Knowledge-leanness and simplicity For train-ing, C-PHRASE requires a large, syntactically-parsed corpus (more precisely, it only requires theconstituent structure assigned by the parser, as itis blind to syntactic labels).
Both large unan-notated corpora and efficient pre-trained parsersare available for many languages, making the C-PHRASE knowledge demands feasible for practi-cal purposes.
There is no need to parse the sen-tences we want to build representations for at testtime, since the component word vectors are sim-ply added.
The only parameters of the model arethe word vectors; specifically, no extra parametersare needed for composition (composition modelssuch as the one presented in Socher et al (2012)require an extra parameter matrix for each wordin the vocabulary, and even leaner models such asthe one of Guevara (2010) must estimate a param-eter matrix for each composition rule in the gram-mar).
This makes C-PHRASE as simple as addi-tive and multiplicative composition (Mitchell and973Lapata, 2010),3but C-PHRASE is both more ef-fective in compositional tasks (see evaluation be-low), and it has the further advantage that it learnsits own word vectors, thus reducing the number ofarbitrary choices to be made in modeling.Supervision Unlike many recent compositionmodels (Kalchbrenner and Blunsom, 2013; Kalch-brenner et al, 2014; Socher et al, 2012; Socheret al, 2013, among others), the context-predictionobjective of C-PHRASE does not require anno-tated data, and it is meant to provide general-purpose representations that can serve in differ-ent tasks.
C-PHRASE vectors can also be usedas initialization parameters for fully supervised,task-specific systems.
Alternatively, the currentunsupervised objective could be combined withtask-specific supervised objectives to fine-tune C-PHRASE to specific purposes.Sensitivity to syntactic structure During train-ing, C-PHRASE is sensitive to syntactic structure.To cite an extreme example, boy flowers will bejoined in a context-predicting phrase in ?these areconsidered [boy flowers]?, but not in ?he gave[the boy] [flowers]?.
A more common case isthat of determiners, that will only occur in phrasesthat also contain the following word, but not nec-essarily the preceding one.
Sentence composi-tion at test time, on the other hand, is additive,and thus syntax-insensitive.
Still, the vectors be-ing combined will reflect syntactic generalizationslearned in training.
Even if C-PHRASE producesthe same representation for red+car and car+red,this representation combines a red vector that, dur-ing training, has often occurred in the modifierposition of adjective-noun phrases, whereas carwill have often occurred in the corresponding headposition.
So, presumably, the red+car=car+redvector will encode the adjective-noun asymmetryinduced in learning.
While the model won?t beable to distinguish the rare cases in which car redis genuinely used as a phrase, in realistic scenar-ios this won?t be a problem, because only red carwill be encountered.
In this respect, the successesand failures of C-PHRASE can tell us to what ex-tent word order information is truly distinctive inpractice, and to what extent it can instead be re-constructed from the typical role that words playin sentences.3We do not report results for component-wise multiplica-tive in our evaluation because it performed much worse thanaddition in all the tasks.Comparison with traditional syntax-sensitiveword representations Syntax has often beenexploited in distributional semantics for a richercharacterization of context.
By relying on a syn-tactic parse of the input corpus, a distributionalmodel can take more informative contexts suchas subject-of-eat vs. object-of-eat into account(Baroni and Lenci, 2010; Curran and Moens,2002; Grefenstette, 1994; Erk and Pad?o, 2008;Levy and Goldberg, 2014a; Pad?o and Lapata,2007; Rothenh?ausler and Sch?utze, 2009).
In thisapproach, syntactic information serves to selectand/or enrich the contexts that are used to buildrepresentations of target units.
On the other hand,we use syntax to determine the target units thatwe build representations for (in the sense that wejointly learn representations of their constituents).The focus is thus on unrelated aspects of model in-duction, and we could indeed use syntax-mediatedcontexts together with our phrasing strategy.
Cur-rently, given eat (red apples), we treat eat aswindow-based context of red apples, but we couldalso take the context to be object-of-eat.3 Evaluation3.1 Data setsSemantic relatedness of words In this classiclexical task, the models are required to quantifythe degree of semantic similarity or relatedness ofpairs of words in terms of cosines between the cor-responding vectors.
These scores are then com-pared to human gold standards.
Performance is as-sessed by computing the correlation between sys-tem and human scores (Spearman correlation inall tasks except rg, where it is customary to re-port Pearson).
We used, first of all, the MEN(men) data set of Bruni et al (2014), that is splitinto 1K pairs for training/development, and 1Kpairs for testing.
We used the training set to tunethe hyperparameters of our model, and report per-formance on the test set.
The C-BOW modelof Baroni et al (2014b) achieved state-of-the artperformance on MEN test.
We also evaluate onthe widely used WordSim353 set introduced byFinkelstein et al (2002), which consists of 353word pairs.
The WordSim353 data were split byAgirre et al (2009) into similarity (wss) and re-latedness (wsr) subsets, focusing on strictly taxo-nomic (television/radio) vs. broader topical cases(Maradona/football), respectively.
State-of-the-art performance on both sets is reported by Baroni974et al (2014b), with the C-BOW model.
We fur-ther consider the classic data set of Rubenstein andGoodenough (1965) (rg), consisting of 65 nounpairs.
We report the state-of-the-art from Hassanand Mihalcea (2011), which exploited Wikipedia?slinking structure.Concept categorization Systems are asked togroup a set of nominal concepts into broader cate-gories (e.g.
arthritis and anthrax into illness; ba-nana and grape into fruit).
As in previous work,we treat this as an unsupervised clustering task.We feed the similarity matrix produced by a modelfor all concepts in a test set to the CLUTO toolkit(Karypis, 2003), that clusters them into n groups,where n is the number of categories.
We usestandard CLUTO parameters from the literature,and quantify performance by cluster purity withrespect to the gold categories.
The Almuhareb-Poesio benchmark (Almuhareb, 2006) (ap) con-sists of 402 concepts belonging to 21 categories.A distributional model based on carefully chosensyntactic relations achieved top ap performance(Rothenh?ausler and Sch?utze, 2009).
The ESSLLI2008 data set (Baroni et al, 2008) (esslli) consistsof 6 categories and 42 concepts.
State of the artwas achieved by Katrenko and Adriaans (2008) byusing full-Web queries and manually crafted pat-terns.Semantic analogy The last lexical task we pickis analogy (an), introduced in Mikolov et al(2013c).
We focus on their semantic challenge,containing about 9K questions.
In each question,the system is given a pair exemplifying a relation(man/king) and a test word (woman); it is thenasked to find the word (queen) that instantiates thesame relation with the test word as that of the ex-ample pair.
Mikolov et al (2013c) subtract thevector of the first word in a pair from the sec-ond, add the vector of the test word and look forthe nearest neighbor of the resulting vector (e.g.,find the word whose vector is closest to king -man + woman).
We follow the method introducedby Levy and Goldberg (2014b), which returns theword x maximizingcos(king,x)?cos(woman,x)cos(man,x).
Thismethod yields better results for all models.
Per-formance is measured by accuracy in retrievingthe correct answer (in our search space of 180Kwords).
The current state of the art on the seman-tic part and on the whole data set was reached byPennington et al (2014), who trained their wordrepresentations on a huge corpus consisting of 42Bwords.Sentential semantic relatedness Similarly toword relatedness, composed sentence representa-tions can be evaluated against benchmarks wherehumans provided relatedness/similarity scores forsentence pairs (sentences with high scores, such as?A person in a black jacket is doing tricks on a mo-torbike?/?A man in a black jacket is doing trickson a motorbike?
from the SICK data-set, tend tobe near-paraphrases).
Following previous work onthese data sets, Pearson correlation is our figureof merit, and we report it between human scoresand sentence vector cosine similarities computedby the models.
SICK (Marelli et al, 2014) (sick-r)was created specifically for the purpose of evalu-ating compositional models, focusing on linguisticphenomena such as lexical variation and word or-der.
Here we report performance of the systemson the test part of the data set, which contains 5Ksentence pairs.
The top performance (from theSICK SemEval shared task) was reached by Zhaoet al (2014) using a heterogeneous set of featuresthat include WordNet and extra training corpora.Agirre et al (2012) and Agirre et al (2013) cre-ated two collections of sentential similarities con-sisting of subsets coming from different sources.From these, we pick the Microsoft Research videodescription dataset (msrvid), where near para-phrases are descriptions of the same short video,and the OnWN 2012 (onwn1) and OnWN 2013(onwn2) data sets (each of these sets contains 750pairs).
The latter are quite different from othersentence relatedness benchmarks, since they com-pare definitions for the same or different wordstaken from WordNet and OntoNotes: these glossesoften are syntactic fragments (?cause somethingto pass or lead somewhere?
), rather than full sen-tences.
We report top performance on these tasksfrom the respective shared challenges, as sum-marized by Agirre et al (2012) and Agirre et al(2013).
Again, the top systems use feature-rich,supervised methods relying on distributional sim-ilarity as well as other sources, such as WordNetand named entity recognizers.Sentential entailment Detecting the presenceof entailment between sentences or longer pas-sages is one of the most useful features that thecomputational analysis of text could provide (Da-gan et al, 2009).
We test our model on the SICK975entailment task (sick-e) (Marelli et al, 2014).All SICK sentence pairs are labeled as ENTAIL-ING (?Two teams are competing in a footballmatch?/?Two groups of people are playing foot-ball?
), CONTRADICTING (?The brown horse isnear a red barrel at the rodeo?/?The brown horseis far from a red barrel at the rodeo?)
or NEU-TRAL (?A man in a black jacket is doing tricks ona motorbike?/?A person is riding the bicycle onone wheel?).
For each model, we train a simpleSVM classifier based on 2 features: cosine simi-larity between the two sentence vectors, as givenby the models, and whether the sentence pair con-tains a negation word (the latter has been shownto be a very informative feature for SICK entail-ment).
The current state-of-the-art is reached byLai and Hockenmaier (2014), using a much richerset of features, that include WordNet, the denota-tion graph of Young et al (2014) and extra trainingdata from other resources.Sentiment analysis Finally, as sentiment analy-sis has emerged as a popular area of applicationfor compositional models, we test our methods onthe Stanford Sentiment Treebank (Socher et al,2013) (sst), consisting of 11,855 sentences frommovie reviews, using the coarse annotation into2 sentiment degrees (negative/positive).
We fol-low the official split into train (8,544), develop-ment (1,101) and test (2,210) parts.
We train anSVM classifier on the training set, using the sen-tence vectors composed by a model as features,and report accuracy on the test set.
State of theart is obtained by Le and Mikolov (2014) with theParagraph Vector approach we describe below.3.2 Model implementationThe source corpus we use to build the lex-ical vectors is created by concatenating threesources: ukWaC,4a mid-2009 dump of the En-glish Wikipedia5and the British National Corpus6(about 2.8B words in total).
We build vectors forthe 180K words occurring at least 100 times inthe corpus.
Since our training procedure requiresparsed trees, we parse the corpus using the Stan-ford parser (Klein and Manning, 2003).C-PHRASE has two hyperparameters (see Sec-tion 2 above), namely basic window size (c1) andheight-dependent window enlargement factor (c2).4http://wacky.sslmit.unibo.it5http://en.wikipedia.org6http://www.natcorp.ox.ac.ukMoreover, following Mikolov et al (2013b), dur-ing training we sub-sample less informative, veryfrequent words: this option is controlled by a pa-rameter t, resulting in aggressive subsampling ofwords with relative frequency above it.
We tuneon MEN-train, obtaining c1= 5, c2= 2 andt = 10?5.
As already mentioned, sentence vec-tors are built by summing the vectors of the wordsin them.In lexical tasks, we compare our model to thebest C-BOW model from Baroni et al (2014b),7and to a Skip-gram model built using the same hy-perparameters as C-PHRASE (that also led to thebest MEN-train results for Skip-gram).In sentential tasks, we compare our modelagainst adding the best C-BOW vectors pre-trained by Baroni and colleagues,8and adding ourSkip-gram vectors.
We compare the additive ap-proaches to two sophisticated composition mod-els.
The first is the Practical Lexical Function(PLF) model of Paperno et al (2014).
This is alinguistically motivated model in the tradition ofthe ?functional composition?
approaches of Co-ecke et al (2010) and Baroni et al (2014a), andthe only model in this line of research that hasbeen shown to empirically scale up to real-life sen-tence challenges.
In short, in the PLF model allwords are represented by vectors.
Words actingas argument-taking functions (such as verbs or ad-jectives) are also associated to one matrix for eachargument they take (e.g., each transitive verb hasa subject and an object matrix).
Vector represen-tations of arguments are recursively multiplied byfunction matrices, following the syntactic tree upto the top node.
The final sentence representa-tion is obtained by summing all the resulting vec-tors.
The PLF approach requires syntactic parsingboth in training and in testing and, more cumber-somely, to train a separate matrix for each argu-ment slot of each function word (the training ob-jective is again a context-predicting one).
Here,we report PLF results on msrvid and onwn2 fromPaperno et al (2014), noting that they also usedtwo simple but precious cues (word overlap andsentence length) we do not adopt here.
We usedtheir pre-trained vectors and matrices also for theSICK challenges, while the number of new ma-7For fairness, we report their results when all tasks wereevaluated with the same set of parameters, tuned on rg: thisis row 8 of their Table 2.8http://clic.cimec.unitn.it/composes/semantic-vectors.html976trices to estimate made it too time-consuming toimplement this model in the onwn1 and sst tasks.Finally, we test the Paragraph Vector (PV) ap-proach recently proposed by Le and Mikolov(2014).
Under PV, sentence representations arelearned by predicting the words that occur in them.This unsupervised method has been shown bythe authors to outperform much more sophisti-cated, supervised neural-network-based composi-tion models on the sst task.
We use our own imple-mentation for this approach.
Unlike in the originalexperiments, we found the PV-DBOW variant ofPV to consistently outperform PV-DM, and so wereport results obtained with the former.Note that PV-DBOW aims mainly at providingrepresentations for sentences, not words.
Whenwe do not need to induce vectors for sentencesin the training corpus, i.e., only train to learnsingle words?
representations and the softmaxweights, PV-DBOW essentially reduces to Skip-gram.
Therefore, we produce the PV-DBOW vec-tors for the sentences in the evaluation data setsusing the softmax weights learned by Skip-gram.However, it is not clear that, if we were to train PV-DBOW jointly for words and sentences, we wouldget word vectors as good as those that Skip-graminduces.4 ResultsThe results on the lexical tasks reported in Table 1prove that C-PHRASE is providing excellent wordrepresentations, (nearly) as good or better than theC-BOW vectors of Baroni and colleagues in allcases, except for ap.
Whenever C-PHRASE is notclose to the state of the art results, the latter reliedon richer knowledge sources and/or much largercorpora (ap, esslli, an).Turning to the sentential tasks (Table 2), we firstremark that using high-quality word vectors (suchas C-BOW) and summing them leads to good re-sults in all tasks, competitive with those obtainedwith more sophisticated composition models.
Thisconfirms the observation made by Blacoe and La-pata (2012) that simple-minded composition mod-els are not necessarily worse than advanced ap-proaches.
Still, C-PHRASE is consistently betterthan C-BOW in all tasks, except sst, where the twomodels reach the same performance level.C-PHRASE is outperforming PV on all tasksexcept sick-e, where the two models have the sameperformance, and onwn2, where PV is slightlybetter.
C-PHRASE is outperforming PLF by alarge margin on the SICK sets, whereas the twomodels are equal on msrvid, and PLF better ononwn2.
Recall, however, that on the latter twobenchmarks PLF used extra word overlap and sen-tence length features, so the comparison is not en-tirely fair.The fact that state-of-the-art performance iswell above our models is not surprising, since theSOA systems are invariably based on a wealthof knowledge sources, and highly optimized fora task.
To put some of our results in a broaderperspective, C-PHRASE?s sick-r performance is1% better than the median result of systems thatparticipated in the SICK SemEval challenge, andcomparable to that of Beltagy et al (2014), whoentered the competition with a system combiningdistributional semantics with a supervised proba-bilistic soft logic system.
For sick-e (the entail-ment task), C-PHRASE?s performance is less thanone point below the median of the SemEval sys-tems, and slightly above that of the Stanford sub-mission, that used a recursive neural network witha tensor layer.Finally, the performance of all our models, in-cluding PV, on sst is remarkably lower than thestate-of-the-art performance of PV as reported byLe and Mikolov (2014).
We believe that thecrucial difference is that these authors estimatedPV vectors specifically on the sentiment treebanktraining data, thus building ad-hoc vectors encod-ing the semantics of movie reviews.
We leave itto further research to ascertain whether we couldbetter fine-tune our models to sst by including thesentiment treebank training phrases in our sourcecorpus.Comparing vector lengths of C-BOW and C-PHRASE We gather some insight into how theC-PHRASE objective might adjust word represen-tations for composition with respect to C-BOW bylooking at how the length of word vectors changesacross the two models.9While this is a very coarsemeasure, if a word vector is much longer/shorter(relative to the length of other word vectors of thesame model) for C-PHRASE vs. C-BOW, it meansthat, when sentences are composed by addition,the effect of the word on the resulting sentencerepresentation will be stronger/weaker.9We performed the same analysis for C-PHRASE andSkip-gram, finding similar general trends to the ones we re-port for C-PHRASE and C-BOW.977men wss wsr rg ap esslli anSkip-gram 78 77 66 80 65 82 63C-BOW 80 78 68 83 71 77 68C-PHRASE 79 79 70 83 65 84 69SOA 80 80 70 86 79 91 82Table 1: Lexical task performance.
See Section 3.1 for figures of merit (all in percentage form) andstate-of-the-art references.
C-BOW results (tuned on rg) are taken from Baroni et al 2014b.sick-r sick-e msrvid onwn1 onwn2 sstSkip-gram 70 72 74 66 62 78C-BOW 70 74 74 69 63 79C-PHRASE 72 75 79 70 65 79PLF 57 72 79 NA 67 NAPV 67 75 77 66 66 77SOA 83 85 88 71 75 88Table 2: Sentential task performance.
See Section 3.1 for figures of merit (all in percentage form) andstate-of-the-art references.
The PLF results on msrvid and onwn2 are taken from Paperno et al 2014.The relative-length-difference test returns thefollowing words as the ones that are most severelyde-emphasized by C-PHRASE compared to C-BOW: be, that, an, not, they, he, who, when,well, have.
Clearly, C-PHRASE is weightingdown grammatical terms that tend to be context-agnostic, and will be accompanied, in phrases, bymore context-informative content words.
Indeed,the list of terms that are instead emphasized byC-PHRASE include such content-rich, monose-mous words as gnawing, smackdown, demograph-ics.
This is confirmed by a POS-level analysisthat indicates that the categories that are, on av-erage, most de-emphasized by C-PHRASE are:determiners, modals, pronouns, prepositions and(more surprisingly) proper nouns.
The ones thatare, in relative terms, more emphasized are: -ingverb forms, plural and singular nouns, adjectivesand their superlatives.
While this reliance on con-tent words to the detriment of grammatical termsis not always good for sentential tasks (?not al-ways good?
means something very different from?always good?!
), the convincing comparative per-formance of C-PHRASE in such tasks suggeststhat the semantic effect of grammatical terms isin any case beyond the scope of current corpus-based models, and often not crucial to attain com-petitive results on typical benchmarks (think, e.g.,of how little modals, one of the categories that C-PHRASE downplays the most, will matter whendetecting paraphrases that are based on picture de-scriptions).We also applied the length difference test towords in specific categories, finding similar pat-terns.
For example, looking at -ly adverbs only,those that are de-emphasized the most by C-PHRASE are recently, eventually, originally, no-tably and currently ?
all adverbs denoting tempo-ral factors or speaker attitude.
On the other hand,the ones that C-PHRASE lengthens the most, rel-ative to C-BOW, are clinically, divinely, ecolog-ically, noisily and theatrically: all adverbs withmore specific, content-word-like meanings, thatare better captured by distributional methods, andare likely to have a bigger impact on tasks such asparaphrasing or entailment.Effects of joint optimization at word andphrase levels As we have argued before, C-PHRASE is able to obtain good word representa-tions presumably because it learns to predict howthe context of a word changes in the presence ofdifferent collocates.
To gain further insight intothis claim, we looked at the nearest neighboursof some example terms, like neural, network andneural network (the latter, composed by addition)both in C-PHRASE and C-BOW.
The results forthis particular example can be appreciated in Ta-ble 3.Interestingly, while for C-BOW we observesome confusion between the meaning of the indi-vidual words and the phrase, C-PHRASE seemsto provide more orthogonal representations forthe lexical items.
For example, neural in C-978C-BOW C-PHRASEneural network neural network neural network neural networkneuronal networks network neuronal networks networkneurons superjanet4 neural cortical internetwork neuralhopfield backhaul networks connectionist wans perceptroncortical fiber-optic hopfield neurophysiological network.
networksconnectionist point-to-multipoint packet-switched sensorimotor multicasting hebbianfeed-forward nsfnet small-world sensorimotor nsfnet neuronsfeedforward multi-service local-area neocortex networking neocortexneuron circuit-switched superjanet4 electrophysiological tymnet connectionistbackpropagation wide-area neuronal neurobiological x.25 neuronalTable 3: Nearest neighbours of neural, network and neural network both for C-BOW and C-PHRASEBOW contains neighbours that fit well with neu-ral network, like hopfield, connectionist and feed-forward.
Conversely, neural network has neigh-bours that correspond to network like local-areaand packet-switched.
In contrast, C-PHRASEneighbours for neural are mostly related to thebrain sense of the word, e.g., cortical, neurophys-iological, etc.
(with the only exception of connec-tionist).
The first neighbour of neural network, ex-cluding its own component words, quite sensibly,is perceptron.5 ConclusionWe introduced C-PHRASE, a distributional se-mantic model that is trained on the task of pre-dicting the contexts surrounding phrases at all lev-els of a hierarchical sentence parse, from singlewords to full sentences.
Consequently, word vec-tors are induced by taking into account not onlytheir contexts, but also how co-occurrence withother words within a syntactic constituent is af-fecting these contexts.C-PHRASE vectors outperform state-of-the-artC-BOW vectors in a wide range of lexical tasks.Moreover, because of the way they are induced,when C-PHRASE vectors are summed, they pro-duce sentence representations that are as good orbetter than those obtained with sophisticated com-position methods.C-PHRASE is a very parsimonious approach:The only major resource required, comparedto a completely knowledge-free, unsupervisedmethod, is an automated parse of the training cor-pus (but no syntactic labels are required, nor pars-ing at test time).
C-PHRASE has only 3 hyperpa-rameters and no composition-specific parameter totune and store.Having established a strong empirical baselinewith this parsimonious approach, in future re-search we want to investigate the impact of possi-ble extensions on both lexical and sentential tasks.When combining the vectors, either for inductionor composition, we will try replacing plain addi-tion with other operations, starting with somethingas simple as learning scalar weights for differentwords in a phrase (Mitchell and Lapata, 2010).We also intend to explore more systematic waysto incorporate supervised signals into learning, tofine-tune C-PHRASE vectors to specific tasks.On the testing side, we are fascinated by thegood performance of additive models, that (at testtime, at least) do not take word order nor syntacticstructure into account.
We plan to perform a sys-tematic analysis of both existing benchmarks andnatural corpus data, both to assess the actual im-pact that such factors have on the aspects of mean-ing we are interested in (take two sentences in anentailment relation: how often does shuffling thewords in them make it impossible to detect entail-ment?
), and to construct new benchmarks that aremore challenging for additive methods.The C-PHRASE vectors described in this paperare made publicly available at: http://clic.cimec.unitn.it/composes/.AcknowledgmentsWe thank Gemma Boleda and the anonymous re-viewers for useful comments.
We acknowledgeERC 2011 Starting Independent Research Grantn.
283554 (COMPOSES).ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceed-ings of HLT-NAACL, pages 19?27, Boulder, CO.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: a979pilot on semantic textual similarity.
In Proceedingsof *SEM, pages 385?393, Montreal, Canada.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic Textual Similarity.
In Proceedingsof *SEM, pages 32?43, Atlanta, GA.Abdulrahman Almuhareb.
2006.
Attributes in LexicalAcquisition.
Phd thesis, University of Essex.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional Memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-itors.
2008.
Bridging the Gap between SemanticTheory and Computational Simulations: Proceed-ings of the ESSLLI Workshop on Distributional Lex-ical Semantic.
FOLLI, Hamburg.Marco Baroni, Raffaella Bernardi, and Roberto Zam-parelli.
2014a.
Frege in space: A program forcompositional distributional semantics.
LinguisticIssues in Language Technology, 9(6):5?110.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014b.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof ACL, pages 238?247, Baltimore, MD.Islam Beltagy, Stephen Roller, Gemma Boleda, KatrinErk, and Raymond Mooney.
2014.
UTexas: Nat-ural language semantics using distributional seman-tics and probabilistic logic.
In Proceedings of Se-mEval, pages 796?801, Dublin, Ireland, August.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP, pages546?556, Jeju Island, Korea.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.James Curran and Marc Moens.
2002.
Improvementsin automatic thesaurus extraction.
In Proceedings ofthe ACL Workshop on Unsupervised Lexical Acqui-sition, pages 59?66, Philadelphia, PA.Ido Dagan, Bill Dolan, Bernardo Magnini, and DanRoth.
2009.
Recognizing textual entailment: ratio-nale, evaluation and approaches.
Natural LanguageEngineering, 15:459?476.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
General estimation and evaluation of com-positional distributional semantic models.
In Pro-ceedings of ACL Workshop on Continuous VectorSpace Models and their Compositionality, pages 50?58, Sofia, Bulgaria.Katrin Erk and Sebastian Pad?o.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP, pages 897?906, Honolulu,HI.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer, Boston, MA.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the EMNLP GEMSWorkshop, pages 33?37, Uppsala, Sweden.Zellig Harris.
1954.
Distributional structure.
Word,10(2-3):1456?1162.Samer Hassan and Rada Mihalcea.
2011.
Semanticrelatedness using salient semantic analysis.
In Pro-ceedings of AAAI, pages 884?889, San Francisco,CA.Felix Hill, KyungHyun Cho, S?ebastien Jean, Col-ine Devin, and Yoshua Bengio.
2014.
Notall neural embeddings are born equal.
InProceedings of the NIPS Learning SemanticsWorkshop, Montreal, Canada.
Published on-line: https://sites.google.com/site/learningsemantics2014/.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentconvolutional neural networks for discourse com-positionality.
In Proceedings of ACL Workshop onContinuous Vector Space Models and their Compo-sitionality, pages 119?126, Sofia, Bulgaria.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of ACL, pages655?665, Baltimore, MD.George Karypis.
2003.
CLUTO: A clustering toolkit.Technical Report 02-017, University of MinnesotaDepartment of Computer Science.Sophia Katrenko and Pieter Adriaans.
2008.
Qualiastructures and their impact on the concrete nouncategorization task.
In Proceedings of the ESS-LLI Workshop on Distributional Lexical Semantics,pages 17?24, Hamburg, Germany.980Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL, pages423?430, Sapporo, Japan.Alice Lai and Julia Hockenmaier.
2014.
Illinois-lh: Adenotational and distributional approach to seman-tics.
In Proceedings of the 8th International Work-shop on Semantic Evaluation (SemEval 2014), pages329?334, Dublin, Ireland, August.
Association forComputational Linguistics and Dublin City Univer-sity.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
arXivpreprint arXiv:1405.4053.Omer Levy and Yoav Goldberg.
2014a.
Dependency-based word embeddings.
In Proceedings of ACL(Volume 2: Short Papers), pages 302?308, Balti-more, Maryland.Omer Levy and Yoav Goldberg.
2014b.
Linguistic reg-ularities in sparse and explicit word representations.In Proceedings of CoNLL, pages 171?180, Ann Ar-bor, MI.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014.
SemEval-2014 Task 1: Evaluationof compositional distributional semantic models onfull sentences through semantic relatedness and tex-tual entailment.
In Proceedings of SemEval, pages1?8, Dublin, Ireland.Oren Melamud, Ido Dagan, Jacob Goldberger, IdanSzpektor, and Deniz Yuret.
2014.
Probabilisticmodeling of joint-context in distributional similar-ity.
In Proceedings of CoNLL, pages 181?190, AnnArbor, MI.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
http://arxiv.org/abs/1301.3781/.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119, LakeTahoe, NV.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL,pages 746?751, Atlanta, Georgia.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Denis Paperno, Nghia The Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.
InProceedings of ACL, pages 90?99, Baltimore, MD.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of EMNLP, pages1532?1543, Doha, Qatar.Klaus Rothenh?ausler and Hinrich Sch?utze.
2009.Unsupervised classification with dependency basedword spaces.
In Proceedings of the EACL GEMSWorkshop, pages 17?24, Athens, Greece.Herbert Rubenstein and John Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Proceedings of EMNLP, pages 1631?1642,Seattle, WA.Michael Tomasello.
2003.
Constructing a Language:A Usage-Based Theory of Language Acquisition.Harvard University Press, Cambridge, MA.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Young, Alice Lai, Micah Hodosh, and JuliaHockenmaier.
2014.
From image descriptions tovisual denotations: New similarity metrics for se-mantic inference over event descriptions.
Transac-tions of the Association for Computational Linguis-tics, 2:67?78.Jiang Zhao, Tiantian Zhu, and Man Lan.
2014.
Ecnu:One stone two birds: Ensemble of heterogenousmeasures for semantic relatedness and textual entail-ment.
In Proceedings of the 8th International Work-shop on Semantic Evaluation (SemEval 2014), pages271?277, Dublin, Ireland, August.
Association forComputational Linguistics and Dublin City Univer-sity.981
