Evaluation tool for rule-based anaphora resolution methodsCatalina BarbuSchool of Humanities, Languagesand Social SciencesUniversity of WolverhamptonStafford StreetWolverhampton WV1 1SBUnited Kingdomc.barbu@wlv.ac.ukRuslan MitkovSchool of Humanities, Languagesand Social SciencesUniversity of WolverhamptonStafford StreetWolverhampton WV1 1SBUnited Kingdomr.mitkov@wlv.ac.ukAbstractIn this paper we argue that comparativeevaluation in anaphora resolutionhas to be performed using the samepre-processing tools and on the sameset of data.
The paper proposes anevaluation environment for comparinganaphora resolution algorithms whichis illustrated by presenting the resultsof the comparative evaluation ofthree methods on the basis of severalevaluation measures.1 IntroductionThe evaluation of any NLP algorithm or systemshould indicate not only its efficiency orperformance, but should also help us discoverwhat a new approach brings to the current stateof play in the field.
To this end, a comparativeevaluation with other well-known or similarapproaches would be highly desirable.We have already voiced concern (Mitkov,1998a), (Mitkov, 2000b) that the evaluation ofanaphora resolution algorithms and systems isbereft of any common ground for comparison duenot only to the difference of the evaluation data,but also due to the diversity of pre-processingtools employed by each anaphora resolutionsystem.
The evaluation picture would notbe accurate even if we compared anaphoraresolution systems on the basis of the same datasince the pre-processing errors which wouldbe carried over to the systems?
outputs mightvary.
As a way forward we have proposedthe idea of the evaluation workbench (Mitkov,2000b) - an open-ended architecture whichallows the incorporation of different algorithmsand their comparison on the basis of the samepre-processing tools and the same data.
Ourpaper discusses a particular configuration of thisnew evaluation environment incorporating threeapproaches sharing a common ?knowledge-poorphilosophy?
: Kennedy and Boguraev?s (1996)parser-free algorithm, Baldwin?s (1997) CogNiacand Mitkov?s (1998b) knowledge-poor approach.2 The evaluation workbench foranaphora resolutionIn order to secure a ?fair?, consistent andaccurate evaluation environment, and toaddress the problems identified above, wehave developed an evaluation workbench foranaphora resolution which allows the comparisonof anaphora resolution approaches sharingcommon principles (e.g.
similar pre-processingor resolution strategy).
The workbench enablesthe ?plugging in?
and testing of anaphoraresolution algorithms on the basis of the samepre-processing tools and data.
This developmentis a time-consuming task, given that we have tore-implement most of the algorithms, but it isexpected to achieve a clearer assessment of theadvantages and disadvantages of the differentapproaches.
Developing our own evaluationenvironment (and even reimplementing someof the key algorithms) also alleviates theimpracticalities associated with obtaining thecodes of original programs.Another advantage of the evaluationworkbench is that all approaches incorporatedcan operate either in a fully automatic modeor on human annotated corpora.
We believethat this is a consistent way forward because itwould not be fair to compare the success rate ofan approach which operates on texts which areperfectly analysed by humans, with the successrate of an anaphora resolution system whichhas to process the text at different levels beforeactivating its anaphora resolution algorithm.
Infact, the evaluations of many anaphora resolutionapproaches have focused on the accuracy ofresolution algorithms and have not taken intoconsideration the possible errors which inevitablyoccur in the pre-processing stage.
In the real-world, fully automatic resolution must dealwith a number of hard pre-processing problemssuch as morphological analysis/POS tagging,named entity recognition, unknown wordrecognition, NP extraction, parsing, identificationof pleonastic pronouns, selectional constraints,etc.
Each one of these tasks introduces errors andthus contributes to a drop in the performance ofthe anaphora resolution system.1 As a result, thevast majority of anaphora resolution approachesrely on some kind of pre-editing of the text whichis fed to the resolution algorithm, and some ofthe methods have only been manually simulated.By way of illustration, Hobbs?
naive approach(1976; 1978) was not implemented in its originalversion.
In (Dagan and Itai, 1990; Dagan andItai, 1991; Aone and Bennett, 1995; Kennedyand Boguraev, 1996) pleonastic pronouns areremoved manually2 , whereas in (Mitkov, 1998b;Ferrandez et al, 1997) the outputs of the part-of-speech tagger and the NP extractor/ partial parserare post-edited similarly to Lappin and Leass(1994) where the output of the Slot UnificationGrammar parser is corrected manually.
Finally,Ge at al?s (1998) and Tetrault?s systems (1999)1For instance, the accuracy of tasks such as robustparsing and identification of pleonastic pronouns is far below100% See (Mitkov, 2001) for a detailed discussion.2In addition, Dagan and Itai (1991) undertook additionalpre-editing such as the removal of sentences for which theparser failed to produce a reasonable parse, cases wherethe antecedent was not an NP etc.
; Kennedy and Boguraev(1996) manually removed 30 occurrences of pleonasticpronouns (which could not be recognised by their pleonasticrecogniser) as well as 6 occurrences of it which referred to aVP or prepositional constituent.make use of annotated corpora and thus do notperform any pre-processing.
One of the veryfew systems3 that is fully automatic is MARS,the latest version of Mitkov?s knowledge-poorapproach implemented by Evans.
Recent workon this project has demonstrated that fullyautomatic anaphora resolution is more difficultthan previous work has suggested (Ora?san et al,2000).2.1 Pre-processing toolsParserThe current version of the evaluationworkbench employs one of the high performance?super-taggers?
for English - Conexor?s FDGParser (Tapanainen and Ja?rvinen, 1997).
Thissuper-tagger gives morphological informationand the syntactic roles of words (in most ofthe cases).
It also performs a surface syntacticparsing of the text using dependency links thatshow the head-modifier relations between words.This kind of information is used for extractingcomplex NPs.In the table below the output of the FDG parserrun over the sentence: ?This is an input file.?
isshown.1 This this subj:>2 @SUBJ PRON SG2 is be main:>0 @+FMAINV V3 an an det:>5 @DN> DET SG4 input input attr:>5 @A> N SG5 file file comp:>2 @PCOMPL-S N SG$.$<s>Example 1: FDG output for the text This is aninput file.Noun phrase extractorAlthough FDG does not identify the nounphrases in the text, the dependencies establishedbetween words have played an important role inbuilding a noun phrase extractor.
In the exampleabove, the dependency relations help identifyingthe sequence ?an input file?.
Every noun phraseis associated with some features as identifiedby FDG (number, part of speech, grammaticalfunction) and also the linear position of the verbthat they are arguments of, and the number ofthe sentence they appear in.
The result of the NP3Apart from MUC coreference resolution systems whichoperated in a fully automatic mode.extractor is an XML annotated file.
We chosethis format for several reasons: it is easily read,it allows a unified treatment of the files used fortraining and of those used for evaluation (whichare already annotated in XML format) and it isalso useful if the file submitted for analysis toFDG already contains an XML annotation; inthe latter case, keeping the FDG format togetherwith the previous XML annotation would leadto a more difficult processing of the input file.It also keeps the implementation of the actualworkbench independent of the pre-processingtools, meaning that any shallow parser can beused instead of FDG, as long as its output isconverted to an agreed XML format.An example of the overall output of the pre-processing tools is given below.<P><S><w ID=0 SENT=0 PAR=1 LEMMA="this" DEP="2"GFUN="SUBJ" POS="PRON" NR="SG">This</w><w ID=1SENT=0 PAR=1 LEMMA="be" DEP="0" GFUN="+FMAINV"POS="V"> is </w><COREF ID="ref1"><NP> <w ID=2SENT=0 PAR=1 LEMMA="an" DEP="5" GFUN="DN" POS="DET"NR="SG">an </w> <w ID=3 SENT=0 PAR=1 LEMMA="input"DEP="5" GFUN="A" POS="N" NR="SG">input</w><w ID=4SENT=0 PAR=1 LEMMA="file" DEP="2" GFUN="PCOMPL"POS="N" NR="SG">file</w> </NP></COREF><w ID=5SENT=0 PAR=1 LEMMA="."
POS="PUNCT">.</w> </s><s><COREF ID="ref2" REF="ref1"><NP><w ID=0 SENT=1PAR=1 LEMMA="it" DEP="2" GFUN="SUBJ" POS="PRON"> It</w></NP></COREF> <w ID=1 SENT=1 PAR=1 LEMMA="be"DEP="3" GFUN="+FAUXV" POS="V">is </w><w ID=2 SENT=1PAR=1 LEMMA="use" DEP="0" GFUN="-FMAINV" POS="EN">used</w><w ID=3 SENT=1 PAR=1 LEMMA="for" DEP="3"GFUN="ADVL" POS="PREP">for</w> <NP><w ID=4 SENT=1PAR=1 LEMMA="evaluation" DEP="4" GFUN="PCOMP"POS="N"> evaluation</w></NP> <w ID=5 SENT=0 PAR=1LEMMA="."
POS="PUNCT">.</w></s></p>Example 2: File obtained as result of the pre-processing stage (includes previous coreferencean-notation) for the text This is an input file.
Itis used for evaluation.2.2 Shared resourcesThe three algorithms implemented receive asinput a representation of the input file.
Thisrepresentation is generated by running anXML parser over the file resulting from thepre-processing phase.
A list of noun phrases isexplicitly kept in the file representation.
Eachentry in this list consists of a record containing:?
the word form?
the lemma of the word or of the head of thenoun phrase?
the starting position in the text?
the ending position in the text?
the part of speech?
the grammatical function?
the index of the sentence that contains thereferent?
the index of the verb whose argument thisreferent isEach of the algorithms implemented forthe workbench enriches this set of data withinformation relevant to its particular needs.Kennedy and Boguraev (1996), for example,need additional information about whether acertain discourse referent is embedded or not,plus a pointer to the COREF class associated tothe referent, while Mitkov?s approach needs ascore associated to each noun phrase.Apart from the pre-processing tools, theimplementation of the algorithms included in theworkbench is built upon a common program-ming interface, which allows for some basicprocessing functions to be shared as well.
Anexample is the morphological filter applied overthe set of possible antecedents of an anaphor.2.3 Usability of the workbenchThe evaluation workbench is easy to use.
Theuser is presented with a friendly graphicalinterface that helps minimise the effort involvedin preparing the tests.
The only informationshe/he has to enter is the address (machineand directory) of the FDG parser and thefile annotated with coreferential links to beprocessed.
The results can be either specific toeach method or specific to the file submittedfor processing, and are displayed separatelyfor each method.
These include lists of thepronouns and their identified antecedents inthe context they appear as well as informationas to whether they were correctly solved ornot.
In addition, the values obtained for thefour evaluation measures (see section 3.2) andseveral statistical results characteristic of eachmethod (e.g.
average number of candidatesfor antecedents per anaphor) are computed.Separately, the statistical values related to theannotated file are displayed in a table.
We shouldnote that (even though this is not the intendedusage of the workbench) a user can also submitunannotated files for processing.
In this case,the algorithms display the antecedent found foreach pronoun, but no automatic evaluation can becarried out due to the lack of annotated testingdata.2.4 Envisaged extensionsWhile the workbench is based on the FDGshallow parser at the moment, we plan to updatethe environment in such a way that two differentmodes will be available: one making use ofa shallow parser (for approaches operating onpartial analysis) and one employing a full parser(for algorithms making use of full analysis).Future versions of the workbench will includeaccess to semantic information (WordNet) toaccommodate approaches incorporating suchtypes of knowledge.3 Comparative evaluation ofknowledge-poor anaphora resolutionapproachesThe first phase of our project includedcomparison of knowledge-poorer approacheswhich share a common pre-processingphilosophy.
We selected for comparativeevaluation three approaches extensively cited inthe literature: Kennedy and Boguraev?s parser-free version of Lappin and Leass?
RAP (Kennedyand Boguraev, 1996), Baldwin?s pronounresolution method (Baldwin, 1997) and Mitkov?sknowledge-poor pronoun resolution approach(Mitkov, 1998b).
All three of these algorithmsshare a similar pre-processing methodology: theydo not rely on a parser to process the input andinstead use POS taggers and NP extractors; nordo any of the methods make use of semanticor real-world knowledge.
We re-implementedall three algorithms based on their originaldescription and personal consultation with theauthors to avoid misinterpretations.
Since theoriginal version of CogNiac is non-robust andresolves only anaphors that obey certain rules, forfairer and comparable results we implemented the?resolve-all?
version as described in (Baldwin,1997).
Although for the current experimentswe have only included three knowledge-pooranaphora resolvers, it has to be emphasised thatthe current implementation of the workbenchdoes not restrict in any way the number orthe type of the anaphora resolution methodsincluded.
Its modularity allows any such methodto be added in the system, as long as the pre-processing tools necessary for that method areavailable.3.1 Brief outline of the three approachesAll three approaches fall into the category offactor-based algorithms which typically employa number of factors (preferences, in the caseof these three approaches) after morphologicalagreement checks.Kennedy and BoguraevKennedy and Boguraev (1996) describe analgorithm for anaphora resolution based onLappin and Leass?
(1994) approach but withoutemploying deep syntactic parsing.
Their methodhas been applied to personal pronouns, reflexivesand possessives.
The general idea is to constructcoreference equivalence classes that have anassociated value based on a set of ten factors.
Anattempt is then made to resolve every pronoun toone of the previous introduced discourse referentsby taking into account the salience value of theclass to which each possible antecedent belongs.Baldwin?s CogniacCogNiac (Baldwin, 1997) is a knowledge-poor approach to anaphora resolution basedon a set of high confidence rules which aresuccessively applied over the pronoun underconsideration.
The rules are ordered accordingto their importance and relevance to anaphoraresolution.
The processing of a pronoun stopswhen one rule is satisfied.
The original versionof the algorithm is non-robust, a pronoun beingresolved only if one of the rules is applied.
Theauthor also describes a robust extension of thealgorithm, which employs two more weak rulesthat have to be applied if all the others fail.Mitkov?s approachMitkov?s approach (Mitkov, 1998b) is arobust anaphora resolution method for technicaltexts which is based on a set of boosting andimpeding indicators applied to each candidatefor antecedent.
The boosting indicators assigna positive score to an NP, reflecting a positivelikelihood that it is the antecedent of the currentpronoun.
In contrast, the impeding ones applya negative score to an NP, reflecting a lack ofconfidence that it is the antecedent of the currentpronoun.
A score is calculated based on theseindicators and the discourse referent with thehighest aggregate value is selected as antecedent.3.2 Evaluation measures usedThe workbench incorporates an automatic scoringsystem operating on an XML input file where thecorrect antecedents for every anaphor have beenmarked.
The annotation scheme recognised bythe system at this moment is MUC, but supportfor the MATE annotation scheme is currentlyunder developement as well.We have implemented four measures forevaluation: precision and recall as defined byAone and Bennett (1995)4 as well as success rateand critical success rate as defined in (Mitkov,2000a).
These four measures are calculated asfollows:?
Precision = number of correctly resolvedanaphor / number of anaphors attempted tobe resolved?
Recall = number of correctly resolvedanaphors / number of all anaphors identifiedby the system?
Success rate = number of correctly resolvedanaphors / number of all anaphors?
Critical success rate = number of correctlyresolved anaphors / number of anaphorswith more than one antecedent after amorphological filter was appliedThe last measure is an important criterionfor evaluating the efficiency of a factor-basedanaphora resolution algorithm in the ?criticalcases?
where agreement constraints alone cannotpoint to the antecedent.
It is logical to assumethat good anaphora resolution approaches should4This definition is slightly different from the one used in(Baldwin, 1997) and (Gaizauskas and Humphreys, 2000).For more discussion on this see (Mitkov, 2000a; Mitkov,2000b).have high critical success rates which are closeto the overall success rates.
In fact, in most casesit is really the critical success rate that matters:high critical success rates naturally imply highoverall success rates.Besides the evaluation system, the workbenchalso incorporates a basic statistical calculatorwhich addresses (to a certain extent) the questionas to how reliable or realistic the obtainedperformance figures are - the latter depending onthe nature of the data used for evaluation.
Someevaluation data may contain anaphors which aremore difficult to resolve, such as anaphors thatare (slightly) ambiguous and require real-worldknowledge for their resolution, or anaphors thathave a high number of competing candidates, orthat have their antecedents far away both in termsof sentences/clauses and in terms of number of?intervening?
NPs etc.
Therefore, we suggest thatin addition to the evaluation results, informationshould be provided in the evaluation data as tohow difficult the anaphors are to resolve.5 To thisend, we are working towards the development ofsuitable and practical measures for quantifyingthe average ?resolution complexity?
of theanaphors in a certain text.
For the time being, webelieve that simple statistics such as the numberof anaphors with more than one candidate,and more generally, the average number ofcandidates per anaphor, or statistics showing theaverage distance between the anaphors and theirantecedents, could serve as initial quantifyingmeasures (see Table 2).
We believe that thesestatistics would be more indicative of how ?easy?or ?difficult?
the evaluation data is, and shouldbe provided in addition to the information on thenumbers or types of anaphors (e.g.
intrasententialvs.
intersentential) occurring or coverage (e.g.personal, possessive, reflexive pronouns in thecase of pronominal anaphora) in the evaluationdata.3.3 Evaluation resultsWe have used a corpus of technical texts manuallyannotated for coreference.
We have decided on5To a certain extent, the critical success rate definedabove addresses this issue in the evaluation of anaphoraresolution algorithms by providing the success rate for theanaphors that are more difficult to resolve.Success Rate PrecisionFile Number ofwordsNumber ofpronounsAnaphoricpronouns Mitkov Cogniac K&B Mitkov Cogniac K&BACC 9617 182 160 52.34% 45.0% 55.0% 42.85% 37.18% 48.35%WIN 2773 51 47 55.31% 44.64% 63.82% 50.98% 41.17% 58.82%BEO 6392 92 70 48.57% 42.85% 55.71% 36.95% 32.60% 42.39%CDR 9490 97 85 71.76% 67.05% 74.11% 62.88% 58.76% 64.95%Total 28272 422 362 56.9% 49.72% 61.6% 48.81% 42.65% 52.84%Table 1: Evaluation resultsAverage referential distanceFile Pronouns Personal Possesive Reflexive Intrasententialanaphors Sentences NPsAverage no ofantecedentsACC 182 161 18 3 90 1.2 4.2 9.4WIN 51 40 11 0 41 1.1 4.1 11.9BEO 92 74 18 0 56 1.4 5.1 12.9CDR 97 85 10 2 54 1.4 3.7 9.2Total 422 360 57 5 241 1.275 4.275 10.85Table 2: Statistical resultsthis genre because both Kennedy&Boguraev andMitkov report results obtained on technical texts.The corpus contains 28,272 words, with19,305 noun phrases and 422 pronouns, out ofwhich 362 are anaphoric.
The files that wereused are: ?Beowulf HOW TO?
(referred in Table1 as BEO), ?Linux CD-Rom HOW TO?
(CDR),?Access HOW TO?
(ACC), ?Windows Help file?(WIN).
The evaluation files were pre-processedto remove irrelevant information that might alterthe quality of the evaluation (tables, sequencesof code, tables of contents, tables of references).The texts were annotated for full coreferentialchains using a slightly modified version ofthe MUC annotation scheme.
All instances ofidentity-of-reference direct nominal anaphorawere annotated.
The annotation was performedby two people in order to minimize human errorsin the testing data (see (Mitkov et al, 2000) forfurther details).Table 1 describes the values obtained for thesuccess rate and precision6 of the three anaphoraresolvers on the evaluation corpus.
The overallsuccess rate calculated for the 422 pronounsfound in the texts was 56.9% for Mitkov?smethod, 49.72% for Cogniac and 61.6% forKennedy and Boguraev?s method.Table 2 presents statistical results on theevaluation corpus, including distribution of6Note that, since the three approaches are robust, recallis equal to precision.pronouns, referential distance, average number ofcandidates for antecedent per pronoun and typesof anaphors.7As expected, the results reported in Table 1do not match the original results published byKennedy and Boguraev (1996), Baldwin (1997)and Mitkov (1998b) where the algorithms weretested on different data, employed differentpre-processing tools, resorted to different degreesof manual intervention and thus provided nocommon ground for any reliable comparison.By contrast, the evaluation workbench enablesa uniform and balanced comparison of thealgorithms in that (i) the evaluation is done onthe same data and (ii) each algorithm employsthe same pre-processing tools and performsthe resolution in fully automatic fashion.
Ourexperiments also confirm the finding of Orasan,Evans and Mitkov (2000) that fully automaticresolution is more difficult than previouslythought with the performance of all the threealgorithms essentially lower than originallyreported.4 ConclusionWe believe that the evaluation workbench foranaphora resolution proposed in this paper7In Tables 1 and 2, only pronouns that are treatedas anaphoric and hence tried to be resolved by the threemethods are included.
Therefore, pronouns in first andsecond person singular and plural and demonstratives do notappear as part of the number of pronouns.alleviates a long-standing weakness in the areaof anaphora resolution: the inability to fairlyand consistently compare anaphora resolutionalgorithms due not only to the difference ofevaluation data used, but also to the diversity ofpre-processing tools employed by each system.In addition to providing a common ground forcomparison, our evaluation environment ensuresthat there is fairness in terms of comparingapproaches that operate at the same level ofautomation: formerly it has not been possibleto establish a correct comparative picture due tothe fact that while some approaches have beentested in a fully automatic mode, others havebenefited from post-edited input or from a pre- (ormanually) tagged corpus.
Finally, the evaluationworkbench is very helpful in analysing thedata used for evaluation by providing insightfulstatistics.ReferencesChinatsu Aone and Scot W. Bennett.
1995.Evaluating automated and manual acquisition ofanaphora resolution rules.
In Proceedings ofthe 33th Annual Meeting of the Association forComputational Linguistics (ACL ?95), pages 122?129.Breck Baldwin.
1997.
Cogniac: High precisioncoreference with limited knowledge and linguisticresources.
In R. Mitkov and B. Boguraev, editors,Operational factors in practical, robust anaphoraresolution for unrestricted texts, pages 38 ?
45.Ido Dagan and Alon Itai.
1990.
Automaticprocessing of large corpora for the resolutionof anaphora references.
In Proceedings of the13th International Conference on ComputationalLinguistics (COLING?90), volume III, pages 1?3.Ido Dagan and Alon Itai.
1991.
A statistical filter forresolving pronoun references.
In Y.A.
Feldman andA.
Bruckstein, editors, Artificial Intelligence andComputer Vision, pages 125 ?
135.
Elsevier SciencePublishers B.V.Antonio Ferrandez, Manolo Palomar, and L. Moreno.1997.
Slot unification grammar and anaphoraresolution.
In Proceedings of the InternationalConference on Recent Advances in NaturalLanguage Proceeding (RANLP?97), pages 294?299.Robert Gaizauskas and Kevin Humphreys.
2000.Quantitative evaluation of coreference algorithms inan information extraction system.
In Simon Botleyand Antony Mark McEnery, editors, Corpus-based and Computational Approaches to DiscourseAnaphora, Studies in Corpus Linguistics, chapter 8,pages 145 ?
169.
John Benjamins PublishingCompany.Niyu Ge, J. Hale, and E. Charniak.
1998.
A statisticalapproach to anaphora resolution.
In Proceedingsof the Sixth Workshop on Very Large Corpora,COLING-ACL ?98, pages 161 ?
170, Montreal,Canada.Jerry Hobbs.
1976.
Pronoun resolution.
Researchreport 76-1, City College, City University of NewYork.Jerry Hobbs.
1978.
Pronoun resolution.
Lingua,44:339?352.Christopher Kennedy and Branimir Boguraev.
1996.Anaphora for everyone: pronominal anaphoraresolution without a parser.
In Proceedings of the16th International Conference on ComputationalLinguistics (COLING?96), pages 113?118,Copenhagen, Denmark.Shalom Lappin and H.J.
Leass.
1994.
Analgorithm for pronominal anaphora resolution.Computational Linguistics, 20(4):535 ?
562.Ruslan Mitkov, R. Evans, C. Orasan, C. Barbu,L.
Jones, and V. Sotirova.
2000.
Coreferenceand anaphora: developing annotating tools,annotated resources and annotation strategies.In Proceedings of the Discourse, Anaphora andReference Resolution Conference (DAARC2000),pages 49?58, Lancaster, UK.Ruslan Mitkov.
1998a.
Evaluating anaphoraresolution approaches.
In Proceedings of theDiscourse Anaphora and Anaphora ResolutionColloquium (DAARC?2), pages 164 ?
172,Lancaster, UK.Ruslan Mitkov.
1998b.
Robust pronoun resolutionwith limited knowledge.
In Proceedings of the18th International Conference on ComputationalLinguistics (COLING?98/ACL?98, pages 867 ?
875.Morgan Kaufmann.Ruslan Mitkov.
2000a.
Towards a more consistent andcomprehensive evaluation of anaphora resolutionalgorithms and systems.
In Proceedings of theDiscourse, Anaphora and Reference ResolutionConference (DAARC2000), pages 96 ?
107,Lancaster, UK.Ruslan Mitkov.
2000b.
Towards more comprehensiveevaluation in anaphora resolution.
In Proceedingsof the Second International Conference onLanguage Resources and Evaluation, volume III,pages 1309 ?
1314, Athens, Greece.Ruslan Mitkov.
2001.
Outstanding issues in anaphoraresolution.
In Al.
Gelbukh, editor, ComputationalLinguistics and Intelligent Text Processing, pages110?125.
Springer.Constantin Ora?san, Richard Evans, and RuslanMitkov.
2000.
Enhancing preference-basedanaphora resolution with genetic algorithms.
InProceedings of Natural Language Processing -NLP2000, pages 185 ?
195.
Springer.P.
Tapanainen and T. Ja?rvinen.
1997.
A non-projective dependency parser.
In Proceedings ofthe 5th Conference of Applied Natural LanguageProcessing, pages 64 ?
71, Washington D.C., USA.Joel R. Tetreault.
1999.
Analysis of syntax-basedpronoun resolution methods.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics (ACL ?99), pages 602 ?605, Maryland, USA.
