SOME CHART-BASED TECHNIQUES FORPARSING ILL-FORMED INPUTChris S. MellishDeparlment ofArtificial Intelligence,University of Edinburgh,80 South Bridge,EDINBURGH EH1 1HN,Scotland.ABSTRACTWe argue for the usefulness of an active chart as thebasis of a system that searches for the globally mostplausible xplanation of failure to syntactically parsea given input.
We suggest semantics-free, grammar-independent techniques for parsing inputs displayingsimple kinds of ill-formedness and discuss the searchissues involved.THE PROBLEMAlthough the ultimate solution to the problemof processing ill-formed input must ake into accountsemantic and pragmatic factors, nevertheless it isimportant to understand the limits of recovery stra-tegies that age based entirely on syntax and which areindependent of any particular grammar.
The aim ofOtis work is therefore to explore purely syntactic andgranmmr-independent t chniques to enable ato recover from simple kinds of iil-formedness in rex.tual inputs.
Accordingly, we present a generalisedparsing strategy based on an active chart which iscapable of diagnosing simple ?nvrs(unknown/mi.uq~elled words, omitted words, extranoise words) in sentences (from languages describedby context free phrase slructur?
grammars without e-productions).
This strategy has the advantage thatthe recovery process can run after a standard (activechart) parser has terminated unsuccessfully, withoutcausing existing work to be reputed or the originalparser to be slowed down in any way, and that,unlike previous ystems, it allows the full syntacticcontext o be exploited in the determination of a"best" parse for an ill-formed sentence.EXPLOITING SYNTACTIC CONTEXTWeischedel and Sondheimer (1983) present anapproach to processing ill-formed input based on amodified ATN parser.
The basic idea is, when an ini-tial p~s@ fails, to select he incomplete parsing paththat consumes the longest initial portion of the input,apply a special rule to allow the blocked parse tocontinue, and then to iterate this process until asuccessful parse is generated.
The result is a "hilloclimbing" search for the "best" parse, relying at eachpoint on the "longest path" heuristic.
Unfortunately,sometimes this heuristic will yield several possibleparses, for instance with the sentence:The snow blocks I" te road(no partial parse getting past the point shown) wherethe parser can fail expecting either a verb or a deter-miner.
Moreover, sometimes the heuristic will causethe most "obvious" error to be missed:He said that he snow the road TThe paper will T the best news is the Timeswhere we might suspect that there is a missing verband a misspelled "with" respectively.
In all thesecases, the "longest path" heuristic fails to indicateunambiguously the minimal change that would benecessary to make the whole input acceptable as asentence.
This is not surprising, as the left-fight biasof an ATN parser allows the system to take noaccount of the right context of a possible problemelement.Weischedel and Sondheimer's use of the"longest path" heuristic is similar to the use of locallyleast-cost error recovery in Anderson andBackhouse's (1981) scheme for compilers.
It seemsto be generally accepted that any form of globally"minimum-distance" error correction will be toocostly to implement (Aho and Ullman, 1977).
Suchwork has, however, not considered heuristicapproaches, such as the one we are developing.Another feature of Weischedel andSondheimer's system is the use of grammar-slx~ificrecovery rules ("meta-rules" in their terminology).The same is true of many other systems for dealingwith ill-formed input (e.g.
Carhonell and Hayes(1983), Jensen et al (1983)).
Although grammar-specific recovery rules are likely in the end always tobe more powerful than grammar-independent rules, itdoes seem to be worth investigating how far one canget with rules that only depend on the grammar for-ma//sm used.102IOT tbe T gardener T c?llects T manure T ff T the T antumn7 T J ,  1 2 3 4 5 6 ,<Need S from 0 to 7><Need NP+VP from 0 to 7><Need VP from 2 to 7><Need VP+PP from 2 to 7><Need PP from 4 to 7><Need P+NP from 4 to 7><Need P from4 to 5>(hypoth~s)(by top-down rule)(by fundamental rule with NP found bottom-up)(by top-down rule)(by fundamental rule with VP found bottom-up)(by top-down rule)(by fundamental rule with NP found bottom-up)Figure 1: Focusing on an emx.In _~.~_pting an ATN parser to compare partialparses, Weischedel and Sondheimer have alreadyintroduced machinery to represent several alternativepartial parses imultaneously.
From this, it is a rela-tively small step to introduce a well-formed substringtable, or even an active chart, which allows for a glo-hal assessment of the state of the parser.
If the gram-mar form~fi~m is also changed to a declarative for-malism (e.g.
CF-PSGs, DCGs (Pereira and Warren1980), patr-ll (Shieber 1984)), then there is a possi-bility of constructing other partial parses that do notstart at the beginning of the input.
In this way, rightcontext can play a role in the determination f the~est" parse.WHAT A CHART PARSER LEAVES BEHINDThe information that an active chart parserleaves behind for consideration by a "post mortem"obviously depends on the parsing sWategy used (Kay1980, Gazdar and Mellish 1989).
Act/re edges areparticularly important fx~n the point of view of diag-nosing errors, as an unsatisfied active dge suggests aplace where an input error may have occurred.
Sowe might expect o combine violated expectationswith found constituents to hypothesise completeparses.
For simplicity, we assume here that thegrammar is a simple CF-PSG, although there areobvious generalisations.
(Left-right) top-down pars/ng is guaranteed tocreate active edges for each kind of phrase that couldcontinue a partial parse starling at the beginning ofthe input.
On the other hand, bottom-up arsing (bywhich we mean left corner parsing without op-downfiltering) is guaranteed to find all complete consti-merits of every possible parse.
In addition, whenevera non-empty initial segment of a rule RHS has beenfound, the parser will create active edges for the kindof phrase predicted to occur after this segment.
Top-down parsing will always create an edge for a phrasethat is needed for a parse, and so it will alwaysindicate by the presence of an unsatisfied active edgethe first ester point, if there is one.
If a subsequenterror is present, top-down parsing will not alwayscreate an active edge corresponding to it, because thesecond may occur within a constituent that will notbe predicted until the first error is corrected.
Simi-larly, fight-to-left top-down parsing will always indi-cate the last error point, and a combination ofthe twowill find the first and last, but not necessarily anyerror points in between.
On the other hand, bottom-up parsing will only create an active edge for eacherror point that comes immediately after a sequenceof phrases corresponding to an initial segment of theRI-IS of a grammar rule.
Moreover, it will not neces-sarily refine its predictions to the most detailed level(e.g.
having found an NP, it may predict theexistence of a following VP, but not the existence oftypes of phrases that can start a VP).
Weisobedel andSondheimer's approach can be seen as an incremen-tal top-down parsing, where at each stage the right-most tin.riffled active edge is artificially allowed tobe safistied in some way.
As we have seen, there isno guarantee that this sort of hill-climbing will findthe "best" solution for multiple errors, or even forsingle errors.
How can we combine bottom-up andtop-down parsing for a more effective solution?FOCUSING ON AN ERROROur basic stramgy is to run a bottom-up arserover the input and then, if this fails to find a completeparse, to run a modified top-down parser over theresulting chart to hypothesise possible completeparses.
The modified top-down parser attempts tofind the minimal errors that, when taken account of,enable a complete parse to be constructed.
Imaginethat a bottom-up arser has already run over the input"the gardener collects manure if the autumn".
ThenFigure 1 shows (informally) how a top-down parsermight focus on a possible rror.
To implement thiskind of reasoning, we need a top-down parsing rulethat knows how to refine a set of global needs and a103fundamental rule that is able m incorporate foundconstituents from either directim.
When we mayencounter multiple rotors, however, we need toexpress multiple needs (e.g.
<Need N from 3 to 4 andPP from 8 to I0>).
We also need to have a fimda-mental rule that can absorb found phrases firom any-where in a relevant portion of the chart (e.g.
given arule "NP --+ Det Adj N" and a sequence "as marvel-lous sihgt", we need to be able to hypothesi~ that"as" should be a Det and "sihgt" a N).
To saverepealing work, we need a version of the top-downrule that stops when it reaches an appropriatecategory that has already been found bottom-up.Finally, we need to handle both "anchored" and"unanchored" needs.
In an anchored need (e.g.<Need NP from 0 to 4>) we know the beginning andend of the portion of the chart within which thesearch is to take place.
In looking for a NP VPsequence in "the happy blageon su'mpled the bait",however, we can't initially find a complete (initial)NP or (final) VP and hence don't know where in thechart these phrases meeL We express this by <NeedNP from 0 to *, VP f~om * to 6>, the symbol "*"denoting a position in the chart that remains to bedetermined.GENERALISED TOP-DOWN PARSINGIf we adopt a chart parsing suategy with onlyedges that carry informafim about global needs,thee will be considerable dupficated effort.
Forinstance, the further efinement of the two edges:<Need NP hem 0 to 3 and V from 9 to 10><Need NP from 0 to 3 and Adj from 10 to 11>can lead to any analysis of possible NPs between 0and 3 being done twice.
Restricting the possible for-mat of edges in this way would be similar to allowingthe "functional composition rule" (Steedman 1987) instandard chart parsing, and in general this is not donefor efficiency reasons.
Instead, we need to produce asingle edge that is "in charge" of the computationlooking for NPs between 0 and 3.
When poss?ole NPsare then found, these then need to be combined withthe original edges by an appropriate form of the fun-damental rule.
We are thus led to the following formfor a generalised edge in our chart parser:.<C from S to E needsC$1 fi'om $1 toel,cs2 from s2 to e2.. o .C$, from $.
to e,>where C is a category, the c$~ are lists of categories(which we will show inside square brackets),.
S, E,the si and the e~ ate positions in the chart (or the spe-cial symbol "*~).
The presence of an edge of thiskind in the chart indicates that the parser is attempt-ing to find a phrase of category C covering the por-tion of the chart from S to E, but that in order tosucceed it must still satisfy all the needs listed.
Eachneed specifies a sequence of categories csl that mustbe found contiguously to occupy the portion of thechart extending from st to ei.Now that he format of the edges is defined, wecan be precise about the parsing rules used.
Ourmodified chart parsing rules are shown in Figure 2.The modified top-down ru/e allows us to refine aneed into a more precise one, using a rule of thegrammar (the extra conditions on the rule preventfurther efinement where a phrase of a given categoryhas already been found within the precise part of thechart being considezed).
The modified fundamentalru/e allows a need to be satisfied by an edge that iscompletely ~ti~fied (i.e.
an inactive dge, in the stan-dard terminology).
A new rule, the simplificationru/~, is now required to do the relevant housekeepingwhen one of an edge's needs has been completelysatisfied.
One way that these rules could run wouldbe as follows.
The chart starts off with the inactiveedges left by bottom-up arsing, together with a sin-gle "seed" edge for the top-down phase <GOAL from0 to n needs \[S\] from 0 to n>, where n is the finalposition in the chart.
At any point the fundamentalrule is run as much as possible.
When we canproceed no further, the first need is refined by thetop-down rule (hopefully search now beinganchored).
The fundamental rule may well againapply, taking account of smaller phrases that havealready been found.
When this has run, the top-downrule may then further efine the system's expectationsabout the parts of the phrase that cannot be found.And so on.
This is just the kind of "focusing" that wediscussed in the last section..
If an edge expressesneeds in several separate places, the first will eventu-ally get resolved, the simplification rule will thenapply and the rest of the needs will then be workedon.For this all to make sense, we must assume thatall hypothesised needs can eventually be resolved(otherwise the rules do not suffice for more than oneerror to be narrowed own).
We can ensure this byintroducing special rules for recoguising the mostprimitive kinds of errors.
The results of these rulesmust obviously be scored in some way, so that errorsare not wildly hypothesised in all sorts of places.I04Top-down rule:<C from S toe  needs \[cl...csl\] from sl  to e:,  cs2 fzom s2 to e2 .... cs.
from s.  toe .>c I ~ RHS (in the grammar)<cl from sl toe needs RHS from sx toe>where e = ff csl is not empty or e 1 ffi * then * else e x(el = * or CSl is non-empty or there is no category cl from sl to e:)Fundamental rule:<C from S mE needs \[...cs n c l ...cs n\] from s l to e x, cs 2 ...><c ~ from S ~ to El needs <nothing>><C fxom S toe  needs csn  from sx to S t, csx2 fxom E t to el ,  cs2 ...>(sl < Sx,  e l  = * or El < e : )Simplification rule:<C fxom S toE  needs ~ from s to s, c$2 from s2 to e2, ... cs.
from s. me,,><C from S toe  needs cs2 from s2 to e2, ... cs.
fxom s.  toe .>Garbage rule:<C fronts toE needs I\] from sl to e l ,  c$2 from s2 to e2, ... cs.
f roms,  toe .><C fronts  toE  needs cs2 from s2 to e2, ... cs.
from s. me.>(s, ~el)Empty category rule:<C from S toE  needs \[cl...csl\] from s to s, cs2 from s2 to e2 .... ca.
from s.  toe .><C fxom S toE  needs cs2 from s2 to e2.
... cs.
f~om s,  toe ,>Unknown word rule:<C from S toe  needs \[cl...csl\] from sl  to ex, cs2 from s2 to e2 .... cs.
fzom s. toe .><C from S toE  needs cs~ from st+l  to ex, cs2 from s2 to e2, ... cs.
from s.  toe .>(cl a lexical category, sl < the end of the chart andthe word at s i not of category c ~).Figure 2: Generalised Top-down Parsing RulesSEARCH CONTROL AND EVALUATIONFUNCTIONSEven without the extra rules for recognisingprimitive errors, we have now introduced a largeparsing search space.
For instance, the new funda-mental rule means that top-down processing can takeplace in many different parts of the chart.
Chartparsers already use the notion of an agenda, in whichpossible additions to the chart are given priority, andso we have sought o make use of this in organising aheuristic search for the "best" poss~le parse.
Wehave considered a number of parameters for decidingwhich edges hould have priority:MDE (mode of formation)We prefer edgesthat arise from the fundamental rule to those thatarise from the rap-down rule; we disprefer edges thatarise from unanchored applications of the top-downnile.PSF (penalty so far) Edges resulting from thegarbage, empty category and unknown word rules aregiven penalty scores.
PSF counts the penalties thathave been accumulated sofar in an edge.PB (best penalty) This is an estimate of thebest possible penalty that this edge, when complete.could have.
This score can use the PSF, together withinformation about the parts of the chart covered - for105instance, the number of words in these parts whichdo not have lexical entries.GU$ (the ma~um number of words that havebeen used so far in a partial parse using this edge)We prefer edges that lead to parses accounting formore words of the input.PBG (the best possible penalty for any com-plete hypothesis nvolving this edge).
This is a short-fall score in the sense of Woeds (1982).UBG (the best possible number of words thatcould be used in any complete hypothesis containingthis edge).In our implementation, each rule calculateseach of these scores for the new edge from those ofthe contributing edges.
We have experimented with anumber of ways of using these scores in comparingtwo possible edges to be added to the chart.
Atpresent, he most promising approach seems to be tocompare in mm the scores for PBG, MDE, UBG,GUS, PSF and PB.
As soon as a difference in scoresis encountered, the edge that wins on this account ischosen as the preferred one.
Putting PBG first in thissequence ensures that the first solution found will bea solution with a minimal penalty score.The rules for computing scores need to makeestimates about he possible penalty scores that mightarise from attempting to find given types of phrasesin given parts of the chart.
We use a number ofheuristics to compute these.
For instance, the pres.ence of a word not appearing in the lexicon meansthat every parse covering that word must have anon-zero penalty score.
In general, an attempt to findan instance of a given category in a given portion ofthe chart must produce a penalty score if the boltom-up parsing phase has not yielded an inactive dge ofthe correct kind within that portion.
Finally, the factthat the grammar is assumed to have no e-productions means that an attempt o find a longsequence of categories in a short piece of chart isdoomed to produce a penalty score; similarly asequence of lexical categories cannot be foundwithout penalty in a pordon of chart hat is too long.Some of the above scoring parameters score anedge according what sorts of parses it could contri-bute to, not just according to bow internally plausibleit seems.
This is desirable, as we wish the construc-tion of globally most plausible solutions to drive theparsing.
On the other hand, it introduces a number ofproblems for chart organisation.
As the same edge(apart from its score) may be generated in differentways, we may end up with multiple possible scoresfor it.
It would make sense at each point to considerthe best of the possible scores associated with anedge to be the current score.
In this way we wouldnot have to repeat work for every differently scoredversion of an edge.
But consider the followingscenario:Edge A is added to the chart.
Later edge Bis spawned using A and is placed in theagenda.
Subsequently A's scc~e increasesbecause it is derived in a new and betterway.
This should affect B's score (andhence B's position on the agenda).If the score of an edge increases then the scores ofedges on the agenda which were spawned from itshould also increase.
To cope with this sort of prob-lem, we need some sort of dependency analysis, amechanism for the propagation of changes and aneasily resorted agenda.
We have not addressed theseproblems o far - our cterent implementation treatsthe score as an integral part of an edge and suffersfiom the resulting duplication problem.PRELIMINARY EXPERIMENTSTo see whether the ideas of this paper makesense in practice, we have performed some very prel-iminaw experiments, with an inefficient implementa-tion of the chart parser and a small CF-PSG (84 rulesand 34 word lexicon, 18 of whose entries indicatecategory ambiguity) for a fragment of English.
Wegenerated random sentences (30 of each length con-sidered) from the grammar and then introduced ran-dom ocxunences of specific types of errors into thesesentences.
The errors considered were none (i.e.
leav-ing the correct sentence as it was), deleting a word,adding a word (either a completely unknown word ora word with an entry in the lexicon) and substitutinga completely unknown word for one word of the sen-tence.
For each length of original sentence, there,~ts were averaged over the 30 sentences ran-domly generated.
We collected the following statis-tics (see Table 1 for the results):BU cyc/e$ - the number of cycles taken (seebelow) to exhaust he chart in the initial (standard)bottom-up arsing phase.#$olns - the number of different "solutions"found.
A "solution" was deemed to be a descriptionof a possible set of errors which has a minimalpenalty score and if corrected would enable a com-plete parse to be constructed.
Possible errors wereadding an extra word, deleting a word and substitut-ing a word for an instance of a given lexical category.106Table 1: Preliminary experimental resultsErrorNoneDelete one wordAdd unknown wordAdd known wordSubst unknown wordLength of original3691236912BU cycles, , #Solns31 i69 1135 1198 117 550 5105 6155 7'3 29 16 60 29 105 212 156 3369123691237 372 3137 3170 517 249 296 2150 3First Last TD cycles0 0 00 0 00 0 00 0 014 39 5018 73 11427 137 35033 315 10029 17 6524 36 13539 83 526132 289 192229 51 88.d43 88 21658 124 56899 325 177517 28 4623 35 10538 56 30042 109 1162The penalty associated with a given set of errors wasthe number of em3~ in the set.First - the number of cycles of generalisedtop-down parsing required to find the first solution.Last - the number of cycles of generalised top-down parsing required to find the last solution.TD cyc/es - the number of cycles of generalisedtop-down parsing required to exhaust all possibilitiesof sets of errors with the same penalty as the firstsolution found.It was important to have an implementation-independent measure of the amount of work done bythe parser, and for this we used the concept of a"cycle" of the chart parser.
A "cycle" in this contextrepresents he activity of the parser in removing oneitem from the agenda, adding the relevant edge to thechart and adding to the agenda ny new edges thatare suggested by the rules as a result of the new addi-tion.
For instance, in conventional top-down chartparsing a cycle might consist of removing the edge<S from 0 to 6 needs \[NP VI'\] from 0 to 6> from thefront of the agenda, adding this to the chart and thenadding new edges to the agenda, as follows.
Ftrst ofall, for each edge of the form <NP from 0 to a needs0> in the chart the fundamental rule determines that<S from 0 to 6 needs \[VP\] from ct to 6> should beadded.
Secondly, for each rule NP -.., 7 in the gram-mar the top-down rule determines that <NP from 0 to* needs y from 0 to *> should be added.
With gen-eralised top-down parsing, there are more rules to beconsidered, but the idea is the same.
Actually, for thetop-down rule our implementation schedules a wholecollection of single additions ("apply the top downrule to edge a") as a single item on the agenda.
Whensuch a request reaches the front of the queue, theactual new edges are then computed and themselvesadded to the agenda.
The result of this strategy is tomake the agenda smaller but more structured, at thecost of some extra cycles.EVALUATION AND FUTURE WORKThe preliminary results show that, for smallsentences and only one error, enumerating all thepossible minimum-penalty errors takes no worse than10 times as long as parsing the correct sentences.Finding the first minimal-penalty error can also bequite fast.
There is, however, a great variabilitybetween the types of error.
Errors involving com-pletely unknown words can be diagnosed reasonably107quickly because the presence of an unknown wordallows the estimation of penalty scores to be quiteaccurate (the system still has to work out whether theword can be an addition and for what categories itcan substitute for an instance of, however).
We havenot yet considered multiple errors in a sentence, andwe can expect he behaviour to worsten dramaticallyas the number of errors increases.
Although Table 1does not show this, there is also a great deal of varia-bility between sentences of the same length with thesame kind of introduced error.
It is noticeable thaterrors towards the end of a sentence are harder todiagnose than those at the start.
This reflects the leR-fight orientation of the parsing rules - an attempt tofind phrases tarting to the right of an error will havea PBG score at least one more than the estimated PB,whereas an attempt m find phrases in an open-endedportion of the chart starting before an error may havea PBG score the same as the PB (as the error mayoccur within the phrases to be found).
Thus moreparsing attempts will be relegated to the lower partsof the agenda in the first case than in the second.One disturbing fact about the statistics is thatthe number of minimal-penalty solutions may bequite large.
For instance, the ill-formed sentence:who has John seen on that hadwas formed by adding the extra word "had" to thesentence "who has John seen on that".
Our parserfound three other possible single errors to account forthe sentence.
The word "on" could have been anadded word, the word "on" could have been a substi-tution for a complementiser and there could havebeen a missing NP after "on".
This large number ofsolutions could be an artefact of our particular gram-ram" and lexicon; certainly it is unclear how oneshould choose between possible solutions in agrammar-independent way.
In a few cases, the intro-duction of a random error actually produced a gram-matical sentence - this occurred, for instance, twicewith sentences of length 5 given one random A__ddedword.At this stage, we cannot claim that our experi-ments have done anything more than indicate a cer-tain concreteness to the ideas and point to a numberof unresolved problems.
It remains to be seen howthe performance will scale up for a realistic grammarand parser.
There are a number of detailed issues toresolve before a really practical implementation fthe above ideas can be produced.
The indexing stra-tegy of the chart needs to be altered to take intoaccount he new parsing rules, and remaining prob-lems of duplication of effort need to be addressed.For instance, the generalised version of the funda-mental rule allows an active edge to combine with aset of inactive dges atisfying its needs in any order.The scoring of errors is another a r~ whichshould be better investigated.
Where extra words areintroduced accidentally into a text, in practice theyare perhaps unlikely to be words that are already inthe lexicon.
Thus when we gave our system sen-tences with known words added, this may not havebeen a fair test.
Perhaps the scoring system shouldprefer added words to be words outside the lexicon,substituted words to substitute for words in opencategories, deleted words to be non-content words,and so on.
Perhaps also the confidence of the systemabout possible substitutions could take into accountwhether a standard spelling corrector can rewrite theacnmi word to a known word of the hypothesisedcategory.
A more sophisticated rror scoring strategycould improve the system's behaviour considerablyfor real examples (it might of course make lessdifference for random examples like the ones in ourexperiments).Finally the behaviour of the approach withrealistic grammars written in more expressive nota-tions needs to be established.
At present, we areinvestigating whether any of the current ideas can beused in conjunction with Allport's (1988) "interest-ing corner" parser.ACKNOWLEDGEMENTSThis work was done in conjunction with theSERC-supported project GR/D/16130.
I amcurrently supported by an SERC Advanced Fellow-ship.REFERENCESAho, Alfred V. and Ullman, Jeffrey D. 1977 Princi-ples of Compiler Design.
Addison-Wesley.Allpo~ David.
1988 The TICC: Parsing InterestingText.
In: Proceedings of the Second ACLConference on Applied Natural LanguageProcessing, Austin, Texas.Anderson, S. O. and Backhouse, Roland C. 1981Locally Least-Cost Error-Recovery inEarley's Algorithm.
ACM TOPIAS 3(3):318-347.Carbonell, Jaime G. and Hayes, Philip J.
1983Recovery Strategies for Parsing108Extragrammafical Language.
A/CL 9(3-4):123-146.Gazdar, Gerald and Mellish, Chris.
1989 Natura/Language Processing in LISP - An Intro-duction to Computational Linguistics.Addison-Wesley.Jensen, Karen, Heidom, George E., Miller, Lance A.and Ravin, Yael.
1983 Parse Fitting andProse Fitting: Getting a Hold on Ill.Formedness.
A/C/, 9(3-4): 147-160.Kay, Matin.
1980 Algorithm Schemata nd DataStructures in Syntactic Processing.Research Report CSL-80-12, Xerox PARC.Pereir& Fernando C. N. and Warren, David I-L D.1980 Definite Clause Grammars forLanguage Analysis - A Survey of the For-malism and a Comparison with AugmentedTransition Networks.
Artifu:ial Intelli-gence 13(3): 231-278.Shieber, Stuart M. 1984 The Design of a ComputerLanguage for Linguistic Information.
InProceedings ofCOLING-84, 362-366.Steedman, Mark.
1987 Combinatow Grammars andHuman Language ~ g .
In: Garfield,J., Ed., Modularity in KnowledgeRepresentation a d Natural Language Pro-ceasing.
Bradford Books/MIT Press.Weischedel, Ralph M. and 5ondheimer.
Norman K.1983 Meta-rules as a Basis for ~ gHI-Formed Input.
AICL 9(3-4): 161-177.Woods, Williant A.
1982 Optimal Search Strategiesfor Speech Understanding Control.Artificial Intelligence 18(3): 295-326.109
