PRACTICAL  PARSING OF  GENERAL IZED PHRASE STRUCTURE GRAMMARSAnthony J. FisherDepartment of Computer ScienceThe University of YorkHeslington, York YO1 5DD, U.K.An efficient algorithm is described for parsing a dialect of generalized phrase structure grammar(GPSG).
A practical parsing system, based on the algorithm, is presented.
The dialect of GPSG whichthe parsing system accepts is smaller, but considerably "purer" (closer to the original definition ofGPSG) and mathematically "cleaner" than that which is accepted by other practical parsing systems.In particular, the parsing system correctly implements feature co-occurrence restrictions, subject onlyto the restriction that the FCR set can be expressed in clausal form as a set of Horn clauses.1 INTRODUCTIONThe generalized phrase structure grammar (GPSG)(Gazdar et al 1985) is one of the most recent, andcurrently one of the most popular, formalisms used bylinguists to describe the syntax of natural (human)languages.
A GPSG is basically a context-free grammar(CFG), whose non-terminals are complex symbolscalled categories.
A category is a set of features.
TheCFG is augmented by:?
a set of conventions or constraints that govern theautomatic "propagation" of features between catego-ries on different nodes of the parse tree; and?
a set of propositions (Boolean formulas whose literalsdenote the presence of a feature in a category) that arerequired to hold for the category on each node of theparse tree.
These propositions are known as featureco-occurrence restrictions (FCRs).A GPSG also contains feature specification defaults(FSDs).
An FSD behaves like an FCR in all respectssave one: if an FSD, when taken in conjunction with theset of FCRs, cannot by the addition of features be madeto hold on a category c on a given node, the FSD issimply ignored.
Although this sounds straightforward,the precise effect of FSDs is most unclear.
The originaldefinition attempts to explain the effect of FSDs mainlyby giving examples, although there are a few mathemat-ical definitions that, however, appear to confuse thedefinition rather than to clarify it.
A clear, formaldefinition of the effect of FSDs is urgently required.Because the present definition is so obscure, it wasreluctantly decided to exclude FSDs from considerationin this paper.Because a GPSG is so closely related to a CFG, itwas thought hat the well-known efficient parsing tech-niques for CFGs could be applied, with modifications,to GPSGs, and that GPSGs would therefore be compu-tationally tractable.
Recently, however, Ristad (1985)has shown that this is not the case, and that theunrestricted GPSG parsing problem is NP-complete (onthe total problem size, viz.
grammar plus input sen-tence).
Even before Ristad's result was known, workersin this field had found the practical problems caused bythe interaction of FCRs, FSDs, and the propagationconventions difficult to surmount.
Briscoe's comments(1986) are typical: "Finally, the concept of privilegedfeature, its interaction with feature specification de-faults and the bi-directionality of the head featureconvention are all so complex that it is debatable howmuch use they would be in a practical system (even ifwe did manage to implement hem)" (p. 1); "Theinteraction of feature co-occurrence restrictions, fea-ture specification defaults and feature propagationproved very hard to implement/understand" (p. 2).This paper does not address the issues of ID/LPparsing and metarules.
Barton (1985) has shown that theID/LP parsing problem is NP-complete (on the totalproblem size).
He argued that a previous result ofShieber (1983), which purported to give a G 2 parsingmethod for ID/LP grammars, was incorrect.
Bartonclaimed that Shieber's algorithm is exponential in theworst case.
Barton's result alone might be considered aCopyright 1989 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/89/010139-148503.00Computational Linguistics, Volume 15, Number 3, September 1989 139Anthony J. Fisher Practiced Parsing of Generalized Phrase Structure Grammarsstrong hint that ID/LP parsing is inappropriate, both asa practical computer-based parsing method and as apossible model for the way in whic\]h people parsesentences.
It is of course probable that, for manygrammars, a sufficiently efficient implementation ofID/LP could be obtained, either by means of a pre-processor or by using Shieber's algorithm, but this hasnot been attempted in the present implementation.
Theomission of ID/LP does not appear seriously to limit theusefulness of the parsing system, at least when appliedto grammars of English.The question of metarules deserves a less cursorydiscussion.
It is known (Gazdar et al ~985:65-67) thatthe addition of metarules to a GPSG does not alter theCF properties of the grammar.
The finite closure rule,which (stated informally) prohibits a metarule fromreprocessing its own output, implies that the addition ofa single metarule to a GPSG can generate only a finitenumber of phrase structure (PS) rules for each PS rule inthe grammar.
However, it is easy to write a metarulethat will match every PS rule in the grammar andgenerate, say, two output rules.
A second metarule canthen be written which does the same.
It is clear,therefore, that in the worst case the size of the inducedset of PS rules grows exponentially with the number ofmetarules.
For reasons put forward by Thompson(1982), most, if not all, GPSG parsing systems handlemetarules by employing a precompilation phase togenerate the induced set of PS rules.
If this method isemployed, the parsing time will, in the worst case, growexponentially with the number of metarules.
In prac-tice, however, exponential behaviour can safely bepresumed to be rare, since a typical metarule "triggers"on only a small number of rules and generates only asmall number of rules.
Preliminary investigations with a"real" grammar of English suggest that the precompi-lation phase generates an output grammar (containingonly PS rules) that is about twice as big as the inputgrammar (which contains metarules and PS rules).The metarules of GPSG are related to the hyperrulesof a van Wijngaarden grammar (VWG) (van Wijngaar-den et al 1976), as has often been observed (see, e.g.,Gazdar et al 1985:65).
There exist various direct meth-ods of parsing VWGs, which do not rely on the priorexpansion of hyperrules to generate a (usually large) setof PS rules.
Wegner's method (1980) and Fisher'smethod (1985) are, unfortunately, exponential in theworst case; however, the exponential behaviour of bothalgorithms tems from the fact that the finite closureproperty of GPSGs does not apply to VWGs, and the setof induced PS rules might be infinite.
It is possible thatthese algorithms could be modified to handle the veryrestricted metarules of GPSG in polynomial time.Besides the omission of FSDs, ID/LP, andmetarules, there is one other respect in which the typeof grammar under consideration differs from GPSG asoriginally described.
For formal purposes, the originalset of feature-propagation conventions seemed ratherbaroque and unduly specialised.
It was felt that it wouldbe preferable to substitute for the "head feature con-vention", the "foot feature principle" and the "controlagreement principle" of the original definition a moregeneral mechanism.
Consequently, we assume through-out most of this paper that features may be specifiedsimply as "percolating" (from a node to its mother) or"trickling" (from a node to its daughter).
The propaga-tion convention (percolating or trickling or neither orboth) can be specified individually for each feature.
It isstressed that this simplification is introduced inorder tosimplify the description of the formalism and of thealgorithm.
In section 4, the restrictions are relaxed, andit is shown how conventions that are closely related tothe HFC, FFP, and CAP of "standard" GPSG (orGPSG 85, as it is known) can be accommodated.Having removed from consideration ID/LP parsing,metarules, and FSDs, and having simplified (tempo-rarily) the feature propagation conventions, we are leftwith a much smaller and more manageable formalism.The main thesis of this paper is that, given theserestrictions, the simple requirement that the FCRs beexpressible as a set of Horn clauses is sufficient oensure parsability in time order pg2G2n 3, where p is ameasure of the degree of ambiguity of the grammar, K isthe size of the alphabet of features, G is a measure of thesize of the grammar, and n is the length of the sentence.The exact meanings ofp and G are made formal ater.
Itshould be pointed out (lest false hopes be raised) that pis not iLn general independent of n, and for some gram-mars p is in fact a worse-than-polynomial function of n,making the algorithm worse-than-polynomial on n in theworst case.
In practice, however, the algorithm can beimplemented quite efficiently in such a way that p - nfor many linguistically plausible grammars.Previous implementations.
There have been several pre-vious attempts to parse GPSGs.
Several workers havemade wholesale changes to the definition of GPSG inorder to make the formalism easier to parse.
The dialectthat the present algorithm accepts is considered to be"purer" and nearer to the original than that accepted bymost other algorithms.
A brief summary of GPSGimplementations is given by Gazdar (1983).
The "of-ficial" definition of GPSG has changed since the list waspublished, and some of the implementations listed areno longer available.
A more recent implementation isthat of Harrison and Maxwell (1986).2 DEVINITIONSPreliminary definitions.
The following definitions, ofstandard terms of formal anguage theory, are given in,for example, Salomaa (1973).An alphabet is a finite non-empty set.
The elementsof an alphabet are called letters.
A word over analphabet V is a finite string consisting of zero or moreletters of V, whereby the same letter may occur140 Computational Linguistics, Volume 15, Number 3, September 1989Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammarsseveral times.
The set of all words over an alphabet Vis denoted by W(V).
For any V, W(V) is infinite.We denote by H(S) the powerset of S, which is the setof all subsets of a set S.Definitions.
A generalised phrase structure grammar(GPSG) is an ordered 7-tuple G = (VF, VT, Xo, R, F, Fe,Fr), where:VF is a finite set of features;V T is a finite set of terminals, VF N VT~ = ~);Xo is the starting category, a finite subset of VF;R is the set of rules, a finite set of ordered pairs PQ, such that P is a subset of V F (i.e.
P ~ H(VF)),and Q is a word over the alphabet V = II(V F) UVT;F is the FCR set, a function from II(VF) to {true,false};F e is the set of percolating features, a subset of VF'~andFar is the set of trickling features, a subset of V e.For P, Q ~ W(V), we say that P derives Q, writtenP f f  Q, iff 3 an integer n and a,/3 ~ W(V), P ' ,  Q'i(i = 1 .
.
.
.
.
n), P", Q"i (i = 1 .
.
.
.
.
n) E V suchthat the following all hold:1. aP ' /3  = Pand aQ'  I Q'2 ?
?
?
Q'n/3 = Q2.
P"--> Q"l Q"2 ?
?
?
Q"n E R3.
\[Extension:\](i) P "C  P '(ii) if Q'i ~ vr: Q"i = Q'i (i = I, 2 .
.
.
.
.
n)i fQ ' iCH(VF) :  Q"i C Q'i (i = 1,2 .
.
.
.
.
n)4.
\[FCR constraints:\]F(P')5.
\[Propagation Constraints:\](i) Q'i ~ VT v (Q'i ('1 Fe) C_ P' (i = 1, 2 .
.
.
.
.
n)(ii) Q'~ E VT v (P' fq FT) C_ Q'i (i = 1, 2 .
.
.
.
.
n).We denote by i f *  the reflexive and transitive closureof ~ .The language generated by G, written L(G), isdefined byL(G) = {P I P ~ W(Vr), X o =>* P}.End of definitions.Informally, the definitions of GPSG, ~,  ~*  andL(G) are the standard definitions of a context-freegrammar, modified by defining the non-terminal of thestandard CFG definition as a set of features.
Further-more, the standard efinition of ~ has been extended totake into account feature matching by the free additionof features to categories pecified in rules (extension),the FCR constraints, and the propagation constraints.The original definition of GPSGs postulated a set ofFCRs whose conjunction is required to hold; this con-junction has been collapsed into a single Boolean func-tion F in our definition.
F is required to hold on eachnon-terminal node of the parse tree by virtue of condi-tion 4 above.
(It is unnecessary to specify that F(Q'i)must hold; this is implied by the use of the reflexive andtransitive closure of ~ in the definition of L(G).
)Finally, condition 5(i) requires that, if P'  is the motherof a non-terminal node Q'i (considering P'  and Q'i asnodes in a parse tree), then for each feature fwh ich  hasbeen defined as percolating from daughter to mother(i.e.
is a member of Fe), if f is present on the daughternode, then it must be present also on the mother.Condition 5(ii) is the corresponding statement for trick-ling features.Our definition is given in terms of features that can beeither present or absent from a category, whereas in theoriginal definition a feature has a value.
This distinctionis merely a mathematical device to simplify the defini-tion and the discussion of the algorithm which willfollow.
Our definition can be related to the originaldefinition by reading " feature"  as "feature-value pair".For example, a " rea l"  GPSG might contain a featurePAST which can take a value which is either + or - .We interpret hat as two separate features, say PAST+and PAST- .
The standard efinition would require thata category may not contain both PAST+ and PAST- .This can be expressed by conjoining the FCR -q(PAST+/~ PAST- )  to the FCR set.
Such an FCR is called agroup FCR.3 A PARSING ALGORITHM FOR GPSGsThe algorithm belongs to the class of algorithms thatobtain a grammar G', variously called a skeleton gram-mar or an underlying grammar, from a given grammar Gand then parse according to G'.
In these algorithms theskeleton grammar G' is chosen such that L(G) C_ L(G'),so, if the parse according to G' fails, the sentence can berejected immediately.
If the parse succeeds, it is neces-sary to check some additional constraints, typically byexamining the parse tree, to ensure that the sentence isindeed acceptable to the more restrictive, given, gram-mar G. The extra checking process typically annotatesthe parse tree with extra information but does notchange its shape.
At the end of the checking process,either the sentence is rejected as not conforming to G,or the sentence is accepted, in which case the annotatedparse tree is the parse tree of the sentence according toG.
Wegner's (1980) algorithm for VWGs belongs to thisclass.In the present algorithm, the skeleton grammar G' isa GPSG that is obtained from a given GPSG G byneglecting some of the FCRs and the percolating featurepropagation constraints.
The skeleton grammar can beparsed by a simple modification of Earley's (1970)algorithm.
The algorithm comprises a precompilationphase, in which the skeleton grammar G' is obtainedfrom the given grammar G, followed by three parse-timephases that are executed one after the other.Precompilation.
Given a GPSGG = (V F, V r, X o, R, F, F e, Far),first define the side-effect-free FCR set F' algorithmi-cally in the following manner.Computational Linguistics, Volume 15, Number 3, September 1989 141Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars1.
Convert F to clausal form, in other words aconjunction of disjunctions of terms, each ofwhich is either an unnegated literal feature or anegated literal feature.
(This can be doneuniquely, apart from questions of ordering ofterms; see, e.g., Loveland (1978:32ff).)2.
Remove from the clause set al clauses (i.e.,disjunctions) that contain one or more unnegatedliterals, leaving behind only those clauses thatcontain only negated literals.The resulting set of clauses represents the side-effect-free FCR set F' .
Since F'  was obtained from F byremoving clauses, the resulting function F'  cannot bemore restrictive than F; in other words, F ~ F'.The reason for removing clauses that contain unne-gated literals is that the evaluation of FCRs can, ingeneral, cause the instantiation of new features on anode.
This can be viewed as a "side effect" of theevaluation, whose primary function is to filter outinadmissible parses.
Side effects are difficult to handle,because they interact with each other and with otheraspects of the grammar, in particular with the propaga-tion constraints.
For example, a new feature added as aresult of a "non-side-effect-free" FCR clause mightcause some other clause, which was satisfied before thenew feature was added, to become false.
This is notpossible, however, if no clause contains an unnegatedliteral: each clause can be satisfied only by the absenceof one or more stated features, and if the requiredfeatures are not absent, the clause yields false m thereis no mechanism in GPSG for removing features from acategory.Now define the skeleton grammar G' of G byG' = (Vp, VT., Xo, R, F', O, FT).Clearly L(G) C_ L(G'), since whenever conditions 4 and5 of section 2 hold for a derivation according to G, theywill also hold for a corresponding derivation accordingto G'.
(Remember that F D F'.)
Informally, G' is morepermissive than G. For the same reason, each parseaccording to G has a corresponding parse according toG', differing only in the distribution of features amongcategories on nodes.
In other words, each parse treeaccording to G is the same "shape"  as some parse tree(of the same sentence) according to G'.Phase 1.
We now parse G' by applying a modified formof Earley's algorithm (Earley 1970; see also Pulman1985, Ritchie and Thompson 1984).
(The reader isassumed to be familiar with Earley's algorithm, inparticular with the r61e played by the predictor in addingnew states to a state set.)
The algorithm is extended sothat it creates a parse tree as the parse progresses.
Amethod for doing this is described briefly by Earley(1970) and in more detail by Earley (1968).There is no need to handle the percolating featurepropagation constraint at this stage, because in G' Fe isempty.
There is no need to consider what happens whenthe evaluation of an FCR causes a new feature to beadded to a category, since the FCR set F'  is side-effect-free.
'iCe can therefore treat G' as a CFG whosenon-terminals are categories, provided that we allow forextension (condition 3 of section 2) and the tricklingfeature; constraint (condition 5(ii)).
This is done in thefollowi~ng way.A category appears on a node by virtue of theappearance of a category cs on the right-hand side of arule R 1 and tlae appearance of a "matching" category c2on the.'
left-lhand side of a rule R 2.
The extensioncondition permits the free addition of features to Cl andto c 2 to generate the category c which appears on thenode.
By the extension condition, c~ C_ c and c 2 C_ c,which implies that c _~ (c~ U Cz).
Now let us neglect forthe moment the trickling feature constraint.
Since weare ignoring the non-side-effect-free FCRs and the prop-agation constraints, any superset of c I L I  c 2 whichsatisfies F'(c) will suffice; consequently, we take thesmallest superset, namely c~ U c2, which is the leastupper bound of c~ and c2 under the ordering relation ofextension (see Gazdar et al 1985:39).Now we consider the trickling feature constraint.The effect of this constraint is to instantiate extrafeatures on certain categories: those features that be-long to a mother category and which are also membersof F~- must be instantiated on each daughter category.The category that is instantiated on the node of theparse tree is the smallest superset of c~ U c2 whichcontains all of its mother's trickling features, which is cnU c z tA (c o fq Fr), where c o is the category of the mothernode.To determine the category to place on a node of theparse tree, therefore, the algorithm needs to know:?
the a priori category c~ on the right-hand side of arule;?
the a priori category c2 on the left-hand side of therule which "matches"  c~;?
the fully-evaluated a posteriori category on themother of the node to which a category is currentlybeing assigned.All of this information is available to the predictor inEarley's algorithm.
This follows from the fact thatEarley's algorithm is "top down",  which means that thefull category on a node is known before any of thatnode's daughters are considered.We now consider how Earley's algorithm can bemodified to parse the skeleton grammar in the manneroutlined above.
A state in Earley's standard algorithmcan be written as X --~ a .
/3 ,  which signifies that thealgoritZhm is considering the rule X --> a/3, and hassuccessfully matched the a with some portion of thesentence being parsed.
The predictor is applied to statesX ---> o, ?
Y/3, which have a non-terminal to the right ofthe dot:.
The predictor adds new states Y --> ?
7 for eachrule Y ~ 3' with matching non-terminal Y.In the new algorithm, a state is written (Co) c --~ a ?/3.142 Computational Linguistics, Volume 15, Number 3, September 1989Anthony J. Fisher Practical Parsing of Generalized Phrase Structure GrammarsAs in the standard algorithm, this signifies that thealgorithm is considering the rule c --~ t~fl, and hassuccessfully matched the ~.
The extra category c ocontains the features that are passed from mother todaughter and which will ultimately appear on a node ofthe parse tree.The predictor in the new algorithm is applied tostates (Co) c --~ t~ ?
Clfl with a non-terminal category tothe right of the dot.
The predictor adds new states(C 1 U C2 U (C O OFr )  ) C 2 ~ ?
')/for each rule c 2 -~ 3' such that F'(c~ U c2 U (Co A FT))holds?
(At first sight it appears that this entails a searchthrough all of the rules of the grammar, but a means ofavoiding a full search is presented in section 4.
)The r61es of the scanner and of the completer inEarley's standard algorithm are unchanged in the newalgorithm?
The initial state that is entered to start theparse is (~) ~ --~ ?
X 0.
The associated "dummy"  rule-~ Xo is not considered part of the grammar, and isexempt from being matched by the predictor.It would be quite possible to use a "bottom up"algorithm in place of Earley's algorithm, in which ther61es of trickling and percolating features would bereversed.
It is not possible to handle both percolatingand trickling features in phase 1, since a provisionaldecision at some point deep down in the parse tree toinstantiate a feature on a certain category would ingeneral cause changes to the membership of categoriesin remote parts of the tree.Phase 2.
The "parse tree" that is generated by Earley'salgorithm is in general not a tree at all; it is a directedgraph?
Besides non-terminal nodes and terminal nodes,the graph will in general contain branching nodes thatpoint to alternative daughters of a non-terminal node.
Itis by this means that multiple parses, arising from anambiguous entence, are represented?
If the degree ofambiguity of the sentence with respect o the skeletongrammar is infinite, the finite graph must representinfinitely many distinct parse trees; in this case thegraph is cyclic.
We assume that the degree of ambiguityis finite, in which case the graph is a directed acyclicgraph (DAG).
A DAG differs from a tree in that whereaseach node in a tree (except he root) has precisely oneparent, a node in a DAG may have more than oneparent?
In other words, a DAG represents commonsub-trees only once; a single sub-tree may be descendedfrom several parents.
DAGs are often used in theconstruction of compilers for computer programminglanguages.Let p be the degree of ambiguity of the skeletongrammar, i.e., the number of distinct parse trees repre-sented by the DAG.
We expand the DAG, generating pdistinct parse trees?
This can easily be done by means ofconventional tree processing techniques, provided thatp is finite, in other words if the graph is acyclic.Phase 3.
Each distinct parse tree is examined in turn.For each tree, sufficient features are added to thecategories on each node of the tree to cause the tree toreflect a parse according to the original GPSG G. Thisentails the evaluation of F and of the propagationconstraints on each category, and the construction of acategory on each node which satisfies all of the con-straints.
Once again, the smallest possible category isconstructed.
That is, if a category c satisfies all of theconstraints, and so does a larger category c U c', wechoose c. It is debatable whether this is the correctbehaviour; some might argue that separate parse treesought to be constructed in which all possible legalextensions are shown.
However, the resulting set ofparse trees would then in general be very large, and it isdifficult to believe that this behaviour is desirable.
Oursmallest category is similar to the most general unifier ofa set of expressions in mathematical logic; as in logic,particular, less general instances can be derived fromthe most general case, but it is the most general (leastfully specified) case that is of most interest.We assume that the FCR set F is expressed in clausalform and that each clause (i.e., each disjunction) is aHorn clause (a clause with either zero or one unnegatedliteral; see, e.g., Loveland (1978:99)).
Number theclauses F l .
.
.
.
.
F M.We denote by M(N) the mother of the node N, if itexists (i.e.
unless N is the root of its tree).
We denote byC(N) the category on the node N.Let the distinct parse trees produced by phase 2 beT I .
.
.
.
.
Tp.
The algorithm unify, defined below, isapplied to each T i in turn, for i = 1 .
.
.
.
.
p.unify (T): Let the non-terminal nodes in T be NI,?
.
., N N.1.2.2.1.2.1.1.2.1.2.2.2.2.2.1.2.2.2.3.3.1.3.1.1.3.1.2.3.1.3.3.1.3.1.3.1.3.2.3.1.3.2.1.3.1.3.2.2.4.5.set again := false;fo r j  = 1 .
.
.
.
.
Ndoi f  Nj is not the root of T and(C(M(~))  n FT) ~ C(~) t l ,  enc (~)  := c(~)  u (C(M(~))  n F~);set again := true;i f  Nj is not the root of T and(C(~)  n Fp) ~ C(~(~) )  thenC(M(~)) := C(M(~))  U (C(~)  n Fp);set again := true;fo r j  = 1 .
.
.
.
, Ndofo r  k = 1 ..... /M r dolet f+ be the set of unnegated literals inlet f_ be the set of negated literals in Fk;f_ ~ C(N 9 theni f  f+ = 0 then  fail;i f  f+ ~ C(Nj) thenc(~)  := c (~)  u f?
;set again := true;t~ again then  go to  step 1;output T.Proof of the algorithm.
First notice that the flag again isset (in steps 2.1.2, 2.2.2, and 3.1.3.2.2) whenever afeature is added to a category that is not already in thatcategory, and at no other time.
Since there are onlyfinitely many features, steps 1 to 4 are repeated onlyfinitely many times, so the procedure terminates.Computational Linguistics, Volume 15, Number 3, September 1989 143Anthony J. Fisher Practical Parsing of Generalized Phrase Structure GrammarsNext observe that on successful termination, again isfalse, so steps 2.1 and 2.2 must have been obeyed foreach node with the conditions in 2.1 and 2.2 false eachtime.
Consequently, on successful termination, thepropagation constraints h01d for each node.Finally, on successful termination, the FCR set Falso holds for each node, for the following reasons.
Step3.1.3 checks the negated literals in the clause F k againstthe category C(Nj.).
If the condition in 3.1.3 is false,there is at least one negated literal in F k which is indeedabsent from C(Nj), so the clause F k is satisfied.
If, onthe other hand, the condition in 3.1.3 is true, none of thenegated literals can possibly be satisfied, since featuresmay not be removed from a category, only added.
SinceFk is a Horn clause, there is at most one unnegatedliteral in F k, so f?
is either empty or has one member.
Iff?
is empty, the clause can not be satisfied, so thealgorithm fails.
If f?
is not empty, the feature is added tothe category if it is not already there, and the clause isthereby satisfied.
The addition of the new feature mightinvalidate previously satisfied clauses or propagationconstraints, o the flag again is set which causes thepropagation constraints and FCR clauses to be checkedafresh.
As noted, the process will eventually terminatewith all propagation constraints and all FCR clausessatisfied, or else the algorithm will fail, in which casethe sentence does not belong to the language generatedby the original grammar G.End of proof.Now consider what would happen if one of theclauses were not a Horn clause.
The algorithm wouldnot know which of the several features from f?
to add instep 3.1.3.2.1 in order to satisfy the clause.
The onlysolution would seem to be to generate copies of theparse tree, and to follow through each choice of featurefrom f?
on a different copy of the parse tree, finallypresenting the user of the parsing system with all of theparse trees.
This would cause a combinatorial explo-sion, since the splitting and copying would have to bedone at each level of the parse tree at which theparticular feature in question is instantiated.The linguistic consequences of the Horn clause re-striction are not clear, but experience with the parsingsystem suggests that they are not severe.
The Hornclause restriction prohibits the grammar writer fromwriting FCRs such as\[PRD +\] A \[VFORM\] D \[VFORM PAS\] v \[VFORM PRP\](Gazdar et al 1985:111), in which a disjunction ofnon-negated literals appears on the right of D .
It is insuch FCRs that the Horn clause restriction appears inits true colours, as a mechanism for curbing a combina-torial explosion or, to put it another way, a mechanismfor prohibiting a source of non-determinism.
If theconsequences of forbidding such FCRs later appear toosevere, the possibility will be investigated ofmoving thenon-determinism from the FCRs into the rules of thegrammar, by replacing an FCR like the one above by anew FCR\[PRI) +~ A \[VFORM\] D \[F\]where F is a new feature, and adding appropriate rulesto the grammar.
The details of this have yet to beworked out; it is presented as a possible solution to aproblem that has not yet arisen.Time and space bounds.
The following parameters arerelevant to a consideration ftime and space bounds forthe algorithm:?
p, the degree of ambiguity of the skeleton grammar;?
K, the cardinality of Vr;?
G, the number of rules in the grammar;?
n, the length of the sentence being parsed.Earley's algorithm, as is well known, operates intime order G2n 3.
Earley's proof of the time complexityof his algorithm (Earley 1970) is in no way affected bythe elaboration of the predictor to handle feature match-ing.
In particular, the number of states in a state setdoes not increase with K. Although K features may inprinciple be combined to construct 2K different catego-ries, the algorithm generates new categories by exten-sion only when they are required.
In fact, if a newfeature specification is added to a rule that is previouslyunspecified for that feature, the state sets will eitherremain the same size or become smaller, since adding afeature restricts the range of rules that the rule inquestion will "match".
Speaking informally, it is under-specified rules that cause the problems; the more fullyspecified are the rules, the closer is the GPSG to a CFG,and the fewer are the states that are needed.The factor K does, however, enter into the timebound for phase 1 in the following manner.
Although thenumber of "primitive steps" (Earley's terminology)that are executed by the modified algorithm is indepen-dent of K, the time taken to complete certain primitivesteps, in particularly the addition of a state to a state setand the feature matching operation in the predictor, isproportional to K. The overall time bound is thereforeKG2n 3,,The expansion of the DAG to yield p distinct parsetrees can be done by conventional tree processingtechniques in time proportional to p, the number ofnodes in each tree, and the size of a node (which affectsthe time taken to copy a node).
This gives a bound oforder pKGn 2 for phase 2.The algorithm unify contains three nested loops(steps 3 and 3.1, and the again loop).
An upper boundon the number of nodes is a constant times Gn 2, and anupper bound on the number of times round the againloop i,; the number of features, K. To simplify theanalysis, we take M - G. (Formally, we define G to bethe sum of the number of rules in the grammar and thenumber of clauses in the FCR set.)
Moreover, the setoperaffon C in step 3.1.3 can realistically be expected to144 Computational Linguistics, Volume 15, Number 3, September 1989Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammarstake time proportional to K, although the operationsinvolving f+ can be done in constant time, since f+ haseither zero or one member.
A time bound for unify istherefore g2G2n 2.
Finally, unify is obeyed p times,which gives a time bound for phase 3 of order pK2G2n 2.It is unfortunate that, as noted earlier, p is not ingeneral independent of n. To see why this is so, con-sider the following grammar.Non-terminal lphabet: {S}Terminal alphabet: {a}Rules: S---> S SS ----> aIt has been shown (Church and Patil 1982) that thenumber of distinct rees generated, for a sentence oflength n, grows factorially with n. This means that thealgorithm as a whole will take factorial time to parse asentence of length n according to this grammar.
This isa matter of concern, because constructions similar tothis example are commonly used to handle coordina-tion.
It is even possible in principle for p to be infinite,in which case the algorithm will not terminate (althoughthe advertised time bound ofpK2G2n 3will still hold!).
Inpractice, however, no grammar has been encounteredwhich unavoidably has infinite p. (Self-referential rulesof the form X ----> X have occasionally appeared, butthese were always traced to an error in the grammar.
)Considerable effort has been expended in an attemptto improve the theoretical worst-case performance ofthe algorithm when p is a finite valued but rapidlyincreasing function of n. It might be possible to combinephases 2 and 3, employing "lazy evaluation" (a tech-nique often used in functional programming) toexpandthe DAG only when necessary.
If this were done, muchof the DAG might remain unexpanded, with consequentsavings in time and space.
The problem with thisapproach is that some features are required to percolateright to the root of their tree, and a given branchingpoint might have different (and incompatible) featurespercolating to it from each of its alternative descen-dants.
It turns out to be often necessary to expand theDAG all the way back to the root, in which case little issaved by using lazy evaluation.
It is worth pointing outthat, in cases (such as the example) in which thealgorithm is least efficient, the output is often verylarge, consisting of many parse trees.
In many (but notall) of these cases, the time taken is asymptoticallylinear in the length of the output, i.e.
the number ofnodes in the set of parse trees displayed.
Surely, noalgorithm can ever behave sublinearly on the length ofits output.
Furthermore, as discussed later, in cases inwhich this problem does not arise, the execution time isdominated by phase 1.
We therefore have an algorithmthat:?
behaves as well as one of the best general CF parsingalgorithms, for all unambiguous grammars and formany ambiguous grammars;?
takes time that is linear in the length of the output forsome "problem" grammars; and?
takes a very long time in a small number of reallyawkward cases.The time bounds for the three phases are KG2n 3,pKGn 2, and pK2G2n 2.
This gives an overall worst-casetime bound of order pK2G2n 3.The space bound is of order pKGn 3 in the worst case,for the following reasons.
Earley's algorithm requiresspace proportional to KGn 2 to hold the states, n for thestate sets (that is, the list-processing overhead), andKGn 3 for the DAG.
The grammar itself requires paceproportional to KG.
The p distinct parse trees requirespace proportional to KGn 2 each.
Phase 3 does notrequire any working storage.
The worst-case spacebound is therefore of order pKGn 3.4 IMPLEMENTATIONA practical GPSG parsing system has been constructed,based on the algorithm just described.
The systemcomprises a table generator and a parser.
The systemwas originally written in the programming languageBCPL, and ran on a VAX 780 computer under the Unixoperating system.
The system has recently been re-implemented in C to run on a Sun 3/50 workstation.
TheSun version generally runs several times faster than theVAX version.
The parse times given below relate to theslower VAX implementation.The table generator performs the precompilationphase of the algorithm.
It generates a tabular epresen-tation of the skeleton grammar, which the parser caninterpret more efficiently than it could the "raw" rules,and it converts the FCR set into clausal form.
The tablegenerator also performs various checks to ensure, as faras possible, that the grammar is well formed.
Besidesthe obvious syntactic hecks (to detect such errors as acomma in the wrong place), the table generator checksthat the FCR set is not identically false, that there are noobvious blind alleys or non-reachable categories (this isnot checked rigorously), and that various other subtle"well-formedness" conditions are satisfied.
This errorchecking has proved very useful in practice, sinceGPSGs are notoriously difficult o debug.The input grammar is written in the notation ofGazdar et al (1985), with a few concessions to thelimitations of the typical computer input device.
Inparticular, features have values, and what we havereferred to as a feature is, in the notation accepted bythe table generator, a feature-value pair, written If v\].Each distinct feature value pair is associated by thetable generator with a particular bit in a computer word.A category is represented bya set of bits, i.e., by a wordwith several bits set, one for each feature-value pair inthe set.
Category valued features correspond to trees,and a distinct bit is allocated to each terminal node ofthe tree.
For example, the feature PAST, with twoComputational Linguistics, Volume 15, Number 3, September 1989 145Anthony J. Fisher \]Practical Parsing of Generalized Phrase Structure Grammarsvalues + and - ,  would have two bits allocated to it, andfor a category valued feature SLASH, the values \[N +,V - \ ]  and \ [N- ,  V +\] would be allocated four bits.
Notethat, in any given grammar, the depth of the treeinduced by a category valued feature is finite; further-more, the range of possible values of a category valuedfeature is known at table generation time, so it is knownat this stage how many bits to allocate to the feature.The representation f feature-value pairs by bit posi-tions in a computer word allows the very efficientlogical instructions of the computer (N,  t.J, -7 ), whichoperate on a whole word of bits at a time, to be used.As explained earlier, a feature in GPSG 85 may takeat most one value at a time, since a GPSG 85 feature isin fact a function.
This restriction is expressed byconjoining an FCR, known as a group FCR, to the FCRset.
For example, if the grammar contains the two-valued feature PAST referred to above, the FCR-a(\[PAST +\] /k  \[PAST -\])would be conjoined to the FCR set.
In general, thepresence of an n-valued feature f, with values v I .
.
.
.
.vn, entails the addition of the FCRs""l(\[f Vii /~ If Vj'\]) for each i, j = 1 .
.
.
.
.
n, i < j.When converted to clausal form these FCRs become then(n- 1)/2 clauses~\[f vi\] ~/"7\[\]" vj\] for each i, j = 1 .
.
.
.
.
n, i < jwhose inclusion in the set of clauses presented to theparser would make the table very large.
Consequently,these group clauses are abbreviated.
For each n-valuedfeature f with n - 2, a group mask is included in theparser table which has one bit set for each feature-valuepair whose conjunction is to be prohibited.
The parserchecks these group masks whenever it consults the FCRclause set.
If g is a group mask and c is a maskrepresenting a category, the parser has only to checkthat (g N c) has not more than one bit set.It has been observed that, in practice, it is likely thatthe explicit FCR set supplied by the grammar writer willcontain mostly non-side-effect-free clauses.
However,the (notional) group FCRs are, by definition, side-effect-free.
Because of this, the algorithm is modifiedfor implementation in the following way.
The FCR setF' which is used in the definition of the skeletongrammar is taken to be just the set of notional groupFCRs; any "genuine" FCRs, be they side-effect-free ornot, are excluded from F'.
Furthermore, the tablegenerator ensures that the full FCR set F is satisfiedoneach node at table generation time.
For example, if thegrammar contains the FCR\[NOM\] 3 \[NFORM NORM\],then a category \[NOM +\] occurring in a phrase struc-ture rule would be rewritten by the table generator as\[NOM +~, NFORM NORM\].
These modifications in-crease the efficiency of the implementation, and enablecertain errors to be detected at table generation time.In practice, most GPSGs closely resemble traditionalCFGs, with most categories fully specified for the"major" features N, V, and perhaps BAR.
Conse-quently, the group FCR constraints ensure that theskeleton grammar also resembles a traditional CFG, andis certainly not, in practice, massively ambiguous.
In-deed, the table generator insists that categories in rulesare written as X\[Y\], where X is a name (a traditionalnon-terminal), and Y is a category.
The non-terminal Xis defined (by the grammar writer) to stand for some setof major features.
This convention is perhaps contro-versial, but Gazdar et al (1985) is full of such rules, andthe linguists who use the parsing system have notgrumbled yet.
The convention does allow the tablegenerator to check the grammar more stringently thanwould otherwise be the case, and it enables the parser tobe made considerably more efficient, by dividing the setof all categories (which must be searched by the predic-tor) into disjoint subsets.
The convention has no theo-retical significance; the program would work without it.The head feature convention.
The grammar writer is ableto denote certain on-terminals on the right-hand side ofa rule as head non-terminals, which correspond to thehead symbols of traditional X-bar syntax.
This is doneby prefixing the name of the non-terminal in the rule bya star.
The percolation and trickling of features can berestricted to occur only between a mother and a headdaughter.
There are thus nine possible propagationbehaviours for any feature:one ofnot tricklingtrickling, but only to head daughterstrickling to all daughterstogether with one ofnot percolatingpercolating, but only from head daughterspercolating from any daughterThe head feature convention is simulated by defininghead fi~atures to trickle, but only to heads.
This isadequate in most situations, but it falls short of thebehaviour postulated by Gazdar et al (1985:94ff).
Inparticular, the notion of free feature specification sets isnot accommodated.
This causes problems in, for exam-ple, the treatment of conjunctions, in which the con-joined constituents are conventionally all heads.
GPSG85 allows a rule that in our notation would be writtenNP: *NP \[CONJ and\], *NP \[CONJ NIL\].In the present implementation, any PER feature (forexample) which happens to be present on the motherwould trickle to both head daughters, thereby forcingagreement between the daughters.
Our solution hasbeen to make the daughters non-heads, which is unat-tractive;, but which has been made to work.The foot feature principle.
The foot feature principle ismore of a problem than the head feature convention.
It146 Computational Linguistics, Volume 15, Number 3, September 1989Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammarsis clear that foot features ought to percolate, but thesituation is more complicated than this.
In a rule such asS: NP, S \[SLASH NP\]the SLASH feature (which is a foot feature) must beprevented from percolating from the node that is gen-erated by extension from the right-hand-side S. This isachieved by forbidding any feature that has been de-clared to be a foot feature (e.g., SLASH and WH) topercolate from a node on which the feature appears byvirtue of its appearance on the right-hand side of aphrase structure rule.
This is easy to implement.This is only a partial solution to the problem, how-ever.
The rule given above correctly generatesthe telephone Carol tested.It is not possible, however, by this mechanism toprevent* the telephone Carol tested the telephone,in which "Carol tested the telephone" is correctlyparsed as an S, but in which a SLASH NP specificationis "gratuitously" instantiated in order to satisfy theextension conditions imposed by the rule given above.To solve this and other problems, a tree is now definedto be admissible only if each non-terminal node of thetree satisfies the foot condition, which is related to theoriginal FFP of GPSG 85.
The foot condition is definedas follows.Define a lexical node of a parse tree as a node thatimmediately dominates a terminal node.
(A gap, whichis explicitly denoted in the grammar by the word GAP,is a terminal node.)
Define an interior node as a nodethat is neither terminal nor lexical.
An interior node issaid to meet he foot condition (FC) iffeach foot featurethat it contains appears also on at least one daughterfrom which it can legally percolate.
A lexical node issaid to meet he FC iff each foot feature that it containsappears also on the left-hand side of the lexical rule thatgave rise to the lexical node.This definition implies that the FC cannot cause theinstantiation of any features.
In this respect, the FCdiffers from the propagation conventions, which add thenecessary features to make the conditions hold.
The FCmechanism operates on the tree as it is after FCRs andpropagation conditions have been enforced.
It does notalter the tree; it merely checks that the foot condition istrue on each node.
Note that all of this follows from thedefinition.
It is not necessary to put forward a proce-dural definition of the FC, which would fit ill with thenon-procedural definition of GPSG.
In contrast o theGPSG 85 FFP, the FC readily permits a straightfor-ward, efficient, and deterministic implementation.The control agreement principle.
A mechanism has beenprovided for specifying horizontal propagation of fea-tures in a way similar to that implied by the controlagreement principle of Gazdar et al (1985).
Sistercategories ina rule may be designated control sisters (byprefixing the name of the non-terminal by a dollar).
Aset of control features is defined by the grammar writer,analogous to the sets of trickling and percolating fea-tures.
Each non-terminal node N has associated with itan extra node N'.
If a node No has daughters NI .
.
.
.
.N n, then N o' is called the stepmother of each N l .
.
.
.
.Nn.
If N i is a control sister, then any control features inC(N;) are required to percolate to the stepmother, andany features on the stepmother a e required to trickle toeach stepdaughter that is a control sister.
The effect isthat control features present on a control sister areforced to appear on each other control sister (which hasthe same mother).One consequence of this modified CAP is that agree-ment is mutual, or bidirectional, whereas in the CAP ofGPSG 85 it is unidirectional.
Another consequence isthat, in the present implementation, it is impossible bythese means to express agreement between (for exam-ple) the daughter NP and the NP "under the slash" inS: NP, S \[SLASH NP\].This has not yet proved to be a problem; such agree-ment can easily be accommodated by defining appro-priate propagation constraints for those features (suchas PER, PLU and NFORM) that must agree.Metarules.
Despite the misgivings expressed earlierconcerning the possible xponential growth in grammarsize, a form of metarule mechanism has been incorpo-rated.
Metarules are implemented by precompilation bythe table generator.
In fact, there is a separate metarulepreprocessing program, called metagee, which runs as aUnix filter, passing the expanded set of rules to the tablegenerator proper.
It would be possible to process sep-arated ID/LP rules by means of a similar preprocessor.This has not been done.Form of the parser table.
The output from the tablegenerator, the table which is interpreted by the parser,comprises:?
an encoded list of rules, with a pointer from eachoccurrence of a non-terminal on the right-hand side ofa rule to a list of rules with matching left-hand sidenon-terminals;?
an encoded lexicon;?
a list of non-terminal names, feature names andfeature value names;?
a set of group masks;?
a set of FCR clauses, each comprising two bit masks.One mask (word of bits) represents the category f?and the other represents he category f_.Performance.
The parsing system has been tested with agrammar for a subset of English.
The grammar contains512 rules after metarule expansion, comprising 228non-lexical rules and 284 lexical rules.
There are 107feature value pairs.
There are 18 FCRs, which, whenconverted to clausal form, yield only 39 clauses.
Thesize of the parser table is about 63,000 bytes, about 93%of which is occupied by the encoded rules.
The remain-ing 7% (4,500 bytes) comprises the tables of bit masksComputational Linguistics, Volume 15, Number 3, September 1989 147Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammarsthat represent the FCRs and the propagation masks, atable of non-terminal names, and the lexicon.
The tablegenerator takes about two minutes to compile thegrammar.Typical parse times are given in figure 1.
As the tableLength ofsentence Parse time (seconds)(words) Phase 1 Phase 2+3 Total3 0.8 0.6 1.46 2.3 1.1 3.49 3.9 2.3 6.212 3.6 2.9 6.515 4.9 7.9t 12.8t 4.2s excluding the time taken to format and print he treesFigure 1.shows, for simple short sentences (unambiguous sen-tences of fewer than 15 words), phase 1 consistentlytakes more time than phases 2 and 3 together.
Forsentences of moderate ambiguity, the times for phase 1and phase 2+3 are comparable.
The 15-word sentencefor which a time is given in the table iswhich number ought Carol to have dialed on thetelephone the happy engineer was testing?which, the parser correctly reports, is ambiguous (it hastwo parses).
Phase 1 yields a DAG that represents fourparses.
Phase 2 expands this into four distinct rees, twoof which are then ruled out by phase 3.
The figures forphase 2+3 include the t ime taken to format and print thetrees, which for the longer sentences i not insignif icant,amount ing to almost half of  the processing time for the15-word sentence.ACKNOWLEDGMENTSThe parsing system described in this paper was developed for use ina research project that is funded by British Telecom ResearchLaboratories (Rl8).I am grateful to C.J.
Cullen and S.J.
Harlow for their comments onthis paper at various stages of revision.REFERENCESBarton, G. Edward, Jr. 1985 On the complexity of ID/LP parsing.Computational Linguistics !1(4): 205-218.Briscoe, Edward J.
1986 Grammar Development Environment.
Inter-nal report, Department of Linguistics and Modern English Lan-guage, University of Lancaster, U.K.Church, Kenneth and Patil, Ramesh 1982 Coping with syntacticambiguity or how to put the block in the box on the table.Computational Linguistics 8(3-4): 139-149.Earley, Jay 1968 An efficient context-free parsing algorithm.
Ph.D.thesis, Department of Computer Science, Carnegie-Mellon Uni-versity, Pittsburgh, PA.Earley, Jay 1970 An efficient context-free parsing algorithm.
Commu-nications of the Association for Computing Machinery 13(2):94-102.Fisher, Anthony J.
1985 Practical LL(1)-based parsing of van Wijn-gaarden grammars.
Acta Informatica 21: 559-584.Gazdar, Gerald 1983 Recent computer implementations of phrasestructure grammars.
Internal report, Cognitive Studies Pro-gramme, University of Sussex, U.K.Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan1985 Generalized Phrase Structure Grammar.
Basil Blackwell,Oxford, U.K.Harrison and Maxwell 1986 A New Implementation f r GPSG.
Proc.6th Canadian Conf.
on A.I.
(CSCSI-86), May 21-23, i~cole Poly-technique de Montreal, Montreal, Quebec: 78-83.Loveland, Donald W. 1978 Automated Theorem Proving: a LogicalBasis.
North-Holland, Amsterdam, the Netherlands.Pulman, S.G. 1985 Generalised phrase structure grammar, Earley'salgorithm, and the minimisation of recursion.
In: Sparck Jones,Karen; and Wilks, Yorick (eds.)
1985 Automatic Natural Lan-guage Parsing.
Ellis Horwood, Chichester, U.K.Ristad, Eric S. 1985 GPSG recognition is NP-hard.
Artificial Intelli-gence Memo no.
837, MIT Artificial Intelligence Laboratory,Cambridge, MA.Salomaa, Arto 1973 Formal Languages.
Academic Press, London,Enghmd.Shieber, Stuart M. 1983 Direct parsing of ID/LP grammars.
Technicalrepot~ 291R, SRI International, Menlo Park, CA.
Also in: Linguis-tics and Philosophy 7(2): 135-154.Thompson, Henry S. 1982 Handling metarules in a parser for GPSG.Research paper no.
175, Department of Artificial Intelligence,University of Edinburgh, U.K. Also in: Barlow, M.; Flickinger,D.
; and Sag, I.A.
(eds.)
Developments in Generalized PhraseStructure Grammar.
Stanford Working Papers in GrammaticalTheory 2: 26-37.
Indiana University Linguistics Club, U.S.A.Ritchie, Graeme; and Thompson, Henry S. 1984 Natural anguageprocessing, ln: O'Shea, Tim and Eisenstadt, Marc 1984 ArtificialIntelligence: Tools, Techniques and Applications.
Harper andRow, New York, NY.Wegner, L.M.
1980 On parsing two-level grammars.
Acta Informatica14: 175-193.Wijngaarden, A. van; Mailloux, B.J.
; Peck, J.E.L.
; Koster, C.H.A.
;Sintzoff, M.; Lindsey, C.H.
; Meertens, L.G.L.T.
; and Fisker,R.G.
(eds.)
1976 Revised Report on the Algorithmic LanguageAlgol 68.
Springer-Verlag, Berlin, W. Germany.148 Computational Linguistics, Volume 15, Number 3, September 1989
