Coling 2010: Poster Volume, pages 1032?1040,Beijing, August 2010Designing Agreement Features for Realization RankingRajakrishnan Rajkumar and Michael WhiteDepartment of LinguisticsThe Ohio State University{raja,mwhite}@ling.osu.eduAbstractThis paper shows that incorporating lin-guistically motivated features to ensurecorrect animacy and number agreement inan averaged perceptron ranking model forCCG realization helps improve a state-of-the-art baseline even further.
Tradition-ally, these features have been modelled us-ing hard constraints in the grammar.
How-ever, given the graded nature of grammat-icality judgements in the case of animacywe argue a case for the use of a statisti-cal model to rank competing preferences.Though subject-verb agreement is gener-ally viewed to be syntactic in nature, a pe-rusal of relevant examples discussed in thetheoretical linguistics literature (Kathol,1999; Pollard and Sag, 1994) points to-ward the heterogeneous nature of Englishagreement.
Compared to writing gram-mar rules, our method is more robust andallows incorporating information from di-verse sources in realization.
We also showthat the perceptron model can reduce bal-anced punctuation errors that would other-wise require a post-filter.
The full modelyields significant improvements in BLEUscores on Section 23 of the CCGbank andmakes many fewer agreement errors.1 IntroductionIn recent years a variety of statistical models forrealization ranking that take syntax into accounthave been proposed, including generative mod-els (Bangalore and Rambow, 2000; Cahill andvan Genabith, 2006; Hogan et al, 2007; Guo etal., 2008), maximum entropy models (Velldal andOepen, 2005; Nakanishi et al, 2005) and averagedperceptron models (White and Rajkumar, 2009).To our knowledge, however, none of these mod-els have included features specifically designed tohandle grammatical agreement, an important taskin surface realization.
In this paper, we show thatincorporating linguistically motivated features toensure correct animacy and verbal agreement inan averaged perceptron ranking model for CCGrealization helps improve a state-of-the-art base-line even further.
We also demonstrate the utilityof such an approach in ensuring the correct pre-sentation of balanced punctuation marks.Traditionally, grammatical agreement phenom-ena have been modelled using hard constraintsin the grammar.
Taking into consideration therange of acceptable variation in the case of ani-macy agreement and facts about the variety of fac-tors contributing to number agreement, the ques-tion arises: tackle agreement through grammarengineering, or via a ranking model?
In ourexperience, trying to add number and animacyagreement constraints to a grammar induced fromthe CCGbank (Hockenmaier and Steedman, 2007)turned out to be surprisingly difficult, as hard con-straints often ended up breaking examples thatwere working without such constraints, due to ex-ceptions, sub-regularities and acceptable variationin the data.
With sufficient effort, it is conceiv-able that an approach incorporating hard agree-ment constraints could be refined to underspec-ify cases where variation is acceptable, but evenso, one would want a ranking model to capturepreferences in these cases, which might vary de-pending on genre, dialect or domain.
Given that1032a ranking model is desirable in any event, we in-vestigate here the extent to which agreement phe-nomena can be more robustly and simply handledusing a ranking model alone, with no hard con-straints in the grammar.We also show here that the perceptron modelcan reduce balanced punctuation errors that wouldotherwise require a post-filter.
As White and Ra-jkumar (2008) discuss, in CCG it is not feasibleto use features in the grammar to ensure that bal-anced punctuation (e.g.
paired commas for NP ap-positives) is used in all and only the appropriateplaces, given the word-order flexibility that cross-ing composition allows.
While a post-filter is areasonably effective solution, it can be prone tosearch errors and does not allow balanced punctu-ation choices to interact with other choices madeby the ranking model.The starting point for our work is a CCG re-alization ranking model that incorporates Clark &Curran?s (2007) normal-form syntactic model, de-veloped for parsing, along with a variety of n-gram models.
Although this syntactic model playsan important role in achieving top BLEU scoresfor a reversible, corpus-engineered grammar, anerror analysis nevertheless revealed that many er-rors in relative pronoun animacy agreement andsubject-verb number agreement remain with thismodel.
In this paper, we show that features specif-ically designed to better handle these agreementphenomena can be incorporated into a realizationranking model that makes many fewer agreementerrors, while also yielding significant improve-ments in BLEU scores on Section 23 of the CCG-bank.
These features make use of existing corpusannotations ?
specifically, PTB function tags andBBN named entity classes (Weischedel and Brun-stein, 2005) ?
and thus they are relatively easy toimplement.1.1 The Graded Nature of AnimacyAgreementTo illustrate the variation that can be found withanimacy agreement phenomena, consider first an-imacy agreement with relative pronouns.
In En-glish, an inanimate noun can be modified by a rel-ative clause introduced by that or which, while ananimate noun combines with who(m).
With somenouns though ?
such as team, group, squad, etc.?
animacy status is uncertain, and these can befound with all the three relative pronouns (who,which and that).
Google counts suggest that allthree choices are almost equally acceptable, as theexamples below illustrate:(1) The groups who protested against plans toremove asbestos from the nuclear subma-rine base at Faslane claimed victory whenit was announced the government intendsto dispose of the waste on site.
(The Glas-gow Herald; Jun 25, 2010)(2) Mr. Dorsch says the HIAA is work-ing on a proposal to establish a privatelyfunded reinsurance mechanism to helpcover small groups that ca n?t get insur-ance without excluding certain employees.
(WSJ0518.35)1.2 The Heterogeneous Nature of NumberAgreementSubject-verb agreement can be described as a con-straint where the verb agrees with the subject interms of agreement features (number and person).Agreement has often been considered to be a syn-tactic phenomenon and grammar implementationsgenerally use syntactic features to enforce agree-ment constraints (e.g.
Velldal and Oepen, 2005).However a closer look at our data and a surveyof the theoretical linguistics literature points to-ward a more heterogeneous conception of Englishagreement.
Purely syntactic accounts are prob-lematic when the following examples are consid-ered:(3) Five miles is a long distance to walk.
(Kim, 2004)(4) King prawns cooked in chili salt and pep-per was very much better, a simple dishsucculently executed.
(Kim, 2004)(5) ?
I think it will shake confidence one moretime , and a lot of this business is based onclient confidence .
?
(WSJ1866.10)(6) It ?s interesting to find that a lot of the ex-pensive wines are n?t always walking outthe door .
(WSJ0071.53)1033In Example (3) above, the subject and deter-miner are plural while the verb is singular.
In(4), the singular verb agrees with the dish, ratherthan with individual prawns.
Measure nouns suchas lot, ton, etc.
exhibit singular agreement withthe determiner a, but varying agreement with theverb depending on the head noun of the measurenoun?s of -complement.
As is also well known,British and American English differ in subject-verb agreement with collective nouns.
Kathol(1999) proposes an explanation where agreementis determined by the semantic properties of thenoun rather than by its morphological properties.This accounts for all the cases above.
In the lightof this explanation, specifying agreement featuresin the logical form for realization could perhapssolve the problem.
However, the semantic viewof agreement is not completely convincing due tocounterexamples like the following discussed inthe literature (reported in Kim (2004)):(7) Suppose you meet someone and they aretotally full of themselves(8) Those scissors are missing.In Example (7), the pronoun they used in ageneric sense is linked to the singular antecedentsomeone, but its plural feature triggers pluralagreement with the verb.
Example (8) illustrates asituation where the subject scissors is arguably se-mantically singular, but exhibits plural morphol-ogy and plural syntactic agreement with both thedeterminer as well as the verb.
Thus this suggeststhat English has a set of heterogeneous agree-ment patterns rather than purely syntactic or se-mantic ones.
This is also reflected in the pro-posal for a hybrid agreement system for English(Kim, 2004), where the morphology tightly in-teracts with the system of syntax, semantics, oreven pragmatics to account for agreement phe-nomena.
Our machine learning-based approachapproximates the insights discussed in the theoret-ical linguistics literature.
Writing grammar rulesto get these facts right proved to be surprisinglydifficult (e.g.
discerning the actual nominal headcontributing agreement feature in cases like areasof the factory were/*was vs. a lot of wines are/*is)and required a list of measure nouns and parti-tive quantifiers.
We investigate here the extentto which a machine learning?based approach is asimpler, practical alternative for acquiring the rel-evant generalizations from the data by combininginformation from various information sources.The paper is structured as follows.
Section 2provides CCG background.
Section 3 describesthe features we have designed for animacy andnumber agreement as well as for balanced punc-tuation.
Section 4 presents our evaluation of theimpact of these features in averaged perceptron re-alization ranking models, tabulating specific kindsof errors in the CCGbank development section aswell as overall automatic metric scores on Sec-tion 23.
Section 5 compares our results to thoseobtained with related systems.
Finally, Section 6concludes with a summary of the paper?s contri-butions.2 Background2.1 Surface Realization with CombinatoryCategorial Grammar (CCG)CCG (Steedman, 2000) is a unification-based cat-egorial grammar formalism which is defined al-most entirely in terms of lexical entries that en-code sub-categorization information as well assyntactic feature information (e.g.
number andagreement).
Complementing function applicationas the standard means of combining a head with itsargument, type-raising and composition supporttransparent analyses for a wide range of phenom-ena, including right-node raising and long dis-tance dependencies.
An example syntactic deriva-tion appears in Figure 1, with a long-distancedependency between point and make.
Seman-tic composition happens in parallel with syntacticcomposition, which makes it attractive for gener-ation.OpenCCG is a parsing/generation library whichworks by combining lexical categories for wordsusing CCG rules and multi-modal extensions onrules (Baldridge, 2002) to produce derivations.Conceptually these extensions are on lexical cate-gories.
Surface realization is the process by whichlogical forms are transduced to strings.
OpenCCGuses a hybrid symbolic-statistical chart realizer(White, 2006) which takes logical forms as in-put and produces sentences by using CCG com-1034He has a point he wants to makenp sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np> >T >Bnp s/(s\np) sto\np/np>Bsdcl\np/np>Bsdcl/npnp\np<np>sdcl\np<sdclFigure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [.
.
.
]aa1heh3he h2<Det><Arg0> <Arg1><TENSE>pres<NUM>sg<Arg0>w1 want.01m1<Arg1><GenRel><Arg1><TENSE>presp1pointh1have.03make.03<Arg0>s[b]\np/npnp/nnpns[dcl]\np/nps[dcl]\np/(s[to]\np)npFigure 2: Semantic dependency graph from theCCGbank for He has a point he wants to make[.
.
.
], along with gold-standard supertags (cate-gory labels)binators to combine signs.
Edges are groupedinto equivalence classes when they have the samesyntactic category and cover the same parts ofthe input logical form.
Alternative realizationsare ranked using integrated n-gram or perceptronscoring, and pruning takes place within equiva-lence classes of edges.
To more robustly supportbroad coverage surface realization, OpenCCGgreedily assembles fragments in the event that therealizer fails to find a complete realization.To illustrate the input to OpenCCG, considerthe semantic dependency graph in Figure 2.
Inthe graph, each node has a lexical predication(e.g.
make.03) and a set of semantic features(e.g.
?NUM?sg); nodes are connected via depen-dency relations (e.g.
?ARG0?).
(Gold-standard su-pertags, or category labels, are also shown; seeSection 2.2 for their role in hypertagging.)
In-ternally, such graphs are represented using Hy-brid Logic Dependency Semantics (HLDS), adependency-based approach to representing lin-guistic meaning (Baldridge and Kruijff, 2002).
InHLDS, each semantic head (corresponding to anode in the graph) is associated with a nominalthat identifies its discourse referent, and relationsbetween heads and their dependents are modeledas modal relations.For our experiments, we use an enhanced ver-sion of the CCGbank (Hockenmaier and Steed-man, 2007)?a corpus of CCG derivations derivedfrom the Penn Treebank?with Propbank (Palmeret al, 2005) roles projected onto it (Boxwell andWhite, 2008).
Additionally, certain multi-wordNEs were collapsed using underscores so that theyare treated as atomic entities in the input to therealizer.
To engineer a grammar from this cor-pus suitable for realization with OpenCCG, thederivations are first revised to reflect the lexical-ized treatment of coordination and punctuation as-sumed by the multi-modal version of CCG that isimplemented in OpenCCG (White and Rajkumar,2008).
Further changes are necessary to supportsemantic dependencies rather than surface syntac-tic ones; in particular, the features and unifica-tion constraints in the categories related to seman-tically empty function words such complemen-tizers, infinitival-to, expletive subjects, and case-marking prepositions are adjusted to reflect theirpurely syntactic status.10352.2 HypertaggingA crucial component of the OpenCCG realizer isthe hypertagger (Espinosa et al, 2008), or su-pertagger for surface realization, which uses amaximum entropy model to assign the most likelylexical categories to the predicates in the inputlogical form, thereby greatly constraining the real-izer?s search space.1 Category label prediction isdone at run-time and is based on contexts withinthe directed graph structure as shown in Figure 2,instead of basing category assignment on linearword and POS context as in the parsing case.3 Feature DesignThe features we employ in our baseline perceptronranking model are of three kinds.
First, as in thelog-linear models of Velldal & Oepen and Nakan-ishi et al, we incorporate the log probability of thecandidate realization?s word sequence accordingto our linearly interpolated language models as asingle feature in the perceptron model.
Since ourlanguage model linearly interpolates three com-ponent models, we also include the log prob fromeach component language model as a feature sothat the combination of these components can beoptimized.
Second, we include syntactic featuresin our model by implementing Clark & Curran?s(2007) normal form model in OpenCCG.
The fea-tures of this model are listed in Table 1; theyare integer-valued, representing counts of occur-rences in a derivation.
Third, we include dis-criminative n-gram features (Roark et al, 2004),which count the occurrences of each n-gram thatis scored by our factored language model, ratherthan a feature whose value is the log probabilitydetermined by the language model.
Table 2 de-picts the new animacy, agreement and punctuationfeatures being introduced as part of this work.
Thenext two sections describe these features in moredetail.3.1 Animacy and Number AgreementUnderspecification as to the choice of pronoun inthe input leads to competing realizations involv-ing the relative pronouns who, that, which etc.
The1The approach has been dubbed hypertagging since it op-erates at a level ?above?
the syntax, moving from semanticrepresentations to syntactic categories.Feature Type ExampleLexCat + Word s/s/np + beforeLexCat + POS s/s/np + INRule sdcl ?
np sdcl\npRule + Word sdcl ?
np sdcl\np + boughtRule + POS sdcl ?
np sdcl\np + VBDWord-Word ?company, sdcl ?
np sdcl\np, bought?Word-POS ?company, sdcl ?
np sdcl\np, VBD?POS-Word ?NN, sdcl ?
np sdcl\np, bought?Word + ?w ?bought, sdcl ?
np sdcl\np?
+ dwPOS + ?w ?VBD, sdcl ?
np sdcl\np?
+ dwWord + ?p ?bought, sdcl ?
np sdcl\np?
+ dpPOS + ?p ?VBD, sdcl ?
np sdcl\np?
+ dpWord + ?v ?bought, sdcl ?
np sdcl\np?
+ dvPOS + ?v ?VBD, sdcl ?
np sdcl\np?
+ dvTable 1: Baseline features: Basic and dependencyfeatures from Clark & Curran?s (2007) normalform model; distances are in intervening words,punctuation marks and verbs, and are capped at 3,3 and 2, respectivelyFeature ExampleAnimacy featuresNoun Stem + Wh-pronoun researcher + whoNoun Class + Wh-pronoun PER DESC + whoNumber featuresNoun + Verb people + areNounPOS + Verb NNS + areNoun + VerbPOS people + VBPNounPOS + VerbPOS NNS + VBPNoun of + Verb lot of + areNoun of + VerbPOS lot of + VBPNounPOS of + Verb NN of + areNounPOS of + VerbPOS NN of + VBPNoun of + of-complementPOS + VerbPOS lot of + NN + VBZNounPOS of + of-complementPOS + VerbPOS NN of + NN + VBZNoun of + of-complementPOS + Verb lot of + NN + isNounPOS of + of-complementPOS + Verb NN of + NN + isPunctuation featureBalanced Punctuation Indicator $unbalPunct=1Table 2: New features introducedexisting ranking models (n-gram models as wellas perceptron) often allow the top-ranked outputto have the relative pronoun that associated withanimate nouns.
The existing normal form modeluses the word forms as well as part-of-speech tagbased features.
Though this is useful for associ-ating proper nouns (tagged NNP or NNPS) withwho, for other nouns (as in consumers who vs.consumers that/which), the model often prefersthe infelicitous pronoun.
So here we designed fea-tures which also took into account the named en-tity class of the head noun as well as the stem ofthe head noun.
These features aid the discrimi-native n-gram features (PERSON, which has highnegative weight).
As the results section discusses,1036NE classes like PER DESC contribute substan-tially towards animacy preferences.For number agreement, we designed threeclasses of features (c.f.
Number Agr row in Table2).
Each of these classes results in 4 features.
Dur-ing feature extraction, subjects of the verbs taggedVBZ and VBP and verbs was, were were iden-tified using the PTB NP-SBJ function tag anno-tation projected on to the appropriate argumentsof lexical categories of verbs.
The first classof features encoded all possible combinations ofsubject-verb word forms and parts of speech tags.In the case of NPs involving of-complements likea lot of ... (Examples 5 and 6), feature classes 2and 3 were extracted (class 1 was excluded).
Class2 features encode the fact that the syntactic headhas an associated of-complement, while class 3features also include the part of speech tag of thecomplement.
In the case of conjunct/disjunct VPsand subject NPs, the feature specifically lookedat the parts of speech of both the NPs/VPs form-ing the conjunct/disjunct.
The motivation behindsuch a design was to glean syntactic and semanticgeneralizations from the data.
During feature ex-traction, from each derivation, counts of animacyand agreement features were obtained.3.2 Balanced PunctuationA complex issue that arises in the design of bi-directional grammars is ensuring the proper pre-sentation of punctuation.
Among other things, thisinvolves the task of ensuring the correct realiza-tion of commas introducing noun phrase apposi-tives.
(9) John, CEO of ABC, loves Mary.
(10) * John, CEO of ABC loves Mary.
(11) Mary loves John, CEO of ABC.
(12) * Mary loves John, CEO of ABC,.
(13) Mary loves John, CEO of ABC, madly.
(14) * Mary loves John, CEO of ABC madly.As of now, n-gram models rule out exampleslike 12 above.
All the other unacceptable ex-amples are ruled out using a post-filter on real-ized derivations.
As described in White and Ra-jkumar (2008), the need for the filter arises be-cause a feature-based approach appears to be in-adequate for dealing with the class of examplespresented above in CCG.
This approach involvesthe incorporation of syntactic features for punctu-ation into atomic categories so that certain combi-nations are blocked.
To ensure proper appositivebalancing sentence finally, the rightmost elementin the sentence should transmit a relevant fea-ture to the clause level, which the sentence-finalperiod can then check for the presence of right-edge punctuation.
However, the feature schemadoes not constrain cases of balanced punctuationin cases involving crossing composition and ex-traction.
However, in this paper we explore a sta-tistical approach to ensure proper balancing of NPapposition commas.
The first step in this solutionis the introduction of a feature in the grammarwhich indicates balanced vs. unbalanced marks.We modified the result categories of unbalancedappositive commas and dashes to include a fea-ture marking unbalanced punctuation, as follows:(15) , ` np?1?unbal=comma\?np?1?/?np?2?Then, during feature extraction, derivationswere examined to detect categories such asnpunbal=comma , and checked to make sure this NPis followed by another punctuation mark in thestring such as a full stop.
The feature indicates thepresence or absence of unbalanced punctuation inthe derivation.4 Evaluation4.1 Experimental ConditionsFor the experiments reported below, we used alexico-grammar extracted from Sections 02?21 ofour enhanced CCGbank with collapsed NEs, ahypertagging model incorporating named entityclass features, and a trigram factored languagemodel over words, named entity classes, part-of-speech tags and supertags.
Perceptron trainingevents were generated for each training sectionseparately.
The hypertagger and POS/supertaglanguage model were trained on all the trainingsections, while separate word-based models weretrained excluding each of the training sections inturn.
Event files for 26530 training sentences withcomplete realizations were generated, with an av-erage n-best list size of 18.2.
The complete set ofmodels is listed in Table 3.1037Model Descriptionfull-model All the feats from models belowagr-punct Baseline Feats + Punct + Num-Agrwh-punct Baseline Feats + Punct + Animacy-Agrbaseline-punct Baseline Feats + Punctbaseline Log prob + n-gram +Syntactic featuresTable 3: Legend for experimental conditions4.2 ResultsRealization results on the development and testsections are given in Table 4.
For the develop-ment section, in terms of both exact matches andBLEU scores, the model with all the three featuresdiscussed above (agreement, animacy and punc-tuation) performs better than the baseline whichdoes not have any of these features.
However, us-ing these criteria, the best performing model is ac-tually the model which has agreement and punc-tuation features.
The model containing all thefeatures does better than the punctuation-featureonly model, but performs slightly worse than theagreement-punctuation model.
Section 23, thetest section, confirms that the model with all thefeatures performs better than the baseline model.We calculated statistical significance for the mainresults using bootstrap random sampling.2 Af-ter re-sampling 1000 times, significance was cal-culated using a paired t-test (999 d.f.).
The re-sults indicated that the model with all the fea-tures in it (full-model) exceeded the baseline withp < 0.0001 .
However, exact matches andBLEU scores do not necessarily reflect the extentto which important grammatical flaws have beenreduced.
So to judge the effectiveness of the newfeatures, we computed the percentage of errors ofeach type that were present in the best Section 00realization selected by each of these models.
Alsonote that our baseline results differ slightly fromthe corresponding results reported in White andRajkumar (2009) in spite of using the same featureset because quotes were introduced into the cor-pus on which these experiments were conducted.Previous results were based on the original CCG-bank text where quotation marks are absent.Table 6 reports results of the error analysis.
It2Scripts for running these tests are available athttp://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htmSection Model %Exact %Compl.
BLEU00 baseline 38.18 82.47 0.8341baseline-punct 37.97 82.47 0.8340wh-punct 38.93 82.53 0.8360full-model 40.47 82.53 0.8403agr-punct 40.84 82.53 0.841423 baseline 38.98 83.39 0.8442full-model 40.09 83.35 0.8446Table 4: Results (98.9% coverage)?percentageof exact match and grammatically complete real-izations and BLEU scoresModel METEOR TERPbaseline 0.9819 0.0939baseline-punct 0.9819 0.0939wh-punct 0.9827 0.0923agr-punct 0.9821 0.0902full-model 0.9826 0.0909Table 5: Section 00 METEOR and TERP scorescan be seen that the punctuation-feature is effec-tive in reducing the number of sentences with un-balanced punctuation marks.
Similarly, the fullmodel has fewer animacy mismatches and justabout the same number of errors of the other twotypes, though it performs slightly worse than theagreement-only model in terms of BLEU scoresand exact matches.
We also manually examinedthe remaining cases of animacy agreement errorsin the output of the full model here.
Of the remain-ing 18 errors, 14 were acceptable paraphrases in-volving object relative clauses (eg.
wsj 0083.40 ...the business that/?
a company can generate).
Wealso provide METEOR and TERP scores for thesemodels (Table 5).
In recently completed work onthe creation of a human-rated paraphrase corpusto evaluate NLG systems, our analyses showedthat BLEU, METEOR and TERP scores correlatemoderately with human judgments of adequacyand fluency, and that the most reliable system-level comparisons can be made only by lookingat all three metrics.4.3 ExamplesTable 7 presents four examples where thefull model differs from the baseline.
Examplewsj 0003.8 illustrates an example where the NEtag PER DESC for researchers helps the percep-tron model enforce the correct animacy agree-ment, while the two baseline models prefer the1038Ref-wsj 0003.8 full,agr,wh neither Lorillard nor the researchers who studied the workers were aware of any research onsmokers of the Kent cigarettesbaseline,baseline-punct neither Lorillard nor the researchers that studied the workers were aware of any research onsmokers of the Kent cigarettes .Ref-wsj 0003.18 agr-punct, full the plant , which is owned by Hollingsworth & Vose Co. , was under contract with lorillard to make the cigarette filters .baselines, wh the plant , which is owned by Hollingsworth & Vose Co. , were under contract with lorillard to make the cigarette filters .Ref-wsj 0018.6 agr-punct, full model while many of the risks were anticipated when minneapolis-based Cray Research first announced the spinoff ...agr-punct, full while many of the risks were anticipated when minneapolis-based Cray Research first announced the spinoff ...baselines while many of the risks was anticipated when minneapolis-based Cray Research announced the spinoff ...Ref-wsj 0070.4 agr-punct, full Giant Group is led by three Rally ?s directors , Burt Sugarman , James M. Trotter III and William E. Trotter II that lastmonth indicated that they hold a 42.5 % stake in Rally ?s and plan to seek a majority of seats on ...all others Giant Group is led by three Rally ?s directors , Burt Sugarman , James M. Trotter III and William E. Trotter II that lastmonth indicated that they holds a 42.5 % stake in Rally ?s and plans to seek a majority of seats on ...Ref-wsj 0047.5 ... the ban wo n?t stop privately funded tissue-transplant research or federally funded fetal-tissue researchthat does n?t involve transplants .agr, full ... the ban wo n?t stop tissue-transplant privately funded research or federally funded fetal-tissue researchthat does n?t involve transplants .baselines, wh ... the ban wo n?t stop tissue-transplant privately funded research or federally funded fetal-tissue researchthat do n?t involve transplants .Table 7: Examples of realized outputModel #Punct-Errs %Agr-Errs %WH-Errsbaseline 39 11.05 22.44baseline-punct 0 10.79 20.77wh-punct 11 10.87 13.53agr-punct 8 4.0 21.84full-model 10 4.31 15.53Table 6: Error analysis of Section 00 complete re-alizations (total of 1554 agreement cases; total of207 WH-pronoun cases)that realization.
Example wsj 0003.18 illustratesan instance of simple subject-verb agreement be-ing enforced by the models containing the agree-ment features.
Example wsj 0070.4 presents amore complex situation where a single subjecthas to agree with both verbs in a conjoined verbphrase.
The last example in Table 7 shows thecase of a NP subject which is a disjunction of twoindividual NPs.
In both these cases, while thebaseline models do not enforce the correct choice,the models with the agreement features do get thisright.
This is because our agreement features aresensitive to the properties of both NP and VP con-juncts/disjuncts.
In addition, most of the realiza-tions involving of -complements are also rankedcorrectly.
In the final example sentence provided(i.e.
wsj 0018.6), the models with the agreementfeatures are able to enforce the correct the agree-ment constraints in the phrase many of the riskswere in contrast to the baseline models.5 ConclusionIn this paper, we have shown for the first timethat incorporating linguistically motivated fea-tures to ensure correct animacy and number agree-ment in a statistical realization ranking modelyields significant improvements over a state-of-the-art baseline.
While agreement has tradition-ally been modelled using hard constraints in thegrammar, we have argued that using a statisticalranking model is a simpler and more robust ap-proach that is capable of learning competing pref-erences and cases of acceptable variation.
Ourapproach also approximates insights about agree-ment which have been discussed in the theoret-ical linguistics literature.
We have also shownhow a targeted error analysis can reveal substan-tial reductions in agreement errors, whose impacton quality no doubt exceeds what is suggestedby the small BLEU score increases.
As futurework, we also plan to learn such patterns fromlarge amounts of unlabelled data and use modelslearned thus to rank paraphrases.AcknowledgementsThis work was supported in part by NSF grant IIS-0812297 and by an allocation of computing timefrom the Ohio Supercomputer Center.
Our thanksalso to Robert Levine and the anonymous review-ers for helpful comments and discussion.1039ReferencesBaldridge, Jason and Geert-Jan Kruijff.
2002.
Cou-pling CCG and Hybrid Logic Dependency Seman-tics.
In Proc.
ACL-02.Baldridge, Jason.
2002.
Lexically Specified Deriva-tional Control in Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh.Bangalore, Srinivas and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gen-eration.
In Proc.
COLING-00.Boxwell, Stephen and Michael White.
2008.
Project-ing Propbank roles onto the CCGbank.
In Proc.LREC-08.Cahill, Aoife and Josef van Genabith.
2006.
Ro-bust PCFG-based generation using automaticallyacquired LFG approximations.
In Proc.
COLING-ACL ?06.Clark, Stephen and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Espinosa, Dominic, Michael White, and DennisMehay.
2008.
Hypertagging: Supertagging for sur-face realization with CCG.
In Proc.
ACL-08: HLT.Guo, Yuqing, Josef van Genabith, and Haifeng Wang.2008.
Dependency-based n-gram models forgeneral purpose sentence realisation.
In Proc.COLING-08.Hockenmaier, Julia and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.Hogan, Deirdre, Conor Cafferkey, Aoife Cahill, andJosef van Genabith.
2007.
Exploiting multi-wordunits in history-based probabilistic generation.
InProc.
EMNLP-CoNLL.Kathol, Andreas.
1999.
Agreement and theSyntax-Morphology Interface in HPSG.
In Levine,Robert D. and Georgia M. Green, editors, Studiesin Contemporary Phrase Structure Grammar, pages223?274.
Cambridge University Press, Cambridge.Kim, Jong-Bok.
2004.
Hybrid Agreement in English.Linguistics, 42(6):1105?1128.Nakanishi, Hiroko, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation ofan HPSG-based chart generator.
In Proc.
IWPT-05.Palmer, Martha, Dan Gildea, and Paul Kingsbury.2005.
The proposition bank: A corpus annotatedwith semantic roles.
Computational Linguistics,31(1).Pollard, Carl and Ivan Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University Of ChicagoPress.Roark, Brian, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative languagemodeling with conditional random fields and theperceptron algorithm.
In Proc.
ACL-04.Steedman, Mark.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.Velldal, Erik and Stephan Oepen.
2005.
Maximumentropy models for realization ranking.
In Proc.
MTSummit X.Weischedel, Ralph and Ada Brunstein.
2005.
BBNpronoun coreference and entity type corpus.
Tech-nical report, BBN.White, Michael and Rajakrishnan Rajkumar.
2008.A more precise analysis of punctuation for broad-coverage surface realization with CCG.
In Proc.of the Workshop on Grammar Engineering AcrossFrameworks (GEAF08).White, Michael and Rajakrishnan Rajkumar.
2009.Perceptron reranking for CCG realization.
In Pro-ceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages410?419, Singapore, August.
Association for Com-putational Linguistics.White, Michael.
2006.
Efficient Realization of Coor-dinate Structures in Combinatory Categorial Gram-mar.
Research on Language and Computation,4(1):39?75.1040
