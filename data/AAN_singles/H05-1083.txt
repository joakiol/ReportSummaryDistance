Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 660?667, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMulti-Lingual Coreference Resolution With Syntactic FeaturesXiaoqiang Luo and Imed Zitouni1101 Kitchawan RoadIBM T.J. Watson Research CenterYorktown Heights, NY 10598, U.S.A.{xiaoluo, izitouni}@us.ibm.comAbstractIn this paper, we study the impact of agroup of features extracted automatically frommachine-generated parse trees on coreferenceresolution.
One focus is on designing syn-tactic features using the binding theory as theguideline to improve pronoun resolution, al-though linguistic phenomenon such as apposi-tion is also modeled.
These features are ap-plied to the Arabic, Chinese and English coref-erence resolution systems and their effective-ness is evaluated on data from the AutomaticContent Extraction (ACE) task.
The syntacticfeatures improve the Arabic and English sys-tems significantly, but play a limited role in theChinese one.
Detailed analyses are done to un-derstand the syntactic features?
impact on thethree coreference systems.1 IntroductionA coreference resolution system aims to group togethermentions referring to the same entity, where a mention isan instance of reference to an object, and the collection ofmentions referring to the same object in a document forman entity.
In the following example:(I) ?John believes himself to be the best student.
?mentions are underlined.
The three mentions ?John?,?himself?, ?the best student?
are of type name, pronoun 1,and nominal, respectively.
They form an entity since theyall refer to the same person.Syntactic information plays an important role in corefer-ence resolution.
For example, the binding theory (Haege-man, 1994; Beatrice and Kroch, 2000) provides a goodaccount of the constraints on the antecedent of Englishpronouns.
The theory relies on syntactic parse trees to de-termine the governing category which defines the scope1?Pronoun?
in this paper refers to both anaphor and normalpronoun.of binding constraints.
We will use the theory as a guide-line to help us design features in a machine learningframework.Previous pronoun resolution work (Hobbs, 1976; Lappinand Leass, 1994; Ge et al, 1998; Stuckardt, 2001) explic-itly utilized syntactic information before.
But there areunique challenges in this study: (1) Syntactic informa-tion is extracted from parse trees automatically generated.This is possible because of the availability of statisticalparsers, which can be trained on human-annotated tree-banks (Marcus et al, 1993; Xia et al, 2000; Maamouriand Bies, 2004) for multiple languages; (2) The bind-ing theory is used as a guideline and syntactic structuresare encoded as features in a maximum entropy corefer-ence system; (3) The syntactic features are evaluated onthree languages: Arabic, Chinese and English (one goalis to see if features motivated by the English language canhelp coreference resolution in other languages).
All con-trastive experiments are done on publicly-available data;(4) Our coreference system resolves coreferential rela-tionships among all the annotated mentions, not just forpronouns.Using machine-generated parse trees eliminates the needof hand-labeled trees in a coreference system.
How-ever, it is a major challenge to extract useful informa-tion from these noisy parse trees.
Our approach is encod-ing the structures contained in a parse tree into a set ofcomputable features, each of which is associated with aweight automatically determined by a machine learningalgorithm.
This contrasts with the approach of extractingrules and assigning weights to these rules by hand (Lap-pin and Leass, 1994; Stuckardt, 2001).
The advantageof our approach is robustness: if a particular structure ishelpful, it will be assigned a high weight; if a feature isextracted from a highly noisy parse tree and is not in-formative in coreference resolution, it will be assigneda small weight.
By avoiding writing rules, we automati-cally incorporate useful information into our model and atthe same time limit the potentially negative impact fromnoisy parsing output.6602 Statistical Coreference Resolution ModelOur coreference system uses a binary entity-mentionmodel PL(?|e, m) (henceforth ?link model?)
to score theaction of linking a mention m to an entity e. In our im-plementation, the link model is computed asPL(L = 1|e, m) ?
maxm?
?eP?L(L = 1|e, m?, m), (1)where m?
is one mention in entity e, and the basic modelbuilding block P?L(L = 1|e, m?, m) is an exponential ormaximum entropy model (Berger et al, 1996):P?L(L|e, m?, m) =exp{?i ?igi(e, m?, m, L)}Z(e, m?, m) , (2)where Z(e, m?, m) is a normalizing factor to ensure thatP?L(?|e, m?, m) is a probability, {gi(e, m?, m, L)} are fea-tures and {?i} are feature weights.Another start model is used to score the action of creatinga new entity with the current mention m. Since startinga new entity depends on all the partial entities created inthe history {ei}ti=1, we use the following approximation:PS(S = 1|e1, e2, ?
?
?
, et, m) ?1 ?
max1?i?tPL(L = 1|ei, m) (3)In the maximum-entropy model (2), feature (typically bi-nary) functions {gi(e, m?, m, ?)}
provide us with a flex-ible framework to encode useful information into thethe system: it can be as simple as ?gi(e, m?, m, L =1) = 1 if m?
and m have the same surface string,?
or?gj(e, m?, m, L = 0) = 1 if e and m differ in num-ber,?
or as complex as ?gl(e, m?, m, L = 1) = 1 if m?c-commands m and m?
is a NAME mention and m is apronoun mention.?
These feature functions bear similar-ity to rules used in other coreference systems (Lappin andLeass, 1994; Mitkov, 1998; Stuckardt, 2001), except thatthe feature weights {?i} are automatically trained over acorpus with coreference information.
Learning featureweights automatically eliminates the need of manuallyassigning the weights or precedence of rules, and opensthe door for us to explore rich features extracted fromparse trees, which is discussed in the next section.3 Syntactic FeaturesIn this section, we present a set of features extractedfrom syntactic parse trees.
We discuss how we approx-imately compute linguistic concepts such as governingcategory (Haegeman, 1994), apposition and dependencyrelationships from noisy syntactic parse trees.
Whileparsing and parse trees depend on the target language,the automatic nature of feature extraction from parse treesmakes the process language-independent.V(1)(GC)(Sub) (gov)likesNP1 NP2SJohnVPhimself.VPVJohn likesNP1 NP2S (GC)(Sub)(2)(gov)him.VVPdescriptionNP5Miss Smith?sbelievesJohnS(gov)(Sub)(GC)(3)NP1NP2NP6NP3of herself.NP4PPPFigure 1: GC examples.3.1 Features Inspired by Binding TheoryThe binding theory (Haegeman, 1994) concerning pro-nouns can be summarized with the following principles:1.
A reflexive or reciprocal pronoun (e.g., ?herself?
or?each other?)
must be bound in its governing cate-gory (GC).2.
A normal pronoun must be free in its governing cat-egory.The first principle states that the antecedent of a reflexiveor reciprocal pronoun is within its GC, while the secondprinciple says that the antecedent of a normal pronoun isoutside its GC.
While the two principles are simple, theyall rely on the concept of governing category, which isdefined as the minimal domain containing the pronoun inquestion, its governor, and an accessible subject.The concept GC can best be explained with a few exam-ples in Figure 1, where the label of a head constituentis marked within a box, and GC, accessible subject, andgovernor constituents are marked in parentheses with?GC?, ?Sub?
and ?gov.?
Noun-phrases (NP) are num-bered for the convenience of referencing.
For example,in sub-figure (1) of Figure 1, the governor of ?himself?is ?likes,?
the subject is ?John,?
hence the GC is the en-tire sentence spanned by the root ?S.?
Since ?himself?is reflexive, its antecedent must be ?John?
by Principle1.
The parse tree in sub-figure (2) is the same as thatin sub-figure (1), but since ?him?
is a normal pronoun,its antecedent, according to Principle 2, has to be out-side the GC, that is, ?him?
cannot be coreferenced with?John.?.
Sentence in sub-figure (3) is slightly more com-plicated: the governor of ?herself?
is ?description,?
andthe accessible subject is ?Miss Smith.?
Thus, the govern-ing category is NP6.
The first principle implies that theantecedent of ?herself?
must be ?Miss Smith.
?It is clear from these examples that GC is very usefulin finding the antecedent of a pronoun.
But the last ex-ample shows that determining GC is not a trivial matter.Not only is the correct parse tree required, but extra in-formation is also needed to identify the head governor661and the minimal constituent dominating the pronoun, itsgovernor and an accessible subject.
Determining the ac-cessible subject itself entails checking other constraintssuch as number and gender agreement.
The complexityof computing governing category, compounded with thenoisy nature of machine-generated parse tree, prompts usto compute a set of features that characterize the struc-tural relationship between a candidate mention and a pro-noun, as opposed to explicitly identify GC in a parse tree.These features are designed to implicitly model the bind-ing constraints.Given a candidate antecedent or mention m1 and a pro-noun mention m2 within a parsed sentence, we first testif they have c-command relation, and then a set of count-ing features are computed.
The features are detailed asfollows:(1) C-command ccmd(m1, m2) : A constituent X c-commands another constituent Y in a parse tree if the firstbranching node dominating X also dominates Y .
The bi-nary feature ccmd(m1, m2) is true if the minimum NPdominating m1 c-commands the minimum NP dominat-ing m2.
In sub-figure (1) of Figure 1, NP1 c-commandsNP2 since the first branching node dominating NP1 is Sand it dominates NP2.If ccmd(m1, m2) is true, we then define the c-commandpath T (m1, m2) as the path from the minimum NP dom-inating m2 to the first branching node that dominates theminimum NP dominating m1.
In sub-figure (1) of Fig-ure 1, the c-command path T (?John?, ?himself?)
wouldbe ?NP2-VP-S.?
(2) NP count(m1, m2): If ccmd(m1, m2) is true,then NP count(m1, m2) counts how many NPs areseen on the c-command path T (m1, m2), exclud-ing two endpoints.
In sub-figure (1) of Figure 1,NP count(?John?, ?himself?)
= 0 since there is no NPon T (?John?, ?himself?).
(3) V P count(m1, m2): similar to NP count(m1, m2),except that this feature counts how many verb phrases(VP) are seen on the c-command path.
In sub-figure (1)of Figure 1, V P count(?John?, ?himself?)
is true sincethere is one VP on T (?John?, ?himself?).
(4) S count(m1, m2): This feature counts how manyclauses are seen on the c-command path whenccmd(m1, m2) is true.
In sub-figure (1) of Figure 1,S count(?John?, ?himself?)
= 0 since there is no clauselabel on T (?John?, ?himself?
).These features are designed to capture information in theconcept of governing category when used in conjunctionwith attributes (e.g., gender, number, reflexiveness) of in-dividual pronouns.
Counting the intermediate NPs, VPsand sub-clauses implicitly characterizes the governor ofa pronoun in question; the presence or absence of a sub-clause indicates whethere or not a coreferential relation isacross clause boundary.3.2 Dependency FeaturesIn addition to features inspired by the binding theory, aset of dependency features are also computed with thehelp of syntactic parse trees.
This is motivated by exam-ples such as ?John is the president of ABC Corporation,?where ?John?
and ?the president?
refer to the same per-son and should be in the same entity.
In scenarios likethis, lexical features do not help, while the knowledgethat ?John?
left-modifies the verb ?is?
and the ?the presi-dent?
right-modifies the same verb would be useful.Given two mentions m1 and m2 in a sentence, we com-pute the following dependency features:(1)same head(m1, m2): The feature compares the bi-lexical dependencies ?m1, h(m1)?, and ?m2, h(m2)?,where h(x) is the head word which x modifies.
The fea-ture is active only if h(m1) = h(m2), in which case itreturns h(m1).
(2)same POS(m1, m2): To get good coverage of de-pendencies, we compute a feature same POS(m1, m2),which examines the same dependency as in (1) andreturns the common head part-of-speech (POS) tag ifh(m1) = h(m2).The head child nodes are marked with boxes inFigure 1.
For the parse tree in sub-figure (1),same head(?John?, ?him?)
would return ?likes?
as?John?
left-modifies ?likes?
while ?him?
right-modifies?likes,?
and same POS(?John?, ?him?)
would return?V?
as the POS tag of ?likes?
is ?V.?
(3) mod(m1, m2): the binary feature is true if m1modifies m2.
For parse tree (2) of Figure 1,mod(?John?, ?him?)
returns false as ?John?
does notmodify ?him?
directly.
A reverse order featuremod(m2, m1) is computed too.
(4) same head2(m1, m2): this set of features examinesecond-level dependency.
It compares the head word ofh(m1), or h(h(m1)), with h(m2) and returns the com-mon head if h(h(m1)) = h(m2).
A reverse order featuresame head2(m2, m1) is also computed.
(5) same POS2(m1, m2): similar to (4), except that itcomputes the second-level POS.
A reverse order featuresame POS2(m2, m1) is computed too.
(6) same head22(m1, m2): it returns the commonsecond-level head if h(h(m1)) = h(h(m2)).3.3 Apposition and Same-Parent FeaturesApposition is a phenomenon where two adjacent NPs re-fer to the same entity, as ?Jimmy Carter?
and ?the formerpresident?
in the following example:(II) ?Jimmy Carter, the former president of US, is visit-ing Europe.
?Note that not all NPs separated by a comma are neces-sarily appositive.
For example, in ?John called Al, Bob,and Charlie last night,?
?Al?
and ?Bob?
share a same NP662parent and are separated by comma, but they are not ap-positive.To compute the apposition feature appos(m1, m2) formention-pair (m1, m2), we first determine the minimumdominating NP of m1 and m2.
The minimum dominatingNP of a mention is the lowest NP, with an optional modi-fying phrase or clause, that spans the mention.
If the twominimum dominating NPs have the same parent NP, andthey are the only two NP children of the parent, the valueof appos(m1, m2) is true.
This would exclude ?Al?
and?Bob?
in ?John called Al, Bob, and Charlie last night?from being computed as apposition.We also implement a feature same parent(m1, m2)which tests if two mentions m1 and m2 are dominatedby a common NP.
The feature helps to prevent the systemfrom linking ?his?
with ?colleague?
in the sentence ?Johncalled his colleague.
?All the features described in Section 3.1-3.3 are com-puted from syntactic trees generated by a parser.
Whilethe parser is language dependent, feature computationboils down to encoding the structural relationship of twomentions, which is language independent.
To test the ef-fectiveness of the syntactic features, we integrate theminto 3 coreference systems processing Arabic, Chineseand English.4 Experimental Results4.1 Data and System DescriptionAll experiments are done on true mentions of theACE (NIST, 2004) 2004 data.
We reserve part of LDC-released 2004 data as the development-test set (hence-forth ?devtest?)
as follows: documents are sorted by theirdate and time within each data source (e.g., broadcastnews (bnews) and news wire (nwire) are two differentsources) and the last 25% documents of each data sourceare reserved as the devtest set.
Splitting data on chrono-logical order simulates the process of a system?s devel-opment and deployment in the real world.
The devtestset statistics of three languages (Arabic, Chinese andEnglish) is summarized in Table 1, where the numberof documents, mentions and entities is shown on row 2through 4, respectively.
The rest of 2004 ACE data to-gether with earlier ACE data is used as training.Arabic Chinese English#-docs 178 166 114#-mentions 11358 8524 7008#-entities 4428 3876 2929Table 1: Devtest Set Statistics by LanguageThe official 2004 evaluation test set is used as the blindtest set on which we run our system once after the systemdevelopment is finished.
We will report summary resultson this test set.As for parser, we train three off-shelf maximum-entropyparsers (Ratnaparkhi, 1999) using the Arabic, Chineseand English Penn treebank (Maamouri and Bies, 2004;Xia et al, 2000; Marcus et al, 1993).
Arabic wordsare segmented while the Chinese parser is a character-based parser.
The three parsers have a label F-measureof 77%, 80%, and 86% on their respective test sets.
Thethree parsers are used to parse both ACE training and testdata.
Features described in Section 3 are computed frommachine-generated parse trees.Apart from features extracted from parse trees, our coref-erence system also utilizes other features such as lex-ical features (e.g., string matching), distance featurescharacterized as quantized word and sentence distances,mention- and entity-level attribute information (e.g, ACEdistinguishes 4 types of mentions: NAM(e), NOM(inal),PRE(modifier) and PRO(noun)) found in the 2004 ACEdata.
Details of these features can be found in (Luo etal., 2004).4.2 Performance MetricsThe official performance metric in the ACE task is ACE-Value (NIST, 2004).
The ACE-Value is an entity-basedmetric computed by subtracting a normalized cost from1 (so it is unbounded below).
The cost of a system isa weighted sum of costs associated with entity misses,false alarms and errors.
This cost is normalized againstthe cost of a nominal system that outputs no entity.
Aperfect coreference system gets 100% ACE-Value whilea system outputting many false-alarm entities could get anegative value.The default weights in ACE-Value emphasize names, andseverely discount pronouns: the relative importance of apronoun is two orders of magnitude less than that of aname.
So the ACE-Value will not be able to accurately re-flect a system?s improvement on pronouns2.
For this rea-son, we compute an unweighted entity-constrained men-tion F-measure (Luo, 2005) and report all contrastiveexperiments with this metric.
The F-measure is com-puted by first aligning system and reference entities suchthat the number of common mentions is maximizedand each system entity is constrained to align with atmost one reference entity, and vice versa.
For exam-ple, suppose that a reference document contains threeentities: {[m1], [m2, m3], [m4]} while a system outputsfour entities: {[m1, m2], [m3], [m5], [m6]}, where {mi :i = 1, 2, ?
?
?
, 6} are mentions, then the best alignmentfrom reference to system would be [m1] ?
[m1, m2],[m2, m3] ?
[m3] and other entities are not aligned.
Thenumber of common mentions of the best alignment is 22Another possible choice is the MUC F-measure (Vilain etal., 1995).
But the metric has a systematic bias for systemsgenerating fewer entities (Bagga and Baldwin, 1998) ?
see Luo(2005).
Another reason is that it cannot score single-mentionentity.663(i.e., m1 and m3), thus the recall is 24 and precision is25 .
Due to the one-to-one entity alignment constraint, theF-measure here is more stringent than the accuracy (Geet al, 1998; Mitkov, 1998; Kehler et al, 2004) computedon antecedent-pronoun pairs.4.3 Effect of Syntactic FeaturesWe first present the contrastive experimental results onthe devtest described in sub-section 4.1.Two coreference systems are trained for each language:a baseline without syntactic features, and a system in-cluding the syntactic features.
The entity-constrained F-measures with mention-type breakdown are presented inTable 2.
Rows marked with Nm contain the number ofmentions, while rows with ?base?
and ?+synt?
are F-measures for the baseline and the system with the syn-tactic features, respectively.The syntactic features improve pronoun mentions acrossthree languages ?
not surprising since features inspiredby the binding theory are designed to improve pronouns.The pronoun improvement on the Arabic (from 73.2%to 74.6%) and English (from 69.2% to 72.0%) system isstatistically significant (at above 95% confidence level),but change on the Chinese system is not.
For Arabic,the syntactic features improve Arabic NAM, NOM andPRE mentions, probably because Arabic pronouns aresometimes attached to other types of mentions.
For Chi-nese and English, the syntactic features do not practicallychange the systems?
performance.As will be shown in Section 4.5, the baseline systemswithout syntactic features are already competitive, com-pared with the results on the coreference evaluation track(EDR-coref) of the ACE 2004 evaluation (NIS, 2004).
Soit is nice to see that syntactic features further improve agood baseline on Arabic and English.ArabicMention TypeNAM NOM PRE PRO TotalNm 2843 3438 1291 3786 11358base 86.8 73.2 86.7 73.2 78.2+synt 88.4 76.4 87.4 74.6 80.1ChineseNm 4034 3696 - 794 8524base 95.4 77.8 - 65.9 85.0+synt 95.2 77.7 - 66.5 84.9EnglishNm 2069 2173 835 1931 7008base 92.0 73.4 88.7 69.2 79.6+synt 92.0 75.3 87.8 72.0 80.8Table 2: F-measure(%) Breakdown by Mention Type:NAM(e), NOM(inal), PRE(modifier) and PRO(noun).Chinese data does not have the PRE type.4.4 Error AnalysesFrom the results in Table 2, we know that the set of syn-tactic features are working in the Arabic and English sys-tem.
But the results also raise some questions: Are thereinteractions among the the syntactic features and otherfeatures?
Why do the syntactic features work well forArabic and English, but not Chinese?
To answer thesequestions, we look into each system and report our find-ings in the following sections.4.4.1 English SystemOur system uses a group of distance features.
One ob-servation is that information provided by some syntacticfeatures (e.g., V P count(m1, m2) etc) may have over-lapped with some of the distance features.
To test if thisis the case, we take out the distance features from the En-glish system, and then train two systems, one with thesyntactic features, one without.
The results are shownin Table 3, where numbers on the row ?b-dist?
are F-measures after removing the distance features from thebaseline, and numbers on the row ?b-dist+synt?
are withthe syntactic features.Mention TypeNAM NOM PRE PRO Totalb-dist 84.2 68.8 74.6 63.3 72.5b-dist+synt 90.7 74.2 87.8 69.0 79.3Table 3: Impact of Syntactic Features on English Sys-tem After Taking out Distance Features.
Numbers areF-measures(%).As can be seen, the impact of the syntactic features ismuch larger when the distance features are absent in thesystem: performance improves across all the four men-tion types after adding the syntactic features, and theoverall F-measure jumps from 72.5% to 79.3%.
ThePRE type gets the biggest improvement since features ex-tracted from parse trees include apposition, same-parenttest, and dependency features, which are designed to helpmention pairs in close distance, just as in the case of PREmentions.Comparing the numbers in Table 3 with the English base-line of Table 2, we can also conclude that distance fea-tures and syntactic features lead to about the same levelof performance when the other set of features is notused.
When the distance features are used, the syntac-tic features further help to improve the performance ofthe NOM and PRO mention type, albeit to a less degreebecause of information overlap between the two sets offeatures.4.4.2 Chinese SystemResults in Table 2 show that the syntactic features are notso effective for Chinese as for Arabic and English.
The664first thing we look into is if there is any idiosyncrasy inthe Chinese language.In Table 4, we list the statistics collected over the trainingsets of the three languages: the second row are the totalnumber of mentions, the third row the number of pronounmentions, the fourth row the number of events where thec-command feature ccmd(m1, m2) is used, and the lastrow the average number of c-command features per pro-noun (i.e., the fourth row divided by the third row).
Apronouns event is defined as a tuple of training instance(e, m1, m2) where m1 is a mention in entity e, and thesecond mention m2 is a pronoun.From Table 4, it is clear that Chinese pronoun distributionis very different: pronoun mentions account for about8.7% of the total mentions in Chinese, while 29.0% ofArabic mentions and 25.1% of English mentions are pro-nouns (the same disparity can be observed in the devtestset in Table 2).
This is because Chinese is a pro-drop lan-guage (Huang, 1984): for example, in the Chinese Penntreebank version 4, there are 4933 overt pronouns, but5750 pro-drops!
The ubiquity of pro-drops in Chineseresults in signigicantly less pronoun training events.
Con-sequently, the pronoun-related features are not trained aswell as in English and Arabic.
One way to quantify thisis by looking at the average number of c-command fea-tures on a per-pronoun basis: as shown in the last row ofTable 4, the c-command feature is seen more than twiceoften in Arabic and English as in Chinese.
Since low-count features are filtered out, the sparsity of pronounevents prevent many compound features (e.g., conjunc-tion of syntactic and distance features) from being trainedin the Chinese system, which explains why the syntacticfeatures do not help Chinese pronouns.Arabic Chinese English#total-mentions 31706 33851 58202#pron-mentions 9183 2941 14635#-ccmd-event 10236 1260 13691#ccmd/pron 1.14 0.428 0.936Table 4: Distribution of Pronoun Mentions and Fre-quency of c-command Features4.4.3 Arabic SystemAs stated in Table 4, 29.0% of Arabic mentions are pro-nouns, compared to a slightly lower number (25.1%) forEnglish.
This explains the relatively high positive impactof the syntactic features on the Arabic coreference sys-tem, compared to English and Chinese systems.
To un-derstand how syntactic features work in the Arabic sys-tem, we examine two examples extracted from the de-vtest set: (1) the first example shows the negative impactof syntactic features because of the noisy parsing output,and (2) the second example proves the effectiveness ofthe syntactic features to find the dependency between twomentions.
In both examples, the baseline system and thesystem with syntactic features give different results.Let?s consider the following sentence:.
.
.
A ?
D??
?A ?
?Y??
@ ?JKQ??
@Q.J?K?
.
.
.... its-capital?
Jerusalem?
Israel?
consider?
and .... .
.
?
JKY???
??Q???
@ Q?
??
@ 	??JJ???
??
@ YKQKA ?J?of-the-city?
the-Eastern?
the-half?
the-Palestininan?
want?
whileThe English text shown above is a word-to-word trans-lation of the Arabic text (read from right-to-left).
In thisexample, the parser wrongly put the nominal mention?Y ?
?
@ (Jerusalem) and the pronominal mention?
JKY??
@ (the-city) under the same constituent, which acti-vates the same parent feature.
The use of the featuresame parent(?Y??
@, ?
JKY??
@) leads to the two mentionsbeing put into different entities.
This is because thereare many cases in the training data where two mentionsunder the same parent are indeed in different entities: asimilar English example is ?John called his sister?, where?his?
and ?sister?
belong to two different entities.
Thesame parent feature is a strong indicator of not puttingthem into the same entity.??
+ ?
?A g + ?+??J?
A?
P + ?
@ + 	?A??
+ ?PAm.' + ?
@ + H@ + ?m?
+ ?
@ + I. ?
E +.
.
.
+ ??
+ 	?
@ + ?m.k + H.kAn + Al + zqAqywn + y + HAwl + wn+ nhb + Al + mHl + At + Al + tjAry + p+ b + Hjp + An + hm + ...was + the + zqAqywn + present-verb-marker y + trying + plural-verb-marker wn+ to-steal + the + office + s + the + commercial + s+ with + excuse + that + they + ...Table 5: An example where syntactic features help to linkthe PRO mention ??
(hm) with its antecedent, the NAMmention 	??
J?
A?
Q ?
@ (AlzqAqywn): top ?
Arabic sen-tence; middle ?
corresponding romanized sentence; bot-tom ?
token-to-token English translation.Table 5 shows another example in the devtest set.
The toppart presents the segmented Arabic text, the middle partis the corresponding romanized text, and the bottom partcontains the token-to-token English translation.
Note thatArabic text reads from right to left and its correspondingromanized text from left to right (i.e., the right-most Ara-bic token maps to the left-most romanized token).
Theparser output the correct syntactic structure: Figure 2shows a portion of the system-generated parse tree.
It canbe checked that NP1 c-commands NP2 and the group offeatures inspired by the binding theory are active.
Thesefeatures help to link the PRO(onominal) mention ?
?
(hm) with the NAM(e) mention 	??J?
A?
Q?
@ (AlzqAqywn).Without syntactic features theses two mentions were splitinto different entities.665Al+zqAqywn AnVPNP1SBARVPhmNP2SkAnFigure 2: A Portion of the Syntactic Tree.4.5 ACE 2004 ResultsTo get a sense of the performance level of our system, wereport the results on the ACE 2004 official test set withboth the F-measure and the official ACE-Value metric.This data is used as the blind test set which we run oursystem only once.Results are summarized in Table 6, where the second row(i.e.
?base?)
contains the baseline numbers, and the thirdrow (i.e., ?+synt?)
contains the numbers from systemswith the syntactic features.
Columns under ?F?
are F-measure and those under ?AV?
are ACE-Value.
The lastrow Nm contains the number of mentions in the three testsets.Arabic Chinese EnglishF AV F AV F AVbase 80.1 88.0 84.7 92.7 80.6 90.9+synt 81.5 88.9 84.7 92.8 82.0 91.6Nm 11358 11178 10336Table 6: Summary Results on the 2004 ACE EvaluationData.The performance of three full (?+synt?)
systems is re-markably close to that on the devtest set(cf.
Table 2):For Arabic, F-measure is 80.1 on the devtest vs. 81.5here; For Chinese, 84.9 vs. 84.7; And for English, 80.8vs.
82.0.
The syntactic features again help Arabic andEnglish ?
statistically very significant in F-measure, buthave no significant impact on Chinese.
The performanceconsistency across the devtest and blind test set indicatesthat the systems are well trained.The F-measures are computed on all types of mentions.Improvement on mention-types targeted by the syntacticfeatures is larger than the lump-sum F-measure.
For ex-ample, the F-measure for English pronouns on this test setis improved from 69.5% to 73.7% (not shown in Table 6due to space limit).
The main purpose of Table 6 is to geta sense of performance level correspondence between theF-measure and ACE-Value.Also note that, for Arabic and English, the difference be-tween the ?base?
and ?+synt?
systems, when measuredby ACE-Value, is much smaller.
This is not surprisingsince ACE-Value heavily discounts pronouns and is in-sensitive to improvement on pronouns ?
the very reasonwe adopt the F-measure in Section 4.3 and 4.4 when re-porting the contrastive experiment results.5 Related WorkMany researchers have used the syntactic information intheir coreference system before.
For example, Hobbs(1976) uses a set of rules that are applied to parse trees todetermine the antecedent of a pronoun.
The rule prece-dence is determined heuristically and no weight is used.Lappin and Leass (1994) extracted rules from the out-put of the English Slot Grammar (ESG) (McCord, 1993).Rule weights are assigned manually and the system re-solves the third person pronouns and reflexive pronounsonly.
Ge et al (1998) uses a non-parametrized statisti-cal model to find the antecedent from a list of candidatesgenerated by applying the Hobbs algorithm to the EnglishPenn Treebank.
Kehler et al (2004) experiments mak-ing use of predicate-argument structure extracted from alarge TDT-corpus.
Compared with these work, our workuses machine-generated parse trees from which trainablefeatures are extracted in a maximum-entropy coreferencesystem, while (Ge et al, 1998) assumes that correct parsetrees are given.
Feature weights are automatically trainedin our system while (Lappin and Leass, 1994; Stuckardt,2001) assign weights manually.There are a large amount of published work (Morton,2000; Soon et al, 2001; Ng and Cardie, 2002; Yang etal., 2003; Luo et al, 2004; Kehler et al, 2004) usingmachine-learning techniques in coreference resolution.But none of these work tried to compute complex lin-guistic concept such as governing category 3 .
Our workdemonstrates how relevant linguistic knowledge can bederived automatically from system-generated parse treesand encoded into computable and trainable features in amachine-learning framework.6 ConclusionsIn this paper, linguistic knowledge is used to guide us todesign features in maximum-entropy-based coreferenceresolution systems.
In particular, we show how to com-pute a set of features to approximate the linguistic notionssuch as governing category and apposition, and how tocompute the dependency features using syntactic parsetrees.
While the features are motivated by examining En-glish data, we see significant improvements on both En-glish and Arabic systems.
Due to the language idiosyn-crasy (e.g., pro-drops), we do not see the syntactic fea-tures change the Chinese system significantly.3Ng and Cardie (2002) used a BINDING feature, but it isnot clear from their paper how the feature was computed andwhat its impact was on their system.666AcknowledgmentsThis work was partially supported by the Defense Ad-vanced Research Projects Agency and monitored bySPAWAR under contract No.
N66001-99-2-8916.
Theviews and findings contained in this material are thoseof the authors and do not necessarily reflect the positionof policy of the Government and no official endorsementshould be inferred.Suggestions for improving the paper from the anonymousreviewers are gratefully acknowledged.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings of theLinguistic Coreference Workshop at The First Interna-tional Conference on Language Resources and Evalua-tion (LREC?98), pages 563?566.Santorini Beatrice and Anthony Kroch.
2000.
The syn-tax of natural language: An online introduction using theTrees program.
www.ling.upenn.edu/beatrice/syntax-textbook.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguistics,22(1):39?71, March.Niyu Ge, John Hale, and Eugene Charniak.
1998.
Astatistical approach to anaphora resolution.
In Proc.
ofthe sixth Workshop on Very Large Corpora.Liliane Haegeman.
1994.
Introduction to Governmentand Binding Theory.
Basil Blackwell Inc., 2nd edition.J.
Hobbs.
1976.
Pronoun resolution.
Technical report,Dept.
of Computer Science, CUNY, Technical ReportTR76-1.C.-T. James Huang.
1984.
On the distribution and refer-ence of empty pronouns.
Linguistic Inquiry, 15:531?574.Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-sandr Simma.
2004.
The (Non)utility of predicate-argument frequencies for pronoun interpretation.
InDaniel Marcu Susan Dumais and Salim Roukos, editors,HLT-NAACL 2004: Main Proceedings, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association for Com-putational Linguistics.Shalom Lappin and Herbert J. Leass.
1994.
An al-gorithm for pronominal anaphora resolution.
Computa-tional Linguistics, 20(4), December.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm based onthe bell tree.
In Proc.
of ACL.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Procs.
of HLT/EMNLP.Mohamed Maamouri and Ann Bies.
2004.
Develop-ing an Arabic treebank: Methods, guidelines, procedures,and tools.
In Proceedings of the Workshop on Compu-tational Approaches to Arabic Script-based Languages,COLING.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the Penntreebank.
Computational Linguistics, 19(2):313?330.Michael McCord.
1993.
Heuristics for broad-coveragenatural language parsing.
In Proc.
ARPA Human Lan-guage Technology Workshop.R.
Mitkov.
1998.
Robust pronoun resolution with limitedknowledge.
In Procs.
of the 36th ACL/17th COLING,pages 869?875.Thomas S. Morton.
2000.
Coreference for NLP applica-tions.
In In Proceedings of the 38th Annual Meeting ofthe Association for Computational Linguistics.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Proc.
ofACL, pages 104?111.NIST.
2004.
Proceedings of ACE Evaluation and PIMeeting 2004 Workshop, Alexandria, VA, September.NIST.
2004.
The ACE evaluation plan.www.nist.gov/speech/tests/ace/index.htm.Adwait Ratnaparkhi.
1999.
Learning to parse natu-ral language with maximum entropy models.
MachineLearning, 34:151?178.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreference res-olution of noun phrases.
Computational Linguistics,27(4):521?544.Roland Stuckardt.
2001.
Design and enhanced evalua-tion of a robust anaphor resolution algorithm.
Computa-tional Linguistics, 27(4).M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, , andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In In Proc.
of MUC6, pages 45?52.F.
Xia, M. Palmer, N. Xue, M.E.
Okurowski, J. Kovarik,F.D.
Chiou, S. Huang, T. Kroch, and M. Marcus.
2000.Developing guidelines and ensuring consistency for Chi-nese text annotation.
In Proc of the 2nd Intl.
Conf.
onLanguage Resources and Evaluation (LREC 2000).Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitionlearning approach.
In Proc.
of ACL.667
