Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 378?386,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPCross-Domain Dependency Parsing Using a Deep Linguistic GrammarYi ZhangLT-Lab, DFKI GmbH andDept of Computational LinguisticsSaarland UniversityD-66123 Saarbru?cken, Germanyyzhang@coli.uni-sb.deRui WangDept of Computational LinguisticsSaarland University66123 Saarbru?cken, Germanyrwang@coli.uni-sb.deAbstractPure statistical parsing systems achieveshigh in-domain accuracy but performspoorly out-domain.
In this paper, wepropose two different approaches to pro-duce syntactic dependency structures us-ing a large-scale hand-crafted HPSG gram-mar.
The dependency backbone of anHPSG analysis is used to provide generallinguistic insights which, when combinedwith state-of-the-art statistical dependencyparsing models, achieves performance im-provements on out-domain tests.
?1 IntroductionSyntactic dependency parsing is attracting moreand more research focus in recent years, par-tially due to its theory-neutral representation, butalso thanks to its wide deployment in variousNLP tasks (machine translation, textual entailmentrecognition, question answering, information ex-traction, etc.).
In combination with machine learn-ing methods, several statistical dependency pars-ing models have reached comparable high parsingaccuracy (McDonald et al, 2005b; Nivre et al,2007b).
In the meantime, successful continuationof CoNLL Shared Tasks since 2006 (Buchholz andMarsi, 2006; Nivre et al, 2007a; Surdeanu et al,2008) have witnessed how easy it has become totrain a statistical syntactic dependency parser pro-vided that there is annotated treebank.While the dissemination continues towards var-ious languages, several issues arise with suchpurely data-driven approaches.
One commonobservation is that statistical parser performancedrops significantly when tested on a dataset differ-ent from the training set.
For instance, when using?The first author thanks the German Excellence Clusterof Multimodal Computing and Interaction for the support ofthe work.
The second author is funded by the PIRE PhDscholarship program.the Wall Street Journal (WSJ) sections of the PennTreebank (Marcus et al, 1993) as training set, testson BROWN Sections typically result in a 6-8%drop in labeled attachment scores, although the av-erage sentence length is much shorter in BROWNthan that in WSJ.
The common interpretation isthat the test set is heterogeneous to the training set,hence in a different ?domain?
(in a loose sense).The typical cause of this is that the model overfitsthe training domain.
The concerns over randomchoice of training corpus leading to linguisticallyinadequate parsing systems increase over time.While the statistical revolution in the fieldof computational linguistics gaining high pub-licity, the conventional symbolic grammar-basedparsing approaches have undergone a quiet pe-riod of development during the past decade, andreemerged very recently with several large scalegrammar-driven parsing systems, benefiting fromthe combination of well-established linguistic the-ories and data-driven stochastic models.
The ob-vious advantage of such systems over pure statis-tical parsers is their usage of hand-coded linguis-tic knowledge irrespective of the training data.
Acommon problem with grammar-based parser isthe lack of robustness.
Also it is difficult to de-rive grammar compatible annotations to train thestatistical components.2 Parser Domain AdaptationIn recent years, two statistical dependency parsingsystems, MaltParser (Nivre et al, 2007b) andMSTParser (McDonald et al, 2005b), repre-senting different threads of research in data-drivenmachine learning approaches have obtained highpublicity, for their state-of-the-art performances inopen competitions such as CoNLL Shared Tasks.MaltParser follows the transition-based ap-proach, where parsing is done through a seriesof actions deterministically predicted by an oraclemodel.
MSTParser, on the other hand, follows378the graph-based approach where the best parsetree is acquired by searching for a spanning treewhich maximizes the score on either a partiallyor a fully connected graph with all words in thesentence as nodes (Eisner, 1996; McDonald et al,2005b).As reported in various evaluation competitions,the two systems achieved comparable perfor-mances.
More recently, approaches of combiningthese two parsers achieved even better dependencyaccuracy (Nivre and McDonald, 2008).
Grantedfor the differences between their approaches, bothsystems heavily rely on machine learning methodsto estimate the parsing model from an annotatedcorpus as training set.
Due to the heavy cost ofdeveloping high quality large scale syntacticallyannotated corpora, even for a resource-rich lan-guage like English, only very few of them meetsthe criteria for training a general purpose statisti-cal parsing model.
For instance, the text style ofWSJ is newswire, and most of the sentences arestatements.
Being lack of non-statements in thetraining data could cause problems, when the test-ing data contain many interrogative or imperativesentences as in the BROWN corpus.
Therefore, theunbalanced distribution of linguistic phenomenain the training data leads to inadequate parser out-put structures.
Also, the financial domain specificterminology seen in WSJ can skew the interpreta-tion of daily life sentences seen in BROWN.There has been a substantial amount of work onparser adaptation, especially from WSJ to BROWN.Gildea (2001) compared results from differentcombinations of the training and testing data todemonstrate that the size of the feature modelcan be reduced via excluding ?domain-dependent?features, while the performance could still be pre-served.
Furthermore, he also pointed out that if theadditional training data is heterogeneous from theoriginal one, the parser will not obtain a substan-tially better performance.
Bacchiani et al (2006)generalized the previous approaches using a maxi-mum a posteriori (MAP) framework and proposedboth supervised and unsupervised adaptation ofstatistical parsers.
McClosky et al (2006) and Mc-Closky et al (2008) have shown that out-domainparser performance can be improved with self-training on a large amount of unlabeled data.
Mostof these approaches focused on the machine learn-ing perspective instead of the linguistic knowledgeembraced in the parsers.
Little study has been re-ported on approaches of incorporating linguisticfeatures to make the parser less dependent on thenature of training and testing dataset, without re-sorting to huge amount of unlabeled out-domaindata.
In addition, most of the previous work havebeen focusing on constituent-based parsing, whilethe domain adaptation of the dependency parsinghas not been fully explored.Taking a different approach towards parsing,grammar-based parsers appear to have muchlinguistic knowledge encoded within the gram-mars.
In recent years, several of these linguisti-cally motivated grammar-driven parsing systemsachieved high accuracy which are comparable tothe treebank-based statistical parsers.
Notably arethe constraint-based linguistic frameworks withmathematical rigor, and provide grammatical anal-yses for a large variety of phenomena.
For in-stance, the Head-Driven Phrase Structure Gram-mar (Pollard and Sag, 1994) has been success-fully applied in several parsing systems for morethan a dozen of languages.
Some of these gram-mars, such as the English Resource Grammar(ERG; Flickinger (2002)), have undergone overdecades of continuous development, and provideprecise linguistic analyses for a broad range ofphenomena.
These linguistic knowledge are en-coded in highly generalized form according to lin-guists?
reflection for the target languages, and tendto be largely independent from any specific do-main.The main issue of parsing with precision gram-mars is that broad coverage and high precision onlinguistic phenomena do not directly guarantee ro-bustness of the parser with noisy real world texts.Also, the detailed linguistic analysis is not alwaysof the highest interest to all NLP applications.
Itis not always straightforward to scale down thedetailed analyses embraced by deep grammars toa shallower representation which is more acces-sible for specific NLP tasks.
On the other hand,since the dependency representation is relativelytheory-neutral, it is possible to convert from otherframeworks into its backbone representation in de-pendencies.
For HPSG, this is further assisted bythe clear marking of head daughters in headedphrases.
Although the statistical components ofthe grammar-driven parser might be still biasedby the training domain, the hand-coded grammarrules guarantee the basic linguistic constraints tobe met.
This not to say that domain adaptation is379HPSG DBExtractionHPSG DBFeature ModelsMSTParserFeature ModelMaltParserFeature ModelSection 3.1Section 3.3McDonaldet al, 2005Nivreet al, 2007Nivre andMcDonald,2008Section 4.2Section 4.3Figure 1: Different dependency parsing modelsand their combinations.
DB stands for dependencybackbone.not an issue for grammar-based parsing systems,but the built-in linguistic knowledge can be ex-plored to reduce the performance drop in pure sta-tistical approaches.3 Dependency Parsing with HPSGIn this section, we explore two possible applica-tions of the HPSG parsing onto the syntactic de-pendency parsing task.
One is to extract depen-dency backbone from the HPSG analyses of thesentences and directly convert them into the tar-get representation; the other way is to encode theHPSG outputs as additional features into the ex-isting statistical dependency parsing models.
Inthe previous work, Nivre and McDonald (2008)have integrated MSTParser and MaltParserby feeding one parser?s output as features into theother.
The relationships between our work andtheir work are roughly shown in Figure 1.3.1 Extracting Dependency Backbone fromHPSG Derivation TreeGiven a sentence, each parse produced by theparser is represented by a typed feature structure,which recursively embeds smaller feature struc-tures for lower level phrases or words.
For thepurpose of dependency backbone extraction, weonly look at the derivation tree which correspondsto the constituent tree of an HPSG analysis, withall non-terminal nodes labeled by the names of thegrammar rules applied.
Figure 2 shows an exam-ple.
Note that all grammar rules in ERG are ei-ther unary or binary, giving us relatively deep treeswhen compared with annotations such as PennTreebank.
Conceptually, this conversion is sim-ilar to the conversions from deeper structures toGR reprsentations reported by Clark and Curran(2007) and Miyao et al (2007).np_title_cmpndms_n2 proper_npsubjhgeneric_proper_neHaagplay_v1hcompproper_npgeneric_proper_neElianti.playsMs.Figure 2: An example of an HPSG derivation treewith ERGMs.
Haag plays Elianti.hcompnp_title_cmpnd subjhFigure 3: An HPSG dependency backbone struc-tureThe dependency backbone extraction works byfirst identifying the head daughter for each bi-nary grammar rule, and then propagating the headword of the head daughter upwards to their par-ents, and finally creating a dependency relation, la-beled with the HPSG rule name of the parent node,from the head word of the parent to the head wordof the non-head daughter.
See Figure 3 for an ex-ample of such an extracted backbone.For the experiments in this paper, we used July-08 version of the ERG, which contains in total185 grammar rules (morphological rules are notcounted).
Among them, 61 are unary rules, and124 are binary.
Many of the binary rules areclearly marked as headed phrases.
The gram-mar also indicates whether the head is on the left(head-initial) or on the right (head-final).
How-ever, there are still quite a few binary rules whichare not marked as headed-phrases (according tothe linguistic theory), e.g.
rules to handle coor-dinations, appositions, compound nouns, etc.
Forthese rules, we refer to the conversion of the PennTreebank into dependency structures used in theCoNLL 2008 Shared Task, and mark the heads ofthese rules in a way that will arrive at a compat-ible dependency backbone.
For instance, the leftmost daughters of coordination rules are markedas heads.
In combination with the right-branchinganalysis of coordination in ERG, this leads to thesame dependency attachment in the CoNLL syn-tax.
Eventually, 37 binary rules are marked witha head daughter on the left, and 86 with a headdaughter on the right.Although the extracted dependency is similar to380the CoNLL shared task dependency structures, mi-nor systematic differences still exist for some phe-nomena.
For example, the possessive ??s?
is an-notated to be governed by its preceding word inCoNLL dependency; while in HPSG, it is treated asthe head of a ?specifier-head?
construction, hencegoverning the preceding word in the dependencybackbone.
With several simple tree rewritingrules, we are able to fix the most frequent inconsis-tencies.
With the rule-based backbone extractionand repair, we can finally turn our HPSG parseroutputs into dependency structures1.
The unla-beled attachment agreement between the HPSGbackbone and CoNLL dependency annotation willbe shown in Section 4.2.3.2 Robust Parsing with HPSGAs mentioned in Section 2, one pitfall of using aprecision-oriented grammar in parsing is its lackof robustness.
Even with a large scale broad cover-age grammar like ERG, using our settings we onlyachieved 75% of sentential coverage2.
Given thatthe grammar has never been fine-tuned for the fi-nancial domain, such coverage is very encourag-ing.
But still, the remaining unparsed sentencescomprise a big coverage gap.Different strategies can be taken here.
Onecan either keep the high precision by only look-ing at full parses from the HPSG parser, of whichthe analyses are completely admitted by gram-mar constraints.
Or one can trade precision forextra robustness by looking at the most proba-ble incomplete analysis.
Several partial parsingstrategies have been proposed (Kasper et al, 1999;Zhang and Kordoni, 2008) as the robust fallbacksfor the parser when no available analysis can bederived.
In our experiment, we select the se-quence of most likely fragment analyses accord-ing to their local disambiguation scores as the par-tial parse.
When combined with the dependencybackbone extraction, partial parses generate dis-joint tree fragments.
We simply attach all frag-ments onto the virtual root node.1It is also possible map from HPSG rule names (togetherwith the part-of-speech of head and dependent) to CoNLLdependency labels.
This remains to be explored in the future.2More recent study shows that with carefully designedretokenization and preprocessing rules, over 80% sententialcoverage can be achieved on the WSJ sections of the PennTreebank data using the same version of ERG.
The numbersreported in this paper are based on a simpler preprocessor,using rather strict time/memory limits for the parser.
Hencethe coverage number reported here should not be taken as anabsolute measure of grammar performance.3.3 Using Feature-Based ModelsBesides directly using the dependency backboneof the HPSG output, we could also use it for build-ing feature-based models of statistical dependencyparsers.
Since we focus on the domain adapta-tion issue, we incorporate a less domain dependentlanguage resource (i.e.
the HPSG parsing outputsusing ERG) into the features models of statisticalparsers.
As mordern grammar-based parsers hasachieved high runtime efficency (with our HPSGparser parsing at an average speed of?3 sentencesper second), this adds up to an acceptable over-head.3.3.1 Feature Model with MSTParserAs mentioned before, MSTParser is a graph-based statistical dependency parser, whose learn-ing procedure can be viewed as the assignmentof different weights to all kinds of dependencyarcs.
Therefore, the feature model focuses on eachkind of head-child pair in the dependency tree, andmainly contains four categories of features (Mc-donald et al, 2005a): basic uni-gram features, ba-sic bi-gram features, in-between POS features, andsurrounding POS features.
It is emphasized by theauthors that the last two categories contribute alarge improvement to the performance and bringthe parser to the state-of-the-art accuracy.Therefore, we extend this feature set by addingfour more feature categories, which are similar tothe original ones, but the dependency relation wasreplaced by the dependency backbone of the HPSGoutputs.
The extended feature set is shown in Ta-ble 1.3.3.2 Feature Model with MaltParserMaltParser is another trend of dependencyparser, which is based on transitions.
The learningprocedure is to train a statistical model, which canhelp the parser to decide which operation to take ateach parsing status.
The basic data structures are astack, where the constructed dependency graph isstored, and an input queue, where the unprocesseddata are put.
Therefore, the feature model focuseson the tokens close to the top of the stack and alsothe head of the queue.Provided with the original features used inMaltParser, we add extra ones about the toptoken in the stack and the head token of the queuederived from the HPSG dependency backbone.The extended feature set is shown in Table 2 (thenew features are listed separately).381Uni-gram Features: h-w,h-p; h-w; h-p; c-w,c-p; c-w; c-pBi-gram Features: h-w,h-p,c-w,c-p; h-p,c-w,c-p; h-w,c-w,c-p; h-w,h-p,c-p; h-w,h-p,c-w; h-w,c-w; h-p,c-pPOS Features of words in between: h-p,b-p,c-pPOS Features of words surround: h-p,h-p+1,c-p-1,c-p; h-p-1,h-p,c-p-1,c-p; h-p,h-p+1,c-p,c-p+1; h-p-1,h-p,c-p,c-p+1Table 1: The Extra Feature Set for MSTParser.
h: the HPSG head of the current token; c: the currenttoken; b: each token in between; -1/+1: the previous/next token; w: word form; p: POSPOS Features: s[0]-p; s[1]-p; i[0]-p; i[1]-p; i[2]-p; i[3]-pWord Form Features: s[0]-h-w; s[0]-w; i[0]-w; i[1]-wDependency Features: s[0]-lmc-d; s[0]-d; s[0]-rmc-d; i[0]-lmc-dNew Features: s[0]-hh-w; s[0]-hh-p; s[0]-hr; i[0]-hh-w; i[0]-hh-p; i[0]-hrTable 2: The Extended Feature Set for MaltParser.
s[0]/s[1]: the first and second token on the top ofthe stack; i[0]/i[1]/i[2]/i[3]: front tokens in the input queue; h: head of the token; hh: HPSG DB head ofthe token; w: word form; p: POS; d: dependency relation; hr: HPSG rule; lmc/rmc: left-/right-most childWith the extra features, we hope that the train-ing of the statistical model will not overfit the in-domain data, but be able to deal with domain in-dependent linguistic phenomena as well.4 Experiment Results & Error AnalysesTo evaluate the performance of our differentdependency parsing models, we tested our ap-proaches on several dependency treebanks for En-glish in a similar spirit to the CoNLL 2006-2008Shared Tasks.
In this section, we will first de-scribe the datasets, then present the results.
Anerror analysis is also carried out to show both prosand cons of different models.4.1 DatasetsIn previous years of CoNLL Shared Tasks, sev-eral datasets have been created for the purposeof dependency parser evaluation.
Most of themare converted automatically from existing tree-banks in various forms.
Our experiments adhereto the CoNLL 2008 dependency syntax (Yamadaet al 2003, Johansson et al 2007) which wasused to convert Penn-Treebank constituent treesinto single-head, single-root, traceless and non-projective dependencies.WSJ This dataset comprises of three portions.The larger part is converted from the Penn Tree-bank Wall Street Journal Sections #2?#21, andis used for training statistical dependency parsingmodels; the smaller part, which covers sentencesfrom Section #23, is used for testing.Brown This dataset contains a subset of con-verted sentences from BROWN sections of thePenn Treebank.
It is used for the out-domain test.PChemtb This dataset was extracted from thePennBioIE CYP corpus, containing 195 sentencesfrom biomedical domain.
The same dataset hasbeen used for the domain adaptation track of theCoNLL 2007 Shared Task.
Although the originalannotation scheme is similar to the Penn Treebank,the dependency extraction setting is slightly dif-ferent to the CoNLLWSJ dependencies (e.g.
thecoordinations).Childes This is another out-domain test set fromthe children language component of the TalkBank,containing dialogs between parents and children.This is the other datasets used in the domain adap-tation track of the CoNLL 2007 Shared Task.
Thedataset is annotated with unlabeled dependencies.As have been reported by others, several system-atic differences in the original CHILDES annota-tion scheme has led to the poor system perfor-mances on this track of the Shared Task in 2007.Two main differences concern a) root attach-ments, and b) coordinations.
With several sim-ple heuristics, we change the annotation scheme ofthe original dataset to match the Penn Treebank-based datasets.
The new dataset is referred to asCHILDES*.4.2 HPSG Backbone as Dependency ParserFirst we test the agreement between HPSG depen-dency backbone and CoNLL dependency.
Whileapproximating a target dependency structure withrule-based conversion is not the main focus of thiswork, the agreement between two representationsgives indication on how similar and consistent thetwo representations are, and a rough impression ofwhether the feature-based models can benefit fromthe HPSG backbone.382# sentence ?
w/s DB(F)% DB(P)%WSJ 2399 24.04 50.68 63.85BROWN 425 16.96 66.36 76.25PCHEMTB 195 25.65 50.27 61.60CHILDES* 666 7.51 67.37 70.66WSJ-P 1796 (75%) 22.25 71.33 ?BROWN-P 375 (88%) 15.74 80.04 ?PCHEMTB-P 147 (75%) 23.99 69.27 ?CHILDES*-P 595 (89%) 7.49 73.91 ?Table 3: Agreement between HPSG dependencybackbone and CoNLL 2008 dependency in unla-beled attachment score.
DB(F): full parsing mode;DB(P): partial parsing mode; Punctuations are ex-cluded from the evaluation.The PET parser, an efficient parser HPSG parseris used in combination with ERG to parse thetest sets.
Note that the training set is not used.The grammar is not adapted for any of these spe-cific domain.
To pick the most probable read-ing from HPSG parsing outputs, we used a dis-criminative parse selection model as describedin (Toutanova et al, 2002) trained on the LOGONTreebank (Oepen et al, 2004), which is signifi-cantly different from any of the test domain.
Thetreebank contains about 9K sentences for whichHPSG analyses are manually disambiguated.
Thedifference in annotation make it difficult to sim-ply merge this HPSG treebank into the training setof the dependency parser.
Also, as Gildea (2001)suggests, adding such heterogeneous data to thetraining set will not automatically lead to perfor-mance improvement.
It should be noted that do-main adaptation also presents a challenge to thedisambiguation model of the HPSG parser.
Alldatasets we use in our should be considered out-domain to the HPSG disambiguation model.Table 3 shows the agreement between the HPSGbackbone and CoNLL dependency in unlabeled at-tachment score (UAS).
The parser is set in eitherfull parsing or partial parsing mode.
Partial pars-ing is used as a fallback when full parse is notavailable.
UAS are reported on all complete testsets, as well as fully parsed subsets (suffixed with?-p?
).It is not surprising to see that, without a de-cent fallback strategy, the full parse HPSG back-bone suffers from insufficient coverage.
Since thegrammar coverage is statistically correlated to theaverage sentence length, the worst performance isobserved for the PCHEMTB.
Although sentencesin CHILDES* are significantly shorter than thosein BROWN, there is a fairly large amount of lesswell-formed sentences (either as a nature of childlanguage, or due to the transcription from spokendialogs).
This leads to the close performance be-tween these two datasets.
PCHEMTB appears to bethe most difficult one for the HPSG parser.
Thepartial parsing fallback sets up a good safe net forsentences that fail to parse.
Without resorting toany external resource, the performance was sig-nificantly improved on all complete test sets.When we set the coverage of the HPSG gram-mar aside and only compare performance on thesubsets of these datasets which are fully parsedby the HPSG grammar, the unlabeled attachmentscore jumps up significantly.
Most notable isthat the dependency backbone achieved over 80%UAS on BROWN, which is close to the perfor-mance of state-of-the-art statistical dependencyparsing systems trained on WSJ (see Table 5 andTable 4).
The performance difference across datasets correlates to varying levels of difficulties inlinguists?
view.
Our error analysis does confirmthat frequent errors occur in WSJ test with finan-cial terminology missing from the grammar lexi-con.
The relative performance difference betweenthe WSJ and BROWN test is contrary to the resultsobserved for statistical parsers trained on WSJ.To further investigate the effect of HPSG parsedisambiguation model on the dependency back-bone accuracy, we used a set of 222 sentencesfrom section of WSJ which have been parsed withERG and manually disambiguated.
Comparingto the WSJ-P result in Table 3, we improved theagreement with CoNLL dependency by another8% (an upper-bound in case of a perfect disam-biguation model).4.3 Statistical Dependency Parsing withHPSG FeaturesSimilar evaluations were carried out for the statis-tical parsers using extra HPSG dependency back-bone as features.
It should be noted that the per-formance comparison between MSTParser andMaltParser is not the aim of this experiment,and the difference might be introduced by the spe-cific settings we use for each parser.
Instead, per-formance variance using different feature modelsis the main subject.
Also, performance drop onout-domain tests shows how domain dependentthe feature models are.For MaltParser, we use Arc-Eager algo-383rithm, and polynomial kernel with d = 2.
ForMSTParser, we use 1st order features and a pro-jective decoder (Eisner, 1996).When incorporating HPSG features, two set-tings are used.
The PARTIAL model is derived byrobust-parsing the entire training data set and ex-tract features from every sentence to train a uni-fied model.
When testing, the PARTIAL model isused alone to determine the dependency structuresof the input sentences.
The FULL model, on theother hand is only trained on the full parsed subsetof sentences, and only used to predict dependencystructures for sentences that the grammar parses.For the unparsed sentences, the original modelswithout HPSG features are used.Parser performances are measured usingboth labeled and unlabeled attachment scores(LAS/UAS).
For unlabeled CHILDES* data, onlyUAS numbers are reported.
Table 4 and 5 summa-rize results for MSTParser and MaltParser,respectively.With both parsers, we see slight performancedrops with both HPSG feature models on in-domain tests (WSJ), compared with the originalmodels.
However, on out-domain tests, full-parseHPSG feature models consistently outperform theoriginal models for both parsers.
The difference iseven larger when only the HPSG fully parsed sub-sets of the test sets are concerned.
When we lookat the performance difference between in-domainand out-domain tests for each feature model, weobserve that the drop is significantly smaller forthe extended models with HPSG features.We should note that we have not done anyfeature selection for our HPSG feature models.Nor have we used the best known configurationsof the existing parsers (e.g.
second order fea-tures in MSTParser).
Admittedly the results onPCHEMTB are lower than the best reported resultsin CoNLL 2007 Shared Task, we shall note that weare not using any in-domain unlabeled data.
Also,the poor performance of the HPSG parser on thisdataset indicates that the parser performance dropis more related to domain-specific phenomena andnot general linguistic knowledge.
Nevertheless,the drops when compared to in-domain tests areconstantly decreased with the help of HPSG analy-ses features.
With the results on BROWN, the per-formance of our HPSG feature models will rank2nd on the out-domain test for the CoNLL 2008Shared Task.Unlike the observations in Section 4.2, the par-tial parsing mode does not work well as a fall-back in the feature models.
In most cases, itsperformances are between the original models andthe full-parse HPSG feature models.
The partialparsing features obscure the linguistic certainty ofgrammatical structures produced in the full model.When used as features, such uncertainty leadsto further confusion.
Practically, falling back tothe original models works better when HPSG fullparse is not available.4.4 Error AnalysesQualitative error analysis is also performed.
Sinceour work focuses on the domain adaptation, wemanually compare the outputs of the original sta-tistical models, the dependency backbone, and thefeature-based models on the out-domain data, i.e.the BROWN data set (both labeled and unlabeledresults) and the CHILDES* data set (only unlabeledresults).For the dependency attachment (i.e.
unlabeleddependency relation), fine-grained HPSG featuresdo help the parser to deal with colloquial sen-tences, such as ?What?s wrong with you??.
Theoriginal parser wrongly takes ?what?
as the root ofthe dependency tree and ??s?
is attached to ?what?.The dependency backbone correctly finds out theroot, and thus guide the extended model to makethe right prediction.
A correct structure of ?...,were now neither active nor really relaxed.?
is alsopredicted by our model, while the original modelwrongly attaches ?really?
to ?nor?
and ?relaxed?to ?were?.
The rich linguistic knowledge fromthe HPSG outputs also shows its usefulness.
Forexample, in a sentence from the CHILDES* data,?Did you put dolly?s shoes on?
?, the verb phrase?put on?
can be captured by the HPSG backbone,while the original model attaches ?on?
to the adja-cent token ?shoes?.For the dependency labels, the most diffi-culty comes from the prepositions.
For example,?Scotty drove home alone in the Plymouth?, allthe systems get the head of ?in?
correct, whichis ?drove?.
However, none of the dependency la-bels is correct.
The original model predicts the?DIR?
relation, the extended feature-based modelsays ?TMP?, but the gold standard annotation is?LOC?.
This is because the HPSG dependencybackbone knows that ?in the Plymouth?
is an ad-junct of ?drove?, but whether it is a temporal or384Original PARTIAL FULLLAS% UAS% LAS% UAS% LAS% UAS%WSJ 87.38 90.35 87.06 90.03 86.87 89.91BROWN 80.46 (-6.92) 86.26 (-4.09) 80.55 (-6.51) 86.17 (-3.86) 80.92 (-5.95) 86.58 (-3.33)PCHEMTB 53.37 (-33.8) 62.11 (-28.24) 54.69 (-32.37) 64.09 (-25.94) 56.45 (-30.42) 65.77 (-24.14)CHILDES* ?
72.17 (-18.18) ?
74.91 (-15.12) ?
75.64 (-14.27)WSJ-P 87.86 90.88 87.78 90.85 87.12 90.25BROWN-P 81.58 (-6.28) 87.41 (-3.47) 81.92 (-5.86) 87.51 (-3.34) 82.14 (-4.98) 87.80 (-2.45)PCHEMTB-P 56.32 (-31.54) 65.26 (-25.63) 59.36 (-28.42) 69.20 (-21.65) 60.69 (-26.43) 70.45 (-19.80)CHILDES*-P ?
72.88 (-18.00) ?
76.02 (-14.83) ?
76.76 (-13.49)Table 4: Performance of the MSTParser with different feature models.
Numbers in parentheses areperformance drops in out-domain tests, comparing to in-domain results.
The upper part represents theresults on the complete data sets, and the lower part is on the fully parsed subsets, indicated by ?-P?.Original PARTIAL FULLLAS% UAS% LAS% UAS% LAS% UAS%WSJ 86.47 88.97 85.39 88.10 85.66 88.40BROWN 79.41 (-7.06) 84.75 (-4.22) 79.10 (-6.29) 84.58 (-3.52) 79.56 (-6.10) 85.24 (-3.16)PCHEMTB 61.05 (-25.42) 71.32 (-17.65) 61.01 (-24.38) 70.99 (-17.11) 60.93 (-24.73) 70.89 (-17.51)CHILDES* ?
74.97 (-14.00) ?
75.64 (-12.46) ?
76.18 (-12.22)WSJ-P 86.99 89.58 86.09 88.83 85.82 88.76BROWN-P 80.43 (-6.56) 85.78 (-3.80) 80.46 (-5.63) 85.94 (-2.89) 80.62 (-5.20) 86.38 (-2.38)PCHEMTB-P 63.33 (-23.66) 73.54 (-16.04) 63.27 (-22.82) 73.31 (-15.52) 63.16 (-22.66) 73.06 (-15.70)CHILDES*-P ?
75.95 (-13.63) ?
77.05 (-11.78) ?
77.30 (-11.46)Table 5: Performance of the MaltParser with different feature models.locative expression cannot be easily predicted atthe pure syntactic level.
This also suggests a jointlearning of syntactic and semantic dependencies,as proposed in the CoNLL 2008 Shared Task.Instances of wrong HPSG analyses have alsobeen observed as one source of errors.
For most ofthe cases, a correct reading exists, but not pickedby our parse selection model.
This happens moreoften with the WSJ test set, partially contributingto the low performance.5 Conclusion & Future WorkSimilar to our work, Sagae et al (2007) also con-sidered the combination of dependency parsingwith an HPSG parser, although their work was touse statistical dependency parser outputs as softconstraints to improve the HPSG parsing.
Nev-ertheless, a similar backbone extraction algorithmwas used to map between different representa-tions.
Similar work also exists in the constituent-based approaches, where CFG backbones wereused to improve the efficiency and robustness ofHPSG parsers (Matsuzaki et al, 2007; Zhang andKordoni, 2008).In this paper, we restricted our investigation onthe syntactic evaluation using labeled/unlabeledattachment scores.
Recent discussions in theparsing community about meaningful cross-framework evaluation metrics have suggested touse measures that are semantically informed.
Inthis spirit, Zhang et al (2008) showed that the se-mantic outputs of the same HPSG parser helps inthe semantic role labeling task.
Consistent withthe results reported in this paper, more improve-ment was achieved on the out-domain tests in theirwork as well.Although the experiments presented in this pa-per were carried out on a HPSG grammar for En-glish, the method can be easily adapted to workwith other grammar frameworks (e.g.
LFG, CCG,TAG, etc.
), as well as on langugages other thanEnglish.
We chose to use a hand-crafted grammar,so that the effect of training corpus on the deepparser is minimized (with the exception of the lex-ical coverage and disambiguation model).As mentioned in Section 4.4, the performanceof our HPSG parse selection model varies acrossdifferent domains.
This indicates that, althoughthe deep grammar embraces domain independentlinguistic knowledge, the lexical coverage and thedisambiguation process among permissible read-ings is still domain dependent.
With the map-ping between HPSG analyses and their depen-dency backbones, one can potentially use existingdependency treebanks to help overcome the insuf-ficient data problem for deep parse selection mod-els.385ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, and RichardSproat.
2006.
Map adaptation of stochastic grammars.Computer speech and language, 20(1):41?68.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceedingsof the 10th Conference on Computational Natural Lan-guage Learning (CoNLL-X), New York City, USA.Stephen Clark and James Curran.
2007.
Formalism-independent parser evaluation with ccg and depbank.
InProceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 248?255, Prague,Czech Republic.Jason Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings of the16th International Conference on Computational Linguis-tics (COLING-96), pages 340?345, Copenhagen, Den-mark.Dan Flickinger.
2002.
On building a more efficient grammarby exploiting types.
In Stephan Oepen, Dan Flickinger,Jun?ichi Tsujii, and Hans Uszkoreit, editors, CollaborativeLanguage Engineering, pages 1?17.
CSLI Publications.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of the 2001 Conference on Em-pirical Methods in Natural Language Processing, pages167?202, Pittsburgh, USA.Walter Kasper, Bernd Kiefer, Hans-Ulrich Krieger, C.J.Rupp, and Karsten Worm.
1999.
Charting the depths ofrobust speech processing.
In Proceedings of the 37th An-nual Meeting of the Association for Computational Lin-guistics (ACL 1999), pages 405?412, Maryland, USA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof english: The penn treebank.
Computational Linguis-tics, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007.Efficient HPSG parsing with supertagging and CFG-filtering.
In Proceedings of the 20th International JointConference on Artificial Intelligence (IJCAI 2007), pages1671?1676, Hyderabad, India.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adaptation.In Proceedings of the 21st International Conference onComputational Linguistics and the 44th Annual Meetingof the Association for Computational Linguistics, pages337?344, Sydney, Australia.David McClosky, Eugene Charniak, and Mark Johnson.2008.
When is self-training effective for parsing?
InProceedings of the 22nd International Conference onComputational Linguistics (Coling 2008), pages 561?568,Manchester, UK.Ryan Mcdonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL?05),pages 91?98, Ann Arbor, Michigan.Ryan McDonald, Fernando Pereira, Kiril Ribarov, and JanHajic.
2005b.
Non-Projective Dependency Parsing us-ing Spanning Tree Algorithms.
In Proceedings of HLT-EMNLP 2005, pages 523?530, Vancouver, Canada.Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii.
2007.
To-wards framework-independent evaluation of deep linguis-tic parsers.
In Proceedings of the GEAF07 Workshop,pages 238?258, Stanford, CA.Joakim Nivre and Ryan McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
In Pro-ceedings of ACL-08: HLT, pages 950?958, Columbus,Ohio, June.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a.The CoNLL 2007 shared task on dependency parsing.In Proceedings of EMNLP-CoNLL 2007, pages 915?932,Prague, Czech Republic.Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov, andErwin Marsi.
2007b.
Maltparser: A language-independent system for data-driven dependency parsing.Natural Language Engineering, 13(1):1?41.Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-dal, Dorothee Beermann, John Carroll, Dan Flickinger,Lars Hellan, Janne Bondi Johannessen, Paul Meurer,Torbj?rn Nordga?rd, and Victoria Rose?n.
2004.
Som a?kapp-ete med trollet?
Towards MRS-Based Norwegian?English Machine Translation.
In Proceedings of the 10thInternational Conference on Theoretical and Methodolog-ical Issues in Machine Translation, Baltimore, USA.Carl J. Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of Chicago Press,Chicago, USA.Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii.
2007.
Hpsgparsing with shallow dependency constraints.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 624?631, Prague,Czech Republic.Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu?
?sMa`rquez, and Joakim Nivre.
2008.
The CoNLL-2008shared task on joint parsing of syntactic and semanticdependencies.
In Proceedings of the 12th Conferenceon Computational Natural Language Learning (CoNLL-2008), Manchester, UK.Kristina Toutanova, Christoper D. Manning, Stuart M.Shieber, Dan Flickinger, and Stephan Oepen.
2002.
Parseranking for a rich HPSG grammar.
In Proceedings of the1st Workshop on Treebanks and Linguistic Theories (TLT2002), pages 253?263, Sozopol, Bulgaria.Yi Zhang and Valia Kordoni.
2008.
Robust Parsing with aLarge HPSG Grammar.
In Proceedings of the Sixth Inter-national Language Resources and Evaluation (LREC?08),Marrakech, Morocco.Yi Zhang, Rui Wang, and Hans Uszkoreit.
2008.
Hy-brid Learning of Dependency Structures from Heteroge-neous Linguistic Resources.
In Proceedings of the TwelfthConference on Computational Natural Language Learn-ing (CoNLL 2008), pages 198?202, Manchester, UK.386
