Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 285?289, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUKP-BIU: Similarity and Entailment Metrics for Student Response AnalysisTorsten Zesch?
Omer Levy?
Iryna Gurevych?
Ido Dagan??
Ubiquitous Knowledge Processing Lab ?
Natural Language Processing LabComputer Science Department Computer Science DepartmentTechnische Universita?t Darmstadt Bar-Ilan UniversityAbstractOur system combines text similarity measureswith a textual entailment system.
In the maintask, we focused on the influence of lexical-ized versus unlexicalized features, and howthey affect performance on unseen questionsand domains.
We also participated in the pi-lot partial entailment task, where our systemsignificantly outperforms a strong baseline.1 IntroductionThe Joint Student Response Analysis and 8th Rec-ognizing Textual Entailment Challenge (Dzikovskaet al 2013) brings together two important dimen-sions of Natural Language Processing: real-worldapplications and semantic inference technologies.The challenge focuses on the domain of middle-school quizzes, and attempts to emulate the metic-ulous marking process that teachers do on a dailybasis.
Given a question, a reference answer, and astudent?s answer, the task is to determine whetherthe student answered correctly.
While this is nota new task in itself, the challenge focuses on em-ploying textual entailment technologies as the back-bone of this educational application.
As a conse-quence, we formalize the question ?Did the studentanswer correctly??
as ?Can the reference answer beinferred from the student?s answer??.
This questioncan (hopefully) be answered by a textual entailmentsystem (Dagan et al 2009).The challenge contains two tasks: In the maintask, the system must analyze each answer as awhole.
There are three settings, where each one de-fines ?correct?
in a different resolution.
The highest-resolution setting defines five different classes or?correctness values?
: correct, partially correct, con-tradictory, irrelevant, non-domain.
In the pilot task,critical elements of the answer need to be analyzedseparately.
Each such element is called a facet, andis defined as a pair of words that are critical in an-swering the question.
As there is a substantial dif-ference between the two tasks, we designed siblingarchitectures for each task, and divide the main partof the paper accordingly.Our goal is to provide a robust architecture for stu-dent response analysis, that can generalize and per-form well in multiple domains.
Moreover, we areinterested in evaluating how well general-purposetechnologies will perform in this setting.
We there-fore approach the challenge by combining two suchtechnologies: DKPro Similarity ?an extensive suiteof text similarity measures?
that has been success-fully applied in other settings like the SemEval 2012task on semantic textual similarity (Ba?r et al 2012a)or reuse detection (Ba?r et al 2012b).BIUTEE, the Bar-Ilan University Textual Entail-ment Engine (Stern and Dagan, 2011), which hasshown state-of-the-art performance on recognizingtextual entailment challenges.
Our systems use bothtechnologies to extract features, and combine themin a supervised model.
Indeed, this approach worksrelatively well (with respect to other entries in thechallenge), especially in unseen domains.2 Background2.1 Text SimilarityText similarity is a bidirectional, continuous func-tion which operates on pairs of texts of any lengthand returns a numeric score of how similar one textis to the other.
In previous work (Mihalcea et al2852006; Gabrilovich and Markovitch, 2007; Landaueret al 1998), only a single text similarity measurehas typically been applied to text pairs.
However,as recent work (Ba?r et al 2012a; Ba?r et al 2012b)has shown, text similarity computation can be muchimproved when a variety of measures are combined.In recent years, UKP lab at TU Darmstadt has de-veloped DKPro Similarity1, an open source toolkitfor analyzing text similarity.
It is part of theDKPro framework for natural language processing(Gurevych et al 2007).
DKPro Similarity excelsat the tasks of measuring semantic textual simi-larity (STS) and detecting text reuse (DTR), hav-ing achieved the best performance in previous chal-lenges (Ba?r et al 2012a; Ba?r et al 2012b).2.2 Textual EntailmentThe textual entailment paradigm is a generic frame-work for applied semantic inference (Dagan et al2009).
The most prevalent task of textual entailmentis to recognize whether the meaning of a target nat-ural language statement (H for hypothesis) can beinferred from another piece of text (T for text).
Ap-parently, this core task underlies semantic inferencein many text applications.
The task of analyzing stu-dent responses is one such example.
By assigningthe student?s answer as T and the reference answeras H , we are basically asking whether one can in-fer the correct (reference) answer from the student?sresponse.
In recent years, Bar-Ilan University hasdeveloped BIUTEE (Stern and Dagan, 2011), an ex-tensive textual entailment recognition engine.
BI-UTEE tries to convert T (represented as a depen-dency tree) to H .
It does so by applying a series ofknowledge-based transformations, such as synonymsubstitution, active-passive conversion, and more.BIUTEE is publicly available as open source.23 Main TaskIn this section, we explain how we approached themain task, in which the system needs to analyze eachanswer as a whole.
After describing our system?s ar-chitecture, we explain how we selected training datafor the different scenarios in the main task.
We then1code.google.com/p/dkpro-similarity-asl2cs.biu.ac.il/?nlp/downloads/biuteeprovide the details for each submitted run, and fi-nally, our empirical results.3.1 System DescriptionWe build a system based on the Apache UIMAframework (Ferrucci and Lally, 2004) and DKProLab (Eckart de Castilho and Gurevych, 2011).
Weuse DKPro Core3 for preprocessing.
Specifically,we used the default DKPro segmenter, TreeTaggerPOS tagger and chunker, Jazzy Spell Checker, andthe Stanford parser.4 We trained a supervised model(Naive Bayes) using Weka (Hall et al 2009) withfeature extraction based on clearTK (Ogren et al2008).
The following features have been used:BOW features Bag-of-word features are based onthe assumption that certain words need to appear ina correct answer.
We used a mixture of token uni-grams, bigrams, and trigrams, where each n-gram isa binary feature that can either be true or false for adocument.5 Additionally, we also used the numberof tokens in the student answer as another feature inthis group.Syntactic Features We extend BOW featureswith syntactic functions by adding dependency andphrase n-grams.
Dependency n-grams are combina-tions of two tokens and their dependency relation.Phrase n-grams are combinations of the main verband the noun phrase left and right of the verb.
Inboth cases, we use the 10 most frequent n-grams.Basic Similarity Features This group of featurescomputes the similarity between the reference an-swer and the student answer.
In case there is morethan one reference answer, we compute all pairwisesimilarity scores and add the minimum, maximum,average, and median similarity.6Semantic Similarity Features are very similar tothe basic similarity features, except that we use se-mantic similarity measures in order to bridge a pos-sible vocabulary gap between the student and refer-ence answer.
We use the ESA measure (Gabrilovich3code.google.com/p/dkpro-core-asl/4DKPro Core v1.4.0, TreeTagger models v20130204.0,Stanford parser PCFG model v20120709.0.5Using the 750 most frequent n-grams gave good results onthe training set, so we also used this number for the test runs.6As basic similarity measures, we use greedy string tiling(Wise, 1996) with n = 3, longest common subsequence andlongest common substring (Allison and Dix, 1986), and wordn-gram containment(Lyon et al 2001) with n = 2.286and Markovitch, 2007) based on concept vectorsbuild from WordNet, Wiktionary, and Wikipedia.Spelling Features As spelling errors might be in-dicative of the answer quality, we use the number ofspelling errors normalized by the text length as anadditional feature.Entailment Features We run BIUTEE (Stern andDagan, 2011) on the test instance (as T ) with eachreference answer (as H), which results in an arrayof numerical entailment confidence values.
If thereis more than one reference answer, we compute allpairwise confidence scores and add the minimum,maximum, average, and median confidence.3.2 Data Selection RegimeThere are three scenarios under which our systemis expected to perform.
For each one, we chose (a-priori) a different set of examples for training.Unseen Answers Classify new answers to famil-iar questions.
Train on instances that have the samequestion as the test instance.Unseen Questions Classify new answers to un-seen (but related) questions.
Partition the questionsaccording to their IDs, creating sets of related ques-tions, and then train on all the instances that sharethe same partition as the test instance.Unseen Domains Classify new answers to unseenquestions from unseen domains.
Use all availabletraining data from the same dataset.3.3 Submitted RunsThe runs represent the different levels of lexicaliza-tion of the model which we expect to have stronginfluence in each scenario:Run 1 uses all features as described above.
Weexpect the BOW features to be helpful for the Un-seen Answers setting, but to be misleading for un-seen questions or domains, as the same word indi-cating a correct answer for one question might cor-respond to a wrong answer for another question.Run 2 uses only non-lexicalized features, i.e.
allfeatures except the BOW and syntactic features, inorder to assess the impact of the lexicalization thatoverfits on the topic of the questions.
We expect thisrun to be less sensitive to the topic changes in theUnseen Questions and Unseen Domains settings.Run 3 uses only the basic similarity and the en-tailment features.
It should indicate the baseline per-Unseen Unseen UnseenTask Run Answers Questions Domains2-way1 .734 .678 .6712 .665 .644 .6773 .662 .625 .6773-way1 .670 .573 .5722 .595 .561 .5773 .574 .540 .5765-way1 .590 .376 .4072 .495 .397 .3713 .461 .394 .376Table 1: Main task performance for the SciEntsBank testset.
We show weighted averageF1 for the three scenarios.Cor.
Par Con.
Irr.
Non.Correct 903 463 164 309 78Partially Correct 219 261 93 333 80Contradictory 61 126 91 103 36Irrelevant 209 229 119 476 189Non-Domain 0 0 0 2 18Table 2: Confusion matrix of Run 1 in the 5-way UnseenDomains scenario.
The vertical axis is the real class, thehorizontal axis is the predicted class.formance that can be expected without targeting thesystem towards a certain topic.3.4 Empirical ResultsTable 1 shows the F1-measure (weighted average)of the three runs.
As was expected for the UnseenAnswers scenario, Run 1 using a lexicalized modeloutperformed the other two runs.
However, in theother scenarios Run 1 is not significantly better, aslexicalized features do not have the same impact ifthe question or the domain changes.Table 2 shows the confusion matrix of Run 1 inthe 5-way Unseen Domains scenario.
The Correctcategory was classified quite reliably, but the Irrele-vant category was especially hard.
While the refer-ence answer provides some clues for what is corrector incorrect, the range of things that are ?irrelevant?for a given question is potentially very big and thuscannot be easily learned.
We also see that the systemability to distinguish Correct and Partially Correctanswers need to be improved.It is difficult to provide an exact assessment of oursystem?s performance (with respect to other systemsin the challenge), since there are multiple tasks, sce-287narios, datasets, and even metrics.
However, we cansafely say that our system performed above averagein most settings, and showed competitive results inthe Unseen Domains scenario.4 Pilot TaskIn the pilot task each facet needs to be analyzed sep-arately, which requires some changes in the systemarchitecture.4.1 System DescriptionWe segmented and lemmatized the input data usingthe default DKPro segmenter and the TreeTaggerlemmatizer.
The partial entailment system is com-posed of three methods: Exact, WordNet, and BI-UTEE.
These were combined in different combina-tions to form the different runs.Exact In this baseline method, we represent astudent answer as a bag-of-words containing all to-kens and lemmas appearing in the text.
Lemmasare used to account for minor morphological dif-ferences, such as tense or plurality.
We then checkwhether both facet words appear in the set.WordNet checks whether both facet words, ortheir semantically related words, appear in the stu-dent?s answer.
We use WordNet (Fellbaum, 1998)with the Resnik similarity measure (Resnik, 1995)and count a facet term as matched if the similarityscore exceeds a certain threshold (0.9, empiricallydetermined on the training set).BIUTEE processes dependency trees instead ofbags of words.
We therefore added a pre-processingstage that extracts a path in the dependency parsethat represents the facet.
This is done by first pars-ing the entire reference answer, and then locating thetwo nodes mentioned in the facet.
We then find theirlowest common ancestor (LCA), and extract the pathfrom the facet?s first word to the second through theLCA.
BIUTEE can now be given the student an-swer and the pre-processed facet, and try to recog-nize whether the former entails the latter.4.2 Submitted RunsIn preliminary experiments using the provided train-ing data, we found that the very simple Exact Matchbaseline performed surprisingly well, with 0.96 pre-cision and 0.32 recall on positive class instances (ex-pressed facets).
We therefore decided to use this fea-Unseen Unseen UnseenAnswers Questions DomainsBaseline .670 .688 .731Run 1 .756 .710 .760Run 2 .782 .765 .816Run 3 .744 .733 .770Table 3: Pilot task performance across different scenar-ios.
The scores are F1-measures (weighted average).ture as an initial filter, and employ the others forclassifying the ?harder?
cases.
Training BIUTEEonly on these cases, while dismissing easy ones, im-proved our system?s performance significantly.Run 1: Exact OR WordNet This is essentiallyjust the WordNet feature on its own, because everyinstance that Exact classifies as positive is also pos-itive by WordNet.Run 2: Exact OR (BIUTEE AND WordNet) Ifthe instance is non-trivial, this configuration requiresthat both BIUTEE and WordNet Match agree on pos-itive classification.
Equivalent to the majority rule.Run 3: Exact OR BIUTEE BIUTEE increasesrecall of expressed facets at the expense of precision.4.3 Empirical ResultsTable 3 shows the F1-measure (weighted average) ofeach run in each scenario, including Exact Match asa quite strong baseline.
In the majority of cases, Run2 that combines entailment and WordNet-based lex-ical matching, significantly outperformed the othertwo.
It is interesting to note that the systems?
perfor-mance does not degrade in ?harder?
scenarios; this isa result of the non-lexicalized nature of our methods.Unfortunately, our system was the only submissionin this track, so we do not have any means to performrelative comparison.5 ConclusionWe combined semantic textual similarity with tex-tual entailment to solve the problem of student re-sponse analysis.
Though our features were not tai-lored for this task, they proved quite indicative, andadapted well to unseen domains.
We believe that ad-ditional generic features and knowledge resourcesare the best way to improve on our results, whileretaining the same robustness and generality as ourcurrent architecture.288AcknowledgementsThis work has been supported by the Volkswagen Foundation aspart of the Lichtenberg-Professorship Program under grant No.I/82806, and by the European Community?s Seventh Frame-work Programme (FP7/2007-2013) under grant agreement no.287923 (EXCITEMENT).
We would like to thank the MinervaFoundation for facilitating this cooperation with a short termresearch grant.ReferencesLloyd Allison and Trevor I. Dix.
1986.
A bit-stringlongest-common-subsequence algorithm.
InformationProcessing Letters, 23:305?310.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012a.
UKP: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the 6th International Work-shop on Semantic Evaluation and the 1st Joint Confer-ence on Lexical and Computational Semantics, pages435?440, June.Daniel Ba?r, Torsten Zesch, and Iryna Gurevych.
2012b.Text reuse detection using a composition of text sim-ilarity measures.
In Proceedings of the 24th In-ternational Conference on Computational Linguistics(COLING 2012), pages 167?184, December.Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.2009.
Recognizing textual entailment: Rationale,evaluation and approaches.
Natural Language Engi-neering, 15(4):i?xvii.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
Semeval-2013 task 7: The joint student re-sponse analysis and 8th recognizing textual entailmentchallenge.
In *SEM 2013: The First Joint Conferenceon Lexical and Computational Semantics, Atlanta,Georgia, USA, 13-14 June.
Association for Compu-tational Linguistics.Richard Eckart de Castilho and Iryna Gurevych.
2011.A lightweight framework for reproducible parame-ter sweeping in information retrieval.
In Proceed-ings of the 2011 workshop on Data infrastructurEs forsupporting information retrieval evaluation (DESIRE?11), New York, NY, USA.
ACM.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.David Ferrucci and Adam Lally.
2004.
UIMA: An ar-chitectural approach to unstructured information pro-cessing in the corporate research environment.
Natu-ral Language Engineering, 10(3-4):327?348.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-basedexplicit semantic analysis.
In Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence (IJCAI 2007), pages 1606?1611.Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.2007.
Darmstadt Knowledge Processing Repositorybased on UIMA.
In Proceedings of the 1st Work-shop on Unstructured Information Management Ar-chitecture at Biannual Conference of the Society forComputational Linguistics and Language Technology,Tu?bingen, Germany, April.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11(1).Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes, 25(2&3):259?284.Caroline Lyon, James Malcolm, and Bob Dickerson.2001.
Detecting short passages of similar text inlarge document collections.
In Proceedings of the6th Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2001), pages 118?125,Pittsburgh, PA USA.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the 21stNational Conference on Artificial Intelligence, pages775?780, Boston, MA.Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.2008.
ClearTK: A UIMA Toolkit for Statistical Nat-ural Language Processing.
In Towards EnhancedInteroperability for Large HLT Systems: UIMA forNLP workshop at Language Resources and EvaluationConference (LREC).Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceedingsof the 14th International Joint Conference on ArtificialIntelligence (IJCAI 1995), pages 448?453.Asher Stern and Ido Dagan.
2011.
A confidencemodel for syntactically-motivated entailment proofs.In Proceedings of the 8th International Conferenceon Recent Advances in Natural Language Processing(RANLP 2011), pages 455?462.Michael J.
Wise.
1996.
YAP3: Improved detection ofsimilarities in computer program and other texts.
InProceedings of the 27th SIGCSE Technical Sympo-sium on Computer Science Education (SIGCSE 1996),pages 130?134, Philadelphia, PA.289
