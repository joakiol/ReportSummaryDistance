In: Proceedings of CoNLL-2000 and LLL-2000, pages 107-110, Lisbon, Portugal, 2000.Shallow Parsing by Inferencing with Classifiers*Vas in  Punyakanok  and Dan RothDepar tment  of Computer  ScienceUniversity of Illinois at Urbana-ChampaignUrbana,  IL 61801, USA{punyakan, danr}@cs.uiuc.eduAbst ractWe study the problem of identifying phrasestructure.
We formalize it as the problem ofcombining the outcomes of several different clas-sifiers in a way that provides a coherent in-ference that satisfies some constraints, and de-velop two general approaches for it.
The firstis a Markovian approach that extends stan-dard HMMs to allow the use of a rich obser-vations structure and of general classifiers tomodel state-observation dependencies.
The sec-ond is an extension of constraint satisfaction for-malisms.
We also develop efficient algorithmsunder both models and study them experimen-tally in the context of shallow parsing.
11 Ident i fy ing  Phrase  St ructureThe problem of identifying phrase structure canbe formalized as follows.
Given an input stringO =< ol, 02, .
.
.
,  On >, a phrase is a substringof consecutive input symbols oi, oi+l,.
.
.
,oj .Some external mechanism is assumed to consis-tently (or stochastically) annotate substrings asphrases 2.
Our goal is to come up with a mech-anism that, given an input string, identifies thephrases in this string, this is a fundamental taskwith applications in natural language (Church,1988; Ramshaw and Marcus, 1995; Mufioz etal., 1999; Cardie and Pierce, 1998).The identification mechanism works by usingclassifiers that process the input string and rec-ognize in the input string local signals which* This research is supported by NSF grants IIS-9801638,SBR-9873450 and IIS-9984168.1Full version is in (Punyakanok and Roth, 2000).2We assume here a single type of phrase, and thuseach input symbol is either in a phrase or outside it.
Allthe methods we discuss can be extended to deal withseveral kinds of phrases in a string, including differentkinds of phrases and embedded phrases.are indicative to the existence of a phrase.
Lo-cal signals can indicate that an input symbol ois inside or outside a phrase (IO modeling) orthey can indicate that an input symbol o opensor closes a phrase (the OC modeling) or somecombination of the two.
In any case, the lo-cal signals can be combined to determine thephrases in the input string.
This process, how-ever, needs to satisfy some constraints for theresulting set of phrases to be legitimate.
Sev-eral types of constraints, such as length and or-der can be formalized and incorporated into themechanisms studied here.
For simplicity, we fo-cus only on the most basic and common con-straint - we assume that phrases do not overlap.The goal is thus two-fold: to learn classifiersthat recognize the local signals and to combinethese in a ways that respects the constraints.2 Markov  Mode l ingHMM is a probabilistic finite state automatonused to model the probabilistic generation ofsequential processes.
The model consists ofa finite set S of states, a set (9 of observa-tions, an initial state distribution P1 (s), a state-transition distribution P(s\[s') for s, # E S andan observation distribution P(o\[s) for o E (9and s 6 S. 3In a supervised learning task, an observa-tion sequence O --< o l ,o2, .
.
.
On > is super-vised by a corresponding state sequence S =<sl, s2,.
?
?
sn >.
The supervision can also be sup-plied, as described in Sec.
1, using the local sig-nals.
Constraints can be incorporated into theHMM by constraining the state transition prob-ability distribution P(s\]s').
For example, setP(sV)  = 0 for all s, s' such that the transitionfrom s ~ to s is not allowed.aSee (Rabiner, 1989) for a comprehensive tutorial.107Combining HMM and classifiers (artificialneural networks) has been exploited in speechrecognition (Morgan and Bourlard, 1995), how-ever, with some differences from this work.2.1 HMM wi th  Classif iersTo recover the most likely state sequence inHMM, we wish to estimate all the requiredprobability distributions.
As in Sec.
1 we as-sume to have local signals that indicate thestate.
That is, we are given classifiers withstates as their outcomes.
Formally, we assumethat Pt(slo ) is given where t is the time step inthe sequence.
In order to use this informationin the HMM framework, we computePt(o\[s) = Pt(slo)Pt(o)/Pt(s).
(1)instead of observing the conditional probabilityPt (ols) directly from training data, we computeit from the classifiers' output.
Pt(s) can be cal-culated by Pt(s) = Es'eS P(sls')Pt- l(s ')  wherePl(s) and P(sls' ) are the two required distri-bution for the HMM.
For each t, we can treatPt(ols ) in Eq.
1 as a constant r/t because our goalis only to find the most likely sequence of statesfor given observations which are the same forall compared sequences.
Therefore, to computethe most likely sequence, standard ynamic pro-gramming (Viterbi) can still be applied.2.2 P ro jec t ion  based  Markov  Mode lIn HMMs, observations are allowed to dependonly on the current state and long term depen-dencies are not modeled.
Equivalently, from theconstraint point of view, the constraint struc-ture is restricted by having a stationary proba-bility distribution of a state given the previousone.
We attempt o relax this by allowing thedistribution of a state to depend, in additionto the previous state, on the observation.
For-mally, we make the independence assumption:P(  s t lS t - l  , S t -  2 ,  .
.
.
, s l  , o t ,  o t -1 ,  .
.
.
, 01)= P(stlSt_l,Ot).
(2)Thus, we can find the most likely state sequenceS given O by maximizingnP(SIO) = II\[P(stls~,..., 8t-1, O)\]Pl(slIO)t=2n= H\[P(stlst_l,ot)\]Pl(sl lOl).
(3)t=2Hence, this model generalizes the standardHMM by combining the state-transition prob-ability and the observation probability into onefunction.
The most likely state sequence canstill be recovered using the dynamic program-ming algorithm over the Eq.3.In this model, the classifiers' decisions are in-corporated in the terms P(sls' ,o ) and Pl(slo ).In learning these classifiers we project P(sls ~, o)to many functions Ps' (slo) according to the pre-vious states s ~.
A similar approach has beendeveloped recently in the context of maximumentropy classifiers in (McCallum et al, 2000).3 Const ra in t  Sat i s fac t ion  w i thClass i f ie rsThe approach is based on an extension ofthe Boolean constraint satisfaction formal-ism (Mackworth, 1992) to handle variables thatare outcomes of classifiers.
As before, we as-sume an observed string 0 =< ol,o2,.
.
.
On >and local classifiers that, w.l.o.g., take two dis-tinct values, one indicating the openning aphrase and a second indicating closing it (OCmodeling).
The classifiers provide their outputsin terms of the probability P(o) and P(c), giventhe observation.To formalize this, let E be the set of all possi-ble phrases.
All the non-overlapping constraintscan be encoded in: f --/ke~ overlaps ej (-~eiV-~ej).Each solution to this formulae corresponds to alegitimate set of phrases.Our problem, however, is not simply to findan assignment  : E -+ {0, 1} that satisfies fbut rather to optimize some criterion.
Hence,we associate a cost function c : E ~ \[0,1\]with each variable, and then find a solution ~-of f of minimum cost, c(~-) = n Ei=lIn phrase identification, the solution to the op-timization problem corresponds to a shortestpath in a directed acyclic graph constructedon the observation symbols, with legitimatephrases (the variables in E) as its edges andtheir costs as the weights.
Each path in thisgraph corresponds to a satisfying assignmentand the shortest path is the optimal solution.A natural cost function is to use the classi-fiers probabilities P(o) and P(c) and define, fora phrase e = (o, c), c(e) = 1 - P(o)P(c) whichmeans that the error in selecting e is the er-ror in selecting either o or c, and allowing those108to overlap 4.
The constant in 1 - P(o)P(c) bi-ases the minimization to prefers selecting a fewphrases, possibly no phrase, so instead we min-imize -P(o) P(c).4 Sha l low Pars ingThe above mentioned approaches are evaluatedon shallow parsing tasks, We use the OC mod-eling and learn two classifiers; one predictingwhether there should be a open in location tor not, and the other whether there should aclose in location t or not.
For technical reasonsit is easier to keep track of the constraints ifthe cases --1 o and --1 c are separated according towhether we are inside or outside a phrase.
Con-sequently, each classifier may output three pos-sible outcomes O, nOi,  nOo (open, not openinside, not open outside) and C, nCi,  nCo,resp.
The state-transition diagram in figure 1captures the order constraints.
Our modeling ofthe problem is a modification of our earlier workon this topic that has been found to be quitesuccessful compared to other learning methodsattempted on this problem (Mufioz et al, 1999)and in particular, better than the IO modelingof the problem (Mufioz et al, 1999).Figure 1: State-transition diagramfor thephrase recognition problem.The classifier we use to learn the states asa function of the observations i SNoW (Roth,1998; Carleson et al, 1999), a multi-class clas-sifter that is specifically tailored for large scalelearning tasks.
The SNoW learning architec-ture learns a sparse network of linear functions,in which the targets (states, in this case) arerepresented as linear functions over a commonfeature space.
Typically, SNoW is used as aclassifier, and predicts using a winner-take-all4Another solution in which the classifiers' uggestionsinside each phrase axe also accounted for is possible.mechanism over the activation value of the tax-get classes in this case.
The activation valueitself is computed using a sigmoid function overthe linear sum.
In this case, instead, we normal-ize the activation levels of all targets to sum to 1and output the outcomes for all targets (states).We verified experimentally on the training datathat the output for each state is indeed a dis-tribution function and can be used in furtherprocessing as P(slo ) (details omitted).5 ExperimentsWe experimented both with base noun phrases(NP) and subject-verb patterns (SV) and showresults for two different representations of theobservations (that is, different feature sets forthe classifiers) - part of speech (POS) tags onlyand POS with additional exical information(words).
The data sets used are the standarddata sets for this problem (Ramshaw and Max-cus, 1995; Argamon et al, 1999; Mufioz etal., 1999; Tjong Kim Sang and Veenstra, 1999)taken from the Wall Street Journal corpus inthe Penn Treebank (Marcus et al, 1993).For each model we study three different clas-sifiers.
The simple classifier corresponds to thestandard HMM in which P(ols ) is estimated i-rectly from the data.
The NB (naive Bayes) andSNoW classifiers use the same feature set, con-junctions of size 3 of POS tags (+ words) in awindow of size 6 around the target word.The first important observation is that theSV task is significantly more difficult than theNP task.
This is consistent for all models andall features ets.
When comparing between dif-ferent models and features ets, it is clear thatthe simple HMM formalism is not competitivewith the other two models.
What is interest-ing here is the very significant sensitivity to thewider notion of observations (features) used bythe classifiers, despite the violation of the prob-abilistic assumptions.
For the easier NP task,the HMM model is competitive with the oth-ers when the classifiers used are NB or SNOW.In particular, a significant improvement in bothprobabilistic methods is achieved when their in-put is given by SNOW.Our two main methods, PMM and CSCL,perform very well on predicting NP and SVphrases with CSCL at least as good as any othermethods tried on these tasks.
Both for NPs and109Table 1: Results (F~=l) of different methodsand comparison to previous works on NP andSV recognition.
Notice that, in case of simple,the data with lexical features are too sparse todirectly estimate the observation probability sowe leave these entries empty.Method POS POSModel\[ Classifier only +wordsSNoW 90.64 92.89HMM NB 90.50 92.26Simple 87.83SNoW 90.61 92.98NP PMM NB 90.22 91.98Simple 61.44SNoW 90.87 92.88CSCL NB 90.49 91.95Simple 54.42Ramshaw & Marcus 90.6 92.0Argamon et al 91.6 N/AMufioz et al 90.6 92.8Tjong Kim SangVeenstra N/A 92.37SNoW 64.15 77.54HMM NB 75.40 78.43Simple 64.85SNoW 74.98 86.07PMM NB 74.80 84.80Simple 40.18SNoW 85.36 90.09CSCL NB 80.63 88.28Simple 59.27Argamon et al 86.5 N/Ai Mufioz et al 88.1 92.0SVSVs, CSCL performs better than the probabilis-tic method, more significantly on the harder,SV, task.
We attr ibute it to CSCL's ability tocope better with the length of the phrase andthe long term dependencies.Our methods compare favorably with otherswith the exception to SV in (Mufioz et al,1999).
Their method is fundamentally simi-lar to our CSCL; however, they incorporatedthe features from open in the close classifier al-lowing to exploit the dependencies between twoclassifiers.
We believe that this is the main fac-tor of the significant difference in performance.6 Conc lus ionWe have addressed the problem of combiningthe outcomes of several different classifiers in away that provides a coherent inference that sat-isfies some constraints, While the probabilisticapproach extends tandard and commonly usedtechniques for sequential decisions, it seemsthat the constraint satisfaction formalisms cansupport complex constraints and dependenciesmore flexibly.
Future work will concentrate onthese formalisms.Re ferencesS.
Argamon, I. Dagan, and Y. Krymolowski.
1999.A memory-based approach to learning shallownatural anguage patterns.
Journal of Experimen-tal and Theoretical Artificial Intelligence, 10:1-22.C.
Cardie and D. Pierce.
1998.
Error-driven prun-ing of treebanks grammars for base noun phraseidentification.
In Proc.
of ACL-98, pages 218-224.A.
Carleson, C. Cumby, J. Rosen, and D. Roth.1999.
The SNoW learning architecture.
Tech.
Re-port UIUCDCS-R-99-2101, UIUC Computer Sci-ence Department, May.K.
W. Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
InProe.
of A CL Conference on Applied Natural Lan-guage Processing.A.
K. Mackworth.
1992.
Constraint Satisfaction.
InStuart C. Shapiro, editor, Encyclopedia of Artifi-cial Intelligence, pages 285-293.
Vol.
1, 2 nd ed.M.
P. Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Lin-guistics, 19(2):313-330, June.A.
McCallum, D. Freitag, and F. Pereira.
2000.Maximum entropy Markov models for informationextraction and segmentation.
In Proc.
of ICML-2000.N.
Morgan and H. Bourlard.
1995.
Continuousspeech recognition.
IEEE Signal Processing Mag-azine, 12(3):25-42.M.
Mufioz, V. Punyakanok, D. Roth, and D. Zimak.1999.
A learning approach to shallow parsing.
InProc.
of EMNLpo VLC'99.V.
Punyakanok and D. Roth.
2000.
Inference withclassifiers.
Tech.
Report UIUCDCS-R-2000-2181,UIUC Computer Science Department, July.L.
R. Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recog-nition.
Proc.
of the IEEE, 77(2):257-285.L.
A. Ramshaw and M. P. Marcus.
1995.
Textchunking using transformation-based learning.
InProc.
of WVLC'95.D.
Roth.
1998.
Learning to resolve natural lan-guage ambiguities: A unified approach.
In Proc.of AAAI'98, pages 806-813.E.
F. Tjong Kim Sang and J. Yeenstra.
1999.
Rep-resenting text chunks.
In Proc.
of EA CL'99.110
