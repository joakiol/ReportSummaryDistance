Multi-Site Data  Collection and Evaluationin Spoken Language UnderstandingL.
Hirschman, M. Bates, D. Dahl, W. Fisher, \].
Garofolo,D.
Pallett, K. Hunicke-Smith, P. Price, A. Rudnicky, and E. Tzoukermann*Contact: Lynette H i rschmanNE43-643 Spoken Language Systems GroupMIT  Laboratory for Computer  Science, Cambridge,  MA 02139e-mail: lynet te@goldilocks.lcs.mit.eduABSTRACTThe Air Travel Information System (ATIS) domain serves asthe common task for DARPA spoken language system re-search and development.
The approaches and results possi-ble in this rapidly growing area are structured by availablecorpora, annotations of that data, and evaluation methods.Coordination of this crucial infrastructure is the charter ofthe Multi-Site AT IS  Data COllection Working group (MAD-COW) .
We focus here on selection of training and test data,evaluation of language understanding, and the continuingsearch for evaluation methods that will correlate well withexpected performance of the technology in applications.1.
INTRODUCTIONData availability and evaluation procedures structure re-search possibilities: the type and amount of trainingdata affects the performance of existing algorithms andlimits the development of new algorithms; and evalua-tion procedures document progress, and force researchchoices in a world of limited resources.
The recent rapidprogress in spoken language understanding owes much toour success in collecting and distributing a large corpusof speech, transcriptions, and associated materials basedon human-machine interactions in the air travel domain.A tight feedback loop between evaluation methodologyand evaluation results has encouraged incremental ex-tension to the evaluation methodology, to keep pace withthe technology development.
The paper reports on thedata collection and evaluation efforts co-ordinated byMADCOW over the past year.The multi-site data collection paradigm \[3, 4\] distributesthe burden of data collection, provides data rapidly, ed-ucates multiple sites about data collection issues, andresults in a more diverse pool of data than could be ob-tained with a single collection site.
The resulting datarepresents a wide range of variability in speaker char-acteristics, speech style, language style and interactionstyle.
It has allowed individual sites to experiment withdata collection methods: by replacing various system*This paper was written the auspices of the Multi-Site ATISData Collection Working group (MADCOW).
In addition to theauthors, many other people, listed under the Acknowledgementssection, made important contributions to this work.components  with a human,  we collect the kind of data wecan a im for in the future, while completely automatedsystems help us to focus on the major  current issuesin system accuracy and speed.
Sites have also exper-imented with interface strategies: spoken output only,tabular output only, response summaries,  spoken andwritten paraphrase, and system initiative may be moreor less appropriate for different users and different tasksand all can dramatically affect the resulting data.MADCOW's  recent accompl ishments include:?
Release of 14,000 utterances for training and test,including speech and transcriptions;?
Release of annotations for almost I0,000 of theseutterances (7500 training utterances and three testsets of 2300 utterances total), balanced by site;?
A bug reporting and bug fix mechanism,  to maintainthe quality and consistency of the training data;?
An  evaluation schedule that delivered training dataand froze changes in the principles of interpretation Iseveral months  before the evaluation;?
An  experiment with "end-to-end" evaluation thatpermits evaluation of aspects of the system not pre-viously evaluable.Table I shows the breakdown of all training data and Ta-ble 2 shows the breakdown for just the annotated data?2.
CURRENT EVALUATIONMETHODOLOGYWhen the ATIS task was developed in 1990 \[9\], lit-tle work had been done on formal evaluation of under-standing for natural language interfaces.
3 In the ab-sence of a generally accepted semantic representation,1These are the principles that define how various vague or dif-ficult phrases are to be interpreted; see section 2.1 below.2A class A utterance can be interpreted by itself, with no ad-ditional context; a class D utterance requires an earlier "context-setting" utterance for its interpretation; and a class X utterancecannot be evaluated in terms of a reference database answer.~This coincides with the beginnings of formal evaluation forwritten text, via the Message Understanding Conferences (MUCs)19SiteAT&TBBNCMUMITSRITOTALSpeakers  Scenarios Utterances57 200 210062 307 227747 214 2219182 625 491090 148 2478438 1494 13984SiteATTBBNCMUMITSRITotalClass A Class D Class X Total457 36.6% 497 39.8% 295 23.6% 1249 16.6%858 56.2% 357 23.4% 312 20.4% 1527 20.3%562 37.6% 340 22.7% 594 39.7% 1496 19.9%663 37.7% 680 38.7% 414 23.6% 1757 23.4%676 45.7% 618 41.8% 184 12.4% 1478 19.7%3216 42.8% 2492 33.2% 1799 24.0% 7507 100.0%Table 1: Multi-site ATIS Data Summary Table 2: Annotated Training Data  Summarythe DARPA SLS community focused instead on "theright answer," as defined in terms of a database querytask (air travel planning).
This permitted evaluation bycomparing "canonical" database answers to the systemanswers using a comparator program \[1\].
This approachwas felt to be far easier, given proper definition of terms,than to agree on a standard semantic representation.The original evaluation methodology was defined onlyfor context-independent (class A) utterances.
However,this left approximately half the data as unevaluable(see Table 2).
Over the next two years, the evaluationmethod was extended to cover context~dependent queries(class D utterances), it was tightened by requiring thata correct answer lie within a minimal answer and a max-imal answer (see section 2.1), and it was made morerealistic by presenting utterances in scenario order, asspoken during the data collection phase, with no infor-mation about the class of an utterance.
Thus, we nowcan evaluate on approximately 75% of the data (all butclass X data - see Tables 2 and 4).
We also introduced aWeighted Error metric because we believed, at least insome applications, wrong answers might be worse than"no answer" :4WeightedError =~(No_Answer) + 2 ?
#(Wrong_Answer).2.1 .
The  Eva luat ion  Mechan ismThe comparator-based evaluation method compareshuman annotator-generated canonical ("reference")database answers to system generated answers.
Theannotators first classify utterances into context-independent (A), context-dependent (D) and unevalu-able (X) classes.
Each evaluable utterance (class A orD) is then given minimal and maximal reference an-\[8\].
The MUC evaluation uses a domain-specific template as thebasis for evaluation.
To date, the goal of a domain-independent se-mantic representation, perhaps analogous to the minimal bracket-ing of the Penn Treebank database \[2\] for parsing, remains elusive.4A recent experiment \[5\] showed that for one system, subjectswere able to detect a system error before making their next query in90% of the cases.
In the remaining 10%, a system error caused thesubject o lose several turns before recovering, leading to a reducedestimated weighting factor of 1.25 for errors in that system.swers.
The minimal reference answer is generated usingNLParse 5 and the maximal answer is generated algorith-mically from the minimal answer.
A correct answer mustinclude all of the tuples contained in the minimal answerand no more tuples than contained in the maximal an-swer.The Principles of Interpretation document  provides anexplicit interpretation for vague natural language expres-sions, e.g., "red-eye flight" or "mid-afternoon," and spec-ifies other factors necessary to define reference answers,e.g., how context can override ambiguity in certain cases,or how utterances should be classified if they depend onprevious unevaluable utterances.
This document  servesas a point of common reference for the annotators andthe system developers, and permits evaluation of sen-tences that otherwise would  be too vague to have a well-defined database reference answer.The initial Principles of Interpretation was implementedin 1990.
The document is now about 10 pages long, andincludes interpretation decisions based on some 10,000ATIS utterances.
The document continues to grow butat a significantly slower rate, as fewer new issues arise.
Itis remarkable that such a small document has sufficed toprovide well-defined interpretations for a corpus of thissize.
This demonstrates that rules for the interpreta-tion of natural anguage utterances, at least in the ATISdomain, can be codified well enough to support an au-tomatic evaluation process.
Because this procedure wasexplicit and well-documented, two new sites were ableto participate in the most recent evaluation (November1992).2 .2 .
Tes t ing  on  the  MADCOW DataThe test data selection procedure was designed to en-sure a balanced test set.
Test data for the November1992 evaluation were chosen using procedures similar tothose for the November 1991 test \[3\].
As sites submit-ted data to NIST, NIST set aside approximately 20% ofthe utterances to create a pool of potential test data;some 1200 utterances constituted the November 19915NLParse is a database access product of Texas Instruments.20test  pool; 1300 utterances constituted the November1992 test pool.NIST's goal was to select approximately 1000 test utter-ances from the test data pool, evenly balanced amongthe five collection sites (AT~T, BBN, CMU, MIT, andSRI).
Utterances were selected by session, i.e., utterancesoccurring in one problem-solving scenario were selectedas a group, avoiding sessions that seemed to be extremeoutliers (e.g., in number of class X utterances, total num-ber of utterances, or number of repeated utterances).Because the test pool contained only marginally moreutterances than were needed for the test, it was not pos-sible to simultaneously balance the test set for numberof speakers, gender, or subject-scenarios.
The test setcontained 1002 utterances.
6 The breakdown of the datais shown in Table 3.NIST verified and corrected the original transcriptions.However, some uncertainty about the transcriptions re-mained, due to inadequacies in the specifications for thetranscription of difficult-to-understand speech, such assotto  voce speech.
After the transcriptions were verified,the data were annotated by the SRI annotation group toproduce categorizations and reference answers.
A periodfor adjudication followed the test, where testing sitescould request changes to the test data categorizations,reference answers, and transcriptions.
The final post-adjudication classification of the test data set is shownin Table 4.
Final evaluation results are reported in \[6\].Collecting Site Speakers Scenarios UtterancesATTBBNCMUMITSRI7; 1M/ 6F7; 3M/ 4F4; 4M/ OF10; 3M/ 7F9; 5M/ 4F2228123719200201200201200Total 37; 16M/21F 118 1002Table 3: Multi-site AT IS  Test Data  November  19923.
L IM ITAT IONS OF  THECURRENT EVALUATIONThe current data collection and evaluation paradigmcaptures importan t dimensions of system behavior.However, we must constantly re-assess our evaluationprocedures in terms of our goals, to ensure that our eval-uation procedures help us to assess the suitability of aparticular technology for a particular application, and6The data recorded using the Sennheiser head:mounted noise-cancelling microphone were used as the test material for "offi-cial" speech recognition (SPREC) and spoken language system(SLS, NL) testing.
For a subset of the utterances in the officialtest sets, recordings were also made using a desk-mounted Crownmicrophone.to ensure that benchmark scores will correlate well withuser satisfaction and efficiency when the technology istransferred to an application.The advantage of using a pre-recorded corpus for evalu-ation is clear: the same data are used as input to all sys-tems under evaluation, and each system's et of answersis used to automatically generate a benchmark score.This approach ensures a uniform input across all sys-tems and removes human involvement from the bench-mark testing process (except hat human annotators de-fine the reference answers).
Any annotated set of datacan be used repeatedly for iterative training.
However,some of these same strengths also impose limitations onwhat  we can evaluate.First, there is the issue of the match  between the ref-erence answer and the user's need for useful informa-tion.
The  comparator  method  can count answers as cor-rect despite system misunderstanding.
For example, ifa system misrecognizes "Tuesday" as "Wednesday"  andthe user realizes that the flight information shown is forWednesday  flights, the user may appropriately believethat the answer  is wrong.
However ,  if all flights havedaily departures, the database answer  will be canoni-cally correct.
On  the other hand, useful (but not strictlycorrect) answers will be counted wrong, because there isno "partially correct" category for answers.Second, mixed initiative in human-machine dialogue willbe required for technology transfer in many spokenlanguage understanding applications.
But the evalua-tion paradigm actively discourages experimentation withmixed initiative.
A query that is a response to a system-initiated query is classified as unevaluable if the user'sresponse can only be understood in the context of thesystem's query.
During evaluation, any system responsethat is a query will automatically be counted as incorrect(since only database answers can be correct).The use of pre-recorded ata also preserves artifacts ofthe data collection system.
For example, much of thetest data were collected using systems or componentsof systems to generate responses, rather than a humanalone.
As a result, the data include many instances ofsystem errors that affect the user's next query.
A usermay have to repeat a query several times, or may cor-rect some error that the data collection system (but notthe system under evaluation) made.
These are artificialphenomena that would disappear if the data collectionand evaluation systems were identical.Finally, the current paradigm does not take into accountthe speed of the response, which greatly affects the over-all interaction.
Demonstration systems at several sites21SiteATTBBNCMUMITSRITotal:Class A Class D Class X Total48 (24.0%)97 (48.3%)76 (38.0%)100 (49.8%)106 (53.0%)41 (20.5%)27 (13.4%)66 (33.0%)67 (33.3%)46 (23.0%)111 (55.5%)77 (38.3%)58 (29.0%)34 (16.9%)48 (24.0%)200 (20.0%)201 (20.1%)200 (20.0%)201 (20.1%)200 (20.0%)427 (42.6%) 247 (24.7%) 328 (32.7%) 1002 (100.0%)Table 4: Breakdown of Test Data by Classhave begun to diverge from those used in benchmarkevaluations, in part, because the requirements of demon-strating or using the system are quite different from therequirements for generating reference database answers.These limitations of the comparator-based valuationpreclude the evaluation of certain strategies that arelikely to be crucial in technology transfer.
In particular,we need to develop metrics that keep human subjects inthe loop and support human-machine interaction.
How-ever, the use of human subjects introduces new issuesin experimental design.
Over the past year, MADCOWhas begun to address these issues by designing a trialend-to-end evaluation.4.
END-TO-END EVALUATIONEXPERIMENTThe end-to-end evaluation, designed to complement thecomparator-based evaluation, included 1) objective mea-sures such as timing information, and time to task com-pletion, 2) human-derived judgements on correctness ofsystem answers and user solutions (logfile evaluation),and 3) a user satisfaction questionnaire.The unit of analysis for the new evaluation was a sce-nario, as completed by a single subject, using a partic-ular system.
This kept the user in the loop, permittingeach system to be evaluated on its own inputs and out-puts.
The use of human evaluators allowed for assess-ing partial correctness, and provided the opportunity toscore other system actions, such as mixed initiatives, er-ror responses and diagnostic messages.
The end-to-endevaluation i cluded both task-level metrics (whether sce-narios had been solved correctly and the time it took asubject o solve a scenario) and utterance-level metrics(query characteristics, system response characteristics,the durations of individual transactions).An experimental evaluation took place in October 1992,to assess feasibiiity of the new evaluation method.
Wedefined a common experimental design protocol and acommon set of subject instructions (allowing some lo-cal variation).
Each site submitted to NIST four travelplanning scenarios that had a well-defined "solution set".From these, NIST assembled two sets of four scenarios.Each site then ran eight subjects, each doing four scenar-ios, in a counter-balanced design.
Five systems partici-pated: the BBN, CMU, MIT and SRI spoken languagesystems, and the Paramax system using typed input.A novel feature of the end-to-end experiment was thelogfile evaluation.
This technique, developed at MIT\[7\], is based on the logiile which records and times-tamps all user/system interactions.
A human evalu-ator, using an interactive program, 7 can review eachuser/system interaction and evaluate it by type of userrequest, type of system response, and correctness or ap-propriateness of response.
For user requests, the follow-ing responses were distinguished: 1) New Information,2) Repeat, 3) Rephrase, or 4) Unevaluable.
For systemresponses, the evaluators categorized each response asfollows:Answer: further evaluated as Correct, IncorrectPartially Correct or Can't Decide;System Initiated Directive: further evaluated asAppropriate, Inappropriate, or Can't Decide;Diagnostic Message: further evaluated as Appropriate,Inappropriate, or Can't Decide;Failure-to-Understand Message: no further evaluation.The evaluator also assessed the scenario solution accord-ing to whether the subject finished and whether the an-swer belonged to the defined solution set.To facilitate determination f the correctness of individ-ual system responses, we agreed to follow the Princi-ples of Interpretation, at least to the extent hat an an-swer judged correct by these Principles would not becounted incorrect.
For this experiment, logfile evalua-tion was performed independently b Bill Fisher (NIST)and Kate Hunicke-Smith (SRI Annotation), as well as byvolunteers at MIT and BBN.
This gave us experience inlooking at the variability among evaluators of different7The program was developed by David Goodine at MIT; theevaluator instructions were written by Lynette Hirschman, withhelp from Lyn Bates, Christine Pao and the rest of MADCOW.22levels of experience.
We found that any two evaluatorsagreed about 90% of the time, and agreement amongmultiple evaluators decreased proportionally.5.
LESSONS LEARNEDThe experiment provided useful feedback on the risksand advantages of end-to-end evaluation, and will guideus in refining the procedure.
For the initial trial, wemade methodological compromises in several areas: asmall number of subjects, no control over cross-site sub-ject variability, few guidelines in developing or select-ing scenarios.
These compromises seemed reasonable toget the experiment started; however, the next iterationof end-to-end evaluation will need to introduce method-ological changes to provide statistically valid data.5 .1 .
Sources  o f  Var iab i l i tyValid comparisons of systems across sites require controlover major  sources of variability, so that the differencesof interest can emerge.
The  use of human subjects inthe evaluation creates a major  source of variability, dueto differences in the subjects pools available at varioussites and the characteristics of individuals.
We can min-imize some of these differences, for example, by trainingall subjects to the same criterion across sites (to accountfor differences in background and familiarity with thedomain),  by using many subjects f rom each site (so thatany one subject's idiosyncrasies have less of an effect onthe results), and by ensuring that procedures for subjectrecruitment and data collection across sites are as Similaras possible (we made a serious effort in this direction, butmore  could be done to reduce the cross-site variabilitythat is otherwise confounded with the system under eval-uation).
An  alternative would  be to perform the eval-uation at a common site.
This would  allow for greateruniformity in the data collection procedure, it could in-crease the uniformity of the subject pool, and wouldallow use of powerful experimental techniques (such aswithin-subject designs).
Such a common-s i te  evaluation,however, would  pose other challenges, including the portof each system to a common site and platform, and thecomplex  design needed to assess potential scenario ordereffects, system order effects, and their interaction.Another  source of variability is the set of travel plan-ning scenarios the subjects were asked to solve.
Certainscenarios posed serious problems for all systems; a fewscenarios posed particular problems for specific systems.However,  the data suggest that there was a subset thatcould perform a reasonable diagnostic function.5.2.
Logf i le  Eva luat ionSomewhat unexpectedly, we found that logfile evaluationwas a useful tool for system developers in identifyingdialogue-related problems in their systems.
The evalu-ator interface allowed for rapid evaluation (about 5-15minutes per scenario).
However, the evaluator instruc-tions need refinement, the interface needs minor exten-sions, and most important, we need to design a proce-dure to produce a statistically reliable logfile evaluationscore by combining assessments from multiple evalua-tors.A remaining thorny problem is the definition of  correct,partially correct, and incorrect answers.
For this experi-ment, we used the Principles of Interpretation documentto define a correct answer, so that we would not need todevelop a new document for these purposes.
For the nextevaluation, we need definitions that reflect utility to theuser, not just canonical correctness, sFinally, we found that we could not rely on subjects tocorrectly complete the scenarios presented to them.
Insome cases, the subject was not able to find the answer,and in other cases, the subject did not follow directionsregarding what information to provide in the answer.This made it difficult to compute accurate statistics forscenario-level metrics such as task completion and taskcompletion time; this problem was exacerbated by thelimited amount of data we collected.6.
FUTURE D IRECT IONSWe view evaluation as iterative; at each evaluation,we assess our procedures and try to improve them.The comparator-based evaluation is now stable and theNovember 1992 evaluation ran very smoothly.
The avail-ability of an expanded atabase will require a new datacollection effort.
Increasing emphasis on portability mayhave an impact on evaluation technology.
In addition, weplan to continue our experiments with end-to-end evalu-ation, to work out some of the methodological problemsdescribed in the previous ection.The ATIS relational database has been expanded from 11cities to 46 cities, to provide a more realistic task sup-porting more challenging scenarios.
The database wasconstructed using data from the Official Airline Guideand now includes 23,457 flights (compared to 765 flights).The set of new cities was limited to 46 because it was feltthat a larger set would result in an unwieldy databaseSOriginally, we had wanted to compare logfile scores tocomparator-based scores.
However, for several sites, the datacollection and evaluation systems had diverged and it was notpossible to simultaneously interact with the subjects and providecomparator-style answers.
Therefore, we were not able to performthis experiment.23and would thus require the sites to devote too many re-sources to issues peripheral to their research, such asdatabase management and query optimization.
Datacollection on this larger database is now beginning.The portability of the technology (from application toapplication, and from language to language) becomes anincreasing challenge as the technology improves, sincemore potential applications become possible.
It stilltakes many hours of data collection and several personmonths of system development to port an applicationfrom one domain (e.g., air travel) to another similar do-main (e.g., schedule management).
Evaluating portabil-ity is still more challenging.
Evaluation has a significantcost: the comparator-based method requires the deft-nition of a training corpus and its collection, definingprinciples of interpretation, and (most expensively) theannotation ofdata.
Therefore, if we believe that regularevaluations play an important role in guiding research,we need to find cost-effective ways of evaluating systems.End-to-end evaluation may provide some low-overheadtechniques for quickly evaluating system performance innew domains.With the end-to-end evaluation experiment, We havemade progress in creating a procedure that accuratelyassesses the usability of current spoken language tech-nology and provides useful feedback for the improvementof this technology.
The procedure needs to be further e-fined to reliably identify differences among systems andit must embody principles that can assess trengths andweaknesses of different systems for different purposes.In developing evaluation procedures that involve humaninteractions, we need to carefully assess the validity ofthe measures we use.
For example a measure such asthe number of utterances per scenario may seem rele-vant (e.g., the subject was frustrated with answers andhad to repeat a question several times), but in fact mayreflect irrelevant aspects of the process (the subject wasintrigued by the system and wanted to push its limits invarious ways).
Meaningful evaluation will require met-rics that have been systematically investigated and havebeen shown to measure relevant properties.MADCOW has played a central role in developing andcoordinating the multi-site data collection and evalua-tion paradigm.
It will also play an active role in definingnew methodologies, such as end-to-end evaluation, tosupport evaluation of interactive spoken language sys-tems.
We believe that end-to-end evaluation will allow usto assess the trade-offs among various component-leveldecisions (in speech recognition, natural anguage pro-cessing and interface design), bringing spoken languagesystems closer to eventual deployment.7.
ACKNOWLEDGEMENTSWe would particularly like to acknowledge the contribu-tion of Nancy Dahlgren at NIST; prior to her accident,Nancy made important contributions to the annotationand debugging of the data.
We greatly missed her par-ticipation during the final evaluation.In addition to the authors, the following people made avaluable contribution to the process: at ATT: J. Wilpon;at BBN: R. Bobrow, R. Ingria, J. Makhoul, V. Shaked,and D. Stallard; at CMU: C. Neelan, E. Thayer, andR.
Weide; at MIT: D. Goodine, J. Polifroni, C. Pao, M.Phillips, and S. Seneff; at NIST: N. Dahlgren, J. Fiscus,and B. Tjaden; at Paramax: L. Norton, and R. Nilson;and at SRI: H. Bratt, R. Moore, E. Shriberg, and E.Wade.REFERENCES1.
Bates, M., S. Boisen, and J. Makhoul, "Developingan Evaluation Methodology for Spoken Language Sys-tems," Proc.
Third DARPA Speech and Language Work-shop, R. Stern (ed.
), Morgan Kaufmann, June 1990.2.
Black, E., et al, "A Procedure for Quantitatively Com-paring the Syntactic Coverage of English Grammars,"Proc.
Third DARFA Speech and Language Workshop,P.
Price (ed.
), Morgan Kanfmann, June 1991.3.
Hirschman, L., et al, "Multi-Site Data Collection for aSpoken Language Corpus", Proc.
Fifth Speech and Natu-ral Language Workshop, M. Marcus (ed.
), Morgan Kanf-mann, Arden House, NY, February 1992.4.
Hirschman, L., et al, "Multi-Site Data Collection for aSpoken Language Corpus," Proc.
of the ICSLP, Banff,Canada, October 1992.5.
Hirschman, L. and C. Pao, "The Cost of Errors in aSpoken Language Systems," submitted to Eurospeech-93, Berlin 1993.6.
Pallett, D., Fiscus, J., Fisher, W., and J. Garofolo,"Benchmark Tests for the Spoken Language Program,"Proc.
DARPA Human Language Technology Workshop,Princeton, March 1993.7.
Polifroni, J., Hirschman, L., Seneff, S. and V. Zue, "Ex-periments in Evaluating Interactive Spoken LanguageSystems" Proc.
DARPA Speech and Natural LanguageWorkshop, M. Marcus (ed.
), Arden House, NY, Febru-axy 1992.8.
Proc.
Fourth Message Understanding Conf., MorganKaufmann, McLean, June 1992.9.
Price P., "Evaluation of Spoken Language Systems: TheATIS Domain," Proc.
Third DARPA Speech and Lan-guage Workshop, P. Price (ed.
), Morgan Kaufmann,June 1990.24
