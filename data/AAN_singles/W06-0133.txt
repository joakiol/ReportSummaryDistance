Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 185?188,Sydney, July 2006. c?2006 Association for Computational LinguisticsMaximum Entropy Word Segmentation of Chinese TextAaron J. JacobsDepartment of LinguisticsUniversity of Texas at Austin1 University Station B5100Austin, TX 78712-0198 USAaaronjacobs@mail.utexas.eduYuk Wah WongDepartment of Computer SciencesUniversity of Texas at Austin1 University Station C0500Austin, TX 78712-0233 USAywwong@cs.utexas.eduAbstractWe extended the work of Low, Ng, andGuo (2005) to create a Chinese word seg-mentation system based upon a maximumentropy statistical model.
This systemwas entered into the Third InternationalChinese Language Processing Bakeoff andevaluated on all four corpora in their re-spective open tracks.
Our system achievedthe highest F-score for the UPUC corpus,and the second, third, and seventh high-est for CKIP, CITYU, and MSRA respec-tively.
Later testing with the gold-standarddata revealed that while the additions wemade to Low et al?s system helped our re-sults for the 2005 data with which we ex-perimented during development, a numberof them actually hurt our scores for thisyear?s corpora.1 SegmenterOur Chinese word segmenter is a modification ofthe system described by Low et al (2005), whichthey entered in the 2005 Second International Chi-nese Word Segmentation Bakeoff.
It uses a max-imum entropy (Ratnaparkhi, 1998) model whichis trained on the training corpora provided for thisyear?s bakeoff.
The maximum entropy frameworkused is the Python interface of Zhang Le?s maxi-mum entropy modeling toolkit (Zhang, 2004).1.1 Properties in common with Low et alAs with the system of Low et al, our systemtreats the word segmentation problem as a tag-ging problem.
When segmenting a string of Chi-nese text, each character can be assigned one offour boundary tags: S for a character that standsalone as a word, B for a character that begins amulti-character word, M for a character in a multi-character word which neither starts nor ends theword, and E for a character that ends a multi-character word.
The optimal tag for a given char-acter is chosen based on features derived fromthe character?s surrounding context in accordancewith the decoding algorithm (see Section 1.2).All of the feature templates of Low et al?s sys-tem are utilized in our own (with a few slight mod-ifications):1.
Cn (n = ?2,?1, 0, 1, 2)2.
CnCn+1 (n = ?2,?1, 0, 1)3.
C?1C14.
Pu(C0)5.
T (C?2)T (C?1)T (C0)T (C1)T (C2)6.
Lt07.
Cnt0 (n = ?1, 0, 1)In the above feature templates, Ci refers to thecharacter i positions away from the character un-der consideration, where negative values indicatecharacters to the left of the present position.
Thepunctuation feature Pu(C0) is added only if thecurrent character is a punctuation mark, and thefunction T maps characters to various numbersrepresenting classes of characters.
In addition tothe numeral, date word, and English letter classesof Low et al?s system, we added classes for punc-tuation and likely decimal points (which are de-fined by a period or the character ?
occurringbetween two numerals).
L is defined to be thelength of the longest wordW in the dictionary thatmatches some sequence of characters around C0185in the current context and t0 is the boundary tag ofC0 inW .
The dictionary features are derived fromthe use of the same online dictionary from PekingUniversity that was used by Low et alIn order to improve out-of-vocabulary (OOV)recall rates, we followed the same procedure asLow et al?s system in using the other three train-ing corpora as additional training material whentraining a model for a particular corpus:1.
Train a model normally using the given cor-pus.2.
Use the resulting model to segment the othertraining corpora from this year?s bakeoff, ig-noring the pre-existing segmentation.3.
Let C be a character in one of the other cor-pora D. If C is assigned a tag t by the modelwith probability p, t is equivalent to the tagassigned by the actual training corpus D, andp is less than 0.8, then add C (along withits associated features) as additional trainingmaterial.4.
Train a new model using all of the originaltraining data along with the new data derivedfrom the other corpora as described in theprevious step.This procedure was carried out when trainingmodels for all of the corpora except CKIP.
Themodel for that corpus was trained solely with itsown training data due to time and memory con-cerns as well as the fact that our scores during de-velopment for the corresponding corpus (AS) in2005 did not seem to benefit from the addition ofdata from the other corpora.We adopt the same post-processing step as Lowet al?s system: after segmenting a body of text,any sequence of 2 to 6 words whose total lengthis at least 3 characters and whose concatenationis found as a single word elsewhere in the seg-menter?s output is joined together into that sin-gle word.
Empirical testing showed that this pro-cess was actually detrimental to results in the 2005CITYU data, so it was performed only for theUPUC, MSRA, and CKIP corpora.1.2 Decoding algorithmWhen segmenting text, to efficiently compute themost likely tag sequence our system uses theViterbi algorithm (Viterbi, 1967).
Only legal tagsequences are considered.
This is accomplishedby ignoring illegal state transitions (e.g.
from a Btag to an S tag) during decoding.
At each stagethe likelihood of the current path is estimated bymultiplying the likelihood of the path which it ex-tends with the probability given by the model ofthe assumed tag occurring given the surroundingcontext and the current path.
To keep the problemtractable, only the 30 most likely paths are kept ateach stage.The advantage of such an algorithm comes inits ability to ?look ahead?
compared to a simpleralgorithm which just chooses the most likely tagat each step and goes on.
Such an algorithm islikely to run into situations where choosing themost likely tag for one character forces the choiceof a very sub-optimal tag for a later character bymaking impossible the choice of the best tag (e.g.if S is the best choice but the tag assigned for theprevious character was B).
In contrast, the Viterbialgorithm entertains multiple possibilities for thetagging of each character, allowing it to choose aless likely tag now as a trade-off for a much morelikely tag later.1.3 Other outcome-independent featuresTo the feature templates of Low et al?s systemdescribed in Section 1.1, we added the followingthree features which do not depend on previoustagging decisions but only on the current charac-ter?s context within the sentence:1.
The surname feature is set if the currentcharacter is in our list of common surnamecharacters, as derived from the Peking Uni-versity dictionary.2.
The redup-next feature is set if C1 isequal to C0.
This is to handle reduplicationwithin words, such as in the case of ZZ ?particularly clear?.3.
The redup-prev feature is set if C?1 isequal to C0.These features were designed to give the systemhints in cases where we saw it make frequent er-rors in the 2005 data.1.4 Outcome-dependent featuresIn addition to the features previously discussed,we added a number of features to our system thatare outcome-dependent in the sense that their re-alization for a given character depends upon how186the previous characters were segmented.
Thesework in conjunction with the Viterbi algorithmdiscussed in Section 1.2 to make it so that a givencharacter in a sentence can be assigned a differentset of features each time it is considered, depend-ing on the path currently being extended.1.
If the current character is one of the placecharacters such as Q or G which com-monly occur at the end of a three-characterword and the length of the current word(as determined by previous tagging decisionson the current path) including the currentcharacter is equal to three, then the featureplace-char-and-len-3 is set.2.
If the situation is as described above ex-cept the next character in the current con-text is the place character, then the featurenext-place-char-and-len-2 is set.3.
If the current character is I and the wordbefore the previous word is an enumeratingcomma (), then the feature deng-listis set.
This is intended to capture situationswhere a list of single-word items is presented,followed byI to mean ?and so on?.4.
If the current character is I and the thirdword back is an enumerating comma, thenthe feature double-word-deng-list isset.5.
If the length of the previous word is at least 2and is equal to the length of the current word,then the feature symmetry is set.6.
If the length of the previous word is at least 2and is one more than the length of the currentword, then the feature almost-symmetryis set.7.
Similar features are added if the length of thecurrent word is equal to (or one less than) thelength of the word before the last and the lastword is a comma.These features were largely designed to help al-leviate problems the model had with situations inwhich it would otherwise be difficult to discern thecorrect segmentation.
For example, in one devel-opment data set the model incorrectly groupedIat the end of a list (which should be a word on itsown) with the following character to formI, aword found in the dictionary.1.5 Simplified normalizationTo derive the most benefit from the additionaltraining data obtained as described in Section 1.1,before generating any sort of features from char-acters in training and test data, all characters arenormalized by the system to their simplified vari-ants (if any) using data from version 4.1.0 of theUnicode Standard.
This is intended to improve theutility of additional data from the traditional Chi-nese corpora when training models for the sim-plified corpora, and vice versa.
Due to the re-sults of some empirical testing, this normalizationwas only performed when training models for theUPUC and MSRA corpora; in our testing it didnot actually help with the scores for the traditionalChinese corpora.2 ResultsTable 1 lists our official results for the bakeoff.The columns show F scores, recall rates, precisionrates, and recall rates on out-of-vocabulary andin-vocabulary words.
Out of the participants inthe bakeoff whose scores were reported, our sys-tem achieved the highest F score for UPUC, thesecond-highest for CKIP, the seventh-highest forMSRA, and the third-highest for CITYU.Corpus F R P ROOV RIVUPUC 0.944 0.949 0.939 0.768 0.966CKIP 0.954 0.959 0.949 0.672 0.972MSRA 0.960 0.959 0.961 0.711 0.968CITYU 0.969 0.971 0.967 0.795 0.978Table 1: Our 2006 SIGHAN bakeoff results.The system?s F score forMSRAwas higher thanfor UPUC or CKIP, but it did particularly poorlycompared to the rest of the contestants when oneconsiders how well it performed for the other cor-pora.
An analysis of the gold-standard files forthe MSRA test data show that out of all of thecorpora, MSRA had the highest percentage ofsingle-character words and the smallest percent-age of two-character and three-character words.Moreover, its proportion of words over 5 char-acters in length was five times that of the othercorpora.
Most of the errors our system made onthe MSRA test set involved incorrect groupingsof true single-character words.
Another compar-atively high proportion involved very long words,especially names with internal syntactic structure187(e.g.
-??Zi}?X,]!h?
?h').Our out of vocabulary scores were fairly highfor all of the corpora, coming in first, fourth,fifth, and third places in UPUC, CKIP, MSRA,and CITYU respectively.
Much of this can be at-tributed to the value of using an external dictionaryand additional training data, as illustrated by theexperiments run by Low et al (2005) with theirmodel.3 Further testingIn order to get some idea of how each of our ad-ditions to Low et al?s system contributed to ourresults, we ran a number of experiments with thegold-standard segmentations distributed after thecompletion of the bakeoff.
We stripped out allof the additions and then added them back in oneby one, segmenting and scoring the test data eachtime.
What we found is that our system actu-ally performed best with the implementation ofthe Viterbi algorithm (which raised F scores by anaverage of about 0.09 compared to simply choos-ing the most likely tag at each stage) but withoutany of the extra outcome-dependent or indepen-dent features.
There were only two exceptions tothis:?
The system achieved slightly higher OOV re-call rates for the MSRA and CITYU corporawith the place-char and deng-listfeatures than without.?
The system achieved a very small increasein F score for the UPUC corpus with theplace-char feature than without.Besides these small differences, the model wasbest off without any of the features enumerated inSections 1.3 and 1.4, obtaining the scores listed inTable 2.
This is a surprising result, as in our testingthe added features helped to improve the F scoresand OOV recall rates of the system when dealingwith the 2005 bakeoff data, even if only by a smallamount in some cases.It should be noted that in our testing during de-velopment, even when we strove to create a systemwhich matched as closely as possible the one de-scribed by Low et al (2005), we were unable toachieve scores for the 2005 bakeoff data as highas their system did.
Why this was the case re-mains a mystery to us.
It is possible that at leastCorpus F R P ROOV RIVUPUC 0.948 0.954 0.943 0.781 0.970CKIP 0.957 0.962 0.952 0.698 0.973MSRA 0.964 0.963 0.964 0.731 0.971CITYU 0.974 0.976 0.972 0.816 0.983Table 2: Our results without the extra features.some of the gap is due to implementation differ-ences.
In particular, the maximum entropy toolkitutilized along with the training algorithms chosenseem likely candidates for sources of the disparity.4 ConclusionsUsing a maximum entropy approach based on amodification of the system described by Low, Ng,and Guo (2005), our system was able to achievea respectable level of accuracy when evaluated onthe corpora of the word segmentation task of theThird International Chinese Language ProcessingBakeoff.
Implementing the Viterbi decoding algo-rithm was very beneficial for F scores and OOVrecall rates.
However, it should be investigatedwhether the rest of the added features, especiallythe outcome-dependent ones, are useful in generalor if they were only beneficial for the 2005 testdata due to some pattern in that data, after whichthey were modeled.ReferencesJin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.2005.
A maximum entropy approach to Chi-nese word segmentation.
In Fourth SIGHANWorkshop on Chinese Language Processing, pages161?164.
URL http://www.aclweb.org/anthology/I05-3025.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania.Andrew J. Viterbi.
1967.
Error bounds for convolu-tional codes and an asymptotically optimum decod-ing algorithm.
IEEE Transactions on InformationTheory, 13(2):260?269.Le Zhang, 2004.
Maximum Entropy ModelingToolkit for Python and C++.
URL http://homepages.inf.ed.ac.uk/s0450736/.188
