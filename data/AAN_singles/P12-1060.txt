Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572?581,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCross-Lingual Mixture Model for Sentiment ClassificationXinfan Meng?
?Furu Wei?
Xiaohua Liu?
Ming Zhou?
Ge Xu?
Houfeng Wang?
?MOE Key Lab of Computational Linguistics, Peking University?Microsoft Research Asia?
{mxf, xuge, wanghf}@pku.edu.cn?
{fuwei,xiaoliu,mingzhou}@microsoft.comAbstractThe amount of labeled sentiment data in En-glish is much larger than that in other lan-guages.
Such a disproportion arouse interestin cross-lingual sentiment classification, whichaims to conduct sentiment classification in thetarget language (e.g.
Chinese) using labeleddata in the source language (e.g.
English).Most existing work relies on machine trans-lation engines to directly adapt labeled datafrom the source language to the target lan-guage.
This approach suffers from the limitedcoverage of vocabulary in the machine transla-tion results.
In this paper, we propose a gen-erative cross-lingual mixture model (CLMM)to leverage unlabeled bilingual parallel data.By fitting parameters to maximize the likeli-hood of the bilingual parallel data, the pro-posed model learns previously unseen senti-ment words from the large bilingual paralleldata and improves vocabulary coverage signifi-cantly.
Experiments on multiple data sets showthat CLMM is consistently effective in two set-tings: (1) labeled data in the target language areunavailable; and (2) labeled data in the targetlanguage are also available.1 IntroductionSentiment Analysis (also known as opinion min-ing), which aims to extract the sentiment informa-tion from text, has attracted extensive attention inrecent years.
Sentiment classification, the task ofdetermining the sentiment orientation (positive, neg-ative or neutral) of text, has been the most exten-sively studied task in sentiment analysis.
There is?Contribution during internship atMicrosoft ResearchAsia.already a large amount of work on sentiment classi-fication of text in various genres and in many lan-guages.
For example, Pang et al (2002) focus onsentiment classification of movie reviews in English,and Zagibalov and Carroll (2008) study the problemof classifying product reviews in Chinese.
Duringthe past few years, NTCIR1 organized several pi-lot tasks for sentiment classification of news articleswritten in English, Chinese and Japanese (Seki etal., 2007; Seki et al, 2008).For English sentiment classification, there are sev-eral labeled corpora available (Hu and Liu, 2004;Pang et al, 2002; Wiebe et al, 2005).
However, la-beled resources in other languages are often insuf-ficient or even unavailable.
Therefore, it is desir-able to use the English labeled data to improve senti-ment classification of documents in other languages.One direct approach to leveraging the labeled datain English is to use machine translation engines as ablack box to translate the labeled data from Englishto the target language (e.g.
Chinese), and then us-ing the translated training data directly for the devel-opment of the sentiment classifier in the target lan-guage (Wan, 2009; Pan et al, 2011).Although the machine-translation-based methodsare intuitive, they have certain limitations.
First,the vocabulary covered by the translated labeleddata is limited, hence many sentiment indicativewords can not be learned from the translated labeleddata.
Duh et al (2011) report low overlappingbetween vocabulary of natural English documentsand the vocabulary of documents translated to En-glish from Japanese, and the experiments of Duh1http://research.nii.ac.jp/ntcir/index-en.html572et al (2011) show that vocabulary coverage has astrong correlation with sentiment classification ac-curacy.
Second, machine translation may change thesentiment polarity of the original text.
For exam-ple, the negative English sentence ?It is too good tobe true?
is translated to a positive sentence in Chi-nese ??????????
by Google Translate(http://translate.google.com/), which literally means?It is good and true?.In this paper we propose a cross-lingual mixturemodel (CLMM) for cross-lingual sentiment classifi-cation.
Instead of relying on the unreliable machinetranslated labeled data, CLMM leverages bilingualparallel data to bridge the language gap between thesource language and the target language.
CLMM isa generative model that treats the source languageand target language words in parallel data as gener-ated simultaneously by a set of mixture components.By ?synchronizing?
the generation of words in thesource language and the target language in a parallelcorpus, the proposed model can (1) improve vocabu-lary coverage by learning sentiment words from theunlabeled parallel corpus; (2) transfer polarity labelinformation between the source language and targetlanguage using a parallel corpus.
Besides, CLMMcan improve the accuracy of cross-lingual sentimentclassification consistently regardless of whether la-beled data in the target language are present or not.We evaluate the model on sentiment classificationof Chinese using English labeled data.
The exper-iment results show that CLMM yields 71% in accu-racy when no Chinese labeled data are used, whichsignificantly improves Chinese sentiment classifica-tion and is superior to the SVMand co-training basedmethods.
When Chinese labeled data are employed,CLMMyields 83% in accuracy, which is remarkablybetter than the SVM and achieve state-of-the-art per-formance.This paper makes two contributions: (1) we pro-pose a model to effectively leverage large bilin-gual parallel data for improving vocabulary cover-age; and (2) the proposed model is applicable in bothsettings of cross-lingual sentiment classification, ir-respective of the availability of labeled data in thetarget language.The paper is organized as follows.
We review re-lated work in Section 2, and present the cross-lingualmixture model in Section 3.
Then we present the ex-perimental studies in Section 4, and finally concludethe paper and outline the future plan in Section 5.2 Related WorkIn this section, we present a brief review of the re-lated work on monolingual sentiment classificationand cross-lingual sentiment classification.2.1 Sentiment ClassificationEarly work of sentiment classification focuses onEnglish product reviews or movie reviews (Pang etal., 2002; Turney, 2002; Hu and Liu, 2004).
Sincethen, sentiment classification has been investigatedin various domains and different languages (Zag-ibalov and Carroll, 2008; Seki et al, 2007; Seki etal., 2008; Davidov et al, 2010).
There exist twomain approaches to extracting sentiment orientationautomatically.
The Dictionary-based approach (Tur-ney, 2002; Taboada et al, 2011) aims to aggregatethe sentiment orientation of a sentence (or docu-ment) from the sentiment orientations of words orphrases found in the sentence (or document), whilethe corpus-based approach (Pang et al, 2002) treatsthe sentiment orientation detection as a conventionalclassification task and focuses on building classifierfrom a set of sentences (or documents) labeled withsentiment orientations.Dictionary-based methods involve in creating orusing sentiment lexicons.
Turney (2002) derivessentiment scores for phrases by measuring the mu-tual information between the given phrase and thewords ?excellent?
and ?poor?, and then uses the av-erage scores of the phrases in a document as thesentiment of the document.
Corpus-based meth-ods are often built upon machine learning mod-els.
Pang et al (2002) compare the performanceof three commonly used machine learning models(Naive Bayes, Maximum Entropy and SVM).
Ga-mon (2004) shows that introducing deeper linguisticfeatures into SVM can help to improve the perfor-mance.
The interested readers are referred to (Pangand Lee, 2008) for a comprehensive review of senti-ment classification.2.2 Cross-Lingual Sentiment ClassificationCross-lingual sentiment classification, which aimsto conduct sentiment classification in the target lan-guage (e.g.
Chinese) with labeled data in the source573language (e.g.
English), has been extensively stud-ied in the very recent years.
The basic idea is to ex-plore the abundant labeled sentiment data in sourcelanguage to alleviate the shortage of labeled data inthe target language.Most existing work relies on machine translationengines to directly adapt labeled data from the sourcelanguage to target language.
Wan (2009) proposesto use ensemble method to train better Chinese sen-timent classification model on English labeled dataand their Chinese translation.
English Labeled dataare first translated to Chinese, and then two SVMclassifiers are trained on English andChinese labeleddata respectively.
After that, co-training (Blum andMitchell, 1998) approach is adopted to leverage Chi-nese unlabeled data and their English translation toimprove the SVM classifier for Chinese sentimentclassification.
The same idea is used in (Wan, 2008),but the ensemble techniques used are various vot-ing methods and the individual classifiers used aredictionary-based classifiers.Instead of ensemblemethods, Pan et al (2011) usematrix factorization formulation.
They extend Non-negative Matrix Tri-Factorization model (Li et al,2009) to bilingual view setting.
Their bilingual viewis also constructed by using machine translation en-gines to translate original documents.
Prettenhoferand Stein (2011) use machine translation engines ina different way.
They generalize Structural Corre-spondence Learning (Blitzer et al, 2006) to multi-lingual setting.
Instead of using machine translationengines to translate labeled text, the authors use it toconstruct the word translation oracle for pivot wordstranslation.Lu et al (2011) focus on the task of jointly im-proving the performance of sentiment classificationon two languages (e.g.
English and Chinese) .
theauthors use an unlabeled parallel corpus instead ofmachine translation engines.
They assume paral-lel sentences in the corpus should have the samesentiment polarity.
Besides, they assume labeleddata in both language are available.
They proposea method of training two classifiers based on maxi-mum entropy formulation to maximize their predic-tion agreement on the parallel corpus.
However, thismethod requires labeled data in both the source lan-guage and the target language, which are not alwaysreadily available.3 Cross-Lingual Mixture Model forSentiment ClassificationIn this section we present the cross-lingual mix-ture model (CLMM) for sentiment classification.We first formalize the task of cross-lingual sentimentclassification.
Then we describe the CLMM modeland present the parameter estimation algorithm forCLMM.3.1 Cross-lingual Sentiment ClassificationFormally, the task we are concerned about is to de-velop a sentiment classifier for the target language T(e.g.
Chinese), given labeled sentiment data DS inthe source language S (e.g.
English), unlabeled par-allel corpus U of the source language and the targetlanguage, and optional labeled dataDT in target lan-guage T .
Aligning with previous work (Wan, 2008;Wan, 2009), we only consider binary sentiment clas-sification scheme (positive or negative) in this paper,but the proposed method can be used in other classi-fication schemes with minor modifications.3.2 The Cross-Lingual Mixture ModelThe basic idea underlying CLMM is to enlargethe vocabulary by learning sentiment words from theparallel corpus.
CLMM defines an intuitive genera-tion process as follows.
Suppose we are going togenerate a positive or negative Chinese sentence, wehave two ways of generating words.
The first wayis to directly generate a Chinese word according tothe polarity of the sentence.
The other way is to firstgenerate an English word with the same polarity andmeaning, and then translate it to a Chinese word.More formally, CLMM defines a generative mix-ture model for generating a parallel corpus.
The un-observed polarities of the unlabeled parallel corpusare modeled as hidden variables, and the observedwords in parallel corpus are modeled as generated bya set of words generation distributions conditionedon the hidden variables.
Given a parallel corpus, wefit CLMM model by maximizing the likelihood ofgenerating this parallel corpus.
By maximizing thelikelihood, CLMM can estimate words generationprobabilities for words unseen in the labeled data butpresent in the parallel corpus, hence expand the vo-cabulary.
In addition, CLMM can utilize words inboth the source language and target language for de-574termining polarity classes of the parallel sentences.POSNEGPOSNEG...?SourceTargetUu wtwsFigure 1: The generation process of thecross-lingual mixture modelFigure 1 illustrates the detailed process of gener-ating words in the source language and target lan-guage respectively for the parallel corpus U , fromthe four mixture components in CLMM.
Particu-larly, for each pair of parallel sentences ui ?
U , wegenerate the words as follows.1.
Document class generation: Generating thepolarity class.
(a) Generating a polarity class cs from aBernoulli distribution Ps(C).
(b) Generating a polarity class ct from aBernoulli distribution Pt(C)2.
Words generation: Generating the words(a) Generating source language wordsws froma Multinomial distribution P (ws|cs)(b) Generating target language words wt froma Multinomial distribution P (wt|ct)3.
Words projection: Projecting the words ontothe other language(a) Projecting the source language wordsws totarget language words wt by word projec-tion probability P (wt|ws)(b) Projecting the target language words wt tosource language words ws by word projec-tion probability P (ws|wt)CLMM finds parameters by using MLE (Maxi-mum Likelihood Estimation).
The parameters to beestimated include conditional probabilities of wordto class, P (ws|c) and P (wt|c), and word projectionprobabilities, P (ws|wt) and P (wt|ws).
We will de-scribe the log-likelihood function and then show howto estimate the parameters in subsection 3.3.
Theobtained word-class conditional probability P (wt|c)can then be used to classify text in the target lan-guages using Bayes Theorem and the Naive Bayesindependence assumption.Formally, we have the following log-likelihoodfunction for a parallel corpus U2.L(?|U) =|Us|?i=1|C|?j=1|Vs|?s=1[Nsi log(P (ws|cj) + P (ws|wt)P (wt|cj))]+|Ut|?i=1|C|?j=1|Vt|?t=1[Nti log(P (wt|cj) + P (wt|ws)P (ws|cj))](1)where ?
is the model parameters;Nsi (Nti) is the oc-currences of thewordws (wt) in document di; |Ds| isthe number of documents; |C| is the number of classlabels; Vs and Vt are the vocabulary in the source lan-guage and the vocabulary in the target language.|Us|and |Ut| are the number of unlabeled sentences in thesource language and target language.Meanwhile, we have the following log-likelihoodfunction for labeled data in the source language Ds.L(?|Ds) =|Ds|?i=1|C|?j=1|Vs|?s=1Nsi logP (ws|cj)?ij (2)where ?ij = 1 if the label of di is cj , and 0 otherwise.In addition, when labeled data in the target lan-guage is available, we have the following log-likelihood function.L(?|Dt) =|Dt|?i=1|C|?j=1|Vt|?t=1Nti logP (wt|cj)?ij (3)Combining the above three likelihood functionstogether, we have the following likelihood function.L(?|Dt, Ds, U) = L(?|U) + L(?|Ds) + L(?|Dt)(4)Note that the third term on the right hand side(L(?|Dt)) is optional.2For simplicity, we assume the prior distribution P (C) isuniform and drop it from the formulas.5753.3 Parameter EstimationInstead of estimating word projection probability(P (ws|wt) and P (wt|ws)) and conditional proba-bility of word to class (P (wt|c) and P (ws|c)) si-multaneously in the training procedure, we estimatethem separately since the word projection probabil-ity stays invariant when estimating other parame-ters.
We estimate word projection probability usingword alignment probability generated by the Berke-ley aligner (Liang et al, 2006).
The word align-ment probabilities serves two purposes.
First, theyconnect the corresponding words between the sourcelanguage and the target language.
Second, they ad-just the strength of influences between the corre-sponding words.
Figure 2 gives an example of wordalignment probability.
As is shown, the three words?tour de force?
altogether express a positive mean-ing, while in Chinese the same meaning is expressedwith only one word ????
(masterpiece).
CLMMuse word alignment probability to decrease the in-fluences from ????
(masterpiece) to ?tour?, ?de?and ?force?
individually, using the word projectionprobability (i.e.
word alignment probability), whichis 0.3 in this case.Herman Melville's Moby Dick was a tour de force.???
????
?
??????
??
??
?1  1  .5  .5  1  1  .3  .
3  .
3Figure 2: Word Alignment ProbabilityWe use Expectation-Maximization (EM) algo-rithm (Dempster et al, 1977) to estimate the con-ditional probability of word ws and wt given classc, P (ws|c) and P (wt|c) respectively.
We derive theequations for EM algorithm, using notations similarto (Nigam et al, 2000).In the E-step, the distribution of hidden variables(i.e.
class label for unlabeled parallel sentences) iscomputed according to the following equations.P (cj |usi) = Z(cusi = cj) =?ws?usi [P (ws|cj) +?P (ws|wt)>0 P (ws|wt)P (wt|cj)]?cj?ws?usi [P (ws|cj) +?P (ws|wt)>0 P (ws|wt)P (wt|cj)](5)P (cj |uti) = Z(cuti = cj) =?wt?uti [P (wt|cj) +?P (wt|ws)>0 P (wt|ws)P (ws|cj)]?cj?wt?uti [P (wt|cj) +?P (wt|ws)>0 P (wt|ws)P (ws|cj)](6)whereZ(cusi = cj)(Z(cuti) = cj)is the probabilityof the source (target) language sentence usi (uti) inthe i-th pair of sentences ui having class label cj .In the M-step, the parameters are computed by thefollowing equations.P (ws|cj) =1 +?|Ds|i=1 ?s(i)NsiP (cj |di)|V | +?|Vs|s=1 ?
(i)NsiP (cj |di)(7)P (wt|cj) =1 +?|Dt|i=1 ?t(i)NtiP (cj |di)|V | +?|Vt|t=1 ?
(i)NtiP (cj |di)(8)where ?s(i) and ?t(i) are weighting factor to con-trol the influence of the unlabeled data.
We set ?s(i)(?t(i))to ?s(?t)when di belongs to unlabeleddata, 1 otherwise.
When di belongs to labeled data,P (cj |di) is 1 when its label is cj and 0 otherwise.When di belongs to unlabeled data, P (cj |di) is com-puted according to Equation 5 or 6.4 Experiment4.1 Experiment Setup and Data SetsExperiment setup: We conduct experiments ontwo common cross-lingual sentiment classificationsettings.
In the first setting, no labeled data in thetarget language are available.
This setting has real-istic significance, since in some situations we need toquickly develop a sentiment classifier for languagesthat we do not have labeled data in hand.
In thiscase, we classify text in the target language usingonly labeled data in the source language.
In the sec-ond setting, labeled data in the target language arealso available.
In this case, a more reasonable strat-egy is to make full use of both labeled data in thesource language and target language to develop thesentiment classifier for the target language.
In ourexperiments, we consider English as the source lan-guage and Chinese as the target language.Data sets: For Chinese sentiment classification,we use the same data set described in (Lu et al,2011).
The labeled data sets consist of two Englishdata sets and one Chinese data set.
The English dataset is from the Multi-Perspective Question Answer-ing (MPQA) corpus (Wiebe et al, 2005) and the NT-CIR Opinion Analysis Pilot Task data set (Seki etal., 2008; Seki et al, 2007).
The Chinese data setalso comes from the NTCIR Opinion Analysis Pi-lot Task data set.
The unlabeled parallel sentences576are selected from ISI Chinese-English parallel cor-pus (Munteanu and Marcu, 2005).
Following thedescription in (Lu et al, 2011), we remove neutralsentences and keep only high confident positive andnegative sentences as predicted by a maximum en-tropy classifier trained on the labeled data.
Table 1shows the statistics for the data sets used in the ex-periments.
We conduct experiments on two data set-tings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN+ NTCIR-CH.MPQA NTCIR-EN NTCIR-CHPositive 1,471(30%) 528 (30%) 2,378 (55%)Negative 3,487(70%) 1,209(70%) 1,916(44%)Total 4,958 1,737 4,294Table 1: Statistics about the DataCLMM includes two hyper-parameters (?s and?t) controlling the contribution of unlabeled paralleldata.
Larger weights indicate larger influence fromthe unlabeled data.
We set the hyper-parametersby conducting cross validations on the labeled data.WhenChinese labeled data are unavailable, we set?tto 1 and ?s to 0.1, since no Chinese labeled data areused and the contribution of target language to thesource language is limited.
When Chinese labeleddata are available, we set ?s and ?t to 0.2.To prevent long sentences from dominating the pa-rameter estimation, we preprocess the data set bynormalizing the length of all sentences to the sameconstant (Nigam et al, 2000), the average length ofthe sentences.4.2 Baseline MethodsFor the purpose of comparison, we implement thefollowing baseline methods.MT-SVM:We translate the English labeled data toChinese using Google Translate and use the transla-tion results to train the SVM classifier for Chinese.SVM: We train a SVM classifier on the Chineselabeled data.MT-Cotrain: This is the co-training based ap-proach described in (Wan, 2009).
We summarizethe main steps as follows.
First, two monolingualSVM classifiers are trained on English labeled dataand Chinese data translated from English labeleddata.
Second, the two classifiers make prediction onChinese unlabeled data and their English translation,respectively.
Third, the 100 most confidently pre-dicted English and Chinese sentences are added tothe training set and the twomonolingual SVMclassi-fiers are re-trained on the expanded training set.
Thesecond and the third steps are repeated for 100 timesto obtain the final classifiers.Para-Cotrain: The training process is the same asMT-Cotrain.
However, we use a different set of En-glish unlabeled sentences.
Instead of using the corre-sponding machine translation of Chinese unlabeledsentences, we use the parallel English sentences ofthe Chinese unlabeled sentences.Joint-Train: This is the state-of-the-art method de-scribed in (Lu et al, 2011).
This model use En-glish labeled data and Chinese labeled data to obtaininitial parameters for two maximum entropy clas-sifiers (for English documents and Chinese docu-ments), and then conduct EM-iterations to updatethe parameters to gradually improve the agreementof the two monolingual classifiers on the unlabeledparallel sentences.4.3 Classification Using Only English LabeledDataThe first set of experiments are conducted on us-ing only English labeled data to create the sentimentclassifier for Chinese.
This is a challenging task,since we do not use any Chinese labeled data.
AndMPQA and NTCIR data sets are compiled by differ-ent groups using different annotation guidelines.Method NTCIR-EN MPQA-ENNTCIR-CH NTCIR-CHMT-SVM 62.34 54.33SVM N/A N/AMT-Cotrain 65.13 59.11Para-Cotrain 67.21 60.71Joint-Train N/A N/ACLMM 70.96 71.52Table 2: Classification Accuracy Using OnlyEnglish Labeled DataTable 2 shows the accuracy of the baseline sys-tems as well as the proposed model (CLMM).
Asis shown, sentiment classification does not bene-fit much from the direct machine translation.
ForNTCIR-EN+NTCIR-CH, the accuracy of MT-SVM577is only 62.34%.
For MPQA-EN+NTCIR-CH, theaccuracy is 54.33%, even lower than a trivialmethod, which achieves 55.4% by predicting all sen-tences to be positive.
The underlying reason is thatthe vocabulary coverage in machine translated datais low, therefore the classifier learned from the la-beled data is unable to generalize well on the testdata.
Meanwhile, the accuracy of MT-SVM onNTCIR-EN+NTCIR-CH data set is much better thanthat on MPQA+NTCIR-CH data set.
That is be-cause NTCIR-EN and NTCIR-CH cover similar top-ics.
The other two methods using machine translateddata, MT-Cotrain and Para-Cotrain also do not per-form verywell.
This result is reasonable, because theinitial Chinese classifier trained on machine trans-lated data (MT-SVM) is relatively weak.
We alsoobserve that using a parallel corpus instead of ma-chine translations can improve classification accu-racy.
It should be noted that we do not have the resultfor Joint-Train model in this setting, since it requiresboth English labeled data and Chinese labeled data.4.4 Classification Using English and ChineseLabeled DataThe second set of experiments are conducted onusing both English labeled data and Chinese labeleddata to develop the Chinese sentiment classifier.
Weconduct 5-fold cross validations on Chinese labeleddata.
We use the same baseline methods as describedin Section 4.2, but we use natural Chinese sentencesinstead of translated Chinese sentences as labeleddata in MT-Cotrain and Para-Cotrain.
Table 3 showsthe accuracy of baseline systems as well as CLMM.Method NTCIR-EN MPQA-ENNTCIR-CH NTCIR-CHMT-SVM 62.34 54.33SVM 80.58 80.58MT-Cotrain 82.28 80.93Para-Cotrain 82.35 82.18Joint-Train 83.11 83.42CLMM 82.73 83.02Table 3: Classification Accuracy Using English andChinese Labeled DataAs is seen, SVMperforms significantly better thanMT-SVM.
One reason is that we use natural Chi-nese labeled data instead of translated Chinese la-beled data.
Another reason is that we use 5-foldcross validations in this setting, while the previoussetting is an open test setting.
In this setting, SVMis a strong baseline with 80.6% accuracy.
Never-theless, all three methods which leverage an unla-beled parallel corpus, namely Para-Cotrain, Joint-Train and CLMM, still show big improvements overthe SVM baseline.
Their results are comparable andall achieve state-of-the-art accuracy of about 83%,but in terms of training speed, CLMM is the fastestmethod (Table 4).
Similar to the previous setting,Wealso have the same observation that using a parallelcorpus is better than using translations.Method Iterations Total TimePara-Cotrain 100 6 hoursJoint-Train 10 55 secondsCLMM 10 30 secondsTable 4: Training Speed Comparison4.5 The Influence of Unlabeled Parallel DataWe investigate how the size of the unlabeled par-allel data affects the sentiment classification in thissubsection.
We vary the number of sentences in theunlabeled parallel from 2,000 to 20,000.
We useonly English labeled data in this experiment, sincethis more directly reflects the effectiveness of eachmodel in utilizing unlabeled parallel data.
From Fig-ure 3 and Figure 4, we can see that when more unla-beled parallel data are added, the accuracy of CLMMconsistently improves.
The performance of CLMMis remarkably superior than Para-Cotrain and MT-Cotrain.
When we have 10,000 parallel sentences,the accuracy of CLMM on the two data sets quicklyincreases to 68.77% and 68.91%, respectively.
Bycontrast, we observe that the performance of Para-Cotrain and MT-Cotrain is able to obtain accuracyimprovement only after about 10,000 sentences areadded.
The reason is that the two methods use ma-chine translated labeled data to create initial Chineseclassifiers.
As is depicted in Table 2, these classifiersare relatively weak.
As a result, in the initial itera-tions of co-training based methods, the predictionsmade by the Chinese classifiers are inaccurate, andco-training based methods need to see more parallel578Number of SentencesAccuracy6264666870ll l ll ll l l l5000 10000 15000 20000Modell CLMM MT?Cotrain Para?CotrainFigure 3: Accuracy with different size ofunlabeled data for NTICR-EN+NTCIR-CHNumber of SentencesAccuracy55606570llll ll l l l l5000 10000 15000 20000Modell CLMM MT?Cotrain Para?CotrainFigure 4: Accuracy with different size ofunlabeled data for MPQA+NTCIR-CHNumber of SentencesAccuracy65707580l ll ll ll500 1000 1500 2000 2500 3000 3500Modell CLMM Joint?Train Para?Cotrain SVMFigure 5: Accuracy with different size oflabeled data for NTCIR-EN+NTCIR-CHNumber of SentencesAccuracy65707580l ll ll ll500 1000 1500 2000 2500 3000 3500Modell CLMM Joint?Train Para?Cotrain SVMFigure 6: Accuracy with different size oflabeled data for MPQA+NTCIR-CHsentences to refine the initial classifiers.4.6 The Influence of Chinese Labeled DataIn this subsection, we investigate how the size ofthe Chinese labeled data affects the sentiment classi-fication.
As is shown in Figure 5 and Figure 6, whenonly 500 labeled sentences are used, CLMM is capa-ble of achieving 72.52% and 74.48% in accuracy onthe two data sets, obtaining 10% and 8% improve-ments over the SVM baseline, respectively.
Thisindicates that our method leverages the unlabeleddata effectively.
When more sentences are used,CLMM consistently shows further improvement inaccuracy.
Para-Cotrain and Joint-Train show simi-lar trends.
When 3500 labeled sentences are used,SVM achieves 80.58%, a relatively high accuracyfor sentiment classification.
However, CLMM andthe other two models can still gain improvements.This further demonstrates the advantages of expand-ing vocabulary using bilingual parallel data.5 Conclusion and Future WorkIn this paper, we propose a cross-lingual mix-ture model (CLMM) to tackle the problem of cross-lingual sentiment classification.
This method hastwo advantages over the existing methods.
First, theproposed model can learn previously unseen senti-ment words from large unlabeled data, which are notcovered by the limited vocabulary in machine trans-lation of the labeled data.
Second, CLMM can ef-fectively utilize unlabeled parallel data regardless ofwhether labeled data in the target language are usedor not.
Extensive experiments suggest that CLMMconsistently improve classification accuracy in bothsettings.
In the future, we will work on leverag-ing parallel sentences and word alignments for othertasks in sentiment analysis, such as building multi-lingual sentiment lexicons.Acknowledgment We thank Bin Lu and Lei Wang fortheir help.
This research was partly supported by National HighTechnology Research and Development Program of China (863Program) (No.
2012AA011101) and National Natural ScienceFoundation of China (No.91024009, No.60973053)579ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, page 120?128.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the eleventh annual conference on Computa-tional learning theory, page 92?100.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,page 241?249.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), page 1?38.Kevin Duh, Akinori Fujino, and Masaaki Nagata.
2011.Is machine translation ripe for Cross-Lingual sentimentclassification?
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, page 429?433,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Michael Gamon.
2004.
Sentiment classification on cus-tomer feedback data: noisy data, large feature vectors,and the role of linguistic analysis.
InProceedings of the20th international conference on Computational Lin-guistics, page 841.Mingqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowledgediscovery and data mining, page 168?177.Tao Li, Yi Zhang, and Vikas Sindhwani.
2009.
A non-negative matrix tri-factorization approach to sentimentclassification with lexical prior knowledge.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, page 244?252, Suntec, Singapore, August.Association for Computational Linguistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the main con-ference on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, page 104?111.Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.Tsou.
2011.
Joint bilingual sentiment classificationwith unlabeled parallel corpora.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies-Volume 1, page 320?330.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Kamal Nigam, Andrew Kachites McCallum, SebastianThrun, and Tom Mitchell.
2000.
Text classificationfrom labeled and unlabeled documents using EM.
Ma-chine learning, 39(2):103?134.Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang.2011.
Cross-lingual sentiment classification via bi-view non-negative matrix tri-factorization.
Advancesin Knowledge Discovery and Data Mining, page289?300.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing-Volume 10, page 79?86.Peter Prettenhofer and Benno Stein.
2011.
Cross-lingualadaptation using structural correspondence learning.ACM Transactions on Intelligent Systems and Technol-ogy (TIST), 3(1):13.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-HsiChen, Noriko Kando, and Chin-Yew Lin.
2007.Overview of opinion analysis pilot task at NTCIR-6.In Proceedings of NTCIR-6 Workshop Meeting, page265?278.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin.2008.
Overview of multilingual opinion analysis taskat NTCIR-7.
In Proc.
of the Seventh NTCIR Workshop.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-Based meth-ods for sentiment analysis.
Comput.
Linguist., page toappear.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguistics,page 417?424.Xiaojun Wan.
2008.
Using bilingual knowledge and en-semble techniques for unsupervised chinese sentimentanalysis.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, EMNLP?08, page 553?561, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Xiaojun Wan.
2009.
Co-training for cross-lingual senti-ment classification.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL and580the 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1-Volume 1,page 235?243.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation,39(2):165?210.Taras Zagibalov and John Carroll.
2008.
Automatic seedword selection for unsupervised sentiment classifica-tion of chinese text.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics-Volume 1, page 1073?1080.581
