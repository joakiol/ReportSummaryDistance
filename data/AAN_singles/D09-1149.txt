Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437?1445,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSemi-Supervised Learning for Semantic Relation Classification usingStratified Sampling StrategyLonghua Qian   Guodong Zhou   Fang Kong   Qiaoming ZhuJiangsu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and Technology, Soochow University1 Shizi Street, Suzhou, China 215006{qianlonghua,gdzhou,kongfang,qmzhu}@suda.edu.cnAbstractThis paper presents a new approach toselecting the initial seed set using stratifiedsampling strategy in bootstrapping-basedsemi-supervised learning for semantic relationclassification.
First, the training data ispartitioned into several strata according torelation types/subtypes, then relation instancesare randomly sampled from each stratum toform the initial seed set.
We also investigatedifferent augmentation strategies in iterativelyadding reliable instances to the labeled set, andfind that the bootstrapping procedure may stopat a reasonable point to significantly decreasethe training time without degrading too muchin performance.
Experiments on the ACERDC 2003 and 2004 corpora show thestratified sampling strategy contributes morethan the bootstrapping procedure itself.
Thissuggests that a proper sampling strategy iscritical in semi-supervised learning.1 IntroductionWith the dramatic increase in the amount oftextual information available in digital archivesand the WWW, there has been growing interestin techniques for automatically extractinginformation from text documents.
InformationExtraction (IE) is such a technology that IEsystems are expected to identify relevantinformation (usually of pre-defined types) fromtext documents in a certain domain and put themin a structured format.According to the scope of the NIST AutomaticContent Extraction (ACE) program (ACE, 2000-2007), current research in IE has three mainobjectives: Entity Detection and Tracking (EDT),Relation Detection and Characterization (RDC),and Event Detection and Characterization (EDC).This paper focuses on the ACE RDC subtask,where many machine learning methods havebeen proposed, including supervised methods(Miller et al, 2000; Zelenko et al, 2002; Culottaand Soresen, 2004; Kambhatla, 2004; Zhou et al,2005; Zhang et al, 2006; Qian et al, 2008),semi-supervised methods (Brin, 1998; Agichteinand Gravano, 2000; Zhang, 2004; Chen et al,2006; Zhou et al, 2008), and unsupervisedmethods (Hasegawa et al, 2004; Zhang et al,2005).Current work on semantic relation extractiontask mainly uses supervised learning methods,since it achieves relatively better performance.However this method requires a large amount ofmanually labeled relation instances, which isboth time-consuming and laborious.
In thecontrast, unsupervised methods do not needdefinitions of relation types and hand-tagged data,but it is difficult to evaluate their performancesince there are no criteria for evaluation.Therefore, semi-supervised learning has receivedmore and more attention, as it can balance theadvantages and disadvantages betweensupervised and unsupervised methods.
With theplenitude of unlabeled natural language data athand, semi-supervised learning can significantlyreduce the need for labeled data with onlylimited sacrifice in performance.
Specifically, abootstrapping algorithm chooses the unlabeledinstances with the highest probability of beingcorrectly labeled and use them to augmentlabeled training data iteratively.Although previous work (Yarowsky, 1995;Blum and Mitchell, 1998; Abney, 2000; Zhang,2004) has tackled the bootstrapping approachfrom both the theoretical and practical point ofview, many key problems still remain unresolved,such as the selection of initial seed set.
Since thesize of the initial seed set is usually small (e.g.1437100 instances), the imbalance of relation types ormanifold structure (cluster structure) in it willseverely weaken the strength of bootstrapping.Therefore, it is critical for a bootstrappingapproach to select the most appropriate initialseed set.
However, current systems (Zhang, 2004;Chen et al, 2006) use a randomly samplingstrategy, which fails to explore the affinity natureamong the training instances.
Alternatively,Zhou et al (2008) bootstrap a set of weightedsupport vectors from both labeled and unlabeleddata using SVM.
Nevertheless, the initial labeleddata is still randomly generated only to ensurethat there are at least 5 instances for everyrelation subtype.This paper presents a new approach toselecting the initial seed set based on stratifiedsampling strategy in the bootstrapping procedurefor semi-supervised semantic relationclassification.
The motivation behind thestratified sampling is that every relation typeshould be as much as possible represented in theinitial seed set, thus leading to more instanceswith diverse structures being added to the labeledset.
In addition, we also explore differentstrategies to augment reliably classified instancesto the labeled data iteratively, and attempt to finda stoppage criterion for the iteration procedure togreatly decrease the training time, other thanusing up all the unlabeled set.The rest of this paper is organized as follows.First, Section 2 reviews related work on semi-supervised relation extraction.
Then we presentan underlying supervised learner in Section 3.Section 4 details various key aspects of thebootstrapping procedure, including the stratifiedsampling strategy.
Experimental results arereported in Section 5.
Finally we conclude ourwork in Section 6.2 Related WorkWithin the realm of information extraction,currently there are several representative semi-supervised learning systems for extractingrelations between named entities.DIPRE (Dual Iterative Pattern RelationExpansion) (Brin, 1998) is a system based onbootstrapping that exploits the duality betweenpatterns and relations to augment the targetrelation starting from a small sample.
However,it only extracts simple relations such as (author,title) pairs from the WWW.
Snowball (Agichteinand Gravano, 2000) is another bootstrapping-based system that extracts relations fromunstructured text.
Snowball shares much incommon with DIPRE, including the use of boththe bootstrapping framework and the patternmatching approach to extract new unlabeledinstances.
Due to pattern matching techniques,their systems are hard to be adapted to thegeneral problem of relation extraction.Zhang (2004) approaches the relationclassification problem with bootstrapping on topof SVM.
He uses various lexical and syntacticfeatures in the BootProject algorithm based onrandom feature projection to extract top-levelrelation types in the ACE corpus.
Evaluationshows that bootstrapping can alleviate the burdenof hand annotations for supervised learningmethods to a certain extent.Chen et al (2006) investigate a semi-supervised learning algorithm based on labelpropagation for relation extraction, where labeledand unlabeled examples and their distances arerepresented as the nodes and the weights ofedges respectively in a connected graph, then thelabel information is propagated from any vertexto nearby vertices through weighted edgesiteratively, finally the labels of unlabeledexamples are inferred after the propagationprocess converges.Zhou et al (2008) integrate the advantages ofSVM bootstrapping in learning critical instancesand label propagation in capturing the manifoldstructure in both the labeled and unlabeled data,by first bootstrapping a moderate number ofweighted support vectors through a co-trainingprocedure from all the available data, and thenapplying label propagation algorithm via thebootstrapped support vectors.However, in most current systems, the initialseed set is selected randomly such that they maynot adequately represent the inherent structure ofunseen examples, hence the power ofbootstrapping may be severely weakened.This paper presents a simple yet effectiveapproach to generate the initial seed set byapplying the stratified sampling strategy,originated from statistics theory.
Furthermore,we try to employ the same stratified strategy toaugment the labeled set.
Finally, we attempt tofind a reasonable criterion to terminate theiteration process.3 Underlying Supervised LearningA semi-supervised learning system usuallyconsists of two relevant components: anunderlying supervised learner and a1438bootstrapping algorithm on top of it.
In thissection we discuss the former, while the latterwill be described in the following section.In this paper, we select Support VectorMachines (SVMs) as the underlying supervisedclassifier since it represents the state-of-the-art inthe machine learning research community, andthere are good implementations of the algorithmavailable.
Specifically, we use LIBSVM (Changet al, 2001), an effective tool for support vectorclassification, since it supports multi-classclassification and provides probability estimationas well.For each pair of entity mentions, we extractand compute various lexical and syntacticfeatures, as employed in a state-of-the-artrelation extraction system (Zhou et al, 2005).
(1) Words: According to their positions, fourcategories of words are considered: a) the wordsof both the mentions; b) the words between thetwo mentions; c) the words before M1; and d)the words after M2.
(2) Entity type: This category of featuresconcerns about the entity types of both thementions.
(3) Mention Level: This category of featuresconsiders the entity level of both the mentions.
(4) Overlap: This category of features includesthe number of other mentions and words betweentwo mentions.
Typically, the overlap features areusually combined with other features such asentity type and mention level.
(5) Base phrase chunking: The base phrasechunking is proved to play an important role insemantic relation extraction.
Most of thechunking features concern about the headwordsof the phrases between the two mentions.In this paper, we do not employ any deepsyntactic or semantic features (such asdependency tree, full parse tree etc.
), since theycontribute quite limited in relation extraction.4 Bootstrapping & Stratified SamplingWe first present the self-bootstrapping algorithm,and then discuss several key problems onbootstrapping in the order of initial seedselection, augmentation of labeled data andstoppage criterion for iteration.4.1 Bootstrapping AlgorithmFollowing Zhang (2004), we define a basic self-bootstrapping strategy, which keeps augmentingthe labeled data set with the modelsstraightforwardly trained from previouslyavailable labeled data as follows:Require: labeled seed set LRequire: unlabeled data set URequire: batch size SRepeatTrain a single classifier on LRun the classifier on UFind at most S instances in U that the classifier hasthe highest prediction confidenceAdd them into LUntil: no data points available or the stoppagecondition is reachedAlgorithm self-bootstrappingFigure 1.
Self-bootstrapping algorithmIn order to measure the confidence of theclassifier?s prediction, we compute the entropyof the label probability distribution that theclassifier assigns to the class label on an example(the lower the entropy, the higher the confidence):logni iiH p p= ??
(1)Where n denotes the total number of relationclasses, and pi denotes the probability of currentexample being classified as the ith class.4.2 Stratified Sampling for Initial SeedsNormally, the number of available labeledinstances is quite limited (usually less than 100instances) when the iterative bootstrappingprocedure begins.
If the distribution of the initialseed set fails to approximate the distribution ofthe test data, the augmented data generated frombootstrapping would not capture the essence ofrelation types, and the performance on the testset will significantly decrease even only after oneor two rounds of iterations.
Therefore, theselection of initial seed set plays an importantrole in bootstrapping-based semantic relationextraction.Sampling is a part of statistical practiceconcerned with the selection of individualobservations, which is intended to yield someknowledge about a population of interest.
Whendealing with the task of semi-supervisedsemantic relation classification, the population isthe training set of relation instances from theACE RDC corpora.
We compare two practicalsampling strategies as follows:(1) Randomly sampling, which picks the initialseeds from the training data using a randomscheme.
Each element thus has an equalprobability of selection, and the population is not1439subdivided or partitioned.
Currently, most workon semi-supervised relation extraction employsthis method.
However, since the size of the initialseed set is very small, they are not guaranteed tocapture the statistical properties of the wholetraining data, let alne of the test data.
(2) Stratified sampling.
When the populationembraces a number of distinct categories,stratified sampling (Neyman, 1934) can beapplied to this case.
First, the population can beorganized by these categories into separate"strata", then a sample is selected within each"stratum" separately, and randomly.
Generally,the sample size is normally proportional to therelative size of the strata.
The main motivationfor using a stratified sampling design is to ensurethat particular groups within a population areadequately represented in the sample.It is well known that the number of theinstances for each relation type in the ACE RDCcorpora is greatly unbalanced  (Zhou et al, 2005)as shown in Table 1 for the ACE RDC 2004corpus.
When the relation instances for a specificrelation type occurs frequently in the initial seedset, the classifier will achieve good performanceon this type, otherwise the classifier can hardlyrecognize them from the test set.
In order forevery type of relations to be properly represented,the stratified sampling strategy is applied to theseed selection procedure.Types Subtypes Train TestLocated 593 145Near 70 17PHYSPart-Whole 299 79Business 134 39Family 101 20PER-SOCOther 44 11Employ-Executive 388 101Employ-Staff 427 112Employ-Undetermined 66 12Member-of-Group 152 39Subsidiary 169 37Partner 10 2EMP-ORGOther 64 16User-or-Owner 160 40Inventor-or-Man.
8 1ARTOther 1 1Ethnic 31 8Ideology 39 9OTHER-AFFOther 43 11Citizen-or-Resid.
226 47Based-In 165 50GPE-AFFOther 31 8DISC  224 55Total  3445 860Table 1.
Numbers of relations on the ACE RDC2004: break down by relation types and subtypesFigure 2 illustrates the stratified samplingstrategy we use in bootstrapping, where RSETdenotes the training set, V is the stratificationvariable, and SeedSET denotes the initial seed set.First, we divide the relation instances intodifferent strata according to available properties,such as major relation type (considering reverserelations or not) and relation subtype(considering reverse relations or not).
Thenwithin every stratum, a certain number ofinstances are sampled randomly, and this numberis normally proportional to the size of thatstratum in the whole population.
However, whenthis number is 0 due to the rounding of realnumbers, it is set to 1.
Also it must be ensuredthat the total number of instances being sampledis NS.
Finally, these instances form the initialseed set and can be used as the input to theunderlying supervised learning for thebootstrapping procedure.Require: RSET ={R1,R2,?,RN}Require: V = {v1, v2,?,vK}Require: SeedSET with the size of NS (100)Initialization:SeedSET = NULLSteps:z Group RSET into K strata according to thestratified variable V, i.e.
:RSET={RSET1,RSET2,?,RSETK}z Calculate the class prior probability for eachstratum i={1,2,?,K})(/)( RSETNUMRSETNUMP ii =z Caculate the number of intances being sampledfor each stratumNPN ii ?=If Ni =0 then Ni=1z Calculate the difference of numbers as follows:?=?
?=KiiS NNN1z If N?>0 then add Ni (i=1,2,?,|N?|) by 1If N?<0 then subtract 1 from Ni (i=1,2,...,|N?|)z For each i from 1 to KSelect Ni instances from RESTi randomlyAdd them into SeedSETFigure 2.
Stratefied Sampling for initial seeds4.3 Augmentation of labeled dataAfter each round of iteration, some newlyclassified instances with the highest confidencecan be augmented to the labeled training data.Nevertheless, just like the selection of initial seedset, we still wish that every stratum would berepresented as appropriately as possible in the1440instances added to the labeled set.
In this paper,we compare two kinds of augmentation strategiesavailable:(1) Top n method: the classified instances arefirst sorted in the ascending order by theirentropies (i.e.
decreasing confidence), and thenthe top n (usually 100) instances are chosen to beadded.
(2) Stratified method: in order to make theadded instances representative for their stratum,we first select m (usually greater than n)instances with the highest confidence, then wechoose n instances from them using the stratifiedstrategy.4.4 Stoppage of IterationsIn a self-bootstrapping procedure, as theiterations go on, both the reliable and unreliableinstances are added to the labeled datacontinuously, hence the performance willfluctuate in a relatively small range.
The keyquestion here is how we can know when thebootstrapping procedure reaches its bestperformance on the test data.
The bootstrappingalgorithm by Zhang (2004) stops after it runs outof all the training instances, which may take arelatively long time.
In this paper, we present amethod to determine the stoppage criterion basedon the mean entropy as follows:Hi <= p    (2)Where Hi denotes the mean entropy of theconfidently classified instances being augmentedto the labeled data in each iteration, and pdenotes a threshold for the mean entropy, whichwill be fixed through empirical experiments.This criterion is based on the assumption thatwhen the mean entropy becomes less than orequal to a certain threshold, the classifier wouldachieve the most reliable confidence on theinstances being added to the labeled set, and itmay be impossible to yield better performancesince then.
Therefore, the iteration may stop atthat reasonable point.5 ExperimentationThis section aims to empirically investigate theeffectiveness of the bootstrapping-based semi-supervised learning we discussed above forsemantic relation classification.
In particular,different methods for selecting the initial seed setand augmenting the labeled data are evaluated.5.1 Experimental SettingWe use the ACE corpora as the benchmark data,which are gathered from various newspapers,newswire and broadcasts.
The ACE 2004 corpuscontains 451 documents and 5702 positiverelation instances.
It defines 7 relation types and23 subtypes between 7 entity types.
For easyreference with related work in the literature,evaluation is also done on 347 documents(including nwire and bnews domains) and 4305relation instances using 5-fold cross-validation.That is, these relation instances are first dividedinto 5 sets, then, one of them (about 860instances) is used as the test data set, while theothers are regarded as the training data set, fromwhich the initial seed set is sampled.
In the ACE2003 corpus, the training set consists of 674documents and 9683 positive relation instanceswhile the test data consists of 97 documents and1386 positive relation instances.
The ACE RDC2003 task defines 5 relation types and 24subtypes between 5 entity types.The corpora are first parsed using Collins?sparser (Collins, 2003) with the boundaries of allthe entity mentions kept.
Then, the parse treesare converted into chunklink format usingchunklink.pl 1.
Finally, various useful lexical andsyntactic features, as described in Subsection 3.1,are extracted and computed accordingly.
For thepurpose of comparison, we define our task as theclassification of the 5 or 7 major relation types inthe ACE RDC 2003 and 2004 corpora.For LIBSVM parameters, we adopted thepolynomial kernel, and c is set to 10, g is set to0.15.
Under this setting, we achieved the bestclassification performance.5.2 Experimental ResultsIn this subsection, we compare and discuss theexperimental results using various samplingstrategies, different augmentation methods, anditeration stoppage criterion.Comparison of sampling strategies in selectingthe initial seed setTable 2 and Table 3 show the initial and thehighest classification performance ofPrecision/Recall/F-measure for various samplingstrategies of the initial seed set on 7 majorrelation types of the ACE RDC 2004 corpusrespectively when the size of initial seed set L is100, the batch size S is 100, and the top 1001 http://ilk.kub.nl/~sabine/chunklink/1441instances with the highest confidence are addedat each iteration.
Table 2 also lists the number ofstrata for stratified sampling methods from whichthe initial seeds are randomly chosenrespectively.
Table 3 additionally lists the timeneeded to complete the bootstrapping process (ona PC with a Pentium IV 3.0G CPU and 1Gmemory).
In this paper, we consider thefollowing five experimental settings whensampling the initial seeds:z Randomly Sampling: as described inSubsection 4.2.z Stratified-M Sampling: the strata aregrouped in terms of major relation typeswithout considering reverse relations.z Stratified-MR Sampling: the strata aregrouped in terms of major relation types,including reverse relations.z Stratified-S Sampling: the strata aregrouped in terms of relation subtypeswithout considering reverse subtypes.z Stratified-SR Sampling: the strata aregrouped in terms of relation subtypes,including reverse subtypes.For each sampling strategies, we performed 20trials and computed average scores and the totaltime on the test set over these 20 trials.Sampling strategiesfor initial seeds# ofstrat.
P(%) R(%) FRandomly 1 66.1 65.9 65.9Stratified-M 7 69.1 66.5 67.7Stratified-MR 13 69.3 67.3 68.2Stratified-S 30 69.8 67.7 68.7Stratified-SR 39 69.9 68.5 69.2Table 2.
The initial performance of applyingvarious sampling strategies to selecting the initialseed set on the ACE RDC 2004 corpusSampling strategiesfor initial seedsTime(min) P(%) R(%) FRandomly 52 68.6 66.2 67.3Stratified-M 65 71.0 66.9 68.8Stratified-MR 65 71.6 67.0 69.2Stratified-S 71 72.7 67.8 70.1Stratified-SR 77 72.9 68.4 70.6Table 3.
The highest performance of applyingvarious sampling strategies in selecting the initialseed set on the ACE RDC 2004 corpusThese two tables jointly indicate that the self-bootstrapping procedure for all samplingstrategies can moderately improve theclassification performance by ~1.2 units in F-score, which is also verified by Zhang (2004).Furthermore, they show that:z The most improvements in performancecome from improvements in precision.
Actually,for some settings the recalls even decreaseslightly.
The reason may be that due to the natureof self-bootstrapping, the instances augmented ateach iteration are always those which are themost similar to the initial seed instances,therefore the models trained from them wouldexhibit higher precision on the test set, while itvirtually does no help for recall.z All of the four stratified sampling methodsoutperform the randomly sampling method tovarious degrees, both in the initial performanceand the highest performance.
This means thatsampling of the initial seed set based onstratification by major/sub relation types can behelpful to relation classification, largely due tothe performance improvement of the initial seedset, which is caused by adequate representationof instances for every relation type.z Of all the four stratified sampling methods,the Stratified-SR sampling achieves the bestperformance of 72.9/68.4/70.6 in P/R/F.Moreover, the more the number of stratagenerated by the sampling strategy, the moreappropriately they would be represented in theinitial seed set, and the better performance it willyield.
This also implies that the hierarchy ofrelation types/subtypes in the ACE RDC 2004corpus is fairly reasonably defined.z An important conclusion, which can bedraw accordingly, is that the F-scoreimprovement of Stratified-SR sampling overRandomly sampling in initial performance (3.3units) is significantly greater than the F-scoreimprovement gained by bootstrapping itselfusing Randomly sampling (1.4 units).
This meansthat the sampling strategy of the initial seed set iseven more important than the bootstrappingalgorithm itself for relation classification.z It is interesting to note that the time neededto bootstrap increases with the number of strata.The reason may be that due to more diversestructures in the labeled data for stratifiedsampling, the SVM needs more time todifferentiate between instances, i.e.
more time tolearn the models.Comparison of different augmentationstrategies of training dataFigure 3 compares the performance of F-scorefor two augmentation strategies: the Top nmethod and the stratified method, over variousinitial seed sampling strategies on the ACE RDC2004 corpus.
For each iteration, a variable1442number (m is ranged from 100 to 500) ofclassified instances in the decreasing order ofconfidence are first chosen as the base examples,then at most 100 examples are selected from thebase examples to be augmented to the labeled set.Specifically, when m is equal to 100, the wholeset of the base example is added to the labeleddata, i.e.
degenerated to the Top n augmentationstrategy.
On the other hand, when m is greaterthan 100, we wish we would select examples ofdifferent major relation types from the baseexamples according to their distribution in thetraining set, in order to achieve the performanceimprovement as much as the stratified samplingdoes in the selection of the initial seed set.646566676869707172100 200 300 400 500# Base examplesF-scoreRandomlyStratified-MStratified-MRStratified-SStratified-SRFigure 3.
Comparison of two augmentationstrategies over different sampling strategies inselecting the initial seed set.This figure shows that, except for randomlysampling strategy, the stratified augmentationstrategies improve the performance.
Nevertheless,this result is far from our expectation in twoways:z The performance improvement in F-score istrivial, at most 0.4 units on average.
The reasonmay be that, although we try to add as many as100 classified instances to the labeled dataaccording to the distribution of every majorrelation type in the training set, the top minstances with the highest confidence are usuallyfocused on certain relation types (e.g.
PHSY andPER-SOC), this leads to the stratifiedaugmentation failing to function effectively.Hence, all the following experiments will onlyadopt Top n method for augmenting the labeleddata.z With the increase of the number of the baseexamples, the performance fluctuates slightly,thus it is relatively difficult to recognize wherethe optima is.
We think there are twocontradictory factors that affect the performance.While the reliability of the instances extractedfrom the base examples decreases with theincrease of the number of base examples, theprobability of extracting instances of morerelation types increases with the increase of thenumber of the base examples.
These two factorsinversely interact with each other, leading to thefluctuation in performance.Comparison of different threshold values forstoppage criterionWe compare the performance andbootstrapping time (20 trials with the same initialseed set) when applying stoppage criterion inFormula (2) with different threshold p overvarious sampling strategies on the ACE RDC2004 corpus in Figure 4 and Figure 5respectively.
These two figures jointly show that:64656667686970710 0.2 0.22 0.24 0.26 0.28 0.3pF-scoreRandomlyStratified-MStratified-MRStratified-SStratified-SRFigure 4.
Performance for different p values01020304050607080900 0.2 0.22 0.24 0.26 0.28 0.3pTime(min)RandomlyStratified-MStratified-MRStratified-SStratified-SRFigure 5.
Bootstrapping time for different pvaluesz The performance decreases slowly while thebootstrapping time decreases dramatically withthe increase of p from 0 to 0.3.
Specifically,when the p equals to 0.3, the bootstrapping timetends to be neglected, while the performance isalmost similar to the initial performance.
Itimplies that we can find a reasonable point foreach sampling strategy, at which the time fallsgreatly while the performance nearly does notdegrade.1443Bootproject LP-js Stratified Bootstrapping Relation typesP R F P R F P R FROLE 78.5 69.7 73.8 81.0 74.7 77.7 74.7 86.3 80.1PART 65.6 34.1 44.9 70.1 41.6 52.2 66.4 47.0 55.0AT 61.0 84.8 70.9 74.2 79.1 76.6 74.9 66.1 70.2NEAR - - - 13.7 12.5 13.0 100.0 2.9 5.6SOC 47.0 57.4 51.7 45.0 59.1 51.0 65.2 79.0 71.4Average 67.9 67.4 67.6 73.6 69.4 70.9 73.8 73.3 73.5Table 4.
Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpusz Clearly, if the performance is the primaryconcern, then p=0.2 may be the best choice inthat we can get ~30% saving on the time at thecost of only ~0.08 loss in F-score on average.
Ifthe time is a primary concern, then p=0.22 is areasonable threshold in that we get ~50% savingon the time at the cost of ~0.25 units loss in F-score on average.
This suggests that ourproposed stoppage criterion is effective toterminate the bootstrapping procedure withminor performance loss.Comparison of Stratified Bootstrapping withBootproject and Label propagationTable 4 compares Bootproject (Zhang, 2004),Label propagation (Chen et al, 2006) with ourStratified Bootstrapping on the 5 major types ofthe ACE RDC 2003 corpus.Both Bootproject and Label propagationselect 100 initial instances randomly, and at eachiteration, the top 100 instances with the highestconfidence are added to the labeled data.Differently, we choose 100 initial seeds usingstratified sampling strategy; similarly, the top100 instances with the highest confidence areaugmented to the labeled data at each iteration.Due to the lack of comparability followed fromthe different size of the labeled data used in(Zhou et al, 2008), we omit their results here.This table shows that our stratifiedbootstrapping procedure significantlyoutperforms both Bootproject and LabelPropagation methods on the ACE RDC corpus,with the increase of 5.9/4.1 units in F-score onaverage respectively.
Stratified bootstrappingconsistently outperforms Bootproject in everymajor relation type, while it outperforms LabelPropagation in three of the major relation types,especially SOC type, with the exception of ATand NEAR types.
The reasons may be follows.Although there are many AT relation instances inthe corpus, they are scattered divergently inmulti-dimension space so that they tend to berelatively difficult to be recognized via SVM.For the NEAR relation instances, they occur leastfrequently in the whole corpus, so it is very hardfor them to be identified via SVM.
By contrast,even small size of labeled instances can be fullyutilized to correctly induce the unlabeledinstances via LP algorithm due to its ability toexploit manifold structures of both labeled andunlabeled instances (Chen et al, 2006).In general, these results again suggest that thesampling strategy in selecting the initial seed setplays a critical role for relation classification, andstratified sampling can significantly improve theperformance due to proper selection of the initialseed set.6 ConclusionThis paper explores several key issues in semi-supervised learning based on bootstrapping forsemantic relation classification.
The applicationof stratified sampling originated from statisticstheory to the selection of the initial seed setcontributes most to the performanceimprovement in the bootstrapping procedure.
Inaddition, the more strata the training data isdivided into, the better performance will beachieved.
However, the augmentation of thelabeled data using the stratified strategy fails tofunction effectively largely due to theunbalanced distribution of the confidentlyclassified instances, rather than the stratifiedsampling strategy itself.
Furthermore, we alsopropose a mean entropy-based stoppage criterionin the bootstrapping procedure, which cansignificantly decrease the training time with littleloss in performance.
Finally, it also shows thatour method outperforms other state-of-the-artsemi-supervised ones.AcknowledgmentsThis research is supported by Project 60673041and 60873150 under the National NaturalScience Foundation of China, Project2006AA01Z147 under the ?863?
National High-Tech Research and Development of China,1444Project BK2008160 under the Jiangsu NaturalScience Foundation of China, and the NationalResearch Foundation for the Doctoral Programof Higher Education of China under Grant No.20060285008.
We would also like to thank theexcellent and insightful comments from the threeanonymous reviewers.ReferencesS.
Abney.
Bootstrapping.
2002.
In Proceedings of the40th Annual Meeting of the Association forComputational  Linguistics (ACL 2002).ACE 2002-2007.
The Automatic Content Extraction(ACE) Projects.
2007. http//www.ldc.upenn.edu/Projects/ACE/.E.
Agichtein and L. Gravano.
2000.
Snowball:Extracting relations from large plain-textcollections.
In Proceedings of the 5th ACMinternational Conference on Digital Libraries(ACMDL 2000).A.
Blum and T. Mitchell.
1996.
Combining labeledand unlabeled data with co-training.
In COLT:Proceedings of the workshop on ComputationalLearning Theory.
Morgan Kaufmann Publishers.S.
Brin.
1998.
Extracting patterns and relations fromthe world wide web.
In WebDB Workshop at 6thInternational Conference on Extending DatabaseTechnology (EDBT 98).C.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
http://www.csie.ntu.edu.tw/~cjlin/libsvm.M.
Collins.
2003.
Head-Driven Statistics Models forNatural Language Parsing.
Computationallinguistics, 29(4): 589-617.J.X.
Chen, D.H. Ji, and L.T.
Chew.
2006.
RelationExtraction using Label Propagation Based Semisupervised Learning.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and the 44th Annual Meeting of theAssociation of Computational Linguistics(COLING/ACL 2006), pages 129-136.
July 2006,Sydney, Australia.A.
Culotta and J. Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proceedings ofthe 42nd Annual Meeting of the Association ofComputational Linguistics (ACL 2004), pages 423-439.
21-26 July 2004, Barcelona, Spain.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.Discovering Relations among Named Entities fromLarge Corpora.
In Proceedings of the 42nd AnnualMeeting of the Association of ComputationalLinguistics (ACL 2004).
21-26 July 2004,Barcelona, Spain.N.
Kambhatla.
Combining lexical, syntactic andsemantic features with Maximum Entropy modelsfor extracting relations.
In Proceedings of the 42ndAnnual Meeting of the Association ofComputational Linguistics (ACL 2004)(posters),pages 178-181.
21-26 July 2004, Barcelona, Spain.S.
Miller, H. Fox, L. Ramshaw, and R. Weischedel.2000.
A novel use of statistical parsing to extractinformation from text.
In Proceedings of the 6thApplied Natural Language Processing Conference.29 April-4 May 2000, Seattle, USA.J.
Neyman.
1934.
On the Two Different Aspects ofthe Representative Method: The Method ofStratified Sampling and the Method of PurposiveSelection.
Journal of the Royal Statistical Society,97(4): 558-625.L.H.
Qian, G.D. Zhou, Q.M.
Zhu, and P.D Qian.
2008.Exploiting constituent dependencies for treekernel-based semantic relation extraction.
InProceedings of The 22nd International Conferenceon Computational Linguistics (COLING 2008),pages 697-704.
18-22 August 2008, Manchester,UK.D.
Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
In theProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics (ACL95), pages 189-196.
26-30 June 1995, MIT,Cambridge, Massachusetts, USA.D.
Zelenko, C. Aone, and A. Richardella.
2003.Kernel Methods for Relation Extraction.
Journal ofMachine Learning Research, (2): 1083-1106.M.
Zhang, J. Zhang, J. Su, and G.D. Zhou.
2006.
AComposite Kernel to Extract Relations betweenEntities with both Flat and Structured Features.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th AnnualMeeting of the Association of ComputationalLinguistics (COLING/ACL 2006), pages 825-832.Sydney, Australia.M.
Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L.Tan.
2005.
Discovering Relations between NamedEntities from a Large Raw Corpus Using TreeSimilarity-Based Clustering.
In Proceedings of the2nd international Joint Conference on NaturalLanguage Processing (IJCNLP-2005), pages 378-389.
Jeju Island, Korea.Z.
Zhang.
2004.
Weakly-supervised relationclassification for Information Extraction.
InProceedings of ACM 13th conference onInformation and Knowledge Management (CIKM2004).
8-13 Nov 2004, Washington D.C., USA.G.D.
Zhou, J. Su, J. Zhang, and M. Zhang.
2005.Exploring various knowledge in relation extraction.In Proceedings of the 43rd Annual Meeting of theAssociation of Computational Linguistics (ACL2005), pages 427-434.
Ann Arbor, USA.G.D.
Zhou, J.H.
Li, L.H.
Qian, and Q.M.
Zhu.
2008.Semi-Supervised Learning for Relation Extraction.In Proceedings of the 3rd International JointConference on Natural Language Processing(IJCNLP-2008), page 32-38.
7-12 January 2008,Hyderabad, India.1445
