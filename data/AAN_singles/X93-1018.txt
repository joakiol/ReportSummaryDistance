COMPARING HUMAN AND MACHINE PERFORMANCE FORNATURAL LANGUAGE INFORMATION EXTRACTION:Results from the Tipster Text EvaluationCraig A. WillInstitute for Defense AnalysesComputer and Software Engineering Division1801 N. Beauregard StreetAlexandria, VA 22311ABSTRACTThis paper presents results from a study comparing human perfor-mance on the text of natural language information extraction withthat of machine xtraction systems that were developed as part ofthe ARPA Tipster program.
Information extraction is shown to bea difficult ask for both humans and machines.
Evidence for one setof text material, English Microelectronics, indicated that a humananalyst produces about half the errors as does machine systems.INTRODUCTIONIn evaluating the state of technology for extracting informa-tion from natural language text by machine, it is valuable tocompare the performance of machine xtraction systemswith that achieved by humans performing the same task.The purpose of this paper is to present some results from acomparative study of human and machine performance forone of the information extraction tasks used in the Tipster/MUC-5 evaluation that can help assess the maturity andapplicability of the technology.The Tipster program, through the Institute for DefenseAnalyses (IDA) and several collaborating U.S. governmentagencies, produced a corpus of filled "templates" --StlUC-tured information extracted from text.
This corpus was usedboth in the development of machine xtraction systems bycontractors and in the evaluation of the developed systems.Production of templates was performed by human analystsextracting the data from the text and structuring it, using aset of structuring rules for "filling" the templates and com-puter software that made it easier for analysts to organizeinformation.
Because of this rather extensive effort by ana-lysts to create these templates, it was possible to study theperformance of humans for this task in some detail and todevelop methods for comparing this performance with thatof machines participating in the Tipster/MUC-5 evaluation.The texts that the templates were filled from were newspa-per and technical magazine articles concerned either withjoint business ventures or microelectronics fabrication tech-nology.
Each topic domain used text in two languages,English and Japanese.
This paper discusses preparation oftemplates and presents detailed results for human andmachine performance; a shorter paper \[1\] discusses prepa-ration of templates and basic results.The primary motivation for this study was to provide reli-able data that would allow machine xtraction performanceto be compared with that of humans.
The MUC and Tipsterprograms have included extensive fforts to develop mea-surements hat can objectively evaluate the performance ofthe different machine systems.
However, although thesemeasures are capable of reliably discriminating between theperformance of different machine systems, they are not veryuseful by themselves in evaluating how near the technologyis to providing reliable performance and the extent to whichit is ready to be used in applications.
Sundheim \[2\] initiatedhuman performance study for extraction by providing esti-mates of human performance for the task used in the MUC-4 evaluation; the present study provides human data for theTipster/MUC-5 evaluation that was produced under rela-tively controlled conditions and with methods and statisticalmeasures that assess the reliability of the data.A second motivation for the study was for its value in help-ing produce better quality templates so as to allow high-quality system development and reliable evaluation.
Thequality and consistency of the templates being producedwere monitored as analysts were trained and gained experi-ence, and particular efforts were made to identify the causesof errors and inconsistency soas to develop strategies forreducing error and increasing consistency.A third motivation for studying human performance was tobetter understand the nature of the extraction task and therelative performance of humans compared with machineson different aspects of the task.
Such an understanding canparticularly help in the construction of human-machineintegrated systems that are designed to make the best use of179what are at the present ime rather different abilities ofhumans and machines \[3\].This paper is organized as follows:The paper begins with a discussion of how the templateswere prepared, with particular emphasis on the strategiesthat were used that served to minimize rrors and maximizeconsistency, including detailed fill rules, having more thanone analyst code a given template, and the use of softwaretools with error detection capabilities.The paper next describes the results of an investigation i tothe extent to which template codings made by analysts thatare playing different roles in the production of a particulartemplate influence the resulting key, which provides clues tothe effectiveness of the quality control strategies used in thetemplate preparation process.The results of an experimental test of different methods ofscoring human performance are then presented, with thegoal of selecting a method that is statistically reliable, mini-mizes bias, and has other desirable characteristics.
Data thatindicates overall evels of human performance on the task,variability among analysts, and reliability of the data arethen presented.The results of an investigation i to the development of ana-lyst skill are then presented, with the significant questionbeing the need to understand whether the performance levelsbeing measured truly reflect analysts who have a high levelof skill.The performance of humans for information extraction isthen compared with that of machine systems, in terms ofboth errors and metrics that attempt to separate out two dif-ferent aspects of performance, recall and precision,The results of a study comparing the effect of key prepara-tion on the evaluation of machine performance are then pre-sented.
This is particularly relevant to the question of howkeys should be future MUC and Tipster evaluations.A study is then presented of the extent o which machinesand humans agree on the relative difficulty of particular tem-plates.The results of a pilot study in which the performance ofhumans and machines is compared for particular kinds ofinformation, to see what information machines are compar-atively worse or better than humans in extracting, is thenpresented.A final section of the paper makes ome general conclusionsabout the results and their implications for assessing thematurity and applicability of extraction technology.THE PREPARATION OF  TEMPLATESThe development of templates for the English Microelec-tronics corpus began in the fall of 1992.
It began with aninteragency committee that developed the basic templatestructure and, when ~t had evolved to the point that ~t was rel-atively stable, two experienced analysts were added to theproject so that they could begin training toward eventualproduction work.
About two months after that, two moreproduction analysts joined the project.The template structure was object-oriented, with a templateconsisting of a number of objects, or template buildingblocks with related information.
Each object consisted ofslots, which could be either fields containing specific infor-mation or pointers, that is, references, toother objects.
Slotfields can either be set fills, which are filled by one or morecategorical choices defined for that field, or by string fills, inwhich text is copied verbatim from the original article.
Incases of ambiguity, analysts could provide alternative fillsfor a given slot.
In addition, analysts could include com-ments when desired to note unusual problems or explain whya particular coding was selected.
Comments were not scoredand were primarily used when analysts compared two ormore codings of a given article to determine which was themore correct.
For more information on the template designsee \[4\].
Also see \[5\] for a discussion of selection of the arti-cles in the corpora nd preparation of the data, and \[6\] for adiscussion of the different extraction tasks, domains, andlanguages.Previous experience with the English and Japanese JointVentures corpus had made it clear that producing templateswith a high degree of quality and consistency is a difficultand time-consuming task, and we attempted tomake the bestuse of what had been learned in that effort in producing tem-plates for English Microelectronics with quality and consis-tency appropriate to both the needs of the project and theresources we had available.
"Quality" refers to minimizing the level of actual error byeach analyst.
"Error" includes the following: (1) Analystsmissing information contMned in or erroneously interpretingthe meaning of an article; (2) Analysts forgetting or misap-plying a fill rule; (3) Analysts misspelling a word or makinga keyboarding (typographical) error or the analogous errorwith a mouse; and (4) Analysts making an error in construct-ing the object-oriented structure, such as failing to create an180object, failing to reference an object, providing an incorrectreference to an object, or creating an extraneous object.
"Consistency" refers to minimizing the level of legitimateanalytical differences among different analysts.
"Legitimateanalytical differences" include the following: (1) Differentinterpretations of ambiguous language in an article; (2) Dif-ferences in the extent to which analysts were able or willingto infer information from the article that is not directly stat-ed; and (3) Different interpretations of a fill rule and how itshould be applied (or the ability or willingness to infer a ruleif no rule obviously applies).To improve quality and consistency, three steps were taken:Development of Fill RulesFirst, a set of relatively detailed rules for extracting informa-tion from articles and structuring it as an object-orientedtemplate was developed (with the rules for English Micro-electronics a 40-page, single-spaced docamen0.
These ruleswere created by a group of analysts who met periodically todiscuss problems and to agree on how to handle particularcases via a general rule.
One person (who was not one of theproduction analysts) served as the primary person maintain-ing the rules.
Because of the highly technical nature of thetopic domain, an expert in microelectronics fabrication alsoattended the meetings and resolved many problems thatrequired technical knowledge.Coding by Multiple AnalystsThe second step was the development of a procedure inwhich two analysts participated in coding nearly half of thearticles, and the reconciliation of different codings to pro-duce final versions.
For 300 articles in a "high quality"development set and for the 300 articles in the test set, thefollowing procedure was followed: Two analysts first inde-pendently coded each article, with the resulting codings pro-vided to one of these same analysts, who produced a finalversion, or "key".
The remaining 700 development tem-plates were coded by only one analyst, with each of fouranalysts coding some portion of the 700 articles.
The pur-pose of the two-analyst procedure was to correct inadvertenterrors in the initial coding and to promote consistency, byallowing the final analyst to change his or her coding afterseeing an independent coding by a different analyst.
Theprocedure also promoted consistency inthe long run by pro-viding analysts with examples of codings made by otheranalysts so that hey could see how other analysts handled agiven problem.
It also helped improve the fill rules by allow-ing analysts to detect recurring problems that could be dis-cussed at a meeting and lead to a change in the fill rules.Software Support ToolsThe third step was the development of software tools thathelped analysts to minimize rrors, detect certain kinds oferrors, and support the process of comparing initial codings.One such tool was the template-filling tool developed by BillOgden and Jim Cowie at New Mexico State University(known as Locke in the version designed for English Micro-electronics).
This tool, which runs on a Sun workstation anduses the Xwindows graphical user interface, provided aninterface that allowed analysts to easily visualize the rela-tionships among objects and thus avoid errors in linkingobjects together.
The tool also allowed analysts to copy textfrom the original article by selecting it with a mouse, enter-ing it verbatim into a template slot, thus eliminating key-stroke errors.
In addition, the Locke tool has checkingfacilities that allowed analysts to detect such problems asunreferenced or missing objects.
A second tool was the Tip-ster scoring program (developed by Nancy Chinchor andGary Dunca at SAIC \[8\]) which provided analysts makingkeys with a printout of possible errors and differencesbetween the initial codings.
Another program, written byGerry Reno at the Department ofDefense at Fort Meade, didfinal checking of the syntax of completed keys.The four analysts who coded templates all had substantialexperience as analysts for U.S. government agencies.
In allcases analysts making the keys were unaware of the identityof the analyst producing a particular coding.
Analysts didoften claim that they could often identify the analyst codinga particular article by the comments included in the templatecoding or the number of alternative fills added, althoughwhen this was investigated further it appeared that hey werenot necessarily correct in their identification.In addition to the templates and keys created for the devel-opment and test sets described above, a small number ofcodings and keys were made for the purpose of studyinghuman performance on the extraction task.
In February,1993, at about he time of the 18-month Tipster evaluation,a set of 40 templates in the development set were coded byall analysts for this purpose.
Similarly, for 120 templates ofthe 300-template st set that was coded in June and July,1993 extra codings were made by the two analysts thatwould have not normally participated in coding those arti-cles, resulting in codings by all 4 analysts for 120 articles.181INFLUENCE OF  ANALYSTS PLAY INGD IFFERENT ROLES ON KEYThe two-analyst procedure for making keys used for EnglishMicroelectronics was intended as an efficient compromisebetween the extremes of using a single analyst and the pro-cedure that had been used for English Joint Ventures inwhich two analysts would independently make codings andprovide the codings to a third analyst who would make akey.It is of interest o know whether this form of checking isachieving its intended result--that of improving the qualityand consistency of templates.
We can investigate this indi-rectly by measuring the level of influence analysts playingdifferent roles (and producing different codings) have on thekey that is produced.
The question of influence is also ofinterest--as will be seen in the next section--for its impli-cations in understanding the characteristics of differentmethods for measuring human performance.To investigate this influence, data from the set of 120 tem-plates that all analysts coded were analyzed separately basedon the role in the production of the key played by the partic-ular coding.
Figure 1 shows the relationship between differ-ent codings and the analysts producing them.
Analyst 1produces, based on the original article, what will be calledhere the primary coding of the article.
Analyst 2 producesindependently the secondary coding of the article.
The pri-mary and secondary codings are then provided to Analyst 1,who produces the key.
Analysts 3 and 4 also produce othercodings of the article that have no effect on the key.
Eachanalyst plays a particular role (Analyst 1, 2, 3, or 4) for 30 ofthe 120 templates.Note that when Analyst 1 uses the primary and secondarycodings in making the final coding, or key, there is a naturalbias toward the primary coding.
This is primarily becauseAnalyst 1 created that coding, but also because the analysttypically does not create the key from scratch with the Locketool, but modifies the primary coding (probably reinforcingthe tendency to use the primary coding unless there is a sig-nificant reason for changing it).Figure 2 shows the results of this analysis, with performanceexpressed in terms of error per response fill, as calculatedwith the methodology described by Nancy Chinchor andBeth Sundheim \[7\] and implemented by the SAIC scoringprogram \[8\].
All objects in the template were scored, andscoring was done in "key to key" mode, meaning that bothcodings were allowed to contain alternatives for each slot.ANALYST i SECONDARY2 CODING, 1-1 ANALYST CODING 3ooo.-IFigure 1: Procedure for Coding Templates and MakingKeys for the 120 Articles Coded by All Analysts(See the Appendix for details of producing the error scoresand calculation of statistical parameters and tests.
)40=O3o,20 15.5~" lOPRIMARYCODINGVSKEY(N=120)27. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.iiiiiiiiiiiiii!~i~iiiiiiiiiiiiiii~l iiiiiiiii~iii~iiiiiiiiiiiii~i~i~iii .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ililililililililililililililililil .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.= .
.
.
.
.
: - : - : - : - : - : - : - : - : - : - : - : - : - : - : - : - : .SECONDARYCODINGVSKEY(N=120)37.4OTHERCODINGVSKEY(N=240)Figure 2: Error for Coding Compared with Key Dependingupon Role of Coding in Making KeyThe data is shown for three conditions, with each conditionreflecting the accuracy, in terms of percent error, of a codingplaying a particular ole (or no role) in producing akey.
Theconditions are: (1) error for the primary coding when mea-sured against he key (shown as an unfilled vertical bar); (2)error for the secondary coding when measured against hekey (shown as a light gray vertical bar); and (3) error for oth-er codings when measured against the key (shown as a darkgray vertical bar).Also shown for all conditions in the form of error bars is thestandard error of the mean.
Because the mean shown is cal-182culated from a small sample it can be different from thedesired true population mean, with the sample mean only themost likely value of the population mean.
The standard errorbars show the range expected for the true population mean.That mean can be expected to be within the error bars shown68% of the time.
See the appendix for details about how thestandard error of the mean was calculated.Figure 3 makes clear the role of different codings: the prima-ry coding is made by the analyst who also later made the key,while the secondary and other codings were made by otheranalysts.
The primary and secondary codings are both usedin making the key, while the other codings are not used.PRIMARY SECONDARY OTHERMADEBYUSEDINAnalyst whoAlso LaterMade KeyUsed inMakingKeyOtherAnalystUsed inMakingKeyOtherAnalystNotUsed inMakingKeyFigure 3: Characteristics of Different Coding RolesThe result (in Figure 2) that he primary coding when com-pared to the key shows a mean error considerably above zeroindicates that analysts quite substantially change their cod-ing from their initial version in producing the key.
Presum-ably, this results from the analyst finding errors or more-desirable ways of coding, and means that quality and consis-tency is improved in the final version.
(All differencesclaimed here are statistically significant--see Appendix fordetails).The result hat he secondary coding when compared to thekey shows a mean error that is substantially above that of theprimary coding condition indicates that he analyst's original(primary) coding does in fact influence the key more strong-ly than does the secondary coding (produced by another ana-lyst).
At the same time, it is clear that the secondary codingdoes itself substantially influence the key, since the meanerror for the secondary coding is substantially less than thatfor the ;'other" codings, which are not provided to the analystmaking the key and thus have no influence on it.
This pro-vides good evidence that analysts are indeed making use ofthe information in the secondary coding to a substantialextent.
This probably resulted in an improvement in bothquality and consistency of the templates above what wouldbe the case if only a single coder (even with repeated check-ing) was used, although we do not have direct evidence ofsuch improvement and the extent of its magnitude is notclear.METHODS FOR SCORING HUMANPERFORMANCEBefore human performance for information extraction canbe effectively compared with machine performance, it isnecessary to develop a method for scoring responses byhuman analysts.The problem of measuring machine performance has beensolved in the case of the MUC-5 and Tipster evaluations byproviding (1) high-quality answer keys produced in the man-ner described in the previous ection; and (2) a scoring meth-odology and associated computer program.The primary additional problem posed when attempting tomeasure the performance of humans performing extractionO144I-- ZtLIotU50403020 m10 ?0WHO MADE KEY;ALL OTHER |NDEP 5THANALYSTS ANALYSTS ANALYSTS ANALYSTSN D m DN=120 N=90 N=30 N=60TO 803932 31 32 32ANALYST ANALYST ANALYST ANALYSTA B C DFigure 4: Comparison of Methods for Sconng Human Performance183is that of "who decides what he correct answer is?"
In thecase of the English Microeleclronics analysts, the four ana-lysts whose performance we are attempting to measurebecame---once they had substantial Ixaining and practice--the primary source of expertise about he task, with theirknowledge and skill often outstripping that of others whowere supervising and advising them.
This made it especiallydifficult o measure the performance ofparticular analysts.We approached the problem of determining the best methodfor scoring humans empirically: We compared experimen-tally four different methods for sconng codings by humananalysts.
In general, the criteria is objectivity, statistical reli-ability, and a perhaps difficult-to-define "fairness" or plausi-bility of making appropriate comparisons, both betweendifferent human analysts and between humans andmachines.In evaluating different scoring methods, the 120 templates inthe Tipster/MUC-5 test set that had been coded by all fouranalysts were used.
As was described in the previous ec-tion, keys for each template in this set were made by one ofthe analysts, using as inputs codings done independently bthe analyst making the key and one other analyst.
Each ofthe 4 analysts made keys for 30 of the 120 templates and alsoserved as the independent analyst providing a coding to theanalyst making the keys for a different set of 30 templates.In addition, for 60 of the 120 templates, a fifth analyst madea second key from codings made by the four analysts.Figure 4 shows data comparing the four sconng methods foreach of the four analysts.
The data is shown in terms of per-cent error, with all objects scored, and with "key toresponse" scoring being used.
In key to response scoring,alternative fills for slots are allowed only in the key, but notin the coding being scored.
Since in the data collected here,analysts did normally add alternative fills (since their goalwas to make keys), these alternatives were removed beforescoring, with the first alternative listed assumed to be themost likely one and thus kept, and others deleted.
The pur-pose of using key-to-response scoring was so that he result-ing data could be directly compared with data from machinesystems, which produced only one fill for each slot.
Scoringwas done in batch mode, meaning that human analysts werenot used (as they are in interactive mode) to judge cases inwhich strings did not completely match.In the "All Analysts" condition, all 120 templates made byeach analyst were scored, using as keys those made by all 4analysts (including the analyst being scored).
In the "OtherAnalysts" condition, only templates that have keys made byanalysts other than the analyst being scored were used insconng each analyst (with each analyst having 90 templatesof the 120 templates coded by that analyst scored).
In the"Independent Analysts" condition, only templates for whichthe analyst being scored neither made the key nor produceda coding that was used as an input for making the key wereused in scoring each analyst.
(This resulted in from 30 to 80templates being scored for each analyst, depending upon theanalyst.)
In the "5th Analyst" condition, a5th analyst madethe answer keys (with 60 templates scored in this condition).This 5th analyst did not code production templates but wasin charge of maintaining the fill rules and the overall man-agement of the English Microelectronics template codingeffort.The "All Analysts" condition showed the most consistentperformance across analysts, with a variance calculated fromthe means for each analyst of 1.82 (N=4).
The "Other Ana-lysts" condition was nearly as consistent, with a variance of3.16.
The "Independent Analysts" and "5th Analyst" condi-tions were much less consistent, with variances of 9.08 and30.19, respectively.
The high variance of the "IndependentAnalysts" condition, however, esulted only from the perfor-mance of analyst D, who had a very small sample size, only30 templates.
If analyst D is left out, the variance becomesonly 0.32 for this condition.
The high variability across ana-lysts for the 5th analyst could be a result either of the smallsample size or, more likely, a tendency for the 5th analyst tocode articles in a manner more similar to some analysts(especially analyst C) than others (especially analyst B).The subjective opinions of the analysts and their co-workerssuggested that all English Microeleclxonics analysts herewere generally at the same level of skill, which is consistentwith the above data.
(This was not true of the English JointVenture analysts, for example, where both the data and theopinions of analysts and others uggested considerable vari-ability of skill among analysts.)
However, it should be notedthat all of the conditions in which analysts are being scoredby other analysts run the risk of making the differencesamong analysts artificially low.
Consider, for example, thecase of a very skilled analyst being scored against a keymade by an analyst who is poorly skilled.
The more skilledanalyst islikely to have some correct responses scored incor-reectly, while a less-skilled analyst may have his or her incor-rect responses scored as correct.
However, the patterns ofanalyst skill elicited by the different scoring methods do notshow any reliable evidence of such differences in skill, andit appears that analysts have similar levels of skill and thatany effect of a"regression toward the mean" of mean analystscores is minimal.Figure 5 shows the same data comparing scoring methodsthat was shown in the previous figure, but in this figure datahas been combined to show means for all analysts in each ofthe scoring conditions.
This combining allows the overalldifferences among the different scoring methods to be seen184more clearly.
In addition, combining the data in this wayincreases reliability of the overall mean.403020~10 ?CWHO MADE KEY;ALL OTHER INDEP 5THANALYSTS ANALYSTS ANALYSTS ANALYSTSN D m ElN=480 N=360 N=240 N=24033.229.8 mR \[ 28.3MEAN ALL ANALYSTSFigure 5: Comparison of Scoring Methods Using Mean ofAll AnalystsFigure 6 shows a summary of the characteristics of differentscoring methods as discussed above.
The "All Analysts","Other Analysts", and "Independent Analysts" methods alluse the expertise of the most practiced (production) analysts.method appeared to have some bias, presumably because ofa coding style more similar to some analysts than others.Finally, the "All Analysts", Other Analysts", and "Indepen-dent Analysts" methods had relatively high statistical reli-ability, while the "5th analyst" method had much lessreliability.Figure 7 shows a Recall-Precision scatterplot for the fouranalysts and for each of the four conditions shown in Figure4.
Analysts cored by the "All Analysts" method are shownas solid circles, while analysts cored by the "Other Ana-lysts" method are shown as solid triangles.
Analysts coredby the "Independent Analysts" method are shown as deltas,and analysts cored by the "5th Analyst" method are shownas solid squares.
Note that only the upper fight-hand quad-rant of the usual 0-100% recall-precision graph is shown.Performance is expressed in terms of recall and precision,which are measures borrowed from information retrievalthat allow assessment of two independent aspects of perfor-mance.
Recall is a measure of the extent to which all relevantinformation i an article has been extracted, while precisionis a measure of the extent to which information that has beenentered into a template is correct.
Details of the method forcalculating recall-precision scores for the Tipster/MUC-5evaluation can be found in the paper by Chinchor and Sund-heim \[7\].SCORINGMETHODEXPEITnSEOFANALYSTMA~NGKEYALL ANALYSTS HIGHOTHER ANALYSTS HIGHINDEP.
ANALYST$5TH ANALYSTHIGHMODERATETO ~GHmASDUETOSELF-SCO~NGSUBSTANTIALSOMENONENONE:ERTAIN I STATISTICALNLALYSTS | RELIABILITYNONE HIGHNONE HIGHNONE HIGHPROBABLY HIGHSOMEFigure 6: Characteristics of Scoring MethodsTo make the key, while the "5th analyst" method uses ananalyst he expertise of which is somewhat more question-able because of putting much less time into actual codingarticles.
The "All Analysts" method appeared to have sub-stantial bias (producing artificially low error scores) fromanalysts coring their own codings, while the "Other Ana-lysts" method appeared to produce some (but less) such bias.Neither the "Independent Analysts" nor the "5th Analyst"method suffered from this kind of bias.
The "All Analysts","Other Analysts", and "Independent Analysts" methods areunbiased with respect o particular analysts (because ofcounterbalancing to control for this), but the "5th Analyst"10090z 80OuJa.
7060i - IALL ANALYSTS5TH ANALYST B~)~X , "1E VOTHER ANALYSTSINDEP.
ANALYSTS50 * , l * I * I i50 60 70 80 90RECALLFigure 7: Recall Versus Precision Scores for HumanAnalysts Scored by Different Methods10(185THE DEVELOPMENT OF ANALYSTSK ILLwere used in determining the differences between templatecodings.In interpreting the levels of performance shown by analystsfor the extraction task, and, particularly, when comparinghuman performance with that of machines, it is important toknow how skilled the analysts are compared to how theymight be with additional training and practice.
Comparingmachine performance with humans who are less than fullyskilled would result in overstating the comparative perfor-mance of the machines.Four analysts were used in production template coding, allhaving had experience as professional nalysts.
One analysthad about 6 years of such experience, another 9 years ofexperience, a third 10 years of experience, and the fourthabout 30 years of experience.
All were native speakers ofEnglish.
None of the analysts had any expertise in micro-electronics fabrication.We compared the skill of analysts at two different stages intheir development by analyzing two sets of templates, eachcoded at a different time.
The first, or "18 month" set, wascoded in early February, 1993, at about he same time as the18 month Tipster machine evaluation, after analysts hadbeen doing production coding for about 3 months.
The sec-ond, or "24 month" set was coded in June and July, 1993,somewhat before the 24 month Tipster/MUC-5 machineevaluation, and toward the end of the template coding pro-cess, when fill rules were at their most developed stage andanalysts at their highest level of skill.
There was some dif-ference in expected skill between the two pairs of analysts,since one pair (analysts A and B) had begun work in Sep-tember, although their initial work primarily involved cod-ing of templates on paper and attending meetings to discussthe template design and fill rules, and during this period theydid not code large numbers of templates.
The second pairbegan work in November, and did not begin production cod-ing of templates until a few weeks after the first analysts.Data for the 18 month condition was produced by first hav-ing all analysts code all articles of a 40-article set in thedevelopment set.
Each analyst was then provided with print-outs of all 4 codings for a set of 10 articles, and asked tomake keys for those articles.
Data for the 24 month condi-tion was produced as described previously for the "All Ana-lysts" condition, using the 120 templates that all analystscoded in the test set, with each of the 4 analysts making keysfor 30 of the 120 templates.
Note that analysts making thekeys in the 18 month condition used as inputs the codings ofall 4 analysts, while analysts making the keys in the 24month condition used as inputs the codings of only 2 ana-lysts.
In both conditions and for all analysts, "key to key"scoring was used, in which all alternatives in both codingsFigure 8 shows data, in terms of percent error, for each of thetwo pairs of analysts in both the 18 month and 24 month con-ditions.
The pairing of analysts i based on when they startedwork, with analysts A and B ("Early Starting Analysts")beginning work on the project before analysts C and D("Later Starting Analysts").
Note that analysts who startedearly appeared to make slightly fewer errors in the 18 monthcondition (27%) than in the 24 month condition (28.3%),although the difference is not statistically significant.403 0  , ,0m 20I - , ,zU, Io1018 MO 24 MO NmN:80 N=24028.3: : : : : : : : : : : : : : : : :5 ::5 : : : : : : : : : : : : : : : : :;i;iiiiiii !
iiiii.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.- - .
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.- : - : - : - : - : - : - : - : - : - :EARLY STARTINGANALYSTS(A& B)Figure 8: Performance ofAnalystsor LaterALL ANALYSTS MADE KEYS36.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.iiiiiiiiii~i!i!!iiiiiiiiiiiiiii!!
!iiiiiiiiiiiiii 30,5. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.- : - : - : - : - : - : - : - : - : - : - : - :.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- .
.. .
.
.
.
.
.
.
.
.
.
.
-.
.
.
.
.
.
.
.w .
- , .
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- .
.
.
.
.- .
- .
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.- .
.
.
.
.
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
: : : : : : : : : : : : : : : : : : : : : : : :- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.-.-.-.-.-.-.-.-.- - - -: : : : : : : : : : : : : : : : : : : : : : : :.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
:  .
.
.
.
.
.
.
.
.
-:-:-:-:-:-:-:-:-:-:-:-:- .
.
.
.
.
.
.
.
.
.
.
.
.
.
!i!i!i!iiiiiiiii~i~i~i~i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.LATER STARTINGANALYSTS(C & D)Who Stated EarlyThis difference can be explained at least in part by the differ-ence in the method of making the keys.
In the 18 month con-dition, all 4 analyst codings influenced the key, while in the24 month condition only 2 of the analyst codings influencedthe key.
This results in the 18 month condition producingscores that are artificially low in terms of errors, comparedto the 24 month condition.
The difference, based on the datain Figure 5, can be estimated at from about 4 to 8 percentagepoints.
Thus, it appears that analysts who started early didnot improve their skill, or improved it minimally, betweenthe 18 and 24 month tests.
However, analysts who started lat-er did appear to learn substantially, with error scores of 36%in the 18 month condition and 30.5% in the 24 month condi-tion, with the amount of learning for the analysts who startedlater probably somewhat more than shown because of thedifference in the method of making keys.
Because of the dif-ferences in scoring methods between the conditions and thesmall sample size in the 28 month condition, the aboveresults are only suggestive.
An alternative approach toassessing the development ofskill of analysts (that does notrequire keys for scoring) compares the pattern of disagree-186.iCD,o '13  I "1 "'1i38 46 C 35.5 40 \[ +2.5 +6c II I42 42 42 I D 41 34 38 D +1 +8 +8\[A B C A B C A B C18 MONTH 24 MONTH DIFFERENCEN= 40 N= 120 18 MO- 24 MOFigure 9: Disagreement Matrices for 18 Month and 24Month Tipster Tests and Differences Between theTwo Matricesment among analysts for the 18-month and 24-month tests,and is more convincing.
Such a pattern can be constructedby running the scoring program for a given set of templatesin "key to key" mode (so that it calculates a measure of dis-agreement between two codings) for all pairs of codings forthe four analysts.Figure 9 shows such patterns, termed here "DisagreementMatrices", for the 18- and 24-month tests, along with a third"Difference" matrix (shown at the far right) created by sub-tracting scores in the 24-month matrix from those in the 18-month one, resulting in a measure of the extent o whichconsistency between particular analysts has improved.
Notethat all cells of the Difference matrix have positive scores,indicating that consistency between all pairs of analysts hasincreased.Figure 10 shows comparisons of the scores from the Differ-ence matrix for three specific ases of analyst pairs.
For theEarly Starting Analysts pair (A and B), shown at the far left,consistency between the two analysts increased by only onepercentage point, suggesting that even at the 18-month test,these analysts were already near their maximum level ofskill.
For the Later Starting Analysts (C and D), however,shown at the far fight, consistency between the two analystsincreased by 6 percentage points, indicating that these ana-lysts were still developing their skill.
For the case where themean of all pairs of early-late analysts (AC, AD, BC, andBD) is calculated, shown as the middle vertical bar, consis-tency increased by an average of 4.375 percentage points,indicating that the less-skilled analysts had increased theirconsistency with the more-skilled analysts.The general finding here is that (1) the analysts who startedearlier improved their skill minimally from the 18 to 24month tests; and (2) analysts who started later improvedtheir skill considerably.
Because by the time of the 24 monthtest the later starting analysts had as much or more practicecoding templates as did the early starting analysts at the timeof the 18 month test, it is reasonable to assume that theirO CCCCUJt -  ZUJotUa .6- -1  +6S m4- -3 - -2 - -1 - -+1mBETWEENEARLY STARTINGANALYSTS(AB)*4.4BETWEENEARLY AND LATERSTARTINGANALYSTS(AC AD BC BD)BETWEENEARLY STARTINGANALYSTS(CD)Figure 10: Improvement in Consistency From18-Month to 24-Month Testsincrease in skill reflects an early part of the learning curveand that by the 24 month test all analysts have started toreach an asymptotic level of skill.The evidence in the literature for the development of skill inhumans uggests that skill continues to develop, if slowly,for years or decades even on simple tasks, and it can beexpected that continued practice on information extractionby these analysts would increase their level of skill.
Howev-er, it does appear that the analysts were very highly skilledby the end of the study and were at an appropriate l vel ofskill for comparison with machine performance.COMPARISON OF  HUMAN ANDMACHINE PERFORMANCEThe most critical question in the Tipster/MUC-5 evaluationis that of how performance of the machine xtraction sys-tems compares with that of humans performing the sametask.Figure 11 shows mean performance, in percent error, for the4 human analysts, using the "Independent Analysts" condi-tion discussed in a previous ection and shown in Figure 5,for the 120 articles coded by all analysts from the EnglishMicroelectronics test set.
Also shown is the correspondingmachine performance for the same articles for the threemachine systems in the Tipster/MUC-5 evaluation that hadthe best official scores for English Microeleclronics.The differences are very clear, with the mean error forhuman analysts about half that of the machine scores.
Both187or)i .uOoU)OILl7(| "60 ?50  ?40  -FOR HUMANS:FOR MACHINES:33.2MEANHUMAN ANALYSTSINDEPENDENT ANALYSTS MADE KEYSALL ANALYSTS MADE KEYS6862 63X Y Z3 BEST MACHINESFigure 11: Comparison of Human and MachinePerformancethe human and machine scores are highly reliable, as isshown by the standard error bars.Figure 12 shows essentially the same data expressed interms of recall and precision.10080z 609.4O20.UMANANALYSTS ?
IDD3 BEST MACHINES0 , I * l i I .
I ,0 20 40 60 80 100RECALLFigure 12: Comparison of Human and MachineUsing Recall and Precision ScoresWhat is surprising about his data is not that the machineshave a seemingly rather high error rate, but that the humanrate is so high.
The recall-precision diagram suggests thatmachines can have even more similar performance tohumans on either ecall or precision, if one is willing to tradeof the other to achieve it.
Machine performance is likely tobe at least somewhat better than this in a real system, sinceresource constraints forced developers to run incompletesystems (that, for example, did not fill in slots for whichinformation was infrequently encountered).The performance data shown in the figure, other data, andthe subjective accounts of individual analysts and their co-workers upport the general conclusion that for this group ofanalysts the level of skill for information extraction was verysimilar for each analyst.
This uses the "Other Analysts" scor-ing method, with recall and precision scores for individualanalysts not particularly meaningful for the otherwise morereliable "Independent Analysts" condition.
(See Figure 7 forrecall and precision scores for all scoring conditions).EFFECT OF  METHOD OF KEYPREPARATION ON MACHINEPERFORMANCEt' A practical consideration i  evaluating machind perfor- ?mance of importance for future evaluations ( uch as MUC-6) is the extent o which it is necessary or desirable to useelaborate checking schemes to prepare test templates, or188whether templates prepared by a single analyst will serve aswell.In an attempt to provide some data relevant to this issue, theperformance of the three best machine systems was mea-sured using two different sets of keys.
In one condition("Normal key") the keys used for evaluating the machineswere those normally used in the 24 month evaluation, for the120-article set for which templates were coded by all ana-lysts and a checked version produced by a particular analystusing codings of multiple analysts.
In the other condition("Orig.
Coding"), the keys used for evaluating the machineswere the original unchecked templates coded by all 4 ana-lysts.Figure 13 shows the resulting data for both conditions foreach of the three machines.
For all machines, there is littledifference (and none that is signitican0 between perfor-mance between the two conditions.NORMAL ORIG.
CODINGKEY USED USED AS KEY70 70.87O6050ODEm 40UJk-Z 30 UJ(..,1 g?UJ 2010VISHNU SHIVA BRAHMAMACHINE SYSTEMFigure 13: Performance of3 Best Machines MeasuredIndividually with Key or Original CodingFigure 14 shows the same data, but combined for all threemachines.
Again, there is no significance difference, andbecause of the large sample size and resulting smallstandard error, the result is highly reliable.
This finding mayseem surprising given the results presented earlier thatshow substantial differences between original and(checked) final codings.
The difference can be explained bythe relative precision involved.
Comparisons betweenoriginal and final codings by analysts might be seen asanalogous to different shades of colors: if an originalDEODEUJI -ZILloDE uJo.70 - m60"50-4O3O20100NORMALKEY USED66\ \ \ \ \ \ \ \ \x \ \ \ \ \ \ \ \\ \ x \x \x \x\ \ \ \ \ \ \ \ \~ \ \ \ \xxx \1N=320)ORIG.
CODINGUSED AS KEY67(N=1287)Figure 14: Performance of3 Best Machines MeasuredTogether with Key or Original Codinganalyst codes light green, while a second analyst produces achecked version of dark green, a measure of differencesmay show a substantial magnitude.
At the same time, themachines may be producing codings ranging from blue toorange.
While comparing light green with orange may yieldconsiderable differences, it is plausible that there may belittle or no difference between the magnitudes resultingwhen orange is compared first with light green and thenwith dark green.
It can be expected that as machineperformance improves, there will be an increasingdifference between evaluations using original and checkedkeys.AGREEMENT ON DIFFICULTY OFPARTICULAR TEMPLATESThe extent o which different analysts (and machines) agreeon which templates are difficult and which are easy is ofinterest in understanding the task and human and machineperformance for the task.This was measured first by obtaining scores for differentanalysts for particular templates, and calculating Pearsonproduct-moment correlation coefficients for correspondingtemplates between pairs of analysts.Figure 15 shows these correlations, with correlationsbetween the 4 analysts hown at the far left, correlationsbetween the 3 best machines shown correlation between ran-189domly in the center, and selected pairs of humans andmachines shown at the far fight.Correlations among humans were relatively low, with R 2from .04 to .20 (median at 0.13).
Correlations amongmachines were moderate, with R z from .21 to .44.Correlations between a particular human and a particularmachine were low to moderate, with R 2 from .07 to .21.BC .15 .11 VISHNU I .44  I- - ID .15 .09 .20 I BRAHMAA B C SHIVA VISHNUHUMANS MACHINESDATA SHOWN AS R 2D .16 .15A .21 .07~SHNU SHIVAHUMANSVSMACHINESFigure 15: Correlation of Scores on Particular TemplatesBetween Different AnalystsA second approach to studying the same issue was taken byEASYNN=47HARD 70 71IIN=46ANALYST A ANALYST B VISHNU SHIVAHUMANS MACHINESFigure 16: Performance for Easy and Hard Templates byIndividual Analysts and Machinesdividing a set of 93 temples into two groups, either"easy" or"hard".
The division was made by first calculating an errorscore for each template when one analyst is measuredagainst another as akey.
This was done for all 93 temples for2 pairs of analysts, with the mean difference calculated forboth pairs for each template.
The templates were then divid-ed into "easy" and "hard" groups, with the "easy" group con-sisting of those templates with the lower mean differencescores and the "hard" group consisting of those templateswith the higher scores.This was intended a a way of constructing a simulation of acorpus and task that was easier (and harder) than the Tipstertask, which was viewed by many in the Tipster project asexcessively demanding.
The hypothesis was that themachines might do comparatively better than humans on the"'easy" set than on the "hard" set.The results are shown in Figure 16 for two analysts and twomachines.
The opposite of the expected (and hoped-for)hypothesis appeared to be the case.
The human analysts pro-duced roughly twice as many errors on the "hard" set of tem-plates as on the "easy" set, while the machines were onlysomewhat better on the "easy" versus "hard" set.Figure 17 shows the same data, but in terms of the means foreach pair of humans and machines.
In addition, data (at thefar fight) is presented in which the mean machine rror isdivided by the mean human error for each set, thus normal-izing the difference.
Here the comparative differencebetween machine and human was much larger for the "easy"set compared with the "hard" set.Whether this method allows a realistic simulation of theeffects of difficulty of the text and task is unclear and themeaning of this data is hardto interpret.
It would be valuableto develop tests ets for future MUC and Tipster evaluationsthat could effectively assess the effect and nature of text andtask difficulty.70"60,50,4O"3(TEASYNN=47HARD 70.___5I IN=46 543517.5::::::::::::::::::::::::::iiiiii!i?ii~!iiiiiiiiiiiii~!iiiiiiiiiiiiili:i:i:~:~:!:i::.:.:.:.:.:.:.!!iiiiiiiiii~i!iiiiiiiiiiiiiiiiiiiii!iii!i!iiiiiiiiiiiiii!iiiiiiiiiiiiiiii~i~ji!i!!
:::::::5:::::4.03.02.01.0" ' "  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0.0MEAN MEAN MACHINEHUMANS MACHINES MEAN__HUMANFigure 17: Performance for Easy and Hard Templates byMeans for Humans and Machines190COMPARING HUMAN AND MACHINEPERFORMANCE FOR SPECIF ICINFORMATIONA particularly interesting question about human andmachine performance is that of how the two compare for dif-ferent aspects of the extraction task.
Such differences aremost easily seen by comparing human and machine perfor-mance on different slots in a template object or on an entireobject.This issue was investigated by making use of a 60-templatesubset of the MUC-5/Tipster test set that was coded by allanalysts and for which a key was made by the 5th analyst.The scoring program, in addition to calculating overallscores for templates or sets of templates, also providesscores for each individual slot and object in the template.Scores for each of the four human analysts and the three bestmachine systems were obtained for each object and slotusing the scoring program.
Only those objects and slots withat least 10 examples of nonblank responses inthe keys werefurther scored.Because of the wide disparity between scores for humansand for machines, the data representing performance oneach slot and object were normalized in the following man-ner: First, performance for each slot and object for the 4human analysts was averaged by calculating a mean errorfor each slot and object.
A rank order score was thenassigned to each slot and object, reflecting the lowest tohighest comparative p rformance for humans for that slot orobject.Finally, a calculation was made of the comparative differ-ence in performance in scores for particular slots and objectsbetween humans and machines, by sublracting the rankorder score for machines from the rank order score fromhumans for the corresponding slot or object.Figure 18 shows the results, listing the 5 comparatively"worst" slots from the point of view of the machines andthen the 5 comparatively "best" slots from the point of themachines.
For further comparison, one slot in which the per-formance of humans and machines are comparatively equalis listed.The column at the left shows the rank order differencescore, with +30 indicating <equip>name, the slot for whichmachines did the worst compared with humans.
The twocolumns at the right list the error scores for humans andmachines, with the <equip>name slot resulting in amachine score of 84.3% error, but a human score of 18.0%error.
Note that this extreme comparative difference resultsMACHINE COMPARATIVELY WORSEDIFF SLOT MACHINE HUMAN+30 <equip> Name 84.3 18.0+12.5 <litho> equip 68 20+11 <layering> equip 67 19.3+10 <pkg> device 76.3 22.8+10 <pkg> pl..count 60 17.5MACHINE COMPARATIVELY BETTER-20 <layering> type 49.0 29.5-17 <layering> OBJ 45 25.6-13 <layering> film 66 37.8-13 <pkfl> unit 68 46-8 <litho> type 57 24.25COMPARATIVELY EQUAL0 <device> function 68.7 29.3Figure 18: Comparisons of Human and MachinePerformance on Specific SlotsITom two factors: the machines did particularly bad on thisslot (84.3%) compared to their overall performance(68.7%), and humans did particularly well on the slot(18.0%) compared to their overall performance (29.3%).This data is essentially a pilot experiment towards investi-gating the question of how human and machine performancemight compare on specific tasks, with slot and object fills thebest available way of obtaining this information with thegiven darns Without detailed investigation, we can only spec-ulate on the reasons for the results.
It appears, however, thatmany or all machine developers, pressed for time particular-ly in the case of microelectronics, simply did not bother tocode specific slots, viewing them as unimportant tothe finalscore.
Those slots would likely appear in a list of slots thatmachines did comparatively bad on (though it may also benecessary for humans to do particularly badly on the slots aswell).
It appears that, in the case of the slots that machinesdid comparatively well on, that these were slots with largesets of categorical fi ls, with the set sufficiently large and theitems sufficiently obscure that humans had a difficult imeremembering them well enough to effectively detect hemwhen they appeared in text.
Because these words (or acro-nyms) tended to be context-free, relatively simple strategiesfor detecting these keywords and matching them to slotscould be used.
This does suggest that he abilities of humansand machines are quite different, and that an approach inwhich an integrated human-machine system is used rather191than a machine-only system, as is described in \[3\], might beappropriate.CONCLUSIONSThe present study has shown that on the English Microelec-tronics extraction task, the best machine system performswith an error rate,of about 62%, a little less than twice thatof the 33% error produced by highly skilled and experiencedhuman analysts.This level of performance suggests that machine xtractionsystems are still far away from achieving high-qualityextraction with the more difficult texts and extraction prob-lems characterized by the Tipster corpus.
However, machineperformance is close enough to the human level to suggestthat practical extraction systems could be built today bycareful selection of both the text and the extraction task, andperhaps making use of integrated human-machine systemsthat can harness the abilities of both humans and machinesfor extraction rather than depending upon a machine-onlysystem.ACKNOWLEDGEMENTSThe following persons contributed to the effort resulting in thehuman performance measurements reported here: DeborahJohnson, Catherine Steiner, Diane Heavener, and Mario Severinoserved as analysts for the English Microelectronics material, andMary Ellen Okurowski made keys to allow comparison with allanalysts.
Susanne Smith served as a technical consultant onmicroelectronics fabrication.
Beth Sundheim, Nancy Chinchor,and Kathy Daley helped in various ways, particularly with respectto the scoring program used.
Nancy Chinchor also provided somestatistical advice.
Boyan Onyshkevych also helped in defining theproblem and approaches to attacking it, and was a coauthor onsome early presentations of pilot work on human analystperformance atthe Tipster 12-month meeting in September, 1992and the National Science Foundation Workshop on MachineTranslation Evaluation on November 2-3, 1992, both in SanDiego.
Mary Ellen Okurowski provided valuable discussionsabout he human performance work and comments on this paper.Larry Reeker helped with project management of the overalltemplate collection effort, and provided comments on this paper.REFERENCES1.
Will, Craig A., "Comparing Human and Machine Performancefor Natural Language Information Extraction: Results forEnglish Microelectronics from the MUC-5 Evaluation.
"Proceedings ofthe Fifth Message Understanding Confer-ence (MUC-5).
Baltimore, MD, August 25-27, 1993.
SanMateo, CA: Morgan Kaufmann, Inc., 1994.2.
Sundheim, Beth.
"Overview of the Fourth Message Understand-ing Evaluation and Conference."
Proceedings ofthe FourthMessage Understanding Conference (MUC-4) (p. 18 and20).
McLean, VA, June 16-18, 1992.
San Mateo, CA: Mor-gan Kaufmann, Inc., 1992.3.
Will, Craig A., and Reeker, Larry H. "Issues in the Design ofHuman-Machine Systems for Natural Language Informa-tion Extraction."
Presented at the 18-month Tipster meet-ing, February 22-24, 1993, Williamsburg, VA. Paperavailable from authors.4.
Onyshkevych, Boyan A.
"Template Design for InformationExtraction."
Proceedings ofthe TIPSTER Text Program,Phase One.
San Mateo, CA: Morgan Kaufmann, Inc., 1994.5.
Carlson, Lynn, Onyshkevych, Boyan A., and Okurowski, MaryEllen.
"Corpora and Data Preparation for InformationExtraction."
Proceedings ofthe TIPSTER Text Program,Phase One.
San Mateo, CA: Morgan Kaufmann, Inc., 1994.6.
Onyshkevych, Boyan, Okurowski, Mary Ellen, and Carlson,Lynn.
"Tasks, Domains, and Languages for InformationExtraction."
Proceedings ofthe TIPSTER Text Program,Phase One.
San Mateo, CA: Morgan Kaufmann, Inc., 1994.7.
Chinchor, Nancy, and Sundheim, Beth.
"MUC-5 EvaluationMetrics."
Proceedings ofthe Fifth Message UnderstandingConference (MUC-5).
Baltimore, MD, August 25-27, 1993.San Mateo, CA: Morgan Kaufmann, Inc., 1994.8.
SAIC.
"Tipster/MUC-5 Scoring System User's Manual."
Ver-sion 4.3, August 16, 1993.
San Diego, CA: Science Appli-cations International Corporation.APPENDIX:Details of Statistical Measurements and TestsPerformance is expressed in terms of error per response fill, usingthe methodology described by Nancy Chinchor and BethSundheim \[7\] and implemented by the SAIC scoring program \[8\].Error is defined in this methodology b  the following formula:incorrect + (partial x 0.5) + missing + spuriousError =correct + partial + incorrect + missing + spuriouswhere each variable represents a count of the number of responsesfalling into each category.
A correct response occurs when theresponse for a particular slot matches exactly the key for that slot.A partial response occurs when the response is similar to the key,according to certain rule8 used by the scorer.
An incorrectresponse does not match the key.
A spurious response occurs whena response is nonblank, but file key is blank, while a missingresponse occurs when a response is blank but the key is nonblank.192The scoring program is typically given a set of templates andprovides an error score, based on all slots in all of the templates.
Inthis paper data is usually reported as means in terms of this errorscore.
However, statistical parameters describing variability areestimated by having the scoring program generate scores for eachtemplate, even though the means of the data reported here arecalculated across a set of templates.
Only about 80% of templatesproduce an independent score, and only those templates are usedin estimating statistical parameters.
Thus, in many cases two Nsare given, with the larger number the number of templates scoredand the smaller number the number of individual template scoresused in estimating the variance, calculating the standard error andconfidence intervals, and performing statistical tests.In the remainder of this Appendix, details are provided for datapresented in each Figure, as indicated:Figure 2: In the "primary" and "secondary" conditions, 120templates were scored, 30 for each analyst.
In the "other"condition, 240 templates were scored, 60 for each analyst.
Themean for the primary condition was statistically different fromzero at a level of p <.0001 (z=6.74).
The standard error of themean for the primary condition was 2.30 and the 95% confidenceinterval (indicating that 95% of the time the true population mean.can be found within this interval) was from 10.9 to 20.7.
Of the120 templates (each of which con~ibuted to the score shown asthe mean), 92 templates codd be scored independently, and thusN=92 was used for statistical tests.
The standard error of the meanfor the secondary condition was 2.76 and the 95% confidenceinterval from 21.5 to 32.5.
N was 95.
The means for the primaryand secondary conditions are statistically different at a level of p<.01 (t=3.19).
The standard error of the mean for the "other"condition was 2.13, and the 95% confidence interval was from33.2 to 41.6, with an N of 193.
The means for the secondary andother conditions were significantly different (p <.01, t=2.88).Figure 4: The standard error of the mean for the "All analysts"condition for the 4 analysts (A,B,C, and D, respectively) was asfollows: 2.8.2.6, 3.1, and 2.9.
For the "Other analysts" condition:3.4, 3.2, 3.7, and 3.3.
For "Independent analysts": 4.1, 3.7, 3.9,and 5.7.
For "5th analyst": 4.7, 4.5, 4.1, and 4.4.Figure 5: The mean across analysts in the "All Analysts"condition is 25.3, with a standard error of the mean of 1.45 and a95% confidence interval from 22.4 to 28.2 (N=374).
The meanacross analysts in the "Other Analysts" condition is 29.8, with astandard error of the mean of 1.74 and a 95% confidence intervalfrom 26.39 to 33.21 (N=283).
The mean across analysts in the"Independent Analysts" condition is 33.2, with a standard error ofthe mean of 2.14 and a 95% confidence interval from 29.0 to 37.4(N=190).
The mean across analysts in the "5th Analyst" conditionis 28.3, with a standard error of the mean of 2.24 and a 95%confidence interval from 24.0 to 32.6 (N=187).
The mean of the"All analysts" condition is significantly different from that of the"Other analysts" condition (t=2.O0), while the mean of the "Otheranalysts" condition is not significantly different from that of the"independent analysts" condition.Figure 7: In the "All Analysts" condition, analyst A had recall andprecision scores of 84 and 86.5, respectively, analyst B 81 and88.5, analyst C 82.5 and 85.5, and analyst D 82 and 86.5.
In the"Other Analysts" condition, analyst A had recall and precisionscores of 79 and 79, analyst B 72 and 81, analyst C 79 and 75, andanalyst D 78 and 82.
In the "Independent Analysts" condition,Analyst A had recall and precision scores of 81 and 78, Analyst B72 and 83, Analyst C 79 and 79, and Analyst D 73 and 75,respectively.
In the "5th analyst" condition, analyst A had recalland precision scores of 81 and 83, analyst B 69 and 80, analyst C86 and 86, and analyst D 81 and 86, respectively.Figure 11: The standard error of the mean for the human analystswas 2.14, and the 95% confidence interval was from 29.0 to 37.4(N=190).
The mean error for system X (Vishnu) was 62%, with astandard error of 2.41 and a 95% confidence interval from 47.28 to66.72.
The mean error for system Y (Shiva) was 63%, with astandard error of 2.19 and a 95% confidence interval from 58.71 to67.29, while the mean error for system Z (Brahma)was 68%, witha standard error of 2.27and a 95% confidence interval from 63.55to 72.45.
The difference between the mean human scores and themean for the best machine was statistically significant (p<.001,t=8.58).Figure 12: The human analysts had recall and precision scores of79 and 79%, 72 and 81%, 78 and 82%, and 79 and 75%,respectively.
The three best machines, in conlxast, had recall, andprecision scores of 45 and 57%, 53 and 49%, and 41 and 51%,respectively.
These data differ slightly from the official scoring formachine performance because they use only the 120 article subset,not the full 300 article test set.
In addition, the official scoring ofthe machines used interactive scoring, in which human scorerswere allowed to give partial credit for some answers, while thisscoring was done noninteractively.
Note, however, that non-interactive scoring was used for all data in this paper, socomparisons between humans and machines are possible.
The useof noninteractive scoring for both machine and human data couldbias the result slightly, because of the possibility that humansmight be better at providing partially or fully correct answers thatdon't obviously match the key, but again the difference is likely tobe slight.Figure 13 :120  templates were used in the "Normal key"condition, while 480 templates were used in the "Orig.
Coding"condition in the calculation of the mean.Figure 14 :360  templates were used in the "Normal key"condition, and 1440 used in the "Orig.
Coding" in calculating themean.
320 and 1287 were used, respectively, in calculating theerror.193
