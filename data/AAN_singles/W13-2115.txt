Proceedings of the 14th European Workshop on Natural Language Generation, pages 115?124,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsGenerating student feedback from time-series data using ReinforcementLearningDimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam and Oliver LemonDepartment of Mathematical and Computer SciencesHeriot-Watt UniversityEdinburgh, Scotland{dg106, h.hastie, sc445, o.lemon} @hw.ac.ukAbstractWe describe a statistical Natural LanguageGeneration (NLG) method for summarisa-tion of time-series data in the context offeedback generation for students.
In thispaper, we initially present a method forcollecting time-series data from students(e.g.
marks, lectures attended) and use ex-ample feedback from lecturers in a data-driven approach to content selection.
Weshow a novel way of constructing a rewardfunction for our Reinforcement Learningagent that is informed by the lecturers?method of providing feedback.
We eval-uate our system with undergraduate stu-dents by comparing it to three baselinesystems: a rule-based system, lecturer-constructed summaries and a Brute Forcesystem.
Our evaluation shows that thefeedback generated by our learning agentis viewed by students to be as good as thefeedback from the lecturers.
Our findingssuggest that the learning agent needs totake into account both the student and lec-turers?
preferences.1 IntroductionData-to-text generation refers to the task of auto-matically generating text from non-linguistic data(Reiter and Dale, 2000).
The goal of this work isto develop a method for summarising time-seriesdata in order to provide continuous feedback tostudents across the entire semester.
As a casestudy, we took a module in Artificial Intelligenceand asked students to fill out a very short diary-type questionnaire on a weekly basis.
Questionsincluded, for example, number of deadlines, num-ber of classes attended, severity of personal issues.These data were then combined with the marksfrom the weekly lab reflecting the students?
per-formance.
As data is gathered each week in thelab, we now have a set of time-series data and ourgoal is to automatically create feedback.
The goalis to present a holistic view through these diary en-tries of how the student is doing and what factorsmay be affecting performance.Feedback is very important in the learning pro-cess but very challenging for academic staff tocomplete in a timely manner given the large num-ber of students and the increasing pressures onacademics?
time.
This is where automatic feed-back can play a part, providing a tool for teachersthat can give insight into factors that may not beimmediately obvious (Porayska-Pomsta and Mel-lish, 2013).
As reflected in NSS surveys1, stu-dents are not completely satisfied with how feed-back is currently delivered.
The 2012 NSS survey,for all disciplines reported an 83% satisfaction ratewith courses, with 70% satisfied with feedback.This has improved from recent years (in 2006 thiswas 60% for feedback) but shows that there isstill room for improvement in how teachers deliverfeedback and its content.In the next section (Section 2) a discussion ofthe related work is presented.
In Section 3, a de-scription of the methodology is given as well asthe process of the data collection from students,the template construction and the data collectionwith lecturers.
In Section 4, the ReinforcementLearning implementation is described.
In Section5, the evaluation results are presented, and finally,in Sections 6 and 7, a conclusion and directionsfor future work are discussed.2 Related WorkReport generation from time-series data has beenresearched widely and existing methods have beenused in several domains such as weather forecasts(Belz and Kow, 2010; Angeli et al 2010; Sripadaet al 2004), clinical data summarisation (Hunter1http://www.thestudentsurvey.com/115et al 2011; Gatt et al 2009), narrative to assistchildren with communication needs (Black et al2010) and audiovisual debriefs from sensor datafrom Autonomous Underwater Vehicles missions(Johnson and Lane, 2011).The two main challenges for time-series datasummarisation are what to say (Content Selec-tion) and how to say it (Surface Realisation).
Inthis work we concentrate on the former.
Previ-ous methods for content selection include GriceanMaxims (Sripada et al 2003); collective con-tent selection (Barzilay and Lapata, 2004); andthe Hidden Markov model approach for contentselection and ordering (Barzilay and Lee, 2004).NLG systems tend to be very domain-specificand data-driven systems that seek to simultane-ously optimize both content selection and sur-face realisation have the potential to be moredomain-independent, automatically optimized andlend themselves to automatic generalization (An-geli et al 2010; Rieser et al 2010; Dethlefsand Cuayahuitl, 2011).
Recent work on reportgeneration uses statistical techniques from Ma-chine Translation (Belz and Kow, 2010), super-vised learning (Angeli et al 2010) and unsuper-vised learning (Konstas and Lapata, 2012).Here we apply Reinforcement Learning meth-ods (see Section 4 for motivation) which have beensuccessfully applied to other NLG tasks, such asTemporal Expressions Generation (Janarthanamet al 2011), Lexical Choice (Janarthanam andLemon, 2010), generation of adaptive restaurantsummaries in the context of a dialogue system(Rieser et al 2010) and generating instructions(Dethlefs and Cuayahuitl, 2011).3 MethodologyFigure 1: Methodology for data-driven feedbackreport generationFigure 1 shows graphically our approach to the de-velopment of a generation system.
Firstly, we col-lected data from students including marks, demo-graphic details and weekly study habits.
Next, wecreated templates for surface realisation with thehelp of a Teaching and Learning expert.
Thesetemplates were used to generate summaries thatwere rated by lecturers.
We used these ratings totrain the learning agent.
The output of the learningagent (i.e.
automatically generated feedback re-ports) were finally evaluated by the students.
Eachof these steps are discussed in turn.3.1 Time-series Data Collection fromStudentsThe data were collected during the weekly lab ses-sions of a Computer Science module which wastaught to third year Honours and MSc studentsover the course of a 10 week semester.
We re-cruited 26 students who were asked to fill in aweb-based diary-like questionnaire.
Initially, weasked students to provide some demographic de-tails (age, nationality, level of study).
In addition,students provided on a weekly basis, informationfor nine factors that could influence their perfor-mance.
These nine factors were motivated fromthe literature and are listed here in terms of effort(Ames, 1992), frustration (Craig et al 2004) , dif-ficulty (Person et al 1995; Fox, 1993) and per-formance (Chi et al 2001).
Effort is measuredby three factors: (1) how many hours they studied;(2) the level of revision they have done; (3) as wellas the number of lectures (of this module) they at-tended.
Frustration is measured by (4) the levelof understandability of the content; (5) whetherthey have had other deadlines; and whether theyfaced any (6) health and/or (7) personal issues andat what severity.
The difficulty of the lab exercisesis measured by (8) the students?
perception of dif-ficulty.
Finally, (9) marks achieved by the studentsin each weekly lab was used as a measure of theirperformance.3.2 Data TrendsInitially, the data were processed so as to iden-tify the existing trend of each factor during thesemester, (e.g.
number of lectures attending de-creases).
The tendencies of the data are estimatedusing linear least-squares regression, with eachfactor annotated as INCREASING, DECREAS-ING or STABLE.
In addition, for each student weperform a comparison between the average of each116Type Description ExamplesAVERAGE describes the factor data by either averaging the values given bythe student,?You spent 2 hours studying the lecture materialon average?.
(HOURS STUDIED)or by comparing the student?s average with the class average(e.g.
if above the mean value for the class, we say that the ma-terial is challenging).
?You found the lab exercises very challenging?.
(DIFFICULTY)TREND discusses the trend of the data, e.g.
increasing, decreasing orstable.
?Your workload is increasing over thesemester?.
(DEADLINES)WEEKS talks about specific events that happened in one or more weeks.
?You have had other deadlines during weeks 5,6 and 9?.
(DEADLINES)OTHER all other expressions that are not directly related to data.
?Revising material during the semester will im-prove your performance?.
(REVISION)Table 1: The table explains the different template types.factor and the class average of the same factor.3.3 Template GenerationThe wording and phrasing used in the templates todescribe the data were derived from working withand following the advice of a Learning and Teach-ing (L&T) expert.
The expert provided consulta-tion on how to summarise the data.
We derived 4different kinds of templates for each factor: AV-ERAGE, TREND, WEEKS and OTHER based ontime-series data on plotted graphs.
A descriptionof the template types is shown in Table 1.In addition, the L&T expert consulted on howto enhance the templates so that they are ap-propriate for communicating feedback accord-ing to the guidelines of the Higher EducationAcademy (2009), for instance, by including moti-vating phrases such as ?You may want to plan yourstudy and work ahead?.3.4 Data Collection from LecturersThe goal of the Reinforcement Learning agent isto learn to generate feedback at least as well aslecturers.
In order to achieve this, a second datacollection was conducted with 12 lecturers partic-ipating.The data collection consisted of three stageswhere lecturers were given plotted factor graphsand were asked to:1. write a free style text summary for 3 students(Figure 2);2. construct feedback summaries using the tem-plates for 3 students (Figure 3);3. rate random feedback summaries for 2 stu-dents (Figure 4).We developed the experiment using the GoogleWeb Toolkit for Web Applications, which facil-itates the development of client-server applica-tions.
The server side hosts the designed tasks andstores the results in a datastore.
The client side isresponsible for displaying the tasks on the user?sbrowser.In Task 1, the lecturers were presented with thefactor graphs of a student (one graph per factor)and were asked to provide a free-text feedbacksummary for this student.
The lecturers were en-couraged to pick as many factors as they wantedand to discuss the factors in any order they founduseful.
Figure 2 shows an example free text sum-mary for a high performing student where the lec-turer decided to talk about lab marks and under-standability.
Each lecturer was asked to repeat thistask 3 times for 3 randomly picked students.In Task 2, the lecturers were again asked to con-struct a feedback summary but this time they weregiven a range of sentences generated from the tem-plates (as described in Section 2.3).
They wereasked to use these to construct a feedback report.The number of alternative utterances generated foreach factor varies depending on the factor and thegiven data.
In some cases, a factor can have 2 gen-erated utterances and in other cases up to 5 (witha mean of 3 for each factor) and they differenti-ate in the style of trend description and wording.Again the lecturer was free to choose which fac-tors to talk about and in which order, as well asto decide on the template style he/she prefers forthe realisation through the template options.
Fig-ure 3 shows an example of template selection forthe same student as in Figure 2.In Task 3, the lecturers were presented with theplotted factor graphs plus a corresponding feed-back summary that was generated by randomlychoosing n factors and their templates, and wereasked to rate it in a scale between 0-100 (100 forthe best summary).
Figure 4 shows an example of117Figure 2: The interface of the 1st task of the data collection: the lecturer consults the factor graphs andprovides feedback in a free text format.Figure 3: The interface of the 2nd task of data collection: the lecturer consults the graphs and constructsa feedback summary from the given templates (this graph refers to the same student as Figure 2).a randomly generated summary for the same stu-dent as in Figure 2.4 Learning a Time-Series GenerationPolicyReinforcement Learning (RL) is a machine learn-ing technique that defines how an agent learns totake optimal actions in a dynamic environment soas to maximize a cumulative reward (Sutton andBarto, 1998).
In our framework, the task of con-tent selection of time-series data is presented as aMarkov Decision problem.
The goal of the agentis to learn to choose a sequence of actions thatobtain the maximum expected reward in the longrun.
In this section, we describe the Reinforce-ment Learning setup for learning content selection118Figure 4: The interface of the 3rd task of data col-lection: the lecturer consults the graphs and ratesthe randomly generated feedback summary (thisgraph refers to the same student as Figures 2 and3).from time-series data for feedback report gener-ation.
Summarisation from time-series data is anopen challenge and we aim to research other meth-ods in the future, such as supervised learning, evo-lutionary algorithms etc.4.1 Actions and StatesIn this learning setup, we focused only on select-ing the correct content, i.e.
which factors to talkabout.
The agent selects a factor and then decideswhether to talk about it or not.
The state consistsof a description of the factor trends and the num-ber of templates that have been selected so far.
Anexample of the initial state of a student can be:<marks increased, lectures attended stable,hours studied increased, understandability stable,difficulty increased, health issues stable, per-sonal issues stable, revision increased, 0>The agent explores the state space by selecting afactor and then by deciding whether to talk aboutit or not.
If the agent decides to talk about theselected factor, it chooses the template in a greedyway, i.e.
it chooses for each factor the templatethat results in a higher reward.
After an action hasbeen selected, it is deleted from the action space.4.1.1 OrderingIn order to find out in which order the lectur-ers describe the factors, we transformed the feed-back summaries into n-grams of factors.
For in-stance, a summary that talks about the student?sperformance, the number of lectures that he/sheattended, potential health problems and revisiondone can be translated into the following ngram:start, marks, lectures attended, health issues, re-vision, end.
We used the constructed n-grams tocompute the bigram frequency of the tokens in or-der to identify which factor is most probable to bereferred to initially, which factors follow particu-lar factors and which factor is usually talked aboutin the end.
It was found that the most frequent or-dering is: start, marks, hours studied, understand-ability, difficulty, deadlines, health issues, per-sonal issues, lectures attended, revision, end.4.2 Reward FunctionThe goal of the reward function is to optimise theway lecturers generate and rate feedback.
Giventhe expert annotated summaries from Task 1, theconstructed summaries from Task 2 and the ratingsfrom Task 3, we derived the multivariate rewardfunction:Reward = a +n?i=1bi ?
xi + c ?
lengthwhere X = {x1, x2, ..., xn} represents thecombinations between the data trends observed inthe time-series data and the corresponding lectur-ers?
feedback (i.e.
whether they included a factorto be realised or not and how).
The value xi forfactor i is defined by the function:xi =??????????
?1, the combination i of a factor trendand a template type is included inthe feedback0, if not.For instance, the value of x1 is 1 if marks wereincreased and this trend is realised in the feedback,otherwise it is 0.
In our domain n = 90 in order tocover all the different combinations.
The lengthstands for the number of factors selected, a is theintercept, bi and c are the coefficients for xi andlength respectively.In order to model the reward function, we usedlinear regression to compute the weights from thedata gathered from the lecturers.
Therefore, thereward function is fully informed by the data pro-vided by the experts.
Indeed, the intercept a, thevector weights b and the weight c are learnt bymaking use of the data collected by the lecturersfrom the 3 tasks discussed in Section 3.4.The reward function is maximized (Reward= 861.85) for the scenario (i.e.
each student?sdata), content selection and preferred templatestyle shown in Table 2 (please note that this sce-nario was not observed in the data collection).119Factor Trend Templatedifficulty stable NOT MENTIONEDhours studied stable TRENDunderstandability stable NOT MENTIONEDdeadlines increase WEEKShealth issues stable WEEKSpersonal issues stable WEEKSlectures att.
stable WEEKSrevision stable OTHERmarks increase TRENDTable 2: The table shows the scenario at which thereward function is maximised.The reward function is minimized (Reward =-586.0359) for the scenario shown in Table 3(please note that this scenario also was not ob-served in the data collection).Factor Trend Templatedifficulty increase AVERAGEhours studied stable NOT MENTIONEDunderstandability decrease AVERAGEdeadlines * *health issues increase TRENDpersonal issues stable TRENDlectures att.
stable NOT MENTIONEDrevision stable AVERAGEmarks stable TRENDTable 3: The table shows the scenario at which thereward function is minimised (* denotes multipleoptions result in the same minimum reward).4.3 TrainingWe trained a time-series generation policyfor 10,000 runs using the Tabular Temporal-Difference Learning (Sutton and Barto, 1998).During the training phase, the learning agent gen-erated feedback summaries.
When the construc-tion of the summary begins, the length of the sum-mary is 0.
Each time that the agent adds a template(by selecting a factor), the length is incremented,thus changing the state.
It repeats the process untilit decides for all factors whether to talk about themor not.
The agent is finally rewarded at the end ofthe process using the Reward function describedin Section 3.2.
Initially, the learning agent selectsfactors randomly, but gradually learns to identifyfactors that are highly rewarding for a given datascenario.
Figure 5 shows the learning curve of theagent.Figure 5: Learning curve for the learning agent.The x-axis shows the number of summaries pro-duced and y- axis the total reward received foreach summary.5 EvaluationWe evaluated the system using the reward func-tion and with students.
In both these evaluations,we compared feedback reports generated usingour Reinforcement Learning agent with four otherbaseline systems.
Here we present a brief descrip-tion of the baseline systems.Baseline 1: Rule-based system.
This systemselects factors and templates for generation using aset of rules.
These hand-crafted rules were derivedfrom a combination of the L&T expert?s adviceand a student?s preferences and is therefore a chal-lenging baseline and represents a middle groundbetween the L&T expert?s advice and a student?spreferences.
An example rule is: if the mark aver-age is less than 50% then refer to revision.Baseline 2: Brute Force system.
This systemperforms a search of the state space, by exploringrandomly as many different feedback summariesas possible.
The Brute Force algorithm is shownbelow:Algorithm 1 Brute Force algorithmI n p u t d a t a : Df o r n = 0 .
.
.
1 0 , 0 0 0c o n s t r u c t randomly f e e d b a c k [ n ]a s s i g n getReward [ n ]i f ge tReward [ n]>getReward [ n?1]b e s t F e e d b a c k = f e e d b a c k [ n ]e l s eb e s t F e e d b a c k = f e e d b a c k [ n?1]r e t u r n b e s t F e e d b a c kIn each run the algorithm constructs a feedbacksummary, then it calculates its reward, using thesame reward function used for the ReinforcementLearning approach, and if the reward of the newfeedback is better than the previous, it keeps the120new one as the best.
It repeats this process for10,000 times for each scenario.
Finally, the algo-rithm returns the summary that scored the highestranking.Baseline 3: Lecturer-produced summaries.These are the summaries produced by the lectur-ers, as described in Section 2.4, for Task 2 usingtemplate-generated utterances.Baseline 4: Random system: The Randomsystem constructs feedback summaries by select-ing factors and templates randomly as described inTask 3 (in Section 3.4).5.1 Evaluation with Reward FunctionTable 4 presents the results of the evaluation per-formed using the Reward Function, comparingthe learned policy with the four baseline systems.Each system generated 26 feedback summaries.On average the learned policy scores significantlyhigher than any other baseline for the given sce-narios (p <0.05 in a paired t-test).Time-Series Summarisation Systems RewardLearned 243.82Baseline 1: Rule-based 107.77Baseline 2: Brute Force 241.98Baseline 3: Lecturers 124.62Baseline 4: Random 43.29Table 4: The table summarises the average re-wards that are assigned to summaries producedfrom the different systems.5.2 Evaluation with StudentsA subjective evaluation was conducted using 1styear students of Computer Science as participants.We recruited 17 students, who were all English na-tive speakers.
The participants were shown 4 feed-back summaries in a random order, one generatedby the learned policy, one from the rule-based sys-tem (Baseline 1), one from the Brute Force system(Baseline 2) and one summary produced by a lec-turer using the templates (Baseline 3).
Given thepoor performance of the Random system in termsof reward, Baseline 4 was omitted from this study.Overall there were 26 different scenarios, as de-scribed in Section 3.1.
All summaries presentedto a participant were generated from the same sce-nario.
The participants then had to rank the sum-maries in order of preference: 1 for the most pre-ferred and 4 for the least preferred.
Each partici-pant repeated the process for 4.5 scenarios on aver-age (the participant was allowed to opt out at anystage).
The mode values of the rankings of thepreferences of the students are shown in Table 5.The web-based system used for the evaluation isshown in Figure 6.System Mode of RankingsLearned 3rdBaseline 3: Lecturers 3rdBaseline 1: Rule-based 1stBaseline 2: Brute Force 4thTable 5: The table shows the mode value of therankings of the preference of the students.We ran a Mann-Whitney?s U test to evaluate thedifference in the responses of our 4-point LikertScale question between the Learned system andthe other three baselines.
It was found that, forthe given data, the preference of students for thefeedback generated by the Learned system is asgood as the feedback produced by the experts, i.e.there is no significant difference between the meanvalue of the rankings of the Learned system andthe lecturer-produced summaries (p = 0.8) (Base-line 3).The preference of the users for the Brute Forcesystem does not differ significantly from the sum-maries generated by the Learned system (p =0.1335).
However, the computational cost of theBrute Force is higher because each time that thealgorithm sees a new scenario it has to run ap-proximately 3k times to reach a good summary (asseen in Figure 7) and about 10k to reach an optimalone, which corresponds to 46 seconds.
This delaywould prohibit the use of such a system in time-critical situations (such as defence) and in live sys-tems such as tutoring systems.
In addition, theprocessing time would increase with more compli-cated scenarios and if we want to take into accountthe ordering of the content selection and/or if wehave more variables.
In contrast, the RL methodneeds only to be trained once.Finally, the users significantly preferred thesummaries produced by the Rule-based system(Baseline 1) to the summaries produced by theLearned system.
This is maybe because of the factthat in the rule-based system some knowledge ofthe end user?s preferences (i.e.
students) was takeninto account in the rules which was not the casein the other three systems.
This fact suggests that121Figure 6: The interface for the evaluation: the students viewed the four feedback summaries and rankedthem in order of preference.
From left to right, the summaries as generated by: an Expert (Baseline 3),the Rule based system (Baseline 1), the Brute Force algorithm (Baseline 2), the Learned system.Figure 7: The graphs shows the number of cyclesthat the Brute Force algorithm needs to achievespecific rewards.students?
preferences should be taken into accountas they are the receivers of the feedback.
This canalso be generalised to other areas, where the ex-perts and the end users are not the same groupof people.
As the learned policy was not trainedto optimise for the evaluation criteria, in future,we will explore reward functions that bear in mindboth the expert knowledge and the student?s pref-erences.6 ConclusionWe have presented a statistical learning approachto summarisation from time-series data in the areaof feedback reports.
In our reports, we took intoaccount the principles of good feedback provisionas instructed by the Higher Education Academy.We also presented a method for data gatheringfrom students and lecturers and show how we canuse these data to generate feedback by presentingthe problem as a Markov Decision Process andoptimising it using Reinforcement Learning tech-niques.
We also showed a way of constructing adata-driven reward function that can capture de-pendencies between the time-series data and therealisation phrases, in a similar way that the lec-turers do when providing feedback.
Finally, ourevaluation showed that the learned report genera-tion policy generates reports as well as lecturers.7 Future WorkWe aim to conduct further qualitative research inorder to explore what factors and templates stu-dents find useful to be included in the feedbackand inform our reward function with this informa-tion as well as what we have observed in the lec-turer data collection.
This way, we hope, not onlyto gain insights into what is important to studentsand lecturers but also to develop a data-driven ap-proach that, unlike the rule-based system, does notrequire expensive and difficult-to-obtain expert in-put from Learning and Teaching experts.
In ad-dition, we want to compare RL techniques withsupervised learning approaches and evolutionaryalgorithms.
Finally, we want to unify content se-122lection and surface realisation, therefore we willextend the action space in order to include actionsfor template selection.8 AcknowledgementsThe research leading to this work has re-ceived funding from the EC?s FP7 programme:(FP7/2011-14) under grant agreement no.
248765(Help4Mood).ReferencesCarole Ames.
1992.
Classrooms: Goals, Structures,and Student Motivation.
Journal of Educational Psy-chology, 84(3):p261-71.Gabor Angeli, Percy Liang and Dan Klein.
2010.
Asimple domain-independent probabilistic approachto generation.
EMNLP ?10: Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing.Regina Barzilay and Mirella Lapata.
2004.
Collec-tive content selection for concept-to-text generation.HLT ?05: Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
HLT-NAACL2004: Proceedings of the Human Language Tech-nology Conference of the North American Chapterof the Association for Computational Linguistics.Anja Belz and Eric Kow.
2010.
Extracting parallelfragments from comparable corpora for data-to-textgeneration.
INLG ?10: Proceedings of the 6th Inter-national Natural Language Generation Conference.Rolf Black, Joe Reddington, Ehud Reiter, NavaTintarev, and Annalu Waller.
2010.
Using NLG andSensors to Support Personal Narrative for Childrenwith Complex Communication Needs.
SLPAT ?10:Proceedings of the NAACL HLT 2010 Workshop onSpeech and Language Processing for Assistive Tech-nologies.Michelene T.H.
Chi, Stephanie A. Siler, HeisawnJeong, Takashi Yamauchi, Robert G. Hausmann.2001.
Learning from human tutoring.
Journal ofCognitive Science, 25(4):471-533.Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,Barry Gholson.
2004.
Affect and learning: an ex-ploratory look into the role of affect in learning withAutoTutor.
Journal of Educational Media, 29:241-250.Nina Dethlefs and Heriberto Cuayahuitl.
2011.Combining hierarchical reinforcement learning andbayesian networks for natural language generationin situated dialogue.
ENLG ?11: Proceedings of the13th European Workshop on Natural Language Gen-eration.Barbara Fox.
1993.
The Human Tutorial DialogueProject: Issues in the Design of Instructional Sys-tems.
Lawrence Erlbaum Associates, Hillsdale,New Jersey.Albert Gatt, Francois Portet, Ehud Reiter, JamesHunter, Saad Mahamood,Wendy Moncur, and So-mayajulu Sripada.
2009.
From Data to Text in theNeonatal Intensive Care Unit: Using NLG Technol-ogy for Decision Support and Information Manage-ment.
Journal of AI Communications, 22:153-186.Higher Education Academy.
2009.
Providing individ-ual written feedback on formative and summativeassessments.
http://www.heacademy.ac.uk/assets/documents/resources/database/id353_senlef_guide.pdf.Last modified September 16.Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,Cindy Sykes, and D Westwater.
2011.
BT-Nurse:Computer Generation of Natural Language ShiftSummaries from Complex Heterogeneous MedicalData.
Journal of the American Medical InformaticsAssociation,18:621-624.Srinivasan Janarthenam, Helen Hastie, Oliver Lemon,Xingkun Liu.
2011.
?The day after the day after to-morrow??
A machine learning approach to adaptivetemporal expression generation: training and evalu-ation with real users.
SIGDIAL ?11: Proceedings ofthe SIGDIAL 2011 Conference.Srinivasan Janarthanam and Oliver Lemon.
2010.Adaptive Referring Expression Generation in Spo-ken Dialogue Systems: Evaluation with Real Users.SIGDIAL ?10: Proceedings of the 11th AnnualMeeting of the Special Interest Group on Discourseand Dialogue.Nicholas A. R. Johnson and David M. Lane.
2011.Narrative Monologue as a First Step Towards Ad-vanced Mission Debrief for AUV Operator Situa-tional Awareness.
In the 15th International Confer-ence on Advanced Robotics.Ioannis Konstas and Mirella Lapata.
2012.
Unsuper-vised concept-to-text generation with hypergraphs.NAACL HLT ?12: Proceedings of the 2012 Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies.Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan andArthur C. Graesser.
1995.
Pragmatics and Peda-gogy: Conversational Rules and Politeness Strate-gies May Inhibit Effective Tutoring.
Journal of Cog-nition and Instruction, 13(2):161-188.Kaska Porayska-Pomsta and Chris Mellish.
2013.Modelling human tutors?
feedback to inform naturallanguage interfaces for learning.
International Jour-nal of Human-Computer Studies,71(6):703724.123Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation systems.
Cambridge Univer-sity Press.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising Information Presentation for Spoken Di-alogue Systems.
ACL ?10: Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics.Somayajulu Sripada, Ehud Reiter, I Davy, and KNilssen.
2004.
Lessons from Deploying NLG Tech-nology for Marine Weather Forecast Text Gener-ation.
In Proceedings of PAIS session of ECAI-2004:760-764.Somayajulu Sripada, Ehud Reiter, Jim Hunter, and JinYu.
2003.
Generating English Summaries of TimeSeries Data using the Gricean Maxims.
KDD ?03:Proceedings of the ninth ACM SIGKDD interna-tional conference on Knowledge discovery and datamining.Richart Sutton and Andrew Barto.
1998.
Reinforce-ment Learning.
MIT Press.124
