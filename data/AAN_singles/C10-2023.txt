Coling 2010: Poster Volume, pages 197?205,Beijing, August 2010Improving Reordering with Linguistically Informed Bilingual n-gramsJosep Maria CregoLIMSI-CNRSjmcrego@limsi.frFranc?ois YvonLIMSI-CNRS & Univ.
Paris Sudyvon@limsi.frAbstractWe present a new reordering model es-timated as a standard n-gram languagemodel with units built from morpho-syntactic information of the source andtarget languages.
It can be seen as amodel that translates the morpho-syntacticstructure of the input sentence, in contrastto standard translation models which takecare of the surface word forms.
We takeadvantage from the fact that such unitsare less sparse than standard translationunits to increase the size of bilingual con-text that is considered during the trans-lation process, thus effectively account-ing for mid-range reorderings.
Empiricalresults on French-English and German-English translation tasks show that ourmodel achieves higher translation accu-racy levels than those obtained with thewidely used lexicalized reordering model.1 IntroductionWord ordering is one of the major issues in statis-tical machine translation (SMT), due to the manyword order peculiarities of each language.
It iswidely accepted that there is a need for struc-tural information to account for such differences.Structural information, such as Part-of-speech(POS) tags, chunks or constituency/dependencyparse trees, offers a greater potential to learngeneralizations about relationships between lan-guages than models based on word surface forms,because such ?surfacist?
models fail to infer gen-eralizations from the training data.The word ordering problem is typically decom-posed in a number of related problems which canbe further explained by a variety of linguistic phe-nomena.
Accordingly, we can sort out the re-ordering problems into three categories based onthe kind of linguistic units involved and/or thetypical distortion distance they imply.
Roughlyspeaking, we face short-range reorderings whensingle words are reordered within a relativelysmall window distance.
It consist of the easi-est case as typically, the use of phrases (in thesense of translation units of the phrase-based ap-proach to SMT) is believed to adequately performsuch reorderings.
Mid-range reorderings involvereorderings between two or more phrases (trans-lation units) which are closely positioned, typi-cally within a window of about 6 words.
Manyalternatives have been proposed to tackle mid-range reorderings through the introduction of lin-guistic information in MT systems.
To the bestof our knowledge, the authors of (Xia and Mc-Cord, 2004) were the first to address this prob-lem in the statistical MT paradigm.
They auto-matically build a set of linguistically groundedrewrite rules, aimed at reordering the source sen-tence so as to match the word order of the targetside.
Similarly, (Collins, et al2005) and (Popovicand Ney, 2006) reorder the source sentence us-ing a small set of hand-crafted rules for German-English translation.
(Crego and Marin?o, 2007)show that the ordering problem can be more accu-rately solved by building a source-sentence wordlattice containing the most promising reorderinghypotheses, allowing the decoder to decide for thebest word order hypothesis.
Word lattices are builtby means of rewrite rules operating on POS tags;such rules are automatically extracted from thetraining bi-text.
(Zhang, et al2007) introduceshallow parse (chunk) information to reorder thesource sentence, aiming at extending the scope oftheir rewrite rules, encoding reordering hypothe-ses in the form of a confusion network that isthen passed to the decoder.
These studies tacklemid-range reorderings by predicting more or lessaccurate reordering hypotheses.
However, none197of them introduce a reordering model to be usedin decoding time.
Nowadays, most of SMT sys-tems implement the well known lexicalized re-ordering model (Tillman, 2004).
Basically, foreach translation unit it estimates the probabilityof being translated monotone, swapped or placeddiscontiguous with respect to its previous trans-lation unit.
Integrated within the Moses (Koehn,et al2007) decoder, the model achieves state-of-the-art results for many translation tasks.
Oneof the main reasons that explains the success ofthe model is that it considers information of thesource- and target-side surface forms, while theabove mentionned approaches attempt to hypoth-esize reorderings relying only on the informationcontained on the source-side words.Finally, long-range reorderings imply reorder-ings in the structure of the sentence.
Such re-orderings are necessary to model the translationfor pairs like Arabic-English, as English typicallyfollows the SVO order, while Arabic sentenceshave different structures.
Even if several attemptsexist which follow the above idea of making theordering of the source sentence similar to the tar-get sentence before decoding (Niehues and Kolss,2009), long-range reorderings are typically betteraddressed by syntax-based and hierarchical (Chi-ang, 2007) models.
In (Zollmann et al, 2008),an interesting comparison between phrase-based,hierarchical and syntax-augmented models is car-ried out, concluding that hierarchical and syntax-based models slightly outperform phrase-basedmodels under large data conditions and for suf-ficiently non-monotonic language pairs.Encouraged by the work reported in (Hoangand Koehn, 2009), we tackle the mid-range re-ordering problem in SMT by introducing a n-gram language model of bilingual units built fromPOS information.
The rationale behind such amodel is double: on the one hand we aim at in-troducing morpho-syntactic information into thereordering model, as we believe it plays an im-portant role for predicting systematic word or-dering differences between language pairs; at thesame time that it drastically reduces the sparse-ness problem of standard translation units builtfrom surface forms.
On the other hand, n-gramlanguage modeling is a robust approach, that en-ables to account for arbitrary large sequences ofunits.
Hence, the proposed model takes care ofthe translation adequacy of the structural informa-tion present in translation hypotheses, here intro-duced in the form of POS tags.
We also show howthe new model compares to a widely used lexical-ized reordering model, which we have also im-plemented in our particular bilingual n-gram ap-proach to SMT, as well as to the widely knownMoses SMT decoder, a state-of-the-art decoderperforming lexicalized reordering.The remaining of this paper is as follows.
InSection 2 we briefly describe the bilingual n-gramSMT system.
Section 3 details the bilingual n-gram reordering model, the main contribution ofthis paper, and introduces additional well knownreordering models.
In Section 4, we analyze thereordering needs of the language pairs consideredin this work and we carry out evaluation experi-ments.
Finally, we conclude and outline furtherwork in Section 5.2 Bilingual n-gram SMTOur SMT system defines a translation hypothesist given a source sentence s, as the sentence whichmaximizes a linear combination of feature func-tions:t?I1 = argmaxtI1{ M?m=1?mhm(sJ1 , tI1)}(1)where ?m is the weight associated with the fea-ture hm(s, t).
The main feature is the log-score ofthe translation model based on bilingual n-grams.This model constitutes a language model of a par-ticular bi-language composed of bilingual unitswhich are typically referred to as tuples (Marin?o etal., 2006).
In this way, the translation model prob-abilities at the sentence level are approximated byusing n-grams of tuples:p(sJ1 , tI1) =K?k=1p((s, t)k|(s, t)k?1 .
.
.
(s, t)k?n+1)where s refers to source t to target and (s, t)k tothe kth tuple of the given bilingual sentence pairs,sJ1 and tI1.
It is important to notice that, sinceboth languages are linked up in tuples, the context198information provided by this translation model isbilingual.
As for any standard n-gram languagemodel, our translation model is estimated over atraining corpus composed of sentences of the lan-guage being modeled, in this case, sentences ofthe bi-language previously introduced.
Transla-tion units consist of the core elements of any SMTsystem.
In our case, tuples are extracted from aword aligned corpus in such a way that a uniquesegmentation of the bilingual corpus is achieved,allowing to estimate the n-gram model.
Figure 1presents a simple example illustrating the uniquetuple segmentation for a given word-aligned pairof sentences (top).Figure 1: Tuple extraction from an aligned sen-tence pair.The resulting sequence of tuples (1) is furtherrefined to avoid NULL words in source side of thetuples (2).
Once the whole bilingual training datais segmented into tuples, n-gram language modelprobabilities can be estimated.
Notice from theexample that the English source words perfect andtranslations have been reordered in the final tu-ple segmentation, while the French target wordsare kept in their original order.
During decoding,sentences to be translated are encoded in the formof word lattices containing the most promising re-ordering hypotheses, so as to reproduce the wordorder modifications introduced during the tupleextraction process.
Hence, at decoding time, onlythose reordering hypotheses encoded in the wordlattice are examined.
Reordering hypotheses areintroduced following a set of reordering rules au-tomatically learned from the bi-text corpus wordalignments.Following on the previous example, the ruleperfect translations ; translations perfect pro-duces the swap of the English words that is ob-served for the French and English pair.
Typically,POS information is used to increase the general-ization power of such rules.
Hence, rewrite rulesare built using POS instead of surface word forms.See (Crego and Marin?o, 2007) for details on tuplesextraction and reordering rules.3 Reordering ModelsIn this section, we detail three different reorderingmodels implemented in our SMT system.
As pre-viously outlined, the purpose of reordering mod-els is to accurately learn generalizations for theword order modifications introduced on the sourceside during the tuple extraction process.3.1 Source n-gram Language ModelWe employ a n-gram language model estimatedover the source words of the training corpus af-ter being reordered in the tuple extraction process.Therefore, the model scores a given source-sidereordering hypothesis according to the reorder-ings performed in the training sentences.POS tags are used instead of surface formsin order to improve generalization and to reducesparseness.
The model is estimated as any stan-dard n-gram language model, described by thefollowing equation:p(sJ1 , tI1) =J?j=1p(stj |stj?1, .
.
.
, stj?n+1) (2)where stj relates to the POS tag used for the jthsource word.The main drawback of this model is the lackof knowledge of the hypotheses on the target-side.
The probability assigned to a sequence ofsource words is only conditioned to the sequenceof source words.3.2 Lexicalized Reordering ModelA broadly used reordering model for phrase-basedsystems is lexicalized reordering (Tillman, 2004).It introduces a probability distribution for eachphrase pair that indicates the likelihood of being199translated monotone, swapped or placed discon-tiguous to its previous phrase.
The ordering ofthe next phrase with respect to the current phraseis typically also modeled.
In our implementa-tion, we modified the three orientation types andconsider: a consecutive type, where the originalmonotone and swap orientations are lumped to-gether, a forward type, specifying discontiguousforward orientation, and a backward type, spec-ifying discontiguous backward orientation.
Em-pirical results showed that in our case, the neworientations slightly outperform the original ones.This may be explained by the fact that the modelis applied over tuples instead of phrases.Counts of these three types are updated foreach unit collected during the training process.Given these counts, we can learn probability dis-tributions of the form pr(orientation|(st)) whereorientation ?
{c, f, b} (consecutive, forwardand backward) and (st) is a translation unit.Counts are typically smoothed for the estimationof the probability distribution.
A major weaknessof the lexicalized reordering model is due to thefact that it does not considers phrase neighboring,i.e.
a single probability is learned for each phrasepair without considering its context.
An additionalconcern is the problem of sparse data: translationunits may occur only a few times in the trainingdata, making it hard to estimate reliable probabil-ity distributions.3.3 Linguistically Informed Bilingualn-gram Language ModelThe bilingual n-gram LM is estimated as a stan-dard n-gram LM over translation units built fromPOS tags represented as:p(sJ1 , tI1) =K?k=1p((st)tk|(st)tk?1 .
.
.
(st)tk?n+1)where (st)tk relates to the kth translation unit,(st)k, built from POS tags instead of words.This model aims at alleviating the drawbacks ofthe previous two reordering models.
On the onehand it takes into account bilingual informationto model reordering.
On the other hand it con-siders the phrase neighboring when estimating thereordering probability of a given translation unit.Figure 2 shows the sequence of translation unitsbuilt from POS tags, used in our previous exam-ple.Figure 2: Sequence of POS-tagged units used toestimate the bilingual n-gram LM.POS-tagged units used in our model are ex-pected to be much less sparse than those built fromsurface forms, allowing to estimate higher orderlanguage models.
Therefore, larger bilingual con-text are introduced in the translation process.
Thismodel can also be seen as a translation model ofthe sentence structure.
It models the adequacy oftranslating sequences of source POS tags into tar-get POS tags.Note that the model is not limited to usingPOS information.
Rather, many other informa-tion sources could be used (supertags, additionalmorphology features, etc.
), allowing to model dif-ferent translation properties.
However, we musttake into account that the degree of sparsity of themodel units, which is directly related to the in-formation they contain, affects the level of bilin-gual context finally introduced in the translationprocess.
Since more informed units may yieldmore accurate predictions, more informed unitsmay also force the model to fall to lower n-grams.Hence, the degree of accuracy and generalizationpower of the model units must be carefully bal-anced to allow good reordering predictions forcontexts as large as possible.As any standard language model, smoothing isneeded.
Empirical results showed that Kneser-Ney smoothing (Kneser and Ney, 1995) achievedthe best performance among other options (mea-sured in terms of translation accuracy).3.4 Decoding IssuesA straightforward implementation of the threemodels is carried out by extending the log-linearcombination of equation (1) with the new features.Note that no additional decoding complexity isintroduced in the baseline decoding implementa-tion.
Considering the bilingual n-gram languagemodel, the decoder must know the POS tags for200each tuple.
However, each tuple may be taggeddifferently, as words with same surface form mayhave different POS tags.We have implemented two solutions for this sit-uation.
Firstly, we assume that each tuple has asingle POS-tagged version.
Accordingly, we se-lect a single POS-tagged version out of the mul-tiple choices (the most frequent).
Secondly, allPOS-tagged versions of each tuple are allowed.The second choice implies using more accuratePOS-tagged tuples to model reordering, however,it overpopulates the search space with spurioushypotheses, as multiple identical units (with dif-ferent POS tags) are considered.Our first empirical findings showed no differ-ences in translation accuracy for both configura-tions.
Hence, in the remaining of this paper weonly consider the first solution (a single POS-tagged version of each tuple).
The training cor-pus composed of tagged units out of which ournew model is estimated is accordingly modified tocontain only those tagged units considered in de-coding.
Note that most of the ambiguity present inword tagging is resolved by the fact that transla-tion units may contain multiple source and targetside words.4 Evaluation FrameworkIn this section, we perform evaluation experi-ments of our novel reordering model.
First, wegive details of the corpora and baseline systememployed in our experiments and analyze the re-ordering needs of the translation tasks, French-English and German-English (in both directions).Finally, we evaluate the performance of our modeland contrast results with other reordering modelsand translation systems.4.1 CorporaWe have used the fifth version of the EPPS and theNews Commentary corpora made available in thecontext of the Fifth ACL Workshop on StatisticalMachine Translation.
Table 1 presents the basicstatistics for the training and test data sets.
Ourtest sets correspond to news-test2008 and new-stest2009 file sets, hereinafter referred to as Tuneand Test respectively.French, German and English Part-of-speechtags are computed by means of the TreeTagger 1toolkit.
Additional German tags are obtained us-ing the RFTagger 2 toolkit, which annotates textwith fine-grained part-of-speech tags (Schmid andLaws, 2008) with a vocabulary of more than 700tags containing rich morpho-syntactic information(gender, number, case, tense, etc.).Lang.
Sent.
Words Voc.
OOV RefsTrainFrench 1.75 M 52.4 M 137 k ?
?English 1.75 M 47.4 M 138 k ?
?TuneFrench 2, 051 55.3 k 8, 957 1, 282 1English 2, 051 49.2 k 8, 359 1, 344 1TestFrench 2, 525 72.8 k 10, 832 1, 749 1English 2, 525 65.1 k 9, 568 1, 724 1TrainGerman 1, 61 M 42.2 M 381 k ?
?English 1, 61 M 44.2 M 137 k ?
?TuneGerman 2, 051 47, 8 k 10, 994 2, 153 1English 2, 051 49, 2 k 8, 359 1, 491 1TestGerman 2, 525 62, 8 k 12, 856 2, 704 1English 2, 525 65, 1 k 9, 568 1, 810 1Table 1: Statistics for the training, tune and testdata sets.4.2 System DetailsAfter preprocessing the corpora with standard tok-enization tools, word-to-word alignments are per-formed in both directions, source-to-target andtarget-to-source.
In our system implementation,the GIZA++ toolkit3 is used to compute theword alignments.
Then, the grow-diag-final-and(Koehn et al, 2005) heuristic is used to obtain thealignments from which tuples are extracted.In addition to the tuple n-gram translationmodel, our SMT system implements six addi-tional feature functions which are linearly com-1www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger2www.ims.uni-stuttgart.de/projekte/corplex/RFTagger3http://www.fjoch.com/GIZA++.html201bined following a discriminative modeling frame-work (Och and Ney, 2002): a target-languagemodel which provides information about the tar-get language structure and fluency; two lexiconmodels, which constitute complementary trans-lation models computed for each given tuple;a ?weak?
distance-based distortion model; andfinally a word-bonus model and a tuple-bonusmodel which are used in order to compensate forthe system preference for short translations.All language models used in this work areestimated using the SRI language modelingtoolkit4.
According to our experience, Kneser-Ney smoothing (Kneser and Ney, 1995) and in-terpolation of lower and higher n-grams optionsare used as they typically achieve the best per-formance.
Optimization work is carried out bymeans of the widely used MERT toolkit5 whichhas been slightly modified to perform optimiza-tions embedding our decoder.
The BLEU (Pap-ineni et al, 2002) score is used as objective func-tion for MERT and to evaluate test performance.4.3 Reordering in German-English andFrench-English TranslationTwo factors are found to greatly impact the overalltranslation performance: the morphological mis-match between languages, and their reorderingneeds.
The vocabulary size is strongly influencedby the number of word forms for number, case,tense, mood, etc., while reordering needs refer tothe difference in their syntactic structure.
In thiswork, we are primarily interested on the reorder-ing needs of each language pair.
Figure 3 displaysa quantitative analysis of the reordering needs forthe language pairs under study.Figure 3 displays the (%) distribution of thereordered sequences, according to their size, ob-served for the training bi-texts of both translationtasks.
Word alignments are used to determine re-orderings.
A reordering sequence can also be seenas the sequence of words implied in a reorder-ing rule.
Hence, we used the reordering rules ex-tracted from the training corpus to account for re-ordering sequences.
Coming back to the exampleof Figure 1, a single reordering sequence is found,4http://www.speech.sri.com/projects/srilm/5http://www.statmt.org/moses/which considers the source words perfect transla-tions.51015202530354045502 3 4 5 6 7 >=8Size (words)fr-ende-enFigure 3: Size (in words) of reorderings (%) ob-served in training bi-texts.As can be seen, the French-English andGerman-English pairs follow a different distribu-tion of reorderings according to their size.
Alower number of short-range reorderings are ob-served for the German-English task while a highernumber of long-range reorderings.
Consideringmid-range reorderings (from 5 to 7 words), theFrench-English pair shows a lower percentage (?14%) than the German-English (?
22%).
A simi-lar performance is expected when considering theopposite translation directions.
Note that reorder-ings are extracted from word-alignments, an au-tomatic process which is far notoriously error-prone.
The above statistics must be accordinglyconsidered.4.4 ResultsTranslation accuracy (BLEU) results are given intable 2 for the same baseline system performingdifferent reordering models: source 6-gram LM(sLM); lexicalized reordering (lex); bilingual 6-gram LM (bLM) assuming a single POS-taggedversion of each tuple.
In the case of the German-English translation task we also report results forthe bilingual 5-gram LM built from POS tags ob-tained from RFTagger containing a richer vocab-ulary tag set (b+LM).
For comparison purposes,we also show the scores obtained by the Mosesphrase-based system performing lexicalized re-ordering.
Models of both systems are built sharingthe same training data and word alignments.202The worst results are obtained by the sLMmodel.
The fact that it only considers source-language information results clearly relevant toaccurately model reordering.
A very similarperformance is shown by our bilingual n-gramsystem and Moses under lexicalized reordering(bLM and Moses), slightly lower results areobtained by the n-gram system under French-English translation.Config Fr;En En;Fr De;En En;DesLM 22.32 21.97 17.11 12.23lex 22.46 22.09 17.31 12.38bLM 23.03 22.32 17.37 12.58b+LM ?
?
17.57 12.92Moses 22.81 22.33 17.22 12.45Table 2: Translation accuracy (BLEU) results.When moving from lex to bLM, our systemincreases its accuracy results for both tasks andtranslation directions.
In this case, results areslightly higher than those obtained by Moses(same results for English-to-French).
Finally, re-sults for translations performed with the bilingualn-gram reordering model built from rich GermanPOS tags (b+LM) achieve the highest accuracyresults for both directions of the German-Englishtask.
Even though results are consistent for alltranslation tasks and directions they fall withinthe statistical confidence margin.
Add ?2.36to French-English results and ?1.25 to German-English results for a 95% confidence level.
Verysimilar results were obtained when estimating ourmodel for orders from 5 to 7.In order to better understand the impact of theproposed reordering model, we have measured theaccuracy of the reordering task.
Hence, isolat-ing the reordering problem from the more generaltranslation problem.
We use BLEU to account then-gram matching between the sequence of sourcewords aligned to the 1-best translation hypothe-sis, i.e.
the permutation of the source words out-put by the decoder, and the permutation of sourcewords that monotonizes the word alignments withrespect to the target reference.
Note that in or-der to obtain the word alignments of the test setswe re-aligned the entire corpus after including thetest set.
Table 3 shows the BLEU results of thereordering task.
Bigram, trigram and 4gram pre-cision scores are also given.Pair Config BLEU (2g/3g/4g)Fr;En lex 71.69 (75.0/63.4/55.6)bLM 71.98 (75.3/63.7/56.0)En;Fr lex 72.92 (75.5/65.0/57.6)bLM 73.25 (75.8/65.4/58.1)De;En lex 62.12 (67.3/52.1/42.5)b+LM 63.29 (68.3/53.5/44.0)En;De lex 62.72 (67.9/52.8/43.1)b+LM 63.36 (68.6/53.6/43.8)Table 3: Reordering accuracy (BLEU) results.As can be seen, the bilingual n-gram reorderingmodel shows higher results for both translationtasks and directions than lexicalized reordering,specially for German-English translation.
Ourmodel also obtains higher values of n-gram pre-cision for all values of n.Next, we validate the introduction of additionalbilingual context in the translation process.
Fig-ure 4 shows the average size of the translationunit n-grams used for the test set according to dif-ferent models (German-English), the surface form3-gram language model (main translation model),and the new reordering model when built from thereduced POS tagset (POS) and using the rich POStagset (POS+).05101520253035400 1 2 3 4 5 6 7 8 9Size (units)word-based bilingual unitsPOS-based bilingual unitsPOS+-based bilingual unitsFigure 4: Size of translation unit n-grams (%)seen in test for different n-gram models.As expected, translation units built from the re-duced POS tagset are less sparse, enabling us to203introduce larger n-grams in the translation pro-cess.
However, the fact that they achieve lowertranslation accuracy scores (see Table 2) indicatesthat the probabilities associated to these large n-grams are less accurate.
It can also be seen thatthe model built from the rich POS tagset uses ahigher number of large n-grams than the languagemodel built from surface forms.The availability of mid-range n-grams validatesthe introduction of additional bilingual contextachieved by the new model, leading to effec-tively modeling mid-range reorderings.
Noticeadditionally that considering the language modelbuilt from surface forms, only a few 4-grams ofthe test set are seen in the training set, whichexplains the small reduction in performance ob-served when translating with a bilingual 4-gramlanguage model (internal results).
Similarly, theresults shown in Figure 4 validates the choice ofusing bilingual 5-grams for b+LM and 6-gramsfor bLM .Finally, we evaluate the mismatch between thereorderings collected on the training data, andthose output by the decoder.
Table 4 shows thepercentage of reordered sequences found for the1-best translation hypothesis of the test set ac-cording to their size.
The French-to-English andGerman-to-English tasks are considered.Pair Config 2 3 4 5 6 7 ?
8Fr;En lex 58 23 10 5 2 1 1bLM 57 23 11 4 2.5 1.5 1De;En lex 33 24 22 14 5 1.5 0.5b+LM 35 25 19 13 5 2.5 0.5Table 4: Size (%) of the reordered sequences ob-served when translating the test set.Very similar distributions are observed for bothreordering models.
In parallel, distributions arealso comparable to those presented in Figure 3for reorderings collected from the training bi-text,with the exception of long-range and very short-range reorderings.
This may be explained by thefact that system models, in special the distortionpenalty model, typically prefer monotonic trans-lations, while the system lacks a model to supportlarge-range reorderings.5 Conclusions and Further WorkWe have presented a new reordering model basedon bilingual n-grams with units built from lin-guistic information, aiming at modeling the struc-tural adequacy of translations.
We compared ournew reordering model to the widely used lexical-ized reordering model when implemented in ourbilingual n-gram system as well as using Moses,a state-of-the-art phrase-based SMT system.Our model obtained slightly higher transla-tion accuracy (BLEU) results.
We also analysedthe quality of the reorderings output by our sys-tem when performing the new reordering model,which also outperformed the quality of those out-put by the system performing lexicalized reorder-ing.
The back-off procedure used by standardlanguage models allows to dynamically adapt thescope of the context used.
Therefore, in the caseof our reordering model, back-off allows to con-sider always as much bilingual context (n-grams)as possible.
The new model was straightfor-ward implemented in our bilingual n-gram sys-tem by extending the log-linear combination im-plemented by our decoder.
No additional decod-ing complexity was introduced in the baseline de-coding implementation.Finally, we showed that mid-range reorder-ings are present in French-English and German-English translations and that our reordering modeleffectively tackles such reorderings.
However, wesaw that long-range reorderings, also present inthese tasks, are yet to be addressed.We plan to further investigate the use of differ-ent structural information, such as supertags, andtags conveying different levels of morphology in-formation (gender, number, tense, mood, etc.)
fordifferent language pairs.AcknowledgmentsThis work has been partially funded by OSEO un-der the Quaero program.ReferencesF.
Xia and M. McCord.
Improving a Statistical MTSystem with Automatically Learned Rewrite Pat-terns.
In Proc.
of the COLING 2004, 508?514,Geneva, Switzerland, August 2004.204D.
Chiang.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228, June2007.H.
Hoang and Ph.
Koehn.
Improving Mid-Range Re-Ordering Using Templates of Factors.
In Proc.
ofthe EACL 2009, 372?379, Athens, Greece, March2009.J.
M. Crego and J.
B. Marin?o.
Improving statisticalMT by coupling reordering and decoding.
In Ma-chine Translation, 20(3):199?215, July 2007.Marin?o, Jose?
and Banchs, Rafael E. and Crego, JosepMaria and de Gispert, Adria and Lambert, Patrickand Fonollosa, J.A.R.
and Costa-jussa`, Marta N-gram Based Machine Translation.
In Computa-tional Linguistics, 32(4):527?549, 2006Ch.
Tillman.
A Unigram Orientation Model for Sta-tistical Machine Translation.
In Proc.
of the HLT-NAACL 2004, 101?104, Boston, MA, USA, May2004.M.
Collins, Ph.
Koehn and I. Kucerova.
Clause Re-structuring for Statistical Machine Translation.
InProc.
of the ACL 2005, 531?540, Ann Arbor, MI,USA, June 2005.Ph.
Koehn, H. Hoang, A. Birch, Ch.
Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen, Ch.Moran, R. Zens, Ch.
Dyer, O. Bojar, A. Constantinand E. Herbst.
Moses: Open Source Toolkit for Sta-tistical Machine Translation.
In Proc.
of the ACL2007, demonstration session, prague, Czech Repub-lic, June 2007.Y.
Zhang, R. Zens and H. Ney Improved Chunk-levelReordering for Statistical Machine Translation.
InProc.
of the IWSLT 2007, 21?28, Trento, Italy, Oc-tober 2007.H.
Schmid and F. Laws.
Estimation of ConditionalProbabilities with Decision Trees and an Applica-tion to Fine-Grained POS Tagging.
In Proc.
of theCOLING 2008, 777?784, Manchester, UK, August2008.F.J.
Och and H. Ney.
Improved statistical alignmentmodels.
In Proc.
of the ACL 2000, 440?447, HongKong, China, October 2000.Ph.
Koehn, A. Axelrod, A. Birch, Ch.
Callison-Burch,M.
Osborne and D. Talbot.
Edinburgh System De-scription for the 2005 IWSLT Speech TranslationEvaluation.
In Proc of the IWSLT 2005, Pittsburgh,PA, October 2005.F.
J. Och and H. Ney.
Discriminative Training andMaximum Entropy Models for Statistical MachineTranslation.
In Proc.
of the ACL 2002.
295?302,Philadelphia, PA, July 2002.A.
Stolcke.
SRLIM: an extensible language model-ing toolkit.
Proc.
of the INTERSPEECH 2002.
901?904, Denver, CO, September 2008.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
Bleu:a method for automatic evaluation of machine trans-lation.
In Proc.
of the ACL 2002, 311?318, Philadel-phia, PA, July 2002.R.
Kneser and H. Ney.
Improved backing-off for m-gram language modeling.
In Proc.
of the ICASSP1995.
181?184, Detroit, MI, May 1995.A.
Zollmann, A. Venugopal, F. J. Och and J. Ponte.A Systematic Comparison of Phrase-Based, Hierar-chical and Syntax-Augmented Statistical MT.
InProc.
of the COLING 2008.
1145?1152, Manch-ester, UK, August 2008.M.
Popovic and H. Ney.
POS-based Word Reorderingsfor Statistical Machine Translation.
In Proc.
of theLREC 2006.
1278?1283, Genoa, Italy, May 2006.J.
Niehues and M. Kolss.
A POS-Based Model forLong-Range Reorderings in SMT.
In Proc.
of theWMT 2009.
206?214, Athens Greece, March 2009.205
