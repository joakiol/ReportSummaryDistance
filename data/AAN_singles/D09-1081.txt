Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 775?783,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPEstimating Semantic Distance Using Soft Semantic Constraintsin Knowledge-Source?Corpus Hybrid ModelsYuval Marton?
?, Saif Mohammad?, and Philip Resnik??
?Department of Linguistics and?Laboratory for Computational Linguistics and Information Processing,Institute for Advanced Computer Studies.University of Maryland, College Park, MD 20742-7505, USA.
{ymarton,saif,resnik}@umiacs.umd.eduAbstractStrictly corpus-based measures of seman-tic distance conflate co-occurrence infor-mation pertaining to the many possiblesenses of target words.
We propose acorpus?thesaurus hybrid method that usessoft constraints to generate word-sense-aware distributional profiles (DPs) fromcoarser ?concept DPs?
(derived from aRoget-like thesaurus) and sense-unawaretraditional word DPs (derived from rawtext).
Although it uses a knowledgesource, the method is not vocabulary-limited: if the target word is not in thethesaurus, the method falls back grace-fully on the word?s co-occurrence infor-mation.
This allows the method to accessvaluable information encoded in a lexicalresource, such as a thesaurus, while stillbeing able to effectively handle domain-specific terms and named entities.
Exper-iments on word-pair ranking by semanticdistance show the new hybrid method tobe superior to others.1 IntroductionSemantic distance is a measure of the closenessin meaning of two concepts.
People are consis-tent judges of semantic distance.
For example, wecan easily tell that the concepts of ?exercise?
and?jog?
are closer in meaning than ?exercise?
and?theater?.
Studies asking native speakers of a lan-guage to rank word pairs in order of semantic dis-tance confirm this?average inter-annotator corre-lation on ranking word pairs in order of semanticdistance has been repeatedly shown to be around0.9 (Rubenstein and Goodenough, 1965; Resnik,1999).A number of natural language tasks such as ma-chine translation (Lopez, 2008) and word sensedisambiguation (Banerjee and Pedersen, 2003;McCarthy, 2006), can be framed as semanticdistance problems.
Thus, developing automaticmeasures that are in-line with human notions ofsemantic distance has received much attention.These automatic approaches to semantic distancerely on manually created lexical resources such asWordNet, large amounts of text corpora, or both.WordNet-based information content measureshave been successful (Hirst and Budanitsky,2005), but there are significant limitations on theirapplicability.
They can be applied only if a Word-Net exists for the language of interest (which isnot the case for the ?low-density?
languages); andeven if there is a WordNet, a number of domain-specific terms may not be encoded in it.
On theother hand, corpus-based distributional measuresof semantic distance, such as cosine and ?-skewdivergence (Dagan et al, 1999), rely on raw textalone (Weeds et al, 2004; Mohammad, 2008).However, when used to rank word pairs in orderof semantic distance or correct real-word spellingerrors, they have been shown to perform poorly(Weeds et al, 2004; Mohammad and Hirst, 2006).Mohammad and Hirst (2006) and Patwardhanand Pedersen (2006) argued that word sense ambi-guity is a key reason for the poor performance oftraditional distributional measures, and they pro-posed hybrid approaches that are distributional innature, but also make use of information in lexicalresources such as published thesauri and WordNet.However, both these approaches can be applied toestimate the semantic distance between two termsonly if both terms exist in the lexical resource theyrely on.
We know lexical resources tend to havelimited vocabulary and a large number of domain-775specific terms are usually not included.It should also be noted that similarity valuesfrom different distance measures are not compa-rable (even after normalization to the same scale),that is, a similarity score of .75 as per one distancemeasure does not correspond to the same seman-tic distance as a similarity score of .75 from an-other distance measure.1Thus if one uses twoindependent distance measures, in this case: oneresource-reliant and one only corpus-dependent,then these two measures are not comparable (andhence cannot be used in tandem), even if bothrely?partially or entirely?on distributional cor-pus statistics.We propose a hybrid semantic distance methodthat inherently combines the elements of aresource-reliant measure and a strictly corpus-dependent measure by imposing resource-reliantsoft constraints on the corpus-dependent model.We choose the Mohammad and Hirst (2006)method as the resource-reliant method and notone of the WordNet-based measures because, un-like the WordNet-based measures, the Moham-mad and Hirst method is distributional in natureand so lends itself immediately for combinationwith traditional distributional similarity measures.Our new hybrid method combines concept?wordco-occurrence information (the Mohammad andHirst distributional profiles of thesaurus concepts(DPC)) with word?word co-occurrence informa-tion, to generate word-sense-biased distributionalprofiles.
The ?pure?
corpus-based distributionalprofile (a.k.a.
co-occurrence vector, or word asso-ciation vector), for some target word u, is biasedwith soft constraints towards each of the conceptsc that list u in the thesaurus, to create a distribu-tional profile that is specific to u in the sense thatis most related to the other words listed under c.Thus, this method can make more fine-grained distinctions than the Mohammad and Hirstmethod, and yet uses word sense information.2Our proposed method falls back gracefully to relyonly on word-word co-occurrence information ifany of the target terms is not listed in the lexical re-source.
Experiments on the word-pair ranking task1All we can infer is that if w1and w2have a similarityscore of .75 and w3and w4have a score of .5 by the samedistance measure, then w1?w2are closer in meaning thanw3?w4.2Even though Mohammad and Hirst (2006) use thesauruscategories as coarse concepts, their algorithm can be appliedusing more finer-grained thesaurus word groupings (para-graphs and semicolon units), as well.on three different datasets show that the our pro-posed hybrid measure outperforms all other com-parable distance measures.Mohammad and Hirst (2007) show that theirmethod can be used to compute semantic dis-tance in a resource poor language L1by com-bining its text with a thesaurus in a resource-richlanguage L2using an L1?L2bilingual lexicon tocreate cross-lingual distributional profiles of con-cepts, that is, L2word co-occurrence profiles ofL1thesaurus concepts.
Since our method makesuse of the Mohammad and Hirst DPCs, it can justas well make use of their cross-lingual DPCs, tocompute semantic distance in a resource-poor lan-guage, just as they did.
We leave that for futurework.2 Background and Related WorkStrictly speaking, semantic distance/closeness isa property of lexical units?a combination of thesurface form and word sense.3Two terms are con-sidered to be semantically close if there is a lex-ical semantic relation between them.
Such a re-lation may be a classical relation such as hyper-nymy, troponymy, meronymy, and antonymy, orit may be what have been called an ad-hoc non-classical relation, such as cause-and-effect (Mor-ris and Hirst, 2004).
If the closeness in meaningis due to certain specific classical relations such ashypernymy and troponymy, then the terms are saidto be semantically similar.
Semantic relatednessis the term used to describe the more general formof semantic closeness caused by any semantic re-lation (Hirst and Budanitsky, 2005).
So the nounsliquid and water are both semantically similar andsemantically related, whereas the nouns boat andrudder are semantically related, but not similar.The next three sub-sections describe three kindsof automatic distance measures: (1) lexical-resource-based measures that rely on a manuallycreated resource such as WordNet; (2) corpus-based measures that rely only on co-occurrencestatistics from large corpora; and (3) hybrid mea-sures that are distributional in nature, and that alsoexploit the information in a lexical resource.2.1 Lexical-resource-based measuresWordNet is a manually-created hierarchical net-work of nodes (taxonomy), where each node in3The notion of semantic distance can be generalized, ofcourse, to larger units such as phrases, sentences, passages,and so on (Landauer et al, 1998).776the network represents a fine-grained concept orword sense.
An edge between two nodes rep-resents a lexical semantic relation such as hy-pernymy and troponymy.
WordNet-based mea-sures consider two terms to be close if they occurclose to each other in the network (connected byonly a few arcs), if their definitions share manyterms (Banerjee and Pedersen, 2003; Patwardhanand Pedersen, 2006), or if they share a lot of infor-mation (Lin, 1998; Resnik, 1999).
The length ofeach arc/link (distance between nodes) can be as-sumed a unit length, or can be computed from cor-pus statistics.
Within WordNet, the is-a hierarchyis much more well-developed than that of otherlexical semantic relations.
So, not surprisingly,the best WordNet-based measures are those thatrely only on the is-a hierarchy.
Therefore, theyare good at measuring semantic similarity (e.g.,doctor?physician), but not semantic relatedness(e.g., doctor?scalpel).
Further, the measures canonly be used in languages that have a (sufficientlydeveloped) WordNet.
WordNet sense informationhas been criticized to be too fine grained (Agirreand Lopez de Lacalle Lekuona, 2003; Navigli,2006).
See Hirst and Budanitsky (2005) for a com-prehensive survey of WordNet-based measures.2.2 Corpus-based measuresStrictly corpus-based measures of distributionalsimilarity rely on the hypothesis that words thatoccur in similar context tend to be semanticallyclose (Firth, 1957; Harris, 1940).
The set ofcontexts of each target word u is represented byits distributional profile (DP)?the set of wordsthat tend to co-occur with u within a certain dis-tance, along with numeric scores signifying thisco-occurrence tendency with u.
Then measuressuch as cosine or ?-skew divergence are used todetermine how close the DPs of the two targetwords are.
See Section 3 for more details and re-lated work.
These measures are very appealingbecause they rely simply on raw text, but, as de-scribed earlier, when used to rank word pairs inorder of semantic distance, or to correct real-wordspelling errors, they perform poorly, comparedto the WordNet-based measures.
See Weeds etal.
(2004), Mohammad (2008), and Curran (2004)for detailed surveys of distributional measures.As Mohammad and Hirst (2006) point out, theDP of a word u conflates information about thepotentially many senses of u.
For example, con-sider the following.
The noun bank has two senses?river bank?
and ?financial institution?.
Assumethat bank, when used in the ?financial institu-tion?
sense, co-occurred with the noun money 100times in a corpus.
Similarly, assume that bank,when used in the ?river bank?
sense, co-occurredwith the noun boat 80 times.
So the DP of bankwill have co-occurrence information with moneyas well as boat:DPW(bank):money,100; boat,80; bond,70; fish,77; .
.
.Assume that the DP of the word ATM is:DPW(ATM):money,120; boat,0; bond,90; fish,0; .
.
.Thus the distributional distance of bank with ATMwill be some sort of an average of the seman-tic distance between the ?financial institution?
and?ATM?
senses and the semantic distance betweenthe ?river bank?
and ?ATM?
senses.
However, invarious natural language tasks, we need the se-mantic distance between the intended senses ofbank and ATM, which often also tends to be thesemantic distance between their closest senses.2.3 Hybrid measuresBoth Mohammad and Hirst (2006) and Patward-han and Pedersen (2006) proposed measures thatare not only distributional in nature but also relyon a lexical resource to exploit the manually en-coded information therein as well as to overcomethe sense-conflation problem (described in sec-tion 2.2).
Since we essentially combine the Mo-hammad and Hirst method with a ?pure?
word-based distributional measure to create our hybridapproach, we briefly describe their method here.Mohammad and Hirst (2006) generate separatedistributional profiles for the different senses ofa word, without using any sense-annotated data.They use the categories in a Roget-style thesaurus(Macquaries (Bernard, 1986)) as coarse senses orconcepts.
There are about 1000 categories in athesaurus, and each category has on average 120closely related words.
A word may be found inmore than one category if it has multiple meaning.They use a simple unsupervised algorithm to de-termine the vector of words that tend to co-occurwith each concept and the corresponding strengthof association (a measure of how strong the ten-dency to co-occur is).
The target word u will beassigned one DPC for each of the concepts that777list u.
Below are example DPCs of the two con-cepts pertaining to bank:4DPC(?fin.
inst.?
):money,1000; boat,32; bond,705; fish,0; .
.
.DPC(?river bank?
):money,5; boat,863; bond,0; fish,948; .
.
.The distance between two words u, v is deter-mined by calculating the closeness of each of theDPCs of u to each of DPCs of v, and the closestDPC-pair distance is chosen.Mohammad and Hirst (2006) show that their ap-proach performs better than other strictly corpus-based approaches that they experimented with.However, all those experiments were on word-pairs that were listed in the thesaurus.
Their ap-proach is not applicable otherwise.
In Sections 3and 4 we show how cosine?log-likelihood-ratio(or any comparable distributional measure) can becombined with the Mohammad and Hirst DPCs toform a hybrid approach that is not limited to thevocabulary of a lexical resource.Erk and Pad?o (2008) proposed a way of rep-resenting a word sense in context by biasing thetarget word?s DP according to the context sur-rounding a target (specific) occurrence of the tar-get word.
They use dependency relations and se-lectional preferences of the target word and com-bine multiple DPs of words appearing in the con-text of the target occurrence, in a manner so asto give more weight to words co-occurring withboth the target word and the target occurrence?scontext words.
The advantage of this approachis that it does not rely on a thesaurus or Word-Net.
Its disadvantage is that it relies on depen-dency relations and selectional preferences infor-mation, and that the context information it uses inorder to determine the word sense is quite limited(only the words surrounding a single occurrenceof the and hence the representation of that sensemight not be sufficiently accurate.
Their approacheffectively assumes that each occurrence of a wordhas a unique sense.3 Distributional Measures with SoftSemantic ConstraintsTraditional distributional profiles of words (DPW)give word?word co-occurrence frequencies.
Forexample, DPW(u) gives the number of times4The relatively large co-occurrence frequency values forDPCs as compared to DPWs is because a concept can be ref-ered to by many words (on average 100).the target word u co-occurs with with all otherwords:5DPW(u):w1,f(u,w1); w2,f(u,w2); w3,f(u,w3); .
.
.where f stands for co-occurrence frequency (andcan be generalized to stand for any strengthof association (SoA) measure such as the log-likelihood ratio).
Mohammad and Hirst createconcept?word co-occurrence vectors, ?distribu-tional profiles of concepts?
(DPCs), from non-annotated corpus.
For example, DPC(c) gives thenumber of times the concept (thesaurus category)c co-occurs with all the words in a corpus.DPC(c):w1,f(c,w1); w2,f(c,w2); w3,f(c,w3); .
.
.A target word u that appears under thesaurus con-cepts c1, ..., cnwould be assigned to DPC(c1), ...,DPC(cn).
Therefore, if a target word v also ap-pears under some same concept c, the DPCs of uand v would be indistinguishable.We propose the creation of distributional pro-files of word senses (DPWS(uc)) that approximatethe SoA of the target word u, when used in sensec, with each of the words in the corpus:DPWS(uc):w1,f(uc,w1); w2,f(uc,w2); w3,f(uc,w3); .
.
.In order to get exact counts, one needs sense-annotated data.
However, such data is expensiveto create, and is scarce.
Therefore, we proposeestimating these counts from the DPW and DPCcounts:f(uc, wi) = p(c|wi)?
f(u,wi) (1)where the conditional probability p(c|wi) is calcu-lated from the co-occurrence frequencies in DPCs;and the co-occurrence count f(u,wi) is calcu-lated from DPWs.
If the target word is not inthe thesaurus?s vocabulary, then we assume uni-form distribution over all concepts, and in prac-tice use a single sense, and take the conditionalprobability to be 1.
Since the method takes sense-proportional co-occurrence counts, we will referto this method as the hybrid-sense-proportional-counts method (or, hybrid-prop for short).5The dimensions of the DP co-occurrence vector can bedefined arbitrarily, and do not have to correspond to the wordsin the vocabulary.
The most notable alternative representationis the Latent Semantic Analysis and its variants (Landauer etal., 1998; Finkelstein et al, 2002; Budiu et al, 2006).778For example, below is the DPWS of bank inthe ?financial institution?
sense, calculated fromits DPW and DPCs:DPW(bank):money,100; boat,80; bond,70; fish,77; .
.
.DPC(?fin.
inst.?
):money,1000; boat,32; bond,705; fish,0; .
.
.DPC(?river bank?
):money,5; boat,863; bond,0; fish,948; .
.
.DPWS(bank?fin.inst.?):money,(10001000+5?
100); boat,(3232+863?
80);bond,(705705+0?
70); fish,(00+948?
77); .
.
.Once the DPWS are calculated, any counts-based SoA and distance measures can be ap-plied.
For example, in this work we use log-likelihood ratio (Dunning, 1993) to determinethe SoA between a word sense and co-occurringwords, and cosine to determine the distance be-tween two DPWS?s log likelihood vectors (Mc-Donald, 2000).
We also contrast this measure withcosine of conditional probabilities vectors.
Giventwo target words, we determine the distance be-tween each of their DPWS pairings and the closestDPWS-pair distance is chosen.3.1 The hybrid-sense-filtered-counts methodSince the DPCs are created in an unsupervisedmanner, they are expected to be somewhat noisy.Therefore, we also experimented with a variant ofthe method proposed above, that simply makes useof whether the conditional probability p(c|wi) isgreater than 0 or not:f(uc, wi) ={f(u,wi) If p(c|wi) > 00 Otherwise(2)Since this method essentially filters out collocatesthat are likely not relevant to the target sense c ofthe target word u, we will refer to this methodas the hybrid-sense-filtered-counts method (or,just hybrid-filt for short).
Below is an examplehybrid-filtered DPWS of bank in the ?financial in-stitution?
sense:DPWS(bank?fin.inst.?
:money,100); boat,80; bond,70; .
.
.Note that the collocate fish is now filtered out,whereas bank?s co-occurrence counts with money,boat, and bond are left as is (and not sense-proportionally attenuated).4 EvaluationWe evaluated various methods on the task ofranking word pairs in order of semantic dis-tance.
These methods included our sense-biasedmethods as well as several baselines: the Mo-hammad and Hirst (2006) DPC-based methods,the traditional word-based distributional similar-ity methods, and several Latent Semantic Analysis(LSA)-based methods.
We used three testsets andtheir corresponding human judgment gold stan-dards: (1) the Rubenstein and Goodenough (1965)set of 65 noun pairs?denoted RG-65; (2) theWordSimilarity-353 (Finkelstein et al, 2002) setof 353 noun pairs (which include the RG-65pairs) of which we discarded of one repeatingpair?denoted WS-353; and (3) the Resnik andDiab (2000) set of 27 verb pairs?denoted RD-00.4.1 Corpora and Pre-processingWe generated distributional profiles (DPWsand DPCs) from the British National Corpus(BNC) (Burnard, 2000), which is a balanced cor-pus.
We lowercased the characters, and strippednumbers, punctuation marks, and any SGML-likesyntactic tags, but kept sentence boundary mark-ers.
The BNC contained 102,100,114 tokens of546,299 types (vocabulary size) after tokenization.For the verb set, we also lemmatized this corpus.We considered two words as co-occurring ifthey occurred in a window of?5 words from eachother.
We stoplisted words that co-occurred withmore than 2000 word types.4.2 ResultsThe Spearman rank correlations of the automaticrankings of the RG-65, WS353, and RD-00 test-sets with the corresponding gold-standard humanrankings is listed in Table 1.6The higher theSpearman rank correlation, the more accurate isthe distance measure.4.2.1 Results on the RG-65 testsetBaselines.
We replicated the traditional word-based distributional distance measure using co-sine of vectors (DPs) containing conditional prob-abilities (word-cos-cp).
Its rank correlation of.53 is close to the correlation of .54 reported inMohammad and Hirst (2006), hereafter MH06.We replicated the MH06 concept-based approach6Certain experiments were not pursued as they were re-dundant in supporting our claims.779Method RG-65 WS-353 RD-00Baselines (replicated):Traditional distributional measuresword-cos-cp .53 .31 .46word-cos-ll .70 .54 .51word-cos-pmi .62 .43 .57Mohammad and Hirst methods and variantsconcept-cos-cp .62 .38 .41concept*-cos-cp .65 .33 .43concept-cos-ll .60 .37 .43concept*-cos-ll .64 .25 .27concept*-cos-pmi .40 .19 .28Other (LSA and variants)LSA .56 .47 .55GLSA-cos-pmi .18 n.p.
n.p.GLSA-cos-ll .47 n.p.
.29New methods:hybrid-prop-cos-ll .72 .49 .53hybrid-prop*-cos-ll .69 .46 .45hybrid-filt-cos-ll .73 .54 .38hybrid-filt*-cos-ll .77 .54 .39hybrid-prop*-cos-pmi .58 .43 .71hybrid-filt*-cos-pmi .61 .42 .64Table 1: Spearman rank correlation on RG-65,WS-353, and RD-00 testsets, trained on BNC.?*?
indicates the use of a smaller bootstrappedconcept?word co-occurrence matrix.
?n.p.?
indi-cates that the experiment was not pursued.
(concept-cos-cp), and its bootstrapped variant thatuses a smaller concept?word co-occurrence matrix(concept*-cos-cp).
The latter yielded a correla-tion score .65, close to the .69 reported in MH06.We also experimented with cosine of PMI vec-tors (word-cos-pmi) which obtained a correlationof .62.
Log likelihood ratios (word-cos-ll) gavebest results among the baseline methods (.70), andso we it more often in the implementations of ourhybrid method.We conducted experiments with LSA and itsGLSA variants (Budiu et al, 2006) as additionalbaselines.
A limited vocabulary of the 33,000most frequent words in the BNC and all test wordswas used in these experiments.
(A larger vocab-ulary was computationally expensive and 33,000is also the vocabulary size used by Budiu etal.
(2006) in their LSA experiments.
)New Methods: The hybrid method variantsproposed in this paper (hybrid-prop-cos-ll andhybrid-filt-cos-ll) were the best performers on theRG-65 test set.
Particularly, they performed betterthan both the traditional word-distance measures(word-cos-ll), and our concept-based methods?variants of the MH06 method that are used withlikelihood ratios (concept-cos-ll, concept*-cos-ll).
The -pmi methods were all poorer performersthan their -ll counterparts.
The -pmi hybrid vari-ants obtained higher scores than the concept-basedones, but almost the same scores as the word-based ones.4.2.2 Results on WS-353 and RD-00 testsetsOn WS-353, all our hybrid methods out-performed their concept counterparts, and wereon par with their word-based counterparts.
OnRD-00, word-cos-pmi out-performed all otherword-based methods, and the hybrid -pmi meth-ods were best performers with scores of .64 and.71.
Our word-cos-ll, hybrid-prop-cos-ll, andthe two hybrid pmi results on RD-00 are betterthan any non-WordNet results reported by Resnikand Diab (2000), including their syntax-informedmethods?the variants of Lin (?distrib?, .43) andDorr (?LCS?, .39).
In fact, our hybrid*-prop-cos-pmi and hybrid*-filt-cos-pmi results reach corre-lation levels of the WordNet-based methods re-ported there (.66?.68).
Also, on WS-353, ourhybrid sense-filtered variants and word-cos-ll ob-tained a correlation score higher than published re-sults using WordNet-based measures (Jarmasz andSzpakowicz, 2003) (.33 to .35) and Wikipedia-based methods (Ponzetto and Strube, 2006) (.19to .48); and very close to the results obtained bythesaurus-based (Jarmasz and Szpakowicz, 2003)(.55) and LSA-based methods (Finkelstein et al,2002) (.56).The lower correlation scores of all measures onthe WS-353 test set are possibly due to it hav-ing politically biased word pairs (examples in-clude: Arafat?peace, Arafat?terror, Jerusalem?Palestinian) for which BNC texts are likely to in-duce low correlation with the human raters of WS-353.
This testset alo has disproportionately manyterms from the news domain.The concept methods performed poorly on WS-353 partly because many of the target words donot exist in the thesaurus.
For instance, therewere 17 such word types that occurred in 20 WS-353 testset word pairs.
When excluding thesepairs, concept-cos-cp goes up from .38 to .45, andconcept*-cos-pmi from .19 to .24.
Interestingly,results of the hybrid methods show that they werelargely unaffected by the out-of-vocabulary prob-lem on the WS-353 dataset.On the verbs dataset RD-00, while hybrid-prop-cos-ll fared slightly better than word-cos-ll, usingthe smaller matrix seemed to hurt performance of780hybrid*-prop-cos-ll compared to word-cos-ll.
Butresults suggest that the -pmi methods might serveas a better measure than -ll for verbs, although thisclaim should be tested more rigorously.Human judgments of semantic distance are lessconsistent on verb-pairs than on noun-pairs, as re-flected in inter-rater agreement measures in Resnikand Diab (2000) and others).
Thus, not surpris-ingly, the scores of almost all measures are lowerfor the verb data than the RG-65 noun data.5 DiscussionThe hybrid methods proposed in this paper ob-tained higher accuracies than all other methods onthe RG-65 testset (all of whose words were in thepublished thesaurus), and on the RD-00 testset,and their performance was at least respectable onthe WS-353 testset (many of whose words werenot in the published thesaurus).
This is in con-trast to the concept-distance methods which suf-fer greatly when the target words are not in thelexical resource (here, the thesaurus) they rely on,even though these methods can make use of co-occurrence information of words not in the the-saurus with concepts from the thesaurus.Amongst the two hybrid methods proposed, thesense-filtered-counts method performed betterusing the smaller bootstrapped concept?word co-occurrence matrix whereas the sense-proportionalmethod performed better using the larger concept?word co-occurrence matrix.
We believe this is be-cause the bootstrapping method proposed in Mo-hammad and Hirst (2006) has the effect of reset-ting to 0 the small co-occurrence counts.
Thenoise from these small co-occurrence counts af-fects the sense-filtered-counts method more ad-versely (since any non-zero value will cause theinclusion of the corresponding collocate?s full co-occurrence count) and so the bootstrapped matrixis more suitable for this method.The results also show that the cosine of log-likelihood ratios method mostly performs betterthan cosine of conditional probabilities and thepmi methods on the noun sets.
This furthersupports the claim by Dunning (1993) that log-likelihood ratio is much less sensitive than pmito low counts.
Interestingly, on the verb set, thepmi methods, and especially hybrid*-prop-cos-pmi, did extremely well.
Further investigation isneeded in order to determine if pmi is indeed moresuitable for verb semantic similarity, and why.6 ConclusionTraditional distributional similarity conflates co-occurrence information pertaining to the manysenses of the target words.
Mohammad andHirst (2006) show how distributional measurescan be used to compute distance between verycoarse word senses or concepts (thesaurus cat-egories), and even obtain better results thantraditional distributional similarity.
However,their method requires that the target words belisted in the thesaurus, which is often not thecase for domain-specific terms and named enti-ties.
In this paper, we proposed hybrid meth-ods (hybrid-sense-filtered-counts and hybrid-sense-proportional-counts) that combine word?word co-occurrence information (traditional dis-tributional similarity) with word?concept co-occurrence information (Mohammad and Hirst,2006), with soft constraints in such a mannerthat the method makes use of information en-coded in the thesaurus when available, and de-grades gracefully if the target word is not listedin the thesaurus.
Our method generates word-sense-biased distributional profiles (DPs) fromnon-annotated corpus-based word-based DPs andcoarser-grained aggregated thesaurus-based ?con-cept DPs?
(DPCs).
We showed that the hybridmethod correlates with human judgments of se-mantic distance in most cases better than any ofthe other methods we replicated.We are now interested in improving seman-tic distance measures for verb?verb, adjective?adjective, and cross-part-of-speech pairs, by ex-ploiting specific information pertaining to theseparts of speech in lexical resources in addition topurely co-occurrence information.AcknowledgmentsWe thank Mona Diab for her help with her verbtest set, Raluca Budiu for her help and clarifica-tions regarding the GLSA method and its imple-mentation details, and the anonymous reviewersfor their valuable feedback.
This work was sup-ported, in part, by the National Science Founda-tion under Grant No.
IIS-0705832, and in part, bythe Human Language Technology Center of Ex-cellence.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the sponsor.781ReferencesEneko Agirre and Oier Lopez de Lacalle Lekuona.2003.
Clustering WordNet word senses.
In Pro-ceedings of the 1st International Conference onRecent Advances in Natural Language Processing(RANLP-2003), Borovets, Bulgaria.Satanjeev Banerjee and Ted Pedersen.
2003.
Ex-tended gloss overlaps as a measure of semantic re-latedness.
In Proceedings of the Eighteenth Inter-national Joint Conference on Artificial Intelligence(IJCAI-03), pages 805?810, Acapulco, Mexico.John R. L. Bernard, editor.
1986.
The Macquarie The-saurus.
Macquarie Library, Sydney, Australia.Raluca Budiu, Christiaan Royer, and Peter Pirolli.2006.
Modeling information scent: A compari-son of LSA, PMI and GLSA similarity measureson common tests and corpora.
In Proceedings ofRIAO?07, Pittsburgh, PA.Lou Burnard.
2000.
Reference Guide for the BritishNational Corpus.
Oxford University ComputingServices, Oxford, England, world edition edition.James R. Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, School of Informatics,University of Edinburgh, Edinburgh, UK.Ido Dagan, Lillian Lee, and Fernando Pereira.
1999.Similarity-based models of cooccurrence probabili-ties.
Machine Learning, 34(1?3):43?69.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Katrin Erk and Sebastian Pad?o.
2008.
A struc-tured vector space model for word meaning in con-text.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2086), pages 897?906, Honolulu, HI.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.John R. Firth.
1957.
A synopsis of linguistic theory193055.
Studies in Linguistic Analysis, (special vol-ume of the Philological Society):132.
DistributionalHypothesis.Zellig S. Harris.
1940. Review of Louis H. Gray, foun-dations of language (New York: Macmillan, 1939).Language, 16(3):216?231.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lexicalcohesion.
NLE, 11(1):87?111.Mario Jarmasz and Stan Szpakowicz.
2003.
Ro-get?s Thesaurus and semantic similarity.
In Pro-ceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP-2003), pages 212?219, Borovets, Bulgaria.Thomas Landauer, Peter Foltz, and Darrell Laham.1998.
Introduction to latent semantic analysis.
Dis-course Processes, 25:259 ?
284.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th In-ternational Conference on Machine Learning, page296304, San Francisco, CA.Adam Lopez.
2008.
Statistical machine translation.ACM Computing Surveys, 40(3):149.Diana McCarthy.
2006.
Relating WordNet senses forword sense disambiguation.
In Proceedings of theEuropean Chapter of the Association for Computa-tional Linguistics Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholin-guistics Together, pages 17?24, Trento, Italy.S.
McDonald.
2000.
Environmental determinants oflexical processing effort.
Ph.D. thesis, University ofEdinburgh, Edinburgh, UK.Saif Mohammad and Graeme Hirst.
2006.
Distribu-tional measures of concept-distance: A task-orientedevaluation.
In Proceedings of EMNLP.Saif Mohammad, Iryna Gurevych, Graeme Hirst, andTorsten Zesch.
2007.
Cross-lingual distribu-tional profiles of concepts for measuring seman-tic distance.
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP/CoNLL-2007), pages 571?580,Prague, Czech Republic.Saif Mohammad.
2008.
Measuring Semantic Distanceusing Distributional Profiles of Concepts.
Ph.D. the-sis, Department of Computer Science, University ofToronto, Toronto, Canada.Jane Morris and Graeme Hirst.
2004.
Non-classicallexical semantic relations.
In Proceedings of theWorkshop on Computational Lexical Semantics, Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 46?51, Boston, Mas-sachusetts.Roberto Navigli.
2006.
Meaningful clustering ofsenses helps boost word sense disambiguation per-formance.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association, pages 105?112, Sydney, Australia.Siddharth Patwardhan and Ted Pedersen.
2006.
Us-ing WordNet based context vectors to estimate thesemantic relatedness of concepts.
In Proceedings ofMaking Sense of Sense EACL Workshop, pages 1?8.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics (NAACL-2006),pages 192?199, New York, NY.Philip Resnik and Mona Diab.
2000.
Measuring verbsimilarity.
In 22nd Annual Meeting of the CognitiveScience Society (COGSCI2000), Philadelphia, PA.Philip Resnik.
1999.
Semantic similarity in a taxon-omy: An information-based measure and its appli-cation to problems of ambiguity in natural language.JAIR, 11:95?130.782Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th Interna-tional Conference on Computational Linguistics(COLING-04), pages 1015?1021, Geneva, Switzer-land.783
