Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 221?224, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Labeling Using Complete Syntactic AnalysisMihai SurdeanuTechnical University of Catalunyasurdeanu@lsi.upc.eduJordi TurmoTechnical University of Catalunyaturmo@lsi.upc.eduAbstractIn this paper we introduce a semantic rolelabeling system constructed on top of thefull syntactic analysis of text.
The la-beling problem is modeled using a richset of lexical, syntactic, and semantic at-tributes and learned using one-versus-allAdaBoost classifiers.Our results indicate that even a simple ap-proach that assumes that each semantic ar-gument maps into exactly one syntacticphrase obtains encouraging performance,surpassing the best system that uses par-tial syntax by almost 6%.1 IntroductionMost current semantic role labeling (SRL) ap-proaches can be classified in one of two classes:approaches that take advantage of complete syntac-tic analysis of text, pioneered by (Gildea and Juraf-sky, 2002), and approaches that use partial syntac-tic analysis, championed by the previous CoNLLshared task evaluations (Carreras and Ma`rquez,2004).However, to the authors?
knowledge, a clear anal-ysis of the benefits of using full syntactic analysisversus partial analysis is not yet available.
On onehand, the additional information provided by com-plete syntax should intuitively be useful.
But, onthe other hand, the state-of-the-art of full parsingis known to be less robust and perform worse thanthe tools used for partial syntactic analysis, whichwould decrease the quality of the information pro-vided.
The work presented in this paper contributesto this analysis by introducing a model that is en-tirely based on the full syntactic analysis of text,generated by a real-world parser.2 System Description2.1 Mapping Arguments to SyntacticConstituentsOur approach maps each argument label to one syn-tactic constituent, using a strategy similar to (Sur-deanu et al, 2003).
Using a bottom-up approach,we map each argument to the first phrase that has theexact same boundaries and climb as high as possiblein the syntactic tree across unary production chains.Unfortunately, this one-to-one mapping betweensemantic arguments and syntactic constituents is notalways possible.
One semantic argument may bemapped to many syntactic constituents due to: (a)intrinsic differences between the syntactic and se-mantic representations, and (b) incorrect syntacticstructure.
Figure 1 illustrates each one of these sit-uations: Figure 1 (a) shows a sentence where eachsemantic argument correctly maps to one syntac-tic constituent; Figure 1 (b) illustrates the situationwhere one semantic argument correctly maps to twosyntactic constituents; and Figure 1 (c) shows a one-to-many mapping caused by an incorrect syntacticstructure: argument A0 maps to two phrases, the ter-minal ?by?
and the noun phrase ?Robert Goldberg?,due to the incorrect attachment of the last preposi-tional phrase, ?at the University of California?.Using the above observations, we separate one-221rising consumer pricesVBG NN NNSNPP A1developed by Robert Goldberg at the University of CaliforniaNPPPNPNPPPVPP A0 AM?LOCThe luxury auto maker last year sold 1,214 cars in the U.S.PPNPVPNPNPSA0 A1PAM?TMP AM?LOC(b)(a) (c)Figure 1: Mapping semantic arguments to syntactic constituents: (a) correct one?to-one mapping; (b) correctone-to-many mapping; (c) one-to-many mapping due to incorrect syntax.
(a) (b) (c)Training 96.06% 2.49% 1.45%Development 91.36% 4.83% 3.81%Table 1: Distribution of semantic arguments accord-ing to their mapping to syntactic constituents ob-tained with the Charniak parser: (a) one-to-one, (b)one-to-many, all syntactic constituents have sameparent, (c) one-to-many, syntactic constituents havedifferent parents.to-many mappings in two classes: (a) when the syn-tactic constituents mapped to the semantic argumenthave the same parent (Figure 1 (b)) the mapping iscorrect and/or could theoretically be learned by asequential SRL strategy, and (b) when the syntac-tic constituents mapped to the same argument havedifferent parents, the mapping is generally causedby incorrect syntax.
Such cases are very hard to belearned due to the irregularities of the parser errors.Table 1 shows the distribution of semantic argu-ments into one of the above classes, using the syn-tactic trees provided by the Charniak parser.
For theresults reported in this paper, we model only one-to-one mappings between semantic arguments andsyntactic constituents.
A subset of the one-to-manymappings are addressed with a simple heuristic, de-scribed in Section 2.4.2.2 FeaturesThe features incorporated in the proposed modelare inspired from the work of (Gildea and Juraf-sky, 2002; Surdeanu et al, 2003; Pradhan et al,2005; Collins, 1999) and can be classified into fiveclasses: (a) features that capture the internal struc-ture of the candidate argument, (b) features extractedThe syntactic label of the candidate constituent.The constituent head word, suffixes of length 2, 3, and 4,lemma, and POS tag.The constituent content word, suffixes of length 2, 3, and4, lemma, POS tag, and NE label.
Content words, whichadd informative lexicalized information different fromthe head word, were detected using the heuristicsof (Surdeanu et al, 2003).The first and last constituent words and their POS tags.NE labels included in the candidate phrase.Binary features to indicate the presence of temporal cuewords, i.e.
words that appear often in AM-TMP phrasesin training.For each TreeBank syntactic label we added a feature toindicate the number of such labels included in thecandidate phrase.The sequence of syntactic labels of the constituentimmediate children.Table 2: Argument structure featuresThe phrase label, head word and POS tag of theconstituent parent, left sibling, and right sibling.Table 3: Argument context featuresfrom the argument context, (c) features that describeproperties of the target predicate, (d) features gener-ated from the predicate context, and (e) features thatmodel the distance between the predicate and the ar-gument.
These five feature sets are listed in Tables 2,3, 4, 5, and 6.2.3 ClassifierThe classifiers used in this paper were devel-oped using AdaBoost with confidence rated predic-tions (Schapire and Singer, 1999).
AdaBoost com-bines many simple base classifiers or rules (in ourcase decision trees of depth 3) into a single strongclassifier using a weighted-voted scheme.
Each baseclassifier is learned sequentially from weighted ex-amples and the weights are dynamically adjusted ev-ery learning iteration based on the behavior of the222The predicate word and lemma.The predicate voice.
We currently distinguish five voicetypes: active, passive, copulative, infinitive, and progressive.A binary feature to indicate if the predicate is frequent - i.e.it appears more than twice in the training partition - or not.Table 4: Predicate structure featuresSub-categorization rule, i.e.
the phrase structure rule thatexpands the predicate immediate parent, e.g.NP?
VBG NN NNS for the predicate in Figure 1 (b).Table 5: Predicate context featuresThe path in the syntactic tree between the argument phraseand the predicate as a chain of syntactic labels along withthe traversal direction (up or down).The length of the above syntactic path.The number of clauses (S* phrases) in the path.The number of verb phrases (VP) in the path.The subsumption count, i.e.
the difference between thedepths in the syntactic tree of the argument and predicateconstituents.
This value is 0 if the two phrases share thesame parent.The governing category, which indicates if NParguments are dominated by a sentence (typical forsubjects) or a verb phrase (typical for objects).We generalize syntactic paths with more than 3elements using two templates:(a) Arg ?
Ancestor ?
Ni ?
Pred, where Arg is theargument label, Pred is the predicate label, Ancestoris the label of the common ancestor, and Ni is instantiatedwith all the labels between Pred and Ancestor inthe full path; and(b) Arg ?
Ni ?
Ancestor ?
Pred, where Ni isinstantiated with all the labels between Arg andAncestor in the full path.The surface distance between the predicate and theargument phrases encoded as: the number of tokens, verbterminals (VB*), commas, and coordinations (CC) betweenthe argument and predicate phrases, and a binary feature toindicate if the two constituents are adjacent.A binary feature to indicate if the argument starts with apredicate particle, i.e.
a token seen with the RP* POStag and directly attached to the predicate in training.Table 6: Predicate-argument distance featurespreviously learned rules.We trained one-vs-all classifiers for the top 24most common arguments in training (includingR-A* and C-A*).
For simplicity we do not la-bel predicates.
Following the strategy proposedby (Carreras et al, 2004) we select training exam-ples (both positive and negative) only from: (a) thefirst S* phrase that includes the predicate, or (b)from phrases that appear to the left of the predicatein the sentence.
More than 98% of the argumentsfall into one of these classes.At prediction time the classifiers are combined us-ing a simple greedy technique that iteratively assignsto each predicate the argument classified with thehighest confidence.
For each predicate we consideras candidates all AM attributes, but only numberedattributes indicated in the corresponding PropBankframe.2.4 Argument Expansion HeuristicsWe address arguments that should map to morethan one terminal phrase with the following post-processing heuristic: if an argument is mapped toone terminal phrase, its boundaries are extendedto the right to include all terminal phrases that arenot already labeled as other arguments for the samepredicate.
For example, after the system tags ?con-sumer?
as the beginning of an A1 argument in Fig-ure 1, this heuristic extends the right boundary ofthe A1 argument to include the following terminal,?prices?.To handle inconsistencies in the treatment ofquotes in parsing we added a second heuristic: argu-ments are expanded to include preceding/followingquotes if the corresponding pairing quote is alreadyincluded in the argument constituent.3 Evaluation3.1 DataWe trained our system using positive examples ex-tracted from all training data available.
Due to mem-ory limitations on our development machines weused only the first 500,000 negative examples.
In theexperiments reported in this paper we used the syn-tactic trees generated by the Charniak parser.
Theresults were evaluated for precision, recall, and F1using the scoring script provided by the task orga-nizers.3.2 Results and DiscussionTable 7 presents the results obtained by our system.On the WSJ data, our results surpass with almost 6%the results obtained by the best SRL system that usedpartial syntax in the CoNLL 2004 shared task eval-uation (Hacioglu et al, 2004).
Even though thesenumbers are not directly comparable (this year?sshared task offers more training data), we considerthese results encouraging given the simplicity ofour system (we essentially model only one-to-one223Precision Recall F?=1Development 79.14% 71.57% 75.17Test WSJ 80.32% 72.95% 76.46Test Brown 72.41% 59.67% 65.42Test WSJ+Brown 79.35% 71.17% 75.04Test WSJ Precision Recall F?=1Overall 80.32% 72.95% 76.46A0 87.09% 85.21% 86.14A1 79.80% 72.23% 75.83A2 74.74% 58.38% 65.55A3 83.04% 53.76% 65.26A4 77.42% 70.59% 73.85A5 0.00% 0.00% 0.00AM-ADV 57.82% 46.05% 51.27AM-CAU 49.38% 54.79% 51.95AM-DIR 62.96% 40.00% 48.92AM-DIS 72.19% 76.25% 74.16AM-EXT 60.87% 43.75% 50.91AM-LOC 64.19% 52.34% 57.66AM-MNR 63.90% 44.77% 52.65AM-MOD 98.09% 93.28% 95.63AM-NEG 96.15% 97.83% 96.98AM-PNC 55.22% 32.17% 40.66AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 79.17% 73.41% 76.18R-A0 84.85% 87.50% 86.15R-A1 75.00% 71.15% 73.03R-A2 60.00% 37.50% 46.15R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 68.00% 80.95% 73.91R-AM-MNR 30.00% 50.00% 37.50R-AM-TMP 60.81% 86.54% 71.43V 0.00% 0.00% 0.00Table 7: Overall results (top) and detailed results onthe WSJ test (bottom).mappings between semantic arguments and syntac-tic constituents).
Only 0.14% out of the 75.17% Fmeasure obtained on the development partition areattributed to the argument expansion heuristics in-troduced in Section 2.4.4 ConclusionsThis paper describes a semantic role labeling sys-tem constructed on top of the complete syntacticanalysis of text.
We model semantic arguments thatmap into exactly one syntactic phrase (about 90%of all semantic arguments in the development set)using a rich set of lexical, syntactic, and semanticattributes.
We trained AdaBoost one-versus-all clas-sifiers for the 24 most common argument types.
Ar-guments that map to more than one syntactic con-stituent are expanded with a simple heuristic in apost-processing step.Our results surpass with almost 6% the results ob-tained by best SRL system that used partial syntax inthe CoNLL 2004 shared task evaluation.
Althoughthe two evaluations are not directly comparable dueto differences in training set size, the current resultsare encouraging given the simplicity of our proposedsystem.5 AcknowledgementsThis research has been partially funded by the Euro-pean Union project ?Computers in the Human Inter-action Loop?
(CHIL - IP506909).
Mihai Surdeanu isa research fellow within the Ramo?n y Cajal programof the Spanish Ministry of Education and Science.We would also like to thank Llu?
?s Ma`rquez andXavi Carreras for the help with the AdaBoost classi-fier, for providing the set of temporal cue words, andfor the many motivating discussions.ReferencesX.
Carreras and L. Ma`rquez.
2004.
Introduction to the CoNLL-2004 shared task: Semantic role labeling.
In Proceedings ofCoNLL 2004 Shared Task.X.
Carreras, L. Ma`rquez, and G. Chrupa?a.
2004.
Hierarchicalrecognition of propositional arguments with perceptrons.
InProceedings of CoNLL 2004 Shared Task.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
PhD Dissertation, University of Penn-sylvania.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3).K.
Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Ju-rafsky.
2004.
Semantic role labeling by tagging syntacticchunks.
In Proceedings of CoNLL 2004 Shared Task.S.
Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin,and D. Jurafsky.
2005.
Support vector learning for semanticargument classification.
To appear in Journal of MachineLearning.R.
E. Schapire and Y.
Singer.
1999.
Improved boosting algo-rithms using confidence-rated predictions.
Machine Learn-ing, 37(3).M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003.Using predicate-argument structures for information extrac-tion.
In Proceedings of ACL 2003.224
