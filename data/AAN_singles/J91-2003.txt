Semantics of ParagraphsWlodek  Zadrozny  ?Karen JensewIBM T. J. Watson Research CenterWe present a computational theory of the paragraph.
Within it we formally define coherence,give semantics to the adversative conjunction "but" and to the Gricean maxim of quantity,and present some new methods for anaphora resolution.The theory precisely characterizes the relationship between the content of the paragraphand background knowledge needed for its understanding.
This is achieved by introducing anew type of logical theory consisting of an object level, corresponding to the content of theparagraph, a referential level, which is a new logical level encoding background knowledge,and a metalevel containing constraints on models of discourse (e.g.
a formal version of Griceanmaxims).
We propose also specific mechanisms of interaction between these levels, resemblingboth classical provability and abduction.
Paragraphs are then represented by a class of structurescalled p-models.1.
IntroductionLogic and knowledge have been often discussed by linguists.
Anaphora is anotherprominent subject in linguistic analyses.
Not so frequently examined are different ypesof cohesion.
And it is quite rare to find the word "paragraph" in articles or booksabout natural language understanding, although paragraphs are grammatical unitsand units of discourse.
But it is possible to speak formally about he role of backgroundknowledge, cohesion, coherence and anaphora--all within one, flexible and natural,logical system--if one examines the semantic role of the linguistic construct called aparagraph.Paragraphs have been sometimes described, rather loosely, as "units of thought.
"We establish a correspondence b tween them and certain types of logical models,thereby making the characterization f paragraphs more precise.
The correspondencegives us also an opportunity to identify and attack--with some success, we believe--three interesting and important problems: (1) how to define formally coherence andtopic, (2) how to resolve anaphora, and (3) what is the formal meaning of linkages (con-nectives) such as but, however, and, certainly, usually, because, then, etc.
These questionsare central from our point of view because: (1) the "unity" of a paragraph stems fromits coherence, while the "aboutness" of thought can be, at least to some extent, de-scribed as existence of a topic; (2) without determining reference of pronouns andphrases, the universes of the models are undefined; and (3) the linkages, which makesentences into paragraphs, have semantic roles that must be accounted for.
We can ex-plain then the process of building a computational model of a paragraph (a p-model) asan interaction between its sentences, background knowledge to which these sentencesrefer, and metatheoretical operators that indicate types of permitted models.?
P.O.
Box 704, Yorktown Heights, NY 10598(~ 1991 Association for Computational LinguisticsComputational Linguistics Volume 17, Number 2At this point the reader may ask: what is so special about paragraphs; does allthis mean that a chapter, a book or a letter do not have any formal counterparts?
Webelieve they do.
But we simply do not yet know how corresponding formal structureswould be created from models of paragraphs.
~Ib answer this question we may needmore advanced theories of categorization and learning than exist today.
On the otherhand, the paragraph is the right place to begin: it is the next grammatical unit afterthe sentence; connectives providing cohesion operate here, not at the level of an entirediscourse; and it is the smallest reasonable domain of anaphora resolution.
Further-more, we will argue, it is the smallest domain in which topic and coherence can bedefined.The formalization of paragraph structure requires the introduction of a new typeof logical theory and a corresponding class of models.
As we know, the usual log-ical structures consist of an object level theory T and provability relation F-; withinthe context of the semantics of natural anguage, the object theory contains a logicaltranslation of the surface form of sentences, and F- is the standard provability relation(logical consequence).
In mathematical logic, this scheme is sometimes extended byadding a metalevel assumption, for instance postulating the standardness of naturalnumbers.
In artificial intelligence, a metarule typically, the closed world assumptionof circumscription---can beused in dealing with theoretical questions, like the frameproblem.
But a formal account of natural language understanding requires more.
Itrequires at least a description (a) of how background knowledge about objects andrelations that the sentences describe is used in the process of understanding, and (b) ofgeneral constraints on linguistic communications, asexpressed for instance in Griceanmaxims.
It is well known that without the former it is impossible to find references ofpronouns or attachments of prepositional phrases; background knowledge, as it turnsout, is also indispensable in establishing coherence.
We have then reasons for introduc-ing a new logical level--a referential level R, which codes the background knowledge.As for Gricean maxims, we show that they can be expressed formally and can be usedin a computational model of communication.
We include them in a metalevel M, whichcontains global constraints on models of a text and definitions of meta-operators suchas the conjunction but.
We end up with three-level logical theories (M, T, R, ~-R + M),where a provability relation ~-~ + M, based on R and M, can be used, for example, toestablish the reference of pronouns.This work is addressed primarily to our colleagues working on computationalmodels of natural language; but it should be also of interest o linguists, logicians,and philosophers.
It should be of interest o linguists because the notion that a para-graph is equivalent o a model is something concrete to discuss; because p-modelsare as formal as formal languages (and therefore something satisfyingly theoreticalto argue about); and because new directions for analysis are opened beyond the sen-tence.
The work should be of interest o logicians because it introduces a new type ofthree-level theory, and corresponding models.
The theory of these structures, whichare based on linguistic constructs, will differ from classical model theory--for instance,by the fact that names of predicates of an object theory matter, because they connectthe object heory with the referential level.
This work should be of interest o philoso-phers for many of the same reasons: it makes more sense to talk about the meaningof a paragraph than about the meaning of a sentence.
The following parallel can bedrawn: a sentence is meaningful only with respect o a model of a paragraph, exactlyas the truth value of a formula can be computed only with respect o a given model.Moreover, it is possible in this framework to talk about meaning without mentioningthe idea of possible worlds.
However, we do not identify meaning with truth condi-tions; in this paper, the meaning of a sentence is its role in the model of the paragraph172Zadrozny and Jensen Semantics of Paragraphsin which this sentence occurs.
Our intuitive concept of meaning is similar to Lakoff's(1987) Idealized Cognitive Model (ICM).
Needless to say, we believe in the possibil-ity of formalizing ICMs, although in this paper we will not try to express, in logic,prototype ffects, metaphors, or formal links with vision.The paper is presented in six sections.
In Section 2, we discuss the grammaticalfunction of the paragraph and we show, informally, how a formal model of a para-graph might actually be built.
In Section 3 we give the logical preliminaries to ouranalysis.
We discuss a three-part logical structure that includes a referential level, andwe introduce a model for plausible meaning.
Section 4 discusses paragraph coherence,and Section 5 constructs a model of a paragraph, a p-model, based on the informationcontained in the paragraph itself and background information contained in the ref-erential evel R. Section 5 further motivates the use of the referential level, showinghow it contributes to the resolution of anaphoric reference.
In Section 6, we broadenour presentation of the metalevel, introducing some metalevel axioms, and sketchingways by which they can be used to reduce ambiguity and construct new models.
Wealso show metalevel rules for interpreting "but."2.
The Paragraph as a Discourse Unit2.1 Approaches to Paragraph AnalysisRecent syntactic theory--that is, in the last 30 years--has been preoccupied withsentence-level analysis.
Within discourse theory, however, some significant work hasbeen done on the analysis of written paragraphs.
We can identify four different linguis-tic approaches to paragraphs: prescriptivist, psycholinguist, extualist, and discourse-oriented.The prescriptivist approach is typified in standard English grammar textbooks,such as Warriner (1963).
In these sources, a paragraph is notionally defined as some-thing like a series of sentences that develop one single topic, and rules are laid downfor the construction of an ideal (or at least an acceptable) paragraph.
Although thesedictates are fairly clear, the underlying notion of topic is not.An example of psycholinguistically oriented research work can be found in Bondand Hayes (1983).
These authors take the position that a paragraph is a psychologicallyreal unit of discourse, and, in fact, a formal grammatical unit.
Bond and Hayes foundthree major formal devices that are used, by readers, to identify a paragraph: (1) therepetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference;and (3) paragraph length, as determined by spatial and/or sentence-count i formation.Other psycholinguistic studies that confirm the validity of paragraph units can befound in Black and Bower (1979) and Haberlandt et al (1980).The textualist approach to paragraph analysis is exemplified by E. J. Crothers.
Hiswork is taxonomic, in that he performs detailed escriptive analyses of paragraphs.
Helists, classifies, and discusses various types of inference, by which he means, generally,"the linguistic-logical notions of consequent and presupposition" Crothers (1979:112)have collected convincing evidence of the existence of language chunks--real struc-tures, not just orthographic conventions--that are smaller than a discourse, larger thana sentence, generally composed of sentences, and recursive in nature (like sentences).These chunks are sometimes called "episodes," and sometimes "paragraphs."
Accord-ing to Hinds (1979), paragraphs are made up of segments, which in turn are made up ofsentences or clauses, which in turn are made up of phrases.
Paragraphs therefore givehierarchical structure to sentences.
Hinds discusses three major types of paragraphs,and their corresponding segment types.
The three types are procedural (how-to), ex-pository (essay), and narrative (in this case, spontaneous conversation).
For each type,173Computational Linguistics Volume 17, Number 2its segments are distinguished by bearing distinct relationships to the paragraph topic(which is central, but nowhere clearly defined).
Segments themselves are composedof clauses and regulated by "switching" patterns, uch as the question-answer patternand the remark-reply pattern.2.2 Our View of Paragraphs: An Informal SketchAlthough there are other discussions of the paragraph as a central element of discourse(e.g.
Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al 1980), allof them share a certain limitation in their formal techniques for analyzing paragraphstructure.
Discourse linguists how little interest in making the structural descriptionsprecise enough so that a computational grammar of text could adapt them and usethem.
Our interest, however, lies precisely in that area.We suggest that the paragraph is a grammatical nd logical unit.
It is the small-est linguistic representation f what, in logic, is called a "model," and it is the firstreasonable domain of anaphora resolution, and of coherent thought about a centraltopic.A paragraph can be thought of as a grammatical unit in the following sense: itis the discourse unit in which a functional (or a predicate-argument) structure can bedefinitely assigned to sentences/strings.
For instance, Sells (1985, p. 8) says that thesentence "Reagan thinks bananas," which is otherwise strange, is in fact acceptable ifit occurs as an answer to the question "What is Kissinger's favorite fruit?"
The pairingof these two sentences may be said to create a small paragraph.
Our point is that anacceptable structure can be assigned to the utterance "Reagan thinks bananas" onlywithin the paragraph inwhich this utterance occurs.
We believe that, in general, no unitlarger than a paragraph is necessary to assign a functional structure to a sentence, andthat no smaller discourse fragment, such as two (or one) neighboring sentences, willbe sufficient for this task.
That is, we can ask in the first sentence of a paragraph aboutKissinger's favorite fruit, elaborate the question and the circumstances in the next fewsentences, and give the above answer at the end.
We do not claim that a paragraph isnecessarily described by a set of grammar rules in some grammar formalism (althoughit may be); rather, it has the grammatical role of providing functional structures thatcan be assigned to strings.The logical structure of paragraphs will be analyzed in the next sections.
At thispoint we would like to present some intuitions that led to this analysis.
But first wewant to identify our point of departure.
In order to resolve anaphora nd to establishthe coherence or incoherence of a text, one must appeal to the necessary backgroundknowledge.
Hence, a formal analysis of paragraphs must include a formal descriptionof background knowledge and its usage.
Furthermore, this background knowledgecannot be treated as a collection of facts or formulas in some formal anguage, becausethat would preclude dealing with contradictory word senses, or multiple meanings.Secondly, this background knowledge is not infinite and esoteric.
In fact, to a large ex-tent it can be found in standard reference works such as dictionaries and encyclopedias.To argue for these points, we can consider the following paragraph: 1In the summer of 1347 a merchant ship returning from the Black Sea entered theSicilian port of Messina bringing with it the horrifying disease that came to beknown as the Black Death.
It struck rapidly.
Within twenty-four hours ofinfection and the appearance ofthe first small black pustule came an agonizingdeath.
The effect of the Black Death was appalling.
In less than twenty ears half1 J. Burke, The Day the Universe Changed.
1986.
Little, Brown & Co., Boston, Massachusetts, p. 55.174Zadrozny and Jensen Semantics of Paragraphsthe population of Europe had been killed, the countryside devastated, and aperiod of optimism and growing economic welfare had been brought o asudden and catastrophic end.The sentences that compose a paragraph must stick together; to put it more tech-nically, they must cohere.
This means very often that they show cohesion in the senseof Halliday (1976)--semantic l nks between elements.
Crucially, also, the sentences ofa paragraph must all be related to a topic.However, in the example paragraph, very few instances can be found here of theformal grammatical devices for paragraph cohesion.
There are no connectives, andthere are only two anaphoric pronouns (both occurrences of "it'0.
In each case, thereare multiple possible referents for the pronoun.
The paragraph is coherent because ithas a topic: "Black Death"; all sentences mention it, explicitly or implicitly.Notice that resolving anaphora precedes the discovery of a topic.
A few wordsabout his will illustrate the usage of background knowledge.
By parsing with syntacticinformation alone, we show that resolution of the first "it" reference hinges on theproper attachment of the participial clause "bringing within it... ".
If the "bringing"clause modifies "Messina," then "Messina" is the subject of '`bringing" and must bethe referent for "it."
If the clause modifies "port," then "port" is the desired referent;if the clause is attached at the level of the main verb of the sentence, then "ship" isthe referent.But syntactic relations do not suffice to resolve anaphora: Hobbs' (1976) algorithmfor resolving the reference of pronouns, depending only on the surface syntax ofsentences in the text, when applied to "it" in the example paragraph, fails in bothcases to identify the most likely referent NP.Adding selectional restrictions (semantic feature information, Hobbs 1977) doesnot solve the problem, because isolated features offer only part of the backgroundknowledge necessary for reference disambiguation.
Later, Hobbs (1979, 1982) proposeda knowledge base in which information about language and the world would beencoded, and he emphasized the need for using "salience" in choosing facts from thisknowledge base.We will investigate the possibility that the structure of this knowledge base canactually resemble the structure of, for example, natural language dictionaries.
Theprocess of finding referents could then be automated.Determining that the most likely subject for "bringing," in the first sentence, is thenoun "ship" is done in the following fashion.
The first definition for "bring" in W7(Webster's Seventh Collegiate Dictionary) is "to convey, lead, carry, or cause to come alongwith one..." The available possible subjects for "bringing" are "Messina," port," and"ship."
"Messina" is listed in the Pronouncing Gazetteer of W7, which means that itis a place (and is so identified in the subtitle of the Gazetteer).
So we can substitutethe word "place" for the word "Messina."
Then we check the given definitions for thewords "place," port," and "ship" in both dictionaries.
LDOCE (Longman Dictionary ofContemporary English) proves particularly useful at this point.
Definitions for "place"begin: "a particular part of space... ".
Definitions for "port" include: "harbour... "; "anopening in the side of a ship... ".
But the first entry for "ship" in LDOCE reads "a largeboat for carrying people or goods... ".
This demonstrates a very quick connection withthe definition for the verb "bring," since the word "carry" occurs in both definitions.It requires much more time and effort to find a connection between "bring" and eitherof the other two candidate subject words "place" or "port."
Similar techniques can beused to assign "disease" as the most probable referent for the second "it" anaphor inour example paragraph.175Computational Linguistics Volume 17, Number 2Equally significant in this instance is the realization that a dictionary points tosynonym and paraphrase r lations, and thereby verifies the cohesiveness of the pas-sage.
Through the dictionary (LDOCE again), we establish shared-word relationshipsbetween and among the words "disease," "Black death," infection," "death," "killed,"and "end."
Note that there is no other means, short of appealing to human under-standing or to some hand-coded body of predicate assertions, for making these rela-tionships.This demonstrates that information eeded to identify and resolve anaphoric ref-erences can be found, to an interesting extent, in standard ictionaries and thesauri.
(Other reference works could be treated as additional sources of world knowledge.
)This type of consultation uses existing natural language texts as a referential evel forprocessing purposes.
It is the lack of exactly this notion of referential level that hasstood in the way of other linguists who have been interested in the paragraph as aunit.
Crothers (1979, p. 112), for example, bemoans the fact that his "theory lacks aworld knowledge component, a mental 'encyclopedia,' which could be invoked to gen-erate inferences... "  With respect to that independent source of knowledge, our maincontributions are two.
First, we identify its possible structure (a collection of partiallyordered theories) and make formal the choice of a most plausible interpretation.
Inother words, we recognize it as a separate logical evel--the referential level.
Second,we suggest that natural language reference works, like dictionaries and thesauri, canquite often fill the role of the referential level.3.
The Logic of ReferenceThe goal of this section is to introduce a formalism describing how background knowl-edge is used in understanding text.
The term "logic of reference" denotes a formaldescription of this process of consulting various sources of information in order toproduce an interpretation of a text.
The formalist will be presented in a number ofsteps in which we will elaborate one simple example:Example 1Entering the port, a ship brought a disease.This sentence can be translated into the logical formula (ignoring only the pasttense of "bring" and the progressive of "enter'9:Definition S: enter(xl~ x2) & ship(x1) & port(x2) & bring(x3~ x4) & disease(x4) & xl  = s& x2 = m & x3 = s & x4 = d,where s, m, d, are constants.We adopt he three-level semantics as a formal tool for the analysis of paragraphs.This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework fordefault and commonsense r asoning.
It should not come as a surprise that we cannow use this apparatus for text/discourse analysis; after all, many natural anguageinferences are based on defaults, and quite often they can be reduced to choosing mostplausible interpretations of predicates.
For instance, relating "they" to "apples" in thesentence (cf.
Haugeland 1985 p. 195; Zadrozny 1987a):We bought the boys apples because they were so cheap176Zadrozny and Jensen Semantics of Paragraphscan be an example of such a most plausible choice.The main ideas of the three-level semantics can be stated as follows:1.
Reasoning takes place in a three-level structure consisting of an objectlevel, a referential evel, and a metalevel.2.
The object level is used to describe the current situation, and in our caseis reserved for the formal representation f paragraph sentences.
For thesake of simplicity, the object level will consist of a first order theory.3.
The referential level, denoted by R, consists of theories representingbackground knowledge, from which information relevant o theunderstanding of a given piece of text has to be extracted.
It constrainsinterpretations of the predicates of an object theory.
Its structure and theextraction methods will be discussed below.4.
Understanding has as its goal construction of an interpretation of thetext, i.e.
building some kind of model.
Since not all logically permissiblemodels are linguistically appropriate, one needs a place, namely themetalevel, to put constraints on types of models.
Gricean maxims belongthere; Section 6 will be devoted to a presentation of the metalevel rulescorresponding to them.We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that naturallanguage programs, such as on-line grammars and dictionaries, can be used as referen-tial levels for commonsense r asoning--for example, to disambiguate PP attachment.This means that information contained in grammars and dictionaries can be used toconstrain possible interpretations of the logical predicates of an object-level theory.The referential structures we are going to use are collections of logical theories,but the concept of reference is more general.
Some of the intuitions we associate withthis notion have been very well expressed by Turner (1987, pp.
7-8):... Semantics i constrained by our models of ourselves and our worlds.
We havemodels of up and down that are based by the way our bodies actually function.Once the word "up" is given its meaning relative to our experience with gravity,it is not free to "slip" into its opposite.
"Up" means up and not down .
.
.
.
Wehave a model that men and women couple to produce offspring who are similarto their parents, and this model is grounded in genetics, and the semantics ofkinship metaphor is grounded in this model.
Mothers have a different role thanfathers in this model, and thus there is a reason why "Death is the father ofbeauty" fails poetically while "Death is the mother of beauty" succeeds ....It is precisely this "grounding" of logical predicates in other conceptual structuresthat we would like to capture.
We investigate here only the "grounding" in logical the-ories.
However, it is possible to think about constraining linguistic or logical predicatesby simulating physical experiences (cf.
Woods 1987).We assume here that a translation of the surface forms of sentences into a logicalformalism is possible.
Its details are not important for our aim of giving a semanticinterpretation of paragraphs; the main theses of our theory do not depend on a logicalnotation.
So we will use a very simple formalism, like the one above, resembling thestandard first order language.
But, obviously, there are other possibilities--for instance,the discourse representation structures (DRS's) of Kamp (1981), which have been usedto translate a subset of English into logical formulas, to model text (identified with a listof sentences), to analyze a fragment of English, and to deal with anaphora.
The logical177Computational Linguistics Volume 17, Number 2notation of Montague (1970) is more sophisticated, and may be considered another pos-sibility.
Jackendoff's (1983) formalism is richer and resembles more closely an Englishgrammar.
Jackendoff (1983, p. 14) writes "it would be perverse not to take as a workingassumption that language is a relatively efficient and accurate ncoding of the infor-mation it conveys."
Therefore a formalism of the kind he advocates would probably bemost suitable for an implementation f our semantics.
It will also be a model for oursimplified logical notation (cf.
Section 5).
We can envision asystem that uses data struc-tures produced by a computational grammar to obtain the logical form of sentences.3.1 Finite Representations, Finite TheoriesUnless explicitly stated otherwise, we assume that formulas are expressed in a certain(formal) language L without equality; the extension L(=) of L is going to be used onlyin Section 5 for dealing with noun phrase references.
This means that natural languageexpressions such as "A is B," "A is the same as B," etc.
are not directly represented bylogical equality; similarly, "not" is often not treated as logical negation; cf.
Hintikka(1985).All logical notions that we are going to consider, such as theory or model, willbe finitary.
For example, a model would typically contain fewer than a hundred ele-ments of different logical sorts.
Therefore these notions, and all other constructs weare going to define (axioms, metarules, definitions etc.)
are computational, lthoughusually we will not provide explicit algorithms for computing them.
The issues ofcontrol are not so important for us at this point; we restrict ourselves to describingthe logic.
This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff(1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu-tational linguistics.
As a logical postulate it is not very radical; it is possible within afinitary framework to develop that part of mathematics that is used or has potentialapplications in natural science, such as mathematical analysis (cf.
Mycielski 1981).On the other hand, a possible obstacle to our strategy of using only finite objects isthe fact that the deductive closure of any set of formulas is not finite in standard logic,while, clearly, we will have to deduce new facts from formal representations of textand background knowledge.
But there are several ways to avoid this obstruction.
Forexample, consider theories consisting of universal formulas without function symbols.Let Th(T) of such a theory T be defined as T plus ground clauses/sets of literals prov-able from T in standard logic.
It is easily seen that it is a closure, i.e.
Th(Th(T)) = Th(T);and obviously, it is finite, for finite T. It makes ense then to require that logical conse-quences of paragraph sentences have similar finite representations.
However, in ordernot to limit the expressive power of the formal language, we should proceed in aslightly different manner.
The easiest way to achieve the above requirement is by pos-tulating that all universes of discourse are always finite, and therefore all quantifiersactually range over finite domains.
In practice, we would use those two and othertricks: we could forbid more than three quantifier changes, because ven in mathe-matics more than three are rare; we could restrict he size of universes of discourse tosome large number such as 1001; we could allow only a fixed finite nesting of func-tion symbols (or operators) in formulas; etc.
The intention of this illustration was toconvince the reader that we now can introduce the following set of definitions.Definitions?
A theory is a finite set of sentences Sent (formulas without free variablesin some formal anguage).178Zadrozny and Jensen Semantics of Paragraphs?
A deductive closure operator is a function Th : P(Sent) --* P(Sent)(a) T c Th(T), for any T,(b) Th(Th(T)) = Th(T),(c) Th(T) is finite, for finite T; additionally, we require it to beground, for ground T.?
A theory T is consistent if there is no formula ~ such that both ~ and -~belong to Th(T) (and inconsistent o herwise).?
A model of a theory T is defined, as usual, as an interpretation defined ona certain domain, which satisfies all formulas of T. The collection of all(finite) models of a theory T will be denoted by Mods(T).?
The set of all subformulas of a collection of formulas F is denoted byForm(F).
~ is a ground instance of a formula G if ~ contains no variables,and ~ = ~,  for some substitution 0.Thus, we do not require Th(T) to be closed under substitution i stances of tautologies.Although in this paper we take modus ponens as the main rule of inference, in generalone can consider deductive closures with respect o weaker, nonstandard logics, (cf.Levesque 1984; Frisch 1987; Patel-Schneider 1985).
But we won't pursue this topicfurther here.3.2 The Structure of Background KnowledgeBackground knowledge is not a simple list of meaning postulates--it has a structureand it may contain contradictions and ambiguities.
These actualities have to be takeninto account in any realistic model of natural language understanding.
For instance, theverb "enter" is polysemous.
But, unless context specifies otherwise, "to come in" is amore plausible meaning than "to join a group."
Assuming some logical representationof this knowledge, we can write thatenter(x, y) --* {come_in (x~ y); place(y)}enter(x, y) --* {join(x, y)&group(y);.
.
.
}(el)(e2)and e2 ~enter el.Two things should be explained now about this notation:Meanings of predicates/words are represented on the right-hand sides ofthe arrows as collections of formulas--i.e., theories.
The main idea is thatthese mini-theories of predicates appearing in a paragraph will jointlyprovide enough constraints o exclude implausible interpretations.
(Onecan think of meanings as regions in space, and of constraints as sets oflinear inequalities approximating these regions).
How this can be done,we will show in a moment.These theories are partially ordered; and their partial orders are writtenas  <enter(x,y), or <enter, or <i, or simply <, depending on context.
This isour way of making formal the asymmetry of plausibility of differentmeanings of a predicate.
Again, a way of exploiting it will be shownbelow.179Computational Linguistics Volume 17, Number 2DefinitionA referential level R is a structureR = {(?, <?)
: ?
E Formulae}where---for each ?
- -  <?
is a partially ordered (by a relation ofplausibility) collection of implications ?
~ T?.The term ?
--* T stands for the theory {?
--* ~- : ~- E T}, and~) ---* {?1, ?2, ' '  "} abbreviates {?
--* ?1, ?
--* ?2, .
.
.
}.It is convenient to assume also that all formulas, except "laws"--whichare supposed to be always true---have the least preferred emptyinterpretation ~.We suppose also that interpretations are additionally ranked accordingto the canonical partial ordering on subformulas.
The ranking provides anatural method of dealing with exceptions, as in the case of finding aninterpretation of ~ & p & fl with R containing (~ --* y), (c~ & fl ~ -~y),where -~y would be preferred to ywif both are consistent, and bothdefaults are equally preferred.
This means that preference is given tomore specific information.
For instance, the sentence The officer went outand struck the flag will get the reading "lowered the flag," if theappropriate theory of strike(x, y)&flag(y) is part of backgroundknowledge; if not, it will be understood as "hit the flag.
"The referential level (R, <) may contain the theories listed below.
Since we viewa dictionary as an (imperfect) embodiment of a referential level, we have derived theformulas in every theory T?
from a dictionary definition of the term ?.
We believe thateven such a crude model can be useful in practice, but a refinement of this model willbe needed to have a sophisticated theory of a working natural anguage understandingsystem.enter(x, y) --.
{come_in(x, y); place(y);...} (el)/* enter--to come into a place */enter(x, y) --* join(x, y)&group(y); typically : professionals(y)} (e2)/* enter--to join a group; typically of professionals */ship(x) --* {large_boat(x); 3y carry(x, y)&(people(y) v goods(y));...} (shl)/* ship--a large boat for carrying people or goods on the sea */ship(x) ---, {(large,aircraft(x) V space_vehicle(x));...}bring(x, y) --* {carry(x, y);...}bring(x, y) --.
{cause(x, y);...}disease(y) ---, {illness(y);...}/* disease(sh2)(bl)/* bring--to carry */(b2)/* bring--to cause */(de1)illness caused by an infection */180Zadrozny and Jensen Semantics of Paragraphsdisaster(y) ~ {.
.
.
; 3x cause(y, x)&harm(x) .
.
.
}port(x) ~ {harbor(x);.
.
.
}( .
.
.
)(dr1)/* disaster---causes a harm */(pl)/* port--harbor */Note: We leave undefined the semantics of adverbs uch as typically in (e2).
This ad-verb appears in the formula s an operator; our purpose in choosing this representationis to call the reader's attention to the fact that for any real applications the theorieswill have to be more complex and very likely written in a higher order language (cf.Section 4).The theories, which we describe here only partially, restricting ourselves to theirrelevant parts, represent the meanings of concepts.
We assume as before that (el) ismore plausible than (e2), i.e.
e2 <enter el; similarly, for (shl), (sh2) and <ship, etc.
Thisparticular ordering of theories is based on the ordering of meanings of the correspond-ing words in dictionaries (derived and less frequent meanings have lower priority).But one can imagine deriving such orderings by other means, such as statistics.The partial order <enter has the theories {el, e2, ~} as its domain; ~ is the leastpreferred empty interpretation corresponding to our lack of knowledge about thepredicate; it is used when both (el) and (e2) are inconsistent with a current objecttheory.
The domain is ordered by the relation of preference ~<enter e2 <enter el.
Thetheory (el) will always be used in constructing theories and models of paragraphsin which the expression "enter" (in any grammatical form) appears, unless assumingit would lead to an inconsistency.
In such a case, the meaning of "to enter" wouldchange to (e2), or some other theory belonging to R.We would like to stress three points now: (1) the above implications are basedon the definitions that actually occur in dictionaries; (2) the ordering can actually befound in some dictionaries--it is not our own arbitrary invention; (3) it is natural totreat a dictionary definition as a theory, since it expresses "the analysis of a set offacts in their relation to one another," different definitions corresponding to possibledifferent analyses.
(Encyclopedia articles are even more theory-like.)
In this sense, thenotion of a referential level is a formalization of a real phenomenon.Obviously, dictionaries or encyclopedias do not include all knowledge an agentmust have to function in the world, or a program should possess in order to under-stand any kind of discourse.
Although the exact boundary between world knowledgeand lexical knowledge is impossible to draw, we do know that lexicons usually containvery little information about human behavior or temporal relations among objects ofthe world.
Despite all these differences, we may assume that world knowledge andlexical knowledge (its proper subset) have a similar formal structure.
And in the ex-amples that we present, it is the structure that matters.3.3 How to Use Background KnowledgeThe next few pages will be devoted to an analysis of the interaction of backgroundknowledge with a logical representation f a text.
We will describe two modes of suchan interaction; both seem to be present in our understanding of language.
One exploitsdifferences in plausibility of the meanings of words and phrases, in the absence ofcontext (e.g., the difference between a central and a peripheral sense, or between afrequent and a rare meaning).
The other one takes advantage of connections betweenthose meanings.
We do not claim that this is the only possible such analysis; rather,181Computational Linguistics Volume 17, Number 2we present a formal model which can perhaps be eventually disproved and replacedby a better one.
As far as we know, this is the first such formal proposal.3.3.1 Dominance.
In Figure I the theories of "enter," ship," etc.
and the partial ordersare represented graphically; more plausible theories are positioned higher.
A paththrough this graph chooses an interpretation of the sentence S. For instance, the pathf int  = {el, shl~ pl, bl, dl} and S say together thatA large boat (ship) that carries people or goods came into the harbor and carried adisease (illness).Since it is the "highest" path, fint is the most plausible (relative to R) interpretation ofthe words that appear in the sentence.
Because it is also consistent, it will be chosenas a best interpretation of S, (cf.
Zadrozny 1987a, 1987b).
Another theory, consistingof f~ = {el, sh2, pl, b2~ dl} and S, saying thatA space vehicle came into the harbor and caused a disease~illnessis less plausible according to that ordering.
As it turns out, f~ is never constructed inthe process of building an interpretation of a paragraph containing the sentence S,unless assuming fint would lead to a contradiction, for instance within the higher levelcontext of a science fiction story.The collection of these most plausible consistent interpretations of a given theoryT is denoted by PT< (T).
Then fint belongs to PT< (Th({S})), but this is not true for f'.Note: One should remember that, in general, because all our orderings are partial,there can be more than one most plausible interpretation of a sentence or a paragraph,and more that one "next best" interpretation.
Moreover, to try to impose a total orderon all the paths (i.e.
the cartesian product defined in Section 3.3.2) would be a mistake;it would mean that ambiguities, represented in our formalism by existence of morethan one ("best'0 interpretation of a text, are outlawed.3.3.2 Cartesian Products.
Formalization of the interpretation produced in 3.3.1. is pre-sented below.
Any path through the graph of Figure 1 is an element .of the cartesianproduct I'I~Esubformulas(S) (~I, of the partial orderings.Figure 2 explains the geometric intuitions we associate with the product and theordering.
The product itself is given by the following definition:DefinitionLet F be a collection of formulas ~e, e G m, for some natural number m; and let, foreach e, <e be a partial ordering on the collection theoriesof ~be.
Define:{?e .... ,?e G,}I I(F) ~- 1--\[ (e= 0c: (Ve G m)(31 < ne)~(e) = ~de -'~ Z~\]}e~mWe denote by < the partial order induced on II(F) by the orderings <e and the canoni-cal ordering of subformulas (a formula is "greater" than its subformulas).
The geomet-rical meaning of this ordering can be expressed as "higher paths are more importantprovided they pass through the most specific nodes.
"182Zadrozny and Jensen Semantics of Paragraphs/ / /Sf in t~ .,..,,~1.e I c - - - - -s  h I bit _.. i f l .
-  j "~  J ~.....,,1~1e2 sh2, b2" a aI I Ia a QFigure 1The partial ordering of theories of the referential level R and the ordering of interpretations.Since (shl) and (bl) dominate (respectively) (sh2) and (b2), the path f, nt represents a moreplausible interpretation than ft.<1 <2 <5"flag" "strike" "strike ~ flag"(2)"cloth" "hit/anger" "lower" j (I)(4)"tail" (3)"music"Figure 2The cartesian product I-I <i=-<1 x <2 x K3 can be depicted as a collection of all paths throughthe graphs representing the partial orderings; a path chooses only one element from eachordering--thus (1) and (2) are "legal" paths, while (4) is not.
Also, more plausible theoriesappear higher: "cloth" > "music" > ~.
More specific paths are preferred: Assuming that allhigher paths, like path (1), are excluded by inconsistency, path (2) is the most plausibleinterpretation f c~&fl, and it is preferred to (3).
(More explanations in the text).To make Figure 2 more intuitive we assigned some meanings to the partial orders.Thus, (1 represents some possible meanings of "flag," shown with the help of the "keywords;" the meaning "piece of cloth" preferred to "deer's tail."
The word "strike" hasdozens of meanings, and we can imagine that the meaning of the transitive verb beingrepresented by (2, with "hit in anger" at the top, then "hit, e.g.
a ball" and "discover"equally preferred, and then all other possible meanings.
The trivial (3 representing183Computational Linguistics Volume 17, Number 2"strike a flag" should remind us that we already know all that from Section 3.2.
Noticethat path (2) does not give us the correct interpretation f "strike the flag," which iscreated from "cloth"~-"lower.
"Each element of the cartesian product I-\[ <i represents a set of possible meanings.These meanings can be combined in various ways, the simplest of which consists oftaking their union as we did in 3.3.1.
But a paragraph isn't just a sum of its sentences,as a sentence isn't simply a concatenation f its phrases.
The cohesion devices--suchas "but," "unless," since"--arrange s ntences together, and they also have semanticfunctions.
This is reflected, for instance, in the way various pieces of backgroundknowledge are pasted together.
Fortunately, at this point we can abstract from this byintroducing an operator variable ?
whose meaning will be, as a default, that of a settheoretic union, U; but, as we describe it in Section 6.2, it can sometimes be changedto a more sophisticated join operator.
There, when considering the semantics of "but,"we'll see that referential level theories can be combined in slightly more complicatedways.
In other words, a partial theory corresponding to a paragraph cannot be just asum of the theories of its sentencesIthe arrangement of those theories hould obeythe metalevel composition rules, which give the semantics of connectives.
However,from a purely formal point of view, @ can be any function producing a theory from acollection of theories.The cartesian product represents all possible amalgamations of these elementarytheories.
In other words, this product is the space of possible combinations of mean-ings, some of which will be inconsistent with the object level theory T. We can imme-diately exclude the inconsistent combinations, eliminating at least some nonsense:I:I(F) = {f E II(F) : ?f is consistent with T}It remains now to fill in the details of the construction of PT<.
We assume that a textP can be translated into a (ground) theory/5 (a set of logical sentences); T = Th(P) isthe set of logical consequences of P. We denote by F the set Form(Th(_fi))--the set of allsubformulas of Th(/5), about which we shall seek information at the referential levelR.
If F = {~bl(C'~),... ~b,(C'n)} (~/is a collection of constants that are arguments of ~bi),is this theory, we have to describe a method of augmenting it with the backgroundknowledge.
We can assume without loss of generality that each ~i(~i) in F has, in R, acorresponding partial order <i of theories of ~i(xi).
We now substitute the constants c'ifor the variables xi inside the theories of <i.
With a slight abuse of notation, we will usethe same symbol <i for the new ordering.
The product spaces II(F) and I:I(F) can thenbe defined as before, with the new orderings in place of the ones with variables.
Noticethat if only some of the variables of ~bi(~i) were bound by c'i, the same constructionwould work.
We have arrived then at a general method of linking object level formulaswith their theories from R.Now we can define PT< (T) of the theory T as the set of most likely consistenttheories of T given by (H(F), <), where F = Form(T):PT<(T) = {TUT' :T '= ?f and f is a maximal element of (I~I(F), <)}Notice that PT< (T) can contain more than one theory, meaning that T is ambiguous.This is a consequence of the fact that the cartesian product is only partially orderedby <.
The main reason for using ground instances ~i(Ci) in modifying the orderingsis the need to deal with multiple occurrences of the same predicate, as inJohn went to the bank by the bank.184Zadrozny and Jensen Semantics of ParagraphsThe above construction is also very close in spirit to Poole's (1988) method for defaultreasoning, where object theories are augmented by ground instances of defaults.3.3.3 Coherence Links.
The reasoning that led to the intended interpretation fi t in ourdiscussion of dominance was based on the partial ordering of the theories of R. Wewant to exploit now another property of the theories of R--their coherence.
Findingan interpretation for a natural anguage text or sentence typically involves an appealto coherence.
Consider$2: Entering the port, a ship brought adisaster.Using the coherence link between (b2) and (dr1) (cf.
Section 3.2)--the presence ofcause(*, ,) in the theories of "bring" and "disaster"--we can find a partial coherentinterpretation T E PTc(Th({S2})) of $2.
In this interpretation, theories explaining themeanings of terms are chosen on the basis of shared terms.
This makes (b2) ("to bring"means "to cause") plausible and therefore it would be included in T. The formalizationof all this is given below:Definitions?
The set of all theories about the formulas of T is defined as:Here, we ignore the ordering, because we are interested only inconnections between concepts (represented by words).?
If t, t r E G(T), t ~ t I, share a predicate, we say that there is a c-linkbetween t and tC A c-path is defined as a chain of c-links; i.e.
ift = ~ --* T and t ~ = ~' --* T' belong to a c-path, then ~ ~ ~'.Under this condition, for any predicate, only one of its theories willbelong to a c-path.
A c-path therefore chooses only one meaning for eachterm.?
C(T) will denote the set of all c-paths in G(T) consistent with T, i.e.
foreach p E C(T), Op td T is consistent.This construction is like the one we have encountered when definingI~I(T).
The details should be filled out exactly as before; we leave this tothe reader.?
We define PTc(T) of a theory T as the set of most coherent consistenttheories of T given by C(T):PTc(T) = {T U T' : T' -- ?p and p is a C - maximal element of C(T)}Going back to $2, PTc(Th(S2)) contains also the interpretation based onthe coherence link between "ship" and "bring," which involves "carry.
"Based on the just-described coherence relations, we conclude thatsentence $2 is ambiguous; it has two interpretations, based on the twosenses of "bring."
Resolution of ambiguities involves factors beyond thescope of this section--for instance, Gricean maxims and topic (Section 6),or various notions of context (cf.
Small et al 1988).
We will continue the185Computational Linguistics Volume 17, Number 2topic of the interaction of object level theories with backgroundknowledge by showing how the two methods of using backgroundknowledge can be combined.3.3.4 Partial Theories.
A finer interpretation of an object level theory T--its partialtheory--is obtained by the iteration:PT(T) = PT< (PTc(Th(T) )PT is well defined after we specify that PT of a set of theories is the set of the PTs(for both < and C):PT{~( {T~, T2,...} ) = PT(~(T1) U PT,\](T2) U...Notice that coherence does not decide between (el) and (e2) given the above R, butthe iteration produces two theories of $2, both of which assert that the meaning of"ship entered" is "ship came.
"A ship~boat came into the harbor/port and caused~brought a disaster.A ship~boat came into the harbor/port and carried/brought a disaster.PT({S1}) contains only one interpretation based on fint"A ship~boat came into the harbor~port and carried~brought a disease.Partial theories will be the main syntactic onstructs in the subsequent sections.
Inparticular, the p-models will be defined as some special models or partial theories ofparagraphs.3.4 Summary and DiscussionWe have shown that finding an interpretation of a sentence depends on two graph-theoretical properties--coherence and dominance.
Coherence is a purely "associative"property; we are interested only in the existence of links between represented con-cepts/theories.
Dominance uses the directionality of the partial orders.A partial theory PT(T) of an object theory T corresponding to a paragraph isobtained by joining most plausible theories or sentences, collocations, and words ofthe paragraph.
However, this simple picture must be slightly retouched to accountfor semantic roles of inter- and intra-sentential connectives uch as "but," and toassure consistency of the partial theory.
These modifications have complicated thedefinitions a little bit.
The above definitions capture the fact that even if, in principle,any consistent combination of the mini-theories about predicates can be extended toan interpretation, we are really interested only in the most plausible ones.
The theoryPT(T) is called "partial" because it does not contain all knowledge about predicates--less plausible properties are excluded from consideration, although they are accessibleshould an inconsistency appear.
Moreover, the partiality is related to the unutilizedpossibility of iterating the operator PT (cf.
Section 4).How can we now summarize what we have learned about he three logical levels?To begin with, one should notice that they are syntactically distinct.
If object leveltheories are expressed by collections of first order formulas, metalevel definitions--e.g., to express as a default hat ?
is a set theoretical union--require another language,186Zadrozny and Jensen Semantics of Paragraphssuch as higher order logic or set theory, where one can define predicates dealingwith models, consistency, and provability.
Even if all background knowledge weredescribed, as in our examples, by sets of first order theories, because of the preferencesand inconsistencies of meanings, we could not treat R as a flat database of facts--sucha model simply would not be realistic.
Rather, R must be treated as a separate logicallevel for these syntactic reasons, and because of its function--being a pool of possiblyconflicting semantic onstraints.The last point may be seen better if we look at some differences between oursystem and KRYPTON, which also distinguishes between an object heory and back-ground knowledge (cf.
Brachman et al 1985).
KRYPTON's A-box, encoding the objecttheory as a set of assertions, uses standard first order logic; the T-box contains informa-tion expressed in a frame-based language quivalent to a fragment of FOL.
However,the distinction between the two parts is purely functional--that is, characterized interms of the system's behavior.
From the logical point of view, the knowledge baseis the union of the two boxes, i.e.
a theory, and the entailment is standard.
In oursystem, we also distinguish between the "definitional" and factual information, butthe "definitional" part contains collections of mutually excluding theories, not justof formulas describing a semantic network.
Moreover, in addition to proposing thisstructure of R, we have described the two mechanisms for exploiting it, "coherence"and "dominance," which are not variants of the standard first order entailment, butabduction.The idea of using preferences among theories is new, hence it was described inmore detail.
"Coherence," as outlined above, can be understood as a declarative (orstatic) version of marker passing (Hirst 1987; Charniak 1983), with one difference: theactivation spreads to theories that share a predicate, not through the IS-A hierarchy,and is limited to elementary facts about predicates appearing in the text.The metalevel rules we are going to discuss in Section 6, and that deal with theGricean maxims and the meaning of "but," can be easily expressed in the languagesof set theory or higher order logic, but not everything expressible in those languagesmakes sense in natural language.
Hence, putting limitations on the expressive powerof the language of the metalevel will remain as one of many open problems.4.
Coherence of ParagraphsWe are now in a position to use the notion of the referential level in a formal definitionof coherence and topic.
Having done that, we will turn our attention to the resolutionof anaphora, linking it with the provability relation (abduction) t-R+M and a metarulepostulating that a most plausible model of a paragraph is one in which anaphors havereferences.
Since the example paragraph we analyze has only one connective ("and"),we can postpone a discussion of connectives until Section 6.Building an interpretation f a paragraph does not mean finding all of its possiblemeanings; the implausible ones should not be computed at all.
This viewpoint hasbeen reflected in the definition of a partial theory as a most plausible interpretationof a sequence of predicates.
Now we want to restrict he notion of a partial theory byintroducing the formal notions of topic and coherence.
We can then later (Section 5.2)define p-models--a category of models corresponding to paragraphs--as models ofcoherent theories that satisfy all metalevel conditions.The partial theories pick up from the referential level the most obvious or themost important information about a formula.
This immediate information may beinsufficient to decide the truth of certain predicates.
It would seem therefore thatthe iteration of the PT operation to form a closure is needed (cf.
Zadrozny 1987b).187Computational Linguistics Volume 17, Number 2However, there are at least three arguments against iterating PT.
First of all, iterationwould increase the complexity of building a model of a paragraph; infinite iterationwould almost certainly make impossible such a construction i real time.
Secondly, thecooperative principle of Grice (1975, 1978), under the assumption that referential levelsof a writer and a reader are quite similar, implies that the writer should structure thetext in a way that makes the construction of his intended model easy for the reader;and this seems to imply that he should appeal only to the most direct knowledge of thereader.
Finally, it has been shown by Groesser (1981) that the ratio of derived to explicitinformation ecessary for understanding a piece of text is about 8:1; furthermore, ourreading of the analysis of five paragraphs by Crothers (1979) strongly suggests thatonly the most direct or obvious inferences are being made in the process of building amodel or constructing a theory of a paragraph.
Thus, for example, we can expect hatin the worst case only one or two steps of such an iteration would be needed to findanswers to wh-questions.Let P be a paragraph, let /3 = ($1,..., Sn) be its translation into a sequence oflogical formulas.
The set of all predicates appearing in X will be denoted by Pred(X).DefinitionLet T be a partial theory of a paragraph P. A sequence of predicates appearing in i6,denoted by Tp, is called a topic of the paragraph P, if it is a longest sequence satisfyingthe conditions (1) and (2) below:1.
For all "sentences" Si, (a) or (b) or (c) holds:(a) Direct reference to the topic:Tp C Pred(Si)(b) Indirect reference to the topic:If ?
E Pred(Si) & (?
-~ T?)
E T, then Tp C Pred(T?
)(c) Direct reference to a previous entence:If ?
E Pred(Si) & (~ ~ T?)
E T then Pred(Si_l)MPred(?
~ TV~ ) # 9~2.
Either (i) or (ii) is satisfied:(i) Existence of a topic sentence: Tp C Pred(Si), for some sentence Si;(ii) Existence of a topic sentence: a theory of Tp belongs to R, i.e.
if 0 isthe conjunction of predicates of Tp then 0 ~ To E R, forsome To.The last two conditions ay that either the discussed concept (topic) already exists inthe background knowledge or it must be introduced in a sentence.
For instance, wecan see that the sentence The effect of the Black Death was appalling can be assumed tobe a topic sentence.The first three conditions make the requirements for a collection of sentences tohave a topic.
Either every sentence talks about he topic (as, for instance, the first twosentences of the paragraph about the Black Death), or a sentence refers to the topic188Zadrozny and Jensen Semantics of Paragraphsthrough background knowledge----the topic appears in a theory about an entity or arelation of the sentence (in the case of Within twenty-four hours of infection .
.
.
.
"infec-tion" can be linked to "disease'--cf.
Sections 2 and 4.2), or else a sentence laboratesa fragment of the previous sentence (the theme The effect of... being developed in Inless than... ).The definition allows a paragraph to have more than one topic.
For instance, aparagraph consisting ofJohn thinks Mary is pretty.
John thinks Mary is intelligent.
John wants to marryher.can be either about {John(\]), Mary(m), think(j, m, pretty(m))}, or about John, Mary, andmarrying.
(Notice that the condition 2 (i) forbids us merging the two topics into alarger one).
Thus paragraphs can be ambiguous about what constitutes their topics.The point is that they should have one.It is also clear that what constitutes a topic depends on the way the content ofparagraph sentences i represented.
In the last case, if "pretty" were translated into apredicate, and not into a modifier of m (i.e.
an operator), "John thinking about Mary"could not be a topic, for it wouldn't be the longest sequence of predicates atisfyingthe conditions (1) and (2).We'd like to put forward a hypothesis that this relationship between topics andrepresentations can actually be useful: Because the requirement that a well-formedparagraph should have a topic is a very natural one (and we can judge pretty wellwhat can be a topic and what can't), we can obtain a new method for judging semanticrepresentations.
Thus, if the naive first order representation containing pretty(m) asone of the formulas gives a wrong answer as to what is the topic of the above, oranother, paragraph, we can reject it in favor of a (higher order) representation iwhich adjectives and adverbs are operators, not predicates, and which provides uswith an intuitively correct opic.
Such a method can be used in addition to the standardcriteria for judging representations, such as elegance and ability to express emanticgeneralizations.DefinitionA partial theory T E PT(P) of the paragraph P is coherent iff the paragraph P has atopic.A random permutation of just any sentences about a disease wouldn't be coher-ent.
But it would be premature to jump to the conclusion that we need more than justexistence of a topic as a condition for coherence.
Although it may be the case that itwill be necessary in the future to introduce notions like "temporal coherence," "deicticcoherence," or "causal coherence," there is no need to start multiplying beings now.We can surmise that the random permutations we talk about would produce an in-consistent theory; hence, the temporal, causal, and other aspects would be dealt withby consistency.
But of course at this point it is just a hypothesis.An important aspect of the definition is that coherence has been defined as a prop-erty of representation--in our case, it is a property of a formal theory.
The existence ofthe topic, the direct or indirect allusion to it, and anaphora (which will be addressedbelow) take up the issue of formal criteria for a paragraph definition, which was raised189Computational Linguistics Volume 17, Number 2by Bond and Hayes (1983) (cf.
also Section 2.1).
The question of paragraph length canprobably be attended to by limiting the size of p-models, perhaps after introducingsome kind of metric on logical data structures.Still, our definition of coherence may not be restrictive nough: two collectionsof sentences, one referring to "black" (about black pencils, black pullovers, and blackpoodles), the other one about "death" (war, cancer, etc.
), connected by a sentencereferring to both of these, could be interpreted as one paragraph about he new, broadertopic "black + death."
This problem may be similar to the situation in which currentformal grammars allow nonsensical but parsable collections of words (e.g., "colorlessgreen ideas... '9, while before the advent of Chomskyan formalisms, a sentence wasdefined as the smallest meaningful collection of words; Fowler (1965, p. 546) gives 10definitions of a sentence.It then seems worth differentiating between the creation of a new concept like"black + death," with a meaning given by a paraphrase of the example collection ofsentences, and the acceptance of the new concept--storing it in R. In our case theconcept "black + death," which does not refer to any normal experiences, would bediscarded as useless, although the collection of sentences would be recognized as astrange, even if coherent, paragraph.We can also hope for some fine-tuning of the notion of topic, which would preventmany offensive xamples.
This approach is taken in computational syntactic grammars(e.g.
Jensen 1986); the number of unlikely parses is severely reduced whenever pos-sible, but no attempt is made to define only the so-called grammatical strings of alanguage.Finally, as the paragraph is a natural domain in which word senses can be reliablyassigned to words or sentences can be syntactically disambiguated, larger chunks ofdiscourse may be needed for precise assignment of topics, which we view as anothertype of disambiguation.
Notice also that for coherence, as defined above, it does notmatter whether the topic is defined as a longest, a shortest, or--simply--a sequence ofpredicates satisfying the conditions (1) and (2); the existence of a sequence is equivalentwith the existence of a shortest and a longest sequence.
The reason for choosing alongest sequence as the topic is our belief that the topic should rather contain moreinformation about a paragraph than less.4.1 Comparison with Other ApproachesAt this point it may be proper to comment on the relationship between our theory ofcoherence and theories advocated by others.
We are going to make such a comparisonwith the theories proposed by J. Hobbs (1979, 1982) that represent a more computa-tionally oriented approach to coherence, and those of T.A.
van Dijk and W. Kintch(1983), who are more interested in addressing psychological nd cognitive aspects ofdiscourse coherence.
The quoted works seem to be good representatives for each ofthe directions; they also point to related literature.The approach we advocate is compatible with the work of these researchers, webelieve.
There are, however, some interesting differences: first of all, we emphasize therole of paragraphs; econd, we talk about formal principles regulating the organizationand use of knowledge in language understanding; and third, we realize that naturallanguage text (such as an on-line dictionary) can, in many cases, provide the type ofcommonsense background information that Hobbs (for example) advocated but didn'tknow how to access.
(There are also some other, minor, differences.
For instance, ourthree-level semantics does not appeal to possible worlds, as van Dijk and Kintch do;neither is it objectivist, as Hobbs' semantics eems to be.
)190Zadrozny and Jensen Semantics of ParagraphsWe shall discuss only the first two points, since the third one has already beenexplained.The chief difference between our approach and the other two lies in identifying theparagraph as a domain of coherence.
Hobbs, van Dijk, and Kintch distinguish between"local" coherence~a property of subsequent sentences--and "global" coherence---aproperty of discourse as a whole.
Hobbs explains coherence in terms of an inventoryof "local," possibly computable, coherence relations, like "elaboration," "occasion,"etc.
(Mann and Thompson 1983 give an even more detailed list of coherence relationsthan Hobbs.)
Van Dijk and Kintch do this too, but they also describe "macrostructures"representing the global content of discourse, and they emphasize psychological andcognitive strategies used by people in establishing discourse coherence.
Since we havelinked coherence to models of paragraphs, we can talk simply about "coherence"--without adjectives--as  property of these models.
To us the first "local" domain seemsto be too small, and the second "global" one too large, for constructing meaningfulcomputational models.
To be sure, we believe relations between pairs of sentences areworth investigating, especially in dialogs.
However, in written discourse, the smallestdomain of coherence is a paragraph, very much as the sentence is the basic domainof grammaticality (although one can also judge the correctness of phrases).To see the advantage of assuming that coherence is a property of a fragment of atext/discourse, and not a relation between subsequent sentences, let us consider forinstance the textJohn took a train from Paris to Istanbul.
He likes spinach.According to Hobbs (1979, p. 67), these two sentences are incoherent.
However, thesame fragment, augmented with the third sentence Mary told him yesterday that theFrench spinach crop failed and Turkey is the only country...
(ibid.)
suddenly (for Hobbs)becomes coherent.
It seems that any analysis of coherence in terms of the relationbetween subsequent sentences cannot explain this sudden change; after all, the firsttwo sentences didn't change when the third one was added.
On the other hand, thischange is easily explained when we treat the first two sentences as a paragraph:if the third sentence is not a part of the background knowledge, the paragraph isincoherent.
And the paragraph obtained by adding the third sentence is coherent.Moreover, coherence here is clearly the result of the existence of the topic "John likesspinach.
"We derive coherence from formal principles regulating the organization and useof knowledge in language understanding.
Although, like the authors discussed above,we stress the importance of inferencing and background knowledge in determiningcoherence, we also address the problem of knowledge organization; for us the cen-tral problem is how a model emerges from such an organization.
Hobbs sets forthhypotheses about the interaction of background knowledge with sentences that areexamined at a given moment; van Dijk and Kintch provide a wealth of psychologicalinformation on that topic.
But their analyses of how such knowledge could be usedare quasi-formal.
Our point of departure is different: we assume a certain simple struc-ture of the referential level (partial orders) and a natural way of using the knowledgecontained there ("coherence links" + "most plausible = first").
Then we examine whatcorresponds to "topic" and "coherence"---they become mathematical concepts.
In thissense our work refines these concepts, changes the way of looking at them by linkingthem to the notion of paragraph, and puts the findings of the other researchers into anew context.191Computational Linguistics Volume 17, Number 25.
Models of ParagraphsWe argue below that paragraphs can be mapped into models with small, finite uni-verses.
We could have chosen another, more abstract semantics, with infinite models,but in this and all cases below we have in mind computational reasons for this enter-prise.
Thus, as in the case of Kamp's (1981) DRS, we shall construct a kind of Herbrandmodel of texts, with common and proper names translated into unary predicates, in-transitive verbs into unary predicates, and transitive verbs into binary predicates.
Inbuilding the logical model M of a collection of formulas S corresponding to the sen-tences of a paragraph, we assume that the universe of M contains constants introducedby elements of S, usually by ones corresponding toNPs, and possibly by some formu-las picked by the construction from the referential level.
However, we are interestednot in the relationship between truth conditions and representations of a sentence,but in a formalization of the way knowledge is used to produce a representation f asection of text.
Therefore we need not only a logical description of the truth conditionsof sentences, as presented by Kamp, but also a formal analysis of how backgroundknowledge and metalevel operations are used in the construction of models.
Thisextension is important and nontrivial; we doubt that one .can deal effectively withcoherence, anaphora, presuppositions or the semantics of connectives without it.
Wehave begun presenting such an analysis in Section 3, and we continue now.5.1 The Example Revisited: Preparation for Building a ModelWe return now to the example paragraph, to illustrate how the interaction betweenan object heory and a referential level produces a coherent interpretation f the text(i.e., a p-model) and resolves the anaphoric references.
The method will be similarto, but more formal than, what was presented in Section 2.
In order not to bore thereader with the same details all over again, we will use a shorter version of the sametext.Example 2PI: In 1347 a ship entered the port of Messina bringing with it the diseasethat came to be known as the Black Death.P2: It struck rapidly.P3: Within twenty-four hours of infection came an agonizing death.5.1.1 Translation to Logic.
The text concerns events happening in time.
Naturally, wewill use a logical notation in which formulas may have temporal and event compo-nents.
We assume that any formal interpretation f time will agree with the intuitiveone.
So it is not necessary now to present a formal semantics here.
The reader mayconsult recent papers on this subject (e.g.
Moens and Steedman 1987; Webber 1987) tosee what a formal interpretation f events in time might look like.
Since sentences canrefer to events described by other sentences, we may need also a quotation operator;Perlis (1985) describes how first order logic can be augmented with such an operator.Extending and revising Jackendoff's (1983) formalism seems to us a correct methodto achieve the correspondence b tween syntax and semantics expressed in the gram-matical constraint ("that one should prefer a semantic theory that explains otherwisearbitrary generalizations about the syntax and the lexicon"---ibid.
).However, as noted before, we will use a simplified version of such a logical no-tation; we will have only time, event, result, and property as primitives.
After theseremarks we can begin constructing the model of the example paragraph.
We assume192Zadrozny and Jensen Semantics of Paragraphsthat constants are introduced by NPs.
We have then(i) Constants  s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d),infection(i), death(b), year(1347).
(ii) Formulae$1: \[t ime: year(1347); event : enter(s,m) & ship(s) & port(m) &bring(xo, d) & disease(d) & name(d, BlackDeath)  & (Xo -- s Vx0 = d V x0 -- m)\]$2: t ime : past; event : rapidly: strike(yo) & (yo -- s V yo = m Vy0 --  d)\]$3: 3t, t'{\[time : t; infection(i)\] & \[time: t' c (t, t + 24h);event :come(b) & death(b) & agonizing(b)\]}The notation t ime : ~(t); event : fl should be understood as meaningthat the event described by the formula fl took place in (or during) thetime period described by the formula c~(t), t ranges over instants of time(not intervals).Note.
We assume that "strike" is used intransitively.
But our construction of thep-models of the paragraph would look exactly the same for the transitive meaning,except that we would be expected to infer that people were harmed by the illness.5.1.2 Referential Level.
We use only standard dictionaries as a source of backgroundinformation.
The content of this information is of secondary importance we want tostress the formal, logical side of the interaction between the referential level and theobject theory.
Therefore we represent both in our simplified logical notation, and notin English.
All formulas at the referential level below have been obtained by a directtranslation of appropriate ntries in Webster's and Longman.
The translation in thiscase was manual, but could be automated.?
Referent ia l  leve l  (a fragment):ship(x) --, {large: boat(x); 3ycarry(x,y) & (people(y) v goods(y)) & agent(x);...} (shl)/* ship--a large boat for carrying people or goods on the sea */bring(x, y) --* {carry(x, y); ...} (bl)strike(x, y) --* {hit(x, y); agent(x) & patient(y);...}strike(x) --~ {hit(x); agent(x);...}strike(x) --* {illness(x) & By suddenly:harm(x, y);...}/* bring--to carry */(sla)(slb)/* strike---to hit */(s2 _ex)/* strike---to harm suddenly; "they were struck by illness */193Computational Linguistics Volume 17, Number 2disease(y) ---* {illness(y) & 3z (infection(z) & causes(z,y));...} (dl)/* disease illness caused by an infection */death(x) --~ {3t, y\[x =' \[t ime:t;  event :die(y) & (ereature.
(y) v plant(y))\]'\]} (de_l)/* death--an event in which a creature or a plant dies */come(x) --* {3t\[time : t; event :arrive(x)\]} (ct_l)/* to come--to arrive (...) in the course of time */infection(x) --~ {3e, y, z\[e = event :infect(y, z) & person(y) & disease(z))\]& x -- result(e)} (i_1)/* infection--the r sult of being infected by a disease */agonizing(x) ~ {3y causes(x, y) & pain(y)} (a_l)/* agonizing---causing great pain */enter(x, y) - .
{come_in(x, y); place(y)} (e_l)/* enter--to come into a place */We have shown, in Section 3, the role of preferences in building the model of a para-graph.
Therefore, to make our exposition clearer, we assume that all the above theoriesare equally preferred.
Still, some interesting things will happen before we arrive at ourintended model.5.1.3 Provability and Anaphora Resolution.
To formalize a part of the process forfinding antecedents of anaphors, we have to introduce a new logical notion--the re-lation of weak R + M-abduction.
This relation would hold, for instance, between theobject heory of our example paragraph and a formula expressing the equality of twoconstants, i and i', denoting (respectively) the "infection" in the sentence Within twenty-four hours of infection .
.
.
.
and the "infection" of the theory (dl)--a disease is an illnesscaused by an infection.
This equality i = i' cannot be proven, but it may be reasonablyassumed--we know that in this case the infection i' caused the illness, which, in turn,caused the death.The necessity of this kind of merging of arguments has been recognized before:Charniak and McDermott (1985) call it abductive unification~matching, Hobbs (1978, 1979)refers to such operations using the terms knitting or petty conversational implicature.Neither Hobbs nor Charniak and McDermott ried then to make this notion precise,but the paper by Hobbs et al (1988) moves in that direction.
The purpose of thissubsection is to formalize and explain how assumptions like that one above can bemade.DefinitionA formula ~ is weakly provable from an object heory T, expressed as T t-r ~, iff thereexists a partial theory T E PT(T) such that T F ~b, i.e.
T proves ~ in logic.
(We call F-r"weak" because it is enough to find one partial theory proving a given formula.
)As an example, in the case of the three-sentence paragraph, we have a partialtheory T1 based on (slb) saying that " 'it' hits rapidly," and T2 saying that "an illness('it') harms rapidly" (s2_ex).
Thus both statements are weakly provable.194Zadrozny and Jensen Semantics of ParagraphsSince we view the metalevel constraints M rather as rules for choosing models thanas special inference rules, the definition of the R+M-abduction is model-theoretic, notproof-theoretic:DefinitionA preferred model of a theory T is an element of Mods(T) that satisfies metalevel con-straints contained in M. The set of all preferred models of T is denoted by PM(T).A formula 4 of L(=), the language with equality, is weakly R + M-abductible froman object heory T, denoted by T ~-a+M G iff there exists a partial theory T E PT(T) anda preferred model M E PM(T) such that M ~ G i.e.
4 is true in at least one preferredmodel of the partial theory T.Note: The notions of strong provability and strong R + M-abduction can be in-troduced by replacing "there exists" by "all" in the above definitions (cf.
Zadrozny1987b).
We will have, however, no need for "strong" notions in this paper.
Also, in apractical system, "satisfies" should be probably replaced by "violates fewest.
"Obviously, it is better to have references of pronouns resolved than not.
Afterall, we assume that texts make sense, and that authors know these references.
Thatapplies to references of noun phrases too.
On the other hand, there must be somerestrictions on possible references; we would rather assume that "spinach" ~ "train"(i.e.
V x,y)(spinach(x) & train(y) --, x # y)), or "ship" # "disease."
Two elementaryconditions limiting the number of equalities are: an equality N1 = N2 may be assumedonly if either N1 and N2 are listed as synonyms (or paraphrases) or their equality isexplicitly asserted by the partial theory T. Of course there are other conditions, like"typically, the determiner 'a' introduces a new entity, while 'the' refers to an alreadyintroduced constant."
(But notice that in our example paragraph "infection" appearswithout an article.)
All these, and other, guidelines can be articulated in the form ofmetarules.We define another partial order, this time on models Mods(T) of a partial theoryT of a paragraph: M1 >= M2, if M1, satisfies more R + M-abductible qualities thanM2.
The principle articulating preference for having the references resolved can nowbe expressed asMetarule 1Assume that T E PT(P) is a partial theory of a paragraph P. Every preferred modelM E PM(T) is a maximal element of the ordering >= of Mods(T).To explain the meaning of the metarule, let us analyze the paragraph (P1, P2, P3)and the background knowledge needed for some kind of rudimentary understandingof that text.
The rule (i_1) (infection is a result of being infected by a disease... ), dealingwith the infection i, introduces a disease dl; we also know about the existence of thedisease d in 1347.
Now, notice that there may be many models satisfying the objecttheory of the paragraph P augmented by the background knowledge.
But we can findtwo among them: in one, call it M1, d and dl are identical; in the other one, M2, theyare distinct.
The rule says that only the first one has a chance to be a preferred modelof the paragraph; it has more noun phrase references resolved than the other model,or--formally--it satisfies more R + M-abductible qualities, and therefore M1 >= M2.This reasoning, as the reader surely has noticed, resembles the example aboutinfections from the beginning of this section.
The difference between the cases lies inthe equality d = dl being the result of a formal choice of a model, while i = i ~ wasn'tproved, just "reasonably" assumed.195Computational Linguistics Volume 17, Number 2In interpreting texts, knowledge of typical subjects and typical objects of verbshelps in anaphora resolution (cf.
Braden-Harder & Zadrozny 1990).
Thus if we knowthat A farmer grows vegetables, either having obtained this information directly froma text, or from R, we can reasonably assume tlhat He also grows some cotton refers tothe farmer, and not to a policeman mentioned in the same paragraph.
Of course, thisshould be only a defeasible assumption, if nothing indicates otherwise.
We now wantto express this strategy as a metarule:Metarule 2Let us assume that it is known that P(a, b) & Q(a) & R(b), and it is not known thatP(a', X), for any X.
Then models in which P(a, c) & R'(c) holds are preferred to modelsin which P(a',c) & R'(c) is true.One can think of this rule as a model-theoretic version of Ockham's razor orabduction; it says "minimize the number of things that have the property P(,, ,),"and it allows us to draw certain conclusions on the basis of partial information.
Weshall see it in action in Section 5.2.We have no doubts that various other metarules will be necessary; clearly, ourtwo metarules cannot constitute the whole theory of anaphora resolution.
They areintended as an illustration of the power of abduction, which in this framework helpsdetermine the universe of the model (that is the set of entities that appear in it).
Otherfactors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping(Webber 1983) must play a role, too.
Determining the relative importance of thosefactors, the above metarules, and syntactic lues, appears to be an interesting topic initself.Note: In our translation from English to logic we are assuming that "it" is anaphoric(with the pronoun following the element hat it refers to), not cataphoric (the otherway around).
This means that the "it" that brought he disease in P1 will not be con-sidered to refer to the infection "i" or the death "d" in P3.
This strategy is certainlythe right one to start out with, since anaphora is always the more typical direction ofreference in English prose (Halliday and Hasan 1976, p. 329).Since techniques developed elsewhere may prove useful, at least for comparison,it is worth mentioning at this point that the proposed metarules are distant cousinsof "unique-name assumption" (Genesereth and Nilsson 1987), "domain closure as-sumption" (ibid.
), "domain circumscription" (cf.
Etherington and Mercer 1987), andtheir kin.
Similarly, the notion of R + M-abduction is spiritually related to the "abduc-tive inference" of Reggia (1985), the "diagnosis from first principles" of Reiter (1987),"explainability" of Poole (1988), and the subset principle of Berwick (1986).
But, ob-viously, trying to establish precise connections for the metarules or the provabilityand the R + M-abduction would go much beyond the scope of an argument for thecorrespondence of paragraphs and models.
These connections are being examinedelsewhere (Zadrozny forthcoming).5.2 p-ModelsThe construction of a model of a paragraph, a p-model, must be based on the in-formation contained in the paragraph itself (the object theory) and in the referentiallevel while the metalevel restricts ways that the model can be constructed, or, in otherwords, provides criteria for choosing a most plausible model(s), if a partial theory isambiguous.
This role of the metarules will be clearly visible in finding references ofpronouns in a simple case requiring only a rule postulating that these references be196Zadrozny and Jensen Semantics of Paragraphssearched for, and in a more complex case (in Section 5) when they can be found onlyby an interplay of background knowledge and (a formalization of) Gricean maxims.DefinitionM is a p-model of a paragraph P iff there exists a coherent partial theory T E PT(P)such that M E PM(T).Having defined the notion of a p-model, we can mimic now, in logic, the rea-soning presented in Section 2.2.
Using background information and the translation ofsentences, we build a p-model of the paragraph.
This involves determining the ref-erences of the pronoun "it," and deciding whether "struck" in the sentence It struckrapidly means "hit" (slb) or "harmed" (s2_ex).
We have then two meanings of "strike"and a number of possibilities for the pronouns.We begin by constructing the two classes of preferred models given by (slb) and(s2_ex), respectively.
It is easily seen that, in the models of the first class, based on{$2, slb}, (that is {rapidly:strike(yo) and strike(x) --~ hit(x)...}), together with all otheravailable information, do not let us R+M-abduct anything about y0, i.e., the referent forthe subject pronoun "it" in P2 (it struck rapidly).
On the other hand, from {$2, s2_ex, dl}we R + M-abduct hat y0 = d, i.e.
the disease struck rapidly.
That is the case becauses2_ex implies that the agent hat "struck rapidly" is actually an illness.
From rapidly :strike(yo), strike(x) -* illness(x) & .... disease(y) --, illness(y) & .... and disease(d) we caninfer illness(yo) and illness(d); by the Metarule (1) we conclude that Y0 -- d. In otherwords, the referent for the subject "it" is "disease."
Thus the Metarule (1) immediatelyeliminates all the models from the first class given by (slb), in which "struck" means"hit.
"Notice that we cannot prove in classical logic that the ship has brought the disease.But we are allowed to assume it by the above formal rule as the most plausible stateof affairs, or--in other words--we prove it in our three-level logic.We are left then with models of the three sentences ($1, $2, $3) that contain {$2,s2_ex, dl}; they all satisfy y0 = d. We now use {Sl,shl,bl}(enter(s,m) & ship(s) &bring(xo~ d) & ...; ship(s) --~ (3y)carry(s,y) & ...; Vz\[bring(xo, z) --* carry(xo, z)\]).
Fromthese facts we can conclude by Metarule (1) that x0 = s: a "ship" is an agent hat carriesgoods; to "bring" means to "carry"; and the disease has been brought by something--we obtain carry(xo, d) and carry(s, y); and then by Metarule (2), carry(s, d).
That is, thereferent for the pronoun "it" in P1 (... bringing with it the disease... ) should be "ship.
"Observe that we do not assert about the disease that it is a kind of goods or people;the line of reasoning oes as follows: since ships are known to carry people or goods,and ports are not known to carry anything, we may assume that the ship carried thedisease along with its standard cargo.Having resolved all pronoun references, with no ambiguity left, we conclude thatthe class PM(P) consists of only one model, based on the the partial theory{$1, $2, $3, shl, bl, e_l, s2_ex, dl, de1, il, al, ctl}.The model describes a situation in which the ship came into the port/harbor; theship brought he disease; the disease was caused by an infection; the disease harmedrapidly, causing a painful death; and so on.The topic Tp of (P1, P2, P3) is the disease(x).
The first sentence talks about it; thesecond one refers to it using the pronoun "it," and the third one extends our knowl-edge about the topic, since "disease' is linked to "infection" through dl.
Furthermore,197Computational Linguistics Volume 17, Number 2f -1II place(m)!
r' i h?rb?rCm)IPI sh ip (s ) - -~  port(m) j -- time :1:547 I0s m -~I name: Messin?ls ~ nome : Block Deoth l, ,  , d ZI \] in fec t ion~ness  (d)Jp l ill suddenly I/ /F'" I I o event (e') II II II IP3 .- a.q in-time (24+ t) Ib t?
I- -deoth(~ ~\[ io infecti?n (i) Il?g?nizing (b)-~ ' o e,Lent (e)IIi person(p) diseose(d)l IFigure 3The p-model for the example paragraph"disease" is the only noun phrase mentioned or referred to in all three sentences; thesentence $1 is the topic sentence.
The p-model of the paragraph is represented byFigure 3.But notice that our definition of topic licences also other analyses, for example, onein which all the predicates of the first sentence constitute the topic of the paragraph, $2elaborates $1 (in the sense of condition I (c) of the definition of topic), and $3 elaborates$2.
Based on the larger context, we prefer the first analysis; however, a computationalcriterion for such a preference remains as an open problem.6.
On the Ro le  of  the Meta leve lWe have already seen examples of the application of metalevel rules.
In the analysisof the paragraph, we applied one such rule expressing our commonsense knowledgeabout the usage of pronouns.
In this section we discuss two other sources of met-alevel axioms: Gricean cooperative principles, which reduce the number of possible198Zadrozny and Jensen Semantics of Paragraphsinterpretations of a text or an utterance; and connectives and modalities--such as"but, .... unless," or "maybe"---which refer to the process of constructing the modelsor partial theories, and to some operations on them (see Figure 4).We can see then two applications of metarules: in constructing models of a textfrom representations of sentences, and in reducing, or constraining, the ambiguityof the obtained structure.
We begin by showing how to formalize the latter.
In thenext subsection (6.1), assuming the Gricean maxims to be constraints on languagecommunication, either spoken or written, we use their formal versions in buildingpartial theories.
A specific instance of the rule of "quantity" turns out to be applicableto anaphora resolution.
That example will end our discussion of anaphora in thisarticle.The last topic we intend to tackle is the semantic role of conjunctions.
In subsection6.2 we present ametalevel axiom dealing with the semantic role of the adversative con-junction "but;" then we talk about some of its consequences for constructing modelsof text.
This will complete our investigation of the most important issues concerningparagraph structure: coherence (how one can determine that a paragraph expressesa "thought'0, anaphora (how one can compute "links" between entities that a para-graph talks about), and cohesion (what makes a paragraph more than just a sum ofsentences).
Of course, we will not have final answers to any of these problems, butwe do believe that the/a direction of search for computational models of text will bevisible at that point.We assume aflat structure of the metalevel, envisioning it as a collection of (closed)formulas written in the language of set theory or higher order logic.
In either of the twotheories it is possible to define the notions of a model, satisfiability, provability, etc.
forany first order language (cf.
e.g.
Shoenfield 1967); therefore the metalevel formulas cansay how partial theories hould be constructed (specifying for instance the meaning of0) and what kinds of models are admissible.
The metarules thus form a logical theoryin a special anguage, such as the language of ZF-set theory.
However, for the sake ofreadability, we express all of them in English.6.1 A Formalization of Gricean MaximsA Gricean Cooperative Principle applies to text, too.
For instance, in normal writingpeople do not express common knowledge about typical functions of objects.
In fact,as the reader may check for himself, there is nothing in Gricean maxims that does notapply to written language.
That the maxims play a semantic role is hardly surprising.But that they can be axiomatized and used in building formal models of texts is new.We present in the next couple of paragraphs our formalization of the first maxim, andsketch axiomatizations of the others.
Then we will apply the formal rule in an example.Gricean maxims, after formalization, belong to the rnetalevel.
This can be seen fromour formalization of the rule "don't say too much."
To this end we define redundancyof a partial theory T of a paragraph as the situation in which some sentences can belogically derived from other sentences and from the theory T in a direct manner:(3S E/5)(3cr E R)\[a E T& e : ~b --~ ~b &/St- ~ & {~} U (/5- {S}) F- S\]The meaning of this formula can be explained as follows: a paragraph P has beentranslated into its formal version/5 and is to be examined for redundancy.
Its partialtheory PT(/5) has also been computed.
The test will turn positive if, for some sentenceS, we can find a rule/theorem a = ~ --* ~ in PT(P) such that the sentence S is implied(in a classical ogic) by the other sentences and ~.
For example, if the paragraphabout Black Death were to contain also the sentence The ship carried people or goods, or199Computational Linguistics Volume 17, Number 2?
Quantity.
Say neither too much nor too little.?
Quality.
Try to make your contribution one that is true.?
Relation.
Be relevant.?
Manner.
Avoid obscurity and ambiguity; be brief and orderly.Figure 4The Gricean maximsboth, which (in its logical form) belongs to R, it would be redundant: ~ = (shl), there.Similarly, the definition takes care of the redundancy resulting from a simple repetition.Metarule Gla(nonredundancy) If T1, T2 C PT(P) and T1 is less redundant han T2, then the theoryT1 is preferred to T2.
(Where "less redundant" means that the number of redundantsentences in T1 is smaller than in T2)The relevant half of the Maxim of Quantity has been expressed by Gla.
Howwould we express the other maxims?
The "too little" part of the first maxim mightbe represented as a preference for unambiguous partial theories.
The second maximhas been assumed all the t ime--when constructing partial theories or models, thesentences of a paragraph are assumed to be true.
The Maxim of Manner seems to usto be more relevant for critiquing the style of a written passage or for natural anguagegeneration; in the case of text generation, it can be construed as a requirement thatthe produced text be coherent and cohesive.We do not claim that Gla is the best or unique way of expressing the rule "assumethat the writer did not say too much."
Rather, we stress the possibility that one canaxiomatize and productively use such a rule.
We shall see this in the next example:two sentences, regarded as a fragment of paragraph, are a variation on a theme byHobbs (1979).Example 3The captain is worried because the third officer can open his safe.
He knows the combination.The above metarule postulating "nonredundancy" implies that "he" = "the thirdofficer, .... his" = "the captain's" are the referents of the pronouns.
This is because theformulasafe(x) --, (owns(y~ x) & cmbntn(z~ x) --, knows(y~ z) & can_open(y~ x)) E Tsafe,belongs to R, since it is common knowledge about safes that they have owners, andalso combinations that are known to the owners.
Therefore "his" = "the third officer's"would produce a redundant formula, corresponding to the sentence The third officercan open the third officer's safe.
By the same token, The captain knows the combinationwould be redundant too.200Zadrozny and Jensen Semantics of ParagraphsWe now explain the details of this reasoning.
One first proves that "his" = "thecaptain's."
Indeed, if "his" = "the third officer's," then our example sentence wouldmean?
The captain is worried because the third officer can open the third officer's safe;in logic:captain(x) & worry(x,s) & sentence(s)& s = 'the third officer can open the third officer's afe.
"We assume also, based on common knowledge about worrying, that worry(x', s') -~S.
That is, one worries about things that might possibly be or become true (S denotesthe logical formula corresponding to the sentence s, cf.
Section 3); but one doesn'tworry about things that are accepted as (almost) always true (such as the law ofgravity), so that worry(x', 's') ~ -~(S E Tf), where f ranges over subformulas of S.In our case, S immediately follows from Tsafe and X, where X = safe(sf) & third_offi-cer(o) & owns(o, sf)--the fact that "the third officer can open the third officer's afe" isa consequence of general knowledge about the ownership of safes.
And therefore theinterpretation with "his" = "the captain's" is preferred as less redundant by the ruleGla.
This theory contains representations of the two sentences, the theory of safes, atheory of worrying, and the equality "his" = "captain's.
"It remains to prove that "he" = "the third officer."
Otherwise we haveP1.
The captain is worried because the third officer can open the captain's afe.P2.
?
The captain knows the combination.Clearly, the last sentence is true but redundant--the theory of "safe" and P1 entail P2:{P1} U Tsafe t- P2We are left with the combinationQ1.
The captain is worried because the third officer can open the captain's afe.Q2.
?
The third officer knows the combination.In this case, Q2 does not follow from {Q1} u Tsafe and therefore Q1, Q2 is preferred toP1, P2 (by Gla).
We obtain thenThe captain is worried because the third officer can open the captain's afe.The third officer knows the combinationas the most plausible interpretation f our example sentences.Note: The reader must have noticed that we did not bother to distinguish thesentences P1, P2, Q1 and Q2 from their logical forms.
Representing "because" and"know" adequately should be considered a separate topic; representing the rest (inthe first order convention of this paper) is trivial.6.1.1 Was the Use of a Gricean Maxim Necessary?
Can one deal effectively with theproblem of reference without axiomatized Gricean maxims, for instance by using only201Computational Linguistics Volume 17, Number 2"petty conversational implicature" (Hobbs 1979), or the metarules of Section 5.2?
Itseems to us that the answer is no.As a case in point, consider the process of finding the antecedent of the anaphor"he" in the sentencesJohn can open Bill's safe.
He knows the combination.Hobbs (1979, 1982) proves "he" = "John" by assuming the relation of "elaboration"between the sentences.
(Elaboration is a relation between two segments of a text.
Itintuitively means "expressing the same thought from a different perspective," but hasbeen defined formally as the existence of a proposition implied by both segments--here the proposition is "John can open the safe".)
However, if we change the pair tothe tripleBill has a safe under the painting of his yacht.
John can open Bill's safe.
He knowsthe combinationthe relation of elaboration holds between the segment consisting of the first two sen-tences of the triple and each of the two possible readings: John knows the combinationand Bill knows the combination.
In this case, elaboration cannot choose the correct ref-erent, but the rule Gla can and does.
Clearly, an elaboration should not degenerateinto redundancy; the Gricean maxims are to keep it fresh.As we have observed, correct interpretations cannot be chosen by an interaction ofan object level theory and a referential level alone, because coherence, plausibility andconsistency are too weak to weed out wrong partial theories.
Metarules are necessary.True, the captain knew the combination, but it was consistent that "his" might havereferred to "the third officer's.
"6.2 Semantics of the Conjunction "But"Any analysis of natural anguage text, to be useful for a computational system, willhave to deal with coherence, anaphora, and connectives.
We have examined so farthe first two concepts; we shall present now our view of connectives to complete theargument about paragraphs being counterparts of models.
We present ametalevel rulethat governs the behavior of the conjunction "but;" we formalize the manner in which"but" carries out the contradiction.
Then we derive from it two rules that preventinfelicitous uses of "but.
"Connectives are function words--like conjunctions and some adverbs--that reresponsible simultaneously for maintaining cohesiveness within the text and for sig-naling the nature of the relationships that hold between and among various text units.
"And," "or," and "but" are the three main coordinating connectives in English.
How-ever, "but" does not behave quite like the other two--semantically, "but" signals acontradiction, and in this role it seems to have three subfunctions:..Opposition (called "adversative" or "contrary-to-expectation" byHalliday and Hasan 1976; cf.
also Quirk et al 1972, p. 672).The ship arrived but the passengers could not get off.The yacht is cheap but elegant.Comparison.
In this function, the first conjunct is not so directlycontradicted by the second.
A contradiction exists, but we may have to202Zadrozny and Jensen Semantics of Paragraphsgo through additional levels of implication to find it.
Consider thesentence:.That basketball player is short, but he's very quick.Affirmation.
This use of "but" always follows a negative clause, andactually augments the meaning of the preceding clause by addingsupporting information:The disease not only killed thousands of people, but also ended a periodof economic welfare.In this section we consider only the first, or adversative, function of the coordinatingconjunction "but.
"6.2.1 The Semantic Function of "But" "But" introduces an element of surprise intodiscourse.
Because it expresses some kind of contradiction, "but" has no role in thepropositional calculus equivalent to the roles filled by "and" and "or."
Although thereare logical formation rules using the conjunction operator ("and") and the disjunctionoperator ("or"), there is no "but" operator.
What, then, is the semantic role of "but"?We believe that its function should be described at the metalevel as one of many rulesguiding the construction of partial theories.
This is expressed below.Metarule (BUT)The formulas ?
but k~, ~' but ~1 .
.
.
.
of a (formal representation f) paragraph P areto be interpreted as follows:In the construction of any T E PT(P) instead of taking @f to be the union Uof rr --* T~, take the union of ~ --* T?/{k~, ~', .
.
.
}.The symbol cr ---* T~,/{qt, ?d',...} denotes a maximal consistent with {or, ~, k~',...} sub-theory of r~ --* T?, and in general T/T  t will be a maximal consistent with T' subtheoryof T."But" is then an order to delete from background information everything contra-dicting ?
, but to use what remains.
Notice that "and" does not have this meaning; amodel for ?
and ?
will not contain any part of a theory that contradicts either of theclauses ?
or ?d.Typically this rule will be used to override defaults, to say that the expectedconsequences of the first conjunct hold except for the fact expressed by the secondconjunct; for instance: We were coming to see you, but it rained (so we didn't).
The ruleBUT is supposed to capture the "contrary-to-expectation" function of "but.
"We present now a simple example of building a model of a one-sentence paragraphcontaining "but."
We will use this example to explain how the rule BUT can be used.Using background information presented below, we will construct a partial model forthis one-sentence paragraph.203Computational Linguistics Volume 17, Number 2Example 4This yacht is cheap, but it is elegant.Referential level (a fragment)cheap(x) --, {~elegant(x); poor_quality(x);-~expensive} (cl)expensive(x) ~ -~cheap( x ) (encl)yacht(x) ~ {ship(x) & small(x)} (yl)elegant(x) --, { ~cheap( x ) ~ .
.
. }
(el)elegant(x) & yacht(x) --, {status-symbol(x)} (e_yl)Note: Compare (yl) with (cl); in (yl) smallness is a property of a ship; this wouldbe more precisely expressed as yacht(x) --* \[ship(x); property:  small(x)\].
This trick al-lows us not to conclude that "a big ant is big," or "a small elephant is small.
"We ignore the problem of multiple meanings (theories) of predicates, and assumethe trivial ordering in which all formulas are equally preferred.
(But note that (e_yl)is still preferred to (el) as a more specific theory of "elegant;" cf.
Section 3.
)Construction of the model: In our case ?
___ yacht(yo) & cheap(yo) and ?
=__elegant(yo ).
ThenTcheap -= {-~elegant(x); poor_quality(x) ; -~expensive(x) }Tyacht = {ship(x) & small(x)}Telcgant = {expensive(x)}Tyacht & elegant = {status_symbol(x)}Tcheap/~d = {poor-quality(x) ; -~expensive(x)}Tyacht/'  = TyachtTyacht&elegant / @ = Tyacht&elegantWe can now use the Metarule (BUT) and construct the partial theories of the sentence.In this case, there is only one:PT(~ but ~) = {T}~ whereT = {yacht(yo)~ elegant(yo)~cheap(yo)~ship(yo) & small(yo)~status-symbol (Yo )~ poor_quality(yo ) }In other words, the yacht in question is a poor quality small and elegant ship servingas an inexpensive status symbol.The partial model of the theory T is quite trivial: it consists of one entity repre-senting the yacht and of a bunch of its attributes.
However, the size of the model isnot important here; what counts is the method of derivation of the partial theory.6.2.2 Confirming the Analysis.
The Metarule (BUT) is supposed to capture the "con-trary-to-expectation" function of "but."
The next two rules follow from our formaliza-204Zadrozny and Jensen Semantics of Paragraphstion; their validity indirectly supports the plausibility of our analysis of "but.
"BUT_C1 : ff but -~  is incorrect, if ?
--* ?
is a "law."e.g.
Henry was murdered but not killed.Our referential level is a collection of partially ordered theories; we have expressedthe fact that a theory of some ?
is a "law" is by deleting the empty interpretation f ?from the partial order.
If we accept he definition of a concept as given by necessaryand sufficient conditions, the theories would all appear as laws.
If we subscribe to amore realistic view where definitions are given by a collection of central/prototypicaland peripheral conditions, only the peripheral ones can be contradicted by "but."
Ineither formalization we get BUT_C1 as a consequence: Since "laws" cannot be deleted,BUT can't be applied, and hence its use in those kinds of sentences would be incorrect.W.
Labov (1973) discussed sentences of the form,This is a chair but you can sit on it.The sentence is incorrect, since the function "one can sit on it" belongs to the core ofthe concept "chair"; so--contrary to the role of "but"--the sentence does not containany surprising new elements.
Using the Metarule (BUT) and the cooperative principleof Grice, we getBUT_C2: ?
but ?
is incorrect, if ?
-* k~ is a "law.
"The Metarule (BUT) gives the semantics of "but;" the rules BUT_C1 and BUT_C2 followfrom it (after formalization i a sufficiently strong rnetalanguage such as type theoryor set theory).
We can link all of them to procedures for constructing and evaluatingmodels of text.
Are they sufficient?
Certainly not.
We have not dealt with the otherusages of '%ut;" neither have we shown how to deal with the apparent asymmetryof conclusions: cheap but elegant seems to imply "worth buying," but elegant but cheapdoesn't; we have ignored possible prototypical effects in our semantics.
However, wedo believe that other ules, dealing with "but" or with other connectives, can be conve-niently expressed in our framework.
(The main idea is that one should talk explicitlyand formally about relations between text and background knowledge, and that thisknowledge is more than just a list of facts--it has structure, and it is ambiguous.
)Furthermore, the semantics of "but" as described above is computationally tractable.We also believe that one could not present a similarly natural account of the se-mantics of "but" in traditional logics, because classical logics withstand contradictionswith great difficulty.
Contradiction, however, is precisely what "but" expresses.
No-tice that certain types of coordinating conjunctions often have their counterparts inclassical logic: copulative (and, or, neither-nor, besides, sometimes etc.
), disjunctive (likeeither-or), illative (hence, for that reason).
Others, such as explanatory (namely or viz.
)or causal (for) conjunctions can probably be expressed somehow, for better or worse,within a classical framework.
Thus the class of adversative conjunctions (but, still, andyet, nevertheless) is in that sense unique.7.
ConclusionsWe hope that the reader has found this paper coherent, and its topic--the correspon-dence between paragraphs and models--interesting.
Our strategy was to divide the205Computational Linguistics Volume 17, Number 2subject into three subtopics: a theory of anaphora, which corresponds to the logicaltheory of equality in p-models; a theory of background knowledge, expressed as thelogical theory of reference in the three-level semantics; and principles of communicationencoded in metarules.
These principles include Gricean maxims and the semantics ofcohesion, and specify a model theory for the three-level semantics.
The framework re-sulting from putting these theories together is computational, empirical, and verifiable(even if incomplete); furthermore, it has strong links to already existing natural an-guage processing systems.
In particular, the new logical level--the referential level--isexemplified by on-line dictionaries and other reference works, from which we extractbackground information about defaults and plausibility rankings.We also hope that the reader would be inclined to share our belief that naturallanguage text can be properly and usefully analyzed by means of a three-level seman-tics that includes an object level, a metalevel, and a referential level.
We believe thatthe coherence of an essay, a paper, or a book can be described by an extension of ourtheory.
The work of van Dijk and Kintch (1983) on "macrostructures" could probablyform the basis for such an expansion.
Similarly, much of the abovementioned workby Hobbs, Webber, Grosz, and Sidner can be incorporated into this framework.Our main intention was to demonstrate hat formal notions of background knowl-edge can be used to?
define coherence, make it semantically distinct from mere consistency,and link it formally with the notion of a topic;?
define a class of p-models--logical models of paragraphs;?
provide a semantics for "but" (which exemplifies our understanding ofgrammatical cohesion);?
express the Gricean maxims formally, and use them in a computationalmodel of communication (which seems to contradict Allen 1988, p. 464).Moreover, we tried to convince the reader that paragraph is an important linguisticunit, not only because of its pragmatic importance xemplified by coherence and linksto background knowledge, but also because of its role in assignment of syntacticstructures (viz.
ellipsis) and in semantics (viz.
its possible role in evaluating semanticrepresentations).A great many issues have been omitted from our analysis.
Thus, although we areaware that anaphora resolution and consistency depend on previously processed text,the problem of connecting a paragraph to such text has been conveniently ignored.Notice that this doesn't make our thesis about paragraphs being units of semanticprocessing any weaker, we have not claimed that paragraphs are independent.
Thequestions of how to translate from natural anguage to a logical notation needs a lotof attention; we have merely assumed that this can be done.
Continuing this list, wehave accepted a very classical theory of meaning, given by Tarski: the truth is whatis satisfied in a model.
This theory should be refined, for instance by formalizingLakoff's (1987) concept of radial categories, and proposing mechanisms for exploitingit.
By the same token, the concept of reference has to be broadened to include iconic(e.g., visual and tactile) information.
And certainly it would be .nice to have a moredetailed theory describing the role of the metalevel.
In particular, we can imaginethat the simple structure of a collection of set theoretic formulas can be replaced bysomething more interesting.
We leave this as another open problem.We have shown that it is possible to develop a formal system with an explicitrelationship between background knowledge and text, showing mechanisms that take206Zadrozny and Jensen Semantics of Paragraphsadvantage ofpreference, coherence, and contradiction (the reality of these phenomenahas never been disputed, but their semantic functions had not been investigated).
Weshould also mention that we have also begun some work on actually checking theempirical validity of this model (cf.
e.g.
Braden-Harder and Zadrozny 1990), using on-line dictionaries (LDOCE and Webster's) as the referential level.
We know, of course,that existing dictionaries are very imperfect, but (1) they can be accessed and usedby computer programs; (2) they are getting better, as they are very systematicallycreated with help of computers (see Sinclair 1987 for an account of how COBUILDwas constructed); (3) obviously, we can imagine useful new ones, like a Dictionary ofPragmatics; (4) we believe that we can explore the new inference mechanisms even insuch unrefined environments.Although the scheme we have proposed certainly needs further efinement, webelieve that it is correct in two of its most important aspects: first, in the separationof current paragraph analysis (the object theory) from background information (thereferential level); and second, in asserting that the function of a paragraph is to allowthe building of a model of the situation described in the paragraph.
This model can bestored, maybe modified, and subsequently used as a reference for processing followingparagraphs.Finally, the paper can be viewed as an argument that the meaning of a sentencecannot be defined outside of a context, just as the truth value of a formula cannot becomputed in a vacuum.
A paragraph is the smallest example of such a context--it is"a unit of thought.
"AcknowledgmentsWe gratefully acknowledge the helpprovided by our anonymous but diligentreferees, by Graeme Hirst and SusanMcRoy, and by our many colleagues at IBMResearch, all for whom helped us to avoidworst consequences of Murphy's Lawduring the writing of this paper.ReferencesAllen, J.
(1987).
Natural LanguageUnderstanding.
Benjamin/Cummings.Menlo Park, California.Berwick, R.C.
(1986).
"Learning fromPositive-Only Examples: The SubsetPrinciple and Three Case Studies."
InMichalski, R.S.
et al, eds.
MachineLearning Vol.
II.
Morgan Kaufmann.
LosAltos, California: 625-645.Black, J.B., and Bower, G.H.
(1979).
"Episodes as Chunks in NarrativeMemory."
Journal of Verbal Learning andVerbal Behavior 18: 309-318.Bond, S.J., and Hayes, J.R. (1983).
CuesPeople Use to Paragraph Text.
manuscript.Dept.
of Psychology, Carnegie-MellonUniversity.Brachman, R.J., Fikes, R.E., and Levesque,H.J.
(1985).
"Krypton: A FunctionalApproach to Knowledge Representation.
"In Brachman, R.J., and Levesque, H.J.,eds.
Readings in Knowledge Representation.Morgan Kaufmann.
Los Altos, California:411-430.Braden-Harder, L. and Zadrozny, W. (1989).Lexicons for Broad Coverage Semantics.
IBMResearch Division Report, RC 15568.Chafe, W.L.
(1979).
"The Flow of Thoughtand the Flow of Language."
In Givon, T.,ed., Syntax and Semantics, Vol.
12.Academic Press.
New York, New York.Charniak, E. (1983).
"Passing Markers: ATheory of Contextual Influence inLanguage Comprehension."
CognitiveScience, 7(3): 171-190.Charniak, E., and McDermott, D. (1985).Introduction to Artificial Intelligence.Addison-Wesley.
Reading, Massachusetts.Crothers, E.J.
(1979).
Paragraph StructureInference.
Ablex Publishing Corp.,Norwood, New Jersey.Etherington, D.W., and Mercer, R.E.
(1987).
"Domain Circumscription: AReevaluation."
Computational Intelligence3: 94--99.Frisch, A.
(1987).
"Inference withoutChaining."
Proc.
IJCAI-87.
Milan, Italy:515-519.Fowler, H.W.
(1965).
A Dictionary of ModernEnglish Usage.
Oxford University Press.New York, New York.Genesereth, M.R., and Nilsson, N.J. (1987).Logical Foundations of Artificial Intelligence.Morgan Kaufmann.
Los Altos, California.Grice, H.P.
(1978).
"Further Notes on Logic207Computational Linguistics Volume 17, Number 2and Conversation."
In Cole, P., ed.
Syntaxand Semantics 9:Pragmatics.
AcademicPress.
New York, New York: 113-128.Grice, H.P.
(1975).
"Logic andConversation."
In Cole, P., and Morgan,J.L., eds., Syntax and Semantics 3: SpeechActs.
Academic Press.
New York NewYork: 41-58.Groesser, A.C. (1981).
Prose ComprehensionBeyond the Word.
Springer.
New York,New York.Grosz, B.J.
(1978).
DISCOURSEKNOWLEDGE--Section 4 of Walker, D.E.,ed., Understanding Spoken Language.North-Holland.
New York, New York:229-344.Grosz, B.J.
(1977).
"The Representation a dUse of Focus in a System forUnderstanding Dialogs."
Proc.
IJCAI-77.W.
Kaufmann.
Los Altos, California:67-76.Haberlandt, K., Berian, C., and Sandson, J.(1980).
"The Episode Schema in StoryProcessing."
Journal of Verbal Learning andVerbal Behavior 19: 635--650.Halliday, M.A.K., and Hasan, R. (1976).Cohesion in English.
Longman Group Ltd.London.Haugeland, J.
(1985).
Artificial Intelligence:The Very Idea.
MIT Press.
Cambridge,Massachusetts.Hinds, J.
(1979).
"Organizational Patterns inDiscourse."
In Givon, T., ed., Syntax andSemantics, Vol.
12.
Academic Press.
NewYork, New York.Hintikka, J.
(1985).
The Game of Language.D.
Reidel.
Dordrecht.Hirst, G. (1987).
Semantic Interpretation a dthe Resolution of Ambiguity.
CambridgeUniversity Press.
Cambridge.Hobbs, J.R. (1982).
"Towards anUnderstanding of Coherence inDiscourse."
In Lehnert, W.G., and Ringle,M.H., eds.
Strategies for Natural LanguageProcessing.
Lawrence Erlbaum.
Hillsdale,New Jersey: 223-244.Hobbs, J.R. (1979).
"Coherence andCoreference."
Cognitive Science 3: 67-90.Hobbs, J.R. (1978).
"Resolving PronounReferences."
Lingua 44: 311-338.Hobbs, J.R. (1977).
38 Examples of ElusiveAntecedents from Published Texts.Research Report 77-2.
Department ofComputer Science, City College, CUNY,New York.Hobbs, J.R. (1976).
Pronoun Resolution.Research Report 76-1.
Dept.
of ComputerScience.
City College, CUNY, New York.Hobbs, J., Stickel, M., Martin, P., andEdwards, D. (1988).
"Interpretation asAbduction."
In Proc.
of 26th AnnualMeeting of the Association for ComputationalLinguistics, ACL: 95-103.Jackendoff, R. (1983).
Semantics andCognition.
The MIT Press.
Cambridge,Massachusetts.Jensen, K. (1988).
"Issues in Parsing."
InA.
Blaser, ed., Natural Language at theCompute, r. Springer-Verlag, Berlin,Germany: 65-83.Jensen, K. (1986).
Parsing Strategies in aBroad-Coverage Grammar of English.Research Report RC 12147.
IBMT.J.
Watson Research Center, YorktownHeights, New York.Jensen, K., and Binot, J.-L.
(1988).
"Disambiguating Prepositional PhraseAttachments by Using On-line DictionaryDefinitions."
Computational Linguistics13.3-4.251-260 (special issue on thelexicon).Johnson-Laird, P.N.
(1983).
Mental Models.Harvard University Press.
Cambridge,Massachusetts.Kamp, H. (1981).
"A Theory of Truth andSemantic Representation."
In Groenendijk,J.A.G., et al eds., Formal Methods in theStudy of Language, I. MathematischCentrum, Amsterdam: 277-322.Labov, W. (1973).
"The Boundaries of Wordsand Their Meanings."
In Fishman, J., NewWays of Analyzing Variation in English.Georgetown U.
Press.
Washington, D.C.:340-373.Lakoff, G. (1987).
Women, Fire and DangerousThings.
The University of Chicago Press.Chicago, Illinois.Levesque, H.J.
(1984).
"A Logic of Implicitand Explicit Beliefs."
Proc.
AAAI-84.AAAI: 198-202.Longacre, R.E.
(1979).
The Paragraph as aGrammatical Unit.
In Givon, T., ed.,Syntax and Semantics, Vol.
12.
AcademicPress.
New York, New York.Longman Dictionary of Contemporary English.(1978).
Longman Group Ltd., London.Lyons, J.
(1969).
Introduction to TheoreticalLinguistics.
Cambridge University Press.Cambridge, England.Mann, W.C., and Thompson, S.A. (1983).Relational Propositions in Discourse.Information Sciences Institute ResearchReport 83-115.Moens, M., and Steedman, M.
(1987).
"Temporal Ontology in NaturalLanguage."
Proc.
25th Annual Meeting ofthe ACL.
Stanford, California: 1-7.Montague, R. (1970).
"Universal Grammar.
"Theoria 36: 373-398.Mycielski, J.
(1981).
"Analysis withoutActual Infinity."
Journal of Symbolic Logic,46(3): 625-633.208Zadrozny and Jensen Semantics of ParagraphsPatel-Schneider, P.S.
(1985).
"A DecidableFirst Order Logic for KnowledgeRepresentation."
Proc.
IJCAI-85.
AAAI:455-458.Perlis, D. (1985).
"Languages with SelfReference I: Foundations."
ArtificialIntelligence 25: 301-322.Poole, D. (1988).
"A Logical Framework forDefault Reasoning."
Artificial Intelligence36(1): 27-47.Quirk" R., Greenbaum, S., Leech, G., andSvartvik, J.
(1972).
A Grammar ofContemporary English.
Longman GroupLtd.
London.Reggia, J.A.
(1985).
"Abductive Inference.
"In Karma, K.N., ed., Expert Systems inGovernment Symposfum.
IEEE: 484-489.Reiter, R. (1987).
"A Theory of Diagnosisfrom First Principles."
ArtificialIntelligence, 32(1): 57-95.Sidner, C. (1983).
"Focusing in theComprehension f Definite Anaphora."
InBrady, M., and Berwick" R., eds.,Computational Models of Discourse, MITPress.
Cambridge, Massachusetts:330-367.Sells, P. (1985).
Lectures on ContemporarySyntactic Theories.
CSLI lecture notes;no.
3.Shoenfield, J.R. (1987).
Mathematical Logic.Addison-Wesley.
New York" New York.Sinclair, J.M., ed.
(1987).
Looking Up.
Anaccount of the COBUILD project.
CollinsELT, London.Small, S.L., Cottrell, G.W., and Tanenhaus,M.K., eds.
(1988).
Lexical AmbiguityResolution.
Morgan Kaufmann.
San Mateo,California.Turner, M. (1987).
Death is the Mother ofBeauty.
The University of Chicago Press.Chicago, Illinois.van Dijk, T.A., and Kintch, W. (1983).Strategies of Discourse Comprehension.Academic Press.
Orlando, Florida.Warriner, J.E.
(1963).
English Grammar andComposition.
Harcourt, Brace & World,Inc., New York New York.Webber, B.
(1983).
"So What Can We TalkAbout Now?"
In Brad~ M., and Berwick,R., eds., Computational Models of Discourse.MIT Press.
Cambridge, Massachusetts:147-154.Webber, B.
(1987).
"The Interpretation fTense in Discourse."
Proc.
25th AnnualMeeting of the ACL, ACL: 147-154.Webster's Seventh New Collegiate Dictionary.(1963).
Merriam-Webster, Inc. Springfield,Massachusetts.Woods, W. (1987).
"Don't Blame the Tool.
"Computational Intelligence 3(3): 228-237.Zadrozny, W. (1987a).
"Intended Models,Circumscription and CommonsenseReasoning."
Proc.
IJCAI-87: 909-916.Zadrozny, W. (1987b).
"A Theory of DefaultReasoning."
Proc.
AAAI-87.
Seattle,Washington: 385-390.Zadrozny, W. (unpublished).
The Logic ofAbduction.209
