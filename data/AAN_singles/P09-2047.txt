Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 185?188,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPQuery Segmentation Based on Eigenspace SimilarityChao Zhang?
?Nan Sun?Xia Hu?Tingzhu Huang?Tat-Seng Chua?
?School of Applied Math?School of ComputingUniversity of Electronic Science National University of Singapore,and Technology of China,Chengdu, 610054, P.R.
China Computing 1, Singapore 117590zhangcha@comp.nus.edu.sg {sunn,huxia,chuats}@comp.nus.edu.sgtzhuang@uestc.edu.cnAbstractQuery segmentation is essential to queryprocessing.
It aims to tokenize querywords into several semantic segments andhelp the search engine to improve theprecision of retrieval.
In this paper, wepresent a novel unsupervised learning ap-proach to query segmentation based onprincipal eigenspace similarity of query-word-frequency matrix derived from webstatistics.
Experimental results show thatour approach could achieve superior per-formance of 35.8% and 17.7% in F-measure over the two baselines respec-tively, i.e.
MI (Mutual Information) ap-proach and EM optimization approach.1 IntroductionPeople submit concise word-sequences to searchengines in order to obtain satisfying feedback.However, the word sequences are generally am-biguous and often fail to convey the exact informa-tion to search engine, thus severely, affecting theperformance of the system.
For example, giventhe query ?free software testing tools download?.A simple bag-of-words query model cannot ana-lyze ?software testing tools?
accurately.
Instead, itreturns ?free software?
or ?free download?
whichare high frequency web phrases.
Therefore, howto segment a query into meaningful semantic com-ponents for implicit description of user?s intentionis an important issue both in natural language pro-cessing and information retrieval fields.There are few related studies on query segmen-tation in spite of its importance and applicabilityin many query analysis tasks such as query sug-gestion, query substitution, etc.
To our knowl-edge, three approaches have been studied in pre-vious works: MI (Mutual Information) approach(Jones et al, 2006; Risvik et al, 2003), supervisedlearning approach (Bergsma and Wang, 2007) andEM optimization approach (Tan and Peng, 2008).However, MI approach calculates MI value justbetween two adjacent words that cannot handlelong entities.
Supervised learning approach re-quires a sufficiently large number of labeled train-ing data, which is not conducive in real applica-tions.
EM algorithm often converges to a localmaximum that depends on the initial conditions.There are also many relevant research on Chineseword segmentation (Teahan et al, 2000; Peng andSchuurmans, 2001; Xu et al, 2008).
However,they cannot be applied directly to query segmenta-tion (Tan and Peng, 2008).Under this scenario, we propose a novel unsu-pervised approach for query segmentation.
Dif-fering from previous work, we first adopt the n-gram model to estimate the query term?s frequencymatrix based on word occurrence statistics on theweb.
We then devise a new strategy to select prin-cipal eigenvectors of the matrix.
Finally we cal-culate the similarity of query words for segmen-tation.
Experimental results demonstrate the ef-fectiveness of our approach as compared to twobaselines.2 MethodologyIn this Section, we introduce our proposed querysegmentation approach, which is based on queryword frequency matrix principal eigenspace simi-larity.
To facilitate understanding, we first presenta general overview of our approach in Section 2.1and then describe the details in Section 2.2-2.5.2.1 OverviewFigure 1 briefly shows the main procedure ofour proposed query segmentation approach.
Itstarts with a query which consists of a vector ofwords{w1w2?
?
?wn}.
Our approach first build aquery-word frequency matrix M based on webstatistics to describe the relationship between any185two query words (Step 1).
After decomposing M(step 2), the parameter k which defines the num-ber of segments in the query is estimate in Step 3.Besides, a principal eigenspace of M is built andthe projection vectors({?i}, i ?
[1, n]) associatedwith each query-word are obtained (Step 4).
Simi-larities between projection vectors are then calcu-lated, which determine whether the correspondingtwo words should be segmented together (Step5).If the number of segmented components is notequal to k, our approach modifies the threshold ?and repeats steps 5 and 6 until the correct k num-ber of segmentations are obtained(Step 7).Input: one n words query: w1w2?
?
?wn;Output: k segmented components of query;Step 1: Build a frequency matrix M (Section2.2);Step 2: Decompose M into sorted eigenvaluesand eigenvectors;Step 3: Estimate parameter k (Section 2.4);Step 4: Build principal eigenspace with firstk eigenvectors and get the projection({?i}) of M in principal eigenspace(Section 2.3);Step 5: Segment the query: if (?i??Tj)/(??i????j?)
?
?, segment wiand wjto-gether (Section 2.5)Step 6: If the number of segmented parts doesnot equal to k, modify ?, go to step 5;Step 7: output the right segmentationsFigure 1: Query Segmentation based on query-word-frequency matrix eigenspace similarity2.2 Frequency MatrixLet W = w1, w2, ?
?
?
, wnbe a query of n words.We can build the relationships of any two wordsusing a symmetric matrix: M = {mi,j}n?nmi,j=????
?F (wi) if i = jF (wiwi+1?
?
?wj) if i < jmj,iif i > j(1)F (wiwi+1?
?
?wj) =count(wiwi+1?
?
?wj)?ni=1wi(2)Here mi,jdenotes the correlation between(wi?
?
?wj?1) and wj, where (wi?
?
?wj?1) meansa sequence and wjis a word.
Considering the dif-ference of each matrix element mi,j, we normalizemi,jwith:mi,j= 2 ?mi,j/(mi,i+mj,j) (3)F (?)
is a function measuring the frequency ofquery words or sequences.
To improve the preci-sion of measurement and reduce the computationcost, we adopt the approach proposed by (Wanget al, 2007) here.
First, we extract the relevantdocuments associated with the query via GoogleSoap Search API.
Second, we count the numberof all possible n-gram sequences which are high-lighted in the titles and snippets of the returneddocuments.
Finally, we use Eqn.
(2) to estimatethe value of mi,j.2.3 Principal EigenspaceAlthough matrix M depicts the correlation ofquery words, it is rough and noisy.
Underthis scenario, we transform M into its princi-pal eigenspace which is spanned by k largesteigenvectors, and each query word is denotedby the corresponding eigenvector in the principaleigenspace.Since M is a symmetric positive definite ma-trix, its eigenvalues are real numbers and thecorresponding eigenvectors are non-zero and or-thotropic to each other.
Here, we denote the eigen-values of M as : ?
(M) = {?1, ?2, ?
?
?
, ?n}and ?1?
?2?
?
?
?
?
?n.
All eigenvaluesof M have corresponding eigenvectors:V (M) ={x1, x2, ?
?
?
, xn}.Suppose that principal eigenspace M(M ?Rn?k) is spanned by the first k eigenvectors, i.e.M = Span{x1, x2, ?
?
?xk}, then row i of M canbe represented by vector ?iwhich denotes the i-thword for similarity calculation in Section 2.5, and?iis derived from:{?T1, ?T2, ?
?
?
, ?Tn}T= {x1, x2, ?
?
?
, xk} (4)Section 2.4 discusses the details of how to selectthe parameter k.2.4 Parameter k SelectionPCA (principal component analysis) (Jolliffe,2002) often selects k principal components by thefollowing criterion:k is the smallest integer which satisfies:?ki=1?i?ni=1?i?
Threshold (5)186where n is the number of eigenvalues.
When ?k?
?k+1, Eqn.
(5) is very effective.
However, accord-ing to the Gerschgorin circle theorem, the non-diagonal values of M are so small that the eigen-values cannot be distinguished easily.
Under thiscircumstance, a prefixed threshold is too restric-tive to be applied in complex situations.
Thereforea function of n is introduced into the threshold asfollows:?ki=1?i?ni=1?i?
(n?
1n)2(6)If k eigenvalues are qualified to be the princi-pal components, then the threshold in Eqn.
(5) can-not be lower than 0.5, and need not be higher thann?1n.
If the length of the shortest query we seg-mented is 4, we choose (n?1n)2because it will besmaller thann?1nand larger than 0.5 with n nosmaller than 4.The k eigenvectors will be used to segment thequery into k meaningful segments (Weiss, 1999;Ng et al, 2001).
In the k-dimensional principaleigenspace, each dimension of the space describesa semantic concept of the query.
When one eigen-value is bigger, the corresponding dimension con-tains more query words.2.5 Similarity ComputationIf the word i and word j are co-occurrence, ?iand ?jare approximately parallel in the principaleigenspace; otherwise, they are approximately or-thogonal to each other.
Hence, we measure thesimilarity of ?iand ?jwith inner-product to per-form the segmentation (Weiss, 1999; Ng et al,2001).
Selecting a proper threshold ?, we segmentthe query using Eqn.
(7):S(wi, wj) ={1, (?i?
?Tj)/(??i?
?
??j?)
?
?0, (?i?
?Tj)/(??i?
?
??j?)
< ?
(7)If S(wi, wj) = 1, wiand wjshould be segmentedtogether, otherwise, wiand wjbelong to differentsemantic concepts respectively.
Here, we denotethe total number of segments of the query as inte-ger m.As mentioned in Section 2.4, m should be equalto k, therefore, the threshold ?
is modified by kand m. We set the initial value ?
= 0.5 and modifyit with binary search method until m = k. If k islarger than m, it means ?
is too small to be a properthreshold, i.e.
some segments should be furthersegmented.
Otherwise, ?
is too large that it shouldbe reduced.3 Experiments3.1 Data setWe experiment on the data set published by(Bergsma and Wang, 2007).
This data set com-prises 500 queries which were randomly takenfrom the AOL search query database and eachquery.
These queries are all segmented manuallyby three annotators (the results are referred as A,B and C).We evaluate our results on the five test data sets(Tan and Peng, 2008), i.e.
we use A, B, C, theintersection of three annotator?s results (referredto as D) and the conjunction of three annotator?sresults (referred to as E).
Besides, three evaluationmetrics are used in our experiments (Tan and Peng,2008; Peng and Schuurmans, 2001), i.e.
Precision(referred to as Prec), Recall and F-Measure (re-ferred to as F-mea).3.2 Experimental resultsTwo baselines are used in our experiments: one isMI based method (referred to as MI), and the otheris EM optimization (referred to as EM).
Since theEM proposed in (Tan and Peng, 2008) is imple-mented with Yahoo!
web corpus and only GoogleSoap Search API is available in our study, weadopt t-test to evaluate the performance of MIwith Google data (referred to as MI(G)) and Ya-hoo!
web corpus (referred to as MI(Y)).
With thevalues of MI(Y) and MI(G) in Table 1 we get thep-value (p = 0.316 ?
0.05), which indicates thatthe performance of MI with different corpuses hasno significant difference.
Therefore, we can de-duce that, the two corpuses have little influence onthe performance of the approaches.
Here, we de-note our approach as ?ES?, i.e.
Eigenspace Simi-larity approach.Table 1 presents the performance of the threeapproaches, i.e.
MI (MI(Y) and MI(G)), EM andour proposed ES on the five test data sets using thethree mentioned metrics.
From Table 1 we findthat ES achieves significant improvements as com-pared to the other two methods in any metric anddata set we used.For further analysis, we compute statistical per-formance on mathematical expectation and stan-dard deviation as shown in Figure 2.
We observea consistent trend of the three metrics increasingfrom left to right as shown in Figure 2, i.e.
EMperforms better than MI and ES is the best amongthe three approaches.187MI(Y) MI(G) EM ESPrec 0.469 0.548 0.562 0.652A Recall 0.534 0.489 0.555 0.699F-mea 0.499 0.517 0.558 0.675Prec 0.408 0.449 0.568 0.632B Recall 0.472 0.391 0.578 0.659F-mea 0.438 0.418 0.573 0.645Prec 0.451 0.503 0.558 0.614C Recall 0.519 0.440 0.561 0.649F-mea 0.483 0.469 0.559 0.631Prec 0.510 0.574 0.640 0.772D Recall 0.550 0.510 0.650 0.826F-mea 0.530 0.540 0.645 0.798Prec 0.582 0.672 0.715 0.834E Recall 0.654 0.734 0.721 0.852F-mea 0.616 0.702 0.718 0.843Table 1: Performance of different approaches.Figure 2: Statistical performance of approachesFirst, we observe that, EM (Prec: 0.609, Recall:0.613, F-mea: 0.611) performs much better thanMI (Prec: 0.549, Recall: 0.513, F-mea: 0.529).This is because EM optimizes the frequencies ofquery words with EM algorithms.
In addition, itshould be noted that, the recall of MI is especiallyunsatisfactory, which is caused by its shortcomingon handling long entities.Second, when compared with EM, ES also hasmore than 15% increase in the three reference met-rics (15.1% on Prec, 20.2% on Recall and 17.7%on F-mea).
Here all increases are statistically sig-nificant with p-value closed to 0.
In depth anal-ysis indicates that this is because ES makes gooduse of the frequencies of query words in its princi-pal eigenspace, while EM algorithm trains the ob-served data (frequencies of query words) by sim-ply maximizing them using maximum likelihood.4 Conclusion and Future workWe proposed an unsupervised approach for querysegmentation.
After using n-gram model to es-timate term frequency matrix using term occur-rence statistics from the web, we explored a newmethod to select principal eigenvectors and calcu-late the similarities of query words for segmenta-tion.
Experiments demonstrated the effectivenessof our approach, with significant improvement insegmentation accuracy as compared to the previ-ous works.Our approach will be capable of extracting se-mantic concepts from queries.
Besides, it can ex-tended to Chinese word segmentation.
In future,we will further explore a new method of parame-ter k selection to achieve higher performance.ReferencesS.
Bergsma and Q. I. Wang.
2007.
Learning NounPhrase Query Segmentation.
In Proc of EMNLP-CoNLLR.
Jones, B. Rey, O. Madani, and W. Greiner.
2006.Generating query substitutions.
In Proc of WWW.I.T.
Jolliffe.
2002.
Principal Component Analysis.Springer, NY, USA.Andrew Y. Ng, Michael I. Jordan, Yair Weiss.
2001.On spectral clustering: Analysis and an algorithmIn Proc of NIPS.F.
Peng and D. Schuurmans.
2001.
Self-SupervisedChinese Word Segmentation.
Proc of the 4th Int?lConf.
on Advances in Intelligent Data Analysis.K.
M. Risvik, T. Mikolajewski, and P. Boros.
2003.Query Segmentation for Web Search.
In Proc ofWWW.Bin Tan, Fuchun Peng.
2008.
Unsupervised QuerySegmentation Using Generative Language Modelsand Wikipedia.
In Proc of WWW.W.
J. Teahan Rodger Mcnab Yingying Wen Ian H. Wit-ten .
2000.
A compression-based algorithm for Chi-nese word segmentation Computational Linguistics.Xin-Jing Wang, Wen Liu, Yong Qin.
2007.
A Search-based Chinese Word Segmentation Method.
In Procof WWW.Yair Weiss.
1999.
Segmentation using eigenvectors: aunifying view.
Proc.
IEEE Int?l Conf.
Computer Vi-sion, vol.
2, pp.
975-982.Jia Xu, Jianfeng Gao, Kristina Toutanova, Hermann.2008.
Bayesian Semi-Supervised Chinese Word Seg-mentation for Statistical Machine Translation.
InProc of COLING.188
