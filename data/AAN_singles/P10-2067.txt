Proceedings of the ACL 2010 Conference Short Papers, pages 365?370,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsActive Learning-Based Elicitation for Semi-Supervised Word AlignmentVamshi Ambati, Stephan Vogel and Jaime Carbonell{vamshi,vogel,jgc}@cs.cmu.eduLanguage Technologies Institute, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213, USAAbstractSemi-supervised word alignment aims toimprove the accuracy of automatic wordalignment by incorporating full or par-tial manual alignments.
Motivated bystandard active learning query samplingframeworks like uncertainty-, margin- andquery-by-committee sampling we proposemultiple query strategies for the alignmentlink selection task.
Our experiments showthat by active selection of uncertain andinformative links, we reduce the overallmanual effort involved in elicitation ofalignment link data for training a semi-supervised word aligner.1 IntroductionCorpus-based approaches to machine translationhave become predominant, with phrase-based sta-tistical machine translation (PB-SMT) (Koehn etal., 2003) being the most actively progressing area.The success of statistical approaches to MT canbe attributed to the IBM models (Brown et al,1993) that characterize word-level alignments inparallel corpora.
Parameters of these alignmentmodels are learnt in an unsupervised manner us-ing the EM algorithm over sentence-level alignedparallel corpora.
While the ease of automati-cally aligning sentences at the word-level withtools like GIZA++ (Och and Ney, 2003) has en-abled fast development of SMT systems for vari-ous language pairs, the quality of alignment is typ-ically quite low for language pairs like Chinese-English, Arabic-English that diverge from the in-dependence assumptions made by the generativemodels.
Increased parallel data enables better es-timation of the model parameters, but a large num-ber of language pairs still lack such resources.Two directions of research have been pursuedfor improving generative word alignment.
Thefirst is to relax or update the independence as-sumptions based on more information, usuallysyntactic, from the language pairs (Cherry andLin, 2006; Fraser and Marcu, 2007a).
The sec-ond is to use extra annotation, typically word-levelhuman alignment for some sentence pairs, in con-junction with the parallel data to learn alignmentin a semi-supervised manner.
Our research is inthe direction of the latter, and aims to reduce theeffort involved in hand-generation of word align-ments by using active learning strategies for care-ful selection of word pairs to seek alignment.Active learning for MT has not yet been ex-plored to its full potential.
Much of the litera-ture has explored one task ?
selecting sentencesto translate and add to the training corpus (Haf-fari and Sarkar, 2009).
In this paper we exploreactive learning for word alignment, where the in-put to the active learner is a sentence pair (S, T )and the annotation elicited from human is a set oflinks {aij , ?si ?
S, tj ?
T}.
Unlike previous ap-proaches, our work does not require elicitation offull alignment for the sentence pair, which couldbe effort-intensive.
We propose active learningquery strategies to selectively elicit partial align-ment information.
Experiments in Section 5 showthat our selection strategies reduce alignment errorrates significantly over baseline.2 Related WorkResearchers have begun to explore models thatuse both labeled and unlabeled data to buildword-alignment models for MT.
Fraser and Marcu(2006) pose the problem of alignment as a searchproblem in log-linear space with features com-ing from the IBM alignment models.
The log-365linear model is trained on available labeled datato improve performance.
They propose a semi-supervised training algorithm which alternates be-tween discriminative error training on the la-beled data to learn the weighting parameters andmaximum-likelihood EM training on unlabeleddata to estimate the parameters.
Callison-Burchet al (2004) also improve alignment by interpolat-ing human alignments with automatic alignments.They observe that while working with such datasets, alignments of higher quality should be givena much higher weight than the lower-quality align-ments.
Wu et al (2006) learn separate modelsfrom labeled and unlabeled data using the standardEM algorithm.
The two models are then interpo-lated to use as a learner in the semi-supervisedalgorithm to improve word alignment.
To ourknowledge, there is no prior work that has lookedat reducing human effort by selective elicitation ofpartial word alignment using active learning tech-niques.3 Active Learning for Word AlignmentActive learning attempts to optimize performanceby selecting the most informative instances to la-bel where ?informativeness?
is defined as maximalexpected improvement in accuracy.
The objectiveis to select optimal instance for an external expertto label and then run the learning method on thenewly-labeled and previously-labeled instances tominimize prediction or translation error, repeat-ing until either the maximal number of externalqueries is reached or a desired accuracy level isachieved.
Several studies (Tong and Koller, 2002;Nguyen and Smeulders, 2004; Donmez and Car-bonell, 2008) show that active learning greatlyhelps to reduce the labeling effort in various clas-sification tasks.3.1 Active Learning SetupWe discuss our active learning setup for wordalignment in Algorithm 1.
We start with an un-labeled dataset U = {(Sk, Tk)}, indexed by k,and a seed pool of partial alignment links A0 ={akij , ?si ?
Sk, tj ?
Tk}.
This is usually an emptyset at iteration t = 0.
We iterate for T itera-tions.
We take a pool-based active learning strat-egy, where we have access to all the automaticallyaligned links and we can score the links basedon our active learning query strategy.
The querystrategy uses the automatically trained alignmentmodel Mt from current iteration t for scoring thelinks.
Re-training and re-tuning an SMT systemfor each link at a time is computationally infeasi-ble.
We therefore perform batch learning by se-lecting a set of N links scored high by our querystrategy.
We seek manual corrections for the se-lected links and add the alignment data to thecurrent labeled data set.
The word-level alignedlabeled data is provided to our semi-supervisedword alignment algorithm for training an align-ment model Mt+1 over U .Algorithm 1 AL FOR WORD ALIGNMENT1: Unlabeled Data Set: U = {(Sk, Tk)}2: Manual Alignment Set : A0 = {akij ,?si ?Sk, tj ?
Tk}3: Train Semi-supervised Word Alignment using(U , A0)?M04: N : batch size5: for t = 0 to T do6: Lt = LinkSelection(U ,At,Mt,N )7: Request Human Alignment for Lt8: At+1 = At + Lt9: Re-train Semi-Supervised Word Align-ment on (U,At+1)?Mt+110: end forWe can iteratively perform the algorithm for adefined number of iterations T or until a certaindesired performance is reached, which is mea-sured by alignment error rate (AER) (Fraser andMarcu, 2007b) in the case of word alignment.
Ina more typical scenario, since reducing human ef-fort or cost of elicitation is the objective, we iterateuntil the available budget is exhausted.3.2 Semi-Supervised Word AlignmentWe use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi-supervised word alignment.
Manual alignmentsare incorporated in the EM training phase of thesemodels as constraints that restrict the summationover all possible alignment paths.
Typically in theEM procedure for IBM models, the training pro-cedure requires for each source sentence position,the summation over all positions in the target sen-tence.
The manual alignments allow for one-to-many alignments and many-to-many alignmentsin both directions.
For each position i in the sourcesentence, there can be more than one manuallyaligned target word.
The restricted training willallow only those paths, which are consistent with366the manual alignments.
Therefore, the restrictionof the alignment paths reduces to restricting thesummation in EM.4 Query Strategies for Link SelectionWe propose multiple query selection strategies forour active learning setup.
The scoring criteria isdesigned to select alignment links across sentencepairs that are highly uncertain under current au-tomatic translation models.
These links are diffi-cult to align correctly by automatic alignment andwill cause incorrect phrase pairs to be extracted inthe translation model, in turn hurting the transla-tion quality of the SMT system.
Manual correc-tion of such links produces the maximal benefit tothe model.
We would ideally like to elicit the leastnumber of manual corrections possible in order toreduce the cost of data acquisition.
In this sectionwe discuss our link selection strategies based onthe standard active learning paradigm of ?uncer-tainty sampling?
(Lewis and Catlett, 1994).
We usethe automatically trained translation model ?t forscoring each link for uncertainty, which consists ofbidirectional translation lexicon tables computedfrom the bidirectional alignments.4.1 Uncertainty Sampling: BidirectionalAlignment ScoresThe automatic Viterbi alignment produced bythe alignment models is used to obtain transla-tion lexicons.
These lexicons capture the condi-tional distributions of source-given-target P (s/t)and target-given-source P (t/s) probabilities at theword level where si ?
S and tj ?
T .
We de-fine certainty of a link as the harmonic mean of thebidirectional probabilities.
The selection strategyselects the least scoring links according to the for-mula below which corresponds to links with max-imum uncertainty:Score(aij/sI1, t1J) =2 ?
P (tj/si) ?
P (si/tj)P (tj/si) + P (si/tj)(1)4.2 Confidence Sampling: PosteriorAlignment probabilitiesConfidence estimation for MT output is an in-teresting area with meaningful initial exploration(Blatz et al, 2004; Ueffing and Ney, 2007).
Givena sentence pair (sI1, tJ1 ) and its word alignment,we compute two confidence metrics at alignmentlink level ?
based on the posterior link probabilityas seen in Equation 5.
We select the alignmentlinks that the initial word aligner is least confi-dent according to our metric and seek manual cor-rection of the links.
We use t2s to denote com-putation using higher order (IBM4) target-given-source models and s2t to denote source-given-target models.
Targeting some of the uncertainparts of word alignment has already been shownto improve translation quality in SMT (Huang,2009).
We use confidence metrics as an activelearning sampling strategy to obtain most informa-tive links.
We also experimented with other con-fidence metrics as discussed in (Ueffing and Ney,2007), especially the IBM 1 model score metric,but it did not show significant improvement in thistask.Pt2s(aij , tJ1 /sI1) =pt2s(tj/si,aij?A)?Mi pt2s(tj/si)(2)Ps2t(aij , sI1/tJ1 ) =ps2t(si/tj ,aij?A)?Ni ps2t(si/tj)(3)Conf1(aij/S, T ) =2?Pt2s?Ps2tPt2s+Ps2t(4)(5)4.3 Query by CommitteeThe generative alignments produced differ basedon the choice of direction of the language pair.
WeuseAs2t to denote alignment in the source to targetdirection and At2s to denote the target to sourcedirection.
We consider these alignments to be twoexperts that have two different views of the align-ment process.
We formulate our query strategyto select links where the agreement differs acrossthese two alignments.
In general query by com-mittee is a standard sampling strategy in activelearning(Freund et al, 1997), where the commit-tee consists of any number of experts, in this casealignments, with varying opinions.
We formulatea query by committee sampling strategy for wordalignment as shown in Equation 6.
In order tobreak ties, we extend this approach to select thelink with higher average frequency of occurrenceof words involved in the link.Score(aij) = ?
(6)where ?
=??
?2 aij ?
As2t ?At2s1 aij ?
As2t ?At2s0 otherwise4.4 Margin SamplingThe strategy for confidence based sampling onlyconsiders information about the best scoring link367conf(aij/S, T ).
However we could benefit frominformation about the second best scoring link aswell.
In typical multi-class classification prob-lems, earlier work shows success using such a?margin based?
approach (Scheffer et al, 2001),where the difference between the probabilities as-signed by the underlying model to the first bestand second best labels is used as a sampling cri-teria.
We adapt such a margin-based approach tolink-selection using the Conf1 scoring functiondiscussed in the earlier sub-section.
Our margintechnique is formulated below, where a?1ij anda?2ij are potential first best and second best scor-ing alignment links for a word at position i in thesource sentence S with translation T .
The wordwith minimum margin value is chosen for humanalignment.
Intuitively such a word is a possiblecandidate for mis-alignment due to the inherentconfusion in its target translation.Margin(i) =Conf1(a?1ij/S, T ) ?Conf1(a?2ij/S, T )5 Experiments5.1 Data SetupOur aim in this paper is to show that active learn-ing can help select the most informative alignmentlinks that have high uncertainty according to agiven automatically trained model.
We also showthat fixing such alignments leads to the maximumreduction of error in word alignment, as measuredby AER.
We compare this with a baseline wherelinks are selected at random for manual correction.To run our experiments iteratively, we automatethe setup by using a parallel corpus for which thegold-standard human alignment is already avail-able.
We select the Chinese-English language pair,where we have access to 21,863 sentence pairsalong with complete manual alignment.5.2 ResultsWe first automatically align the Cn-En corpus us-ing GIZA++ (Och and Ney, 2003).
We thenuse the learned model in running our link selec-tion algorithm over the entire corpus to determinethe most uncertain links according to each activelearning strategy.
The links are then looked up inthe gold-standard human alignment database andcorrected.
In case a link is not present in thegold-standard data, we introduce a NULL align-ment, else we propose the alignment as given inFigure 1: Performance of active sampling strate-gies for link selectionthe gold standard.
We select the partial align-ment as a set of alignment links and provide it toour semi-supervised word aligner.
We plot per-formance curves as number of links used in eachiteration vs. the overall reduction of AER on thecorpus.Query by committee performs worse than ran-dom indicating that two alignments differing indirection are not sufficient in deciding for uncer-tainty.
We will be exploring alternative formula-tions to this strategy.
We observe that confidencebased metrics perform significantly better than thebaseline.
From the scatter plots in Figure 1 1 wecan say that using our best selection strategy oneachieves similar performance to the baseline, butat a much lower cost of elicitation assuming costper link is uniform.We also perform end-to-end machine transla-tion experiments to show that our improvementof alignment quality leads to an improvement oftranslation scores.
For this experiment, we traina standard phrase-based SMT system (Koehn etal., 2007) over the entire parallel corpus.
We tuneon the MT-Eval 2004 dataset and test on a subsetof MT-Eval 2004 dataset consisting of 631 sen-tences.
We first obtain the baseline score whereno manual alignment was used.
We also train aconfiguration using gold standard manual align-ment data for the parallel corpus.
This is the max-imum translation accuracy that we can achieve byany link selection algorithm.
We now take thebest link selection criteria, which is the confidence1X axis has number of links elicited on a log-scale368System BLEU METEORBaseline 18.82 42.70Human Alignment 19.96 44.22Active Selection 20% 19.34 43.25Table 1: Alignment and Translation Qualitybased method and train a system by only selecting20% of all the links.
We observe that at this pointwe have reduced the AER from 37.09 AER to26.57 AER.
The translation accuracy as measuredby BLEU (Papineni et al, 2002) and METEOR(Lavie and Agarwal, 2007) also shows improve-ment over baseline and approaches gold standardquality.
Therefore we achieve 45% of the possibleimprovement by only using 20% elicitation effort.5.3 Batch SelectionRe-training the word alignment models after elic-iting every individual alignment link is infeasible.In our data set of 21,863 sentences with 588,075links, it would be computationally intensive to re-train after eliciting even 100 links in a batch.
Wetherefore sample links as a discrete batch, and trainalignment models to report performance at fixedpoints.
Such a batch selection is only going to besub-optimal as the underlying model changes withevery alignment link and therefore becomes ?stale?for future selections.
We observe that in some sce-narios while fixing one alignment link could po-tentially fix all the mis-alignments in a sentencepair, our batch selection mechanism still samplesfrom the rest of the links in the sentence pair.
Weexperimented with an exponential decay functionover the number of links previously selected, inorder to discourage repeated sampling from thesame sentence pair.
We performed an experimentby selecting one of our best performing selectionstrategies (conf ) and ran it in both configurations- one with the decay parameter (batchdecay) andone without it (batch).
As seen in Figure 2, thedecay function has an effect in the initial part ofthe curve where sampling is sparse but the effectgradually fades away as we observe more samples.In the reported results we do not use batch decay,but an optimal estimation of ?staleness?
could leadto better gains in batch link selection using activelearning.Figure 2: Batch decay effects on Conf-posteriorsampling strategy6 Conclusion and Future WorkWord-Alignment is a particularly challengingproblem and has been addressed in a completelyunsupervised manner thus far (Brown et al, 1993).While generative alignment models have been suc-cessful, lack of sufficient data, model assump-tions and local optimum during training are wellknown problems.
Semi-supervised techniques usepartial manual alignment data to address some ofthese issues.
We have shown that active learningstrategies can reduce the effort involved in elicit-ing human alignment data.
The reduction in ef-fort is due to careful selection of maximally un-certain links that provide the most benefit to thealignment model when used in a semi-supervisedtraining fashion.
Experiments on Chinese-Englishhave shown considerable improvements.
In futurewe wish to work with word alignments for otherlanguage pairs like Arabic and English.
We havetested out the feasibility of obtaining human wordalignment data using Amazon Mechanical Turkand plan to obtain more data reduce the cost ofannotation.AcknowledgmentsThis research was partially supported by DARPAunder grant NBCHC080097.
Any opinions, find-ings, and conclusions expressed in this paper arethose of the authors and do not necessarily reflectthe views of the DARPA.
The first author wouldlike to thank Qin Gao for the semi-supervisedword alignment software and help with runningexperiments.369ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
2004.
Confidence estimation for machinetranslation.
In Proceedings of Coling 2004, pages 315?321, Geneva, Switzerland, Aug 23?Aug 27.
COLING.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: parameter estimation.Computational Linguistics, 19(2):263?311.Chris Callison-Burch, David Talbot, and Miles Osborne.2004.
Statistical machine translation with word- andsentence-aligned parallel corpora.
In ACL 2004, page175, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Colin Cherry and Dekang Lin.
2006.
Soft syntactic con-straints for word alignment through discriminative train-ing.
In Proceedings of the COLING/ACL on Main con-ference poster sessions, pages 105?112, Morristown, NJ,USA.Pinar Donmez and Jaime G. Carbonell.
2008.
Optimizing es-timated loss reduction for active sampling in rank learning.In ICML ?08: Proceedings of the 25th international con-ference on Machine learning, pages 248?255, New York,NY, USA.
ACM.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervisedtraining for statistical word alignment.
In ACL-44: Pro-ceedings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, pages 769?776, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Alexander Fraser and Daniel Marcu.
2007a.
Getting thestructure right for word alignment: LEAF.
In Proceedingsof the 2007 Joint Conference on EMNLP-CoNLL, pages51?60.Alexander Fraser and Daniel Marcu.
2007b.
Measuring wordalignment quality for statistical machine translation.
Com-put.
Linguist., 33(3):293?303.Yoav Freund, Sebastian H. Seung, Eli Shamir, and NaftaliTishby.
1997.
Selective sampling using the query by com-mittee algorithm.
Machine.
Learning., 28(2-3):133?168.Qin Gao and Stephan Vogel.
2008.
Parallel implementa-tions of word alignment tool.
In Software Engineering,Testing, and Quality Assurance for Natural Language Pro-cessing, pages 49?57, Columbus, Ohio, June.
Associationfor Computational Linguistics.Gholamreza Haffari and Anoop Sarkar.
2009.
Active learn-ing for multilingual statistical machine translation.
InProceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International Joint Con-ference on Natural Language Processing of the AFNLP,pages 181?189, Suntec, Singapore, August.
Associationfor Computational Linguistics.Fei Huang.
2009.
Confidence measure for word alignment.In Proceedings of the Joint ACL and IJCNLP, pages 932?940, Suntec, Singapore, August.
Association for Compu-tational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of theHLT/NAACL, Edomonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL Demon-stration Session.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: an auto-matic metric for mt evaluation with high levels of corre-lation with human judgments.
In WMT 2007, pages 228?231, Morristown, NJ, USA.David D. Lewis and Jason Catlett.
1994.
Heterogeneous un-certainty sampling for supervised learning.
In In Proceed-ings of the Eleventh International Conference on MachineLearning, pages 148?156.
Morgan Kaufmann.Hieu T. Nguyen and Arnold Smeulders.
2004.
Active learn-ing using pre-clustering.
In ICML.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, pages 19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation ofmachine translation.
In ACL 2002, pages 311?318, Mor-ristown, NJ, USA.Tobias Scheffer, Christian Decomain, and Stefan Wrobel.2001.
Active hidden markov models for information ex-traction.
In IDA ?01: Proceedings of the 4th Interna-tional Conference on Advances in Intelligent Data Anal-ysis, pages 309?318, London, UK.
Springer-Verlag.Simon Tong and Daphne Koller.
2002.
Support vector ma-chine active learning with applications to text classifica-tion.
Journal of Machine Learning, pages 45?66.Nicola Ueffing and Hermann Ney.
2007.
Word-level con-fidence estimation for machine translation.
Comput.
Lin-guist., 33(1):9?40.Hua Wu, Haifeng Wang, and Zhanyi Liu.
2006.
Boost-ing statistical word alignment using labeled and unlabeleddata.
In Proceedings of the COLING/ACL on Main con-ference poster sessions, pages 913?920, Morristown, NJ,USA.
Association for Computational Linguistics.370
