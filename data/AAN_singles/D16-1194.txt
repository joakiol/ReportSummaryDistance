Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1892?1900,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNon-uniform Language Detection in Technical WritingWeibo Wang1, Abidalrahman Moh?d1, Aminul Islam2Axel J. Soto3, and Evangelos E. Milios11Faculty of Computer Science, Dalhousie University, Canada{weibo,amohd,eem}@cs.dal.ca2School of Computing and Informatics, University of Louisiana at Lafayette, USAaminul@louisiana.edu3School of Computer Science, University of Manchester, UKaxel.soto@manchester.ac.ukAbstractTechnical writing in professional environ-ments, such as user manual authoring, re-quires the use of uniform language.
Non-uniform language detection is a novel task,which aims to guarantee the consistency fortechnical writing by detecting sentences in adocument that are intended to have the samemeaning within a similar context but use dif-ferent words or writing style.
This paper pro-poses an approach that utilizes text similarityalgorithms at lexical, syntactic, semantic andpragmatic levels.
Different features are ex-tracted and integrated by applying a machinelearning classification method.
We tested ourmethod using smart phone user manuals, andcompared its performance against the state-of-the-art methods in a related area.
The experi-ments demonstrate that our approach achievesthe upper bound performance for this task.1 IntroductionTechnical writing, such as creating device operationmanuals and user guide handbooks, is a special writ-ing task that requires accurate text to describe a cer-tain product or operation.
To avoid ambiguity andbring accurate and straightforward understanding toreaders, technical writing requires consistency in theuse of terminology and uniform language (Farkas,1985).
There are always demands from modern in-dustries to improve the quality of technical docu-ments in cost-efficient ways.Non-uniform Language Detection (NLD) aims toavoid inner-inconsistency and ambiguity of techni-cal content by identifying non-uniform sentences.Such sentences are intended to have the same mean-ing or usage within a similar context but use differ-ent words or writing style.
However, even thoughnon-uniform sentences tend to have similar word-ing, similar sentence pairs do not necessarily indi-cate a non-uniform language instance.
For example,here are four similar sentence pairs cited from theiPhone user manual (Apple Inc., 2015), where onlytwo pairs are true non-uniform language instances:(1) tap the screen to show the controls.tap the screen to display the controls.
(2) tap the screen to show the controls.tap the screen to display the controls.
(3) if the photo hasn?t been downloaded yet, tapthe download notice first.if the video hasn?t been downloaded yet, tapthe download notice first.
(4) you can also turn blue tooth on or off in con-trol center.you can also turn wi-fi and blue tooth on oroff in control center.As we can see above, the pattern of differencewithin each sentence pair could be between oneword and one word, or one word and multiple words,or one sentence having extra words or phrases thatthe other sentence does not have.
Each pattern couldbe a true or false non-uniform language instancedepending on the content and context.
The word?show?
and ?display?
are synonyms in Example (1).Both sentences convey the same meaning, so theyare an instance of non-uniform language.
In Exam-ple (2), even though ?enter?
and ?write?
are not syn-onyms, since the two sentences describe the same1892operation, they should be considered as non-uniformlanguage as well.
In Example (3), even though theonly different words between the sentences, ?photo?and ?video?, are both media contents, because theyare different objects, they should not be regardedas non-uniform language.
In Example (4), it is afalse candidate because each sentence mentions dif-ferent functions.
However, the two sentences are un-equal in length, thus it is hard to know what the ex-tra phrase ?wi-fi and?
should be compared against.Therefore, it is challenging to distinguish true andfalse occurrences of non-uniform cases based on textsimilarity algorithms only, and finer grained analy-ses need to be applied.
To address the problem ofNLD, this paper proposes a methodology for detect-ing non-uniform language within a technical docu-ment at the sentence level.
A schematic diagram ofour approach is shown in Figure 1.Start TextContentStage 1:SimilarSentenceDetectorCandidateSentencePairsStage 2:SentencePairAnalysisIndependentFeaturesStage 3:ClassificationNon-uniformLanguageSentencePairsEndFigure 1: Schematic diagram of our approachIt is worth to mention that NLD is similar to Pla-giarism Detection and Paraphrase Detection (PD) asall these tasks aim to capture similar sentences withthe same meaning (Das and Smith, 2009).
However,the goal of authors in plagiarism and paraphrasingis to change as many words as possible to increasethe differences between texts, whereas in technicalwriting, the authors try to avoid such differences, butthey do not always succeed and thus NLD solutionsare needed.
Cases of plagiarism and paraphrasingwith high lexical differences will be typically clas-sified as NLD negative, and cases with low lexicaldifferences will be typically classified as NLD pos-itive.
While true positive cases for both NLD andPD can exist, there are not likely to happen in prac-tice since textual differences in PD tend to be muchhigher than in NLD.To address the NLD task, Natural Language Pro-cessing (NLP) techniques at lexical, syntactic, se-mantic, and pragmatic levels are utilized.
Our ap-proach also integrates resources such as Part-of-Speech (POS) tagger (Bird et al, 2009), Word-Net (Miller et al, 1990), Google Tri-gram Method(GTM) (Islam et al, 2012; Mei et al, 2015), andFlickr1.
Analyses from different perspectives areapplied, and the results are regarded as independentfeatures that are finally integrated by applying a clas-sification method based on Support Vector Machine(SVM).
A ground truth dataset created from threesmart phone user manuals is used for evaluation, andit is made publicly available2.
The experiments onthis dataset demonstrate that the proposed solutionto the NLD task is the most efficient method to date,and the final result is close to the upper bound per-formance.2 Related WorkNLD is closely related to PD, which aims to de-tect sentences that have essentially the same mean-ing.
However, paraphrase is a restatement usingdifferent words to make it appear different fromthe original text.
PD techniques cannot performwell on the NLD task as they focus on variationsat a coarser granularity.
We reviewed studies inthe PD area, and found the Recursive Auto-Encoder(RAE) (Socher et al, 2011), and the Semantic TextSimilarity (STS) (Islam and Inkpen, 2008) to bethe state-of-the-art methods using supervised andunsupervised-based PD, respectively.
However, allthe four examples provided in the introduction sec-tion would be recognized as paraphrases by theseanalyzers, even though only two of the pairs are realnon-uniform language cases.
Thus, state-of-the-artPD techniques are unable to make accurate judg-ments on these instances since PD do not addressthe necessary level of detail for the NLD task.Another related area to NLD is near-duplicate textdetection.
It focuses on short text such as mobilephone short messages, or tweets, which are intendedto have the same meaning but differ in terms of in-1Flickr: https://www.flickr.com/2The resource is available at: https://goo.gl/6wRchr1893formal abbreviations, transliterations, and networklanguages (Gong et al, 2008).
The detection andelimination of near-duplicate text is of great im-portance for other text language processing such asclustering, opinion mining, and topic detection (Sunet al, 2013).
However, the studies in this area focuson reducing the comparison time in large scale textdatabases and creating informal abbreviation corpus,rather than exploring the text similarity methods.Basic similarity methods, such as Longest CommonSubstring (LCS) are utilized, but they are not suffi-cient to address the NLD task as LCS captures thematching words and their order between texts andusing LCS alone will give high recall and low preci-sion for the NLD task.
For the following NLD neg-ative example, LCS returns a high similarity score:(5) If the photo hasn?t been downloaded yet, tapthe download notice first.If the music hasn?t been downloaded yet, tapthe download notice first.Examples of this type are common in technical writ-ing, so other features are needed besides LCS to rec-ognize NLD positives.There is a research domain named near-duplicatedocument detection, which seems literally related toNLD, but also represents a different task.
It focuseson documents that are identical in terms of writtencontent but differ in a small portion of the docu-ment such as advertisements, counters and times-tamps (Manku et al, 2007).
Such documents areimportant to be identified for web crawling and theautomatic collection of digital libraries.
Since thisarea focuses on the variations between two docu-ments, especially the variations on metadata, ratherthan the written content within one document, theirproposed solutions are not a good fit for the NLDtasks.3 Non-uniform Language DetectionAs we have shown in Figure 1, a framework consist-ing of three stages is proposed to address the NLDtask.
The first stage extracts candidate sentencepairs that have high text similarity within a docu-ment.
The second stage performs comprehensiveanalyses on each candidate sentence pair.
The anal-yses are performed at lexical, syntactical, semantic,and pragmatic levels, where multiple NLP resourcessuch as POS tagger, WordNet, GTM, and Flickr areutilized.
The final stage integrates all the analysisresults by applying a classification method based onSVM to classify the candidate sentence pairs as trueor false cases of non-uniform language.3.1 Stage 1: Similar Sentences DetectionTo extract the candidate sentence pairs, three textsimilarity algorithms are combined and applied atthe sentence level.
GTM is an unsupervised corpus-based approach for measuring semantic relatednessbetween texts.
LCS focuses on the word order ofsentences.
Cosine Similarity provides bag-of-wordsimilarity.
GTM, LCS, and Cosine Similarity areused to filter out the pairs based on semantics, sen-tence structure, and word frequency, respectively.The filtering thresholds were set by running ex-periments at the sentence level on the iPhone usermanual (Apple Inc., 2015).
Algorithm 1 is used toset the filtering threshold for each average sentencelength3.We utilize a sentence detector and a tokenizer4 todivide the text of the manual into a sentence set ofn sentence pairs (Line 2).
We separately run Algo-rithm 1 three times to set the threshold sets for GTM,LCS, and Cosine.
The thresholds are set based onthe lengths of both sentences of a sentence pair.
Theaverage length starts from 2 and is increased by oneonce the threshold for the current length is set.
Wediscovered that once the sentence length goes above10, the thresholds vary little.
Therefore, we stop thealgorithm when the threshold for pairs of averagelength equal to 10 is found (Line 6).For each different average length, the algorithmstarts by asking the user to input an initial similar-ity threshold and an increasing step value (Line 4-5).
An initial threshold range is generated based onthe user setting.
The lower bound of the range is Tand the upper bound of the range is T+Step (Line9-10).
Then the algorithm would loop over all thesentence pairs (Line 11-20) and add the pairs withinthe current threshold range into set C (Line 14-16).3See the Example (4) in Section 1, where two sentenceswithin one sentence pair could be unequal in length, thus wecompute the average length to represent the length of each can-didate pair.4OpenNLP: https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html1894Input : User ManualOutput: Threshold-Length_List [(T1, L1), ...]1 begin2 S[n]??
SentenceDetector(User Manual)3 L??
2 /*Initial average length of a sentence pair*/4 T ?
?Similarity threshold5 Step?
?Threshold increasing step6 while (L ?
10) do7 C ??
?
/*Initialize the output sentence container.
*/8 do9 Tlow ??
T10 Tup ??
T + Step11 for (i=0; i<n; i++) do12 for (j=0; j<n; j++) do13 AvgL??
(S[i] + S[j])/214 if AvgL ?
[L?
1, L) then15 if (Tlow ?
Sim(S[i], S[j])) and(Sim(S[i], S[j]) ?
Tup) then16 C add??
(S[i], S[j])17 end18 end19 end20 end21 T ?
?T+Step22 while (Check(C)=True) /*Checked by human,23 True when all the sentence pairs are not instances ofnon-uniform language.
*/ ;24 Threshold-Length_List add??
(Tlow, L)25 L++;26 end27 endAlgorithm 1: Setting similarity thresholdsThe similarity of sentence pairs above the previousthreshold and below the current threshold are cap-tured and analyzed (Line 15-16).
If they consist ofall false non-uniform language candidates, we repeatthe loop with a higher threshold to filter more falsecandidates.
Once we discover that a true candidate isfiltered by the current thresholds, we stop increasingand set the prior value as the threshold to maximizethe recall ratio.
The whole experiment is repeatedfor different sentence pair lengths.
The final thresh-olds for different similarity methods are shown inFigure 2.To filter the sentence pairs, we applied the thresh-olds of the three text similarity algorithms.
For ex-ample, assume there are two sentences of nine-wordlength on average.
The similarity scores of this pairhave to be above all the GTM, LCS and Cosinethresholds (which are 0.943, 0.836, and 0.932, ac-cording to Figure 2) to make it a candidate instance.By applying the thresholds shown in Figure 2,candidate pairs could be detected in reasonable scalein terms of the size of the corpus, and achieve goodFigure 2: Candidate filtering thresholdsrecall ratio as well.
As for precision, around 40% ofthe candidates are true non-uniform language cases,where the remaining candidates are supposed to befiltered in the second stage.3.2 Stage 2: Sentence Pair AnalysisIn this stage, we aim to determine for the two sen-tences of a candidate pair whether they describe thesame object or operation using different words orwriting style (i.e., true non-uniform language) orthey just appear similar but actually have differentintended meanings, by using the following features.3.2.1 Part-of-Speech Tagging AnalysisPOS tags are added for each candidate pair usingNLTK (Bird et al, 2009) tagger to gain a grammati-cal view over the sentences.As Table 1 shows, some differences in sentencecontent can be captured using POS tags, but somecannot.
Thus, it is necessary to make further syntac-tic and semantic analysis to distinguish true candi-dates from false ones.We categorized the different POS tags into the fol-lowing groups shown in Table 2.
The different POStags are mapped to different categories, which arethen used as one more feature of the sentence pairrepresentation.3.2.2 Character N-gram AnalysisIn the character N-gram analysis, the relatednessbetween the different words of each candidate pairis calculated in terms of character unigram, bigram1895Candidate Sentence Pair with POS Tag Ground TruthLink/NNPyour/PRPdevice/NNto/TOiTunes/NNSstores/NNS TrueLink/NNPyour/PRPdevice/NNto/TOiTunes/NNSstore/NN Candidatego/VBto/TOsettings/NNS>/SYSgeneral/JJ>/SYSaccessibility/NN>/SYSaudio/NN Falsego/VBto/TOsettings/NNS>/SYSgeneral/JJ>/SYSaccessibility/NN>/SYSvideo/NN CandidateHold/VBthe/DTpower/NNbutton/NNfor/INtwo/NNseconds/NNSto/TOshutdown/NNthe/DTdevice/NN TrueHold/VBthe/DTpower/NNbutton/NNfor/INtwo/NNseconds/NNSto/TOshut/VBNdown/RBthe/DTdevice/NN CandidateHold/VBthe/DTpower/NNbutton/NNfor/INtwo/NNseconds/NNSto/TOturn/VBNoff/INthe/DTdevice/NN TrueHold/VBthe/DTpower/NNbutton/NNfor/INtwo/NNseconds/NNSto/TOshut/VBNdown/RBthe/DTdevice/NN CandidateTable 1: POS analysis on candidate sentence pairsLabel Description Example1 Equal length, same POS tag /NN vs. /NN, /VB vs. /VB2 Equal length, plural noun with singular noun /NN vs. /NNS3 Equal length, different POS /NN vs. /VB4 Unequal length, extra article /NN vs. /DT/NN5 Unequal length, extra conjunction /NN vs. /CC/NN6 Unequal length, extra adjective /NN vs. /JJ/NN7 Other POS tag types.
/NN vs. N/ATable 2: POS tag categorizingand trigram similarity.
The character N-gram fre-quencies with a window size from 1 to 3 is firstlycalculated.
Then, the N-gram distance based on thefrequencies is calculated using the Common N-gramdistance (CNG) (Ke?elj and Cercone, 2004):d( f1, f2) = ?n?dom( f1)?dom( f2)( f1(n)?
f2(n)f1(n)+ f2(n)2)2 (1)where dom( fi) is the domain of function fi.
Inthe equation above, n represents a certain N-gramunit.
fi(n) represents the frequency of n in sen-tence i (i=1,2).
If n does not appear in sentence i,fi(n)=0.
The lower bound of the N-gram distance is0 (when the two units to be compared are exactly thesame).
The higher the value of N-gram distance, thelarger the difference, thus there is no upper bound.CNG was demonstrated to be a robust measure ofdissimilarity for character N-grams in different do-mains (Wo?kowicz and Ke?elj, 2013).3.2.3 WordNet Lexical Relation AnalysisFor a given candidate sentence pair, if the differ-ent wordingF are synonymous to each other, there isa high likelihood that the two sentences try to conveythe same meaning but using different expressions.On the other hand, if the different parts of a candi-date pair are not related at the lexical level, then itis reasonable to assume that this pair is describingdifferent objects/actions and thus they might not beinstances of non-uniform language.WordNet is utilized here to analyze the lexical re-lationship within each candidate pair to determinewhether they are synonyms to each other.
To per-form this analysis, we only used synset informa-tion from WordNet, and we only considered wordsas synonyms if they belong to a same synset.
Therationale is that a similar sentence pair tends to bean instance of non-uniform language if the differ-ent words are synonyms, rather than having other1896relationships such as hypernymy, hyponymy, andantonymy.
Therefore, we do not deem necessary toinclude these relationships into our analysis.
For ex-ample, given a similar sentence pair:(6) if the photo hasn?t been downloaded yet, tapthe download notice first.if the video hasn?t been downloaded yet, tapthe download notice first.The sentence pair above is not a non-uniform lan-guage instance.
However, the relatedness score be-tween ?photo?
and ?video?
given by Wu-Palmer met-ric (Wu and Palmer, 1994) using WordNet is 0.6,which is fairly high compared to a random wordpair.
Yet we do not know how these words are re-lated, e.g., ?photo is a kind of video?, ?photo is apart of video?, or ?photo and video are examples ofmedia content?.
Thus, we might make wrong judg-ments based on such a similarity score.
However,using synset information, we know that these wordsare not synonyms and thus probably not suggesting anon-uniform language instance.
Therefore, we con-sidered as one more feature of our classifier whethermismatching words belong to the same synset.3.2.4 GTM Word Relatedness AnalysisBesides text similarity, GTM also measures se-mantic relatedness between words.
To find the re-latedness between a pair of words, GTM takes intoaccount all the trigrams that start and end with thegiven pair of words and then normalizes their meanfrequency using unigram frequency of each of thewords as well as the most frequent unigram in theGoogle Web 1T N-gram corpus (Brants and Franz,2006), and extends the word relatedness method tomeasure document relatedness.3.2.5 Flickr Related Concept AnalysisIn some cases, word to word relatedness existsthat goes beyond dictionary definitions, such asmetonymy, in which a thing or concept is called notby its own name but rather by the name of some-thing associated in meaning with that thing or con-cept (K?vecses and Radden, 1998).
Metonymy de-tection is actually a task at the pragmatic level ofNLP area, which can be appied for NLD in techni-cal writing.Flickr is a popular photo sharing website that sup-ports time and location metadata and user taggingfor each photo.
Since the tags are added by humansand aim to describe or comment on a certain photo,the tags are somehow related from a human perspec-tive.
As a result, Flickr becomes a large online re-source with the potential to find metonymy relation-ships in text.Flickr made available statistical informationabout their dataset that can be used to query relatedconcepts of a certain word or phrase online.
Weutilized this resource to detect whether the differentparts within a candidate sentence pair are related atthe pragmatic level.
A boolean value that indicatesmetonymy relationship is obtained and regarded asanother feature of our sentence pair representationfor our NLD analysis.
Table 3 gives some examplesof relatedness that could be discovered in this stage.Different Content Is Metonymyaeroplane, A380 Truefilm, hollywood Trueapple, iPhone Trueaudio, grayscale FalseTable 3: Example of analysis using Flickr3.3 Stage 3: SVM ClassificationAll the metrics described above are regarded as fea-tures of our candidate sentence pairs.
To makea comprehensive judgment based on these dif-ferent signals, a classification method based onSVM (Vladimir and Vapnik, 1995) is applied.
Weimplemented the SVM classification using "e1071"package5 in R.Using our labeled corpus, we trained an SVMmodel on 61.5% of the data and used the remainingfor testing.4 Experiments and EvaluationIn this section, we present the dataset, experimentalwork and results, including results using other base-line methods for comparative purposes.4.1 Experiment DataWe downloaded smart phone user manuals ofiPhone (Apple Inc., 2015), LG (LG, 2009) and Sam-sung (Samsung, 2011), which are available online5https://cran.r-project.org/web/packages/e1071/1897as three raw datasets.
Then, we performed Stage 1three times on the three different datasets, and iden-tified 325 candidate sentence pairs (650 sentences)as part of Stage 1, which is considered as our can-didate dataset.
Before applying the sentence anal-ysis and classification stages, each candidate sen-tence pair in the dataset was labeled by three differ-ent annotators as true or false.
Then the ground truthfor each instance is generated by annotators?
voting.The annotators worked separately to label the sen-tence pairs.
Cases of disagreement were sent againto the annotators to double-check their judgement.Some statistics from the manuals are shown in Table4.DataSourceData Volume(Pages)Candidate Pairs(True, False)iPhone 196 208 (102, 106)LG 274 54 (16, 38)Samsung 190 63 (32, 31)Table 4: Experiment data distributionTo prepare for the SVM based classification stage,we split the dataset into a training set DStrain, and atesting set DStest.
Considering that the data distri-bution is nearly balanced in terms of true and falseinstances, DStrain was formed by randomly select-ing 200 instances from the dataset and the remaining125 instances were used for DStest.4.2 Evaluation Methods and ResultsThe performance of each annotator against the ma-jority voting is evaluated in terms of Precision, Re-call, Accuracy, and F-measure.
These results alongwith the number of true/ false, positive/ negativecases for each annotator are presented in Table 5.Parameters Expert 1 Expert 2 Expert 3True-positive 130 99 125True-negative 161 164 166False-positive 20 51 25False-negative 14 11 9Precision 86.67 66.00 83.33Recall 90.27 90.00 93.28Accuracy 89.54 80.92 89.54F-Measure 88.43 76.15 88.03Table 5: Evaluation of annotators performanceTo measure the agreement among annotators, theFleiss?
Kappa test (Fleiss and Cohen, 1973) is used.Fleiss?
Kappa is an extension of Cohen?s Kappa (Co-hen, 1968).
Unlike Cohen?s Kappa, which onlymeasures the agreement between two annotators,Fleiss?
Kappa measures the agreement among threeor more annotators.
In our case, we have 3 anno-tators (the annotator number n is 3), each annotatorlabeled 325 candidate pairs (the subject volume N is325), each candidate pair is labeled either 0 or 1 (thevalue of category k is 2).
The final Fleiss?
KappaValue is 0.545, which indicates a moderate agree-ment level (0.41-0.60) based on the Kappa Interpre-tation Model (Fleiss and Cohen, 1973).
In otherwords, the performance of the annotators reveal thatthe NLD task is not simple, since there are manycases that are ambiguous and hard to make accuratejudgments on, even for humans.As Table 5 shows, the best performance of an-notators is highlighted and regarded as the upperbound performance (UB) of the NLD task on ourdataset.
The state-of-the-art unsupervised PD sys-tem named STS (Islam and Inkpen, 2008), as wellas the state-of-the-art supervised PD system namedRAE (Socher et al, 2011), are utilized to generatethe baselines of the NLD task.
STS uses the simi-larity score of 0.5 as the threshold to evaluate theirmethod in the PD task.
RAE applies supervisedlearning to classify a pair as a true or false instanceof paraphrasing.
These approaches are utilized onour evaluation as baselines for the NLD task.After defining the upper bound and baselineperformances, we evaluated our proposed method,which we name as Non-uniform Language Detect-ing System (NLDS), by training the SVM classifieron DStrain, and then performing classification usingthe SVM classifier on DStest.
The result is shown inTable 6 as the NLDS method.
The first row presentsthe upper bound performance and the following tworows present the baseline performances.To assess the importance of each feature utilizedin the proposed framework, we performed a featureablation study (Cohen and Howe, 1988) on N-gram,POS analysis, lexical analysis (GTM and WordNet),and Flickr, separately on the DStest dataset.
The re-sults are listed in Table 6.A series of cross-validation and Student?s t-testsare applied after running NLDS, STS, RAE, and UB1898Method R(%) P (%) A(%) F1(%)UB 92.38 86.67 89.54 88.43STS 100 46.15 46.15 63.16RAE 100 46.40 46.40 63.39Uni-gram 11.11 35.29 52.80 16.90Bi-gram 44.44 61.54 64.00 51.61Tri-gram 50.00 62.79 65.60 55.67POS 77.78 72.77 78.40 76.52Lexical 85.18 59.74 68.80 70.23Flickr 48.96 94.00 74.00 64.38NLDS 80.95 96.22 88.80 87.93Table 6: Evaluation of NLDSmethods on the F-measure metric.
The tests revealthat the performance of NLDS is significantly bet-ter than STS and RAE, no significant differencescould be found between UB and NLDS.
These re-sults demonstrate that NLDS would represent an ef-fective approach for NLD that is on pair with anno-tator judgement and overcomes state-of-the-art ap-proaches for related tasks.4.3 DiscussionAs Table 6 shows, the PD systems STS and RAE re-gard all the test cases as true non-uniform languagecases, so the recall ratio is 1 but the precision is low.It is worth noting that by using character N-gramanalysis alone, it is not possible to obtain good re-sults.
This is because the character N-gram analysisusing a probabilistic method is unable to capture anydifference or relatedness in the meaning, while theNLD task relies heavily on discovering such relat-edness.
The reason we applied the N-gram analysisis to use it as a supplementary method to catch dif-ferences such as between ?shut down?
(two words)and ?shutdown?
(one word), or some spelling errors.POS analysis provides a syntactic perspec-tive for the text instances.
For instances,?then(/RB)?
versus ?and(/CC)?, and ?store(/NN)?versus ?stores(/NNS)?, the differences can be re-flected in POS tags.
Yet, POS analysis alonecould not capture the difference between words suchas ?writing(/VBG)?
versus ?entering(/VBG)?
sincethey share the same POS tag.
These features makePOS analysis outperform the character N-gram anal-ysis, but not semantic-based approaches.Lexical analysis (GTM and WordNet) achievesthe best recall ratio since it can provide semantic re-latedness, which is the most important aspect for theNLD task.
Flickr is utilized as a supplementary re-source to provide pragmatic relatedness.By combining the different types of analysesabove, the differences of each sentence pair are an-alyzed at different NLP levels and thus, the relat-edness and difference from structural, grammati-cal, syntactic, semantic and pragmatic perspectivescan be captured and integrated by the classificationmethod.5 ConclusionsThis paper proposes NLDS to detect non-uniformlanguage for technical writings at sentence level.Text, stream-based, and word similarity algorithmsat the lexical, syntactic, semantic, and pragmaticlevels are integrated through an SVM-based classi-fication method.
To evaluate the proposed method,three annotators manually labeled all the candidateinstances identified in Stage 1.
Then we assignedthe ground truth for each instance pair by annota-tors?
voting.
Fleiss?
Kappa test is applied to reflectthe difference of human judgments and thus to re-veal the difficulty of this task.We also evaluated each annotator against theground truth, and defined the best performance ofhuman as the upper bound performance for this task.With the generated ground truth, a series of experi-ments using our implemented system were carriedout with different smart phone user manuals data.We evaluated the results by comparing the outcomeof the classifier with the results using each singlefeature, as well as the state-of-the-art PD methods.Considering the different annotators?
judgmentsas reflected by Fleiss?
Kappa Value, the NLD task isfairly difficult.
Yet, the performance of our systemis close to human performance.
The experiments re-veal that our solution is the most effective method todate and the performance is close to the upper boundthat we defined.
As for future work, we would applydeeper analysis on true non-uniform language pairsto indicate which sentence of the pair fits better withthe style and language of the rest of the document.We would then provide a semi-automatic correctionfunction to facilitate authors with the task of remov-ing non-uniform language occurrences.1899AcknowledgmentsThis research work was supported by Innovatia Inc.and NSERC.
We are thankful to our colleagues An-drew Albert, David Crowley, and Erika Allen whoproposed and defined this NLD task, and providedexpertise that contributed on the preparation of thispaper.ReferencesApple Inc. 2015. iPhone User Guide For iOS 8.4 Soft-ware.
https://manuals.info.apple.com/MANUALS/1000/MA1565/en_US/iphone_user_guide.pdf.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural language processing with Python. "
O?Reilly Me-dia, Inc.".Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.Paul R Cohen and Adele E Howe.
1988.
How evalua-tion guides AI research: The message still counts morethan the medium.
AI magazine, 9(4):35.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement provision for scaled disagreement or partialcredit.
Psychological bulletin, 70(4):213.Dipanjan Das and Noah A Smith.
2009.
Paraphrase iden-tification as probabilistic quasi-synchronous recogni-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pages 468?476.
Association for Computational Linguistics.David K Farkas.
1985.
The concept of consistency inwriting and editing.
Journal of Technical Writing andCommunication, 15(4):353?364.Joseph L Fleiss and Jacob Cohen.
1973.
The equiva-lence of weighted kappa and the intraclass correlationcoefficient as measures of reliability.
Educational andpsychological measurement.Caichun Gong, Yulan Huang, Xueqi Cheng, and ShuoBai.
2008.
Detecting near-duplicates in large-scaleshort text databases.
In Advances in Knowledge Dis-covery and Data Mining, pages 877?883.
Springer.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data (TKDD), 2(2):10.Aminul Islam, Evangelos Milios, and Vlado Ke?elj.2012.
Text similarity using google tri-grams.
InAdvances in Artificial Intelligence, pages 312?317.Springer.Vlado Ke?elj and Nick Cercone.
2004.
CNG methodwith weighted voting.
In Ad-hoc Authorship Attri-bution Competition.
Proceedings 2004 Joint Interna-tional Conference of the Association for Literary andLinguistic Computing and the Association for Comput-ers and the Humanities (ALLC/ACH 2004), G?teborg,Sweden.Zolt?n K?vecses and G?nter Radden.
1998.
Metonymy:Developing a cognitive linguistic view.
Cognitive Lin-guistics (includes Cognitive Linguistic Bibliography),9(1):37?78.LG.
2009.
LG600G User Guide.
https://www.tracfone.com/images/en/phones/TFLG600G/manual.pdf.Gurmeet Singh Manku, Arvind Jain, and AnishDas Sarma.
2007.
Detecting near-duplicates for webcrawling.
In Proceedings of the 16th internationalconference on World Wide Web, pages 141?150.
ACM.Jie Mei, Xinxin Kou, Zhimin Yao, Andrew Rau-Chaplin,Aminul Islam, Abidalrahman Moh?d, and Evange-los E. Milios.
2015.
Efficient computation ofco-occurrence based word relatedness.
DemoURL:http://ares.research.cs.dal.ca/gtm/.George A Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J Miller.
1990.
In-troduction to wordnet: An on-line lexical database*.International journal of lexicography, 3(4):235?244.Samsung.
2011.
Samsung 010505d5 cellphone user manual.
http://cellphone.manualsonline.com/manuals/mfg/samsung/010505d5.html?p=53.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural Infor-mation Processing Systems 24.Yifang Sun, Jianbin Qin, and Wei Wang.
2013.
Nearduplicate text detection using frequency-biased signa-tures.
In Web Information Systems Engineering?WISE2013, pages 277?291.
Springer.Vapnik N Vladimir and V Vapnik.
1995.
The nature ofstatistical learning theory.Jacek Wo?kowicz and Vlado Ke?elj.
2013.
Evaluationof n-gram-based classification approaches on classicalmusic corpora.
In Mathematics and computation inmusic, pages 213?225.
Springer.Zhibiao Wu and Martha Palmer.
1994.
Verbs se-mantics and lexical selection.
DemoURL:http://ws4jdemo.appspot.com/?mode=w&s1=&w1=photo&s2=&w2=video.1900
