Extracting Constraints on Word Usage from Large Text CorporaKathleen McKeown and Rebecca PassonneauDepartment ofComputer Science450 Computer Science BuildingColumbia UniversityPROJECT GOALSOur research focuses on the identification of word usage con-straints fi'om large text corpora.
Such constraints are usefulboth for the problem of selecting vocabulary for languagegeneration and for disambiguating lexical meaning in inter-pretation.
We are developing systems that can automaticallyextract such constraints from corpora nd empirical methodsfor analyzing text.
Identified constraints will be representedin a lexicon that will be tested computationally aspart of anatural language system.
We are also identifying lexical con-straints for machine translation using the aligned Hansard cor-pus as training data and are identifying many-to-many wordalignments.One primary class of constraints we are examining is lexical;that is, constraints on word usage arriving from collocations(word pairs or phrases that commonly appear together).
Weare also looking at constraints deriving from domain scaleswhich influence use of scalar adjectives and determiners, con-straints on temporal markers and tense, constraints on refer-ence over text, and constraints on cue words and phrases thatmay be used to convey explicit information about discoursestructure.RECENT RESULTS?
Licensed Xtract, a collocation extraction system withwindows interface, to 26 sites, including 1 commercialsite.?
Completed implementation f a collocation translationsystem called Champollion, which uses an incremental,statistical lgorithm and ported Champollion from a DOSto Unix platform., Evaluated Champollion using 3 human judges on 2 year'sworth of Hansard's data, yielding an average of 77%accuracy on one year and 61% accuracy on the otherwith problems on the second set due to reference databasecorpus ize.?
Developed a system that automatically extracts relevantdata from text corpora nd corpora-based databases forlinguistic tests proposed for determining which is themarked ement of a pair of antonymous adjectives.Assessed the statistical significance of the markedness re-suits, showing that some simple linguistic tests are goodindicators of markedness, while others fail to performwell.
We combined the tests using a smoothed log-linearregression model yielding a small, but statistically sig-nificant improvement.Using the results of a corpus analysis of automaticallycollected pairs of conjoined adjectives, designed a graphmodel to separate adjectives into sub-scales (positive andnegative) according to the sub-graphs formed.Implemented and evaluated a genetic programming al-gorithm to induce decision trees for cue word disam-biguation based on lexical and part of speech data in alarge text corpus.Analyzed istribution of NPs in Pear corpus of oral nar-ratives, leading to new, more comprehensive formulationof the factors correlated with definiteness.Analyzed co-occurrence data of 16 aspectual verbs inBrown corpus (e.g., 'begin', 'continue') to rank them bytheir ability to categorize aspectual class of their verbarguments.
Six aspectual verbs account for 90% of thedifferent verbs appearing as arguments.PLANS FOR THE COMING YEARFor collocation translation, we are continuing evaluation ofChampollion, using several years worth of the Hansard as areference corpus and testing on separate data.
We will alsoinvestigate methods for including prepositions in the transla-tion algorithm.
For scalar adjectives, we are implementing thegraph model so that we can automatically partially order de-ments of groups of semantically related scalar adjectives thatare produced by a program we implemented in earlier years.For analysis of aspectual verbs, we plan to compare aspectualdata derived from Brown with similar data from a new corpus,or to integrate a supplementary classification method (e.g., useof independent semantic net) with the current distributionalmodel.452
