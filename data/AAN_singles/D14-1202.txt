Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1881?1890,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCoarse-grained Candidate Generation and Fine-grained Re-ranking forChinese Abbreviation PredictionLongkai Zhang Houfeng Wang Xu SunKey Laboratory of Computational Linguistics (Peking University)Ministry of Education, Chinazhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cnAbstractCorrectly predicting abbreviations giventhe full forms is important in many natu-ral language processing systems.
In thispaper we propose a two-stage method tofind the corresponding abbreviation givenits full form.
We first use the contextualinformation given a large corpus to get ab-breviation candidates for each full formand get a coarse-grained ranking throughgraph random walk.
This coarse-grainedrank list fixes the search space inside thetop-ranked candidates.
Then we use a sim-ilarity sensitive re-ranking strategy whichcan utilize the features of the candidatesto give a fine-grained re-ranking and se-lect the final result.
Our method achievesgood results and outperforms the state-of-the-art systems.
One advantage of ourmethod is that it only needs weak super-vision and can get competitive results withfewer training data.
The candidate genera-tion and coarse-grained ranking is totallyunsupervised.
The re-ranking phase canuse a very small amount of training datato get a reasonably good result.1 IntroductionAbbreviation Prediction is defined as finding themeaningful short subsequence of characters giventhe original fully expanded form.
As an example,?HMM?
is the abbreviation for the correspond-ing full form ?Hidden Markov Model?.
Whilethe existence of abbreviations is a common lin-guistic phenomenon, it causes many problems likespelling variation (Nenadi?c et al., 2002).
The dif-ferent writing manners make it difficult to identifythe terms conveying the same concept, which willhurt the performance of many applications, suchas information retrieval (IR) systems and machinetranslation (MT) systems.Previous works mainly treat the Chinese ab-breviation generation task as a sequence labelingproblem, which gives each character a label to in-dicate whether the given character in the full formshould be kept in the abbreviation or not.
Thesemethods show acceptable results.
However theyrely heavily on the character-based features, whichmeans it needs lots of training data to learn theweights of these context features.
The perfor-mance is good on some test sets that are similar tothe training data, however, when it moves to an un-seen context, this method may fail.
This is alwaystrue in real application contexts like the social me-dia where there are tremendous new abbreviationsburst out every day.A more intuitive way is to find the full-abbreviation pairs directly from a large text cor-pus.
A good source of texts is the news texts.
Ina news text, the full forms are often mentionedfirst.
Then in the rest of the news its correspondingabbreviation is mentioned as an alternative.
Theco-occurrence of the full form and the abbrevia-tion makes it easier for us to mine the abbreviationpairs from the large amount of news texts.
There-fore, given a long full form, we can generate itsabbreviation candidates from the given corpus, in-stead of doing the character tagging job.For the abbreviation prediction task, the candi-date abbreviation must be a sub-sequence of thegiven full form.
An intuitive way is to selectall the sub-sequences in the corpus as the can-didates.
This will generate large numbers of ir-relevant candidates.
Instead, we use a contextualgraph random walk method, which can utilize thecontextual information through the graph, to selecta coarse grained list of candidates given the fullform.
We only select the top-ranked candidates toreduce the search space.
On the other hand, thecandidate generation process can only use limitedcontextual information to give a coarse-grainedranked list of candidates.
During generation, can-1881didate level features cannot be included.
There-fore we propose a similarity sensitive re-rankingmethod to give a fine-grained ranked list.
We thenselect the final result based on the rank of eachcandidate.The contribution of our work is two folds.Firstly we propose an improved method for abbre-viation generation.
Compared to previous work,our method can perform well with less trainingdata.
This is an advantage in the context of so-cial media.
Secondly, we build a new abbreviationcorpus and make it publicly available for future re-search on this topic.The paper is structured as follows.
Section 1gives the introduction.
In section 2 we describethe abbreviation task.
In section 3 we describethe candidate generation part and in section 4 wedescribe the re-ranking part.
Experiments are de-scribed in section 5.
We also give a detailed anal-ysis of the results in section 5.
In section 6 relatedworks are introduced, and the paper is concludedin the last section.2 Chinese Abbreviation PredictionSystemChinese Abbreviation Prediction is the task ofselecting representative characters from the longfull form1.
Previous works mainly use the se-quence labeling strategies, which views the fullform as a character sequence and give each char-acter an extra label ?Keep?
or ?Skip?
to indicatewhether the current character should be kept inthe abbreviation.
An example is shown in Table1.
The sequence labeling method assumes thatthe character context information is crucial to de-cide the keep or skip of a character.
However,we can give many counterexamples.
An exam-ple is ??????
(Peking University) and ??????
(Tsinghua University), whose abbrevia-tions correspond to ????
and ????
respec-tively.
Although sharing a similar character con-text, the third character ???
is kept in the first caseand is skipped in the second case.We believe that a better way is to extract theseabbreviation-full pairs from a natural text corpuswhere the full form and its abbreviation co-exist.Therefore we propose a two stage method.
Thefirst stage generates a list of candidates given alarge corpus.
To reduce the search space, we adopt1Details of the difference between English and Chineseabbreviation prediction can be found in Zhang et al.
(2012).Full form ?
?
?
?Status Skip Keep Keep SkipResult ?
?Table 1: The abbreviation ????
of the full form??????
(Hong Kong University)graph random walk to give a coarse-grained rank-ing and select the top-ranked ones as the can-didates.
Then we use a similarity sensitive re-ranking method to decide the final result.
Detaileddescription of the two parts is shown in the follow-ing sections.3 Candidate Generation through GraphRandom Walk3.1 Candidate Generation and GraphRepresentationChinese abbreviations are sub-sequences of thefull form.
We use a brute force method to selectall strings in a given news article that is the sub-sequence of the full form.
The brute force methodis not time consuming compared to using morecomplex data structures like trie tree, because ina given news article there are a limited number ofsub-strings which meet the sub-sequence criteriafor abbreviations.
When generating abbreviationcandidates for a given full form, we require thefull form should appear in the given news articleat least once.
This is a coarse filter to indicate thatthe given news article is related to the full form andtherefore the candidates generated are potentiallymeaningful.The main motivation of the candidate genera-tion stage in our approach is that the full form andits abbreviation tend to share similar context in agiven corpus.
To be more detailed, given a wordcontext window w, the words that appear in thecontext window of the full form tend to be sim-ilar to those words in the context window of theabbreviations.We use a bipartite graph G(Vword, Vcontext, E)to represent this phenomena.
We build bipartitegraphs for each full form individually.
For a givenfull form vfull, we first extract all its candidateabbreviations VC.
We have two kinds of nodesin the bipartite graph: the word nodes and thecontext nodes.
We construct the word nodes asVword= VC?
{vfull}, which is the node set ofthe full form and all the candidates.
We constructthe context nodes Vcontextas the words that appear1882in a fixed window of Vword.
To reduce the size ofthe graph, we make two extra assumptions: 1) Weonly consider the nouns and verbs in the contextand 2) context words should appear in the vocab-ulary for more than a predefined threshold (i.e.
5times).
Because G is bipartite graph, the edges Eonly connect word node and context nodes.
Weuse the number of co-occurrence of the candidateand the context word as the weight of each edgeand then form the weight matrix W .
Details of thebipartite graph construction algorithm are shownin Table 2.
An example bipartite graph is shownin figure 1.Figure 1: An example of the bipartite graph rep-resentation.
The full form is ??????
(HongKong University), which is the first node on theleft.
The three candidates are ???
?, ????,???
?, which are the nodes on the left.
Thecontext words in this example are ?????
(TsuiLap-chee, the headmaster of Hong Kong Uni-versity), ????
(Enrollment), ????
(Hold), ????
(Enact), ????
(Subway), which are the nodeson the right.
The edge weight is the co-occurrenceof the left word and the right word.3.2 Coarse-grained Ranking Using RandomWalksWe perform Markov Random Walk on the con-structed bipartite graph to give a coarse-grainedranked list of all candidates.
In random walk, awalker starts from the full form source node S(in later steps, vi) and randomly walks to anothernode vjwith a transition probability pij.
In ran-dom walk we assume the walker do the walking ntimes and finally stops at a final node.
When thewalking is done, we can get the probability of eachnode that the walker stops in the end.
Becausethe destination of each step is selected based ontransition probabilities, the word node that sharesmore similar contexts are more likely to be the fi-nal stop.
The random walk method we use is sim-ilar to those defined in Norris (1998); Zhu et al.
(2003); Sproat et al.
(2006); Hassan and Menezes(2013); Li et al.
(2013).The transition probability pijis calculated us-ing the weights in the weight matrix W and thennormalized with respect to the source node viwiththe formula pij=wij?lwil.
When the graph ran-dom walk is done, we get a list of coarse-rankedcandidates, each with a confidence score derivedfrom the context information.
By performing thegraph random walk, we reduce the search spacefrom exponential to the top-ranked ones.
Now weonly need to select the final result from the candi-dates, which we will describe in the next section.4 Candidate Re-rankingAlthough the coarse-grained ranked list can serveas a basic reference, it can only use limited in-formation like co-occurrence.
We still need a re-ranking process to decide the final result.
The rea-son is that we cannot get any candidate-specificfeatures when the candidate is not fully gener-ated.
Features such as the length of a candidate areproved to be useful to rank the candidates by pre-vious work.
In this section we describe our secondstage for abbreviation generation, which we use asimilarity sensitive re-ranking method to find thefinal result.4.1 Similarity Sensitive Re-rankingThe basic idea behind our similarity sensitive re-ranking model is that we penalize the mistakesbased on the similarity of the candidate and thereference.
If the model wrongly selects a less sim-ilar candidate as the result, then we will attach alarge penalty to this mistake.
If the model wronglychooses a candidate but the candidate is similar tothe reference, we slightly penalize this mistake.The similarity between a candidate and the ref-erence is measured through character similarity,which we will describe later.1883Input: the full form vfull, news corpus UOutput: bipartite graph G(Vword, Vcontext, E)Candidate vector Vc= ?, Vcontext= ?for each document d in Uif d contains vfulladd all words w in the window of vfullinto Vcontextfor each n-gram s in dif s is a sub-sequence of vfulladd s into Vcadd all word w in the window of s into Vcontextend ifend forend ifend forVword= Vc?
{vfull}for each word viin Vwordfor each word vjin Vcontextcalculate edge weight in E based on co-occurrenceend forend forReturn G(Vword, Vcontext, E)Table 2: Algorithm for constructing bipartite graphsWe first give some notation of the re-rankingphase.1.
f(x, y) is a scoring function for a given com-bination of x and y, where x is the original fullform and y is an abbreviation candidate.
For agiven full form xiwith K candidates, we assumeits corresponding K candidates are y1i,y2i,...,yKi.2.
evaluation function s(x, y) is used to mea-sure the similarity of the candidate to the refer-ence, where x is the original full form and y is oneabbreviation candidate.
We require that s(x, y)should be in [0, 1] and s(x, y) = 1 if and only if yis the reference.One choice for s(x, y) may be the indicatorfunction.
However, indicator function returns zerofor all false candidates.
In the abbreviation predic-tion task, some false candidates are much closer tothe reference than the rest.
Considering this, weuse a Longest Common Subsequence(LCS) basedcriterion to calculate s(x, y).
Suppose the lengthof a candidate is a, the length of the reference is band the length of their LCS is c, then we can defineprecision P and recall R as:P =ca,R =cb,F =2 ?
P ?RP +R(1)It is easy to see that F is a suitable s(x, y).Therefore we can use the F-score as the value fors(x, y).3.
?
(x, y) is a feature function which returns am dimension feature vector.
m is the number offeatures in the re-ranking.4.
~w is a weight vector with dimension m.~wT?
(x, y) is the score after re-ranking.
The candi-date with the highest score will be our final result.Given these notations, we can now describe ourre-ranking algorithm.
Suppose we have the train-ing set X = {x1, x2, ..., xn}.
We should find theweight vector ~w that can minimize the loss func-tion:Loss(~w) =n?i=1k?j=1((s(xi, y1i)?
s(xi, yji))?
I(~wT?
(xi, yji) ?
~wT?
(xi, y1i)))(2)1884I(x) is the indicator function.
It equals to 1if and only if x ?
0.
I(j) = 1 means that thecandidate which is less ?similar?
to the referenceis ranked higher than the reference.
Intuitively,Loss(~w) is the weighted sum of all the wronglyranked candidates.It is difficult to optimize Loss(~w) becauseLoss(~w) is discontinuous.
We make a relaxationhere2:L(~w) =n?i=1k?j=1((s(xi, y1i)?
s(xi, yji))?11 + e?~wT(?(xi,yji)??
(xi,y1i)))?12n?i=1k?j=1((s(xi, y1i)?
s(xi, yji))?
I(~wT?
(xi, yji) ?
~wT?
(xi, y1i)))=12Loss(~w)(3)From the equations above we can see that2L(~w) is the upper bound of our loss functionLoss(~w).
Therefore we can optimize L(~w) to ap-proximate Loss(~w).We can use optimization methods like gradientdescent to get the ~w that minimize the loss func-tion.
Because L is not convex, it may go into a lo-cal minimum.
In our experiment we held out 10%data as the develop set and try random initializa-tion to decide the initial ~w.4.2 Features for Re-rankingOne advantage of the re-ranking phase is that itcan now use features related to candidates.
There-fore, we can use a variety of features.
We list themas follows.1.
The coarse-grained ranking score from thegraph random walk phase.
From the de-scription of the previous section we know thatthis score is the probability a ?walker?
?walk?from the full form node to the current candi-date.
This is a coarse-grained score becauseit can only use the information of words in-side the window.
However, it is still informa-tive because in the re-ranking phase we can-not collect this information directly.2To prove this we need the following two inequalities: 1)when x ?
0, I(x) ?21+e?xand 2) s(xi, y1i) ?
s(xi, yji) ?0.2.
The character uni-grams and bi-grams inthe candidate.
This kind of feature cannotbe used in the traditional character taggingmethods.3.
The language model score of the candi-date.
In our experiment, we train a bi-gramlanguage model using Laplace smoothing onthe Chinese Gigaword Data3.4.
The length of the candidate.
Intuitively,abbreviations tend to be short.
Thereforelength can be an important feature for the re-ranking.5.
The degree of ambiguity of the candidate.We first define the degree of ambiguity diof acharacter cias the number of identical wordsthat contain the character.
We then define thedegree of ambiguity of the candidate as thesum of all diin the candidates.
We need a dic-tionary to extract this feature.
We collect allwords in the PKU data of the second Interna-tional Chinese Word Segmentation Bakeoff4.6.
Whether the candidate is in a word dictio-nary.
We use the PKU dictionary in feature5.7.
Whether all bi-grams are in a word dictio-nary.
We use the PKU dictionary in feature5.8.
Adjacent Variety(AV) of the candidate.
Wedefine the left AV of the candidate as theprobability that in a corpus the character infront of the candidate is a character in thefull form.
For example if we consider the fullform ??????
(Peking University) and thecandidate ???
?, then the left AV of ???
?is the probability that the character preced-ing ????
is ???
or ???
or ???
or ???
ina corpus.
We can similarly define the rightAV, with respect to characters follow the can-didate.The AV feature is very useful because in somecases a substring of the full form may have a con-fusingly high frequency.
In the example of ??????
(Peking University), an article in the corpusmay mention ??????
(Peking University) and3http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2003T094http://www.sighan.org/bakeoff2005/1885??????
(Tokyo University) at the same time.Then the substring ?????
may be included inthe candidate generation phase for ?????
?with a high frequency.
Because the left AV of ?????
is high, the re-ranker can easily detect thisfalse candidate.In practice, all the features need to be scaled inorder to speed up training.
There are many waysto scale features.
We use the most intuitive scal-ing method.
For a feature value x, we scale it as(x?mean)/(max?min).
Note that for languagemodel score and the score of random walk phase,we scale based on their log value.5 Experiments5.1 Dataset and Evaluation metricsFor the dataset, we collect 3210 abbreviation pairsfrom the Chinese Gigaword corpus.
The abbre-viation pairs include noun phrases, organizationnames and some other types.
The Chinese Gi-gaword corpus contains news texts from the year1992 to 2007.
We only collect those pairs whosefull form and corresponding abbreviation appearin the same article for at least one time.
For fullforms with more than one reasonable reference,we keep the most frequently used one as its refer-ence.
We use 80% abbreviation pairs as the train-ing data and the rest as the testing data.We use the top-K accuracy as the evaluationmetrics.
The top-K accuracy is widely used as themeasurement in previous work (Tsuruoka et al.,2005; Sun et al., 2008, 2009; Zhang et al., 2012).
Itmeasures what percentage of the reference abbre-viations are found if we take the top k candidateabbreviations from all the results.
In our experi-ment, we compare the top-5 accuracy with base-lines.
We choose the top-10 candidates from thegraph random walk are considered in re-rankingphase and the measurement used is top-1 accuracybecause the final aim of the algorithm is to detectthe exact abbreviation, rather than a list of candi-dates.5.2 Candidate ListTable 3 shows examples of the candidates.
In ouralgorithm we further reduce the search space toonly incorporate 10 candidates from the candidategeneration phase.K Top-K Accuracy1 6.84%2 19.35%3 49.01%4 63.70%5 73.60%Table 4: Top-5 accuracy of the candidate genera-tion phase5.3 Comparison with baselinesWe first show the top-5 accuracy of the candidategeneration phase Table 4.
We can see that, justlike the case of using other feature alone, usingthe score of random walk alone is far from enough.However, the first 5 candidates contain most of thecorrect answers.
We use the top-5 candidates plusanother 5 candidates in the re-ranking phase.We choose the character tagging method as thebaseline method.
The character tagging strategyis widely used in the abbreviation generation task(Tsuruoka et al., 2005; Sun et al., 2008, 2009;Zhang et al., 2012).
We choose the ?SK?
labelingstrategy which is used in Sun et al.
(2009); Zhanget al.
(2012).
The ?SK?
labeling strategy gives eachcharacter a label in the character sequence, with?S?
represents ?Skip?
and ?K?
represents ?Keep?.Same with Zhang et al.
(2012), we use the Con-ditional Random Fields (CRFs) model in the se-quence labeling process.The baseline method mainly uses the charac-ter context information to generate the candidateabbreviation.
To be fair we use the same fea-ture set in Sun et al.
(2009); Zhang et al.
(2012).One drawback of the sequence labeling method isthat it relies heavily on the character context inthe full form.
With the number of new abbrevi-ations grows rapidly (especially in social medialike Facebook or twitter), it is impossible to let themodel ?remember?
all the character contexts.
Ourmethod is different from theirs, we use a more in-tuitive way which finds the list of candidates di-rectly from a natural corpus.Table 5 shows the comparison of the top-5 accu-racy.
We can see that our method outperforms thebaseline methods.
The baseline model performswell when using character features (Column 3).However, it performs poorly without the charac-ter features (Column 2).
In contrast, without thecharacter features, our method (Column 4) worksmuch better than the sequence labeling method.1886Full form Reference Generated Candidates #Enum #Now?????
(Depart-ment of InternationalPolitics)???
???,???,????,???,??,??,?
?30 7??????
(Non-nuclear Countries)???
??,??,??,???,???,???,????,????,????,?????,???,??,?
?62 13????
(Drug traf-ficking)??
???,???,??,??,??
14 5??????????????
(YangtzeJoint River EconomicDevelopment Inc.)????
??,??,????,????,????,????,????,????,??????,??????,??????,??????,????????,??????,????,??,??,??,??,?
?16382 20Table 3: Generated Candidates.
#Enum is the number of candidates generated by enumerating all possi-ble candidates.
#Now is the number of candidates generated by our method.When we add character features, our method (Col-umn 5) still outperforms the sequence labelingmethod.K CRF-char Our-char CRF Our1 38.00% 48.60% 53.27% 55.61%2 38.16% 70.87% 65.89% 73.10%3 39.41% 81.78% 72.43% 81.96%4 55.30% 87.54% 78.97% 87.57%5 62.31% 89.25% 81.78% 89.27%Table 5: Comparison of the baseline method andour method.
CRF-char (?-?
means minus) is thebaseline method without character features.
CRFis the baseline method.
Our-char is our methodwithout character features.
We define characterfeatures as the features that consider the charac-ters from the original full form as their parts.5.4 Performance with less training dataOne advantage of our method is that it onlyrequires weak supervision.
The baselinemethod needs plenty of manually collectedfull-abbreviation pairs to learn a good model.In our method, the candidate generation andcoarse-grained ranking is totally unsupervised.The re-ranking phase needs training instancesto decide the parameters.
However we can usea very small amount of training data to get areasonably good model.
Figure 2 shows the resultof using different size of training data.
We cansee that the performance of the baseline methodsdrops rapidly when there are less training data.In contrast, when using less training data, ourmethod does not suffer that much.Figure 2: Top-1 accuracy when changing the sizeof training data.
For example, ?50%?
means ?us-ing 50% of all the training data?.5.5 Comparison with previous workWe compare our method with the method in theprevious work DPLVM+GI in Sun et al.
(2009),which outperforms Tsuruoka et al.
(2005); Sunet al.
(2008).
We also compare our method withthe web-based method CRF+WEB in Zhang et al.(2012).
Because the comparison is performed ondifferent corpora, we run the two methods on ourdata.
Table 6 shows the top-1 accuracy.
Wecan see that our method outperforms the previous1887methods.System Top-K AccuracyDPLVM+GI 53.29%CRF+WEB 54.02%Our method 55.61%Table 6: Comparison with previous work.
Thesearch results of CRF+WEB is based on March 9,2014 version of the Baidu search engine.5.6 Error AnalysisWe perform cross-validation to find the errors andlist the two major errors below:1.
Some full forms may correspond to morethan one acceptable abbreviation.
In thiscase, our method may choose the one that isindeed used as the full form?s abbreviation innews texts, but not the same as the standardreference abbreviations.
The reason for thisphenomenon may lie in the fact that the veri-fication data we use is news text, which tendsto be formal.
Therefore when a reference isoften used colloquially, our method may missit.
We can relieve this by changing the corpuswe use.2.
Our method may provide biased informationwhen handling location sensitive phrases.Not only our system, the system of Sun et al.
(2009); Zhang et al.
(2012) also shows thisphenomenon.
An example is the case of ????????
(Democracy League of HongKong).
Because most of the news is aboutnews in mainland China, it is hard for themodel to tell the difference between the ref-erence ?????
and a false candidate ????
(Democracy League of China).Another ambiguity is ??????
(TsinghuaUniversity), which has two abbreviations ????
and ????.
This happens because thefull form itself is ambiguous.
Word sense dis-ambiguation can be performed first to handlethis kind of problem.6 Related WorkAbbreviation generation has been studied duringrecent years.
At first, some approaches maintaina database of abbreviations and their correspond-ing ?full form?
pairs.
The major problem of puredatabase-building approach is obvious.
It is im-possible to cover all abbreviations, and the build-ing process is quite laborious.
To find these pairsautomatically, a powerful approach is to find thereference for a full form given the context, whichis referred to as ?abbreviation generation?.There is research on using heuristic rulesfor generating abbreviations Barrett and Grems(1960); Bourne and Ford (1961); Taghva andGilbreth (1999); Park and Byrd (2001); Wren et al.
(2002); Hearst (2003).
Most of them achievedhigh performance.
However, hand-crafted rulesare time consuming to create, and it is not easy totransfer the knowledge of rules from one languageto another.Recent studies of abbreviation generation havefocused on the use of machine learning tech-niques.
Sun et al.
(2008) proposed an SVM ap-proach.
Tsuruoka et al.
(2005); Sun et al.
(2009)formalized the process of abbreviation generationas a sequence labeling problem.
The drawback ofthe sequence labeling strategies is that they relyheavily on the character features.
This kind ofmethod cannot fit the need for abbreviation gen-eration in social media texts where the amount ofabbreviations grows fast.Besides these pure statistical approaches, thereare also many approaches using Web as a corpusin machine learning approaches for generating ab-breviations.
Adar (2004) proposed methods to de-tect such pairs from biomedical documents.
Jainet al.
(2007) used web search results as well assearch logs to find and rank abbreviates full pairs,which show good result.
The disadvantage is thatsearch log data is only available in a search en-gine backend.
The ordinary approaches do nothave access to search engine internals.
Zhang et al.
(2012) used web search engine information to re-rank the candidate abbreviations generated by sta-tistical approaches.
Compared to their approaches,our method only uses a fixed corpus, instead of us-ing collective information, which varies from timeto time.Some of the previous work that relate to ab-breviations focuses on the task of ?abbreviationdisambiguation?, which aims to find the correctabbreviation-full pairs.
In these works, machinelearning approaches are commonly used (Park andByrd, 2001; HaCohen-Kerner et al., 2008; Yuet al., 2006; Ao and Takagi, 2005).
We focus onanother aspect.
We want to find the abbreviation1888given the full form.
Besides, Sun et al.
(2013) alsoworks on abbreviation prediction but focuses onthe negative full form problem, which is a littledifferent from our work.One related research field is text normalization,with many outstanding works (Sproat et al., 2001;Aw et al., 2006; Hassan and Menezes, 2013; Linget al., 2013; Yang and Eisenstein, 2013).
Whilethe two tasks share similarities, abbreviation pre-diction has its identical characteristics, like thesub-sequence assumption.
This results in differentmethods to tackle the two different problems.7 ConclusionIn this paper, we propose a unified framework forChinese abbreviation generation.
Our approachcontains two stages: candidate generation andre-ranking.
Given a long term, we first gener-ate a list of abbreviation candidates using the co-occurrence information.
We give a coarse-grainedrank using graph random walk to reduce the searchspace.
After we get the candidate lists, we can usethe features related to the candidates.
We use asimilarity sensitive re-rank method to get the finalabbreviation.
Experiments show that our methodoutperforms the previous systems.AcknowledgmentsThis research was partly supported by Na-tional Natural Science Foundation of China(No.61370117,61333018,61300063),MajorNational Social Science Fund ofChina(No.12&ZD227), National High Tech-nology Research and Development Program ofChina (863 Program) (No.
2012AA011101), andDoctoral Fund of Ministry of Education of China(No.
20130001120004).
The contact author ofthis paper, according to the meaning given tothis role by Key Laboratory of ComputationalLinguistics, Ministry of Education, School ofElectronics Engineering and Computer Science,Peking University, is Houfeng Wang.
We thankKe Wu for part of our work is inspired by hisprevious work at KLCL.ReferencesAdar, E. (2004).
Sarad: A simple and ro-bust abbreviation dictionary.
Bioinformatics,20(4):527?533.Ao, H. and Takagi, T. (2005).
Alice: an algorithmto extract abbreviations from medline.
Journalof the American Medical Informatics Associa-tion, 12(5):576?586.Aw, A., Zhang, M., Xiao, J., and Su, J.
(2006).
Aphrase-based statistical model for sms text nor-malization.
In Proceedings of the COLING/ACLon Main conference poster sessions, pages 33?40.
Association for Computational Linguistics.Barrett, J. and Grems, M. (1960).
Abbreviatingwords systematically.
Communications of theACM, 3(5):323?324.Bourne, C. and Ford, D. (1961).
A study ofmethods for systematically abbreviating englishwords and names.
Journal of the ACM (JACM),8(4):538?552.HaCohen-Kerner, Y., Kass, A., and Peretz, A.(2008).
Combined one sense disambiguationof abbreviations.
In Proceedings of the 46thAnnual Meeting of the Association for Compu-tational Linguistics on Human Language Tech-nologies: Short Papers, pages 61?64.
Associa-tion for Computational Linguistics.Hassan, H. and Menezes, A.
(2013).
Social textnormalization using contextual graph randomwalks.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 1577?1586, Sofia, Bulgaria.
Association for Compu-tational Linguistics.Hearst, M. S. (2003).
A simple algorithm foridentifying abbreviation definitions in biomed-ical text.Jain, A., Cucerzan, S., and Azzam, S. (2007).Acronym-expansion recognition and ranking onthe web.
In Information Reuse and Integration,2007.
IRI 2007.
IEEE International Conferenceon, pages 209?214.
IEEE.Li, J., Ott, M., and Cardie, C. (2013).
Identify-ing manipulated offerings on review portals.
InEMNLP, pages 1933?1942.Ling, W., Dyer, C., Black, A. W., and Trancoso, I.(2013).
Paraphrasing 4 microblog normaliza-tion.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural LanguageProcessing, pages 73?84, Seattle, Washington,USA.
Association for Computational Linguis-tics.Nenadi?c, G., Spasi?c, I., and Ananiadou, S. (2002).Automatic acronym acquisition and term varia-tion management within domain-specific texts.1889In Third International Conference on LanguageResources and Evaluation (LREC2002), pages2155?2162.Norris, J. R. (1998).
Markov chains.
Number2008.
Cambridge university press.Park, Y. and Byrd, R. (2001).
Hybrid text miningfor finding abbreviations and their definitions.In Proceedings of the 2001 conference on em-pirical methods in natural language processing,pages 126?133.Sproat, R., Black, A. W., Chen, S., Kumar, S.,Ostendorf, M., and Richards, C. (2001).
Nor-malization of non-standard words.
ComputerSpeech & Language, 15(3):287?333.Sproat, R., Tao, T., and Zhai, C. (2006).
Namedentity transliteration with comparable corpora.In Proceedings of the 21st International Confer-ence on Computational Linguistics and the 44thannual meeting of the Association for Computa-tional Linguistics, pages 73?80.
Association forComputational Linguistics.Sun, X., Li, W., Meng, F., and Wang, H. (2013).Generalized abbreviation prediction with nega-tive full forms and its application on improv-ing chinese web search.
In Proceedings of theSixth International Joint Conference on NaturalLanguage Processing, pages 641?647, Nagoya,Japan.
Asian Federation of Natural LanguageProcessing.Sun, X., Okazaki, N., and Tsujii, J.
(2009).
Ro-bust approach to abbreviating terms: A discrim-inative latent variable model with global infor-mation.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Nat-ural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 905?913.
Associationfor Computational Linguistics.Sun, X., Wang, H., and Wang, B.
(2008).
Pre-dicting chinese abbreviations from definitions:An empirical learning approach using supportvector regression.
Journal of Computer Scienceand Technology, 23(4):602?611.Taghva, K. and Gilbreth, J.
(1999).
Recognizingacronyms and their definitions.
InternationalJournal on Document Analysis and Recogni-tion, 1(4):191?198.Tsuruoka, Y., Ananiadou, S., and Tsujii, J.
(2005).A machine learning approach to acronym gen-eration.
In Proceedings of the ACL-ISMB Work-shop on Linking Biological Literature, Ontolo-gies and Databases: Mining Biological Seman-tics, pages 25?31.
Association for Computa-tional Linguistics.Wren, J., Garner, H., et al.
(2002).
Heuristicsfor identification of acronym-definition patternswithin text: towards an automated construc-tion of comprehensive acronym-definition dic-tionaries.
Methods of information in medicine,41(5):426?434.Yang, Y. and Eisenstein, J.
(2013).
A log-linearmodel for unsupervised text normalization.
InProceedings of the 2013 Conference on Empir-ical Methods in Natural Language Processing,pages 61?72, Seattle, Washington, USA.
Asso-ciation for Computational Linguistics.Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,J.
(2006).
A large scale, corpus-based approachfor automatically disambiguating biomedicalabbreviations.
ACM Transactions on Informa-tion Systems (TOIS), 24(3):380?404.Zhang, L., Li, S., Wang, H., Sun, N., and Meng,X.
(2012).
Constructing Chinese abbreviationdictionary: A stacked approach.
In Proceedingsof COLING 2012, pages 3055?3070, Mumbai,India.
The COLING 2012 Organizing Commit-tee.Zhu, X., Ghahramani, Z., Lafferty, J., et al.
(2003).Semi-supervised learning using gaussian fieldsand harmonic functions.
In ICML, volume 3,pages 912?919.1890
