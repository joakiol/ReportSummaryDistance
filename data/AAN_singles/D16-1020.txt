Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 203?213,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning Connective-based Word Representationsfor Implicit Discourse Relation IdentificationChloe?
BraudCoAStaL, Dep of Computer ScienceUniversity of CopenhagenUniversity Park 5, 2100 Copenhagen, Denmarkbraud@di.ku.dkPascal DenisMagnet Team, INRIA Lille ?
Nord Europe59650 Villeneuve dAscq, Francepascal.denis@inria.frAbstractWe introduce a simple semi-supervised ap-proach to improve implicit discourse relationidentification.
This approach harnesses largeamounts of automatically extracted discourseconnectives along with their arguments to con-struct new distributional word representations.Specifically, we represent words in the spaceof discourse connectives as a way to directlyencode their rhetorical function.
Experimentson the Penn Discourse Treebank demonstratethe effectiveness of these task-tailored repre-sentations in predicting implicit discourse re-lations.
Our results indeed show that, despitetheir simplicity, these connective-based rep-resentations outperform various off-the-shelfword embeddings, and achieve state-of-the-artperformance on this problem.1 IntroductionA natural distinction is often made between ex-plicit and implicit discourse relations depending onwhether they are lexicalized by a connective or not,respectively.
To illustrate, the Contrast relation inexample (1a) is triggered by the connective but,while it is not overtly marked in example (1b).1Given the lack of strong explicit cues, the identi-fication of implicit relations is a much more chal-lenging and still open problem.
The typically lowperformance scores for this task also hinder the de-velopment of text-level discourse parsers (Lin et al,2010; Xue et al, 2015): implicit discourse relations1These examples are taken from documents wsj 0008 andwsj 0037, respectively, of the PDTB.account for around half of the data for different gen-res and languages (Prasad et al, 2008; Sporleder andLascarides, 2008; Taboada, 2006; Subba and Di Eu-genio, 2009; Soria and Ferrari, 1998; Versley andGastel, 2013).
(1) a.
The house has voted to raise the ceiling to$3.1 trillion, but the Senate isn?t expectedto act until next week at the earliest.b.
That?s not to say that the nutty plot of ?AWild Sheep Chase?
is rooted in reality.
It?simaginative and often funny.The difficulty of this task lies in its dependence ona wide variety of linguistic factors, ranging fromsyntax, lexical semantics and also world knowl-edge (Asher and Lascarides, 2003).
In order to dealwith this issue, a common approach is to exploithand-crafted resources to design features captur-ing lexical, temporal, modal, or syntactic informa-tion (Pitler et al, 2009; Park and Cardie, 2012).
Bycontrast, more recent work show that using simplelow-dimensional word-based representations, eithercluster-based or distributed (aka word embeddings),yield comparable or better performance (Rutherfordand Xue, 2014; Braud and Denis, 2015), while dis-pensing with feature engineering.While standard low-dimensional word represen-tations appear to encode relevant linguistic infor-mation, they have not been built with the specificrhetorical task in mind.
A natural question is there-fore whether one could improve implicit discourserelation identification by using word representationsthat are more directly related to the task.
The203problem of learning good representation for dis-course has been recently tackled by Ji and Eisen-stein (2014) on the problem of text-level discourseparsing.
Their approach uses two recursive neuralnetworks to jointly learn the task and a transforma-tion of the discourse segments to be attached.
Whilethis type of joint learning yields encouraging results,it is also computationally intensive, requiring longtraining times, and could be limited by the relativelysmall amount of manually annotated data available.In this paper, we explore the possibility of learn-ing a distributional word representation adapted tothe task by selecting relevant rhetorical contexts,in this case discourse connectives, extracted fromlarge amounts of automatically detected connectivesalong with their arguments.
Informally, the as-sumption is that the estimated word-connective co-occurrence statistics will in effect give us an im-portant insight to the rhetorical function of differentwords.
The learning phase in this case is extremelysimple, as it amounts to merely estimating co-occurrence frequencies, potentially combined with areweighting scheme, between each word appearingin a discourse segment and its co-occurring connec-tive.
To assess the usefulness of these connective-based representations,2 we compare them with pre-trained word representations, like Brown clustersand other word embeddings, on the task of implicitdiscourse relation identification.
Our experimentson the Penn Discourse Treebank (PDTB) (Prasad etal., 2008) show that these new representations de-liver improvements over systems using these genericrepresentations and yield state-of-the-art results, andthis without the use of other hand-crafted features,thus also alleviating the need for external linguis-tic resources (like lexical databases).
Thus, our ap-proach could be easily extended to resource-poorlanguages as long as connectives can be reliablyidentified on raw texts.Section 2 summarizes related work.
In Section 3,we detail our connective-based distributional wordrepresentation approach.
Section 4 presents the au-tomatic annotation of the explicit examples used tobuild the word representation.
In Section 5, we de-scribe our comparative experiments on the PDTB.2Available at https://bitbucket.org/chloebt/discourse-data.2 Related WorkImplicit discourse relation identification has at-tracted growing attention since the release of thePDTB, the first discourse corpus to make the distinc-tion between explicit and implicit examples.
Withinthe large body of research on this problem, we iden-tify two main strands directly relevant to our work.2.1 Finding the Right Input RepresentationThe first work on this task (Marcu and Echihabi,2002), which pre-dates the release of the PDTB, pro-posed a simple word-based representation: they usethe Cartesian product of words appearing in the twosegments.
Given the knowledge-rich nature of thetask, following studies attempted to exploit varioushand-crafted resources and pre-processing systemsto enrich their model with information on modality,polarity, tense, lexical semantics, and syntax, possi-bly combined with feature selection methods (Pitleret al, 2009; Lin et al, 2009; Park and Cardie,2012; Biran and McKeown, 2013; Li and Nenkova,2014).
Interestingly, Park and Cardie (2012) con-cluded on the worthlessness of word-based features,as long as hand-crafted linguistic features were used.More recent studies however reversed this conclu-sion (Rutherford and Xue, 2014; Braud and Denis,2015), demonstrating that word-based features canbe effective provided they were not encoded usingthe sparse one-hot representation, but instead with adenser one (cluster based or distributed).
This papertakes one step further by testing whether learning asimple task-specific, distributional word representa-tion could lead to further improvements.As noted, some previous work have also at-tempted to learn discourse-specific representationfor the related problem of discourse parsing.
Thus,Ji and Eisenstein (2014) reports improvements onthe RST Discourse Treebank (Carlson et al, 2001),by jointly learning a combination of the discourseunits, represented by bag-of-words in a one-hot en-coding, along with the sequence of actions of theirshift-reduce parser.
Our approach is attractively sim-pler, since training reduces to collecting frequencycounts, and it can easily generate representations forunseen words without having to retrain the wholesystem.2042.2 Leveraging Explicit Discourse DataAnother line of work, also initiated in (Marcu andEchihabi, 2002), propose to deal with the sparsenessof the word pair representation by using additionaldata automatically annotated using discourse con-nectives.
An appeal of this strategy is that one caneasily identify explicit relations in raw data, as per-formance are high on this task (Pitler et al, 2009)and it is even possible to rely on simple heuris-tics (Marcu and Echihabi, 2002; Sporleder and Las-carides, 2005; Lan et al, 2013).
It has been shown,however, that using explicit examples as additionaldata for training an implicit relation classifier de-grades performance, due to important distributiondifferences (Sporleder and Lascarides, 2008).Recent attempts to overcome this issue involvedomain adaptation strategies (Braud and Denis,2014; Ji et al, 2015), sample selection (Rutherfordand Xue, 2015; Wang et al, 2012), or multi-task al-gorithms (Lan et al, 2013).
However, it generallyinvolves longer training time since models are builton a massive amount of data, the strategy requir-ing a large corpus of explicit examples to overcomethe noise induced by the automatic annotation strat-egy.
In this paper, we circumvent this problem byusing explicit data only for learning our word repre-sentations and not for estimating the parameters ofour implicit classification model.
Some aspects ofthe present work are similar to Biran and McKeown(2013) in that they also exploit explicit data to com-pute co-occurrence statistics between word pairs andconnectives.
But the perspective is reversed, as theyrepresent connectives in the contexts of co-occurringword pairs, with the aim of deriving similarity fea-tures between each implicit example and each con-nective.
Furthermore, their approach did not outper-form state-of-the-art systems.3 The Connective Vector Space ModelOur discourse-based word representation model is asimple variant of the standard vector space model(Turney and Pantel, 2010): that is, it represents in-dividual words in specific co-occurring contexts (inthis case, discourse connectives) that define the di-mensions of the underlying vector space.
Our spe-cific choice of contexts was guided by two main con-siderations.
On the one hand, we aim at learningword representations that live in a relatively low-dimensional space, so as to make learning a classifi-cation function over that space feasible.
The numberof parameters of that function grows proportionallywith that of the input size.
Although there is oftena lack of consensus among linguists as to the exactdefinition of discourse connectives, they neverthe-less form a closed class.
For English, the PDTB rec-ognizes 100 distinct connectives.
On the other hand,we want to learn a vectorial representation that cap-tures relevant aspects of the problem, in this casethe rhetorical contribution of words.
Adapting Har-ris (1954)?s famous quote, we make the assumptionthat words occurring in similar rhetorical contextstend to have similar rhetorical meanings.
Discourseconnectives are by definition strong rhetorical cues.As an illustration, Pitler et al (2009) found that con-nectives alone unambiguously predict a single rela-tion in 94% of the PDTB level 1 data.
By using con-nectives as contexts, we are thus linking each wordto a relation (or a small set of relations), namelythose that can be triggered by this connective.
Notethat for level 2 relations in the PDTB, the connec-tives are much more ambiguous (86.77% reportedin (Lin et al, 2010)), and it could be also the caseif we expand the list of forms considered as connec-tives for English, or if we try to deal with other lan-guages and domains.
We however believe that theset of relations that can be triggered by a connectiveis limited (not all relations can be expressed by thesame connective), and that one attractive feature ofour strategy is precisely to keep this ambiguity.Before turning to the details of how we constructour distributional connective-based model, note thatwe decided to learn a unique representation for anyindividual word, irrespective of its position (with)ina particular segment.
That is, we represent both ar-guments of a connective as a single bag of words.Other designs are of course possible: we could di-rectly learn distinct word representation for left andright segment words, or even the pair of words (Con-rath et al, 2014), to take into account the fact thatsome relations are oriented (e.g.
Reason contains thecause in the first argument and Result in the secondone).
An obvious drawback of these more expres-sive representations is that they would need muchmore data to compute a robust estimate of the fre-quency counts.205but while beforeWord Freq.
TF-IDF PPMI-IDF Freq.
TF-IDF PPMI-IDF Freq.
TF-IDF PPMI-IDFreality 12 0.0 0.0 13 0.0 0.0 10 0.0 0.0not 142 0.37 0.36 201 0.18 0.06 0 0.0 0.0week 0 0.0 0.0 110 0.10 0.04 90 0.12 0.12Table 1: Illustrative example of association measures between connectives and words.3.1 Building the Distributional RepresentationOur discourse-based representations of words areobtained by computing a matrix of co-occurrencebetween the words and the chosen contexts.
Thefrequency counts are then weighted in order to high-light relevant associations.
More formally, we noteV the set of the n words appearing in the arguments,and C the set of the m connective contexts.
We buildthe matrix F, of size n ?m, by computing the fre-quency of each element of V with each element ofC.
We note fi,j the frequency of the word wi ?
Vappearing in one argument of the connective cj ?
C.We use two standard weighting functions on theseraw frequencies: the normalized Term Frequency(TF), eq.
(1), and the Positive Pointwise Mutual In-formation (PPMI), eq.
(2), which is a version of thePMI where negative values are ignored (with pi,jthe joint probability that the word wi appears withconnective cj , and pi,?
and p?,j , relative frequencyof resp.
wi and cj).
These two measures are thennormalized by multiplying the value by the InverseDocument Frequency (IDF) for a word wi, eq.
(3),as in (Biran and McKeown, 2013).
In the final ma-trices, the ith row corresponds to the m-dimensionalvector for the ith word of V .
The jth column is avector corresponding to the jth connective.TFi,j = fi,j?nk=1 fk,j(1)PPMIi,j = max(0, log( pi,jpi,?
p?,j)) (2)IDFi = log( m?mk=1 fi,k)(3)Table 1 illustrates the weighting of the words usingthe TF and the PPMI normalized with IDF.
For in-stance, the presence of the negation ?not?
is pos-itively linked to Contrast through but and whilewhereas it receives a null or a very small weightwith the temporal connective before.
The final vec-tor for this word, < 0.37, 0.18, 0.0 > with TF-IDFor < 0.36, 0.06, 0.0 > with PPMI-IDF, is intendedto guide the implicit model toward a contrastive re-lation, thus potentially helping in identifying the re-lation in example (1b).
In contrast, the word ?week?is more likely to be found in the arguments of tem-poral relations that can be triggered by before butalso while, an ambiguity kept in our representationwhereas approaches based on using explicit exam-ples as new training data generally choose to anno-tate them using the most frequent sense associatedwith the connective, often limiting themselves to theless ambiguous ones (Marcu and Echihabi, 2002;Sporleder and Lascarides, 2008; Lan et al, 2013;Braud and Denis, 2014; Rutherford and Xue, 2015).Finally, a word occuring with all connectives, notdiscriminant, such as ?reality?
is associated with anull weight for all dimensions: it thus has no impacton the model.Since we have 100 connectives for the PDTB, therepresentation is already of quite low dimensional-ity.
However, it has been shown (Turney and Pan-tel, 2010) that using a dimensionality reduction al-gorithm could help capturing the latent dimensionsbetween the words and their contexts and reducingthe noise.
We thus also test versions with a reductionComponents Analysis (PCA) (Jolliffe, 2002).3.2 Using the Word-based RepresentationSo far, our distributional framework associates aword with a d-dimensional vector (where d ?
m).We now need to represent a pair of arguments(i.e., the spans of text linked by a relation), mod-eled here as a pair of bags of words.
Following(Braud and Denis, 2015), we first sum all word vec-tors contained in each segment, thus obtaining a d-dimensional vector for each segment.
We then com-bine the two segment vectors to build a compos-ite vector representing the pair of arguments, by ei-206ther concatenating the two segment vectors (lead-ing to a 2d-dimensional vector) or by computingthe Kronecker product between them (leading toa d2-dimensional vector).
Finally, these segment-pair representations will be normalized using the L2norm to avoid segment size effects.
These will thenbe used as the input of a classification model, asdescribed in Section 5.
Given these combinationschemes, it should be clear that despite the fact thateach individual word receives a unique vectorial rep-resentation irrespective of its position, the param-eters of the classification model associated with agiven word are likely to be different depending ofwhether it appears in the left or right segment.4 Automatic Annotation of ExplicitExamplesIn order to collect reliable word-connective co-occurrence frequencies, we need a large corpuswhere the connectives and their arguments havebeen identified.
We therefore rely on automaticannotation of raw data, instead of using the rela-tively small amount of explicit examples manuallyannotated in the PDTB (roughly 18, 000 examples).Specifically, we used the Bllip corpus3 composedof news articles from the LA Times, the WashingtonPost, the New York Times and Reuters and containing310 millions of words automatically POS-tagged.Identifying the Connectives and their ArgumentsWe have two tasks to perform: identifying the con-nectives and extracting their arguments.4 Ratherthan relying on manually defined patterns to anno-tate explicit examples (Marcu and Echihabi, 2002;Sporleder and Lascarides, 2008; Rutherford andXue, 2015), we use two binary classification modelsinspired by previous works on the PDTB (Pitler andNenkova, 2009; Lin et al, 2010): the first one iden-tifies the connectives and the second one localizesthe arguments between inter- and intra-sentential,an heuristic being then used to decide on the exactboundaries of the arguments.Discourse connectives are words (e.g., but, since)3https://catalog.ldc.upenn.edu/LDC2008T134Note that contrary to studies using automatically annotatedexplicit examples as new training data, we do not need to anno-tate the relation triggered by the connective.or grammaticalized multi-word expressions (e.g., assoon as, on the other hand) that may trigger a dis-course relation.
However, these forms can also ap-pear without any discourse reading, such as becausein: He can?t sleep because of the deadline.
We thusneed to disambiguate these forms between discourseand non discourse readings, a task that has provento be quite easy on the PDTB (Pitler and Nenkova,2009).
This is the task performed by our first binaryclassifier: a pattern-matching is used to identify allpotential connectives, and the model predicts if theyhave discourse reading in context.We then need to extract the arguments of the iden-tified connectives, that is the two spans of text linkedby the connective.
This latter task has proven to beextremely hard on the PDTB (Lin et al, 2010; Xueet al, 2015) because of some annotation principlesthat make the possible types of argument very di-verse.
As first proposed in (Lin et al, 2010), we thussplit this task into two subtasks: identifying the rel-ative positions of the arguments and delimiting theirexact boundaries.For an explicit example in the PDTB, one argu-ment, called Arg2, is linked to the connective, andthus considered as easy to extract (Lin et al, 2010).The other argument, called Arg1, may be located atdifferent places relative to Arg2 (Prasad et al, 2008):we call intra-sentential the examples where Arg1 isa clause within the same sentence as Arg2 (60.9%of the explicit examples in the PDTB), and inter-sentential the other examples, that is Arg1 is foundin the previous sentence, in a non-adjacent previ-ous sentence (9%) or in a following sentence (lessthan 0.1%).
In this work, we build a localizationmodel by only considering these two coarse cases ?the example is either intra- or inter-sentential.
Notethat this distinction is similar to what has been donein (Lin et al, 2010): more precisely, these authorsdistinguish between ?same-sentence?
and ?previoussentence?
and ignore the cases where the Arg1 is ina following sentence.
We rather choose to includethem as being also inter-sentential.
When the posi-tion of Arg1 has been predicted, an heuristic is incharge of finding the exact boundaries of the argu-ments.Here, the problem is that in addition to the vari-ety of locations, the annotators were almost free tochoose any boundary for an argument in the PDTB:207an argument can cover only a part of a sentence, anentire sentence or several sentences.
Statistical ap-proaches intended to solve this task lead for nowto low performance even when complex sequentialmodels are used, and they often rely on the syntacticconfigurations (Lin et al, 2010; Xue et al, 2015).We thus decided to define an heuristic to performthis task, following the simplifying assumptions alsoused in previous work since (Marcu and Echihabi,2002).
We assume that: (1) Arg1 is either in thesame sentence as Arg2 or in the previous one, (2)an argument covers at most one sentence and (3)a sentence contains at most two arguments.
As itcan be deduced from (1), our final model ignores thefiner distinctions one can make for the position ofinter-sentential examples (i.e.
we never extract Arg1from a non-adjacent previous sentence or a follow-ing one).According to these assumptions, once a connec-tive is identified, knowing its localization is almostenough to identify the boundaries of its arguments.More precisely, if a connective is predicted as inter-sentential, then our heuristic picks the entire pre-ceding sentence as Arg1, Arg2 being the sentencecontaining the connective, according to assumptions(1) and (2).
If a connective is predicted as intra-sentential, then the sentence containing the connec-tive is split into two segments ?
according to (3) ?,more precisely, the sentence is split around the con-nective using the punctuation and making it neces-sary to have a verb in each argument.Settings We thus built two models using thePDTB: one to identify the discourse markers (con-nective vs not connective), and one to identify theposition of the arguments with respect to the con-nective (inter- vs intra-sentential).
The PDTB con-tains 18, 459 explicit examples for 100 connectives.For both models, we use the same split of the dataas in (Lin et al, 2014).
The test set contains 923positive instances of connectives and 2, 075 nega-tive instances, and 546 inter-sentential and 377 intra-sentential examples.
Both models are built using alogistic regression model optimized on the develop-ment set (see Section 5), and the same simple featureset (Lin et al, 2014; Johannsen and Sgaard, 2013)without syntactic information.
With C the connec-tive, F the following word and P the previous one,our features are: C, P+C, C+F, C-POS5, P-POS, F-POS, P-POS+C-POS and C-POS+F-POS.Results Our model identifies discourse connectivewith a micro-accuracy of 92.9% (macro-F1 91.5%).These scores are slightly lower than the state-of-the-art in micro-accuracy, but high enough to rely onthis annotation.
When applying our model to theBllip data, we found 4 connectives that correspondto no examples.
We thus have examples for only 96connectives.
For distinguishing between inter- andintra-sentential examples, we get a micro-accuracyof 96.1% (macro-F1 96.0), with an F1 of 96.7 for theintra- and 95.3 for the inter-sentential class, againclose enough to the state-of-the-art (Lin et al, 2014).Coverage Using these models on Bllip, we areable to extract around 3 million connectives, alongwith their arguments.
Our word representation hasa large vocabulary (see Table 2) compared to exist-ing off-the-shelf word vectors, with only 2, 902 outof vocabulary (OOV) tokens in set of implicit rela-tions.6# words # OOVHLBL 246, 122 5, 439CnW 268, 810 5, 638Brown 247, 339 5, 413H-PCA 178, 080 7, 042Bllip 422,199 2,902Table 2: Lexicon coverage for Brown clusters (Brown et al,1992), Collobert and Weston (CnW ) (Collobert and Weston,2008) and hierarchical log-bilinear embeddings (HLBL) (Mnihand Hinton, 2007) using the implementation in (Turian et al,2010), Hellinger PCA (H-PCA) (Lebret and Collobert, 2014)and our connective-based representation (Bllip).5 ExperimentsOur experiments investigate the relevance of ourconnective-based representations for implicit dis-course relation identification, recast here as multi-class classification problem.
That is, we aim at eval-uating the usefulness of having a word representa-tion linked to the task, compared to using generic5The connective POS is either the node covering the con-nective, or the POS of its first word if no such node exists.6Training and development sets, only.208Relation Train Dev TestTemporal 665 93 68Contingency 3, 281 628 276Comparison 1, 894 401 146Expansion 6, 792 1, 253 556Total 12, 632 2, 375 1, 046Table 3: Number of examples in train, dev, test.word representations (either one-hot, cluster-basedor distributed), and whether they encode all the in-formation relevant to the task, thus comparing sys-tems with or without additional hand-crafted fea-tures.5.1 DataThe PDTB (Prasad et al, 2008) is the largest corpusannotated for discourse relations, formed by news-paper articles from the Wall Street Journal.
It con-tains 16, 053 pairs of spans of text annotated withone or more implicit relations.
The relation set isorganized in a three-level hierarchy.
We focus onthe level 1 coarse-grained relations and keep onlythe first relation annotated.
We use the most spreadsplit of the data, used in (Rutherford and Xue, 2014;Rutherford and Xue, 2015; Braud and Denis, 2015)among others, that is sections 2-20 for training and21-22 for testing.
The other sections are used for de-velopment.
The number of examples per relation isreported in Table 3.
It can be seen that the dataset ishighly imbalanced, with the relation Expansion ac-counting for more than 50% of the examples.5.2 SettingsFeature Set Our main features are based on thewords occurring in the arguments.
We test simplebaselines using raw tokens.
The first one uses theCartesian product of the tokens, a feature template,generally called ?Word pairs?, used in most of theprevious study for this task as in (Marcu and Echi-habi, 2002; Pitler et al, 2009; Lin et al, 2011; Braudand Denis, 2015; Ji et al, 2015).
It is the sparsestrepresentation one can build from words, and it cor-responds to using the combination scheme based onthe Kronecker product to combine the one-hot vec-tors representing each word.
We also report resultswith a less sparse version where the vectors are com-bined using concatenation.We also compare our systems to previous ap-proaches that make use of word based representa-tions but not linked to the task.
We implement thesystems proposed in (Braud and Denis, 2015) inmulticlass, that is using the Brown clusters (Brownet al, 1992), the Collobert and Weston (Collobertand Weston, 2008) and the hierarchical log-bilinearembeddings (Mnih and Hinton, 2007) using theimplementation in (Turian et al, 2010)7, and theHPCA (Lebret and Collobert, 2014)8.
We usethe combination schemes described in Section 3 tobuild vector representations for pairs of segments.For these systems and ours, using the connective-based representations, the dimensionality of the finalmodel depends on the number of dimensions d of therepresentation used and on the combination scheme?
the concatenation leading to 2d dimensions and theKronecker product to d2.All the word representations used ?
the off-the-shelf representations as well as our connective-basedrepresentation (see Section 4) ?
are solely or mainlytrained on newswire data, thus on the same domainas our evaluation data.
The CnW embeddings weuse in this paper, with the implementation in (Turianet al, 2010), as well as the HLBL embeddings havebeen obtained using the RCV1 corpus, that is oneyear of Reuters English newswire.
The H-PCA havebeen built on the Wikipedia, the Reuters corpus andthe Wall street Journal.
We thus do not expect anyout-of-domain issue when using these representa-tions.Finally, we experiment with additional featuresproposed in previous studies and well describedin (Pitler et al, 2009; Park and Cardie, 2012): pro-duction rules9, information on verbs (average verbphrases length and Levin classes), polarity (Wilsonet al, 2005), General Inquirer tags (Stone and Kirsh,1966), information about the presence of numbersand modals, and first, last and first three words.
Weconcatenate these features to the ones built usingword representations.7http://metaoptimize.com/projects/wordreprs/8http://lebret.ch/words/9We use the gold standard parses provided in the Penn Tree-bank (Marcus et al, 1993).209Model We train a multinomial multiclass logisticregression model.10 In order to deal with the classimbalance issue, we use a sample weighting schemewhere each instance has a weight inversely propor-tional to the frequency of the class it belongs to.Parameters We optimize the hyper-parameters ofthe algorithm, that is the regularization norm (L1or L2), and the strength of the regularization C ?
{0.001, 0.005, 0.01, 0.1, 0.5, 1, 5, 10, 100}.
Whenusing additional features or one-hot sparse encod-ings over the pairs of raw tokens, we also optimizea filter on the features by defining a frequency cut-off t ?
{1, 2, 5, 10, 15, 20}.
We evaluate the un-supervised representations with different number ofdimensions.
We test versions of the Brown clus-ters with 100, 320, 1, 000 and 3, 200 clusters, ofthe Collobert and Weston embeddings with 25, 50,100 and 200 dimensions, of the hierarchical log-bilinear embeddings with 50 and 100 dimensions,and of the Hellinger PCA with 50, 100 and 200 di-mensions.
Finally, the distributional representationsof words based on the connective are built using ei-ther no PCA ?
thus corresponding to 96 dimensions?, or a PCA11 keeping the first k dimensions withk ?
{2, 5, 10, 50}.12 We optimize both the hyper-parameters of the algorithm and the number of di-mensions of the unsupervised representation on thedevelopment set based on the macro-F1 score, themost relevant measure to track when dealing withimbalanced data.5.3 ResultsOur results are summarized in Table 4.
Using ourconnective-based word representation allows im-provements of above 2% in macro-F1 over the base-line systems based on raw tokens (One-hot), thecompetitive systems using pre-trained representa-tions (Brown and Embed.)
and the state-of-the-artresults in terms of macro-F1 (R&X 15).
These im-provements demonstrate the efficiency of the repre-sentation for this task.We found that using an unsupervised word repre-sentation generally leads to improvements over the10http://scikit-learn.org/dev/index.html.11Implemented in scikit-learn, applied with default settings.12Keeping resp.
11.3%, 36.6%, 56.2% or 95.3% of the vari-ance of the data.Representation Macro-F1 Acc.One-hot ?
39.0 48.6One-hot ?
40.2 50.2Best Brown ?
37.5 50.6Best Brown ?
40.6 51.2Best Embed.
?
41.0 51.7Best Embed.
?
41.6 50.1Best dense + add feat.
40.8 51.2Bllip TF-IDF ?
41.4 51.0Bllip TF-IDF ?
40.1 50.0Bllip PPMI-IDF ?
38.9 48.2Bllip PPMI-IDF ?
42.2?
52.5Best Bllip + add feat.
42.8?
51.7R&X 15 40.5 57.1Table 4: Results for multiclass experiments.
R&X 15 are thescores reported in (Rutherford and Xue, 2015) ; One-hot: one-hot encoding of raw tokens ; Brown and Embed.
: pre-trainedrepresentations ; Bllip: connective based representation.
?
p ?0.1 compared to One-hot ?
with t-test and Wilcoxon.use of raw tokens (One-hot), a conclusion in linewith the results reported in (Braud and Denis, 2015)for binary systems.
However, contrary to their find-ings, in multiclass, the best results are not obtainedusing the Brown clusters, but rather the dense, realvalued representations (Embed.
and Bllip).
Further-more, concerning the combination schemes, the con-catenation (?)
generally outperforms the Kroneckerproduct (?
), in effect favoring lower dimensionalmodels.More importantly, the distributional representa-tions based on connectives (Bllip) allows perfor-mance at least similar or even better than those ob-tained with the other dense representations uncon-nected to the task (Embed.).
While simply based onweighted co-occurrence counts, thus really easy andfast to build, these representations generally outper-form the ones learned using neural networks (seeCnW and HLBL in Figure 1).
Besides, our sec-ond best representation is also distributional, namelyHPCA (see Figure 1).
These result are thus in linewith the conclusions in (Lebret and Collobert, 2014)for other NLP tasks: distributional representations,while simpler to obtain, may allow similar resultsthan distributed ones.2102 5 10 25 50 100 20025303540Number of dimensionsF 1onthedevsetBllip PPMI ?CnW ?HLBL ?H-PCA ?Figure 1: F1 scores on dev against the number of dimensions.Our best results with Bllip are obtained withoutthe use of a dimensionality reduction method, thuskeeping the 96 dimensions corresponding to the con-nectives identified in the raw data.
Our new wordrepresentation like the other low-dimensional onesyield higher scores as one increases the number ofdimensions (see Figure 1).
This could be a limita-tion of our strategy, since the number of connectivesin the PDTB is fixed.
However, one could easilyexpand our model to include additional lexical ele-ments that might have a rhetorical function such asmodals or specific expressions such as one reason is.We also tested the addition of hand-crafted fea-tures traditionally used for the task.
We found that,either using a pre-trained word representation or ourrepresentation based on connectives, adding thesefeatures leads to small or even no improvements andsuggest that these representations already encode theinformation provided by these features.
This con-clusion has however to be nuanced: when looking atthe scores per relation reported in Table 5, the useof the connective based word representation aloneallows the best performance for Temporal and Con-tingency, but the addition of new features dramat-ically increase the scores for Comparison showingthat some information are missing for this relation.Moreover, this relation is the one taking the mostadvantage of the addition of explicit data in (Ruther-ford and Xue, 2015), demonstrating that these datacould probably provide even more information thanthe ones we leverage through our representations.Finally, our results are similar or even better thanthose reported in (Rutherford and Xue, 2015) interms of macro-F1.
Our systems correspond how-ever to a lower micro-accuracy.
Looking at thescores per relation in Table 5, we found that we ob-tain better results for all the relations except Expan-sion, the most represented, which could explain theloss in accuracy.
It is noteworthy that we generallyobtain better results even without the additional fea-tures used in this work.
Moreover, our systems re-quires lower training time (since we only train onimplicit examples) and alleviate the need for thesample selection strategy used to deal with the dis-tribution differences between the two types of data.Bllip PPMI-IDF ?
Bllip + add feat R&X 15Rel Prec F1 Prec F1 Prec F1Temp 23.0 29.9 23.7 27.9 38.5 14.7Cont 49.6 47.1 46.7 46.3 49.3 43.9Comp 35.9 27.7 35.0 34.3 44.9 34.2Exp 62.8 64.0 63.7 62.6 61.4 69.1Table 5: Scores per relation for multiclass experiments, ?R&X15?
are the scores reported in (Rutherford and Xue, 2015).6 ConclusionWe presented a new approach to leverage infor-mation from explicit examples for implicit relationidentification.
We showed that building distribu-tional representations linked to the task through con-nectives allows state-of-the-art performance and al-leviates the need for additional features.
Futurework includes extending the representations to newcontexts ?
such as the Alternative Lexicalization an-notated in the PDTB, the modals or some adverbs?
using more sophisticated weighting schemes (Le-bret and Collobert, 2014) and testing this strategyfor other languages and domains.AcknowledgementsWe thank the three anonymous reviewers for theircomments.
Chloe?
Braud was funded by the ERCStarting Grant LOWLANDS No.
313695.
Pas-cal Denis was supported by ERC Grant STACNo.
269427, and by a grant from CPER Nord-Pasde Calais/FEDER DATA Advanced data science andtechnologies 2015-2020.211ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofConversation.
Cambridge University Press.Or Biran and Kathleen McKeown.
2013.
Aggregatedword pair features for implicit discourse relation dis-ambiguation.
In Proceedings of ACL.Chloe?
Braud and Pascal Denis.
2014.
Combining naturaland artificial examples to improve implicit discourserelation identification.
In Proceedings of COLING.Chloe?
Braud and Pascal Denis.
2015.
Comparing wordrepresentations for implicit discourse relation classifi-cation.
In Proceedings of EMNLP.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.2001.
Building a discourse-tagged corpus in theframework of rhetorical structure theory.
In Proceed-ings of the Second SIGdial Workshop on Discourseand Dialogue.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML.Juliette Conrath, Stergos Afantenos, Nicholas Asher, andPhilippe Muller.
2014.
Unsupervised extraction of se-mantic relations using discourse cues.
In Proceedingsof Coling.Zellig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Yangfeng Ji and Jacob Eisenstein.
2014.
Representationlearning for text-level discourse parsing.
In Proceed-ings of ACL.Yangfeng Ji, Gongbo Zhang, and Jacob Eisenstein.
2015.Closing the gap: Domain adaptation from explicitto implicit discourse relations.
In Proceedings ofEMNLP.Anders Johannsen and Anders Sgaard.
2013.
Disam-biguating explicit discourse connectives without ora-cles.
In Proceedings of IJCNLP.Ian Jolliffe.
2002.
Principal component analysis.
WileyOnline Library.Man Lan, Yu Xu, and Zhengyu Niu.
2013.
Leveragingsynthetic discourse data via multi-task learning for im-plicit discourse relation recognition.
In Proceedings ofACL.Re?mi Lebret and Ronan Collobert.
2014.
Word emded-dings through Hellinger PCA.
In Proceedings of ACL.Junyi Jessy Li and Ani Nenkova.
2014.
Reducing spar-sity improves the recognition of implicit discourse re-lations.
In Proceedings of SIGDIAL.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of EMNLP.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
APDTB-styled end-to-end discourse parser.
Technicalreport, National University of Singapore.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Au-tomatically evaluating text coherence using discourserelations.
In Proceedings of ACL-HLT.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.A PDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering, 20:151?184.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of ACL.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.
InProceedings of ICML.Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through feature setoptimization.
In Proceedings of SIGDIAL Conference.Emily Pitler and Ani Nenkova.
2009.
Using syntax todisambiguate explicit discourse connectives in text.
InProceedings of the ACL-IJCNLP.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In Proceedings of ACL-IJCNLP.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.
InProceedings of LREC.Attapol Rutherford and Nianwen Xue.
2014.
Discover-ing implicit discourse relations through Brown clusterpair representation and coreference patterns.
In Pro-ceedings of EACL.Attapol Rutherford and Nianwen Xue.
2015.
Improvingthe inference of implicit discourse relations via classi-fying explicit discourse connectives.
In Proceedingsof NAACL-HLT.Claudia Soria and Giacomo Ferrari.
1998.
Lexical mark-ing of discourse relations - some experimental find-ings.
In Proceedings of the ACL Workshop on Dis-course Relations and Discourse Markers.Caroline Sporleder and Alex Lascarides.
2005.
Exploit-ing linguistic cues to classify rhetorical relations.
InProceedings of RANLP-05.Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetoricalrelations: An assessment.
Natural Language Engi-neering, 14:369?416.212Philip J.
Stone and John Kirsh.
1966.
The General In-quirer: A Computer Approach to Content Analysis.MIT Press.Rajen Subba and Barbara Di Eugenio.
2009.
An effec-tive discourse parser that uses rich linguistic informa-tion.
In Proceedings of ACL-HLT.Maite Taboada.
2006.
Discourse markers as signals (ornot) of rhetorical relations.
Journal of Pragmatics,38:567?592.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof ACL.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning : Vector space models of seman-tics.
Journal of Artificial Intelligence Research, pages141?188.Yannick Versley and Anna Gastel.
2013.
Linguistic testsfor discourse relations in the Tu?Ba-D/Z corpus of writ-ten German.
Dialogue & Discourse, 4(2):142?173.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
2012.
Im-plicit discourse relation recognition by selecting typ-ical training examples.
In Proceedings of COLING2012: Technical Papers.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT-EMNLP.Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, RashmiPrasad, Christopher Bryant, and Attapol Rutherford.2015.
The CoNLL-2015 shared task on shallow dis-course parsing.
In Proceedings of CoNLL.213
