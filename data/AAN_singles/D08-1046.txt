Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 438?446,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLegal Docket-Entry Classification: Where Machine Learning stumblesRamesh Nallapati and Christopher D. ManningNatural Language Processing GroupDepartment of Computer ScienceStanford UniversityStanford, CA 94305{nmramesh,manning}@cs.stanford.eduAbstractWe investigate the problem of binary text clas-sification in the domain of legal docket entries.This work presents an illustrative instance ofa domain-specific problem where the state-of-the-art Machine Learning (ML) classifierssuch as SVMs are inadequate.
Our investiga-tion into the reasons for the failure of theseclassifiers revealed two types of prominent er-rors which we call conjunctive and disjunctiveerrors.
We developed simple heuristics to ad-dress one of these error types and improve theperformance of the SVMs.
Based on the in-tuition gained from our experiments, we alsodeveloped a simple propositional logic basedclassifier using hand-labeled features, that ad-dresses both types of errors simultaneously.We show that this new, but simple, approachoutperforms all existing state-of-the-art MLmodels, with statistically significant gains.
Wehope this work serves as a motivating exampleof the need to build more expressive classifiersbeyond the standard model classes, and to ad-dress text classification problems in such non-traditional domains.1 IntroductionText Classification is a widely researched area, withpublications spanning more than a decade (Yangand Liu, 1999).
Although earlier models used logicbased rules (Apte?
et al, 1994) and decision trees(Lewis and Ringuette, 1994), recently the emphasishas been on statistical classifiers such as the naiveBayes model (McCallum and Nigam, 1998), logis-tic regression (Zhang and Oles, 2001) and supportvector machines (Joachims, 1998).
Although severalcomplex features were considered for classification,eventually researchers have settled down to simplebag-of-words features such as unigrams and sometimes bigrams (Dumais et al, 1998), thereby com-pletely ignoring the grammar and other semantic in-formation in the text.
Despite this fact, the state-of-the-art performance is close to or above 90% onF1 scores on most standard test collections such asReuters, 20 newsgroups, etc.
(Bekkerman et al,2003).
As such, most researchers and practitionersbelieve text classification technology has reached amature state, where it is suitable for deployment inreal life applications.In this work, we present a text classification prob-lem from the legal domain which challenges someof our understanding of text classification problems.In the new domain, we found that the standard MLapproaches using bag-of-words features perform rel-atively poorly.
Not only that, we noticed that thelinear form (or even polynomial form) used by theseclassifiers is inadequate to capture the semantics ofthe text.
Our investigation into the shortcomings ofthe traditional models such as SVMs, lead us to builda simple propositional logic based classifier usinghand-labeled features that outperforms these strongbaselines.Although the new model by itself is interesting,the main objective of our work is to present the textclassification community with an interesting prob-lem where the current models are found inadequate.Our hope is that the new problem will encourageresearchers to continue to build more sophisticatedmodels to solve classification problems in diverse,438non-traditional domains.The rest of the paper is organized as follows.
Insection 2, we introduce the problem of legal docketentry classification and describe the data with somerepresentative examples.
In section 3, we describethe experiments performed with SVMs and severalof its variants.
We also identify the shortcomingsof the current classifiers in this section.
In section3.2, we present results from using human selectedfeatures for the classification problem and motivatetheir application for the docket entry classificationusing propositional logic in subsection 3.3.
We alsoshow that simple propositional logic using humanselected features and their labels outperforms thestate-of-the-art classifiers.
We conclude the discus-sion in section 4, where we argue the case for moresophisticated classifiers for specialized domains.2 Docket Entry ClassificationIn this section, we introduce the problem of legaldocket entry classification.In any US district court of law, information on thechronological events in a case is usually entered ina document called the case docket.
Each entry in adocket lists an event that occured on a specific datesuch as pleading, appeal, order, jury trial, judgment,etc.
The entries are brief descriptions of the events innatural language.
Sometimes, a single docket entrycan list multiple events that take place on the sameday.
Table 1 displays a sample docket for a case.Identifying various events in a court case is a cru-cial first step to automatically understanding the pro-gression of a case and also in gathering aggregatestatistics of court cases for further analysis.
Whilesome events such as ?Complaint?
may be easy toidentify using regular expressions, others are muchmore complex and may require sophisticated mod-eling.In this work, we are primarily interested in iden-tifying one such complex event called ?Order re:Summary Judgment?.
Summary Judgment is a le-gal term which means that a court has made a deter-mination (a judgment) without a full trial.1 Such ajudgment may be issued as to the merits of an entirecase, or of specific issues in that case.
Typically, one1See e.g., Wikipedia for more information:http://en.wikipedia.org/wiki/Summary judgmentof the parties (plaintiff or defendant) involved in thecase moves a motion for summary judgment, (usu-ally) in an attempt to eliminate the risk of losing atrial.
In an ?Order re: Summary Judgment?
event,the court may grant or deny a motion for summaryjudgment upon inspecting all the evidence and factsin the case.
The task then, is to identify all docketentries in a set of cases that list occurrences of ?Or-der re: Summary Judgment?
events.
We will callthem OSJ events in short.A few typical positive and negative docket entriesfor the OSJ event from various cases are shown intable 2.
The examples require some explanation.Firstly, all orders granting, denying or amendingmotions for full or partial summary judgment areconsidered OSJs.
However, if the motion is deniedas moot or denied without prejudice, it is not an OSJevent, as shown in the negative examples 1 and 2in table 2.
This is because in such cases, no de-cision was made on substantive issues of the case.Also, there are other kinds of orders that are issuedwith reference to a summary judgment motion thatdo not fall into the category of OSJ, such as negativeexamples 3 through 9.
To elaborate further, negativeexample 3 is about amending the deadline for fil-ing a summary judgment motion, but not a summaryjudgment motion itself.
Likewise, in negative exam-ple 4, the judge denies a motion to shorten time ona motion to vacate the order on summary judgment,but not the motion on summary judgment itself.
Theother negative examples are very similar in spirit andwe leave it as an exercise to the reader to interpretwhy they are negatively labeled.On first glance, it appears that a standard classifiermay do a good job on this data, since the classifica-tion seems to depend mostly on certain key wordssuch as ?granting?, ?denying?, ?moot?, etc.
Also no-tice that some of the docket entries contain multipleevents, but as long as it contains the ?order re: sum-mary judgment?
event, it falls into the positive class.This seems very similar to the standard case, wherea document may belong to multiple topics, but it isstill identified as on-topic by a binary classifier onthe corresponding topic.Hence, as a first step, we attempted using a stan-dard SVM classifier.439# Date Filed Text1 10/21/2002 Original Complaint with JURY DEMAND filed.
Cause: 35:271Patent Infringement Modified on 10/24/2002 (Entered: 10/22/2002)2 10/21/2002 Form mailed to Commissioner of Patents and Trademarks.
(poa)3 10/28/2002 Return of service executed as to Mathworks Inc 10/23/02Answer due on 11/12/02 for Mathworks Inc (poa) (Entered: 10/28/2002)4 11/4/2002 Unopposed Motion by Mathworks Inc The to extend time to answer orotherwise respond to pla?s complaint (ktd) (Entered: 11/05/2002)5 11/5/2002 ORDER granting [4-1] motion to extend time to answer or otherwiserespond to pla?s complaint, ans reset answer due on 11/27/02 for Mathworks Inc?
?
?
?
?
?
?
?
?
?
?
?Table 1: An example (incomplete) docket: each row in the table corresponds to a docket-entry2.1 DataWe have collected 5,595 docket entries from severalcourt cases on intellectual property litigation, thatare related to orders pertaining to summary judg-ment, and hand labeled them into OSJ or not OSJcategories.2 The hand-labeling was done by a sin-gle legal expert, who practised law for a number ofyears.
In all, 1,848 of these docket entries fall intothe OSJ category.In all our experiments, we split the entire data ran-domly into 20 disjoint subsets, where each set hasthe same proportion of positive-to-negative exam-ples as the original complete set.
For all the clas-sifiers we used in this work, we performed 20-foldcross validation.
We compute F1 scores on the held-out data of each run and report overall F1 score asthe single point performance measure.
We also per-form statistical significance tests using the resultsfrom the 20 cross-validation runs.2.2 PreprocessingBefore we ran our classifiers, we removed all punc-tuation, did casefolding, removed stopwords andstemmed the words using the Porter stemmer.
Weused unigrams and bigrams as our basic features.3We considered all the words and bigrams as bi-nary features and did not use any TF-IDF weight-ing.
Our justification for this decision is as fol-lows: the docket text is typically very short and it is2The data can be made available free of cost upon request.Please email the first author for more information.3In our preliminary experiments, we found that a combina-tion of unigrams and bigrams works better than unigrams alone.usually rare to see the same feature occurring mul-tiple times in a docket entry.
In addition, unlikein standard text classification, some of the featuresthat are highly frequent across docket entries suchas ?denying?,?granting?, etc., are also the ones thatare highly discriminative.
In such a case, down-weighting these features using IDF weights mightactually hurt performance.
Besides (Dumais et al,1998) found that using binary features works as wellas using TF-IDF weights.In addition, we also built a domain specific sen-tence boundary detector using regular expressions.4For constructing the features of a docket entry, weonly consider those sentences in the entry that con-tain the phrase ?summary judgment?
and its vari-ants.5 Our preliminary experiments found that thishelps the classifier focus on the relevant features,helping it to improve precision while not altering itsrecall noticeably.3 Experiments and results3.1 Basic SVMFirst we implemented the standard linear SVM6 onthis problem with only word-based features (uni-grams and bigrams) as the input.
Quite surprisingly,the model achieves an F1 score of only 79.44% asshown in entry 1 of table 5.
On inspection, we no-4It works well in most cases but is far from perfect, due tothe noisy nature of the data.5The variants include ?sum jgm?, ?S/J?, ?summary adjudi-cation?, ?summary jgm?, etc.6All our SVM experiments were performed us-ing the libsvm implementation downloadable fromhttp://www.csie.ntu.edu.tw/?cjlin/libsvm/440REPRESENTATIVE POSITIVE EXAMPLES1.
ORDER denying [36-1] motion for summary judgment on dfts Ranbaxy invalidity defenses by pltfs.
(signed byJudge Garrett E. Brown, Jr.)2.
ORDER GRANTING IN PART AND DENYING IN PART DEFENDANTS?
MOTION FOR SUMMARYJUDGMENT3.
ORDER re 78 MOTION to Amend/Correct Motion for Summary Judgment and supporting documents, filed byDefendant Synergetics USA, Inc. ; ORDERED GRANTED.4.
MEMORANDUM AND ORDER re: 495 Third MOTION for Partial Summary Judgment Dismissing Mon-santo?s Defenses Related to Dr. Barnes filed by Bayer BioScience N.V., motion is GRANTED IN PART ANDDENIED IN PART.5.
ORDER GRANTING IN PART PLTF S/J MOT; GRANTING IN PART PLTF MOT/CLARIFY; GRANTINGDEFT MOT/CLARIFY; PRTL S/J STAYED.6.
ORDER by Chief Judge Joe B. McDade.
Court is granting in part and denying in part Deere?s motion forreconsideration and clarification [42-2]; granting Toro?s motion for summary judgment of non-infringement [45-1]; denying Deere?s motion for summary judgment [58-1];7.
ORDER GRANTING DEFT.
MOTION FOR S/J AND DENYING PLTF.
MOTIONS FOR S/J AND TO SUP-PLEMENT.REPRESENTATIVE NEGATIVE EXAMPLES1.
ORDER - denying w/out prejudice 17 Motion for Summary Judgment, denying w/out prejudice 49 Motion toAmend/Correct .
Signed by Judge Kent A. Jordan on 1/23/06.2.
Order denying as moot motion for summary judgment.3.
Order granting 53 Motion to Amend/Correct the deadline for filing summary jgm motions will be moved 12/1/03to 12/8/034.
ORDER by Judge Claudia Wilken denying plaintiff?s motion to shorten time on motion to vacate portions ofCourt?s order on cross-motion for summary judgment on patent issues [695-1] [697-1]5.
MEMORANDUM AND ORDER: by Honorable E. Richard Webber, IT IS HEREBY ORDERED that DefendantAventis shall have 10 days from the date of this order to demonstrate why the Court should not grant summaryjudgment to Monsanto of non-infringement of claims 1-8 and 12 of the ?565 patent and claim 4 of the ?372patent.6.
ORDER by Judge Claudia Wilken DENYING motion for an order certifying for immediate appeal portions ofthe courts?
2/6/03 order granting in part plaintiff?s motion for partial summary judgment [370-1]7.
ORDER by Judge William Alsup denying in part 12 Motion to Consolidate Cases except as to one issue, grantingin part for collateral estoppel 20 Motion for Summary Judgment8.
ORDER ( Chief Mag.
Judge Jonathan G. Lebedoff / 9/11/02) that the court grants Andersen?s motion and ordersthat Andersen be allowed to bring its motions for summary judgment9.
ORDER by Judge Susan J. Dlott denying motion to strike declaration of H Bradley Hammond attached to mem-orandum in opposition to motion for partial summary judgment as to liability on the patent infringement andvalidity claims [40-1] [47-1] [48-1]Table 2: Order: re Summary Judgment: positive and negative docket entries.
The entries are reproduced as they are.441ticed that the SVM assigns high weights to manyspurious features owing to their strong correlationwith the class.As a natural solution to this problem, we selectedthe top 100 features7 using the standard informationgain metric (Yang and Pedersen, 1997) and ran theSVM on the pruned feature set.
As one would ex-pect, the performance of the SVM improved signif-icantly to reach an F1 score of 83.08% as shown inentry 2 of the same table.
However, it is still a far cryfrom the typical results on standard test beds wherethe performance is above 90% F1.
We suspectedthat training data was probably insufficient, but alearning curve plotting performance of the SVM asa function of the amount of training data reached aplateau with the amount of training data we had, sothis problem was ruled out.To understand the reasons for its inferior perfor-mance, we studied the features that are assignedthe highest weights by the classifier.
Although theSVM is able to assign high weights to several dis-criminative features such as ?denied?, and ?granted?,it also assigns high weights to features such as?opinion?, ?memorandum?, ?order?, ?judgment?, etc.,which have high co-occurrence rates with the posi-tive class, but are not very discriminative in terms ofthe actual classification.This is indicative of the problems associated withstandard feature selection algorithms such as infor-mation gain in these domains, where high correla-tion with the label does not necessarily imply highdiscriminative power of the feature.
Traditional clas-sification tasks usually fall into what we call the?topical classification?
domain, where the distribu-tion of words in the documents is a highly discrimi-native feature.
On such tasks, feature selection algo-rithms based on feature-class correlation have beenvery successful.
In contrast, in the current problem,which we call ?semantic classification?, there seemto be a fixed number of domain specific operativewords such as ?grant?, ?deny?, ?moot?, ?strike?, etc.,which, almost entirely decide the class of the docketentry, irrespective of the existence of other highlycorrelated features.
The information gain metric aswell as the SVM are not able to fully capture such7We tried other numbers as well, but top 100 featuresachieves the best performance.features in this problem.We leave the problem of accurate feature selec-tion to future work, but in this work, we address theissue by asking for human intervention, as we de-scribe in the next section.
One reason for seekinghuman assistance is that it will give us an estimateof upperbound performance of an automatic featureselection system.
In addition, it will also offer us ahint as to whether the poor performance of the SVMis because of poor feature selection.
We will aim toanswer this question in the next section.3.2 Human feature selectionUsing human assistance for feature selection is a rel-atively new idea in the text classification domain.
(Raghavan et al, 2006) propose a framework inwhich the system asks the user to label documentsand features alternatively.
They report that this re-sults in substantial improvement in performance es-pecially when the amount of labeled data is mea-gre.
(Druck et al, 2008) propose a new General-ized Expectation criterion that learns a classificationfunction from labeled features alone (and no labeleddocuments).
They showed that feature labeling canreduce annotation effort from humans compared todocument labeling, while achieving almost the sameperformance.Following this literature, we asked our annotatorsto identify a minimal but definitive list of discrim-inative features from labeled data.
The annotatorswere specifically instructed to identify the featuresthat are most critical in tagging a docket entry oneway or the other.
In addition, they were also askedto assign a polarity to each feature.
In other words,the polarity tells us whether or not the features be-long to the positive class.
Table 3 lists the completeset of features identified by the annotators.As an obvious next step, we trained the SVM inthe standard way, but using only the features from ta-ble 3 as the pruned set of features.
Remarkably, theperformance improves to 86.77% in F1, as shown inentry 3 of table 5.
Again, this illustrates the unique-ness of this dataset, where a small number of handselected features (< 40) makes a huge differencein performance compared to a state-of-the-art SVMcombined with automatic feature selection.
We be-lieve this calls for more future work in improvingfeature selection algorithms.442Label FeaturesPositive grant, deny, amend, reverse,adopt, correct, reconsider, dismissNegative strike, proposed, defer, adjourn,moot, exclude, change, extend,leave, exceed, premature, unseal,hearing, extend, permission,oral argument, schedule, ex parte,protective order, oppose,without prejudice, withdraw,response, suspend, request,case management order,to file, enlarge, reset, supplementplacing under seal, show causereallocate, taken under submissionTable 3: Complete set of hand-selected features: morpho-logical variants not listedNotice that despite using human assistance, theperformance of the SVM is still not at a desirablelevel.
This clearly points to deficiencies in the modelother than poor feature selection.
To understand theproblem, we examined the errors made by the SVMand found that there are essentially two types of er-rors: conjunctive and disjunctive.
Representative ex-amples for both kinds of errors are displayed in ta-ble 4.
The first example in the table correspondsto a conjunctive error, where the SVM is unable tomodel the binary switch like behavior of features.In this example, although ?deny?
is rightly assigneda positive weight and ?moot?
is rightly assigned anegative weight, when both features co-occur in adocket entry (as in ?deny as moot?
), it makes the la-bel negative.8 However, the combined weight of thelinear SVM is positive since the absolute value ofthe weight assigned to ?deny?
is higher than that of?moot?, resulting in a net positive score.
The secondexample falls into the category of disjunctive errors,where the SVM fails to model disjunctive behav-ior of sentences.
In this example, the first sentencecontains an OSJ event, but the second and third sen-tences are negatives for OSJ.
As we have discussedearlier, this docket entry belongs to the OSJ categorysince it contains at least one OSJ event.
However, we8This is very similar to the conjunction of two logical vari-ables where the conjunction of the variables is negative when atleast one of them is negative.
Hence the name conjunctive error.see that the negative weights assigned by the SVMto the second and third sentences result in an overallnegative classification.As a first attempt, we tried to reduce the conjunc-tive errors in our system.
Towards this objective,we built a decision tree9 using the same featureslisted in table 3.
Our intuition was that a decisiontree makes a categorical decision at each node in thetree, hence it could capture the binary-switch likebehavior of features.
However, the performance ofthe decision tree is found to be statistically indistin-guishable from the linear SVM as shown in entry4 of table 5.
As an alternative, we used an SVMwith a quadratic kernel, since it can also capture suchpairwise interactions of features.
This resulted in afractional improvement in performance, but is againstatistically indistinguishable from the decision tree.We also tried higher order polynomial kernels andthe RBF kernel, but the performance got no better.10It is not easy to analyze the behavior of non-linearkernels since they operate in a higher kernel space.Our hypothesis is that polynomial functions capturehigher order interactions between features, but theydo not capture conjunctive behavior precisely.As an alternative, we considered the followingheuristic: whenever two or more of the hand selectedfeatures occur in the same sentence, we mergedthem to form an n-gram.
The intuition behind thisheuristic is the following: using the same exampleas before, if words such as ?deny?
and ?moot?
oc-cur in the same sentence, we form the bigram ?deny-moot?, forcing the SVM to consider the bigram as aseparate feature.
We hope to capture the conjunctivebehavior of some features using this heuristic.
Theresult of this approach, as displayed in entry 6 oftable 5, shows small but statistically significant im-provement over the quadratic SVM, confirming ourtheory.
We also attempted a quadratic kernel usingsentence level n-grams, but it did not show any im-provement.Note that all the models and heuristics we usedabove only address conjunctive errors, but not dis-junctive errors.
From the discussion above, we sus-pect the reader already has a good picture of what9We used the publicly available implementation fromwww.run.montefiore.ulg.ac.be/?francois/software/jaDTi/10We also tried various parameter settings for these kernelswith no success.4431.
DOCKET ENTRY: order denying as moot [22-1] motion for summary judgment ( signed by judgefederico a. moreno on 02/28/06).FEATURES (WEIGHTS): denying (1.907), moot (-1.475)SCORE: 0.432; TRUE LABEL: Not OSJ; SVM LABEL: OSJ2.
DOCKET ENTRY: order granting dfts?
37 motion for summary judgment.
further ordered denyingas moot pla?s cross-motion 42 for summary judgment.
denying as moot dfts?
motion to strike pla?scross-motion for summary judgment 55 .
directing the clerk to enter judgment accordingly.
signed byjudge mary h murguia on 9/18/07FEATURES (WEIGHTS): granting (1.64), denying (3.57), strike(-2.05) moot(-4.22)SCORE: -1.06; TRUE LABEL: OSJ; SVM LABEL: Not OSJTable 4: Representative examples for conjunctive and disjunctive errors of the linear SVM using hand selected featuresan appropriate model for this data might look like.The next section introduces this new model devel-oped using the intuition gained above.3.3 Propositional Logic using Human Featuresand LabelsSo far, the classifiers we considered received a per-formance boost by piggybacking on the human se-lected features.
However, they did not take into ac-count the polarity of these features.
A logical nextstep would be to exploit this information as well.
Anappropriate model would be the generalized expec-tation criterion model by (Druck et al, 2008) whichlearns by matching model specific label expectationsconditioned on each feature, with the correspondingempirical expectations.
However, the base modelthey use is a logistic regression model, which is alog-linear model, and hence would suffer from thesame limitations as the linear SVM.
There is alsoother work on combining SVMs with labeled fea-tures using transduction on unlabeled examples, thatare soft-labeled using labeled features (Wu and Sri-hari, 2004), but we believe it will again suffer fromthe same limitations as the SVM on this domain.In order to address the conjunctive and disjunc-tive errors simultaneously, we propose a new, butsimple approach using propositional logic.
We con-sider each labeled feature as a propositional variable,where true or false corresponds to whether the la-bel of the feature is positive or negative respectively.Given a docket entry, we first extract its sentences,and for each sentence, we extract its labeled features,if present.
Then, we construct a sentence-level for-mula formed by the conjunction of the variables rep-resenting the labeled features.
The final classifier isa disjunction of the formulas of all sentences in thedocket entry.
Formally, the propositional logic basedclassifier can be expressed as follows:C(D) = ?N(D)i=1 (?Mij=1L(fij)) (1)where D is the docket entry, N(D) is its numberof sentences, Mi is the number of labeled featuresin the ith sentence, fij is the jth labeled feature inthe ith sentence and L() is a mapping from a fea-ture to its label, and C(D) is the classification func-tion where ?true?
implies the docket entry containsan OSJ event.The propositional logic model is designed to ad-dress the within-sentence conjunctive errors andwithout-sentence disjunctive errors simultaneously.Clearly, the within-sentence conjunctive behavior ofthe labeled features is captured by applying logicalconjunctions to the labeled features within a sen-tence.
Similarly, the disjunctive behavior of sen-tences is captured by applying disjunctions to thesentence-level clauses.
This model requires no train-ing, but for reasons of fairness in comparison, at test-ing time, we used only those human features (andtheir labels) that exist in the training set in eachcross-validation run.
The performance of this newapproach, listed in table 5 as entry 7, is slightly bet-ter than the best performing SVM in entry 6.
Thedifference in performance in this case is statisticallysignificant, as measured by a paired, 2-tailed t-test at95% confidence level (p-value = 0.007).Although the improvement for this model is sta-tistically significant, it does not entirely match our444# Model Recall (%) Precision (%) F1 (%)1 Linear SVM with uni/bigrams only 75.19 84.21 79.442 Linear SVM with uni/bigrams only FS100 82.47 83.69 83.08*3 Linear SVM with HF only 84.68 88.97 86.77*4 Decision Tree with HF only 85.22 89.38 87.255 Quadratic SVM with HF only 84.14 90.98 87.436 Linear SVM with HF sentNgrams 84.63 93.37 88.78*7 Propositional Logic with HF and their labels 85.71 93.45 89.67*Table 5: Results for ?Order re: Summary Judgment?
: FS100 indicates that only top 100 features were selected usingInformation Gain metric; HF stands for human built features, sentNgrams refers to the case where all the human-builtfeatures in a given sentence were merged to form an n-gram feature.
A ?*?
next to F1 value indicates statisticallysignificant result compared to its closest lower value, measured using a paired 2-tailed T-test, at 95% confidence level.The highest numbers in each column are highlighted using boldface.expectations.
Our data analysis showed a variety oferrors caused mostly due to the following issues:?
Imperfect sentence boundary detection: sincethe propositional logic model considers sen-tences as strong conjunctions, it is more sen-sitive to errors in sentence boundary detectionthan SVMs.
Any errors would cause the modelto form conjunctions with features in neighbor-ing sentences and deliver an incorrect labeling.?
Incomplete feature set: Some errors are causedbecause the feature set is not complete.
For ex-ample, negative example 4 in table 2 is taggedas positive by the new model.
This error couldhave been avoided if the word ?shorten?
hadbeen identified as a negative feature.?
Relevant but bipolar features: Although ourmodel assumes that the selected features ex-hibit binary nature, this may not always be true.For example the word allow is sometimes usedas a synonym for ?grant?
which is a positive fea-ture, but other times, as in negative example 8in table 2, it exhibits negative polarity.
Henceit is not always possible to encode all relevantfeatures into the logic based model.?
Limitations in expressiveness: Some naturallanguage sentences such as negative example5 in table 2 are simply beyond the scope of theconjunctive and disjunctive formulations.4 Discussion and ConclusionsClearly, there is a significant amount of work tobe done to further improve the performance of thepropositional logic based classifier.
One obviousline of work is towards better feature selection inthis domain.
One plausible technique would be touse shallow natural language processing techniquesto extract the operative verbs acting on the phrase?summary judgment?, and use them as the prunedfeature set.Another potential direction would be to extend theSVM-based system to model disjunctive behaviorof sentences.11 One way to accomplish this wouldbe to classify each sentence individually and then tocombine the outcomes using a disjunction.
But forthis to be implemented, we would also need labelsat the sentence level during training time.
One couldprocure these labels from annotators, but as an alter-native, one could learn the sentence-level labels inan unsupervised fashion using a latent variable at thesentence level, but a supervised model at the docket-entry level.
Such models may also be appropriate fortraditional document classification where each doc-ument could be multi-labeled, and it is somethingwe would like attempt in the future.In addition, instead of manually constructing thelogic based system, one could also automaticallylearn the rules by using ideas from earlier workon ILP (Muggleton, 1997), FOIL (Quinlan andCameron-Jones, 1993), etc.11Recall that the heuristics we presented for SVMs only ad-dress the conjunctive errors.445To summarize, we believe it is remarkable thata simple logic-based classifier could outperform anSVM that is already boosted by hand picked fea-tures and heuristics such as sentence level n-grams.This work clearly exposes some of the limitations ofthe state-of-the-art models in capturing the intrica-cies of natural language, and suggests that there ismore work to be done in improving the performanceof text based classifiers in specialized domains.
Assuch, we hope our work motivates other researcherstowards building better classifiers for this and otherrelated problems.AcknowledgmentsThe authors would like to thank Stanford universityfor financial support.ReferencesChidanand Apte?, Fred Damerau, and Sholom M. Weiss.1994.
Automated learning of decision rules for textcategorization.
ACM Trans.
Inf.
Syst., 12(3):233?251.R.
Bekkerman, R. El-Yaniv, N. Tishby, and Y. Win-ter.
2003.
Distributional word clusters vs. words fortext categorization.
Journal of Machine Learning Re-search.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using general-ized expectation criteria.
In Proceedings of ACM Spe-cial Interest Group on Information Retreival, (SIGIR).Susan Dumais, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive learning algorithmsand representations for text categorization.
In CIKM?98: Proceedings of the seventh international con-ference on Information and knowledge management,pages 148?155, New York, NY, USA.
ACM.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In ECML-98: 10th European Conference on MachineLearning.D.D.
Lewis and M. Ringuette.
1994.
Comparison of twolearning algorithms for text categorization.
In ThirdAnnual Symposium on Document Analysis and Infor-mation Retrieval.A.
McCallum and K. Nigam.
1998.
A comparison ofevent models for Na?
?ve Bayes text classification .
InAAAI-98 Workshop on Learning for Text Categoriza-tion.Stephen Muggleton.
1997.
Inductive Logic Program-ming: 6th International Workshop: Seleted Papers.Springer.J.R.
Quinlan and R.M.
Cameron-Jones.
1993.
Foil: amid-term report.
In Proceedings of European Confer-ence on Machine Learning.Hema Raghavan, Omid Madani, and Rosie Jones.
2006.Active learning with feedback on features and in-stances.
J. Mach.
Learn.
Res., 7:1655?1686.Xiaoyun Wu and Rohini Srihari.
2004.
Incorporatingprior knowledge with weighted margin support vectormachines.
In KDD ?04: Proceedings of the tenth ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 326?333, New York,NY, USA.
ACM.Yiming Yang and Xin Liu.
1999.
A re-examination oftext categorization methods.
In SIGIR ?99: Proceed-ings of the 22nd annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 42?49, New York, NY, USA.
ACM.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In ICML ?97: Proceedings of the Fourteenth Interna-tional Conference on Machine Learning, pages 412?420, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.Tong Zhang and Frank J. Oles.
2001.
Text categorizationbased on regularized linear classification methods.
In-formation Retrieval, 4(1):5?31.446
