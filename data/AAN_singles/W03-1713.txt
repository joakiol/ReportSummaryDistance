News-Oriented Automatic Chinese Keyword IndexingLi Sujian1lisujian@pku.edu.cnWang Houfeng1wanghf@pku.edu.cnYu Shiwen1Yusw@pku.edu.cnXin Chengsheng2csxin@peoplemail.com.cn1Institute of Computational Linguistics, Peking University, 1008712The Information Center of PEOPLE?S DAILY, 100733AbstractIn our information era, keywords are veryuseful to information retrieval, text clus-tering and so on.
News is always a do-main attracting a large amount ofattention.
However, the majority of newsarticles come without keywords, and in-dexing them manually costs highly.
Aim-ing at news articles?
characteristics andthe resources available, this paper intro-duces a simple procedure to index key-words based on the scoring system.
In theprocess of indexing, we make use of somerelatively mature linguistic techniques andtools to filter those meaningless candidateitems.
Furthermore, according to the hi-erarchical relations of content words,keywords are not restricted to extractingfrom text.
These methods have improvedour system a lot.
At last experimental re-sults are given and analyzed, showing thatthe quality of extracted keywords are sat-isfying.1 IntroductionWith more and more information flowing into ourlife, it is very important to lead people to gainmore important information in time as short aspossible.
Keywords are a good solution, whichgive a brief summary of a document?s content.With keywords, people can quickly find what theyare most interested in and read them carefully.That will save us a lot of time.
In addition, key-words are also useful to the research of informationretrieval, text clustering, and topic search [Frank1999].
Manually indexing keywords will costhighly.
Thus, automatically indexing keywordsfrom text is of great interests.News is always the main domain that peoplepay a large amount of attention to.
Unfortunately,only a small fraction of documents in this fieldhave keywords.
However, compared to unre-stricted text, news articles are relatively easy toextract keywords from, because they have the fol-lowing characteristics.
Firstly, a news document isalways short in length, and usually, only importantwords or phrases repeat.
Secondly, as a rule, thepurpose of news articles is to illustrate an event ora thing for readers.
Then this kind of articles usu-ally place more emphasis on some name entitiessuch as persons, places, organizations and so on.Lastly, important content often occurs the first timein the title, or in the anterior part of the whole text,especially the first paragraph or the first sentencein every paragraph.
These characteristics will helpus in keywords indexing.Several methods have been proposed for ex-tracting English keywords from text.
For example,Witten[1999] adopted Na?ve Bayes techniques, andTurney[1999] combined decision trees and geneticalgorithm in his system.
These systems achievedsatisfying results.
However, they need a largeamount of training documents with keywords,which are just what we are in need of now.
For theChinese language, some researchers adopt thestructure of PAT tree and make use of mutual in-formation to obtain keywords [Chien 1997, Yang2002].
Unfortunately, the construction of PAT treewill cost a lot of space and time.
In this paper,aiming at the characteristics of news-oriented arti-cles, resources and techniques of current situation,we will introduce a simple procedure to indexkeywords from text.
Section 2 will describe thearchitecture of the whole system.
In section 3, wewill introduce every module in detail, includinghow to obtain candidate keywords, how to filterout the meaningless items, and how to score possi-ble keyword candidates according to their featurevalues.
In section 4, experimental results will begiven and analyzed.
At last, we will end with theconclusion.233.1System OverviewKeyword indexing can also be called keyword ex-traction.
The definition of a keyword is not re-stricted to one word in our conception.
Here, akeyword can be seen as a Chinese character string,which might consist of more than one Chineseword.
These character strings can summarize thecontent of the document they are in.Aiming at the task of keywords indexing, oursystem is designed and composed of three modules.As in figure 1, the first module is to recognizesome Chinese character strings according to theirfrequency, and pick out those named entities in thetext as the candidate keywords.
The second mod-ule is a filter to remove all the meaningless charac-ter strings from the set of candidates.
And the thirdmodule is a selector, which evaluates every candi-date according to its feature values and choosefrom the candidate set those keywords with higherscore.
The higher score a character string has, themore content it will cover of the article it is in.In our system, there are three kinds of lexicons.The lexicon of proper nouns is used to recognizenamed entities.
The general lexicon includes Chi-nese words in common use, which is adopted forthe segmentation and POS tagging of the text.
Andthe lexicon of content words is used to expand theset of keywords.
They will be introduced in detailin the following section.System DesignRecognizer ModuleCharacter strings RecognizerSelectorFilterSegmentationfilterKeywordsexpansionFeaturecomputationChinese characterstringsPOS taggingfilterFilter of items withpunctuations andfunction wordsFilter of overlappedand dependent itemsRecognizer throughFrequency StatisticsNamed EntitiesRecognizerOriginal textCandidate items withfeature valuesFig.1.
System ArchitecturekeywordsProper NounsLexiconContent WordsLexiconGeneralLexiconIt can be seen that one document is composed of aset of character strings.
Every character string hasits frequency in the document.
In general, thosecharacter strings that occur several times can re-flect the topic of the document.
So, we take themout as keyword candidates.
In addition, named en-tities, such as person names, place names, organi-zations, translation terms, titles of person and so on,are usually very important for the document with-out reference to their frequency.
They will also bepicked out from the text by named entities recog-nizer and input into the filter module with othercharacter strings.Unlike English, there are no explicit wordboundaries in Chinese sentences, which makes itespecially difficult to tell whether a characterstring is composed of one word or more than oneword.
Due to this characteristic, we don?t use adictionary, but get those character strings only ac-cording to their frequency statistics.
We set athreshold value as 2 for the Chinese characterstrings considering the length of news documents.Suppose that a character string is c1c2?cn, andf(c1c2?cn) represents its frequency, then we ex-tract c1c2?cn from text only if f(c1c2?cn) equals toor is more than 2.
That is, only a character stringoccurs two or more than two times, it can be se-lected as a candidate keyword.There are two kinds of named entities.
The firstare those which have rules of composition, mainlyChinese names and foreign terms.
They can berecognized with statistical and rule-based methodscombined.
Chinese names are composed of familynames and first names, whose lengths are respec-tively 1 or 2 Chinese characters.
Furthermore,there is a relatively stable set of family names,which often provide the anchor to search a name.For foreign terms, there are a relatively set of Chi-nese characters which are generally used astranslation characters.
Due to the limitation of thepaper?s length, we don?t introduce the process ofrecognition in detail here.
The other kind ofnamed entities is mainly composed of propernouns which represent names of places, organiza-tions, person titles, etc.
They often occur in newsdocuments, but don?t have rules of composition.Thus, we collect such words into our proper nounslexicon.
Then the module can find these namedentities through looking up in this lexicon.3.2 Filter ModuleSo far, Chinese character strings are generated onlythrough frequency statistics.
Thus, some of themstand out just because of simple repetition and areprobably not meaningful units of language.
Weneed to filter out those meaningless items.
As infigure 1, we adopt four kinds of filters in filtermodule.
They work as follows.
(1) Filter of Overlapped and Dependent Itemsevident that such character strings can?t serve asFor two character strings S1 and S2, with S1 as asubstring of S2, and the frequency of S1 is equal tothat of S2, then S1 is overlapped by S2.
In fact, wecan set a threshold td for f(S1)-f(S2), where thefunction f(.)
represents the frequency of somecharacter string.
If the value of f(S1)-f(S2) is lessthan td, then the string S1 is dependent on S2.Here, the overlapped and dependent substring willbe removed from the candidate set.
(2) Filter of Items with Punctuations and Func-tion wordsThe recognizer module treats equally all symbolsin the text, such as Chinese characters andpunctuations, etc.
Thus when conducting theprocess of frequency statistics, for a characterstring, there might exist some punctuations andfunction words such as ??
?, ??
?, ??
?, ??
?, etc.These punctuations and function words usuallyoccur in the head or tail of a character string.
It?scharacter strings can?t serve as keywords of an ar-ticle, and they should be deleted from the candi-date set.
(3) Segmentation FilterWe find the first occurrence position of every can-didate keyword and get the sentence at the position.Then the sentence is segmented.
According to thesegmented result, we can verify whether the char-acter string is meaningful.
First of all, we get thesegmentation result of the character string in thesegmented sentence.
Suppose the character stringci?cj in the original text with the sentencec1c2?ci-1ci?cjcj+1?cn as its context, if thesegmentation tool segments ci-1ci or cjcj+1 into oneword, then ci?cj will not be regarded as an inte-grated unit.
That is, this item will be seen asmeaningless and filtered out from the set of candi-date keywords.
Here we don?t adopt the method ofconducting frequency statistics of words after seg-mentation, but use segmentation tool after fre-quency statistics of character strings.
There aresome reasons.
Above all, although the segmenta-tion technique is relatively mature, its precision isstill not high enough.
Then, for the same characterstring, its segmentation results often differ in dif-ferent sentences.
Thus, it?s difficult to compute thefrequency of a character string precisely.
Further-more, now we only need to segment one sentencefor a candidate keyword.
That will save us a greatdeal of time.
(4) POS FilterBecause keywords provide a brief summary forone document, they should be words or phrasesthat represent some meaning units such as nounsand noun phrases.
Therefore, a single word whosepart of speech is preposition, adverb, adjective, orconjunctive is filtered out.
At the same time, verbphrases, adjective phrases, preposition phrases arealso excluded from the candidate set.
The same assegmentation filter, we only do the POS taggingfor the sentence where every candidate keywordoccurs the first time.
If a candidate item is made ofmore than one word, it will have a sequence ofPOS tags according to which we can assign aphrase category.
The POS tags or phrase catego-ries are the basis for POS filtering.Only conducting frequency statistics of charac-ter strings can?t refine the candidate set well, andwe utilize the relatively mature linguistic segmen-tation and POS tagging techniques so that we canfurther improve the quality of the candidate key-words.
Here, the general lexicon with about60,000 Chinese words is applied to the processesof segmentation and POS tagging.3.3 Selector ModuleAfter several filtering, now we can get a reducedset of candidate keywords.
Most character stringsin the set are meaningful and reflect the content ofthe document to some extent.
For every candidatenow, we adopt several features to describe it.
Thefeatures include frequency, length, position of thefirst occurrence, part of speech and whether it is aproper noun or in a pair of specific punctuations, asin table 1.
At the same time, through the process-ing of several linguistic tools in filter module, wecan assign a value to every feature in every candi-date item.feature meaning of featurefreq Frequency of an itemlen Length of an itemis_noun Whether an item is a noun phrasein_title Whether the first occurrence of an item is in the title of one documentin_seg1Whether the first occurrence of anitem is in the first paragraph of onedocumentis_properWhether an item is a proper noun,for example: person name, organi-zation, translation term, placename, title of a person etc.in_signWhether an item is bracketed by apair of specific punctuations suchas ????
and ???
?.Table 1.
Features of candidate keywordsWe can find that the candidate set is still toolarge to select from it the keywords.
Then we willconduct feature calculation to refine the candidateset.
We have known that every candidate item hasa feature-value set.
These feature values are ourbasis to evaluate every candidate item.
We com-pute a score for every candidate keyword throughthe module of feature computation.
The higher thescore, the more relevant the candidate is to thedocument.We compute the percentage how much manuallyindexed keywords of different lengths cover in theset of automatically generated candidates.
As infigure 2, Length represents the length of keywordsand percentage denotes the corresponding percent-age that keywords of this length are in the set.
Thehigher the percentage, the more likely the key-words of this length are to be selected.
Therefore,we can make a conclusion that the score of a can-didate is directly proportional to the percentage ofits length.
Then we can acquire the relation be-tween score and length of a candidate.
At the sametime, we can also see that the score is directlyproportional to a candidate?s frequency.
Inaddition, score is relevant to other features in table1.
Thus, we get formula 1, as following.Fig.
2.
Relations between Percentage Selectedand Length of Keywords?????=??
?= ?
?otherwise0feature i  thesatisfiesck  if1)()1.7)((100ln)()(th)(Ffi2ckffwcklenckFreqckscoreickii(1)Where ck represents a candidate keyword, thefunction freq(ck) gets the frequency of ck, len(ck)represents its length, that is, the number of Chinesecharacters every item includes.
F represents all thebinary features of a candidate keyword as in table1.
Every feature except the features of freq and lenare denoted by fi.
fi(ck) is a binary function and itsvalue is 0 or 1.
If a candidate item ck satisfies theith feature, then the value is set to 1, otherwise, it?sset to 0.  wi is the corresponding weight of featurefi.
For features is_noun, in_title, in_seg1,is_proper and in_sign, we set their weights to 7, 13,5, 11 and 3 respectively by experience.
After eachcandidate keyword gets a score, we choose thosewhose scores rank higher as keywords.??(physicaltraining)????(physicalmanagement)????(sports)??
(trackand field)??
(ball) ......???(pingpang)???(badminton)??
(football) ...Fig.3.
A Sample Tree Structure of Content WordsNow the keywords we get are all selected fromthe original text.
However, some keywords mayexpress the content of the document, but they don?toccur in the text.
Therefore, we have constructedone list of content words with hierarchical relationsas in figure 3.
That is content words lexicon.
Thelexicon contains about 1,200 words which are of-ten used as keywords.
As the content words lexi-con available now, we can look up in it and expandobtained keywords to a higher level, i.e., if a se-lected keyword has a parent in the lexicon, the par-ent word will be expanded as a keyword.4 Experimental Results and AnalysisWe select 37 news articles from China Daily as ourtesting material from which experts have manuallyextracted keywords.
There are 23 articles aboutnational politics, 10 articles of international poli-tics, and 4 sports news articles.
Here, we auto-matically extracted keywords from them andevaluated the results with the standard measures ofprecision and recall, which are defined as follows:Where P represents precision, and R representsrecall.
In general, these two measures in one sys-tem are opposite to each other.
When precision ishigher, recall will be lower.
Otherwise, when pre-cision is improved, recall will decrease.
In table 2,we illustrate our experimental results.
The firstthree rows give measures for articles about differ-ent styles and the figures in parentheses representthe number of articles.
The fourth row gives theaverage measure of our system.
For comparison,we also illustrate the results of Chien?s [1997]PAT-tree-based method from his experiments inthe last row.
From this table, we can see that moreemphasis is placed on precision in Chien?s system.However, we incline to enhancing recall when pre-cision and recall are assured relatively balanced.When precision is lower, perhaps more noise isintroduced into the set of candidate keywords.
Be-cause we have adopted segmentation and POS tag-ging tools which can verify whether a candidatecharacter string is a meaningful unit and found thatthe noise introduced now is more or less relevantto the content of the article, we don?t have to worrymore about precision.
Therefore, we hope togenerate more keywords automatically under thecondition that the number of noise words is ac-cepted.Recall PrecisionNational politics (23) 0.452 0.401International Politics (10) 0.644 0.594Sports news (4) 0.629 0.482Average 0.523 0.462Chien?s (exact match) 0.30 0.43Table 2.
Experimental ResultsIt has to be pointed out that there are no satis-factory results in extracting keywords from texts[Chien, 1997].
Although some keywords extractedare the same as manually extracted ones in mean-ing, they are often different due to one or twocharacters mismatched.
According to our analysisof experimental results, though only 46% of ex-tracted keywords appear in the set of manual key-words, the rest are also relevant to the text andadapt to the need of information retrieval.
At thesame time, about 52% of the manual keywords aregenerated by the automatically indexing method,however, we can often find a substitute for most ofthe rest in the set of automatically generated key-words.manually indexing keywords ofnumberrecognized  keywords genuine ofnumber Rllyautomatica indexing keywords ofnumberrecognized  keywords genuine ofnumber P==Most of the keywords missed occur only oncein the text, but they are mostly proper nouns ofplaces, organizations or titles of person.
And thisreveals that we need to further improve the tech-niques to recognize proper nouns.5 Conclusion and Future WorkWe have described a system for automatically in-dexing keywords from texts.
One document is in-putted into the recognizer module, the filtermodule and the selector module consecutively,with keywords output.
Here we utilize the maturetechniques available now such as string frequencystatistics, segmentation and POS tagging tools.Then, according to features, we propose ourmethod to evaluate directly every candidate key-word and select those with higher scores as key-words.
At the same time, we break through thetradition of generating keywords only from theoriginal text and acquire some keywords throughlooking up in the lexicon of content words withhierarchical relations.
The experimental resultsshow that our system can perform comparably tothe state of the art.Owing to the limit of the training corpus, theparameters in scoring formula are set by experi-ence values.
With our method, we can cumulatemore and more documents with keywords.
Thenwe can adopt machine-learning methods to conductkeyword indexing, which can make parametersmore objective.
That will be our further work.References[Chien 1997] Chien, L. F., PAT-Tree-Based KeywordExtraction for Chinese Information Retrieval, Pro-ceedings of the ACM SIGIR International Confer-ence on Information Retrieval, 1997, pp.
50--59.
[Frank 1999] Frank E., Paynter G.W., Witten I.H., Gut-win C., and Nevill-Manning C.G., Domain-specifickeyphrase extraction, Proc.
Sixteenth InternationalJoint Conference on Artificial Intelligence, MorganKaufmann Publishers, San Francisco, CA, 1999, pp.668-673.
[Lai 2002] Yu-Sheng Lai, Chung-Hsien Wu, Meaning-ful term extraction and discriminative term selectionin text categorization via unknown-word methodol-ogy, ACM Transactions on Asian Language Informa-tion Processing (TALIP), Vol.1, No.1, March 2002,pp.
34-64.
[Liu 1998] Liu Ting, Wu Yan, Wang Kaizhu, An Chi-nese Word Automatic Segmentation System Basedon String Frequency Statistics Combined with WordMatching, Journal of Chinese Information Processing,Vol.12, No.1, 1998, pp.
17-25.
[Ong 1999] T. Ong and H. Chen,  Updateable PAT-TreeApproach to Chinese Key Phrase Extraction UsingMutual Information: A Linguistic Foundation forKnowledge Management, Proceedings of the SecondAsian Digital Libaray Conference, Taipei, Taiwan,Novemeber 8-9, 1999.
[Turney 1999] Turney, P.D., Learning to Extract Key-phrases from Text, NRC Technical Report ERB-1057,National Research Council, Canada, 1999.
[Witten 1999] Witten I.H., Paynter G.W., Frank E.,Gutwin C., and Nevill-Manning C.G., KEA: Practicalautomatic keyphrase extraction, Proc.
DL '99, 1999,pp.
254-256.
[Yang 2002] Wenfeng Yang, Chinese keyword extrac-tion based on max-duplicated strings of the docu-ments, Proceedings of the 25th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, 2002, pp.
439-440.
