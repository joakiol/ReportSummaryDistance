Improving Automatic Indexing through Concept Combinationand Term EnrichmentChr i s t ian  Jacquemin*LIMSI-CNRSBP 133, F-91403 ORSAY Cedex, FRANCEj acquemin@limsi, frAbst rac tAlthough indexes may overlap, the output ofan automatic indexer is generally presented asa fiat and unstructured list of terms.
Our pur-pose is to exploit term overlap and embed-ding so as to yield a substantial qualitativeand quantitative improvement in automatic in-dexing through concept combination.
The in-crease in the volume of indexing is 10.5% forfree indexing and 52.3% for controlled indexing.The resulting structure of the indexed corpus isa partial conceptual nalysis.1 Overv iewThe method, proposed here for improving au-tomatic indexing, builds partial syntactic stru-ctures by combining overlapping indexes.
It iscomplemented by a method for term acquisitionwhich is described in (Jacquemin, 1996).
Thetext, thus structured, is reindexed; new indexesare produced and new candidates are discove-red.Most NLP approaches to automatic indexingconcern free indexing and rely on large-scaleshallow parsers with a particular concern fordependency relations (Strzalkowski, 1996).
Forthe purpose of controlled indexing, we exploitthe output of a NLP-based indexer and the stru-ctural relations between terms and variants inorder to (1) enhance the coverage of the in-dexes, (2) incrementally build an a posterioriconceptual analysis of the document, and, (3)interweave controlled indexing, free indexing,and thesaurus acquisition.
These 3 goals areachieved by CONPARS (CONceptual PARSer),presented in this paper and illustrated by Fi-gure 1.
CONPARS is based on the output of* We thank INIST-CNRS for providing us with thesauriand corpora in the agricultural domain and AFIRST forsupporting this research through the SKETCHI project.a part-of-speech tagger for French described in(Tzoukermann and Radev, 1997) and FASTR,a controlled indexer (Jacquemin et al, 1997).All the experiments reported in this paper areperformed on data in the agricultural domain:\[AGRIC\] a 1.18-million word corpus, \[AGRO-VOC\] a 10,570-term controlled vocabulary, and\[AGR-CAND\] a 15,875-term list acquired byACABIT (Daille, 1997) from \[AGRIC\].Augmented indexingFigure 1: Overall Architecture of CONPARS2 Basic  Cont ro l led  Index ingThe preprocessing of the corpus by the tag-ger yields a morphologically analyzed text,with unambiguous syntactic ategories.
Then,the tagged corpus is automatically indexed byFASTR which retrieves occurrences of multi-word terms or variants (see Table 1).595Table 1: Indexing of a Sample SentenceLa variation mensuelle de la respiration du sol etses rapports avec l'humiditd et la tempdrature dusol ont dtd analysdes dans le sol super\]iciel d'unefor~t tropicale.
(The monthly variation of the respi-ration of the soil and its connections with the mois-ture and the temperature of the soil have been ana-lyzed in the surface soil of a tropical forest.
)il 007019 Respiration du sol Occurrencerespiration du sol (respiration of the soil)i2 002904 Sol de for~t Embedding2so_.__l superficiel d'une \]or~t (surf.
soil of a forest)i3 012670 Humiditd du sol Coordination1humiditd et la tempdrature du sol(moisture and the temperature of the soil)i4 007034 Tempdrature du sol Occurrencetempdrature du sol (temperature of the soil)i5 007035 Analyse de sol VerbTransflanalysdes clans le sol (analyzed in the soil)i6 007809 For~t tropicale Occurrencefor~t tropicale (tropical forest)Each variant is obtained by generating termvariations through local transformations com-posed of an input lexico-syntactic structureand a corresponding output transformed struc-ture.
Thus, VerbTransfl is a verbalization whichtransforms a Noun-Preposition-Noun term intoa verb phrase represented by the variation pat-tern V 4 (Adv ?
(Prep ?
Art \[ Prep) A ?)
N3:1VerbTransfl( N1 Prep2 N3 ) (1)= V4 (Adv ?
(Prep ?
Art J Prep) A ?)
N3{MorphFamily(N1) = MorphFamily(V4)}The constraint following the output structurestates that V4 belongs to the same morphologi-cal family as N1, the head noun of the term.VerbTransfl recognizes analys~es\[v\] dans\[prep\]le\[nrt\] sOl\[N\] (analyzed in the soil) as a variantof analyse\[N\] de\[Prep\] sol\[N\] (soil analysis).Six families of term variations are accountedfor by our implementation forFrench: coordina-tion, compounding/decompounding, termem-bedding, verbalization (of nouns or adjectives),nominalization (of nouns, adjectives, or verbs),and adjectivization (of nouns, adjectives, orverbs).
Each index in Table 1 corresponds to1The following abbreviations are used for the catego-ries: V = verb, N = noun, Art = article, hdv --- adverb,Conj = conjunction, Prep --- preposition, Punc -- punc-tuation.a unique term; it is referenced by its identifier,its string, and a unique variation of one of theaforementioned types (or a plain occurrence).3 Conceptua l  Phrase  Bu i ld ingThe indexes extracted at the preceding step aretext chunks which generally build up a correctsyntactic structure: verb phrases for verbaliza-tions and, otherwise, noun phrases.
When over-lapping, these indexes can be combined and re-placed by their head words so as to condenseand structure the documents.
This process isthe reverse operation of the noun phrase decom-position described in (Habert et al, 1996).The purpose of automatic indexing entails thefollowing characteristics of indexes:?
frequently, indexes overlap or are embed-ded one in another (with \[AGR-CAND\],35% of the indexes overlap with anotherone and 37% of the indexes are embed-ded in another one; with \[AGROVOC\], therates are respectively 13% and 5%),?
generally, indexes cover only a small fra-ction of the parsed sentence (with \[AGR-CAND\], the indexes cover, on average, 15%of the surface; with \[AGROVOC\], the ave-rage coverage is 3%),?
generally, indexes do not correspond tomaximal structures and only include partof the arguments of their head word.Because of these characteristics, the construc-tion of a syntactic structure from indexes is likesolving a puzzle with only part of the clues, andwith a certain overlap between these clues.Text  S t ruc tur ingThe construction of the structure consists of thefollowing 3 steps:S tep  1.
The syntactic head of terms is deter-mined by a simple noun phrase grammar of thelanguage under study.
For French, the followingregular expression covers 98% of the term struc-tures in the database \[AGROVOC\] (Mod is anyadjectival modifier and the syntactic head is thenoun in bold face):Mod* N N ?
(Mod I (Prep Art ?
Mod* N N ?
Mod*))*The second source of knowledge about synta-ctic heads is embodied in transformations.
For596instance, the syntactic head of the verbalizationin (1) is the verb in bold typeface.Step 2.
A partial relation between the indexesof a sentence is now defined in order to rankin priority the indexes that should be groupedfirst into structures (the most deeply embeddedones).
This definition relies on the relative spa-tial positions of two indexes i and j and theirsyntactic heads H(i) and H(j):Def init ion 3.1 ( Index pr ior i ty)  Let i and jbe two indexes in the same sentence.
The rela-tive priority ranking of i and j is:i~ j  ?~ ( i= j )  V (H( i )=n( j )A iC j )V (H( i )?H( j )AH( i )e j  A n(j)?_i)This relation is obviously reflexive.
It is nei-ther transitive nor antisymmetric.
It can, howe-ver, be shown that this relation is not cyclic for3 elements: i~ j  A jT~k =?
-~(kT~i).
(Thisproperty is not demonstrated here, due to thelack of space.
)The linguistic motivations of Definition 3.1are linked to the composite structure built atStep 3 according to the relative priorities tatedby T~.
We now examine, in turn, the 4 cases ofterm overlap:1.
Head embedding: 2 indexes i and j, witha common head word and such that i isembedded into j, build a 2-level structure:H(i) H(i)H(i)This structuring is illustrated by napped'eau (sheet of water) which combineswith nappe d'eau souterraine (undergroundsheet of water) and produces the 2-levelstructure \[\[nappe d'eau\] souterraine\] (\[un-derground ~ of water\]\]).
(Head wordsare underlined.)
In this case, i has a higherpriority than j; it corresponds to (H(i) =H(j)  A i C_ j) in Definition 3.1.2.
Argument embedding: 2 indexes i and j,with different head words and such that thehead word of i belongs to j and the headword of j does not belong to i, combine asfollows:n(j) H(j) H(i)14(0This structuring is illustrated by napped'eau which combines with eau souter-raine (underground water) and producesthe structure \[nappe d~.eau souterraine\]\](\[sheet of \[underground water.\]\]).
Here, ihas a higher priority than j; it correspondsto (H(i) ~ H(j)  A H(i) ?
j A g ( j )  ~ i)in Definition 3.1.3.
Head overlap: 2 indexes i and j, witha common head word and such that iand j partially overlap, are also combi-ned at Step 3 by making j a substructureof i.
This combination is, however, non-deterministic since no priority ordering isdefined between these 2 indexes.
There-fore, it does not correspond to a conditionin Definition 3.1.H(i)In our experiments, this structure cor-responds to only one situation: a headword with pre- and post-modifiers uchas importante activitd (intense activity)and activivtg de ddgradation mdtabolique(activity of metabolic degradation).With \[-AGR-CAND\], this configurationis encountered only 27 times (.1% ofthe index overlaps) because premodifiersrarely build correct term occurrences inFrench.
Premodifiers generally correspondto occasional characteristics such as size,height, rank, etc.4.
The remaining case of overlapping indexeswith different head words and reciprocal in-clusions of head words is never encounte-red.
Its presence would undeniably denotea flaw in the calculus of head words.Step 3.
A bottom-up structure of the sentencesis incrementally built by replacing indexes bytrees.
The indexes which are highest ranked by597the Step 2 are processed first according to thefollowing bottom-up algorithm:1. build a depth-1 tree whose daughter nodesare all the words in the current sentenceand whose head node is S,2.
for all the indexes i in the current sentence,selected by decreasing order of priority,(a) mark all the the depth-1 nodes whichare a lexical leaf of i or which are thehead node of a tree with at least oneleaf in i,(b) replace all the marked nodes by aunique tree whose head features arethe features of H(i), and whose depth-1 leaves are all the marked nodes.When considering the sentence given inTable 1, the ordering of the indexes after Step 2is the following: i2 > i5, i6 > i2, and i4 > i3.
(They all result from the argument embeddingrelation.)
The algorithm yields the followingstructure of the sample sentence:f...la respiration et ses rapports avec l'humidit~ ont dt~ analvs~esrespiration du sol humidit~ et la temperature analys~es dans le soltemperature du sol sol superficiel d'une for~tfor~t tropicaleText Condensat ionThe text structure resulting from this algorithmcondenses the text and brings closer words thatwould otherwise remain separated by a largenumber of arguments or modifiers.
Because ofthis condensation, a reindexing of the structu-red text yields new indexes which are not ex-tracted at the first step.Let us illustrate the gains from reindexingon a sample utterance: l'dvolution au cours dutemps du sol et des rendements (temporal evo-lution of soils and productivity).
At the firststep of indexing, ~volution au cours du temps(lit.
evolution over time) is recognized as a va-riant of dvolution dans le temps (lit.
evolutionwith time).
At the second step of indexing, thedaughter nodes of the top-most ree build thecondensed text: l'dvolution du sol et des rende-ments (evolution of soils and productivity):1st stepl'~volution au cours du temps du sol el des rendements2nd stepl'~volution du sol et des rendementsl'~volution au cours du tempsThis condensed text allows for another index ex-traction: dvolution du sol et des rendements, aCoordination variant of dvolution du rendement(evolution of productivity).
This index was notvisible at the first step because of the additionalmodifier au cours du temps (temporal).
(Reite-rated indexing is preferable to too unconstrai-ned transformations which burden the systemwith spurious indexes.
)Both processes--text s ructuring, presentedhere, and term acquisition, described in (Jac-quemin, 1996)--reinforce each other.
On theone hand, acquisition of new terms increases thevolume of indexes and thereby improves textstructuring by decreasing the non-conceptualsurface of the text.
On the other hand, textcondensation triggers the extraction of new in-dexes, and thereby furnishes new possibilitiesfor the acquisition of terms.4 Eva luat ionQual i tat ive evaluat ion:  The volume of in-dexing is characterized by the surface of thetext occupied by terms or their combinations--we call it the conceptual surface.
Figure 2shows the distribution of the sentences in re-lation to their conceptual surface.
For instance,in 8,449 sentences among the 62,460 sentencesof \[AGRIC\], the indexes occupy from 20 to 30%of the surface (3rd column).This figure indicates that the structures builtfrom free indexing are significantly richer thanthose obtained from controlled indexing.
Thenumber of sentences i  a decreasing exponen-tial function of their conceptual surface (a linearfunction with a log scale on the y axis).Figure 3 illustrates how the successive stepsof the algorithm contribute to the final size ofthe incremental indexing.
For each mode of59810 s~ 10 4N 10 3~ 10 2~ 10 I~100........ Free indexing........
Controlled indexing10 20 30 40 50 60 70 80 90 100% of conceptual sufaceFigure 2: Conceptual Surface of SentencesTable 2: Increase in the volume of indexingAcquisition Condensation TotalControlled 49.3% 3.0% 52.3%Free 5.8% 4.7% 10.5%indexing two curves are plotted: the phrasesresulting from initial indexing and from rein-dexing due to text condensation (circles) andthe phrases due to term acquisition (asterisks).For instance, at step3, free indexing yields 309indexes and reindexing 645.
The correspondingpercentages are reported in Table 2.The indexing with the poorest initial volume(controlled indexing) is the one that benefitsbest from term acquisition.
Thus, concept com-bination and term enrichment tend to compen-sate the deficiencies of the initial term list byextracting more knowledge from the corpus.10 5,"~ 10 4.103102~.
10'I0 ~~ o Free indexing* Free acquisition"'.... ~_._~.~..
..-.@-..
Controlled indexing.
"'-_.
~ .
.
.
.
* .... o ....
Controlled acquisition2 3 4 5 6 7 8# stepFigure 3: Step-by-step Number of PhrasesQual i tat ive valuat ion:  Table 3 indicates thenumber of overlapping indexes in relation totheir type.
It provides, for each type, the rate ofsuccess of the structuring algorithm.
This eva-Table 3: Incremental Structure BuildingHead Argument Totalembedding embeddingDistribution 27.0% 73.0% 100%# correct 128 346 474Precision 79.0% 91.1% 87.5%luation results from a human scanning of 542randomly chosen structures.5 Conc lus ionThis study has presented CONPARS, a toolfor enhancing the output of an automatic in-dexer through index combination and term en-richment.
Ongoing work intends to improve theinteraction of indexing and acquisition throughself-indexing of automatically acquired terms.Re ferencesB6atrice Daille.
1997.
Study and implementa-tion of combined techniques for automatic ex-traction of terminology.
In J. L. Klavans andP.
Resnik, ed., The Balancing Act: Combi-ning Symbolic and Statistical Approaches toLanguage, p. 49-66.
MIT Press, Cambridge.Benoit Habert, Elie Naulleau, and Adeline Na-zarenko.
1996.
Symbolic word clustering formedium size corpora.
In Proceedings of CO-LING'96, p. 490-495, Copenhagen.Christian Jacquemin, Judith L. Klavans, andEvelyne Tzoukermann.
1997.
Expansion ofmulti-word terms for indexing and retrievalusing morphology and syntax.
In Proceedingsof ACL-EACL'97, p. 24-31.Christian Jacquemin.
1996.
A symbolic andsurgical acquisition of terms through varia-tion.
In S. Wermter, E. Riloff, and G. Sche-ler, ed., Connectionist, Statistical and Symbo-lic Approaches to Learning for NLP, p. 425-438.
Springer, Heidelberg.Tomek Strzalkowski.
1996.
Natural languageinformation retrieval.
Information Processing~J Management, 31(3):397-417.Evelyne Tzoukermann and Dragomir R. Radev.1997.
Use of weighted finite state transducersin part of speech tagging.
In A. Kornai, ed.,Extended Finite State Models of Language.Cambridge University Press.599
