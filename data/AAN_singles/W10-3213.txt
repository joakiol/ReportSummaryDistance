Proceedings of the 8th Workshop on Asian Language Resources, pages 95?102,Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language ProcessingDzongkha Word SegmentationSithar Norbu, Pema Choejey, Tenzin DendupResearch DivisionDepartment of Information Technology &Telecom{snorbu, pchoejay, tdendup}@dit.gov.btSarmad Hussain, Ahmed MauzCenter for Research in Urdu Language ProcessingNational University of Computer & EmergingSciences{sarmad.hussain, ahmed.mauz}@nu.edu.pkAbstractDzongkha,  the  national  language  ofBhutan, is continuous in written formand it fails to mark the word boundary.Dzongkha word segmentation is one ofthe  fundamental  problems  and  aprerequisite  that  needs  to  be  solvedbefore  more  advanced  Dzongkha  textprocessing and other natural  languageprocessing  tools  can  be  developed.This paper presents our initial attemptat segmenting Dzongkha sentences intowords.
The  paper  describes  theimplementation of Maximal  Matching(Dictionary based Approach) followedby bigram techniques  (Non-dictionarybased  Approach)  in  segmenting  theDzongkha  scripts.
Although  the  usedtechniques  are  basic  and  naive,  itprovides  a  baseline  of  the  Dzongkhaword  segmentation  task.
Preliminaryexperimental  results  show  percentageof  segmentation  accuracy.
However,the segmentation accuracy is dependenton the  type  of  document  domain  andsize and quality of the lexicon and thecorpus.
Some of the related issues forfuture directions are also discussed.Keywords:  Dzongkha  script,  wordsegmentation,  maximal  matching,  bigramtechnique, smoothing technique.1    IntroductionSegmentation of a sentence into word is one ofthe  necessary  preprocessing  tasks  and  isessential  in  the  analysis  of  natural  languageprocessing.
This  is  because  word  is  bothsyntactically  and  semantically,  thefundamental  unit  for  analyzing  languagestructure.
Like  in  any  other  languageprocessing task, Dzongkha word segmentationis also viewed as one of the fundamental andforemost  steps in Dzongkha related languageprocessing tasks.The most challenging features of Dzongkhascript is the lack of word boundary separationbetween  the  words1.
So,  in  order  to  do  thefurther  linguistic  and  natural  languageprocessing  tasks,  the  scripts  should  betransformed into a chain of words.
Therefore,segmenting  a  word  is  an  essential  role  inNatural  Language  Processing.
Like  Chinese,Japanese  and  Korean  (CJK)  languages,Dzongkha  script  being  written  continuouslywithout  any  word  delimiter  causes  a  majorproblem in natural language processing tasks.But,  in  case  of  CJK,  Thai,  and  Vietnameselanguages,  many  solutions  have  beenpublished  before.
For  Dzongkha,  this  is  thefirst  ever  word  segmentation  solution  to  bedocumented.In  this  paper,  we  describe  the  Dzongkhaword segmentation, which is performed firstlyusing the Dictionary based approach where theprinciple  of  maximal  matching  algorithm  isapplied  to  the  input  text.
Here,  given  thecollection  of  lexicon,  the  maximal  matchingalgorithm selects the segmentation that yieldsthe minimum number of words token from allpossible segmentations of  the input  sentence.Then,  it  uses  non-dictionary  based  approachwhere  bigram  technique  is  applied.
Theprobabilistic  model  of  a  word  sequence  is1http://www.learntibetan.net/grammar/sentence.htm95studied  using  the  Maximum  LikelihoodEstimation  (MLE).
The  approach  using  theMLE has an obvious disadvantage because ofthe  unavoidably  limited  size  of  the  trainingcorpora (Nuges, 2006).
To this problem of datasparseness,  the  idea  of  Katz  back-off  modelwith  Good-Turing  smoothing  technique  isapplied.2    Dzongkha ScriptDzongkha language is the official and nationallanguage of  Bhutan.
It  is  spoken as  the  firstlanguage  by  approximately  130,000  peopleand as the second language by about 470,000people (Van Driem and Tshering, 1998).Dzongkha  is  very  much  related  to  Sino-Tibetan  language  which  is  a  member  ofTibeto-Burmese  language  family.
It  is  analphabetic  language,  with  phoneticcharacteristics  that  mirror  those  of  Sanskrit.Like many of the alphabets of India and SouthEast  Asia,  the  Bhutanese  script  calledDzongkha script is also a syllabic2.
A syllablecan  contain  as  little  as  one  character  or  asmany as six characters.
And a word can be ofone syllable, two syllable or multiple syllables.In the written form, Dzongkha script contains adot, called Tsheg (  ? )
that serve as syllable andphrase delimiter, but words are not separated atall.For example,Dzongkha Transliteration English Syllables?????
dmarpo red Single-syllabled????????
slop-pon Teacher Two-syllabled??????????
hjam-tog-to easy Three-syllabled???????????
har-ri-hur-ri crowdedness/confusion Four-syllabledTable 1: Different syllabled Dzongkha scripts.The  sentence  is  terminated  with  a  verticalstroke  called Shad (   ?
).
This  Shad acts  as  afull_stop.
The  frequent  appearance  of2http://omniglot.com/writing/tibetan.htmwhitespace in the Dzongkha sentence serves asa phrase boundary or comma, and is a faithfulrepresentation  of  speech:  after  all  in  speech,we pause not between words, but either aftercertain phrases or at the end of sentence.The  sample  dzongkha  sentence  reads  asfollows:???????????????????????????
?????????????
???????????????
??????????????????????
????????????????????????
?????????????????????????????
??????????????????????????
????????????????
?????????????
??????????????????????????????????????
?????????????????????????????????????????????????????????
??????
????
????????????????????????
(English Translation of example text)[The  Dzongkha  Development  Commission  isthe  leading  institute  in  the  country  for  theadvancement  of  Dzongkha,  the  nationallanguage  of  Bhutan.
It  is  an  independentorganization established by the Fourth King ofBhutan,  His  Majesty the  King  Jigme  SingyeWangchuck, in 1986.
]3    Materials and MethodsSince,  our  language  has  no  word  boundarydelimiter,  the  major  resource  for  Dzongkhaword segmentation  is  a  collection of  lexicon(dictionary).
For  such  languages,  dictionariesare  needed  to  segment  the  running  texts.Therefore, the coverage of a dictionary plays asignificant  role  in  the  accuracy  of  wordsegmentation (Pong and Robert, 1994).The dictionary that we used contains 23,333word  lists/lexicons.
The  lexicons  werecollected  from  ?Dzongkha  Dictionary?,  2ndEdition, Published by Dzongkha DevelopmentAuthority,  Ministry  of  Education,  2005,(ddc@druknet.bt).
The  manually  segmentedtext corpus containing 41,739 tokens are alsoused  for  the  method.
The  text  corpora  werecollected  from  different  sources  likenewspaper articles, dictionaries, printed books,etc.
and  belong  to  domains  such  as  WorldAffairs,  Social  Sciences,  Arts,  Literatures,Adventures, Culture and History.
Some textslike poetry and songs were added manually.96Table  below  gives  the  glimpse  of  textualdomains contained in the text corpora used forthe method (Chungku et al, 2010).Domain Sub domain (%)World Affairs Bilateral relations 12%Social Science Political Science 2%Arts Poetry/Songs/Ballad 9%Literatures Essays/Letters/Dictionary 72%Adventures Travel Adventures 1%Culture Culture Heritage/Tradition 2%History Myths/Architecture 2%Table 2:  Textual domain contained in CorpusFigure  1  below  shows  the  Dzongkha  WordSegmentation Process.Figure  1:  Dzongkha  Word  SegmentationProcess.Dzongkha  word  segmentation  implements  aprinciple  of  maximal  matching  algorithmfollowed by statistical (bigram) method.
It usesa word list/lexicon at first to segment the rawinput sentences.
It then uses MLE principles toestimate  the  bigram  probabilities  for  eachsegmented words.
All  possible segmentationof an input sentence by Maximal Matching arethen  re-ranked  and  picked  the  mostly  likelysegmentation  from  the  set  of  possiblesegmentations  using  a  statistical  approach(bigram technique).
This is to decide the bestpossible  segmentation  among  all  the  words(Huor et al, 2007) generated by the maximalmatching  algorithm.
These  mechanisms  aredescribed in the following3.1    Maximal Matching AlgorithmThe basic idea of Maximal matching algorithmis, it first generates all possible segmentationsfor  an  input  sentence  and  then  selects  thesegmentation  that  contains  the  minimumnumber  of  word  tokens.
It  uses  dictionarylookup.We used the following steps to segment thegiven input sentence.1.
Read  the  input  of  string  text.
If  aninput  line  contains  more  than  onesentence,  a  sentence  separator  isapplied  to  break  the  line  intoindividual sentences.2.
Split input string of text by Tsheg(   ?
)into syllables.3.
Taking the next syllables, generate allpossible strings4.
If the number of string is greater thann for some value n3?
Look  up  the  series  of  string  in  thedictionary to find matches, and assignsome weight-age4 accordingly.?
Sort the string on the given weight-age?
Delete  (number  of  strings  ?
n)  lowcount strings.5.
Repeat from Step 2 until all syllablesare processed.The  above  mentioned  steps  produced  allpossible segmented words from the given inputsentence based on the provided lexicon.
Thus,the overall accuracy and performance dependson the coverage of lexicon (Pong and Robert,1994).3The greater the value of n, the better the chances ofselecting the sentence with the fewest words fromthe possible segmentation.4If the possible string is found in the dictionaryentries, the number of syllable in the string iscounted.
Then, the weight-age for the string iscalculated as (number of syllable)2 else it carries theweight-age 0973.2    Bigram Method(a)    Maximum Likelihood Estimation5In the bigram method, we make theapproximation that the probability of a worddepends on identifying the immediatelypreceding word.
That is, we calculate theprobability of next word given the previousword, as follows:P ?w1n?=?
i=1n P ?wi/w i?1?where?
P ?wi /wi?1?= count ?wi?1w i ?count ?wi?1 ?where?
count ?wi?1wi ?
is  a  total  occurrenceof  a  word  sequence  w i?1wi in  thecorpus, and?
count ?wi?1?
is a total occurrence of aword w i?1 in the corpus.To make  P ?wi /wi?1?
meaningful  for  i=1 ,we  use  the  distinguished  token  <s>  at  thebeginning of the sentence; that is, we pretendw0 = <s>.
In addition, to make the sum of theprobabilities  of  all  strings  equal  1,  it  isnecessary to place a distinguished token </s>at the end of the sentence.One of the key problems with the MLE isinsufficient  data.
That  is,  because  of  theunavoidably limited size of the training corpus,vast majority of the word are uncommon andsome of the bigrams may not occur at all in thecorpus, leading to zero probabilities.Therefore,  following  smoothing  techniqueswere used to count the probabilities of unseenbigram.
(b)    Smoothing Bigram ProbabilisticThe  above  problem  of  data  sparsenessunderestimates the probability of some of thesentences  that  are  in  the  test  set.
Thesmoothing technique helps to prevent errors bymaking  the  probabilities  more  uniform.Smoothing  is  the  process  of  flattening  a5P.M, Nugues.
An Introduction to LanguageProcessing with Perl and Prolog: An Outline ofTheories, Implementation, and Application withSpecial Consideration of English, French, andGerman (Cognitive Technologies) (95 ?
104).probability distribution implied by a languagemodel  so that  all  reasonable  word sequencescan  occur  with  some  probability.
This  ofteninvolves  adjusting  zero  probabilities  upwardand high probabilities  downwards.
This way,smoothing  technique  not  only  helps  preventzero probabilities but  the overall  accuracy ofthe  model  are  also  significantly  improved(Chen and Goodman, 1998).In Dzongkha word segmentation, Katz back-off  model  based  on  Good-Turing  smoothingprinciple is applied to handle the issue of datasparseness.
The  basic  idea  of  Katz  back-offmodel is to use the frequency of n-grams and ifno n-grams are available, to back off to  (n-1)grams,  and  then  to  (n-2) grams  and  so  on(Chen and Goodman, 1998).The  summarized  procedure  of  Katzsmoothing technique is given by the followingalgorithm:6Pkatz ?wi?wi?1 ?={ C ?wi?1 /wi ?
ifr>kdrC ?wi?1 /wi ?
ifk?r>0?
?wi?1 ?P ?wi ?
ifr=0 }where?
r is the frequency of bigram counts?
k  is  taken  for  some  value  in  therange of  5  to  10,  other  counts  arenot re-estimated.?
dr =r?r ?
?k+1 ?nK+1n11?
?k+1 ?
nk+1n1??
?wi?1?
=1?
?wi :r>0PKatz ?wi?w i?1?1?
?wi :r>0PKatz ?w i ?With the above equations, bigrams with non-zero count  r  are discounted according to the6X.
Huang, A. Acero, H.-W.Hon, Spoken LanguageProcessing: A Guide to Theory, Algorithm andSystem Development, (Prentice-Hall Inc., NewJersey 07458, 2001), 559 - 561.98discount  ratio  dr= r?r  i.e.,  the  countsubtracted  from  the  non-zero  count  areredistributed  among  the  zero  count  bigramsaccording to the next lower-order distribution,the unigram model.4    Evaluations and ResultsSubjective evaluation has been performed bycomparing  the  experimental  results  with  themanually segmented tokens.
The method wasevaluated  using  different  sets  of  testdocuments from various domains consisting of714  manually  segmented  words.
Table  3summarizes the evaluation results.Document text Correct Detect(Correctly segmentedtokens / total no.
ofwords)AccuracyAstrology.txt 102/116 87.9%dzo_linux.txt 85/93 91.4%movie_awards.txt 76/84 90.5%News.txt 78/85 91.8%Notice.txt 83/92 90.2%Religious.txt 63/73 89.0%Song.txt 57/60 95.0%Tradition.txt 109/111 98.2%Total 653/714 91.5%Table 3: Evaluation ResultsAccuracy in %age are measured as:Accuracy(%) = NT?100where?
N is  the  number  of  correctlysegmented tokens?
T is  the  total  number  of  manuallysegmented tokens/ Total number ofwords.We have taken the extract of different test datahoping it may contain fair amount of generalterms, technical terms and common nouns.
Themanually segmented corpus containing 41,739tokens are used for the method.In the sample comparison below, the symbol(   ? )
does  not  make  the  segmentation  unit'smark,  but  (   ? )
takes  the  segmentation  unit'smark,  despite  its  actual  mark  for  comma  orfull_stop.
The  whitespace in the sentence arephrase boundary or comma,  and is  a faithfulrepresentation of speech where we pause notbetween words, but either after certain phrasesor at the end of sentence.Consider the sample input sentence:??????????????????
?????????????????????????????????
?????????????????????????????
???????????????
????????????????????????
??????????????????????????????????????????????
???????
?????????????????????????????????????????????????????????
?Manually  segmented  sentence  of  the  sampleinput sentence:??????????????????
??????????????????????????????????????????????????????????????
???????????????
????????????????????????
??????????????????????????????????????????????
???????
?????????????????????????????????????????????????????????
?Using maximal matching algorithm:??????
???
?????
????
??????
????????
???
??????????
????
??
???????
????
??
?????
???
????????
???????
???
?????
?????????
????????
??????????????
???
????????
????
????
????
????
????????????
???
????
?????
???
???????
????
????????
???????
????
????
???
??
??
?????
???
?System segmented version of the sample inputsentence: Underlined text shows the incorrectsegmentation.??????????????????
??????????????????????????????????????????????????????????????
???????????????????????????????????????
??????????????????????????????99????????????????
?????????????????????????????????????????????????????????????
???
?5    DiscussionsDuring the process of word segmentation, it isunderstood  that  the  maximal  matchingalgorithm is simply effective and can produceaccurate segmentation only if all the words arepresent  in  the  lexicon.
But  since  not  all  theword entry can be found in lexicon database inreal  application,  the  performance  of  wordsegmentation  degrades  when  it  encounterswords that are not in the lexicon (Chiang et al,1992).Following are the significant problems withthe  dictionary-based  maximal  matchingmethod  because  of  the  coverage  of  lexicon(Emerson, 2000):?
incomplete and inconsistency of thelexicon database?
absence of technical domains in thelexicon?
transliterated foreign names?
some  of  the  common  nouns  notincluded in the lexicon?
lexicon/word  lists  do  not  containsgenitive  endings  ???
(expresses  thegenitive relationship as a quality orcharacteristic of the second element,for  example,  ?????????
'son  of  apauper') and  ??
(first  singularpossessive,  for  example,  ???????
?which  actually  is ?????????
'mydaughter') that  indicates  possessionor a part-to-whole relationship, likeEnglish 'of'.A Dzongkha sentence like:?????????????
???????????????
???
?may include the following ambiguous possiblesegmentation based on simple dictionarylookup:1.??????????????????????????????
?this | Dzongkha | of | research | writtendocument | is2.??????????????????????????????
?this | Dzongkha | of | arrange together | search/expose | written document | is3.??????????????????????????????
?this | fortress | mouth/surface | of | research |written document | isThese  problems  of  ambiguous  worddivisions, unknown proper names, are lessenedand solved partially when it is re-ranked usingthe bigram techniques.
Still the solution to thefollowing issues needs to be discussed in thefuture.
Although the texts were collected fromwidest range of domains possible, the lack ofavailable  electronic  resources  of  informativetext adds to the following issues:?
small  number  of  corpus  were  notvery impressive for the method?
ambiguity  and  inconsistent  ofmanual  segmentation of a token inthe  corpus  resulting  inincompatibility  and  sometimes  inconflict.Ambiguity  and  inconsistency  occursbecause of  difficulties  in  identifying  a  word.Since the manual segmentation of corpus entrywas  carried  out  by  humans  rather  thancomputer, such humans have to be well skilledin identifying or understanding what a word is.The problem with the Dzongkha scripts thatalso hampers the accuracy of dzongkha wordsegmentation  includes  the  issues  such  asambiguous  use  of  Tsheg  (   ? )
in  differentdocuments.
There  are  two  different  types  ofTsheg: Unicode 0F0B (  ? )
called Tibetan markinter  syllabic  tsheg is  a  normal  tsheg thatprovides  a  break  opportunity.
Unicode  0F0C(  ? )
called Tibetan Mark Delimiter Tsheg Bstaris  a  non-breaking  tsheg and  it  inhibits  linebreaking.For example,input sentence with Tsheg 0F0B:??????????????????????
?????????????????????
????????????????????
?achieves 100% segmentation as follow:100???????
???
???????
?????
???????
???
????
????
?????
????
???
?????
????
??
?whereas  the  same input  sentence with Tsheg0F0C is incorrectly segmented as follows:???????????????????????
??????????????????????????????????????????
?There are also cases like shortening of words,removing  of  inflectional  words  andabbreviating of words for the convenience ofthe writer.
But  this  is  not  so reflected in thedictionaries, thus affecting the accuracy of thesegmentation.Following words has a special abbreviated wayof writing a letter or sequence of letters at theend of a syllable as?????
as ?????????
as ??
?etc..6    Conclusion and Future worksThis  paper  describes  the  initial  effort  insegmenting  the  Dzongkha  scripts.
In  thispreliminary  analysis  of  Dzongkha  wordsegmentation,  the  preprocessing  andnormalizations are not dealt with.
Numberings,special  symbols  and  characters  are  also  notincluded.
These issues will have to be studiedin the future.
A lot of discussions and worksalso  have  to  be  done  to  improve  theperformance of word segmentation.
Althoughthe study was a success,  there are still  someobvious limitations, such as its dependency ondictionaries/lexicon, and the current Dzongkhalexicon  is  not  comprehensive.
Also,  there  isabsence  of  large  corpus  collection  fromvarious  domains.
Future  work  may  includeoverall improvement of the method for betterefficiency,  effectiveness and functionality,  byexploring  different  algorithms.
Furthermore,the inclusion of POS Tag sets  applied on n-gram techniques which is proven to be helpfulin handling the unknown word problems mightenhance  the  performance  and  accuracy.Increasing  corpus  size  might  also  help  toimprove the results.AcknowledgmentThis research work was carried out as a part ofPAN  Localization  Project(http://www.PANL10n.net)  with  the  aid  of  agrant  from  the  International  DevelopmentResearch  Centre  (IDRC),  Ottawa,  Canada,administered through the Center of Research inUrdu Language Processing (CRULP), NationalUniversity  of  Computer  and  EmergingSciences  (NUCES),  Pakistan.
The  researchteam would also like to express the gratitude toall the PAN Localization Project members ofBhutanese  team  based  at  Department  ofInformation  Technology  and  Telecom,  fortheir  efforts  in  collecting,  preparing  andproviding  with  the  lexicon,  corpus,  usefultraining  and  testing  materials  and  finally  forthe their valuable support and contribution thatmade this research successful.ReferencesChen,  Stanley  F.,  Joshua  Goodman,  1998.
AnEmpirical  Study  of  Smoothing  Techniques  forLanguage Modeling,  Computer  Science Group,Harvard University, Cambridge, MassachusettsChiang,  T-Hui., J-Shin Chang,,  M-Yu Lin,  K-YihSu,  2007.
Statistical  models  for  wordsegmentation  and  unknown  word  resolution.Department of Electrical Engineering , NationalTsing Hua University, Hsinchu, Taiwan.Chungku.,  Jurmey  Rabgay,  Gertrud  Faa?,  2010.NLP  Resources  for  Dzongkha.
Department  ofInformation Technology & Telecom, Ministry ofInformation  &  Communications,  Thimphu,Bhutan.Durrani,  Nadir  and  Sarmad Hussain,  2010.
UrduWord  Segmentation.
Human  LanguageTechnologies:  11th  Annual  Conference  of  theNorth American Chapter of the Association forComputational  Linguistics,  Los  Angeles,  June2010.Emerson,  Thomas.
2000.
Segmenting  Chinese  inUnicode.
16th International Unicode conference,Amsterdam, The Netherlands, March 2000Haizhou,  Li  and  Yuan  Baosheng,  1998.
ChineseWord Segmentation.
Language, Information andComputation (PACLIC12), 1998.Haruechaiyasak,  C.,  S  Kongyoung,  M.N.
Dailey,2008.
A  Comparative  Study  on  Thai  Word101Segmentation  Approaches.
In  Proceedings  ofECTI-CON, 2008.Huang,  X.,  A.  Acero,  H.-W.  Hon,  2001.
SpokenLanguage  Processing:  A  Guide  to  Theory,Algorithm and System Development (pp.
539 ?578).
Prentice-Hall Inc., New Jersey 07458.Huor,  C.S.,  T.  Rithy,   R.P.
Hemy,  V.  Navy,  C.Chanthirith,  C.  Tola,  2007.
Word  Bigram  VsOrthographic Syllable Bigram in Khmer WordSegmentation.
PAN  Localization  WorkingPapers 2004 - 2007.
PAN Localization Project,National University of Computer and EmergingSciences, Lahore, Pakistan.Jurafsky, D., A. Acero, H.-W. Hon, 1999.
Speechand  Language  Processing:  An  Introduction  toNatural  Language  Processing,  ComputationalLinguistics and Speech Recognition (pp.
189 ?230).
Prentice-Hall Inc., New Jersey 07458.Nugues,  P.M. 2006.
An Introduction to LanguageProcessing with Perl and Prolog: An Outline ofTheories, Implementation, and Application withSpecial  Consideration of  English,  French,  andGerman  (Cognitive  Technologies)  (pp.
87  ?104).
Springer-Verlag Berlin HeidelbergPong,  L.W.
and  Robert.
1994.
Chinese  wordsegmentation  based  on  maximal  matching andbigram  techniques.
Retrieved  from  TheAssociation  for  Computational  Linguistics  andChinese  Language  Processing.
On-line:http://www.aclclp.org.tw/rocling/1994/P04.pdfSunthi,  Thepchai.
2007.
Word  Segmentation  andPOS  tagging.
ADD-2  Workshop,  SIIT,NECTEC, Thailand.Van Driem, George.
and Karma Tshering, (Collab),?Languages  of  Greater  Himalayan  Region?,1998.102
