Proceedings of NAACL-HLT 2015, pages 26?30,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsAn AMR parser for English, French, German, Spanish and Japaneseand a new AMR-annotated corpusLucy Vanderwende, Arul Menezes, Chris QuirkMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{lucyv,arulm,chrisq}@microsoft.comAbstractIn this demonstration, we will present ouronline parser1 that allows users to submit anysentence and obtain an analysis following thespecification of AMR (Banarescu et al, 2014)to a large extent.
This AMR analysis is gener-ated by a small set of rules that convert a na-tive Logical Form analysis provided by a pre-existing parser (see Vanderwende, 2015) intothe AMR format.
While we demonstrate theperformance of our AMR parser on data setsannotated by the LDC, we will focus attentionin the demo on the following two areas: 1) wewill make available AMR annotations for thedata sets that were used to develop our parser,to serve as a supplement to the LDC data sets,and 2) we will demonstrate AMR parsers forGerman, French, Spanish and Japanese thatmake use of the same small set of LF-to-AMRconversion rules.1 IntroductionAbstract Meaning Representation (AMR) (Bana-rescu et al, 2014) is a semantic representation forwhich a large amount of manually-annotated datais being created, with the intent of constructing andevaluating parsers that generate this level of se-mantic representation for previously unseen text.1 Available at: http://research.microsoft.com/msrsplatAlready one method for training an AMR parserhas appeared in (Flanigan et al, 2014), and we an-ticipate that more attempts to train parsers will fol-low.
In this demonstration, we will present ourAMR parser, which converts our existing semanticrepresentation formalism, Logical Form (LF), intothe AMR format.
We do this with two goals: first,as our existing LF is close in design to AMR, wecan now use the manually-annotated AMR datasetsto measure the accuracy of our LF system, whichmay serve to provide a benchmark for parserstrained on the AMR corpus.
We gratefullyacknowledge the contributions made by Banarescuet al (2014) towards defining a clear and interpret-able semantic representation that enables this typeof system comparison.
Second, we wish to con-tribute new AMR data sets comprised of the AMRannotations by our AMR parser of the sentenceswe previously used to develop our LF system.These sentences were curated to cover a wide-range of syntactic-semantic phenomena, includingthose described in the AMR specification.
We willalso demonstrate the capabilities of our parser togenerate AMR analyses for sentences in French,German, Spanish and Japanese, for which no man-ually-annotated AMR data is available at present.2 Abstract Meaning RepresentationAbstract Meaning Representation (AMR) is a se-mantic representation language which aims to as-sign the same representation to sentences that have26the same basic meaning (Banarescu et al, 2014).Some of the basic principles are to use a graph rep-resentation, to abstract away from syntactic idio-syncrasies (such as active/passive alternation), tointroduce variables corresponding to entities, prop-erties and events, and to ground nodes to OntoNo-tes (Pradhan et al, 2007) wherever possible.As a semantic representation, AMR describes theanalysis of an input sentence at both the conceptualand the predicative level, as AMR does not anno-tate individual words in a sentence (see annotationguidelines, introduction).
AMR, for example, pro-vides a single representation for the constructionsthat are typically thought of as alternations: ?it istough to please the teacher?
and ?the teacher istough to please?
have the same representation inAMR, as do actives and their passive variant, e.g.,?a girl read the book?
and ?the book was read by agirl?.
AMR also advocates the representation ofnominative constructions in verbal form, so that ?Iread about the destruction of Rome by the Van-dals?
and ?I read how the Vandals destroyedRome?
have the same representation in AMR, withthe nominal ?destruction?
recognized as having thesame basic meaning as the verbal ?destroy?.
Suchdecisions are part-conceptual and part-predicative,and rely on the OntoNotes lexicon having entriesfor the nominalized forms.
AMR annotators alsocan reach in to OntoNotes to represent ?the soldierwas afraid of battle?
and ?the soldier feared bat-tle?
: linking ?be afraid of?
to ?fear?
depends on theOntoNotes frameset at annotation time.3 Logical FormThe Logical Form (LF) which we convert to AMRvia a small set of rules is one component in abroad-coverage grammar pipeline (seeVanderwende, 2015, for an overview).
The goal ofthe LF is twofold: to compute the predicate-argument structure for each clause (?who did whatto whom when where and how??)
and to normalizediffering syntactic realizations of what can be con-sidered the same ?meaning?.
In so doing, conceptsthat are possibly distant in the linear order of thesentence or distant in the constituent structure canbe brought together, because the Logical Form isrepresented as a graph, where linear order is nolonger primary.
In addition to alternations and pas-sive/active, other operations include:  unboundeddependencies, functional control, indirect objectparaphrase, and assigning modifiers.As in AMR, the Logical Form is a directed, la-beled graph.
The nodes in this graph have labelsthat are either morphologically or derivationallyrelated to the input tokens, and the arcs are labeledwith those relations that are defined to be semantic.Surface words that convey syntactic informationonly (e.g.
by in a passive construction, do-support,singular/passive, or (in)definite articles) are notpart of the graph, their meaning, however is pre-served as annotations on the conceptual nodes(similar to the Prague T-layer, Haji?
et al, 2003).Figure 1.
The LF representation of "African elephants,which have been hunted for decades, have large tusks.
"In Figure 1, we demonstrate that native LF uses re-entrancy in graph notation, as does AMR, whenev-er an entity plays multiple roles in the graph.
Notehow the node elephant1 is both the Dsub of have1and the Dobj of hunt1.
The numerical identifierson the leaf nodes are a unique label name, not asense identification.We also point out that LF attempts to interpretthe syntactic relation as a general semantic relationto the degree possible, but when it lacks infor-mation for disambiguation, LF preserves the ambi-guity.
Thus, in Figure 1, the identified semanticrelations are: Dsub (?deep subject?
), Attrib (at-tributive), Dobj (?deep object?
), but also the under-specified relation ?for?.The canonical LF graph display proceeds fromthe root node and follows a depth first explorationof the nodes.
When queried, however, the graphcan be viewed with integrity from the perspectiveof any node, by making use of relation inversions.Thus, a query for the node elephant1 in Figure 1returns elephant1 as the DsubOf have1 and alsothe DobjOf hunt1.274 LF to AMR conversionThe description of LF in section 3 emphasized theclose similarity of LF and AMR.
Thus, conversionrules can be written to turn LF into AMR-similaroutput, thus creating an AMR parser.
To convertthe majority of the relations, only simple renamingis required; for example LF Dsub is typically AMRARG0, LF Locn is AMR location, and so on.We use simple representational transforms toconvert named entities, dates, times, numbers andpercentages, since the exact representation of thesein AMR are slightly different from LF.Some of the more interesting transforms to en-courage similarity between LF and AMR are map-ping modal verbs can, may and must to possibleand obligate in AMR and adjusting how the copulais handled.
In both AMR and LF the arguments ofthe copula are moved down to the object of thecopula, but in LF the vestigial copula remains,whereas in AMR it is removed.5 EvaluationUsing smatch (Cai and Knight, 2013), we comparethe performance of our LF system to the JAMRsystem of Flanigan et al (2014).
Both systems relyon the Illinois Named Entity Tagger (Ratinov andRoth, 2009).
LF strives to be a broad coverage par-ser without bias toward a particular domain.
There-fore, we wanted to evaluate across a number ofcorpora.
When trained on all available data, JAMRshould be less domain dependent.
However, thenewswire data is both larger and important, so wealso report numbers for JAMR trained on proxydata alone.To explore the degree of domain dependence ofthese systems, we evaluate on several genres pro-vided by the LDC: DFA (discussion forums datafrom English), Bolt (translated discussion forumdata), and Proxy (newswire data).
We did not ex-periment on the consensus, mt09sdl, or Xinhuasubsets because the data was pre-tokenized.
Thistokenization must be undone before our parser isapplied.We evaluate in two conditions: ?without wordsense annotations?
indicates that the specific sensenumbers were discarded in both the gold standardand the system output; ?with word sense annota-tions?
leaves the sense annotations intact.The AMR specification requires that concepts,wherever possible, be annotated with a sense IDreferencing the OntoNotes sense inventory.
Recallthat the LF system intentionally does not have aword sense disambiguation component due to theinherent difficulty of defining and agreeing upontask-independent sense inventories (Palmer et al2004, i.a.).
In order to evaluate in the standardevaluation setup, we therefore construct a word-sense disambiguation component for LF lemmas.Our approach is quite simple: for each lemma, wefind the predominant sense in the training set(breaking ties in favor of the lowest sense ID), anduse that sense for all occurrences of the lemma intest data.
For those lemmas that occur in the testbut not in the training data, we attempt to find averb frame in OntoNotes.
If found, we use thelowest verb sense ID not marked with DO NOTTAG; otherwise, the lemma is left unannotated forsense.
Such a simple system should perform wellbecause 95% of sense-annotated tokens in theproxy training set use the predominant sense.
Anobvious extension would be sensitive to parts-of-speech.As shown in Table 1, the LF system outper-forms JAMR in broad-domain semantic parsing, asmeasured by macro-averaged F1 across domains.This is primarily due to its better performance ondiscussion forum data.
JAMR, when trained onnewswire data, is clearly the best system on news-wire data.
Adding training data from other sourcesleads to improvements on the discussion forumTest without word sense annotations  Test with word sense annotationsSystem  Proxy DFA Bolt Average  Proxy DFA Bolt AverageJAMR: proxy  64.4 40.4 44.2 49.7  63.3 38.1 42.6 48.0JAMR: all  60.9 44.5 47.5 51.0  60.1 43.2 46.0 49.8LF  59.0 50.7 52.6 54.1  55.2 46.9 49.2 50.4Table 1.
Evaluation results: balanced F-measure in percentage points.
JAMR (proxy) is the system ofFlanigan et al (2014) trained on only the proxy corpus; JAMR (all) is the system trained on all data inLDC2014T12; and LF is the system described in this paper.
We evaluate with and without sense annota-tions in three test corpora.28data, but at the cost of accuracy on newswire.
Thelack of sophisticated sense disambiguation in LFcauses a substantial degradation in performance onnewswire.6 Data Sets for LF developmentThe LF component was developed by authoringrules that access information from a rich lexiconconsisting of several online dictionaries as well asinformation output by a rich grammar formalism.Authoring these LF rules is supported by a suite oftools that allow iterative development of an anno-tated test suite (Suzuki, 2002).
We start by curatinga sentence corpus that exemplifies the syntacticand semantic phenomena that the LF is designed tocover; one might view this sentence corpus as theLF specification.
When, during development, thesystem outputs the desired representation, that LFis saved as ?gold annotation?.
In this way, the goldannotations are produced by the LF system itself,automatically, and thus with good system internalconsistency.
We note that this method of systemdevelopment is quite different from SemBankingAMR, but is similar to the method described inFlickinger et al (2014).As part of this demonstration, we share with par-ticipants the gold annotations for the curated sen-tence corpora used during LF development,currently 550 sentences that are vetted to producecorrect LF analyses.
Note that the example in Fig-ure 2 requires a parser to handle both the pas-sive/active alternation as well as control verbs.
Webelieve that there is value in curated targeted da-tasets to supplement annotating natural data; e.g.,AMR clearly includes control phenomena in itsspec (the first example is ?the boy wants to go?
)but in the data, there are only 3 instances of ?per-suade?
in the amr-release-1.0-training-proxy, e.g.,and no instances in the original AMR-bank.7 AMR parsers for French, German,Spanish and JapaneseThe demonstrated system includes not only a par-ser for English, but also parsers for French, Ger-man, Spanish and Japanese that produce analysesat the LF level.
Thus, using the same set of conver-sion rules, we demonstrate AMR annotations gen-erated by our parsers in these additional languages,for which there are currently no manually-annotated AMR SemBanks.
Such annotations maybe useful to the community as initial analyses thatcan be manually edited and corrected where theiroutput does not conform to AMR-specificationsalready.
Consider Figures 3-6 and the brief de-scription of the type of alternation they are intend-ed to demonstrate in each language.Input: el reptil se volte?, quit?ndoselo de encima.Gloss: the crocodile rolled over, throwing it off.
(v / voltear:ARG0 (r / reptil):manner (q / quitar:ARG0 r:ARG1 (x / "?l"):prep-de (e / encima)))Figure 3 AMR in Spanish with clitic construction.Input: Et j'ai vu un petit bonhomme tout ?
fait ex-traordinaire qui me consid?rait gravement.Gloss: And I saw a small chap totally extraordinarywho me looked seriously.
(e / et:op (v / voir:ARG0 (j / je):ARG1 (b / bonhomme:ARG0-of (c / "consid?rer":ARG1 j:mod (g / gravement)):mod (p / petit):mod (e2 / extraordinaire:degree (t / "tout_?_fait")))))Figure 4 AMR in French with re-entrant node ?j?# Pat was persuaded by Chris to eat the apple.
(p / persuade:ARG0 (p2 / person:name (c / name :op1 Chris)):ARG2 (e / eat:ARG0 (p4 / person:name (p3 / name :op1 Pat)):ARG1 (a / apple)):ARG1 p3)Figure 2.
LF-AMR for the input sentence ?Pat waspersuaded by Chris to eat the apple?, with both pas-sive and control constructions.29Input: Die dem wirtschaftlichen Aufschwung zuverdankende sinkende Arbeitslosenquote f?hre zuh?heren Steuereinnahmen.Gloss: The the economic upturn to thank-for sink-ing unemployment rate led to higher tax-revenue(f / "f?hren":ARG0 (a / Arbeitslosenquote:ARG0-of (s / sinken):ARG0-of (v / verdanken:ARG2 (a2 / Aufschwung:mod (w / wirtschaftlich)):degree (z / zu))):prep-zu (s2 / Steuereinnahme:mod (h / hoch)))Figure 5 AMR in German for complex participialconstructionInput: ??
?
?
??
?
??
?
????
?Gloss: eastern_lands various feudal_lordsserve_monarchy swear-CAUS-PASTFigure 6.
AMR in Japanese illustrating a causativeconstruction8 ConclusionIn the sections above, we have attempted tohighlight those aspects of the system that will bedemonstrated.
To summarize, we show a systemthat:?
Produces AMR output that can be comparedto the manually-annotated LDC resources.
Avail-able at: http://research.microsoft.com/msrsplat,?
Produces AMR output for a new data setcomprised of the sentences selected for the devel-opment of our LF component.
This curated data setwas selected to represent a wide range of phenom-ena and representational challenges.
These sen-tences and their AMR annotations are available at:http://research.microsoft.com/nlpwin-amr?
Produces AMR annotations for French, Ger-man, Spanish and Japanese input, which may beused to speed-up manual annotation/correction inthese languages.AcknowledgementsWe are grateful to all our colleagues who worked onNLPwin.
For this paper, we especially recognize KarenJensen, Carmen Lozano, Jessie Pinkham, MichaelGamon and Hisami Suzuki for their work on LogicalForm.
We also acknowledge Jeffrey Flanigan and hisco-authors for their contributions of making the JAMRmodels and code available.ReferencesLaura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2014.
Abstract Meaning Representation(AMR) 1.2.1 Specification.
Available athttps://github.com/amrisi/amr-guidelines/blob/master/amr.mdShu Cai and Kevin Knight.
2013.
Smatch: an evaluationmetric for semantic feature structures.
In Proceedingsof ACL.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, ChrisDyer, and Noah Smith.
2014.
A DiscriminativeGraph-Based Parser for the Abstract Meaning Repre-sentation.
In Proceedings of ACL 2014.Dan Flickinger, Emily M. Bender and Stephan Oepen.2014.
Towards an Encyclopedia of CompositionalSemantics: Documenting the Interface of the EnglishResource Grammar.
In Proceedings of LREC.Jan Haji?, Alena B?hmov?, Haji?ov?, Eva, and Hladk?,Barbara.
(2003).
The Prague Dependency Treebank:A Three Level Annotation Scenario.
In Abeill?,Anne, editor, Treebanks: Building and Using Anno-tated Corpora.
Kluwer Academic Publishers.Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang.2004.
Different Sense Granularities for Different Ap-plications.
In Proceedings of Workshop on ScalableNatural Language Understanding.Sameer.
S. Pradhan, Eduard Hovy, Mitch Marcus, Mar-tha Palmer, Lance Ramshaw, and Ralph Weischedel.2007.
OntoNotes: A Unified Relational SemanticRepresentation.
In Proceedings of the InternationalConference on Semantic Computing (ICSC ?07).Hisami Suzuki.
2002.
A development environment forlarge-scale multi-lingual parsing systems.
In Pro-ceedings of the 2002 workshop on Grammar engi-neering and evaluation - Volume 15, Pages 1-7.Lucy Vanderwende.
2015.
NLPwin ?
an introduction.Microsoft Research tech report no.
MSR-TR-2015-23, March 2015.30
