Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1524?1533,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSmart SelectionPatrick PantelMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAppantel@microsoft.comMichael GamonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAmgamon@microsoft.comAriel FuxmanMicrosoft Research1065 La Avenida St.Mountain View, CA 94043, USAarielf@microsoft.comAbstractNatural touch interfaces, common now indevices such as tablets and smartphones,make it cumbersome for users to selecttext.
There is a need for a new text selec-tion paradigm that goes beyond the highacuity selection-by-mouse that we have re-lied on for decades.
In this paper, we in-troduce such a paradigm, called Smart Se-lection, which aims to recover a user?s in-tended text selection from her touch input.We model the problem using an ensemblelearning approach, which leverages mul-tiple linguistic analysis techniques com-bined with information from a knowledgebase and a Web graph.
We collect a datasetof true intended user selections and simu-lated user touches via a large-scale crowd-sourcing task, which we release to theacademic community.
We show that ourmodel effectively addresses the smart se-lection task and significantly outperformsvarious baselines and standalone linguisticanalysis techniques.1 IntroductionThe process of using a pointing device to selecta span of text has a long history dating back tothe invention of the mouse.
It serves to accessfunctions on text spans, such as copying/pasting,looking up a word in a dictionary, searching theWeb, or accessing other accelerators.
As con-sumers move from traditional PCs to mobile de-vices (e.g., tablets and smartphones), touch inter-action is replacing the pointing devices of yore.Although more intuitive and arguably a more natu-ral form of interaction, touch offers much less acu-ity (colloquially referred to as the fat finger prob-lem).
To select multi-word spans today, mobiledevices require an intricate series of gestures thatresults in cumbersome user experiences1.
Conse-quently, there is an opportunity to reinvent the wayusers select text in such devices.Our task is, given a single user touch, to pre-dict the span that the user likely intended to se-lect.
We call this task smart selection.
We re-strict our prediction task to cases where a user in-tends to perform research on a text span (dictio-nary/thesaurus lookup, translation, searching).
Wespecifically consider operations on text spans thatdo not form a single unit (i.e., an entity, a concept,a topic, etc.)
to be out of scope.
For example, fullsentences, paragraph and page fragments are outof scope.Smart selection, as far as we know, is a new re-search problem.
Yet there are many threads of re-search in the NLP community that identify multi-word sequences, which have coherent properties.For example, named-entity recognizers identifyentities such as people/places/organizations, chun-kers and parsers identify syntactic constituentssuch as noun phrases, key phrase detectors or termsegmentors identify term boundaries.
While eachof these techniques retrieve meaningful linguisticunits, our problem is a semantic one of recoveringa user?s intent, and as such none alone solves theentire smart selection problem.In this paper, we model the problem of smartselection using an ensemble learning approach.We leverage various linguistic techniques, such asthose discussed above, and augment them withother sources of information from a knowledge1In order to select a multi-word span, a user would firsthave to touch on either word, then drag the left and rightboundary handles to expand it to the adjacent words.1524base and a web graph.
We evaluate our meth-ods using a novel dataset constructed for ourtask.
We construct our dataset of true user-intended selections by crowdsourcing the task ofa user selecting spans of text in a researchingtask.
We obtain 13,681 data points.
For each in-tended selection, we construct test cases for eachindividual sub-word, simulating the user select-ing via touch.
The resulting testset consists of33,912 ?simulated selection, intended selection?-pairs, which we further stratify into head, torso,and tail subsets.
We release the full dataset andtestset to the academic community for further re-search on this new NLP task.
Finally, we empir-ically show that our ensemble model significantlyimproves upon various baseline systems.In summary, the major contributions of our re-search are:?
We introduce a new natural language pro-cessing task, called smart selection, whichaims to address an important problem in textselection for touch-enabled devices;?
We conduct a large crowd-sourced user studyto collect a dataset of intended selections andsimulated user selections, which we releaseto the academic community;?
We propose a machine-learned ensemblemodel for smart selection, which combinesvarious linguistic annotation methods withinformation from a large knowledge base andweb graph;?
We empirically show that our model can ef-fectively address the smart selection task.2 Related WorkRelated work falls into three broad categories: lin-guistic unit detection, human computer interaction(HCI), and intent detection.2.1 Linguistic Unit DetectionSmart selection is closely related to the detectionof syntactic and semantic units: user selections areoften entities, noun phrases, or concepts.
A firstapproach to solving smart selection is to select anentity, noun phrase, or concept that subsumes theuser selection.
However, no single approach alonecan cover the entire smart selection problem.
Forexample, consider an approach that uses a state-of-the-art named-entity recognizer (NER) (Chinchor,1998; Tjong Kim Sang and De Meulder, 2003;Finkel et al, 2005; Ratinov and Roth, 2009).
Wefound in our dataset (see Section 3.2) that onlya quarter of what users intend to select consistsin fact of named entities.
Although an NER ap-proach can be very useful, it is certainly not suf-ficient.
The remainder of the data can be partiallyaddressed with noun phrase (NP) detectors (Ab-ney, 1991; Ramshaw and Marcus, 1995; Mu?noz etal., 1999; Kudo and Matsumoto, 2001) and lists ofitems in a knowledge base (KB), but again, each isnot alone sufficient.
NP detectors and KB-basedmethods are further very susceptible to the gen-eration of false positives (i.e., text contains manynested noun phrases and knowledge base items in-clude highly ambiguous terms).In our work, we leverage all three techniques inorder to benefit from their complementary cover-age of user selections.
We further create a novelunit detector, called the hyperlink intent model.Based on the assumption that Wikipedia anchortexts are similar in nature to what users would se-lect in a researching task, it models the problemof recovering Wikipedia anchor texts from partialselections.2.2 Human Computer InteractionThere is a substantial amount of research in theHCI community on how to facilitate interactionof a user with touch and speech enabled devices.To give but a few examples of trends in this field,Gunawardana et al (2010) address the fat fingerproblem in the use of soft keyboards on mobile de-vices, Kumar et al (2012) explore a novel speechinteraction paradigm for text entry, and Sakamotoet al (2013) introduce a technique that combinestouch and voice input on a mobile device for im-proved navigation of user interface elements suchas commands and controls.
To the best of ourknowledge, however, the problem of smart selec-tion as we defined it has not been addressed.2.3 Intent detectionThere is a long line of research in the web lit-erature on understanding user intent.
The clos-est to smart selection is query recommendation(Baeza-Yates et al, 2005; Zhang and Nasraoui,2006; Boldi et al, 2008), where the goal is to sug-gest queries that may be related to a user?s intent.Query recommendation techniques are based ei-ther on clustering queries by their co-clicked URLpatterns (Baeza-Yates et al, 2005) or on leverag-ing co-occurrences of sequential queries in web1525search sessions (Zhang and Nasraoui, 2006; Boldiet al, 2008; Sadikov et al, 2010).
The key dif-ference from smart selection is that in our task theoutput is a selection that is relevant to the contextof the document where the original selection ap-pears (e.g., by adding terms neighboring the selec-tion).
In query recommendation, however, there isno notion of a document being read by the userand, instead, the recommendations are based ex-clusively on the aggregation of behavior of multi-ple users.3 Problem Setting and Data3.1 Smart Selection DefinitionLet D be the set of all documents.
We define aselection to be a character ?offset, length?-tuple ina document d ?
D. Let S be the set of all possibleselections in D and let Sdbe the set of all possibleselections in d.We define a scored smart selection, ?, in a doc-ument d, as a pair ?
= ?x, y?
where x ?
Sdis aselection and y ?
R+is a score for the selection.We formally define the smart selection function?
as producing a ranked scored list of all possi-ble selections from a document and user selectionpair2:?
: D?
S?
(?1, ..., ?|Sd|| xi?
Sd, yi?
yi+1)(1)Consider a user who selects s in a document d.Let ?
be the target selection that best captures whatthe user intended to select.
We define the smartselection task as recovering ?
given the pair ?d, s?.Our problem then is to learn a function ?
that bestrecovers the target selection from any user selec-tion.Note that even for a human, reconstructing anintended selection from a single word selection isnot trivial.
While there are some fairly clear cutcases such as expanding the selection ?Obama?to Barack Obama in the sentence ?While inDC, Barack Obama met with...?, there are caseswhere the user intention depends on extrinsic fac-tors such as the user?s interests.
For example, ina phrase ?University of California at Santa Cruz?with a selection ?California?, some (albeit proba-bly few) users may indeed be interested in the stateof California, others in the University2The output consists of a ranked list of selections insteadof a single selection to allow experiences such as proposingan n-best list to the user.of California system of universities, andyet others specifically in the University ofCalifornia at Santa Cruz.
In the nextsection, we describe how we obtained a dataset oftrue intended user selections.3.2 DataIn order to obtain a representative dataset for thesmart selection task, we focus on a real-world ap-plication of users interacting with a touch-enablede-reader device.
In this application, a user is read-ing a book and chooses phrases for which shewould like to get information from resources suchas a dictionary, Wikipedia, or web search.
Yet, be-cause of the touch interface, she may only touchon a single word.3.2.1 Crowdsourced Intended SelectionsWe obtain the intended selections through the fol-lowing crowdsourcing exercise.
We use the en-tire collection of textbooks in English from Wik-ibooks3, a repository of publicly available text-books.
The corpus consists of 2,696 textbooks thatspan a large variety of categories such as Comput-ing, Humanities, Science, etc.
We first producea uniform random sample of 100 books, and thensample one paragraph from each book.
The result-ing set of 100 paragraphs is then sent to the crowd-sourcing system.
Each paragraph is evaluated by100 judges, using a pool of 152 judges.
For eachparagraph, we request the judges to select com-plete phrases for which they would like to ?learnmore in resources such as Wikipedia, search en-gines and dictionaries?, i.e., our true user intendedselections.
As a result of this exercise, we obtain13,681 judgments, corresponding to 4,067 uniqueintended selections.
The distribution of number ofunique judges who selected each unique intendedselection, in a log-log scale, is shown in Figure1.
Notice that this is a Zipfian distribution since itfollows a linear trend in the log-log scale.Intuitively, the likelihood that a phrase is ofinterest to a user correlates with the number ofjudges who select that phrase.
We thus use thenumber of judges who selected each phrase as aproxy for the likelihood that the phrase will bechosen by users.The resulting dataset consists of 4,067 ?d, ?
?-pairs where d is a Wikibook document paragraphand ?
is an intended selection, along with the num-ber of judges who selected it.
We further assigned3Available at http://wikibooks.org.15260246810120 1 2 3 4 5 6LOG 2(Unique intendedselections)LOG2(Unique judges that selected the intended selection)Figure 1: Zipfian distribution of unique intendedselections vs. the number of judges who selectedthem, in log-log scale.each pair to one of five randomly chosen folds,which are used for cross-validation experiments.3.2.2 Testset ConstructionWe define a test case as a triple ?d, s, ??
wheres is a simulated user selection.
For each ?d, ?
?-pair in our dataset we construct n correspond-ing test cases by simulating the user selections{?d, ?, s1?, .
.
.
, ?d, ?, sn?
}where s1, .
.
.
, sncorre-spond to the individual words in ?
.
In other words,each word in ?
is considered as a candidate userselection.We discard all target selections that only a sin-gle judge annotated since we observed that thesemostly contained errors and noise, such as full sen-tences or nonsensical long sentence fragments.Our first testset, labeled TALL, is the resultingtraffic-weighted multiset.
That is, each test case?d, s, ??
appears k times, where k is the numberof judges who selected ?
in d. TALLconsists of33,913 test cases.We further utilize the distribution of judgmentsin the creation of three other testsets.
Followingthe stratified sampling methodology commonlyemployed in the IR community, we constructtestsets for the frequently, less frequently, andrarely annotated intended selections, which wecall HEAD, TORSO, and TAIL, respectively.
Weobtain these testsets by first sorting each uniqueselection according to their frequency of occur-rence, and then partitioning the set so that HEADcorresponds to the elements at the top of the listthat account for 20% of the judgments; TAIL cor-responds to the elements at the bottom also ac-counting for 20% of the judgments; and TORSOcorresponds to the remaining elements.
The re-sulting test sets, THEAD, TTORSO, TTAILconsist of114, 2115, and 5798 test cases, respectively4.Test sets along with fold assignmentsand annotation guidelines are avail-able at http://research.microsoft.com/en-us/downloads/eb42522c-068e-404c-b63f-cf632bd27344/.3.3 DiscussionOur focus on single word selections is motivatedby the touchscreen scenario presented in Sec-tion 1.
Although our touch simulation assumesthat each word in a target selection is equally likelyto be selected by a user, in fact we expect this dis-tribution to be non-uniform.
For example, usersmay tend to select the first or last word more fre-quently than words in the middle of the target se-lection.
Or perhaps users tend to select nouns andverbs more frequently than function words.
Weconsider this out of scope for our paper, but view itas an important avenue of future investigation.
Fi-nally, for non-touchscreen environments, such asthe desktop case, it would also be interesting tostudy the problem on multi-word user selections.To get an idea of the kind of intended selectionsthat comprise our dataset, we broke them down ac-cording to whether they referred to named entitiesor not.
Perhaps surprisingly, the fraction of namedentities in the dataset is quite low, 24.3%5.
Therest of the intended selections mostly correspondto concepts and topics such as embouchure forma-tion, vocal fold relaxation, NHS voucher values,time-domain graphs, etc.4 ModelAs argued in Section 1, existing techniques,such as NER taggers, chunkers, Knowledge Baselookup, etc., are geared towards aspects of thetask (i.e., NEs, concepts, KB entries), but not thetask as a whole.
We can, however, combine theoutputs of these systems with a learned ?meta-model?.
The meta-model ranks the combined can-didates according to a criterion that is derived fromdata that resembles real usage of smart selectionas closely as possible.
This technique is known4We stress that TALLis a multi-set, reflecting the over-all expected user traffic from our 100 judges per paragraph.THEAD, TTORSO, TTAIL, in contrast, are not multi-sets sincejudgment frequency is already accounted for in the stratifi-cation process, as commonly done in the IR community.5Becker et al (2012) report a similar finding, showing thatonly 26% of questions, which a user might ask after readinga Wikipedia article, are focused on named entities.1527in the machine learning community as ensemblelearning (Dietterich, 1997).Our ensemble approach, described in this sec-tion, serves as our main implementation of thesmart selection function ?
of Equation 1.
Each ofthe ensemble members are themselves a separateimplementation of ?
and will be used as a pointof comparison in our experiments.
Below, we de-scribe the ensemble members before turning to theensemble learner.4.1 Ensemble Members4.1.1 Hyperlink Intent ModelThe Hyperlink Intent Model (HIM), which lever-ages web graph information, is a machine-learnedsystem based on the intuition that anchor texts inWikipedia are good representations of what usersmight want to learn about.
We build upon the factthat Wikipedia editors write anchor texts for enti-ties, concepts, and things of potential interest forfollow-up to other content.
HIM learns to recoveranchor texts from their single word subselections.Specifically, HIM iteratively decides whether toexpand the current selection (initially a singleword) one word to the left or right via greedy bi-nary decisions, until a stopping condition is met.At each step, two binary classifiers are consulted.The first one scores the left expansion decisionand the second one scores the right expansion de-cision.
In addition, we use the same two classi-fiers to evaluate the expansion decision ?from theoutside in?, i.e., from the word next to the currentselection (left and right, respectively) to the clos-est word in the current selection.
If the probabil-ity for expansion of any model exceeds a prede-fined threshold, then the most probable expansionis chosen and we continue the iteration with thenewly expanded selection as input.
The algorithmis illustrated in Figure 2.We automatically create our training set for HIMby first taking a random sample of 8K Wikipediaanchor texts.
We treat each anchor text as an in-tended selection, and each word in the anchor textas a simulated user selection.
For each word to theleft (or the right) of the user selection that is partof the anchor text, we create a positive training ex-ample.
Similarly, for each word to the left (or theright) that is outside of the anchor text, we create anegative training example.
We include additionalnegative examples using random word selectionsfrom Wikipedia content.
For this purpose we sam-Left Context Right ContextCurrent selection Candidate RightSelectedWord 2Candidate Left SelectedWord 1Context 1Context 2Context 4 Context 3Figure 2: Hyperlink Intent Model (HIM) decodingflow for smart selection.ple random words that are not part of an anchortext.
Our final data consists of 2.6M data points,with a 1:20 ratio of positive to negative examples6.We use logistic regression as the classificationalgorithm for our binary classifiers.
The fea-tures used by each model are computed over threestrings: the current selection s (initially the single-word simulated user selection), the candidate ex-pansion word w, and one word over from theright or left of s. The features fall into five fea-ture families: (1) character-level features, includ-ing capitalization, all-cap formatting, characterlength, presence of opening/closing parentheses,presence and position of digits and non-alphabeticcharacters, and minimum and average characteruni/bi/trigram frequencies (based on frequency ta-bles computed offline from Wikipedia article con-tent); (2) stopword features, which indicate thepresence of a stop word (from a stop word list);(3) tf.idf scores precomputed from Wikipedia con-tent statistics; (4) knowledge base features, whichindicate whether a string matches an item or a sub-string of an item in the knowledge base describedin Section 4.1.2 below; and (5) lexical features,which capture the actual string of the current se-lection and the candidate expansion word.4.1.2 Unit SpottingOur second qualitative class of ensemble membersuse notions of unit that are either based on linguis-tic constituency or knowledge base presence.
Thegeneral process is that any unit that subsumes theuser selection is treated as a smart selection can-didate.
Scoring of candidates is by normalizedlength, under the assumption that in general themost specific (longest) unit is more likely to be theintended selection.6Note that this training set is generated automatically andis, by design, of a different nature than the manually labeleddata we use to train and test the ensemble model.1528Our first unit spotter, labeled NER is gearedtowards recognizing named entities.
We usea commercial and proprietary state-of-the-artNER system, trained using the perceptron algo-rithm (Collins, 2002) over more than a millionhand-annotated labels.Our second approach uses purely syntactic in-formation and treats noun phrases as units.
We la-bel this model as NP.
For this purpose we parse thesentence containing the user selection with a syn-tactic parser following (Ratnaparkhi, 1999).
Wethen treat every noun phrase that subsumes theuser selection as a candidate smart selection.Finally, our third unit spotter, labeled KB, isbased on the assumption that concepts and otherentries in a knowledge base are, by nature, thingsthat can be of interest to people.
For our knowl-edge base lookup, we use a proprietary graph con-sisting of knowledge from Wikipedia, Freebase,and paid feeds from various providers from do-mains such as entertainment, local, and finance.4.1.3 HeuristicsOur third family of ensemble members imple-ments simple heuristics, which tend to be high pre-cision especially in the HEAD of our data.The first heuristic, representing the currenttouch-enabled selection paradigm seen in many oftoday?s tablets and smartphones, is labeled CUR.
Itsimply assumes that the intended selection is al-ways the user-selected word.The second is a capitalization-based heuristic(CAP), which simply expands every selected capi-talized word selection to the longest uninterruptedsequence of capitalized words.4.2 Ensemble LearningIn this section, we describe how we train our meta-learner, labeled ENS, which takes as input the can-didate lists produced by the ensemble membersfrom Section 4.1, and scores each candidate, pro-ducing a final scored ranked list.We use logistic regression as a classification al-gorithm to address this task.
Our 22 features inENS consist of three main classes: (1) featuresrelated to the individual ensemble members; (2)features related to the user selection; and (3) fea-tures related to the candidate smart selection.
For(1), the features consist of whether a particularensemble member generated the candidate smartselection and its score for that candidate.
If thecandidate smart selection is not in the candidatelist of an ensemble member, its score is set tozero.
For both (2) and (3), features account forlength and capitalization properties of the user se-lection and the candidate smart selection (e.g., to-ken length, ratio of capitalized tokens, ratio of cap-italized characters, whether or not the first and lasttokens are capitalized.
)Although training data for the HIM model wasautomatically generated from Wikipedia, for ENSwe desire training data that reflects the true ex-pected user experience.
For this, we use five-fold cross-validation over our data collection de-scribed in Section 3.2.
That is, to decode a foldwith our meta-learner, we train ENS with the otherfour folds.
Note that every candidate selection fora ?document, user selection?-pair, ?d, s?, for thesame d and s, are assigned to a single fold, hencethe training process does not see any user selectionfrom the test set.5 Experimental Results5.1 Experimental SetupRecall our testsets TALL, THEAD, TTORSO, and TTAILfrom Section 3.2.2, where a test case is defined asa triple ?d, s, ?
?, and where d is a document, s is auser selection, and ?
is the intended user selection.In this section, we describe our evaluation metricand summarize the system configurations that weevaluate.5.1.1 MetricIn our evaluation, we apply the smart selectionfunction ?
(d, s) (see Eq.
1) to each test case andmeasure how well it recovers ?
.Let A be the set of ?d, ?
?-pairs from our datasetdescribed in Section 3.2.1 that corresponds to atestset T. Let T?d,?
?be the set of all test casesin T with a fixed d and ?
.
We define the macroprecision of a smart selection function, P?, as fol-lows:P?=1| A |??d,???AP?
(d, ?)
(2)P?
(d, ?)
=1| T?d,??|??d,s,???T?d,??P?
(d, s, ?)P?
(d, s, ?)
=1| ?
(d, s) |????
(d,s)I(?, ?
)I(?, ?)
={1 if ?
= ?x, y?
?
x = ?0 otherwise1529CP@1 CP@2 CP@3 CP@4 CP@5CUR 39.3 - - - -CAP 48.9 51.0 51.2 51.8 51.8NER 43.5 - - - -NP 34.1 50.2 55.5 57.1 57.6KB 50.2 50.8 50.9 50.9 50.9HIM 48.1 48.8 48.8 48.8 48.8ENS 56.8?76.0?82.6?85.2?86.6?Table 1: Smart selection performance, as a func-tion of CP, on TALL.
?and?indicate statisticalsignificance with p = 0.01 and 0.05, respectively.An oracle ensemble would achieve an upper boundCP of 87.3%.We report cumulative macro precision atrank (CP@k) in our experiments since ourtestsets contain a single true user-intendedselection for each test case7.
However,this is an overly conservative metric sincein many cases an alternative smart selectionmight equally please the user.
For example,if our testset contains a user intended selec-tion ?
= The University of SouthernCalifornia, then given the simulated selec-tion ?California?, both ?
and University ofSouthern California would most likelyequally satisfy the user intent (whereas the latterwould be considered incorrect in our evaluation).In fact, the ideal testset would further evaluate thedistance or relevance of the smart selection to theintended user selection.
We would then find per-haps that Southern California is a morereasonable smart selection than of SouthernCalifornia.
However, precisely defining sucha relevance function and designing the guidelinesfor a user study is non-trivial and left for futurework.5.1.2 SystemsIn our experiments, we evaluate the follow-ing systems, each described in detail in Sec-tion 4: Passthrough (CUR), Capitalization (CAP),Named-Entity Recognizer (NER), Noun Phrase(NP), Knowledge Base (KB), Hyperlink IntentModel (HIM), Ensemble (ENS).5.2 ResultsTable 1 reports the smart selection performance onthe full traffic weighted testset TALL, as a func-7Because there is only a single true intended selection foreach test case, Recall@k = CP@k.tion of CP@k. Our ensemble approach recoversthe true user-intended selection in 56.8% of thecases.
In its top-2 and top-3 ranked smart selec-tions, the true user-intended selection is retrieved76.0% and 82.6% of the time, respectively.
In po-sition 1, ENS significantly outperforms all othersystems with 95% confidence.
Moreover, we no-tice that the divergence between ENS and the othersystems greatly increases for K ?
2, where thesignificance is now at the 99% level.The CUR system models the selection paradigmof today?s consumer touch-enabled devices (i.e., itassumes that the intented selection is always thetouched word).
Without changing the user inter-face, we report a 45% improvement in predictingwhat the user intended to select over this baseline.If we changed the user interface to allow two orthree options to be displayed to the user, then wewould improve by 93% and 110%, respectively.For CUR and NER, we report results only atK = 1 since these systems only ever return a sin-gle smart selection.
Note also that when no namedentity is found by NER, or no noun phrase is foundby NP or no knowledge base entry is found by KB,the corresponding systems return the original userselection as their smart selection.CAP does not vary much across K: when theintended selection is a capitalized multi-word, thelongest string tends to be the intended selection.The same holds for KB.Whereas Table 1 reports the aggregate expectedtraffic performance, we further explore the per-formance against the stratified THEAD, TTORSO, andTTAILtestsets.
The results are summarized in Ta-ble 2.
As outlined in Section 3.2, the HEAD se-lections tend to be disproportionately entities andcapitalized terms when compared to the TORSOand TAIL.
Hence CAP, NER and KB perform muchbetter on the HEAD.
In fact, on the HEAD, CAP per-forms statistically as well as the ENS model.
Thismeans that at position 1, for systems that need tofocus only on the HEAD, a very simple solution isadequate.
For TORSO and TAIL, however, ENSperforms better.
At positions 2 and 3, across allstrata, the ENS model significantly outperforms allother systems (with 99% confidence).Next, we studied the relative contribution ofeach ensemble member to the ENS model.
Fig-ure 3 illustrates the results of the ablation study.The ensemble member that results in the biggestperformance drop when removed is HIM.
Perhaps1530HEAD TORSO TAILCP@1 CP@2 CP@3 CP@1 CP@2 CP@3 CP@1 CP@2 CP@3CUR 48.5 - - 36.7 - - 26.6 - -CAP 74.2 74.7 74.8 43.0 45.0 45.1 26.1 27.4 28.2NER 60.6 - - 39.2 - - 26.7 - -NP 52.3 64.9 69.4 31.0 48.2 53.8 20.0 32.2 35.7KB 66.7 66.7 66.7 47.0 47.9 48.1 29.9 30.1 30.1HIM 64.4 65.7 65.7 44.7 45.2 45.4 27.9 28.2 28.2ENS 75.8 91.8?96.5?52.7?73.7?81.5?32.4?50.7?58.5?Table 2: Smart selection performance, as a function of CP, on the THEAD, TTORSO, and TTAILtestsets.
?and?indicate statistical significance with p = 0.01 and 0.05, respectively.
An oracle ensemble wouldachieve an upper bound CP of 98.5%, 86.8% and 64.8% for THEAD, TTORSO, and TTAIL, respectively.0.70.750.80.850.90.951S mart Selection Cumulative Precision @ Rank (ALL)Ensemble Member AblationE NS-H IM-KB-NE R0.50.60.70.80.911 2 3 4 5CumulativePrecisionRankSmart Selection Cumulative Precision @ Rank (ALL)Ensemble Member AblationENS- HIM- KB- NE R- NPFigure 3: Ablation of ensemble model membersover TALL.
Each consecutive model removes onemember specified in the series name.surprisingly, a first ablation of either the CAP orKB model, two of the better individual performingmodels from Table 1, leads to an ablated-ENS per-formance that is nearly identical to the full ENSmodel.
One possible reason is that both tend togenerate similar candidates (i.e., many entities inour KB are capitalized).
Although the HIM modelas a standalone system does not outperform sim-ple linguistic unit selection models, it appears tobe the most important contributor to the overallensemble.5.3 Error Analysis: Oracle EnsembleWe begin by assessing an upper bound for our en-semble, i.e., an oracle ensemble, by assuming thatif a correct candidate is generated by any ensem-ble member, the oracle ensemble model places itin first position.
For TALLthe oracle performanceis 87.3%.
In other words, our choice of ensemblemembers was able to recover a correct smart se-lection as a candidate in 87.3% of the user studycases.
For THEAD, TTORSO, and TTAIL, the oracleperformance is 98.5%, 86.8%, and 64.8%, respec-tively.Although our ENS model?s CP@3 is within 2-6points of the oracle, there is room to significantlyimprove our CP@1, see Table 1 and Table 2.
Weanalyze this opportunity by inspecting a randomsample of 200 test cases where ENS produced anincorrect smart selection in position 1.
The break-down of these cases is: 1 case from THEAD; 50cases from TTORSO; 149 cases from TTAIL, i.e.,most errors occur in the TAIL.For 146 of these cases (73%), not a single en-semble member produced the correct target selec-tion ?
as a candidate.
We analyze these cases indetail in Section 5.4.
Of the remaining cases, 25,10, 9, 4, 4, and 2 were correct in positions 2, 3, 4,5, 6, 7, respectively.
Table 3 lists some examples.In 18 cases (33%), the result in position 1 isvery reasonable given the context and user selec-tion (see lines 1-4 in Table 3 for examples).
Oftenthe target selection was also found in second po-sition.
These cases highlight the need for a morerelaxed, relevance-based user study, as pointed outat the end of Section 5.1.1.We attributed 7 (13%) of the cases to data prob-lems: some cases had a punctuation as a sole char-acter user selection, some had a mishandled es-caped quotation character, and some had a UTF-8encoding error.The remaining 29 (54%) were truly model er-rors.
Some examples are shown in lines 5-8 in Ta-ble 3.
We found three categories of errors here.First, our model has learned a strong prior on pre-ferring the original user selection (see exampleline 5).
From a user experience point of view,when the model is unsure of itself, it is in factbetter not to alter her selection.
Second, we alsolearned a strong capitalization prior, i.e., to trustthe CAP member (see example line 6).
Finally, wenoticed that we have difficulty handling user selec-tions consisting of a stopword (we noted determin-ers, prepositions, and the word ?and?).
Adding afew simple features to ENS based on a stopwordslist or a list of closed-class words should addressthis problem.1531Text Snippet User Selection ENS 1st Result1 ?The Russian conquest of the South Caucasus in the 19th century split thespeech community across two states...?Caucasus South Caucasus2 ?...are generally something that transportation agencies would like to mini-mize...?transportation transportation agencies3 ?The vocal organ of birds, the syrinx, is located at the base of the blackbird?strachea.
?vocal vocal organ4 ?An example of this may be an idealised waveform like a square wave...?
waveform idealised waveform5 ?Tickets may be purchased from either the ticket counter or from automaticmachines...?counter counter6 ?PBXT features include the following: MVCC Support: MVCC stands forMulti-version Concurrency Control.
?MVCC MVCC Support7 ?Centers for song production pathways include the High vocal center; ro-bust nucleus of archistriatum (RA); and the tracheosyringeal part of the hy-poglossal nucleus...?robust robust nucleus8 ?...and get an 11gR2 RAC cluster database running inside virtual ma-chines...?cluster RAC clusterTable 3: Position 1 errors when applying ENS to our test cases.
The text snippet is a substring of aparagraph presented to our judges with the target selection (? )
indicated in bold.5.4 Error Analysis: Ensemble MembersOver all test cases, the distribution of cases with-out a correct candidate generated by an ensem-ble member in the HEAD, TORSO, TAIL is 0.3%,34.6%, and 65.1%, respectively.
We manually in-spected a random sample of 100 such test cases.The majority of them, 83%, were large sentencefragments, which we consider out of scope ac-cording to our prediction task definition outlinedin Section 1.
The average token length of the tar-get selection ?
for these was 15.3.
In compari-son, we estimate the average token length of thetask-admissable cases to be 2.7 tokens.
Althoughmost of these long fragment selections seem tobe noise, a few cases are statements that a userwould reasonably want to know more about, suchas: (i) ?Talks of a merger between the NHL andthe WHA were growing?
or (ii) ?NaN + NaN *1.0i?.In 10% of the cases, we face a punctuation-handling issue, and in each case our ensemble wasable to generate a correct candidate when fixingthe punctuation.
For example, for the book title?
= What is life?, our ensemble found thecandidate What is life, dropping the ques-tion mark.
For ?
= Near Earth Asteroid.our ensemble found Near Earth Asteroid,dropping the period.
Similar problems occurredwith parentheses and quotation marks.In two cases, our ensemble members droppeda leading ?the?
token, e.g., for ?
= the HumeHighway, we found Hume Highway.Finally, 2 cases were UTF-8 encoding mistakes,leaving five ?true error?
cases.6 Conclusion and Future WorkWe introduced a new paradigm, smart selection,to address the cumbersome text selection capabil-ities of today?s touch-enabled mobile devices.
Wereport 45% improvement in predicting what theuser intended to select over current touch-enabledconsumer platforms, such as iOS, Android andWindows.
We release to the community a datasetof 33, 912 crowdsourced true intended user selec-tions and corresponding simulated user touches.There are many avenues for future work, includ-ing understanding the distribution of user toucheson their intended selection, other interesting sce-narios (e.g., going beyond the e-reader towardsdocument editors and web browsers may show dif-ferent distributions in what users select), leverag-ing other sources of signal such as a user?s profile,her interests and her local session context, and ex-ploring user interfaces that leverage n-best smartselection prediction lists, for example by provid-ing selection options to the user after her touch.With the release of our 33, 912-crowdsourceddataset and our model analyses, it is our hope thatthe research community can help accelerate theprogress towards reinventing the way text selec-tion occurs today, the initial steps for which wehave taken in this paper.7 AcknowledgmentsThe authors thank Aitao Chen for sharing hisNER tagger for our experiments, and BernhardKohlmeier, Pradeep Chilakamarri, Ashok Chan-dra, David Hamilton, and Bo Zhao for their guid-ance and valuable discussions.1532ReferencesSteven.
P. Abney.
1991.
Parsing by chunks.
InRobert C. Berwick, Steven P. Abney, and CarolTenny, editors, Principle-Based Parsing: Computa-tion and Psycholinguistics, pages 257?278.
Kluwer,Dordrecht.Ricardo Baeza-Yates, Carlos Hurtado, and MarceloMendoza.
2005.
Query recommendation usingquery logs in search engines.
In Current Trendsin Database Technology-EDBT 2004 Workshops,pages 588?596.
Springer.Lee Becker, Sumit Basu, and Lucy Vanderwende.2012.
Mind the gap: Learning to choose gaps forquestion generation.
In Proceedings of NAACL HLT?12, pages 742?751.Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-ora Donato, Aristides Gionis, and Sebastiano Vigna.2008.
The query-flow graph: model and applica-tions.
In Proceedings of CIKM ?08, pages 609?618.ACM.Nancy A. Chinchor.
1998.
Named entity task defini-tion.
In Proceedings of the Seventh Message Under-standing Conference (MUC-7), Fairfax, VA.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof EMNLP.Thomas G. Dietterich.
1997.
Machine Learning Re-search - Four Current Directions.
AI Magazine,18:4:97?136.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In In ACL, pages 363?370.Asela Gunawardana, Tim Paek, and Christopher Meek.2010.
Usability guided key-target resizing for softkeyboards.
In Proceedings of IUI ?10, pages 111?118.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings of NAACL?01, pages 1?8.Anuj Kumar, Tim Paek, and Bongshin Lee.
2012.Voice typing: A new speech interaction model fordictation on touchscreen devices.
In Proceedings ofCHI?12, pages 2277?2286.Marcia Mu?noz, Vasin Punyakanok, Dan Roth, and DavZimak.
1999.
A learning approach to shallow pars-ing.
In Proceedings of EMNLP/VLC, pages 168?178.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learning.In Proceedings of the 3rd ACL Workshop on VeryLarge Corpora, pages 82?94.
Cambridge MA, USA.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of CoNLL-2009, pages 147?155.Adwait Ratnaparkhi.
1999.
Learning to parse natu-ral language with maximum entropy models.
Mach.Learn., 34(1-3):151?175, February.Eldar Sadikov, Jayant Madhavan, Lu Wang, and AlonHalevy.
2010.
Clustering query refinements by userintent.
In Proceedings of the 19th international con-ference on World wide web, pages 841?850.
ACM.Daisuke Sakamoto, Takanori Komatsu, and TakeoIgarashi.
2013.
Voice augmented manipulation: us-ing paralinguistic information to manipulate mobiledevices.
In Mobile HCI, pages 69?78.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL-2003, pages 142?147.
Ed-monton, Canada.Zhiyong Zhang and Olfa Nasraoui.
2006.
Miningsearch engine query logs for query recommendation.In Proceedings of the 15th international conferenceon World Wide Web, pages 1039?1040.
ACM.1533
