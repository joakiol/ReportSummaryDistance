Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1253?1262,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMachine Comprehension with Discourse RelationsKarthik NarasimhanCSAIL, MITkarthikn@csail.mit.eduRegina BarzilayCSAIL, MITregina@csail.mit.eduAbstractThis paper proposes a novel approachfor incorporating discourse informationinto machine comprehension applications.Traditionally, such information is com-puted using off-the-shelf discourse analyz-ers.
This design provides limited oppor-tunities for guiding the discourse parserbased on the requirements of the targettask.
In contrast, our model induces re-lations between sentences while optimiz-ing a task-specific objective.
This ap-proach enables the model to benefit fromdiscourse information without relying onexplicit annotations of discourse structureduring training.
The model jointly iden-tifies relevant sentences, establishes rela-tions between them and predicts an an-swer.
We implement this idea in a discrim-inative framework with hidden variablesthat capture relevant sentences and rela-tions unobserved during training.
Our ex-periments demonstrate that the discourseaware model outperforms state-of-the-artmachine comprehension systems.11 IntroductionThe task of machine comprehension concerns theautomatic extraction of answers from a given pas-sage.
Often, the relevant information required toanswer a question is distributed across multiplesentences.
Understanding the relation(s) betweenthese sentences is key to finding the correct an-swer.
Consider the example in fig.
1.
To answerthe question about why Sally put on her shoes , weneed to infer that She put on her shoes and Shewent outside to walk are connected by a causalityrelation.1Code and data are available at http://people.csail.mit.edu/karthikn/mcdr.Sally liked going outside.
She put on her shoes.She went outside to walk.
[...] Missy the catmeowed to Sally.
Sally waved to Missy the cat.[...]
Sally hears her name.
?Sally, Sally, comehome?, Sally?s mom calls out.
Sally runs hometo her Mom.
Sally liked going outside.Why did Sally put on her shoes?A) To wave to Missy the catB) To hear her nameC) Because she wanted to go outsideD) To come homeFigure 1: Sample story excerpt from a passage inthe MCTest dataset.2Correct answer is in italics.Prior work has demonstrated the value of dis-course relations in related applications such asquestion answering (Jansen et al, 2014).
Tradi-tionally, however, these approaches rely on out-puts from off-the-shelf discourse analyzers, us-ing them as features for target applications.
Suchpipeline designs provide limited opportunities forguiding the discourse parser based on the require-ments of the end task.
Given a wide spectrumof discourse frameworks (Mann and Thompson,1988; Prasad et al, 2008; Wolf and Gibson, 2005),it is not clear a priori what the optimal set of dis-course annotations is for the task.
Moreover, ageneric discourse parser may introduce additionalerrors due to the mismatch between its trainingcorpus and a dataset used in an application.
In fact,the largest discourse treebanks are based on news-paper corpora (Prasad et al, 2008; Carlson et al,2002), which differ significantly in style from textused in machine comprehension corpora (Richard-son et al, 2013).In this paper, we propose a novel approachfor incorporating discourse structure into machine2http://research.microsoft.com/en-us/um/redmond/projects/mctest/1253comprehension applications.
Rather than using astandalone parser that is trained on external su-pervised data to annotate discourse relations, themodel induces relations between sentences whileoptimizing a task-specific objective.
This designbiases the model to learn relations at a granu-larity optimized for the machine comprehensiontask.
In contrast to a generic discourse analyzer,our method can also utilize additional informationavailable in the machine comprehension context.For instance, question types provide valuable cuesfor determining discourse relations, and thus canfacilitate learning.We implement these ideas in a discrimina-tive log-linear model with hidden variables.
Themodel jointly identifies relevant sentences, estab-lishes relations between them and predicts an an-swer.
Since the same set of sentences can give riseto multiple questions, we do not limit the modelto a single discourse relation, but rather model adistribution over possible relations.
During train-ing, we only have access to questions and goldanswers.
Since relevant sentences and their rela-tions are not known, we model them as hiddenvariables.
To guide the model towards linguisti-cally plausible discourse relations, we add a fewseed markers that are typical of each relation.
Themodel predicts relations not only based on the sen-tences, but also incorporates information about thequestion.
By decomposing the dependencies be-tween model components, we can effectively trainthe model using a standard gradient descent ap-proach.We evaluate our model using a recently re-leased machine comprehension dataset (Richard-son et al, 2013).
In this corpus, roughly half ofthe questions rely on multiple sentences in the pas-sage to generate the correct answer.
For baselines,we use the best published results on this dataset.Our results demonstrate that our relation-awaremodel outperforms the individual baselines by upto 5.7% and rivals the performance of a state-of-the-art combination system.
Moreover, we showthat the discourse relations it predicts for sentencepairs exhibit considerable overlap with relationsidentified by human annotators.2 Related WorkMachine Comprehension Following traditionalmethods in question answering, most approachesto machine comprehension focus on analyzing theconnection between the question, candidate an-swer and the document.
For instance, Richardsonet al (2013) show that using word overlap aloneprovides a good starting point for the task.
Usingtextual entailment output (Stern and Dagan, 2011)and embedding-based representations (Iyyer et al,2014) further improves the result.
Even thoughthese methods operate at a paragraph level, theydo not model relations between sentences.
For in-stance, in their work on factoid question answer-ing using recursive neural networks, Iyyer et al(2014) average the sentence vectors element-wisewhen considering more than one sentence.A notable exception is the approach proposedby Berant et al (2014).
Their approach builds ona semantic representation that encodes a numberof inter-event relations, such as cause and enable.These relations straddle the boundary between dis-course and semantic connections, since most ofthem are specific to the domain of interest.
Theserelations are identified in a supervised fashion us-ing a significant amount of manual annotations.
Incontrast, we are interested in extracting discourserelations with minimal additional annotation, re-lying primarily on the available question-answerpairs.
As a result, we look at a smaller set of ba-sic relations that can be learned without explicitannotations.Discourse analysis for Question AnsweringPrior work has established the value of domain-independent discourse relations in question an-swering applications (Verberne et al, 2007; Jansenet al, 2014; Chai and Jin, 2004).
For instance,Verberne et al (2007) propose an answer extrac-tion technique that treats question topics and an-swers as siblings in a Rhetorical Structure The-ory (RST) tree, significantly improving perfor-mance on why-questions.
Chai and Jin (2004) ar-gue that incorporating discourse processing cansignificantly help context question answering, atask in which subsequent questions may refer toentities or concepts in previous questions.
Jansenet al (2014) utilize discourse information to im-prove reranking of human-written answers fornon-factoid questions.
They experiment with bothshallow discourse markers and deep representa-tions based on RST parsers to rerank answers forhow and why-type questions3.While the above approaches vary greatly in3They use data from Yahoo!
Answers and a Biology text-book.1254terms of their design, they incorporate discourseinformation in a similar fashion, adding it as fea-tures to a supervised model.
The discourse in-formation is typically computed using discourseparsers based on frameworks like RST (Feng andHirst, 2014) or PDTB (Lin et al, 2014), trainedusing supervised data.
In contrast, our goal is tolearn discourse relations driven by the task objec-tive.
The set of these relations does not capturethe richness of discourse representations consid-ered in traditional discourse theories (Mann andThompson, 1988; Prasad et al, 2008).
However,we learn them without explicit annotations of dis-course structure, and demonstrate that they im-prove model performance.3 Task Description and ApproachWe focus on the task of machine comprehension,which involves answering questions based on apassage of text.
Concretely, let us consider a pas-sage pi= {Zi,Qi} to consist of a set of sentencesZi= {zin} and a set of questions Qi= {qij},with each question also having a set of answerchoices Aij= {aijk}.
We denote the correct an-swer choice for a question qijas a?ij.
Given a set oftraining passages Ptrainwith questions annotatedwith the correct answer choice, the task is to beable to answer questions accurately in a differentset of passages Ptest.Figure 1 shows an example of a passage, alongwith a question and answer choices.
The only(weak) source of supervision available is the cor-rect answer choice for each question in training.We do not use any extra annotations during train-ing.
We propose joint probabilistic models to ad-dress this task, that can learn to identify single ormultiple relevant sentences given a question, es-tablish a relation between them and score the an-swer choices.We explore three different discriminative mod-els, ranging from a simple one that answers ques-tions using a single sentence in the passage, to onethat infers relations between multiple sentences toscore answer choices.
We defer the description ofthe features used in our models to section 3.1.Model 1 In our first model, we assume that eachquestion can be answered using a single sentencefrom the passage.
Treating the sentence as a hid-den variable, we define a joint model for a sen-tence z ?
Z and an answer choice a ?
Aj, givena question qj.
(1)P (a, z | qj) = P (z | qj) ?
P (a | z, qj)We define the joint probability as a product of twodistributions.
The first is the conditional distribu-tion of sentences in the paragraph given the ques-tion.
This is to help identify the right sentence re-quired to answer the question.
The second compo-nent models the conditional probability of an an-swer given the question q and a sentence z. Forboth component probabilities, we use distributionsfrom the exponential family with features and as-sociated weights:P (z | q) ?
e?1?
?1(q,z)P (a | z, q) ?
e?2?
?2(q,a,z)where ?s are the feature functions and ?s are thecorresponding weight vectors.We cast the learning problem as estimation ofthe parameter weights to maximize the likelihoodof the correct answers in the training data.
We con-sider soft assignments to z and marginalize overall its values to get the likelihood of an answerchoice:(3)P (ajk| qj) =?nP (ajk, zn|qj)This results in the following regularized likeli-hood objective to maximize:(4)L1(?
;Ptrain)= log|Ptrain|?i=1|Qi|?j=1P (a?ij| qij)?
?||?||2Model 2 We now propose a model for the multi-sentence case where we make use of more than asingle relevant sentence pertaining to a question.Considering that a majority of the questions in thedataset can be answered using two sentences, werestrict ourselves to sentence pairs for purposes ofcomputational tractability.
We define the new jointmodel as:(5)P (a, z1, z2| q) = P (z1| q) ?
P (z2| z1, q)?
P (a | z1, z2, q)where the new components are also exponential-family distributions:P (z2| z1, q) ?
e?3?
?3(q,z1,z2)P (a | z1, z2, q) ?
e?2?
?2(q,a,z1,z2)1255Here, we have three components: the condi-tional probability of a sentence z1given q, of a sec-ond sentence z2given q and z1, and of the answera given q and the sentences.4Ideally, we would beable to consider all possible pairs of sentences in agiven paragraph.
However, to reduce computationcosts in practice, we use a sentence window k andconsider only sentences that are at most k awayfrom each other.5We hence maximize:(7)L2(?
;Ptrain) =log|Ptrain|,|Qi|,|Zi|?i =1,j=1,m=1?n?
[m?k,m+k]P (a?ij, zim, zin| qij)?
?||?||2Model 3 In our next model, we aim to cap-ture important relations between sentences.
Thismodel has two novel aspects.
First, we consider adistribution over relations between sentence pairsas opposed to a single relation.
Second, we utilizethe cues from the question as context to resolveambiguities in sentences pairs with multiple plau-sible relations.We add in a hidden variable r ?
R to representthe relation type.
We incorporate features that tiein the question type with the relation type, and thatconnect the type of relation to the lexical and syn-tactic similarities between sentences.
Our relationsetR consists of the following relations:?
Causal : Causes of events or reasons for facts.?
Temporal : Time-ordering of events?
Explanation : Predominantly dealing with how-type questions.?
Other : A relation other than the above6We can now modify the joint probability from(5) by adding in relation type r to get:(8)P (a, r, z1, z2| q) = P (z1| q) ?
P (r | q)?
P (z2| z1, r, q) ?
P (a | z1, z2, r, q)whereP (r | q) ?
e?4?
?4(q,r)(9a)P (z2| z1, r, q) ?
e?3?
?3(q,r,z1,z2)(9b)P (a | z1, z2, r, q) ?
e?2?
?2(q,r,a,z1,z2)(9c)The extra component P (r | q) is the conditionaldistribution of the relation type r depending on the4Since this component replaces the second component inmodel 1, we use the same subscript 2 for its feature set ?.5Including the case where z1= z2.6This includes the no-relation casesquestion.
This is to encourage the model to learn,for instance, that why-questions correspond to thecausal relation.
We also add in extra features toP (z2| z1, r), that help select a sentence pair con-ditioned on a relation.
The likelihood objective tomaximize is:(10)L3(?
;Ptrain)= log?i,j,m,r?R?n?
[m?k,m+k]P (a?ij, zim, zin, r | qij)?
?||?||2We maximize the likelihood objectives usingLBFGS-B (Byrd et al, 1995).
We compute thegradients required using Automatic Differentia-tion (Corliss, 2002).To predict an answer for a test question qj,we simply marginalize over all the hidden vari-ables and choose the answer that maximizesP (ajk| qj):a?j= argmaxkP (ajk|qj)3.1 FeaturesWe use a variety of lexical and syntactic featuresin our model.
We employ the Stanford CoreNLPtool (Manning et al, 2014) to pre-process the data.Other than commonly used features in Q&A sys-tems such as unigram and bigram matches, part-of-speech tags, syntactic features, we also add infeatures specific to our model.We first define some terms used in our descrip-tion.
Entities are coreference-resolved nouns orpronouns.
Actions refer to verbs other than aux-iliary ones such as is, are, was and were.
An en-tity graph is a graph between entities present in asentence.
We create an entity graph by collapsingnodes in the dependency graph and storing the in-termediate nodes between any two entity nodes inthe edge between the nodes.
We refer to the wordsin a question q as q-words and similarly to wordsin an answer a as a-words and those in a sentencez as z-words.
Figure 2 shows an example of an en-tity graph constructed from the dependency graphof a sentence.We divide the features into 4 sets (?1?4), cor-responding to each component probability in (8).Types 1 and 2 are inspired by prior work inquestion classification/answering (Blunsom et al,2006; Jansen et al, 2014).
Feature types 3 and 4are specific to our models, primarily dealing withrelation types.1256My dog also likes spicy sausagepossnsubjadvmoddobjamodrootdog sausagelikesFigure 2: Top: Dependency graph, Bottom: En-tity graph for an example sentence.
Entities are inbold, actions are in italics.Relation Word listCausal because, why, due, soTemporal when, between, soon, before,after, during, then, finally,now, nowadays, firstExplanation how, by, usingTable 1: Seed marker words for relations used bythe model.Type 1 (?1) These features are primarily in-tended to help the model select the most relevantsentence from the passage for a question.
We addcommonly used features such as unigram and bi-gram matches, syntactic root match, entity and ac-tion matches, missed entities/actions (in q but ab-sent in z) and fractional coverage of q-words in z.In addition, we use matches between the edges ofthe entity graph of q and z.
We also have second-order features that are a cross of each feature men-tioned above with the question word (how, what,when, etc.
).Type 2 (?2) Features in ?2capture interactionsbetween the answer a, question q and sentence(s)(z1, z2in models 2,3 or z in model 1).
Forthe first-order features, we use ones similar tothose in ?1for lexical, syntactic, entity and ac-tion matches/misses between a and z.
In addition,we add in a neighbor match feature, which checksfor matches between the neighborhood of a wordfrom a that occurs in z, and q-words.
Another fea-ture we employ is the joint match between z-wordsand the union of a-words and q-words.
Finally, weadd in a sliding window (SW) feature, computingits value as in Richardson et al (2013).Type 3 (?3) The next set of features are spe-cific to only models 2 and 3, used to connect sen-tences z1and z2(and a relation r in model 3 only).Split MC160 MC500Passages Questions Passages QuestionsTrain 70 280 300 1200Dev 30 120 50 200Test 60 240 150 600Table 2: Dataset StatisticsWe use features like the inter-sentence distanceand the presence of relation-specific markers inthe sentences.
We also cross the latter featureswith entity and action matches between z1and z2.For the relation-specific words for each relation(except Other), we use words (see Table 1) de-rived mainly from Marcu (1997)?s list of discoursemarkers.Type 4 (?4) The final set of features (used onlyin model 3) are present to help the model learnconnections between the words in the questionand the relation type r. Specifically, we check ifthe interrogative word in the question matches theclass represented by r. For instance, the word whymatches the Causal relation.For the match-type features of all four types,we use the match count as the feature value if thecount is non-zero.
If the count is zero, we insteadset a corresponding zero feature7to 1.4 Experimental setupData and Setup We run our experiments on arecently compiled dataset for machine comprehen-sion: MCTest (Richardson et al, 2013).
The dataconsists of two distinct sets: MC160 and MC500,which are of different sizes.
Table 2 gives detailson the data splits for each dataset.
Each passagehas 4 questions, with 4 answer choices each.
Thequestions are also annotated into 2 types: single,if the question can be answered using a single sen-tence in the passage, or multi otherwise.
We do notuse the type information in our learning; we onlyuse it for categorizing accuracy during evaluation.We report final results on all our models trainedwith ?
= 0.1, tuned using the Dev sets.Evaluation We report accuracy scores for eachmodel averaged over the questions in the test data.For each question, the system gains 1 point ifit scores the correct answer highest and 0 other-wise.
In case of ties, we use an inverse weighting7For each match feature, like Entity-Match, we have a cor-responding zero feature, Entity-Match-Zero1257ModelMC160 MC500dev test dev testSingle Multi All Single Multi All Single Multi All Single Multi AllSWD 67.92 50.74 58.33 75.89 60.15 67.5 63.95 54.38 58.5 63.23 57.62 60.16RTE 64.15 53.73 58.33 57.14 59.37 58.33 58.13 47.36 52 70.22 42.37 55RTE+SWD 71.69 59.6 65 76.78 62.5 69.16 73.25 57.89 64.5 68.01 59.45 63.33Model 1 78.45 60.57 68.47 83.25 60.35 71.04?74.41 57.01 64.5?70.58 57.77 63.58?Model 2 74.68 60.07 66.52 81.47 64.25 72.29?73.25 61.4 66.5??
66.17 59.9 62.75?M2 + RST 72.79 58.58 64.86 79.68 61.91 70.20?72.09 57.89 64.0?66.54 59.29 62.58Model 3 72.79 60.07 65.69 82.36 65.23 73.23?
?72.09 60.52 65.5?
?68.38 59.9 63.75?Table 3: Accuracy (%) of the different baselines (in italics) and our models.
Single: questions requiringsingle sentence to answer; Multi: questions requiring multiple sentences to answer.
Sentence window(k) = 4 for models 2 and 3.
Best scores are shown in bold.
Statistical significance (shown only for Allcolumns) of p < 0.05 using two-tailed paired t-test:?vs SWD,?vs RTE.scheme to assign partial credit.
So, if three an-swers (including the correct one) tie for the highestscore, the system gains 1/3 points.Baselines We use the systems proposed byRichardson et al (2013) as our baselines.
Thesesystems have the best reported scores on thisdataset.
The first baseline, SWD, uses a slidingwindow to count matches between the passagewords and the words in the answer.
This is thencombined with a score representing the averagedistance between answer and question words inthe passage.
The second baseline, RTE, uses atextual entailment recognizer (Stern and Dagan,2011) to determine if the answer (turned into astatement along with the question) is entailed bythe passage.
The third system, RTE+SWD, is aweighted combination of the first two baselinesand achieves the highest accuracy on the dataset.5 ResultsComprehension accuracy Table 3 shows thatour relation-aware model 3 outperforms individ-ual baselines on both test sets.
On the MC160 testset, the model achieves the best performance of73.23% accuracy, outperforming the SWD base-line by 5.7% and the RTE+SWD combination by4.07%.
The major gains of model 3, which uti-lizes inter-sentential relations, over model 1 can beseen in the accuracy of multi type questions witha jump of almost 5% absolute in accuracy (statis-tically significant with p < 0.05).
On the MC500test set, we again find that model 3, with a scoreof 63.75%, provides a gain of 3.5% over SWD andis comparable to the performance of RTE+SWD(63.33%)The importance of utilizing multiple relevantsentences to score answers is evident from thehigher scores of models 2 and 3 on multi typequestions in both test sets.
However, model 1,which retrieves only a single relevant sentence foreach question, achieves the best scores on the sin-gle type questions up to 83.25% on MC160 test.One reason for this could be the larger searchspace for model 3 over pairs of sentences com-pared to just single sentences for model 1.Table 4 shows the variation of our model?s accu-racy with the question type.
We see that the modeldeals well with what, where and why type ques-tions in MC500, achieving almost 67-69% accu-racy.8The major errors (in MC500) seem to comefrom the how-questions, where the model?s accu-racy is low (48%).
In MC160, the accuracy is evenhigher for what-questions (almost 80%).
On theother hand, the model does slightly worse on why-questions, with only 60% accuracy.RST augmented model Further, we experimentwith adding in relations extracted by a publiclyavailable RST parser (Feng and Hirst, 2012).
Theparser extracts a tree with the passage sentencesas its leaves and relations as interior nodes in thetree.
From this tree, we compute the relation be-tween a pair of sentences as their lowest commonancestor.
If one of the sentences is broken downinto clauses, we use them all to gather multiple re-lations.
We add in features that combine the RST-predicted relation with the interrogation word ofthe question, and with entity and action matchesbetween sentence pairs.We can see from Table 3 that adding in RSTfeatures to model 2 (M2+RST) does not give the8Note that what-questions may also requirecausal/temporal/explanation relations to answer.1258Question MC160 MC500Type Dev Test Dev Testhow 50.00 (10) 71.42 (21) 54.54 (11) 48.83 (43)what 64.40 (59) 79.36 (126) 63.15 (114) 67.19 (317)where 30.76 (13) 91.66 (12) 82.60 (23) 68.96 (58)which 75.00 (4) 33.33 (6) 25.00 (4) 48.00 (25)who 70.50 (17) 67.85 (28) 62.50 (16) 59.74 (77)why 85.71 (14) 59.45 (37) 65.38 (26) 69.35 (62)when 100.0 (2) 80.00 (5) 100.0 (4) 62.50 (8)whose - - - 66.67 (3)(other) 100.0 (1) 40.00 (5) 50.00 (2) 14.28 (7)Table 4: Accuracy (%) of model 3 by question type for question in MC160 and MC500 dev and test sets.Numbers in parentheses indicate the number of questions of each type.same performance as model 3.
In fact, the modelperforms slightly worse than model 2, which doesnot utilize inter-sentential relations.
Our analysisof the RST trees reveals that for a vast majority ofsentence pairs (77%), the RST algorithm predictsthe elaboration relation which does not provide aninformative distinction.5.1 AnalysisTo gain further insight into the workings of ourmodel, we perform several analyses on model 3using human judgements.
We annotate 240 ques-tions from the test set of MC160 with the mostrelevant sentences9in the passage for each ques-tion.
In addition, if they chose more than a sin-gle relevant sentence, we also asked the annota-tors to mark the most appropriate relation (fromour set of relations used in model 3) between thesentence pairs.10We find that 146 question anno-tations contain a single relevant sentence and 94contain multiple sentences.11We obtain 103 sen-tence pairs with annotated relations.Annotation statistics We select a random sub-set of 134 questions from this data to annotatetwice and compute inter-annotator agreement.
Thesecond annotator agreed completely with the sen-tence predictions of the first annotator in 76.11%cases and both annotators agreed on at least onesentence in 94.77% of the questions.
The agree-ment on relations annotated over common sen-9The annotators are native English speakers.10If there were more than two relevant sentences, we askedthem to mark relations between all pairs.
This was a very rareoccurrence though.11We found that some of the multiple questions did not re-quire multiple sentences to answer and conversely, some sin-gle questions required more than one sentence to answer.tence pairs is 68.6%, with ?
= 0.462.
We findthat out of the 103 annotated sentence pairs, 67are next to each other in the passage while 27 areat a distance of two and 9 pairs are at a distance ofthree or more.It has been well documented that identifyingdiscourse relations without explicit markers is sig-nificantly harder than with markers (Pitler et al,2008; Lin et al, 2009; Park and Cardie, 2012).We compute statistics on the presence of discoursemarkers anywhere in the manually picked sen-tence(s) for each question.
We find that only33.89% of these pairs have a relevant discoursemarker present in either sentence.
We considera discourse marker as relevant if it occurs in ourmarker list for the annotated relation.
Further, ifwe only consider markers occurring at the begin-ning or end of the sentences, this number drops to9.23% of sentences.
Since we consider relationsbetween sentence pairs, most explicit markers thatcould help identify these relations would occur atan extremity of either sentence.
We point out thatthese numbers are an over-estimation since manyof the markers occur in syntactic roles as opposedto discourse in the sentences (ex.
so in This is sogood compared to So, he decided to ...).
Thesestatistics reflect the difficulty of the problem sinceoperating over implicit relations is much harder.Sentence Retrieval We analyze our models?ability to predict relevant sentences given only thequestion.
For each question, we order the pairsscored by a model in descending order of theirprobability according to P (z1, z2| q) and com-pare them to the annotated pairs, reporting recallat various thresholds.This is a stringent evaluation primarily due to1259Question R @ 1 R @ 2 R @ 5Type (#) Freq M1 M2 M3 Freq M1 M2 M3 Freq M1 M2 M3how (21) 9.67 29.03 32.25 35.48 9.67 32.25 45.16 48.38 19.35 51.61 58.06 64.51what (126) 3.64 37.85 35.59 35.02 9.37 39.54 47.45 45.76 21.81 53.67 63.84 63.27where (12) 13.63 50.00 50.00 56.25 13.63 50.00 68.75 62.5 45.45 68.75 75.00 81.25which (6) 0.0 21.42 21.42 14.28 9.09 21.42 21.42 14.28 9.09 21.42 35.71 42.85who (28) 2.12 45.45 45.45 42.42 4.25 45.45 60.60 57.57 23.40 63.63 72.72 75.75why (37) 9.09 34.32 31.34 34.32 2.27 35.82 40.29 40.29 38.63 44.77 53.73 52.23when (5) 0.0 57.14 57.14 57.14 0.0 57.14 71.42 71.42 25.00 85.71 100.0 85.71(other) (5) 0.0 42.85 28.57 42.85 0.0 42.85 57.14 57.14 100.0 71.42 71.42 71.42Single (146) 6.84 56.16 53.42 51.36 11.64 58.21 66.43 63.69 33.56 72.60 80.13 80.82Multi (94) 3.88 24.27 23.30 25.72 9.70 25.24 34.46 33.98 29.61 39.32 50.00 50.48Overall (240) 5.11 37.5 35.79 36.36 10.51 38.92 47.72 46.30 31.25 53.12 62.5 63.06Table 5: Recall (%) of relevant sentence(s) in the ranking by models 1, 2 and 3 compared with a match-frequency baseline (Freq) at various thresholds, for different question types in MC160.
Question fre-quencies are in parentheses.
Bold numbers represent best scores.two reasons.
First, we do not use the candidateanswers in selecting relevant sentences.
Second,on the machine comprehension task, the modelpredicts answers by marginalizing over the sen-tences/sentence pairs.
Hence, the model can scoreanswers correctly even if the relevant sentence(s)are not at the top of its sentence distribution calcu-lated here.
We compute the distribution over sen-tence pairs as:P (z1, z2|q) =?r?RP (z1| q) ?
P (r | q) ?
P (z2|z1, r, q)For comparison, we add in a baseline (Freq) thatorders sentences using the sum of unigram andbigram matches with the question (in descendingfrequency).Table 5 shows that our models perform signifi-cantly better than the Freq baseline over all ques-tion types.
For the single-question case, we ob-serve that model 3 ranks the annotated sentenceat the top of its distribution around 51% of thetime and 80% of the time in the top 5.
For multi-sentences, these recall numbers drop to around25% (@1) and 50% (@5).
We also observe thatmodels 2 and 3 perform better than model 1 on themulti-sentence cases.
The similar sentence recallof models 2 and 3 also point to the fact that thegains from model 3 on comprehension accuracyare due to its ability to utilize relations betweenthe sentences.We observe that where, when and who questionshave the highest recalls.
This is likely becausethese questions often have characteristic words oc-curring in the sentences (such as here, there, af-ter, before, him, her).
In contrast, questions askinghow, which and why have lower recalls since theyoften involve reasoning over multiple sentences.What-questions are somewhere in between sincetheir complexity varies from question to question.Relation Retrieval We examine how well ourmodel can predict relations between given sen-tence pairs.
For each annotated pair of sentences,we calculate the relation distribution and computethe relation recall at various thresholds of the rank-ing by probability.
The relation distribution iscomputed as:P (r|z1, z2, q) =P (r | q) ?
P (z2|z1, r, q)?r?
?RP (r?| q) ?
P (z2|z1, r?, q)From table 6, we observe that our model?s topprediction matches the manual annotations (over-all) 51% of the time.
The model predicts causaland other relations more accurately than the othertwo.Relation (#) R @ 1 R @ 2Causal (32) 56.25 75.00Temporal (11) 27.27 54.54Explanation (6) 16.66 33.33Other (54) 57.40 64.81Overall 51.45 65.04Table 6: Recall of annotated relations at variousthresholds in the ordered relation distribution pre-dicted by model 3.
Relation frequencies are inparentheses.6 ConclusionsIn this paper, we propose a new approach for in-corporating discourse information into machine1260comprehension applications.
The key idea is toimplant discourse analysis into a joint model forcomprehension.
Our results demonstrate that thediscourse-aware model outperforms state-of-the-art standalone systems, and rivals the performanceof a system combination.
We also find that fea-tures derived from an off-the-shelf parser do notimprove performance of the model.
Our analysisalso demonstrates that the model accuracy variessignificantly according to the question type.
Fi-nally, we show that the predicted discourse rela-tions exhibit considerable overlap with relationsidentified by human annotators.AcknowledgementsWe thank Lyla Fischer, Rushi Ganmukhi,Hrishikesh Joshi and Deepak Narayanan forhelping with annotating the MC160 test set.
Wealso thank the anonymous ACL reviewers andmembers of MIT?s NLP group for their insightfulcomments and suggestions.
This research isdeveloped in a collaboration of MIT with theArabic Language Technologies (ALT) groupat Qatar Computing Research Institute (QCRI)within the Interactive sYstems for Answer Search(IYAS) project.References[Berant et al2014] Jonathan Berant, Vivek Srikumar,Pei-Chun Chen, Abby Vander Linden, BrittanyHarding, Brad Huang, Peter Clark, and Christo-pher D. Manning.
2014.
Modeling biologicalprocesses for reading comprehension.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1499?1510, Doha, Qatar, October.
Association forComputational Linguistics.
[Blunsom et al2006] Phil Blunsom, Krystle Kocik, andJames R Curran.
2006.
Question classification withlog-linear models.
In Proceedings of the 29th an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 615?616.
ACM.
[Byrd et al1995] Richard H Byrd, Peihuang Lu, JorgeNocedal, and Ciyou Zhu.
1995.
A limited mem-ory algorithm for bound constrained optimization.SIAM Journal on Scientific Computing, 16(5):1190?1208.
[Carlson et al2002] Lynn Carlson, Mary EllenOkurowski, Daniel Marcu, Linguistic DataConsortium, et al 2002.
RST discourse tree-bank.
Linguistic Data Consortium, University ofPennsylvania.
[Chai and Jin2004] Joyce Y Chai and Rong Jin.
2004.Discourse structure for context question answering.In Proceedings of the Workshop on Pragmatics ofQuestion Answering at HLT-NAACL, volume 2004,pages 23?30.
Citeseer.
[Corliss2002] George Corliss.
2002.
Automatic dif-ferentiation of algorithms: from simulation to op-timization, volume 1.
Springer Science & BusinessMedia.
[Feng and Hirst2012] Vanessa Wei Feng and GraemeHirst.
2012.
Text-level discourse parsing with richlinguistic features.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 60?68.Association for Computational Linguistics.
[Feng and Hirst2014] Vanessa Wei Feng and GraemeHirst.
2014.
A linear-time bottom-up discourseparser with constraints and post-editing.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 511?521, Baltimore, Maryland, June.Association for Computational Linguistics.
[Iyyer et al2014] Mohit Iyyer, Jordan Boyd-Graber,Leonardo Claudino, Richard Socher, and HalDaum?e III.
2014.
A neural network for factoidquestion answering over paragraphs.
In EmpiricalMethods in Natural Language Processing.
[Jansen et al2014] Peter Jansen, Mihai Surdeanu, andPeter Clark.
2014.
Discourse complements lexi-cal semantics for non-factoid answer reranking.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 977?986, Baltimore, Maryland,June.
Association for Computational Linguistics.
[Lin et al2009] Ziheng Lin, Min-Yen Kan, andHwee Tou Ng.
2009.
Recognizing implicit dis-course relations in the penn discourse treebank.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Vol-ume 1-Volume 1, pages 343?351.
Association forComputational Linguistics.
[Lin et al2014] Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourseparser.
Natural Language Engineering, pages 1?34.
[Mann and Thompson1988] William C Mann and San-dra A Thompson.
1988.
Rhetorical structure the-ory: Toward a functional theory of text organization.Text, 8(3):243?281.
[Manning et al2014] Christopher D. Manning, MihaiSurdeanu, John Bauer, Jenny Finkel, Steven J.Bethard, and David McClosky.
2014.
The StanfordCoreNLP natural language processing toolkit.
InProceedings of 52nd Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations, pages 55?60.1261[Marcu1997] Daniel Marcu.
1997.
The RhetoricalParsing, Summarization and Generation of NaturalLanguage Texts.
Ph.D. thesis, University of Toronto.
[Park and Cardie2012] Joonsuk Park and Claire Cardie.2012.
Improving implicit discourse relation recog-nition through feature set optimization.
In Proceed-ings of the 13th Annual Meeting of the Special Inter-est Group on Discourse and Dialogue, pages 108?112.
Association for Computational Linguistics.
[Pitler et al2008] Emily Pitler, Mridhula Raghupathy,Hena Mehta, Ani Nenkova, Alan Lee, and Ar-avind K Joshi.
2008.
Easily identifiable discourserelations.
[Prasad et al2008] Rashmi Prasad, Nikhil Dinesh, AlanLee, Eleni Miltsakaki, Livio Robaldo, Aravind KJoshi, and Bonnie L Webber.
2008.
The penn dis-course treebank 2.0.
In LREC.
Citeseer.
[Richardson et al2013] Matthew Richardson, Christo-pher JC Burges, and Erin Renshaw.
2013.
Mctest:A challenge dataset for the open-domain machinecomprehension of text.
In EMNLP, pages 193?203.
[Stern and Dagan2011] Asher Stern and Ido Dagan.2011.
A confidence model for syntactically-motivated entailment proofs.
In Proceedings of theInternational Conference Recent Advances in Nat-ural Language Processing 2011, pages 455?462,Hissar, Bulgaria, September.
RANLP 2011 Organ-ising Committee.
[Verberne et al2007] Suzan Verberne, Lou Boves,Nelleke Oostdijk, and Peter-Arno Coppen.
2007.Evaluating discourse-based answer extraction forwhy-question answering.
In Proceedings of the 30thannual international ACM SIGIR conference on Re-search and development in information retrieval,pages 735?736.
ACM.
[Wolf and Gibson2005] Florian Wolf and Edward Gib-son.
2005.
Representing discourse coherence:A corpus-based study.
Computational Linguistics,31(2):249?287.1262
