Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 28?32,Gothenburg, Sweden, April 26, 2014.c?2014 Association for Computational LinguisticsStudying the Semantic Context of two Dutch Causal ConnectivesIris Hendrickx and Wilbert SpoorenCentre for Language Studies, Radboud University NijmegenP.O.
Box 9103, NL-6500 HD Nijmegen The Netherlandsi.hendrickx, w.spooren@let.ru.nlAbstractWe aim to study the difference of usagebetween two causal connectives in theirsemantic context.
We present an ongo-ing study of two Dutch backward causalconnectives omdat and want.
Previous lin-guistic research has shown that causal con-structions with want are more subjectiveand often express an opinion.
Our hy-pothesis is that the left and right contextsurrounding the connectives are more se-mantically similar in sentences with om-dat than sentences with want.
To test thishypothesis we apply two techniques, La-tent Semantic Analysis and n-gram over-lap.
We show that both methods indeed in-dicate a substantial difference between thetwo connectives but opposite to what wehad expected.1 IntroductionMuch corpus linguistic research has dealt with theissue of subjectivity, i.e.
the degree to which thepresence of the writer or speaker of a text is felt((Sanders and Spooren, 2013), and the referencescited there).
Subjectivity can be located at differ-ent levels in a text.
At the word level, some words(e.g., evaluative adjectives and expletives) imply awriter/speaker evaluation, whereas others do not.At the sentence level, the description of facts is feltto be more objective, whereas opinions are moresubjective.
And at the supra-sentential level, sub-jectivity can get expressed in the type of relationthat links the clauses or sentences.
For example,argumentative relations are more subjective thanstatements.
Interestingly, many languages make adistinction between more objective or more sub-jective causal connectives.
In Dutch, for example,omdat is typically used to express more or less ob-jective backward causal relations, whereas want istypically used for more subjective relations.
How-ever, these connectives are near synonyms and canbe used in the same context as shown in exam-ple 1 and 2.
There is subtle difference in meaningbecause example 1 focuses on the reason relationbetween the two segments whereas 2 focuses onthe argument relation.
As the first segment is anopinion, want is slightly more natural than omdat.
(1) Dat is vooral jammer omdat dehoofdrolspeler uitstekend zingt.
(2) Dat is vooral jammer want dehoofdrolspeler zingt uitstekend.
?That is particularly unfortunate because theprotagonist sings excellent.
?Note the difference in word order: want leadsto a coordinative conjunction while omdat gives asubordinate conjunction.We need more insight into this subtle differencebetween connectives for example to allow naturallanguage generation systems to mimic the choicesthat native speakers of Dutch make intuitively.Another application would be sentiment analysiswhere the difference in subjectivity of various con-nectives can be used to identify subjective or opin-ionated sentences.Presently the corpus linguistic analyses of sub-jective versus objective causal relations have verymuch been a small-scale enterprise, in that corpusexamples were annotated manually.
This is prob-lematic for at least two reasons: manual annota-tion relies on hand coding, with the accompanyingproblems of poor inter-annotator reliability, andthe restricted size of the hand annotated corporalimits the power of statistical generalization.
Best-gen et al.
(2006) suggested to complement thesemanual analyses with automatic analyses.Bestgen and colleagues studied backwardcausal connections in Dutch.
They made use of28two types of automatic analyses: Latent SemanticAnalysis (LSA) (Deerwester et al., 1990) and whatthey call Thematic Text Analysis (Popping, 2000)to show that the semantic connection between firstand second segment is weaker in a want connec-tion than in a omdat connection, and that the firstsegment of want connections contains more sub-jective words than the first segment of omdat con-nections.
The materials that were used by Best-gen et al.
(2006) were texts from a large corpus ofnewspaper language of 16.5 million tokens.The purpose of our current ongoing researchproject is to extend the automatic analyses in twoways: on the one hand we want to reproduce theLSA analysis of Bestgen et al.
using a larger cor-pus of about 30 million tokens; on the other hand,we want to use n-gram analyses to investigate thesemantic connection between the segments in awant versus omdat connection.The use of n-grams to measure semantic over-lap is a well known method, which has been ap-plied in the standard evaluation metrics for taskslike machine translation and automatic summa-rization.
In these tasks automatic systems aim toproduce a text as similar as possible to a manu-ally constructed gold standard text.
To evaluate thequality of these automatically produced text, mea-sures such as BLUE (Papineni et al., 2002) andROUGE (Lin and Hovy, 2003) measure n-gramoverlap between the system text and the gold stan-dard text.
Furthermore, in other types of researchlike in the field of literary studies n-grams havebeen applied, for example to discriminate betweengenres (Louwerse et al., 2008) or for author dis-crimination (Hirst and Feiguina, 2007).Backward causal connectives denotes a causerelation.
The connective is positioned in a sen-tence between the consequence (denoted as Q) andthe cause (denoted as P).
For the sentence in exam-ple 1 Q is the text segment before the connective,and P contains all words after the connective asfollows:Q Dat is vooral jammerP de hoofdrolspeler uitstekend zingt.Our hypothesis is that Q and P are more se-mantically similar in sentences with omdat thansentences with want.
This implies that we expectthe average cosine between P and Q to be smallerin omdat connections than in want connections.We also hypothesize that the number of n-gramsshared between P and Q will be higher in omdatsentences than in want sentences.This paper presents work in progress.
We firstdescribe the SoNaR corpus that was used in thisstudy in section 2.
In section 3 we present theexperimental setup and results of the experimentswith LSA.
In section 4 we detail our approach tocomputing n-grams and we discuss our findingsand the next steps to take in 5.2 Data CollectionUnfortunately neither the corpus nor the data sam-ple used by Bestgen et al.
(2006) was available tous.
For this reason we chose a similar Duch corpusto work with.
The SoNaR corpus (Oostdijk et al.,2013) is a reference corpus of 500 million writ-ten words of contemporary Dutch sampled froma wide variety of sources and genres.
The corpushas been automatically tokenized, part-of-speechtagged and lemmatized.
We took a sample of100K news articles from the SoNaR corpus as ourexperimental data set.
As we are interested in se-mantic overlap, we took the lemmatized versionsof the articles.From this data set, we collected all sentencescontaining the connectives omdat and want.
As weaim to study the semantic relation between Q andP, we only selected sentences that have a mean-ingful Q and P in the same sentence.
We excludedsentences with sentence initial connectives as theyonly contain a P segment.
Sentences with shortQ segments (containing one or two words), weremanually inspected.
A sentence that starts withdat komt omdat ?
this is because?
does not con-tain a meaningful consequence because it refersback to information in a previous sentence.
Onthe other hand, a short Q segment like tevergeefs,want ?in vain, because?
does express a meaning-ful consequence.
In case of sentences with mul-tiple connections, we took the first Q and P andcut off the remainder parts using some handwrittenrules.
Overall we excluded 20% of want sentencesand 25% of omdat sentences.
In total we selected18,260 for omdat and 14,449 sentences for want.Some statistics about the sentences is shown in Ta-ble 1.3 LSALatent Semantic Analysis (LSA) is a mathematicalmethod for representing word meaning similarityin a semantic space based on a term-by-documents29Sentences length Q len P lenomdat 18,260 24.3 11.2 12.1want 14,449 23.5 9.6 12.9Table 1: Number of sentences and average lengthin tokens of the full sentence, Q, and P in the dataset of want and omdat.matrix.
It applies singular value decompositionto this matrix to condense it to a smaller seman-tic representation of around 100 - 500 dimensions(Landauer et al., 1998).We applied LSA to measure the semantic over-lap between Q and P of the omdat and want sen-tences.
We constructed a term-by-document ma-trix based on the SoNaR news sample and con-verted this to an LSA space with 300 dimensions.Each Q and P was projected as a term vector in theLSA space and we computed the cosine similaritybetween each Q and P.To build the document-by-term matrix for LSA,words were lemmatized, and punctuation, digitsand stopwords (based on a stopword list of 221words) were filtered out.In our first analysis we used the top most fre-quent words that occurred at least 15 times, lead-ing to a text matrix of approximately 20,000 doc-uments and 19,000 word terms.
We calculated thecosine between Q and P for each of the omdatand want sequences.
A Welch Two Sample t-testshowed that contrary to expectation the cosine be-tween Q and P was lower for omdat (0.039) thanfor want (0.045; t(29518)=-4.78, p <.001).In a second analysis we chose a sample of a dif-ferent scale and we used a text matrix of 100,000documents and the top 10,000 most frequent wordterms.
A t-test showed that in this case the cosinefor omdat sequences was slightly but significantlyhigher than for want sequences (omdat: 0.048;want: 0.043; t(30175)=3.68, p <.001).In the final section we will go into possible ex-planations for these unexpected and incompatibleresults.4 N-gram overlapIn our study of n-grams, we looked both at pure bi-gram statistics and at n-grams in a broader scope,i.e.
n-grams and skip-grams with a maximallength of 10 tokens.
All n-grams have a minimumlength of 2, and a minimum frequency of 2 in thedatasample.
We use lemmatized words to reducethe influence of morphological information.
Forthe n-gram analysis we used the Colibri softwarepackage developed by Maarten van Gompel1(vanGompel, 2014).
In the left part of Table 2 we showthe bigram statistics and on the right side the n-gram statistics of n-grams that occur at least twicein Q, P, and those occurring in both Q and P. Wepresent the following counts:?
Pattern - The number of distinct n-gram pat-terns (n-gram type count)?
Coverage - The number of unigram word to-kens covered as a fraction of the total numberof unigram tokens.?
Occurrences - Cumulative occurrence countof all the patterns (n-gram token count).We can observe that about 75% of the tokensin Q and P is covered in this bigram analysis,while the n-grams cover around 93% of the words.Zooming in on the bigrams and n-grams that areshared in Q and P, we can see that these coverabout 50% and 75% of the tokens respectively.This shows that we can safely discard n-grams thatoccur only once in our counts and still cover mosttokens in the data sample.Based on the bigram occurrences in our dataset, we computed whether the bigram overlap be-tween Q and P in omdat sentences is larger thanin want sentences.
We used a loglikelihood testto compare the relative frequencies as our samplesdo not have the same size.
We found that 72362bigram occurrences (or 67.8%) overlap in omdatsentences and 58213 bigrams (or 79.4%) for wantsentences (LL2(1)=808.40, p <.01).
This meansthat, contrary to our hypothesis, we found moreoverlap for want sentences.We performed the same computation on thelarger set of n-grams.
We saw that 81573 of n-gram occurrences (44.9%) overlap in omdat sen-tences and 65272 (51.1%) overlap in for want sen-tences (LL2(1)=595.37, p <.01).
This then isagain a confirmation that we find more overlap be-tween Q and P in want sentences.5 ConclusionsIn this paper we report two types of automaticanalyses of the differences between want and om-1available at: http://proycon.github.io/colibri-core/30Category Bigrams n-gramsPatterns Coverage Occurrences Patterns Coverage Occurrencesomdat Q 18931 0.7312 106766 39780 0.9320 181506omdat P 20649 0.7549 118414 45074 0.9380 208809omdat Q&P 7261 0.5042 72362 9213 0.8927 81573want Q 12938 0.7474 73276 27654 0.9350 127723want P 17564 0.7216 94685 37027 0.9271 159125want Q&P 5774 0.4847 58213 7365 0.7943 65272Table 2: Counts of the bigrams and n-grams up to length 10 with minimal frequency 2 in Q, P, and thosen-grams that occur in both Q and P. Patterns refers to n-gram types, Occurrences to n-gram tokens andCoverage refers to word token coverage.dat, which have been claimed to differ in subjec-tivity, i.e.
the degree to which the writer is feltpresent in the text.
One part of our study is a repro-duction of (Bestgen et al., 2006) and assessed thesemantic relationship between Q and P in termsof a LSA cosine for want and omdat.
Contrary tothe findings of Bestgen et al., our first LSA analy-sis showed that the relationship between Q and Pis less strong for omdat than for want.
A secondanalysis found a small difference in the expecteddirection.
In the second part of our study we usedn-gram overlap as a different type of similaritymeasure.
Again, our hypothesis was not borne outin that omdat showed a significantly smaller de-gree of overlap than want.At this moment we cannot explain why the twoLSA experiments presented in section 3 show sig-nificant results in different directions.
In the twoexperiments the same connective sentences wereused, but the semantic space in which they wereprojected was different.
For our LSA analysiswe made use of the software package LSA in R.To rule out the possibility that our results weredue to some implementation peculiarity, we ran asmall test sample with another LSA implementa-tion Gensim (?Reh?u?rek and Sojka, 2010).
Both im-plementations gave us similar cosine values for thesame sample.A noticeable difference with the Bestgen et al.study is the size of the cosines: Bestgen et al.
re-port mean cosines of 0.120 and 0.137 for wantand omdat, respectively, whereas in our study wefound mean cosines of 0.045 and 0.039, respec-tively.
This suggests that our data sample and ex-perimental setup differ substantially from the workof Bestgen et al.
and we did not succeed in re-producing their experiment.
In our analysis thesemantic relationship between Q and P is muchweaker.In order to be able to interpret these results, weadded a baseline experiment.
Here we ran an LSAexperiment with segments composed of randomwords of the exact same size for the omdat andwant sentences.
For omdat this gave us a mean co-sine similarity of 0.007 and for want 0.006.
Thisimplies that the cosines we found are significantlyhigher than comparing random strings of words.Note that the analysis was carried out on a suffi-ciently large corpus and sufficient numbers of oc-currences of want and omdat.
Moreover, the resultthat semantic relationship is stronger in want thanin omdat is corroborated by our n-gram analysis.One possible explanation of the results of the n-gram analysis is the syntactic difference betweenwant and omdat sentences.
In want sentences theword order of Q and P is the same while for om-dat the verb-predicate order is swapped.
The n-grams will pick up this difference.
As a next stepwe plan to run the n-gram analysis with alphabeti-cally ordered n-grams to exclude the effect of thissyntactic difference2.Another line of future research is to make genrecomparisons.
The availability of the SoNaR cor-pus makes it possible to investigate the subjectiv-ity hypothesis for different text genres.Finally we intend to follow up our analysiswith a machine learning experiment to investigatewhether a learner could distinguish a want sen-tence from a omdat sentence by looking at a localcontext window of words to automatically predictwant or omdat.2We wish to thank one of our anonymous reviewers forbringing this suggestion to our attention.31ReferencesYves Bestgen, Liesbeth Degand, and Wilbert Spooren.2006.
Toward automatic determination of the se-mantics of connectives in large newspaper corpora.Discourse Processes, 41(2):175?193.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Graeme Hirst and Olga Feiguina.
2007.
Bigramsof syntactic labels for authorship discrimination ofshort texts.
Literary and Linguistic Computing,22(4):405?417.Thomas K Landauer, Peter W Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic anal-ysis.
Discourse processes, 25(2-3):259?284.C.-Y.
Lin and E.H. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statistics.In Proceedings of the Human Language Technol-ogy Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL), pages 71 ?
78, Edmonton, Canada.Max Louwerse, Nick Benesh, and Bin Zhang, 2008.Directions in Empirical Literary Studies: In honorof Willie van Peer, chapter Computationally discrim-inating literary from non-literary texts, pages 175?191.
John Benjamins Publishing.Nelleke Oostdijk, Martin Reynaert, V?eronique Hoste,and Ineke Schuurman.
2013.
The construction of a500-million-word reference corpus of contemporarywritten dutch.
In Essential Speech and LanguageTechnology for Dutch, pages 219?247.
Springer.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
pages 311?318.R.
Popping.
2000.
Computer-assisted text analysis.Sage, London.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA.T.J.M.
Sanders and W.P.M.S.
Spooren.
2013.
Excep-tions to rules: a qualitative analysis of backwardcausal connectives in Dutch naturalistic discourse.Text & Talk, 33(3):399?420.Maarten van Gompel, 2014.
Colibri Documentation,Colibri Core 0.1.
Centre for Language Studies,Radboud University Nijmegen, The Netherlands.http://proycon.github.io/colibri-core/doc/.32
