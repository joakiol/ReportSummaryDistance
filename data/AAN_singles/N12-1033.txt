2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327?337,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsStylometric Analysis of Scientific ArticlesShane Bergsma, Matt Post, David YarowskyDepartment of Computer Science and Human Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21218, USAsbergsma@jhu.edu, post@cs.jhu.edu, yarowsky@cs.jhu.eduAbstractWe present an approach to automatically re-cover hidden attributes of scientific articles,such as whether the author is a native Englishspeaker, whether the author is a male or a fe-male, and whether the paper was published ina conference or workshop proceedings.
Wetrain classifiers to predict these attributes incomputational linguistics papers.
The classi-fiers perform well in this challenging domain,identifying non-native writing with 95% accu-racy (over a baseline of 67%).
We show thebenefits of using syntactic features in stylom-etry; syntax leads to significant improvementsover bag-of-words models on all three tasks,achieving 10% to 25% relative error reduction.We give a detailed analysis of which wordsand syntax most predict a particular attribute,and we show a strong correlation between ourpredictions and a paper?s number of citations.1 IntroductionStylometry aims to recover useful attributes of doc-uments from the style of the writing.
In some do-mains, statistical techniques have successfully de-duced author identity (Mosteller and Wallace, 1984),gender (Koppel et al, 2003), native language (Kop-pel et al, 2005), and even whether an author has de-mentia (Le et al, 2011).
Stylometric analysis is im-portant to marketers, analysts and social scientistsbecause it provides demographic data directly fromraw text.
There has been growing interest in apply-ing stylometry to the content generated by users ofInternet applications, e.g., detecting author ethnic-ity in social media (Eisenstein et al, 2011; Rao etNative/Non-nativeMale/FemaleConference/WorkshopStylometric Analysis of Scientific ArticlesAbstractWe present an approach to automatically re-cover hidden attributes of scientific articles,such as whether the author is a native En-glish speaker.
We train classifiers to predictthese attributes in computational linguisticspapers.
The classifiers perform well in thischallenging domain, identifying non-nativewriting with 95% accuracy (over a baselineof 67%), and outperforming reasonable base-lines on two other difficult tasks.
We show thebenefits of using syntactic features in stylom-etry; syntax leads to significant improvementsover bag-of-words models on all three tasks,achieving 10% to 25% relative error reduction.We give an insightful analysis of which wordsand syntax most predict a particular attribute,and we show a strong correlation between ourpredictions and a paper?s number of citations.1 IntroductionStylometry aims to recover useful attributes of doc-uments from the style of their writing.
In somedomains, statistical techniques have successfullydeduced author identities (Mosteller and Wallace,1984), gender (Koppel et al, 2003), native language(Koppel et al, 2005), and even whether an authorhas dementia (Le et al, 2011).
Stylometric analysisis important to marketers, analysts and social scien-tists because it provides demographic data directlyfrom raw text.
There has been growing interest inapplying stylometry in Web 2.0 applications, e.g.,detecting the ethnicity of Twitter users (Eisensteinet al, 2011; Rao et al, 2011), or whether a person iswriting deceptive online reviews (Ott et al, 2011).We evaluate stylometric techniques in the noveldomain of scientific writing.
Science is a diffi-cult domain; authors are compelled, often explic-itly by reviewers/submission guidelines, to comply!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2!
"#$%&'"()* +,-$#.).
%/ !
*)',")0* +(")*$'.+1."(-*"!"
#$"%"&' (& (##$)(*+ ') (,')-('.
*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*.
"&'.6* ($'.
*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !"
'$(.& */(%%.6"$% ') #$"3.
*''+"%" (''$.4,'"% .& *)-#,'('.
)&(/ /.&:,.%'.
*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.
'.&: 8.
'+ >?
@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !"
%+)8 '+"4"&"6'% )5 ,%.&: !
"#$%&$'& ()%$*+)!
.& %'0/)-1"'$0E %0&'(F /"(3% ') %.
:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+.
"2.&: GH@ ') I?
@ $"/('.2" "$$)$ $"3,*'.)&<!"
:.2" (& .&%.
:+'5,/ (&(/0%.% )5 8+.
*+ 8)$3%(&3 %0&'(F -)%' #$"3.
*' ( #($'.
*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.
)& 4"'8""& ),$#$"3.*'.
)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.
)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.
'.&:< L& %)-"3)-(.&%7 %'('.%'.
*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'.
"% AN)%'"//"$ (&3 !
(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.
( AS" "' (/<7 IHGGD< K'0/)-"'$.
* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.
(/ %*.
"&1'.%'% 4"*(,%" .'
#$)2.3"% 3"-):$(#+.
* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !
"4 I<H (##/.*('.
)&%7 "<:<73"'"*'.&: '+" "'+&.*.
'0 )5 =8.
''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.
'.&: 3"*"#'.2" )&/.&" $"2.
"8% AU'' "' (/<7 IHGGD<!"
"2(/,('" %'0/)-"'$.
* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*.
"&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.
'/0 40 $"2."8"$%V%,4-.%%.
)& :,.3"/.&"%7 ') *)-#/0!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2!
"#$%&'"()* +,-$#.).
%/ !
*)',")0* +(")*$'.+1."(-*"!"
#$"%"&' (& (##$)(*+ ') (,')-('.
*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*.
"&'.6* ($'.
*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !"
'$(.& */(%%.6"$% ') #$"3.
*''+"%" (''$.4,'"% .& *)-#,'('.
)&(/ /.&:,.%'.
*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.
'.&: 8.
'+ >?
@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !"
%+)8 '+"4"&"6'% )5 ,%.&: !
"#$%&$'& ()%$*+)!
.& %'0/)-1"'$0E %0&'(F /"(3% ') %.
:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+.
"2.&: GH@ ') I?
@ $"/('.2" "$$)$ $"3,*'.)&<!"
:.2" (& .&%.
:+'5,/ (&(/0%.% )5 8+.
*+ 8)$3%(&3 %0&'(F -)%' #$"3.
*' ( #($'.
*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.
)& 4"'8""& ),$#$"3.*'.
)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.
)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.
'.&:< L& %)-"3)-(.&%7 %'('.%'.
*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'.
"% AN)%'"//"$ (&3 !
(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.
( AS" "' (/<7 IHGGD< K'0/)-"'$.
* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.
(/ %*.
"&1'.%'% 4"*(,%" .'
#$)2.3"% 3"-):$(#+.
* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !
"4 I<H (##/.*('.
)&%7 "<:<73"'"*'.&: '+" "'+&.*.
'0 )5 =8.
''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.
'.&: 3"*"#'.2" )&/.&" $"2.
"8% AU'' "' (/<7 IHGGD<!"
"2(/,('" %'0/)-"'$.
* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*.
"&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.
'/0 40 $"2."8"$%V%,4-.%%.
)& :,.3"/.&"%7 ') *)-#/0!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2!
"#$%&'"()* +,-$#.).
%/ !
*)',")0* +(")*$'.+1."(-*"!"
#$"%"&' (& (##$)(*+ ') (,')-('.
*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*.
"&'.6* ($'.
*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !"
'$(.& */(%%.6"$% ') #$"3.
*''+"%" (''$.4,'"% .& *)-#,'('.
)&(/ /.&:,.%'.
*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.
'.&: 8.
'+ >?
@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !"
%+)8 '+"4"&"6'% )5 ,%.&: !
"#$%&$'& ()%$*+)!
.& %'0/)-1"'$0E %0&'(F /"(3% ') %.
:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+.
"2.&: GH@ ') I?
@ $"/('.2" "$$)$ $"3,*'.)&<!"
:.2" (& .&%.
:+'5,/ (&(/0%.% )5 8+.
*+ 8)$3%(&3 %0&'(F -)%' #$"3.
*' ( #($'.
*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.
)& 4"'8""& ),$#$"3.*'.
)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.
)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.
'.&:< L& %)-"3)-(.&%7 %'('.%'.
*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'.
"% AN)%'"//"$ (&3 !
(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.
( AS" "' (/<7 IHGGD< K'0/)-"'$.
* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.
(/ %*.
"&1'.%'% 4"*(,%" .'
#$)2.3"% 3"-):$(#+.
* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !
"4 I<H (##/.*('.
)&%7 "<:<73"'"*'.&: '+" "'+&.*.
'0 )5 =8.
''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.
'.&: 3"*"#'.2" )&/.&" $"2.
"8% AU'' "' (/<7 IHGGD<!"
"2(/,('" %'0/)-"'$.
* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*.
"&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.
'/0 40 $"2."8"$%V%,4-.%%.
)& :,.3"/.&"%7 ') *)-#/0!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2!
"#$%&'"()* +,-$#.).
%/ !
*)',")0* +(")*$'.+1."(-*"!"
#$"%"&' (& (##$)(*+ ') (,')-('.
*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*.
"&'.6* ($'.
*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !"
'$(.& */(%%.6"$% ') #$"3.
*''+"%" (''$.4,'"% .& *)-#,'('.
)&(/ /.&:,.%'.
*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.
'.&: 8.
'+ >?
@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"%)&'8))'+"$3.56*,/' '(%;%< !
"%+)8 '+"4"&"6'% )5 ,%.&: !
"#$%&$'& ()%$*+)!
.& %'0/)-1"'$0E %0&'(F /"(3% ') %.
:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+.
"2.&:GH@ ')I?@$"/('.2""$$)$$"3,*'.)&<!"
:.2" (& .&%.
:+'5,/ (&(/0%.% )58+.
*+8)$3%(&3 %0&'(F-)%' #$"3.
*' ( #($'.
*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.
)&4"'8""& ),$#$"3.*'.
)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.
)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.
'.&:< L& %)-"3)-(.&%7 %'('.%'.
*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'.
"% AN)%'"//"$ (&3 !
(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.
( AS" "' (/<7 IHGGD< K'0/)-"'$.
* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.
(/ %*.
"&1'.%'% 4"*(,%" .'
#$)2.3"% 3"-):$(#+.
* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !
"4 I<H (##/.*('.
)&%7 "<:<73"'"*'.&: '+" "'+&.*.
'0 )5 =8.
''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.
'.&: 3"*"#'.2" )&/.&" $"2.
"8% AU'' "' (/<7 IHGGD<!"
"2(/,('" %'0/)-"'$.
* '"*+&.M,"% .& '+" &)2"/3)-(.& )5!&')#$',& -+'$'#.< K*.
"&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.
'/0 40 $"2."8"$%V%,4-.%%.
)& :,.3"/.&"%7 ') *)-#/0 !"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2W.
:,$"GX Y$"3.
*'.&: +.33"&(''$.4,'"% .&%*."&'.6*($'.*/"%8.
'+ &)$-('.2" #$(*'.
*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.
*(/ */,"% ($" /"%% %(/.
"&' '+(& .& 3)1-(.&% /.
;" %)*.
(/ -"3.
(< Z"' %*.
"&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .'
.% (& .-#)$1'(&' ($"( .& .
'%"/5< K0%'"-% 5)$ %*.
"&'.6* %'0/)-"'$08),/3 :.2" %)*.
)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.
* *)--,&.'.
"%7 (&3 &"88(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.
)& .& %#"*.6* ($'.
*/"% A])+$. "'
(/<7IHGGD< ^,'+)$% -.
:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 .`$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !
"#$%&'"()* 8-.9.:!"
#$"3.
*' 8+"'+"$ (#(#"$ .% 8$.
''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ (8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.
* (&3 4.4/.)-"'$.
* #$"3.*'.
)&<6'7 !
"#$%&'"()* ;'-"5('.:!"
%+)8 '+" 2(/,")5!
"#$%&$'& ()%$*+)!5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4"$+)) !
*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.
*+ +(2" &)' #$"2.
),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*.
"&'7 (&3 #($1'.
*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.
'.&:<!+./" $"*"&' %',3.
"% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.
)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":.
"%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.
'+( *)--)& .&5$(%'$,*',$"< L& (33.'.
)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.
:,$" GX Y$"3.
*'.&: +.33"& (''$.4,'"% .& %*.
"&'.6* ($'.*/"%8.
'+ &)$-('.2" #$(*'.
*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.
*(/ */,"% ($" /"%% %(/.
"&' '+(& .& 3)1-(.&% /.
;" %)*.
(/ -"3.
(< Z"' %*.
"&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .'
.% (& .-#)$1'(&' ($"( .& .
'%"/5< K0%'"-% 5)$ %*.
"&'.6* %'0/)-"'$08),/3 :.2" %)*.
)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.
* *)--,&.'.
"%7 (&3 &"88(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.
)& .& %#"*.6* ($'.
*/"% A])+$. "'
(/<7IHGGD< ^,'+)$% -.
:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !
"#$%&'"()* 8-.9.: !"
#$"3.
*' 8+"'+"$ (#(#"$ .% 8$.
''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.
* (&3 4.4/.)-"'$.
* #$"3.*'.
)&<6'7 !
"#$%&'"()* ;'-"5('.
: !"
%+)8 '+" 2(/,")5 !
"#$%&$'& ()%$*+)!
5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !
*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.
*+ +(2" &)' #$"2.
),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*.
"&'7 (&3 #($1'.
*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.
'.&:<!+./" $"*"&' %',3.
"% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.
)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":.
"%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.
'+( *)--)& .&5$(%'$,*',$"< L& (33.'.
)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.
:,$" GX Y$"3.
*'.&: +.33"& (''$.4,'"% .& %*.
"&'.6* ($'.*/"%8.
'+ &)$-('.2" #$(*'.
*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.
*(/ */,"% ($" /"%% %(/.
"&' '+(& .& 3)1-(.&% /.
;" %)*.
(/ -"3.
(< Z"' %*.
"&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .'
.% (& .-#)$1'(&' ($"( .& .
'%"/5< K0%'"-% 5)$ %*.
"&'.6* %'0/)-"'$08),/3 :.2" %)*.
)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.
* *)--,&.'.
"%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.
)& .& %#"*.6* ($'.
*/"% A])+$. "'
(/<7IHGGD< ^,'+)$% -.
:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !
"#$%&'"()* 8-.9.: !"
#$"3.
*' 8+"'+"$ (#(#"$ .% 8$.
''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.
* (&3 4.4/.)-"'$.
* #$"3.*'.
)&<6'7 !
"#$%&'"()* ;'-"5('.
: !"
%+)8 '+" 2(/,")5 !
"#$%&$'& ()%$*+)!
5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !
*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.
*+ +(2" &)' #$"2.
),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*.
"&'7 (&3 #($1'.
*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.
'.&:<!+./" $"*"&' %',3.
"% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.
)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":.
"%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.
'+( *)--)& .&5$(%'$,*',$"< L& (33.'.
)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.
:,$" GX Y$"3.
*'.&: +.33"& (''$.4,'"% .& %*.
"&'.6* ($'.*/"%8.
'+ &)$-('.2" #$(*'.
*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.
*(/ */,"% ($" /"%% %(/.
"&' '+(& .& 3)1-(.&% /.
;" %)*.
(/ -"3.
(< Z"' %*.
"&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .'
.% (& .-#)$1'(&' ($"( .& .
'%"/5< K0%'"-% 5)$ %*.
"&'.6* %'0/)-"'$08),/3 :.2" %)*.
)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.
* *)--,&.'.
"%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.
)& .& %#"*.6* ($'.
*/"% A])+$. "'
(/<7IHGGD< ^,'+)$% -.
:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !
"#$%&'"()* 8-.9.: !"
#$"3.
*' 8+"'+"$ (#(#"$ .% 8$.
''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.
* (&3 4.4/.)-"'$.
* #$"3.*'.
)&<6'7 !
"#$%&'"()* ;'-"5('.
: !"
%+)8 '+" 2(/,")5 !
"#$%&$'& ()%$*+)!
5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !
*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.
*+ +(2" &)' #$"2.
),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*.
"&'7 (&3 #($1'.
*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.
'.&:<!+./" $"*"&' %',3.
"% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.
)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":.
"%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.
'+( *)--)& .&5$(%'$,*',$"< L& (33.'.
)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'Figure 1: Predicting hidden attributes in scientific articleswith normative practices in spelling and grammar.Moreover, topical clues are less salient than in do-mains like social media.
Yet science is more thanjust a good challenge for stylometry; it is an impor-tant area in itself.
Systems for scientific stylometrywould give sociologists new tools for analyzing aca-demic communities, and new ways to resolve the na-ture of collaboration in specific articles (Johri et al,2011).
Authors might also use these tools, e.g.
tohelp ensure a consistent style in multi-authored pa-pers (Glover and Hirst, 1995).
Our work includes:New Stylometric Tasks: We predict whether apaper is written: (1) by a native or non-native En-glish speaker, (2) by a male or female, and (3) in thestyle of a conference or a workshop paper.
The latteris a novel stylometric and bibliometric prediction.New Stylometric Features: We show the valueof syntactic features for stylometry.
Among others,we describe tree substitution grammar fragments,which have not previously been used in stylometry.TSG fragments are interpretable, efficient, and par-ticularly effective for detecting non-native writing.While recent studies have mostly evaluated sin-gle prediction tasks, we compare different strategiesacross different tasks on a common dataset and witha common infrastructure.
In addition to contrastingdifferent feature types, we also compare differentFigure 1: Predicting hidden attributes in scientific articlesal., 2011), or whether someone is writing deceptiveonline reviews (Ott et al, 2011).We evaluate stylometric techniques in the noveldomain of scientific writing.
Science is a difficultdomain; authors are encouraged, often explicitlyby reviewers/submission-guidelines, to comply withnormative practices in style, spelling and grammar.Moreover, topical clues are less salient than in do-mains like social media.
Success in this challengingdomain can bring us closer to correctly analyzingthe huge volumes of online text that are currentlyunmarked for useful author attributes such as genderand native-language.Yet science is more than just a good stepping-stone for stylometry; it is an important area in itself.Systems for scientific stylometry would give sociol-ogists new tools for analyzing academic communi-ties, and new ways to resolve the nature of collab-oration in specific articles (Johri et al, 2011).
Au-thors might also use these tools, e.g., to help ensurea consistent style in multi-authored papers (Gloverand Hirst, 1995), or to determine sections of a paperneeding revision.327The contributions of our paper include:New Stylometric Tasks: We predict whethera paper is written: (1) by a native or non-nativespeaker, (2) by a male or female, and (3) in the styleof a conference or workshop paper.
The latter is afully novel stylometric and bibliometric prediction.New Stylometric Features: We show the valueof syntactic features for stylometry.
Among others,we describe tree substitution grammar fragments,which have not previously been used in stylometry.TSG fragments are interpretable, efficient, and par-ticularly effective for detecting non-native writing.While recent studies have mostly evaluated sin-gle prediction tasks, we compare different strategiesacross different tasks on a common dataset and witha common infrastructure.
In addition to contrastingdifferent feature types, we compare different train-ing strategies, exploring ways to make use of train-ing instances with label uncertainty.We also provide a detailed analysis that is inter-esting from a sociolinguistic standpoint.
Preciselywhat words distinguish non-native writing?
Howdoes the syntax of female authors differ from males?What are the hallmarks of top-tier papers?
Finally,we identify some strong correlations between ourpredictions and a paper?s citation count, even whencontrolling for paper venue and origin.2 Related WorkBibliometrics is the empirical analysis of scholarlyliterature; citation analysis is a well-known bib-liometric approach for ranking authors and papers(Borgman and Furner, 2001).
Bibliometry and sty-lometry can share goals but differ in techniques.For example, in a work questioning the blindnessof double-blind reviewing, Hill and Provost (2003)predict author identities.
They ignore the articlebody and instead consider (a) potential self-citationsand (b) similarity between the article?s citation listand the citation lists of known papers.
Radev et al(2009a) perform a bibliometric analysis of compu-tational linguistics.
Teufel and Moens (2002) andQazvinian and Radev (2008) summarize scientificarticles, the latter by automatically finding and fil-tering sentences in other papers that cite the targetarticle.Our system does not consider citations; it is mostsimilar to work that uses raw article text.
Hall etal.
(2008) build per-year topic models over scientificliterature to track the evolution of scientific ideas.Gerrish and Blei (2010) assess the influence of indi-vidual articles by modeling their impact on the con-tent of future papers.
Yogatama et al (2011) pre-dict whether a paper will be cited based on both itscontent and its meta-data such as author names andpublication venues.
Johri et al (2011) use per-authortopic models to assess the nature of collaboration ina particular article (e.g., apprenticeship or synergy).One of the tasks in Sarawgi et al (2011) concernedpredicting gender in scientific writing, but they use acorpus of only ten ?highly established?
authors andmake the prediction using twenty papers for each.Finally, Dale and Kilgarriff (2010) initiated a sharedtask on automatic editing of scientific papers writtenby non-native speakers, with the objective of devel-oping ?tools which can help non-native speakers ofEnglish (NNSs) (and maybe some native ones) writeacademic English prose of the kind that helps a pa-per get accepted.
?Lexical and pragmatic choices in academic writ-ing have also been analyzed within the applied lin-guistics community (Myers, 1989; Vassileva, 1998).3 ACL Dataset and PreprocessingWe use papers from the ACL Anthology Network(Radev et al, 2009b, Release 2011) and exploit itsmanually-curated meta-data such as normalized au-thor names, affiliations (including country, avail-able up to 2009), and citation counts.
We con-vert each PDF to text1 but remove text before theAbstract (to anonymize) and after the Acknowledg-ments/References headings.
We split the text intosentences2 and filter any documents with fewer than100 (this removes some short/demo papers, mal-converted PDFs, etc.
?
about 23% of the 13K pa-pers with affiliation information).
In case the textwas garbled, we then filtered the first 3 lines fromevery file and any line with an ?@?
symbol (whichmight be part of an affiliation).
We remove foot-ers like Proceedings of ..., table/figure captions, andany lines with non-ASCII characters (e.g.
mathequations).
Papers are then parsed via the Berke-1Via the open-source utility pdftotext2Splitter from cogcomp.cs.illinois.edu/page/tools328Task Training Set: Dev TestStrict Lenient Set SetNativeL 2127 3963 450 477Venue 2484 3991 400 421Gender 2125 3497 400 409Table 1: Number of documents for each taskley parser (Petrov et al, 2006), and part-of-speech(PoS) tagged using CRFTagger (Phan, 2006).Training sets always comprise papers from 2001-2007, while test sets are created by randomly shuf-fling the 2008-2009 portion and then dividing it intodevelopment/test sets.
We also use papers from1990-2000 for experiments in ?7.3 and ?7.4.4 Stylometric TasksEach task has both a Strict training set, using onlythe data for which we are most confident in the la-bels (as described below), and a Lenient set, whichforcibly assigns every paper in the training periodto some class (Table 1).
All test papers are anno-tated using a Strict rule.
While our approaches forautomatically-assigning labels can be coarse, theyallow us to scale our analysis to a realistic cross-section of academic papers, letting us discover someinteresting trends.4.1 NativeL: Native vs. Non-Native EnglishWe introduce the task of predicting whether a sci-entific paper is written by a native English speaker(NES) or non-native speaker (NNS).
Prior work hasmostly made this prediction in learner corpora (Kop-pel et al, 2005; Tsur and Rappoport, 2007; Wongand Dras, 2011), although there have been attemptsin elicited speech transcripts (Tomokiyo and Jones,2001) and e-mail (Estival et al, 2007).
There hasalso been a large body of work on correcting er-rors in non-native writing, with a specific focus ondifficulties in preposition and article usage (Han etal., 2006; Chodorow et al, 2007; Felice and Pul-man, 2007; Tetreault and Chodorow, 2008; Gamon,2010).We annotate papers using two pieces of associatedmeta-data: (1) author first names and (2) countriesof affiliation.
We manually marked each country forwhether English is predominantly spoken there.
Wethen built a list of common first names of Englishspeakers via the top 150 male and female namesfrom the U.S. census.3 If the first author of a pa-per has an English first name and English-speaking-country affiliation, we mark as NES.4 If none of theauthors have an English first name nor an English-speaking-country affiliation, we mark as NNS.
Weuse this rule to label our development and test data,as well as our Strict training set.
For Lenient train-ing, we decide based solely on whether the first au-thor is from an English-speaking country.4.2 Venue: Top-Tier vs. WorkshopThis novel task aims to distinguish top-tier papersfrom those at workshops, based on style.
We usethe annual meeting of the ACL as our canonical top-tier venue.
For evaluation and Strict training, we la-bel all main-session ACL papers as top-tier, and allworkshop papers as workshop.
For Lenient training,we assign all conferences (LREC, Coling, EMNLP,etc.)
to be top-tier except for their non-main-sessionpapers, which we label as workshop.4.3 Gender: Male vs. FemaleBecause we are classifying an international set ofauthors, U.S. census names (the usual source ofgender ground-truth) provide incomplete informa-tion.
We therefore use the data of Bergsma and Lin(2006).5 This data has been widely used in corefer-ence resolution but never in stylometry.
Each linein the data lists how often a noun co-occurs withmale, female, neutral and plural pronouns; this iscommonly taken as an approximation of the truegender distribution.
E.g., ?bill clinton?
is 98% male(in 8344 instances) while ?elsie wayne?
is 100% fe-male (in 23).
The data also has aggregate countsover all nouns with the same first token, e.g., ?elsie...?
is 94% female (in 255 instances).
For Stricttraining/evaluation, we label papers with the fol-lowing rule based on the first author?s first name:3www.census.gov/genealogy/names/names_files.html We also manually added common nicknames for these,e.g.
Rob for Robert, Chris for Christopher, Dan for Daniel, etc.4Of course, assuming the first author writes each paper isimperfect.
In fact, for some native/non-native collaborations,our system ultimately predicts the 2nd (non-native) author to bethe main writer; in one case we confirmed the accuracy of thisprediction by personal communication with the authors.5www.clsp.jhu.edu/?sbergsma/Gender/329if the name has an aggregate count >30 and fe-male probability >0.85, label as female; otherwiseif the aggregate count is >30 and male probabil-ity >0.85, label male.
This rule captures many ofACL?s unambiguously-gendered names, both male(Nathanael, Jens, Hiroyuki) and female (Widad,Yael, Sunita).
For Lenient training, we assign allpapers based only on whether the male or femaleprobability for the first author is higher.
While po-tentially noisy, there is precedent for assigning a sin-gle gender to papers ?co-authored by researchers ofmixed gender?
(Sarawgi et al, 2011).5 Models and Training StrategiesModel: We take a discriminative approach to sty-lometry, representing articles as feature vectors (?6)and classifying them using a linear, L2-regularizedSVM, trained via LIBLINEAR (Fan et al, 2008).SVMs are state-of-the-art and have been used pre-viously in stylometry (Koppel et al, 2005).Strategy: We test whether it?s better to train witha smaller, more accurate Strict set, or a larger butnoisier Lenient set.
We also explore a third strategy,motivated by work in learning from noisy web im-ages (Bergamo and Torresani, 2010), in which wefix the Strict labels, but also include the remainingexamples as unlabeled instances.
We then optimizea Transductive SVM, solving an optimization prob-lem where we not only choose the feature weights,but also labels for unlabeled training points.
Likea regular SVM, the goal is to maximize the marginbetween the positive and negative vectors, but nowthe vectors have both fixed and imputed labels.
Weoptimize using Joachims (1999)?s software.
Whilethe classifier is trained using a transductive strategy,it is still tested inductively, i.e., on unseen data.6 Stylometric FeaturesKoppel et al (2003) describes a range of featuresthat have been used in stylometry, ranging fromearly manual selection of potentially discriminativewords, to approaches based on automated text cat-egorization (Sebastiani, 2002).
We use the follow-ing three feature classes; the particular features werechosen based on development experiments.6.1 Bow FeaturesA variety of ?discouraging results?
in the text cate-gorization literature have shown that simple bag-of-words (Bow) representations usually perform betterthan ?more sophisticated?
ones (e.g.
using syntax)(Sebastiani, 2002).
This was also observed in sen-timent classification (Pang et al, 2002).
One keyaim of our research is to see whether this is true ofscientific stylometry.
Our Bow representation usesa feature for each unique lower-case word-type inan article.
We also preprocess papers by making alldigits ?0?.
Normalizing digits and filtering capital-ized words helps ensure citations and named-entitiesare excluded from our features.
The feature value isthe log-count of how often the corresponding wordoccurs in the document.6.2 Style FeaturesWhile text categorization relies on keywords, sty-lometry focuses on topic-independent measures likefunction word frequency (Mosteller and Wallace,1984), sentence length (Yule, 1939), and PoS (Hirstand Feiguina, 2007).
We define a style-word to be:(1) punctuation, (2) a stopword, or (3) a Latin abbre-viation.6 We create Style features for all unigramsand bigrams, replacing non-style-words separatelywith both PoS-tags and spelling signatures.7 Eachfeature is an N-gram, the value is its log-count in thearticle.
We also include stylistic meta-features suchas mean-words-per-sentence and mean-word-length.6.3 Syntax FeaturesUnlike recent work using generative PCFGs (Ragha-van et al, 2010; Sarawgi et al, 2011), we use syntaxdirectly as features in discriminative models, whichcan easily incorporate arbitrary and overlapping syn-tactic clues.
For example, we will see that one indi-cator of native text is the use of certain determin-ers as stand-alone noun phrases (NPs), like this inFigure 2.
This contrasts with a proposed non-nativephrase, ?this/DT growing/VBG area/NN,?
where thisinstead modifies a noun.
The Bow features areclearly unhelpful: this occurs in both cases.
The6The stopword list is the standard set of 524 SMART-systemstopwords (following Tomokiyo and Jones (2001)).
Latin ab-breviations are i.e., e.g., etc., c.f., et or al.7E.g., signature ?LC-ing?
means lower-case, ending in ing.These are created via a script included with the Berkeley parser.330we did this using .
.
.PRP VBD DT VBGNPNPVP.
.
.Figure 2: Motivating deeper syntactic features: Theshaded TSG fragment indicates native English, but is notdirectly encoded in Bow, Style, nor standard CFG-rules.Style features are likewise unhelpful; this-VBG alsooccurs in both cases.
We need the deeper knowledgethat a specific determiner is used as a complete NP.We evaluate three feature types that aim to cap-ture such knowledge.
In each case, we aggregate thefeature counts over all the parse trees constituting adocument.
The feature value is the log-count of howoften each feature occurs.
To remove content infor-mation from the features, we preprocess the parsetree terminals: all non-style-word terminals are re-placed with their spelling signature (see ?6.2).CFG Rules: We include a feature for every unique,single-level context-free-grammar (CFG) rule appli-cation in a paper (following Baayen et al (1996),Gamon (2004), Hirst and Feiguina (2007), Wongand Dras (2011)).
The Figure 2 tree would havefeatures: NP?PRP, NP?DT, DT?this, etc.
Such fea-tures do capture that a determiner was used as an NP,but they do not jointly encode which determiner wasused.
This is an important omission; we?ll see thatother determiners acting as stand-alone NPs indicatenon-native writing (e.g., the word that, see ?7.2).TSG Fragments: A tree-substitution grammar is ageneralization of CFGs that allow rewriting to treefragments rather than sequences of non-terminals(Joshi and Schabes, 1997).
Figure 2 gives the exam-ple NP?
(DT this).
This fragment captures both theidentity of the determiner and its syntactic functionas an NP, as desired.
Efficient Bayesian procedureshave recently been developed that enable the train-ing of large-scale probabilistic TSG grammars (Postand Gildea, 2009; Cohn et al, 2010).While TSGs have not been used previously in sty-lometry, Post (2011) uses them to predict sentencegrammaticality (i.e.
detecting pseudo-sentences fol-lowing Okanohara and Tsujii (2007) and Cherry andQuirk (2008)).
We use Post?s TSG training settingsand his public code.8 We parse with the TSG gram-mar and extract the fragments as features.
We alsofollow Post by having features for aggregate TSGstatistics, e.g., how many fragments are of a givensize, tree-depth, etc.
These syntactic meta-featuresare somewhat similar to the manually-defined stylo-metric features of Stamatatos et al (2001).C&J Reranking Features: We also extracted thereranking features of Charniak and Johnson (2005).These features were hand-crafted for reranking theoutput of a parser, but have recently been used forother NLP tasks (Post, 2011; Wong and Dras, 2011).They include lexicalized features for sub-trees andhead-to-head dependencies, and aggregate featuresfor conjunct parallelism and the degree of right-branching.
We get the features using another scriptfrom Post.9 While TSG fragments tile a parse treeinto a few useful fragments, C&J features can pro-duce thousands of features per sentence, and are thusmuch more computationally-demanding.7 Experiments and ResultsWe take the minority class as the positive class:NES for NativeL, top-tier for Venue and female forGender, and calculate the precision/recall of theseclasses.
We tune three hyperparameters for F1-score on development data: (1) the SVM regular-ization parameter, (2) the threshold for classifyingan instance as positive (using the signed hyperplane-distance as the score), and (3) for transductive train-ing (?5), the fraction of unlabeled data to label aspositive.
Statistical significance on held-out test datais assessed with McNemar?s test, p<0.05.
For F1-score, we use the following reasonable Baseline: welabel all instances with the label of the minority class(achieving 100% recall but low precision).7.1 Selection of Syntax and Training StrategyDevelopment experiments showed that using all fea-tures, Bow+Style+Syntax, works best on all tasks,but there was no benefit in combining different8http://github.com/mjpost/dptsg9http://github.com/mjpost/extract-spfeatures.331Syntax Strategy NativeL Venue GenderBaseline 50.5 45.0 28.7CFG Strict 93.5 59.9 42.5CFG Lenient 89.9 64.9 39.5TSG Strict 93.6 60.7 40.0TSG Lenient 90.9 64.4 39.1C&J Strict 90.5 62.3 37.1C&J Lenient 86.2 65.2 39.0Table 2: F1 scores for Bow+Style+Syntax system on de-velopment data: The best training strategy and the bestsyntactic features depend on the task.Syntax features.
We also found no gain from trans-ductive training, but greater cost, with more hyper-parameter tuning and a slower SVM solver.
Thebest Syntax features depend on the task (Table 2).Whether Strict or Lenient training: TSG was bestfor NativeL, C&J was best for Venue, and CFG wasbest for Gender.
These trends continue on test data,where TSG exceeds CFG (91.6% vs. 91.2%).
Forthe training strategy, Strict was best on NativeL andGender, while Lenient was best on Venue (Table 2).This latter result is interesting: recall that for Venue,Lenient training considers all conferences to be top-tier, but evaluation is just on detecting ACL papers.We suggest some reasons for this below, highlight-ing some general features of conference papers thatextend beyond particular venues.For the remainder of experiments on each task,we fix the syntactic features and training strategy tothose that performed best on development data.7.2 Test Results and Feature AnalysisGender remains the most difficult task on test data,but our F1 still substantially outperforms the base-line (Table 3).
Results on NativeL are particu-larly impressive; in terms of accuracy, we classify94.6% of test articles correctly (the majority-classbaseline is 66.9%).
Regarding features, just usingStyle+Syntax always works better than using Bow.Combining all features always works better still.The gains of Bow+Style+Syntax over vanilla Bow arestatistically significant in each case.We also highlight important individual features:NativeL: Table 4 gives Bow and Style featuresfor NativeL.
Some reflect differences in commonFeatures NativeL Venue GenderBaseline 49.8 45.5 33.1Bow 88.8 60.7 42.5Style 90.6 61.9 39.8Syntax 88.7 64.6 41.2Bow+Style 90.4 64.0 45.1Bow+Syntax 90.3 65.8 42.9Style+Syntax 89.4 65.5 43.3Bow+Style+Syntax 91.6 66.7 48.2Table 3: F1 scores with different features on held-out testdata: Including style and syntactic features is superior tostandard Bow features in all cases.native/non-native topics; e.g., ?probabilities?
pre-dicts native while ?morphological?
predicts non-native.
Several features, like ?obtained?, indicate L1interference; i.e., many non-natives have a cognatefor obtain in their native language and thus adopt theEnglish word.
As an example, the word obtainedoccurs 3.7 times per paper from Spanish-speakingareas (cognate obtenir) versus once per native paperand 0.8 times per German-authored paper.Natives also prefer certain abbreviations (e.g.?e.g.?)
while non-natives prefer others (?i.e.
?, ?c.f.?,?etc.?).
Exotic punctuation also suggests native text:the semi-colon, exclamation and question mark allpredict NES.
Note this also varies by region; semi-colons are most popular in NES countries but papersfrom Israel and Italy are close behind.Table 5 gives highly-weighted TSG features forpredicting NativeL.
Note the determiner-as-NP us-age described earlier (?
6.3): these, this and eachpredict native when used as an NP; that-as-an-NPpredicts non-native.
Furthermore, while not all na-tive speakers use a comma before a conjunction ina list, it?s nevertheless a good flag for native writ-ing (?NP?NP, NP, (CC and) NP?).
In terms of non-native syntax, the passive voice is more common(?VP?
(VBZ is) VP?
and ?VP?VBN (PP (IN as) NP)?
).We also looked for features involving determinerssince correct determiner usage is a common diffi-culty for non-native speakers.
We found cases wheredeterminers were missing where natives might haveused one (?NP?JJ JJ NN?
), but also those where a de-terminer might be optional and skipped by a nativespeaker (?NP?
(DT the) NN NNS?).
Note that Table 5332Predicts native Predicts non-nativeBow feature Wt.
Bow feature Wt.initial 2.25 obtained -2.15techniques 2.11 proposed -2.06probabilities 1.38 method -2.06additional 1.23 morphological -1.96fewer 1.02 languages -1.23Style feature Wt.
Style feature Wt.used to 1.92 , i.e.
-2.60JJR NN 1.90 have to -1.65has VBN 1.90 the xxxx-ing -1.61example , 1.75 thus -1.61all of 1.73 usually -1.24?s 1.69 mainly -1.21allow 1.47 , because -1.12has xxxx-ed 1.45 the VBN -1.12may be 1.35 JJ for -1.11; and 1.21 cf -0.97e.g.
1.10 etc.
-0.55must VB 0.99 associated to -0.23Table 4: NativeL: Examples of highly-weighted style andcontent features in the Bow+Style+Syntax system.examples are based on actual usage in ACL papers.We also found that complex NPs were more asso-ciated with native text.
Features such as ?NP?DT JJNN NN NN?, and ?NP?DT NN NN NNS?
predict nativewriting.Non-natives also rely more on boilerplate.
Forexample, the exact phrase ?The/This paper is orga-nized as follows?
occurs 3 times as often in non-native compared to native text (in 7.5% of all non-native papers).
Sentence re-use is only indirectlycaptured by our features; it would be interesting toencode flags for it directly.In general, we found very few highly-weightedfeatures that pinpoint ?ungrammatical?
non-nativewriting (the feature ?associated to?
in Table 4 is arare example).
Our classifiers largely detect non-native writing on a stylistic rather than grammaticalbasis.Venue: Table 6 provides important Bow and Stylefeatures for the Venue task (syntactic features omit-ted due to space).
While some features are topical(e.g.
?biomedical?
), the table gives a blueprint forwriting a solid main-conference paper.
That is, goodpapers often have an explicit probability model (oralgorithm), experimental baselines, error analysis,TSG Fragment ExamplePredicts native English author:NP?NNP CD (Model) (1)NP?
(DT these) six of (these)NP?
(DT that) NN in (that) (language)NP?
(DT this) we did (this) using ...VP?
(VBN used) S (used) (to describe it)NP?NP, NP, (CC and) NP (X), (Y), (and) (Z)NP?
(DT each) (each) consists of ...Predicts non-native English author:VP?
(VBZ is) VP it (is) (shown below)VP?VBN (PP (IN as) NP) (considered) (as) (a term)NP?JJ JJ NN in (other) (large) (corpus)NP?DT JJ (CD one) (a) (correct) (one)NP?
(DT the) NN NNS seen in (the) (test) (data)NP?
(DT that) larger than (that) of ...QP?
(IN about) CD (about) (200,000) wordsTable 5: NativeL: Highly-weighted syntactic features(descending order of absolute weight) and examples inthe Bow+Style+Syntax system.and statistical significance checking.
On the otherhand, there might be a bias at main conferences forfocused, incremental papers; features of workshoppapers highlight the exploration of ?interesting?
newideas/domains.
Here, the objective might only be toshow what is ?possible?
or what one is ?able to?
do.Main conference papers prefer work that improves?performance?
by ?#%?
on established tasks.Gender: The CFG features for Gender are givenin Table 7.
Several of the most highly-weightedfemale features include pronouns (e.g.
PRP$).
Ahigher frequency of pronouns in female writing hasbeen attested previously (Argamon et al, 2003), buthas not been traced to particular syntactic construc-tions.
Likewise, we observe a higher frequency ofnot just negation (noted previously) but adverbs (RB)in general (e.g.
?VP?MD RB VP?).
In terms of Bowfeatures (not shown), the words contrast and com-parison highly predict female, as do topical clueslike verb and resource.
The top-three male Bow fea-tures are (in order): simply, perform, parsing.7.3 Author RankingsWhile our objective is to predict attributes of pa-pers, we also show how that we can identify authorattributes using a larger body of work.
We makeNativeL and Gender predictions for all papers in the333Predicts ACL Predicts WorkshopBow feature Wt.
Bow feature Wt.model 2.64 semantic -2.16probability 1.66 analysis -1.65performance 1.40 verb -1.35baseline 1.36 lexical -1.33= 1.26 study -0.92algorithm 1.18 biomedical -0.87large 1.16 preliminary -0.69error 1.15 interesting -0.69outperforms 1.02 aim -0.64significant 0.96 manually -0.62statistically 0.75 appears -0.54Style feature Wt.
Style feature Wt.by VBG 1.04 able to -0.99#% 0.82 xxxx-ed out -0.77NN over 0.79 further NN -0.71than the 0.79 NN should -0.69improvement 0.75 will be -0.61best 0.71 possible -0.57xxxx-s by 0.70 have not -0.56much JJR 0.67 currently -0.56Table 6: Venue: Examples of highly-weighted style con-tent features in the Bow+Style+Syntax system.1990-2000 era using our Bow+Style+Syntax system.For each author+affiliation with ?3 first-authoredpapers, we take the average classifier score on thesepapers.Table 8 shows cases where our model stronglypredicts native, showing top authors with foreign af-filiations and top authors in English-speaking coun-tries.10 While not perfect, the predictions correctlyidentify some native authors that would be difficultto detect using only name and location data.
For ex-ample, Dekai Wu (Hong Kong) speaks English na-tively; Christer Samuelsson lists near-native Englishon his C.V.; etc.
Likewise, we have also been ableto accurately identify a set of non-native speakerswith common American names that were workingat American universities.Table 9 provides some of the extreme predictionsof our system on Gender.
The extreme male and fe-male predictions are based on both style and content;females tend to work on summarization, discourse,10Note again that this is based on the affiliation of these au-thors during the 1990s; e.g.
Gerald Penn published three paperswhile at the University of Tu?bingen.CFG Rule ExamplePredicts female author:NP?PRP$ NN NN (our) (upper) (bound)QP?RB CD (roughly) (6000)NP?NP, CC NP (a new NE tag), (or) (no NE tag)NP?PRP$ JJ JJ NN (our) (first) (new) (approach)VP?MD RB VP (may) (not) (be useful)ADVP?RB RBR (significantly) (more)Predicts male author:ADVP?RB RB (only) (superficially)NP?NP, SBAR we use (XYZ), (which is ...)S?S: S. (Trust me): (I?m a doctor)S?S, NP VP (To do so), (it) (needs help)WHNP?WP NN depending on (what) (path) is ...PP?IN PRN (in) ((Jelinek, 1976))Table 7: Gender: Highly-weighted syntactic features(descending order of weight) and examples in theBow+Style+Syntax system.Highest NES Scores, non-English-country: GeraldPenn,10 Ezra W. Black, Nigel Collier, Jean-Luc Gauvain,Dan Cristea, Graham J. Russell, Kenneth R. Beesley,Dekai Wu, Christer Samuelsson, Raquel MartinezHighest NES Scores, English-country: Eric V. Siegel,Lance A. Ramshaw, Stephanie Seneff, Victor W. Zue,Joshua Goodman, Patti J.
Price, Stuart M. Shieber, JeanCarletta, Lynn Lambert, Gina-Anne LevowTable 8: Authors scoring highest on NativeL, in descend-ing order, based exclusively on article text.etc., while many males focus on parsing.
We alsotried making these lists without Bow features, butthe extreme examples still reflect topic to some ex-tent.
Topics themselves have their own style, whichthe style features capture; it is difficult to fully sepa-rate style from topic.7.4 Correlation with CitationsWe also test whether our systems?
stylometric scorescorrelate with the most common bibliometric mea-sure: citation count.
To reduce the impact of topic,we only use Style+Syntax features.
We plot re-sults separately for ACL, Coling and Workshop pa-pers (1990-2000 era).
Papers at each venue aresorted by their classifier scores and binned into fivescore bins.
Each point in the plot is the mean-score/mean-number-of-citations for papers in a bin(within-community citation data is via the AAN ?3334Highest Model Scores (Male): John Aberdeen,Chao-Huang Chang, Giorgio Satta, Stanley F. Chen,GuoDong Zhou, Carl Weir, Akira Ushioda, HidekiTanaka, Koichi Takeda, Douglas B. Paul, Hideo Watan-abe, Adam L. Berger, Kevin Knight, Jason M. EisnerHighest Model Scores (Female): Julia B. Hirschberg,Johanna D. Moore, Judy L. Delin, Paola Merlo,Rebecca J. Passonneau, Bonnie Lynn Webber, BethM.
Sundheim, Jennifer Chu-Carroll, Ching-Long Yeh,Mary Ellen Okurowski, Erik-Jan Van Der LindenTable 9: Authors scoring highest (absolute values) onGender, in descending order, based exclusively on arti-cle text.and excludes self citations).
We use a truncatedmean for citation counts, leaving off the top/bottomfive papers in each bin.For NativeL, we only plot papers marked as na-tive by our Strict rule (i.e.
English name/country).Papers with the lowest NativeL-scores receive manyfewer citations, but they soon level off (Figure 3(a)).Many junior researchers at English universities arenon-native speakers; early-career non-natives mightreceive fewer citations than well-known peers.
Thecorrelation between citations and Venue-scores iseven stronger (Figure 3(b)); the top-ranked work-shop papers receive five times as many citationsas the lowest ones, and are cited better than agood portion of ACL papers.
These figures sug-gest that citation-predictors can get useful informa-tion beyond typical Bow features (Yogatama et al,2011).
Although we focused on a past era, stylis-tic/syntactic features should also be more robust tothe evolution of scientific topics; we plan to next testwhether we can better forecast future citations.
Itwould also be interesting to see whether these trendstransfer to other academic disciplines.7.5 Further Experiments on NativeLFor NativeL, we also created a special test corpus of273 papers written by first-time ACL authors (2008-2009 era).
This set closely aligns with the system?spotential use as a tool to help new authors composepapers.
Two (native-speaking) annotators manuallyannotated each paper for whether it was primarilywritten by a native or non-native speaker (consid-ering both content and author names/affiliations).The annotators agreed on 90% of decisions, with an1100.3  0.4  0.5  0.6  0.7  0.8NativeL-ScoreACLColingWorkshop(a)1100.2  0.3  0.4  0.5  0.6Venue-ScoreACLColingWorkshop(b)Figure 3: Correlation between predictions (x-axis) andmean number of citations (y-axis, log-scale).inter-annotator kappa of 66%.
We divided the papersinto a test set and a development set.
We applied ourBow+Style+Syntax system exactly as trained above,except we tuned its hyperparameters on the new de-velopment data.
The system performed quite wellon this set, reaching 68% F1 over a baseline of only27%.
Moreover, the system also reached 90% accu-racy, matching the level of human agreement.8 ConclusionWe have proposed, developed and successfully eval-uated significant new tasks and methods in the sty-lometric analysis of scientific articles, including thenovel resolution of publication venue based on pa-per style, and novel syntactic features based on treesubstitution grammar fragments.
In all cases, oursyntactic and stylistic features significantly improveover a bag-of-words baseline, achieving 10% to 25%relative error reduction in all three major tasks.
Wehave included a detailed and insightful analysis ofdiscriminative stylometric features, and we showeda strong correlation between our predictions and apaper?s number of citations.
We observed evidencefor L1-interference in non-native writing, for dif-ferences in topic between males and females, andfor distinctive language usage which can success-fully identify papers published in top-tier confer-ences versus wokrshop proceedings.
We believe thatthis work can stimulate new research at the intersec-tion of computational linguistics and bibliometrics.335ReferencesShlomo Argamon, Moshe Koppel, Jonathan Fine, andAnat Rachel Shimoni.
2003.
Gender, genre, and writ-ing style in formal written texts.
Text, 23(3), August.Harald Baayen, Fiona Tweedie, and Hans van Halteren.1996.
Outside the cave of shadows: Using syntacticannotation to enhance authorship attribution.
Literaryand Linguistic Computing, 11(3):121?132.Alessandro Bergamo and Lorenzo Torresani.
2010.
Ex-ploiting weakly-labeled web images to improve objectclassification: a domain adaptation approach.
In Proc.NIPS, pages 181?189.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proc.
Coling-ACL,pages 33?40.Christine L. Borgman and Jonathan Furner.
2001.
Schol-arly communication and bibliometrics.
Annual Reviewof Information Science and Technology, 36:3?72.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
ACL, pages 173?180.Colin Cherry and Chris Quirk.
2008.
Discriminative,syntactic language modeling through latent SVMs.
InProc.
AMTA.Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In Proc.
ACL-SIGSEM Workshop onPrepositions, pages 25?30.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
J. Mach.Learn.
Res., 11:3053?3096.Robert Dale and Adam Kilgarriff.
2010.
Helping ourown: Text massaging for computational linguistics asa new shared task.
In Proc.
6th International NaturalLanguage Generation Conference, pages 261?265.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proc.
ACL, pages 1365?1374.Dominique Estival, Tanja Gaustad, Son-Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor English emails.
In Proc.
PACLING, pages 263?272.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874.Rachele De Felice and Stephen G. Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProc.
ACL-SIGSEM Workshop on Prepositions, pages45?50.Michael Gamon.
2004.
Linguistic correlates of style:authorship classification with deep linguistic analysisfeatures.
In Proc.
Coling, pages 611?617.Michael Gamon.
2010.
Using mostly native data to cor-rect errors in learners?
writing: a meta-classifier ap-proach.
In Proc.
HLT-NAACL, pages 163?171.Sean Gerrish and David M. Blei.
2010.
A language-based approach to measuring scholarly impact.
InProc.
ICML, pages 375?382.Angela Glover and Graeme Hirst.
1995.
Detectingstylistic inconsistencies in collaborative writing.
InWriters at work: Professional writing in the comput-erized environment, pages 147?168.David Hall, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Studying the history of ideas using topicmodels.
In Proc.
EMNLP, pages 363?371.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage by non-native speakers.
Nat.
Lang.
Eng., 12(2):115?129.Shawndra Hill and Foster Provost.
2003.
The myth ofthe double-blind review?
: Author identification usingonly citations.
SIGKDD Explor.
Newsl., 5:179?184.Graeme Hirst and Ol?ga Feiguina.
2007.
Bigrams ofsyntactic labels for authorship discrimination of shorttexts.
Literary and Linguistic Computing, 22(4):405?417.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InProc.
ICML, pages 200?209.Nikhil Johri, Daniel Ramage, Daniel McFarland, andDaniel Jurafsky.
2011.
A study of academic collabo-rations in computational linguistics using a latent mix-ture of authors model.
In Proc.
5th ACL-HLT Work-shop on Language Technology for Cultural Heritage,Social Sciences, and Humanities, pages 124?132.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages: BeyondWords, volume 3, pages 71?122.Moshe Koppel, Shlomo Argamon, and Anat Rachel Shi-moni.
2003.
Automatically categorizing written textsby author gender.
Literary and Linguistic Computing,17(4):401?412.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proc.
KDD, pages 624?628.Xuan Le, Ian Lancashire, Graeme Hirst, and ReginaJokel.
2011.
Longitudinal detection of dementiathrough lexical and syntactic changes in writing: Acase study of three British novelists.
Literary and Lin-guistic Computing, 26(4):435?461.Frederick Mosteller and David L. Wallace.
1984.
Ap-plied Bayesian and Classical Inference: The Case ofthe Federalist Papers.
Springer-Verlag.Greg Myers.
1989.
The pragmatics of politeness in sci-entific articles.
Applied Linguistics, 10(1):1?35.336Daisuke Okanohara and Jun?ichi Tsujii.
2007.
Adiscriminative language model with pseudo-negativesamples.
In Proc.
ACL, pages 73?80.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-cock.
2011.
Finding deceptive opinion spam by anystretch of the imagination.
In Proc.
ACL, pages 309?319.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Proc.
EMNLP, pages79?86.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
Coling-ACL, pages433?440.Xuan-Hieu Phan.
2006.
CRFTagger: CRF English POSTagger.
crftagger.sourceforge.net.Matt Post and Daniel Gildea.
2009.
Bayesian learningof a tree substitution grammar.
In Proc.
ACL-IJCNLP,pages 45?48.Matt Post.
2011.
Judging grammaticality with tree sub-stitution grammar derivations.
In Proc.
ACL, pages217?222.Vahed Qazvinian and Dragomir R. Radev.
2008.
Scien-tific paper summarization using citation summary net-works.
In Proc.
Coling, pages 689?696.Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-son, and Pradeep Muthukrishnan.
2009a.
A biblio-metric and network analysis of the field of computa-tional linguistics.
Journal of the American Society forInformation Science and Technology.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009b.
The ACL anthology network cor-pus.
In Proc.
ACL Workshop on Natural LanguageProcessing and Information Retrieval for Digital Li-braries, pages 54?61.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proc.
ACL, pages38?42.Delip Rao, Michael Paul, Clay Fink, David Yarowsky,Timothy Oates, and Glen Coppersmith.
2011.
Hierar-chical bayesian models for latent attribute detection insocial media.
In Proc.
ICWSM, pages 598?601.Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.2011.
Gender attribution: tracing stylometric evidencebeyond topic and genre.
In Proc.
CoNLL, pages 78?86.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Comput.
Surv., 34:1?47.Efstathios Stamatatos, Nikos Fakotakis, and GeorgeKokkinakis.
2001.
Automatic text categorization interms of genre and author.
Computational Linguistics,26(4):471?495.Joel R. Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in ESL writ-ing.
In Proc.
Coling, pages 865?872.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles - experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Laura Mayfield Tomokiyo and Rosie Jones.
2001.You?re not from ?round here, are you?
Naive Bayesdetection of non-native utterances.
In Proc.
NAACL.Oren Tsur and Ari Rappoport.
2007.
Using classi-fier features for studying the effect of native languageon the choice of written second language words.
InProc.
Workshop on Cognitive Aspects of Computa-tional Language Acquisition, pages 9?16.Irena Vassileva.
1998. Who am I/who are we in aca-demic writing?
International Journal of Applied Lin-guistics, 8(2):163?185.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploitingparse structures for native language identification.
InProc.
EMNLP, pages 1600?1610.Dani Yogatama, Michael Heilman, Brendan O?Connor,Chris Dyer, Bryan R. Routledge, and Noah A. Smith.2011.
Predicting a scientific community?s response toan article.
In Proc.
EMNLP, pages 594?604.G.
Udny Yule.
1939.
On sentence-length as a statis-tical characteristic of style in prose: With applica-tion to two cases of disputed authorship.
Biometrika,30(3/4):363?390.337
