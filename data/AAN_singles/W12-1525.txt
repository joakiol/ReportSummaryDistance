INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 136?140,Utica, May 2012. c?2012 Association for Computational LinguisticsThe Surface Realisation Task: Recent Developments and Future PlansAnja BelzComputing, Engineering and MathsUniversity of BrightonBrighton BN1 4GJ, UKa.s.belz@brighton.ac.ukBernd BohnetInstitute for Natural Language ProcessingUniversity of Stuttgart70174 Stuttgartbohnet@ims.uni-stuttgart.deSimon Mille, Leo WannerInformation and Communication TechnologiesPompeu Fabra University08018 Barcelona<firstname>.<lastname>@upf.eduMichael WhiteDepartment of LinguisticsOhio State UniversityColumbus, OH, 43210, USmwhite@ling.osu.eduAbstractThe Surface Realisation Shared Task was firstrun in 2011.
Two common-ground input rep-resentations were developed and for the firsttime several independently developed surfacerealisers produced realisations from the sameshared inputs.
However, the input representa-tions had several shortcomings which we havebeen aiming to address in the time since.
Thispaper reports on our work to date on improv-ing the input representations and on our plansfor the next edition of the SR Task.
We alsobriefly summarise other related developmentsin NLG shared tasks and outline how the dif-ferent ideas may be usefully brought togetherin the future.1 IntroductionThe Surface Realisation (SR) Task was introducedas a new shared task at Generation Challenges 2011(Belz et al, 2011).
Our aim in developing the SRTask was to make it possible, for the first time, todirectly compare different, independently developedsurface realisers by developing a ?common-ground?representation that could be used by all participat-ing systems as input.
In fact, we created two dif-ferent input representations, one shallow, one deep,in order to enable more teams to participate.
Corre-spondingly, there were two tracks in SR?11: In theShallow Track, the task was to map from shallowsyntax-level input representations to realisations; inthe Deep Track, the task was to map from deepsemantics-level input representations to realisations.By the time teams submitted their system outputs,it had become clear that the inputs required by sometypes of surface realisers were more easily derivedfrom the common-ground representation than the in-puts required by other types.
There were other re-spects in which the representations were not ideal,e.g.
the deep representations retained too many syn-tactic elements as stopgaps where no deeper infor-mation had been available.
It was clear that the in-put representations had to be improved for the nextedition of the SR Task.
In this paper, we report onour work in this direction so far and relate it to somenew shared task proposals which have been devel-oped in part as a response to the above difficulties.We discuss how these developments might usefullybe integrated, and outline plans for SR?13, the nextedition of the SR Task.2 SR?11The SR?11 input representations were created bypost-processing the CoNLL 2008 Shared Taskdata (Surdeanu et al, 2008), for the preparation ofwhich selected sections of the WSJ Treebank wereconverted to syntactic dependencies with the Pen-nconverter (Johansson and Nugues, 2007).
Theresulting dependency bank was then merged withNombank (Meyers et al, 2004) and Propbank(Palmer et al, 2005).
Named entity informationfrom the BBN Entity Type corpus was also incorpo-rated.
The SR?11 shallow representation was basedon the Pennconverter dependencies, while the deeprepresentation was derived from the merged Nom-bank, Propbank and syntactic dependencies in a pro-136cess similar to the graph completion algorithm out-lined by Bohnet (2010).Five teams submitted a total of six systems toSR?11 which we evaluated automatically using arange of intrinsic metrics.
In addition, systems wereassessed by human judges in terms of Clarity, Read-ability and Meaning Similarity.The four top-performing systems were all statis-tical dependency realisers that do not make use ofan explicit, pre-existing grammar.
By design, statis-tical dependency realisers are robust and relativelyeasy to adapt to new kinds of dependency inputswhich made them well suited to the SR?11 Task.
Incontrast, there were only two systems that employeda grammar, either hand-crafted or treebank-derived,and these did not produce competitive results.
Bothteams reported substantial difficulties in convertingthe common ground inputs into the ?native?
inputsrequired by their systems.The SR?11 results report pointed towards twokinds of possible improvements: (i) introducing (ad-ditional) tasks where performance would not dependto the same extent on the relation between common-ground and native inputs, e.g.
a text-to-text sharedtask on sentential paraphrasing; and (ii) improvingthe representations themselves.
In the remainder ofthis paper we report on developments in both thesedirections.3 Towards SR?13As outlined above, the first SR Shared Task turnedup some interesting representational issues that re-quired some in-depth investigation.
In the end, itwas this fact that led to the decision to postponethe 2nd SR Shared Task until 2013 in order to al-low enough time to address these issues properly.
Inthis section, we describe our plans for SR?13 to theextent to which they have progressed.3.1 Task definitionAs in the first SR task, the participating teams willbe provided with annotated corpora consisting ofcommon-ground input representations and their cor-responding outputs.
Two kinds of input will be of-fered: deep representations and surface representa-tions.
The deep input representations will be se-mantic graphs; the surface representations syntactictrees.
Both will be derived from the Penn Treebank.The task will consist in the generation of a text start-ing from either of the input representations.3.2 Changes to the input representationsDuring the working group discussions which fol-lowed SR?11, it became apparent that the CoNLLsyntactic dependency trees overlaid with Prop-bank/Nombank relations had turned out to be inade-quate in various respects for the purpose of derivinga suitable semantic representation.
For instance:?
Governed prepositions are not distinguishedfrom semantically loaded prepositions in theCoNLL annotation.
In SR?11, only stronglygoverned prepositions such as give somethingTO someone were removed, but in many casesthe meaning of a preposition which introducesan argument (of a verb, a noun, an adjectiveor an adverb) clearly depends on the predicate:believe IN something, account FOR some-thing, etc.
In those cases, too, the prepositionshould be removed from the semantic annota-tion, since the relisers have to be able to intro-duce non-semantic features un-aided.
On thecontrary, semantically loaded governed prepo-sitions such as live IN a flat/ON a roof/NEXTTO the main street etc.
should be retained inthe annotation.
These prepositions all receiveargumental arcs in PropBank/NomBank, so itis not easy to distinguish between them.
Onepossibility would be to target a restricted list ofprepositions which are void of meaning most ofthe time, and remove those prepositions whenthey introduce arguments.?
The annotation of relative pronouns did notsurvive the conversion of the original PennTreebank to the CoNLL format unscathed: theantecedent of the relative pronoun is sometimeslost or the relative pronoun is not annotated,predominantly because the predicate which therelative pronoun is an argument of was not con-sidered to be a predicate by annotators, as inthe degree TO WHICH companies are irritated.However, in the original constituency annota-tion, the traces allow for retrieving antecedentsand semantic governors, hence using this orig-137inal annotation could be useful in order to get aclean annotation of such phenomena.Agreement has been reached on a range of other is-sues, although the feasibility of implementing thecorresponding changes might have to be furtherevaluated:?
Coordinations should be annotated in the se-mantic representation with the conjunction asthe head of all the conjuncts.
This treatmentwould allow e.g.
an adequate representation ofsharing of dependents among the conjuncts.?
The inversion of ?modifier?
arcs and the intro-duction of meta-semantemes would avoid an-ticipating syntactic decisions such as the direc-tion of non-argumental syntactic edges, and al-low for connecting unconnected parts of the se-mantic structures.?
In order to keep the scope of various phenom-ena intact after inverting non-argumental edges,we should explicitly mark the scope of e.g.negations, quantifiers, quotation marks etc.
asattribute values on the nodes.?
Control arcs should be removed from the se-mantic representation since they do not provideinformation relevant at that level.?
Named entities will be further specified addinga reduced set of named entity types from theBBN annotations.Finally, we will perform automatic and manual qual-ity checks in order to ensure that the proposedchanges are adequately introduced in the annotation.3.3 EvaluationWe will once again follow the main data set divi-sions of the CoNLL?08 data (training set = WSJ Sec-tions 02?21; development set = Section 24; test set =Section 23), with the proviso that we have removed300 randomly selected sentences from the develop-ment set for use in human evaluations.
Of these, weused 100 sentences in SR?11 and will use a different100 in SR?13.Evaluation criteria identified as important forevaluation of surface realisation output in previouswork include Adequacy (preservation of meaning),Fluency (grammaticality/idiomaticity), Clarity, Hu-manlikeness and Task Effectiveness.
We will aim toevaluate system outputs submitted by SR?13 partic-ipants in terms of most of these criteria, using bothautomatic and human-assessed methods.As in SR?11, the automatic evaluation metrics (as-sessing Humanlikeness) will be BLEU, NIST, TERand possibly METEOR.
We will apply text normal-isation to system outputs before scoring them withthe automatic metrics.
For n-best ranked systemoutputs, we will again compute a single score for alloutputs by computing their weighted sum of theirindividual scores, where a weight is assigned to asystem output in inverse proportion to its rank.
Fora subset of the test data we may obtain additional al-ternative realisations via Mechanical Turk for use inthe automatic evaluations.We are planning to expand the range of human-assessed evaluation experiments (assessing Ade-quacy, Fluency and Clarity) to the following meth-ods:1.
Preference Judgement Experiment (C2, C3):Collect preference judgements using an exist-ing evaluation interface (Kow and Belz, 2012)and directly recruited evaluators.
We willpresent sentences in the context of a chunk of5 consecutive sentences to the evaluators, andask for separate judgements for Clarity, Flu-ency and Meaning Similarity.2.
HTER (Snover et al, 2006): In this evaluationmethod, human evaluators are asked to post-edit the output of a system, and the edits arethen categorised and counted.
Crucial to thisevaluation method is the construction of clearinstructions for evaluators and the categorisa-tion of edits.
We will categorise edits as relat-ing to Meaning Similarity, Fluency and/or Clar-ity; we will also consider further subcategorisa-tions.We will once again provide evaluation scripts to par-ticipants so they can perform automatic evaluationson the development data.
These scores serve twopurposes.
Firstly, development data scores must beincluded in participants?
reports.
Secondly, partici-138pants may wish to use the evaluation scripts in de-veloping and tuning their systems.We will report per-system results separately forthe automatic metrics (4 sets of results), and for thehuman-assessed measures (2 sets of results).
Foreach set of results, we will report single-best andn-best results.
For single-best results, we may fur-thermore report results both with and without miss-ing outputs.
We will rank systems, and report sig-nificance of pairwise differences using bootstrap re-sampling where necessary (Koehn, 2004; Zhang andVogel, 2010).
We will separately report correlationbetween human and automatic metrics, and betweendifferent automatic metrics.3.4 Assessing different aspects of realisationseparatelyIn addition, we will consider measuring different as-pects of the realisation performance of participatingsystems (syntax, word order, morphology) since asystem can perform well on one and badly on an-other.
For instance, a system might perform wellon morphological realisation while it has poor re-sults on linearisation.
We would like to capture thisfact.
This may involve asking participating teams tosubmit intermediate representations or identifiers toidentify the reference words.
This more fine-grainedapproach should help us to obtain a more precisepicture of the state of affairs in the field and couldhelp to reveal the respective strengths of differentsurface realisers more clearly.4 Related Developments4.1 Syntactic Paraphrase RankingThe new shared task on syntactic paraphrase rankingdescribed elsewhere in this volume (White, 2012) isintended to run as a follow-on to the main surfacerealisation shared task.
Taking advantage of the hu-man judgements collected to evaluate the surface re-alisations produced by competing systems, the taskis to automatically rank the realisations that differfrom the reference sentence in a way that agrees withthe human judgements as often as possible.
The taskis designed to appeal to developers of surface real-isation systems as well as machine translation eval-uation metrics.
For surface realisation systems, thetask sidesteps the thorny issue of converting inputsto a common representation.
Developers of reali-sation systems that can generate and optionally rankmultiple outputs for a given input will be encouragedto participate in the task, which will test the system?sability to produce acceptable paraphrases and/or torank competing realisations.
For MT evaluationmetrics, the task provides a challenging frameworkfor advancing automatic evaluation, as many of theparaphrases are expected to be of high quality, dif-fering only in subtle syntactic choices.4.2 Content Selection ChallengeThe new shared task on content selection has beenput forward (Bouayad-Agha et al, 2012) to initi-ate work on content selection from a common, stan-dardised semantic-web format input, and thus pro-vide the context for an objective assessment of dif-ferent content selection strategies.
The task con-sists in selecting the contents communicated in ref-erence biographies of celebrities from a large vol-ume of RDF-triples.
The selected triples will beevaluated against a gold triple selection set usingstandard quality assessment metrics.The task can be considered complementary to thesurface realisation shared task in that it contributesto the medium-term goal of setting up a task thatcovers all stages of the generation pipeline.
In fu-ture challenges, it can be explored to what extent andhow the output content plans can be mapped ontosemantic representations that serve as input to thesurface realisers.5 PlansWe are currently working on the new improvedcommon-ground input representation scheme andconverting the data to the new scheme.The provisional schedule for SR?13 looks asfollows:Announcement and call for expres-sions of interest:6 July 2012Preliminary registration and releaseof description of new representations:27 July 2012Release of data and documentation: 2 Nov 2012System Submission Deadline: 10 May 2013Evaluation Period: 10 May?10 Jul 2013Provisional dates for results session: 8?9 Aug 20131396 ConclusionFor a large number of NLP applications (amongthem, e.g., text generation proper, summarisation,question answering, and dialogue), surface realisa-tion (SR) is a key technology.
Unfortunately, sofar in nearly all of these applications, idiosyncratic,custom-made SR implementations prevail.
How-ever, a look over the fence at the language analy-sis side shows that the broad use of standard de-pendency treebanks and semantically annotated re-sources such as PropBank and NomBank that werecreated especially with parsing in mind led to stan-dardised high-quality off-the-shelf parser implemen-tations.
It seems clear that in order to advance thefield of surface realisation, the generation commu-nity also needs adequate resources on which large-scale experiments can be run in search of the surfacerealiser with the best performance, a surface realiserwhich is commonly accepted, follows general trans-parent principles and is thus usable as plug-in in themajority of applications.The SR Shared Task aims to contribute to thisgoal.
On the one hand, it will lead to the creationof NLG-suitable resources in that it will convertthe PropBank into a more semantic and more com-pletely annotated resource.
On the other hand, it willoffer a forum for the presentation and evaluation ofvarious approaches to SR and thus help us to searchfor the best solution to the SR task with the greatestpotential to become a widely accepted off-the-shelftool.AcknowledgmentsWe gratefully acknowledge the contributions to dis-cussions and development of ideas made by theother members of the SR working group: MiguelBallesteros, Johan Bos, Aoife Cahill, Josef van Gen-abith, Pablo Gerva?s, Deirdre Hogan and AmandaStent.ReferencesAnja Belz, Michael White, Dominic Espinosa, DeidreHogan, Eric Kow, and Amanda Stent.
2011.
Thefirst surface realisation shared task: Overview andevaluation results.
In Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation(ENLG?11), pages 217?226.
Association for Compu-tational Linguistics.Bernd Bohnet, Leo Wanner, Simon Mille, and AliciaBurga.
2010.
Broad coverage multilingual deep sen-tence generation with a stochastic multi-level realizer.In Proceedings of the 23rd International Conferenceon Computational Linguistics, Beijing, China.Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner,and Chris Mellish.
2012.
Content selection fromsemantic web data.
In Proceedings of the 7th In-ternational Natural Language Generation Conference(INLG?12).Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InJoakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,and Mare Koit, editors, Proceedings of NODALIDA2007, pages 105?112, Tartu, Estonia.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Eric Kow and Anja Belz.
2012.
LG-Eval: A toolkitfor creating online language evaluation experiments.In Proceedings of the 8th International Conference onLanguage Resources and Evaluation (LREC?12).Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The NomBank project: An interimreport.
In NAACL/HLT Workshop Frontiers in CorpusAnnotation.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: A corpus annotated withsemantic roles.
In Computational Linguistics Journal,pages 71?105.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In InProceedings of Association for Machine Translation inthe Americas, pages 223?231.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL 2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning (CoNLL?08), Manchester, UK.Michael White.
2012.
Shared task proposal: Syntac-tic paraphrase ranking.
In Proceedings of the 7th In-ternational Natural Language Generation Conference(INLG?12).Ying Zhang and Stephan Vogel.
2010.
Significance testsof automatic machine translation evaluation metrics.Machine Translation, 24:51?65.140
