The Basque lexical-sample taskEneko Agirre, Itziar Aldabe, Mikel Lersundi, David Martinez, Eli Pociello, Larraitz Uria(*)IxA NLP group, Basque Country University649 pk.
20.080 Donostia, Spaineneko@si.ehu.esAbstractIn this paper we describe the Senseval 3Basque lexical sample task.
The taskcomprised 40 words (15 nouns, 15 verbs and10 adjectives) selected from the BasqueWordNet.
10 of the words were chosen incoordination with other lexical-sample tasks.The examples were taken from newspapers, anin-house balanced corpus and Internet texts.We additionally included a large set ofuntagged examples, and a lemmatised versionof the data including lemma, PoS and caseinformation.
The method used to hand-tag theexamples produced an inter-tagger agreementof 78.2% before arbitration.
The eightcompeting systems attained results well abovethe most frequent baseline and the best systemfrom Swarthmore College scored 70.4%recall.1 IntroductionThis paper reviews the Basque lexical-sample taskorganized for Senseval 3.
Each participant wasprovided with a relatively small set of labelledexamples (2/3 of 75+15*senses+7*multiwords)and a comparatively large set of unlabelledexamples (roughly ten times more when possible)for around 40 words.
The larger number ofunlabelled data was released with the purpose toenable the exploration of semi-supervised systems.The test set comprised 1/3 of the tagged examples.The sense inventory was taken from the BasqueWordNet, which is linked to WordNet version 1.6(Fellbaum, 1998).
The examples came mainly fromnewspaper texts, although we also used a balancedin-house corpus and texts from Internet.
The wordsselected for this task were coordinated with otherlexical-sample tasks (such as Catalan, English,Italian, Romanian and Spanish) in order to sharearound 10 of the target words.The following steps were taken in order to carryout the task:(*) Authors listed in alphabetic order.1.
set the exercisea.
choose sense inventory from a pre-existingresourceb.
choose target corporac.
choose target wordsd.
lemmatize the corpus automaticallye.
select examples from the corpus2.
hand-tagginga.
define the procedureb.
revise the sense inventoryc.
tagd.
analyze the inter-tagger agreemente.
arbitrateThis paper is organized as follows: Thefollowing section presents the setting of theexercise.
Section 3 reviews the hand-tagging, andSection 4 the details of the final release.
Section 5shows the results of the participant systems.Section 6 discusses some main issues and finally,Section 7 draws the conclusions.2 Setting of the exerciseIn this section we present the setting of theBasque lexical-sample exercise.2.1 BasqueAs Basque is an agglutinative language, thedictionary entry takes each of the elementsnecessary to form the different functions.
Morespecifically, the affixes corresponding to thedeterminant, number and declension case are takenin this order and independently of each other (deepmorphological structure).
For instance, ?etxekoariemaiozu?
can be roughly translated as ?
[to the onein the house] [give it]?
where the underlinedsequence of suffixes in Basque corresponds to ?tothe one in the?.2.2 Sense inventoryWe chose the Basque WordNet, linked toWordNet 1.6, for the sense inventory.
This way,the hand tagging enabled us to check the sensecoverage and overall quality of the BasqueWordNet, which is under construction.
The BasqueWordNet is available at http://ixa3.si.ehu.es/wei3.html.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systems2.3 Corpora usedBeing Basque a minority language it is not easyto find the required number of occurrences for eachword.
We wanted to have both balanced andnewspaper examples, but we also had to includetexts extracted from the web, specially for theuntagged corpus.
The procedure to find examplesfrom the web was the following: for each targetword all possible morphological declensions wereautomatically generated, searched in a search-engine, documents retrieved, automaticallylemmatized (Aduriz et al 2000), filtered usingsome heuristics to ensure quality of context, andfinally filtered for PoS mismatches.
Table 1 showsthe number of examples from each source.2.4 Words chosenBasically, the words employed in this task arethe same words used in Senseval 2 (40 words, 15nouns, 15 verbs and 10 adjectives), only the senseinventory changed.
Besides, in Senseval 3 wereplaced 5 verbs with new ones.
The reason for thisis that in the context of the MEANING project1 weare exploring multilingual lexical acquisition, andthere are ongoing experiments that focus on thoseverbs.
(Agirre et al 2004; Atserias et al 2004).In fact, 10 words in the English lexical-samplehave translations in the Basque, Catalan, Italian,Romanian and Spanish lexical tasks: channel,crown, letter, program, party (nouns), simple(adjective), play, win, lose, decide (verbs).2.5 Selection of examples from corporaThe minimum number of examples for eachword according to the task specifications wascalculated as follows:N=75+15*senses+7*multiwordsAs the number of senses in WordNet is very high,we decided to first estimate the number of sensesand multiwords that really occur in the corpus.
Thetaggers were provided with a sufficient number ofexamples, but they did not have to tag all.
Afterthey had tagged around 100 examples, they wouldcount the number of senses and multiwords thathad occurred and computed the N according tothose counts.The context is constituted of 5 sentences,including the sentence with the target wordappearing in the middle.
Links were kept to thesource corpus, document, and to the newspapersection when applicable.The occurrences were split at random in trainingset (2/3 of all occurrences) and test set (1/3).1 http://www.lsi.upc.es/~nlp/meaning/meaning.htmlTotal (N) (B) (I)# words 40# senses 316# number of tagged examples 7362 5695 924 743# number of untagged examples 62498 - - 62498# tags  9887Table 1: Some figures regarding the task.
N, B and Icorrespond to the source of the examples: newspaper,balanced corpus and Internet respectively.3 Hand taggingThree persons, graduate linguistics students,took part in the tagging.
They are familiar withword senses, as they are involved in thedevelopment of the Basque WordNet.
Thefollowing procedure was defined in the tagging ofeach word.?
Before tagging, one of the linguists (the editor)revised the 40 words in the Basque WordNet.She had to delete and add senses to the words,specially for adjectives and verbs, and wasallowed to check the examples in the corpus.?
The three taggers would meet, read the glossesand examples given in the Basque WordNetand discuss the meaning of each synset.
Theytried to agree and clarify the meaningdifferences among the synsets.
For each wordtwo hand-taggers and a referee is assigned bychance.?
The number of senses of a word in the BasqueWordNet might change during this meeting;that is, linguists could agree that one of theword?s senses was missing, or that a synset didnot fit with a word.
This was done prior tolooking at the corpus.
Then, the editor wouldupdate the Basque WordNet according to thosedecisions before giving the taggers the finalsynset list.
Overall (including first bulletabove), 143 senses were deleted and 92 sensesadded, leaving a total of 316 senses.
Thisreflects the current situation of the BasqueWordNet, which is still under construction.?
Two taggers independently tagged allexamples for the word.
No communication wasallowed while tagging the word.?
Multiple synset tags were allowed, as well asthe following tags: the lemma (in the case ofmultiword terms), U (unassignable), P (propernoun), and X (incorrectly lemmatized).
Thosewith an X were removed from the final release.In the case of proper nouns and multiwordterms no synset tag was assigned.
Sometimesthe U tag was used for word senses which arenot in the Basque WordNet.
For instance, thesense of kanal corresponding to TV channel,which is the most frequent sense in theexamples, is not present in the BasqueWordNet (it was not included in WordNet 1.6).?
A program was used to compute agreementrates and to output those occurrences wherethere was disagreement.
Those occurrenceswere  grouped by the senses assigned.?
A third tagger, the referee, reviewed thedisagreements and decided which one was thecorrect sense (or senses).The taggers were allowed to return more than onesense, and they returned 9887 tags (1.34 peroccurrence).
Overall, the two taggers agreed in atleast one tag 78.2% of the time.
Some wordsattained an agreement rate above 95% (e.g.
nounskanal or tentsio), but others like herri ?town/people/nation?
attained only 52% agreement.On average, the whole tagging task took 54seconds per occurrence for the tagger, and 20seconds for the referee.
However, this averagedoes not include the time the taggers and thereferee spent in the meetings they did tounderstand the meaning of each synset.
Thecomprehension of a word with all its synsetsrequired 45.5 minutes on average.4 Final releaseTable 1 includes the total amount of hand-taggedand untagged examples that were released.
Inaddition to the usual release, the training andtesting data were also provided in a lemmatizedversion (Aduriz et al 2000) which includedlemma, PoS and case information.
The motivationwas twofold:?
to make participation of the teams easier,considering the deep inflection of Basque.?
to factor out the impact of differentlemmatizers and PoS taggers in the systemcomparison.5 Participants and Results5 teams took part in this task: SwarthmoreCollege (swat), Basque Country University(BCU), Instituto per la Ricerca Scientifica eTecnologica (IRST), University of MinnesotaDuluth (Duluth) and University of Maryland(UMD).
All the teams presented supervised systemswhich only used the tagged training data, and noother external resource.
In particular, no systemused the pointers to the full texts, or the additionaluntagged texts.
All the systems used the lemma,PoS and case information provided, except theBCU team, which had additional access to number,determiner and ellipsis information directly fromthe analyzer.
This extra information was notprovided publicly because of representation issues.Prec.
Rec.
Attemptedbasque-swat_hk-bo 71.1  70.4  99.04 %BCU_Basque_svm 69.9  69.9  100.00 %BCU_-_Basque_Comb 69.5  69.5  100.00 %swat-hk-basque 67.0  67.0  100.00 %IRST-Kernels-bas 65.5  65.5  100.00 %swat-basque 64.6  64.6  100.00 %Duluth-BLSS 60.8  60.8  100.00 %UMD_SST1 65.6  58.7  89.42 %MFS 55.8  55.8  100.00 %Table 2: Results of systems and MFS baseline, orderedaccording to Recall.We want to note that due to a bug, a few exampleswere provided without lemmas.The results for the fine-grained scoring areshown in Table 2, including the Most FrequentSense baseline (MFS).
We will briefly describeeach of the systems presented by each team inorder of best recall.?
Swat presented three systems based in thesame set of features: the best one was based onAdaboost, the second on a combination of fivelearners (Adaboost, maximum entropy,clustering system based on cosine similarity,decision lists, and na?ve bayes, combined bymajority voting), and the third on acombination of three systems (the last three).?
BCU presented two systems: the first one basedon Support Vector Machines (SVM) and thesecond on a majority-voting combination ofSVM, cosine based vectors and na?ve bayes.?
IRST participated with a kernel-based method.?
Duluth participated with a system that votesamong three bagged decision trees.?
UMD presented a system based on SVM.The winning system is the one using Adaboostfrom Swat, followed closely by the BCU systemusing SVM.6 DiscussionThese are the main issues we think areinteresting for further discussion.Sense inventory.
Using the Basque WordNetpresented some difficulties to the taggers.
TheBasque WordNet has been built using thetranslation approach, that is, the English synsetshave been ?translated?
into Basque.
The taggershad some difficulties to comprehend synsets, andespecially, to realize what makes a synset differentfrom another.
In some cases the taggers decided togroup some of the senses, for instance, in herri ?town/people/nation?
they grouped 6 senses.
Thisexplains the relatively high number of tags peroccurrence (1.34).
The taggers think that thetagging would be much more satisfactory if theyhad defined the word senses directly from thecorpus.Basque WordNet quality.
There was amismatch between the Basque WordNet and thecorpus: most of the examples were linked to aspecific genre, and this resulted in i) having ahandful of senses in the Basque WordNet that didnot appear in our corpus and ii) having somesenses that were not included in the BasqueWordNet.
Fortunately, we already predicted thisand we had a preparation phase where the editorenriched WordNet accordingly.
Most of thedeletions in the preliminary part were due to thesemi-automatic method to construct the BasqueWordNet.
All in all, we think that tagging corporais the best way to ensure the quality of theWordNets and we plan to pursue this extensivelyfor the improvement of the Basque WordNet.7 Conclusions and future work5 teams participated in the Basque lexical-sample task with 8 systems.
All of the participantspresented supervised systems which used lemma,PoS and case information provided, but none usedthe large amount of untagged senses provided bythe organizers.
The winning system attained 70.4recall.
Regarding the organization of the task, wefound that the taggers were more comfortablegrouping some of the senses in the BasqueWordNet.
We also found that tagging word sensesis essential for enriching and quality checking ofthe Basque WordNet.AcknowledgementsThe work has been partially funded by theEuropean Commission (MEANING project IST-2001-34460).
Eli Pociello has a PhD grant fromthe Basque Government.ReferencesI.
Aduriz, E. Agirre, I. Aldezabal, I. Alegria, X.Arregi, J.M.
Arriola, X. Artola, K. Gojenola, A.Maritxalar, K. Sarasola, M. Urkia.
2000.
AWord-grammar Based Morphological Analyzerfor Agglutinative Languages.
In Proceedings ofthe International Conference on ComputationalLinguistics (COLING).
Saarbrucken, Germany.E.
Agirre, A. Atutxa, K. Gojenola, K. Sarasola.2004.
Exploring portability of syntacticinformation from English to Basque.
InProceedings of the 4rd International Conferenceon Languages Resources and Evaluations(LREC).
Lisbon, Portugal.J.
Atserias, B. Magnini, O. Popescu, E. Agirre, A.Atutxa, G. Rigau, J. Carroll and R. Koeling2004.
Cross-Language Acquisition of SemanticModels for Verbal Predicates.
In Proceedings ofthe 4rd International Conference on LanguagesResources and Evaluations (LREC).
Lisbon,Portugal.C.
Fellbaum.
1998.
WordNet: An electronicLexical Database.
The MIT Press, Cambridge,Massachusetts.
