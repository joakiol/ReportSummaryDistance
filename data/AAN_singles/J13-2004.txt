Mildly Non-Projective Dependency GrammarMarco Kuhlmann*Uppsala UniversitySyntactic representations based on word-to-word dependencies have a long-standing traditionin descriptive linguistics, and receive considerable interest in many applications.
Nevertheless,dependency syntax has remained something of an island from a formal point of view.
Moreover,most formalisms available for dependency grammar are restricted to projective analyses, andthus not able to support natural accounts of phenomena such as wh-movement and cross?serialdependencies.
In this article we present a formalism for non-projective dependency grammarin the framework of linear context-free rewriting systems.
A characteristic property of ourformalism is a close correspondence between the non-projectivity of the dependency treesadmitted by a grammar on the one hand, and the parsing complexity of the grammar on theother.
We show that parsing with unrestricted grammars is intractable.
We therefore study twoconstraints on non-projectivity, block-degree and well-nestedness.
Jointly, these two constraintsdefine a class of ?mildly?
non-projective dependency grammars that can be parsed in polynomialtime.
An evaluation on five dependency treebanks shows that these grammars have a goodcoverage of empirical data.1.
IntroductionSyntactic representations based on word-to-word dependencies have a long-standingtradition in descriptive linguistics.
Since the seminal work of Tesnie`re (1959), theyhave become the basis for several linguistic theories, such as Functional GenerativeDescription (Sgall, Hajic?ova?, and Panevova?
1986), Meaning?Text Theory (Mel?c?uk 1988),and Word Grammar (Hudson 2007).
In recent years they have also been used for a widerange of practical applications, such as information extraction, machine translation, andquestion answering.
We ascribe the widespread interest in dependency structures totheir intuitive appeal, their conceptual simplicity, and in particular to the availability ofaccurate and efficient dependency parsers for a wide range of languages (Buchholz andMarsi 2006; Nivre et al2007).Although there exist both a considerable practical interest and an extensive lin-guistic literature, dependency syntax has remained something of an island from aformal point of view.
In particular, there are relatively few results that bridge betweendependency syntax and other traditions, such as phrase structure or categorial syntax.?
Department of Linguistics and Philology, Box 635, 751 26 Uppsala, Sweden.E-mail: marco.kuhlmann@lingfil.uu.se.Submission received: 17 December 2009; revised submission received: 3 April 2012; accepted for publication:24 May 2012.doi:10.1162/COLI a 00125?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 2Figure 1Nested dependencies and cross?serial dependencies.This makes it hard to gauge the similarities and differences between the paradigms,and hampers the exchange of linguistic resources and computational methods.
Anoverarching goal of this article is to bring dependency grammar closer to the mainlandof formal study.One of the few bridging results for dependency grammar is thanks to Gaifman(1965), who studied a formalism that we will refer to as Hays?Gaifman grammar, andproved it to be weakly equivalent to context-free phrase structure grammar.
Althoughthis result is of fundamental importance from a theoretical point of view, its practicalusefulness is limited.
In particular, Hays?Gaifman grammar is restricted to projectivedependency structures, which is similar to the familiar restriction to contiguous con-stituents.
Yet, non-projective dependencies naturally arise in the analysis of naturallanguage.
One classic example of this is the phenomenon of cross?serial dependenciesin Dutch.
In this language, the nominal arguments of verbs that also select an infinitivalcomplement occur in the same order as the verbs themselves:(i) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch)that Jan Piet Marie saw help read?that Jan saw Piet help Marie read?In German, the order of the nominal arguments instead inverts the verb order:(ii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German)that Jan Piet Marie read help sawFigure 1 shows dependency trees for the two examples.1 The German linearizationgives rise to a projective structure, where the verb?argument dependencies are nestedwithin each other, whereas the Dutch linearization induces a non-projective structurewith crossing edges.
To account for such structures we need to turn to formalisms moreexpressive than Hays?Gaifman grammars.In this article we present a formalism for non-projective dependency grammarbased on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi1987; Weir 1988).
This framework was introduced to facilitate the comparison of various1 We draw the nodes of a dependency tree as circles, and the edges as arrows pointing towards thedependent (away from the root node).
Following Hays (1964), we use dotted lines to help us keeptrack of the positions of the nodes in the linear order, and to associate nodes with lexical items.356Kuhlmann Mildly Non-Projective Dependency Grammargrammar formalisms, including standard context-free grammar, tree-adjoining gram-mar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman andBaldridge 2011).
It also comprises, among others, multiple context-free grammars (Sekiet al1991), minimalist grammars (Michaelis 1998), and simple range concatenationgrammars (Boullier 2004).The article is structured as follows.
In Section 2 we provide the technical back-ground to our work; in particular, we introduce our terminology and notation for linearcontext-free rewriting systems.
An LCFRS generates a set of terms (formal expressions)which are interpreted as derivation trees of objects from some domain.
Each term alsohas a secondary interpretation under which it denotes a tuple of strings, representingthe string yield of the derived object.
In Section 3 we introduce the central notion of alexicalized linear context-free rewriting system, which is an LCFRS in which each ruleof the grammar is associated with an overt lexical item, representing a syntactic head(cf.
Schabes, Abeille?, and Joshi 1988 and Schabes 1990).
We show that this property givesrise to an additional interpretation under which each term denotes a dependency treeon its yield.
With this interpretation, lexicalized LCFRSs can be used as dependencygrammars.In Section 4 we show how to acquire lexicalized LCFRSs from dependency tree-banks.
This works in much the same way as the extraction of context-free grammarsfrom phrase structure treebanks (cf.
Charniak 1996), except that the derivation trees ofdependency trees are not immediately accessible in the treebank.
We therefore presentan efficient algorithm for computing a canonical derivation tree for an input depen-dency tree; from this derivation tree, the rules of the grammar can be extracted in astraightforward way.
The algorithm was originally published by Kuhlmann and Satta(2009).
It produces a restricted type of lexicalized LCFRS that we call ?canonical.?
InSection 5 we provide a declarative characterization of this class of grammars, and showthat every lexicalized LCFRS is (strongly) equivalent to a canonical one, in the sense thatit induces the same set of dependency trees.In Section 6 we present a simple parsing algorithm for LCFRSs.
Although theruntime of this algorithm is polynomial in the length of the sentence, the degree ofthe polynomial depends on two grammar-specific measures called fan-out and rank.We show that even in the restricted case of canonical grammars, parsing is an NP-hard problem.
It is important therefore to keep the fan-out and the rank of a grammaras low as possible, and much of the recent work on LCFRSs has been devoted tothe development of techniques that optimize parsing complexity in various scenariosGo?mez-Rodr?
?guez and Satta 2009; Go?mez-Rodr?
?guez et al2009; Kuhlmann and Satta2009; Gildea 2010; Go?mez-Rodr?
?guez, Kuhlmann, and Satta 2010; Sagot and Satta 2010;and Crescenzi et al2011).In this article we explore the impact of non-projectivity on parsing complexity.
InSection 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS,a measure called block-degree (or gap-degree) (Holan et al1998).
Although there isno theoretical upper bound on the block-degree of the dependency trees needed forlinguistic analysis, we provide evidence from several dependency treebanks showingthat, from a practical point of view, this upper bound can be put at a value of as low as 2.In Section 8 we study a second constraint on non-projectivity called well-nestedness(Bodirsky, Kuhlmann, and Mo?hl 2005), and show that its presence facilitates tractableparsing.
This comes at the cost of a small loss in coverage on treebank data.
Boundedblock-degree and well-nestedness jointly define a class of ?mildly?
non-projectivedependency grammars that can be parsed in polynomial time.Section 9 summarizes our main contributions and concludes the article.357Computational Linguistics Volume 39, Number 22.
Technical BackgroundWe assume basic familiarity with linear context-free rewriting systems (see, e.g., Vijay-Shanker, Weir, and Joshi 1987 and Weir 1988) and only review the terminology andnotation that we use in this article.A linear context-free rewriting system (LCFRS) is a structure G = (N,?, P, S)where N is a set of nonterminals, ?
is a set of function symbols, P is a finite set ofproduction rules, and S ?
N is a distinguished start symbol.
Rules take the formA0 ?
f (A1, .
.
.
, Am) (1)where f is a function symbol and the Ai are nonterminals.
Rules are used for rewritingin the same way as in a context-free grammar, with the function symbols acting asterminals.
The outcome of the rewriting process is a set T(G) of terms, tree-formedexpressions built from function symbols.
Each term is then associated with a stringyield, more specifically a tuple of strings.
For this, every function symbol f comes witha yield function that specifies how to compute the yield of a term f (t1, .
.
.
, tm) from theyields of its subterms ti.
Yield functions are defined by equationsf (?x1,1, .
.
.
, x1,k1?, .
.
.
, ?xm,1, .
.
.
, xm,km?)
= ?
?1, .
.
.
,?k0?
(2)where the tuple on the right-hand side consists of strings over the variables on theleft-hand side and some given alphabet of yield symbols, and contains exactly oneoccurrence of each variable.
For a yield function f defined by an equation of this form,we say that f is of type k1 ?
?
?
km ?
k0, denoted by f : k1 ?
?
?
km ?
k0.
To guarantee thatthe string yield of a term is well-defined, each nonterminal A is associated with afan-out ?
(A) ?
1, and it is required that for every rule (1),f : ?
(A1) ?
?
??
(Am) ?
?
(A0)In Equation (2), the values m and k0 are called the rank and the fan-out of f , respectively.The rank and the fan-out of an LCFRS are the maximal rank and fan-out of its yieldfunctions.Example 1Figure 2 shows an example of an LCFRS for the language { ?anbncndn?
| n ?
0 }.Equation (2) is uniquely determined by the tuple on the right-hand side of theequation.
We call this tuple the template of the yield function f , and use it as thecanonical function symbol for f .
This gives rise to a compact notation for LCFRSs,Figure 2An LCFRS that generates the yield language { ?anbncndn?
| n ?
0 }.358Kuhlmann Mildly Non-Projective Dependency Grammarillustrated in the right column of Figure 2.
In this notation, to save some subscripts,we use the following shorthands for variables: x and x1 for x1,1; x2 for x1,2; x3 for x1,3;y and y1 for x2,1; y2 for x2,2; y3 for x2,3.3.
Lexicalized LCFRSs as Dependency GrammarsRecall the following examples for verb?argument dependencies in German and Dutchfrom Section 1:(iii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 (German)that Jan Piet Marie read help saw(iv) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 (Dutch)that Jan Piet Marie saw help read?that Jan saw Piet help Marie read?Figure 3 shows the production rules of two linear context-free rewriting systems (one forGerman, one for Dutch) that generate these examples.
The grammars are lexicalized inthe sense that each of their yield functions is associated with a lexical item, such as sah orzag (cf.
Schabes, Abeille?, and Joshi 1988 and Schabes 1990).
Productions with lexicalizedyield functions can be read as dependency rules.
For example, the rulesV ?
?x y sah?
(N, V) (German) V ?
?x y1 zag y2?
(N, V) (Dutch)can be read as stating that the verb to see requires two dependents, one noun (N) andone verb (V).
Based on this reading, every term generated by a lexicalized LCFRSdoes not only yield a tuple of strings, but also induces a dependency tree on thesestrings: Each parent?child relation in the term represents a dependency between theassociated lexical items (cf.
Rambow and Joshi 1997).
Thus every lexicalized LCFRS canbe reinterpreted as a dependency grammar.
To illustrate the idea, Figure 4 shows (thetree representations of) two terms generated by the grammars G1 and G2, together withthe dependency trees induced by them.
Note that these are the same trees that we gavefor (iii) and (iv) in Figure 1.Our goal for the remainder of this section is to make the notion of induction formallyprecise.
To this end we will reinterpret the yield functions of lexicalized LCFRSs asoperations on dependency trees.Figure 3Lexicalized linear context-free rewriting systems.359Computational Linguistics Volume 39, Number 2Figure 4Lexicalized linear context-free rewriting systems induce dependency trees.3.1 Dependency TreesBy a dependency tree, we mean a pair (w, D), where w is a tuple of strings, and D isa tree-shaped graph whose nodes correspond to the occurrences of symbols in w, andwhose edges represent dependency relations between these occurrences.
We identifyoccurrences in w by pairs (i, j) of integers, where i indexes the component of w thatcontains the occurrence, and j specifies the linear position of the occurrence withinthat component.
We can then formally define a dependency graph for a tuple ofstringsw = ?a1,1 ?
?
?
a1,n1 , .
.
.
, ak,1 ?
?
?
ak,nk?as a directed graph G = (V, E) whereV = { (i, j) | 1 ?
i ?
k, 1 ?
j ?
ni } and E ?
V ?
VWe use u and v as variables for nodes, and denote edges (u, v) as u ?
v. A dependencytree D for w is a dependency graph for w in which there exists a root node r suchthat for any node u, there is exactly one directed path from r to u.
A dependencytree is called simple if w consists of a single string w. In this case, we write the de-pendency tree as (w, D), and identify occurrences by their linear positions j in w, with1 ?
j ?
|w|.Example 2Figure 5 shows examples of dependency trees.
In pictures of such structures we usedashed boxes to group nodes that correspond to occurrences from the same tuple360Kuhlmann Mildly Non-Projective Dependency GrammarFigure 5Dependency trees.component; however, we usually omit the box when there is only one component.Writing Di as Di = (Vi, Ei) we have:V1 = {(1, 1)} E1 = {}V2 = {(1, 1), (1, 2)} E2 = {(1, 1) ?
(1, 2)}V3 = {(1, 1), (2, 1)} E3 = {(1, 1) ?
(2, 1)}V4 = {(1, 1), (3, 1)} E4 = {(1, 1) ?
(3, 1)}We use standard terminology from graph theory for dependency trees and therelations between their nodes.
In particular, for a node u, the set of descendants of u,which we denote byu, is the set of nodes that can be reached from u by following adirected path consisting of zero or more edges.
We write u < v to express that the node uprecedes the node v when reading the yield from left to right.
Formally, precedence isthe lexicographical order on occurrences:(i1, j1) < (i2, j2) if and only if either i1 < i2 or (i1 = i2 and j1 < j2)3.2 Operations on Dependency TreesA yield function f is called lexicalized if its template contains exactly one yield symbol,representing a lexical item; this symbol is then called the anchor of f .
With everylexicalized yield function f we associate an operation f ?
on dependency trees as follows.Let w1, .
.
.
, wm, w be tuples of strings such thatf (w1, .
.
.
, wm) = wand let Di be a dependency tree for wi.
By the definition of yield functions, everyoccurrence u in an input tuple wi corresponds to exactly one occurrence in the outputtuple w; we denote this occurrence by u?.
Let G be the dependency graph for w thathas an edge u?
?
v?
whenever there is an edge u ?
v in some Di, and no other edges.Because f is lexicalized, there is exactly one occurrence r in the output tuple w that doesnot correspond to any occurrence in some wi; this is the occurrence of the anchor of f .Let D be the dependency tree for w that is obtained by adding to the graph G all edgesof the form r ?
r?i, where ri is the root node of Di.
By this construction, the occurrence rof the anchor becomes the root node of D, and the root nodes of the input dependencytrees Di become its dependents.
We then definef ?
((w1, D1), .
.
.
, (wm, Dm)) = (w, D)361Computational Linguistics Volume 39, Number 2Figure 6Operations on dependency trees.Example 3We consider a concrete application of an operation on dependency trees, illustrated inFigure 6.
In this example we havef = ?x1 b, y x2?
w1 = ?a, e?
w2 = ?c d?
w = f (w1, w2) = ?a b, c d e?and the dependency trees D1, D2 are defined asD1 = ({(1, 1), (2, 1)}, {(1, 1) ?
(2, 1)}) D2 = ({(1, 1), (1, 2)}, {(1, 1) ?
(1, 2)})We show that f ?
((w1, D1), (w2, D2)) = (w, D), where D = (V, E) withV = {(1, 1), (1, 2), (2, 1), (2, 2), (2, 3)}E = {(1, 1) ?
(2, 3), (1, 2) ?
(1, 1), (1, 2) ?
(2, 1), (2, 1) ?
(2, 2)}The correspondences between the occurrences u in the input tuples and the occur-rences u?
in the output tuple are as follows:for w1: (1, 1) = (1, 1) , (2, 1) = (2, 3) for w2: (1, 1) = (2, 1) , (1, 2) = (2, 2)By copying the edges from the input dependency trees, we obtain the intermediatedependency graph G = (V, E?)
for w, whereE?
= {(1, 1) ?
(2, 3), (2, 1) ?
(2, 2)}The occurrence r of the anchor b of f in w is (1, 2); the nodes of G that correspond tothe root nodes of D1 and D2 are r?1 = (1, 1) and r?2 = (2, 1).
The dependency tree D isobtained by adding the edges r ?
r?1 and r ?
r?2 to G.4.
Extraction of Dependency GrammarsWe now show how to extract lexicalized linear context-free rewriting systems fromdependency treebanks.
To this end, we adapt the standard technique for extractingcontext-free grammars from phrase structure treebanks (Charniak 1996).Our technique was originally published by Kuhlmann and Satta (2009).
In recentwork, Maier and Lichte (2011) have shown how to unify it with a similar techniquefor the extraction of range concatenation grammars from discontinuous constituentstructures, due to Maier and S?gaard (2008).
To simplify our presentation we restrictour attention to treebanks containing simple dependency trees.362Kuhlmann Mildly Non-Projective Dependency GrammarFigure 7A dependency tree and one of its construction trees.To extract a lexicalized LCFRS from a dependency treebank we proceed as follows.First, for each dependency tree (w, D) in the treebank, we compute a construction tree,a term t over yield functions that induces (w, D).
Then we collect a set of productionrules, one rule for each node of the construction trees.
As an example, consider Fig-ure 7, which shows a dependency tree with one of its construction trees.
(The analysisis taken from Ku?bler, McDonald, and Nivre [2009].)
From this construction tree weextract the following rules.
The nonterminals (in bold) represent linear positions ofnodes.1 ?
?A?
5 ?
?on x?
(7)2 ?
?x hearing, y?
(1, 5) 6 ?
?the?3 ?
?x1 is y1 x2 y2?
(2, 4) 7 ?
?x issue?
(6)4 ?
?scheduled, x?
(8) 8 ?
?today?Rules like these can serve as the starting point for practical systems for data-driven,non-projective dependency parsing (Maier and Kallmeyer 2010).Because the extraction of rules from construction trees is straightforward, the prob-lem that we focus on in this section is how to obtain these trees in the first place.
Ourprocedure for computing construction trees is based on the concept of ?blocks.
?4.1 BlocksLet D be a dependency tree.
A segment of D is a contiguous, non-empty sequenceof nodes of D, all of which belong to the same component of the string yield.
Thusa segment contains its endpoints, as well as all nodes between the endpoints in theprecedence order.
For a node u of D, a block of u is a longest segment consisting ofdescendants of u.
This means that the left endpoint of a block of u either is the first nodein its component, or is preceded by a node that is not a descendant of u.
A symmetricproperty holds for the right endpoint.Example 4Consider the node 2 of the dependency tree in Figure 7.
The descendants of 2 fall intotwo blocks, marked by the dashed boxes: 1 2 and 5 6 7.363Computational Linguistics Volume 39, Number 2We use u and v as variables for blocks.
Extending the precedence order on nodes,we say that a block u precedes a block v, denoted by u < v, if the right endpoint of uprecedes the left endpoint of v.4.2 Computing Canonical Construction TreesTo obtain a canonical construction tree t for a dependency tree (w, D) we label eachnode u of D with a yield function f as follows.
Let w be the tuple consisting of the blocksof u, in the order of their precedence, and let w1, .
.
.
, wm be the corresponding tuples forthe children of u.
We may view blocks as strings of nodes.
Taking this view, we computethe (unique) yield function g with the property thatg(w1, .
.
.
, wm) = wThe anchor of g is the node u, the rank of g corresponds to the number of childrenof u, the variables in the template of g represent the blocks of these children, and thecomponents of the template represent the blocks of u.
To obtain f , we take the templateof g and replace the occurrence of u with the corresponding lexical item.Example 5Node 2 of the dependency tree shown in Figure 7 has two children, 1 and 5.
We havew = ?1 2, 5 6 7?
w1 = ?1?
w2 = ?5 6 7?
g = ?x 2, y?
f = ?x hearing, y?Note that in order to properly define f we need to assume some order on thechildren of u.
The function g (and hence the construction tree t) is unique up to thespecific choice of this order.
In the following we assume that children are ordered fromleft to right based on the position of their leftmost descendants.4.3 Computing the Blocks of a Dependency TreeThe algorithmically most interesting part of our extraction procedure is the computationof the yield function g. The template of g is uniquely determined by the left-to-rightsequence of the endpoints of the blocks of u and its children.
An efficient algorithm thatcan be used to compute these sequences is given in Table 1.4.3.1 Description.
We start at a virtual root node ?
(line 1) which serves as the parentof the real root node.
For each node next in the precedence order of D, we follow theshortest path from the current node current to next.
To determine this path, we computethe lowest common ancestor lca of the two nodes (lines 4?5), using a set of markingson the nodes.
At the beginning of each iteration of the for loop in line 2, all ancestors ofcurrent (including the virtual root node ?)
are marked; therefore, we find lca by goingupwards from next to the first node that is marked.
To restore the loop invariant, wethen unmark all nodes on the path from current to lca (lines 6?9).
Each time we movedown from a node to one of its children (line 12), we record the information that nextis the left endpoint of a block of current.
Symmetrically, each time we move up from anode to its parent (lines 8 and 17), we record the information that next ?
1 is the rightendpoint of a block of current.
The while loop in lines 15?18 takes us from the last nodeof the dependency tree back to the node ?.364Kuhlmann Mildly Non-Projective Dependency GrammarTable 1Computing the blocks of a simple dependency tree.Input: a string w and a simple dependency tree D for w1: current ?
?
; mark current2: for each node next of D from 1 to |w| do3: lca ?
next; stack ?
[]4: while lca is not marked do loop 15: push lca to stack; lca ?
the parent of lca6: while current = lca do loop 27:  next ?
1 is the right endpoint of a block of current8:  move up from current to the parent of current9: unmark current; current ?
the parent of current10: while stack is not empty do loop 311: current ?
pop stack; mark current12:  move down from the parent of current to current13:  next is the left endpoint of a block of current14:  arrive at next; at this point, current = next15: while current = ?
do loop 416:  |w| is the right endpoint of a block of current17:  move up from current to the parent of current18: unmark current; current ?
the parent of current4.3.2 Runtime Analysis.
We analyze the runtime of our algorithm.
Let m be the totalnumber of blocks of D. Let us write ni for the total number of iterations of the ith whileloop, and let n = n1 + n2 + n3 + n4.
Under the reasonable assumption that every line inTable 1 can be executed in constant time, the runtime of the algorithm clearly is in O(n).Because each iteration of loop 2 and loop 4 determines the right endpoint of a block, wehave n2 + n4 = m. Similarly, as each iteration of loop 3 fixes the left endpoint of a block,we have n3 = m. To determine n1, we note that every node that is pushed to the auxiliarystack in loop 1 is popped again in loop 3; therefore, n1 = n3 = m. Putting everythingtogether, we have n = 3m, and we conclude that the runtime of the algorithm is in O(m).Note that this runtime is asymptotically optimal for the task we are considering.5.
Canonical GrammarsOur extraction technique produces a restricted type of lexicalized linear context-freerewriting system that we will refer to as ?canonical.?
In this section we provide adeclarative characterization of these grammars, and show that every lexicalized LCFRSis equivalent to a canonical one.5.1 Definition of Canonical GrammarsWe are interested in a syntactic characterization of the yield functions that can occurin extracted grammars.
We give such a characterization in terms of four properties,stated in the following.
We use the following terminology and notation.
Consider ayield functionf : k1 ?
?
?
km ?
k , f = ?
?1, .
.
.
,?k?For variables x, y we write x <f y to state that x precedes y in the template of f , thatis, in the string ?1 ?
?
??k.
Recall that, in the context of our extraction procedure, the365Computational Linguistics Volume 39, Number 2components in the template of f represent the blocks of a node u, and the variables inthe template represent the blocks of the children of u.
For a variable xi,j we call i theargument index and j the component index of the variable.Property 1For all 1 ?
i1, i2 ?
m, if i1 < i2 then xi1,1 <f xi2,1.This property is an artifact of our decision to order the children of a node from leftto right based on the position of their leftmost descendants.
A variable with argumentindex i represents a block of the ith child of u in that order.
An example of a yieldfunction that does not have Property 1 is ?x2,1 x1,1?, which defines a kind of ?reverseconcatenation operation.
?Property 2For all 1 ?
i ?
m and 1 ?
j1, j2 ?
ki, if j1 < j2 then xi,j1 <f xi,j2 .This property reflects that, in our extraction procedure, the variable xi,j represents thejth block of the ith child of u, where the blocks of a node are ordered from left to rightbased on their precedence.
An example of a yield function that violates the propertyis ?x1,2 x1,1?, which defines a kind of swapping operation.
In the literature on LCFRSsand related formalisms, yield functions with Property 2 have been called monotone(Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer2010), and non-permuting (Kanazawa 2009).Property 3No component ?h is the empty string.This property, which is similar to ?-freeness as known from context-free grammars,has been discussed for multiple context-free grammars (Seki et al1991, Property N3in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1).
For ourextracted grammars it holds because each component ?h represents a block, and blocksare always non-empty.Property 4No component ?h contains a substring of the form xi,j1xi,j2 .This property, which does not seem to have been discussed in the literature before, is areflection of the facts that variables with the same argument index represent blocks ofthe same child node, and that these blocks are longest segments of descendants.A yield function with Properties 1?4 is called canonical.
An LCFRS is canonical ifall of its yield functions are canonical.Lemma 1A lexicalized LCFRS is canonical if and only if it can be extracted from a dependencytreebank using the technique presented in Section 4.ProofWe have already argued for the ?only if?
part of the claim.
To prove the ?if?
part, itsuffices to show that for every canonical, lexicalized yield function f , one can construct366Kuhlmann Mildly Non-Projective Dependency Grammara dependency tree such that the construction tree extracted for this dependency treecontains f .
This is an easy exercise.
We conclude by noting that Properties 2?4 are also shared by the treebank grammarsextracted from constituency treebanks using the technique by Maier and S?gaard (2008).5.2 Equivalence Between General and Canonical GrammarsTwo lexicalized LCFRSs are called strongly equivalent if they induce the same set ofdependency trees.
We show the following equivalence result:Lemma 2For every lexicalized LCFRS G one can construct a strongly equivalent lexicalizedLCFRS G?
such that G?
is canonical.ProofOur proof of this lemma uses two normal-form results about multiple context-freegrammars: Michaelis (2001, Section 2.4) provides a construction that transforms a mul-tiple context-free grammar into a weakly equivalent multiple context-free grammar inwhich all rules satisfy Property 2, and Seki et al(1991, Lemma 2.2) present a corre-sponding construction for Property 3.
Whereas both constructions are only quoted topreserve weak equivalence, we can verify that, in the special case where the inputgrammar is a lexicalized LCFRS, they also preserve the set of induced dependency trees.To complete the proof of Lemma 2, we show that every lexicalized LCFRS can be castinto normal forms that satisfy Property 1 and Property 4.
It is not hard then to combinethe four constructions into a single one that simultaneously establishes all properties ofcanonical yield functions.
Lemma 3For every lexicalized LCFRS G one can construct a strongly equivalent lexicalizedLCFRS G?
such that G?
only contains yield functions which satisfy Property 1.ProofThe proof is very simple.
Intuitively, Property 1 enforces a canonical naming of thearguments of yield functions.
To establish it, we determine, for every yield function f ,a permutation ?
that renames the argument indices of the variables occurring in thetemplate of f in such a way that the template meets Property 1.
This renaming gives riseto a modified yield function f?.
We then replace every rule A ?
f (A1, .
.
.
, Am) with themodified rule A ?
f?(A?
(1), .
.
.
, A?
(m) ).
Lemma 4For every lexicalized LCFRS G one can construct a strongly equivalent lexicalizedLCFRS G?
such that G?
only contains yield functions which satisfy Property 4.ProofThe idea behind our construction of the grammar G?
is perhaps best illustrated by anexample.
Imagine that the grammar G generates the term t shown in Figure 8a.
The yieldfunction f1 = ?x1 c x2 x3?
at the root node of that term violates Property 4, as its templatecontains the offending substring x2 x3.
We set up G?
in such a way that instead of t itgenerates the term t?
shown in Figure 8b in which f1 is replaced with the yield function367Computational Linguistics Volume 39, Number 2Figure 8The transformation implemented by the construction of the grammar G?
in Lemma 4.f ?1 = ?x1 c x2?.
To obtain f ?1 from f1 we reduce the offending substring x2 x3 to the singlevariable x2.
In order to ensure that t and t?
induce the same dependency tree (shown inFigure 8c), we then adapt the function f2 = ?x1 b, y, x2?
at the first child of the root node:Dual to the reduction, we replace the two-component sequence y, x2 in the template of f2with the single component y x2; in this way we get f ?2 = ?x1 b, y x2?.Because adaptation operations may introduce new offending substrings, we need arecursive algorithm to compute the rules of the grammar G?.
Such an algorithm is givenin Table 2.
For every rule A ?
f (A1, .
.
.
, Am) of G we construct new rules(A, g) ?
f ?
((A1, g1), .
.
.
, (Am, gm))where g and the gi are yield functions encoding adaptation operations.
As an example,the adaptation of the function f2 in the term t may be encoded into the adaptor function?x1, x2 x3?.
The function f ?2 can then be written as the composition of this function and f2:f ?2 = ?x1, x2 x3?
?
f2 = ?x1, x2 x3?
(?x1 b, y, x2?)
= ?x1 b, y x2?The yield function f ?
and the adaptor functions gi are computed based on the templateof the g-adapted yield function f , that is, the composed function g ?
f .
In Table 2 we writethis as f ?
= reduce(f, g) and gi = adapt(f, g, i), respectively.
Let us denote the template ofthe adapted function g ?
f by ?.
An i-block of ?
is a maximal, non-empty substring ofsome component of ?
that consists of variables with argument index i.
To compute thetemplate of gi we read the i-blocks of ?
from left to right and rename the variables bychanging their argument indices from i to 1.
To compute the template of f ?
we take theTable 2Computing the production rules of an LCFRS in which all yield functions satisfy Property 4.Input: a linear context-free rewriting system G = (N,?, P, S)1: P?
?
?
; agenda ?
{(S, ?x?
)}; chart ?
?2: while agenda is not empty3: remove some (A, g) from agenda4: if (A, g) /?
chart then5: add (A, g) to chart6: for each rule A ?
f (A1, .
.
.
, Am) ?
P do7: f ?
reduce(f, g); gi ?
adapt(f, g, i) (1 ?
i ?
m)8: for each i from 1 to m do9: add (Ai, gi) to agenda10: add (A, g) ?
f ?
((A1, g1), .
.
.
, (Am, gm)) to P?368Kuhlmann Mildly Non-Projective Dependency Grammartemplate ?
and replace the jth i-block with the variable xi,j, for all argument indices iand component indices j.Our algorithm is controlled by an agenda and a chart, both containing pairs ofthe form (A, g), where A is a nonterminal of G and g is an adaptor function.
Thesepairs also constitute the nonterminals of the new grammar G?.
The fan-out of a non-terminal is the fan-out of g. The agenda is initialized with the pair (S, ?x?)
where ?x?is the identity function; this pair also represents the start symbol of G?.
To see thatthe algorithm terminates, one may observe that the fan-out of every nonterminal (A, g)added to the agenda is upper-bounded by the fan-out of A.
Hence, there are only finitelymany pairs (A, g) that may occur in the chart, and a finite number of iterations of thewhile-loop.
We conclude by noting that when constructing a canonical grammar, one needs tobe careful about the order in which the individual constructions (for Properties 1?4) arecombined.
One order that works isProperty 3 < Property 4 < Property 2 < Property 16.
Parsing and RecognitionLexicalized linear context-free rewriting systems are able to account for arbitrarily non-projective dependency trees.
This expressiveness comes with a price: In this section weshow that parsing with lexicalized LCFRSs is intractable, unless we are willing to restrictthe class of grammars.6.1 Parsing AlgorithmTo ground our discussion of parsing complexity, we present a simple bottom?up parsingalgorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes,and Pereira 1995).
Several similar algorithms have been described in the literature (Sekiet al1991; Bertsch and Nederhof 2001; Kallmeyer 2010).
We assume that we are given agrammar G = (N,?, P, S) and a string w = a1 ?
?
?
an ?
V?
to be parsed.Item form.
The items of the deduction system take the form[A, l1, r1, .
.
.
, lk, rk]where A ?
N with ?
(A) = k, and the remaining components are indices identifying theleft and right endpoints of pairwise non-overlapping substrings of w. More formally,0 ?
lh ?
rh ?
n, and for all h, h?
with h = h?, either rh ?
lh?
or rh?
?
lh.
The intendedinterpretation of an item of this form is that A derives a term t ?
T(G) that yields thespecified substrings of w, that is,A ?
?G t and yield(t) = ?al1+1 ?
?
?
ar1 , .
.
.
, alk+1 ?
?
?
ark?Goal item.
The goal item is [S, 0, n].
By this item, there exists a term that can be derivedfrom the start symbol S and yields the full string ?w?.369Computational Linguistics Volume 39, Number 2Inference rules.
The inference rules of the deduction system are defined based on therules in P. Each production ruleA ?
f (A1, .
.
.
, Am) with f : k1 ?
?
?
km ?
k , f = ?
?1, .
.
.
,?k?is converted into a set of inference rules of the form[A1, l1,1, r1,1, .
.
.
, l1,k1 , r1,k1]?
?
?
[Am, lm,1, rm,1, .
.
.
, lm,km , rm,km][A, l1, r1, .
.
.
, lk, rk](3)Each such rule is subject to the following constraints.
Let 1 ?
h ?
k, v ?
V?, 1 ?
i ?
m,and 1 ?
j ?
ki.
We write ?
(l, v) = r to assert that r = l + |v| and that v is the substringof w between indices l and r.If ?h = v then ?
(lh, v) = rh (c1)If v xi,j is a prefix of ?h then ?
(lh, v) = li,j (c2)If xi,j v is a suffix of ?h then ?
(ri,j, v) = rh (c3)If xi,j v xi?,j?
is an infix of ?h then ?
(ri,j, v) = li?,j?
(c4)These constraints ensure that the substrings corresponding to the premises of theinference rule can be combined into the substrings corresponding to the conclusion bymeans of the yield function f .Based on the deduction system, a tabular parser for LCFRSs can be implementedusing standard dynamic programming techniques.
This parser will compute a packedrepresentation of the set of all derivation trees that the grammar G assigns to thestring w. Such a packed representation is often called a shared forest (Lang 1994).
Incombination with appropriate semirings, the shared forest is useful for many tasks insyntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009).6.2 Parsing ComplexityWe are interested in an upper bound on the runtime of the tabular parser that we havejust presented.
We can see that the parser runs in time O(|G||w|c), where |G| denotesthe size of some suitable representation of the grammar G, and c denotes the maximalnumber of instantiations of an inference rule (cf.
McAllester 2002).
Let us write c( f ) forthe specialization of c to inference rules for productions with yield function f .
We referto this value as the parsing complexity of f (cf.
Gildea 2010).
Then to show an upperbound on c it suffices to show an upper bound on the parsing complexities of the yieldfunctions that the parser has to handle.
An obvious such upper bound isc( f ) ?
2k +m?i=12kiHere we imagine that we could choose each endpoint in Equation (3) independently ofall the others.
By virtue of the constraints, however, some of the endpoints cannot bechosen freely; in particular, some of the substrings may be adjacent.
In general, to show370Kuhlmann Mildly Non-Projective Dependency Grammaran upper bound c(f ) ?
b we specify a strategy for choosing b endpoints, and then arguethat, given the constraints, these choices determine the remaining endpoints.Lemma 5For a yield function f : k1 ?
?
?
km ?
k we havec( f ) ?
k +m?i=1kiProofWe adopt the following strategy for choosing endpoints: For 1 ?
i ?
k, choose thevalue of lh.
Then, for 1 ?
i ?
m and 1 ?
j ?
ki, choose the value of ri,j.
It is not hardto see that these choices suffice to determine all other endpoints.
In particular, each leftendpoint li?,j?
will be shared either with the left endpoint lh of some component (byconstraint c2), or with some right endpoint ri,j (by constraint c4).
6.3 Universal RecognitionThe runtime of our parsing algorithm for LCFRSs is exponential in both the rank and thefan-out of the input grammar.
One may wonder whether there are parsing algorithmsthat can be substantially faster.
We now show that the answer to this question is likelyto be negative even if we restrict ourselves to canonical lexicalized LCFRSs.
To this endwe study the universal recognition problem for this class of grammars.The universal recognition problem for a class of linear context-free rewritingsystems is to decide, given a grammar G from the class in question and a string w,whether G yields ?w?.
A straightforward algorithm for solving this problem is to firstcompute the shared forest for G and w, and to return ?yes?
if and only if the sharedforest is non-empty.
Choosing appropriate data structures, the emptiness of sharedforests can be decided in linear time and space with respect to the size of the forest.Therefore, the computational complexity of universal recognition is upper-bounded bythe complexity of constructing the shared forest.
Conversely, parsing cannot be fasterthan universal recognition.In the next three lemmas we prove that the universal recognition problem forcanonical lexicalized LCFRSs is NP-complete unless we restrict ourselves to a class ofgrammars where both the fan-out and the rank of the yield functions are bounded byconstants.
Lemma 6, which shows that the universal recognition problem of lexicalizedLCFRSs is in NP, distinguishes lexicalized LCFRSs from general LCFRSs, for which theuniversal recognition problem is known to be PSPACE-complete (Kaji et al1992).
Thecrucial difference between general and lexicalized LCFRSs is the fact that in the latter,the size of the generated terms is bounded by the length of the input string.
Lemma 7and Lemma 8, which establish two NP-hardness results for lexicalized LCFRSs, arestronger versions of the corresponding results for general LCFRSs presented by Satta(1992), and are proved using similar reductions.
They show that the hardness resultshold under significant restrictions of the formalism: to lexicalized form and to canonicalyield functions.
Note that, whereas in Section 5.2 we have shown that every lexicalizedLCFRS is equivalent to a canonical one, the normal form transformation increases thesize of the original grammar by a factor that is at least exponential in the fan-out.Lemma 6The universal recognition problem of lexicalized LCFRSs is in NP.371Computational Linguistics Volume 39, Number 2ProofLet G be a lexicalized LCFRS, and let w be a string.
To test whether G yields ?w?, weguess a term t ?
T(G) and check whether t yields ?w?.
Let |t| denote the length of somestring representation of t. Since the yield functions of G are lexicalized, |t| ?
|w||G|.
Notethat we have|t| ?
|w||G| ?
|w|2 + 2|w||G|+ |G|2 = (|w|+ |G|)2Using a simple tabular algorithm, we can verify in time O(|w||G|) whether a candidateterm t belongs to T(G).
It is then straightforward to compute the string yield of t in timeO(|w||G|).
Thus we have a nondeterministic polynomial-time decider for the universalrecognition problem.
For the following two lemmas, recall the decision problem 3SAT, which is knownto be NP-complete.
An instance of 3SAT is a Boolean formula ?
in conjunctive normalform where each clause contains exactly three literals, which may be either variables ornegated variables.
We write m for the number of distinct variables that occur in ?, and nfor the number of clauses.
In the proofs the index i will always range over values from 1to m, and the index j will range over values from 1 to n.In order to make the grammars in the following reductions more readable, we useyield functions with more than one lexical anchor.
Our use of these yield functionsis severely restricted, however, and each of our grammars can be transformed into aproper lexicalized LCFRS without affecting the correctness or polynomial size of thereductions.Lemma 7The universal recognition problem for canonical lexicalized LCFRSs with unboundedfan-out and rank 1 is NP-hard.ProofTo prove this claim, we provide a polynomial-time reduction of 3SAT.
The basic idea isto use the derivations of the grammar to guess truth assignments for the variables, andto use the feature of unbounded fan-out to ensure that the truth assignment satisfies allclauses.Let ?
be an instance of 3SAT.
We construct a canonical lexicalized LCFRS G and astring w as follows.
Let M denote the m ?
n matrix with entries Mi,j = (vi, cj), that is,entries in the same row share the same variable, and entries in the same column sharethe same clause.
We set up G in such a way that each of its derivations simulates a row-wise iteration over M. Before visiting a new row, the derivation chooses a truth valuefor the corresponding variable, and sticks to that choice until the end of the row.
Thestring w takes the formw = w1 $ ?
?
?
$ wn where wj = cj,1 ?
?
?
cj,m cj,1 ?
?
?
cj,mThis string is built up during the iteration over M in a column-wise fashion, where eachcolumn corresponds to one component of a tuple with fan-out n. More specifically, foreach entry (vi, cj), the derivation generates one of two strings, denoted by ?i,j and ?
?i,j:?i,j = cj,i ?
?
?
cj,m cj,1 ?
?
?
cj,i ?
?i,j = cj,i372Kuhlmann Mildly Non-Projective Dependency GrammarThe string ?i,j is generated only if vi can be used to satisfy cj under the hypothesizedtruth assignment.
By this construction, every successful derivation of G represents atruth assignment that satisfies ?.
Conversely, using a satisfying truth assignment for ?,we will be able to construct a derivation of G that yields w.To see how the traversal of the matrix M can be implemented by the grammar G,consider the grammar fragment in Figure 9.
Each of the rules specifies one possible stepof the iteration for the pair (vi, cj) under the truth assignment vi = true; rules with left-hand side Fi,j (not shown here) specify possible steps under the assignment vi = false.
Lemma 8The universal recognition problem for canonical lexicalized LCFRSs with unboundedrank and fan-out 2 is NP-hard.ProofWe provide another polynomial-time reduction of 3SAT to a grammar G and a string w,again based on the matrix M mentioned in the previous proof.
Also as in the previousreduction, we set up the grammar G to simulate a row-wise iteration over M. The majordifference this time is that the entries of M are not visited during one long rank 1derivation, but during mn rather short fan-out 2 subderivations.
The string w isw = w,1 ?
?
?w,m $ w,1 ?
?
?w,nwhere w,i = ai,1 ?
?
?
ai,n bi,1 ?
?
?
bi,n and w,j = c1,j ?
?
?
cm,j c1,j ?
?
?
cm,jDuring the traversal of M, for each entry (vi, cj), we generate a tuple consisting of twosubstrings of w. The right component of the tuple consists of one the two strings ?i,jand ?
?i,j mentioned previously.
As before, the string ?i,j is generated only if vi can beused to satisfy cj under the hypothesized truth assignment.
The left component consistsof one of two strings, denoted by ?i,j and ?
?i,j:?i,1 = ai,1 ?
?
?
ai,n bi,1 ?i,j = bi,j (1 < j) ?
?i,n = ai,n bi,1 ?
?
?
bi,n ?
?i,j = ai,j (j < n)These strings are generated to represent the truth assignments vi = true and vi = false,respectively.
By this construction, each substring w,i can be derived in exactly one oftwo ways, ensuring a consistent truth assignment for all subderivations that are linkedto the same variable vi.Figure 9A fragment of the grammar used in the proof of Lemma 7.373Computational Linguistics Volume 39, Number 2The grammar G is defined as follows.
There is one rather complex rule to rewritethe start symbol S; this rule sets up the general topology of w. Let I be the m ?
n matrixwith entries Ii,j = (j ?
1)m + i.
Define x1 to be the sequence of variables of the form xh,1,where the argument index i is taken from a row-wise reading of the matrix I; in thiscase, the argument indices in x will simply go up from 1 to mn.
Now define x2 to be thesequence of variables of the form xh,2, where h is taken from a column-wise reading ofthe matrix I.
Then S can be expanded with the ruleS ?
?x1 $x2?
(V1,1, .
.
.
, V1,n, .
.
.
, Vm,1, .
.
.
, Vm,n)Note that there is one nonterminal Vi,j for each variable?clause pair (vi, cj).
These non-terminals can be rewritten using the following rules:Vi,1 ?
?
?i,1, x?
(Ti,1 ) Vi,j ?
?
?i,j, x?
(Ti,j)Vi,n ?
??
?i,n, x?
(Fi,n) Vi,j ?
??
?i,j, x?
(Fi,j)The remaining rules rewrite the nonterminals Ti,j and Fi,j:Ti,j ?
??i,j?
(if vi occurs in cj) Ti,j ?
??
?i,j?Fi,j ?
??i,j?
(if v?i occurs in cj) Fi,j ?
??
?i,j?It is not hard to see that both G and w can be constructed in polynomial time.
7.
Block-DegreeTo obtain efficient parsing, we would like to have grammars with as low a fan-out aspossible.
Therefore it is interesting to know how low we can go without losing too muchcoverage.
In lexicalized LCFRSs extracted from dependency treebanks, the fan-out of agrammar has a structural correspondence in the maximal number of blocks per subtree,a measure known as ?block-degree.?
In this section we formally define block-degree,and evaluate grammar coverage under different bounds on this measure.7.1 Definition of Block-DegreeRecall the concept of ?blocks?
that was defined in Section 4.2.
The block-degree of anode u of a dependency tree D is the number of distinct blocks of u.
The block-degreeof D is the maximal block-degree of its nodes.2Example 6Figure 10 shows two non-projective dependency trees.
For D1, consider the node 2.
Thedescendants of 2 fall into two blocks, marked by the dashed boxes.
Because this is themaximal number of blocks per node in D1, the block-degree of D1 is 2.
Similarly, we canverify that the block-degree of the dependency tree D2 is 3.2 We note that, instead of counting the blocks of each node, one may also count the gaps between theseblocks and define the ?gap-degree?
of a dependency tree (Holan et al1998).374Kuhlmann Mildly Non-Projective Dependency GrammarFigure 10Block-degree.A dependency tree is projective if its block-degree is 1.
In a projective dependencytree, each subtree corresponds to a substring of the underlying tuple of strings.
In a non-projective dependency tree, a subtree may span over several, discontinuous substrings.7.2 Computing the Block-DegreesUsing a straightforward extension of the algorithm in Table 1, the block-degrees of allnodes of a dependency tree D can be computed in time O(m), where m is the totalnumber of blocks.
To compute the block-degree of D, we simply take the maximumover the degrees of each node.
We can also adapt this procedure to test whether D isprojective, by aborting the computation as soon as we discover that some node hasmore than one block.
The runtime of this test is linear in the number of nodes of D.7.3 Block-Degree in Extracted GrammarsIn a lexicalized LCFRS extracted from a dependency treebank, there is a one-to-onecorrespondence between the blocks of a node u and the components of the templateof the yield function f extracted for u.
In particular, the fan-out of f is exactly theblock-degree of u.
As a consequence, any bound on the block-degree of the trees inthe treebank translates into a bound on the fan-out of the extracted grammar.
This hasconsequences for the generative capacity of the grammars: As Seki et al(1991) show,the class of LCFRSs with fan-out k > 1 can generate string languages that cannot begenerated by the class of LCFRSs with fan-out k ?
1.It may be worth emphasizing that the one-to-one correspondence between blocksand tuple components is a consequence of two characteristic properties of extractedgrammars (Properties 3 and 4), and does not hold for non-canonical lexicalizedLCFRSs.Example 7The following term induces a two-node dependency tree with block-degree 1, butcontains yield functions with fan-out 2: ?a x1 x2?
(?b, ??).
Note that the yield functionsin this term violate both Property 3 and Property 4.7.4 Coverage on Dependency TreebanksIn order to assess the consequences of different bounds on the fan-out, we now evaluatethe block-degree of dependency trees in real-world data.
Specifically, we look into five375Computational Linguistics Volume 39, Number 2dependency treebanks used in the 2006 CoNLL shared task on dependency parsing(Buchholz and Marsi 2006): the Prague Arabic Dependency Treebank (Hajic?
et al2004),the Prague Dependency Treebank of Czech (Bo?hmova?
et al2003), the Danish Depen-dency Treebank (Kromann 2003), the Slovene Dependency Treebank (Dz?eroski et al2006), and the Metu-Sabanc?
treebank of Turkish (Oflazer et al2003).
The full data usedin the CoNLL shared task also included treebanks that were produced by conversionof corpora originally annotated with structures other than dependencies, which is apotential source of ?noise?
that one has to take into account when interpreting anyfindings.
Here, we consider only genuine dependency treebanks.
More specifically, ourstatistics concern the training sections of the treebanks that were set off for the task.
Forsimilar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), andMaier and Lichte (2011).Our results are given in Table 3.
For each treebank, we list the number of rulesextracted from that treebank, as well as the number of corresponding dependency trees.We then list the number of rules that we lose if we restrict ourselves to rules with fan-out = 1, or rules with fan-out ?
2, as well as the number of dependency trees that welose because their construction trees contain at least one such rule.
We count rule tokens,meaning that two otherwise identical rules are counted twice if they were extractedfrom different trees, or from different nodes in the same tree.By putting the bound at fan-out 1, we lose between 0.74% (Arabic) and 1.75%(Slovene) of the rules, and between 11.16% (Arabic) and 23.15% (Czech) of the treesin the treebanks.
This loss is quite substantial.
If we instead put the bound at fan-out?
2, then rule loss is reduced by between 94.16% (Turkish) and 99.76% (Arabic), andtree loss is reduced by between 94.31% (Turkish) and 99.39% (Arabic).
This outcomeis surprising.
For example, Holan et al(1998) argue that it is impossible to give atheoretical upper bound for the block-degree of reasonable dependency analyses ofCzech.
Here we find that, if we are ready to accept a loss of as little as 0.02% of therules extracted from the Prague Dependency Treebank, and up to 0.5% of the trees, thensuch an upper bound can be set at a block-degree as low as 2.8.
Well-NestednessThe parsing of LCFRSs is exponential both in the fan-out and in the rank of thegrammars.
In this section we study ?well-nestedness,?
another restriction on the non-projectivity of dependency trees, and show how enforcing this constraint allows us torestrict our attention to the class of LCFRSs with rank 2.Table 3Loss in coverage under the restriction to yield functions with fan-out = 1 and fan-out ?
2.fan-out = 1 fan-out ?
2rules trees rules trees rules treesArabic 5,839 1,460 411 163 1 1Czech 1,322,111 72,703 22,283 16,831 328 312Danish 99,576 5,190 1,229 811 11 9Slovene 30,284 1,534 530 340 14 11Turkish 62,507 4,997 924 580 54 33376Kuhlmann Mildly Non-Projective Dependency Grammar8.1 Definition of Well-NestednessLet D be a dependency tree, and let u and v be nodes of D. The descendants of uand v overlap, denoted byu v, if there exist nodes ul, ur ?u and vl, vr ?vsuch thatul < vl < ur < vr or vl < ul < vr < urA dependency tree D is called well-nested if for all pairs of nodes u, v of Du v implies thatu ?v = ?In other words,u andv may overlap only if u is an ancestor of v, or v is an ancestorof u.
If this implication does not hold, then D is called ill-nested.Example 8Figure 11 shows three non-projective dependency trees.
Both D1 and D2 are well-nested:D1 does not contain any overlapping sets of descendants at all.
In D2, although1and2 overlap, it is also the case that1 ?2.
In contrast, D3 is ill-nested, as2 3 but2 ?3 = ?The following lemma characterizes well-nestedness in terms of blocks.Lemma 9A dependency tree is ill-nested if and only if it contains two sibling nodes u, v and blocksu1,u2 of u and v1,v2 of v such thatu1 < v1 < u2 < v2 (4)ProofLet D be a dependency tree.
Suppose that D contains a configuration of the form (4).This configuration witnesses that the setsu andv overlap.
Because u, v are siblings,u ?v = ?.
Therefore we conclude that D is ill-nested.
Conversely now, supposethat D is ill-nested.
In this case, there exist two nodes u and v such thatu v andu ?v = ?
(?
)Figure 11Well-nestedness and ill-nestedness.377Computational Linguistics Volume 39, Number 2Here, we may assume u and v to be siblings: otherwise, we may replace either u or vwith its parent node, and property (?)
will continue to hold.
Becauseu v, thereexist descendants ul, ur ?u and vl, vr ?v such thatul < vl < ur < vr or vl < ul < vr < urWithout loss of generality, assume that we have the first case.
The nodes ul and ur belongto different blocks of u, say u1 and u2; and the nodes vl and vr belong to different blocksof v, say v1 and v2.
Then it is not hard to verify Equation (4).
Note that projective dependency trees are always well-nested; in these structures,every node has exactly one block, so configuration (4) is impossible.
For every k > 1,there are both well-nested and ill-nested dependency trees with block-degree k.8.2 Testing for Well-NestednessBased on Lemma 9, testing whether a dependency tree D is well-nested can be done intime linear in the number of blocks in D using a simple subsequence test as follows.
Werun the algorithm given in Table 1, maintaining a stack s[u] for every node u.
The firsttime we make a down step to u, we push u to the stack for the parent of u; every othertime, we pop the stack for the parent until we either find u as the topmost element, or thestack becomes empty.
In the latter case, we terminate the computation and report that Dis ill-nested; if the computation can be completed without any stack ever becomingempty, we report that D is well-nested.To show that the algorithm is sound, suppose that some stack s[p] becomes emptywhen making a down step to some child v of p. In this case, the node v must have beenpopped from s[p] when making a down step to some other child u of p, and that childmust have already been on the stack before the first down step to v. This witnesses theexistence of a configuration of the form in Equation (4).8.3 Well-Nestedness in Extracted GrammarsJust like block-degree, well-nestedness can be characterized in terms of yield functions.Recall the notation x <f y from Section 5.1.
A yield functionf : k1 ?
?
?
km ?
k , f = ?
?1, .
.
.
,?k?is ill-nested if there are argument indices 1 ?
i1, i2 ?
m with i1 = i2 and componentindices 1 ?
j1, j?1 ?
ki1 , 1 ?
j2, j?2 ?
ki2 such thatxi1,j1 <f xi2,j2 <f xi1,j?1 <f xi2,j?2 (5)Otherwise, we say that f is well-nested.
As an immediate consequence of Lemma 9, arestriction to well-nested dependency trees translates into a restriction to well-nestedyield functions in the extracted grammars.
This puts them into the class of whatKanazawa (2009) calls ?well-nested multiple context-free grammars.
?3 These grammars3 Kanazawa (2009) calls a multiple context-free grammar well-nested if each of its rules is non-deleting,non-permuting (our Property 2), and well-nested according to (5).378Kuhlmann Mildly Non-Projective Dependency Grammarhave a number of interesting properties that set them apart from general LCFRSs; inparticular, they have a standard pumping lemma (Kanazawa 2009).
The yield languagesgenerated by well-nested multiple context-free grammars form a proper subhierarchywithin the languages generated by general LCFRSs (Kanazawa and Salvati 2010).
Per-haps the most prominent subclass of well-nested LCFRSs is the class of tree-adjoininggrammars (Joshi and Schabes 1997).Similar to the situation with block-degree, the correspondence between structuralwell-nestedness and syntactic well-nestedness is tight only for canonical grammars.For non-canonical grammars, syntactic well-nestedness alone does not imply structuralwell-nestedness, nor the other way around.8.4 Coverage on Dependency TreebanksTo estimate the coverage of well-nested grammars, we extend the evaluation presentedin Section 7.4.
Table 4 shows how many rules and trees in the five dependency treebankswe lose if we restrict ourselves to well-nested yield functions with fan-out ?
2.
Thelosses reported in Table 3 are repeated here for comparison.
Although the coverageof well-nested rules is significantly smaller than the coverage of rules without thisrequirement, rule loss is still reduced by between 92.65% (Turkish) and 99.51% (Arabic)when compared to the fan-out = 1 baseline.8.5 Binarization of Well-Nested GrammarsOur main interest in well-nestedness comes from the following:Lemma 10The universal recognition problem for well-nested lexicalized LCFRS with fan-out k andunbounded rank can be decided in timeO(|G| ?
|w|2k+2)To prove this lemma, we will provide an algorithm for the binarization of well-nested lexicalized LCFRSs.
In the context of LCFRSs, a binarization is a procedure fortransforming a grammar into an equivalent one with rank at most 2.
Binarization,either explicit at the level or the grammar or implicit at the level of some parsingalgorithm, is essential for achieving efficient recognition algorithms, in particular theusual cubic-time algorithms for context-free grammars.
Note that our binarization onlyTable 4Loss in coverage under the restriction to yield functions with fan-out = 1, fan-out ?
2,and to well-nested yield functions with fan-out ?
2 (last column).fan-out = 1 fan-out ?
2 + well-nestedrules trees rules trees rules trees rules treesArabic 5,839 1,460 411 163 1 1 2 2Czech 1,322,111 72,703 22,283 16,831 328 312 407 382Danish 99,576 5,190 1,229 811 11 9 17 15Slovene 30,284 1,534 530 340 14 11 17 13Turkish 62,507 4,997 924 580 54 33 68 43379Computational Linguistics Volume 39, Number 2preserves weak equivalence; in effect, it reduces the universal recognition problem forwell-nested lexicalized LCFRSs to the corresponding problem for well-nested LCFRSswith rank 2.
Many interesting semiring computations on the original grammar can besimulated on the binarized grammar, however.
A direct parsing algorithm for well-nested dependency trees has been presented by Go?mez-Rodr?
?guez, Carroll, and Weir(2011).The binarization that we present here is a special case of the binarization proposedby Go?mez-Rodr?
?guez, Kuhlmann, and Satta (2010).
They show that every well-nestedLCFRS can be transformed (at the cost of a linear size increase) into a weakly equivalentone in which all yield functions are either constants (that is, have rank 0) or binaryfunctions of one of two types:?x1, .
.
.
, xk1 y1, .
.
.
, yk2?
: k1 k2 ?
(k1 + k2 ?
1) (concatenation) (6)?x1, .
.
.
, xj y1, .
.
.
, yk2 xj+1, .
.
.
, xk1?
: k1 k2 ?
(k1 + k2 ?
2) (wrapping) (7)A concatenation function takes a k1-tuple and a k2-tuple and returns the (k1 + k2 ?
1)-tuple that is obtained by concatenating the two arguments.
The simplest concatenationfunction is the standard concatenation operation ?x y?.
We will write conc : k1 k2 to referto a concatenation function of the type given in Equation (6).
By counting endpoints, wesee that the parsing complexity of concatenation functions isc(conc : k1 k2) ?
2k1 + 2k2 ?
1A wrapping function takes a k1-tuple (for some k1 ?
2) and a k2-tuple and returns the(k1 + k2 ?
2)-tuple that is obtained by ?wrapping?
the first argument around the secondargument, filling some gap in the former.
The simplest function of this type is ?x1 y x2?,which wraps a 2-tuple around a 1-tuple.
We write wrap : k1 k2 j to refer to a wrappingfunction of the type given in Equation (7).
The parsing complexity isc(wrap : k1 k2 j) ?
2k1 + 2k2 ?
2 (for all choices of j)The constants of the binarized grammar have the form ??
?, ?
?, ?
?, and ?a?, where a is theanchor of some yield function of the original grammar.8.5.1 Parsing Complexity.
Before presenting the actual binarization, we determine theparsing complexity of the binarized grammar.
Because the binarization preserves thefan-out of the original grammar, and because in a grammar with fan-out k, for con-catenation functions conc : k1 k2 we have k1 + k2 ?
1 ?
k and for wrapping functionswrap : k1 k2 j we have k1 + k2 ?
2 ?
k, we can rewrite the general parsing complexities asc(conc : k1 k2) ?
2k1 + 2k2 ?
1 = 2(k1 + k2 ?
1) + 1 ?
2k + 1c(wrap : k1 k2 j) ?
2k1 + 2k2 ?
2 = 2(k1 + k2 ?
2) + 2 ?
2k + 2Thus the maximal parsing complexity in the binarized grammar is 2k + 2; this isachieved by wrapping operations.
This gives the bound stated in Lemma 10.380Kuhlmann Mildly Non-Projective Dependency GrammarFigure 12Binarization of well-nested LCFRSs (complex cases).8.5.2 Binarization.
We now turn to the actual binarization.
Consider a ruleA ?
f (A1, .
.
.
, Am)where f is not already a concatenation function, wrapping function, or constant.
Wedecompose this rule into up to three rulesA ?
f ?
(B, C) B ?
f1(B1, .
.
.
, Bm1 ) C ?
f2(C1, .
.
.
, Cm2 )as follows.
We match the template of f against one of three cases, shown schematicallyin Figure 12.
In each case we select a concatenation or wrapping function f ?
(shown inthe right half of the figure), and split up the template of f into two parts defining yieldfunctions f1 and f2, respectively.
In Figure 12, f1 is drawn shaded, and f2 is drawn non-shaded.4 The split of f partitions the variables that occur in the template, in the sense4 In order for these parts to make well-defined templates, we will in general need to rename the variables.We leave this renaming implicit here.381Computational Linguistics Volume 39, Number 2that if for some argument index 1 ?
i ?
m, either f1 or f2 contains any variable withargument index i, then it contains all such variables.
The two sequencesB1, .
.
.
, Bm1 and C1, .
.
.
, Cm2 are obtained from A1, .
.
.
, Amby collecting the nonterminal Ai if the variables with argument index i belong to thetemplate of f1 and f2, respectively.
The nonterminals B and C are fresh nonterminals.
Wedo not create rules for f1 and f2 if they are identity functions.Example 9We illustrate the binarization by showing how to transform the ruleA ?
?x1 a x2 y1, y2, y3 x3?
(A1, A2)The template ?x1 a x2 y1, y2, y3 x3?
is complex and matches Case 3 in Figure 12, becauseits first component starts with the variable x1 and its last component ends with thevariable x3.
We therefore split the template into two smaller parts ?x1 a x2, x3?
and?y1, y2, y3?.
The function ?y1, y2, y3?
is an identity.
We therefore create two rules:A ?
f ?1(X, A2) , f ?1 = wrap : 2 3 1 = ?x1 y1, y2, y3 x2?
X ?
?x1 a x2, x3?
(A1)Note that the index j for the wrapping function was chosen to be j = 2 because therewere more component boundaries between x2 and x3 than between x1 and x2.
Thetemplate ?x1 a x2, x3?
requires further decomposition according to Case 3.
This time, thetwo smaller parts are the identity function ?x1, x2, x3?
and the constant ?a?.
We thereforecreate the following rules:X ?
f ?2(A1, Y) , f ?2 = wrap : 3 1 1 = ?x1 y x2, x3?
Y ?
?a?At this point, the transformation ends.8.5.3 Correctness.
We need to show that the fan-out of the binarized grammar does notexceed the fan-out of the original grammar.
We reason as follows.
Starting from someinitial yield function f0 : k1 ?
?
?
km ?
k, each step of the binarization decomposes someyield function f into two new yield functions f1, f2.
Let us denote the fan-outs of thethree functions by h, h1, h2, respectively.
We haveh = h1 + h2 ?
1 in Case 1 and Case 2 (8)h = h1 + h2 ?
2 in Case 3 (9)From Equation (8) it is clear that in Case 1 and Case 2, both h1 and h2 are upper-bounded by h. In Case 3 we have h1 ?
2, which together with Equation (9) impliesthat h2 ?
h. However, h1 is upper-bounded by h only if h2 ?
2; if h2 = 1, then h1 maybe greater than h. As an example, consider the decomposition of ?x1 a x2?
(fan-out 1) intothe wrapping function ?x1, x2?
(fan-out 2) and the constant ?a?
(fan-out 1).
But becausein Case 3 the index j is chosen to maximize the number of component boundariesbetween the variables xi,j and xi,j+1, the assumption h2 = 1 implies that each of the h1components of f1 contains at least one variable with argument index i?if there were382Kuhlmann Mildly Non-Projective Dependency Grammara component without such a variable, then the two variables that surrounded that com-ponent would have given rise to a different choice of j.
Hence we deduce that h1 ?
ki.9.
ConclusionIn this article, we have presented a formalism for non-projective dependency grammarbased on linear context-free rewriting systems, along with a technique for extractinggrammars from dependency treebanks.
We have shown that parsing with the full classof these grammars is intractable.
Therefore, we have investigated two constraints on thenon-projectivity of dependency trees, block-degree and well-nestedness.
Jointly, thesetwo constraints define a class of ?mildly?
non-projective dependency grammars thatcan be parsed in polynomial time.Our results in Sections 7 and 8 allow us to relate the formal power of an LCFRSto the structural properties of the dependency structures that it induces.
Although wehave used this relation to identify a class of dependency grammars that can be parsedin polynomial time, it also provides us with a new perspective on the question aboutthe descriptive adequacy of a grammar formalism.
This question has traditionally beendiscussed on the basis of strong and weak generative capacity (Bresnan et al1982;Huybregts 1984; Shieber 1985).
A notion of generative capacity based on dependencytrees makes a useful addition to this discussion, in particular when comparingformalisms for which no common concept of strong generative capacity exists.
As anexample for a result in this direction, see Koller and Kuhlmann (2009).We have defined the dependency trees that an LCFRS induces by means of acompositional mapping on the derivations.
While we would claim that compositionalityis a generally desirable property, the particular notion of induction is up for discussion.In particular, our interpretation of derivations may not always be in line with how thegrammar producing these derivations is actually used.
One formalism for which such amismatch between derivation trees and dependency trees has been pointed out is tree-adjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998).Resolving this mismatch provides an interesting line of future work.One aspect that we have not discussed here is the linguistic adequacy of block-degree and well-nestedness.
Each of our dependency grammars is restricted to a finiteblock-degree.
As a consequence of this restriction, our dependency grammars are notexpressive enough to capture linguistic phenomena that require unlimited degreesof non-projectivity, such as the ?scrambling?
in German subordinate clauses (Becker,Rambow, and Niv 1992).
The question whether it is reasonable to assume a boundon the block-degree of dependency trees, perhaps for some performance-based reason,is open.
Likewise, it is not clear whether well-nestedness is a ?natural?
constraint ondependency analyses (Chen-Main and Joshi 2010; Maier and Lichte 2011).Although most of the results that we have presented in this article are of a theo-retical nature, some of them have found their way into practical systems.
In particular,the extraction technique from Section 4 is used by the data-driven dependency parserof Maier and Kallmeyer (2010).AcknowledgmentsThe author gratefully acknowledgesfinancial support from TheGerman Research Foundation(Sonderforschungsbereich 378,project MI 2) and The Swedish ResearchCouncil (diary no.
2008-296).ReferencesBecker, Tilman, Owen Rambow, andMichael Niv.
1992.
The derivationalgenerative power of formal systems,or: Scrambling is beyond LCFRS.
IRCSReport 92-38, University of Pennsylvania,Philadelphia, PA.383Computational Linguistics Volume 39, Number 2Bertsch, Eberhard and Mark-Jan Nederhof.2001.
On the complexity of someextensions of RCG parsing.
In Proceedingsof the Seventh International Workshop onParsing Technologies (IWPT), pages 66?77,Beijing.Bodirsky, Manuel, Marco Kuhlmann, andMathias Mo?hl.
2005.
Well-nesteddrawings as models of syntactic structure.In Proceedings of the 10th Conference onFormal Grammar (FG) and Ninth Meetingon Mathematics of Language (MOL),pages 195?203, Edinburgh.Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,and Barbora Hladka?.
2003.
The PragueDependency Treebank: A three-levelannotation scenario.
In Abeille?, Anne,editor.
Treebanks: Building and Using ParsedCorpora.
Kluwer Academic Publishers,Dordrecht, chapter 7, pages 103?127.Boullier, Pierre.
1998.
Proposal for a naturallanguage processing syntactic backbone.Rapport de recherche 3342, INRIARocquencourt, Paris, France.Boullier, Pierre.
2004.
Range ConcatenationGrammars.
In Harry C. Bunt, John Carroll,and Giorgio Satta, editors, NewDevelopments in Parsing Technology,volume 23 of Text, Speech and LanguageTechnology.
Kluwer Academic Publishers,Dordrecht, pages 269?289.Bresnan, Joan, Ronald M. Kaplan, StanleyPeters, and Annie Zaenen.
1982.Cross-serial dependencies in Dutch.Linguistic Inquiry, 13(4):613?635.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings of theTenth Conference on Computational NaturalLanguage Learning (CoNLL), pages 149?164,New York, NY.Candito, Marie-He?le`ne and Sylvain Kahane.1998.
Can the TAG derivation treerepresent a semantic graph?
An answerin the light of Meaning-Text Theory.In Proceedings of the Fourth Workshop onTree Adjoining Grammars and RelatedFormalisms (TAG+), pages 21?24,Philadelphia, PA.Charniak, Eugene.
1996.
Tree-bankgrammars.
In Proceedings of the13th National Conference on ArtificialIntelligence (AAAI) and Eighth InnovativeApplications of Artificial IntelligenceConference (IAAI), volume 2,pages 1031?1036, Portland, OR.Chen-Main, Joan and Aravind K. Joshi.2010.
Unavoidable ill-nestedness innatural language and the adequacy oftree local-MCTAG induced dependencystructures.
In Proceedings of the TenthInternational Conference on Tree AdjoiningGrammars and Related Formalisms (TAG+),New Haven, CT.
Available at http://dx.doi.org/10.1093/logcom/exs012.Crescenzi, Pierluigi, Daniel Gildea, AndreaMarino, Gianluca Rossi, and Giorgio Satta.2011.
Optimal head-driven parsingcomplexity for linear context-freerewriting systems.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics (ACL),pages 450?459, Portland, OR.Dz?eroski, Sas?o, Tomaz?
Erjavec, NinaLedinek, Petr Pajas, Zdenek Z?abokrtsky,and Andreja Z?ele.
2006.
Towards aSlovene dependency treebank.
In FifthInternational Conference on LanguageResources and Evaluations (LREC),pages 1388?1391, Genoa.Gaifman, Haim.
1965.
Dependency systemsand phrase-structure systems.
Informationand Control, 8(3):304?337.Gildea, Daniel.
2010.
Optimal parsingstrategies for linear context-free rewritingsystems.
In Proceedings of Human LanguageTechnologies: The 2010 Annual Conference ofthe North American Chapter of the Associationfor Computational Linguistics (NAACL),pages 769?776, Los Angeles, CA.Go?mez-Rodr?
?guez, Carlos, John Carroll, andDavid J. Weir.
2011.
Dependency parsingschemata and mildly non-projectivedependency parsing.
ComputationalLinguistics, 37(3):541?586.Go?mez-Rodr?
?guez, Carlos, MarcoKuhlmann, and Giorgio Satta.
2010.Efficient parsing of well-nested linearcontext-free rewriting systems.
InProceedings of Human Language Technologies:The 2010 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL),pages 276?284, New Haven, CT.Go?mez-Rodr?
?guez, Carlos, MarcoKuhlmann, Giorgio Satta, and David J.Weir.
2009.
Optimal reduction of rulelength in linear context-free rewritingsystems.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference ofthe North American Chapter of the Associationfor Computational Linguistics (NAACL),pages 539?547, Boulder, CO.Go?mez-Rodr?
?guez, Carlos and GiorgioSatta.
2009.
An optimal-time binarizationalgorithm for linear context-free rewritingsystems with fan-out two.
In Proceedingsof the Joint Conference of the 47th Annual384Kuhlmann Mildly Non-Projective Dependency GrammarMeeting of the Association for ComputationalLinguistics (ACL) and the FourthInternational Joint Conference on NaturalLanguage Processing of the Asian Federationof Natural Language Processing (IJCNLP),pages 985?993, Singapore.Goodman, Joshua.
1999.
Semiring parsing.Computational Linguistics, 25(4):573?605.Hajic?, Jan, Otakar Smrz?, Petr Zema?nek,Jan S?naidauf, and Emanuel Bes?ka.
2004.Prague Arabic Dependency Treebank:Development in data and tools.
InProceedings of the International Conference onArabic Language Resources and Tools,pages 110?117, Cairo.Havelka, Jir???.
2007.
Beyond projectivity:Multilingual evaluation of constraints andmeasures on non-projective structures.In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 608?615, Prague.Hays, David G. 1964.
Dependency theory:A formalism and some observations.Language, 40(4):511?525.Holan, Toma?s?, Vladislav Kubon?, Karel Oliva,and Martin Pla?tek.
1998.
Two usefulmeasures of word order complexity.In Proceedings of the Workshop onProcessing of Dependency-Based Grammars,pages 21?29, Montre?al.Hudson, Richard.
2007.
Language Networks.The New Word Grammar.
Oxford UniversityPress, Oxford.Huybregts, Riny.
1984.
The weak inadequacyof context-free phrase structure grammars.In Ger de Haan, Mieke Trommelen, andWim Zonneveld, editors, Van periferie naarkern.
Foris, Dordrecht, pages 81?99.Joshi, Aravind K. and Yves Schabes.
1997.Tree-Adjoining Grammars.
In GrzegorzRozenberg and Arto Salomaa, editors,Handbook of Formal Languages, volume 3.Springer, Berlin, pages 69?123.Kaji, Yuichi, Ryuichi Nakanishi, HiroyukiSeki, and Tadao Kasami.
1992.
Theuniversal recognition problems formultiple context-free grammars and forlinear context-free rewriting systems.IEICE Transactions on Information andSystems, E75-D(1):78?88.Kallmeyer, Laura.
2010.
Parsing BeyondContext-Free Grammars.
Springer, Berlin.Kanazawa, Makoto.
2009.
The pumpinglemma for well-nested multiplecontext-free languages.
In Developmentsin Language Theory.
Proceedings of the13th International Conference, DLT 2009,volume 5583 of Lecture Notes in ComputerScience, pages 312?325, Stuttgart.Kanazawa, Makoto and Sylvain Salvati.2010.
The copying power of well-nestedmultiple context-free grammars.In Adrian-Horia Dediu, Henning Fernau,and Carlos Mart?
?n-Vide, editors, Languageand Automata Theory and Applications.Proceedings of the 4th InternationalConference, LATA 2010, volume 6031of Lecture Notes in Computer Science,pages 344?355, Trier.Koller, Alexander and Marco Kuhlmann.2009.
Dependency trees and thestrong generative capacity of CCG.
InProceedings of the 12th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL),pages 460?468, Athens.Kracht, Marcus.
2003.
The Mathematicsof Language, volume 63 of Studies inGenerative Grammar.
Mouton deGruyter, Paris.Kromann, Matthias Trautner.
2003.
TheDanish Dependency Treebank andthe underlying linguistic theory.
InProceedings of the Second Workshop onTreebanks and Linguistic Theories (TLT),pages 217?220, Va?xjo?.Ku?bler, Sandra, Ryan McDonald, andJoakim Nivre.
2009.
DependencyParsing.
Synthesis Lectures on HumanLanguage Technologies.
Morgan andClaypool.Kuhlmann, Marco and Joakim Nivre.2006.
Mildly non-projective dependencystructures.
In Proceedings of the21st International Conference onComputational Linguistics (COLING) and44th Annual Meeting of the Associationfor Computational Linguistics (ACL)Main Conference Poster Sessions,pages 507?514, Sydney.Kuhlmann, Marco and Giorgio Satta.2009.
Treebank grammar techniques fornon-projective dependency parsing.
InProceedings of the 12th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL),pages 478?486, Athens.Lang, Bernard.
1994.
Recognition can beharder than parsing.
ComputationalIntelligence, 10(4):486?494.Li, Zhifei and Jason Eisner.
2009.First- and second-order expectationsemirings with applications tominimum-risk training on translationforests.
In Proceedings of the 2009Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 40?51, Singapore.385Computational Linguistics Volume 39, Number 2Maier, Wolfgang and Laura Kallmeyer.
2010.Discontinuity and non-projectivity: Usingmildly context-sensitive formalisms fordata-driven parsing.
In Proceedings of theTenth International Conference on TreeAdjoining Grammars and Related Formalisms(TAG+), New Haven, CT.Maier, Wolfgang and Timm Lichte.
2011.Characterizing discontinuity in constituenttreebanks.
In Philippe de Groote, MarkusEgg, and Laura Kallmeyer, editors,Formal Grammar.
Proceedings of the 14thInternational Conference, FG 2009, RevisedSelected Papers, volume 5591 of LectureNotes in Computer Science, pages 167?182,Bordeaux.Maier, Wolfgang and Anders S?gaard.
2008.Treebanks and mild context-sensitivity.In Proceedings of the 13th Conference onFormal Grammar (FG), pages 61?76,Hamburg.McAllester, David.
2002.
On the complexityanalysis of static analyses.
Journal of theAssociation for Computing Machinery,49(4):512?537.Mel?c?uk, Igor.
1988.
Dependency Syntax:Theory and Practice.
State Universityof New York Press, Albany, NY.Michaelis, Jens.
1998.
Derivationalminimalism is mildly context-sensitive.In Logical Aspects of ComputationalLinguistics, Third International Conference,LACL 1998, Selected Papers, volume 2014of Lecture Notes in Computer Science,pages 179?198, Grenoble.Michaelis, Jens.
2001.
On Formal Propertiesof Minimalist Grammars.
Ph.D. thesis,Universita?t Potsdam, Potsdam,Germany.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.In Proceedings of the Joint Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP) and ComputationalNatural Language Learning (CoNLL),pages 915?932, Prague.Oflazer, Kemal, Bilge Say, Dilek ZeynepHakkani-Tu?r, and Go?khan Tu?r.
2003.Building a Turkish treebank.
In Abeille?,Anne, editor.
Treebanks: Building andUsing Parsed Corpora.
Kluwer AcademicPublishers, Dordrecht, chapter 15,pages 261?277.Rambow, Owen and Aravind K. Joshi.1997.
A formal look at dependencygrammars and phrase-structuregrammars, with special considerationof word-order phenomena.
In Leo Wanner,editor, Recent Trends in Meaning-TextTheory, volume 39 of Studies in Language,Companion Series.
John Benjamins,Amsterdam, pages 167?190.Rambow, Owen, K. Vijay-Shanker, andDavid J. Weir.
1995.
D-Tree grammars.In Proceedings of the 33rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 151?158,Cambridge, MA.Sagot, Beno?
?t and Giorgio Satta.
2010.Optimal rank reduction for linearcontext-free rewriting systems withfan-out two.
In Proceedings of the 48thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 525?533, Uppsala.Satta, Giorgio.
1992.
Recognition oflinear context-free rewriting systems.In Proceedings of the 30th AnnualMeeting of the Association forComputational Linguistics (ACL),pages 89?95, Newark, DE.Schabes, Yves.
1990.
Mathematical andComputational Aspects of LexicalizedGrammars.
Ph.D. thesis, Universityof Pennsylvania, Philadelphia, PA.Schabes, Yves, Anne Abeille?, andAravind K. Joshi.
1988.
Parsingstrategies with ?lexicalized?
grammars:Application to tree adjoining grammars.In Proceedings of the Twelfth InternationalConference on Computational Linguistics(COLING), pages 578?583, Budapest.Seki, Hiroyuki, Takashi Matsumura,Mamoru Fujii, and Tadao Kasami.1991.
On Multiple Context-FreeGrammars.
Theoretical ComputerScience, 88(2):191?229.Sgall, Petr, Eva Hajic?ova?, and JarmilaPanevova?.
1986.
The Meaning of theSentence in Its Semantic and PragmaticAspects.
Springer, Berlin.Shieber, Stuart M. 1985.
Evidence againstthe context-freeness of natural language.Linguistics and Philosophy, 8(3):333?343.Shieber, Stuart M., Yves Schabes, andFernando Pereira.
1995.
Principlesand implementation of deductiveparsing.
Journal of Logic Programming,24(1?2):3?36.Steedman, Mark and Jason Baldridge.2011.
Combinatory categorial grammar.In Robert D. Borsley and Kersti Bo?rjars,editors, Non-Transformational Syntax:Formal and Explicit Models of Grammar.Wiley-Oxford, Blackwell, chapter 5,pages 181?224.386Kuhlmann Mildly Non-Projective Dependency GrammarTesnie`re, Lucien.
1959.
E?le?ments de syntaxestructurale.
Klinksieck, Paris.Vijay-Shanker, K., David J. Weir, andAravind K. Joshi.
1987.
Characterizingstructural descriptions produced byvarious grammatical formalisms.In Proceedings of the 25th Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 104?111,Stanford, CA.Villemonte de la Clergerie, E?ric.
2002.Parsing mildly context-sensitivelanguages with thread automata.In Proceedings of the 19th InternationalConference on Computational Linguistics(COLING), pages 1?7, Taipei.Weir, David J.
1988.
Characterizing MildlyContext-Sensitive Grammar Formalisms.Ph.D.
thesis, University of Pennsylvania,Philadelphia, PA.387
