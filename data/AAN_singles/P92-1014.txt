INFORMATION RETRIEVAL  US ING ROBUST NATURAL LANGUAGE PROCESSINGTomek Strzalkowski and Barbara Vauthey1"Courant Institute of Mathematical SciencesNew York University715 Broadway,  rm.
704New York, NY 10003tomek@cs.nyu.eduABSTRACTWe developed a prototype information retrieval sys-tem which uses advanced natural language process-ing techniques to enhance the effectiveness of tradi-tional key-word based document retrieval.
The back-bone of our system is a statistical retrieval enginewhich performs automated indexing of documents,then search and ranking in response to user queries.This core architecture is augmented with advancednatural language processing tools which are bothrobust and efficient.
In early experiments, the aug-mented system has displayed capabilities that appearto make it superior to the purely statistical base.INTRODUCTIONA typical information retrieval fiR) task is toselect documents from a database in response to auser's query, and rank these documents according torelevance.
This has been usually accomplished usingstatistical methods (often coupled with manualencoding), but it is now widely believed that thesetraditional methods have reached their limits.
1 Theselimits are particularly acute for text databases, wherenatural language processing (NLP) has long beenconsidered necessary for further progress.
Unfor-tunately, the difficulties encountered in applyingcomputational linguistics technologies to text pro-cessing have contributed to a wide-spread belief thatautomated NLP may not be suitable in IR.
Thesedifficulties included inefficiency, limited coverage,and prohibitive cost of manual effort required tobuild lexicons and knowledge bases for each newtext domain.
On the other hand, while numerousexperiments did not establish the usefulness of NLP,they cannot be considered conclusive because of theirvery limited scale.Another reason is the limited scale at whichNLP was used.
Syntactic parsing of the database con-tents, for example, has been attempted in order toextract linguistically motivated "syntactic phrases",which presumably were better indicators of contentsthan "statistical phrases" where words were groupedsolely on the basis of physical proximity (eg.
"collegejunior" is not the same as "junior college").
Theseintuitions, however, were not confirmed by experi-ments; worse still, statistical phrases regularly out-performed syntactic phrases (Fagan, 1987).
Attemptsto overcome the poor statistical behavior of syntacticphrases has led to various clustering techniques thatgrouped synonymous or near synonymous phrasesinto "clusters" and replaced these by single "meta-terms".
Clustering techniques were somewhat suc-cessful in upgrading overall system performance, buttheir effectiveness was diminished by frequently poorquality of syntactic analysis.
Since full-analysiswide-coverage syntactic parsers were either unavail-able or inefficient, various partial parsing methodshave been used.
Partial parsing was usually fastenough, but it also generated noisy data_" as many as50% of all generated phrases could be incorrect(Lewis and Croft, 1990).
Other efforts concentratedon processing of user queries (eg.
Spack Jones andTait, 1984; Smeaton and van Rijsbergen, 1988).Since queries were usually short and few, even rela-tively inefficient NLP techniques could be of benefitto the system.
None of these attempts proved con-clusive, and some were never properly evaluatedeither.t Current address: Laboratoire d'lnformatique, Unlversitede Fribourg, ch.
du Musee 3, 1700 Fribourg, Switzerland;vauthey@cfmniSl.bitnet.i As far as the aut~natic document retrieval is concerned.Techniques involving various forms of relevance feedback are usu-ally far more effective, but they require user's manual interventionin the retrieval process.
In this paper, we are concerned with fullyautomated retrieval only.2 Standard IR benchmark collections are statistically toosmall and the experiments can easily produce counterintuitiveresults.
For example, Cranfield collection is only approx.
180,000English words, while CACM-3204 collection used in the presentexperiments is approx.
200,000 words.104We believe that linguistic processing of boththe database and the user's queries need to be donefor a maximum benefit, and moreover, the twoprocesses must be appropriately coordinated.
Thisprognosis is supported by the experiments performedby the NYU group (Strzalkowski and Vauthey, 1991;Grishman and Strzalkowski, 1991), and by the groupat the University of Massachussetts (Croft et al,1991).
We explore this possibility further in thispaper.OVERALL DESIGNOur information retrieval system consists of atraditional statistical backbone (Harman and Candela,1989) augmented with various natural anguage pro-cessing components hat assist he system in databaseprocessing (stemming, indexing, word and phraseclustering, selectional restrictions), and translate auser's information request into an effective query.This design is a careful compromise between purelystatistical non-linguistic approaches and those requir-ing rather accomplished (and expensive) semanticanalysis of data, often referred to as 'conceptualretrieval'.
The conceptual retrieval systems, thoughquite effective, are not yet mature nough to be con-sidered in serious information retrieval applications,the major problems being their extreme inefficiencyand the need for manual encoding of domainknowledge (Mauldin, 1991).In our system the database text is first pro-cessed with a fast syntactic parser.
Subsequently cer-tain types of phrases are extracted from the parsetrees and used as compound indexing terms in addi-tion to single-word terms.
The extracted phrases arestatistically analyzed as syntactic ontexts in order todiscover a variety of similarity links between smallersubphrases and words occurring in them.
A furtherfiltering process maps these similarity links ontosemantic relations (generalization, specialization,synonymy, etc.)
after which they are used totransform user's request into a search query.The user's natural language request is alsoparsed, and all indexing terms occurring in them areidentified.
Next, certain highly ambiguous (usuallysingle-word) terms are dropped, provided that theyalso occur as elements in some compound terms.
Forexample, "natural" is deleted from a query alreadycontaining "natural language" because "natural"occurs in many unrelated contexts: "natural number","natural ogarithm", "natural approach", etc.
At thesame time, other terms may be added, namely thosewhich are linked to some query term through admis-sible similarity relations.
For example, "fortran" isadded to a query containing the compound term"program language" via a specification link.
After thefinal query is constructed, the database search fol-lows, and a ranked list of documents i  returned.It should be noted that all the processing steps,those performed by the backbone system, and theseperformed by the natural anguage processing com-ponents, are fully automated, and no human interven-tion or manual encoding is required.FAST PARSING WITH TI'P PARSERTIP  flagged Text Parser) is based on theLinguistic String Grammar developed by Sager(1981).
Written in Quintus Prolog, the parsercurrently encompasses more than 400 grammar pro-ductions.
It produces regularized parse tree represen-tations for each sentence that reflect the sentence'slogical structure.
The parser is equipped with apowerful skip-and-fit recovery mechanism thatallows it to operate effectively in the face of ill-formed input or under a severe time pressure.
In therecent experiments with approximately 6 millionwords of English texts, 3 the parser's peed averagedbetween 0.45 and 0.5 seconds per sentence, or up to2600 words per minute, on a 21 MIPS SparcStationELC.
Some details of the parser are discussedbelow .4T IP is a full grammar parser, and initially, itattempts to generate a complete analysis for eachsentence.
However, unlike an ordinary parser, it has abuilt-in timer which regulates the amount of timeallowed for parsing any one sentence.
If a parse is notreturned before the allotted time elapses, the parserenters the skip-and-fit mode in which it will try to"fit" the parse.
While in the skip-and-fit mode, theparser will attempt o forcibly reduce incompleteconstituents, possibly skipping portions of input inorder to restart processing at a next unattempted con-stituent.
In other words, the parser will favor reduc-tion to backtracking while in the skip-and-fit mode.The result of this strategy is an approximate parse,partially fitted using top-down predictions.
The flag-ments skipped in the first pass are not thrown out,instead they are analyzed by a simple phrasal parserthat looks for noun phrases and relative clauses andthen attaches the recovered material to the main parsestructure.
As an illustration, consider the followingsentence taken from the CACM-3204 corpus:3 These include CACM-3204, MUC-3, and a selection ofnearly 6,000 technical articles extracted from Computer Librarydatabase (aZiff Communications Inc. CD-ROM).4 A complete description can be found in (Strzalkowski,1992).105The method is illustrated by the automatic con-struction of beth recursive and iterative pro-grams opera~-tg on natural numbers, lists, andtrees, in order to construct aprogram satisfyingcertain specifications a theorem induced bythose specifications is proved, and the desiredprogram isextracted from the proof.The italicized fragment is likely to cause additionalcomplications in parsing this lengthy string, and theparser may be better off ignoring this fragment alto-gether.
To do so successfully, the parser must closethe currently open constituent (i.e., reduce a programsatisfying certain specifications to NP), and possiblya few of its parent constituents, removingcorresponding productions from further considera-tion, until an appropriate production is reactivated.In this case, T IP  may force the following reductions:SI ---> to V NP; SA --~ SI; S -~ NP V NP SA, until theproduction S --+ S and S is reached.
Next, the parserskips input to lind and, and resumes normal process-ing.As may be expected, the skip-and-fit strategywill only be effective if the input skipping can be per-formed with a degree of determinism.
This meansthat most of the lexical level ambiguity must beremoved from the input text, prior to parsing.
Weachieve this using a stochastic parts of speech tagger5 to preprocess the text.WORD SUFF IX  TRIMMERWord stemming has been an effective way ofimproving document recall since it reduces words totheir common morphological root, thus allowingmore successful matches.
On the other hand, stem-ming tends to decrease retrieval precision, if care isnot taken to prevent situations where otherwise unre-lated words are reduced to the same stem.
In our sys-tem we replaced a traditional morphological stemmerwith a conservative dictionary-assisted suffix trim-mer.
6 The suffix trimmer performs essentially twotasks: (1) it reduces inflected word forms to their rootforms as specified in the dictionary, and (2) it con-verts nominalized verb forms (eg.
"implementation","storage") to the root forms of corresponding verbs(i.e., "implement", store").
This is accomplished byremoving a standard suffix, eg.
"stor+age", replacingit with a standard root ending ("+e"), and checkingthe newly created word against the dictionary, i.e.,we check whether the new root ("store") is indeed alegal word, and whether the original root ("storage")s Courtesy of Bolt Beranek and Newman.We use Oxford Advanced Learner's Dictionary (OALD).is defined using the new root ("store") or one of itsstandard inflexional forms (e.g., "storing").
Forexample, the following definitions are excerpted fromthe Oxford Advanced Learner's Dictionary (OALD):storage n \[U\] (space used for, money paid for)the storing of goods ...diversion n \[U\] diverting ...procession n \[C\] number of persons, vehicles,ete moving forward and following each other inan orderly way.Therefore, we can reduce "diversion" to "divert" byremoving the suffix "+sion" and adding root formsuffix "+t".
On the other hand, "process+ion" is notreduced to "process".Experiments with CACM-3204 collectionshow an improvement in retrieval precision by 6% to8% over the base system equipped with a standardmorphological stemmer (in our case, the SMARTstemmer).HEAD-MODIF IER  STRUCTURESSyntactic phrases extracted from T IP  parsetrees are head-modifier pairs: from simple word pairsto complex nested structures.
The head in such a pairis a central element of a phrase (verb, main noun,etc.)
while the modifier is one of the adjunct argu-ments of the head.
7 For example, the phrase fastalgorithm for parsing context-free languages yieldsthe following pairs: algorithm+fast,algorithm+parse, parse+language,language+context.free.
Thefollowing types of pairswere considered: (1) a head noun and its left adjec-tive or noun adjunct, (2) a head noun and the head ofits right adjunct, (3) the main verb of a clause and thehead of its object phrase, and (4) the head of the sub-ject phrase and the main verb, These types of pairsaccount for most of the syntactic variants for relatingtwo words (or simple phrases) into pairs carryingcompatible semantic ontent.
For example, the pairretrieve+information s extracted from any of the fol-lowing fragments: information retrieval system;retrieval of information from databases; and informa-tion that can be retrieved by a user-controlledinteractive search process.
An example is shown inFigure 1. g One difficulty in obtaining head-modifier7 In the experiments reported here we extracted head-modifier word pairs only.
CACM collection is too small to warrantgeneration f larger compounds, because oftheir low frequencies.s Note that working with the parsed text ensures a high de-gree of precision i  capturing the meaningful phrases, which isespecially evident when compared with the results usually obtainedfrom either unprocessed or only partially processed text (Lewis andCroft, 1990).106SENTENCE:The techniques are discussed and related to a generaltape manipulation routine.PARSE STRUCTURE:\[\[be\],\[\[verb,\[and,\[discuss\],\[relate\]\]\],\[subject,anyone\],\[object,\[np,\[n,technique\],\[t..pos,the\]\]\],\[to,\[np,\[n,routine\],\[t_pos,a\],\[adj,\[general\]\],\[n__pos,\[np,\[n,manipulation\]\] \],\[n._pos,\[np,\[n,tape\]\]\]\]\]\]\].EXTRACTED PAIRS:\[discuss,technique\], \[relate,technique\],\[routine,general\], \[routine,manipulate\],\[manipulate,tape\]Figure 1.
Extraction of syntactic pairs.pairs of highest accuracy is the notorious ambiguityof nominal compounds.
For example, the phrasenatural language processing should generatelanguage+natural and processing+language, whiledynamic information processing is expected to yieldprocessing+dynamic and processing+information.Since our parser has no knowledge about the textdomain, and uses no semantic preferences, it does notattempt to guess any internal associations within suchphrases.
Instead, this task is passed to the pair extrac-tor module which processes ambiguous parse struc-tures in two phases.
In phase one, all and only unam-biguous head-modifier pairs are extracted, and fre-quencies of their occurrence are recorded.
In phasetwo, frequency information of pairs generated in thefirst pass is used to form associations from ambigu-ous structures.
For example, if language+natural h soccurred unambiguously a number times in contextssuch as parser for natural language, whileprocessing+natural h soccurred significantly fewertimes or perhaps none at all, then we will prefer theformer association as valid.TERM CORRELATIONS FROM TEXTHead-modifier pairs form compound termsused in database indexing.
They also serve asoccurrence contexts for smaller terms, includingsingle-word terms.
In order to determine whethersuch pairs signify any important association betweenterms, we calculate the value of the InformationalContribution (IC) function for each element in a pair.Higher values indicate stronger association, and theelement having the largest value is consideredsemantically dominant.107The connection between the terms co-occurrences and the information they are transmitting(or otherwise, their meaning) was established anddiscussed in detail by Harris (1968, 1982, 1991) asfundamental for his mathematical theory of language.This theory is related to mathematical informationtheory, which formalizes the dependencies betweenthe information and the probability distribution of thegiven code (alphabet or language).
As stated byShannon (1948), information is measured by entropywhich gives the capacity of the given code, in termsof the probabilities of its particular signs, to transmitinformation.
It should be emphasized that, accordingto the information theory, there is no direct relationbetween information and meaning, entropy givingonly a measure of what possible choices of messagesare offered by a particular language.
However, itoffers theoretic foundations of the correlationbetween the probability of an event and transmittedinformation, and it can be further developed in orderto capture the meaning of a message.
There is indeedan inverse relation between information contributedby a word and its probability of occurrence p, that is,rare words carry more information than commonones.
This relation can be given by the function-log p (x) which corresponds to information which asingle word is contributing to the entropy of theentire language.In contrast o information theory, the goal ofthe present study is not to calculate informationalcapacities of a language, but to measure the relativestrength of connection between the words in syntacticpairs.
This connection corresponds to Harris' likeli-hood constraint, where the likelihood of an operatorwith respect to its argument words (or of an argumentword in respect to different operators) is definedusing word-combination frequencies within thelinguistic dependency structures.
Further, the likeli-hood of a given word being paired with anotherword, within one operator-argument structure, can beexpressed in statistical terms as a conditional proba-bility.
In our present approach, the required measurehad to be uniform for all word occurrences, coveringa number of different operator-argument structures.This is reflected by an additional dispersion parame-ter, introduced to evaluate the heterogeneity of wordassociations.
The resulting new formula IC (x, \[x,y \])is based on (an estimate of) the conditional probabil-ity of seeing a word y to the right of the word x,modified with a dispersion parameter for x.lC(x, \[x,y \]) - f~'Ynx + dz -1where f~,y is the frequency of \[x,y \]in the corpus, n~is the number of pairs in which x occurs at the sameposition as in \[x,y\], and d(x) is the dispersionparameter understood as the number of distinct wordswith which x is paired.
When IC(x, \[x,y \]) = 0, x andy never occur together (i.e., f~.y=0); whenIC(x, \[x,y \]) = 1, x occurs only with y (i.e., fx,y = n~and dx = 1).So defined, IC function is asymmetric, a pro-perry found  desirable by Wilks et al (1990) in theirstudy of word co-occurrences in the Longman dic-tionary.
In addition, IC is stable even for relativelylow frequency words, which can be contrasted withFano's mutual information formula recently used byChurch and Hanks (1990) to compute word co-occurrence patterns in a 44 million word corpus ofAssociated Press news stories.
They noted that whilegenerally satisfactory, the mutual information for-mula often produces counterintuitive r sults for low-frequency data.
This is particularly worrisome forrelatively smaller IR collections ince many impor-tant indexing terms would be eliminated from con-sideration.
A few examples obtained from CACM-3204 corpus are listed in Table 1.
IC values for termsbecome the basis for calculating term-to-term simi-larity coefficients.
If two terms tend to be modifiedwith a number of common modifiers and otherwiseappear in few distinct contexts, we assign them asimilarity coefficient, a real number between 0 and 1.The similarity is determined by comparing distribu-tion characteristics for both terms within the corpus:how much information contents do they carry, dotheir information contribution over contexts varygreatly, are the common contexts in which theseterms occur specific enough?
In general we willcredit high-contents erms appearing in identical con-texts, especially if these contexts are not too com-monplace.
9 The relative similarity between twowords Xl and x2 is obtained using the following for-mula (a  is a large constant): l0SIM (x l ,x2) = log (or ~, simy(x t ,x2))ywheresimy(x 1,x2) = MIN (IC (x 1, \[x I ,Y \]),IC (x2, \[x 2,Y \]))* (IC(y, \[xt,y\]) +IC(,y, \[x2,y\]))The similarity function is further normalized withrespect o SIM(xl,xl).
It may be worth pointing outthat the similarities are calculated using term co-9 It would not be appropriate to predict similarity betweenlanguage and logarithm on the basis of their co-occurrence withnaturaLto This is inspired by a formula used by Hindie (1990), andsubsequently modified to take into account the asymmetry of ICmeab- 'ure .word head+modifier IC coeff.distributenormalminimumrelativeretrieveinformsizemediumeditortextsystemparallelreadcharacterimplicatelegalsystemdistributemakerecommendinferdeductiveshareresourcedistribute+normaldistribute+normalminimum+relativeminimum+relativeretrieve +informretrieve+informsize +mediumsize+mediumeditor+texteditor+textsystem+parallelsystem+parallelread+characterread+characterimplicate+legalimplicate+legalsystem+distributesystem+distributemake+recommendmake+recommendinfer+deductiveinfer+deductiveshare +resourceshare+resource0.0400.1150.2000.0160.0860.0040.0090.2500.1420.0250.0010.0140.0230.0070.0350.0830.0020.0370.0240.1420.0950.1420.0540.042Table 1.
IC coefficients obtained from CACM-3204occurrences in syntactic rather than in document-sizecontexts, the latter being the usual practice in non-linguistic clustering (eg.
Sparck Jones and Barber,1971; Crouch, 1988; Lewis and Croft, 1990).Although the two methods of term clustering may beconsidered mutually complementary in certain situa-tions, we believe that more and stronger associationscan be obtained through syntactic-context clustering,given sufficient amount of data and a reasonablyaccurate syntactic parser.
~QUERY EXPANSIONSimilarity relations are used to expand userqueries with new terms, in an attempt o make then Non-syntactic contexts cross sentence boundaries with nofuss, which is helpful with short, succinct documc~nts (such asCACM abstracts), but less so with longer texts; sec also (Grishmanet al, 1986).108final search query more comprehensive (addingsynonyms) and/or more pointed (adding specializa-tions).
12 It follows that not all similarity relations willbe equally useful in query expansion, for instance,complementary relations like the one between algoland fortran may actually harm system's performance,since we may end up retrieving many irrelevantdocuments.
Similarly, the effectiveness of a querycontaining fortran is likely to diminish if we add asimilar but far more general term such as language.On the other hand, database search is likely to missrelevant documents if we overlook the fact that for.tran is a programming language, or that interpolateis a specification of approximate.
We noted that anaverage set of similarities generated from a textcorpus contains about as many "good" relations(synonymy, specialization) as "bad" relations (anto-nymy, complementation, generalization), as seenfrom the query expansion viewpoint.
Therefore anyattempt to separate these two classes and to increasethe proportion of "good" relations hould result inimproved retrieval.
This has indeed been confirmedin our experiments where a relatively crude filter hasvisibly increased retrieval precision.In order to create an appropriate filter, weexpanded the IC function into a global specificitymeasure called the cumulative informational contri-bution function (ICW).
ICW is calculated for eachterm across all contexts in which it occurs.
The gen-eral philosophy here is that a more specificword/phrase would have a more limited use, i.e.,would appear in fewer distinct contexts.
ICW is simi-lar to the standard inverted ocument frequency (idf)measure xcept hat term frequency is measured oversyntactic units rather than document size units./3Terms with higher ICW values are generally con-sidered more specific, but the specificity comparisonis only meaningful for terms which are alreadyknown to be similar.
The new function is calculatedaccording to the following formula:ICt.
(w) if both exist ICR(w)ICW(w)=I~R(w) otherwiseif?nly ICR(w)existsn Query expansion (in the sense considered here, though notquite in the same way) has been used in information retrievalresearch before (eg.
Sparck Jones and Tait, 1984; Harman, 1988),usually with mixed results.
An alternative is to use tenm clusters tocreate new terms, "metaterms", and use them to index the databaseinstead (eg.
Crouch, 1988; Lewis and Croft, 1990).
We found thatthe query expansion approach gives the system more flexibility, forinstance, by making room for hypertext-style topic exploration viauser feedback.t3 We believe that measuring term specificity overdocument-size contexts (eg.
Sparck Jones, 1972) may not be ap-propriate in this case.
In particular, syntax-based contexts allow forwhere (with n~, d~ > 0): 14n~ICL(W) = IC (\[w,_ \]) - d~(n~+d~-l)n~ICR(w) = IC (\[_,w \]) = d~(n~+d~-l)For any two terms wl and w2, and a constant 8 > 1,if ICW(w2)>8* ICW(wl) then w2 is consideredmore specific than ' wl.
In addition, ifSIMno,,(wl,w2)=?~> O, where 0 is an empiricallyestablished threshold, then w2 can be added to thequery containing term wl with weight ~.14 In theCACM-3204 collection:ICW (algol) = 0.0020923ICW(language) = 0.0000145ICW(approximate) = 0.0000218ICW (interpolate) = 0.0042410Therefore interpolate can be used to specializeapproximate, while language cannot be used toexpand algol.
Note that if 8 is well chosen (we used8=10), then the above filter will also help to rejectantonymous and complementary elations, such asSIM~o,~(pl_i, cobol)=0.685 with ICW (pl_i)=O.O175and ICW(cobol)=O.0289.
We continue working todevelop more effective filters.
Examples of filteredsimilarity relations obtained from CACM-3204corpus (and their sim values): abstract graphical0.612; approximate interpolate 0.655; linear ordi-nary 0.743; program translate 0.596; storage buffer0.622.
Some (apparent?)
failures: active digital0.633; efficient new 0.580; gamma beta 0.720.
Moresimilarities are listed in Table 2.SUMMARY OF RESULTSThe preliminary series of experiments with theCACM-3204 collection of computer science abstractsshowed a consistent improvement in performance:the average precision increased from 32.8% to 37.1%(a 13% increase), while the normalized recall wentfrom 74.3% to 84.5% (a 14% increase), in com-parison with the statistics of the base NIST system.This improvement is a combined effect of the newstemmer, compound terms, term selection in queries,and query expansion using filtered similarity rela-tions.
The choice of similarity relation filter has beenfound critical in improving retrieval precisionthrough query expansion.
It should also be pointedout that only about 1.5% of all similarity relationsoriginally generated from CACM-3204 were foundprocessing texts without any internal document structure.14 The filter was most effective at o = 0.57.109wordl word2 SIMnorm*aimalgorithm*adjacency*algebraic*americanassert*buddycommitteecriticalbest-fit* duplexearlierencasegiveincompleteleadmeanmethodmemorymatchlowerprogressrangeround-offremotepurposemethodpairsymbolstandardinfertime-share*symposiumfmalfirst-fitreliablepreviousminimum-areapresentmiss*trail*standardtechniquestoragerecognizeupper*trendvarietytruncateteletype0.4340.5290.4990.5140.7190.7830.6220.4690.6800.8710.4370.5500.9910.4580.8500.8900.6340.5710.6130.5630.8410.4440.6000.9180.509Table 2.
Filtered word similarities (* indicates themore specific term).admissible after filtering, contributing only 1.2expansion on average per query.
It is quite evidentsignificantly larger corpora are required to producemore dramatic results.
15 ~6 A detailed summary isgiven in Table 3 below.These results, while quite modest by IR stun-dards, are significant for another reason as well.
Theywere obtained without any manual intervention i tothe database or queries, and without using any otherts KL Kwok (private communication) has suggested that helow percentage of admissible relations might be similar to thephenomenon f 'tight dusters' which while meaningful are so fewthat heir impact is small.
:s A sufficiently large text corpus is 20 million words ormore.
This has been paRially confirmed by experiments performedat the University of Massachussetts (B. Croft, private comrnunica-don).110base surf.trim Tests query exp.Recall Precision0.7640.6740.5470.4490.3870.3290.2730.1980.1460.0930.0790.000.100.200.300.400.500.600.700.800.901.000.7750.6880.5470.4790A210.3560.2800.2220.1700.1120.0870.7930.7000.5730.4860.4210.3720.3040.2260.1740.1140.090Avg.
Prec.
0.328 0.356 0.371% change 8.3 13.1Norm Rec.
0.743 0.841 0.842Queries 50 50 50Table 3.
Recall/precision statistics for CACM-3204information about he database except for the text ofthe documents (i.e., not even the hand generated key-word fields enclosed with most documents wereused).
Lewis and Croft (1990), and Croft et al (1991)report results imilar to ours but they take advantageof Computer Reviews categories manually assignedto some documents.
The purpose of this research is toexplore the potential of automated NLP in dealingwith large scale IR problems, and not necessarily toobtain the best possible results on any particular datacollection.
One of our goals is to point a feasibledirection for integrating NLP into the traditional IR.ACKNOWLEDGEMENTSWe would like to thank Donna Harman ofNIST for making her IR system available to us.
Wewould also like to thank Ralph Weischedel, MarieMeteer and Heidi Fox of BBN for providing andassisting in the use of the part of speech tagger.
KLKwok has offered many helpful comments on an ear-lier draft of this paper.
In addition, ACM has gen-erously provided us with text data from the ComputerLibrary database distributed by Ziff CommunicationsInc.
This paper is based upon work suppened by theDefense Advanced Research Project Agency underContract N00014-90-J-1851 from the Office of NavalResearch, the National Science Foundation underGrant 1RI-89-02304, and a grant from the SwissNational Foundation for Scientific Research.
We alsoacknowledge a support from Canadian Institute forRobotics and Intelligent Systems (IRIS).REFERENCESChurch, Kenneth Ward and Hanks, Patrick.
1990.
"Word association orms, mutual informa-tion, and lexicography."
ComputationalLinguistics, 16(1), MIT Press, pp.
22-29.Croft, W. Bruce, Howard R. Turtle, and David D.Lewis.
1991.
"The Use of Phrases and Struc-tured Queries in Information Retrieval.
"Proceedings ofACM SIGIR-91, pp.
32-45.Crouch, Carolyn J.
1988.
"A cluster-based approachto thesaurus construction."
Proceedings ofACM SIGIR-88, pp.
309-320.Fagan, Joel L. 1987.
Experiments in AutomatedPhrase Indexing for Document Retrieval: AComparison of Syntactic and Non-SyntacticMethods.
Ph.D. Thesis, Department of Com-puter Science, CorneU University.Grishman, Ralph, Lynette Hirschman, and Ngo T.Nhan.
1986.
"Discovery procedures for sub-language selectional patterns: initial experi-ments".
ComputationalLinguistics, 12(3), pp.205-215.Grishman, Ralph and Tomek Strzalkowski.
1991.
"Information Retrieval and Natural LanguageProcessing."
Position paper at the workshopon Future Directions in Natural Language Pro-cessing in Information Retrieval, Chicago.Harman, Donna.
1988.
"Towards interactive queryexpansion."
Proceedings of ACM SIGIR-88,pp.
321-331.Harman, Donna and Gerald Candela.
1989.
"Retrieving Records from a Gigabyte of texton a Minicomputer Using Statistical Rank-ing."
Journal of the American Society forInformation Science, 41(8), pp.
581-589.Harris, Zelig S. 1991.
A Theory of language andInformation.
A Mathematical Approach.Cladendon Press.
Oxford.Harris, Zelig S. 1982.
A Grammar of English onMathematical Principles.
Wiley.Harris, Zelig S. 1968.
Mathematical Structures ofLanguage.
Wiley.Hindle, Donald.
1990.
"Noun classification frompredicate-argument structures."
Proc.
28Meeting of the ACL, Pittsburgh, PA, pp.
268-275.Lewis, David D. and W. Bruce Croft.
1990.
"TermClustering of Syntactic Phrases".
Proceedingsof ACM SIGIR-90, pp.
385-405.Mauldin, Michael.
1991.
"Retrieval Performance inFerret: A Conceptual Information RetrievalSystem."
Proceedings ofACM SIGIR-91, pp.347-355.Sager, Naomi.
1981.
Natural Language InformationProcessing.
Addison-Wesley.Salton, Gerard.
1989.
Automatic Text Processing:the transformation, analysis, and retrieval ofinformation by computer.
Addison-Wesley,Reading, MA.Shannon, C. E. 1948.
"A mathematical theory ofcommunication."
Bell System TechnicalJournal, vol.
27, July-October.Smeaton, A. F. and C. J. van Rijsbergen.
1988.
"Experiments on incorporating syntactic pro-cessing of user queries into a documentretrieval strategy."
Proceedings of ACMSIGlR-88, pp.
31-51.Sparck Jones, Karen.
1972.
"Statistical interpreta-tion of term specificity and its application inretrieval."
Journal of Documentation, 28(1),pp.
ll-20.Sparck Jones, K. and E. O. Barber.
1971.
"Whatmakes automatic keyword classification effec-five?"
Journal of the American Society forInformation Science, May-June, pp.
166-175.Sparck Jones, K. and J. I. Tait.
1984.
"Automaticsearch term variant generation."
Journal ofDocumentation, 40(1), pp.
50-66.Strzalkowski, Tomek and Barbara Vauthey.
1991.
"Fast Text Processing for InformationRetrieval.'"
Proceedings of the 4th DARPASpeech and Natural Language Workshop,Morgan-Kaufman, pp.
346-351.Strzalkowski, Tomek and Barbara Vauthey.
1991.
"'Natural Language Processing in AutomatedInformation Retrieval."
Proteus ProjectMemo #42, Courant Institute of MathematicalScience, New York University.Strzalkowski, Tomek.
1992.
"TYP: A Fast andRobust Parser for Natural Language.
"Proceedings of the 14th International Confer-ence on Computational Linguistics (COL-ING), Nantes, France, July 1992.Wilks, Yorick A., Dan Fass, Cheng-Ming Guo,James E. McDonald, Tony Plate, and Brian M.Slator.
1990.
"Providing machine tractabledictionary tools."
Machine Translation, 5 pp.99-154.111
