Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 8?14,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDependency Forest based Word AlignmentHitoshi Otsuki1, Chenhui Chu2, Toshiaki Nakazawa2, Sadao Kurohashi11Graduate School of Informatics, Kyoto University2Japan Science and Technology Agency{otsuki, kuro}@nlp.ist.i.kyoto-u.ac.jp, {chu,nakazawa}@pa.jst.jpAbstractA hierarchical word alignment model thatsearches for k-best partial alignments ontarget constituent 1-best parse trees hasbeen shown to outperform previous mod-els.
However, relying solely on 1-bestparses trees might hinder the search forgood alignments because 1-best trees arenot necessarily the best for word align-ment tasks in practice.
This paper in-troduces a dependency forest based wordalignment model, which utilizes target de-pendency forests in an attempt to mini-mize the impact on limitations attributableto 1-best parse trees.
We present howk-best alignments are constructed overtarget-side dependency forests.
Alignmentexperiments on the Japanese-English lan-guage pair show a relative error reductionof 4% of the alignment score compared toa model with 1-best parse trees.1 IntroductionIn statistical machine translation (SMT), wordalignment plays an essential role in obtainingphrase tables (Och and Ney, 2004; Koehn et al,2003) or syntactic transformation rules (Chiang,2007; Shen et al, 2008).
IBM models (Brown etal., 1993), which are based on word sequences,have been widely used for obtaining word align-ments because they are fast and their implementa-tion is available as GIZA++.1Recently, a hierarchical alignment model(whose implementation is known as Nile2) (Riesaet al, 2011), which performs better than IBMmodels, has been proposed.
In the hierarchi-cal alignment model, both source and target con-1http://www.statmt.org/moses/giza/GIZA++.html2http://jasonriesa.github.io/nile/stituency trees are used for incorporating syntacticinformation as features, and it searches for k-bestpartial alignments on the target constituent parsetrees.
It achieved significantly better results thanthe IBM Model4 in Arabic-English and Chinese-English word alignment tasks, even though themodel was trained on only 2,280 and 1,102 par-allel sentences as gold standard alignments.
How-ever, their models rely only on 1-best source andtarget side parse trees, which are not necessarilygood for word alignment tasks.In SMT, forest-based decoding has been pro-posed for both constituency and dependency parsetrees (Mi et al, 2008; Tu et al, 2010).
A forestis a compact representation of n-best parse trees.It provides more alternative parse trees to choosefrom during decoding, leading to significant im-provements in translation quality.
In this paper, weborrow this idea to build an alignment model us-ing dependency forests rather than 1-best parses,which makes it possible to provide the model withmore alternative parse trees that may be suitablefor word alignment tasks.
The motivation of usingdependency forests instead of constituency forestsin our model is that dependency forests are moreappropriate for alignments between language pairswith long-distance reordering, such as the one westudy in this paper.
This is because they are moresuitable for capturing the complex semantic rela-tions of words in a sentence (Kahane, 2012).We conducted alignment experiments on theJapanese-English language pair.
Experimental re-sults show a relative error reduction of 4% of thealignment score compared to the model with 1-best parse trees.2 Model Description2.1 Dependency ForestWe first briefly explain dependency forests that areused in our model before describing the alignment8Figure 1: Bottom-up search for alignments overtarget-side dependency forest (This forest encodes2-best parse trees for the sentence ?he saw a girlwith a telescope.?
The source sentence is ??
(He)????
(telescope)?
(with)???
(girl)???(saw)?.
There are two interpretations for this sen-tence; either ?with a telescope?
depends on ?saw?or ?boy.?
)construction method.
A dependency forest is rep-resented by a hypergraph ?V,E?, where V is a setof nodes and E is a set of hyperedges.A hyperedge e connects nodes in the forest andis defined to be a triple ?tails(e), head(e), score?,where tails(e) is a set of dependents of e, head(e)is the head of e, and score is the score of ethat is usually obtained by heuristics (Tu et al,2010).
For example, e1in Figure 1 is equalto ?
(he0,1, boy2,4, with4,7), saw0,7, 1.234?.
In ourmodel, we use Algorithm 1 to compute hyperedgescores.
Edges in a hyperedge are defined to bethe ones obtained by connecting each tail with thehead (Line 11).
Hyperedge score is the sum of allthe scores of edges in it (Line 12).
The score ofan edge is the normalized sum of the scores of allparses which contain the edge (Line 7).Every node in a dependency forest correspondsto a word attached with a span, which is a range ofword indices covered by the node.
Following (Tuet al, 2010), a span is represented in the form i, j,which indicates the node covers all the words fromi-th to (j ?
1)-th word.
This requires dependencyforests to be projective.
Separate nodes are usedfor a word if the nodes in dependency trees havedifferent spans.
For example, in Figure 1 there aretwo nodes for the word ?boy?
because they havedifferent spans (i.e., (2, 4) and (2, 7)).The construction of a dependency forest fromInput : n-best dependency parses {Ti}ni=1of a sentenceScore of TiScoreiOutput: A forest F of {Ti}ni=11 F =CreateForestStructure({Ti}ni=1)2 edgeScores = {}3 minScore = Min({Scorei}ni=1)4 for i = 1 to n do5 Scorei?
= minScore6 for edge ?
Tido7 edgeScores [edge] + =1nScorei8 end9 end10 for hyperEdge ?
F do11 for edge ?
hyperEdge do12 hyperEdge.score+ =edgeScores [edge]13 end14 endAlgorithm 1: Computation of a hyperedge scoredependency trees is done by sharing the commonnodes and edges (Line 1).
The common nodesare those with the same span and part-of-speech(POS) .
Note that the dependency forest obtainedfrom this method does not necessarily encode ex-actly the dependency trees from which they arecreated.
Usually there are more trees that can beextracted from the dependency forests (Boullier etal., 2009).
In our experiment, when we use theterm ?a n-best dependency forest?, we indicate adependency forest that is created from n-best de-pendency trees.2.2 Finding Alignments over ForestFollowing the hierarchical alignment model(Riesa et al, 2011), our model searches for thebest alignment by constructing partial alignments(hypotheses) over target dependency forests in abottom-up manner as shown in Figure 1.The algorithm for constructing alignments isshown in Algorithm 2.
Note that source depen-dency forests are included in the input to the al-gorithm.
This is optional but can be included forricher features.
Each node in the forest has partialalignments sorted by alignment scores.
Becauseit is computationally expensive to keep all possi-ble partial alignments for each node, we keep abeam size of k. A partial alignment for a node isan alignment matrix for target words that are cov-9ered by the node.
In Figure 1, each partial align-ment is represented as a black square.
Scores ofthe partial alignments are a linear combination offeatures.
There are two types of features: local andnon-local features.
A feature f is defined to be lo-cal if and only if it can be factored among the lo-cal productions in a tree, and non-local otherwise(Huang, 2008).We visit the nodes in the topological order, toguarantee that we visit a node after visiting all itstail nodes (Line 1).
For each node, we first gen-erates partial alignments, which are one columnalignment matrices for its word.
Because of timecomplexity, we only generates null, single link anddouble link alignment (Line 5).
A single and dou-ble link alignment refer to a column matrix havingexactly one and two alignments, respectively, asshown in Figure 1.
For each partial alignment, wecompute its score using local features (Line 7) andpushed to a priority queue Bv(Line 8).
These par-tial alignments are represented by black squares ina blue container in Figure 1.
Then, we computepartial alignments for the target words covered bythe node, by combining tails?
partial alignmentsand one column alignments for its word using non-local features (Line 10 - 14), which is representedby the orange arrows in Figure 1. k-best com-bined partial alignments are put in Yv(Line 14).They are represented by black squares in a yellowcontainer in Figure 1.
Here, we use cube prun-ing (Chiang, 2007) to get the approximate k-bestcombinations.
Note that in the search over con-stituency parse trees, one column alignment ma-trices are generated only on the leaf node (Riesa etal., 2011), whereas we generate them also on non-leaf nodes in the search over dependency forests.2.3 FeaturesThe features we used include those used in Nileexcept for the automatically extracted rule andconstellation features.
This is because these fea-tures are not easily applicable to dependencyforests.
As shown in our experiments, these fea-tures have a contribution to the alignment score.However, our primary purpose is to show the ef-fect of using forests on alignment quality.Several features in Nile such as source-targetPOS local feature and coordination feature haveto be customized for dependency forests, becauseit is possible that there are multiple nodes that cor-respond to the same word.
We decided to considerall nodes corresponding to a word by counting theInput : Source and target sentence s, tDependency forest Fsover sDependency forest Ftover tSet of feature functions hWeight vector wBeam size kOutput: A k-best list of alignments over sand t1 for v ?TopologicalSort(Ft) do2 links = ?3 Bv= ?4 i = word-index-of(v)5 links = {(0, i)}?SingleLinks(i)?DoubleLinks(i)6 for link ?
links do7 score = w ?
h(links, v, s, t, Fs, Ft)8 Push(Bv, ?score, link?, k)9 end10 for hyperEdge ?InHyperEdges(v)do11 c = hyperEdge.tail12 Push(?v, ?Yc1, ?
?
?
, Yc|c|, Bv?
)13 end14 Yv=CubePruning(?v, k, w, h, v, s,t, Fs, Ft)15 endAlgorithm 2: Construction of alignmentsfrequency of each POS tag of a node, and normal-izing it with the total frequency of POS tags in theforest.
For example, suppose there are four nodeswhich correspond to the same word, whose POStags are JJ, VBG, JJ, VGZ.
In this case the features?src-tgt-pos-feature-JJ=0.5?, ?src-tgt-pos-feature-VBG=0.25?
and ?src-tgt-pos-feature-VBZ=0.25?are activated.Besides the features used in Nile, our modeluses a contiguous alignment local feature and ahyperedge score non-local feature.
The contigu-ous alignment feature fires when a target word isaligned to multiple source words, and these wordsare contiguous on a forest.
Preliminary experi-ments showed, however, that none of these fea-tures contributed to the improvement of the align-ment score.3 Experiments3.1 Experimental SettingsWe conducted alignment experiments on theJapanese-English language pair.
For dependency10parsers, we used KNP (Kawahara and Kurohashi,2006) for Japanese and Berkeley Parser (Petrovand Klein, 2007) for English.
We converted con-stituent parse trees obtained by Berkeley Parser todependency parse trees using rules.3We used 300,100, 100 sentences from ASPEC-JE2for train-ing, development and test data, respectively.4Ourmodel as well as Nile has a feature called thirdparty alignment feature, which activates for analignment link that is presented in the alignmentof a third party model.
The beam size k was setto 128.
We used different number of parse treesto create a target forest, e.g., 1, 10, 20, 50, 100and 200.5The baseline in this experiment is amodel with 1-best parse trees on the target side.For reference, we also experimented on Nile6,the Bayesian subtree alignment model (Nakazawamodel) (Nakazawa and Kurohashi, 2011) and IBMModel4.7We used Nile without automatically ex-tracted rule features and constellation features tomake a fair comparison with our model.3.2 ResultsTable 1 shows the alignment results evaluated onprecision, recall and F-score for each experimen-tal setting.
The first row shows the names of dif-ferent experimental settings.
Each number in therow shows the number of n-best parse trees usedto create target forests.We can observe that using forests improves thescore.
However, the improvement does not mono-tonically increase with the number of trees on thetarget side.
When 100-best is used in target side,it achieved the highest error reduction of 4% com-pared to the baseline model.8We also conducted experiments on differentnumber of beam size k, e.g, 200 and 300, fromthe insight that a larger number of trees encodedin a forest indicates that more noisy partial align-ments are generated, using the same k as the 1-bestmodel is not sufficient.
However, we could not ob-serve significant improvements.3The conversion program is available athttps://github.com/hitochan777/mt-tools/releases/tag/1.0.14http://lotus.kuee.kyoto-u.ac.jp/ASPEC/5In the experiments, we used 1-best parse trees for thesource side.
Although our model also allows to use forests onthe source side, preliminary experiments showed that usingforests on the source side does not improve the alignmentscore.6Note that Nile uses 1-best constituency parse tree7The alignments from Nakazawa model and IBM Model4 were symmetrized with the grow-diag-final heuristic.8(82.39?
81.66) / (100?
81.66) ?
4%4 DiscussionWe observed the improvement of alignments byusing forests.
We checked whether good parsetrees were chosen when higher F-scores wereachieved.
It turned out that better parse trees led tohigher F-scores, as shown in Figure 2a, but it wasnot always the case.Figure 2a shows an improved example by us-ing 100-best trees on the target side.
In the fig-ure, we can observe that ???
and ?of?
are cor-rectly aligned.
We observe that the English 1-bestparse tree is incorrect, whereas 100-best modelwere able to choose a better tree.Figure 2b shows a worsened example by using200-best trees on the target side.
We can see thatthe 200-best model aligned many words unneces-sarily and the wrong tree is chosen even thoughthe 1-best parse is good.
There were many casesin which forests are harmful for alignments.
Thereare two possible reasons.
Firstly, most of the fea-tures in our model comes from Nile, but they arenot informative enough to choose better parsesfrom forests.
Secondly, our model is likely to suf-fer from the data sparseness because using forestsgenerates more noise than 1-best parses.For our model to benefit from forests we haveto consider the following: Firstly, our model?s fea-ture is based on the assumption that source andtarget forests contain trees with similar structuresto each other.
However the projectivity of forestsprohibits our model from generating (choosing)target trees that are similar to the ones in sourceforests.
Secondly, we observed the cases where noparse in forests captures the correct root and thedifference of n-best parses are mainly POS tags ofwords.Our model performs on par with Nile becauseour model is based on Nile.
However, ourmodel outperforms the Nakazawa model and IBMModel4.
This is because our model is supervisedbut these models are unsupervised.
The Nakazawamodel outperformed IBM Model4 because it uti-lizes dependency trees, which provide richer in-formation.5 Related WorkStudies have been conducted to make use of morealternatives to cope with the unreliability of 1-best results.
Liu et al (2009) proposed a struc-ture called weighted alignment matrix, which en-codes the distribution over all possible alignments.11Model 1 10 20 50 100 150 200 Nile Nakazawa IBM Model 4Precision 82.56 82.90 83.51 83.28 83.77 83.34 83.39 83.26 70.59 63.21Recall 80.79 80.88 80.62 80.75 81.05 80.66 80.75 81.52 82.67 74.25F-score 81.66 81.87 82.04 81.99 82.39 81.98 82.05 82.38 76.16 68.29Table 1: Precision, Recall and F-score for ASPEC-JE.
The numbers in the first row refer to the numberof k-best parse trees used to generate forests.
(a) 1-best and 100-best model comparison in the alignment of?onto a single row of detectors on?
and ??
(on) ?
(of) ??(single)?
(row)?
(of)???
(detectors)?
(b) 1-best and 200-best model comparison in the alignment of?in comparison with one in 2000?
and ?2030(2030) ?
(year)??
(in)?Figure 2: Alignment result: Black boxes represent golden alignments.
Triangles represent 1-best modelalignments.
Circles represent the alignments of proposed model.
Black and red arcs represent 1-bestparses and chosen parses respectively.They introduced a way to extract phrase pairsand estimate their probabilities.
Their proposedmethod outperformed the baseline which uses n-best alignments.
Venugopal et al (2008) usedn-best alignments and parses to generate fractioncounts used for machine translation downstreamestimation.
While their approaches are to use n-best alignments already obtained from some align-ment models, our model finds k-best list of align-ments for given sentences.Mi et al (2008) and Tu et al (2010) used packedconstituency forests and dependency forests re-spectively for decoding.
The best path that is suit-able for translation is chosen from the forest dur-ing decoding, leading to significant improvementin translation quality.
Note that they do not useforests for obtaining word alignments.The approaches for modeling word alignmentcan be divided into two categories: discrimi-native models (Dyer et al, 2011; Setiawan etal., 2010) and generative models (Brown et al,1993; Nakazawa and Kurohashi, 2011).
Gener-ative models such as the IBM models (Brown etal., 1993) have the advantage that they do not re-quire golden alignment training data annotated byhumans.
However, it is difficult to incorporatearbitrary features in these models.
On the otherhand, discriminative models can incorporate arbi-trary features such as syntactic information, butthey generally require gold training data, which ishard to obtain in large scale.
For discriminativemodels, word alignment models using deep neuralnetwork have been proposed recently (Tamura etal., 2014; Songyot and Chiang, 2014; Yang et al,2013).6 ConclusionIn this work, we proposed a hierarchical alignmentmodel based on dependency forests, which ad-vanced an alignment model that uses constituencyparse trees (Riesa et al, 2011) to allow to use moresuitable parse trees for word alignment.
Experi-mental results on the Japanese-English languagepair show a relative error reduction of 4% of thealignment score compared to a model with 1-bestparse trees that using forest on the target side.Our future work will involve the implementa-tion of missing features, because the automatictranslation rule features had a large contributionto the improvement of alignment quality in Nile.12The experimental results show that Nile, whichuses 1-best constituency parses , had almost thesame F-score as our proposed method with 100-best parse trees.
It will be interesting to see theeffect of using forests in Nile.Moreover, we are considering to investigate theefficacy of our model with different parsers andlanguage pairs.Finally, we are also considering using trainingdata with richer information such as the one de-scribed in (Li et al, 2010).AcknowledgementsWe thank Graham Neubig for his valuable com-ments during a pre-submission mentoring pro-gram, which had greatly improved the quality ofthe manuscript.
We also thank the anonymous re-viewers for their helpful comments.ReferencesPierre Boullier, Alexis Nasr, and Beno?
?t Sagot.
2009.Constructing parse forests that include exactly then-best pcfg trees.
In Proceedings of the 11thInternational Conference on Parsing Technologies(IWPT?09), pages 117?128, Paris, France, October.Association for Computational Linguistics.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational linguistics, 19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Chris Dyer, Jonathan Clark, Alon Lavie, and Noah ASmith.
2011.
Unsupervised word alignment with ar-bitrary features.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 409?419.
Association for Computational Lin-guistics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Association forComputational Linguistics, pages 586?594.Sylvain Kahane.
2012.
Why to choose dependencyrather than constituency for syntax: a formal pointof view.
J. Apresjan, M.-C. L ?Homme, M.-C.Iomdin, J. Milicevic, A. Polgu`ere, and L. Wanner, ed-itors, Meanings, Texts, and other exciting things: AFestschrift to Commemorate the 80th Anniversary ofProfessor Igor A. Mel ?cuk, pages 257?272.Daisuke Kawahara and Sadao Kurohashi.
2006.
Afully-lexicalized probabilistic model for japanesesyntactic and case structure analysis.
In Proceedingsof the Human Language Technology Conference ofthe NAACL, Main Conference, pages 176?183, NewYork City, USA, June.
Association for Computa-tional Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Xuansong Li, Niyu Ge, Stephen Grimes, StephanieStrassel, and Kazuaki Maeda.
2010.
Enrichingword alignment with linguistic tags.
In LanguageResources and Evaluation Conference.Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Pro-cessing: Volume 2-Volume 2, pages 1017?1026.
As-sociation for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Association for ComputationalLinguistics, pages 192?199.Toshiaki Nakazawa and Sadao Kurohashi.
2011.Bayesian subtree alignment model based on depen-dency trees.
In International Joint Conference onNatural Language Processing, pages 794?802.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Association for Computational Linguistics.Jason Riesa, Ann Irvine, and Daniel Marcu.
2011.Feature-rich language-independent syntax-basedalignment for statistical machine translation.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 497?507.Association for Computational Linguistics.Hendra Setiawan, Chris Dyer, and Philip Resnik.
2010.Discriminative word alignment with a function wordreordering model.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 534?544.
Association for Com-putational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of ACL-08: HLT, pages 577?585,Columbus, Ohio, June.
Association for Computa-tional Linguistics.13Theerawat Songyot and David Chiang.
2014.
Improv-ing word alignment using word similarity.
In Em-pirical Methods in Natural Language Processing,pages 1840?1845.
Citeseer.Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.2014.
Recurrent neural networks for word align-ment model.
In Antenna Measurement TechniquesAssociation, pages 1470?1480.Zhaopeng Tu, Yang Liu, Young-Sook Hwang, QunLiu, and Shouxun Lin.
2010.
Dependency for-est for statistical machine translation.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 1092?1100.
Associa-tion for Computational Linguistics.Ashish Venugopal, Andreas Zollmann, Noah A Smith,and Stephan Vogel.
2008.
Wider pipelines: N-bestalignments and parses in mt training.
In Proceed-ings of Antenna Measurement Techniques Associa-tion, pages 192?201.
Citeseer.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and NenghaiYu.
2013.
Word alignment modeling with contextdependent deep neural network.
In Antenna Mea-surement Techniques Association, pages 166?175.14
