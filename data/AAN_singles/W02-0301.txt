Tuning Support Vector Machines for Biomedical Named Entity RecognitionJun?ichi Kazama?
Takaki Makino?
Yoshihiro Ohta?
Jun?ichi Tsujii?
??
Department of Computer Science, Graduate School of Information Science and Technology,University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan?
Department of Complexity Science and Engineering, Graduate School of Frontier Sciences,University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan?
Central Research Laboratory, Hitachi, Ltd., Kokubunji, Tokyo 185-8601, Japan?
CREST, JST (Japan Science and Technology Corporation)AbstractWe explore the use of Support Vector Ma-chines (SVMs) for biomedical named en-tity recognition.
To make the SVM train-ing with the available largest corpus ?
theGENIA corpus ?
tractable, we propose tosplit the non-entity class into sub-classes,using part-of-speech information.
In ad-dition, we explore new features such asword cache and the states of an HMMtrained by unsupervised learning.
Experi-ments on the GENIA corpus show that ourclass splitting technique not only enablesthe training with the GENIA corpus butalso improves the accuracy.
The proposednew features also contribute to improvethe accuracy.
We compare our SVM-based recognition system with a systemusing Maximum Entropy tagging method.1 IntroductionApplication of natural language processing (NLP) isnow a key research topic in bioinformatics.
Sinceit is practically impossible for a researcher to graspall of the huge amount of knowledge provided inthe form of natural language, e.g., journal papers,there is a strong demand for biomedical informationextraction (IE), which extracts knowledge automati-cally from biomedical papers using NLP techniques(Ohta et al, 1997; Proux et al, 2000; Yakushiji etal., 2001).The process called named entity recognition,which finds entities that fill the information slots,e.g., proteins, DNAs, RNAs, cells etc., in thebiomedical context, is an important building block insuch biomedical IE systems.
Conceptually, namedentity recognition consists of two tasks: identifica-tion, which finds the region of a named entity ina text, and classification, which determines the se-mantic class of that named entity.
The following il-lustrates biomedical named entity recognition.
?Thus, CIITAPROTEIN not only acti-vates the expression of class II genesDNAbut recruits another B cell-specificcoactivator to increase transcriptionalactivity of class II promotersDNA inB cellsCELLTYPE.
?Machine learning approach has been applied tobiomedical named entity recognition (Nobata et al,1999; Collier et al, 2000; Yamada et al, 2000;Shimpuku, 2002).
However, no work has achievedsufficient recognition accuracy.
One reason is thelack of annotated corpora for training as is oftenthe case of a new domain.
Nobata et al (1999) andCollier et al (2000) trained their model with only100 annotated paper abstracts from the MEDLINEdatabase (National Library of Medicine, 1999), andYamada et al (2000) used only 77 annotated paperabstracts.
In addition, it is difficult to compare thetechniques used in each study because they used aclosed and different corpus.To overcome such a situation, the GENIA cor-pus (Ohta et al, 2002) has been developed, and atthis time it is the largest biomedical annotated cor-pus available to public, containing 670 annotated ab-stracts of the MEDLINE database.Another reason for low accuracies is that biomed-ical named entities are essentially hard to recognizeusing standard feature sets compared with the namedentities in newswire articles (Nobata et al, 2000).Thus, we need to employ powerful machine learningtechniques which can incorporate various and com-plex features in a consistent way.Support Vector Machines (SVMs) (Vapnik, 1995)and Maximum Entropy (ME) method (Berger et al,1996) are powerful learning methods that satisfysuch requirements, and are applied successfully toother NLP tasks (Kudo and Matsumoto, 2000; Nak-agawa et al, 2001; Ratnaparkhi, 1996).
In this pa-per, we apply Support Vector Machines to biomed-ical named entity recognition and train them withAssociation for Computational Linguistics.the Biomedical Domain, Philadelphia, July 2002, pp.
1-8.Proceedings of the Workshop on Natural Language Processing inthe GENIA corpus.
We formulate the named entityrecognition as the classification of each word withcontext to one of the classes that represent regionand named entity?s semantic class.
Although thereis a previous work that applied SVMs to biomedi-cal named entity task in this formulation (Yamada etal., 2000), their method to construct a classifier us-ing SVMs, one-vs-rest, fails to train a classifier withentire GENIA corpus, since the cost of SVM train-ing is super-linear to the size of training samples.Even with a more feasible method, pairwise (Kre?el,1998), which is employed in (Kudo and Matsumoto,2000), we cannot train a classifier in a reasonabletime, because we have a large number of samplesthat belong to the non-entity class in this formula-tion.
To solve this problem, we propose to split thenon-entity class to several sub-classes, using part-of-speech information.
We show that this technique notonly enables the training feasible but also improvesthe accuracy.In addition, we explore new features such as wordcache and the states of an unsupervised HMM fornamed entity recognition using SVMs.
In the exper-iments, we show the effect of using these featuresand compare the overall performance of our SVM-based recognition system with a system using theMaximum Entropy method, which is an alternativeto the SVM method.2 The GENIA CorpusThe GENIA corpus is an annotated corpus of pa-per abstracts taken from the MEDLINE database.Currently, 670 abstracts are annotated with namedentity tags by biomedical experts and made avail-able to public (Ver.
1.1).1 These 670 abstracts are asubset of more than 5,000 abstracts obtained by thequery ?human AND blood cell AND transcriptionfactor?
to the MEDLINE database.
Table 1 showsbasic statistics of the GENIA corpus.
Since the GE-NIA corpus is intended to be extensive, there exist24 distinct named entity classes in the corpus.2 Ourtask is to find a named entity region in a paper ab-stract and correctly select its class out of these 24classes.
This number of classes is relatively largecompared with other corpora used in previous stud-ies, and compared with the named entity task fornewswire articles.
This indicates that the task withthe GENIA corpus is hard, apart from the difficultyof the biomedical domain itself.1Available via http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/2The GENIA corpus also has annotations for conjunc-tive/disjunctive named entity expressions such as ?human B- orT-cell lines?
(Kim et al, 2001).
In this paper we ignore suchexpressions and consider that constituents in such expressionsare annotated as a dummy class ?temp?.Table 1: Basic statistics of the GENIA corpus# of sentences 5,109# of words 152,216# of named entities 23,793# of words in NEs 50,229# of words not in NEs 101,987Av.
length of NEs (?)
2.11 (1.40)3 Named Entity Recognition Using SVMs3.1 Named Entity Recognition as ClassificationWe formulate the named entity task as the classi-fication of each word with context to one of theclasses that represent region information and namedentity?s semantic class.
Several representations toencode region information are proposed and exam-ined (Ramshaw and Marcus, 1995; Uchimoto et al,2000; Kudo and Matsumoto, 2001).
In this paper,we employ the simplest BIO representation, whichis also used in (Yamada et al, 2000).
We modifythis representation in Section 5.1 in order to acceler-ate the SVM training.In the BIO representation, the region informationis represented as the class prefixes ?B-?
and ?I-?, anda class ?O?.
B- means that the current word is at thebeginning of a named entity, I- means that the cur-rent word is in a named entity (but not at the be-ginning), and O means the word is not in a namedentity.
For each named entity class C, class B-C andI-C are produced.
Therefore, if we have N namedentity classes, the BIO representation yields 2N + 1classes, which will be the targets of a classifier.
Forinstance, the following corresponds to the annota-tion ?Number of glucocorticoid receptorsPROTEIN inlymphocytesCELLTYPE and ...?.Number of glucocorticoid receptorsO O B-PROTEIN I-PROTEINin lymphocytes and ...O B-CELLTYPE O ...3.2 Support Vector MachinesSupport Vector Machines (SVMs) (Cortes and Vap-nik, 1995) are powerful methods for learning a clas-sifier, which have been applied successfully to manyNLP tasks such as base phrase chunking (Kudo andMatsumoto, 2000) and part-of-speech tagging (Nak-agawa et al, 2001).The SVM constructs a binary classifier that out-puts +1 or ?1 given a sample vector x ?
Rn.
The de-cision is based on the separating hyperplane as fol-lows.c(x) =????
?+1 if w ?
x + b > 0, w ?
Rn, b ?
R,?1 otherwiseThe class for an input x, c(x), is determined by see-ing which side of the space separated by the hyper-plane, w ?
x + b = 0, the input lies on.Given a set of labeled training samples{(y1, x1), ?
?
?
, (yL, xL)}, xi ?
Rn, yi ?
{+1,?1},the SVM training tries to find the optimal hy-perplane, i.e., the hyperplane with the maximummargin.
Margin is defined as the distance betweenthe hyperplane and the training samples nearestto the hyperplane.
Maximizing the margin insiststhat these nearest samples (support vectors) existon both sides of the separating hyperplane and thehyperplane lies exactly at the midpoint of thesesupport vectors.
This margin maximization tightlyrelates to the fine generalization power of SVMs.Assuming that |w?xi+b| = 1 at the support vectorswithout loss of generality, the SVM training can beformulated as the following optimization problem.3minimize 12||w||2subject to yi(w ?
xi + b) ?
1, i = 1, ?
?
?
, L.The solution of this problem is known to be writtenas follows, using only support vectors and weightsfor them.f (x) = w ?
x + b=?i?S V syi?ix ?
xi + b (1)In the SVM learning, we can use a function k(xi, x j)called a kernel function instead of the inner prod-uct in the above equation.
Introducing a kernelfunction means mapping an original input x using?
(x), s.t.
?
(xi) ??
(x j) = k(xi, x j) to another, usuallya higher dimensional, feature space.
We constructthe optimal hyperplane in that space.
By using ker-nel functions, we can construct a non-linear separat-ing surface in the original feature space.
Fortunately,such non-linear training does not increase the com-putational cost if the calculation of the kernel func-tion is as cheap as the inner product.
A polynomialfunction defined as (sxi ?
x j + r)d is popular in ap-plications of SVMs to NLPs (Kudo and Matsumoto,2000; Yamada et al, 2000; Kudo and Matsumoto,2001), because it has an intuitively sound interpre-tation that each dimension of the mapped space is a3For many real-world problems where the samples may beinseparable, we allow the constraints are broken with somepenalty.
In the experiments, we use so-called 1-norm soft mar-gin formulation described as:minimize 12||w||2 + CL?i=1?isubject to yi(w ?
xi + b) ?
1 ?
?i, i = 1, ?
?
?
, L,?i ?
0, i = 1, ?
?
?
, L.(weighted) conjunction of d features in the originalsample.3.3 Multi-Class SVMsAs described above, the standard SVM learning con-structs a binary classifier.
To make a named entityrecognition system based on the BIO representation,we require a multi-class classifier.
Among severalmethods for constructing a multi-class SVM (Hsuand Lin, 2002), we use a pairwise method proposedby Kre?el (1998) instead of the one-vs-rest methodused in (Yamada et al, 2000), and extend the BIOrepresentation to enable the training with the entireGENIA corpus.
Here we describe the one-vs-restmethod and the pairwise method to show the neces-sity of our extension.Both one-vs-rest and pairwise methods constructa multi-class classifier by combining many binarySVMs.
In the following explanation, K denotes thenumber of the target classes.one-vs-rest Construct K binary SVMs, each ofwhich determines whether the sample shouldbe classified as class i or as the other classes.The output is the class with the maximum f (x)in Equation 1.pairwise Construct K(K ?
1)/2 binary SVMs, eachof which determines whether the sample shouldbe classified as class i or as class j.
Each binarySVM has one vote, and the output is the classwith the maximum votes.Because the SVM training is a quadratic optimiza-tion program, its cost is super-linear to the size of thetraining samples even with the tailored techniquessuch as SMO (Platt, 1998) and kernel evaluationcaching (Joachims, 1998).
Let L be the number ofthe training samples, then the one-vs-rest methodtakes time in K ?
OS V M(L).
The BIO formula-tion produces one training sample per word, andthe training with the GENIA corpus involves over100,000 training samples as can be seen from Ta-ble 1.
Therefore, it is apparent that the one-vs-rest method is impractical with the GENIA corpus.On the other hand, if target classes are equally dis-tributed, the pairwise method will take time in K(K?1)/2?OS V M(2L/K).
This method is worthwhile be-cause each training is much faster, though it requiresthe training of (K ?
1)/2 times more classifiers.
Itis also reported that the pairwise method achieveshigher accuracy than other methods in some bench-marks (Kre?el, 1998; Hsu and Lin, 2002).3.4 Input FeaturesAn input x to an SVM classifier is a feature repre-sentation of the word to be classified and its context.We use a bit-vector representation, each dimensionof which indicates whether the input matches witha certain feature.
The following illustrates the well-used features for the named entity recognition task.wk,i =??????????
?1 if a word at k,Wk, is the ith wordin the vocabularyV0 otherwise (word feature)posk,i =??????????
?1 if Wk is assigned the ith POS tagin the POS tag list POS0 otherwise (part-of-speech feature)prek,i =??????????
?1 if Wk starts with the ith prefixin the prefix list P0 otherwise (prefix feature)suf k,i =??????????
?1 if Wk starts with the ith suffixin the suffix list S0 otherwise (suffix feature)subk,i =??????????
?1 if Wk contains the ith substringin the substring list SB0 otherwise (substring feature)pck,i =????
?1 if Wk(k < 0) was assigned ith class0 otherwise (preceding class feature)In the above definitions, k is a relative word positionfrom the word to be classified.
A negative value rep-resents a preceding word?s position, and a positivevalue represents a following word?s position.
Notethat we assume that the classification proceeds leftto right as can be seen in the definition of the pre-ceding class feature.
For the SVM classification, wedoes not use a dynamic argmax-type classificationsuch as the Viterbi algorithm, since it is difficult todefine a good comparable value for the confidence ofa prediction such as probability.
The consequencesof this limitation will be discussed with the experi-mental results.Features usually form a group with some vari-ables such as the position unspecified.
In this paper,we instantiate all features, i.e., instantiate for all i,for a group and a position.
Then, it is convenient todenote a set of features for a group g and a positionk as gk (e.g., wk and posk).
Using this notation, wewrite a feature set as {w?1,w0, pre?1, pre0, pc?1}.4This feature description derives the following inputvector.5x = {w?1,1,w?1,2, ?
?
?
,w?1,|V|,w0,1, ?
?
?
,w0,|V|,pre?1,1, ?
?
?
, pre0,|P|, pc?1,1, ?
?
?
, pc?1,K}4We will further compress this as {?w, pre?
[?1,0], pc?1}.5Although a huge number of features are instantiated, onlya few features have value one for a given g and k pair.4 Named Entity Recognition Using MEModelThe Maximum Entropy method, with which wecompare our SVM-based method, defines the prob-ability that the class is c given an input vector x asfollows.P(c|x) = 1Z(x)?i?
fi(c,x)i ,where Z(x) is a normalization constant, and fi(c, x)is a feature function.
A feature function is definedin the same way as the features in the SVM learn-ing, except that it includes c in it like f (c, x) =(c is the jth class) ?
wi,k(x).
If x contains pre-viously assigned classes, then the most probableclass sequence, c?T1 = argmaxc1,???
,cT?Tt=1 P(ct|xt) issearched by using the Viterbi-type algorithm.
Weuse the maximum entropy tagging method describedin (Kazama et al, 2001) for the experiments, whichis a variant of (Ratnaparkhi, 1996) modified to useHMM state features.5 Tuning of SVMs for Biomedical NE Task5.1 Class Splitting TechniqueIn Section 3.3, we described that if target classes areequally distributed, the pairwise method will reducethe training cost.
In our case, however, we have avery unbalanced class distribution with a large num-ber of samples belonging to the class ?O?
(see Table1).
This leads to the same situation with the one-vs-rest method, i.e., if LO is the number of the samplesbelonging to the class ?O?, then the most dominantpart of the training takes time in K ?
OS V M(LO).One solution to this unbalanced class distributionproblem is to split the class ?O?
into several sub-classes effectively.
This will reduce the training costfor the same reason that the pairwise method works.In this paper, we propose to split the non-entityclass according to part-of-speech (POS) informa-tion of the word.
That is, given a part-of-speechtag set POS, we produce new |POS| classes, ?O-p?
p ?
POS.
Since we use a POS tagger that out-puts 45 Penn Treebank?s POS tags in this paper, wehave new 45 sub-classes which correspond to non-entity regions such as ?O-NNS?
(plural nouns), ?O-JJ?
(adjectives), and ?O-DT?
(determiners).Splitting by POS information seems useful for im-proving the system accuracy as well, because in thenamed entity recognition we must discriminate be-tween nouns in named entities and nouns in ordi-nal noun phrases.
In the experiments, we show thisclass splitting technique not only enables the feasi-ble training but also improves the accuracy.5.2 Word Cache and HMM FeaturesIn addition to the standard features, we explore wordcache feature and HMM state feature, mainly tosolve the data sparseness problem.Although the GENIA corpus is the largest anno-tated corpus for the biomedical domain, it is stillsmall compared with other linguistic annotated cor-pora such as the Penn Treebank.
Thus, the datasparseness problem is severe, and must be treatedcarefully.
Usually, the data sparseness is preventedby using more general features that apply to abroader set of instances (e.g., disjunctions).
Whilepolynomial kernels in the SVM learning can effec-tively generate feature conjunctions, kernel func-tions that can effectively generate feature disjunc-tions are not known.
Thus, we should explicitly adddimensions for such general features.The word cache feature is defined as the disjunc-tion of several word features as:wck{k1,???
,kn},i ?
?k?kwk,iWe intend that the word cache feature captures thesimilarities of the patterns with a common key wordsuch as follows.
(a) ?human W?2 W?1 W0?
and ?human W?1 W0?
(b) ?W0 gene?
and ?W0 W1 gene?We use a left word cache defined as lwck,i ?wc{?k,???
,0},i, and a right word cache defined asrwck,i ?
wc{1,???
,k},i for patterns like (a) and (b) inthe above example respectively.Kazama et al (2001) proposed to use as featuresthe Viterbi state sequence of a hidden Markov model(HMM) to prevent the data sparseness problem inthe maximum entropy tagging model.
An HMM istrained with a large number of unannotated texts byusing an unsupervised learning method.
Becausethe number of states of the HMM is usually madesmaller than |V|, the Viterbi states give smoothedbut maximally informative representations of wordpatterns tuned for the domain, from which the rawtexts are taken.The HMM feature is defined in the same way asthe word feature as follows.hmmk,i =??????????
?1 if the Viterbi state for Wk isthe ith state in the HMM?s statesH0 otherwise (HMM feature)In the experiments, we train an HMM using rawMEDLINE abstracts in the GENIA corpus, andshow that the HMM state feature can improve theaccuracy.5.3 Implementation IssuesTowards practical named entity recognition usingSVMs, we have tackled the following implementa-tion issues.
It would be impossible to carry out theexperiments in a reasonable time without such ef-forts.Parallel Training: The training of pairwise SVMshas trivial parallelism, i.e., each SVM can be trainedseparately.
Since computers with two or more CPUsare not expensive these days, parallelization is verypractical solution to accelerate the training of pair-wise SVMs.Fast Winner Finding: Although the pairwisemethod reduces the cost of training, it greatly in-creases the number of classifications needed to de-termine the class of one sample.
For example, forour experiments using the GENIA corpus, the BIOrepresentation with class splitting yields more than4,000 classification pairs.
Fortunately, we can stopclassifications when a class gets K ?
1 votes and thisstopping greatly saves classification time (Kre?el,1998).
Moreover, we can stop classifications whenthe current votes of a class is greater than the others?possible votes.Support Vector Caching: In the pairwise method,though we have a large number of classifiers, eachclassifier shares some support vectors with otherclassifiers.
By storing the bodies of all support vec-tors together and letting each classifier have only theweights, we can greatly reduce the size of the clas-sifier.
The sharing of support vectors also can beexploited to accelerate the classification by cachingthe value of the kernel function between a supportvector and a classifiee sample.6 ExperimentsTo conduct experiments, we divided 670 abstractsof the GENIA corpus (Ver.
1.1) into the train-ing part (590 abstracts; 4,487 sentences; 133,915words) and the test part (80 abstracts; 622 sen-tences; 18,211 words).6 Texts are tokenized by us-ing Penn Treebank?s tokenizer.
An HMM for theHMM state features was trained with raw abstractsof the GENIA corpus (39,116 sentences).7 Thenumber of states is 160.
The vocabulary for theword feature is constructed by taking the most fre-quent 10,000 words from the above raw abstracts,the prefix/suffix/prefix list by taking the most fre-quent 10,000 prefixes/suffixes/substrings.8The performance is measured by precision, recall,and F-score, which are the standard measures for the6Randomly selected set used in (Shimpuku, 2002).
We donot use paper titles, while he used.7These do not include the sentences in the test part.8These are constructed using the training part to make thecomparison with the ME method fair.Table 2: Training time and accuracy with/withoutthe class splitting technique.
The number of trainingsamples includes SOS and EOS (special words forthe start/end of a sentence).no splitting splittingtraining time acc.
time acc.samples (sec.)
(F-score) (sec.)
(F-score)16,000 2,809 37.04 5,581 36.8232,000 13,614 40.65 9,175 41.3648,000 21,174 42.44 9,709 42.4964,000 40,869 42.52 12,502 44.3496,000 - - 21,922 44.93128,000 - - 36,846 45.99named entity recognition.
Systems based on the BIOrepresentation may produce an inconsistent class se-quence such as ?O B-DNA I-RNA O?.
We interpretsuch outputs as follows: once a named entity startswith ?B-C?
then we interpret that the named entitywith class ?C?
ends only when we see another ?B-?or ?O-?
tag.We have implemented SMO algorithm (Platt,1998) and techniques described in (Joachims, 1998)for soft margin SVMs in C++ programming lan-guage, and implemented support codes for pairwiseclassification and parallel training in Java program-ming language.
To obtain POS information requiredfor features and class splitting, we used an EnglishPOS tagger described in (Kazama et al, 2001).6.1 Class Splitting TechniqueFirst, we show the effect of the class splittingdescribed in Section 5.1.
Varying the size oftraining data, we compared the change in thetraining time and the accuracy with and with-out the class splitting.
We used a feature set{?w, pre, suf , sub, pos?[?2,???
,2], pc[?2,?1]} and the in-ner product kernel.9 The training time was mea-sured on a machine with four 700MHz PentiumIIIsand 16GB RAM.
Table 2 shows the results of theexperiments.
Figure 1 shows the results graphi-cally.
We can see that without splitting we soon suf-fer from super-linearity of the SVM training, whilewith splitting we can handle the training with over100,000 samples in a reasonable time.
It is very im-portant that the splitting technique does not sacrificethe accuracy for speed, rather improves the accuracy.6.2 Word Cache and HMM State FeaturesIn this experiment, we see the effect of the wordcache feature and the HMM state feature describedin Section 3.4.
The effect is assessed by theaccuracy gain observed by adding each featureset to a base feature set and the accuracy degra-dation observed by subtracting it from a (com-9Soft margin constant C is 1.0 throughout the experiments.Table 3: Effect of each feature set assessed byadding/subtracting (F-score).
Changes in bold facemeans positive effect.feature set (A) adding (B) sub.
(k=2) (C) sub.
(k=3)Base 42.86 47.82 49.27Left cache 43.25 (+0.39) 47.77 (-0.05) 49.02 (-0.25)Right cache 42.34 (-0.52) 47.81 (-0.01) 49.07 (-0.20)HMM state 44.70 (+1.84) 47.25 (-0.57) 48.03 (-1.24)POS 44.82 (+1.96) 48.29 (+0.47) 48.75 (-0.52)Prec.
class 44.58 (+1.72) 43.32 (-4.50) 43.84 (-5.43)Prefix 42.77 (-0.09) 48.11 (+0.29) 48.73 (-0.54)Suffix 45.88 (+3.02) 47.07 (-0.75) 48.48 (-0.79)Substring 42.16 (-0.70) 48.38 (+0.56) 50.23 (+0.96)plete) base set.
The first column (A) in Ta-ble 3 shows an adding case where the base fea-ture set is {w[?2,???
,2]}.
The columns (B) and(C) show subtracting cases where the base featureset is {?w, pre, suf , sub, pos, hmm?[?k,???
,k], lwck, rwck,pc[?2,?1]} with k = 2 and k = 3 respectively.
Thekernel function is the inner product.
We can see thatword cache and HMM state features surely improvethe recognition accuracy.
In the table, we also in-cluded the accuracy change for other standard fea-tures.
Preceeding classes and suffixes are definitelyhelpful.
On the other hand, the substring feature isnot effective in our setting.
Although the effects ofpart-of-speech tags and prefixes are not so definite,it can be said that they are practically effective sincethey show positive effects in the case of the maxi-mum performance.6.3 Comparison with the ME MethodIn this set of experiments, we compare ourSVM-based system with a named entity recog-nition system based on the Maximum Entropymethod.
For the SVM system, we used the fea-ture set {?w, pre, suf , pos, hmm?[?3,???
,3], lwc3, rwc3,pc[?2,?1]}, which is shown to be the best in the pre-vious experiment.
The compared system is a max-imum entropy tagging model described in (Kazamaet al, 2001).
Though it supports several charactertype features such as number and hyphen and someconjunctive features such as word n-gram, we do notuse these features to compare the performance un-der as close a condition as possible.
The feature setused in the maximum entropy system is expressedas {?w, pre, suf , pos, hmm?[?2,???
,2], pc[?2,?1]}.10 Bothsystems use the BIO representation with splitting.Table 4 shows the accuracies of both systems.
Forthe SVM system, we show the results with the innerproduct kernel and several polynomial kernels.
Therow ?All (id)?
shows the accuracy from the view-10When the width becomes [?3, ?
?
?
, 3], the accuracy de-grades (53.72 to 51.73 in F-score).0500010000150002000025000300003500040000450000  20000  40000  60000  80000  100000  120000  140000Training Time(seconds)Number of training samplesNo splitSplit(a) Training size vs. time0.360.370.380.390.40.410.420.430.440.450.460  5000  10000  15000  20000  25000  30000  35000  40000  45000Term Accuracy (F-Score)Training Time (seconds)No splitSplit(b) Training time vs. accuracyFigure 1: Effect of the class splitting technique.point of the identification task, which only finds thenamed entity regions.
The accuracies for several ma-jor entity classes are also shown.
The SVM systemwith the 2-dimensional polynomial kernel achievesthe highest accuracy.
This comparison may be un-fair since a polynomial kernel has the effect of us-ing conjunctive features, while the ME system doesnot use such conjunctive features.
Nevertheless, thefacts: we can introduce the polynomial kernel veryeasily; there are very few parameters to be tuned;11we could achieve the higher accuracy; show an ad-vantage of the SVM system.It will be interesting to discuss why the SVM sys-tems with the inner product kernel (and the polyno-mial kernel with d = 1) are outperformed by the MEsystem.
We here discuss two possible reasons.
Thefirst is that the SVM system does not use a dynamicdecision such as the Viterbi algorithm, while the MEsystem uses it.
To see this, we degrade the ME sys-tem so that it predicts the classes deterministicallywithout using the Viterbi algorithm.
We found thatthis system only marks 51.54 in F-score.
Thus, it canbe said that a dynamic decision is important for thisnamed entity task.
However, although a method toconvert the outputs of a binary SVM to probabilisticvalues is proposed (Platt, 1999), the way to obtainmeaningful probabilistic values needed in Viterbi-type algorithms from the outputs of a multi-classSVM is unknown.
Solving this problem is certainlya part of the future work.
The second possible rea-son is that the SVM system in this paper does notuse any cut-off or feature truncation method to re-move data noise, while the ME system uses a sim-ple feature cut-off method.12 We observed that theME system without the cut-off only marks 49.11 in11C, s, r, and d12Features that occur less than 10 times are removed.F-score.
Thus, such a noise reduction method isalso important.
However, the cut-off method for theME method cannot be applied without modificationsince, as described in Section 3.4, the definition ofthe features are different in the two approaches.
Itcan be said the features in the ME method is ?finer?than those in SVMs.
In this sense, the ME methodallows us more flexible feature selection.
This is anadvantage of the ME method.The accuracies achieved by both systems can besaid high compared with those of the previous meth-ods if we consider that we have 24 named entityclasses.
However, the accuracies are not sufficientfor a practical use.
Though higher accuracy will beachieved with a larger annotated corpus, we shouldalso explore more effective features and find effec-tive feature combination methods to exploit such alarge corpus maximally.7 ConclusionWe have described the use of Support Vector Ma-chines for the biomedical named entity recognitiontask.
To make the training of SVMs with the GE-NIA corpus practical, we proposed to split the non-entity class by using POS information.
In addition,we explored the new types of features, word cacheand HMM states, to avoid the data sparseness prob-lem.
In the experiments, we have shown that theclass splitting technique not only makes training fea-sible but also improves the accuracy.
We have alsoshown that the proposed new features also improvethe accuracy and the SVM system with the polyno-mial kernel function outperforms the ME-based sys-tem.AcknowledgementsWe would like to thank Dr. Jin-Dong Kim for pro-viding us easy-to-use preprocessed training data.Table 4: Comparison: The SVM-based system and the ME-based system.
(precision/recall/F-score)SVM MEinner product polynomial (s = 0.01, r = 1.0))type # d = 1 d = 2 d = 3All (2,782) 50.7 /49.8 /50.2 54.6 /48.8 /51.5 56.2 /52.8 /54.4 55.1 /51.5 /53.2 53.4 /53.0 /53.2All(id) 71.8 /70.4 /71.1 75.0 /67.1 /70.8 75.9 /71.4 /73.6 75.3 /70.3 /72.7 73.5 /72.9 /73.2protein (709) 47.2 /55.2 /50.8 45.7 /64.9 /53.6 49.2 /66.4 /56.5 48.7 /64.7 /55.6 49.1 /62.1 /54.8DNA (460) 39.9 /37.6 /38.7 48.2 /31.5 /38.1 49.6 /37.0 /42.3 47.9 /37.4 /42.0 47.3 /39.6 /43.1cell line (121) 54.8 /47.1 /50.7 61.2 /43.0 /50.5 60.2 /46.3 /52.3 62.2 /46.3 /53.1 58.0 /53.7 /55.8cell type (199) 67.6 /74.4 /70.8 67.4 /74.9 /71.0 70.0 /75.4 /72.6 68.6 /72.4 /70.4 69.9 /72.4 /71.1lipid (109) 77.0 /61.5 /68.4 83.3 /50.5 /62.9 82.7 /61.5 /70.5 79.2 /56.0 /65.6 68.9 /65.1 /67.0other names (590) 52.5 /53.9 /53.2 60.2 /55.9 /58.0 59.3 /58.0 /58.6 58.9 /57.8 /58.3 59.0 /61.7 /60.3ReferencesA.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.
Amaximum entropy approach to natural language processing.Computational Linguistics, 22(1):39?71.N.
Collier, C. Nobata, and J. Tsujii.
2000.
Extracting the namesof genes and gene products with a hidden Markov model.
InProc.
of COLING 2000, pages 201?207.C.
Cortes and V. Vapnik.
1995.
Support vector networks.
Ma-chine Learning, 20:273?297.C.
Hsu and C. Lin.
2002.
A comparison of methods for multi-class Support Vector Machines.
In IEEE Transactions onNeural Networks.
to appear.T.
Joachims.
1998.
Making large-scale support vector machinelearning practical.
In Advances in Kernel Methods, pages169?184.
The MIT Press.J.
Kazama, Y. Miyao, and J. Tsujii.
2001.
A maximum entropytagger with unsupervised hidden markov models.
In Proc.
ofthe 6th NLPRS, pages 333?340.J.
Kim, T. Ohta, Y. Tateisi, H. Mima, and J. Tsujii.
2001.
XML-based linguistic annotation of corpus.
In Proc.
of the FirstNLP and XML Workshop.U.
Kre?el.
1998.
Pairwise classification and support vectormachines.
In Advances in Kernel Methods, pages 255?268.The MIT Press.T.
Kudo and Y. Matsumoto.
2000.
Use of support vector learn-ing for chunk identification.
In Proc.
of CoNLL-2000 andLLL-2000.T.
Kudo and Y. Matsumoto.
2001.
Chunking with SupportVector Machines.
In Proc.
of NAACL 2001, pages 192?199.T.
Nakagawa, T. Kudoh, and Y. Matsumoto.
2001.
Unknownword guessing and part-of-speech tagging using support vec-tor machines.
In Proc.
of the 6th NLPRS, pages 325?331.National Library of Medicine.
1999.
MEDLINE.
available athttp://www.ncbi.nlm.nih.gov/.C.
Nobata, N. Collier, and J. Tsujii.
1999.
Automatic termidentification and classification in biology texts.
In Proc.
ofthe 5th NLPRS, pages 369?374.C.
Nobata, N. Collier, and J. Tsujii.
2000.
Comparison betweentagged corpora for the named entity task.
In Proc.
of theWorkshop on Comparing Corpora (at ACL?2000), pages 20?27.Y.
Ohta, Y. Yamamoto, T. Okazaki, I. Uchiyama, and T. Tak-agi.
1997.
Automatic construction of knowledge base frombiological papers.
In Proc.
of the 5th ISMB, pages 218?225.T.
Ohta, Y. Tateisi, J. Kim, H. Mima, and Tsujii J.
2002.
TheGENIA corpus: An annotated research abstract corpus inmolecular biology domain.
In Proc.
of HLT 2002.J.
C. Platt.
1998.
Fast training of support vector machines us-ing sequential minimal optimization.
In Advances in KernelMethods, pages 185?208.
The MIT Press.J.
C. Platt.
1999.
Probabilistic outputs for support vector ma-chines and comparisons to regularized likelihood methods.Advances in Large Margin Classifiers.D.
Proux, F. Prechenmann, and L. Julliard.
2000.
A pragmaticinformation extraction strategy for gathering data on geneticinteractions.
In Proc.
of the 8th ISMB, pages 279?285.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunking us-ing transformation-based learning.
In Proc.
of the 3rd ACLWorkshop on Very Large Corpora.A.
Ratnaparkhi.
1996.
A maximum entropy model for part-of-speech tagging.
In Proc.
of the Conference on EmpiricalMethods in Natural Language Processing, pages 133?142.S.
Shimpuku.
2002.
A medical/biological term recognizer witha term hidden Markov model incorporating multiple infor-mation sources.
A master thesis.
University of Tokyo.K.
Uchimoto, M. Murata, Q. Ma, H. Ozaku, and H. Isahara.2000.
Named entity extraction based on a maximum entropymodel and transformation rules.
In Proc.
of the 38th ACL,pages 326?335.V.
Vapnik.
1995.
The Nature of Statistical Learning Theory.Springer Verlag.A.
Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii.
2001.
Eventextraction from biomedical papers using a full parser.
InProc.
of PSB 2001, pages 408?419.H.
Yamada, T. Kudo, and Y. Matsumoto.
2000.
Using sub-strings for technical term extraction and classification.
IPSJSIGNotes, (NL-140):77?84.
(in Japanese).
