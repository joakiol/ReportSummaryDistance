Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145?154,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsLeveraging Verb-Argument Structures to Infer Semantic RelationsEduardo Blanco and Dan MoldovanLymba CorporationRichardson, TX 75080 USA{eduardo,moldovan}@lymba.comAbstractThis paper presents a methodology to in-fer implicit semantic relations from verb-argument structures.
An annotation effortshows implicit relations boost the amountof meaning explicitly encoded for verbs.Experimental results with automaticallyobtained parse trees and verb-argumentstructures demonstrate that inferring im-plicit relations is a doable task.1 IntroductionAutomatic extraction of semantic relations is animportant step towards capturing the meaning oftext.
Semantic relations explicitly encode links be-tween concepts.
For example, in The accident lefthim a changed man, the ?accident?
is the CAUSEof the man undergoing some ?change?.
A questionanswering system would benefit from detectingthis relation when answering Why did he change?Extracting all semantic relations from text is amonumental task and is at the core of languageunderstanding.
In recent years, approaches thataim at extracting a subset of all relations haveachieved great success.
In particular, previous re-search (Carreras and Ma`rquez, 2005; Punyakanoket al., 2008; Che et al., 2010; Zapirain et al., 2010)focused on verb-argument structures, i.e., relationsbetween a verb and its syntactic arguments.
Prop-Bank (Palmer et al., 2005) is the corpus of refer-ence for verb-argument relations.
However, rela-tions between a verb and its syntactic argumentsare only a fraction of the relations present in texts.Consider the statement [Mr. Brown]NP1suc-ceeds [Joseph W. Hibben, who retired lastAugust]NP2and its parse tree (Figure 1).
Verb-argument relations encode that NP1is the AGENTand NP2is the THEME of verb ?succeeds?
(Prop-Bank uses labels ARG0and ARG1).
Any se-mantic relation between ?succeeds?
and conceptsdominated in the parse tree by one of its syntac-tic arguments NP1or NP2, e.g., ?succeeds?
oc-SNP1VPMr.
Brown VBZ NP2succeedsAGENTTHEMETIME-AFTER[Joseph W. Hibben, who]AGENT[retired]v [last August]TIMEFigure 1: Example of parse tree and verb-argument structures (solid arrows).
The relationbetween ?succeeds?
and ?last August?
is missing,but a TIME-AFTER holds (dashed arrow).curred after ?last August?, are missing.
Note thatin this example, verb-argument structures encodethat ?retired ?
has TIME ?last August?, and thisknowledge could be exploited to infer the miss-ing relation.
The work presented here stems fromtwo observations: (1) verbs are semantically con-nected with concepts that are not direct syntac-tic arguments (henceforth, implicit relations); and(2) verb-argument structures can be leveraged toinfer implicit relations.This paper goes beyond verb-argument struc-tures and targets implicit relations like the onedepicted above.
TIME, LOCATION, MANNER,PURPOSE and CAUSE are inferred without im-posing syntactic restrictions between their argu-ments: systems trained over PropBank do not at-tempt to extract these relations.
An annotation ef-fort demonstrates implicit relations reveal as muchas 30% of meaning on top of verb-argument struc-tures.
The main contributions are: (1) empiricalstudy of verb-argument structures and implicit re-lations in PropBank; (2) annotations of implicit re-lations on top of PropBank; (3) novel features ex-tracted from verb-argument structures; and (4) ex-perimental results with features derived from goldand automatically obtained linguistic information,showing implicit relations can be extracted in a re-alistic environment.1452 Related WorkSeveral systems to extract verb-argument struc-tures from plain text have been proposed (Johans-son and Nugues, 2008; Che et al., 2010).
Thework presented here complements them with ad-ditional semantic relations.
The TimeBank corpus(Pustejovsky et al., 2003) and TempEval compe-titions (UzZaman et al., 2013) target events anddetailed temporal information; this work also tar-gets LOCATION, MANNER, PURPOSE and CAUSE.Extracting missing relations is not a new prob-lem.
Early work focused on a very limited domain(Palmer et al., 1986; Tetreault, 2002) or did notattempt to automate the task (Whittemore et al.,1991).
This section focuses on more recent work.Gerber and Chai (2010) augment NomBank an-notations (Meyers et al., 2004) of 10 predicateswith additional core arguments.
Their supervisedsystems obtain F-measures of 42.3 and 50.3 (Ger-ber and Chai, 2012).
Laparra and Rigau (2013a)present a deterministic algorithm and obtain an F-measure of 45.3.
In contrast, our approach doesnot focus on a few selected predicates or core argu-ments.
It targets all predicates and argument mod-ifiers (AM-TMP, AM-MNR, AM-LOC, etc.
), whosemeaning is shared across verbs.The SemEval-2010 Task 10: Linking Eventsand their Participants in Discourse (Ruppenhoferet al., 2009) targeted cross-sentence missing corearguments in both PropBank and FrameNet (Bakeret al., 1998).
Ruppenhofer et al.
(2013) detailthe annotations and results.
The task proved ex-tremely difficult, participants (Chen et al., 2010;Tonelli and Delmonte, 2010) reported overall F-measures around 2 (out of 100).
Posterior work(Silberer and Frank, 2012; Laparra and Rigau,2013b) reported F-measures below 20 for the sametask.
The work presented here does not targetmissing core arguments but modifiers within thesame sentence.
Furthermore, results show our pro-posal is useful in a real environment.Finally, our previous work (Blanco andMoldovan, 2011; Blanco and Moldovan, 2014)proposed composing new relations out of chainsof previously extracted relations.
This approachis unsupervised and accurate (88% with gold an-notations), but inferences are made only betweenthe ends of chains of existing relations.
Our cur-rent proposal also leverages relations previouslyextracted, but productivity is higher and resultswith automatic annotations are presented.
[But]MDIS [the surprisingly durable seven-year economicexpansion]ARG0has [made]v [mincemeat]ARG1[of morethan one forecast]ARG2.Also, financial planners advising on insurance say thatto their knowledge there has not yet been [a taxruling]ARG0[exempting]v [these advance payments]ARG1[from taxes]ARG2.Table 1: Examples of verb-argument structuresfrom PropBank.3 Verb-Argument Structures andImplicit RelationsThroughout this paper, R(x, y) denotes a seman-tic relation R holding between x and y. R(x,y) is interpreted ?x has R y?, e.g., AGENT(took,Bill ) could be read ?took has AGENT Bill?.
Verb-argument structures, or semantic roles, account forsemantic relations between a verb and its syntacticarguments.
In other words, R(x, y) is a semanticrole if ?x?
is a verb and ?y?
a syntactic argumentof ?x?, and all semantic roles with ?x?
as first ar-gument form the verb-argument structure of verb?x?.
Implicit relations are relations R(x, y) wherex is a verb and y is not a syntactic argument of x.The work presented in this paper aims at com-plementing verb-argument structures with implicitsemantic relations.
We follow a practical approachby inferring implicit relations from PropBank?sverb-argument structures.
We believe this is anadvantage since PropBank is well-known in thefield and several tools to predict PropBank annota-tions are documented and publicly available.1 Thework presented here could be incorporated in anyNLP pipeline after role labeling without modifica-tions to other components.
Furthermore, workingon top of PropBank allows us to quantify the im-pact of features derived from gold and automati-cally extracted linguistic information when infer-ring implicit relations (Section 6).3.1 Verb-Argument structures in PropBankPropBank (Palmer et al., 2005) annotates verb-argument structures on top of the syntactic treesof the Penn TreeBank (Marcus et al., 1994).
Ituses a set of numbered arguments2 (ARG0, ARG1,ARG2, etc.)
and modifiers (AM-TMP, AM-MNR,etc.).
Numbered arguments do not share a com-mon meaning across verbs, they are defined on a1E.g., Illinois SRL, http://cogcomp.cs.illinois.edu/page/software; SENNA, http://ml.nec-labs.com/senna/;SwiRL, http://www.surdeanu.info/mihai/swirl/2Numbered arguments are also referred to as core.146SNP1VP1NP2VP2VBD VP3The first hybridcorn seedsVBN S-ADV were VBD PPproducedTIMETHEMEMANNERusing this mechanicalapproachintroducedTHEMETIMEin the 1930sFigure 2: Verb-argument structures (solid arrows) and inferred implicit semantic relation (dashed arrow).AM-LOC: location AM-CAU: causeAM-EXT: extent AM-TMP: timeAM-DIS: discourse connective AM-PNC: purposeAM-ADV: general-purpose AM-MNR: mannerAM-NEG: negation marker AM-DIR: directionAM-MOD: modal verbTable 2: Argument modifiers in PropBank.Label # predicates % predicatesARG079,334 70.26%ARG1106,331 94.17%ARG224,560 21.75%AM-TMP 19,756 17.50%AM-MNR 7,833 6.94%AM-LOC 7,198 6.37%AM-PNC 2,784 2.47%AM-CAU 1,563 1.38%Table 3: Counts of selected PropBank semanticroles.
Total number of predicates is 112,917.verb by verb basis in each frameset.
For exam-ple, ARG2is used to indicate ?created-from, thingchanged?
with verb make and ?entity exemptedfrom?
with verb exempt (Table 1).Unlike numbered arguments, modifiers share acommon meaning across verbs (Table 2).
Somemodifiers are arguably not a semantic relationand are not present in most relation invento-ries (Tratz and Hovy, 2010; Hendrickx et al.,2009).
For example, AM-NEG and AM-MOD sig-nal the presence of negation and modals, e.g.,[wo]AM-MOD[n?t]AM-NEG [go]v. For more informa-tion about PropBank annotations and examples,refer to the annotation guidelines.3Inspecting PropBank annotations one can eas-ily conclude that numbered arguments dominatethe annotations and only a few modifiers are an-3http://verbs.colorado.edu/?mpalmer/projects/ace/PBguidelines.pdfnotated (Table 3).
ARG0and ARG1are presentin most verb-argument structures, other numberedarguments are often not defined in the correspond-ing frameset and are thus not annotated.Examining PropBank one can also concludethat information regarding TIME, LOCATION,MANNER, CAUSE and PURPOSE for a given verbis often present, yet not annotated because the textencoding this knowledge is not a direct syntacticargument of the verb (Section 4.3).
Because of thisfact, we decided to focus on these five relations.3.2 Implicit relations in PropBankTwo scenarios are possible when inferring an im-plicit relation R(x, y): (1) a semantic role R?
(x?, y)exists; or (2) such a semantic role does not exists.In (1), y is a syntactic argument of some verb x?,where x 6= x?
and in (2) that is not the case.
Infer-ences under scenario (1) can be further classifiedinto (1a) when a semantic role R??
(x, y?)
such thaty?
contains y exists; or (1b) when such a semanticroles does not exist.
The remainder of this sectionexemplifies the three scenarios.The example in Figure 1 falls under scenario(1a).
Semantic roles encode, among others, ?re-tired ?
has TIME ?last August?, and ?succeeds?
hasAGENT ?Mr.
Brown?
and THEME ?Joseph W. Hi-bben, who retired last August?.
The second argu-ment of implicit relation TIME-AFTER(succeeds,last August) is a semantic role of ?retired ?
and iscontained in the THEME of ?succeeds?.Figure 2 shows a statement in which implicit re-lation TIME(produced, in the 1930s) could be in-ferred under scenario (1b).
Semantic roles of ?pro-duced ?
only indicate that NP2is the THEME andS-ADV the MANNER; roles of ?introduced ?
indi-cate that NP1is the THEME and ?
[in the 1930s]PP?the TIME.
In this case, there is no connection be-147rs = {TIME, LOCATION, MANNER, CAUSE, PURPOSE};foreach semantic role R?
(x?, y) such that R?
?
rs doforeach verb x in the same sentence dogenerate potential implicit relation R(x, y);Algorithm 1.
Procedure to generate all potentialimplicit relations in scenario (1) (Section 3.2).tween ?produced ?
and ?
[in the 1930s]PP?
or anyother node subsuming this PP in the parse tree.Scenario (2) occurs whenever the second argu-ment of implicit relation R(x, y) is not a syntac-tic argument of a verb.
If it were, a semanticrole R?
(x?, y) would exist and it would fall un-der scenario (1).
For example, in [I]AGENT [gave]v[her]RECIPIENT [a book from 1945]THEME, we couldinfer the implicit semantic relation ?gave occurredafter 1945?.4 Annotating Implicit RelationsInferring all implicit semantic relations is a chal-lenging task.
This paper targets implicit relationsthat can be inferred under scenarios (1a, 1b); sce-nario (2) is reserved for future work.
All poten-tial implicit relations under scenario (1) are gen-erated using Algorithm 1.
A manual annotationeffort discards potential implicit relations that donot hold in order to create a gold standard.4.1 Annotation GuidelinesAnnotators are faced with the task of decidingwhether a potential implicit relation R(x, y) holds.If it does, they mark it with YES, otherwise withNO.
Annotators were initially trained with theoriginal PropBank annotation guidelines4 as thistask is very similar to annotating PropBank se-mantic roles.
Indeed, the only difference is that?y?
is not a syntactic argument of ?x?.After some preliminary annotations, we foundit useful to account for three subtypes of TIME.This way, richer semantic connections are in-ferred.
When the task is to decide whether im-plicit relation TIME(x, y) holds, annotators havefour labels to choose from: (1) TIME-BEFORE: xoccurred before y; (2) TIME-AFTER: x occurredafter y; (3) TIME-SAME x occurred at/during y; and(4) NO: y does not describe temporal informationof x.
If more than one label is valid, annotatorschoose the one encoding the temporal context yof x starting the earliest.
Namely, TIME-BEFORE4http://verbs.colorado.edu/?mpalmer/projects/ace/PBguidelines.pdfhas the highest priority, followed by TIME-SAME,TIME-AFTER and finally NO.Annotation examples are detailed in Section4.2, the more complex annotations involving TIMEare illustrated below.
Consider the following state-ment and PropBank annotations:[The government?s decision]ARG2, v1[reflects]v1[their true desires before[the next election]ARG1, v2, [expected]v2[in late 1991]TIME, v2]ARG1, v1.When annotating potential implicit semantic re-lation R(reflects, in late 1991 ), annotators mayselect TIME-BEFORE, TIME-SAME and TIME-AFTER.
However, they select TIME-BEFORE be-cause it indicates the temporal context of ?reflects?that starts the earliest.4.2 Annotation ExamplesSeveral annotations examples are shown in Ta-ble 4.
Semantic roles for statement (1) in-clude TIME(remain, in 1990 ), MANNER(remain,at about 1,200 cars) and no other TIME or MAN-NER.
Implicit relations reveal two extra seman-tic connections: TIME-BEFORE(said, in 1990 ) andTIME-BEFORE(expects, in 1990 ), i.e., ?said ?
and?expects?
occurred before ?1990 ?.
The potentialimplicit relations MANNER(said, at about 1,200cars) and MANNER(expects, at about 1,200 cars)do not hold and are annotated N.Interpreting statement (2) one can see that ?thispast summer?
is not only indicating the TIME of?proposed ?
; events encoded by verbs ?make?
and?exempt?
occurred after ?this past summer?.
Inthis example, two implicit semantic relations areinferred from a single semantic role.Statement (3) shows that two potential implicitrelations R(x, y) and R(x?, y) sharing the sec-ond argument ?y?
may be assigned different la-bels.
Regarding time, semantic roles only in-clude TIME(report, in December).
Implicit rela-tions add TIME-BEFORE(proposed, in December)and TIME-SAME(allow, in December).Two implicit LOCATION relations are inferredin statement (4): ?discovered ?
and ?preserving?occurred ?in the test-tube experiments?.
The po-tential implicit relation LOCATION(said, in thetest-tube experiments) is discarded (annotated N).Statement (5) shows two potential implicit MAN-NER that can be inferred.
The ?program?
was?aired ?
and ?seen by 12 million viewers?
in the fol-lowing manner: ?With Mr. Vila as host?.148Statement TMP LOC MNR PRP CAUB A S N Y N Y N Y N Y N1: Rolls-Royce said it expects [its U.S. sales]ARG1to [remain]v [steady]ARG3[at about 1,200 cars]MANNER [in 1990]TIME .?
said, [in 1990]TIME X - - -?
expects, [in 1990]TIME X - - -?
said, [at about 1,200 cars]MANNER - X?
expects, [at about 1,200 cars]MANNER - X2: They make the argument in letters to the agency about [rule changes]ARG1[proposed]v [this past summer]TIME that, amongother things, exempt many middle-management executives from government supervision.?
make, [this past summer]TIME - X - -?
exempt, [this past summer]TIME - X - -3: The proposed changes also allow [executives]ARG0to [report]v [exercises of options]ARG1[in December]TIME .?
proposed, [in December]TIME X - - -?
allow, [in December]TIME - - X -4: Two Japanese scientists said they discovered [an antibody that]ARG0, [in laboratory test-tube experiments]LOCATION , [kills]v[AIDS-infected cells]ARG1[while preserving healthy cells]TIME .?
said, [in laboratory test-tube experiments]LOCATION - X?
discovered, [in laboratory test-tube experiments]LOCATION X -?
preserving, [in laboratory test-tube experiments]LOCATION X -5: [With Mr. Vila as host]MANNER , ?
[This Old House]ARG1?
[became]v [one of the Public Broadcasting Service?s top 10programs]ARG2, [airing weekly on about 300 of the network ?s stations and seen by an average of 12 million viewers]AM-ADV .?
airing, [With Mr. Vila as host]MANNER X -?
seen, [With Mr. Vila as host]MANNER X -[6: It]ARG0[raised]v [financing of 300 billion lire]ARG1[for the purchase this summer by another Agnelli-related group ofthe food concern Galbani S.p.A.]PURPOSE , [by selling a chunk of its IFI shares to Mediobanca S.p.A.]MANNER?
selling, [for the purchase this summer by another .
.
.
]PURPOSE X -7: [Greece and Turkey]ARG0, for example, are suspected of [overstating]v [their arsenals]ARG1[in hopes that they can emergefrom the arms-reduction treaty with large remaining forces to deter each other]PURPOSE .?
suspected, [in hopes that they can emerge from the .
.
.
]PURPOSE - X8: .
.
.
the rationalization that [given the country?s lack of natural resources]CAUSE , [they]ARG0[must]AM-MOD [work]v[hard]MANNER [to create value through exports]ARG1and buy food with the surplus.?
create, [given the country?s lack of natural resources]CAUSE X -?
buy, [given the country?s lack of natural resources]CAUSE X -9: Its third-quarter earnings were lower than analysts had forecast, and the company said [it]ARG0had [lowered]v [itsprojections for earnings growth through the end of 1990]ARG1[because of planned price cuts]CAUSE .?
forecast, [because of planned price cuts]CAUSE - X?
said, [because of planned price cuts]CAUSE - XTable 4: Examples of potential implicit relations and their annotations.
All of them but the ones annotatedwith N can be inferred.
B stands for BEFORE, A for AFTER, S for SAME, N for NO and Y for YES.PropBank semantic roles from which implicit relations are generated are indicated between brackets.Statement (6, 7) exemplify potential implicitPURPOSE relations.
While the ?selling?
event instatement (6) has as its purpose ?the purchase[.
.
. ]
?
(label Y), the ?suspected ?
event in statement(7) is clearly not done so that ?they (Greece andTurkey) can emerge from the [.
.
. ]
?
(label N).Finally, statements (8, 9) exemplify potentialimplicit CAUSE relations.
In (8), both ?create?
and?buy?
are done due to the ?country?s lack of naturalresources?.
However, in (9), the analysts ?forecast-ing?
and the company ?saying?
do not have as theircause ?planned price cuts?.4.3 Annotation AnalysisTable 5 shows counts for all potential implicit re-lations annotated.
All labels except N indicate avalid implicit relation.
94.1% of potential implicitrelations generated from a TIME semantic role canbe inferred.
Other roles yield less inferences inrelative terms, but substantial additional mean-ing: LOCATION 39.4%, MANNER 16.7%, PUR-POSE 29.4%, and CAUSE 30.2%.Two annotators performed the annotations.
Asimple script generated all potential implicit rela-tions and prompted for a label: BEFORE, AFTER,SAME or NO if the potential implicit relation wasgenerated from a TIME semantic role; YES or NOotherwise.
Annotators are not concerned with ar-gument identification, as arguments of implicit re-lations are retrieved from the verb-argument struc-tures in PropBank (Algorithm 1).
This makes theannotation process easier and faster.Annotation quality was calculated with twoagreement coefficients: observed agreement (rawpercentage of equal annotations) and Cohen?s ?
(Artstein and Poesio, 2008).
The actual num-149Source No.
Name Descriptionbasicx1,2 word, POS tag x?s surface form and part-of-speech tag3 voice whether x is in active or passive voicey4,5 first word, POS tag first word and part of speech tag in y6,7 last word, POS tag last word and part-of-speech tag in y8,9 head, POS tag head of y and its part-of-speech tag10?12 node, left and right sibling syntactic nodes of y, and its left and right siblings13 subcategory concatenation of y?s children nodesx, y14 direction whether x occurs before or after y15 subsumer common syntactic node between x and y16 path syntactic path between x and ypredstructures x ps 17?31 verb semantic roles flags indicating presence of semantic roles in x psy ps32,33 verb, POS tag verb in y ps and its part-of-speech tag34 arg label semantic role between verb in y ps and y35?49 arg semantic roles flags indicating presence of semantic roles in y psx ps, 50 overlapping semantic role role R??
linking x and y?, where y?
contains yy ps 51 overlapping head head of y?
in semantic role detected in feature 5052 overlapping direct whether feature 51 is the verb in y psTable 6: Complete feature set to determine whether a potential implicit semantic relation R(x, y) shouldbe inferred.
Second column indicates the source: first or second argument (x, y), or their respectivepredicate structures (x ps, y ps).
Features in bold are novel and specially designed for our task.Label # instances % instancesTIMEB 3,033 38.4%A 2,886 36.5%S 1,514 19.2%N 463 5.9%All 7,896 100.0%LOCATIONY 3,345 39.4%N 5,151 60.6%All 8,496 100.0%MANNERY 1,600 16.7%N 7,987 83.3%All 9,587 100.0%PURPOSE Y 821 29.4%N 1,971 70.6%All 2,792 100.0%CAUSEY 404 30.2%N 909 69.2%All 1,313 100.0%Table 5: Number of potential implicit relations (in-stances) annotated and counts for each label.
Totalnumber of instances is 30,084.bers are: 78.16% (observed) / 0.687 (?)
for TIME,86.63% / 0.733 for LOCATION, 93.02% / 0.782for MANNER, 88.60% / 0.734 for PURPOSE, and90.91% / 0.810 for CAUSE.
These agreementsare either comparable or superior to similar pre-vious annotation efforts.
Girju et al.
(2007) re-ported observed agreements between 47.8% and86.1% when annotating 7 semantic relations be-tween nominals, and Bethard et al.
(2008) ob-served agreements of 81.2% and 77.8% (Kappa:0.715 and 0.556) when annotating temporal andcausal relations between event pairs.5 Inferring Implicit RelationsInferring implicit relations is reduced to (1) gener-ating potential implicit relations (Algorithm 1) and(2) labeling them.
The second task determines ifpotential implicit relations should be discarded orinferred, all labels but N indicate potential implicitrelations that should be inferred.
We follow a stan-dard supervised machine learning approach whereeach potential implicit relation is an instance.Instances were divided into training (70%) andtest (30%).
The feature set (Section 5.1) andmodel parameters were tuned using 10-fold strat-ified cross-validation over the training split, andresults (Section 6) are reported using the test split.More features than the ones presented were triedand discarded because they did not improve per-formance, e.g., syntactic path between verbs in theverb-argument structures of x and y, depth of bothstructures, number of tokens in y.5.1 Feature SelectionThe full set of features to determine whether a po-tential implicit relation R(x, y) can be inferred issummarized in Table 6.
Features are classifiedinto basic and predicate structures.
The formerare commonly used by semantic role labelers.
Thelatter exploit the output of role labelers, i.e., verb-argument structures, and, to our knowledge, arenovel.
Results show predicate structures featuresimprove performance (Section 6.2).Basic features are derived from lexical and syn-tactic information.
We do not elaborate more on150Feat No.
Value1,2 succeeds, VBZ3 active4,5 last, JJ6,7 August, NNP8,9 August, NNP10?12 NP, VBD, nil13 JJ-NNP14 after15 VP16 VBZ+VP-NP-SBAR-S-VP-NP17?31 ARG0and ARG1true, rest false32,33 retired, VBD34 AM-TMP35-49 ARG0and AM-TMP true, rest false50 ARG151 Hibben52 falseTable 7: Feature values when deciding ifR(succeeds, last summer) can be inferred from theverb-argument structures in Figure 1.these features, detailed descriptions and examplesare provided by Gildea and Jurafsky (2002).Features (17?52) are derived from the predicatestructures of x and y and specially defined to inferimplicit semantic relations.
Features (17?31, 35?49) are flags indicating the presence of semanticroles in the predicate structures of x and y.Features (32?34) characterize the semantic roleR?
(x?, y) from which the potential implicit relationwas generated.
They specify verb x?, its part-of-speech, and label R?.
Note that x?
is not present inthe potential implicit relation R(x, y), but incorpo-rating this information helps determining whethera relation actually holds as well as label R (TIME-BEFORE, TIME-AFTER, TIME-SAME, etc.
).Finally, features 50?52 apply to inferences un-der scenario (1a) (Section 3.2).
Feature (50) indi-cates the semantic role R??
(x, y?
), if any, such thaty?
contains y.
Feature (51) indicates the head of ar-gument y?
found in feature (50).
Feature (52) cap-tures whether the head calculated in feature (51) isthe verb in the predicate structure of y.Table 7 exemplifies all features when decidingwhether TIME-AFTER(succeeds, last August) canbe inferred from the verb-argument structures inMr.
Brown succeeds Joseph W. Hibben, who re-tired last August (Figure 1).
Table 8 provides anadditional example for features 50?52.6 Experiments and ResultsExperiments were carried out using Support Vec-tor Machines with RBF kernel as implemented inMr.
Corr resigned to pursue other interests, the airline said.ARG0(resigned, Mr. Corr)AM-PNC(resigned, to pursue other interests)ARG0(pursue, Mr. Corr)ARG1(pursue, other interests)ARG0(said, the airline)ARG1(said, Mr. Corr resigned to pursue other interests)feature 50, overlapping sem rel ARG1feature 51, overlapping head resignedfeature 52, overlapping direct trueTable 8: PropBank roles and values for features(50?52) when predicting potential implicit relationR(said, to pursue other interests), labeled N.LIBSVM (Chang and Lin, 2011).
Parameters ?and ?
were tuned by grid search using 10-foldcross validation over training instances.Results are reported using features extractedfrom gold and automatic annotations.
Gold anno-tations are taken directly from the Penn TreeBankand PropBank.
Automatic annotations are ob-tained with Polaris (Moldovan and Blanco, 2012),a semantic parser that among others is trained withPropBank.
Results using gold (automatic) annota-tions are obtained with a model trained with gold(automatic) annotations.6.1 Detailed ResultsTable 9 presents per-relation and overall results.
Ingeneral terms, there is a decrease in performancewhen using automatic annotations.
The differenceis most noticeable in recall and it is due to missingsemantic roles, which in turn are often due to syn-tactic parsing errors.
This is not surprising as inorder for an implicit relation R(x, y) to be gener-ated as potential and fed to the learning algorithmfor classification, a semantic role R?
(x?, y) must beextracted first (Algorithm 1).
However, using au-tomatic annotations brings very little decrease inprecision.
This leads to the conclusion that as longas ?y?
is identified as a semantic role of some verb,even if it is mislabeled, one can still infer the rightimplicit relations.
Since results obtained with au-tomatic parse trees and semantic roles are a realis-tic estimation of performance, the remainder of thediscussion focuses on those.
Results with gold an-notations are provided for informational purposes.Overall results for inferring implicit semanticrelations are encouraging: precision 0.66, recall0.58 and F-measure 0.616.
Direct comparisonwith previous work is not possible because theimplicit relations we aim at inferring have notbeen considered before.
However, we note the top151gold automaticbasic basic + ps basic basic + psP R F P R F P R F P R FTIMEB .66 .72 .689 .72 .74 ?.730 .64 .65 .643 .68 .67 .677A .63 .74 .681 .67 .75 .708 .61 .68 .642 .66 .72 .687S .57 .41 .477 .54 .45 .491 .55 .36 .437 .55 .38 .450LOCATION Y .71 .61 .656 .70 .64 .669 .71 .56 .624 .71 .58 .635MANNER Y .65 .38 .480 .60 .45 .514 .54 .45 .489 .64 .41 .500PURPOSE Y .65 .58 .613 .69 .60 .642 .56 .49 .525 .68 .49 .572CAUSE Y .71 .60 .650 .74 .62 .675 .69 .65 .670 .71 .63 .669All .66 .61 .625 .67 .64 ?.651 .63 .57 .591 .66 .58 ?.616Table 9: Results obtained with the test split using features extracted from gold and automatic annotations,and using basic and predicate structures (ps) features.
Statistical significance between F-measures usingbasic and basic + predicate structures features is indicated with ?
(confidence 95%).performer (Koomen et al., 2005) at CoNLL-2005Shared Task on role labeling obtained the follow-ing F-measures when extracting the same relationsbetween a verb and its syntactic arguments: 0.774(TIME), 0.6033 (LOCATION), 0.5922 (MANNER),0.4541 (PURPOSE) and 0.5397 (CAUSE).The most difficult relations are TIME-SAME andMANNER, F-measures are 0.450 and 0.500 re-spectively.
Even when using gold annotationsthese two relations are challenging: F-measuresare 0.491 for TIME-SAME, an increase of 9.1%,and 0.514 for MANNER, an increase of 2.8%.
Re-sults show that other relations can be inferred withF-measures between 0.635 and 0.687, the only ex-ception is PURPOSE with an F-measure of 0.572.6.2 Feature AblationResults in Table 9 suggest that while implicit rela-tions can be inferred using basic features, it is ben-eficial to complement them with the novel featuresderived from predicate structures.
This is true forall relations except CAUSE when using automaticannotations with a negligible difference of 0.001.When considering all implicit relations, the differ-ence in performance is 0.616 ?
0.591 = 0.025,an increase of 4.2% that is statistically significant(Z-test, confidence 95%).The positive impact of features derived frompredicate structures is most noticeable when infer-ring PURPOSE, with an increase of 8.9% (0.572 ?0.525 = 0.047).
TIME-BEFORE and TIME-AFTERalso benefit, with increases of 5.3% (0.677 ?0.643 = 0.034) and 7.0% (0.687?0.642 = 0.045)respectively.
The improvement predicate struc-tures features bring is statistically significant whentaking into account all relations (confidence 95%).However, due to the lower number of instances,differences in performance when considering in-dividual relations is not statistically significant.7 ConclusionsVerb-argument structures, or semantic roles, com-prise semantic relations between a verb and itssyntactic arguments.
The work presented in thispaper leverages verb-argument structures to inferimplicit semantic relations.
A relation R(x, y) isimplicit if x is a verb and y is not a syntactic ar-gument of x.
The method could be incorporatedinto any NLP pipeline after role labeling withoutmodifications to other components.An analysis of verb-argument structures and im-plicit relations in PropBank has been presented.Out of all potential implicit relations R(x, y), thispaper targets those that can be generated from asemantic role R?
(x?, y), where x 6= x?.
A man-ual annotation effort demonstrates implicit rela-tions yield substantial additional meaning.
Mostof the time (94.1%) a semantic role TIME(x?
, y)is present, we can infer temporal information forother verbs within the same sentence.
Productiv-ity is lower but substantial with other roles: 39.4%(LOCATION), 30.2% (CAUSE), 29.4% (PURPOSE)and 16.7% (MANNER).Experimental results show that implicit rela-tions can be inferred using automatically obtainedparse trees and verb-argument structures.
Stan-dard machine learning is used to decide whether apotential implicit relation should be inferred, andnovel features characterizing the verb-argumentstructures we infer from have been proposed.152ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596, December.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associa-tion for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics -Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,USA.
Association for Computational Linguistics.Steven Bethard, William Corvey, Sara Klingenstein,and James H. Martin.
2008.
Building a Corpus ofTemporal-Causal Structure.
In Proceedings of theSixth International Language Resources and Evalu-ation (LREC?08), pages 908?915, Marrakech, Mo-rocco.
European Language Resources Association(ELRA).Eduardo Blanco and Dan Moldovan.
2011.
Un-supervised learning of semantic relation composi-tion.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies, pages 1456?1465,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.Eduardo Blanco and Dan Moldovan.
2014.
Compo-sition of semantic relations: Theoretical frameworkand case study.
ACM Trans.
Speech Lang.
Process.,10(4):17:1?17:36, January.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Intro-duction to the CoNLL-2005 shared task: semanticrole labeling.
In CONLL ?05: Proceedings of theNinth Conference on Computational Natural Lan-guage Learning, pages 152?164, Morristown, NJ,USA.
Association for Computational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Wanxiang Che, Ting Liu, and Yongqiang Li.
2010.
Im-proving Semantic Role Labeling with Word Sense.In The 2010 Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 246?249, Los Angeles, Califor-nia, June.
Association for Computational Linguis-tics.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A. Smith.
2010.
Semafor: Frame argumentresolution with log-linear models.
In Proceedings ofthe 5th International Workshop on Semantic Evalu-ation, pages 264?267, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Matthew Gerber and Joyce Chai.
2010.
Beyond Nom-Bank: A Study of Implicit Arguments for Nomi-nal Predicates.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 1583?1592, Uppsala, Sweden, July.Association for Computational Linguistics.Matthew Gerber and Joyce Chai.
2012.
Seman-tic role labeling of implicit arguments for nominalpredicates.
Computational Linguistics, 38:755?798,2012.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling Of Semantic Roles.
Computational Lin-guistics, 28:245?288.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of SemanticRelations between Nominals.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 13?18, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Iris Hendrickx, Su N. Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid, Sebastian Pado?, Marco Pennac-chiotti, Lorenza Romano, and Stan Szpakowicz.2009.
SemEval-2010 Task 8: Multi-Way Classifica-tion of Semantic Relations Between Pairs of Nom-inals.
In Proceedings of the Workshop on Seman-tic Evaluations: Recent Achievements and FutureDirections (SEW-2009), pages 94?99, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling of prop-bank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?08, pages 69?78, Stroudsburg, PA, USA.Association for Computational Linguistics.Peter Koomen, Vasin Punyakanok, Dan Roth, andWen T. Yih.
2005.
Generalized Inference withMultiple Semantic Role Labeling Systems.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005), pages181?184, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Egoitz Laparra and German Rigau.
2013a.
Impar: Adeterministic algorithm for implicit semantic role la-belling.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 1180?1189,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Egoitz Laparra and German Rigau.
2013b.
Sources ofevidence for implicit argument resolution.
In Pro-ceedings of the 10th International Conference onComputational Semantics (IWCS 2013) ?
Long Pa-pers, pages 155?166, Potsdam, Germany, March.Association for Computational Linguistics.Mitchel Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a large annotated153corpus of English: The Penn Treebank.
Computa-tional linguistics, 19(2):313?330.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.Annotating Noun Argument Structure for Nom-Bank.
In Proceedings of LREC-2004, pages 803?806, Lisbon, Portugal.Dan Moldovan and Eduardo Blanco.
2012.
Polaris:Lymba?s semantic parser.
In Proceedings of theEighth International Conference on Language Re-sources and Evaluation (LREC-2012), pages 66?72, Istanbul, Turkey, May.
European Language Re-sources Association (ELRA).Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-man, Lynette Hirschman, Marcia Linebarger, andJohn Dowding.
1986.
Recovering implicit infor-mation.
In Proceedings of the 24th Annual Meet-ing of the Association for Computational Linguis-tics, pages 10?19, New York, New York, USA, July.Association for Computational Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Vasin Punyakanok, Dan Roth, and Wen T. Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287, June.James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, et al.
2003.
The timebank corpus.
Corpuslinguistics, 2003:40.Josef Ruppenhofer, Caroline Sporleder, RoserMorante, Collin Baker, and Martha Palmer.
2009.SemEval-2010 Task 10: Linking Events and TheirParticipants in Discourse.
In Proceedings ofthe Workshop on Semantic Evaluations: RecentAchievements and Future Directions (SEW-2009),pages 106?111, Boulder, Colorado, June.
Associa-tion for Computational Linguistics.Josef Ruppenhofer, Russell Lee-Goldman, CarolineSporleder, and Roser Morante.
2013.
Beyondsentence-level semantic role labeling: linking argu-ment structures in discourse.
Language Resourcesand Evaluation, 47(3):695?721.Carina Silberer and Anette Frank.
2012.
Casting im-plicit role linking as an anaphora resolution task.In *SEM 2012: The First Joint Conference onLexical and Computational Semantics, pages 1?10,Montre?al, Canada, 7-8 June.
Association for Com-putational Linguistics.Joel R Tetreault.
2002.
Implicit role reference.
In In-ternational Symposium on Reference Resolution forNatural Language Processing, pages 109?115.Sara Tonelli and Rodolfo Delmonte.
2010.
Venses++:Adapting a deep semantic processing system to theidentification of null instantiations.
In Proceedingsof the 5th International Workshop on Semantic Eval-uation, pages 296?299, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Stephen Tratz and Eduard Hovy.
2010.
A Taxonomy,Dataset, and Classifier for Automatic Noun Com-pound Interpretation.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 678?687, Uppsala, Sweden, July.Association for Computational Linguistics.Naushad UzZaman, Hector Llorens, Leon Derczyn-ski, James Allen, Marc Verhagen, and James Puste-jovsky.
2013.
Semeval-2013 task 1: Tempeval-3:Evaluating time expressions, events, and temporalrelations.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 2:Proceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013), pages 1?9,Atlanta, Georgia, USA, June.
Association for Com-putational Linguistics.Greg Whittemore, Melissa Macpherson, and GregCarlson.
1991.
Event-building through role-fillingand anaphora resolution.
In Proceedings of the 29thAnnual Meeting of the Association for Computa-tional Linguistics, pages 17?24, Berkeley, Califor-nia, USA, June.
Association for Computational Lin-guistics.Be N. Zapirain, Eneko Agirre, Llu?
?s Ma`rquez, and Mi-hai Surdeanu.
2010.
Improving Semantic RoleClassification with Selectional Preferences.
In The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 373?376, Los Angeles, California,June.
Association for Computational Linguistics.154
