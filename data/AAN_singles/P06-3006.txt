Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 31?36,Sydney, July 2006. c?2006 Association for Computational LinguisticsSemantic Discourse Segmentation and Labeling for Route InstructionsNobuyuki ShimizuDepartment of Computer ScienceState University of New York at AlbanyAlbany, NY 12222, USAnobuyuki@shimizu.nameAbstractIn order to build a simulated robot thataccepts instructions in unconstrained nat-ural language, a corpus of 427 route in-structions was collected from human sub-jects in the office navigation domain.
Theinstructions were segmented by the stepsin the actual route and labeled with theaction taken in each step.
This flatformulation reduced the problem to anIE/Segmentation task, to which we appliedConditional Random Fields.
We com-pared the performance of CRFs with a setof hand-written rules.
The result showedthat CRFs perform better with a 73.7%success rate.1 IntroductionTo have seamless interactions with computers, ad-vances in task-oriented deep semantic understand-ing are of utmost importance.
The examples in-clude tutoring, dialogue systems and the one de-scribed in this paper, a natural language interfaceto mobile robots.
Compared to more typical textprocessing tasks on newspapers for which we at-tempt shallow understandings and broad coverage,for these domains vocabulary is limited and verystrong domain knowledge is available.
Despitethis, deeper understanding of unrestricted naturallanguage instructions poses a real challenge, dueto the incredibly rich structures and creative ex-pressions that people use.
For example,?Just head straight through the hallwayignoring the rooms to the left and rightof you, but while going straight your go-ing to eventually see a room facing you,which is north, enter it.?
?Head straight.
continue straight pastthe first three doors until you hit a cor-ner.
On that corner there are two doors,one straight ahead of you and one on theright.
Turn right and enter the room tothe right and stop within.
?These utterances are taken from an office navi-gation corpus collected from undergrad volunteersat SUNY/Albany.
There is a good deal of variety.Previous efforts in this domain include the clas-sic SHRDLU program by Winograd (1972), us-ing a simulated robot, and the more ambitious IBL(Instruction-based Learning for Mobile Robots)project (Lauria et al 2001) which tried to inte-grate vision, voice recognition, natural languageunderstanding and robotics.
This group has yet topublish performance statistics.
In this paper wewill focus on the application of machine learningto the understanding of written route instructions,and on testing by following the instructions in asimulated office environment.2 Task2.1 Input and OutputThree inputs are required for the task:?
Directions for reaching an office, written inunrestricted English.?
A description of the building we are travelingthrough.?
The agent?s initial position and orientation.The output is the location of the office the direc-tions aim to reach.312.2 Corpus CollectionIn an experiment to collect the corpus, (Haas,1995) created a simulated office building modeledafter the actual computer science department atSUNY/Albany.
This environment was set up likea popular first person shooter game such as Doom,and the subject saw a demonstration of the routehe/she was asked to describe.
The subject wrotedirections and sent them to the experimenter, whosat at another computer in the next room.
Theexperimenter tried to follow the directions; if hereaches the right destination, the subject got $1.This process took place 10 times for each subject;instructions that the experimenter could not fol-low correctly were not added to the corpus.
In thismanner, they were able to elicit 427 route instruc-tions from the subject pool of 44 undergraduatestudents.2.3 Abstract MapTo simplify the learning task, the map of ourcomputer science department was abstracted to agraph.
Imagine a track running down the halls ofthe virtual building, with branches into the officedoors.
The nodes of the graph are the intersec-tions, the edges are the pieces of track betweenthem.
We assume this map can either be preparedahead of time, or dynamically created as a result ofsolving Simultaneous Localization and Mapping(SLAM) problem in robotics (Montemerlo et al2003).2.4 System ComponentsSince it is difficult to jump ahead and learn thewhole input-output association as described in thetask section, we will break down the system intotwo components.Front End:RouteInstruction?
ActionListBack End:ActionList?Map?
Start?
GoalThe front-end is an information extraction sys-tem, where the system extracts how one shouldmove from a route instruction.
The back-end is areasoning system which takes a sequence of movesand finds the destination in the map.
We will firstdescribe the front-end, and then show how to inte-grate the back-end to it.One possibility is to keep the semantic repre-sentation close to the surface structure, includingunder-specification and ambiguity, and leaving theback-end to resolve the ambiguity.
We will pursuea different route.
The disambiguation will be donein the front-end; the representation that it passesto the back-end will be unambiguous, describingat most one path through the building.
The taskof the back-end is simply to check the sequenceof moves the front-end produced against the mapand see if there is a path leading to a point in themap or not.
The reason for this is two fold.
One isto have a minimal annotation scheme for the cor-pus, and the other is to enable the learning of thewhole task including the disambiguation as an IEproblem.3 Semantic AnalysisNote that in this paper, given an instruction, onestep in the instruction corresponds to one actionshown to the subject, one episode of action detec-tion and tracking, and one segment of the text.In order to annotate unambiguously, we need todetect and track both landmarks and actions.
Alandmark is a hallway or a door, and an actionis a sequence of a few moves one will make withrespect to a specific landmark.The moves one can make in this map are:(M1).
Advancing to x,(M2).
Turning left/right to face x, and(M3).
Entering x.Here, x is a landmark.
Note that all three moveshave to do with the same landmark, and two orthree moves on the same landmark constitute oneaction.
An action is ambiguous until x is filledwith an unambiguous landmark.
The following isa made-up example in which each move in an ac-tion is mentioned explicitly.a.
?Go down the hallway to the seconddoor on the right.
Turn right.
Enter thedoor.
?But you could break it down even further.b.
?Go down the hallway.
You will seetwo doors on the right.
Turn right andenter the second.
?One can add any amount of extra information to aninstruction and make it longer, which people seemto do.
However, we see the following as well.c.
?Enter the second door on the right.
?In one sentence, this sample contains the advance,the turn and the entering.
In the corpus, the norm32is to assume the move (M1) when an expressionindicating the move (M2) is present.
Similarly, anexpression of move (M3) often implicitly assumesthe move (M1) and (M2).
However, in some casesthey are explicitly stated, and when this happens,the action that involves the same landmark mustbe tracked across the sentences.Since all three samples result in the same action,for the back-end it is best not to differentiate thethree.
In order to do this, actions must be trackedjust like landmarks in the corpus.The following two samples illustrate the need totrack actions.d.
?Go down the hallway until you seetwo doors.
Turn right and enter the sec-ond door on the right.
?In this case, there is only one action in the instruc-tion, and ?turn right?
belongs to the action ?ad-vance to the second door on the right, and thenturn right to face it, and then enter it.?e.
?Proceed to the first hallway on theright.
Turn right and enter the seconddoor on the right.
?There are two actions in this instruction.
The firstis ?advance to the first hallway on the right, andthen turn right to face the hallway.?
The phrase?turn right?
belongs to this first action.
The secondaction is the same as the one in the example (d).Unless we can differentiate between the two, theexecution of the unnecessary turn results in failurewhen following the instructions in the case (d).This illustrates the need to track actions acrossa few sentences.
In the last example, it is impor-tant to realize that ?turn right?
has something to dowith a door, so that it means ?turn right to face adoor?.
Furthermore, since ?enter the second dooron the right?
contains ?turning right to face a door?in its semantics as well, they can be thought of asthe same action.
Thus, the critical feature requiredin the annotation scheme is to track actions andlandmarks.The simplest annotation scheme that can showhow actions are tracked across the sentences isto segment the instruction into different episodesof action detection and tracking.
Note that eachepisode corresponds to exactly one action shownto the subject during the experiment.
The annota-tion is based on the semantics, not on the the men-tions of moves or landmarks.
Since each segmentToken Node Part Transition Partmake ?B-GHL1, 0?
?B-GHL1, I-GHL1, 0, 1?left ?I-GHL1, 1?
?I-GHL1, I-GHL1, 1, 2?, ?I-GHL1, 2?
?I-GHL1, B-EDR1, 2, 3?first ?B-EDR1, 3?
?B-EDR1, I-EDR1, 3, 4?door ?I-EDR1, 4?
?I-EDR1, I-EDR1, 4, 5?on ?I-EDR1, 5?
?I-EDR1, I-EDR1, 5, 6?the ?I-EDR1, 6?
?I-EDR1, I-EDR1, 6, 7?right ?I-EDR1, 7?Table 1: Example Parts: linear-chain CRFsinvolves exactly one landmark, we can label thesegment with an action and a specific landmark.For example,GHR1 := ?advance to the first hallway on theright, then turn right to face it.
?EDR2 := ?advance to the second door on theright, then turn right to face it, then enter it.
?GHLZ := ?advance to the hallway on the left atthe end of the hallway, then turn left to face it.
?EDSZ := ?advance to the door straight ahead ofyou, then enter it.
?Note that GH=go-hall, ED=enter-door,R1=first-right, LZ=left-at-end, SZ=ahead-of-you.The total number of possible actions is 15.This way, we can reduce the front-end task intoa sequence of tagging tasks, much like the nounphrase chunking in the CoNLL-2000 shared task(Tjong Kim Sang and Buchholz, 2000).
Givena sequence of input tokens that forms a route in-struction, a sequence of output labels, with eachlabel matching an input token was prepared.
Weannotated with the BIO tagging scheme used insyntactic chunkers (Ramshaw and Marcus, 1995).make B-GHL1left I-GHL1, I-GHL1first B-EDR1door I-EDR1on I-EDR1the I-EDR1right I-EDR14 Systems4.1 System 1: CRFs4.1.1 Model: A Linear-Chain UndirectedGraphical ModelFrom the output labels, we create the parts in alinear-chain undirected graph (Table 1).
Our useof term part is based on (Bartlett et al 2004).For each pair (xi, yi) in the training set, xi isthe token (in the first column, Table 1), and yi33Transition Node?L0, L, j ?
1, j?
?L, j?no lexicalization no lexicalizationxj?4xj?3xj?2xj?1xjxj+1xj+2xj+3xj?1, xjxj+0, xj+1Table 2: Featuresis the part (in the second and third column, Ta-ble 1).
There are two kinds of parts: node andtransition.
A node part tells us the position andthe label, ?B-GHL1, 0?, ?I-GHL1, 1?, and so on.
Atransition part encodes a transition.
For example,between tokens 0 and 1 there is a transition fromtag B-GHL1 to I-GHL1.
The part that describesthis transition is: ?B-GHL1, I-GHL1, 0, 1?.We factor the score of this linear node-transitionstructure as the sum of the scores of all the parts iny, where the score of a part is again the sum of thefeature weights for that part.To score a pair (xi, yi) in the training set, wetake each part in yi and check the features associ-ated with it via lexicalization.
For example, a part?I-GHL1, 1?
could give rise to binary features suchas,?
Does (xi, yi) contain a label ?I-GHL1??
(NoLexicalization)?
Does (xi, yi) contain a token ?left?
labeledwith ?I-GHL1??
(Lexicalized by x1)?
Does (xi, yi) contain a token ?left?
labeledwith ?I-GHL1?
that?s preceded by ?make??
(Lexicalized by x0, x1)and so on.
The features used in this experiment arelisted in Table 2.If a feature is present, the feature weight isadded.
The sum of the weights of all the partsis the score of the pair (xi, yi).
To representthis summation, we write s(xi, yi) = w?f(xi, yi)where f represents the feature vector and w is theweight vector.
We could also have w?f(xi, {p})where p is a single part, in which case we just writes(p).Assuming an appropriate feature representationas well as a weight vector w, we would like to findthe highest scoring y = argmaxy?
(w?k f(y?, x))given an input sequence x.
We next present a ver-sion of this decoding algorithm that returns thebest y consistent with the map.4.1.2 Decoding: the Viterbi Algorithm andInferring the Path in the MapThe action labels are unambiguous; given thecurrent position, the map, and the action label,there is only one position one can go to.
This back-end computation can be integrated into the Viterbialgorithm.
The function ?go?
takes a pair of (ac-tion label, start position) and returns the end posi-tion or null if the action cannot be executed at thestart position according to the map.
The algorithmchooses the best among the label sequences with alegal path in the map, as required by the condition(cost > bestc ?
end 6= null).
Once the modelis trained, we can then use the modified version ofthe Viterbi algorithm (Algorithm 4.1) to find thedestination in the map.Algorithm 4.1: DECODE PATH(x, n, start, go)for each label y1node[0][y1].cost?
s(?y1, 0?)node[0][y1].end?
start;for j ?
1 to n?
1for each label yj+1bestc?
??;end?
null;for each label yjcost?
node[j][yj ].cost+s(?yj, yj+1, j, j + 1?
)+s(?yj+1, j + 1?);end?
node[j][yj ].end;if (yj 6= yj+1)end?
go(yj+1, end);if (cost > bestc ?
end 6= null)bestc?
cost;if (bestc 6= ??
)node[j + 1][yj+1].cost?
bestc;node[j + 1][yj+1].end?
end;bestc?
??;end?
null;for each label ynif (node[j][yn].cost > bestc)bestc?
node[j][yn].cost;end?
node[j][yn].end;return (bestc, end)344.1.3 Learning: Conditional Random FieldsGiven the above problem formulation, wetrained the linear-chain undirected graphicalmodel as Conditional Random Fields (Lafferty etal, 2001; Sha and Pereira, 2003), one of the bestperforming chunkers.
We assume the probabilityof seeing y given x isP (y|x) = exp(s(x, y))?y?
exp(s(x, y?
))where y?
is all possible labeling for x , Now, givena training set T = {(xiyi)}mi=1, We can learnthe weights by maximizing the log-likelihood,?i logP (yi|xi).
A detailed description of CRFscan be found in (Lafferty et al 2001; Sha andPereira, 2003; Malouf, 2002; Peng and McCallum,2004).
We used an implementation called CRF++which can be found in (Kudo, 2005)4.2 System 2: BaselineSuppose we have clean data and there is no need totrack an action across sentences or phrases.
Then,the properties of an action are mentioned exactlyonce for each episode.For example, in ?go straight and make the firstleft you can, then go into the first door on the rightside and stop?
, LEFT and FIRST occur exactlyonce for the first action, and FIRST, DOOR andRIGHT are found exactly once in the next action.In a case like that, the following baseline algo-rithm should work well.?
Find all the mentions of LEFT/RIGHT,?
For each occurrence of LEFT/RIGHT, lookfor an ordinal number, LAST, or END (= endof the hallway) nearby,?
Also, for each LEFT/RIGHT, look for a men-tion of DOOR.
If DOOR is mentioned, theaction is about entering a door.?
If DOOR is not mentioned aroundLEFT/RIGHT, then the action is aboutgoing to a hallway by default,?
If DOOR is mentioned at the end of an in-struction without LEFT/RIGHT, then the ac-tion is to go straight into the room.?
Put the sequence of action labels together ac-cording to the mentions collected.count average lengthGHL1 128 8.5GHL2 4 7.7GHLZ 36 14.4GHR1 175 10.8GHR2 5 15.8GHRZ 42 13.6EDL1 98 10.5EDL2 81 12.3EDL3 24 13.9EDLZ 28 13.7EDR1 69 10.4EDR2 55 12.9EDR3 6 13.0EDRZ 11 16.4EDSZ 55 16.2Table 3: Steps found in the datasetIn this case, all that?s required is a dictionary ofhow a word maps to a concept such as DOOR.
Inthis corpus, ?door?, ?office?, ?room?, ?doorway?and their plural forms map to DOOR, and the or-dinal number 1 will be represented by ?first?
and?1st?, and so on.5 DatasetAs noted, we have 427 route instructions, and theaverage number of steps was 1.86 steps per in-struction.
We had 189 cases in which a sentenceboundary was found in the middle of a step.
Ta-ble 3 shows how often action steps occurred in thecorpus and average length of the segments.One thing we noticed is that somehow people donot use a short phrase to say the equivalent of ?en-ter the door straight ahead of you?, as seen by theaverage length of EDSZ.
Also, it is more commonto say the equivalent of ?take a right at the end ofthe hallway?
than that of ?go to the second hallwayon the right?, as seen by the count of GHR2 andGHRZ.
The distribution is highly skewed; thereare a lot more GHL1 than GHL2.6 ResultsWe evaluated the performance of the systems us-ing three measures: overlap match, exact match,and instruction follow through, using 6-fold cross-valiadation on 427 samples.
Only the actionchunks were considered for exact match and over-lap match.
Overlap match is a lenient measurethat considers a segmentation or labeling to be cor-35Exact Match Recall Precision F-1CRFs 66.0% 67.0% 66.5%Overlap Match Recall Precision F-1Baseline 62.8% 49.9% 55.6%CRFs 85.7% 87.0% 86.3%Instruction Follow Through success rateBaseline 39.5%CRFs 73.7%Table 4: Recall, Precision, F-1 and Success Raterect if it overlaps with any of the annotated labels.Instruction follow through is the success rate forreaching the destination, and the most importantmeasure of the performance in this domain.
Sincethe baseline algorithm does not identify the tokenlabeled with B-prefix, no exact match comparisonis made.
The result (Table 4) shows that CRFs per-form better with a 73.7% success rate.7 Future WorkMore complex models capable of representinglandmarks and actions separately may be applica-ble to this domain, and it remains to be seen if suchmodels will perform better.
Also, some form ofco-reference resolution or more sophisticated ac-tion tracking should also be considered.AcknowledgementWe thank Dr. Andrew Haas for introducing us tothe problem, collecting the corpus and being verysupportive in general.ReferencesP.
Bartlett, M. Collins, B. Taskar and D. McAllester.2004.
Exponentiated gradient algorithms for large-margin structured classification.
In Advances inNeural Information Processing Systems (NIPS)A. Haas 1995.
Testing a Simulated Robot that FollowsDirections.
unpublishedT.
Kudo 2005.
CRF++: Yet An-other CRF toolkit.
Available athttp://chasen.org/?taku/software/CRF++/J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceed-ings of International Conference on Machine Learn-ing .R.
Malouf.
2002.
A Comparison of Algorithms forMaximum Entropy Parameter Estimation.
In Pro-ceedings of Conference of Computational NaturalLanguage LearningF.
Peng and A. McCallum.
2004.
Accurate Informa-tion Extraction from Research Papers using Condi-tional Random Fields.
In Proceedings of HumanLanguage Technology Conference .F.
Sha and F. Pereira.
2003.
Shallow parsing with con-ditional random fields.
In Proceedings of HumanLanguage Technology Conference .S.
Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E.Klein.
2001.
Personal Robot Training via Natural-Language Instructions.
IEEE Intelligent Systems,16:3, pp.
38-45.C.
Manning and H. Schutze.
1999.
Foundations of Sta-tistical Natural Language Processing.
MIT Press.M.
Montemerlo, S. Thrun, D. Koller, and B. Wegbreit.2003.
FastSLAM 2.0: An improved particle fil-tering algorithm for simultaneous localization andmapping that provably converges.
In Proceedings ofthe International Joint Conference on Artificial In-telligence (IJCAI).L.
Ramshaw and M. Marcus.
1995.
Text chunking us-ing transformation-based learning.
In Proceedingsof Third Workshop on Very Large Corpora.
ACLE.
F. Tjong Kim Sang and S. Buchholz.
2000.
In-troduction to the CoNLL-2000 shared task: Chunk-ing.
In Proceedings of Conference of ComputationalNatural Language Learning .T.
Winograd.
1972.
Understanding Natural Language.Academic Press.36
