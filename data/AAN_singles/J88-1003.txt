GRAMMATICAL CATEGORY DISAMBIGUATION BYSTATISTICAL OPTIMIZATIONSteven J. DeRoseBrown University and the Summer Institute of Linguistics, 7500 W. Camp Wisdom Road,Dallas, TX 75236Several algorithms have been developed in the past that attempt to resolve categorial ambiguities innatural language text without recourse to syntactic or semantic level information.
An innovative method(called "CLAWS") was recently developed by those working with the Lancaster -Oslo/Bergen Corpusof British English.
This algorithm uses a systematic calculation based upon the probabilities ofco-occurrence of particular tags.
Its accuracy is high, but it is very slow, and it has been manuallyaugmented in a number of ways.
The effects upon accuracy of this manual augmentation are notindividually known.The current paper presents an algorithm for disambiguation that is similar to CLAWS but thatoperates in linear rather than in exponential time and space, and which minimizes the unsystematicaugments.
Tests of the algorithm using the million words of the Brown Standard Corpus of English arereported; the overall accuracy is 96%.
This algorithm can provide a fast and accurate front end to anyparsing or natural language processing system for English.Every computer system that accepts natural anguageinput must, if it is to derive adequate representations,decide upon the grammatical category of each inputword.
In English and many other languages, tokens arefrequently ambiguous.
They may represent lexical itemsof different categories, depending upon their syntacticand semantic ontext.Several algorithms have been developed that exam-ine a prose text and decide upon one of the severalpossible categories for a given word.
Our focus will beon algorithms which specifically address this task ofdisambiguation, and particularly on a new algorithmcalled VOLSUNGA, which avoids syntactic-level anal-ysis, yields about 96% accuracy, and runs in far lesstime and space than previous attempts.
The most recentprevious algorithm runs in NP (Non-Polynomial) time,while VOLSUNGA runs in linear time.
This is provablyoptimal; no improvements in the order of its executiontime and space are possible.
VOLSUNGA is also robustin cases of ungrammaticality.Improvements o this accuracy may be made, per-haps the most potentially significant being to includesome higher-level information.
With such additions, theaccuracy of statistically-based algorithms will approach100%; and the few remaining cases may be largely thosewith which humans also find difficulty.In subsequent sections we examine several disambig-uation algorithms.
Their techniques, accuracies, andefficiencies are analyzed.
After presenting the researchcarried out to date, a discussion of VOLSUNGA'sapplication to the Brown Corpus will follow.
The BrownCorpus, described in Kucera and Francis (1967), is acollection of 500 carefully distributed samples of Eng-lish text, totalling just over one million words.
It hasbeen used as a standard sample in many studies ofEnglish.
Generous advice, encouragement, and assis-tance from Henry Kucera and W. Nelson Francis in thisresearch is gratefully acknowledged.1 PREWOUS DISAMBIGUATION ALGORITHMSThe problem of lexical category ambiguity has been littleexamined in the literature of computational linguisticsand artificial intelligence, though it pervades English toan astonishing degree.
About 11.5% of types (vocabu-lary), and over 40% of tokens (running words) in Englishprose are categorically ambiguous (as measured via theBrown Corpus).
The vocabulary breaks down as shownin Table 1 (derived from Francis and Kucera (1982)).Copyright 1988 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material is granted providedthat he copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, orto republish, requires a fee and/or specific permission.0362-613X/88/010031-39503.00Computational Linguistics, Volume 14, Number 1, Winter 1988 31Steven J. DeRose Grammatical Category Disambiguation by Statistical OptimizationNumber of words by degree of ambiguity:Unambiguous (1 tag) 35340Ambiguous (2-7 tags) 41002 tags 37603 tags 2644 tags 615 tags 126 tags 27 tags 1 ("still")Table 1: Degrees of AmbiguityA search of the relevant literature has revealed onlythree previous efforts directed specifically to this prob-lem.
The first published effort is that of Klein andSimmons (1963), a simple system using suffix lists andlimited frame rules.
The second approach to lexicalcategory disambiguation is TAGGIT (Greene and Rubin(1971)), a system of several thousand context-framerules.
This algorithm was used to assign initial tags tothe Brown Corpus.
Third is the CLAWS system devel-oped to tag the Lancaster -Oslo/Bergen (or LOB) Cor-pus.
This is a corpus of British written English, parallelto the Brown Corpus.
Parsing systems always encoun-ter the problem of category ambiguity; but usually thefocus of such systems is at other levels, making theirresponses less relevant for our purposes here.1.1 KLEIN AND SIMMONSKlein and Simmons (1963) describe a method irectedprimarily towards the task of initial categorial taggingrather than disambiguation.
Itsprimary goal is avoiding"the labor of constructing a very large dictionary"(p. 335); a consideration of greater import then thannow.The Klein and Simmons algorithm uses a palette of30 categories, and claims an accuracy of 90% in tagging.The algorithm first seeks each word in dictionaries ofabout 400 function words, and of about 1500 wordswhich "are exceptions to the computational rules used"(p. 339).
The program then checks for suffixes andspecial characters as clues.Last of all, context frame tests are applied.
Thesework on scopes bounded by unambiguous words, as dolater algorithms.
However, Klein and Simmons imposean explicit limit of three ambiguous words in a row.
Foreach such span of ambiguous words, the pair of unam-biguous categories bounding it is mapped into a list.
Thelist includes all known sequences of tags occurringbetween the particular bounding tags; all such se-quences of the correct length become candidates.
Theprogram then matches the candidate sequences againstthe ambiguities remaining from earlier steps of thealgorithm.
When only one sequence is possible, disam-biguation is successful.The samples used for calibration and testing werelimited.
First, Klein and Simmons (1963) performed"hand analysis of a sample \[size unspecified\] of GoldenBook Encyclopedia text" (p. 342).
Later, "\[w\]hen itwas run on several pages from that encyclopedia, itcorrectly and unambiguously tagged slightly over 90%of the words" (p. 344).
Further tests were run on smallsamples from the Encyclopedia Americana and fromScientific American.Klein and Simmons (1963) assert that "\[o\]riginalfears that sequences of four or more unidentified partsof speech would occur with great frequency were notsubstantiated in fact" (p. 3).
This felicity, however, isan artifact.
First, the relatively small set of categoriesreduces ambiguity.
Second, a larger sample wouldreveal both (a) low-frequency ambiguities and (b) manylong spans, as discussed below.1.2 GREENE AND RUBIN (TAGGIT)Greene and Rubin (1971) developed TAGGIT for tag-ging the Brown Corpus.
The palette of 86 tags thatTAGGIT uses has, with some modifications, also beenused in both CLAWS and VOLSUNGA.
The rationaleunderlying the choice of tags is described on pages 3-21of Greene and Rubin (1971).
Francis and Kucera (1982)report that this algorithm correctly tagged approxi-mately 77% of the million words in the Brown Corpus(the tagging was then completed by human post-edi-tors).
Although this accuracy is substantially lower thanthat reported by Klein and Simmons, it should beremembered that Greene and Rubin were the first toattempt so large and varied a sample.TAGGIT divides the task of category assignment intoinitial (potentially ambiguous) tagging, and disambigua-tion.
Tagging is carried out as follows: first, the programconsults an exception dictionary of about 3,000 words.Among other items, this contains all known closed-classwords.
It then handles various special cases, such aswords with initial "$",  contractions, pecial symbols,and capitalized words.
The word's ending is thenchecked against a suffix list of about 450 strings.
Thelists were derived from lexicostatistics of the BrownCorpus.
If TAGGIT has not assigned some tag(s) afterthese several steps, "the word is tagged NN, VB, or JJ\[that is, as being three-ways ambiguous\], in order thatthe disambiguation routine may have something to workwith" (Greene and Rubin (1971), p. 25).After tagging, TAGGIT applies a set of 3300 contextframe rules.
Each rule, when its context is satisfied, hasthe effect of deleting one or more candidates from thelist of possible tags for one word.
If the number ofcandidates i  reduced to one, disambiguation is consid-ered successful subject to human post-editing.
Eachrule can include a scope of up to two unambiguouswords on each side of the ambiguous word to which therule is being applied.
This constraint was determined asfollows:In order to create the original inventory of ContextFrame Tests, a 900-sentence subset of the BrownUniversity Corpus was tagged.
.
,  and its ambiguitieswere resolved manually; then a program was run32 Computational Linguistics, Volume 14, Number 1, Winter 1988Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimizationwhich produced and sorted all possible ContextFrame Rules which would have been necessary toperform this disambiguation automatically.
The rulesgenerated were able to handle up to three consecu-tive ambiguous words preceded and followed by twonon-ambiguous words \[a constraint similar to Kleinand Simmons'\].
However, upon examination oftheserules, it was found that a sequence of two or threeambiguities rarely occurred more than once in agiven context.
Consequently, a decision was made toexamine only one ambiguity at a time with up to twounambiguously tagged words on either side.
The firstrules created were the results of informed intuition(Greene and Rubin (1972), p. 32).1.3 CLAWSMarshall (1983, p. 139) describes the LOB Corpustagging algorithm, later named CLAWS (Booth (1985)),as "similar to those employed in the TAGGITprogram".
The tag set used is very similar, but some-what larger, at about 130 tags.
The dictionary used isderived from the tagged Brown Corpus, rather thanfrom the untagged.
It contains 7000 rather than 3000entries, and 700 rather than 450 suffixes.
CLAWS treatsplural, possessive, and hyphenated words as specialcases for purposes of initial tagging.The LOB researchers began by using TAGGIT onparts of the LOB Corpus.
They noticed thatWhile less than 25% of TAGGIT's context framerules are concerned with only the immediately pre-ceding or succeeding word .
.
,  these rules were ap-plied in about 80% of all attempts to apply rules.
Thisrelative overuse of minimally specified contexts in-dicated that exploitation of the relationship betweensuccessive tags, coupled with a mechanism thatwould be applied throughout a sequence of ambigu-ous words, would produce a more accurate andeffective method of word disambiguation (Marshall(1983), p. 141).The main innovation of CLAWS is the use of a matrixof eolloeational probabilities, indicating the relative like-lihood of co-occurrence ofall ordered pairs of tags.
Thismatrix can be mechanically derived from any pre-taggedcorpus.
CLAWS used "\[a\] large proportion of theBrown Corpus", 200,000 words (Marshall (1983), pp.141, 150).The ambiguities contained within a span of ambigu-ous words define a precise number of complete sets ofmappings from words to individual tags.
Each suchassignment of tags is called a path.
Each path is com-posed of a number of tag collocations, and each suchcollocation has a probability which may be obtainedfrom the collocation matrix.
One may thus approximateeach path's probability by the product of the probabil-ities of all its collocations.
Each path corresponds to aunique assignment of tags to all words within a span.The paths constitute a span network, and the path ofmaximal probability may be taken to contain the "best"tags.Marshall (1983) states that CLAWS "calculates themost probable sequence of tags, and in the majority ofcases the correct tag for each individual word corre-sponds to the associated tag in the most probablesequence of tags" (p. 142).
But a more detailed exami-nation of the Pascal code for CLAWS revealed thatCLAWS has a more complex definition of "most prob-able sequence" than one might expect.
A probabilitycalled "SUMSUCCPROBS" is predicated of eachword.
SUMSUCCPROBS is calculated by loopingthrough all tags for the words immediately preceding,at, and following a word; for each tag triple, an incre-ment is added, defined by:DownGrade(GetSucc(Tag2, Tag3), TagMark) *Get3SeqFactor(Tagl, Tag2, Tag3)GetSucc returns the collocational probability of a tagpair; Get3SeqFactor returns either 1, or a special valuefrom the tag-triple list described below.
DownGrademodifies the value of GetSucc in accordance with RTPsas described below.The CLAWS documentation describes SUMSUCC-PROBS as "the total value of all relationships betweenthe tags associated with this word and the tags associ-ated with the next word .
.
.
\[found by\] simulating allaccesses to SUCCESSORS and ORDER2VALS whichwill be made .
.
.
.  "
The probability of each node of thespan network (or rather, tree) is then calculated in thefollowing way as a tree representing all paths throughwhich the span network is built:PROB = DownGrade(GetSucc(lasttag,currenttag), TagMark) *Get3SeqFactor(...))PROB = PROB/(predecessor'sSUMSUCCPROBS) * (predecessor's PROB)It appears that the goal is to make each tag's proba-bility be the summed probability of all paths passingthrough it.
At the final word of a span, pointers arefollowed back up the chosen path, and tags are chosenen route.We will see below that a simpler definition of optimalpath is possible; nevertheless, there are several advan-tages of this general approach over previous ones.First, spans of unlimited length can be handled(subject to machine resources).
Although earlier re-searchers (Klein and Simmons, Greene and Rubin) havesuggested that spans of length over 5 are rare enough tobe of little concern, this is not the case.
The number ofspans of a given length is a function of that length andthe corpus size; so long spans may be obtained merelyby examining more text.
The total numbers of spans inthe Brown Corpus, for each length from 3 to 19, are:397111, 143447, 60224, 26515, 11409, 5128, 2161, 903,382, 161, 58, 29, 14, 6, 1, 0, I. Graphing the logarithmsComputational Linguistics, Volume 14, Number 1, Winter 1988 33Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimizationof these quantities versus the span length for each,produces a near-perfect s raight line.Second, a precise mathematical definition is possiblefor the fundamental idea of CLAWS.
Whereas earlierefforts were based primarily on ad hoc or subjectivelydetermined sets of rules and descriptions, and employedsubstantial exception dictionaries, this algorithm re-quires no human intervention for set-up; it is a system-atic process.Third, the algorithm is quantitative and analog,rather than artificially discrete.
The various tests andframes employed by earlier algorithms enforced abso-lute constraints on particular tags or collocations oftags.
Here relative probabilities are weighed, and aseries of very likely assignments can make possible aparticular, a priori unlikely assignment with which theyare associated.In addition to collocational probabilities, CLAWSalso takes into account one other empirical quantity:Tags associated with words .
.
,  can be associatedwith a marker @ or %; @ indicates that the tag isinfrequently the correct tag for the associatedword(s) (less than 1 in 10 occasions), % indicates thatit is highly improbable.
.
.
(less than I in 100 oc-casions) .
.
.
.
The word disambiguation program cur-rently uses these markers top devalue transitionmatrix values when retrieving a value from the ma-trix, @ results in the value being halved, % in thevalue being divided by eight (Marshall (1983), p. 149).Thus, the independent probability of each possibletag for a given word influences the choice of an optimalpath.
Such probabilities will be referred to as RelativeTag Probabilities, or RTPs.Other features have been added to the basic algo-rithm.
For example, a good deal of suffix analysis isused in initial tagging.
Also, the program filters itsoutput, considering itself to have failed if the optimal tagassignment for a span is not "more than 90% probable".In such cases it reorders tags rather than actuallydisambiguating.
On long spans this criterion is effec-tively more stringent than on short spans.
A moresignificant addition to the algorithm is thata number of tag triples associated with a scalingfactor have been introduced which may either up-grade or downgrade values in the tree computed fromthe one-step matrix.
For example, the triple \[1\] 'be'\[2\] adverb \[3\] past-tense-verb has been assigned ascaling factor which downgrades a sequence contain-ing this triple compared with a competing sequenceof \[1\] 'be' \[2\] adverb \[3\]-past-participle/adjective, onthe basis that after a form of 'be', past participles andadjectives are more likely than a past tense verb(Marshall (1983), p. 146).A similar move was used near conjunctions, forwhich the words on either side, though separated, aremore closely correlated to each other than either is tothe conjunction itself (Marshall (1983), pp.
146-147).For example, averb/noun ambiguity conjoined to a verbshould probably be taken as a verb.
Leech, Garside,and Atwell (1983, p. 23) describe "IDIOMTAG", whichis applied after initial tag assignment and before disam-biguation.
It wasdeveloped as a means of dealing with idiosyncraticword sequences which would otherwise cause diffi-culty for the automatic tagging .
.
.
.
for example, inorder that is tagged as a single conjunction .
.
.
.
TheIdiom Tagging Program.. .
can look at any combi-nation of words and tags, with or without interveningwords.
It can delete tags, add tags, or change theprobability of tags.
Although this program mightseem to be an ad hoc device, it is worth bearing inmind that any fully automatic language analysis sys-tem has to come to terms with problems of lexicalidiosyncrasy.IDIOMTAG also accounts for the fact that the prob-ability of a verb being a past participle, and not simplypast, is greater when the following word is "by" ,  asopposed to other prepositions.
Certain cases of this sortmay be soluble by making the collocational matrixdistinguish classes of ambiguities---this question is be-ing pursued.
Approximately 1% of running text istagged by IDIOMTAG (letter, G. N. Leech to HenryKucera, June 7, 1985; letter, E. S. Atwell to HenryKucera, June 20, 1985).Marshall notes the possibility of consulting a com-plete three-dimensional matrix of collocational proba-bilities.
Such a matrix would map ordered triples of tagsinto the relative probability of occurrence of each suchtriple.
Marshall points out that such a table would be toolarge for its probable usefulness.
The author has pro-duced a table based upon more than 85% of the BrownCorpus; it occupies about 2 megabytes (uncompressed).Also, the mean number of examples per triple is verylow, thus decreasing accuracy.CLAWS has been applied to the entire LOB Corpuswith an accuracy of "between 96% and 97%" (Booth(1985), p. 29).
Without the idiom list, the algorithm was94% accurate on a sample of 15,000 words (Marshall(1983)).
Thus, the pre-processor tagging of 1% of alltokens resulted in a 3% change in accuracy; thoseparticular assignments must therefore have had a sub-stantial effect upon their context, resulting in changes oftwo other words for every one explicitly tagged.But CLAWS is time- and storage-inefficient i  theextreme, and in some cases a fallback algorithm isemployed to prevent running out of memory, as wasdiscovered by examining the Pascal program code.
Howoften the fallback is employed is not known, nor is itknown what effect its use has on overall accuracy.Since CLAWS calculates the probability of everypath, it operates in time and space proportional to theproduct of all the degrees of ambiguity of the words inthe span.
Thus, the time is exponential (and henceNon-Polynomial) in the span length.
For the longestspan in the Brown Corpus, of length 18, the number ofpaths examined would be 1,492,992.34 Computational Linguistics, Volume 14, Number 1, Winter 1988Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization2 THE LINEAR-TIME ALGORITHM (VOLSUNGA)The algorithm described here depends on a similarempirically-derived transitional probability matrix tothat of CLAWS, and has a similar definition of "optimalpath".
The tagset is larger than TAGGIT's, thoughsmaller than CLAWS', containing 97 tags.
The ultimateassignments of tags are much like those of CLAWS.However, it embodies several substantive changes.Those features that can be algorithmically defined havebeen used to the fullest extent.
Other add-ons have beenminimized.
The major differences are outlined below.First, the optimal path is defined to be the one whosecomponent collocations multiply out to the highestprobability.
The more complex definition applied byCLAWS, using the sum of all paths at each node of thenetwork, is not used.Second, VOLSUNGA overcomes the Non-Polyno-mial complexity of CLAWS.
Because of this change, itis never necessary to resort o a fallback algorithm, andthe program is far smaller.
Furthermore, testing thealgorithm on extensive texts is not prohibitively costly.Third, VOLSUNGA implements Relative Tag Prob-abilities (RTPs) in a more quantitative manner, basedupon counts from the Brown Corpus.
Where CLAWSscales probabilities by 1/2 for RTP < 0.1 (i.e., whereless than 10% of the tokens for an ambiguous word arein the category in question), and by 1/8 for p < 0.01,VOLSUNGA uses the RTP value itself as a factor in theequation which defines probability.Fourth, VOLSUNGA uses no tag triples and noidioms.
Because of this, manually constructing special-case lists is not necessary.
These methods are useful incertain cases, as the accuracy figures for CLAWSshow; but the goal here was to measure the accuracy ofa wholly algorithmic tagger on a standard corpus.Interestingly, if the introduction of idiom tagging wereto make as much difference for VOLSUNGA as forCLAWS, we would have an accuracy of 99%.
Thiswould be an interesting extension.
I believe that thereasons for VOLSUNGA's 96% accuracy without id-iom tagging are (a) the change in definition of "optimalpath", and (b) the increased precision of RTPs.
Thedifference in tag-set size may also be a factor; but mostof the difficult cases are major class differences, uchas noun versus verb, rather than the fine distinctionwhich the CLAWS tag-set adds, such as severalsubtypes of proper noun.
Ongoing research with VOL-SUNGA may shed more light on the interaction of thesefactors.Last, the current version of VOLSUNGA is designedfor use with a complete dictionary (as is the case whenworking with a known corpus).
Thus, unknown wordsare handled in a rudimentary fashion.
This problem hasbeen repeatedly solved via affix analysis, as mentionedabove, and is not of substantial interest here.Computational Linguistics, Volume 14, Number 1, Winter 19882.1 CHOICE OF THE OPTIMAL PATHSince the number of paths over a span is an exponentialfunction of the span length, it may not be obvious howone can guarantee finding the best path, without exam-ining an exponential number of paths (namely all ofthem).
The insight making fast discovery of the optimalpath possible is the use of a Dynamic Programmingsolution (Dano (1975), Dreyfus and Law (1977)).The two key ideas of Dynamic Programming havebeen characterized as"first, the recognition that a given'whole problem' can be solved if the values of the bestsolutions of certain subproblems can be determined.. .
;and secondly, the realization that if one starts at or nearthe end of the 'whole problem,' the subproblems are sosimple as to have trivial solutions" (Dreyfus and Law(1977), p. 5).
Dynamic Programming is closely related tothe study of Graph Theory and of Network Optimiza-tion, and can lead to rapid solutions for otherwiseintractable problems, given that those problems obeycertain structural constraints.
In this case, the con-straints are indeed obeyed, and a linear-time solution isavailable.Consider aspan of length n = 5, with the words in thepath denoted by v, w, x, y, z.
Assume that v and z arethe unambiguous bounding words, and that the otherthree words are each three ways ambiguous.
Subscriptswill index the various tags for each word: w~ will denotethe first tag in the set of possible tags for word w. Everypath must contain vl and zl, since v and z are unambig-uous.
Now consider the partial spans beginning at v,and ending (respectively) ateach of the four remainingwords.
The partial span network ending at w containsexactly three paths.
One of these must be a portion ofthe optimal path for the entire span.
So we save allthree: one path to each tag under w. The probability ofeach path is the value found in the collocation matrixentry for its tag-pair, namely p(v,w i) for i ranging fromone to three.Next, consider the three tags under word x.
One ofthese tags must lie on the optimal path.
Assume it is x~.Under this assumption, we have a complete span oflength 3, for x is unambiguous.
Only one of the paths tox I can be optimal.
Therefore we can disambiguate v .
.
.w .
.
.
x~ under this assumption, namely, as MAX(p(v,wi)* p(wi,Xl) for all w i.Now, of course, the assumption that x~ is on theoptimal path is unacceptable.
However, the key toVOLSUNGA is to notice that by making three suchindependent assumptions, namely for x~, x 2, and x 3, weexhaust all possible optimal paths.
Only a path whichoptimally leads to one of x's tags can be part of theoptimal path.
Thus, when examining the partial spannetwork ending at word y, we need only consider threepossibly optimal paths, namely those leading to xt, x2,and x 3, and how those three combine with the tags of y.At most one of those three paths can lie along theoptimal path to each tag of y; so we have 32, or 9,35Steven J. DeRose Grammatical Category Disambiguation by Statistical OptimizationThe ATman NN VBstill NN VB RBsaw NN VBDher PPO PP$Table 2: Sample Ambiguitiescomparisons.
But only three paths will survive, namely,the optimal path to each of the three tags under y. Eachof those three is then considered as a potential path to z,and one is chosen.This reduces the algorithm from exponential com-plexity to linear.
The number of paths retained at anystage is the same as the degree of ambiguity at thatstage; and this value is bounded by a very small valueestablished by independent facts about the Englishlexicon.
No faster order of speed is po,;sible if eachword is to be considered at all.2.2.
PROCESSING A SAMPLE SPANAs an example, we will consider the process by whichVOLSUNGA would tag "The man still saw her".
Wewill omit a few ambiguities, reducing the number ofpaths to 24 for ease of exposition.
The tags for eachword are shown in Table 2.
The notation is fairlymnemonic, but it is worth clarifying that PPO indicatesa n objective personal pronoun, and PP$ the possessivethereof, while VBD is a past-tense verb.Examples of the various collocational probabilitiesare illustrated in Table 3 (VOLSUNGA does not actu-ally consider any collocation truly impossible, so zerosare raised to a minimal non-zero value when loaded).The product of 1"2"3"2"2"I ambiguities gives 24paths through this span.
In this case, a simple process ofchoosing the best successor for each word in orderwould produce the correct agging (AT NN RB VBDPPO).
But of course this is often not the case.Using VOLSUNGA's method we would first stack"the",  with certainty for the tag AT (we will denote thisby "p(the-AT) = CERTAIN)").
Next we stack "man",and look up the collocational probabilities of all tagpairs between the two words at the top of the stack.
Inthis case they will be p(AT, NN) = 186, and p(AT, VB)= 1.
We save the best (in this case only) path to each ofman-NN and man-VB.
It is sufficient o save a pointerto the tag of " the"  which ends each of these paths,NN PPO PP$ RB VB VBDAT 186 0 0 8 1 8 9NN 40 1 3 40 9 66 186PPO 7 3 16 164 109 16 313PP$ 176 0 0 5 1 1 2RB 5 3 16 71 118 152 128VB 22 694 146 98 9 1 59VBD 11 584 143 160 2 1 91Table 3: Sample Collocational Probabilitiesmaking backward-linked lists (which, in this case, con-verge).Now we stack "still".
For each of its tags (NN, VB,and RB), we choose either the NN or the VB tag of"man" as better, p(still-NN) is the best of:p(man-NN) *p(NN,NN) = 186 *40 = 744p(man-VB) *p(VB,NN) = 1 *22 = 22Thus, the best path to still-NN is AT NN NN.Similarly, we find that the best path to still-RB is ATNN RB, and the best path to still-VB is AT NN RB.This shows the (realistically) overwhelming effect of anarticle on disambiguating an immediately followingnoun/verb ambiguity.At this point, only the optimal path to each of the tagsfor "still" is saved.
We then go on to match each ofthose paths with each of the tags for "saw",  discoveringthe optimal paths to saw-NN and to saw-VB.
The nextiteration reveals the optimal paths to her-PPO andher-PP$, and the final one picks the optimal path to theperiod, which this example treats as unambiguous.
Nowwe have the best path between two certain tags(AT and .
), and can merely pop the stack, followingpointers to optimal predecessors to disambiguate thesequence.
The period becomes the start of the nextspan.2.3 RELATIVE TAG PROBABILITIESInitial testing of the algorithm used only transitionalprobability information.
RTPs had no effect uponchoosing an optimal path.
For example, in decidingwhether to consider the word "t ime" to be a noun or averb, environments uch as a preceding article orproper noun, or a following verb or pronoun, were thesole criteria.
The fact that "t ime" is almost always anoun (1901 instances in the Brown Corpus) rather thana verb (16 instances) was not considered.
Accuracyaveraged 92-93%, with a peak of 93.7%.There are clear examples for which the use of RTPsis important.
One such case which arises in the BrownCorpus is "so that".
"So"  occurs 932 times as aqualifier (QL), 479 times as a subordinating conjunction(CS), and once as an interjection (UH).
The standardtagging for "so that" is "CS CS", but this is anextremely low-frequency collocation, lower than thealternative "UH CS" (which is mainly limited to fic-tion).
Barring strong contextual counter-evidence, "UHCS" is the preferred assignment if RTP information isnot used.
By weighing the RTPs for "so" ,  however, the"UH"  assignment can be avoided.The LOB Corpus would (via idiom tagging) use "CSCS" in this case, employing a special "ditto tag" toindicate that two separate orthographic words consti-tute (at least for tagging purposes) a single syntacticword.
Another example would be "so as to",  tagged'TO TO TO".
Blackwell comments that "it was difficultto know where to draw the line in defining whatconstituted an idiom, and some such decisions eemed36 Computational Linguistics, Volume 14, Number 1, Winter 1988Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimizationto have been influenced by semantic factors.
Nonethe-less, IDIOMTAG had played a significant part in in-creasing the accuracy of the Tagging Suite \[i.e.,CLAWS\ ] .
.
. "
(Blackwell (1985), p. 7).
It may be betterto treat this class of "idioms" as lexical items whichhappen to contain blanks; but RTPs permit correcttagging in some of these cases.The main difficulty in using RTPs is determining howheavily to weigh them relative to collocational informa-tion.
At first, VOLSUNGA multiplied raw relativefrequencies into the path probability calculations; butthe ratios were so high in some cases as to totallyswamp collocational data.
Thus, normalization is re-quired.
The present solution is a simple one; all ratiosover a fixed limit are truncated to that limit.
Implement-ing RTPs increased accuracy by approximately 4%, tothe range 95-97%, with a peak of 97.5% on one smallsample.
Thus, about half of the residual errors wereeliminated.
It is likely that tuning the normalizationwould improve this figure slightly more.2.4 LEARNABILITYVOLSUNGA was not designed with psychological re-ality as a goal, though it has some plausible character-istics.
We will consider a few of these briefly.
Thissection should not be interpreted as more than sugges-tive.First, consider dictionary learning; the program cur-rently assumes that a full dictionary is available.
Thisassumption is nearly true for mature language users, buthumans have little trouble ven with novel exical items,and generally speak of "context" when asked to de-scribe how they figure out such words.
As Ryder andWalker (1982) note, the use of structural analysis basedon contextual c ues allows speakers to compute syntac-tic structures even for a text such as Jabberwocky,where lexical information is clearly insufficient.
Theimmediate syntactic ontext severely restricts the likelychoices for the grammatical category of each neologism.VOLSUNGA can perform much the same task via aminor modification, even if a suffix analysis fails.
Themost obvious solution is simply to assign all tags to theunknown word and find the optimal path through thecontaining span as usual.
Since the algorithm isfast, thisis not prohibitive.
Better, one can assign only those tagswith a non-minimal probability of being adjacent to thepossible tags of neighboring words.
Precisely calculat-ing the mean number of tags remaining under thisapproach is left as a question for further esearch, butthe number is certainly very low.
About 3900 of the 9409theoretically possible tag pairs occur in the BrownCorpus.
Also, all tags marking closed classes (abouttwo-thirds of all tags) may be eliminated from consid-eration.Also, since VOLSUNGA operates from left to right,it can always decide upon an optimum partial result, andcan predict a set of probable successors.
For thesereasons, it is largely robust against ungrammaticality.Shannon (1951) performed experiments ofa similar sort,asking human subjects to predict he next character of apartially presented sentence.
The accuracy of theirpredictions increased with the length of the sentencefragment presented.The fact that VOLSUNGA requires a great deal ofpersistent memory for its dictionary, yet very littletemporary space for processing, is appropriate.
Bycontrast, the space requirements of CLAWS wouldovertax the short-term emory of any language user.Another advantage ofVOLSUNGA is that it requireslittle inherent linguistic knowledge.
Probabilities may beacquired simply through counting instances of colloca-tion.
The results will increase in accuracy as more inputtext is seen.
Previous algorithms, on the other hand,have included extensive manually generated lists ofrules or exceptions.An obvious difference between VOLSUNGA andhumans is that VOLSUNGA makes no use whatsoeverof semantic information.
No account is taken of the highprobability that in a text about carpentry, "saw" ismore likely a noun than in other types of text.
Theremay also be genre and topic-dependent i fluences uponthe frequencies of various syntactic, and hence catego-rial, structures.
Before such factors can be incorporatedinto VOLSUNGA, however, more complete dictionar-ies, including semantic information of at least a rudi-mentary kind, must be available.3 ACCURACY ANALYSIS:3.1 CALIBRATIONVOLSUNGA requires a tagged corpus upon which tobase its tables of probabilities.
The calculation of tran-sitional probabilities is described by Marshall (1983).The entire Brown Corpus (modified by the expansion ofcontracted forms) was analyzed in order to produce thetables used in VOLSUNGA.
A complete dictionary wastherefore available when running the program on thatsame corpus.Since the statistics comprising the dictionary andprobability matrix used by the program were derivedfrom the same corpus analyzed, the results may beconsidered optimal.
On the other hand, the Corpus iscomprehensive enough so that use of other input text isunlikely to introduce statistically significant changes inthe program's performance.
This is especially truebecause many of the unknown words would be (a)capitalized proper names, for which tag assignment istrivial modulo a small percentage at sentence bound-aries, or (b) regular formations from existing words,which are readily identified by suffixes.
Greene andRubin (1971) note that their suffix list "consists mainlyof Romance ndings which are the source of continuingadditions to the language" (p. 41).A natural relationship exists between the size of adictionary, and the percentage of words in an averagetext which it accounts for.
A complete table showing theComputational Linguistics, Volume 14, Number 1, Winter 1988 37Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization#Types Freq Limit1 69,9712 36,4113 28,8524 26,1495 23,23711 9,489135 683236 383408 229693 1451,120 961,791 622,854 394,584 228,478 I016,683 450,406 1Table#Tokens69,971106,382135,234161,383184,620255,503508,350558,024608,933660,149710,137760,838812,448862,357918,046965,3821,014,232%Tokens6.910.513.315.918.225.250.155.060.065.170.075.080.185.090.595.2100.04: Number of Tokens by Frequencyrelationship appears in Kucera and Francis (1967) pp.300-307.
A few representative entries are shown inTable 4.
The "#Types"  column indicates how manyvocabulary items occur at least "Freq Limit" times inthe Corpus.
The "#Tokens" column shows how manytokens are accounted for by those types, and the"%Tokens" column converts this number to a percent-age.
(See also pp.
358-362 in the same volume forseveral related graphs.
)3.2 OVERALL ACCURACYTable 5 lists the accuracy for each genre from theBrown Corpus.
The total token count differs from Table4 due to inclusion of non-lexical tokens, such as punc-tuation.
The figure shown deducts from the error countGenreA: Press ReportageB: Press EditorialC: Press ReviewsD: ReligionE: Skills/HobbiesF: Popular LoreG: Belles LettresH: MiscellaneousJ: LearnedK: General FictionL: Mystery/DetectiveM: Science FictionN: Adventure/WesternP: Romance/Love StoryR: HumorInformative Prose TotalImaginative Prose TotalOverall TotalSize99,1656071639.83238.63181.659108.617169.78969.508179.92767.08356.09013.95667.67368,33720,990847,844294,1291,141,973Table 5: VOLSUNGA Tagging Accuracy% Accuracy96.3696.0996.1296.0195.3495.9996.3596.6696.3895.7295.4795.4095.5895.5495.5596.2095.5796.04those particular instances in which the Corpus tagindicates by an affix that the word is part of a headline,title, etc.
Since the syntax of such structures is oftendeviant, such errors are less significant.
The differencethis makes ranges from 0.09% (Genre L), up to 0.64%(Genre A), with an unweighted mean of 0.31%.
Detailedbreakdowns of the particular errors made for each genreexist in machine-readable form.4 CONCLUSIONThe high degree of lexical category ambiguity in lan-guages such as English poses problems for parsing.Specifically, until the categories of individual wordshave been established, it is difficult to construct aunique and accurate syntactic structure.
Therefore, amethod for locally disambiguating lexical items hasbeen developed.Early efforts to solve this problem relied upon largelibraries of manually chosen context frame rules.
Morerecently, however, work on the LOB Corpus of BritishEnglish led to a more systematic algorithm based uponcombinatorial statistics.
This algorithm operates en-tirely from left to right, and has no inherent limit uponthe number of consecutive ambiguities which may beprocessed.
Its authors report an accuracy of 96-97%.However, CLAWS falls prey to other problems.First, the probabilistic system has been augmented inseveral ways, such as by pre-tagging of categoriallytroublesome "idioms" (this feature contributes 3%towards the total accuracy).
Second, it was not basedupon the most complete statistics available.
Third, andperhaps most significant, it requires non-polynomiallylarge time and space.The algorithm developed here, called VOLSUNGA,addresses these problems.
First, the various additionsto CLAWS (i.e., beyond the use of two-place probabil-ities and RTPs) have been deleted.
Second, the programhas been calibrated by reference to 100% instead of 20%of the Brown Corpus, and has been applied to the entireCorpus for testing.
This is a particularly important estbecause the Brown Corpus provides a long-establishedstandard against which accuracy can be measured.Third, the algorithm has been completely redesigned sothat it establishes the optimal tag assignments in lineartime, as opposed to exponential.Tests on the one million words of the Brown Corpusshow an overall accuracy of approximately 96%, de-spite the non-use of auxiliary algorithms.
Suggestionshave been given for several possible modificationswhich might yield even higher accuracies.The accuracy and speed of VOLSUNGA make itsuitable for use in pre-processing natural anguage inputto parsers and other language understanding systems.Its systematicity makes it suitable also for work incomputational studies of language learning.38 Computational Linguistics, Volume 14, Number 1, Winter 1988Steven J. DeRose Grammatical Category Disambiguation by Statistical OptimizationREFERENCESBeale, Andrew David.
1985 Grammatical Analysis by Computer of theLancaster-Oslo/Bergen (LOB) Corpus of British English Texts.Proceedings of the 23rd Annual Meeting of the Association forComputational Linguistics.
University of Chicago Press, Chicago,Illinois: 293-298.Blackwell, Sue A.
1985 A Survey of Computer-Based English Lan-guage Research.
ICAME News 9: 3-28.
(Available from theNorwegian Computing Centre for the Humanities, Harald Harfa-gres gate 31, P.O.
Box 53, N-5014 Bergen University, Norway.
)Booth, B. M. 1985 Revising CLAWS.
ICAME News 9:29-35.Dano, Sven.
1975 Nonlinear and Dynamic Programming.
Springer-Verlag, New York.Dreyfus, Stuart E. and Law, Averill, M. 1977 The Art and Theory ofDynamic Programming.
Academic Press, New York.Francis, W. Nelson and Kucera, Henry.
1979 Manual of Informationto Accompany A Standard Corpus of Present-Day Edited Ameri-can English, for Use with Digital Computers ("Revised andAmplified" edition).
Department of Linguistics, Brown Univer-sity, Providence, Rhode Island.Francis, W. Nelson and Kucera, Henry.
1982 Frequency Analysis ofEnglish Usage: Lexicon and Grammar.
Houghton-Mifflin Com-pany, Boston, Massachusetts.Greene, Barbara B. and Rubin, Gerald M. 1971 Automated Grammat-ical Tagging of English.
Department of Linguistics, Brown Uni-versity, Providence, Rhode Island.Hirst, Graeme.
1983 Semantic Interpretation Against Ambiguity.Ph.D.
dissertation., Department of Computer Science, BrownUniversity, Providence, Rhode Island.Klein, S. and Simmons, R. F. 1963 A Computational Approach toGrammatical Coding of English Words.
JACM 10: 334-47.Kucera, Henry and Francis, W. Nelson.
1967 Computational Analy-sis of Present-Day American English.
Brown University Press,Providence, Rhode Island.Leech, Geoffrey; Garside, Roger; and Atwell, Erik.
1983 The Auto-matic Grammatical Tagging of the LOB Corpus.
ICAME News 7:13-33.Marshall, Ian.
1983 Choice of Grammatical Word-Class WithoutGlobal Syntactic Analysis: Tagging Words in the LOB Corpus.Computers in the Humanities 17: 139-150.Ryder, Joan and Walker, Edward C. T. 1982 Two Mechanisms ofLexical Ambiguity.
In Mehler, Jacques; Walker, Edward C.T.
;and Garret, Merrill, Eds., Perspectives on Mental Representation.Lawrence Erlbaum Associates, Hillsdale, New Jersey: 134-149.Shannon, Claude E. 1951 Prediction and Entropy of Printed English.Bell System Technical Journal 30: 50--64.Computational Linguistics, Volume 14, Number 1, Winter 1988 39
