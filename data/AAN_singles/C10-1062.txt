Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,Beijing, August 2010Learning to Predict Readability using Diverse Linguistic FeaturesRohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty21Department of Computer ScienceThe University of Texas at Austin{rjkate,mooney}@cs.utexas.edu2IBM Watson Research Center{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.comAbstractIn this paper we consider the problem ofbuilding a system to predict readabilityof natural-language documents.
Our sys-tem is trained using diverse features basedon syntax and language models which aregenerally indicative of readability.
Theexperimental results on a dataset of docu-ments from a mix of genres show that thepredictions of the learned system are moreaccurate than the predictions of naive hu-man judges when compared against thepredictions of linguistically-trained experthuman judges.
The experiments also com-pare the performances of different learn-ing algorithms and different types of fea-ture sets when used for predicting read-ability.1 IntroductionAn important aspect of a document is whether itis easily processed and understood by a humanreader as intended by its writer, this is termedas the document?s readability.
Readability in-volves many aspects including grammaticality,conciseness, clarity, and lack of ambiguity.
Teach-ers, journalists, editors, and other professionalsroutinely make judgements on the readability ofdocuments.
We explore the task of learning toautomatically judge the readability of natural-language documents.In a variety of applications it would be useful tobe able to automate readability judgements.
Forexample, the results of a web-search can be or-dered taking into account the readability of theretrieved documents thus improving user satisfac-tion.
Readability judgements can also be usedfor automatically grading essays, selecting in-structional reading materials, etc.
If documentsare generated by machines, such as summariza-tion or machine translation systems, then they areprone to be less readable.
In such cases, a read-ability measure can be used to automatically fil-ter out documents which have poor readability.Even when the intended consumers of text aremachines, for example, information extraction orknowledge extraction systems, a readability mea-sure can be used to filter out documents of poorreadability so that the machine readers will not ex-tract incorrect information because of ambiguityor lack of clarity in the documents.As part of the DARPA Machine Reading Pro-gram (MRP), an evaluation was designed and con-ducted for the task of rating documents for read-ability.
In this evaluation, 540 documents wererated for readability by both experts and novicehuman subjects.
Systems were evaluated based onwhether they were able to match expert readabil-ity ratings better than novice raters.
Our systemlearns to match expert readability ratings by em-ploying regression over a set of diverse linguisticfeatures that were deemed potentially relevant toreadability.
Our results demonstrate that a richcombination of features from syntactic parsers,language models, as well as lexical statistics allcontribute to accurately predicting expert humanreadability judgements.
We have also consideredthe effect of different genres in predicting read-ability and how the genre-specific language mod-els can be exploited to improve the readability pre-dictions.5462 Related WorkThere is a significant amount of published workon a related problem: predicting the reading diffi-culty of documents, typically, as the school grade-level of the reader from grade 1 to 12.
Some earlymethods measure simple characteristics of docu-ments like average sentence length, average num-ber of syllables per word, etc.
and combine themusing a linear formula to predict the grade level ofa document, for example FOG (Gunning, 1952),SMOG (McLaughlin, 1969) and Flesh-Kincaid(Kincaid et al, 1975) metrics.
These methodsdo not take into account the content of the doc-uments.
Some later methods use pre-determinedlists of words to determine the grade level of adocument, for example the Lexile measure (Sten-ner et al, 1988), the Fry Short Passage measure(Fry, 1990) and the Revised Dale-Chall formula(Chall and Dale, 1995).
The word lists thesemethods use may be thought of as very simplelanguage models.
More recently, language mod-els have been used for predicting the grade levelof documents.
Si and Callan (2001) and Collins-Thompson and Callan (2004) train unigram lan-guage models to predict grade levels of docu-ments.
In addition to language models, Heilmanet al (2007) and Schwarm and Ostendorf (2005)also use some syntactic features to estimate thegrade level of texts.Pitler and Nenkova (2008) consider a differ-ent task of predicting text quality for an educatedadult audience.
Their system predicts readabil-ity of texts from Wall Street Journal using lex-ical, syntactic and discourse features.
Kanungoand Orr (2009) consider the task of predictingreadability of web summary snippets produced bysearch engines.
Using simple surface level fea-tures like the number of characters and syllablesper word, capitalization, punctuation, ellipses etc.they train a regression model to predict readabilityvalues.Our work differs from this previous research inseveral ways.
Firstly, the task we have consid-ered is different, we predict the readability of gen-eral documents, not their grade level.
The doc-uments in our data are also not from any singledomain, genre or reader group, which makes ourtask more general.
The data includes human writ-ten as well as machine generated documents.
Thetask and the data has been set this way because itis aimed at filtering out documents of poor qualityfor later processing, like for extracting machine-processable knowledge from them.
Extractingknowledge from openly found text, such as fromthe internet, is becoming popular but the qualityof text found ?in the wild?, like found throughsearching the internet, vary considerably in qual-ity and genre.
If the text is of poor readability thenit is likely to lead to extraction errors and moreproblems downstream.
If the readers are goingto be humans instead of machines, then also it isbest to filter out poorly written documents.
Henceidentifying readability of general text documentscoming from various sources and genres is an im-portant task.
We are not aware of any other workwhich has considered such a task.Secondly, we note that all of the above ap-proaches that use language models train a lan-guage model for each difficulty level using thetraining data for that level.
However, since theamount of training data annotated with levelsis limited, they can not train higher-order lan-guage models, and most just use unigram models.In contrast, we employ more powerful languagemodels trained on large quantities of generic text(which is not from the training data for readabil-ity) and use various features obtained from theselanguage models to predict readability.
Thirdly,we use a more sophisticated combination of lin-guistic features derived from various syntacticparsers and language models than any previouswork.
We also present ablation results for differ-ent sets of features.
Fourthly, given that the doc-uments in our data are not from a particular genrebut from a mix of genres, we also train genre-specific language models and show that includingthese as features improves readability predictions.Finally, we also show comparison between var-ious machine learning algorithms for predictingreadability, none of the previous work comparedlearning algorithms.3 Readability DataThe readability data was collected and re-leased by LDC.
The documents were collected547from the following diverse sources or genres:newswire/newspaper text, weblogs, newsgroupposts, manual transcripts, machine translation out-put, closed-caption transcripts and Wikipedia arti-cles.
Documents for newswire, machine transla-tion and closed captioned genres were collectedautomatically by first forming a candidate poolfrom a single collection stream and then randomlyselecting documents.
Documents for weblogs,newsgroups and manual transcripts were also col-lected in the same way but were then reviewedby humans to make sure they were not simplyspam articles or something objectionable.
TheWikipedia articles were collected manually, bysearching through a data archive or the live web,using keyword and other search techniques.
Notethat the information about genres of the docu-ments is not available during testing and hencewas not used when training our readability model.A total of 540 documents were collected in thisway which were uniformly distributed across theseven genres.
Each document was then judgedfor its readability by eight expert human judges.These expert judges are native English speakerswho are language professionals and who havespecialized training in linguistic analysis and an-notation, including the machine translation post-editing task.
Each document was also judged forits readability by six to ten naive human judges.These non-expert (naive) judges are native En-glish speakers who are not language professionals(e.g.
editors, writers, English teachers, linguisticannotators, etc.)
and have no specialized languageanalysis or linguistic annotation training.
Both ex-pert and naive judges provided readability judg-ments using a customized web interface and gavea rating on a 5-point scale to indicate how readablethe passage is (where 1 is lowest and 5 is highestreadability) where readability is defined as a sub-jective judgment of how easily a reader can extractthe information the writer or speaker intended toconvey.4 Readability ModelWe want to answer the question whether amachine can accurately estimate readability asjudged by a human.
Therefore, we built amachine-learning system that predicts the read-ability of documents by training on expert hu-man judgements of readability.
The evaluationwas then designed to compare how well machineand naive human judges predict expert humanjudgements.
In order to make the machine?s pre-dicted score comparable to a human judge?s score(details about our evaluation metrics are in Sec-tion 6.1), we also restricted the machine scores tointegers.
Hence, the task is to predict an integerscore from 1 to 5 that measures the readability ofthe document.This task could be modeled as a multi-classclassification problem treating each integer scoreas a separate class, as done in some of the previ-ous work (Si and Callan, 2001; Collins-Thompsonand Callan, 2004).
However, since the classesare numerical and not unrelated (for example, thescore 2 is in between scores 1 and 3), we de-cided to model the task as a regression problemand then round the predicted score to obtain theclosest integer value.
Preliminary results verifiedthat regression performed better than classifica-tion.
Heilman et al (2008) also found that itis better to treat the readability scores as ordinalthan as nominal.
We take the average of the ex-pert judge scores for each document as its gold-standard score.
Regression was also used by Ka-nungo and Orr (2009), although their evaluationdid not constrain machine scores to be integers.We tested several regression algorithms avail-able in the Weka1 machine learning package, andin Section 6.2 we report results for several whichperformed best.
The next section describes thenumerically-valued features that we used as inputfor regression.5 Features for Predicting ReadabilityGood input features are critical to the success ofany regression algorithm.
We used three main cat-egories of features to predict readability: syntac-tic features, language-model features, and lexicalfeatures, as described below.5.1 Features Based on SyntaxMany times, a document is found to be unreadabledue to unusual linguistic constructs or ungram-1http://www.cs.waikato.ac.nz/ml/weka/548matical language that tend to manifest themselvesin the syntactic properties of the text.
There-fore, syntactic features have been previously used(Bernth, 1997) to gauge the ?clarity?
of writtentext, with the goal of helping writers improve theirwriting skills.
Here too, we use several featuresbased on syntactic analyses.
Syntactic analysesare obtained from the Sundance shallow parser(Riloff and Phillips, 2004) and from the EnglishSlot Grammar (ESG) (McCord, 1989).Sundance features: The Sundance system is arule-based system that performs a shallow syntac-tic analysis of text.
We expect that this analysisover readable text would be ?well-formed?, adher-ing to grammatical rules of the English language.Deviations from these rules can be indications ofunreadable text.
We attempt to capture such de-viations from grammatical rules through the fol-lowing Sundance features computed for each textdocument: proportion of sentences with no verbphrases, average number of clauses per sentence,average sentence length in tokens, average num-ber of noun phrases per sentence, average numberof verb phrases per sentence, average number ofprepositional phrases per sentence, average num-ber of phrases (all types) per sentence and averagenumber of phrases (all types) per clause.ESG features: ESG uses slot grammar rules toperform a deeper linguistic analysis of sentencesthan the Sundance system.
ESG may considerseveral different interpretations of a sentence, be-fore deciding to choose one over the other inter-pretations.
Sometimes ESG?s grammar rules failto produce a single complete interpretation of asentence, in which case it generates partial parses.This typically happens in cases when sentencesare ungrammatical, and possibly, less readable.Thus, we use the proportion of such incompleteparses within a document as a readability feature.In case of extremely short documents, this propor-tion of incomplete parses can be misleading.
Toaccount for such short documents, we introducea variation of the above incomplete parse feature,by weighting it with a log factor as was done in(Riloff, 1996; Thelen and Riloff, 2002).We also experimented with some other syn-tactic features such as average sentence parsescores from Stanford parser and an in-house maxi-mum entropy statistical parer, average constituentscores etc., however, they slightly degraded theperformance in combination with the rest of thefeatures and hence we did not include them inthe final set.
One possible explanation could bethat averaging diminishes the effect of low scorescaused by ungrammaticality.5.2 Features Based on Language ModelsA probabilistic language model provides a predic-tion of how likely a given sentence was generatedby the same underlying process that generated acorpus of training documents.
In addition to ageneral n-gram language model trained on a largebody of text, we also exploit language modelstrained to recognize specific ?genres?
of text.
If adocument is translated by a machine, or casuallyproduced by humans for a weblog or newsgroup,it exhibits a character that is distinct from docu-ments that go through a dedicated editing process(e.g., newswire and Wikipedia articles).
Belowwe describe features based on generic as well asgenre-specific language models.Normalized document probability: One obvi-ous proxy for readability is the score assigned toa document by a generic language model (LM).Since the language model is trained on well-written English text, it penalizes documents de-viating from the statistics collected from the LMtraining documents.
Due to variable documentlengths, we normalize the document-level LMscore by the number of words and compute thenormalized document probability NP (D) for adocument D as follows:NP (D) =(P (D|M)) 1|D| , (1)where M is a general-purpose language modeltrained on clean English text, and |D| is the num-ber of words in the document D.Perplexities from genre-specific language mod-els: The usefulness of LM-based features incategorizing text (McCallum and Nigam, 1998;Yang and Liu, 1999) and evaluating readability(Collins-Thompson and Callan, 2004; Heilmanet al, 2007) has been investigated in previouswork.
In our experiments, however, since doc-uments were acquired through several differentchannels, such as machine translation or web logs,549we also build models that try to predict the genreof a document.
Since the genre information formany English documents is readily available, wetrained a series of genre-specific 5-gram LMs us-ing the modified Kneser-Ney smoothing (Kneserand Ney, 1995; Stanley and Goodman, 1996).
Ta-ble 1 contains a list of a base LM and genre-specific LMs.Given a document D consisting of tokenizedword sequence {wi : i = 1, 2, ?
?
?
, |D|}, its per-plexity L(D|Mj) with respect to a LM Mj iscomputed as:L(D|Mj) = e(?
1|D|P|D|i=1 logP (wi|hi;Mj)), (2)where |D| is the number of words in D and hi arethe history words for wi, and P (wi|hi;Mj) is theprobability Mj assigns to wi, when it follows thehistory words hi.Posterior perplexities from genre-specific lan-guagemodels: While perplexities computed fromgenre-specific LMs reflect the absolute probabil-ity that a document was generated by a specificmodel, a model?s relative probability compared toother models may be a more useful feature.
To thisend, we also compute the posterior perplexity de-fined as follows.
Let D be a document, {Mi}Gi=1be G genre-specific LMs, and L(D|Mi) be theperplexity of the document D with respect to Mi,then the posterior perplexity, R(Mi|D), is de-fined as:R(Mi|D) =L(D|Mi)?Gj=1 L(D|Mj).
(3)We use the term ?posterior?
because if a uni-form prior is adopted for {Mi}Gi=1,R(Mi|D) canbe interpreted as the posterior probability of thegenre LM Mi given the document D.5.3 Lexical FeaturesThe final set of features involve various lexicalstatistics as described below.Out-of-vocabulary (OOV) rates: We conjecturethat documents containing typographical errors(e.g., for closed-caption and web log documents)may receive low readability ratings.
Therefore,we compute the OOV rates of a document with re-spect to the various LMs shown in Table 1.
Sincemodern LMs often have a very large vocabulary,to get meaningful OOV rates, we truncate the vo-cabularies to the top (i.e., most frequent) 3000words.
For the purpose of OOV computation, adocument D is treated as a sequence of tokenizedwords {wi : i = 1, 2, ?
?
?
, |D|}.
Its OOV ratewith respect to a (truncated) vocabulary V is then:OOV (D|V) =?Di=1 I(wi /?
V)|D| , (4)where I(wi /?
V) is an indicator function takingvalue 1 if wi is not in V , and 0 otherwise.Ratio of function words: A characteristic of doc-uments generated by foreign speakers and ma-chine translation is a failure to produce certainfunction words, such as ?the,?
or ?of.?
So we pre-define a small set of function words (mainly En-glish articles and frequent prepositions) and com-pute the ratio of function words over the totalnumber words in a document:RF (D) =?Di=1 I(wi ?
F)|D| , (5)where I(wi ?
F) is 1 ifwi is in the set of functionwords F , and 0 otherwise.Ratio of pronouns: Many foreign languages thatare source languages of machine-translated docu-ments are pronoun-drop languages, such as Ara-bic, Chinese, and romance languages.
We conjec-ture that the pronoun ratio may be a good indica-tor whether a document is translated by machineor produced by humans, and for each document,we first run a POS tagger, and then compute theratio of pronouns over the number of words in thedocument:RP (D) =?Di=1 I(POS(wi) ?
P)|D| , (6)where I(POS(wi) ?
F) is 1 if the POS tag of wiis in the set of pronouns, P , and 0 otherwise.Fraction of known words: This feature measuresthe fraction of words in a document that occureither in an English dictionary or a gazetteer ofnames of people and locations.6 ExperimentsThis section describes the evaluation methodol-ogy and metrics and presents and discusses our550Genre Training Size(M tokens) Data Sourcesbase 5136.8 mostly LDC?s GigaWord setNW 143.2 newswire subset of baseNG 218.6 newsgroup subset of baseWL 18.5 weblog subset of baseBC 1.6 broadcast conversation subset of baseBN 1.1 broadcast news subset of basewikipedia 2264.6 Wikipedia textCC 0.1 closed captionZhEn 79.6 output of Chinese to English Machine TranslationArEn 126.8 output of Arabic to English Machine TranslationTable 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).experimental results.
The results of the officialevaluation task are also reported.6.1 Evaluation MetricThe evaluation process for the DARPAMRP read-ability test was designed by the evaluation teamled by SAIC.
In order to compare a machine?spredicted readability score to those assigned bythe expert judges, the Pearson correlation coef-ficient was computed.
The mean of the expert-judge scores was taken as the gold-standard scorefor a document.To determine whether the machine predictsscores closer to the expert judges?
scores thanwhat an average naive judge would predict, asampling distribution representing the underlyingnovice performance was computed.
This was ob-tained by choosing a random naive judge for everydocument, calculating the Pearson correlation co-efficient with the expert gold-standard scores andthen repeating this procedure a sufficient numberof times (5000).
The upper critical value was setat 97.5% confidence, meaning that if the machineperforms better than the upper critical value thenwe reject the null hypothesis that machine scoresand naive scores come from the same distributionand conclude that the machine performs signifi-cantly better than naive judges in matching the ex-pert judges.6.2 Results and DiscussionWe evaluated our readability system on the datasetof 390 documents which was released earlier dur-ing the training phase of the evaluation task.
WeAlgorithm CorrelationBagged Decision Trees 0.8173Decision Trees 0.7260Linear Regression 0.7984SVM Regression 0.7915Gaussian Process Regression 0.7562Naive JudgesUpper Critical Value 0.7015Distribution Mean 0.6517BaselinesUniform Random 0.0157Proportional Random -0.0834Table 2: Comparing different algorithms on the readabilitytask using 13-fold cross-validation on the 390 documents us-ing all the features.
Exceeding the upper critical value of thenaive judges?
distribution indicates statistically significantlybetter predictions than the naive judges.used stratified 13-fold cross-validation in whichthe documents from various genres in each foldwas distributed in roughly the same proportion asin the overall dataset.
We first conducted experi-ments to test different regression algorithms usingall the available features.
Next, we ablated variousfeature sets to determine how much each featureset was contributing to making accurate readabil-ity judgements.
These experiments are describedin the following subsections.6.2.1 Regression AlgorithmsWe used several regression algorithms availablein theWeka machine learning package and Table 2shows the results obtained.
The default values551Feature Set CorrelationLexical 0.5760Syntactic 0.7010Lexical + Syntactic 0.7274Language Model based 0.7864All 0.8173Table 3: Comparison of different linguistic feature sets.in Weka were used for all parameters, changingthese values did not show any improvement.
Weused decision tree (reduced error pruning (Quin-lan, 1987)) regression, decision tree regressionwith bagging (Breiman, 1996), support vector re-gression (Smola and Scholkopf, 1998) using poly-nomial kernel of degree two,2 linear regressionand Gaussian process regression (Rasmussen andWilliams, 2006).
The distribution mean and theupper critical values of the correlation coefficientdistribution for the naive judges are also shown inthe table.Since they are above the upper critical value, allalgorithms predicted expert readability scores sig-nificantly more accurately than the naive judges.Bagged decision trees performed slightly betterthan other methods.
As shown in the followingsection, ablating features affects predictive accu-racy much more than changing the regression al-gorithm.
Therefore, on this task, the choice of re-gression algorithm was not very critical once goodreadability features are used.
We also tested twosimple baseline strategies: predicting a score uni-formly at random, and predicting a score propor-tional to its frequency in the training data.
Asshown in the last two rows of Table 2, these base-lines perform very poorly, verifying that predict-ing readability on this dataset as evaluated by ourevaluation metric is not trivial.6.2.2 Ablations with Feature SetsWe evaluated the contributions of different fea-ture sets through ablation experiments.
Baggeddecision-tree was used as the regression algorithmin all of these experiments.
First we comparedsyntactic, lexical and language-model based fea-tures as described in Section 5, and Table 3 shows2Polynomial kernels with other degrees and RBF kernelperformed worse.the results.
The language-model feature set per-forms the best, but performance improves when itis combined with the remaining features.
The lex-ical feature set by itself performs the worst, evenbelow the naive distribution mean (shown in Ta-ble 2); however, when combined with syntacticfeatures it performs well.In our second ablation experiment, we com-pared the performance of genre-independent andgenre-based features.
Since the genre-based fea-tures exploit knowledge of the genres of text usedin the MRP readability corpus, their utility issomewhat tailored to this specific corpus.
There-fore, it is useful to evaluate the performance of thesystem when genre information is not exploited.Of the lexical features described in subsection 5.3,the ratio of function words, ratio of pronoun wordsand all of the out-of-vocabulary rates except forthe base language model are genre-based features.Out of the language model features described inthe Subsection 5.2, all of the perplexities exceptfor the base language model and all of the poste-rior perplexities3 are genre-based features.
All ofthe remaining features are genre-independent.
Ta-ble 4 shows the results comparing these two fea-ture sets.
The genre-based features do well bythemselves but the rest of the features help fur-ther improve the performance.
While the genre-independent features by themselves do not exceedthe upper critical value of the naive judges?
dis-tribution, they are very close to it and still out-perform its mean value.
These results show thatfor a dataset like ours, which is composed of a mixof genres that themselves are indicative of read-ability, features that help identify the genre of atext improve performance significantly.4 For ap-plications mentioned in the introduction and re-lated work sections, such as filtering less readabledocuments from web-search, many of the inputdocuments could come from some of the commongenres considered in our dataset.In our final ablation experiment, we evaluated3Base model for posterior perplexities is computed usingother genre-based LMs (equation 3) hence it can not be con-sidered genre-independent.4We note that none of the genre-based features weretrained on supervised readability data, but were trained onreadily-available large unannotated corpora as shown in Ta-ble 1.552Feature Set CorrelationGenre-independent 0.6978Genre-based 0.7749All 0.8173Table 4: Comparison of genre-independent and genre-based feature sets.Feature Set By itself Ablatedfrom AllSundance features 0.5417 0.7993ESG features 0.5841 0.8118Perplexities 0.7092 0.8081Posterior perplexities 0.7832 0.7439Out-of-vocabulary rates 0.3574 0.8125All 0.8173 -Table 5: Ablations with some individual feature sets.the contribution of various individual feature sets.Table 5 shows that posterior perplexities performthe strongest on their own, but without them, theremaining features also do well.
When used bythemselves, some feature sets perform below thenaive judges?
distribution mean, however, remov-ing them from the rest of the feature sets de-grades the performance.
This shows that no indi-vidual feature set is critical for good performancebut each further improves the performance whenadded to the rest of the feature sets.6.3 Official Evaluation ResultsAn official evaluation was conducted by the eval-uation team SAIC on behalf of DARPA in whichthree teams participated including ours.
The eval-uation task required predicting the readability of150 test documents using the 390 training docu-ments.
Besides the correlation metric, two addi-tional metrics were used.
One of them computedfor a document the difference between the aver-age absolute difference of the naive judge scoresfrom the mean expert score and the absolute dif-ference of the machine?s score from the mean ex-pert score.
This was then averaged over all thedocuments.
The other one was ?target hits?
whichmeasured if the predicted score for a documentfell within the width of the lowest and the highestexpert scores for that document, and if so, com-System Correl.
Avg.
Diff.
Target HitsOur (A) 0.8127 0.4844 0.4619System B 0.6904 0.3916 0.4530System C 0.8501 0.5177 0.4641Upper CV 0.7423 0.0960 0.3713Table 6: Results of the systems that participated in theDARPA?s readability evaluation task.
The three metrics usedwere correlation, average absolute difference and target hitsmeasured against the expert readability scores.
The uppercritical values are for the score distributions of naive judges.puted a score inversely proportional to that width.The final target hits score was then computed byaveraging it across all the documents.
The uppercritical values for these metrics were computed ina way analogous to that for the correlation met-ric which was described before.
Higher score isbetter for all the three metrics.
Table 6 shows theresults of the evaluation.
Our system performedfavorably and always scored better than the up-per critical value on each of the metrics.
Its per-formance was in between the performance of theother two systems.
The performances of the sys-tems show that the correlation metric was the mostdifficult of the three metrics.7 ConclusionsUsing regression over a diverse combination ofsyntactic, lexical and language-model based fea-tures, we built a system for predicting the read-ability of natural-language documents.
The sys-tem accurately predicts readability as judged bylinguistically-trained expert human judges andexceeds the accuracy of naive human judges.Language-model based features were found to bemost useful for this task, but syntactic and lexicalfeatures were also helpful.
We also found that fora corpus consisting of documents from a diversemix of genres, using features that are indicativeof the genre significantly improve the accuracy ofreadability predictions.
Such a system could beused to filter out less readable documents for ma-chine or human processing.AcknowledgmentThis research was funded by Air Force ContractFA8750-09-C-0172 under the DARPA MachineReading Program.553ReferencesBernth, Arendse.
1997.
Easyenglish: A tool for improv-ing document quality.
In Proceedings of the fifth con-ference on Applied Natural Language Processing, pages159?165, Washington DC, April.Breiman, Leo.
1996.
Bagging predictors.
Machine Learn-ing, 24(2):123?140.Chall, J.S.
and E. Dale.
1995.
Readability Revisited: TheNew Dale-Chall Readability Formula.
Brookline Books,Cambridge, MA.Collins-Thompson, Kevyn and James P. Callan.
2004.
Alanguage modeling approach to predicting reading diffi-culty.
In Proc.
of HLT-NAACL 2004, pages 193?200.Fry, E. 1990.
A readability formula for short passages.
Jour-nal of Reading, 33(8):594?597.Gunning, R. 1952.
The Technique of Clear Writing.McGraw-Hill, Cambridge, MA.Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,and Maxine Eskenazi.
2007.
Combining lexical andgrammatical features to improve readability measures forfirst and second language texts.
In Proc.
of NAACL-HLT2007, pages 460?467, Rochester, New York, April.Heilman, Michael, Kevyn Collins-Thompson, and MaxineEskenazi.
2008.
An analysis of statistical models and fea-tures for reading difficulty prediction.
In Proceedings ofthe Third Workshop on Innovative Use of NLP for Build-ing Educational Applications, pages 71?79, Columbus,Ohio, June.
Association for Computational Linguistics.Kanungo, Tapas and David Orr.
2009.
Predicting the read-ability of short web summaries.
In Proc.
of WSDM 2009,pages 202?211, Barcelona, Spain, February.Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.Chissom.
1975.
Derivation of new readability formulasfor navy enlisted personnel.
Technical Report ResearchBranch Report 8-75, Millington, TN: Naval Air Station.Kneser, Reinhard and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Proc.
ofICASSP-95, pages 181?184.McCallum, Andrew and Kamal Nigam.
1998.
A comparisonof event models for naive Bayes text classification.
In Pa-pers from the AAAI-98 Workshop on Text Categorization,pages 41?48, Madison, WI, July.McCord, Michael C. 1989.
Slot grammar: A system forsimpler construction of practical natural language gram-mars.
In Proceedings of the International Symposium onNatural Language and Logic, pages 118?145, May.McLaughlin, G. H. 1969.
Smog: Grading: A new readabil-ity formula.
Journal of Reading, 12:639?646.Pitler, Emily and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proc.
of EMNLP 2008, pages 186?195,Waikiki,Honolulu,Hawaii, October.Quinlan, J. R. 1987.
Simplifying decision trees.
Interna-tional Journal of Man-Machine Studies, 27:221?234.Rasmussen, Carl and Christopher Williams.
2006.
GaussianProcesses for Machine Leanring.
MIT Press, Cambridge,MA.Riloff, E. and W. Phillips.
2004.
An introduction to the Sun-dance and Autoslog systems.
Technical Report UUCS-04-015, University of Utah School of Computing.Riloff, Ellen.
1996.
Automatically generating extractionpatterns from untagged text.
In Proc.
of 13th Natl.
Conf.on Artificial Intelligence (AAAI-96), pages 1044?1049,Portland, OR.Schwarm, Sarah E. andMari Ostendorf.
2005.
Reading levelassessment using support vector machines and statisticallanguage models.
In Proc.
of ACL 2005, pages 523?530,Ann Arbor, Michigan.Si, Luo and James P. Callan.
2001.
A statistical model forscientific readability.
In Proc.
of CIKM 2001, pages 574?576.Smola, Alex J. and Bernhard Scholkopf.
1998.
A tutorialon support vector regression.
Technical Report NC2-TR-1998-030, NeuroCOLT2.Stanley, Chen and Joshua Goodman.
1996.
An empiricalstudy of smoothing techniques for language modeling.
InProc.
of the 34th Annual Meeting of the Association forComputational Linguistics (ACL-96), pages 310?318.Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith.
1988.The Lexile Framework.
Durham, NC: MetaMetrics.Thelen, M. and E. Riloff.
2002.
A bootstrapping method forlearning semantic lexicons using extraction pattern con-texts.
In Proc.
of EMNLP 2002, Philadelphia, PA, July.Yang, Yiming and Xin Liu.
1999.
A re-examination of textcateogrization methods.
In Proc.
of 22nd Intl.
ACM SI-GIR Conf.
on Research and Development in InformationRetrieval, pages 42?48, Berkeley, CA.554
