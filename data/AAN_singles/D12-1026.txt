Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 276?285, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsN-gram-based Tense Models for Statistical Machine TranslationZhengxian Gong1 Min Zhang2 Chewlim Tan3 Guodong Zhou1?1 School of Computer Science and Technology, Soochow University, Suzhou, China 2150062 Human Language Technology, Institute for Infocomm Research, Singapore 1386323 School of Computing, National University of Singapore, Singapore 117417{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sgAbstractTense is a small element to a sentence, how-ever, error tense can raise odd grammars andresult in misunderstanding.
Recently, tensehas drawn attention in many natural languageprocessing applications.
However, most ofcurrent Statistical Machine Translation (SMT)systems mainly depend on translation modeland language model.
They never consider andmake full use of tense information.
In this pa-per, we propose n-gram-based tense modelsfor SMT and successfully integrate them in-to a state-of-the-art phrase-based SMT systemvia two additional features.
Experimental re-sults on the NIST Chinese-English translationtask show that our proposed tense models arevery effective, contributing performance im-provement by 0.62 BLUE points over a strongbaseline.1 IntroductionFor many NLP applications, such as event extractionand summarization, tense has been regarded as a keyfactor in providing temporal order.
However, tenseinformation has been largely overlooked by currentSMT research.
Consider the following example:SRC:?
?B$?e? , ?U?Ny3??,???N?I???m??
'X"REF:The embargo is a result of the Cold War and does notreflect the present situation nor the partnership between Chinaand the EU.MOSES: the embargo is the result of the cold war, not reflectthe present situation, it did not reflect the partnership with theeuropean union.?
*Corresponding author.Although the translated text produced by Moses1is understandable, it has very odd tense combinationfrom the grammatical aspect, i.e.
with tense incon-sistency (is/does in REF vs. is/did in Moses).
Ob-viously, slight modification, such as changing ?is?into ?was?, can much improve the readability of thetranslated text.
It is also interesting to note that suchmodification can much affect the evaluation.
If wechange ?did?
to ?does?, the BLEU-4 score increasesfrom 22.65 to 27.86 (as matching the 4-gram ?doesnot reflect the?
in REF).
However, if we change ?is?to ?was?, the BLEU score decreases from 22.65 to21.44.The above example seems special.
To testify itsimpact on SMT in wider range, we design a specialexperiment based on the 2005 NIST MT data (seesection 6.1).
This data has 4 references.
We chooseone reference and modify its sentences with errortense2.
After that, we use other 3 references to mea-sure this reference.
The modified reference leads toa sharp drop in BLEU-4 score, from 52.46 to 50.27in all.
So it is not a random phenomenon that tensecan affect translation results.The key is how to detect tense errors and choosecorrect tenses during the translation procedure.
Bycarefully comparing the references with Moses out-put, we obtain the following useful observations,Observation(1): to most simple sentences, coor-dinate verbs should be translated with the same tensewhile they have different tense in Moses output;Observation(2): to some compound sentences,1http://www.statmt.org/moses/2Such changes are small by mainly modifying one auxiliaryverb for a sentence, such as ?is?
was?, ?has?
had?.276the subordinate clause should have the consisten-t tense with its main clause while Moses fails;Observation(3): the diversity of tense usage in adocument is common.
However, in most cases, theneighbored sentences tends to share the same maintense.
In some extreme examples, one tense (past orpresent), even dominates the whole document.One possible solution to model above observa-tions is using rules.
Dorr (2002) refers to six ba-sic English tense structures and defines the possiblepaired combinations of ?present, past, future?.
Butthe practical cases are very complicated, especial-ly in news domain.
There are a lot of complicat-ed sentences in news articles.
Our preliminary in-vestigation shows that such six paired combinationscan only cover limited real cases in Chinese-EnglishSMT.This paper proposes a simple yet effective methodto model above observations.
For each target sen-tence in the training corpus, we first parse it and ex-tract its tense sequence.
Then, a target-side tensen-gram model is constructed.
Such model can beused to estimate the rationality of tense combina-tion in a sentence and thus supervise SMT to reducetense inconsistency errors against Observations (1)and (2) in the sentence-level.
In comparison, Ob-servation (3) actually reflects the tense distributionsamong one document.
After extracting each maintense for each sentence, we build another tense n-gram model in the document-level.
For clarity, thispaper denotes document-level tense as ?inter-tense?and sentence-level tense as ?intra-tense?.After that, we propose to integrate such tensemodels into SMT systems in a dynamic way.
Itis well known there are many errors in the currentMT output (David et al2006).
Unlike previouslymaking trouble with reference texts, the BLEU-4 s-core cannot be influenced obviously by modifyinga small part of abnormal sentences in a static way.In our system, both inter-tense and intra-tense mod-el are integrated into a SMT system via additionalfeatures and thus can supervise the decoding proce-dure.
During decoding, once some words with cor-rect tense can be determined, with the help of lan-guage model and other related features, the smal-l component??tense?
?can affect surrounding wordsand improve the performance of the whole sentence.Our experimental results (see the examples in Sec-tion 6.4) show the effectiveness of this way.Rather than the rule-based model, our models arefully statistical-based.
So they can be easily scaledup and integrated into either phrase-based or syntax-based SMT systems.
In this paper, we employ astrong phrase-based SMT baseline system, as pro-posed in Gong et al2011), which uses document astranslation unit, for better incorporating document-level information.The rest of this paper is organized as follows: Sec-tion 2 reviews the related work.
Section 3 and 4 arerelated to tense models.
Section 3 describes the pre-processing work for building tense models.
Section4 presents how to build target-side tense models anddiscuss their characteristics.
Section 5 shows ourway of integrating such tense models into a SMTsystem.
Session 6 gives the experimental results.
Fi-nally, we conclude this paper in Section 7.2 Related WorkIn this section, we focus on related work on integrat-ing the tense information into SMT.
Since both inter-and intra-tense models need to analyze and extracttense information, we also give a brief overview ontense prediction (or tagging).2.1 Tense PredictionThe tense prediction task often needs to build a mod-el based on a large corpus annotated with temporalrelations and thus its focus is on how to recognize,interpret and normalize time expressions.
As a rep-resentative, Lapata and Lascarides (2006) proposeda simple yet effective data-intensive approach.
Inparticular, they trained models on main and subor-dinate clauses connected with some special tempo-ral marker words, such as ?after?
and ?before?, andemployed them in temporal inference.Another typical task is cross-lingual tense pred-ication.
Some languages, such as English, are in-flectional, whose verbs can express tense via certainstems or suffix, while others, such as Chinese of-ten lack inflectional forms.
Take Chinese to Englishtranslation as example, if Chinese text contains par-ticle word ?
(Le)?, the nearest Chinese verb prefersto be translated into English verb with the past tense.Ye and Zhang (2005), Ye et al2007) and Liu et al(2011) focus on labeling the tenses for keywords in277source-side language.Ye and Zhang (2005) first built a small amoun-t of manually-labeled data, which provide the tensemapping from Chinese text to English text.
Then,they trained a CRF-based tense classifier to labeltense on Chinese documents.
Ye et al2007) fur-ther reported that syntactic features contribute mostto the marking of aspectual information.
Liu et al(2011) proposed a parallel mapping method to au-tomatically generate annotated data.
In particular,they used English verbs to label tense informationfor Chinese verbs via a parallel Chinese-English cor-pus.It is reasonable to label such source-side verb tosupervise the translation process since the tense ofEnglish sentence is often determined by verbs.
Theproblem is that due to the diversity of English ver-b inflection, it is difficult to map such Chinese tenseinformation into the English text.
To our best knowl-edge, although above works attempt to serve forSMT, all of them fail to address how to integratethem into a SMT system.2.2 Machine Translation with TenseDorr (1992) described a two-level knowledge repre-sentation model based on Lexical Conceptual Struc-tures (LCS) for machine translation which integratesthe aspectual information and the lexical-semanticinformation.
Her system is based on an inter-lingualmodel and does not belong to a SMT system.Olsen et al2001) relied on LCS to generateappropriately-tensed English translations for Chi-nese.
In particular, they addressed tense reconstruc-tion on a binary taxonomy (present and past) forChinese text and reported that incorporating lexicalaspect features of telicity can obtain a 20% to 35%boost in accuracy on tense realization.Ye et al2006) showed that incorporating latentfeatures into tense classifiers can boost the perfor-mance.
They reported the tense resolution resultsbased on the best-ranked translation text producedby a SMT system.
However, they did not report thevariation of translation performance after introduc-ing tense information.3 Preprocessing for Tense ModelingIn this paper, tense modeling is done on the target-side language.
Since our experiments are doneon Chinese to English SMT, our tense models arelearned only from the English text.
In the literature,the taxonomy of English tenses typically includesthree basic tenses (i.e.
present, past and future) plustheir combination with the progressive and perfec-tive aspects.
Here, we consider four basic tenses:present, past, future and UNK (unknown) and ignorethe aspectual information.
Furthermore, we assumethat one sentence has only one main tense but maybehas many subordinate tenses.This section describes the preprocessing work ofbuilding tense models, which mainly involves howto extract tense sequence via tense verbs.3.1 Tense VerbsLapata et al006) used syntactic parse trees to findclauses connected with special aspect markers andcollected them to train some special classifiers fortemporal inference.
Inspired by their work, we usethe Stanford parser3 to parse tense sequence for eachsentence.Take the following three typical sentences as ex-amples, (a) is a simple sentence which contains twocoordinate verbs, while (b) and (c) are compoundsentences and (b) contains a quoted text.
(a) Japan?s constitution renounces the right to go to war andprohibits the nation from having military forces except for self-defense.
(b) ?We also hope Hong Kong will not be affected by diseaseslike the severe acute respiratory syndrome again!?
, added Ms.Yang.
(c) Cheng said he felt at home in Hong Kong and he sincerelywished Hong Kong more peaceful and more prosperous.Figure 1 shows the parse tree with Penn Treebankstyle for each sentence, which has strict level struc-tures and POS tags for all the terminal words.
Here,the level structures mainly contribute to extract maintense for each sentence (to be described in Section3.2) and POS tags are utilized to detect tense verbs,i.e.
verbs with tense information.Normally, POS tags in the parse tree can distin-guish five different forms of verbs: the base form(tagged as VB), and forms with overt endings D for3http://nlp.stanford.edu/software/lex-parser.shtml278Figure 1: The Stanford parse trees with Penn Treebank stylepast tense, G for present participle, N for past par-ticiple, and Z for third person singular present.
It isworth noting that VB, VBG and VBN cannot deter-mine the specific tenses by themselves.
In addition,the verbs with POS tag ?MD?
need to be special-ly considered to distinguish future tense from othertenses.Algorithm 1 illustrates how to determine whattense a node has.
If the return value is not ?UNK?,the node belongs to a tense verb.Algorithm 1 Determine the tense of a node.Input:The TreeNode of one parse tree, leafnode;Output:The tense, tense;1: tense = ?UNK ?
?2: Obtaining the POS tag lpostag from leafnode;3: Obtaining the word lword from leafnode;4: if (lpostag in [?V BP ?
?, ?V BZ ??])
then5: tense = ?present?
?6: else if (lpostag == ?V BD??])
then7: tense = ?past?
?8: else if (lpostag == ?MD??])
then9: if (lword in [?will?
?, ?ll?
?, ?shall??])
then10: tense = ?future?
?11: else if (lword in [?would?
?, ?could??])
then12: tense = ?past?
?13: else14: tense = ?present?
?15: end if16: end if17: return tense;3.2 Tense Extraction Based on Tense VerbsAs described in Section 1, the inter-tense(document-level) refers to the main tense ofone sentence and the intra-tense (sentence-level)corresponds to all tense sequence of one sentence.This section introduces how to recognize the maintense and extract all useful tense sequence for eachsentence.The idea of determining the main tense is to findthe tense verb located in the top level of a parse tree.According to the Penn Treebank style, the methodto determine the main tense can be described as fol-lows:(1) Traverse the parse tree top-down until a tree nodecontaining more than one child is identified, denot-ed as Sm .
(2) Consider each child of Sm with tag ?VP?, recursive-ly traverse such ?VP?
node to find a tense verb.
Iffound, use it as the main tense and return the tense;if not, go to step (3).
(3) Consider each child of Sm with tag ?S?, which ac-tually corresponds to subordinate clause of this sen-tence.
Starting from the first subordinate clause, ap-ply the similar policy of step (2) to find the tenseverb.
If not found, search remaining subordinateclauses.
(4) If no tense verb found, return ?UNK?
as the maintense.Here, ?VP?
nodes dominated by Sm directly arepreferred over those located in subordinate clauses.This is to ensure that the main tense is decided bythe top-level tense verb.279Take Figure 1 as an example, the main tense ofsentence (a) and (b) can be determined only by step(2).
The tense verb of ?
(VBZ renounces)?
dominat-ed in the ?VP?
tag determines that (a) is in presenttense.
Similarly the node ?
(VBD added)?
indicatesthat (b) is in past tense.
Sentence (c) needs to be fur-ther treated by step (3) since there is no ?VP?
nodesdominated by Sm directly.
The node ?
(VBD said)?located in the first subordinate clause shows its maintense is ?past?.The next task is to extract the tense sequence foreach sentence.
They are determined by all tenseverbs in this sentence according to the strict top-down order.
For example, the tense sequence ofsentence (a), (b) and (c) are {present, present},{present, future, past} and {past, past, past}.
In or-der to explore whether the main tense of intra-tensemodel has an impact on SMT or not, we introducea special marker ?*?
to denote the main tense.
Sothe tense sequence marked with main tense of (a),(b) and (c) are {present*, present},{present, future,past*} and {past*, past, past}.
It is worth noting, theintra-tense model (see Section 4) based on the lattertense sequence is different to the former.4 N-gram-based Tense Models4.1 Tense N-gram EstimationAfter applying the previous method to extract tensefor an English text corpus, we can obtain a big tensecorpus.Given the current tense is indexed as ti, we callthe previous n ?
1 tenses plus the current tense astense n-gram.Based on the tense corpus, tense n-gram statisticscan be done according to the Formula 1.P (ti|t(i?
(n?1)), ..., t(i?1)) =count(t(i?
(n?1)), .
.
.
, t(i?1), ti)count(t(i?
(n?1)), ..., t(i?1))(1)Here, the function of ?count?
return the tense n-gramfrequency.
In order to avoid doing specific smooth-ing work, we estimate tense n-gram probability us-ing SRI language modeling (SRILM) tool (Stolcke,2002).To compute the probability of intra-tense n-gram,we first extract all tense sequence for each sentenceand put them into a new file.
Based on this new file,we can get the intra-tense n-gram model via SRILMtool.To compute the probability of inter-tense n-gram,we need to extract the main tense for each sentenceat first.
Then, for each document, we re-organizedthe main tenses of all sentences into a special line.After putting all these special lines into a new file,we can use SRILM to obtain the inter-tense n-grammodel.4.2 Characteristic of Tense N-gram ModelsWe construct n-gram-based tense models on EnglishGigaword corpus (LDC2003T05).
This corpus isused to build language model for most SMT sys-tems.
It includes 30221 documents (we remove suchfiles: file size is less than 1K or the number of con-tinuous ?UNK?
tenses is greater than 5).Figure 2 shows the unigram and bigram probabil-ities (Log10-style) for intra-tense and inter-tense.The part (a) and (c) in Figure 2 refer to unigram.The horizontal axis indicts tense type, and the ver-tical axis shows its probabilities.
The parts (a) and(c) also indicate ?present?
and ?past?
are two maintense types in news domain.The part (b) and (d) refer to bigram.
The horizon-tal axis indicts history tense.
Each different color-ful bar indicts one current tense.
The vertical axisshows the transfer possibilities from a history tenseto a current tense.The part (b)4 reflects transfer possibilities of tensetypes in one sentence.
It also slightly reflects somelinguistic information.
For example, in one sen-tence, the probability of co-occurrence of ?present?
present?, ?past ?
past?
and ?future ?
present?is more than other combinations, which can be a-gainst tense inconsistency errors described in Obser-vation (1) and (2) (see Section 1).
However, it seem-s strange that ?present?
past?
exceeds ?present?future?.
We checked our corpus and found a lot ofsentences like this?
?the bill has been .
.
.
, he said.
?.The part (d) shows tense type can be switched be-tween two neighbored sentences.
However, it showsthe strong tendency to use the same tense type for4The co-occurrence of the ?UNK?
tense and other tensetypes in one sentence cannot happen, so the ?UNK?
tense isomitted.280Figure 2: statistics of intra-tense and inter-tense N-gramneighbored sentences.
This statistics conform to theprevious observation (3) very much.5 Integrating N-gram-based Tense Modelsinto SMTIn this section, we discuss how to integrate the pre-vious tense models into a SMT system.5.1 Basic phrase-based SMT systemIt is well known that the translation process of SMTcan be modeled as obtaining the best translation eof the source sentence f by maximizing followingposterior probability(Brown et al1993):ebest = argmaxeP (e|f)= argmaxeP (f |e)Plm(e)(2)where P (e|f) is a translation model and Plm is alanguage model.Our baseline is a modified Moses, which followsKoehn et al2003) and adopts similar six groupsof features.
Besides, the log-linear model ( Och andNey, 2000) is employed to linearly interpolate thesefeatures for obtaining the best translation accordingto the formula 3:ebest = argmaxeM?m=1?mhm(e, f) (3)where hm(e, f) is a feature function, and ?m isthe weight of hm(e, f) optimized by a discrimina-tive training method on a held-out development da-ta(Och, 2003).5.2 The Workflow of Our SystemOur system works as follows:When a hypothesis has covered all source-sidewords during the decoding procedure, the decoderfirst obtains tense sequence for such hypothesis andcomputes intra-tense feature Fs(see Section 5.3).
Atthe same time, it recognizes the main tense of thishypothesis and associate the main tense of previoussentence to compute inter-tense feature Fm (see Sec-tion 5.3).Next, the decoder uses such two additional featurevalues to re-score this hypothesis automatically andchoose one hypothesis with highest score as the finaltranslation.After translating one sentence, the decoder cachesits main tense and pass it to the next sentence.When one document has been processed, the de-coder cleans this cache.In order to successfully implement above work-flow, we should firstly design some related features,then resolve another key problem of determiningtense (especially main tense) for SMT output.
Theyare described in Section 5.3 and 5.4 respectively.5.3 Two Additional FeaturesAlthough the previous tense models show strongtendency to use the consistent tenses for one sen-tence or one document, other tense combinations al-so can be permitted.
So we should use such modelsin a soft and dynamic way.
We design two features:inter-tense feature and intra-tense feature.
And theweight of each feature is tuned by the MERT scriptin Moses packages.Given main tense sequence of one documen-t t1, .
.
.
, tm, the inter-tense feature Fm is calculatedaccording to the following formula:Fm =m?i=2P (ti|ti?
(n?1), .
.
.
, t(i?1)) (4)The P (?)
of formula 4 can be estimated by the for-mula 1.
It is worth noting the first sentence of one281document often scares tense information since it cor-responds to the title at most cases.
To the first sen-tence, the P (?)
value is set to 14 (4 tense types).Given tense sequence of one sentences1, .
.
.
, se (e > 1), the intra-tense feature Fsis calculated as follows:Fs = e?1???
?e?i=2P (si|si?
(n?1), .
.
.
, s(i?1)) (5)Here the square-root operator is used to avoid pun-ishing translations with long tense sequence.
It isworth noting if the sentence only contains one tense,the P (?)
value of formula 5 is also set to 14 .Since the average length of intra-tense sequenceis about 2.5, we mainly consider intra-tense bigrammodel and thus n equals to 2.
55.4 Determining Tense For SMT OutputThe current SMT systems often produce odd transla-tions partly because of abnormal word ordering anduncompleted text etc.
For these abnormal translatedtexts, the syntactic parser cannot work well in ourinitial experiments, so the previous method to parsemain tense and tense sequence of regular texts can-not be applied here too.Fortunately, the solely utilization of Stanford POStagger for our SMT output is not bad although it hasthe same issues described in Och et al2002).
Thereason may be that phrase-based SMT contains shortcontexts that POS tagger can utilize while the syntaxparser fails.Once obtaining a completed hypothesis, the de-coder will pass it to the Stanford POS tagger and ac-cording to tense verbs to get alense sequence forthis hypothesis.
However, since the POS tagger cannot return the information about level structures, thedecoder cannot recognize the main tense from suchtense sequence.Liu et al2011) once used target-side verbs to la-bel tense of source-side verbs.
It is natural to consid-er whether Chinese verbs can provide similar cluesin an opposite direction.Since Chinese verbs have good correlation withEnglish verbs (described in section 6.2), we obtain5In our experiment, the intra-tense bigram model can ob-tain the comparable performance to the trigram model.
And theinter-tense trigram model can not exceed the bigram one.Figure 3: trees for parallel sentencesmain tense for SMT output according to such tenseverb, which corresponds to the ?VV?
(Chinese POSlabels are different to English ones, ?VV?
refers toChinese verb) node in the top level of the source-sideparse tree.
Take Figure 3 as an example, the Englishnode ?
(VBD announced)?
is a tense verb which cantell the main tense for this English sentence.
TheChinese verb ?(VV??)?
in the top-level of theChinese parse tree is just the corresponding part forthis English verb.So, before translating one sentence, the decoderfirst parses it and records the location of one Chinese?VV?
node which located in the top-level, denotesthis location as Sarea.Once a completed hypothesis is generated, ac-cording to the phrase alignment information, the de-coder can map Sarea into the English location Tareaand obtain the main tense according to the POS tagsin Tarea .If Tarea does not contain tense verb, such as theverb POS tags in the list of {VB, VBN, VBG},which cannot tell tense type by themselves, our sys-tem permits to find main tense in the left/right 3words neighbored to Tarea.
And the proportion thatthe top-level verb of Chinese has a verb correspon-dence in English can reach to 83% in this way.6 Experimentation6.1 Experimental Setting for SMTIn our experiment, SRI language modeling toolk-it was used to train a 5-Gram general languagemodel on the Xinhua portion of the Gigaword cor-pus.
Word alignment was performed on the train-ing parallel corpus using GIZA++ ( Och and Ney,2000) in two directions.
For evaluation, the NIST282BLEU script (version 13) with the default setting isused to calculate the BLEU score (Papineni et al2002), which measures case-insensitive matching of4-grams.
To see whether an improvement is statisti-cally significant, we also conduct significance testsusing the paired bootstrap approach (Koehn, 2004).In this paper, ?***?
and ?**?
denote p-values equalto 0.05, and bigger than 0.05, which mean signifi-cantly better, moderately better respectively.Corpus Sentences DocumentsRole NameTrain FBIS 228455 10000Dev NIST2003 919 100Test NIST2005 1082 100Table 1: Corpus statisticsWe use FBIS as the training data, the 2003 NISTMT evaluation test data as the development data, andthe 2005 NIST MT test data as the test data.
Table 1shows the statistics of these data sets (with documentboundaries annotated).6.2 The Correlation of Chinese Verbs andEnglish VerbsIn this section, an additional experiment is designedto show English Verbs have close correspondencewith Chinese Verbs.We use the Stanford POS tagger to tag the Chi-nese and English sentences in our training corpusrespectively at first.
Then we utilize Giza++ to buildalignment for these special Word-POS pairs.
Ac-cording to the alignment results, we find the corre-sponding relation for some special POS tags in twolanguages.Chinese Verb POS English POS NumberVV Verb VBD 89830POS VBP 27276VBZ 32588MD 40378VBG 86025VBN 75019VB 153596In sum: 504712Other Non-Verb 149318Verb Corresponding Ratio 0.77169Table 2: The Chinese and English Verb Pos AlignmentThe ?Number?
column of Table 2 shows the num-bers of Chinese words with ?VV?
tag correspond-ing to English words with different verb POS tags.We found Chinese verbs have more than 77% possi-bilities to align to English verbs in total.
However,our method will fail when some special Chinese sen-tences only contain noun predicates.6.3 Experimental ResultsAll the experiment results are showed on the table 3.Our Baseline is a modified Moses.
The major modi-fication is input and output module in order to trans-late using document as unit.
The performance of ourbaseline exceeds the baseline reported by Gong et al(2011) about 2 percent based on the similar trainingand test corpus.System BLEU BLEU on Test(%)Dev(%) BLEU NISTMoses Md(Baseline) 29.21 28.30 8.4528Baseline+Fm 30.56 28.87(***) 8.7935Baseline+Fs 31.28 28.61(**) 8.5645Baseline+Fs(?)
31.04 28.74(**) 8.6271Baseline+Fm+Fs 31.75 28.88(***) 8.7987Baseline+Fm+Fs(*) 31.42 28.92(***) 8.8201Table 3: The performance of using different feature com-binationsThe system denoted as ?Baseline+Fm?
integratesthe inter-tense feature.
The performance boosts0.57(***) in BLEU score.The system denoted as ?Baseline+Fs?
integratesthe intra-tense feature into the baseline.
The im-provement is less than the inter-tense model, on-ly 0.31(**).
It seems the tenses in one sentencehas more flexible formats than the document-levelones.
It is worth noting, this method can gain high-er performance on the develop data than the one of?Baseline+Fm?
while fail to improve the test data.Maybe the related weight is tuned over-fit.The system denoted as ?Baseline+Fs(*)?
is s-lightly different from ?Baseline+Fs?.
This experi-ment is to check whether the main tense has an im-pact on intra-tense model or not (see Section 3.2).Here, the intra-tense model based on the tense se-quence with main tense marker is slightly differentto the model showed in Figure 2.
The results areslightly better than the previous system by 0.13.Finally, we use the two features together(Baseline+Fm+Fs).
The best way improved theperformance by 0.62(***) in BLEU score over ourbaseline.2836.4 DiscussionTable 4 shows special examples whose intra-tensesare changed in our proposed system.
The exam-ple 1 and 2 show such modification can improvethe BLEU score but the example 3 obtains negativeimpact.
From these examples, we can see not onlytense verbs have changed but also their surroundingwords have subtle variation.No.
BLEU Translation sentence1 8.64 Baseline: the gulf countries , the bahraini royal fam-ily members by the military career of part of thebanned to their marriage stories like children , havebecome the theme of television films .19.71 Ours: the gulf country is a member of the bahrainiroyal family , a risk by military career risks part ofthe banned to their marriage like children , has be-come a story of the television and film industry .2 17.16 Baseline:economists said that the sharp appreciationof the euro , in the recent investigation continues tohave an impact on economic confidence , it is esti-mated that the economy is expected to rebound topick up .24.25 Ours: economists said that the sharp appreciation ofthe euro , in the recent investigation continued tohave an impact on economic confidence and there-fore no reason to predict the economy expected topick up a rebound .3 73.03 Baseline: the middle east news agency said that , af-ter the concerns of all parties concerned in the mid-dle east peace process , israel and palestine , egypt ,the united states , russia and several european coun-tries will hold a meeting in washington .72.95 Ours: the middle east news agency said that after theconcerns of all parties in the middle east peace pro-cess , israel and palestine , egypt , the united states ,russia and several european countries held a meetingin washington .Table 4: Examples with tense variation using intra-tensemodelFrom the results showed on Table 3, thedocument-level tense model seems more effectivethan the sentence-level one.
We manually chooseand analyzed 5 documents with significant improve-ment in the test data.
The part (a) of Figure 4 showsthe main tense distributions of one reference.
Themain tense distributions for the baseline and our pro-posed system are showed in the part (b) and (c) re-spectively.
These documents have different numbersof sentences but all less than 10.
The vertical axis in-dicates different tense: 1 to ?past?, 2 to ?present?, 3to ?future?
and 4 to ?UNK?.
It is obvious that oursystem has closer distributions to the ones of thisreference.The examples in Table 5 indicate the joint impactof inter-tense and intra-tense model on SMT.
Sen-Src:1)????K???^???
, |????
; ????"2)nVd")?|?+ECnd?
?Oc  ??W?
?|?
,??Loc5????gONnVd"??+<??¥?!
?
;"Ref:1)Israeli settlers blockaded a major road to protest a mortar attackon the settlement area.2)PLO leader Abbas had also been allowed to go to the West Banktown of Bethlehem , which is the first time in the past four yearsIsraeli authorities have allowed a senior Palestinian leader to attendChristmas celebrations.Baseline:1)israel has imposed a main road to protest by mortars attack .2)the palestinian leader also visited the west bank cities and townsto bethlehem , which in the past four years , the israeli authoritiesallowed the palestinian leading figures attended the ceremony .Ours:1)israel has imposed a main road to protest against the mortars at-tack .2)leader of the palestinian liberation organization have also beenallowed to go to the west bank towns , bethlehem in the past fouryears .
this is the first time the israeli authorities allow palestinianleading figures attended the ceremony .Table 5: the joint impact of inter- tense and intra-tensemodels on SMTtence 1) and 2) are two neighbored sentences in onedocument.
Both the reference and our output tendto use the same main tense type, but the former is in?past?
tense and the latter is in ?present?
tense.
Thebaseline cannot show such tendency.
Although ourmain tense is different to the reference one, the con-sistent tenses in document level bring better trans-lation results than the baseline.
And the tenses insentence level also show better consistency than thebaseline.7 ConclusionThis paper explores document-level SMT from thetense perspective.
In particular, we focus on how tobuild document-level and sentence-level tense mod-els and how to integrate such models into a popularSMT system.Compared with the inter-tense model which great-ly improves the performance of SMT, the intra-tensemodel needs to be further explored.
The reasons aremany folds, e.g.
the failure to exclude quoted textswhen modeling intra-tense, since tenses in quotedtexts behave much diversely from normal texts.
Inthe future work, we will focus on modeling intra-tense variation according to specific sentence typesand using more features to improve it.284Figure 4: the comparison of the inter-tense distributions for reference, baseline and our proposed systemAcknowledgmentsThis research is supported by part by NUS FRCGrant R252-000-452-112, the National Natural Sci-ence Foundation of China under grant No.90920004and 61003155, the National High Technology Re-search and Development Program of China (863Program) under grant No.2012AA011102.ReferencesP.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra and R.L.Mercer.
1992.
The Mathematics of Statistical Ma-chine Translation: Parameter Estimation.
Computa-tional Linguistics, 19(2):263-311.Vilar David, Jia Xu, DH`aro L. F., and Hermann Ney.2006.
Error Analysis of Machine Translation Output.In Proceedings of the 5th International Conference onLanguage Resources and Evaluation, pages 697-702,Genoa, Italy.Bonnie J. Dorr.
1992.
A parameterized approach tointegrating aspect with lexical-semantics for machinetranslation.
In Proceedings of of ACL-2002, pages257-264.Bonnie J. Dorr and Terry Gaasterland.
2002.
Constraintson the Generation of Tense, Aspect, and ConnectingWords from Temporal Expressions.
Technical Reportsfrom UMIACS.Zhengxian Gong, Min Zhang and Guodong Zhou.2011.
Cached-based Document-level Statistical Ma-chine Translation.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 909-919.Philipp Koehn, Franz Josef Och ,and DanielMarcu.
2003.Statistical Phrase-Based Translation.
In Proceedingsof NAACL-2003, pages 48-54.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, pages 388-395.Mirella Lapata, Alex Lascarides.
2006.
LearningSentence-internal Temporal Relations.
Journal of Ar-tificial Intelligence Research, 27:85-117.Feifan Liu, Fei Liu and Yang Liu.
2011.
Learning fromChinese-English Parallel Data for Chinese Tense Pre-diction.
In Proceedings of IJCNLP-2011, pages 1116-1124,Chiang Mai, Thailand.Franz Josef Och and Hermann Ney.
2000.
Improved Sta-tistical Alignment Models.
In Proceedings of of ACL-2000, pages 440-447.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,et.al.
2002.
A smorgasbord of Features for StatisticalMachine Translation.
In Proceedings of NAACL-2004,pages 440-447.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedings ofACL-2003,pages 160-167, Sapporo, Japan,July.Mari Olsen, David Traum,Carol Van Ess-Dykema andAmy Weinberg.
2001.
Implicit Cues for Explicit Gen-eration: Using Telicity as a Cue for Tense Structure ina Chinese to English MT System.
In Proceedings ofMT Summit VIII, Santiago de Compostella, Spain.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU: A Method for Automatic E-valuation of Machine Translation.
In Proceedings ofACL-2002, pages 311-318.Andreas Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Proceedings of the Internation-al Conference on Spoken Language Processing,pages901-904.Yang Ye and Zhu Zhang.
2005.
Tense tagging for verbsin cross-lingual context: A case study.
In Proceedingsof IJCNLP-2005, pages 885-895.Yang Ye, V.li Fossum, Steven Abney.
2006.
Laten-t features in Temporal Reference Translation.
FifthSIGHAN Workshop on Chinese Language Processing,pages 48-55.Yang Ye, Karl-Michael Schnelder, Steven Abney.
2007.Aspect Marker Generation in English-to-Chinese Ma-chine Translation.
Proceedings of MT Summit XI,pages 521-527,Copenhagen, Denmark.285
