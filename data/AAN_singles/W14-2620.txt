Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 119?127,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsEvaluating Sentiment Analysis Evaluation: A Case Study in SecuritiesTradingSiavash Kazemian Shunan ZhaoDepartment of Computer ScienceUniversity of Toronto{kazemian,szhao,gpenn}@cs.toronto.eduGerald PennAbstractThere are numerous studies suggestingthat published news stories have an im-portant effect on the direction of the stockmarket, its volatility, the volume of trades,and the value of individual stocks men-tioned in the news.
There is even somepublished research suggesting that auto-mated sentiment analysis of news doc-uments, quarterly reports, blogs and/orTwitter data can be productively used aspart of a trading strategy.
This paperpresents just such a family of tradingstrategies, and then uses this application tore-examine some of the tacit assumptionsbehind how sentiment analyzers are gen-erally evaluated, in spite of the contexts oftheir application.
This discrepancy comesat a cost.1 IntroductionAmidst the vast amount of user-generated andprofessionally-produced textual data, analystsfrom different fields are turning to the natural lan-guage processing community to sift through theselarge corpora and make sense of them.
Interna-tional collaborative projects such as the Digginginto Data Challenge (2012) or the Big Data Con-ference sponsored by the Marketing Science In-stitute (2012) are some recent examples of theseinitiatives.The proliferation of opinion-rich text on theWorld Wide Web, which includes anything fromproduct reviews to political blog posts, led to thegrowth of sentiment analysis as a research fieldmore than a decade ago.
The market need to quan-tify opinions expressed in social media and theblogosphere has provided a great opportunity forsentiment analysis technology to make an impactin many sectors, including the financial industry,in which interest in automatically detecting newssentiment in order to inform trading strategies ex-tends back at least 10 years.
In this case, senti-ment takes on a slightly different meaning; posi-tive sentiment is not the emotional and subjectiveuse of laudatory language.
Rather, a news articlethat contains positive sentiment is optimistic aboutthe future financial prospects of a company.Zhang and Skiena (2010) have shown that newssentiment can effectively inform simple marketneutral trading algorithms, producing a maximumyearly return of around 30%, and even more whenusing sentiment from blogs and Twitter data.
Theydid so, however, without an appropriate baseline,making it very difficult to appreciate the signif-icance of this number.
Using a very standardsentiment analyzer, we are able to garner annual-ized returns over twice that percentage (70.1%),and in a manner that highlights some of the bet-ter design decisions that Zhang and Skiena (2010)made, viz., their decision to use raw SVM scoresrather than discrete positive or negative senti-ment classes, and their decision to go long (resp.,short) in the n best- (worst-) ranking securitiesrather than to treat all positive (negative) securi-ties equally.
We trade based upon the raw SVMscore itself, rather than its relative rank within abasket of other securities, and tune a threshold forthat score that determines whether to go long, neu-tral or short.
We sample our stocks for both train-ing and evaluation with and without survivor bias,the tendency for long positions in stocks that arepublicly traded as of the date of the experiment topay better using historical trading data than longpositions in random stocks sampled on the trad-ing days themselves.
Most of the evaluations ofsentiment-based trading either unwittingly adoptthis bias, or do not need to address it because theirreturns are computed over historical periods sobrief.
We also provide appropriate trading base-lines as well as Sharpe ratios to attempt to quan-119tify the relative risk inherent to our experimen-tal strategies.
As tacitly assumed by most of thework on this subject, our trading strategy is notportfolio-limited, and our returns are calculated ona percentage basis with theoretical, commission-free trades.Our motivation for undertaking this study hasbeen to reappraise the evaluation standards forsentiment analyzers.
It is not at all uncommonwithin the sentiment analysis community to eval-uate a sentiment analyzer with a variety of classi-fication accuracy or hypothesis testing scores suchas F-measures, kappas or Krippendorff alphas de-rived from human-subject annotations, even whenmore extensional measures are available.
In secu-rities trading, this would of course include actualmarket returns from historical data.
With Holly-wood films, another popular domain for automaticsentiment analysis, one might refer to box-officereturns or the number of award nominations thata film receives rather than to its star-rankings onreview websites where pile-on and confirmationbiases are widely known to be rampant.
Are theopinions of human judges, paid or unpaid, a suf-ficient proxy for the business cases that actuallydrive the demand for sentiment analyzers?We regret to report that they are not.
We haveeven found a particular modification to our stan-dard financial sentiment analyzer that, when eval-uated against an evaluation test set sampled fromthe same pool of human-subject annotations asthe analyzer?s training data, returns significantlypoorer performance, but when evaluated againstactual market returns, yields significantly betterperformance.
This should worry researchers whorely on classification accuracies and hypothesistests relative to human-subject data, because theimprovements that they report, whether based onbetter feature selection or different pattern recog-nition algorithms, may in fact not be improve-ments at all.The good news, however, is that, based upon ourexperience within this particular domain, trainingon human-subject annotations and then tuning onmore extensional data, in cases where the latterare less abundant, seems to suffice for bringingthe evaluation back to reality.
A likely machine-learning explanation for this is that whenever twounbiased estimators are pitted against each other,they often result in an improved combined perfor-mance because each acts as a regularizer againstthe other.
If true, this merely attests to the relativeindependence of task-based and human-annotatedknowledge sources.
A more HCI-oriented viewwould argue that direct human-subject annotationsare highly problematic unless the annotations havebeen elicited in manner that is ecologically valid.When human subjects are paid to annotate quar-terly reports or business news, they are paid re-gardless of the quality of their annotations, thequality of their training, or even their degree ofcomprehension of what they are supposed to bedoing.
When human subjects post film reviews onweb-sites, they are participating in a cultural activ-ity in which the quality of the film under consider-ation is only one factor.
These sources of annota-tion have not been properly controlled.2 Related Work in Financial SentimentAnalysisStudies confirming the relationship between me-dia and market performance date back to atleast Niederhoffer (1971), who looked at NYTimes headlines and determined that large marketchanges were more likely following world eventsthan on random days.
Conversely, Tetlock (2007)looked at media pessimism and concluded thathigh media pessimism predicts downward prices.Tetlock (2007) also developed a trading strategy,achieving modest annualized returns of 7.3%.
En-gle and Ng (1993) looked at the effects of news onvolatility, showing that bad news introduces morevolatility than good news.
Chan (2003) claimedthat prices are slow to reflect bad news and stockswith news exhibit momentum.
Antweiler andFrank (2004) showed that there is a significant, butnegative correlation between the number of mes-sages on financial discussion boards about a stockand its returns, but that this trend is economicallyinsignificant.
Aside from Tetlock (2007), none ofthis work evaluated the effectiveness of an actualsentiment-based trading strategy.There is, of course, a great deal of work onautomated sentiment analysis as well; see Pangand Lee (2008) for a survey.
More recent de-velopments that are germane to our work includethe use of different information retrieval weightingschemes (Paltoglou and Thelwall, 2010) and theutilization of Latent Dirichlet Allocation (LDA)in a joint sentiment/topic framework (Lin and He,2009).There has also been some work that analyzes the120sentiment of financial documents without actuallyusing those results in trading strategies (Koppeland Shtrimberg, 2004; Ahmad et al., 2006; Fu etal., 2008; O?Hare et al., 2009; Devitt and Ahmad,2007; Drury and Almeida, 2011).
As to the rela-tionship between sentiment and stock price, Dasand Chen (2007) performed sentiment analysis ondiscussion board posts.
Using this analysis, theybuilt a ?sentiment index?
that computed the time-varying sentiment of the 24 stocks in the MorganStanley High-Tech Index (MSH), and tracked howwell their index followed the aggregate price of theMSH itself.
Their sentiment analyzer was basedupon a voting algorithm, although they also dis-cussed a vector distance algorithm that performedbetter.
Their baseline, the Rainbow algorithm, alsocame within 1 percentage point of their reportedaccuracy.
This is one of the very few studies thathas evaluated sentiment analysis itself (as opposedto a sentiment-based trading strategy) against mar-ket returns (versus gold-standard sentiment anno-tations).
Das and Chen (2007) focused exclusivelyon discussion board messages and their evaluationwas limited to the stocks on the MSH, whereaswe focus on Reuters newswire and evaluate overa wide range of NYSE-listed stocks and marketcapitalization levels.Butler and Keselj (2009) try to determine sen-timent from corporate annual reports using bothcharacter n-gram profiles and readability scores.They also developed a sentiment-based tradingstrategy with high returns, but do not report howthe strategy works or how they computed the re-turns, making the results difficult to compare toours.
Basing a trading strategy upon annual re-ports also calls into question the frequency withwhich the trading strategy could be exercised.The work that is most similar to ours is thatof Zhang and Skiena (2010).
They look at bothfinancial blog posts and financial news, forminga market-neutral trading strategy whereby eachday, companies are ranked by their reported sen-timent.
The strategy then goes long and short onequal numbers of positive- and negative-sentimentstocks, respectively.
They conduct their tradingevaluation over the period from 2005 to 2009, andreport a yearly return of roughly 30% when us-ing news data, and yearly returns of up to 80%when they use Twitter and blog data.
Further-more, they trade based upon sentiment rankingrather than pure sentiment analysis, i.e., instead oftrading based on the raw sentiment score of thedocument, they first rank the documents and tradebased on this relative ranking.Zhang and Skiena (2010) compare their strat-egy to two strategies which they term Worst-sentiment Strategy and Random-selection Strat-egy.
The Worst-sentiment Strategy trades the op-posite of their strategy, going short on positive sen-timent stocks and going long on negative senti-ment stocks.
The Random-selection Strategy ran-domly picks stocks to go long and short in.
Astrading strategies, these baselines set a very lowstandard.
Our evaluation compares our strategy tostandard trading benchmarks such as momentumtrading and holding the S&P, as well as to oracletrading strategies over the same trading days.3 Method and Materials3.1 News DataOur dataset consists of a combination of two col-lections of Reuters news documents.
The first wasobtained for a roughly evenly weighted collec-tion of 22 small-, mid- and large-cap companies,randomly sampled from the list of all companiestraded on the NYSE as of 10thMarch, 1997.
Thesecond was obtained for a collection of 20 com-panies randomly sampled from those companiesthat were publicly traded in March, 1997 and stilllisted on 10thMarch, 2013.
For both collectionsof companies, we collected every chronologicallythird Reuters news document about them from theperiod March, 1997 to March, 2013.
The newsarticles prior to 10thMarch, 2005 were used astraining data, and the news articles on or after 10thMarch, 2005 were reserved as testing data.
Wechose to split the dataset at a fixed date rather thanrandomly in order not to incorporate future newsinto the classifier through lexical choice.In total, there were 1256 financial news docu-ments.
Each was labelled by two human annota-tors as being one of negative, positive, or neutralsentiment.
The annotators were instructed to de-termine the state of the author?s belief about thecompany, rather than to make a personal assess-ment of the company?s prospects.
Of the 1256,only the 991 documents that were labelled twiceas negative or positive were used for training andevaluation.121Representation Accuracybm25 freq 81.143%term presence 80.164%bm25 freq with sw 79.827%freq with sw 75.564%freq 79.276%Table 1: Average 10-fold cross validation ac-curacy of the sentiment classifier using differentterm-frequency weighting schemes.
The samefolds were used in all feature sets.3.2 Sentiment Analysis and IntrinsicEvaluationFor each selected document, we first filter out allpunctuation characters and the most common 429stop words.
Our sentiment analyzer is a support-vector machine with a linear kernel function im-plemented using SVMlight(Joachims, 1999).
Wehave experimented with raw term frequencies, bi-nary term-presence features, and term frequen-cies weighted by the BM25 scheme, which hadthe most resilience in the study of information-retrieval weighting schemes for sentiment analysisby Paltoglou and Thelwall (2010).
We performed10 fold cross-validation on the training data, con-structing our folds so that each contains an approx-imately equal number of negative and positive ex-amples.
This ensures that we do not accidentallybias a fold.Pang et al.
(2002) use word presence featureswith no stop list, instead excluding all words withfrequencies of 3 or less.
Pang et al.
(2002) nor-malize their word presence feature vectors, ratherthan term weighting with an IR-based scheme likeBM25, which also involves a normalization step.Pang et al.
(2002) also use an SVM with a linearkernel on their features, but they train and com-pute sentiment values on film reviews rather thanfinancial texts, and their human judges also clas-sified the training films on a scale from 1 to 5,whereas ours used a scale that can be viewed asbeing from -1 to 1, with specific qualitative inter-pretations assigned to each number.
Antweiler andFrank (2004) use SVMs with a polynomial kernel(of unstated degree) to train on word frequenciesrelative to a three-valued classification, but theyonly count frequencies for the 1000 words withthe highest mutual information scores relative tothe classification labels.
Butler and Keselj (2009)also use an SVM trained upon a very different setof features, and with a polynomial kernel of degree3.As a sanity check, we measured the accuracy ofour sentiment analyzer on film reviews by trainingand evaluating on Pang and Lee?s (Pang and Lee,2004) film reviews dataset, which contains 1000positively and 1000 negatively labelled reviews.Pang and Lee conveniently labelled the folds thatthey used when they ran their experiments.
Usingthese same folds, we obtain an average accuracyof 86.85%, which is comparable to Pang and Lee?s86.4% score for subjectivity extraction.Table 1 shows the performance of SVM withBM25 weighting on our Reuters evaluation setversus several baselines.
All baselines are iden-tical except for the term weighting schemes used,and whether stop words were removed.
As can beobserved, SVM-BM25 has the highest sentimentclassification accuracy: 80.164% on average overthe 10 folds.
This compares favourably with pre-vious reports of 70.3% average accuracy over 10folds on financial news documents (Koppel andShtrimberg, 2004).
We will nevertheless adhereto normalized term presence for now, in order tostay close to Pang and Lee?s (Pang and Lee, 2004)implementation.4 Task-based EvaluationIn our second evaluation protocol, we evaluate theaccuracy of the sentiment analyzer by embeddingthe analyzer inside a simple trading strategy, andthen trading with it.Our trading strategy is simple: going long whenthe classifier reports positive sentiment in a newsarticle about a company, and short when the classi-fier reports negative sentiment.
In section 4.1, weuse the discrete polarity returned by the classifierto decide whether go long/abstain/short a stock.
Insection 4.2 we instead use the raw SVM score thatreports the distance of the current document fromthe classifier?s decision boundary.In section 4.3, we hold the trading strategy con-stant, and instead vary the document representa-tion features in the underlying sentiment analyzer.Here, we measure both market return and classifieraccuracy to determine whether they agree.In all three experiments, we compare the per-position returns of trading strategies with the fol-lowing four standards, where the number of daysfor which a position is held remains constant:1.
The momentum strategy computes the price122of the stock h days ago, where h is the hold-ing period.
Then, it goes long for h days ifthe previous price is lower than the currentprice.
It goes short otherwise.2.
The S&P strategy simply goes long on theS&P 500 for the holding period.
This strat-egy completely ignores the stock in questionand the news about it.3.
The oracle S&P strategy computes the valueof the S&P 500 index h days into the future.If the future value is greater than the currentday?s value, then it goes long on the S&P 500index.
Otherwise, it goes short.4.
The oracle strategy computes the value of thestock h days into the future.
If the futurevalue is greater than the current day?s value,then it goes long on the stock.
Otherwise, itgoes short.The oracle and oracle S&P strategies are includedas toplines to determine how close the experimen-tal strategies come to ones with perfect knowledgeof the future.
?Market-trained?
is the same as ?ex-perimental?
at test time, but trains the sentimentanalyzer on the market return of the stock in ques-tion for h days following a training article?s publi-cation, rather than the article?s annotation.4.1 Experiment One: Utilizing SentimentLabels in the Trading StrategyGiven a news document for a publicly traded com-pany, the trading agent first computes the senti-ment class of the document.
If the sentiment ispositive, the agent goes long on the stock on thedate the news is released.
If the sentiment is neg-ative, it goes short.
All trades are made based onthe adjusted closing price on this date.
We evalu-ate the performance of this strategy using four dif-ferent holding periods: 30, 5, 3, and 1 day(s).The returns and Sharpe ratios are presented inTable 2 for the four different holding periods andthe five different trading strategies.
The Sharperatio can be viewed as a return to risk ratio.
Ahigh Sharpe ratio indicates good return for rela-tively low risk.
The Sharpe ratio is calculated asfollows:S =E[Ra?Rb]?var(Ra?Rb),where Rais the return of a single asset and Rbisthe return of a risk-free asset, such as a 10-yearU.S.
Treasury note.Strategy Period Return S. RatioExperimental30 days -0.037% -0.0025 days 0.763% 0.0943 days 0.742% 0.1001 day 0.716% 0.108Momentum30 days 1.176% 0.0665 days 0.366% 0.0453 days 0.713% 0.0961 day 0.017% -0.002S&P30 days 0.318% 0.0595 days -0.038% -0.0163 days -0.035% -0.0171 day 0.046% 0.036Oracle S&P30 days 3.765% 0.9595 days 1.617% 0.9743 days 1.390% 0.9491 day 0.860% 0.909Oracle30 days 11.680% 0.8745 days 5.143% 0.8093 days 4.524% 0.7611 day 3.542% 0.630Market-trained30 days 0.286% 0.0165 days 0.447% 0.0543 days 0.358% 0.0481 day 0.533% 0.080Table 2: Returns and Sharpe ratios for the Experi-mental, baseline and topline trading strategies over30, 5, 3, and 1 day(s) holding periods.The returns from this experimental trading sys-tem are fairly low, although they do beat the base-lines.
A one-way ANOVA test between the ex-perimental strategy, momentum strategy, and S&Pstrategy using the percent returns from the indi-vidual trades yields p values of 0.06493, 0.08162,0.1792, and 0.4164, respectively, thus failing toreject the null hypothesis that the returns are notsignificantly higher.
Furthermore, the means andmedians of all three trading strategies are approx-imately the same and centred around 0.
The stan-dard deviations of the experimental strategy andthe momentum strategy are nearly identical, dif-fering only in the thousandths digit.
The standarddeviations for the S&P strategy differ from theother two strategies due to the fact that the strat-egy buys and sells the entire S&P 500 index andnot the individual stocks described in the news ar-ticles.
There is, in fact, no convincing evidencethat discrete sentiment class leads to an improvedtrading strategy from this or any other study with123Figure 1: Percent returns for 1 day holding periodversus market capitalization of the traded stocks.which we are familiar, based on the details thatthey publish.
One may note, however, that the re-turns from the experimental strategy have slightlyhigher Sharpe ratios than either of the baselines.One may also note that using a sentiment ana-lyzer mostly beats training directly on market data,which to an extent vindicates the use of sentimentannotation as a separate component.Figure 1 shows the market capitalizations ofthe companies for each individual trade plottedagainst the percent return for the 1 day holding pe-riod.
The correlation between the two variables isnot significant.
The graphs for the other holdingperiods are similar.Figure 2 shows the percent change in sharevalue plotted against the raw SVM score for thedifferent holding periods.
We can see a weak cor-relation between the two.
For the 30 days, 5 days,3 days, and 1 day holding periods, the correlationsare 0.017, 0.16, 0.16, and 0.16, respectively.
Theline of best fit is shown.This prompts us to conduct our next experiment.4.2 Experiment Two: Utilizing SVM scoresin Trading Strategy4.2.1 Variable Single ThresholdPreviously, we would label a document as positive(negative) if the score is above (below) 0, because0 is the decision boundary.
However, 0 might notbe the best threshold for providing high returns.To examine this hypothesis, we took the evaluationdataset, i.e.
the dataset with news articles dated onor after March 10, 2005, and divided it into twofolds where each fold has an equal number of doc-uments with positive and negative sentiment.
Weused the first fold to determine an optimal thresh-old value ?
and trade using the data from the sec-ond fold and that threshold.
For every news article,if the SVM score for that article is above (below)?, then we go long (short) on the appropriate stockon the day the article was released.
A separatetheta was determined for each holding period.
Wevaried ?
from ?1 to 1 in increments of 0.1.Using this method, we were able to obtain muchhigher returns.
In order of 30, 5, 3, and 1 day hold-ing periods, the returns were 0.057%, 1.107%,1.238%, and 0.745%.
This is a large improvementover the previous returns, as they are average per-position figures.14.2.2 Safety ZonesFor every news item classified, SVM outputs ascore.
For a binary SVM with a linear kernel func-tion f , given some feature vector x, f(x) can beviewed as the signed distance of x from the de-cision boundary (Boser et al., 1992).
It is thenpossibly justified to interpret raw SVM scores asdegrees to which an article is positive or negative.As in the previous section, we separate the eval-uation set into the same two folds, only now weuse two thresholds, ?
> ?.
We will go long whenthe SVM score is above ?, abstain when the SVMscore is between ?
and ?, and go short when theSVM score is below ?.
This is a strict generaliza-tion of the above experiment, in which ?
= ?.For convenience, we will assume in this sectionthat ?
= ?
?, leaving us again with one parameterto estimate.
We again vary ?
from 0 to 1 in in-crements of 0.1.
Figure 3 shows the returns as afunction of ?
for each holding period on the devel-opment dataset.
If we increased the upper boundon ?
to be greater than 1, then there would be toofew trading examples (less than 10) to reliably cal-culate the Sharpe ratio.
Using this method with?
= 1, we were able to obtain even higher returns:3.843%, 1.851%, 1.691, and 2.251% for the 30,5, 3, and 1 day holding periods, versus 0.057%,1.107%, 1.238%, and 0.745% in the second task-based experiment.4.3 Experiment Three: Feature SelectionLet us now hold the trading strategy fixed (at thefinal one, with safety zones) and turn to the un-derlying sentiment analyzer.
With a good trading1Training directly on market data, by comparison, yields-0.258%, -0.282%, -0.036% and -0.388%, respectively.124Figure 2: Percent change of trade returns plotted against SVM values for the 1, 3, 5, and 30 day holdingperiods in Exp.
1.
Graphs are cropped to zoom in.Figure 3: Returns for the different thresholds on the development dataset for 30, 5, 3, and 1 day holdingperiods in Exp.
2 with safety zone.125Representation Accuracy pi ?
?
30 days 5 days 3 days 1 dayterm presence 80.164% 0.589 0.59 0.589 3.843% 1.851% 1.691% 2.251%bm25 freq 81.143% 0.609 0.61 0.609 1.110% 1.770% 1.781% 0.814%bm25 freq d n copular 62.094% 0.012 0.153 0.013 3.458% 2.834% 2.813% 2.586%bm25 freq with sw 79.827% 0.581 0.583 0.581 0.390% 1.685% 1.581% 1.250%freq 79.276% 0.56 0.566 0.561 1.596% 1.221% 1.344% 1.330%freq with sw 75.564% 0.47 0.482 0.47 1.752% 0.638% 1.056% 2.205%Table 3: Sentiment classification accuracy (average 10-fold cross-validation), Scott?s pi, Krippendorff?s?, Cohen?s ?
and trade returns of different feature sets and term frequency weighting schemes in Exp.
3.The same folds were used for the different representations.
The non-annualized returns are presented incolumns 3-6.strategy in place, it is clearly possible to vary someaspect of the sentiment analyzer in order to deter-mine its best setting in this context.
Is classifier ac-curacy a suitable proxy for this?
Indeed, we mayhope that classifier accuracy will be more portableto other possible tasks, but then it must at leastcorrelate well with task-based performance.We tried another feature representation for doc-uments.
In addition to evaluating those attemptedearlier, we now hypothesize that the passive voicemay be useful to emphasize in our representations,as the existential passive can be used to evade re-sponsibility.
So we add to the BM25 weightedvector the counts of word tokens ending in ?n?
or?d?
as well as the total count of every conjugatedform of the copular verb: ?be?, ?is?, ?am?, ?are?,?were?, ?was?, and ?been?.
These three featuresare superficial indicators of the passive voice.Table 3 presents the returns obtained fromthese 6 feature representations.
The feature setwith BM25-weighted term frequencies plus thenumber of copulars and tokens ending in ?n?,?d?
(bm25 freq d n copular) yields higher returnsthan any other representation attempted on the 5,3, and 1 day holding periods, and the second-highest on the 30 days holding period, But it hasthe worst classification accuracy by far: a full 18percentage points below term presence.
This is avery compelling illustration of how misleading anintrinsic evaluation can be.
Other agreement mea-sures likewise point in the opposite direction.5 ConclusionIn this paper, we examined the application of senti-ment analysis in stock trading strategies.
We builta binary sentiment classifier that achieves high ac-curacy when tested on movie data and financialnews data from Reuters.
In three task-based ex-periments, we evaluated the usefulness of senti-ment analysis in simple trading strategies.
Al-though high annual returns can be achieved bysimply utilizing sentiment labels in a trading strat-egy, they can be improved by incorporating theoutput of the SVM?s decision function.
We haveobserved that classification accuracy alone is notalways an accurate predictor of task-based perfor-mance.
This calls into question the benefit of usingintrinsic sentiment classification accuracy, partic-ularly when the relative cost of a task-based eval-uation may be comparably low.
We have also de-termined that training on human-annotated senti-ment does in fact perform better than training onmarket returns themselves.
So sentiment analysisis an important component, but it must be tunedagainst task data.As for future work, we plan to explore otherways of deriving sentiment labels for supervisedtraining.
It would be interesting to infer the senti-ment of published news from stock price fluctua-tions instead of the reverse.
Given that many fac-tors that affect stock price fluctuations and furtherconsidering the drift that is present in stock pricesas a result of bad published news (Chan, 2003),this mode of inference is not simple and requirescareful consideration and design.Furthermore, we would like to study how senti-ment is defined in the financial world.
In particu-lar, we want to examine the relationship betweenthe precise definition of news sentiment and trad-ing strategy returns.
This study has used a rathergeneral definition of news sentiment.
We are in-terested in exploring if there is a more precise def-inition that can improve trading performance.Our current price data only includes adjustedopening and closing prices.
Most of our news datacontain only the date of the article, not the specifictime.
It is possible that a much shorter-term trad-ing strategy than we can currently test would beeven more successful.126ReferencesKhurshid Ahmad, David Cheng, and Yousif Almas.2006.
Multi-lingual sentiment analysis of financialnews streams.
In Proceedings of the 1st Interna-tional Conference on Grid in Finance.Werner Antweiler and Murray Z Frank.
2004.
Is allthat talk just noise?
the information content of inter-net stock message boards.
The Journal of Finance,59(3):1259?1294.Bernhard E. Boser, Isabelle M. Guyon, andVladimir N. Vapnik.
1992.
A training algo-rithm for optimal margin classifiers.
In Proceedingsof the fifth annual workshop on Computationallearning theory, COLT ?92, pages 144?152, NewYork, NY, USA.
ACM.Matthew Butler and Vlado Keselj.
2009.
Finan-cial forecasting using character n-gram analysis andreadability scores of annual reports.
In Proceedingsof Canadian AI?2009, Kelowna, BC, Canada, May.Wesley S. Chan.
2003.
Stock price reaction to newsand no-news: Drift and reversal after headlines.Journal of Financial Economics, 70(2):223?260.Sanjiv R. Das and Mike Y. Chen.
2007.
Yahoo!
foramazon: Sentiment extraction from small talk on theweb.
Management Science, 53(9):1375?1388.Joseph Davies-Gavin, Clarence Lee, and LinglingZhang.
2012.
Conference summary.
In MarketingScience Institute Conference on Big Data, Decem-ber.Ann Devitt and Khurshid Ahmad.
2007.
Sentimentpolarity identification in financial news: A cohesion-based approach.
In Proceedings of the ACL.Brett Drury and J. J. Almeida.
2011.
Identificationof fine grained feature based event and sentimentphrases from business news stories.
In Proceedingsof the International Conference on Web Intelligence,Mining and Semantics, WIMS ?11, pages 27:1?27:7,New York, NY, USA.
ACM.Robert F. Engle and Victor K. Ng.
1993.
Measuringand testing the impact of news on volatility.
TheJournal of Finance, 48(5):1749?1778.Tak-Chung Fu, Ka ki Lee, Donahue C. M. Sze, Fu-LaiChung, Chak man Ng, and Chak man Ng.
2008.Discovering the correlation between stock time se-ries and financial news.
In Web Intelligence, pages880?883.Thorsten Joachims.
1999.
Making large-scale svmlearning practical.
advances in kernel methods-support vector learning, b. sch?olkopf and c. burgesand a. smola.Moshe Koppel and Itai Shtrimberg.
2004.
Good newsor bad news?
let the market decide.
In AAAI SpringSymposium on Exploring Attitude and Affect in Text,pages 86?88.
Press.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceedings of the 18th ACM conference on Informa-tion and knowledge management, CIKM ?09, pages375?384, New York, NY, USA.
ACM.Victor Niederhoffer.
1971.
The analysis of worldevents and stock prices.
Journal of Business, pages193?219.Neil O?Hare, Michael Davy, Adam Bermingham,Paul Ferguson, P?araic Sheridan, Cathal Gurrin, andAlan F. Smeaton.
2009.
Topic-dependent senti-ment analysis of financial blogs.
In Proceedingsof the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion measurement.Georgios Paltoglou and Mike Thelwall.
2010.
A studyof information retrieval weighting schemes for sen-timent analysis.
In Proceedings of the ACL, pages1386?1395.
Association for Computational Linguis-tics.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe ACL, pages 271?278.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in natu-ral language processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Paul C. Tetlock.
2007.
Giving content to investor sen-timent: The role of media in the stock market.
TheJournal of Finance, 62(3):1139?1168.Christa Williford, Charles Henry, and Amy Friedlan-der.
2012.
One culture: Computationally inten-sive research in the humanities and social sciences.Technical report, Council on Library and Informa-tion Resources, June.Wenbin Zhang and Steven Skiena.
2010.
Tradingstrategies to exploit blog and news sentiment.
In The4th International AAAI Conference on Weblogs andSocial Media.127
