The TUNA Challenge 2008: Overview and Evaluation ResultsAlbert GattDepartment of Computing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKa.gatt@abdn.ac.ukAnja Belz Eric KowNatural Language Technology GroupUniversity of BrightonBrighton BN2 4GJ, UK{asb, eykk10}@brighton.ac.ukAbstractThe TUNA Challenge was a set of three sharedtasks at REG?08, all of which used data fromthe TUNA Corpus.
The three tasks coveredattribute selection for referring expressions(TUNA-AS), realisation (TUNA-R) and end-to-end referring expression generation (TUNA-REG).
8 teams submitted a total of 33 systemsto the three tasks, with an additional submis-sion to the Open Track.
The evaluation useda range of automatically computed measures.In addition, an evaluation experiment was car-ried out using the peer outputs for the TUNA-REG task.
This report describes each task andthe evaluation methods used, and presents theevaluation results.1 IntroductionThe TUNA Challenge 2008 built on the foundationslaid in the ASGRE 2007 Challenge (Belz and Gatt,2007), which consisted of a single shared task, basedon a subset of the TUNA Corpus (Gatt et al, 2007).The TUNA Corpus is a collection of human-authoreddescriptions of a referent, paired with a represen-tation of the domain in which that description waselicited.The 2008 Challenge expanded the scope of theprevious edition in a variety of ways.
This year,there were three shared tasks.
TUNA-AS is the At-tribute Selection task piloted in the 2007 ASGREChallenge, which involves the selection of a set ofattributes which are true of a target referent, andhelp to distinguish it from its distractors in a do-main.
TUNA-R is a realisation task, involving themapping from attribute sets to linguistic descrip-tions.
TUNA-REG is an ?end to end?
referring ex-pression generation task, involving a mapping froman input domain to a linguistic description of a targetreferent.
In addition, there was an Open SubmissionTrack, where participants were invited to submit areport on any interesting research that involved theshared task data, and an Evaluation Track, for whichsubmissions were invited on proposals for evalua-tion methods.
This year?s TUNA Challenge also ex-panded considerably on the evaluation methods usedin the various tasks.
The measures can be dividedinto intrinsic, automatically computed methods, andextrinsic measures obtained through a task-orientedexperiment involving human participants.The training and development data for the Chal-lenge included the full dataset used in the ASGREChallenge, that is, all of the 2007 training, develop-ment and test data.
For the 2008 edition, two newtest sets were constructed.
Test Set 1 was used forTUNA-R, Test Set 2 was used for both TUNA-AS andTUNA-REG.1.1 Overview of submissionsOverall, 8 research groups submitted 33 systems bythe deadline.
Table 1 provides a summary of the sub-missions.
The extrinsic evaluation experiment wascarried out on peer outputs in the TUNA-REG taskonly, using outputs from at most 4 systems per par-ticipating group.
The 10 systems included are indi-cated in boldface in the table.
An additional submis-sion was made by the USP team to the Open Track.No submissions were made to the Evaluation Track.Given the number of submissions, space restrictionsdo not permit us to give an overview of the charac-teristics of the various systems; these can be foundin the reports authored by each participating group,which are included in this volume.198Group Organisation TUNA-AS TUNA-R TUNA-REGATT AT&T Labs Research Inc.ATT-DR-b ATT-R ATT-TemplateS-wsATT-DR-sf ATT-TemplateS-drwsATT-FB-f ATT-Template-wsATT-FB-m ATT-Template-drwsATT-FB-sf ATT-PermuteRank-wsATT-FB-sr ATT-PermuteRank-drwsATT-Dependency-drwsATT-Dependency-wsDIT Dublin Institute of TechnologyDIT-FBI DIT-CBSR DIT-FBI-CBSRDIT-TVAS DIT-RBR DIT-TVAS-RBRGRAPH University of Tilbug etc GRAPH-FP GRAPH-4+B?IS University of Stuttgart IS-FP IS-GT IS-FP-GTJUCSENLP Jadavpur University JU-PTBSGRENIL-UCM Universidad Complutense de Madrid NIL-UCM-MFVF NIL-UCM-BSC NIL-UCM-FVBSOSU Ohio State University OSU-GP OSU-GP?USP University of Sao Paolo USP-EACH-FREQTable 1: Overview of participating teams and systems, by task.
TUNA-REG peer systems whose outputs were includedin the extrinsic, task-based evaluation are shown in boldface.
Systems marked ?
were submissions to TUNA-AS whichmade use of the off-the-shelf ASGRE realiser for their entries to TUNA-REG.Participants in TUNA-AS and TUNA-R were alsogiven the opportunity to submit peer outputs forTUNA-REG, and having them included in the ex-trinsic evaluation, by making the use of off-the-shelf modules.
For systems in TUNA-AS, wemade available a template-based realiser, written byIrene Langkilde-Geary at the University of Brighton.Originally used in the 2007 ASGRE Challenge, thiswas re-used by some TUNA-AS participants to re-alise their outputs.
Systems which made use of thisfacility are marked by a (*) in Table 1.In the rest of this report, we first give an overviewof the tasks and the data used for the Challenge (Sec-tion 2), followed by a description of the evaluationmethods (Section 3).
Section 4 gives the compar-ative evaluation results for each task, followed bya few concluding remarks in Section 5.
In whatfollows, we will use the following terminology, inkeeping with their usage in Belz and Gatt (2007): apeer system is a system submitted to the shared-taskchallenge, while peer output is an attribute set or adescription (in the form of a word string) producedby a peer system.
We will refer to a description inthe TUNA corpus as a reference output.2 Data and task overview2.1 The TUNA DataThe TUNA corpus was constructed via an elicita-tion experiment as part of the TUNA project1.
Eachfile in the data consists of a single pairing of a do-main (representation of entities and their attributes)and a human-authored description (reference output)1http://www.csd.abdn.ac.uk/research/tuna/<TRIAL CONDITION="+/-LOC" ID="..."><DOMAIN><ENTITY ID="..." TYPE="target" IMAGE="..."><ATTRIBUTE NAME="..." VALUE="..." />...</ENTITY><ENTITY ID="..." TYPE="distractor" IMAGE="..."><ATTRIBUTE NAME="..." VALUE="..." />...</ENTITY>...</DOMAIN><WORD-STRING>the string describing the target referent</WORD-STRING><ANNOTATED-WORD-STRING>the string in WORD-STRING annotatedwith attributes in ATTRIBUTE-SET</ANNOTATED-WORD-STRING><ATTRIBUTE-SET>the set of domain attributes in the description</ATTRIBUTE-SET></TRIAL>Figure 1: Format of corpus itemswhich is intended to describe the target referent inthe domain.
Only the singular descriptions in thecorpus were used for the TUNA Challenge.The descriptions in the corpus are subdivided byentity type: there are references to people, and refer-ences to furniture items.
In addition, the elicitationexperiment manipulated a single condition, ?LOC.In the +LOC condition, experimental participantswere told that they could refer to entities using anyof their properties, including their location.
In the?LOC condition, they were discouraged from doingso, though not prevented.Figure 1 is an outline of the XML format used inthe Challenge.
Each file has a root TRIAL nodewith a unique ID and an indication of the experi-mental condition.
The DOMAIN node subsumes 7199ENTITY nodes, which themselves subsume a num-ber of ATTRIBUTE nodes defining the properties ofan entity in attribute-value notation.
The attributesinclude properties such as an object?s colour or aperson?s clothing, and the location of the image inthe visual display which the DOMAIN represents.Each ENTITY node indicates whether it is the targetreferent or one of the six distractors, and also has apointer to the image that it represents.
Images weremade available to the TUNA Challenge participants.The WORD-STRING is the actual de-scription typed by a human author, and theATTRIBUTE-SET is the set of attributes belongingto the referent that the description includes.
TheANNOTATED-WORD-STRING node was onlyprovided in the training and development data,to display how substrings of a human-authoreddescription were mapped to attributes to determinethe ATTRIBUTE-SET.Training and development data: For the TUNAChallenge, the 780 singular corpus instances weredivided into 80% training data and 20% develop-ment data.
This data consists of all the training,development and test data used in the 2007 ASGREChallenge.Test data: Two new test sets were constructed byreplicating the original TUNA elicitation experi-ment.
The new experiment was designed to ensurethat each DOMAIN in the new test sets had tworeference outputs.
Thus, this year?s corpus-basedevaluations are conducted against multiple instancesof each input DOMAIN.
Both sets consisted of 112items, divided equally into furniture and peopledescriptions, sampled from both experimentalconditions (?LOC).
Test Set 1 was used for theTUNA-R Task.
Participants in this task received aversion of the test set whose items consisted of aDOMAIN node and an ATTRIBUTE-SET node.There were 56 unique DOMAINs, each representedtwice in the test set, with two attribute sets from twodifferent human authors.
Because each DOMAINand ATTRIBUTE-SET combination in this testset is unique, the results for this task are reportedbelow over the whole of Test Set 1.
Test Set 2was used for the TUNA-AS and TUNA-REG Tasks.For these tasks, the test items given to participantsconsisted of a DOMAIN node only.
There were112 unique DOMAINs; the evaluations on thesetasks were conducted by comparing each peeroutput to two different reference outputs for eachof these domains.
Therefore, in the TUNA-AS andTUNA-REG tasks, the data presented here averagesover the two outputs per DOMAIN.2.2 The tasksTask 1: Attribute Selection (TUNA-AS): TheTUNA-AS task focused on content determination forreferring expressions, and follows the basic prob-lem definition used in much previous work in thearea: given a domain and a target referent, select asubset of the attributes of that referent which willhelp to distinguish it from its distractors.
The inputsfor this task consisted of a TRIAL node enclosing aDOMAIN node (a representation of entities and prop-erties).
A peer output was a TRIAL node enclos-ing just an ATTRIBUTE-SET node whose childrenwere the attributes selected by a peer system for thetarget entity.Task 2: Realisation (TUNA-R): The TUNA-R taskfocussed on realisation.
The aim was to map anATTRIBUTE-SET node to a word string which de-scribes the ENTITY that is marked as the target suchthat the entity can be identified in the domain.
Theinputs for this task consisted of a TRIAL node en-closing a DOMAIN and an ATTRIBUTE-SET node.A peer output for this task consisted of a TRIALnode enclosing just a WORD-STRING node.Task 3: ?End-to-end?
Referring Expression Gen-eration (TUNA-REG): For the TUNA-REG task, theinput consisted of a DOMAIN, and a peer output wasa word string which described the entity marked asthe target such that the entity could be identified inthe domain.
The input for this task was identical tothat for TUNA-AS, i.e.
a TRIAL node enclosing justa DOMAIN node.
A peer output for this task wasidentical in format to that for the TUNA-R task, i.e.
aTRIAL enclosing just a WORD-STRING node.3 Evaluation methodsThe evaluation methods used in each task, and thequality criteria that they assess, are summarised inTable 2.
Peer outputs from all tasks were evalu-ated using intrinsic methods.
All of these were au-tomatically computed, and are subdivided into (a)200Task Criterion Type MethodsTUNA-AS Humanlikeness Intrinsic Accuracy, Dice, MASIMinimality Intrinsic Proportion of minimal outputsUniqueness Intrinsic Proportion of unique outputsTUNA-R Humanlikeness Intrinsic Accuracy, BLEU, NIST, string-edit distanceTUNA-REG Humanlikeness Intrinsic Accuracy, BLEU, NIST string-edit distanceEase of comprehension Extrinsic Self-paced reading in identification experimentReferential Clarity Extrinsic Speed and accuracy in identification experimentTable 2: Evaluation methods used per taskthose measures that assess humanlikeness, i.e.
thedegree of similarity between a peer output and a ref-erence output; and (b) measures that assess intrin-sic properties of peer outputs.
Peer outputs fromthe TUNA-REG task were also included in a human,task-oriented evaluation, which is extrinsic insofaras it measures the adequacy of a peer output in termsof its utility in an externally defined task.
In the re-mainder of this section, we summarise the propertiesof the intrinsic methods.
Section 3.1 describes theexperiment conducted for the extrinsic evaluation.Dice coefficient (TUNA-AS): This is a set-comparison metric, ranging between 0 and 1, where1 indicates a perfect match between sets.
For twoattribute sets A and B, Dice is computed as follows:Dice(A,B) =2?
|A ?B||A|+ |B|(1)MASI (TUNA-AS): The MASI score (Passonneau,2006) is an adaptation of the Jaccard coefficientwhich biases it in favour of similarity where one setis a subset of the other.
Like Dice, it ranges between0 and 1, where 1 indicates a perfect match.
It is com-puted as follows:MASI(A,B) = ?
?|A ?B||A ?B|(2)where ?
is a monotonicity coefficient defined as fol-lows:?
=??????
?0 if A ?B = ?1 if A = B23 if A ?
B or B ?
A13 otherwise(3)Accuracy (all tasks): This is computed as the pro-portion of the peer outputs of a system which havean exact match to a reference output.
In TUNA-AS,Accuracy was computed as the proportion of times asystem returned an ATTRIBUTE-SET identical tothe reference ATTRIBUTE-SET produced by a hu-man author for the same DOMAIN.
In TUNA-R andTUNA-REG, Accuracy was computed as the propor-tion of times a peer WORD-STRING was identicalto the reference WORD-STRING produced by an au-thor for the same DOMAIN.String-edit distance (TUNA-R, TUNA-REG): Thisis the classic Levenshtein distance measure, used tocompare the difference between a peer output and areference output in the corpus, as the minimal num-ber of insertions, deletions and/or substitutions ofwords required to transform one string into another.The cost for insertions and deletions was set to 1,that for substitutions to 2.
Edit distance is an integerbounded by the length of the longest description inthe pair being compared.BLEU (TUNA-R, TUNA-REG): This is an n-grambased string comparison measure, originally pro-posed by Papineni et al (2002) for evaluation ofMachine Translation systems.
It evaluates a systembased on the proportion of word n-grams (consid-ering all n-grams of length n ?
4 is standard) thatit shares with several reference translations.
UnlikeDice, MASI and String-edit, BLEU is by definition anaggregate measure (i.e.
a single BLEU score is ob-tained for a system based on the entire set of itemsto be compared, and this is generally not equal to theaverage of BLEU scores for individual items).
BLEUranges between 0 and 1.NIST (TUNA-R, TUNA-REG): This is a version ofBLEU, which gives more importance to less frequent(hence more informative) n-grams.
The range ofNIST scores depends on the size of the test set.
LikeBLEU, this is an aggregate measure.Uniqueness (TUNA-AS): This measure was in-cluded for backwards comparability with the ASGREChallenge 2007.
It is defined as the proportion ofpeer ATTRIBUTE-SETs which identify the targetreferent uniquely, i.e.
whose (logical conjunction of)201attributes are true of the target, and of no other entityin the DOMAIN.Minimality (TUNA-AS): This measure was definedas the proportion of peer ATTRIBUTE-SETs whichare minimal, where ?minimal?
means that there isno attribute-set which uniquely identifies the targetreferent in the domain which is smaller.
Note thatthis definition includes Uniqueness as a prerequisite,since the description must identify the target entityuniquely in order to qualify for Minimality.All intrinsic evaluation methods except for BLEUand NIST were computed (a) overall, using the entiretest data set (i.e.
Test Set 1 or 2 as appropriate); and(b) by object type, that is, computing separate valuesfor outputs referring to targets of type furniture andpeople.3.1 Extrinsic evaluation in TUNA-REGThe experiment for the extrinsic evaluation ofTUNA-REG peer outputs combined a self-pacedreading and identification paradigm, comparing thepeer outputs from 10 of the TUNA-REG systemsshown in Table 1, as well as the two sets of human-authored reference outputs for Test Set 2.
We referto the latter as HUMAN-1 and HUMAN-2 in what fol-lows2.In the task given to experimental subjects, a trialconsisted of a description paired with a visual do-main representation corresponding to an item in TestSet 2.
Each trial was split into two phases: (a) in aninitial reading phase, subjects were presented withthe description only.
This phase was terminated bysubjects once they had read the description.
(b) Inthe second, identification phase, subjects saw the vi-sual domain in which the description had been pro-duced, consisting of images of the domain entitiesin the same spatial configuration as that in the testset DOMAIN.
They clicked on the object that theythought was the intended referent of the descriptionthey had read.The experiment yielded three dependent mea-sures: (a) reading time (RT), measured from thepoint at which the description was presented, to the2Note that HUMAN-1 and HUMAN-2 were both sets of de-scriptions randomly sampled from the data collected in the ex-periment.
Each set of human descriptions contains output fromdifferent human authors.point at which a participant called up the next screenvia mouse click; (b) identification time (IT), mea-sured from the point at which pictures (the visualdomain) were presented on the screen to the pointwhere a participant identified a referent by clickingon it; (c) error rate (ER), the proportion of times thewrong referent was identified.This design differs from that used in the 2007ASGRE Challenge, in which descriptions and visualdomains were presented in a single phase (on thesame screen), so that RT and IT were conflated.
Thenew experiment replicates the methodology reportedin Gatt and Belz (2008), in a follow-up study onthe ASGRE 2007 data.
Another difference betweenthe two experiments is that the current one is basedon peer outputs which are themselves realisations,whereas the ASGRE experiment involved attributesets which had to be realised before they could beused.Design: We used a Repeated Latin Squares design,in which each combination of SYSTEM3 and test setitem is allocated one trial.
Since there were 12 lev-els of SYSTEM, but 112 test set items, 8 randomlyselected items (4 furniture and 4 people) were du-plicated, yielding 120 items and 10 12 ?
12 latinsquares.
The items were divided into two sets of 60.Half of the participants did the first 60 items (the first5 latin squares), and the other half the second 60.Participants and procedure: The experiment wascarried out by 24 participants recruited from amongthe faculty and administrative staff of the Univer-sity of Brighton, as well as from among the au-thors?
acquaintances.
Participants carried out theexperiment under supervision in a quiet room on alaptop.
Stimulus presentation was carried out us-ing DMDX, a Win-32 software package for psy-cholinguistic experiments involving time measure-ments (Forster and Forster, 2003).
Participants initi-ated each trial, which consisted of an initial warningbell and a fixation point flashed on the screen for1000ms.
They then read the description and calledup the visual domain to identify the referent.
Trialstimed out after 15000ms.Treatment of outliers and timeouts: Trials which3The SYSTEM independent variable in this experiment in-cludes HUMAN-1 and HUMAN-2.202timed out with no response were discounted fromthe analysis.
Out of a total of (24 ?
60 =) 1440trials, there were 4 reading timeouts (0.3%) and 7identification timeouts (0.5%).
Outliers for RT andIT were defined as those exceeding a threshold ofmean ?2SD.
There were 64 outliers on RT (4.4%)and 191 on IT (13.3%).
Outliers were replaced bythe overall mean for RT and IT (see Ratliff (1993)for discussion of this method).4 Evaluation resultsThis section presents results for each of the tasks.For all measures, except BLEU and NIST, we presentseparate descriptive statistics by entity type (peoplevs.
furniture subsets of the relevant test set), andoverall.4.1 Results for TUNA-ASDescriptive statistics are displayed for all systems inTable 3.
This includes the Accuracy and Minimal-ity scores (proportions), and mean MASI and Dicescores.
Values are displayed by entity type and over-all.
The standard deviation for Dice and MASI isdisplayed overall.
Scores average over both sets ofreference outputs in Test Set 2.
All systems scored100% on Uniqueness, and either 0 or 100% on Min-imality.
These measures are therefore not includedin the significance testing, though Minimality is in-cluded in the correlations reported below.Two 15 (SYSTEM) ?
2 (ENTITY TYPE) uni-variate ANOVAs were conducted on the Dice andMASI scores.
We report significant effects at p ?.001.
There were main effects of SYSTEM (Dice:F (13, 1540) = 193.08; MASI: F (13, 1540) =93.45) and ENTITY TYPE (Dice: F (1, 1540) =91.75; MASI: F (1, 1540) = 168.12), as wellas a significant interaction between the two (Dice:F (13, 1540) = 7.45, MASI: F (13, 1540) = 7.35).Post-hoc Tukey?s comparisons on both Dice andMASI yielded the homogeneous subsets displayed inTable 4.Differences among systems on Accuracy wereanalysed by coding this as an indicator variable: foreach peer output, the variable indicated whether itachieved perfect match with at least one of the tworeference outputs on the same DOMAIN.
A Kruskall-Wallis test showed that the difference between sys-tems was significant (?2 = 275.01, p < .001).Minimality Accuracy Dice MASIMinimality -0.877 -0.959 -0.901Accuracy -0.877 0.973 0.998Dice -0.959 0.973 0.985MASI -0.901 0.998 0.985Table 5: Correlations for TUNA-AS; all values are signif-icant at p ?
.05Pairwise correlations using Pearson?s r are shownin Table 5, for all measures except Uniqueness.
Allcorrelations are positive and significant, with the ex-ception of those involving Minimality, which cor-relates negatively with all other measures (i.e.
thehigher the proportion of minimal descriptions of asystem, the lower its score on humanlikeness, asmeasured by Dice, MASI and Accuracy).
This re-sult corroborates a similar finding in the 2007 AS-GRE Challenge.4.2 Results for TUNA-RTable 6 shows descriptives for the 5 participatingsystems in TUNA-R. Once again, mean Edit scoresand Accuracy proportions are shown both overalland by entity type, while BLEU and NIST are overallaggregate scores.A 15 (SYSTEM) ?
2 (ENTITY TYPE) univariateANOVA was conducted on the Edit Distance scores.There was no main effect of SYSTEM, and no in-teraction, but ENTITY TYPE exhibited a main effect(F (1, 550) = 19.99, p < .001).
Given the lack of amain effect, no post-hoc comparisons between sys-tems were conducted.
A Kruskall-Wallis test alsoshowed no difference between systems on Accu-racy.
Pairwise correlations between all measures areshown in Table 7; this time, the only significant cor-relation is between NIST and BLEU.Edit Accuracy NIST BLEUEdit 0.195 -0.095 0.099Accuracy 0.195 0.837 0.701NIST -0.095 0.837 0.900?BLEU 0.099 0.701 0.900?Table 7: Correlations for the TUNA-R task (?
indicatesp ?
.05).4.3 Results for TUNA-REG4.3.1 Tests on the intrinsic measuresResults for the intrinsic measures on the TUNA-REG task are shown in Table 8.
As in Section 4.1,203Dice MASI Accuracy Minimalityfurniture people both SD furniture people both SD furniture people both bothGRAPH 0.858 0.729 0.794 0.160 0.705 0.465 0.585 0.272 0.53 0.56 0.40 0.00JU-PTBSGRE 0.858 0.762 0.810 0.152 0.705 0.501 0.603 0.251 0.55 0.58 0.41 0.00ATT-DR-b 0.852 0.722 0.787 0.154 0.663 0.441 0.552 0.283 0.52 0.54 0.36 0.00ATT-DR-sf 0.852 0.722 0.787 0.154 0.663 0.441 0.552 0.283 0.50 0.52 0.36 0.00DIT-FBI 0.850 0.731 0.791 0.153 0.661 0.451 0.556 0.280 0.50 0.53 0.36 0.00IS-FP 0.828 0.723 0.776 0.165 0.641 0.475 0.558 0.278 0.52 0.54 0.37 0.00NIL-UCM-MFVF 0.821 0.684 0.753 0.169 0.601 0.383 0.492 0.290 0.44 0.46 0.31 0.00USP-EACH-FREQ 0.820 0.663 0.742 0.176 0.616 0.404 0.510 0.291 0.46 0.48 0.33 0.00DIT-TVAS 0.814 0.684 0.749 0.166 0.580 0.383 0.482 0.285 0.43 0.46 0.29 0.00OSU-GP 0.640 0.443 0.541 0.226 0.352 0.114 0.233 0.227 0.17 0.20 0.06 0.00ATT-FB-m 0.357 0.263 0.310 0.245 0.164 0.119 0.141 0.125 0.13 0.14 0.00 1.00ATT-FB-f 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00ATT-FB-sf 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00ATT-FB-sr 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00Table 3: Descriptives for the TUNA-AS task.
All means are shown by entity type; standard deviations are displayedoverall.Dice MASIATT-FB-f A ATT-FB-f AATT-FB-sf A ATT-FB-sf AATT-FB-sr A ATT-FB-sr AATT-FB-m A ATT-FB-m A BOSU-GP B OSU-GP BUSP-EACH-FREQ C DIT-TVAS CDIT-TVAS C NIL-UCM-MFVF C DNIL-UCM-MFVF C USP-EACH-FREQ C D EIS-FP C ATT-DR-b C D EATT-DR-b C ATT-DR-sf C D EATT-DR-sf C DIT-FBI C D EDIT-FBI C IS-FP C D EGRAPH C GRAPH D EJU-PTBSGRE C JU-PTBSGRE ETable 4: Homogeneous subsets for systems in TUNA-AS.
Systems which do not share a common letter are significantlydifferent at p ?
.05Edit Accuracy NIST BLEUfurniture people both SD furniture people both both bothIS-GT 7.750 9.768 8.759 6.319 0.02 0.00 0.01 0.4526 0.0415NIL-UCM-BSC 7.411 9.143 8.277 6.276 0.05 0.04 0.04 1.7034 0.0784ATT-1-R 7.143 9.268 8.205 6.140 0.02 0.00 0.01 0.1249 0DIT-CBSR 7.054 10.286 8.670 6.873 0.09 0.02 0.05 1.1623 0.0686DIT-RBR 6.929 9.857 8.393 6.668 0.04 0.00 0.02 0.9151 0.0694Table 6: Descriptives for the TUNA-R task.Edit Accuracy BLEU NISTfurniture people both SD furniture people both both bothATT-PermuteRank-ws 8.339 8.304 8.321 3.283 0.00 0 0 0.007 0.0288ATT-Template-ws 8.304 8.161 8.232 3.030 0.00 0 0 0 0.0059ATT-Dependency-ws 8.232 8.000 8.116 3.023 0.00 0 0 0.0001 0.0139ATT-TemplateS-ws 8.214 8.161 8.188 3.063 0.00 0 0 0 0.0057OSU-GP 7.964 13.232 10.598 4.223 0.00 0 0 1.976 0.0236ATT-PermuteRank-drws 7.464 8.411 7.938 3.431 0.02 0.04 0.03 0.603 0.0571DIT-TVAS-RBR 6.893 8.161 7.527 3.358 0.05 0 0.03 1.0233 0.0659ATT-TemplateS-drws 6.786 7.679 7.232 3.745 0.07 0.02 0.04 0.6786 0.0958ATT-Template-drws 6.768 7.696 7.232 3.757 0.07 0.02 0.04 0.6083 0.0929NIL-UCM-FVBS 6.643 8.411 7.527 3.618 0.07 0.04 0.05 1.8277 0.0684IS-FP-GT 6.607 7.304 6.955 3.225 0.05 0.02 0.04 0.8708 0.1086DIT-FBI-CBSR 6.536 7.643 7.089 3.889 0.16 0.05 0.11 0.8804 0.1259ATT-Dependency-drws 6.482 7.446 6.964 3.349 0.07 0 0.04 0.3427 0.0477GRAPH 5.946 9.018 7.482 3.541 0.18 0 0.09 1.141 0.0696Table 8: Descriptives for TUNA-REG on the intrinsic measures.means for the intrinsic measures average over bothsets of reference outputs in Test Set 2.A 15 (SYSTEM) ?2 (ENTITY TYPE) univariateANOVA was conducted on the Edit Distance scores.There were significant main effects of SYSTEM(F (13, 1540) = 8.6, p < .001) and ENTITY TYPE(F (1, 1540) = 47.5, p < .001), as well as a signif-icant interaction (F (13, 1540) = 5.77, p < .001).A Kruskall-Wallis test on Accuracy, coded as an in-dicator variable (see Section 4.2), showed that sys-tems differed significantly on this measure as well(?2 = 26.27, p < .05).Post-hoc Tukey?s comparisons were conducted onEdit Distance; the homogeneous subsets are shownin Table 9.
The table suggests that the main effectof Edit Distance may largely have been due to the204difference between OSU-GP and all other systems.Correlations between these measures are shownin Table 10.
Contrary to the results in Section 4.2,the correlation between BLEU and NIST does notreach significance.
The negative correlations be-tween Edit distance and Accuracy, and between Editand BLEU are as expected, since higher Edit cost im-plies greater distance from a reference output.IS-FP-GT AATT-Dependency-drws ADIT-FBI-CBSR AATT-Template-drws AATT-TemplateS-drws AGRAPH-4+B ADIT-TVAS-RBR ANIL-UCM-FVBS AATT-PermuteRank-drws AATT-Dependency-ws AATT-TemplateS-ws AATT-Template-ws AATT-PermuteRank-ws AOSU-GP BTable 9: Homogeneous subsets for systems in TUNA-REG, Edit Distance measure.
Systems which do not sharea common letter are significantly different at p ?
.05Edit Accuracy NIST BLEUEdit -0.584?
0.250 -0.636?Accuracy -0.584?
0.383 0.807?
?NIST 0.250 0.383 0.371BLEU -0.636?
0.807??
0.371Table 10: Correlations for TUNA-REG (?
indicates p ?.05; ??
indicates p ?
.01).4.3.2 Tests on the extrinsic measuresTable 11 displays the results for the extrinsic mea-sures.
Reading time (RT), identification time (IT)and error rate (ER), are displayed only for the sys-tems that participated in the evaluation experiment,as well as for the two sets of reference outputsHUMAN-1 and HUMAN-2.Separate univariate ANOVAs were conducted test-ing the effect of SYSTEM and ENTITY TYPE on ITand RT.
For IT, there was a significant main effectof SYSTEM (F (11, 1409) = 5.66, p < .001) andENTITY TYPE (F (1, 1409) = 23.507, p < .001),as well as a significant interaction (F (11, 1409) =2.378, p < .05).
The same pattern held for RT, witha main effect of SYSTEM (F (11, 1412) = 9.95, p <.001) and ENTITY TYPE (F (1, 1412) = 9.74, p <.05) and a significant interaction (F (11, 1412) =2.064, p < .05).
A Kruskall-Wallis test conductedon ER showed a significant impact of SYSTEM on theextent to which experimental participants identifiedthe wrong referents (?2 = 35.45, p < .001).
Thehomogeneous subsets yielded by post-hoc Tukey?scomparisons among systems, on both RT and IT, aredisplayed in Table 12.Finally, pairwise correlations were estimated be-tween all three extrinsic measures.
The only sig-nificant correlation was between RT and IT (r =.784, p < .05), suggesting that the longer experi-mental subjects took to read a description, the longerthey also took to identify the target referent.5 ConclusionThe first ASGRE Challenge, held in 2007, was re-garded and presented as a pilot event, for a researchcommunity in which there was growing interest incomparative evaluation on shared datasets.
Refer-ring Expression Generation was an ideal startingpoint, because of its relatively long history withinthe NLG community, and the widespread agreementon inputs, outputs and task definitions.The tasks described and evaluated in this reportconstitute a broadening of scope over the 2007 Chal-lenge.
Like the previous Challenge, the 2008 editionemphasised diversity in terms of the measures ofquality used.
This year, there was also an increasedemphasis on broadening the range of tasks, with theinclusion of realisation and end-to-end referring ex-pressions generation.
This extends the scope of theREG problem, which has traditionally been focussedon content determination (attribute selection) for themost part.
As for evaluation, the diversity of mea-sures can shed light on different aspects of qualityin these tasks.
The fact that the correlation amongmeasures based on different quality criteria is notstraightforward is in itself an argument for maintain-ing this diversity, particularly as comparative evalu-ation exercises such as this one provide the oppor-tunity for further investigation of the nature of theserelationships.Another indicator of the growing diversity in thisyear?s Challenge is the range of algorithmic solu-tions in the three tasks, ranging from new modelsbased on classical algorithms, to data-driven meth-ods, evolutionary algorithms, and graph- and tree-based frameworks.
The body of work representedby submissions to the TUNA-R and TUNA-REG tasksis also interesting for its exploration of how to apply205RT IT ERfurniture people both SD furniture people both SD furniture people bothHUMAN-1 2155.376 2187.737 2171.693 2036.462 1973.369 1911.742 1942.297 809.5139 11.864 6.780 9.322OSU-GP 2080.532 3204.198 2637.644 1555.003 2063.441 2274.690 2167.275 682.8325 6.667 18.966 12.712HUMAN-2 1823.553 2298.467 2061.010 1475.005 1873.621 1945.880 1909.750 761.3386 16.667 5.000 10.833ATT-PremuteRank-drws 1664.911 1420.087 1543.528 1392.729 1765.731 1719.456 1742.788 675.3462 10.000 8.475 9.244DIT-FBI-CBSR 1581.535 1521.799 1551.667 1170.031 1528.119 1932.806 1732.163 694.9878 10.169 10.000 10.084NIL-UCM-FVBS 1561.291 1933.833 1747.562 1428.490 1531.378 1723.148 1627.263 672.9894 6.667 3.333 5.000GRAPH 1499.582 1516.804 1508.193 952.158 1706.153 2026.268 1866.211 704.0210 5.000 5.000 5.000DIT-TVAS-RBR 1485.149 1442.573 1463.861 998.332 1559.953 1734.853 1647.403 588.4615 8.333 13.333 10.833ATT-Dependency-drws 1460.152 1583.887 1522.019 1177.817 1505.059 2078.336 1791.697 725.9459 1.667 18.333 10.000ATT-TemplateS-drws 1341.245 1641.539 1490.130 1098.304 1656.401 1720.365 1687.841 650.8357 3.333 10.345 6.780IS-FT-GT 1292.754 1614.712 1453.733 1374.652 1616.855 1884.557 1750.706 732.4362 6.667 1.667 4.167ATT-PermuteRank-ws 1218.136 1450.603 1334.369 1203.975 1876.680 1831.485 1854.082 688.3493 31.667 13.333 22.500Table 11: Descriptives for the extrinsic measures in TUNA-REG.IT RTNIL-UCM-FVBS A ATT-PermuteRank-ws ADIT-TVAS-RBR A IS-FT-GT AATT-TemplateS-drws A B DIT-TVAS-RBR ADIT-FBI-CBSR A B ATT-TemplateS-drws AATT-PremuteRank-drws A B GRAPH-4+B A BIS-FT-GT A B ATT-Dependency-drws A BATT-Dependency-drws A B ATT-PremuteRank-drws A BATT-PermuteRank-ws A B DIT-FBI-CBSR A BGRAPH-4+B A B NIL-UCM-FVBS A B CHUMAN-2 A B C HUMAN-2 B CHUMAN-1 B C HUMAN-1 C DOSU-GP C OSU-GP DTable 12: Homogeneous subsets for systems in TUNA-REG, extrinsic time measures.
Systems which do not share acommon letter are significantly different at p ?
.05realisation techniques to the specific problem posedby referring expressions.The outcomes of this evaluation exercise are ob-viously not intended to be a ?final word?
on the rightway to carry out evaluation in referring expressionsgeneration.
Rather, comparative results open up thepossibility of improvement and change.
Another im-portant aspect of a shared task of this nature is thatit results in an archive of data that can be furtherexploited, either through follow-up studies, or forthe provision of baselines against which to comparenovel approaches.
We have already used the datafrom ASGRE 2007 for further investigation, particu-larly in the area of extrinsic evaluation.
We plan tocarry out more such studies in the future.AcknowledgementsOur heartfelt thanks to the participants who helpedto make this event a success.
Thanks to Advaith Sid-dharthan, who proposed MASI for TUNA-AS.ReferencesA.
Belz and A. Gatt.
2007.
The attribute selection forgre challenge: Overview and evaluation results.
InProc.
UCNLG+MT: Language Generation and Ma-chine Translation.K.
I. Forster and J. C. Forster.
2003.
DMDX: A win-dows display program with millisecond accuracy.
Be-havior Research Methods, Instruments, & Computers,35(1):116?124.A.
Gatt and A. Belz.
2008.
Attribute selection for refer-ring expression generation: New algorithms and evalu-ation methods.
In Proceedings of the 5th InternationalConference on Natural Language Generation (INLG-08).A.
Gatt, I. van der Sluis, and K. van Deemter.
2007.Evaluating algorithms for the generation of referringexpressions using a balanced corpus.
In Proc.
11thEuropean Workshop on Natural Language Generation(ENLG-07).S.
Papineni, T. Roukos, W. Ward, and W. Zhu.
2002.Bleu: a. method for automatic evaluation of machinetranslation.
In Proc.
40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-02), pages311?318.R.
Passonneau.
2006.
Measuring agreement on set-valued items (MASI) for semantic and pragmatic anno-tation.
In Proc.
5th International Conference on Lan-guage Resources and Evaluation (LREC-06).R.
Ratliff.
1993.
Methods for dealing with reaction timeoutliers.
Psychological Bulletin, 114(3):510?532.206
