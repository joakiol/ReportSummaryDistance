Protein Name Tagging for Biomedical Annotation in TextKaoru Yamamoto?
Taku Kudo?
Akihiko Konagaya?
Yuji Matsumoto?
?Genomic Sciences Center, The Institute of Physical and Chemical Research1-7-22-E209, Suehiro-cho, Tsurumi-ku, Yokohama, 230-0045 Japankaorux@gsc.riken.go.jp, konagaya@gsc.riken.go.jp?Graduate School of Information Science, Nara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192 Japantaku-ku@is.aist-nara.ac.jp, matsu@is.aist-nara.ac.jpAbstractWe explore the use of morphological anal-ysis as preprocessing for protein nametagging.
Our method finds protein namesby chunking based on a morpheme, thesmallest unit determined by the morpho-logical analysis.
This helps to recognizethe exact boundaries of protein names.Moreover, our morphological analyzercan deal with compounds.
This offersa simple way to adapt name descriptionsfrom biomedical resources for languageprocessing.
Using GENIA corpus 3.01,our method attains f-score of 70 points forprotein molecule names, and 75 points forprotein names including molecules, fami-lies and domains.1 IntroductionThis paper describes a protein name tagging methodwhich is a fundamental precursor to information ex-traction of protein-protein interactions (PPIs) fromMEDLINE abstracts.
Previous work in bio-entity(including protein) recognition can be categorizedinto three approaches: (a) exact and approximatestring matching (Hanisch et al, 2003), (b) hand-crafted rule-based approaches (Fukuda et al, 1998)(Olsson et al, 2002), and (c) machine learning (Col-lier et al, 2000), (Kazama et al, 2002).Previous approaches in (b) and (c) ignore thefact that bio-entities have boundary ambiguities.Unlike general English, a space character is nota sufficient token delimiter.
Moreover, name de-scriptions in biomedical resources are mostly com-pounds.
A conventional English preprocessing un-dergoes a pipeline of simple tokenization and part-of-speech tagging.
The tokenization is based on agraphic word1 for the subsequent part-of-speech tag-ging to work.
The conventional paradigm does notproperly handle peculiarities of biomedical English.To remedy the problem, we propose morphologicalanalysis which achieves sophisticated tokenizationand adapts biomedical resources effectively.Our method identifies protein names by chunk-ing based on morphemes, the smallest units deter-mined by morphological analysis.
We do not usegraphic words as a unit of chunking to avoid theunder-segmentation problem.
Suppose that a pro-tein name appears as a substring of a graphic word.Chunking based on graphic words fails, becausegraphic words are too coarsely segmented.
Instead,chunking based on morpheme overcomes the prob-lem, and the exact boundaries of protein names arebetter recognized.Below, we describe our method of protein nametagging, including preprocessing, feature extraction(Section 2), and experimental results (Section 3).We mention related work in bio-entity recognition(Section 4) and give concluding remarks (Section 5).2 Protein Name TaggingOur task is to identify non-overlapping strings thatrepresent protein names in text.
Figure 1 gives an1A graphic word is defined to be a string of contiguousalphanumeric characters with spaces on either sides; may in-clude hyphens and apostrophes, but no other punctuation marks.Quoted from p.125 in Manning and Schu?tze (1999).Plain SentenceProtein Name Tagged SentenceMorphologicalAnalysisBase NPRecognitionFeatureExtractionSVM-basedchunkingFigure 1: Overview of our protein name taggingmethod.an SLP-76-associated substrate ...D                 D         DMMMMan SLP-76-associated substratem m mmmm m mw w wwFigure 2: Definition of terms used in this paper.
?is a cps start and ?
a cps end.
M is a mark andD is a delimiter.
The cps starts and cps ends canbe determined by marks M and delimiters D. ?
isa token found in the dictionary by common prefixsearch.
A bold ?
is the optimal path in the trellis.w is a word.
m is a morpheme.overview.
A plain sentence undergoes morphologi-cal analysis and BaseNP recognition.
The latter pre-processing is to reflect an intuition that most proteinnames are found in noun phrases.
We extract fea-tures from these preprocessing, and represent themas feature vectors.
SVM-based chunking is per-formed using the features to yield a protein nametagged sentence.2.1 Morphological AnalysisOur morphological analysis gives (a) sophisticatedtokenization, (b) part-of-speech tagging and (c) an-notation of value-added information such as thestemmed form of a word, accession numbers tobiomedical resources.
Our morphological analyzerfor biomedical English, cocab2, is inspired by thework of Yamashita and Matsumoto (2000).2.1.1 PreliminariesWe first define terms used in this paper with anillustration in Figure 2.A lexeme is an entry in a dictionary.
A commonprefix search (cps) is a standard technique for look-ing up lexemes in morphological analysis of non-segmented languages.
A dictionary is often a triedata structure so that all possible lexemes that matchwith the prefix starting at a given position in thesentence are retrieved efficiently.
A common pre-fix search start position (cps start) is a position in asentence at which a dictionary lookup can start.
Acommon prefix search end position (cps end) is a po-sition in a sentence by which a matched lexeme mustend.A token is a substring in a sentence which matcheswith a lexeme in the dictionary, and is enclosed by acps start and a cps end.
Note that the matched lex-eme is retrieved from the dictionary by common pre-fix search.
A mark is a special symbol or substringthat by itself can form a token even when it appearswithin a graphic word.
A delimiter is a special sym-bol or code that by itself cannot form a token butcan work to delimit tokens.
Note also that a delim-iter cannot appear on the boundaries of a token, butcan appear inside a token.
Examples of marks anddelimiters are shown in Table 1.A word is a substring in a sentence of which seg-mentation boundary is determined by the morpho-2http://bmkd.gsc.riken.go.jp/?kaorux/r/cocab.html.enTable 1: Marks and delimiters used in cocab.
Marksinclude transcription of Greek alphabets that oftenappear in MEDLINE abstracts.Delimiter Markspace .,:;??%/[]{}!
?%$&-()tab 0123456789CR/LF alpha beta gamma delta epsilonkappa sigma zetalogical analysis.
A morpheme is the smallest unit ofa word which is enclosed a cps start and the nearestcps end to the cps start.The task of morphological analysis is to find thebest pair ?W ?, T ??
of word segmentation W ?
=w?1, .
.
.
, w?n and its parts of speech assignment T ?
=t?1, .
.
.
, t?n, in the sense that the joint probability ofthe word sequence and the tag sequence P (W,T ) ismaximized when W = W ?
and T = T ?.
Formally,?W ?, T ??
= argmax?W,T ?P (W,T ).
(1)The approximate solution for this equation is givenbyW ?
= argmaxWmaxTP (T |W )= argmaxWmaxTP (W |T )P (T )P (W )= argmaxWmaxTP (W |T )P (T )' argmaxWmaxT?ip(wi|ti)p(ti|ti?2, ti?1)andT ?
= argmaxTP (T |W ?)'
argmaxT?ip(w?i |ti)p(ti|ti?2, ti?1).2.1.2 Lexeme-based TokenizationIn order to avoid spurious segmentation, we de-termine cps starts and cps ends in a sentence.
Marksand delimiters in a sentence are used to find cpsstarts and cps ends in the sentence, shown as ?
and?
respectively in Figure 2.Once cps starts and cps ends are determined, theproblem is to solve the equation of morphologicalanalysis.
It consists of (a) finding a set of tokensthat match lexemes in the dictionary, (b) building atrellis from the tokens, and (c) running a Viterbi-likedynamic programming on the trellis to find the paththat best explains the input sentence.In Figure 2, ?
indicates tokens.
Both ?SLP-76?
and ?SLP-76-associated?substrate?
(?
denotes aspace character) are tokens since they are lexemes inthe dictionary, but ?SLP-76-?
is not a token since itis not a lexeme in the dictionary.
It allows a lexeme-based tokenization which can accommodate a tokenthat is shorter than, the same as, or longer than agraphic word.The optimal path in the trellis gives a sequenceof words that the input sentence is ?best?
tokenizedand part-of-speech tagged.
This is the word-basedoutput, shown as a sequence of w in Figure 2.
Inaddition, our morphological analyzer produces themorpheme-based output, given the word-based out-put.
This is a sequence of the smallest units ineach segmented word, shown as a sequence of min Figure 2.
Our chunking is based on morphemesand takes note of words as features to overcome theunder-segmentation problem.2.1.3 Adapting Biomedical ResourcesGENIA Corpus 3.0p3 is used to calculate a wordprobability p(w|t), and a tag probability p(t|t?, t??
)which is modeled by a simple trigram.
To bettercope with biomedical English, we enhance the dic-tionary (i.e.
p(w|t)) in a number of ways.First, we collect human protein names (includingsynonyms) and their accession numbers from pro-tein sequence repositories, SwissProt (SP) (Boeck-mann et al, 2003) and Protein Information Re-source (PIR) (Wu et al, 2002).
We convert eachentry description to a lexeme.
A part-of-speech ofthe lexeme is set to a common noun (NN ) wherethe minimum word probability of NN is assignedfor p(w|t).
An accession number of the entry isalso recorded in the miscellaneous information fieldof the lexeme.
Similarly, Gene Ontology (GO)(Consortium., 2000) terms are converted to lexemeswhere accession number as well as the root cate-gory are kept in the miscellaneous information field.Third, we use UMLS Specialist Lexicon (NLM,2002) to obtain the stemmed form of a lexeme.
Afinal twist is to associate constituent information foreach compound lexeme.
A lexeme is compound if3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/Table 2: A compound lexeme example.
?cost?
islog of an inverse of p(w|t).
?constituents?
are ob-tained from searching single lexemes in the dictio-nary.
?sp?
and ?pir?
are associated with accessionnumbers.
?go?
is associated with an accession num-ber and a root category from molecular function, bi-ological process, and cellular component.key valuesurface string ERK activator kinase 1cost 10583part-of-speech NNreference sp:Q02750stemmed form ?constituents ERK pir:JQ1400,sp:P29323activatorkinase go:016301:molecular function1it consists of multiple morphemes, and single other-wise.
An example of a compound lexeme is shownin Table 2.In the conventional paradigm, a token cannot havea white space character.
However, 71.6 % of namedescription in SP are entries of multiple graphicwords.
This has been a bottleneck in adaptingbiomedical resources into language processing.
Incontrast, our morphological analysis can deal with alexeme with a white space character, and thus offersa simple way to incorporate biomedical resources inlanguage processing.
When a sentence is morpho-logically analyzed, miscellaneous information fieldis attached, which can be used for the feature extrac-tion component.2.2 BaseNP RecognitionBaseNP recognition is applied to obtain approxi-mate boundaries of BaseNPs in a sentence.
TheCoNLL-1999 shared task dataset is used for train-ing with YamCha, the general purpose SVM-basedchunker4.
There are four kinds of chunk tags in theCoNLL-1999 dataset, namely IOB1, IOB2, IOE1,and IOE2 (Tjong Kim Sang and Veenstra, 1999).We follow Kudo and Matsumoto (2001) to train fourBaseNP recognizers, one for each chunk tag.
Theword-based output from the morphological analy-sis is cascaded to each BaseNP recognizer to markBaseNP boundaries.
We collect outputs from the4http://cl.aist-nara.ac.jp/?taku?ku/software/yamcha/).four recognizers, and interpret the tag as outside ofa BaseNP if all recognizers estimate the ?O(utside)?tag, otherwise inside of a BaseNP.
The intention isto distinguish words that are definitely not a con-stituent of a BaseNP (outside) from words that maybe a constituent of a BaseNP (inside).
In this way,we obtain approximate boundaries of BaseNPs in asentence.Introducing BaseNP recognition as a part of pre-processing is motivated by an intuition that mostprotein names reside in a noun phrase.
Our chunkingis based on morphemes.
An indication of whether amorpheme lies within or outside a BaseNP bound-ary seems informative.
In addition, the morpheme-based chunking would have narrower local contextthan the word-based chunking for the same windowsize.
Our intention of approximate BaseNP bound-aries is to provide the feature extraction compo-nent with the top-down information of morpheme?sglobal scope.2.3 Feature ExtractionWe extract four kinds of features from preprocess-ing.
Morphological analysis gives information forboundary features, lexical features and biomedicalfeatures.
BaseNP recognition gives information forsyntactic features.2.3.1 Boundary FeatureOur chunking is based on morphemes of whichboundaries may or may not coincide with graphicwords.
The boundary feature is to reflect the fol-lowing observation.
A general English word tendsto have the same morpheme-based segmentation andword-based segmentation, i.e.
the degree of bound-ary ambiguity is low.
On the other hand, a pro-tein coding word tends to have different morpheme-based segmentation and word-based segmentation,i.e., the degree of boundary ambiguity is high.For each morpheme, we have four binary featureslmor, ldel, rmor, and rdel.
lmor is 1 if the morphemeis the leftmost morpheme of a word tokenized bythe morphological analyzer, and 0 otherwise.
ldelis 1 if the morpheme is the leftmost morpheme of agraphic word, and 0 otherwise.
Similarly, rmor is1 if the morpheme is the rightmost morpheme of aword tokenized by the morphological analyzer, and0 otherwise.
rdel is 1 if the morpheme is the right-most morpheme of a graphic word, and 0 otherwise.2.3.2 Lexical FeatureThe lexical features are multi-valued features.In this work, we consider part-of-speech, stemmedform and string features (e.g.
lower-cased string,upper-case letter, numerals, prefix and suffix).2.3.3 Biomedical FeatureThe biomedical feature is designed to encodebiomedical domain resource information.
The mor-phological analyzer tokenizes into words with rel-evant references to biomedical resources.
In addi-tion, if the word is derived from a compound lexeme,constituent morpheme information is also attached.
(Recall Table 2 for a compound lexeme example.
)The biomedical feature is subdivided into a se-quence feature and an ontology feature.
The se-quence feature refers to a binary feature of accessionnumber reference to SP or PIR.
For each word, sp-word is set to 1 if the word has an accession numberof SP.
For each morpheme, sp-morpheme is set to1 if the morpheme has an accession number of SP.pir-word and pir-morpheme of PIR are the same asthose of SP.
The ontology feature refers to a binaryfeature of accession number reference to GO.
Wehave go-word and go-morpheme for GO.
Supposea sentence contains a compound lexeme in Table 2.For the word ?ERK activator kinase 1?, sp-word isset to 1, but pir-word and go-word are set to 0.
Forthe morpheme ?ERK?, both sp-morpheme and pir-morpheme are set to 1, but go-morpheme is set to0.If sp-word or pir-word are set to 1, it means thatthe word exactly matches with a protein name de-scription in SP or PIR.
Unfortunately, it is rare dueto variant writing of protein names.
However, wecan expect a sort of approximate matching, by con-sidering morpheme-based features sp-morpheme orpir-morpheme.
Moreover, we add ontology fea-tures (go-word, go-morpheme) in order to obtainthesaurus effects.2.3.4 Syntactic FeatureThe syntactic feature is to reflect an intuition thatmost protein names are found in noun phrases.We use two syntactic features, an indicator mor-pheme feature and a headmorpheme candidate fea-ture.
Both features are relevant only for BaseNPconstituent morphemes.Fukuda et al (1998) observe that terms such as?receptor?
or ?enzyme?
that describe the function orcharacteristic of a protein tend to occur in or nearbya protein name.
They use those terms as indicatorsof presence of a protein name.
We also express themas a indicator morpheme feature, but with an addi-tional constraint that indicators are only influentialto morphemes found in the same BaseNP.In addition, Arabic and Roman numerals and tran-scription of Greek alphabets are frequently used tospecify an individual protein.
We call those speci-fiers in this paper.
Without a deep analysis of com-pound words, it is hard to determine the morphemethat a specifier depends on, since the specifier couldbe on the left (?alpha-2 catenin?)
or on the right (?in-terleukin 2?)
of the head morpheme.
We assumethat such specifier morpheme and its head candidatemorpheme exist within the same BaseNP boundaryand express the observation as the headmorphemecandidate feature for each specifier morpheme.With the absence of a powerful parser, the syn-tactic features provides only approximation.
How-ever, indicator morpheme suggests a protein nameexistence and headmorpheme candidate intends todiscriminate specifiers appear nearby protein-codingmorphemes from the rest.2.4 Chunking as Sequential ClassificationOur protein name tagging is formulated asIOB2/IOE2 chunking (Tjong Kim Sang and Veen-stra, 1999).
Essentially, our method is the same asKudo and Matsumoto (2001) in viewing the task asa sequence of classifying each chunk label by SVM.The main difference is that our chunking is based onmorphemes, and uses features described in Section2.3 to serve the needs in protein name tagging.3 Experimental Results3.1 Experiment using Yapex CorpusWe first conduct experiments with Yapex corpus5,the same corpus used in Olsson et al (2002) to geta direct comparison with the good-performing rule-based approach6.
There are 99 abstracts for training5http://www.sics.se/humle/projects/prothalt/6Olsson et al (2002) claim they outperformFukuda et al (1998) evaluated with Yapex corpus.
Todate, Fukuda et al (1998) reports the best result in rule-basedapproach, evaluated with their closed corpus.Table 3: Parameter used in the SVM-based chun-ker YamCha.
See (Kudo and Matsumoto, 2001) formore information about parameters.parameter descriptiontype of kernel polynomialdegree of kernel 2direction of parsing foreward for IOB2, backward for IOE2context window -2 -1, 0, +1, +2multi-class one-vs-restTable 4: Evaluation criteria used in this paper.criteria descriptionstrict count Correct if the boundaries of system andthose of answer matches on Both side.left count Correct if the Left boundary of system andthat of answer matches.right count Correct if the Right boundary of system andthat of answer matches.sloppy count Correct if any morpheme estimated by systemoverlaps with any morpheme defined by answer.and 101 abstracts for testing.Each sentence undergoes preprocessing, featureextraction and SVM-based chunking to obtain a pro-tein name tagged sentence.
We also use YamChafor this task.
Parameters for YamCha are summa-rized in Table 3.
Our evaluation criteria follow thatof Olsson et al (2002).
We calculate the standardmeasures of precision, recall and f-score for eachboundary condition of strict, left, right and sloppydescribed in Table 4.The performance of our method on Yapex corpusis summarized in Tables 5 and 6, along with thatof Yapex protein tagger.7.
Our method achieves asgood result as a hand-crafted rule-based approach,despite the small set of training data (99 abstracts)which works unfavorable to machine learning ap-proaches.
The better performance in strict could beattributed to chunking based on morphemes insteadof words.Yapex has a good recall rate while our methodenjoys a good precision in all boundary conditions.A possible explanation for the low recall is that thetraining data was small (99 abstracts) for SVM togeneralize the characteristics of protein names.
As7Results reported in Olsson et al (2002) are different fromthe Yapex web site.
Gunnar Eriksson has indicated us to quotethe web site as the performance of Yapex protein tagger.Table 5: Results on Yapex corpus (99 abstracts fortraining and 101 abstracts for testing).
P(precision),R(recall) and F(f-score) are shown.
The table showsthe protein tagger with an IOB2 chunking with for-ward parsing.Yapex Protein Tagger SVM (IOB2,forward)P R F P R Fstrict 0.620 0.599 0.610 0.738 0.557 0.635left 0.706 0.682 0.693 0.827 0.625 0.712right 0.749 0.723 0.736 0.789 0.596 0.679sloppy 0.843 0.814 0.828 0.892 0.674 0.768Table 6: The table shows the protein tagger with anIOE2 chunking with backward parsing.Yapex Protein Tagger SVM (IOE2,backward)P R F P R Fstrict 0.620 0.599 0.610 0.738 0.554 0.633left 0.706 0.682 0.693 0.801 0.602 0.688right 0.749 0.723 0.736 0.797 0.599 0.684sloppy 0.843 0.814 0.828 0.880 0.661 0.755we will shortly report in the next subsection, weno longer observe a low recall when training withthe medium-sized (590 abstracts) and the large-sized(1600 abstracts) data.IOB2 chunking with forward parsing gives bet-ter results in left, while IOE2 chunking with back-ward parsing gives better results in right.
The re-sult follows our intuition that IOB2 chunking with aforward parsing intensively learns the left boundarybetween B(egin) and O(utside), while IOE2 chunk-ing with a backward parsing intensively learns theright boundary between E(nd) and O(utside).
Useof a weighted voting of multiple system outputs, asdiscussed in (Kudo and Matsumoto, 2001), is left forfuture research.Effects of each feature in IOB2 chunking with for-ward parsing are summarized in Table 7.
Each fea-ture is assessed by subtracting the focused featurefrom the maximal model in Table 5.
Since the testdataset is only 101 abstracts, it is difficult to observeany statistical significance.
Based on the offsets, theresult suggests that an incorporation of biomedicalfeatures (sequence and ontology) is crucial in pro-tein name tagging.
The contribution of syntactic fea-tures is not as significant as we originally expect.Considering syntactic features we use are approxi-mate features obtained from BaseNP boundaries, theoutcome may be inevitable.
We plan to investigateTable 7: Effects of each feature contribution on strictboundary condition.
The F-score is subtracted fromthe maximal model in IOB2 chuking with forwardparsing (Table 5).
The upper rows show effects of asingle feature removed.
The lower rows show effectsof multiple features with the same class removed.See Section 2.3 for description of each feature.feature F offset ranksequence 0.599 -0.036 1part-of-speech 0.614 -0.021 2string 0.615 -0.020 3ldel and rdel 0.628 -0.007 4indicator term 0.628 -0.007 4headmorpheme candidate 0.632 -0.003 6ontology 0.633 -0.002 7stemmed form 0.634 -0.001 8biomedical 0.594 -0.041 1lexical 0.598 -0.037 2syntactic 0.623 -0.012 3boundary 0.627 -0.008 4further into effective syntactic features such as worddependency from a word dependency parser.3.2 Experiment with GENIA CorpusIn order to experiment our method with a largerdataset, we use GENIA corpus 3.01 released re-cently.
Unlike Yapex corpus, GENIA corpus con-tains 2000 abstracts and uses a hierarchical tagset.For our experiment, we use two definitions for a pro-tein: one to identify G#protein molecule andthe other to identify G#protein X.
The former isa narrower sense of protein names, and more closeto a protein name in Yapex corpus where the proteinname is defined as something that denotes a singlebiological entity composed of one or more aminoacid chain.
The latter covers a broader sense of pro-tein, including families and domains.
We evaluateour method with the two versions of protein namessince the desired granularity of a protein name de-pends on the application.Two datasets are prepared in this experiment.
Oneis GENIA 1.1 subset and the other is GENIA 3.01set.
The GENIA 1.1 subset contains 670 abstractsfrom GENIA 3.01 where the same Medline IDs arealso found in GENIA corpus 1.1.
In addition, wesplit the GENIA 1.1 subset into the test dataset of80 abstracts used in Kazama et al (2002)8 and thetraining dataset of the remaining 590 abstracts.
The8http://www-tsujii.is.s.u-tokyo.ac.jp/ kazama/papers/testidTable 8: Results on GENIA 1.1 subset of 670 ab-stracts (590 abstracts for training and 80 abstractsfor testing).G#protein molecule G#protein XP R F P R Fstrict 0.657 0.604 0.629 0.694 0.695 0.694left 0.687 0.632 0.658 0.755 0.755 0.755right 0.697 0.641 0.667 0.757 0.757 0.757sloppy 0.727 0.669 0.697 0.827 0.828 0.827Table 9: Results on GENIA 3.01 set of 2000 ab-stracts (1600 abstracts for training and 400 abstractsfor testing).G#protein molecule G#protein XP R F P R Fstrict 0.711 0.683 0.697 0.757 0.742 0.749left 0.742 0.712 0.726 0.804 0.788 0.796right 0.752 0.722 0.737 0.805 0.789 0.797sloppy 0.787 0.755 0.771 0.858 0.841 0.850GENIA 3.01 set is an entire set of GENIA corpus3.01.
We randomly split the entire set so that 4/5 ofwhich is used for training the remaining 1/5 is usedfor testing.Results in Tables 8 and 9 show that the broaderclass G#protein X is easier to learn than the nar-rower class G#protein molecule.
Results ofprotein name recognition in Kazama et al (2002)using GENIA 1.1 are 0.492, 0.664 and 0.565 forprecision, recall, f-score respectively.
GENIA1.1 has only one class for protein name (GE-NIA#protein), while GENIA 3.01 has hierarchi-cally organized tags for a protein name class.
As-suming that GENIA#protein in GENIA 1.1 cor-responds to G#protein X in GENIA 3.01, wecould claim that our method gives better resultsto their SVM approach.
The better performancecould be attributed to chunking based on morphemeinstead of graphic words and better adaptation ofbiomedical resources.
Next, we compare Yapexperformance with G#protein molecule trainedwith 1600 abstracts (cf.
Table 5 and Table 9), thoughtagging policy and corpus are different.
Our methodsignificantly outperforms in strict, better in left andright, slightly lost in sloppy.
With a large dataset oftraining data (1600 abstracts), we obtain 70 points off-score for G#protein molecule and 75 pointsof f-score for G#protein X, which are compara-ble to approaches reported in the literature.An increase of training data from 590 abstractsto 1600 abstracts helps the overall performance im-prove, given the corpus error is minimized.
Ourinternal experiments with GENIA 3.0 (the versionwas corrected to GENIA 3.01) reveal that the cor-pus error is critical in our method.
Even corpus er-rors have been successfully removed, it would notbe practical to increase the size of labor-intensiveannotated corpus.
Use of unlabeled data in con-junction with a small but quality set of labeled data.e.g.
Collins and Singer (1999), would have to be ex-plored.4 Related WorkTanabe and Wilbur (2002) use a hybrid approach oftransformation-based learning (Brill Tagger) withrule-based post processing.
An obvious drawback intheir approach as with other rule-based approachesincluding Fukuda et al (1998) is that the approachescannot handle many correlated features.
As pointedout in their paper, errors in the early stage of ruleapplication are often propagated to the later stage,damaging the overall performance.
In contrast, ourmethod can deal with correlated features owing tothe generalization characteristic of SVM.5 ConclusionThis paper describes a method to find protein namesby chunking based on a morpheme, which leads tobetter recognition of protein name boundaries.
Forthis, we propose morphological analysis of whichcore technologies are found in non-segmented lan-guages.
With the large dataset (1600 abstracts fortraining and 400 abstracts for testing in GENIA3.01), we obtain f-score of 70 points for proteinmolecule names and 75 points for protein names, in-cluding molecules, families, domains etc.
The re-sults are comparable to previous approaches in theliterature.
We focus protein names as a case study.However, given annotated corpus of similar size andquality, the same approach can be applied to otherbio-entities such as gene names.AcknowledgmentWe would like to thank Masahi Shimbo of NAISTfor his careful review and helpful suggestions.
Ourappreciation also goes to development teams ofYapex corpus and GENIA corpus to make the sharedresources publicly available.ReferencesB.
Boeckmann, A. Bairoch, R. Apweiler, M.-C. Blatter,A.
Estreicher, E. Gasteiger, M.J. Martin, K. Michoud,C.
O?Donovan, I. Phan, S. Pilbout, and M. Schneider.2003.
The SWISS-PROT protein knowledgebase andits supplement TrEMBL.
Nucleic Acids Res., 31:365?370.N.
Collier, C. Nobata, and J. Tsujii.
2000.
Extractingthe Names of Genes and Gene Products with a HiddenMarkov Model.
COLING, pages 201?207.M.
Collins and Y.
Singer.
1999.
Unsupervised Modelsfor Named Entity Classification.
EMNLP-VLC, pages100?110.The Gene Ontology Consortium.
2000.
Gene ontology:tool for the unification of biology.
Nature Genetics,25:25?29.K.
Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.
1998.Toward information extraction: identifying proteinnames from biological papers.
PSB, pages 705?716.D.
Hanisch, J. Fluck, HT.
Mevissen, and R. Zimmer.2003.
Playing biology?s name game: identifying pro-tein names in scientific text.
PSB, pages 403?414.J.
Kazama, T. Makino, Y. Ohta, and J. Tsujii.
2002.
Tun-ing Support Vector Machines for Biomedical NamedEntity Recognition.
ACL Workshop on NLP inBiomedical Domain, pages 1?8.T.
Kudo and Y. Matsumoto.
2001.
Chunking with Sup-port Vector Machines.
NAACL, pages 192?199.C.D.
Manning and Schu?tze.
1999.
Foundations of Statis-tical Natural Language Processing.
The MIT Press.NLM.
2002.
UMLS Knowledge Sources.
13th edition.F.
Olsson, G. Eriksson, K. Franzen, L. Asker, andP.
Lide?n.
2002.
Notions of Correctness when Evaluat-ing Protein Name Tagger.
COLING, pages 765?771.L.
Tanabe and W. J. Wilbur.
2002.
Tagging gene andprotein names in biomedical text.
Bioinformatics,18(8):1124?1132.E.F.
Tjong Kim Sang and J. Veenstra.
1999.
Represent-ing Text Chunks.
EACL, pages 173?179.C.H.
Wu, H. Huang, L. Arminski, J. Castro-Alvear,Y.
Chen, Z.-Z.
Hu, R.S.
Ledley, K.C.
Lewis, H.-W.Mewes, B.C.
Orcutt, B.E.
Suzek, A. Tsugita, C.R.Vinayaka, L.-S.L.
Yeh, J. Zhang, and W.C. Barker.2002.
The Protein Information Resource: an inte-grated public resource of functional annotation of pro-teins.
Nucleic Acids Res., 30:35?37.T.
Yamashita and Y. Matsumoto.
2000.
Language Inde-pendent Morphological Analysis.
6th Applied NaturalLanguage Processing Conference, pages 232?238.
