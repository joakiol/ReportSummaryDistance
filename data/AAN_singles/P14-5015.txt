Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 85?90,Baltimore, Maryland USA, June 23-24, 2014.c?2014 Association for Computational LinguisticskLogNLP: Graph Kernel?based Relational Learning of Natural LanguageMathias Verbeke?Paolo Frasconi?Kurt De Grave?Fabrizio Costa?Luc De Raedt?
?Department of Computer Science, KU Leuven, Belgium{mathias.verbeke, kurt.degrave, luc.deraedt}@cs.kuleuven.be?Dipartimento di Sistemi e Informatica, Universit`a degli Studi di Firenze, Italy,p-f@dsi.unifi.it?Institut f?ur Informatik, Albert-Ludwigs-Universit?at, Germany,costa@informatik.uni-freiburg.deAbstractkLog is a framework for kernel-basedlearning that has already proven success-ful in solving a number of relational tasksin natural language processing.
In this pa-per, we present kLogNLP, a natural lan-guage processing module for kLog.
Thismodule enriches kLog with NLP-specificpreprocessors, enabling the use of exist-ing libraries and toolkits within an elegantand powerful declarative machine learn-ing framework.
The resulting relationalmodel of the domain can be extended byspecifying additional relational features ina declarative way using a logic program-ming language.
This declarative approachoffers a flexible way of experimentationand a way to insert domain knowledge.1 IntroductionkLog (Frasconi et al., 2012) is a logical and re-lational language for kernel-based learning.
It hasalready proven successful for several tasks in com-puter vision (Antanas et al., 2012; Antanas et al.,2013) and natural language processing.
For ex-ample, in the case of binary sentence classifica-tion, we have shown an increase of 1.2 percentin F1-score on the best performing system in theCoNLL 2010 Shared Task on hedge cue detec-tion (Wikipedia dataset) (Verbeke et al., 2012a).On a sentence labeling task for evidence-basedmedicine, a multi-class multi-label classificationproblem, kLog showed improved results over boththe state-of-the-art CRF-based system of Kim etal.
(2011) and a memory-based benchmark (Ver-beke et al., 2012b).
Also for spatial relation ex-traction from natural language, kLog has shownto provide a flexible relational representation tomodel the task domain (Kordjamshidi et al., 2012).kLog has two distinguishing features.
First, it isable to transform relational into graph-based rep-resentations, which allows to incorporate struc-tural features into the learning process.
Subse-quently, kernel methods are used to work in an ex-tended high-dimensional feature space, which ismuch richer than most of the direct proposition-alisation approaches.
Second, it uses the logicprogramming language Prolog for defining andusing (additional) background knowledge, whichrenders the model very interpretable and providesmore insights into the importance of individual(structural) features.These properties prove especially advantageousin the case of NLP.
The graphical approach ofkLog is able to exploit the full relational represen-tation that is often a natural way to express lan-guage structures, and in this way allows to fullyexploit contextual features.
On top of this rela-tional learning approach, the declarative featurespecification allows to include additional back-ground knowledge, which is often essential forsolving NLP problems.In this paper, we present kLogNLP1, an NLPmodule for kLog.
Starting from a dataset and adeclaratively specified model of the domain (basedon entity-relationship modeling from database the-ory), it transforms the dataset into a graph-basedrelational format.
We propose a general modelthat fits most tasks in NLP, which can be extendedby specifying additional relational features in adeclarative way.
The resulting relational represen-tation then serves as input for kLog, and thus re-sults in a full relational learning pipeline for NLP.kLogNLP is most related to Learning-BasedJava (LBJ) (Rizzolo and Roth, 2010) in that it of-fers a declarative pipeline for modeling and learn-ing tasks in NLP.
The aims are similar, namely ab-stracting away the technical details from the pro-grammer, and leaving him to reason about themodeling.
However, whereas LBJ focuses moreon the learning side (by the specification of con-straints on features which are reconciled at in-ference time, using the constrained conditional1Software available at http://dtai.cs.kuleuven.be/klognlp85Interpretations(small relational DBs)ExtensionalizeddatabaseGraphKernel matrix/feature vectorsStatisticallearnerRaw dataset(sentence)Feature extractionbased on modelDeclarative featureconstructionGraphicalizationFeaturegenerationGraph kernel(NSPDK)kLogkLogNLPkLogNLP(E/R-)modelFigure 1: General kLog workflow extended with the kLogNLP modulemodel framework), due to its embedding in kLog,kLogNLP focuses on the relational modeling, inaddition to declarative feature construction andfeature generation using graph kernels.
kLog in it-self is related to several frameworks for relationallearning, for which we refer the reader to (Fras-coni et al., 2012).The remainder of this paper is organized ac-cording to the general kLog workflow, precededwith the kLogNLP module, as outlined in Fig-ure 1.
In Section 2, we discuss the modeling of thedata, and present a general relational data modelfor NLP tasks.
Also the option to declarativelyconstruct new features using logic programming isoutlined.
In the subsequent parts, we will illustratethe remaining steps in the kLog pipeline, namelygraphicalization and feature generation (Section3), and learning (Section 4) in an NLP setting.
Thelast section draws conclusions and presents ideasfor future work.2 Data ModelingkLog employs a learning from interpretations set-ting (De Raedt et al., 2008).
In learning frominterpretations, each interpretation is a set of tu-ples that are true in the example, and can beseen as a small relational database.
Listing 3, tobe discussed later, shows a concise example.
Inthe NLP setting, an interpretation most commonlycorresponds to a document or a sentence.
Thescope of an interpretation is either determined bythe task (e.g., for document classification, the in-terpretations will at least need to comprise a sin-gle document), or by the amount of context thatis taken into account (e.g., in case the task is sen-tence classification, the interpretation can either bea single sentence, or a full document, dependingon the scope of the context that you want to takeinto account).Since kLog is rooted in database theory, themodeling of the problem domain is done using anentity-relationship (E/R) model (Chen, 1976).
Itgives an abstract representation of the interpreta-tions.
E/R models can be seen as a tool that is tai-worddepRelnextWwordIDdepTypelemmaPOS-tagwordStringnamedEntityhasWordsentIDnextScorefsynonymoussentenceFigure 2: Entity-relationship diagram of thekLogNLP modellored to model the domain at hand.
As the nameindicates, E/R models consist of entities, which wewill represent as purple rectangles, and relations,represented as orange diamonds.
Both entities andrelations can have several attributes (yellow ovals).Key attributes (green ovals) uniquely identify aninstance of an entity.
We will now discuss theE/R model we propose as a starting point in thekLogNLP pipeline.2.1 kLogNLP modelSince in NLP, most tasks are situated at eitherthe document, sentence, or token level, we pro-pose the E/R model in Figure 2 as a general do-main model suitable for most settings.
It is ableto represent interpretations of documents as a se-quence (nextS) of sentence entities, whichare composed of a sequence (nextW) of wordentities.
Next to the sequence relations, also thedependency relations between words (depRel)are taken into account, where each relation hasits type (depType) as a property.
Furthermore,also the coreference relationship between wordsor phrases (coref) and possibly synonymy re-lations (synonymous) are taken into account.The entities in our model also have a primary key,namely wordID and sentID for words and sen-tences respectively.
Additional properties can beattached to words such as the wordString it-self, its lemma and POS-tag, and an indicationwhether the word is a namedEntity.This E/R model of Figure 2 is coded declara-tively in kLog as shown in Listing 1.
The kLogsyntax is an extension of the logical programminglanguage Prolog.
In the next step this script willbe used for feature extraction and generation.
Ev-86ery entity or relationship is declared with the key-word signature.
Each signature is of a certaintype; either extensional or intensional.kLogNLP only acts at the extensional level.
Eachsignature is characterized by a name and a listof typed arguments.
There are three possible ar-gument types.
First of all, the type can be thename of an entity set which has been declaredin another signature (e.g., line 4 in Listing 1; thenextS signature represents the sequence relationbetween two entities of type sentence, namelysent 1 and sent 2).
The type self is used todenote the primary key of an entity.
An example isword id (line 6), which denotes the unique iden-tifier of a certain word in the interpretation.
Thelast possible type is property, in case the argu-ment is neither a reference to another entity nor aprimary key (e.g., postag, line 9).We will first discuss extensional signatures, andthe automated extensional feature extraction pro-vided by kLogNLP, before illustrating how theuser can further enrich the model with intensionalpredicates.1 begin_domain.2 signature sentence(sent_id::self)::extensional.34 signature nextS(sent_1::sentence, sent_2::sentence)::extensional.56 signature word(word_id::self,7 word_string::property,8 lemma::property,9 postag::property,10 namedentity::property11 )::extensional.1213 signature nextW(word_1::word, word_2::word)::extensional.1415 signature corefPhrase(coref_id::self)::extensional.16 signature isPartOfCorefPhrase(coref_phrase::corefPhrase, word::word)::extensional.17 signature coref(coref_phrase_1::corefPhrase, coref_phrase_2::corefPhrase)::extensional.1819 signature synonymous(word_1::word,word_2::word)::extensional.2021 signature dependency(word_1::word,22 word_2::word,23 dep_rel::property24 )::extensional.2526 kernel_points([word]).27 end_domain.Listing 1: Declarative representation of thekLogNLP model2.2 Extensional Feature ExtractionkLog assumes a closed-world, which means thatatoms that are not known to be true, are assumedto be false.
For extensional signatures, this en-tails that all ground atoms need to be listed ex-plicitly in the relational database of interpreta-tions.
These atoms are generated automaticallyby the kLogNLP module based on the kLog scriptand the input dataset.
Considering the defined at-tributes and relations in the model presented inListing 1, the module interfaces with NLP toolk-its to preprocess the data to the relational format.The user can remove unnecessary extensional sig-natures or modify the number of attributes given inthe standard kLogNLP script as given in Listing 1according to the needs of the task under consider-ation.An important choice is the inclusion of thesentence signature.
By inclusion, the gran-ularity of the interpretation is set to the docu-ment level, which implies that more context canbe taken into account.
By excluding this signa-ture, the granularity of the interpretation is set tothe sentence level.Currently, kLogNLP interfaces with the follow-ing NLP toolkits:NLTK The Python Natural Language Toolkit(NLTK) (Bird et al., 2009) offers a suiteof text processing libraries for tokenization,stemming, tagging and parsing, and offers aninterface to WordNet.Stanford CoreNLP Stanford CoreNLP2pro-vides POS tagging, NER, parsing andcoreference resolution functionality.The preprocessing toolkit to be used can beset using the kLogNLP flags mechanism, as il-lustrated by line 3 of Listing 2.
Subsequently,the dataset predicate (illustrated in line 4 ofListing 2) calls kLogNLP to preprocess a givendataset3.
This is done according to the speci-fied kLogNLP model, i.e., the necessary prepro-cessing modules to be called in the preprocess-ing toolkit are determined based on the presenceof the entities, relationships, and their attributes inthe kLogNLP script.
For example, the presence2http://nlp.stanford.edu/software/corenlp.shtml3Currently supported dataset formats are directories con-sisting of (one or more) plain text files or XML files consist-ing of sentence and/or document elements.87of namedentity as a property of word resultsin the addition of a named entity recognizer in thepreprocessing toolkit.
The resulting set of inter-pretations is output to a given file.
In case sev-eral instantiations of a preprocessing module areavailable in the toolkit, the preferred one can bechosen by setting the name of the property accord-ingly.
The names as given in Listing 1 outline thestandard settings for each module.
For instance, incase the Snowball stemmer is preferred above thestandard (Wordnet) lemmatizer in NLTK, it can beselected by changing lemma into snowball asname for the word lemma property (line 8).1 experiment :-2 % kLogNLP3 klognlp_flag(preprocessor,stanfordnlp),4 dataset(?/home/hedgecuedetection/train/?,?trainingset.pl?
),5 attach(?trainingset.pl?
),6 % Kernel parametrization7 new_feature_generator(my_fg,nspdk),8 klog_flag(my_fg,radius,1),9 klog_flag(my_fg,distance,1),10 klog_flag(my_fg,match_type, hard),11 % Learner parametrization12 new_model(my_model,libsvm_c_svc),13 klog_flag(my_model,c,0.1),14 kfold(target, 10, my_model, my_fg).Listing 2: Full predicate for 10-fold classificationexperimentEach interpretation can be regarded as a smallrelational database.
We will illustrate the exten-sional feature extraction step on the CoNLL-2010dataset on hedge cue detection, a binary classifi-cation task where the goal is to detect uncertaintyin sentences.
This task is situated at the sentencelevel, so we left out the sentence and nextSsignatures, as no context from other sentences wastaken into account.
A part of a resulting interpre-tation is shown in Listing 3.1 word(w1,often,often,rb,0,1).2 depRel(w1,w5,adv).3 nextW(w1,w2).4 word(w2,the,the,dt,0,2).5 depRel(w2,w4,nmod).6 nextW(w2,w3).7 word(w3,response,response,nn,0,3).8 nextW(w3,w4).9 depRel(w3,w4,nmod).10 word(w4,may,may,md,0,5).11 nextW(w4,w5).Listing 3: Part of an interpretationOptionally, additional extensional signaturescan easily be added to the knowledge base by theuser, as deemed suitable for the task under consid-eration.
At each level of granularity (document,sentence, or word level), the user is given thecorresponding interpretation and entity IDs, withwhich additional extensional facts can be addedusing the dedicated Python classes.
We will nowturn to declarative feature construction.
The fol-lowing steps are inherently part of the kLog frame-work.
We will briefly illustrate their use in thecontext of NLP.2.3 Declarative Feature ConstructionThe kLog script presented in Listing 1 can nowbe extended using declarative feature constructionwith intensional signatures.
In contrast to ex-tensional signatures, intensional signatures intro-duce novel relations using a mechanism resem-bling deductive databases.
This type of signaturesis mostly used to add domain knowledge about thetask at hand.
The ground atoms are defined implic-itly using Prolog definite clauses.For example, in case of sentence labeling forevidence-based medicine, the lemma of the rootword proved to be a distinguishing feature (Ver-beke et al., 2012b), which can be expressed as1 signature lemmaRoot(sent_id::sentence,lemmaOfRoot::property)::intensional.2 lemmaRoot(S,L) :-3 hasWord(S, I),4 word(I,_,L,_,_,_),5 depRel(I,_,root).Also more complex features can be constructed.For example, section headers in documents (againin the case of sentence labeling using documentcontext) can be identified as follows:1 hasHeaderWord(S,X) :-2 word(W,X,_,_,_,_),3 hasWord(S,W),4 (atom(X) -> name(X,C) ; C = X),5 length(C,Len),6 Len > 4,7 all_upper(C).89 signature isHeaderSentence(sent_id::sentence)::intensional.10 isHeaderSentence(S) :-11 hasHeaderWord(S,_).1213 signature hasSectionHeader(sent_id::sentence, header::property)::intensional.14 hasSectionHeader(S,X) :-15 nextS(S1,S),16 hasHeaderWord(S1,X).17 hasSectionHeader(S,X) :-18 nextS(S1,S),19 not isHeaderSentence(S),20 once(hasSectionHeader(S1,X)).In this case, first the sentences that contain aheader word are identified using the helper pred-88word(often,often,rb,0,1) word(the,the,dt,0,2) word(response,response,nn,0,3) word(variable,variable,nn,0,4) word(may,may,md,0,5)nextWdepRel(adv)nextWdepRel(nmod)nextWdepRel(nmod)nextWdepRel(sbj)Figure 3: Graphicalization of the (partial) interpretation in Listing 3.
For the sake of clarity, attributes ofentities and relationships are depicted inside the respective entity or relationship.r=0 d=2r=1 d=2v uINSTANCE GFEATURESAFigure 4: Illustration of the NSPDK feature concept.
Left: instance G with 2 vertices v, u as roots forneighborhood subgraphs (A, B) at distance 2.
Right: some of the neighborhood pairs, which form theNSPDK features, at distance d = 2 and radius r = 0 and 1 respectively.
Note that neighborhood subgraphscan overlap.icate hasHeaderWord, where a header word isdefined as an upper case string that has more thanfour letters (lines 1-7).
Next, all sentences that rep-resent a section header are identified using the in-tensional signature isHeaderSentence (lines9-11), and each sentence in the paragraphs follow-ing a particular section header is labeled with thisheader, using the hasSectionHeader predi-cate (lines 13-20).Due to the relational approach, the span can bevery large.
Furthermore, since these features aredefined declaratively, there is no need to reprocessthe dataset each time a new feature is introduced,which renders experimentation very flexible4.3 Graphicalization and FeatureGenerationIn this step, a technique called graphicalizationtransforms the relational representations from theprevious step into graph-based ones and derivesfeatures from a grounded entity/relationship dia-gram using graph kernels.
This can be interpretedas unfolding the E/R diagram over the data.
An ex-ample of the graphicalization of the interpretationpart in Listing 3 can be found in Figure 3.From the resulting graphs, features can be ex-tracted using a feature generation technique that isbased on Neighborhood Subgraph Pairwise Dis-4Note that changes in the extensional signatures do re-quire reprocessing the dataset.
However, for different runs ofan experiment with varying parameters for the feature gener-ator or the learner, kLogNLP uses a caching mechanism tocheck if the extensional signatures have changed, when call-ing the dataset predicate.tance Kernel (NSPDK) (Costa and De Grave,2010), a particular type of graph kernel.
Infor-mally the idea of this kernel is to decompose agraph into small neighborhood subgraphs of in-creasing radii r ?
rmax.
Then, all pairs of suchsubgraphs whose roots are at a distance not greaterthan d ?
dmaxare considered as individual fea-tures.
The kernel notion is finally given as the frac-tion of features in common between two graphs.Formally, the kernel is defined as:?r,d(G,G?)
=?A,B?R?1r,d(G)A?,B??R?1r,d(G?)1A?=A??
1B?=B?
(1)whereR?1r,d(G) indicates the multiset of all pairsof neighborhoods of radius r with roots at distanced that exist inG, and where 1 denotes the indicatorfunction and?=the isomorphism between graphs.For the full details, we refer the reader to (Costaand De Grave, 2010).
The neighborhood pairs areillustrated in Figure 4 for a distance of 2 betweentwo arbitrary roots (v and u).In kLog, the feature set is generated in a combi-natorial fashion by explicitly enumerating all pairsof neighborhood subgraphs; this yields a high-dimensional feature space that is much richer thanmost of the direct propositionalization approaches.The result is an extended high-dimensional fea-ture space on which a statistical learning algorithmcan be applied.
The feature generator is initializedusing the new feature generator predicateand hyperparameters (e.g., maximum distance andradius, and match type) can be set using the kLogflags mechanism (Listing 2, lines 6-10).894 LearningIn the last step, different learning tasks can be per-formed on the resulting extended feature space.
Tothis end, kLog interfaces with several solvers, in-cluding LibSVM (Chang and Lin, 2011) and SVMSGD (Bottou, 2010).
Lines 11-15 (Listing 2) illus-trate the initialization of LibSVM and its use for10-fold cross-validation.5 Conclusions and Future WorkIn this paper, we presented kLogNLP, a natu-ral language processing module for kLog.
Basedon an entity-relationship representation of the do-main, it transforms a dataset into the graph-basedrelational format of kLog.
The basic kLogNLPmodel can be easily extended with additionalbackground knowledge by adding relations us-ing the declarative programming language Prolog.This offers a more flexible way of experimenta-tion, as new features can be constructed on topof existing ones without the need to reprocess thedataset.
In future work, interfaces will be addedto other (domain-specific) NLP frameworks (e.g.,the BLLIP parser with the self-trained biomedicalparsing model (McClosky, 2010)) and additionaldataset formats will be supported.AcknowledgmentsThis research is funded by the Research Founda-tion Flanders (FWO project G.0478.10 - StatisticalRelational Learning of Natural Language).
KDGwas supported by ERC StG 240186 ?MiGraNT?.ReferencesLaura Antanas, Paolo Frasconi, Fabrizio Costa, TinneTuytelaars, and Luc De Raedt.
2012.
A relationalkernel-based framework for hierarchical image un-derstanding.
In Lecture Notes in Computer Science,,pages 171?180.
Springer, November.Laura Antanas, McElory Hoffmann, Paolo Frasconi,Tinne Tuytelaars, and Luc De Raedt.
2013.
A re-lational kernel-based approach to scene classifica-tion.
IEEE Workshop on Applications of ComputerVision, 0:133?139.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media, Inc., 1st edition.L?eon Bottou.
2010.
Large-scale machine learning withstochastic gradient descent.
In Proc.
of the 19th In-ternational Conference on Computational Statistics(COMPSTAT?2010), pages 177?187.
Springer.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Peter Pin-Shan Chen.
1976.
The entity-relationshipmodel - toward a unified view of data.
ACM Trans.Database Syst., 1(1):9?36, March.Fabrizio Costa and Kurt De Grave.
2010.
Fast neigh-borhood subgraph pairwise distance kernel.
In Proc.of the 26th International Conference on MachineLearning,, pages 255?262.
Omnipress.Luc De Raedt, Paolo Frasconi, Kristian Kersting, andStephen Muggleton, editors.
2008.
ProbabilisticInductive Logic Programming - Theory and Appli-cations, volume 4911 of Lecture Notes in ComputerScience.
Springer.Paolo Frasconi, Fabrizio Costa, Luc De Raedt, andKurt De Grave.
2012. klog: A language for log-ical and relational learning with kernels.
CoRR,abs/1205.3981.Su Kim, David Martinez, Lawrence Cavedon, and LarsYencken.
2011.
Automatic classification of sen-tences to support evidence based medicine.
BMCBioinformatics, 12(Suppl 2):S5.Parisa Kordjamshidi, Paolo Frasconi, Martijn van Ot-terlo, Marie-Francine Moens, and Luc De Raedt.2012.
Relational learning for spatial relation extrac-tion from natural language.
In Inductive Logic Pro-gramming, pages 204?220.
Springer.David McClosky.
2010.
Any Domain Parsing: Au-tomatic Domain Adaptation for Natural LanguageParsing.
Ph.D. thesis, Brown University, Provi-dence, RI, USA.
AAI3430199.N.
Rizzolo and D. Roth.
2010.
Learning based java forrapid development of nlp systems.
In LREC, Val-letta, Malta, 5.Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,Roser Morante, Walter Daelemans, and LucDe Raedt.
2012a.
Kernel-based logical and re-lational learning with kLog for hedge cue detec-tion.
In Proc.
of the 21st International Conferenceon Inductive Logic Programming, pages 347?357.Springer, March.Mathias Verbeke, Vincent Van Asch, Roser Morante,Paolo Frasconi, Walter Daelemans, and LucDe Raedt.
2012b.
A statistical relational learn-ing approach to identifying evidence based medicinecategories.
In Proc.
of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 579?589.
ACL.90
