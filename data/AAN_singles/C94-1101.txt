A New Mel;hod of' N-gram S(;al:istics forLmge Nulril.
)er of' nall(\] Autorna, tJc \]:.xl;ra, ct on of Words ml(\] Phra,sesfrom Large icxl; Data o\[' ,Ja,pa,nes(;M,~d,:otc, Nagao,  Shil isul(e Moril)(;pa, r l ; i l lo l i t  0\[" l!,\]o,('.trl(:al \]!AIII~II\]C(H'IIIgKyo(,o Univers i tyAbst rac tIn the process of establish in g the it, form ation the-ory, C. F,.
Shannon prol)ose.d the Markov I)ro(:ess asa good model to characterize ~t natural  la.nguage.The core or this ide.a is t;o cah:ula.te the \['re(lU('Ii-des of strings compose(l of 'n characters ('n-grams),but this statistical analysis of large text.
(lata a.,idfor a large n lilts l lever be(HI carried ()tit })eca./ise ofthe memory l imitat ion of (:omputer and the short-age of text data.
Taking advantage of the recentpowerful computers we developed a. new aJgorithmof n-grams of large text data for arbitr~try hu'ge 'na,nd (:alculated successl'ully, within ,'ela, t iv(.
ly shortthlle~ n-grams of some Japa,nese text (la, t~t con-taining between two an(l thirty million chara,(:ters.From this exl)eriment it 1)ecame (:loa,r t\]l&t the au-tomatic extraction or detern,i,tation of words, (:om-l)ound words and (;ol\]ocations i  possible by mutu-ally comparing n-gram statistics for dill'etch t valuesof lt.category :  topical pa,per~ quantitative linguisth:s,large text corpora, text t)rocesshlg1 In t roduct ionClaude E. Shannon estal)lished tile in foH.at io,  the-ory in 19d8 \[1\].
Iris theory included the co,lcepttlutt ~ hmgnage could be a,pproximated by an n-th order Markov model by n to l)e extended t(~infinity.
Since his proposal there were ma.ny tri-~tls to ea.h:ulate n-grams (statistics of 'n c},ara.
(:terstrings of a language) lbr a big text data, of a, la .
-guage, l\[owever computers u1) to tim present o .h inot ca.h:ulate them for a large n 1)e(:ause the cah:u-|ation require(1 hug(; amount of memory space au(\[time.
For example the I'r(,quen('y ea, h;ulati(m of 10-grams of English requires a,t least 2(;win l0 s ~ I() (~giga word memory space.
Therefore tile ca,lcuh~tiouwas done at most for n :: d ,.o 5 wil;h modest textqua, n tit;,,We developed a new method of calcula, ting wgra.ms For large 'n's.
We (1o not In'el)are a tablefor an 'n.
gra.m.
Our methods consists of two stages.The first stage perh)rms I;he sorting of sul)strings ofa text, aim fin(Is out tile lenlKth of t:he prefix partswhich axe the same for th(; a, dja(:ent sul)stritGs iNthe st)rted ta,ble.
The second sta,ge is the (:a,lcuh~,tionof an 'n-gram when it is aal(ed for a sl)ecific n. Onlythe existing 'n, chara,cter combinations require theta, hle entrie,g t'm' (,lie l'requen(:y count, so that we.eed not r(,serve a, big si)ace for 'n-gram table.
Theprogranl we ha,ve develol)e(1 requires 71 bytes for anl cha, ra,cter text of two byte (:ode such as Japa, llcseand Chinese texts and 6!
bytes for an l charactertext o1' English and other F',uropean la,nguages.
Bythe present program '., ca, n be extended up to 255.The program can l)e (:hant~;ed very easily for la,rger'., if it is required.We l)erf.rme(l '.,-l,;ram Irequen(:y (:a,h'ulations forthree (11 fl'(;ren t text data.
We were not so m u(:h in-terested in tile.
entropy wdue of a \]a.nguage \])ut wereillter('ste(I in the extra.orion of varieti(,s ol langua~ei)rOl)(,rties, su(:\]l as wor(ls, (:olnl)oun(I words, (:o\[-\]oca.l.ions and so on.
The cah:ula.tion of fre(luelu:yof o(:(:urren(:es of clul.ra('.t(,r sl, rings is t)articularlyi ln lmrtal l t  to (leterlnilm what is ;~ wor(l in suchla, nguage.s as 3al)alle.~e a,nd Chinese where there isno sl)a.
(:es between words a.nd the determinath)n ofword boundarh~.~ is not so easy.
In this l)aper wewill explain some of our results on 1,hose probh;nls.2 Calcu lat ion of 'n-grams for anarbit, rary  large number  of 'nIt was w!ry difficult to calculate 'n-grams for a largenumber o1" 'n because of the.
memory l imitation ofa computer.
For examph.
', Ja, panese langua,ge ha.sm~t'e thall d000 di/l'ere.t characters a,nd if we want677to have 10-gram frequencies of a Japanese text,we must reserve 4000 l?
entries, which exceed 10 aa.Therefore only 3 or 4-grams were calculated so far.A new method we developed can calculate n_grams for an arbitrary large number of n with areasonable memory size in a reasonable calcula.tiontime.
It consists of two stages.
The first stage is toget a table of alphabetically sorted substrings of atext string and to get the value of coincidence num-ber of prefix characters of adjacently sorted strings.The second stage is to calculate the fl'equency of 'n-grams for M1 the existing ?z character strings fromthe sorted strings for a specific number of n.2.1 First s tage(1) When a text is given it is stored in a computer a.sone long character string.
It may include sentenceboundaries, paragraph boundaries and so oil if theyare regarded as components of text, When a text iscomposed of I characters it occupies 2I hyte mernorybecause a Japanese character is encoded by 16 bitcode.
We prepare another table of the same size (I),each entry of which keeps the pointer to a substri ugof the text string.
This is i l lustrated in l"igure 1.text string ( /characters : 21bytes)V' '""'t"'?r~ ....... \[lli4bytesFigure 1: Text string and tile pointer table to sub-strings.A substring pointed by i-1 is defined as compose, dof the characters fi'om the/-t i t  position to the end ofthe text string (see Figure 1).
We call this substringa word.
The first word is the text string itself, a.ndthe second word is the string which starts fi'om thesecond character and ends at the final ch~u'acter ofthe text string.
Similarly the last word is the finalcharacter of the text string.As the text size is I characters a l)ointer imlsthave at least p bits where 27' _>_ l. In our programwe set p = 32 bits so that we can accept the textsize tip to 2 a2 ~ d giga.
characters.
The.
pointertable represents a set of l words.We apply the dictionary sorting operation to thisset of /words.
It is performed by utilizi ng the point-ers in the pointer t~d)le.
We used comb sort\[2\] whichis an improved version of bubble sort.
The sortingthne is the order of O(llogl).
When the sorting iscompleted the result is the change of pointer posi-tlons in the pointer table., and there, is no replace-ment of actual words.
As we are iuterested in n-grams of 'n less than 255, actual sorting of woMs isperformed for the lertmost 255 or less cha.ra.cters ofwords.
(2) Next we compare two adjacent words in tilel)ointer t~dtle, and count the length of tile prefixparts which are the s~tme ill the two words.
For ex-ample when "extension to the left side ..." and "ex-tension to the right side ..." are two words placedadjacent, the nutrlber is 17.
This is stored in tilet:d)le of coincidence nulnber of prefix characters.
'l?lils is shown hi l,'igure 2.
As we ;ti'e interested ill1 < 'n < 255, one byte is given to an el / t ry Of thistable,.
The total lnemory space required to this firststag(,, operation is 214-4I-I-I = 7I bytes.
For examplewhen a text size is 10 mega Japa.nese the.ratters, 70mega hyte memory Intist be reserved.
This is notdifficult by the preseut-dag conipnl;ers.table of coincidenceI'~tlrnbor e| cl~aracterspointertable1byte 4bytestext string ( /characters : 2lbytes)~ZZ~ZE~Z22~I  :1\[_Figure 2: Sorted poh/ter ta.ble and t~ble of coinci-dence nuiillter of cha.r:-i.cte.rsWe developed two software versions, one by usingmain memory alone, and tile other by using a (list"memory where the software has tile a,dditional op-eral;ions of disc merge sort.
lilly the disc version wecan ha.ndle a text of more than 100 meg~ characterJapanese text.
The.
software was iml>lelnented on ~612SUN SPARC Station.2.2  Second s tageTile second stage is the calculation of n-gra.m fre-quency table.
This is done by using the pointertable and the table of coincidence number of prefixcharacters.
Let us tix n to a certMn number.
Wefirst read out the tirst n characters of the first wordin the pointer table, and see the number in the tableof coincidence number of prefix char~tcters.
If thisis equal to or larger than n it means that the secondword has at least the same n prefix characters withthe first word.
Then we see the next entry of thecoincidence number of I)refix characters a,nd checl(whether it is equal to or larger than n or not.
Wecontinue this operation until we meet the conditionthat the number is smMler than n. The number ofwords checked up to this is the frequency of the nprefix characters of the first word.
At this stage thetirst n prefix characters of the next word is d ifferen t,and so the same operation as the th'st n charactersis performed from here, that is, to che.ck the num-ber in tile coincidence number of prefix charactersto see whether it is equal to or larger than 7z ornot, and so on.
In this way we get the frequencyof the second n prefix characters.
We l,e,'form thisprocess until the last entry of the table.
These op-erations give the n-gram table of the glve.n text.
Wedo not need any extra memory space in this opera-,ion when we print out every n-gram string and itsfl'equency when they ;,re obtained.We calculated n-grams for some diflhrentJapanese texts which were available in electronicform in our 1Mmratory.
These were the followings.1.
Encyclopedic l)ictionary of Coml>uter Science(a .7  M bytes)2.
JournMistic essays from Asahi Newsl)al)er (8M bytes)3.
Miscellaneous texts availM)le in our laboratory(59 M bytes)The first two texts were not large and could \[)(,managed in the main memory.
'l'he third one wasprocessed by using a disc memory l)y a.pi)lyi,lg amerge sort prognun three thnes.
'l'he llrst twotexts were processed within one.
~md two hours hya, standard SUN SPAR.C Station for the first stagementioned above.
The thh'd text required abouttwenty tbur hours.
Calculation of n-gram frequency(the second stage) too\]( less than an hour includingi)rint-out.Extract ion of useful l inguist icinformat ion fl'om n-gram fl'e-quency data3.1  Ent ropyEverybody is interested in the entropy wdue of ;~language.
Shannon's theory s~tys tlmt the.
entropyis cah:ula.ted hy the formula \[3\]H,,(:.)
= r ( .
, .
)where l ' (w) is the prol)ability of occurrence, of w,and the suInma.tion is tb," a.ll the different strings'w of ~z characters appea.ring hi ~L l~mguage_ Theentropy of a langua.ge L is::(:,) = , i raWe cah'ulated .
I I , L (L )  for the text:s mentioned inSection 2 for ~,.
= 1,2,3,...
The results is shownin Figure 3.
Unlike our hfitia.lexpecta.tion tha.t theentropy will converge to a certain constant value be-1.ween 0.G and 1.3 \vllich C. E. Shtulnon esthrutte.dfor English, it cotktimted to decrease to zero.
Wechecked in detail whether our method had some-thing wrong, but there was nothing doubtful.
Ourconclusion for this strange phenomenon was thatthe text quantity of a few mega characters weretoo small to get a meanh,gful statistics for a. large'/Z be .cause  \v(!
h ave  11lo,'e than  ,1000 different ch ar-a.cters in the .lal~;mese language, l,'or English andma.ny other l"tlrope.a,n \]allgtla.g;es which hawe alpha:betic sets of less than fifty cha.racte.rs the situationmay he better.
Ilut still the text quantity of a fewgiga.
byles or more will be necessa.ry to gel: a. n,ean-ingful el,tropy value for ~t = 10 or more.I I  ,~?
l : ', a \[ .
.
,O -~~- .
- - I  ............ ~ ............ i ........... !
............ i............ !
............. t ............A i .. .
.
.
.
i i .
.
.
.
!
.
.
.
.
.
.
.
.
'2 i i .
.
.
.
.
.
.
.
.
.
.
.i i117111 iii;i!i i  71111' ,  b ;  .
.
.
.  '
"-*+"+ .
.
.
.
.  }
;  a .
.
.
.
i .
.
.
.
.I : i 4 , *~*T  ..... ,----*,0  i s  ~0 a~ 10  a~, 40\]"igure :l: EI~tropy curve by n-gram6133.2 Obta in ing  the  longest  compoundwordl?rom the n-gram frequency table we can get manyinteresting information.
When we have a string w(length n) of high fl'equency as shown in Figure 4,we can try to find out the longest string w* whichincludes w by the following process by using then-gram frequency table.2' ,~....~, W :.
'-~'x, frequency.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
_ _ : " "<......... i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I -4 - .
: - t - - - - - !
........ : ...................... i ".,2ZiK2ZZ2Z ' , ,  i X ~ a..................... i i. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.
....Figure 4: Obtaining the longest word w ~ from ahigh fi'equency word fragment w(1) extension to the left: We cut off the last char-acter of w and add a character a: to the leftof w. We call this a cut-and-pasted word.We look for the character x which will givethe maximum frequency to the cut-and-pastedword.
Repeat the same operation step by stepto the left and draw a frequency curve for thesewords.
This operation will be stopped wheuthe frequency curve drops to a certain wdue.This process is performed by seeing the 'u-grain#equeney table alone.
(2) extension to the right: The same ol)eratiot~ a.s(1) is performed by cutting the left characterand adding a character to the right.
(3) extraction of high frequency part: From thefi'equency curve as shown in Figure ,1 we caneasily extract a high fl'equency part a.s thelongest string.
An example is shown in Pig-ure 5The strings extracted in this way are very of-ten compound words of postpositions in Japanese.PostpositionM phrases are usually composed of oneto three words, and are used as if they are com-pound postpositions.
Some extracted exam ples are,partial strings freqtmncies~9-  5 c 101O- ~u ~ k 16895 K ~ h~ :1310C ~ \]fi-'(" 784E \]fi'(" + 78,1:b~C+ ~ 70 770Figure 5: Frequencies of partial strings anding the longest word " 9 -5  C ~ ~)~-e~ 5"obtain-(must do ...)(it is known that ...)(can do ...)(can  ask  ..
.
)3.3 Word  ext rac t ionAfter getting high frequency chat'~cter strings bythe.
above method we can nmke.
consultations withdictiona.ries for these strings.
Then we find outmany strings which are not included in tim dictio-naries.Some a.re phrases(colloca,tions, id iomat ic  expres-slons), some others are terminology words, and un-known (new) words.
From the text data of Pmcyclo-pedic l)ictkmary of Computer Science we extractedmany termhmlogica.1 words.
In general the frequen-cies of n-grams become smaller as n becomes larger.But we had sometimes relatively high frequencywdues in n-grams of large n's.
These were very oilten terminological words or terminological phrases.We extracted such terminological phrases as,?
( .
.
. )
~iiili-e~J~/J,~; k 7" , ,  , /~  a(p,'or;,a,,,s w, ' iu .e , ,  I ,y ( .
.
. )
l ' , , , ,g , , , ,?e)(i)roble, m solving in a, rLificial intelligence)(page re place merit algorit hm)(partial correctness of programs)3.4 Compound wordWe can get more.
interesthlg lnforma, tion whenwe compare data of different;n's.
When wehave acharacter string (length 'n) of high frequency, whichwe may be al)le to de, fine as a word ('iv), we arerecommended to check whether tWO substr\]ngs (W 1and w2) o1' t i le length 'ul a,nd 'n2 ('hi -I-'n2 = 'n) as614Compound word,,~E ~'~;~''~''~',.at~,e,.
(280) =f i !
l~ .~J !
(166)  =~I I~ I~?
(\]ss) ='Pal)h: 1: l)etermination of compound wordproper segmentation iml)roper segmentation~iE ~'~ 154 5 "f ,~  (205s).
~21!
(2(\].,)s) fi:;~t~L_ (tSS), ~t/~\]:~l\[ (:  (;S), @~.,~ (\] (;S)~J.~g(((2,12) .
f in  (1:~50) ~h ' / l ' l  ( \ ]Ss ) ,  ~i'/\[i,IN ( lSS) ,  ~J/lf,\] (188)( ):\[reque.ncy in li;ncyclol)edic Dictionary of Computer Schmce.
.
I .
,Figure 6: Possible segmentation of ;~ word into twocomponentssltown in l,'igure 6 h:we high frequency al)pea.ra.ncein n.t-gram aild n~-gram tables.
If we can find outsuch a situation by she.rising n~ (and 'n~) we cm~conclude that the original character string 'w is aeOlnliollnd Wol'd of W 1 and 102.
~Olll(: eXhllll)les ill'eshown in TM)le 1.3.5  Co l locat ionWe can see whether a particular word w has strongcolloeatioilal relatioils with some other words fromthe n-gram fi'equency results.
We cnn get an 'n-gram tM)le where n is sufficiently la.,'ge, w is theprefix of tltese n-grams, and some words (w ~, 'w",, .
. )
may appear in relatively high freqnency.
Thisis shown in Figure 7, We can find out easily that"tO - -  'tO t aild ?o -- '~o It al'e t%vo allocational expres-sions fl'om this ligurc'.
\]for example we have \[j;~~_1 (effect) and llnd out that \[j;,~9,1!~"~U5 i (reoceive effect) and Fj;~N~'-~j:2.
5 J (give eflbct) haverelatively high frequencies and there are no othCwsignificant combinations in the n-gram tM)le withr J~gNI as the prefix, l-)kN~'(I (ill ;I.ll(l ()lit Ims-pital) b~we ahnost all the t ime I -~@ b~9- .
I  (re-peat) as the Mlowiilg phrase, and so we will be ableto judge that \[~,J~\[?~'~'~,)/) b -_1 i~.an idioma.ticexpression.4 Conc lus ionsWe developed a new method and software for '.,-gram frequency ealcula, tion for n up to 255, andcah:ulated n-grams for some hu'ge text da, ta ofJaq)~ilese, From these da.ta we could derive words,compound words and collocations automatically.W B' ' i- - - - -+-~- -+ i~-- -~ - ~-------%--ii ............................................
!.......
i~; .............. ii; ;~ .............. iFigure 7: l"indilql colloca.1fonal word pairs "w - 'w 'all(1 ' to  ?
- 'u / tWe thinl< tha,t this method is equMly useful lbr hm-guages lil,:o Chiu('.se wh(,re there is no word spacesin a. sentence, a.nd for EUrOl)e:u~ langna?
;es a.s well,al\](l ;I\]so foF sl)eec\[l p\]lOl\]<qlle s(;qttell(-(~s to ~et ll)ol'edeta.iled II M M models.Another possil)ility is that when we get a largetext data wil, h part-speech tags, we can extract highfrequency pa.rt-of..sl)eeeh sequences by this n-gra, mcalcula, tiou ,Jver th(~ pa.rt..ol:.speech data.
Thesema.y be regarded as grammar rules of the primarylevel, l ly tel)hieing these pa,rt-ofspeech se,qllellcesby sis~gle ~lou-terminal symbols we ca.n cah:ula.tenew 'n.--grams, a.nd will be able to get hit';her lewdgra.m m;~r rules.
These e?a.m ph's indical,e that lar.gete?t data, with wLriel, ies of annotations ;tre very i l l l -imrtaut and valual)le h)t" the extra.orion of littguisticinforul:Lti(m I>y c,dcula.tit~g 'n-gralus for la.rger va.lueO\[' 1l..References\[I) (,'.
l'\].
,~hallllOll" A mathemat ica l  theory ofcorn m unicatiou, Bell System Teci~.#)., Vol.27,i)1).379-423, pp.
(~2:b(;56, (19,18).\[2\] SI;ephen Laeey, ll.lchard Box: Nikkei BYTE,November, pp.
:105-312, (1991).\[3\] N. AI)ramson: tllfo)'m:Ltion theory a.nd co(\[-iug, McGraw 11111, (1963).615
