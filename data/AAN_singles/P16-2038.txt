Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 231?235,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDeep multi-task learning with low level tasks supervised at lower layersAnders S?gaardUniversity of Copenhagensoegaard@hum.ku.dkYoav GoldbergBar-Ilan Universityyoav.goldberg@gmail.comAbstractIn all previous work on deep multi-tasklearning we are aware of, all task super-visions are on the same (outermost) layer.We present a multi-task learning architec-ture with deep bi-directional RNNs, wheredifferent tasks supervision can happen atdifferent layers.
We present experimentsin syntactic chunking and CCG supertag-ging, coupled with the additional task ofPOS-tagging.
We show that it is consis-tently better to have POS supervision atthe innermost rather than the outermostlayer.
We argue that this is because ?low-level?
tasks are better kept at the lowerlayers, enabling the higher-level tasks tomake use of the shared representation ofthe lower-level tasks.
Finally, we alsoshow how this architecture can be used fordomain adaptation.1 IntroductionWe experiment with a multi-task learning (MTL)architecture based on deep bi-directional recurrentneural networks (bi-RNNs) (Schuster and Paliwal,1997; Irsoy and Cardie, 2014).
MTL can be seenas a way of regularizing model induction by shar-ing representations (hidden layers) with other in-ductions (Caruana, 1993).
We use deep bi-RNNswith task supervision from multiple tasks, sharingone or more bi-RNNs layers among the tasks.
Ourmain contribution is the novel insight that (whathas historically been thought of as) low-level tasksare better modeled in the low layers of such an ar-chitecture.
This is in contrast to previous work ondeep MTL (Collobert et al, 2011; Luong et al,2015) , in which supervision for all tasks happenat the same (outermost) layer.
Multiple-tasks su-pervision at the outermost layer has a strong tradi-tion in neural net models in vision and elsewhere(Caruana, 1993; Zhang and Zhang, 2014; Yim etal., 2015).
However, in NLP it is natural to thinkof some levels of analysis as feeding into others,typically with low-level tasks feeding into high-level ones; e.g., POS tags as features for syntacticchunking (Sang and Buchholz, 2000) or parsing(Nivre et al, 2007).
Our architecture can be seenas a seamless way to combine multi-task and cas-caded learning.
We also show how the proposedarchitecture can be applied to domain adaptation,in a scenario in which we have high-level task su-pervision in the source domain, and lower-leveltask supervision in the target domain.As a point of comparison, Collobert et al(2011) improved deep convolutional neural net-work models of syntactic chunking by also havingtask supervision from POS tagging at the outer-most level.
In our work, we use recurrent insteadof convolutional networks, but our main contribu-tion is observing that we obtain better performanceby having POS task supervision at a lower layer.While Collobert et al (2011) also experiment withNER and SRL, they only obtain improvementsfrom MTL with POS and syntactic chunking.
Weshow that similar gains can be obtained for CCGsupertagging.Our contributions (i) We present a MTL archi-tecture for sequence tagging with deep bi-RNNs;(ii) We show that having task supervision from alltasks at the outermost level is often suboptimal;(iii) we show that this architecture can be used fordomain adaptation.2 Sequence tagging with deep bi-RNNsNotation We use x1:nto denote a sequence ofn vectors x1, ?
?
?
, xn.
F?(?)
is a function param-eterized with parameters ?.
We write FL(?)
as ashortcut to F?L?
an instantiation of F with a spe-231cific set of parameters ?L.
We use ?
to denote avector concatenation operation.Deep bi-RNNs We use a specific flavor of Re-current Neural Networks (RNNs) (Elman, 1990)called long short-term memory networks (LSTMs)(Hochreiter and Schmidhuber, 1997).
For brevity,we treat RNNs as a black-box abstraction, andLSTMs as an instance of the RNN interface.
Forfurther details on RNNs and LSTMs, see (Gold-berg, 2015; Cho, 2015).
We view RNN as a pa-rameterized function RNN?
(x1:n) mapping a se-quence of n input vectors x1:n, xi?
Rdinto a anoutput vector hn?
Rdout.
The output vector hnis conditioned on all the input vectors x1:n, andcan be thought of as a summary of x1:n. The RNNcan be applied to all prefixes x1:i, 1 ?
i ?
n ofx1:n, resulting in n output vectors h1:n, where h1:isummarizes x1:i.A deep RNN (or k-layer RNN) is composed of kRNN functions RNN1, ?
?
?
, RNNkthat feed intoeach other: the output h`1:nof RNN`becomes theinput of RNN`+1.
Stacking RNNs in this waywas empirically shown to be effective.A bidirectional RNN (Schuster and Paliwal,1997; Irsoy and Cardie, 2014) is composed of twoRNNs, RNNFand RNNR, one reading the se-quence in its regular order, and the other reading itin reverse.
Concretely, given a sequence x1:nanda desired index i, the function BIRNN?
(x1:n, i)is defined as:BIRNN?
(x1:n, i) = vi= hF,i?
hR,ihF,i= RNNF(x1, x2, ?
?
?
, xi)hR,i= RNNR(xn, xn?1, ?
?
?
, xi)The vector vi= BIRNN(x1:n, i) is then a rep-resentation of the ith item in x1:n, taking into ac-count both the entire history x1:iand the entire fu-ture xi:n.Finally, in a deep bidirectional RNN, bothRNNFand RNNRare k-layer RNNs, andBIRNN`(x1:n, i) = v`i= h`F,i?
h`R,i.Greedy sequence tagging with deep bi-RNNsIn a sequence tagging task, we are given an in-put w1, ?
?
?
, wnand need to predict an outputy1, ?
?
?
, yn, yi?
[1, ?
?
?
, |L|], where L is a labelset of interest; i.e., in a POS tagging task, L isthe part-of-speech tagset, and yiis the pos-tag forword wi.If we take the inputs x1:nto correspond to asequence of sentence words w1, ?
?
?
, wn, we canthink of vi= BIRNN(x1:n, i) as inducing an in-finite window around a focus word wi.
We canthen use vias an input to a multiclass classifica-tion function f(vi), to assign a tag y?ito each inputlocation i.
The tagger is greedy: the tagging de-cisions are independent of each other.
However,as shown below and in other recent work usingbi-RNNs for sequence tagging, we can still pro-duce competitive tagging accuracies, because ofthe richness of the representation vithat takes theentire input sequence into account.For a k-layer bi-RNN tagger we get:tag(w1:n, i) = y?i= f(vki)vki= BIRNNk(x1:n, i)x1:n= E(w1), E(w2), ?
?
?
, E(wn)where E as an embedding function mapping eachword in the vocabulary into a demb-dimensionalvector, and vkiis the output of the kth BIRNN layeras defined above.All the parameters (the embedding vectors forthe different vocabulary items, the parameters ofthe different RNNs and the parameters of the clas-sification function f ) are trained jointly in orderto minimize the tagging loss over a sentence.
Theembedding vectors are often initialized using vec-tors that were pre-trained in a semi-supervisedmanner.This sequence tagging architecture was intro-duced to NLP by Irsoy and Cardie (2014).
Asimilar architecture (with an RNN instead of bi-RNN) was applied to CCG supertagging by Xu etal (2015).MTL in deep bi-RNNs In a multi-task learn-ing (MTL) setting, we have several predictiontasks over the same input space.
For example,in sequence tagging, the input may be the wordsin the sentence, and the different tasks can bePOS-tagging, named entity recognition, syntacticchunking, or CCG supertagging.
Note that thedifferent tasks do not have to be traditional NLPtasks, but also, say, two POS-annotated corporawith slightly different guidelines.
Each task hasits own output vocabulary (a task specific tagset),but all of them map the length n input sequenceinto a length n output sequence.Intuitively, although NLP tasks such as POStagging, syntactic chunking and CCG supertag-ging are different than each other, they also sharelot of substructure, e.g., knowing that a word is a232verb can help in determining its CCG supertag andthe syntactic chunk it participate in.
We wouldtherefore like for these models to share parame-ters.The common approach is to share parametersacross most of the network.
In the k-layers deepbi-RNN tagger described above this is naturallyachieved by sharing the bi-RNN part of the net-work across tasks, but training a specialized clas-sification tagger ft(vki) for each task t.This encourages the deep bi-RNN to learn a rep-resentation vkithat is useful for prediction of thedifferent tasks, allowing them to share parameters.Supervising different tasks on different layersPrevious work in NLP on cascaded learning suchas Shen and Sarkar (2005) suggests there is some-times a natural order among the different tasks:some tasks may benefit more from other tasks,than the other way around.
This suggests havingtask supervision for low-level tasks at the lower bi-RNN layers.
This also enables task-specific deeplearning of the high-level tasks.Instead of conditioning all tasks on the outer-most bi-RNN layer, we associate an RNN level`(t) with each task t, and let the task specific clas-sifier feed from that layer, e.g., pos tag(w1:n, i) =fpos(v`(pos)i).
This enables a hierarchy a task withcascaded predictions, as well as deep task-specificlearning for high-level tasks.
This means therewill be layers shared by all tasks and layers thatare specific to some tasks:pos tag(w1:n, i) = fpos(v`(pos)i)chunk tag(w1:n, i) = fchunk(v`(chunk)i)ccg tag(w1:n, i) = fccg(v`(ccg)i)v`i= BIRNN`(x1:n, i)x1:n= E(w1), E(w2), ?
?
?
, E(wn)The Multi-task training protocol We assumeT different training set, D1, ?
?
?
, DT, whereeach Dtcontains pairs of input-output sequences(w1:n, yt1:n), wi?
V , yti?
Lt.
The input vo-cabulary V is shared across tasks, but the outputvocabularies (tagset) Ltare task dependent.At each step in the training process we choosea random task t, followed by a random traininginstance (w1:n, yt1:n) ?
Dt.
We use the tag-ger to predict the labels y?ti, suffer a loss with re-spect to the true labels ytiand update the modelparameters.
Notice that a task t is associatedwith a bi-RNN level `(t).
The update for a sam-ple from task t affects the parameters of ftandBIRNN1, ?
?
?
, BIRNN`(t), but not the param-eters of ft?6=tor BIRNNj>`(t).Implementation details Our implementation isbased the CNN library1for dynamic neural net-works.
We use CNN?s LSTM implementation asour RNN variant.
The classifiers ft() take the formof a linear transformation followed by a softmaxft(v) = argmaxisoftmax(W(t)v+bt)[i], wherethe weights matrix W(t)and bias vector b(t)aretask-specific parameters.
We use a cross-entropyloss summed over the entire sentence.
The net-work is trained using back-propagation and SGDwith batch-sizes of size 1, with the default learn-ing rate.
Development data is used to determinethe number of iterations.We initialize the embedding layer E with pre-trained word embeddings.
We use the Senna em-beddings2in our domain adaptation experiments,but these embeddings may have been inducedfrom data including the test data of our main ex-periments, so we use the Polyglot embeddings inthese experiments.3We use the same dimension-ality for the hidden layers as in our pre-trained em-beddings.3 Experiments and ResultsWe experiment with POS-tagging, syntacticchunking and CCG supertagging.
See examplesof the different tasks below:WORDS Vinken , 61 years oldPOS NNP , CD NNS JJCHUNKS B-NP I-NP I-NP I-NP I-NPCCG N , N/N N (S[adj]\ NP)\ NPIn-domain MTL In these experiments, POS,Chunking and CCG data are from the EnglishPenn Treebank.
We use sections 0?18 for trainingPOS and CCG supertagging, 15?18 for trainingchunking, 19 for development, 20 for evaluatingchunking, and 23 for evaluating CCG supertag-ging.
These splits were motivated by the need forcomparability with previous results.41http://www.github.com/clab/cnn2http://ronan.collobert.com/senna/3http://polyglot.readthedocs.org4In CCG supertagging, we follow common practice andonly evaluate performance with respect to the 425 most fre-quent labels.
For this reason, we also do not calculate anyloss from not predicting the other labels during training (butwe do suffer a loss for tokens tagged with a different labelduring evaluation).233LAYERS DOMAINSCHUNKS POS BROADCAST (6) BC-NEWS (8) MAGAZINES (1) WEBLOGS (6)BI-LSTM3 - 88.98 91.84 90.09 90.363 3 88.91 91.84 90.95 90.433 1 89.48 92.03 91.53 90.78Table 1: Domain adaptation results for chunking across four domains (averages over micro-F1s forindividual files).
The number in brackets is # files per domain in OntoNotes 4.0.
We use the two firstfiles in each folder for POS supervision (for train+dev).We do MTL training for either (POS+chunking)or (POS+CCG), with POS being the lower-leveltask.
We experiment three architectures: singletask training for higher-level tasks (no POS layer),MTL with both tasks feeding off of the outer layer,and MTL where POS feeds off of the inner (1st)layer and the higher-level task on the outer (3rd)layer.
OUr main results are below:POS CHUNKS CCGBI-LSTM- 95.28 91.043 95.30 92.941 95.56 93.26Our CHUNKS results are competitive with state-of-the-art.
Suzuki and Isozaki (2008), for ex-ample, reported an F1-score of 95.15% on theCHUNKS data.
Our model also performs consid-erably better than the MTL model in Collobert etal.
(2011) (94.10%).
Note that our relative im-provements are also bigger than those reported byCollobert et al (2011).
Our CCG super taggingresults are also slighly better than a recently re-ported result in Xu et al (2015) (93.00%).
Ourresults are significantly better (p < 0.05) than ourbaseline, and POS supervision at the lower layer isconsistently better than standard MTL.Additional tasks?
We also experimented withNER (CoNLL 2003), super senses (SemCor), andthe Streusle Corpus of texts annotated with MWEbrackets and super sense tags.
In none of thesecases, MTL led to improvements.
This suggeststhat MTL only works when tasks are sufficientlysimilar, e.g., all of syntactic nature.
Collobert etal.
(2011) also observed a drop in NER perfor-mance and insignificant improvements for SRL.We believe this is an important observation, sinceprevious work on deep MTL often suggests thatmost tasks benefit from each other.Domain adaptation We experiment with do-main adaptation for syntactic chunking, based onOntoNotes 4.0.
We use WSJ newswire as oursource domain, and broadcast, broadcasted news,magazines, and weblogs as target domains.
We as-sume main task (syntactic chunking) supervisionfor the source domain, and lower-level POS su-pervision for the target domains.
The results inTable 1 indicate that the method is effective for do-main adaptation when we have POS supervisionfor the target domain.
We believe this result isworth exploring further, as the scenario in whichwe have target-domain training data for low-leveltasks such as POS tagging, but not for the task weare interested in, is common.
The method is ef-fective only when the lower-level POS supervisionis applied at the lower layer, supporting the im-portance of supervising different tasks at differentlayers.Rademacher complexity is the ability of mod-els to fit random noise.
We use the procedure inZhu et al (2009) to measure Rademacher com-plexity, i.e., computing the average fit to k randomrelabelings of the training data.
The subtask in ourset-up acts like a regularizer, increasing the induc-tive bias of our model, preventing it from learningrandom patterns in data.
Rademacher complex-ity measures the decrease in ability to learn suchpatterns.
We use the CHUNKS data in these exper-iments.
A model that does not fit to the randomdata, will be right in 1/22 cases (with 22 labels).We report the Rademacher complexities relative tothis.LSTM(-3) LSTM(3-3) LSTM(1-3)1.298 1.034 0.990Our deep single task model increases perfor-mance over this baseline by 30%.
In contrast, wesee that when we predict both POS and the tar-get task at the top layer, Rademacher complexityis lower and close to a random baseline.
Interest-ingly, regularization seems to be even more effec-tive, when the subtask is predicted from a lowerlayer.2344 ConclusionMTL and sharing of intermediate representations,allowing supervision signals of different tasks tobenefit each other, is an appealing idea.
However,in case we suspect the existence of a hierarchy be-tween the different tasks, we show that it is worth-while to incorporate this knowledge in the MTLarchitecture?s design, by making lower level tasksaffect the lower levels of the representation.AcknowledgmentsAnders S?gaard was supported by ERC StartingGrant LOWLANDS No.
313695.
Yoav Goldbergwas supported by The Israeli Science Foundationgrant No.
1555/15 and a Google Research Award.ReferencesRich Caruana.
1993.
Multitask learning: a knowledge-based source of inductive bias.
In ICML.Kyunghyun Cho.
2015.
Natural language under-standing with distributed representation.
CoRR,abs/1511.07916.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Jeffrey L. Elman.
1990.
Finding Structure in Time.Cognitive Science, 14(2):179?211, March.Yoav Goldberg.
2015.
A primer on neural networkmodels for natural language processing.
CoRR,abs/1510.00726.Sepp Hochreiter and Juergen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9:1735?1780.Ozan Irsoy and Claire Cardie.
2014.
Opinion Miningwith Deep Recurrent Neural Networks.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages720?728, Doha, Qatar, October.
Association forComputational Linguistics.M.-T. Luong, Q. V. Le, I. Sutskever, O. Vinyals, andL.
Kaiser.
2015.
Multi-task Sequence to SequenceLearning.
ArXiv e-prints, November.Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the conll-2000 shared task chunk-ing.
In Fourth Conference on Computational Natu-ral Language Learning and of the Second LearningLanguage in Logic Workshop, pages 127?132.M.
Schuster and Kuldip K. Paliwal.
1997.
Bidirec-tional recurrent neural networks.
IEEE Transactionson Signal Processing, 45(11):2673?2681, Novem-ber.Hong Shen and Anoop Sarkar.
2005.
Voting betweenmultiple data representations for text chunking.
InProceedings of the 18th Meeting of the CanadianSociety for Computational Intelligence.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In ACL.Wenduan Xu, Michael Auli, and Stephen Clark.
2015.Ccg supertagging with a recurrent neural network.In ACL.Junho Yim, Heechul Jung, ByungIn Yoo amdChangkyu Choi, Dusik Park, and Junmo Kim.
2015.Rotating Your Face Using Multi-task Deep NeuralNetwork.
In CVPR.Cha Zhang and Zhengyou Zhang.
2014.
Improv-ing Multiview Face Detection with Multi-Task DeepConvolutional Neural Networks.
In WACV.Jerry Zhu, Timothy Rogers, and Bryan Gibson.
2009.Human Rademacher complexity.
In NIPS.235
