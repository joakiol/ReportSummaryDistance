Redef in ing similarity in a thesaurus by using corporaHiroyuki ShinnouIbar~ki UniversityDept.
of Systems EngineeringN~k~narusawa, 4-12-1Hitachi, Ibaraki, 316, Japanshinnou@lily, dse.
ibaraki, ac.
j p1 IntroductionThe aim of this paper is to automatically definethe similarity I)etween two nouns which are gener-ally used in various domains.
By these similarities,we can construct a large and general thesaurus.In applications of natural language processing,it is necessary to appropriately measure the sim-ilarity between two nouns.
The similarity is usu-ally calculated from a thesaurus.
Since a hand-made thesaurus is not slfitahle for machine use,and expensive to compile, automatical construc-tion of~a thesaurus has been attempted using cor-pora (Hindle, 1990).
llowever, the thesaurus con-structed by such ways does not contain so manynouns, and these nouns are specified by the usedcorpus.
In other words, we cannot construct hegeneral thesaurus from only a corpus.
This canbe regarded as data sparseness problem that fewnouns appear in the corpus.9b overcome data sparseness, methods to esti-mate the distribution of unseen eooecurrence frornthe distribution of similar words in the seen cooc-currence has been proposed.
Brown et al pro-posed a class-based n-gram model, which general-izes the n-gram model, to predict a word from pre-vious words in a text (Brown et al, 1992).
Theytackled data sparseness by generalizing the wordto the class which contains the word.
Pereira ctal.
also basically used the above method, but theyproposed a soft clustering scheme, in which mem-bership of a word in a class is probabilistic (Pereiraet al, 1993).
Brown and Pereira provide the clus-tering algorithm assigning words to proper classes,based on their own models.
I)agan eL al.
proposeda similarity-based model in which each word isgeneralized, not to its own specific class, but to aset of words which are most similar to it (Dagan etal., 1993).
Using this model, they successfully l)re-dieted which unobserved cooccurrenccs were morelikely than others, and estimated the probabilityof the cooecurrences (Dagan et al, 1994).
How-ever, because these schemes look for similar wordsin the corpus, the number of similarities which wecan define is rather small in comparison with thenunlber of similarities for pairs of the whole.
Thescheme to look for similar words in the corpus hasalready taken the influence of data sparseness.In this paper, we propose a method distinctfrom the above methods, which use a handmadethesaurus to find similar words.
The proposedmethod avoids data sparseness by estimating un-defined similarities from the similarity in the the-saurus and similarities defined by the corpus.Thus, the obtained similarities are the same innmuber as the similarities in the thesaurus, andthey reflect the particularity of the domain towhich the used corpus belongs.
The use of a tlm-saurus can obviously set up the similar word in-dependent of the tort)us, and has an advantagethat some ambiguities in analyzing the corpus aresolved.We have experimented by using Bunrui-goi-hyon(Bmlrui-goi-hyon, 1994), which is a kind ofJapanese handmade thesaurus, and the corpuswhich consists of Japanese economic newspaper5 years articles with about 7.85 M sentences.
Weevaluate the appropriateness of the obtained sim-ilarities.2 Defining the similarityWe call easily judge the similarity of two nounsif they are very similar.
However, the more dif-ferent they arc, the more difficult it is to definetheir similarity.
Thus, we can trust that nouns inthe class corresponding to the "leaf" of BunruLgoi-hyou are similar to each another, and this isnot affected by the domain.
In this paper, wewill refer to the class corresponding to the leaf of'Bunrui-goi-hyou the pr imi t ive  class.
Therefore,tile similarity we have to detine is the silnilaritybetween these classes.This method consists of 4 steps.Step 1 Gather the cooccurrence data from thecorpus.Step 2 Generalize the noun in the cooccurrencedata to the primitive class.S tep  3 Measure the similarity between two prim-itive classes by using the cooccurrence dataobtained in step 2.1131Step  4 Estimate undefined similarities.We will describe ach step in detail in followingsubsections.2.1 Gather ing  cooccur renee  data  (step 1)In order to carry out our method, it is necessary tofirst gather the cooccurrence data from the corpus.If a noun (N), a postpostional particle (P), anda verb (V) appear in a sentence in this order, wepick out the cooccurrence data \[N, P, V\].
In thisstudy, we gathered cooccurrence data only fromthe postpostional particle "wo',  because "wo" isthe most effective postpostional particle for clas-sifying nouns.As a corpus, we used five years of Japanese co-nomic newspaper articles.
The corpus has about7.85 M sentences, and the average number of char-acters in one sentence was about 49.
From thecorpus, we gathered about 4.41 M bits of cooccur-rence data (about 1.48 M types) whose postposi-tional particle was "wo".
From them, we removedthe cooccurrence data whose frequency was 1, orwhose verb does not appear more than 20 times.In all, we obtained about 3.26 M bits of cooccur-fence data, which consisted of about 0.36 M types.These cooccurrence data are used in the next step.2.2 Genera l i z ing  the  word  to the class(step 2)In step 2, we generalize the noun in cooccurrencedata gathered in step 1 to the primitive class towhich this noun belongs.First, we should explain about Bunrui-goi-hyou.Bunrui-goi-hyou is a kind of thesaurus with a tree-like structure that has a maximum depth of level6.
Class IDs are assigned to each "leaf" of the"tree".
Each noun has a class ID correspondingto the meaning of the noun.
The class ID cor-responds to the primitive class.
Bunrui-goi-hyouhas 3,582 primitive classes.Because many nouns, such as compound nouns,are not in Bunrui-goi-hyou, we cannot always gen-eralize all nouns to primitive classes, 86.0% of thenouns in cooccurrence data gathered in step 1could be generalized to primitive classes.In this generalization, the problem of poly-semy arises.
A noun has usually several primitiveclasses because of the polysemy.
We solve somepolysemy from the distribution of nouns in cooc-currence data which have the same verb.
Thiscannot be discussed here for lack of space.
Weonly report that the cooccurrence data gatheredin step 1 contain 572,529 bits of polysemy whichconsisted of 27,918 types, and 472,273 bits of pol-ysemy ( 18,534 types ) were solved.In all, we obtained 2,708,135 bits of general-ized cooccurrence data, which consisted of 115,330types.2.3 Measur ing  the  s imi lar i ty  betweenclasses (step 3)In step 3, we measure the similarity betweentwo primitive classes by using the method givenby Hindle (Hindle, 1990).First, we define the nmtual information MI ofa verb v and a primitive class C as follows.
"Z~mY2M ( ,C)=logs N (eq.1)N NIn the above equation, N is the total number ofcooccurrence data bits, and f(v) and f(C) are thefrequency of v and C in the whole cooccurrencedata set respectively, and f(v, C) is the frequencyof the cooccurrence data \[C, wo, v\].
Next, the sim-ilarity sire of a class Ci and Cj for a verb v isdefined as follows.min(IfI(v, Ci)I, IMI(v, Ci)l)= : f l (v ,  Ci)*MI(v, Cj)>O0 : otherwiseFinally, the similarity of Ci and Cj is measured asfollows.SIM(Ci, Cj) = E sire(v, Ci, Cj )vIn eqnation (eq.1), f (v)  > 0 because v is theverb in a certain cooccurrence data obtained instep 2.
However, f(C) may be equal to 0 becausetile primitive class C is a certain class in all prim-itive classes.
If f(C) = 0, then MI(v, C) cannotbe defined.
So, if f(Ci) = 0 or f(Cj) = 0 for allverb v, then SIM(Ci, Cj) is undefined.2.4 Es t imat ing  the  undef ined  s imi la r i ty(s tep 4)There are 3,582 types of primitive classes, soass2C2 = 6,413,571 similarities must be defined.Through step 3, there were 2,049,566 similaritieswhich had been defined.
This is 32.0 % of thewhole.In step 4, we estimate undefined similaritiesby the thesaurus and defined similarities.
Letus estimate the undefined similarity between theclasses Ca and Cb.
First, we pick out the setof primitive classes {Ca,, Ca2," ", Ca, }, such thateach class has the common parent node as classCa in Bunrui-goi-hyou, that is, the class C(~,is the brother node of class Ca.
By the sameprocess, we pick out the set of primitive classes{Cbl, Cb2,''', Cbj } for class Cb.
The similarity inBunrui-goi-hyou are reliable if its value is large.Thus, it is reliable the defined SIM(C~k,Cb)and the defined SIM(C~,Cb,,) are close to theundefined SIM(C~,Cb).
Therefore, we defineSIM(C~, Cb) by the average of SIM(C~ k , Cb) andSIM(Ca, Cb~).
This process corresponds to thatthe slot in the Fig.l(a) is filled with the aver-age of values in the shade part in the figure.
If1132SIM(C4., Cb).?
C6CbiCa, ' "  ca "'" cai(a) 1st estimationSIM(Ca, Cb)"k / "  "?
Cb\] I cu,0<'"  Ca '" Ca~(b) 2nd estimationSIM(Ca, G,).
.
.
.
.
.
.
.
.
.
.
.
.
.
a: :: ~ ::m m J : , :(c) 3rd estimationFigure 1: Estimation of SIM((/~, C~)the undefined pairs are left through above esti-mations, they are estimated by the ave.rage ofSIM (U,,k, (lb,,).
This process corresponds to thatthe slot in the Fig.l (b) is filled with the averagevalues in the shade part in the figure.
If undefinedpairs still remain, we pick out the set of primitiveclasses, such that the grandmother node of eachclass is the same as that of Ca and (;'~ , and werepeat the above processes (ef.
Fig.l((')).Fig.2 shows the ratio of the number of similar-ities defined in each process.r corpUs.3rd estimation ,~ I1st estimation I 2LII.,,% I, %HFigure 2: ratio of the number of similarities de-fined in each process3 Eva luat ionsFirst, we evaluate the obtained similarities bycomparing them with the similarities in Bunrui-goidlyou.
The similarity in Bunrui-goi-hyou aredefined by the level of the common parent nodeof two classes.
Tab.2 shows the average of simi-larities between two classes, such that these twoclasses have the common parent node whose levelis x in Bunrni-goi-hyou.Tab.2 shows that the larger the similarity inBunrui-goi-hyou is, the larger the obtained sim-ilarity is.
It follows that the obtained similarityis roughly similar to the similarity in Bunrui-goi-hyou.Next, we evaluate the appropriateness of thefirst estimation.
The average of "coefftcient ofvariation >' for similarities used in each first cs->l'he coefficient of variation is the stamtard evia-tion divided by the mean.the level of theCOFIIIIIOII |ntrellt node1average ofobtained similarities2.1603.6906.51.9lO.O9O5 14.8156 ooTable 2: tendency of obtained similaritiestimation is 0.384.
And the coetlicient of variationfor all similarities measured by the corpus is 2.125.It follows that similarities used in first estimationare close to each other.At l~t ,  we evaluate the appropriateness of theobtained similarity by selecting a verbal meaning.In this experiment, to measure the similarity inBunrui-goi-hyou and the similarity obtained byour method.
Because the similarity in Bunrui-goi-hyou is rough, multiple answers may arise.
Inevaluation of the similarity in Bunrui-goi-hyou, wegive a C) if the answer is unique and right, a Aif the answers contain the right answer, and ?
ifthe answers don't contain the right answer.
\[nevaluation of our similarities, we give a C) if thelargest similarity is right, a A if 1st or 2ud largest;similarities is right answer, and ?
if neither of 1stand 2nd largest similarities is the right answer.Tab.1 shows the results of evaluations.
\]'histable shows that the similarity obtained by ourmethod is a little better than the similarity inBunrui-goi-hyou.4 RemarksIt is difficult to extract all knowledge from onlya corpus because of ineoml)lete analysis and datasparseness.
In order to avoid these difilculties, theapproach to use of different resources from the co lpus is promising.
To construct he thesaurus fi'om1133exmnple nounse ~c.
(9)~ ~ (4)~ ~#tzo/3)Our methodu7 (~, ~ ,  ?, ~ .... )~ (~, ,Nv ,~,~!~'  ....)25 (3~, ~'-P ~4,  ~,  ~, .
.
.
)18~pat tern  (num.of mean ings  )1613132217,~(i1919~'  ~N' N&' ~"'  //,~q~, g, ~.~, ~ .... )~, ~E, ~, ~z,u .... )f~, ~ ,  ~, ~ .... )'~ ,  ~, ~m, z, .... )~,.~, ~ v, $, Nm .... )nouns  for test13 (~t I ,  ~M,  I::'--31/, {ziznr~ .
.
.
.9 (:b,~.x.,~,oc:oa: .... )19 ~,~\ ] ' .
,  7U/~f ,~, .
.
.
)is V~,~/, 4:~--Y, ~ .... )s (~ ,~,~,~ .... )is i~,  ~2~, ~JN, i~ , .
.
.
)28 ~,  H~,~N,  ~, .
.
.
)Tota l  \] 184I Bunru i -go i -hyouo l / '1  x O / ' I x14 2 8 17 0 70 2 2 1 1 28 1 7 9 1 61 1 1 1 0 2) 14 0 2 13 2 1 $l7 0 1 7 0 1) ', 12 0 1 10 l 23 1 5 3 1 57 4 8 9 3 716 0 2 14 2 27 0 1 8 0 014 1 3 16 0 213 6 9 16 3 9_.1 116 I 18 I 50 I 124 I 14 14~ ITable h Result of test of verbal meaning selectiona dictionary (Turumaru et al, 1991), and to makeexample data from a usable knowledge (Kanedaet al, 1995) is considered this approach.
The pro-posed method uses the handmade thesaurus as thedifferent resource from the corpus.
In addition,the statistical data from the corpus are weighted.However, it will be important in future researchto investigate how much weight should be givento each bit of data.It is difficult to build knowledge correspondingto each domain from zero.
So it is important oextend and modify the existing knowledge corre-sponding to the purpose of use.
In this method,relatively few bits of cooccurrence data are usedbecause nouns in the cooecurrence data are not onBunrui-goi-hyou.
If we extend Bunrui-goi-hyou,these unused cooccurrence data may be useful.And by using the obtained similarities, we canmodify Bunrui-goi-hyou.
Since our method con-struct a thesaurus from the handmade thesaurusby the corpus, it can be considered a method torefine the handmade thesaurus uch as to be suit-able for the domain of the used corpus.5 Conc lus ionsIn this paper, we proposed a method to definesimilarities between general nouns used in vari-ous domains.
The proposed method redefines thesimilarity in a handmade thesaurus by using cor-pora.
The method avoids data sparseness by esti-mating undefined similarities from the similarityin the thesaurus and similarities defined by cor-pora.
The obtained similarities are obviously thesame in number as the original similarities, andare more appropriate than the original similari-ties in the thesaurus.By using Bnnru~-goi-hyou as the handmade the-saurus and newspaper articles with about 7.85 Msentences as a corpus, we confirmed the appropri-ateness of this method.In the future, we will extend and modifyBunrui-goi-hyou by the cooecurrence data and thesimilarities obtained in this study, and will try toclassify multiple senses of verbs.AcknowledgmentThe corpus used in our experiment is extractedfrom CD-ROMs ('90 - '94)  sold by Nihon KeizaiShinbun company.
We deeply appreciate the Ni-hon Keizai Shinbun company to permit the use ofthis corpus and many people who negotiated withthe company about the use of this corpus.ReferencesBrown,P.F.,Pietra,V.D, deSouza,P.V., Lai,J.C.
andMercer,R.L.
: 1992.
Class-Based n-gram Modelsof Natural Language, Computational Linguistics,Vol.
18,No.4,pp.467-479 (1 92).Dagan,I., Marcus,S, and Markovitch,S.
: 1993.Contextual Word Similarity and Estimation fromSparse Data, In 31th Annual Meeting of the Asso-ciation for Computational Linguistics, pp.164-171.Dagan,I., Pereira,F., and Lee,L.
: \]994.
Similarity-Based Estimation of Word Coocurrencc Probabili-ties, In 32th Annual Meeting of the Association forComputational Linguistics, pp.272-278.Hindle,D.
1990.
Noun classification from predicate-argument s ructures.
In 28th Annual Meeting of theAssociation for Computational Linguistics, pp.268-275.Kaneda,S., Akib%Y., and Ishii,M.
: 1995.
Jireinimotozuku eigodousi scntakuruuru no syuuseigatagakusyuuhou (in Japanese), In Proceedings of thefirst annual meeting of the Assoiation for NaturalLanguage Processing, pp.333 336.Pereira,F., Tishby, N., and Lee,L.
: 1993.
Distribu-tional Clustering of English Word, In 31th An-nual Meeting of the Association for ComputationalLinguistics,pp.183-\]90.The National Language Research Institute : 1994.Bunrui-goi-hyou (in Japanese), Shuuei Publishing.Turumaru,H., Takesita,K., Itami,K., Yanagawa,T.and Yoshida,S.
: 1991.
An Approach to'The-saurus Construction from Japanese Language Dic-tionary (in Japanese), IPS Japan NL-83-16,Vol.91,No.37,91-NL-83, pp.121-128.1134
