Machine Learning Methods forChinese Web Page CategorizationJ i  He  1, Ah-Hwee Tan 2 and Chew-L i ra  Tan  11School of Computing, National University of Singapore10 Kent Ridge Crescent, Singapore 119260(heji,tancl}@comp.nus.edu.sg2Kent Ridge Digital Labs21 Heng Mui Keng Terrace, Singapore 119613ahhwee@krdl.org.sgAbst rac tThis paper eports our evaluation ofk Nearest Neighbor (kNN), SupportVector Machines (SVM), and Adap-tive Resonance Associative Map(ARAM) on Chinese web page clas-sification.
Benchmark experimentsbased on a Chinese web corpusshowed that their predictive per-formance were roughly comparablealthough ARAM and kNN slightlyoutperformed SVM in small cate-gories.
In addition, inserting rulesinto ARAM helped to improve per-formance, especially for small well-defined categories.1 In t roduct ionText categorization refers to the task of au-tomatically assigning one or multiple pre-defined category labels to free text docu-ments.
Whereas an extensive range of meth-ods has been applied to English text cate-gorization, relatively few have been bench-marked for Chinese text categorization.
Typi-cal approaches toChinese text categorization,such as Naive Bayes (NB) (Zhu, 1987), VectorSpace Model (VSM) (Zou et al, 1998; Zou etal., 1999) and Linear List Square Fit (LLSF)(Cao et al, 1999; Yang, 1994), have well stud-ied theoretical basis derived from the informa-tion retrieval research, but are not known tobe the best classifiers (Yang and Liu, 1999;Yang, 1999).
In addition, there is a lack ofpublicly available Chinese corpus for evaluat-ing Chinese text categorization systems.This paper reports our applications ofthree statistical machine learning methods,namely k Nearest Neighbor system (kNN)(Dasarathy, 1991), Support Vector Machines(SVM) (Cortes and Vapnik, 1995), and Adap-tive Resonance Associative Map (ARAM)(Tan, 1995) to Chinese web page categoriza-tion.
kNN and SVM have been reported asthe top performing methods for English textcategorization (Yang and Liu, 1999).
ARAMbelongs to a popularly known family of pre-dictive self-organizing neural networks whichuntil recently has not been used for docu-ment classification.
The trio has been eval-uated based on a Chinese corpus consistingof news articles extracted from People's Daily(He et al, 2000).
This article reports the ex-periments of a much more challenging task inclassifying Chinese web pages.
The Chineseweb corpus was created by downloading fromvarious Chinese web sites covering awide vari-ety of topics.
There is a great diversity amongthe web pages in terms of document length,style, and content.
The objectives of our ex-periments are two-folded.
First, we examineand compare the capabilities of these meth-ods in learning categorization k owledge fromreal-fife web docllments.
Second, we investi-gate if incorporating domain knowledge de-rived from the category description can en-hance ARAM's predictive performance.The rest of this article is organized as fol-lows.
Section 2describes our choice of the fea-ture selection and extraction methods.
Sec-tion 3 gives a sllrnrnary of the kNN and SVM,and presents the less familiar ARAM algo-rithm in more details.
Section 4 presents ourevaluation paradigm and reports the experi-93mental results.2 Feature  Se lec t ion  and  Ext rac t ionA pre-requisite of text categorization is to ex-tract a suitable feature representation f thedocuments.
Typically, word stems are sug-gested as the representation units by infor-mation retrieval research.
However, unlikeEnglish and other Indo-European languages,Chinese text does not have a natural delim-iter between words.
As a consequence, wordsegmentation is a major issue in Chinese doc-ument processing.
Chinese word segmenta-tion methods have been extensively discussedin the literature.
Unfortunately perfect preci-sion and disambiguation cannot be reached.As a result, the inherent errors caused byword segmentation always remains as a prob-lem in Chinese information processing.In our experiments, a word-class bi-grammodel is adopted to segment each trainingdocument into a set of tokens.
The lexi-con used by the segmentation model contains64,000 words in 1,006 classes.
High precisionsegmentation is not the focus of our work.
In-stead we aim to compare different classifier'sperformance on noisy document set as long asthe errors caused by word segmentation arereasonably low.To select keyword features for classifica-tion, X (CHI) statistics is adopted as theranking metric in our experiments.
A priorstudy on several well-known corpora in-cluding Reuters-21578 and OHSUMED hasproven that CHI statistics generally outper-forms other feature ranking measures, suchas term strength (TS), document frequency(DF), mutual information (MI), and informa-tion gain (IG) (Yang and J.P, 1997).During keyword extraction, the documentis first segmented and converted into akeyword frequency vector (t f l ,  t f2 , .
.
.
,  t.f M ) ,where t f i  is the in-document term frequencyof keyword wi, and M is the number of thekeyword features selected.
A term weight-ing method based on inverse document .fre-quency (IDF) (Salton, 1988) and the L1-norm~llzation are then applied on the fre-quency vector to produce the keyword featurevector(X l ,  X2 ,  ?
- ?
, XM)x = max{xi} ' (i)in which xi is computed byzi = (1 + log 2 tf i)  log2 ~ (2)n i  'where n is the number of documents in thewhole training set, and ni is the number oftraining documents in which the keyword wioccurs at least once.3 The  C lass i f iers3.1 k Nearest Neighbork Nearest Neighbor (kNN) is a tradi-tional statistical pattern recognition algo-rithm (Dasarathy, 1991).
It has been studiedextensively for text categorization (Yang andLiu, 1999).
In essence, kNN makes the predic-tion based on the k training patterns that areclosest to the unseen (test) pattern, accord-ing to a distance metric.
The distance metricthat measures the similarity between two nor-malized patterns can be either a simple LI-distance function or a L2-distance function,such as the plain Euclidean distance definedbyD(a ,b)=~s~.
(a~-bi)2.
(3)The class assignment to the test pattern isbased on the class assignment of the closest ktraining patterns.
A commonly used methodis to label the test pattern with the class thathas the most instances among the k nearestneighbors.
Specifically, the class index y(x)assigned to the test pattern x is given byyCx) ..-.. arg'max, {n(dj, )ld.
:j kNN}, (4)where n(dj,  ~) is the number of training pat-tern dj in the k nearest neighbor set that areassociated with class c4.The drawback of kNN is the difficulty indeciding a optimal k value.
Typically it hasto be determined through conducting a seriesof experiments using different values.943.2 Support  Vector  MachinesSupport Vector Machines (SVM) is a rela-tively new class of machine learning tech-niques first introduced by Vapnik (Cortesand Vapnik, 1995).
Based on the structuralrisk minimization principle from the compu-tational learning theory, SVM seeks a decisionsurface to separate the tralning data pointsinto two classes and makes decisions based onthe support vectors that are selected as theonly effective lements from the training set.Given a set of linearly separable pointss = {x  Rnli = 1,2 , .
.
.
,N},  each point xibelongs to one of the two classes, labeled asy iE{-1,+l}.
A separating hyper-plane di-vides S into two sides, each side containingpoints with the same class label only.
Theseparating hyper-plane can be identified bythe pair (w,b) that satisfiesw-x+b=0and y i (w'x i  + b)>l (5)for i = 1, 2 , .
.
.
,  N; where the dot product op-eration ?
is defined byw.
x ---- ~ wixi (6)for vectors w and x.
Thus the goal of theSVM learning is to find the optimal separat-ing hyper-plane (OSH) that has the maximalmargin to both sides.
This can be formula-rized as:minimize ?w.
wsubject o yi(w.x i  + b)>l (7)The points that are closest to the OSH aretermed support vectors (Fig.
1).The SVM problem can be extended to lin-early non-separable case and non-linear case.Various quadratic programming algorithmshave been proposed and extensively studiedto solve the SVM problem (Cortes and Vap-nik, 1995; Joachims, 1998; Joacbims, 1999).During classification, SVM makes decisionbased on the OSH instead of the wholetraining set.
It simply finds out on whichside of the OSH the test pattern is located.0 0 O0o o ,J / / / - - .
.Figure 1: Separating hyperplanes (the setof solid lines), optimal separating hyperpIane(the bold solid line), and support vectors (datapoints on the dashed lines).
The dashed linesidentify the max margin.This property makes SVM highly compet-itive, compared with other traditional pat-tern recognition methods, in terms of com-putational efficiency and predictive accuracy(Yang and Liu, 1999).In recent years, Joachims has done much re-search on the application of SVM to text cat-egorization (Joachims, 1998).
His SVM zightsystem published via http://www-ai.cs.uni-dortmund.de/FORSCHUNG/VERFAHREN/SVM_LIGHT/svm_light.eng.html is used inour benchmark experiments.3.3 Adapt ive  Resonance Associat iveMapAdaptive Resonance Associative Map(ARAM) is a class of predictive serf-organizing neural networks that performsincremental supervised learning of recog-nition categories (pattern classes) andmultidimensional maps of patterns.
AnARAM system can be visualized as twooverlapping Adaptive Resonance Theory(ART) modules consisting of two input fieldsF~ and F1 b with an F2 category field (Tan,1995; Tan, 1997) (Fig.
2).
For classificationproblems, the F~ field serves as the inputfield containing the document feature vectorand the F1 b field serves as the output fieldcontaining the class prediction vector.
TheF2 field contains the activities of the recogni-tion categories that are used to encode thepatterns.95..
I ?/?
Ix !.
'1 x,ARTa A B ARTbFigure 2: The Adaptive Resonance Associa-tive Map architectureWhen performing classification tasks,ARAM formulates recognition categories ofinput patterns, and associates each cate-gory with its respective prediction.
Duringlearning, given an input pattern (documentfeature) presented at the F~ input layerand an output pattern (known class label)presented at the Fib output field, the categoryfield F2 selects a winner that receives thelargest overall input.
The winning node se-lected in F2 then triggers a top-down primingon F~ and F~, monitored by separate resetmechanisms.
Code stabilization is ensuredby restricting encoding to states whereresonance are reached in both modules.By synchronizing the un.qupervised catego-rization of two pattern sets, ARAM learnssupervised mapping between the pattern sets.Due to the code stabilization mechanism,fast learning in a real-time environment isfeasible.The knowledge that ARAM discovers dur-ing learning is compatible with IF-THENrule-based presentation.
Specifically, eachnode in the FF2 field represents a recognitioncategory associating the F~ patterns with theF1 b output vectors.
Learned weight vectors,one for each F2 node, constitute a set of rulesthat link antecedents to consequences.
At anypoint during the incremental learning process,the system architecture can be translated intoa compact set of rules.
Similarly, domainknowledge in the form of IF-THEN rules canbe inserted into ARAM architecture.The ART modules used in ARAM can beART 1, which categorizes binary patterns, oranalog ART modules such as ART  2, ART  2-A, and fuzzy ART, which categorize both bi-nary and analog patterns.
The fuzzy ARAM(Tan, 1995) algorithm based on fuzzy ART(Carpenter et al, 1991) is introduced below.Parameters: Fuzzy ARAM dynamics aredetermined by the choice parameters aa > 0and ab > 0; the learning rates ~a E \[0, 1\] and~b E \[0, 1\]; the vigilance parameters Pa E \[0, 1\]and Pb E \[0, 1\]; and the contribution parame-ter '7 E \[0, 1\].Weight vectors: Each F2 category node jis associated with two adaptive weight tem-plates w~ and w~.
Initially, all category nodesare uncommitted and all weights equal ones.After a category node is selected for encoding,it becomes committed.Category choice: Given the F~ and F1 b in-put vectors A and B, for each F2 node j, thechoice function Tj is defined byIA Aw~l IB A w~l= ~a~ + Iw~'l + (1 --~)~b + Iw~l' (S)where the fuzzy AND operation A is definedby(p A q)i --~ min(pi, qi), (9)and where the norm I-I is defined byIPl -= ~P i  (10)ifor vectors p and q.The system is said to make a choice whenat most one F2 node can become active.
Thechoice is indexed at J whereTj  = ma,x{Tj : for all F2 node j } .
(11)When a category choice is made at node J,yj = 1; andyj =0 for all j ~ J.Resonance or reset: Resonance occurs ifthe match .functions, m~ and m~, meet thevigilance criteria in their respective modules:IA A w~lm~ = \[AI _> pa (12)andm~ = IB A w~l> Pb.
(13)IBI -96Learning then ensues, as defined below.
Ifany of the vigilance constraints is violated,mismatch reset occurs in which the value ofthe choice function Tj  is set to 0 for the du-ration of the i.nput presentation.
The searchprocess repeats to select another new index Juntil resonance is achieved.Learn ing:  Once the search ends, the weightvectors w~ and w~ are updated according tothe equationsW~ (new) - -  (1 ,~ iRa(o ld )  - - .
.
, , , j  +&(A^w3(14)andwb, cnew)~ (i ~ ~_ b(old) = - ,bJWj + ~b(B A wbj (Old))(15)respectively.
Fast learning corresponds to set-ting/~a =/~b = 1 for committed nodes.Classification: During classification, usingthe choice rule, only the F2 node J that re-ceives maximal F~ ~ F2 input Tj predictsARTb output.
In simulations,1 if j = J where T j  > Tky j  = for all k ?
J (16)0 otherwise.The F1 b activity vector x b is given byJThe output prediction vector B is then givenbyB ~ (bl, b2,.
.
,  bN)  = X b (18)where bi indicates the likelihood or confidenceof assigning a pattern to category i.Ru le  insert ion:  Rule insertion proceeds intwo phases.
The first phase parses the rulesfor keyword features.
When a new keyword isencountered, it is added to a keyword featuretable containing keywords obtained throughautomatic feature selection from trainingdocuments.
Based on the keyword featuretable, the second phase of rule insertiontranslates each rule into a M-dimensionalvector a and a N-dimensional vector b, whereM is the total number of features in thekeyword feature table and N is the numberof categories.
Given a rule of  the followingformat,IF Xl ,  X2 ,  - ?
?
, XmTHEN Yl, Y2,.-., Ynwhere xt , .
.
.
,  xm are antecedents andYt , .
.
.
,Yn are consequences, the algorithmderives a pair of vectors a and b such that?
for each index i = 1, .
.
.
,  M,1 ifwi = x j  for some j 6 {1 , .
.
.
,m}ai = 0 otherwise(19)where wi is the i th entry in the keyword fea-ture table; and for each index i = 1, .
.
.
,  N,1 ifwi = y j  for some j E {1 , .
.
.
,n}bi = 0 otherwise(20)where wi  is the class label of the category i.The vector pairs derived from the rules arethen used as training patterns to initialize aARAM network.
During rule insertion, thevigilance parameters Pa and Pb are each setto 1 to ensure that only identical attributevectors are grouped into one recognition cat-egory.
Contradictory symbolic rules are de-tected during rule insertion when identical in-put attribute vectors are associated with dis-tinct output attribute vectors.4 Empi r i ca l  Eva luat ion4.1 The Chinese Web CorpusThe Chinese web corpus, colleeted in-house,consists of web pages downloaded from vari-ous Chinese web sites covering a wide varietyof topics.
Our experiments are based on asubset of the corpus consisting of 8 top-levelcategories and over 6,000 documents.
Foreach category, we conduct binary classifica-tion experiments in which we tag the cur-rent category as the positive category and theother seven categories as the negative cate-gories.
The corpus is further partitioned intotraining and testing data such that the num-ber of the training documents i at least 2.5times of that of the testing documents (Table1).97Table 1: The eight top-level categories in theChinese web corpus, and the training and testsamples by category.Category Description Train Test ArtArt Topic regarding 325 102 Beliefliterature, artBelief Philosophy and 131 40 B/zreligious beliefsBiz Business 2647 727 EduEdu Education 205 77IT  Computer and 1085 309 /Tinternet informaticsJoy Online fresh, 636 216interesting info JoyMed Medical care 155 57 Mealrelated web sitesSci Various kinds 119 39 Sciof scienceTable 2: A sample set of 19 rules generatedbased on the accompanied description of theChinese web categories.
:- ~ (Chinese painting):- ~"  (.pray) ~.~r~ (rabbi):- {~ (promotion) ~ (rrcal estate)~:P (cli~O:- ~-~ (undergradua~) - -~ (supervisor)~2N (campus):- ~:~k (version) ~ (virus)g/~k~ (ffirewan) ~ (program):- ~ '~ (lantern riddle):- ~ (health cam) ~J~ (pmscriplion)\ [~  (medical jurisprudence):- ~fl ~ ~ (supernaturalism)~ (high technology)4.2 Exper iment  Parad igmkNN experiments used the plain Euclideandistance defined by equation (3) as the simi-laxity measure.
On each pattern set contain-ing a varying number of documents, differentvalues of k ranging ~om 1 to 29 were testedand the best results were recorded.
Only oddk were used to ensure that a prediction canalways be made.SVM experiments used the default built-ininductive SVM parameter set in VM tight,which is described in detail on the web siteand elsewhere (Joachims, 1999).ARAM experiments employed a standardset of parameter values of fuzzy ARAM.
Inaddition, using a voting strategy, 5 ARAMsystems were trained using the same set ofpatterns in different orders of presentationand were combined to yield a final predictionvector.To derive domain theory on web page clas-sification, a varying number (ranging from 10to 30) of trainiug documents from each cate-gory were reviewed.
A set of domain knowl-edge consists of 56 rules with about one to 10rules for each category was generated.
Onlypositive rules that link keyword antecedentsto positive category consequences were in-cluded (Table 2).4.3 Per fo rmance  MeasuresOur experiments adopt the most commonlyused performance measures, including the re-call, precision, and F1 measures.
Recall (R) isthe percentage of the documents for a givencategory that are classified correctly.
Preci-sion (P) is the percentage of the predicteddocuments for a given category that are clas-sifted correctly.
Ft rating is one of the com-monly used measures to combine R and P intoa single rating, defined as2RPFt = (R + P)" (21)These scores are calculated for a series of bi-nary classification experiments, one for eachcategory.
Micro-averaged scores and macro-averaged scores on the whole corpus arethen produced across the experiments.
Withmicro-averaging, the performance measuresare produced across the documents by addingup all the documents counts across the differ-ent tests, and calculating using these summedvalues.
With macro-averaging, each categoryis assigned with the same weight and per-formance measures are calculated across thecategories.
It is understandable that micro-averaged scores and macro-averaged scores re-flect a classifier's performance on large cate-gories and small categories respectively (Yangand Liu, 1999).98Table 3:classifiers on the Chinese webkNNCategoryArtBeliefBizEduITJoyMed$ciPredictive performance of the fourP Rcorpus.!
SVMF1 ~P R.440 .398 .402.548 .556 .500.706 .692 .703.365 .602 .074.321 .394 .307.291 .462 .255.494 .330 .544.213!
.137 .179.795 .304.773 .425.724 .689.380 .351.309 .333.381 .236.833 .351.625 .128F~.400.526.698.180.345.328.411.156Micro-ave. .584 .482 .528 .523 .521 .522Macro-ave. .600 .352 .422 .384 .454 .380ARAM ARAMw/rulesCategory P R Fx P R F1ArtBeliefBizEduITJoyMedSei.653 .461 .540.750 .750 .750.742 .622 .677.421 .312 .358.444 .259 .327.600 .208 .309.421 .421 .421.292 .179 .222.706 .471 .565.714 .750 .732.745 .604 .667.420 .273 .331.437 .291 .350.618 .194 .296.448 .456 .452.409 .231 .295Micro-ave. .619 .453 .523 .628 .450 .524Macro-ave. .540 .402 .451 .562 .409 .4614.4 Resu l ts  and  Discuss ionsTable 3 summarizes the three classifier's per-formances on the test corpus in terms of pre-cision, recall, and F1 measures.
The micro-averaged scores produced by the trio, whichwere predominantly determined by the clas-sifters' performance on the large categories(such as Biz, IT, and Joy), were roughly com-parable.
Among the three, kNN seemed tobe marginally better than SVM and ARAM.Inserting rules into ARAM did not have asignificant impact.
This showed that do-main knowledge was not very useful for cat-egories that already have a large number oftraining examples.
The differences in themacro-averaged scores produced by the threeclassifiers, however, were much more signifi-cant.
The macro-averaged F1 score obtainedby ARAM was noticeably better than that ofkNN, which in turn was higher than that ofSVM.
This indicates that ARAM (and kNN)tends to outperform SVM in small categoriesthat have a smaller number of training pat-terns.We are particularly interested in the classi-fier's learning ability on small categories.
Incertain applications, such as personalized con-tent delivery, a large pre-labeled training cor-pus may not be available.
Therefore, a classi-f iefs ability of learning from a small trainingpattern set is a major concern.
The differentapproaches adopted by these three classifiersin learning categorization knowledge are best?
seen in the light of the distinct learning pe-culiarities they exhibit on the small trainingsets.kNN is a lazy learning method in the sensethat it does not carry out any off-line learningto generate a particular category knowledgerepresentation.
Instead, kNN performs on-line scoring to find the training patterns thatare nearest o a test pattern and makes thedecision based on the statistical presumptionthat patterns in the same category have simi-lar feature representations.
The presumptionis basically true to most pattern instances.Thus kNN exhibits a relatively stable perfor-manee across small and large categories.SVM identifies optimal separating hyper-plane (OSH) across the training data pointsand makes classification decisions based onthe representative data instances (known assupport vectors).
Compared with kNN, SVMis more computationally efficient during clas-sification for large-scale training sets.
How-ever, the OSH generated using small train-ing sets may not be very representative, spe-cially when the training patterns are sparselydistributed and there is a relatively narrowmargin between the positive and negative pat-terns.
In our experiments on small train-ing sets including Art, Belief, Edu, and Sci,SVM's performance were generally lower thanthose of kNN and ARAM.ARAM generates recognition categoriesfrom the input training patterns.
The incre-mentally learned rules abstract he major rep-resentations of the training patterns and elim-inate minor inconsistencies in the data pat-terns.
During classifying, it works in a sim-ilar fashion as kNN.
The major difference isthat AI:tAM uses the learned recognition cat-egories as the similarity-scoring unit whereaskNN uses the raw in-processed training pat-terns as the distance-scoring unit.
It follows99that ARAM is notably more scalable thankNN by its pattern abstraction capability andtherefore is more suitable for handling verylarge data sets.The overall improvement in predictive per-formance obtained by inserting rules intoARAM is also of particular interest to us.ARAM's performance was more likely to beimproved by rule insertion in categories thatare well defined and have relatively fewernumbers of training patterns.
As long as auser is able to abstract he category knowl-edge into certain specific rule representa-tion, domain knowledge could complementthe limited knowledge acquired through asmall training set quite effectively.AcknowledgementsWe would like to thank our colleagues, JianSu and Guo-Dong Zhou, for providing theChinese segmentation software and Fon-LinLai for his valuable suggestions in designingthe experiment system.
In addition, we thankT.
Joachims at the University of Dortmundfor making SVM light available.Re ferencesSuqing Cao, Fuhu Zeng, and Huanguang Cao.1999.
A mathematical model for automaticChinese text categorization.
Journal of theChina Society for Scientific and Technical In-formation \[in Chinese\], 1999(1).G.A.
Carpenter, S. Grossberg, and D.B.
Rosen.1991.
Fuzzy ART: Fast stable learning and cat-egorization of analog patterns by an adaptiveresonance system.
Neural Networks, 4:759-771.C.
Cortes and V. Vapnik.
1995.
Support vectornetworks.
Machine learning, 20:273-297.Belur V. Dasarathy.
1991.
Nearest Neighbor (NN)Norms: NN Pattern Classification Techniques.IEEE Computer Society Press, Las Alamitos,California.Ji He, A.-H. Tan, and Chew-Lira Tan.
2000.A comparative study on Chinese text catego-rization methods.
In PRICAI'2000 Interna-tional Workshop on Text and Web Mining, Mel-bourne, August.T.
Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many rel-evant features.
In Proceedings of the EuropeanConference on Machine Learning, Springer.T.
Joachims.
1999.
Making large-Scales SVMlearning Pracical.
Advances in Kernel Methods- Support Vector Learning.
B. Scholkopf, C.Burges and A. Smola (ed.
), MIT Press.Salton.
1988.
Term weighting approaches in au-tomatic text retrieval.
Information Processingand Management, 24(5):513-523.A.-H. Tan.
1995.
Adaptive resonance associativemap.
Neural Networks, 8(3):437--446.A.-H. Tan.
1997.
Cascade ARTMAP: Integrat-ing neural computation and symbolic knowl-edge processing.
IEEE Transactions on NeuralNetworks, 8(2):237-235.Y.
Yang and Pedersen J.P. 1997.
A comparativestudy on feature selection in text categoriza-tion.
In the Fourteenth International Confer-ence on Machine Learning (ICML'97), pages412-420.Y.
Yang and X. Liu.
1999.
A re-examinationof text categorization methods.
In ~nd An-nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval (SIGIR'99), pages 42-49.Y.
Yang.
1994.
Expert network: Effective and ef-ficient learning from human decisions in textcategorization and retrieval.
In 17th AnnualInternational ACM SIGIR Conference on Re-search and Development in Information Re-trieval (SIGIR '94).Y.
Yang.
1999.
An evaluation of statistical ap-proaches to text categorization.
Journal of In-formation Retrieval, 1(1/2):67-88.Lanjuan Zhu.
1987.
The theory and experimentson automatic Chinese documents classification.Journal of the China Society for Scientific andTechnical Information \[in Chinese\], 1987(6).Tao Zou, Ji-Cheng Wang, Yuan Huang, and Fu-Yan Zhang.
1998.
The design and implementa-tion of an automatic Chinese documents classi-fication system.
Journal for Chinese Informa-tion \[in Chinese\], 1998(2).Tao Zou, Yuan Huang, and Fuyan Zhang.
1999.Technology of information mining on WWW.Journal of the China Society for Scientific andTechnical Information \[in Chinese\], 1999(4).100
