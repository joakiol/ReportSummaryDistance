Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 60?67,Sydney, July 2006. c?2006 Association for Computational LinguisticsTracing Actions Helps in Understanding InteractionsBernd LudwigChair for Artificial Intelligence, University of Erlangen-Nu?rnbergAm Weichselgarten 9, D-91058 ErlangenBernd.Ludwig@informatik.uni-erlangen.deAbstractIntegration of new utterances into contextis a central task in any model for ratio-nal (human-machine) dialogues in naturallanguage.
In this paper, a pragmatics-firstapproach to specifying the meaning of ut-terances in terms of plans is presented.
Arational dialogue is driven by the reactionof dialogue participants on how they findtheir expectations on changes in the en-vironment satisfied by their observationsof the outcome of performed actions.
Wepresent a computational model for thisview on dialogues and illustrate it with ex-amples from a real-world application.1 A Pragmatics-First View on DialoguesRational dialogues that are based on GRICE?smaxims of conversation serve for jointly execut-ing a task in the domain of discourse (called theapplication domain) by following a plan that couldsolve the task assigned to the participants of the di-alogue.
Therefore, the interpretation of new con-tributions and their integration into a dialogue iscontrolled by global factors (e.g.
the assumptionthat all dialogue participants behave in a coopera-tive manner and work effectively towards the com-pletion of a joint task) as well as by local factors(e.g.
how does the new contribution serve in com-pleting the current shared plan?
).Ony if these factors are represented in an effec-tive and efficient formal language, dialogue sys-tems can be implemented.
Examples of such mod-els and their implementation are the information-state-update approach (an implemented system isdescribed in (Larsson, 2002)), or ?
more linguisti-cally oriented ?
approaches like the adjacency-pairmodels or intentional models such as GROSZ andSIDNER?s (see (Grosz and Sidner, 1986)).Even if it has been noted often that discoursestructure and task structure are not isomorphic,only a few contributions to dialogue research fo-cus on the question of how both structures inter-fere (see Sect.
2).
In this paper, we emphasizethat it is important to distinguish between the dia-logue situation and the application situation: Theformer is modified whenever speech acts are per-formed, whereas the latter changes according tothe effects of each action being executed.
In thissection, we will use a MAPTASK dialogue to showwhat the notions dialogue situation and applica-tion situation intend to mean.
After presenting re-lated work in Sect.
2, we present our approach firstinformally and then formally by explaining whichAI algorithms we apply in order to turn the infor-mal model into a computationally tractable one.1.1 Talking about Domain SituationsThe main hypothesis of this paper is that modi-fications of the dialogue situation are triggered bychanges of the application situation.
As a responseto a speech act, dialogue participants perform a se-ries of actions aiming at achieving some goal.
Ifthese actions can be executed, the reaction can sig-nal success.
At this point, our understanding of therole of shared plans exceeds that of (Grosz et al,1999): GROSZ and KRAUS define an action to beresolved if it is assumed that an agent is able toexecute the action.
However, in order to under-stand coherence relations in complex dialogues, itis important to know whether an action has actu-ally been executed and what effect it has produced.Consider the following excerpt from a MAPTASKdialogue (MAP 9, quoted from (Carletta, 1992)):R: ++ and ++ you are not quite horizontal you are takinga slight curve up towards um the swamp ++ not obviouslygoing into itG: well sorry I have not got a swampR: you have not got a swamp?G: noR: OKG: start again from the palm beach60G has failed to find the swamp, which means Ghas failed to perform the action necessary to per-form the next one (take a slight curve).In order to solve the current task, R has beenable to organize a solution for the task at handwhich may or may not involve the other dialogueparticipant G. How can R put his solution into ac-tion?
First, he executes each step and, second, val-idates after each step whether all expectations re-lated to it are fulfilled.1.2 Talking about Error and FailureIn the example above, R?s expectations are not metbecause G does not find the swamp on the map.However, this would be a precondition for R tocontinue putting the solution into action that hehas organized.
On the other hand, G understandsthat finding the swamp is very important in the cur-rent task, but he missed to reach that goal.
In orderto share this information with R, G verbalizes hisfailure diagnosis: ?I have not got a swamp.
?This turn makes R realize that his solution doesnot work.
Obviously, R believed his solution to bewell elaborated because he tries to get a confirma-tion of its failure by asking back ?you have not gota swamp??
G?s reacknowledgement is a clear in-dication for R that it is necessary to reorganize hissolution for the current task.
Being a collaborativedialogue participant, he will try to recover fromthat failure to explain the way to the destination.1.3 Domain and Discourse StrategiesFor the purpose of recovery, the dialogue partici-pants try to apply a repair strategy that helps themto reorganize the solution.
Repair strategies arecomplex domain dependent processes of modify-ing tasks and solutions to them.
Even being do-main dependent in detail, there are some strate-gies that are domain independent and are regularlyadapted to particular domains:?
Delay: Maybe it is the best decision to wait abit and try the failed step again.?
Delegation: Maybe someone else can per-form better.?
Replanning: Another solution should befound based on the current error diagnosis.?
Relaxation: Modify some parameters orconstraints of the task so that a tractable so-lution can be found.?
New Tools: Maybe somehow the dialogueparticipant can extend his capabilities in thedomain so that he can achieve the solution us-ing other, more, or stronger tools and means.?
Negotiation: Try to retrieve new helpful in-formation from the user or to come to anagreement of how the task can be modified.?
Cancellation: Sometimes giving up to find asolution is the only remaining possibility.This list is necessarily incomplete as dependingon the particular domain and current situation inwhich a dialogue participant has to act these strate-gies appear in very different fashion.
So, it is hardto decide whether exception handling for a singlecase is taking place or if a particular strategy isbeing applied.
In the example dialogue, G tries tosuggest a replanning by telling to R up to whatpoint he was able to understand R?s explanations.According to his communication strategy, a di-alogue participant tells his deliberations in moreor less detail, sometimes even not at all.
This isthe case in the example dialogue above.
In the lastturn, G does not tell that he wants R to reorganizehis solution.
R must infer this from the content, inparticular from the request to restart the explana-tion at a point that has been passed before the Ghad failed to understand a step in R?s explanation.This example shows that domain strategies andcommunication strategies interfere in a dialogueand that complicated reasoning is necessary toidentify them in order to react appropriately.Our analysis shows that the notion of coher-ence is strongly related with the execution of sin-gle steps in a solution.
Often, coherence cannotbe explained satisfactorily within a discourse, butthe current situation in which an utterance is made,must be taken into consideration as well.2 Related WorkThere are several main research directions on dia-logue understanding.
The one closest to our ap-proach is activity-based dialogue analysis (All-wood, 1997; Allwood, 2000) contrasting BDI-style approaches such as the one by (Cohenand Levesque, 1995).
This research shows howspeech acts are related to expectations expressedby means of language and inspired our approach.However, ALLWOOD does not work out in detailhow the pragmatics of the application domain canbe formalized in a tractable way.
(Carletta, 1992)61shows in a corpus analysis that risk taking is a el-ementary behavior of dialogue participants.
(Bosand Oka, 2002) uses first-order logic in a DRT en-vironment to reason about the logical satisfiabil-ity of a new utterance given a previous discourse.For reasoning about action however, we think thata first-order theorem prover or model builder isnot the ideal tool because it is too general.
Ad-ditionally, in dialogues about acting in an envi-ronment, the primary interest of semantic eval-uation is not whether a formula is true or false,but how a goal or task can be solved.
Therefore,planning is more appropriate than proofing formu-lae.
Work on planning as part of dialogue under-standing is reported in (Zinn, 2004).
This paperdoes not address selecting strategies for error re-covery.
Conflict resolution is addressed in (Chu-Carroll and Carberry, 1996).
However, the pre-sented discourse model is not computationally ef-fective.
(Huber and Ludwig, 2002; Ludwig, 2004)present an interactive system which uses planning,(Yates et al, 2003) and recently (Lieberman andEspinosa, 2006) reported on applying planning asa vehicle for natural language interfaces, but noneof the papers discusses how a dialogue can becontinued when a failure in the application oc-curs.
In the WITAS system (see (Lemon et al,2002)), activities are modelled by activity mod-els, one for each type of activity the system canperform or analyse.
A similiar recipe-based ap-proach is implemented in COLLAGEN (Garlandet al, 2003).
As activities are hard-coded in therespective model, adaptation of the task and dia-logue structure to the needs in a current situationare harder to achieve than in our approach in whichonly goals are specified and activities are selectedby a planner depending on the current state.
In ad-dition, executing plans by verifying preconditionsand effects of an activity that has been carried outrecently lies the basis for a framework of under-standing the pragmatics of a dialogue that is notimplemented for a particular application, but triesto be as generic as possible.3 Problem and Discourse OrganizationA computational approach that aims at analyzingand generating rational ?
i.e.
goal-oriented ?
dia-logues in a given domain must address the issuesof organizing a solution in the application domainas well as in the discourse domain.
Furthermore, itmust provide an effective method to organize so-1 2 3Figure 1: Example data for a classification task.lutions, classify current states in the discourse aswell as in the application situation (are they erro-neous or not?)
and select strategies that promise arecovery in case of an error.3.1 Expectations and ObservationsTo diagnose an error, a dialogue participant mustbe able to determine whether his expectations onhow the environment changes due to an actionmatch his observations.3.2 The Origin of ExpectationsThe expectations of a dialogue participant are de-rived from his organization of a solution to the cur-rent task.
Each step herein has ?
after it has beenexecuted ?
a certain intended impact.
It forms theexpectations that are assigned to a single step.An expectation is met by a set of observations ifthe observations are sufficient to infer the expecta-tion from.
The inference process that is employedin this context may be as simple as a slot-fillingmechanism or as complicated as inference in a for-mal logic.
In the slot-filling case, the inference al-gorithm is to determine whether the semantic typeof the answer given by the user match the type thatwas expected by the dialogue system.However, inference in the sense of this papermay involve difficult computations: Expectationsare generated while a solution is organized.
Eachstep in a solution leads to certain changes in theenvironment that are expected to happen whenthe step is actually executed.
Later in the paper,we will demonstrate how planning algorithms cangenerate such expectations.
Additionally: ?
seeFig.
1) ?
in order to verify expectations of the re-quest ?Fill coffee into the cup!?
image data needto be classified before it can be concluded that theexpectation (image 3) is satisfied.4 Planning SolutionsIn order to illustrate our approach how a naturallanguage dialogue system can organize solutionsfor user requests, we discuss a natural language in-terface for operating a transportation system.
The62produce-coffee:parameters (?c - cup ?j - jura):precondition(and (under-spout ?c)(not (service-request ?j))):effect (and (not (empty ?c)) (ready ?j))Figure 2: Example of a plan operator in PDDLsystem allows to control a model train installationand electronic devices currently on the market.4.1 Organizing a SolutionFirst of all, in order to specify the (pragmatic)capabilities of the whole system, a formal modelof the system is needed that allows the necessarycomputations for organizing solutions.
For thispurpose, we model all functions provided by thesystem in terms of plan operators in the PDDLplanning language.
Fig.
2 shows an example.This operator describes part of the functionalityof the automatic coffee machine that is integratedinto our system: the function produce-coffeecan be executed if there is a cup under the spoutof the machine and if it does not require service(as such filling in water or beans).
These are thepreconditions of the function.
After coffee hasbeen produced, it is expected that the environmentis changed in the following way: the cup is notempty any longer, and the machine is ready again.In order to organize a solution, a task is neededand knowledge about the current state of the envi-ronment.
The latter comes from interpreting sen-sor data, while the former is computed from nat-ural language user input.
For the example request?Fill in a cup of espresso!
?, we assume the currentstate in Fig.
3 to hold and use the formula in Fig.4 as the description of the current task to solve.The example in Fig.
3 assumes that the cup isparked and empty, and the coffee machine and therobot (used for moving cups) are ready.
The taskis formalized as a future state of the environmentin which the cup is parked and the coffee machineis in the mode one small cup (see Fig.
4).To compute a solution, a planning algorithm(we incorporated the FF planner (Hoffmann andNebel, 2001) in our system) uses the information(and (parked cup) (empty cup)(ready jura) (ready robo))Figure 3: The current state of the environment forthe example in Sect.
4about the current state and the intended future stateas input and computes a plan for a number of stepsto execute in order to solve the task (see Fig.
5).In the following, we will consider such a plan asin Fig.
5 as an organized solution for the task to besolved.
Expected changes of the environment aredefined by the effects of each step of the solution.Fig.
6 shows which changes are expected if theplan in Fig.
5 is eventually executed.4.2 Executing a SolutionGiven a plan for a task to be solved, our dialoguesystem executes each step sequentially.
Before astep of the solution is performed, the system ver-ifies each precondition necessary for the step tobe executable.
If all tests succeed, actuators arecommanded to perform everything related to thecurrent step.
Feedback is obtained by interpret-ing sensor input which is used to control whetherthe intended effects have been achieved.
For thefunction produce-coffee above, the follow-ing procedure is executed:produce-coffee (cup c, jura j) {if test(under-spout,c)=falsesignal_error;else {if test(service-request,j)=truesignal_error;else do produce-coffee, c, j;};if test(empty,c)=true signal_error;else {if test(ready,j)=false signal_error;else return;};}In this procedure, each precondition of the func-tion produce-coffee is verified.
If the systemcan infer from the sensor values that a precondi-tion cannot be satisfied, it signals an error.
Thesame is done with all effects when the actuatorshave finished to change the environment.
As wewill discuss in Sect.
6, these error signals are thebasic information for continuing a dialogue whenunexpected changes have been observed.5 Diagnosing ErrorsHow can the dialogue system react if a precondi-tion or effect does not match the system?s expecta-tions?
The primary goal of a dialogue system is to(and (parked cup) (mode-osc jura))Figure 4: The task to be solved63put-cup-on-spout(cup,jura,robo)draw-off-osc(cup,jura)produce-coffee(cup,jura)go-in-place(train)take-cup-off-spout(cup,jura,robo)load-cup-on-waggon(cup,jura,robo,train)park-cup(cup,jura,robo,train)Figure 5: A plan for the task in Fig.
4Step # Action and expected changes1 put-cup-on-spout(cup, jura, robo)(under-spout ?c)(not (robo-loaded ?r ?c))(not (parked ?c))2 draw-off-osc(cup, jura)(not (ready ?j)) (mode-osc ?j)3 produce-coffee(cup, jura)(not (empty ?c)) (ready ?j)4 go-in-place(train)(in-place ?t)5 take-cup-off-spout(cup, jura, robo)(not (under-spout ?c))(robo-loaded ?r ?c)6 load-cup-on-waggon(cup, jura, robo, train)(not (robo-loaded ?r ?c))(train-loaded ?t ?c)7 park-cup(cup, jura, robo, train)(not (train-loaded ?t ?c))(parked ?c)Figure 6: Expected changes in the environmentmeet principles of conversation such as GRICE?smaxims.
Often, however, it is not obvious to theuser how a particular constraint in a plan is re-lated to the current task.
Therefore, a plausible andtransparent explanation of an error brings the diag-nosed mismatch in its context of the current actionand solution for the current task.
At the core ofeach explanation are the unexpected observations.The context of the error is formed by all availablesensor values and the history of past actions whichare steps in the solution (see Fig.
5) for the currenttask.
The example in Fig.
7 shows the state of thesystem after the first four steps of the solution in-troduced in Sect.
4.1 have been carried out.
Af-ter executing take-cup-off-spout however,the observed changes of the system state do notmatch the expected ones: Analyzing the most re-cent images and the robot?s weight sensor, the sys-tem recognizes that the cup still is on the spout, butnot being carried away by the robot.Given the situation in Fig.
7, what is the systemable to or should communicate to the user?
With-out any additional reasoning, the system couldonly tell what it has observed:User: Fill in a cup of espresso!System: The cup is under the spout andthe robot is not loaded with the cup.Such a response would not be very helpful forthe user (especially if he cannot see the situ-ation or is unexperienced in handling the sys-tem).
Our approach to generating plausible expla-nations for failures of the system is to computea diagnosis that takes the context of the failureand the task to be solved into account.
The ap-proach relies on an explicit error model for eachfunction that the system provides in the senseof (Balduccini and Gelfond, 2003): In the caseof take-cup-off-spout the following symp-toms and causes lead to failure of the function:Symptom Cause(under-spout c) wrong positionrobo lost cup?
(robo-loaded r c) arm motor offarm not okdidn?t hold cupThe approach of (Balduccini and Gelfond, 2003)generates models in a formal logic of what couldbe reasons for the failure of the system and inwhich state the system would be in each case.
Ifmore than one model is generated when an unex-pected observation has been made, the system hasto find evidence for each model and compare howreliable a diagnosis is (given the available sensorvalues and their interpretation).
For that purpose,decision trees are trained on past cases of failureand the confidence values for the computed inter-pretations.
(e.g.
to which class does the image inFig.
1?).
Applying such a decision tree on a givensystem state and mismatched expectation resultsin a unique model that is seen as an explanationfor the failure of the system?s function.
In this way,the dialogue system can explain better to the userwhat it thinks has happened during the executionof take-cup-off-spout:User: Fill in a cup of espresso!System: I could not carry the cup to thetrain because I lost the cup on the way.From the viewpoint of discourse analysis, we canconsider the mismatch between observations andexpectations as the main cause for the system re-sponse.
As a consequence, we can consider thelogical relation between the current task, the so-lution organized by the system, and the computederror diagnosis as sufficient conditions for any dis-course relation to hold between the user and the64Actions up to now System state Expected state Error Diagnosisafter go-in-placeput-cup-on-spout (ready jura) nonedraw-off-osc (ready robo) noneproduce-coffee ?
(parked cup) nonego-in-place ?
(empty cup) none?
(robo-loaded robo) none(mode-osc jura) none(in-place train) none(under-spout cup) noneLast Action Observed statetake-cup-off-spout (under-spout cup) ?
(under-spout cup) robo could not?
(robo-loaded robo) (robo-loaded robo) hold the cupFigure 7: Context information for the diagnosis of an errorsystem utterance in the dialogue excerpt above: Interms of TRAUM?s DU acts (Traum, 1994), coher-ence between both utterances is established as areject relation as the purpose of the utteranceis to indicate failure of the task that has been initi-ated by the user request.
To explain the MAPTASKdialogue cited in the introduction, another level ofpragmatic reasoning is required: As already men-tioned in Sect.
1.3, the dialogue system is cooper-ative and tries to find out a way in order to never-theless solve the task as completely as possible.6 Error Repair and Discourse UpdateSuch a way out consists in applying a strategythat is appropriate for the current state of the sys-tem and the interaction with the user.
In the AI(Mitchell, 1997) and robotics (Bekey, 2005) liter-ature, algorithms for applying adaptive strategiesin different situations are all based on the currentstate as input and an evaluation function that helpsselecting an optimal strategy.6.1 Repair Strategies in the ApplicationA favorite algorithm for this kind of interactivecontrol problems is to select the optimal policy outof a set of possibilities.
Before that, an evaluationfunction is trained by reinforcement learning to al-ways select the action that maximizes the rewardobtainable in the current state.
In (Henderson etal., 2005), this machine learning approach was ap-plied to selecting speech acts after training an eval-uation function on a dialogue corpus in which eachutterance was labeled with a speech act.Different from (Henderson et al, 2005), in ourapproach the actions between whom the dialoguesystem can choose are repair strategies instead ofspeech acts.
In our opinion, speech acts are a phe-nomenon of another invisible process ?
text gen-eration ?
but not objects of the decision at thediscourse planning level: the selection of a repairstrategy does not fix the type of a speech act norits content.
The way a repair strategy works and ?as a consequence ?
has influence on the flow of adialogue is that, firstly, it modifies the current taskand, secondly, seeks a new solution that will be ex-ecuted later on.
Future speech acts then are a resultof performing single steps of the new solution.To recover the take-cup-off-spout func-tion, the system may have the option to fill an-other cup and try to bring this one to its destina-tion.
It must be noted, however, that this optiondepends to a large extent on the availability of an-other empty cup, the readiness of the robot and thecoffee machine and sufficient resources like beans,water, and time to complete the task.
All these pa-rameters influence the computation of the rewardand the risk to be assigned to this domain-specificvariant of a New Tools-strategy (see Sect.
1.3).6.2 Effects on Discourse UpdateThe MAPTASK dialogue in Sect.
1.1 even is some-what more complicated: G understands that hedoes not have the capability to repair the misun-derstanding as there is too much information miss-ing.
Therefore, he initiates a Negotiation-strategyin which he switches the topic of the dialogue tothe domain of strategies for MAPTASK.
G pro-poses a new strategy with a slightly modified taskto R. It is exactly this logical relation that explainsthe coherence between the turns in this dialogue.In this case, the coherence cannot be establishedby reasoning in one single domain.In terms of the Conversation Acts Theory by(Traum and Hinkelman, 1992) and (Poesio andTraum, 1998; Traum, 1994), the discourse seg-ment related to the solution for a task can be called65multiple discourse unit (MDU).
Consequently, theconversation acts for MDU are a trace of the di-alogue participant?s decisions on which interac-tions are needed to solve the task and how theycould be verbalized best.
Argumentation is basedon the formal knowledge about the domain, thecurrent task, and a solution proposed for it.
Thismeans that an analysis of the current state of thesystem and the dialog provides facts that can beused as conditions for the applicability of a speechact.
Equally, facts about the system are conditionsfor the applicability of a system function at a cer-tain point of time.
It follows directly from this ob-servation that planning argumentation acts can beviewed as a special kind of classical planning inAI.
However, due to the interactive nature of sucha dialogue task, it must be possible to react flexiblyand directly on mismatches between expectationsand observations for speech acts and the intendedchanges during the course of a dialogue.Therefore, in this paper dialogue managementis seen as a special case of reactive planning.
Asshown above, discourse relations are derived frommeta-information about the state of executing aplan for the current task.
The discourse relationsserve as preconditions for speech acts effectuatingthe update of the dialogue state.6.3 Diagnosing Linguistic ErrorsOur model of relating pragmatics and interactioncan be extended to discourse pragmatics as well.It is particularly helpful to understand groundingacts in the utterance unit level (see (Traumand Hinkelman, 1992)).
In this case, the (?appli-cation?)
domain is that of understanding language.The task to be solved is to extract words from aspeech signal and to construct meaning from thosewords.
Error diagnoses occur frequently and op-tions caused by ambiguities of natural languagehave to be tested whether they can help to repaira diagnosed error automatically.
If not, the di-agnoses as symptoms of misunderstanding haveto be assigned to possible causes.
Strategic de-cisions have to be made how to communicate thecauses and possible suggestion for repairs to theuser.
This reasoning results in grounding acts thatwould be hard to analyze otherwise.
This idea canbe applied to negotiating speech acts as well.
Thedifficult task, however, is to implement a diagnosisalgorithm for failure in syntax analysis, (composi-tional) semantics, and speech act analysis.7 Understanding User UtterancesThere are implications of our approach for com-putational semantics: In order to see whether auser utterance meets the system?s expectations, itis necessary to analyze which domain the utter-ance refers to.
For this purpose, expectations fordiscourse and system state are maintained sepa-rately.
Each new contribution must satisfy the dis-course expectations (e.g.
an answer should followa question) and pragmatic expectations (the con-tent of the contribution must extend without con-tradictions what is known about the current solu-tion.
To test this, a model (in the sense of formallogic) is computed for the conjunction of the newcontent and the currently available information.As discussed above, it may happen in dialoguesthat the focus is switched to another topic, i.e.another domain, and the coherence can be estab-lished only when taking this domain shift into ac-count.
In order to be able to detect such a do-main shift, we define the meaning of performativewords depending on whether they refer to the hid-den reasoning processes that are part of our ap-proach, the discourse control domain, or to states,objects, and functions in the current applicationssituation: In the MAPTASK example, the utteranceStart from the palm beach refers to the process ofstrategy selection and organization of a solution,but not to the domain of explanations in a map.8 ConclusionsThe presented approach allows dialogue under-standing to take into account that the (human) di-alogue participant the system is interacting with is(at least) equally able to diagnose errors and mis-matches between observations and expectationsand generates utterances intended to update the di-alogue state according to these findings.
There-fore, for establishing the coherence of a user utter-ance, there are always several options: firstly, theuser continues the current solution, secondly, hediagnoses failure and reports about it, and thirdly,he switches the focus to another domain includingdiscourse update and repair strategies.For these options, our approach devises a com-putational model able to explain dialogues inwhich coherence of turns is difficult analyze.
Inthis way, more natural dialogues can be analyzedand generated.
As the approach incorporates amodel for how talking about actions is related toacting in a formalized domain, it serves as a basis66for constructing natural language assistance sys-tems, e.g.
for a great range of electronic devices.ReferencesJens Allwood.
1997.
Notes on dialog and coopera-tion.
In Kristina Jokinen, David Sadek, and DavidTraum, editors, Proceedings of the IJCAI 97 Work-shop on Collaboration, Cooperation, and Conflict inDialogue Systems, Nagoya, August.Jens Allwood.
2000.
An activity based approach topragmatics.
In Harry C. Bunt and B.
Black, editors,Abduction, Belief, and Context in Dialogue, Studiesin Computational Pragmatics.
John Benjamins, Am-sterdam.Marcello Balduccini and Michael Gelfond.
2003.
Di-agnostic reasoning with a-prolog.
Theory and Prac-tice of Logic Programming, 3(4?5):425?461, July.George A. Bekey.
2005.
Autonomous Robots: FromBiological Inspiration to Implementation and Con-trol.
MIT Press.Johan Bos and Tetsushi Oka.
2002.
An inference-based approach to dialogue system design.
In Pro-ceedings of the 19th International Conference onComputational Linguistics, pages 113?119.Jean Carletta.
1992.
Risk-Taking and Recovery inTask-Oriented Dialogue.
Ph.D. thesis, University ofEdinburgh.Jennifer Chu-Carroll and Sandra Carberry.
1996.
Con-flict detection and resolution in collaborative plan-ning.
In Intelligent Agents: Agent Theories, Archi-tectures, and Languages, volume 2 of Lecture Notesin Artificial Intelligence, pages 111?126.
SpringerVerlag.Phil R. Cohen and Hector J. Levesque.
1995.
Commu-nicative actions for artificial agents.
In Proceedingsof the First International Conference on Multi-AgentSystems, pages 65?72, San Francisco, CA, June.Andrew Garland, Neal Lesh, and Charles Rich.
2003.Responding to and recovering from mistakes duringcollaboration.
In Gheorghe Tecuci, David W. Aha,Mihai Boicu, Michael T. Cox, George Ferguson, andAustin Tate, editors, Proceedings of the IJCAI Work-shop on Mixed-Initiative Intelligent Systems, pages59?64, August.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Barbara J. Grosz, Luke Hunsberger, and Sarit Kraus.1999.
Planning and acting together.
AI Magazine,20(4):23?34.James Henderson, Oliver Lemon, and KallirroiGeorgila.
2005.
Hybrid reinforcement/supervisedlearning for dialogue policies from communicatordata.
In Proc.
IJCAI workshop on Knowledge andReasoning in Practical Dialogue Systems, Edin-burgh (UK).Jo?rg Hoffmann and Bernhard Nebel.
2001.
Theff planning system: Fast plan generation throughheuristic search.
Journal of Artificial IntelligenceResearch, 14:253?302.Alexander Huber and Bernd Ludwig.
2002.
A naturallanguage multi-agent system for controlling modeltrains.
In Proceedings AI, Simulation, and Planningin High Autonomy Systems (AIS 2002), pages 145?149, Lissabon.Staffan Larsson.
2002.
Issue-based Dialogue Man-agement.
Ph.D. thesis, Department of Linguistics,Go?teborg University, Go?teborg, Sweden.Oliver Lemon, Alexander Gruenstein, Alexis Battle,and Stanley Peters.
2002.
Multi-tasking and collab-orative activities in dialogue systems.
In Proceed-ings of the 3rd SIGdial Workshop on Discourse andDialogue, pages 113?124, Philadelphia.Henry Lieberman and Jose?
Espinosa.
2006.
Agoal-oriented interface to consumer electronics us-ing planning and commonsene reasoning.
In Pro-ceedings of the 11th International Conference on In-telligent User Interfaces.
ACM, ACM Press.Bernd Ludwig.
2004.
A pragmatics-first approach tothe analysis and generation of dialogues.
In SusanneBiundo, Rhom Fru?hwirth, and Gu?nther Palm, edi-tors, Proc.
KI-2004 (27th Annual German Confer-ence on AI (KI-2004), pages 82?96, Berlin.
Springer.Tom M. Mitchell.
1997.
Machine Learning.
McGraw-Hill.Massimo Poesio and David Traum.
1998.
Towardsan axiomatisation of dialogue acts.
In J. Hulstijnand A. Nijholt, editors, Proceedings of the TwenteWorkshop on the Formal Semantics and Pragmaticsof Dialogues, pages 207?222, Enschede.David R. Traum and Elizabeth A. Hinkelman.
1992.Conversation acts in task-oriented spoken dialogue.Computational Intelligence, 8(3):575?592.David Traum.
1994.
A Computational Theoryof Grounding in Natural Language Conversation.Ph.D.
thesis, Computer Science Department, Uni-versity of Rochester.Alexander Yates, Oren Etzioni, and Daniel Weld.2003.
A reliable natural language interface to house-hold appliances.
In Proceedings of the 8th Inter-national Conference on Intelligent User Interfaces,pages 189?196.
ACM, ACM Press.Claus Zinn.
2004.
Flexible dialogue managementin natural-language enhanced tutoring.
In Kon-vens 2004 Workshop on Advanced Topics in Model-ing Natural Language Dialog, pages 28?35, Viena,September.67
