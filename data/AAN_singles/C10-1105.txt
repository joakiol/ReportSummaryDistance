Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 931?939,Beijing, August 2010Inducing Fine-Grained Semantic Classes viaHierarchical and Collective ClassificationAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at Dallas{altaf,vince}@hlt.utdallas.eduAbstractResearch in named entity recognition andmention detection has typically involved afairly small number of semantic classes,which may not be adequate if seman-tic class information is intended to sup-port natural language applications.
Moti-vated by this observation, we examine theunder-studied problem of semantic sub-type induction, where the goal is to au-tomatically determine which of a set of92 fine-grained semantic classes a nounphrase belongs to.
We seek to improve thestandard supervised approach to this prob-lem using two techniques: hierarchicalclassification and collective classification.Experimental results demonstrate the ef-fectiveness of these techniques, whetheror not they are applied in isolation or incombination with the standard approach.1 IntroductionSemantic class determination refers to the taskof classifying a noun phrase (NP), be it a nameor a nominal, as one of a set of pre-defined se-mantic classes.
A semantic class classifier is abasic text-processing component in many high-level natural language processing (NLP) applica-tions, including information-extraction (IE) sys-tems and question-answering (QA) systems.
Inrecent years, supervised semantic class determi-nation has been tackled primarily in the context of(1) coreference resolution (e.g., Ng (2007), Huanget al (2009)), where semantic classes are inducedand subsequently used to disallow coreference be-tween semantically incompatible NPs, and (2) themention detection task in the ACE evaluations(e.g., Florian et al (2004; 2006)), where the goalis to identify the boundary of a mention (i.e., anoun phrase that belongs to one of the pre-definedACE semantic classes), its mention type (e.g., pro-noun, name), and its semantic class.
The outputof a mention detector is then used by downstreamIE components, which typically include a coref-erence resolution system and a relation extractionsystem.
Owing in part to its potentially large in-fluence on downstream IE components, accuratesemantic class determination is crucial.Over the years, NLP researchers have focusedon a relatively small number of semantic classes inboth NE recognition and mention detection: sevenclasses in the MUC-6 and MUC-7 NE recognitiontask, four classes in the CoNLL 2002 and 2003NE recognition shared task, and seven classes inthe ACE 2005 mention detection task.
Given thatone of the uses of semantic class information isto support NLP applications, it is questionablewhether this purpose can be adequately served bysuch a small number of semantic classes.
For ex-ample, given the question ?Which city was thefirst Olympic Games held in?
?, it would be help-ful for a QA system to know which NEs are cities.However, virtually all of the existing NE recog-nizers and mention detectors can only determinewhether an NE is a location or not.Our goal in this paper is to tackle the under-studied problem of determining fine-grained se-mantic classes (henceforth semantic subtypes).More specifically, we aim to classify an NP asone of the 92 fine-grained, domain-independentsemantic classes that are determined to be use-ful for supporting the development of QA and931IE systems in the ACE and AQUAINT programs.These 92 semantic subtypes have been used tomanually annotate the NPs in the BBN Entity TypeCorpus (Weischedel and Brunstein, 2005).
Giventhe availability of this semantic subtype-annotatedcorpus, we adopt a supervised machine learn-ing approach to semantic subtype determination.Specifically, given (the boundary of) an NP, wetrain a classification model to determine which ofthe 92 semantic subtypes it belongs to.More importantly, we seek to improve the stan-dard approach to semantic subtype induction de-scribed above by proposing two techniques.
Thefirst technique, collective classification, aims toaddress a common weakness in the standard su-pervised learning paradigm, where a classifierclassifies each instance independently of the oth-ers and is unable to exploit any relational informa-tion between a pair (or a subset) of the instancesthat may be helpful for classification.
The sec-ond technique, hierarchical classification, exploitsthe observation that these 92 semantic subtypescan be grouped into a smaller number of coarse-grained semantic types (henceforth semantic su-pertypes).
With this two-level hierarchy, learningcan proceed in a sequential fashion: given an NP,we first determine its semantic supertype and thenclassify it as one of the semantic subtypes thatfall under the predicted supertype in the hierar-chy.
Empirical results show that these two tech-niques, when applied in isolation to the standardlearning approach to subtype induction, can sig-nificantly improve its accuracy, and the best resultis achieved when they are applied in combination.The rest of the paper is organized as follows.Section 2 provides an overview of the 92 seman-tic subtypes and the evaluation corpus.
In Sec-tion 3, we present our baseline semantic subtypeclassification system.
Sections 4 and 5 introducecollective classification and hierarchical classifi-cation respectively, and describe how these twotechniques can be used to improve the baselinesemantic subtype classifier.
We show evaluationresults in Section 6 and conclude in Section 7.2 Semantic SubtypesAs noted before, each name and nominal in theBBN Entity Type Corpus is annotated with one ofthe 92 semantic subtypes.
In our experiments, weuse all the 200 Penn Treebank Wall Street Journalarticles in the corpus, yielding 17,292 NPs that areannotated with their semantic subtypes.Table 1 presents an overview of these subtypes.Since they have been manually grouped into 29supertypes, we also show the supertypes in the ta-ble.
More specifically, the first column shows thesupertypes, the second column contains a brief de-scription of a supertype, and the last column liststhe subtypes that correspond to the supertype inthe first column.
In cases where a supertype con-tains only one subtype (e.g., PERSON), the super-type is not further partitioned into different sub-types; for classification purposes, we simply treatthe subtype as identical to its supertype (and hencethe two always have the same name).
A detaileddescription of these supertypes and subtypes canbe found in Weischedel and Brunstein (2005).
Fi-nally, we show the class distribution: the paren-thesized number after each subtype is the percent-age of the 17,292 NPs annotated with the subtype.3 Baseline Classification ModelWe adopt a supervised machine learning approachto train our baseline classifier for determining thesemantic subtype of an NP.
This section describesthe details of the training process.Training corpus.
As mentioned before, we usethe Wall Street Journal articles in the BBN EntityType Corpus for training the classifier.Training instance creation.
We create onetraining instance for each annotated NP, NPi,which is either a name or a nominal, in each train-ing text.
The classification of an instance is its an-notated semantic subtype value, which is one ofthe 92 semantic subtypes.
Each instance is repre-sented by a set of 33 features1, as described below.1.
Mention String (3): Three features are de-rived from the string of NPi.
Specifically, we em-ploy the NP string as a feature.
If NPi containsmore than one token, we create one feature foreach of its constituent tokens.
Finally, to distin-guish the different senses of a nominal, we create1As we will see, since we employ an exponential model,an instance may be represented by fewer than 33 features.932Supertype Brief Description SubtypesPERSON Proper names of people.
Person (9.2).PERSON DESC Any head word of a common noun Person Desc (16.8).referring to a person or group of people.NORP This type is named after its subtypes: Nationality (2.9), Religion (0.1), Political (0.6),nationality, religion, political, etc.
Other (0.1).FACILITY Names of man-made structures, including Building (0.1), Bridge (0.02), Airport (0.01),infrastructure, buildings, monuments, Attraction (0.01), Highway Street (0.05),camps, farms, mines, ports, etc.
Other (0.1).FACILITY DESC Head noun of a noun phrase describing Building (0.5), Bridge (0.05), Airport (0.01),buildings, bridges, airports, etc.
Highway Street (0.2), Attraction (0.02), Other (0.5).ORGANIZATION Names of companies, government Government (3.6), Corporation (8.3), Political (0.5),agencies, educational institutions and Educational (0.3), Hotel (0.04), City (0.01),other institutions.
Hospital (0.01), Religious (0.1), Other (0.7).ORG DESC Heads of descriptors of companies, Government (2.1), Corporation (4.3), Political (0.2),educational institutions and other Educational (0.1), Religious (0.1), Hotel (0.1),governments, government agencies, etc.
City (0.01), Hospital (0.02), Other (0.7).GPE Names of countries, cities, states, Country (4.2), City (3.2), State Province (1.4),provinces, municipalities, boroughs.
Other (0.1).GPE DESC Heads of descriptors of countries, cities, Country (0.8), City (0.3), State Province (0.3),states, provinces, municipalities.
Other (0.1).LOCATION Names of locations other than GPEs.
River (0.03), Lake Sea Ocean (0.05), Region (0.2),E.g., mountain ranges, coasts, borders, Continent (0.1), Other (0.2).planets, geo-coordinates, bodies of water.PRODUCT Name of any product.
It does not Food (0.01), Weapon (0.02), Vehicle (0.2),include the manufacturer).
Other (0.2).PRODUCT DESC Descriptions of weapons and vehicles Food (0.01), Weapon (0.2), Vehicle (0.97),only.
Cars, buses, machine guns, missiles, Other (0.02).bombs, bullets, etc.DATE Classify a reference to a date or period.
Date (7.99), Duration (1.9), Age (0.5), Other (0.4).TIME Any time ending with A.M. or P.M. Time (0.5).PERCENT Percent symbol or the actual word percent.
Percent (2.07).MONEY Any monetary value.
Money (2.9).QUANTITY Used to classify measurements.
E.g., 4 1D (0.11), 2D (0.08), 3D (0.1), Energy (0.01),miles, 4 grams, 4 degrees, 4 pounds, etc.
Speed (0.01), Weight (0.1), Other (0.04).ORDINAL All ordinal numbers.
E.g., First, fourth.
Ordinal (0.6).CARDINAL Numerals that provide a count or quantity.
Cardinal (5.1).EVENT Named hurricanes, battles, wars, sports War (0.03), Hurricane (0.1), Other (0.24).events, and other named events.PLANT Any plant, flower, tree, etc.
Plant (0.2).ANIMAL Any animal class or proper name of an Animal (0.7).animal, real or fictional.SUBSTANCE Any chemicals, elements, drugs, and Food (1.1), Drug (0.46), Chemical (0.23), Other (0.9).foods.
E.g., boron, penicillin, plutonium.DISEASE Any disease or medical condition.
Disease (0.6).LAW Any document that has been made into Law (0.5).a law.
E.g., Bill of Rights, Equal Rights.LANGUAGE Any named language.
Language (0.2).CONTACT INFO Address, phone.
Address (0.01), Phone (0.04).GAME Any named game.
Game (0.1).WORK OF ART Titles of books, songs and other creations.
Book (0.16), Play (0.04), Song (0.03), Painting (0.01),Other (0.4).Table 1: The 92 semantic subtypes and their corresponding supertypes.a feature whose value is the concatenation of thehead of NPi and its WordNet sense number.22We employ the sense number that is manually annotatedfor each NP in the WSJ corpus as part of the OntoNotesproject (Hovy et al, 2006).2.
Verb String (3): If NPi is governed by a verb,the following three features are derived from thegoverning verb.
First, we employ the string of thegoverning verb as a feature.
Second, we createa feature whose value is the semantic role of the933governing verb.3 Finally, to distinguish the differ-ent senses of the governing verb, we create a fea-ture whose value is the concatenation of the verband its WordNet sense number.3.
Semantic (5): We employ five semantic fea-tures.
First, if NPi is an NE, we create a featurewhose value is the NE label of NPi, as determinedby the Stanford CRF-based NE recognizer (Finkelet al, 2005).
However, if NPi is a nominal, we cre-ate a feature that encodes the WordNet semanticclass of which it is a hyponym, using the manu-ally determined sense of NPi.4 Moreover, to im-prove generalization, we employ a feature whosevalue is the WordNet synset number of the headnoun of a nominal.
If NPi has a governing verb,we also create a feature whose value is the Word-Net synset number of the verb.
Finally, if NPi is anominal, we create a feature based on its WordNetequivalent concept.
Specifically, for each entitytype defined in ACE 20055, we create a list con-taining all the word-sense pairs in WordNet (i.e.,synsets) whose glosses are compatible with thatentity type.6 Then, given NPi and its sense, we usethese lists to determine if it belongs to any ACE2005 entity type.
If so, we create a feature whosevalue is the corresponding entity type.4.
Morphological (8).
If NPi is a nominal, wecreate eight features: prefixes and suffixes oflength one, two, three, and four.5.
Capitalization (4): We create four cap-italization features to determine whether NPiIsAllCap, IsInitCap, IsCapPeriod, andIsAllLower (see Bikel et al (1999)).6.
Gazetteers (8): We compute eight gazetteer-based features, each of which checks whether NPiis in a particular gazetteer.
The eight dictionariescontain pronouns (77 entries), common words andwords that are not names (399.6k), person names(83.6k), person titles and honorifics (761), vehi-3We also employ the semantic role that is manually anno-tated for each NP in the WSJ corpus in OntoNotes.4The semantic classes we considered are person, location,organization, date, time, money, percent, and object.5The ACE 2005 entity types include person, organization,GPE, facility, location, weapon, and vehicle.6Details of how these lists are constructed can be foundin Nicolae and Nicolae (2006).cle words (226), location names (1.8k), companynames (77.6k), and nouns extracted from Word-Net that are hyponyms of PERSON (6.3k).7.
Grammatical (2): We create a feature thatencodes the part-of-speech (POS) sequence of NPiobtained via the Stanford POS tagger (Toutanovaet al, 2003).
In addition, we have a feature thatdetermines whether NPi is a nominal or not.We employ maximum entropy (MaxEnt) mod-eling7 for training the baseline semantic subtypeclassifier.
MaxEnt is chosen because it providesa probabilistic classification for each instance,which we will need to perform collective classi-fication, as described in the next section.4 Collective ClassificationOne weakness of the baseline classification modelis that it classifies each instance independently.
Inparticular, the model cannot take into account re-lationships between them that may be helpful forimproving classification accuracy.
For example,if two NPs are the same string in a given doc-ument, then it is more likely than not that theyhave the same semantic subtype according to the?one sense per discourse?
hypothesis (Gale et al,1992).
Incorporating this kind of relational infor-mation into the feature set employed by the base-line system is not an easy task, since each featurecharacterizes only a single NP.To make use of the relational information, onepossibility is to design a new learning procedure.Here, we adopt a different approach: we performcollective classification, or joint probabilistic in-ference, on the output of the baseline model.
Theidea is to treat the output for each NP, which isa probability distribution over the semantic sub-types, as its prior label/class distribution, and con-vert it into a posterior label/class distribution byexploiting the available relational information asan additional piece of evidence.
For this purpose,we will make use of factor graphs.
In this section,we first give a brief overview of factor graphs8,and show how they can be used to perform joint7We use the MaxEnt implementation available athttp://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html8See Bunescu and Mooney (2004) and Loeliger (2004)for a detailed introduction to factor graphs.934inference for semantic subtype determination.4.1 Factor GraphsFactor graphs model optimization problems ofan objective function g, which is a real-valuedfunction of n random variables X1, ..., Xn.
Weassume that g can be decomposed into a productof m factors.
In other words, g (X1, ..., Xn) =f1 (s1 (X1, ..., Xn)) ...fm (sm (X1, ..., Xn)),where each factor fk is a real-valued functionof some subset of X1, ... , Xn, denoted assk (X1, ..., Xn).
Each fk can be thought of as afeature function that computes the compatibilityof an assignment of values to the variables insk (X1, ..., Xn) with respect to a user-definedfeature.
Hence, a larger function value is moredesirable, as it corresponds to a more compatibleassignment of values to the variables involved.A factor graph consists of two types of nodes:variable nodes and factor nodes.
Each randomvariable Xi is represented by a variable node, andeach factor fk is represented by a factor node.Each factor node fk is connected only to the nodescorresponding to sk.
This results in a bipartitegraph, where edges exist only between a variablenode and a factor node.Given this graph, there are several methods forfinding an optimal assignment of the random vari-ables X1, ..., Xn such that the objective functiong is maximized.
Exact inference using the sum-product algorithm (Kschischang et al, 2001) ispossible if there are no cycles in the graph; other-wise a belief propagation algorithm, such as loopybelief propagation (Murphy et al, 1999), can beapplied.
Although there are no cycles in our factorgraphs, we choose to use loopy belief propagationas our inferencer, since it performs approximateinference and is therefore computationally moreefficient than an exact inferencer.4.2 Application to Subtype InferenceTo apply joint inference to semantic subtype in-duction, we create one factor graph for each testdocument, where each variable node is randomvariable Xi over the set of semantic subtype la-bels L and represents an NP, NPi, in the docu-ment.
To retain the prior probabilities over thesemantic subtype labels lq ?
L obtained from thebaseline classification model, each variable nodeis given a factor f (Xi) = P (Xi = lq).
If noadditional factors that model the relation betweentwo nodes/instances are introduced, maximizingthe objective function for this graph (by maximiz-ing the product of factors) will find an assignmentidentical to the one obtained by taking the mostprobable semantic subtype label assigned to eachinstance by the baseline classifier.Next, we exploit the relationship between tworandom variables.
Specifically, we want to en-courage the inference algorithm to assign thesame label to two variables if there exists a rela-tion between the corresponding NPs that can pro-vide strong evidence that they should receive thesame label.
To do so, we create a pairwise fac-tor node that connects two variable nodes if theaforementioned relation between the underlyingNPs is satisfied.
However, to implement this idea,we need to address two questions.First, which relation between two NPs can pro-vide strong evidence that they have the same se-mantic subtype?
We exploit the coreference re-lation.
Intuitively, the coreference relation is areasonable choice, as coreferent entities are likelyto have the same semantic subtype.
Here, wenaively posit two NPs as coreferent if at least oneof the following conditions is satisfied: (1) theyare the same string after determiners are removed;(2) they are aliases (i.e., one is an acronym orabbreviation of the other); and (3) they are bothproper names and have at least one word in com-mon (e.g., ?Delta?
and ?Delta Airlines?
).9Second, how can we define a pairwise factor,fpair, so that it encourages the inference algo-rithm to assign the same label to two nodes?
Onepossibility is to employ the following definition:fpair(Xi, Xj)= P (Xi = lp, Xj = lq),where lp, lq ?
L={1 if lp = lq0 otherwiseIn essence, fpair prohibits the assignment of dif-ferent labels to the two nodes it connects.
In our9The third condition can potentially introduce many falsepositives, positing ?Bill Clinton?
and ?Hillary Clinton?
ascoreferent, for instance.
However, this kind of false positivesdoes not pose any problem for us, since the two NPs involvedbelong to the same semantic subtype (i.e., PERSON).935experiments, however, we ?improve?
fpair by in-corporating semantic supertype information intoits definition, as shown below:fpair(Xi, Xj)= P (Xi = lp, Xj = lq),where lp, lq ?
L={Psup(sup(lp)|NPi)Psup(sup(lq)|NPj) if lp = lq0 otherwiseIn this definition, sup(lq) is the supertype of lqaccording to the semantic type hierarchy shownin Section 2, and Psup(sup(lq)|NPj) is the proba-bility that NPj belongs to sup(lq) according to thesemantic supertype classification model Psup (seeSection 5 for details on how this model can betrained).
In essence, we estimate the joint proba-bility by (1) assuming that the two events are inde-pendent, and then (2) computing each event usingsupertype information.
Intuitively, this definitionallows fpair to favor those label assignments thatare more compatible with the predictions of Psup.After graph construction, we apply an infer-encer to compute a marginal probability distribu-tion over the labels for each node/instance in thegraph by maximizing the objective function g, andoutput the most probable label for each instanceaccording to its marginal distribution.5 Hierarchical ClassificationThe pairwise factor fpair defined above exploitssupertype information in a soft manner, meaningthat the most probable label assigned to an NP byan inferencer is not necessarily consistent with itspredicted supertype (e.g., an NP may receive Ho-tel as its subtype even if its supertype is PERSON).In this section, we discuss how to use supertypeinformation for semantic subtype classification ina hard manner so that the predicted subtype isconsistent with its supertype.To exploit supertype information, we first traina model, Psup, for determining the semantic su-pertype of an NP using MaxEnt.
This model istrained in essentially the same way as the base-line model described in Section 3.
In particular,it is trained on the same set of instances using thesame feature set as the baseline model.
The onlydifference is that the class value of each traininginstance is the semantic supertype of the associ-ated NP rather than its semantic subtype.Next, we train 29 supertype-specific classifi-cation models for determining the semantic sub-type of an NP.
For instance, the ORGANIZATION-specific classification model will be used to clas-sify an NP as belonging to one of its subtypes(e.g., Government, Corporation, Political agen-cies).
A supertype-specific classification model istrained much like the baseline model.
Each in-stance is represented using the same set of fea-tures as in the baseline, and its class label is itssemantic subtype.
The only difference is that themodel is only trained only on the subset of theinstances for which it is intended.
For instance,the ORGANIZATION-specific classification modelis trained only on instances whose class is a sub-type of ORGANIZATION.After training, we can apply the supertype clas-sification model and the supertype-specific sub-type classification model to determine the se-mantic subtype of an NP in a hierarchical fash-ion.
Specifically, we first employ the supertypemodel to determine its semantic supertype.
Then,depending on this predicted semantic supertype,we use the corresponding subtype classificationmodel to determine its subtype.6 EvaluationFor evaluation, we partition the 200 Wall StreetJournal Articles in the BBN Entity Type corpusinto a training set and a test set following a 80/20ratio.
As mentioned before, each text in the EntityType corpus has its NPs annotated with their se-mantic subtypes.
Test instances are created fromthese texts in the same way as the training in-stances described in Section 3.
To investigatewhether we can benefit from hierarchical and col-lective classifications, we apply these two tech-niques to the Baseline classification model in iso-lation and in combination, resulting in the foursets of results in Tables 2 and 3.The Baseline results are shown in the secondcolumn of Table 2.
Due to space limitations, it isnot possible to show the result for each semanticsubtype.
Rather, we present semantic supertyperesults, which are obtained by micro-averagingthe corresponding semantic subtype results andare expressed in terms of recall (R), precision (P),and F-measure (F).
Note that only those semantic936Baseline only Baseline+HierarchicalSemantic Supertype R P F R P F1 PERSON 91.9 89.7 90.8 88.8 91.1 89.92 PERSON DESC 91.3 87.8 89.5 92.1 89.8 91.03 SUBSTANCE 60.0 66.7 63.2 70.0 58.3 63.64 NORP 87.8 90.3 89.0 91.9 90.7 91.35 FACILITY DESC 72.7 88.9 80.0 68.2 93.8 79.06 ORGANIZATION 76.6 73.8 75.2 78.5 73.2 75.87 ORG DESC 75.0 70.7 72.8 75.8 75.2 75.58 GPE 75.6 73.9 74.7 77.0 75.4 76.29 GPE DESC 60.0 75.0 66.7 70.0 70.0 70.010 PRODUCT DESC 53.3 88.9 66.7 53.3 88.9 66.711 DATE 85.0 85.0 85.0 84.5 85.4 85.012 PERCENT 100.0 100.0 100.0 100.0 100.0 100.013 MONEY 83.9 86.7 85.3 88.7 96.5 92.414 QUANTITY 22.2 100.0 36.4 66.7 66.7 66.715 ORDINAL 100.0 100.0 100.0 100.0 100.0 100.016 CARDINAL 96.0 77.4 85.7 94.0 81.0 87.0Accuracy 81.56 82.60Table 2: Results for Baseline only and Baseline with hierarchical classification.Baseline+Collective Baseline+BothSemantic Supertype R P F R P F1 PERSON 93.8 98.1 95.9 91.9 100.0 95.82 PERSON DESC 93.9 88.5 91.1 92.6 89.5 91.03 SUBSTANCE 60.0 85.7 70.6 70.0 63.6 66.74 NORP 89.2 93.0 91.0 90.5 94.4 92.45 FACILITY DESC 63.6 87.5 73.7 68.2 93.8 79.06 ORGANIZATION 85.8 76.2 80.7 87.4 76.3 81.37 ORG DESC 75.8 74.1 74.9 75.8 74.6 75.28 GPE 74.1 75.8 74.9 81.5 81.5 81.59 GPE DESC 60.0 60.0 60.0 70.0 77.8 73.710 PRODUCT DESC 53.3 88.9 66.7 53.3 88.9 66.711 DATE 85.0 85.4 85.2 85.0 86.3 85.612 PERCENT 100.0 100.0 100.0 100.0 100.0 100.013 MONEY 83.9 86.7 85.3 90.3 96.6 93.314 QUANTITY 22.2 100.0 36.4 66.7 66.7 66.715 ORDINAL 100.0 100.0 100.0 100.0 100.0 100.016 CARDINAL 96.0 78.7 86.5 94.0 83.9 88.7Accuracy 83.70 85.08Table 3: Results for Baseline with collective classification and Baseline with both techniques.supertypes with non-zero scores are shown.
As wecan see, only 16 of the 29 supertypes have non-zero scores.10 Among the ?traditional?
seman-tic types, the Baseline yields good performancefor PERSON, but only mediocre performance forORGANIZATION and GPE.
While additional ex-periments are needed to determine the reason, wespeculate that this can be attributed to the fact thatPERSON and PERSON DESC have only one seman-tic subtype (which is the supertype itself), whereas10The 13 supertypes that have zero scores are all under-represented classes, each of which accounts for less than onepercent of the instances in the dataset.ORGANIZATION and GPE have nine and four sub-types, respectively.
The classification accuracy isshown in the last row of the table.
As we can see,the Baseline achieves an accuracy of 81.56.Results obtained when hierarchical classifica-tion is applied to the Baseline are shown in thethird column of Table 2.
In comparison to theBaseline, accuracy rises from 81.56 to 82.60.
Thisrepresents an error reduction of 5.6%, and the dif-ference between these two accuracies is statisti-cally significant at the p = 0.04 level.1111All significance test results in this paper are obtained us-ing Approximate Randomization (Noreen, 1989).937Results obtained when collective classificationalone is applied to the Baseline are shown inthe second column of Table 3.
In this case, theprior probability distribution over the semanticsubtypes that is needed to create the factor asso-ciated with each node in the factor graph is sim-ply the probabilistic classification of the test in-stance that the node corresponds to.
In compar-ison to the Baseline, accuracy rises from 81.56to 83.70.
This represents an error reduction of11.6%, and the difference is significant at thep = 0.01 level.
Also, applying collective clas-sification to the Baseline yields slightly better re-sults than applying hierarchical classification tothe Baseline, and the difference in their results issignificant at the p = 0.002 level.Finally, results obtained when both hierarchi-cal and collective classification are applied to theBaseline are shown in the third column of Table3.
In this case, the prior distribution needed tocreate the factor associated with each node in thefactor graph is provided by the supertype-specificclassification model that is used to classify the testinstance in hierarchical classification.
In compar-ison to the Baseline, accuracy rises from 81.56to 85.08.
This represents an error reduction of19.1%, and the difference is highly significant(p < 0.001).
Also, applying both techniques tothe Baseline yields slightly better results than ap-plying only collective classification to the Base-line, and the difference in their results is signifi-cant at the p = 0.003 level.6.1 Feature AnalysisNext, we analyze the effects of the seven featuretypes described in Section 3 on classification ac-curacy.
To measure feature performance, we takethe best-performing system (i.e., Baseline com-bined with both techniques), begin with all sevenfeature types, and iteratively remove them one byone so that we get the best accuracy.
The re-sults are shown in Table 4.
Across the top line,we list the numbers representing the seven featureclasses.
The feature class that corresponds to eachnumber can be found in Section 3, where they areintroduced.
For instance, ?2?
refers to the fea-tures computed based on the governing verb.
Thefirst row of results shows the system performance1 3 7 4 2 5 681.4 75.8 83.3 83.7 84.1 85.2 85.680.4 74.9 84.3 85.3 85.3 86.180.4 78.3 83.9 86.5 86.781.8 76.2 85.2 87.675.4 83.4 84.666.2 80.9Table 4: Results of feature analysis.after removing just one feature class.
In thiscase, removing the sixth feature class (Gazetteers)improves accuracy to 85.6, while removing themention string features reduces accuracy to 81.4.The second row repeats this, after removing thegazetteer features.Somewhat surprisingly, using only mentionstring, semantic, and grammatical features yieldsthe best accuracy (87.6).
This indicates thatgazetteers, morphological features, capitalization,and features computed based on the governingverb are not useful.
Removing the grammati-cal features yields a 3% drop in accuracy.
Afterthat, accuracy drops by 4% when semantic fea-tures are removed, whereas a 18% drop in accu-racy is observed when the mention string featuresare removed.
Hence, our analysis suggests thatthe mention string features are the most useful fea-tures for semantic subtype prediction.7 ConclusionsWe examined the under-studied problem of se-mantic subtype induction, which involves clas-sifying an NP as one of 92 semantic classes,and showed that two techniques ?
hierarchi-cal classification and collective classification ?can significantly improve a baseline classificationmodel trained using an off-the-shelf learning al-gorithm on the BBN Entity Type Corpus.
In par-ticular, collective classification addresses a ma-jor weakness of the standard feature-based learn-ing paradigm, where a classification model classi-fies each instance independently, failing to capturethe relationships among subsets of instances thatmight improve classification accuracy.
However,collective classification has not been extensivelyapplied in the NLP community, and we hope thatour work can increase the awareness of this pow-erful technique among NLP researchers.938AcknowledgmentsWe thank the three anonymous reviewers for theirinvaluable comments on an earlier draft of the pa-per.
This work was supported in part by NSFGrant IIS-0812261.ReferencesBikel, Daniel M., Richard Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Machine Learning: Special Issue onNatural Language Learning, 34(1?3):211?231.Bunescu, Razvan and Raymond J. Mooney.
2004.Collective information extraction with relationalmarkov networks.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics, pages 483?445.Finkel, Jenny Rose, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 363?370.Florian, Radu, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
A sta-tistical model for multilingual entity detection andtracking.
In HLT-NAACL 2004: Main Proceedings,pages 1?8.Florian, Radu, Hongyan Jing, Nanda Kambhatla, andImed Zitouni.
2006.
Factorizing complex mod-els: A case study in mention detection.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 473?480.Gale, William, Ken Church, and David Yarowsky.1992.
One sense per discourse.
In Proceedingsof the 4th DARPA Speech and Natural LanguageWorkshop, pages 233?237.Hovy, Eduard, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.Ontonotes: The 90% solution.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers, pages57?60.Huang, Zhiheng, Guangping Zeng, Weiqun Xu, andAsli Celikyilmaz.
2009.
Accurate semantic classclassifier for coreference resolution.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing, pages 1232?1240.Kschischang, Frank, Brendan J. Frey, and Hans-Andrea Loeliger.
2001.
Factor graphs and the sum-product algorithm.
IEEE Transactions on Informa-tion Theory, 47:498?519.Loeliger, Hans-Andrea.
2004.
An introduction tofactor graphs.
IEEE Signal Processing Magazine,21(1):28?41.Murphy, Kevin P., Yair Weiss, and Michael I. Jordan.1999.
Loopy belief propagation for approximate in-ference: An empirical study.
In Proceedings of theFifteenth Conference on Uncertainty in Artificial In-telligence, pages 467?475.Ng, Vincent.
2007.
Semantic class induction andcoreference resolution.
In Proceedings of the 45thAnnual Meeting of the Association of Computa-tional Linguistics, pages 536?543.Nicolae, Cristina and Gabriel Nicolae.
2006.
Best-Cut: A graph algorithm for coreference resolution.In Proceedings of the 2006 Conference on Empiri-cal Methods in Natural Language Processing, pages275?283.Noreen, Eric W. 1989.
Computer Intensive Methodsfor Testing Hypothesis: An Introduction.
John Wi-ley & Sons.Toutanova, Kristina, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency net-work.
In HLT-NAACL 2003: Proceedings of theMain Conference, pages 173?180.Weischedel, Ralph and Ada Brunstein.
2005.
BBNpronoun coreference and entity type corpus.
In Lin-guistic Data Consortium, Philadelphia.939
