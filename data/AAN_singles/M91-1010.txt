ITP INTERPRETEXT SYSTEM:MUC-3 TEST RESULTS AND ANALYSI SKathleen Dahlgre nCarol LordHajime WadaJoyce McDowel lEdward P .
Stabler, Jr.Intelligent Text Processing, Inc .1310 Montana Avenue, Suite 20 1Santa Monica, CA 90403213-576-4910Internet: 72550,1670@compuserve .comIntelligent Text Processing is a small start-up company participating in the MUC-3 exercise fo rthe first time this year .
Our system, Interpretext, is based on a prototype text understandin gsystem.
With three full-time and three part-time people, dividing time between MUC-3 and othe rcontract projects, ITP made maximum use of modest resources .SLOT POS ACT COR PAR INC ICR IPA SPU MIS NO?
REC PRE OVG FALMatched Only 794 479 243 91 76 30 76 78 393400 35 58 1 6Matched/Missing 1372 479 234 91 76 30 76 78 971793 20 58 1 6All Templates 1372 604 234 91 76 30 76 203 9711031 20 46 34Set Fills Only 575 191 99 31 25 8 31 36 420492 .
20 60 19 0Figure 1 .
Intelligent Text Processing Final Scores Test 2ITP's results are shown in Figure 1 .
The ITP system was second highest in precision (46%) whenall templates were considered, and at the same time achieved a credible recall percentage (20%) .Our overgeneration rate was second best (34%) .
ITP was a very close second in both precisio nand overgeneration, as the top percentages were 48 and 33 to ITP's 46 and 34 .
The major limitin gfactor in ITP's MUC-3 performance was parser failure.
We are building a parser with widecoverage and a comprehensive approach to disambiguation .
Because our parser is not yetcomplete, in order to participate in the MUC-3 exercise we used a parser on loan .It proved to lack the robustness necessary to parse the MUC-3 messages, failing on 50% o fthe sentences .
For those sentences which it did parse, the Interpretext system returned precisesemantic interpretations .
ITP's word-based approach required minimal reorientation in shifting tothe new domain of terrorism texts; the main new material was the straightforward addition of arelatively small number of new words to the syntactic and naive semantic lexicons, not whole ne wsemantic modules .
The semantic structures and analyses already implemented proved to beappropriate for texts in the new domain .The source of the precision in our performance was the Cognitive Model built by the Natura lLanguage Understanding Module .
The Cognitive Model contains specific reference marker sidentifying events and individuals in the text .
The same events and individuals are given the sam e7 9reference markers by the Anaphora Resolution Module .
The Cognitive Model distinguishe sbetween events, individuals and sets .
It directly displays the argument structure of events .
Thus ,to find a terrorist incident, the template-filling code looked for an event which implied harm ,damage or some other consequence of terrorism in the Naive Semantics for the verb naming th eevent.
The agent of the event had to be described as having a role in clandestine activity, th egovernment or the military .
The ITP naive semantic lexicon distinguishes between nouns whichnames objects and nouns which name events, so that the template-filling code had only to look forevents, even those introduced by phrases such as the destruction of homes in .
.
.Furthermore, the Cognitive Model connects head nouns with prepositional phrase modifier sand adjectival or nominal modifiers via the same reference marker .
Thus the template-filling codecould look for a variety of modifiers of an individual as a source of information about theindividual .
For example, the phrase member of the guerrilla troop connects member with troopand guerrilla, so that the template-filling code could recognize a semantically empty term likemember as referring to an agent.
This type of connection works everywhere, not just with theparticular string pattern member of the guerrilla troop .
Furthermore, it is much more precise than apattern-matching method which would find guerrilla as perpetrator everywhere it occurs, evenwhen a phrase like "member of the guerrilla troop" is the object of a verb which implies harm, andis therefore not indicative of guerrilla terrorism .Another source of precision is that the formal semantic module interprets the cardinality o fsets .
"None", "plural" or "three" come out in the formal representation as the number of objects i na set.
Finding target number and amount of injury and damage is trivial given a precise treatmen tof cardinality in the formal semantics.Finally, the Cognitive Model indicated discourse segments .
These are portions of the tex twhich function as a unit around one topic .
The recognition of segments simplified the anaphor aresolution and the process of identifying the same individuals and events with each other .
Itprevented the overgeneration of templates .
Some competitor systems generated a new template foreach sentence containing a terrorism word and then they had to try to merge them .
Withoutsegment information, merging was very difficult .A Cognitive Model with this level of precision can be built only when a deep natural languag eanalysis of the text is performed .
Syntactic, formal semantic, discourse semantic and pragmatic (o rnaive semantic) complexities of text are addressed by the ITP Natural Language Understandin gModule.
Some researchers have rejected a principled linguistic approach as hopeless at this stag ein the history of computational linguistic research.
They assume that the only feasible methods ar estatistical .
Such systems match to certain string patterns and rely upon the statistical probabilitythat they co-occur with a particular semantic interpretation .
The problem is that many times th epattern occurs in phrase which is irrelevant, or has the opposite meaning to the predicted one .
Thepattern can occur in the scope of a negative or modal, as in the bomb did not explode, and producea false alarm for a pattern-matching method .
Such methods will tend to over-generate templates ,because patterns indicate a terrorist incident where there is none .
For the same false alarm texts ,more precise linguistic analysis can correctly rule out a terrorist incident .Furthermore, the patterns for matching must be coded anew for each domain .
In contrast ,ITP Naive Semantic and syntactic lexicons need only be built once, and they work across al ldomains.
For MUC-3 we added to an existing naive semantic lexicon prepared originally for text sin other domains .In summary, ITP was precise in the MUC-3 fills for the sentences which our loaner parserwas able to process .
When our own parser is available, ITP's technology will vastly improve i nrecall.80Naive SemanticsThe basic approach to template-filling involved looking at feature types in the naive semanti cknowledge for verbs and nouns .
The feature types inspected had already been present in th etheory and in the system prior to MUC-3 .
The verb feature "consequence of event" was importan tfor recognizing terrorist incidents, because if the typical consequence of an event was damage orharm, it triggered a template fill .
The theory of Naive Semantics as described in Dahlgren[1 ]identifies that feature type as important in lexical semantics and reasoning about discourse .Similarly, the "rolein" feature was used to distinguish between clandestine agents, governmen tagents and military agents .
Again, that feature type was antecedently present in our theory .Test SettingsThe effect of the MUC-3 reader was to exclude any sentences which did not contain a terro rword, saving processing time .
This setting tended to reduce precision, because a sentence like Shesucceeded contains no terrorism word, but could be very significant in the recognition of a terroris tincident .
Recall was implicitly set very low by the fact that the parser was able to parse only 50 %of the input.Level of EffortThe greatest effort by ITP was the six years of research that went into the Natural Languag eUnderstanding Module .
As for MUC-3-specific tasks, Table I indicates the level of effort on eac hone.
ITP made a detailed linguistic analysis of the terrorism domain, and the way that terroris tincidents were described in the first messages sent out by NOSC, and in the DEV messages .
Theanalysis guided the expansion of the lexicons and the writing of the template-filling code .
DuringTest 1 we identified both parser failure and parse time to be problems in our performance .Therefore, for Test 2 we built a reader which could handle dates, abbreviations, and so on, an dwould return a sentence only if it contained a terrorism word.
In addition, we pruned the output toshorten sentences for the parser .
These tactics will not be necessary once our own wide-coverag eparser is completed.
The template-filling code took about as much of our time as the reader andpruner.
Each element of the code reasons from the Cognitive Model using generalized lexica lreasoning or DRS reasoning .
The temporal-locative reasoning is general and will be used in otherapplications .Tasks Estimated Person-weeksLinguistic analysis of terrorism domain 4Syntactic Lexicon expansion 2Naive Semantic Lexicon expansion 3Reader, pruner 4Temporal, locative reasoning 2Template-filling code 4Table 1.
MUC-3 specific Tasks and their Estimated Person-WeeksLimiting FactorThe main limiting factors were the parser and resources .
With more persons and time, wecould have written code for all of the fills and debugged the template-filling code thoroughly .Given the modest resources we had, we were forced to run the test before we had thoroughl ydebugged the code .
In particular, our code for recognizing and building up proper names was i n8 1place, but failed during the test in most cases .
That explained our performance on Perpetrato rOrganization .
Given that we missed the latter, we of course could not get Category of Inciden tcorrect for any of the State-sponsored Violence cases either .TrainingTraining took place on the first 100 DEV messages, and on Test 1 messages with the ne wkey.
We did not have sufficient resources to fully debug and repeatedly test prior to MUC-3 week.The system improved dramatically between Test 1 and Test 2 (from recall of 3 to recall of 20) .Improvement was mainly due to expansion of the template-filling code and the introduction o fpruning to get more parses .Success and FailureFor those sentences which we were able to parse, the reasoning performed well for inciden trecognition, segmentation (separating different incidents in the same message), perpetrator an dtarget recognition.
The only exceptions were perpetrators or targets with long proper names .
Wehave an approach to these, but didn't get it working in time.
The fills which failed were perpetratororganization (because of names), and target nationality .
The latter code is working fine (it looks t osee whether any descriptor of an individual is a foreign nation name or adjective) .
The failureswere due to missing the whole template because of parsing, or missing the target in a recognize dtemplate .
In addition, our target number code was not fully operational at the time of the test .
Wewould most like to rewrite the template-filling code in even more general reasoning algorithm swhich could be used in applications beyond the terrorism domain .
Our system's capabilities mak epossible a question-answering system which could reply to English queries like Who did it?
andHow many people were killed?
.ReusabilityEverything but the template-filling code is reusable in a different application .
All of thewords we added to the lexicons have all of their senses common in American English .
They canbe used in any domain.
As for the template-filling code, we plan to extract generalizable reasonin galgorithms for use in other domains .
Again, the code is reusable because it is a principled, generallinguistic approach rather than a pattern-matching approach .What we learnedWe learned that anything a person wants to say or write can be said in an extremely larg enumber of different ways .
Therefore, a robust deep natural language understanding system mus thave a wide-coverage parser and formal semantics which directly display the similarity of conten tacross many possible forms of expression .
A sound theoretical approach such as DRT i sparticularly appropriate for a data extraction task.
Secondly, we learned that natural languagesystems require ample testing against real-world texts .
And, third, a system in which wordmeanings are central, developed to interpret text in the domains of geography and finance, canfunction in the domain of terrorism with the addition of a relatively small number of lexical items .82References,[1] Dahlgren, K. (1988) .
Naive Semantics for Natural Language Understanding .
KluwerAcademic Publishers, Norwell, Mass .
[2] Dahlgren,K.
(1989) .
"Coherence Relation Assignment," in Proceedings of the Cognitiv eScience Society, pp.588-596 .
[3] Kamp, H. (1981) .
"A Theory of Truth and Semantic Representation, " in Gronendijk, J .
; T.Janssen; and M. Stokhof, editors, Formal Methods in the Study of Language,Mathematisch Centrum, Amsterdam .83
