Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 50?59,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsRichness of the Base and Probabilistic Unsupervised Learning inOptimality TheoryGaja JaroszDepartment of Cognitive ScienceJohns Hopkins UniversityBaltimore, MD 21218jarosz@cogsci.jhu.eduAbstractThis paper proposes an unsupervisedlearning algorithm for Optimality Theo-retic grammars, which learns a completeconstraint ranking and a lexicon givenonly unstructured surface forms and mor-phological relations.
The learning algo-rithm, which is based on the Expectation-Maximization algorithm, graduallymaximizes the likelihood of the observedforms by adjusting the parameters of aprobabilistic constraint grammar and aprobabilistic lexicon.
The paper presentsthe algorithm?s results on three con-structed language systems with differenttypes of hidden structure: voicing neu-tralization, stress, and abstract vowels.
Inall cases the algorithm learns the correctconstraint ranking and lexicon.
The paperargues that the algorithm?s ability to iden-tify correct, restrictive grammars is due inpart to its explicit reliance on the Opti-mality Theoretic notion of Richness of theBase.1 IntroductionIn Optimality Theory or OT (Prince and Smolen-sky, 1993) grammars are defined by a set of rankeduniversal and violable constraints.
The function ofthe grammar is to map underlying or lexical formsto valid surface forms.
The task of the learner is tofind the correct grammar, or correct ranking ofconstraints, as well as the set of underlying formsthat correspond to overt surface forms given onlythe surface forms and the set of universal con-straints.The most well known algorithms for learningOT grammars (Tesar, 1995; Tesar and Smolensky,1995; Boersma, 1997, 1998; Prince and Tesar,1999; Boersma and Hayes, 2001) are supervisedlearners and focus on the task of learning theconstraint ranking, given training pairs that mapunderlying forms to surface forms.
Recent workhas focused on the task of unsupervised learning ofOT grammars, where only unstructured surfaceforms are provided to the learner.
Some of thiswork focuses on grammar learning without trainingdata (Tesar, 1998; Tesar, 1999; Hayes, 2004;Apoussidou and Boersma, 2004).
The remainder ofthis work tackles the problem of learning theranking and lexicon simultaneously, the problemaddressed in the present paper (Tesar et al, 2003;Tesar, 2004; Tesar and Prince, to appear; Merchantand Tesar, to appear).
These proposals adopt analgebraic approach wherein learning the lexiconinvolves iteratively eliminating potentialunderlying forms by determining that they havebecome logically impossible, given certainassumptions about the learning problem.1 Inparticular, one simplifying assumption of previouswork requires that mappings be one-to-one andonto.
This assumption prohibits input-outputmappings with deletion and insertion as well as1An alternative algorithm is proposed in Escudero (2005), butit has not been tested computationally.50constraints that evaluate such mappings.
This workrepresents a leap forward toward the accuratemodeling of human language acquisition, but theidentification of a general-purpose, unsupervisedlearner of OT remains an open problem.In contrast to previous work, this paper proposesa gradual, probabilistic algorithm for unsupervisedOT learning based on the Expectation Maximiza-tion algorithm (Dempster et al, 1977).
Because thealgorithm depends on gradually maximizing anobjective function, rather than on wholly eliminat-ing logically impossible hypotheses, it is not cru-cial to prohibit insertion or deletion.A major challenge posed by unsupervised learn-ing of OT is that of learning restrictive grammarsthat generate only grammatical forms.
In previouswork, the preference for restrictive grammars isimplemented by encoding a bias into the rankingalgorithm that favors ranking constraints that pro-hibit marked structures as high as possible.
In con-trast, the solution proposed here involves acombination of likelihood maximization and ex-plicit reliance on Richness of the Base, an OT prin-ciple requiring that the set of potential underlyingforms be universal.
This combination favors re-strictive grammars because grammars that map a?rich?
lexicon onto observed forms with highprobability are preferred.
The proposed model istested on three constructed language systems, eachexemplifying a different type of hidden structure.2 Learning Probabilistic OTWhile the primary task of the grammar is to mapunderlying forms to overt forms, the grammar?ssecondary role is that of a filter ?
ruling out un-grammatical forms no matter what underlying formis fed to the grammar.
The role of the grammar asfilter follows from the OT principle of Richness ofthe Base, according to which the set of possibleunderlying forms is universal (Prince and Smolen-sky 1993).
In other words, the grammar must berestrictive and not over-generate.
The requirementthat grammars be restrictive complicates the learn-ing problem - it is not sufficient to find a combina-tion of underlying forms and constraint rankingthat yields the set of observed surface forms: theconstraint ranking must yield only grammaticalforms irrespective of the particular lexical itemsselected for the language.In classic OT, constraint ranking is categoricaland non-probabilistic.
In recent years various sto-chastic versions of OT have been proposed to ac-count for free variation (Boersma and Hayes,2001), lexically conditioned variation (Anttila,1997), child language acquisition (Legendre et al,2002) and the modeling of frequencies associatedwith these phenomena.
In addition to these advan-tages, probabilistic versions of OT are advanta-geous from the point of view of learnability.
Inparticular, the Gradual Learning Algorithm forStochastic OT (Boersma, 1997, 1998; Boersma andHayes, 2001) is capable of learning in spite ofnoisy training data and is capable of learning vari-able grammars in a supervised fashion.
In addi-tion, probabilistic versions of OT and variants ofOT (Goldwater and Johnson, 2003; Rosenbach andJaeger, 2003) enable learning of OT via likelihoodmaximization, for which there exist many estab-lished algorithms.
Furthermore, as this paper pro-poses, unsupervised learning of OT usinglikelihood maximization combined with Richnessof the Base provides a natural solution to thegrammar-as-filter problem due to the power ofprobabilistic modeling to use negative evidenceimplicitly.The algorithm proposed here relies on a prob-abilistic extension of OT in which each possibleconstraint ranking is assigned a probability P(r).Thus, the OT grammar is a probability distributionover constraint rankings rather than a single con-straint ranking.
This notion of probabilistic OT issimilar to - but less restricted than - Stochastic OT,in which the distribution over possible rankings isgiven by the joint probability over independentlynormally distributed constraints with fixed, equalvariance.
The advantage of the present model iscomputational simplicity, but the proposed learn-ing algorithm does not depend on any particularinstantiation of probabilistic OT.Tables 1 and 2 illustrate the proposed probabilis-tic version of OT with an abstract example.
Table1 shows the violation marks assigned by three con-straints, A, B and C, to five candidate outputs O1-O5 for the underlying form, or input /I/.
To com-pute the winner of an optimization, constraints areapplied to the candidate set in order according totheir rank.
Candidates continue to the next con-straint if they have the fewest (or tie for fewest)constraint violation marks (indicated by asterisks).In this way the winning or optimal candidate, the51candidate that violates the higher-ranked con-straints the least, is selected.constraintsinput: /I/ A B CO1 * *O2 **  *O3  **O4  * **candidatesO5 *  **Table 1.
OT Candidates and Constraint ViolationsThe third column of Table 2 identifies the win-ner under each possible ranking of the three con-straints.
For example, if the ranking is A >> B >>C, constraint A eliminates all but O3 and O4, thenconstraint B eliminates O3, designating O4 as thewinner.
The remainder of Table 2 illustrates theproposed probabilistic instantiation of OT.
Thefirst column shows the probability P(r) that thegrammar assigns to each ranking in this example.The probability of each ranking determines theprobability with which the winner under that rank-ing will be selected for the given input.
In otherwords, it defines the conditional probability Pr(Ok |I), shown in the fourth column, of the kth outputcandidate given the input /I/ under the ranking r.The last column shows the total conditional prob-ability for each candidate after summing acrossrankings.
For instance, O3 is the winner under twoof the rankings, and thus its total conditional prob-ability P(O3 | I) is found by summing over the con-ditional probabilities under each ranking.
The totalconditional probability P(O3 | I) refers to the prob-ability that underlying form /I/ will surface as O3,and this probability depends on the grammar.P(r) ranking winner Pr(Ok | I) P(Ok | I)0.20 A>>B>>C O4 0.2 0.20.15 A>>C>>B O3 0.150.05 C>>A>>B O3 0.050.20.10 B>>A>>C O5 0.1 0.10.00 B>>C>>A O2 0.0 0.00.50 C>>B>>A O1 0.5 0.5Table 2: Probabilistic OTIn addition to the conditional probability as-signed by the grammar, this model relies on aprobability distribution P(I | M) over possible un-derlying forms for a given morpheme M.  Thisproperty of the model implements the standard lin-guistic proposition that each morpheme has a con-sistent underlying form across contexts, while thegrammar drives allomorphic variation that mayresult in the morpheme having different surfacerealizations in different contexts.
Rather than iden-tifying a single underlying form for each mor-pheme, this model represents the underlying formas a distribution over possible underlying forms,and this distribution is constant across contexts.
Todetermine the probability of an underlying form fora morphologically complex word, the product ofthe morpheme?s individual distributions is taken ?the probability of an underlying form is taken to beindependent of morphological context.
For exam-ple, suppose that some morpheme Mk has two pos-sible underlying forms, I1 and I2, and the twounderlying forms are equally likely.
This meansthat the conditional probabilities of both underly-ing forms are 50%: P(I1 | Mk) = P(I2 | Mk) = 50%.In sum, the probabilistic model described hereconsists of a grammar and lexicon, both of whichare probabilistic.
The task of learning involvesselecting the appropriate parameter settings of boththe grammar and lexicon simultaneously.3 Expectation Maximization and Richnessof the Base in OTThis section presents the details of the learningalgorithm for probabilistic OT.
First, in Section3.1 the objective function and its properties arediscussed.
Next, Section 3.2 proposes the solutionto the grammar-as-filter problem, which involvesrestricting the search space available to the learn-ing algorithm.
Finally, Section 3.3 describes thelikelihood maximization algorithm ?
the input tothe algorithm, the initial state, and the form of thesolution.3.1 The Objective FunctionThe learning algorithm relies on the following ob-jective function:PH (O | M) = [PH (Ok | Mk )]Fkk?= [ PH (Ok & Ik, j | Mk )j ]Fkk?= [ PH (Ok | Ik, j )PH (Ik, j | Mk )j ]Fkk?52The likelihood of the data, or set of overt surfaceforms, PH(O | M) depends on the parameter set-tings, the probability distributions over rankingsand underlying forms, under the hypothesis H.  Itis also conditional on M, the set of observed mor-phemes, which are annotated in the data providedto the algorithm.
M is constant, however, and doesnot differ between hypotheses for the same dataset.
Under this model each unique surface form Okis treated independently, and the likelihood of thedata is simply the product of the probability ofeach surface form, raised to the power correspond-ing to its observed frequency Fk.
Each surfaceform Ok is composed of a set of morphemes Mk,and each of these morphemes has a set of underly-ing forms Ik,j.
The probability of each surface formPH(Ok | Mk) is found by summing the joint distribu-tion PH(Ok & Ik,j | Mk) over all possible underlyingforms Ik,J for morphemes Mk that compose Ok.Finally, the joint probability is simply the productof the conditional probability PH(Ok | Ik,j) and lexi-cal probability PH(IK,j | Mk), both of which weredefined in the previous section.The primary property of this objective functionis that it is maximal only when the hypothesis gen-erates the observed data with high probability.
Inother words, the grammar must map the selectedlexicon onto observed surface forms without wast-ing probability mass on unobserved forms.
Be-cause there are two parameters in the model, thiscan be accomplished by adjusting the ranking dis-tributions or by adjusting lexicon distributions.The probability model itself does not specifywhether the grammar or the lexicon should be ad-justed in order to maximize the objective function.In other words, the objective function is indifferentto whether the restrictions observed in the lan-guage are accounted for by having a restrictivegrammar or by selecting a restrictive lexicon.
Asdiscussed in Section 2, according to Richness ofthe Base, only the first option is available in OT:the grammar must be restrictive and must neutral-ize noncontrastive distinctions in the language.The next subsection addresses the proposed solu-tion ?
a restriction of the search procedure that fa-vors maximizing probability by restricting thegrammar rather than the lexicon.3.2 Richness of the BaseAlthough the notion of a restrictive grammar isintuitively clear, it is difficult to implement for-mally.
Previous work on OT learnability (Tesar,1995; Tesar and Smolensky, 1995; Smolensky1996; Tesar, 1998, Tesar, 1999; Tesar et al, 2003;Tesar and Prince, to appear; Hayes, 2004) has pro-posed the heuristic of Markedness over Faithful-ness during learning to favor restrictive grammars.In OT there are two basic types of constraints,markedness constraints, which penalize dis-preferred surface structures, and faithfulness con-straints, which penalize nonidentical mappingsfrom underlying to surface forms.
In general, arestrictive grammar will have markedness con-straints ranked high, because these constraints willrestrict the type of surface forms that are allowedin a language.
On the other hand, if faithfulnessconstraints are ranked high, all the distinctions in-troduced into the lexicon will surface.
Thus, aheuristic preferring markedness constraints to rankhigh whenever possible does in general prefer re-strictive grammars.
However, the markedness overfaithfulness heuristic does not exhaust the notionof restrictiveness.
In particular, markedness overfaithfulness does not favor grammar restrictivenessthat follows from particular rankings betweenmarkedness constraints or between faithfulnessconstraints.This work aims to provide a general solutionthat does not require distinguishing various typesof constraints ?
the proposed solution implementsRichness of the Base explicitly in the initial stateof the lexicon.
Specifically, the solution involvesrequiring that initial distributions over the lexiconbe uniform, or rich.
Although the objective func-tion alone does not prefer restrictive grammarsover restrictive lexicons, a lexicon constrained tobe uniform, or nonrestrictive, will in turn force thegrammar to be restrictive.
Another way to thinkabout it is that a restrictive grammar is one thatcompresses the input distributions maximally bymapping as much of the lexicon onto observed sur-face forms as possible.
By requiring the lexicon tobe rich the proposed solution relies on the objec-tive function?s natural preference for grammarsthat maximally compress the lexicon.
The objec-tive function prefers restrictive grammars in thissituation because restrictive grammars will allowthe highest probability to be assigned to observed53forms.
In contrast, if the lexicon is not rich, thereis nothing for the grammar to compress, and theobjective function?s natural preference for com-pression will not be employed.
The next subsectiondiscusses the algorithm and the initialization of theparameters in more detail.3.3 Likelihood Maximization AlgorithmAs discussed above, the goal of the learning algo-rithm is to find the probability distributions overrankings and lexicons that maximize the probabil-ity assigned to the observed set of data accordingto the objective function.
In addition, any regulari-ties present in the data should be accommodated bythe grammar rather than by restricting the lexicon.As in previous work on unsupervised learning ofOT, the algorithm assumes knowledge of OT con-straints, the possible underlying forms of overtforms, and sets of candidate outputs and their con-straint violation profiles for all possible underlyingforms.
While the present version of the algorithmreceives this information as input, recent work incomputational OT (Riggle, 2004; Eisner, 2000)suggests that this information is formally derivablefrom the constraints and overt surface forms andcan be generated automatically.In addition, the algorithm receives informationabout the morphological relations between ob-served surface forms.
Specifically, output formsare segmented into morphemes, and the mor-phemes are indexed by a unique identifier.
Thisinformation, which has also been assumed in pre-vious work, cannot be derived directly from theconstraints and observed forms but is a necessarycomponent of a model that refers to underlyingforms of morphemes.
The present work assumesthis information is available to the learner althoughSection 5 will discuss the possibility of learningthese morphological relations in conjunction withthe learning of phonology.The set of potential underlying forms is derivedfrom observed surface forms, morphological rela-tions, and the constraint set.
On the one hand theset of potential underlying forms, which is initiallyuniformly distributed, should be rich enough toconstitute a rich base for the reasons discussed ear-lier.
On the other hand, the set should be re-stricted enough so that the search space is not toolarge and so that the grammar is not pressured tofavor mapping underlying forms to completelyunrelated surface forms.
For this reason, potentialunderlying forms are derived from surface formsby considering all featural variants of surfaceforms for features that are evaluated by the gram-mar.
Of these potential underlying forms, onlythose that can yield each of the observed surfaceallomorphs of the morpheme under some rankingof the constraints are included.
This formulationdiffers substantially from previous work, whichaimed to construct the lexicon via discrete steps,the first of which involved permanently setting thevalues for features that do not alternate.
In contrast,the approach taken here aims to create a rich initiallexicon, to compel the selection of a restrictivegrammar.In addition to featural variants, variants of sur-face forms that differ in length are included if theyare supported by allomorphic alternation.
In par-ticular, featural variants of all the observed surfaceallomorphs of the morpheme are considered as po-tential underlying forms for the morpheme if eachof the observed surface forms can be generatedunder some ranking.
Including these types of un-derlying forms extends previous work, which didnot allow segmental insertion or deletion or con-straints that evaluate these unfaithful mappings,such as MAX and DEP.The algorithm initializes both the lexicon andgrammar to uniform probability distributions.
Thismeans that all rankings are initially equally likely.Likewise, all potential underlying forms for a mor-pheme are initially equally likely.
Thus, the prob-ability distributions begin unbiased, but choosingan unbiased lexicon initially begins the searchthrough parameter space at a position that favorsrestrictive grammars.
The experiments in the fol-lowing section suggest that this choice of initializa-tion correctly selects a restrictive final grammar.The learning algorithm itself is based on the Ex-pectation Maximization algorithm (Dempster et al,1977) and alternates between an expectation stageand a maximization stage.
During the expectationstage the algorithm computes the likelihood of theobserved surface forms under the current hypothe-sis.
During the maximization stage the algorithmadjusts the grammar and lexicon distributions inorder to increase the likelihood of the data.
Theprobability distribution over rankings is adjustedaccording to the following re-estimation formula:54PH +1(r) =FkFkk ?PH (Ok | r,Mk )PH (Ok | Mk )k Intuitively, this formula re-estimates the prob-ability of a ranking for state H+1 in proportion tothe ranking?s contribution to the overall probabilityat state H. The algorithm re-estimates the probabil-ity distribution for an underlying form according toan analogous formula:PH +1(Ik, j | M i) = FkFkk?PH (Ok & Ik, j | M i)PH (Ok | M i)kIntuitively, the re-estimate of the probability ofan underlying form Ik,j for state H+1 is propor-tional to the contribution that underlying formmakes to the total probability due to morpheme Miat state H. The algorithm continues to alternatebetween the two stages until the distributions con-verge, or until the change between one stage andthe next reaches some predetermined minimum.
Atthis point the resulting distributions are taken tocorrespond to the learned grammar and lexicon.4 ExperimentsThis section describes the results of experimentswith three artificial language systems with differ-ent types of hidden structure.
In all experimentspresented here, each unique surface form is as-sumed to occur with frequency 1.4.1 Voicing NeutralizationThe first test set is an artificial language system(Tesar and Prince, to appear) exhibiting voicingneutralization.
The constraint set includes five con-straints:?
NOVOI - No voiced obstruents?
NOSFV- No syllable-final voiced obstruents?
IVV - No intervocalic voiceless consonants?
IDVOI - Surface voicing must match underly-ing voicing?
MAX - Input segments must have output cor-respondentsThese five constraints can describe a number oflanguages, but of particular interest are languagesin which voicing contrasts are neutralized in one ormore positions.
Such languages, three of whichare shown below, test the algorithm?s ability toidentify correct and restrictive grammars.
The par-tial rankings shown below correspond to the neces-sary rankings that must hold for these languages;each partial ranking actually corresponds to severaltotal rankings of the constraints.
Also shown beloware the morphologically analyzed surface forms foreach language that are provided as input to the al-gorithm.
The subscripts in these forms indicatemorpheme identities, while the hyphens segmentthe words into separate morphemes.
For example,tat1,2 means that the surface form ?tat?
could bederived from either morpheme 1 or 2 in this lan-guage.?
(A) Final devoicing, contrast intervocalically:?
NOSFV, MAX >> IDVOI >> IVV, NOVOI?
tat1,2; dat3,4; tat1-e5; tad2-e5; dat3-e5; dad4-e5?
(B) Final devoicing and intervocalic voicing:?
NOSFV, MAX, IVV >> IDVOI, NOVOI?
tat1,2; dat3,4; tad1,2-e5; dad3,4-e5?
(C) No voiced obstruents:?
MAX, NOVOI >> IDVOI, IVV?
tat1,2,3,4; tat1,2,3,4-e5In language C, it would be possible to maximizethe objective function by selecting a restrictivelexicon rather than a restrictive grammar.
In par-ticular, /tat/ could be selected as the underlyingform for morphemes 1-4 in order to account for thelack of voiced obstruents in the observed surfaceforms.
In this case, the objective function couldjust as well be satisfied by an identity grammarmapping underlying /tat/ to surface ?tat?.
However,as discussed in Section 2, such a grammar wouldviolate the principle of Richness of the Base byputting the restriction against voiced obstruentsinto the lexicon rather than the grammar.
Thus, thislanguage tests not only whether the algorithm findsa maximum, but also whether the maximum corre-sponds to a restrictive grammar.In fact, for all three languages above, the algo-rithm converges on the correct, restrictive gram-mars and correct lexicons.
Specifically, the finalgrammars for each of the languages above con-verge on probability distributions that distribute theprobability mass equally among the total rankingsconsistent with the partial orders above.
For ex-ample, for language C the algorithm converges on55a distribution that assigns equal probability to the20 total rankings consistent with the partial ordergiven by MAX, NOVOI >> IDVOI, IVV.The initial uniform lexicon for language C isshown in Table 3.
Here the numbers 1-5 refer tomorpheme indices, and the possible underlyingforms for each morpheme are uniformly distrib-uted.
This initial lexicon favors a grammar that canmap as much of the rich lexicon as possible ontosurface forms with no voiced obstruents.
Withthese constraints, this translates into rankingNOVOI above IDVOI and IVV.
As the algorithmbegins learning the lexicon and continues to refineits hypothesis for this language, nothing drives thealgorithm to abandon the initial rich lexicon.
Thus,in the final state, the lexicon for this language isidentical to the initial lexicon.
In general, the finallexicon will be uniformly distributed over underly-ing forms that differ in noncontrastive features.1 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25%2 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25%3 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25%4 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25%5 /e/ - 100%Table 3.
Initial Lexicon for Language C4.2 Grammatical and Lexical StressThe next set of languages from the PAKA system(Tesar et al, 2003) test the ability of the algorithmto identify grammatical stress (most restrictive),lexical stress (least restrictive), and combinationsof the two.
The constraint set includes:?
MAINLEFT - Stress the leftmost syllable?
MAINRIGHT - Stress the rightmost syllable?
FAITHACCENT - Stress an accented syllable?
FAITHACCENTROOT - Stress an accented rootsyllablePossible languages and their corresponding par-tial orders ranging from least restrictive to mostrestrictive are shown below.
In the first two lan-guages, the least restrictive languages, lexical dis-tinctions in stress are realized faithfully, whilegrammatical stress surfaces only in forms with nounderlying stress.
In the final two languages stressis entirely grammatical; underlying distinctions areneutralized in favor of a regular surface stress pat-tern.
Finally, the middle language is a combinationof lexical and grammatical stress, requiring that thealgorithm learn that a contrast in roots is preserved,while a contrast in suffixes is neutralized.?
Full contrast: roots and suffixes contrast instress, default left:?
F >> ML >> MR, FAR?
p?1-ka3; pa1-g?4; b?2-ka3; b?2-ga4?
Full contrast: roots and suffixes contrast instress, default right:?
F >> MR >> ML, FAR?
pa1-k?3; pa1-g?4; b?2-ka3; ba2-g?4?
Root contrast only, default right:?
FAR >> MR >> ML?
pa1-k?3; pa1-g?4; b?2-ka3; b?2-ga4?
Predictable left stress:?
ML >> FAR, F, MR?
p?1-ka3; p?1-ga4; b?2-ka3; b?2-ga4?
Predictable right stress:?
MR >> FAR, F, ML?
pa1-k?3; pa1-g?4; ba2-k?3; ba2-g?4In all cases the algorithm learns the correct, re-strictive grammars corresponding to the partialorders shown above.
As before, the final lexiconassigns uniform probability to all underlying formsthat differ in noncontrastive features.
For example,in the case of the language with root contrast only,the final lexicon selects a unique lexical item forroot morphemes and maintains a uniform probabil-ity distribution over stressed and unstressed under-lying forms for suffixes.4.3 Abstract Underlying VowelsThe final experiment tests the algorithm on anartificial language, based on Polish, with abstractunderlying vowels that never surface faithfully.Although the particular phenomenon exhibited bySlavic alternating vowels is rare, the general phe-nomenon wherein underlying forms do not corre-spond to any surface allomorph is not uncommonand should be accommodated by the learning algo-rithm.
This language presents a challenge for pre-vious work on unsupervised learning of OTbecause alternations in the number of segments areobserved in morpheme 3.
The morphologically56annotated input to the algorithm for this languageis shown in Table 4.kater1 vatr2 sater3kater1-a4 vatr2-a4 satr3-a4Table 4.
Yer Language Surface FormsIn this language morphemes 1, 2 and 4 exhibit noalternation while morpheme 3 alternates betweensater and satr depending on the context.
The con-straints for this language, based on Jarosz (2005),are shown below:?
*E = *[+HIGH][-ATR]?
DEP-V?
MAX-V?
*COMPLEXCODA?
IDENT[HIGH]1 2 3 4/kater/ /vatr/ /satEr/ /-a/Table 5.
Desired Final LexiconIn the proposed analysis of this language, the ab-stract underlying [E], which is a [+high] version of[e], is neutralized on the surface and exhibits tworepairs systematically depending on the context.
Itdeletes in general, but if a complex coda is at stake,the vowel surfaces as [e] by violatingIDENT[HIGH].
The required partial ranking for thislanguage is shown below while the desired lexiconis shown in Table 5.
{*E, {DEP-V >> *COMPLEXCODA }} >>IDENT[HIGH] >> MAX-VThe algorithm successfully learns the correct rank-ing above and the lexicon in Table 5.
Specifically,the final grammar assigns equal probability to allthe rankings consistent with the above partial or-der.
The final lexicon selects a single underlyingform for each morpheme as shown in Table 5 be-cause all underlying distinctions in this languageare contrastive.4.4 DiscussionIn summary, the algorithm is able to find a cor-rect grammar and lexicon combination for all ofthe language systems discussed.
As discussed inSection 3, the objective function itself does notfavor restrictive grammars, but the ability of thealgorithm to learn restrictive grammars in theseexperiments suggests that initializing the lexiconsto uniform distributions does compel the learningalgorithm to select restrictive grammars rather thanrestrictive lexicons.While the experiments presented in this sectionfocus on the task of learning a grammar and lexi-con simultaneously, the proposed algorithm is alsocapable of learning grammars from structurallyambiguous forms.
The same likelihood maximiza-tion procedure proposed here could be used forunsupervised learning of grammars that assign fullstructural description to overt forms.
Future direc-tions include testing the algorithm on languagedata of this sort.5 ConclusionIn sum, this paper has presented an unsupervised,probabilistic algorithm for OT learning.
The paperargues that combining the OT principle of Rich-ness of the Base and likelihood maximization pro-vides a novel and general solution to the problemof finding a restrictive grammar.
The proposedsolution involves explicitly implementing Richnessof the Base in the initialization of the lexicon inorder to fully utilize the properties of the objectivefunction.
By relying on Richness of the Base andlikelihood maximization, the algorithm is able touse negative evidence implicitly to find restrictivegrammars.
The algorithm is shown to be successfulon three constructed languages featuring differenttypes of neutralization and hidden structure.One potential extension of the proposed algo-rithm involves combining a system for unsuper-vised learning of morphological relations with theproposed algorithm for learning phonology.
Sev-eral algorithms have been proposed for automati-cally inducing morphological relations, like thoseassumed by the present learner (Goldsmith, 2001;Snover and Brent, 2001).
The task of uncoveringmorphological relations is complicated by allo-morphic alternations that obscure the underlyingidentity of related morphemes.
While these algo-rithms are very promising, their performance maybe significantly enhanced if they were combinedwith an algorithm that models such phonologicalalternations.In conclusion, this is the first proposed unsuper-vised algorithm for OT learning that takes advan-57tage of the power of probabilistic modeling to learna grammar and lexicon simultaneously.
This paperdemonstrates that combining OT theoretic princi-ples with results from computational languagelearning is a worthwhile pursuit that may informboth disciplines.
In this case the theoretical princi-ple of Richness of the Base has provided a novelsolution to a learning problem, but at the sametime, this work also informs theoretical OT byproviding a formal characterization of this theo-retical principle.
Future work includes testing onlarger, more realistic languages, including lan-guage data with noise and variation, in order todetermine the algorithm?s resistance to noise andability to model variable grammars like those ob-served in natural languages and in human languageacquisition.AcknowledgementsI would like to thank Paul Smolensky for his in-valuable feedback on this work and for his sugges-tions on the preparation of this paper.
I am alsograteful to Luigi Burzio, Robert Frank, Jason Eis-ner, and members of the Johns Hopkins LinguisticsResearch Group (especially Joan Chen-Main,Adam Wayment, and Sara Finley) for additionalcomments and helpful discussion.ReferencesApoussidou, Diana and Paul Boersma.
2004.
Compar-ing Different Optimality-Theoretic Learning Algo-rithms:the Case of Metrical Phonology.
Proceedingsof the 2004 Spring Symposium Series of the Ameri-can Association for Artificial Intelligence.Anttila, Arto.
1997.
Deriving variation from grammar.In F. Hinskens, R. Van Hout and W. L.
Wetzels(eds.)
Variation, Change and Phonological Theory.Amsterdam, John Benjamins.Boersma, Paul.
1997.
How we Learn Variation, Option-ality, and Probability.
Proc.
Institute of Phonetic Sci-ences of the University of Amsterdam 21:43-58.Boersma, P. 1998.
Functional Phonology.
Doctoral Dis-sertation, University of Amsterdam.
The Hague: Hol-land Academic Graphics.Boersma, P. and B. Hayes.
2001.
Empirical Tests of theGradual Learning Algorithm.
Linguistic Inquiry32(1):45-86.Dempster, A.P., N.M. Laird, and D.B.
Rubin.
1977.Maximum Likelihood from incomplete data via theEM Algorithm.
Journal of Royal Statistics Society.39(B):1-38Eisner, Jason.
2000.
Easy and hard constraint ranking inoptimality theory: Algorithms and complexity.
In Ja-son Eisner, Lauri Karttunen and Alain Th?riault(eds.
), Finite-State Phonology: Proceedings of the5th Workshop of the ACL Special Interest Group inComputational Phonology (SIGPHON), pages 22-33,Luxembourg, August.Escudero, Paola.
2005.
Linguistic Perception and Sec-ond Language Acquisition.Explaining the attainmentof optimal phonological categorization.
Doctoral dis-sertation, Utrecht University.Goldsmith, John.
2001.
Unsupervised Learning of Mor-phology of a Natural Language.
Computational Lin-guistics, 27: 153-198.Goldwater, Sharon and Mark Johnson.
2003.
LearningOT constraint rankings using a maximum entropymodel.
In Jennifer Spenader, Anders Eriksson andOsten Dahl (eds.
), Proceedings of the StockholmWorkshop on Variation within Optimality Theory.Stockholm University, pages 111-120.Hayes, Bruce.
2004.
Phonological acquisition in Opti-mality Theory:  the early stages.
Appeared 2004 inKager, Rene, Pater, Joe, and Zonneveld, Wim, (eds.
),Fixing Priorities: Constraints in Phonological Ac-quisition.
Cambridge University Press.Jarosz, Gaja.
2005.
Polish Yers and the Finer Structureof Output-Output Correspondence.
31st Annual Meet-ing of the Berkeley Linguistics Society, Berkeley,California.Lari, K. and S.J.
Young.
1990.
The estimation of sto-chastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language.4:35-56Legendre, Geraldine, Paul Hagstrom, Anne Vainikkaand Marina Todorova.
2002.
Partial Constraint Or-dering in Child French Syntax.
to appear LanguageAcquisition 10(3).
189-227.Merchant, Nazarr?, and Bruce Tesar.
to appear.
Learn-ing underlying forms by searching restricted lexicalsubspaces.
In The Proceedings of Chicago Linguis-tics Society 41.
ROA-811.Pereira, F. and Y. Schabes.
1992.
Inside-Outside re-estimation from partially bracketed corpora.
In Pro-ceedings of the ACL 1992, Newark, Delaware.Prince, Alan and Paul Smolensky.
1993.
OptimalityTheory: Constraint Interaction in Generative Gram-mar.
Technical Report 2, Center for Cognitive Sci-ence, Rutgers University.58Prince, Alan, and Bruce Tesar.
1999.
Learning phono-tactic distributions.
Technical Report RuCCS-TR-54,Rutgers Center for Cognitive Science, Rutgers Uni-versity.Riggle, Jason.
2004.
Generation, Recognition, andLearning in Finite State Optimality Theory.
Ph.D.Dissertation, UCLA, Los Angeles, California.Rosenbach, Anette and Gerhard Jaeger.
2003.
Cumula-tivity in Variation: testing different versions of Sto-chastic OT empirically.
Presented at the SeventhWorkshop on Optimality Theoretic Syntax, Univer-sity of Nijmegen.Smolensky, Paul.
1996.
The initial state and `richness ofthe base' in Optimality Theory.
Technical ReportJHU-CogSci-96-4, Department of Cognitive Science,Johns Hopkins University.Snover, Matthew and Michael R. Brent.
2001 A Bayes-ian Model for Morpheme and Paradigm Identifica-tion.
In Proceedings of the 39th Annual Meeting ofthe ACL, pages 482-490.
Association for Computa-tional Linguistics.Tesar, Bruce.
1995.
Computational Optimality Theory.Ph.D.
thesis, University of Colorado at Boulder,June.Tesar, Bruce.
1998.
An iterative strategy for languagelearning.
Lingua 104:131-145.
ROA-177.Tesar, Bruce.
1999.
Robust interpretive parsing in met-rical stress theory.
In The Proceedings of SeventeenthWest Coast Conference on Formal Linguistics, pp.625-639.
ROA-262.Tesar, Bruce.
2004.
Contrast analysis in phonologicallearning.
Manuscript, Linguistics Dept., RutgersUniversity.
ROA-695.Tesar, Bruce, John Alderete, Graham Horwood, Nazarr?Merchant, Koichi Nishitani, and Alan Prince.
2003.?Surgery in language learning?.
In The Proceedingsof Twenty-Second West Coast Conference on FormalLinguistics, pp.
477-490.
ROA-619.Tesar, Bruce and Alan Prince.
to appear.
?Using phono-tactics to learn phonological alternations.?
Revisedversion will appear in The Proceedings of CLS 39,Vol.
II: The Panels.
ROA-620.Tesar, Bruce and Paul Smolensky.
1995.
?The Learn-ability of Optimality Theory?.
In Proceedings of theThirteenth West Coast Conference on Formal Lin-guistics, 122-137.59
