Efficient Deep Processing of JapaneseMelanie SIEGELDFKI GmbHStuhlsatzenhausweg 366123 Saarbr?cken, Germanysiegel@dfki.deEmily M. BENDERCSLI Stanford220 Panama StreetStanford, CA, 94305-4115, USAbender@csli.stanford.eduAbstractWe present a broad coverage Japanesegrammar written in the HPSG formalismwith MRS semantics.
The grammar iscreated for use in real world applications,such that robustness and performance issuesplay an important role.
It is connected to aPOS tagging and word segmentation tool.This grammar is being developed in amultilingual context, requiring MRSstructures that are easily comparable acrosslanguages.IntroductionNatural language processing technology hasrecently reached a point where applications thatrely on deep linguistic processing are becomingfeasible.
Such applications (e.g.
messageextraction systems, machine translation anddialogue understanding systems) require naturallanguage understanding, or at least anapproximation thereof.
This, in turn, requiresrich and highly precise information as the outputof a parse.
However, if the technology is tomeet the demands of real-world applications,this must not come at the cost of robustness.Robustness requires not only wide coverage bythe grammar (in both syntax and semantics), butalso large and extensible lexica as well asinterfaces to preprocessing systems for namedentity recognition, non-linguistic structures suchas addresses, etc.
Furthermore, applicationsbuilt on deep NLP technology should beextensible to multiple languages.
This requiresflexible yet well-defined output structures thatcan be adapted to grammars of many differentlanguages.
Finally, for use in real-worldapplications, NLP systems meeting the abovedesiderata must also be efficient.In this paper, we describe the development ofa broad coverage grammar for Japanese that isused in an automatic email response application.The grammar is based on work done in theVerbmobil project (Siegel 2000) on machinetranslation of spoken dialogues in the domain oftravel planning.
It has since been greatlyextended to accommodate written Japanese andnew domains.The grammar is couched in the theoreticalframework of Head-Driven Phrase StructureGrammar (HPSG) (Pollard & Sag 1994), withsemantic representations in Minimal RecursionSemantics (MRS) (Copestake et al 2001).HPSG is well suited to the task of multilingualdevelopment of broad coverage grammars: It isflexible enough (analyses can be shared acrosslanguages but also tailored as necessary), andhas a rich theoretical literature from which todraw analyzes and inspiration.
Thecharacteristic type hierarchy of HPSG alsofacilitates the development of grammars that areeasy to extend.
MRS is a flat semanticformalism that works well with typed featurestructures and is flexible in that it providesstructures that are under-specified for scopalinformation.
These structures give compactrepresentations of ambiguities that are oftenirrelevant to the task at hand.HPSG and MRS have the further advantagethat there are practical and useful open-sourcetools for writing, testing, and efficientlyprocessing grammars written in theseformalisms.
The tools we are using in thisproject include the LKB system (Copestake2002) for grammar development, [incr tsdb()](Oepen & Carroll 2000) for testing the grammarand tracking changes, and PET (Callmeier2000), a very efficient HPSG parser, forprocessing.
We also use the ChaSen tokenizerand POS tagger (Asahara & Matsumoto 2000).While couched within the same generalframework (HPSG), our approach differs fromthat of Kanayama et al(2000).
The workdescribed there achieves impressive coverage(83.7% on the EDR corpus of newspaper text)with an underspecified grammar consisting of asmall number of lexical entries, lexical typesassociated with parts of speech, and sixunderspecified grammar rules.
In contrast, ourgrammar is much larger in terms of the numberof lexical entries, the number of grammar rules,and the constraints on both,1 and takescorrespondingly more effort to bring up to thatlevel of coverage.
The higher level of detailallows us to output precise semanticrepresentations as well as to use syntactic,semantic and lexical information to reduceambiguity and rank parses.1 Japanese HPSG SyntaxThe fundamental notion of an HPSG is the sign.A sign is a complex feature structurerepresenting information of different linguisticlevels of a phrase or lexical item.
The attribute-value matrix of a sign in the Japanese HPSG isquite similar to a sign in the LinGO EnglishResource Grammar (henceforth ERG)(Flickinger 2000), with information about theorthographical realization of the lexical sign inPHON, syntactic and semantic information inSYNSEM, information about the lexical status inLEX, nonlocal information in NONLOC, headinformation that goes up the tree in HEAD andinformation about subcategorization in SUBCAT.The grammar implementation is based on asystem of types.
There are 900 lexical types thatdefine the syntactic, semantic and pragmaticproperties of the Japanese words, and 188 typesthat define the properties of phrases and lexicalrules.
The grammar includes 50 lexical rules forinflectional and derivational morphology and 47phrase structure rules.
The lexicon contains 5100stem entries.
As the grammar is developed foruse in applications, it treats a wide range of1 We do also make use of generic lexical entries forcertain parts of speech as a means of extending ourlexicon.
See section 3 below.basic constructions of Japanese.
Only some ofthese phenomena can be described here.1.1 SubcategorizationThe structure of SUBCAT is different from theERG SUBCAT structure.
This is due todifferences in subcategorization betweenJapanese and English.
A fundamental differenceis the fact that, in Japanese, verbal arguments arefrequently omitted.
For example, arguments thatrefer to the speaker, addressee, and otherarguments that can be inferred from context areoften omitted in spoken language.
Additionally,optional verbal arguments can scramble.
On theother hand, some arguments are not onlyobligatory, but must also be realized adjacent tothe selecting head.To account for this, our subcategorizationcontains the attributes SAT and VAL.
The SATvalue encodes whether a verbal argument isalready saturated (such that it cannot besaturated again), optional or adjacent.
VALcontains the agreement information for theargument.
When an argument is realized, itsSAT value on the mother node is specified as satand its SYNSEM is unified with its VAL value onthe subcategorizing head.
The VAL value on themother is none.
Adjacency must be checked inevery rule that combines heads and arguments oradjuncts.
This is the principle of adjacency,stated as follows:In a headed phrase, the SUBCAT.SAT valueon the non-head daughter must not containany adjacent arguments.
In a head-complement structure, the SUBCAT.SATvalue of the head daughter must not containany adjacent arguments besides the non-head daughter.
In a head-adjunct structure,the SUBCAT.SAT value of the head daughtermust not contain any adjacent arguments.1.2 Verbal inflectionJapanese verb stems combine with endings thatprovide information about honorification, tense,aspect, voice and mode.
Inflectional rules for thedifferent types of stems prepare the verb stemsfor combination with the verbal endings.
Forexample, the verb stem yomu must be inflectedto yon to combine with the past tense ending da.Morphological features constrain thecombination of stem and ending.
In the aboveexample, the inflectional rule changes the mucharacter to the n character and assigns the valuend-morph to the morphological featureRMORPH-BIND-TYPE.
The ending da selectsfor a verbal stem with this value.Endings can be combined with other endings,as in -sase-rare-mashi-ta (causative-potential-honorific-past), but not arbitrarily:*-sase-mashi-rare-ta*-sase-ta-mashi-rare-sase-ta-rare-mashi-taThis is accounted for with two kinds of ruleswhich realize mutually selected elements.
In thecombination of stem and ending, the verb stemselects for the verbal ending via the head featureSPEC.
In the case of the combination of twoverbal endings, the first ending selects for thesecond one via the head feature MARK.
In bothcases, the right element subcategorizes for theleft one via SUBCAT.VAL.SPR.
Using thismechanism, it is possible to control the sequenceof verbal endings: Verb stems select verbalendings via SPEC and take no SPR, derivationalmorphemes (like causative or potential) selecttense endings or other derivational morphemesvia MARK and subcategorize for verb stemsand/or verb endings via SPR (sase takes onlyverb stems), and tense endings take verb stemsor endings as SPR and take no MARK or SPEC(as they occur at the end of the sequence).1.3 Complex PredicatesA special treatment is needed for Japaneseverbal noun + light verb constructions.
In thesecases, a word that combines the qualities of anoun with those of a verb occurs in aconstruction with a verb that has only marginalsemantic information.
The syntactic, semanticand pragmatic information on the complex is acombination of the information of the two.Consider example 1.
The verbal nounbenkyou contains subcategorization information(transitive), as well as semantic information (thebenkyou-relation and its semantic arguments).The light verb shi-ta supplies tense information(past).
Pragmatic information can be supplied byboth parts of the construction, as in the formalform o-benkyou shi-mashi-ta.
The rule thatlicenses this type of combination is the vn-light-rule, a subtype of the head-marker-rule.Example 1:Benkyou shi-ta.study do-past'Someone has studied.
'Japanese auxiliaries combine with verbs andprovide either aspectual or perspectiveinformation or information about honorification.In a verb-auxiliary construction, the informationabout subcategorization is a combination of theSUBCAT information of verb and auxiliary,depending on the type of auxiliary.
The ruleresponsible for the information combination inthese cases is the head-specifier-rule.
We havethree basic types of auxiliaries.
The first type isaspect auxiliaries.
These are treated as raisingverbs, and include such elements as iru (roughly,progressive) and aru (roughly, perfective), ascan be seen in example 2.
The other two classesof auxiliaries provide information aboutperspective or the point of view from which asituation is being described.
Both classes ofauxiliaries add a ni (dative) marked argument tothe argument structure of the whole predicate.The classes differ in how they relate theirarguments to the arguments of the verb.
Oneclass (including kureru 'give'; see example 3) aretreated as subject control verbs.
The other class(including morau 'receive', see example 4)establishes a control relation between the ni-marked argument and the embedded subject.Example 2:Keeki wo tabe-te iru.cake ACC eat progressive'Someone is eating cake.
'Example 3:Sensei wa watashi ni hon woteacher TOP  I DAT book ACCkatte kure-ta.buy give-past'The teacher bought me a book.
'Example 4:Watashi  ga sensei ni hon woI  NOM teacher DAT book ACCkatte morat-ta.buy get-past'The teacher bought me a book.
'1.4 Particles in a type hierarchyThe careful treatment of Japanese particles isessential, because they are the most frequentlyoccurring words and have various centralfunctions in the grammar.
It is difficult, becauseone particle can fulfill more than one functionand they can co-occur, but not arbitrarily.
TheJapanese grammar thus contains a type hierarchyof 44 types for particles.
See Siegel (1999) for amore detailed description of relevant phenomenaand solutions.1.5 Numeral ExpressionsNumber names, such as sen kyuu hyaku juu'1910' constitute a notable exception to thegeneral head-final pattern of Japanese phrases.We found Smith's (1999) head-medial analysisof English number names to be directlyapplicable to the Japanese system as well(Bender 2002).
This analysis was easilyincorporated into the grammar, despite theoddity of head positioning, because the typehierarchy of HPSG is well suited to express thepartial generalizations that permeate naturallanguage.On the other hand, number names inJapanese contrast sharply with number names inEnglish in that they are rarely used without anumeral classifier.Example 5:Juu *(hiki no) neko ga ki-ta.ten   CL GEN cat NOM arrive-past'Ten cats arrived.
'The grammar provides for 'true' numeralclassifiers like hon, ko, and hiki, as well asformatives like en 'yen' and do 'degree' whichcombine with number names just like numeralclassifiers do, but never serve as numeralclassifiers for other nouns.
In addition, there area few non-branching rules that allow barenumber names to surface as numeral classifierphrases with specific semantic constraints.1.6 Pragmatic informationSpoken language and email correspondence bothencode references to the social relation of thedialogue partners.
Utterances can express socialdistance between addressee and speaker andthird persons.
Honorifics can even expressrespect towards inanimates.
Pragmaticinformation is treated in the CONTEXT layer ofthe complex signs.
Honorific information isgiven in the CONTEXT.BACKGROUND andlinked to addressee and speaker anchors.The expression of empathy or in-group vs.out-group is quite prevalent in Japanese.
Onemeans of expressing empathy is the perspectiveauxiliaries discussed above.
For example, twoauxiliaries meaning roughly 'give' (ageru andkureru) contrast in where they place theempathy.
In the case of ageru, it is with thegiver.
In the case of kureru, it is with therecipient.
We model this within the sign bypositing a feature EMPATHY within CONTEXTand linking it to the relevant arguments' indices.2 Japanese MRS SemanticsIn the multilingual context in which thisgrammar has been developed, a high premium isplaced on parallel and consistent semanticrepresentations between grammars for differentlanguages.
Ensuring this parallelism enables thereuse of the same downstream technology, nomatter which language is used as input.Integrating MRS representations parallel tothose used in the ERG into the Japanesegrammar took approximately 3 months.
Ofcourse, semantic work is on-going, as every newconstruction treated needs to be given a suitablesemantic representation.
For the most part,semantic representations developed for Englishwere straightforwardly applicable to Japanese.This section provides a brief overview of thosecases where the Japanese constructions weencountered led to innovations in the semanticrepresentations and/or the correspondencebetween syntactic and semantic structures.
Dueto space limitations, we discuss these analyses ingeneral terms and omit technical details.2.l Nominalization and Verbal NounsNominalization is of course attested in Englishand across languages.
However, it is much moreprevalent in Japanese than in English, primarilybecause of verbal nouns.
As noted in Section1.3 above, a verbal noun like benkyou 'study' canappear in syntactic contexts requiring nouns, or,in combination with a light verb, in contextsrequiring verbs.
One possible analysis wouldprovide two separate lexical entries, one withnominal and one with verbal semantics.However, this would not only be redundant(missing the systematic relationship betweenthese uses of verbal nouns) but would alsocontradict the intuition that even in its nominaluse, the arguments of benkyou are still present.Example 6:Nihongo no benkyou wo hajimeru.Japanese GEN study ACC begin'Someone begins the study of Japanese.
'In order to capture this intuition, we opted for ananalysis that essentially treats verbal nouns asunderlyingly verbal.
The nominal uses areproduced by a lexical rule which nominalizes theverbal nouns.
The semantic effect of this rule isto provide a nominal relation which introduces avariable which can in turn be bound byquantifiers.
The nominal relation subordinatesthe original verbal relation supplied by theverbal noun.
The rule is lexical as we have notyet found any cases where the verb's argumentsare clearly filled by phrases in the syntax.
Ifthey do appear, it is with genitive marking (e.g.,nihongo no in the example above).
In order toreduce ambiguity, we leave the relationshipbetween these genitive marked NPs and thenominalized verbal noun underspecified.
Thereis nothing in the syntax to disambiguate thesecases, and we find that they are better left todownstream processing, where there may beaccess to world knowledge.2.2 Numeral ClassifiersAs noted in Section1.5, the internal syntax ofnumber names is surprisingly parallel betweenEnglish and Japanese, but their external syntaxdiffers dramatically.
English number names canappear directly as modifiers of NPs and aretreated semantically as adjectives in the ERG.Japanese number names can only modify nounsin combination with numeral classifiers.
Inaddition, numeral classifier phrases can appearin NP positions (akin to partitives in English).Finally, some numeral-classifier-like elementsdo not serve the modifier function but can onlyhead phrases that fill NP positions.This constellation of facts required thefollowing innovations: a representation ofnumbers that doesn't treat them as adjectives (inMRS terms, a feature structure without the ARGfeature), a representation of the semanticcontribution of numeral classifiers (a relationbetween numbers and the nouns they modify,this time with an ARG feature), and a set ofrules for promoting numeral classifier phrases toNPs that contribute the appropriate nominalsemantics (underspecified in the case of ordinarynumeral classifiers or specific in the case ofwords like en 'yen').2.3 Relative Clauses and AdjectivesThe primary issue in the analysis of relativeclauses and adjectives is the possibility ofextreme ambiguity, due to several intersectingfactors:  Japanese has rampant pro-drop anddoes not have any relative pronouns.
Inaddition, a head noun modified by a relativeclause need not correspond to any gap in therelative clause, as shown by examples like thefollowing (Matsumoto 1997):Example 7:atama ga yoku naru honhead NOM better become book'a book that makes one smarter'Therefore, if we were to posit an attributiveadjective + noun construction (distinct from therelative clause + noun possibility) we wouldhave systematic ambiguities for NPs like akaihon ('red book'), ambiguities which could neverbe resolved based on information in thesentence.
Instead, we have opted for a relativeclause analysis of any adjective + nouncombination in which the adjective couldpotentially be used predicatively.
Furthermore,because of gapless relative clauses like the onecited above, we have opted for a non-extractionanalysis of relative clauses.2Nonetheless, the well-formedness constraintson MRS representations require that there be2 There is in fact some linguistic evidence forextraction in some relative clauses in Japanese  (seee.g., Baldwin 2001).
However, we saw no practicalneed to allow for this possibility in our grammar, andparticularly not one that would justify the increase inambiguity.
There is also evidence that someadjectives are true attributives and cannot be usedpredicatively (Yamakido 2000).
These are handled bya separate adjective + noun rule restricted to justthese cases.some relationship between the head noun andthe relative clause.
We picked the topic relationfor this purpose (following Kuno 1973).
Thetopic relation is introduced into the semantics bythe relative clause rule.
As with main clausetopics (which we also give a non-extractionanalysis), we rely on downstream anaphoraresolution to refine the relationship.2.4 SummaryFor the most part, semantic representations andthe syntax-semantic interface already workedout in the ERG were directly applicable to theJapanese grammar.
In those cases whereJapanese presented problems not yetencountered (or at least not yet tackled) inEnglish, it was fairly straightforward to work outsuitable MRS representations and means ofbuilding them up.
Both of these points illustratethe cross-linguistic validity and practical utilityof MRS representations.3 Integration of a MorphologicalAnalyzerAs Japanese written text does not have wordsegmentation, a preprocessing system isrequired.
We integrated ChaSen (Asahara &Matsumoto 2000), a tool that provides wordsegmentation as well as POS tags andmorphological information such as verbalinflection.
As the lexical coverage of ChaSen ishigher than that of the HPSG lexicon, defaultpart-of-speech entries are inserted into thelexicon.
These are triggered by the part-of-speech information given by ChaSen, if there isno existing entry in the lexicon.
These specificdefault entries assign a type to the word thatcontains features typical to its part-of-speech.
Itis therefore possible to restrict the lexicon tothose cases where the lexical informationcontains more than the typical information for acertain part-of-speech.
This default mechanismis often used for different kinds of names and'ordinary' nouns, but also for adverbs,interjections and verbal nouns (where weassume a default transitive valence pattern).33 Kanayama et al (2000) use a similar mechanism formost words.
They report only 105 grammar-inherentlexical entries.The ChaSen lexicon is extended with a domain-specific lexicon, containing, among others,names in the domain of banking.For verbs and adjectives, ChaSen givesinformation about stems and inflection that isused in a similar way.
The inflection type istranslated to an HPSG type.
These types interactwith the inflectional rules in the grammar suchthat the default entries are inflected just as'known' words would be.In addition to the preprocessing done byChaSen, an additional (shallow) preprocessingtool recognizes numbers, date expressions,addresses, email addresses, URLs, telephonenumbers and currency expressions.
The outputof the preprocessing tool replaces theseexpressions in the string with placeholders.
Theplaceholders are parsed by the grammar usingspecial placeholder lexical entries.4 Robustness and Performance IssuesThe grammar is aimed at working with real-world data, rather than at experimenting withlinguistic examples.
Therefore, robustness andperformance issues play an important role.While grammar development is carried out inthe LKB (Copestake 2002), processing (both inthe application domain and for the purposes ofrunning test suites) is done with the highlyefficient PET parser (Callmeier 2000).
Figures 1and 2 show the performance of PET parsing ofhand-made and real data, respectively.Phenomenon  items#etasks?filter%edges?first?
(s)total?
(s)tcpu?
(s)gc?
(s)space?
(kb)Total 742 946 95.7 303 0.06 0.11 0.11 0 833Fig.1 Performance parsing banking data, generatedby [incr tsdb()]Phenomenon items#etasks?filter%edges?first?
(s)total?
(s)tcpu?
(s)tgc?
(s)space?
(kb)Total 316 2020 96.5 616 0.23 0.26 0.26 0 1819Fig.2 Performance parsing document request data,generated by [incr tsdb()]One characteristic of real-world data is thevariety of punctuation marks that occur and thepotential for ambiguity that they bring.
In ourgrammar, certain punctuation marks are givenlexical entries and processed by grammar rules.Take, for example, quotation marks.
Ignoringthem (as done in most development-orientedgrammars and smaller grammars), leads to asignificant loss of structural information:Example 8:"Botan wo osu" to it-tabutton ACC push COMPL say-past'Someone said: ?push the button.
"?The formative to is actually ambiguous betweena complementizer and a conjunction.
Since thephrase before to is a complete sentence, thisstring is ambiguous if one ignores the quotationmarks.
With the quotation marks, however, onlythe complementizer to is possible.
Given thehigh degree of ambiguity inherent in broad-coverage grammars, we have found it extremelyuseful to parse punctuation rather than ignore it.The domains we have been working on (likemany others) contain many date and numberexpressions.
While a shallow tool recognizesgeneral structures, the grammar contains rulesand types to process these.Phenomena occurring in semi-spontaneouslanguage (email correspondence), such asinterjections (e.g.
maa 'well'), contracted verbforms (e.g.
tabe-chatta < tabete-shimatta'(someone) ate it all up'), fragmentary sentences(e.g.
bangou: 1265 'number: 1265') and NPfragments (e.g.
bangou?
'number?')
must becovered as well as the 'ordinary' completesentences found in more carefully edited text.Our grammar includes types, lexical entries, andgrammar rules for dealing with such phenomena.Perhaps the most important performanceissue for broad coverage grammars is ambiguity.At one point in the development of thisgrammar, the average number of readingsdoubled in two months of work.
We currentlyhave two strategies for addressing this problem:First, we include a mechanism into the grammarrules that chooses left-branching rules in casesof compounds, genitive modification andconjuncts, as we don?t have enough lexical-semantic information represented to choose theright dependencies in these cases.4 Secondly, weuse a mechanism for hand-coding readingpreferences among rules and lexical entries.4Consider, for example, genitive modification: Thesemantic relationship between modifier and modifieeis dependent on their semantic properties: toukyou nokaigi - 'the meeting in Tokyo', watashi no hon - 'mybook'.
More lexical-semantic information is neededto choose the correct parse in more complexstructures, such as in watashi no toukyou no imooto ?
?My sister in Tokyo?.Restrictions like head-complement preferred tohead-adjunct are quite obvious.
Others requiredomain-specific mechanisms that shall besubject of further work.
Stochasticdisambiguation methods being developed for theERG by the Redwoods project at StanfordUniversity (Oepen et al 2002) should beapplicable to this grammar as well.5 EvaluationThe grammar currently covers 93.4% ofconstructed examples for the banking domain(747 sentences) and 78.2% of realistic emailcorrespondence data (316 sentences), concerningrequests for documents.
During three months ofwork, the coverage in the banking domainincreased 48.49%.
The coverage of thedocument request data increased 51.43% in thefollowing two weeks.Phenomenon totalitems#positiveitems#wordstring%lexicalitems?parseranalyses?totalresults#overallcoverage%Total 747 747 101 75.24 6.54 698 93.4Fig.3 Coverage of banking data, generated by[incr tsdb()]Phenomenon totalitems#positiveitems#wordstring%lexicalitems?parseranalyses?totalresults#overallcoverage%Total 316 316 1.00 83.90 39.91 247 78.2Fig.4 Coverage of document request data, generatedby [incr tsdb()]We applied the grammar to unseen data in oneof the covered domains, namely the FAQ site ofa Japanese bank.
The coverage was 61%.
91.2%of the parses output were associated with allwell-formed MRSs.
That means that we couldget correct MRSs in 55.61% of all sentences.ConclusionWe described a broad coverage Japanesegrammar, based on HPSG theory.
It encodessyntactic, semantic, and pragmatic information.The grammar system is connected to amorphological analysis system and uses defaultentries for words unknown to the HPSG lexicon.Some basic constructions of the Japanesegrammar were described.
As the grammar isaimed at working in applications with real-worlddata, performance and robustness issues areimportant.The grammar is being developed in amultilingual context, where much value isplaced on parallel and consistent semanticrepresentations.
The development of thisgrammar constitutes an important test of thecross-linguistic validity of the MRS formalism.The evaluation shows that the grammar is ata stage where domain adaptation is possible in areasonable amount of time.
Thus, it is apowerful resource for linguistic applications forJapanese.In future work, this grammar could be furtheradapted to another domain, such as the EDRnewspaper corpus (including a headlinegrammar).
As each new domain is approached,we anticipate that the adaptation will becomeeasier as resources from earlier domains arereused.
Initial evaluation of the grammar onnew domains and the growth curve of grammarcoverage should bear this out.ReferencesAsahara, Masayuki and Yuji Matsumoto (2000).Extended Models and Tools for High-performancePart-of-speech Tagger.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics, Coling 2000, 21-27.
Saarbr?cken,Germany.Baldwin, Timothy (2001).
Making Lexical Sense ofJapanese-English Machine Translation: ADisambiguation Extravaganza.
PhD thesis, TokyoInstitute of Technology.Bender, Emily M. (2002).
Number Names inJapanese: A Head-Medial Construction in a Head-Final Language.
Paper presented at the 76thannual meeting of the LSA, San Francisco.Callmeier, Ulrich (2000).
PET ?
a platform forexperimentation with efficient HPSG processingtechniques.
Journal of Natural LanguageEngineering, Special Issue on Efficient Processingwith HPSG: Methods, Systems, Evaluation, pages99-108.Copestake, Ann (2002).
Implementing TypedFeature-Structure Grammars.
Stanford: CSLI.Copestake, Ann, Alex Lascarides, and Dan Flickinger(2001).
An Algebra for Semantic Construction inConstraint-based Grammars.
Proceedings of the39th Annual Meeting of the Association forComputational Linguistics (ACL 2001), Toulouse,France.Flickinger, Dan (2000).
On Building a MoreEfficient Grammar by Exploiting Types.
NaturalLanguage Engineering 6(1) (Special Issue onEfficient Processing with HPSG), pages 15-28.Kanayama, Hiroshi, Kentaro Torisawa, YutakaMitsuishi and Jun?ichi Tsujii (2000).
A HybridJapanese Parser with Hand-crafted Grammar andStatistics.
In Proceedings of the 18th InternationalConference on Computational Linguistics, Coling2000.
Saarbr?cken, Germany.Kuno, Susumu (1973).
The Structure of the JapaneseLanguage.
Cambridge, MA: The MIT Press.Matsumoto, Yoshiko (1997).
Noun-ModifyingConstructions in Japanese: A Frame SemanticApproach.
John Benjamins.Oepen, Stephan and John Carroll (2000).Performance Profiling for Parser Engineering.Journal of Natural Language Engineering, SpecialIssue on Efficient Processing with HPSG: Methods,Systems, Evaluation, pages 81-97.Oepen, Stephan, Kristina Toutanova, Stuart Shieber,Chris Manning, Dan Flickinger and ThorstenBrants (2002).
The LinGO Redwoods Treebank.Motivation and Preliminary Applications.
InProceedings of the 19th International Conferenceon Computational Linguistics, Coling 2002.
Tapei,Taiwan.
.Pollard, Carl and Ivan A.
Sag (1994).
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress.Siegel, Melanie (1999).
The Syntactic Processing ofParticles in Japanese Spoken Language.
In: Wang,Jhing-Fa and Wu, Chung-Hsien (eds.
):Proceedings of the 13th Pacific Asia Conferenceon Language, Information and Computation,Taipei 1999.Siegel, Melanie (2000).
HPSG Analysis of Japanese.In: W. Wahlster (ed.
): Verbmobil: Foundations ofSpeech-to-Speech Translation.
Springer Verlag.Smith, Jeffrey D. (1999).
English number names inHPSG.
In Gert Webelhuth, Andreas Kathol, andJean-Pierre Koenig (eds.
), Lexical andConstructional Aspects of Linguistic Explanation.Stanford: CSLI.
145-160.Yamakido, Hiroko (2000).
Japanese attributiveadjectives are not (all) relative clauses.
In RogerBillerey and Brook Danielle Lillehaugen (eds.
),WCCFL 19: Proceedings of the 19th West CoastConference on Formal Linguistics.
Somerville,MA: Cascadilla Press.
588-602.
