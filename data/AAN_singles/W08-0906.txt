Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 44?52,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAnswering Learners?
Questions by Retrieving Question Paraphrasesfrom Social Q&A SitesDelphine Bernhard and Iryna GurevychUbiquitous Knowledge Processing LabComputer Science DepartmentTechnische Universita?t Darmstadt, Hochschulstra?e 10D-64289 Darmstadt, Germany{delphine|gurevych}@tk.informatik.tu-darmstadt.deAbstractInformation overload is a well-known prob-lem which can be particularly detrimental tolearners.
In this paper, we propose a methodto support learners in the information seek-ing process which consists in answering theirquestions by retrieving question paraphrasesand their corresponding answers from socialQ&A sites.
Given the novelty of this kind ofdata, it is crucial to get a better understand-ing of how questions in social Q&A sites canbe automatically analysed and retrieved.
Wediscuss and evaluate several pre-processingstrategies and question similarity metrics, us-ing a new question paraphrase corpus col-lected from the WikiAnswers Q&A site.
Theresults show that viable performance levels ofmore than 80% accuracy can be obtained forthe task of question paraphrase retrieval.1 IntroductionQuestion asking is an important component of effi-cient learning.
However, instructors are often over-whelmed with students?
questions and are thereforeunable to provide timely answers (Feng et al, 2006).Information seeking is also rendered difficult by thesheer amount of learning material available, espe-cially online.
The use of advanced information re-trieval and natural language processing techniquesto answer learners?
questions and reduce the diffi-culty of information seeking is henceforth particu-larly promising.
Question Answering (QA) systemsseem well suited for this task since they aim at gen-erating precise answers to natural language ques-tions instead of merely returning documents con-taining answers.
However, QA systems have to beadapted to meet learners?
needs.
Indeed, learnersdo not merely ask concrete or factoid questions, butrather open-ended, explanatory or methodologicalquestions which cannot be answered by a single sen-tence (Baram-Tsabari et al, 2006).
Despite a recenttrend to render the tasks more complex at large scaleQA evaluation campaigns such as TREC or CLEF,current QA systems are still ill-suited to meet theserequirements.A first alternative to full-fledged QA consists inmaking use of already available question and answerpairs extracted from archived discussions.
For in-stance, Feng et al (2006) describe an intelligent dis-cussion bot for answering student questions in fo-rums which relies on answers retrieved from an an-notated corpus of discussions.
This renders the taskof QA easier since answers do not have to be gener-ated from heterogeneous documents by the system.The scope of such a discussion bot is however inher-ently limited since it relies on manually annotateddata, taken from forums within a specific domain.We propose a different solution which consists intapping into the wisdom of crowds to answer learn-ers?
questions.
This approach provides the com-pelling advantage that it utilises the wealth of al-ready answered questions available in online socialQ&A sites.
The task of Question Answering canthen be boiled down to the problem of finding ques-tion paraphrases in a database of answered ques-tions.
Question paraphrases are questions whichhave identical meanings and expect the same answerwhile presenting alternate wordings.
Several meth-ods have already been proposed to identify question44paraphrases mostly in FAQs (Tomuro and Lytinen,2004) or search engine logs (Zhao et al, 2007).In this paper, we focus on the problem of questionparaphrase identification in social Q&A sites withina realistic information seeking scenario: given a userquestion, we want to retrieve the best matching ques-tion paraphrase from a database of previously an-swered questions in order to display the correspond-ing answer.
The use of social Q&A sites for ed-ucational applications brings about new challengeslinked to the variable quality of social media content.As opposed to questions in FAQs, which are subjectto editorial control, questions in social Q&A sitesare often ill-formed or contain spelling errors.
It istherefore crucial to get a better understanding of howthey can be automatically analysed and retrieved.
Inthis work, we focus on several pre-processing strate-gies and question similarity measures applied to thetask of identifying question paraphrases in a socialQ&A site.
We chose WikiAnswers which has beenranked by comScore as the first fastest growing do-main of the top 1,500 in the U.S. in 2007.The remainder of the paper is organised as fol-lows.
Section 2 first discusses related work onparaphrase identification and question paraphrasing.Section 3 then presents question and answer repos-itories with special emphasis on social Q&A sites.Our methods to identify question paraphrases are de-tailed in section 4.
Finally, we present and analysethe experimental results obtained in section 5 andconclude in section 6.2 Related WorkThe identification of question paraphrases in ques-tion and answer repositories is related to researchfocusing on sentence paraphrase identification (sec-tion 2.1) and query paraphrasing (section 2.2).
Thespecific features of question paraphrasing have alsoalready been investigated (section 2.3).2.1 Sentence Paraphrase IdentificationParaphrases are alternative ways to convey the sameinformation (Barzilay and McKeown, 2001).
Para-phrases can be found at different levels of lin-guistic structure: words, phrases and whole sen-tences.
While word and phrasal paraphrases canbe assimilated to the well-studied notion of syn-onymy, sentence level paraphrasing is more difficultto grasp and cannot be equated with word-for-wordor phrase-by-phrase substitution since it might en-tail changes in the structure of the sentence (Barzi-lay and Lee, 2003).
In practice, sentence para-phrases are identified using various string and se-mantic similarity measures which aim at captur-ing the semantic equivalence of the sentences beingcompared.
String similarity metrics, when appliedto sentences, consist in comparing the words con-tained in the sentences.
There exist many differentstring similarity measures: word overlap (Tomuroand Lytinen, 2004), longest common subsequence(Islam and Inkpen, 2007), Levenshtein edit distance(Dolan et al, 2004), word n-gram overlap (Barzilayand Lee, 2003) etc.
Semantic similarity measuresare obtained by first computing the semantic simi-larity of the words contained in the sentences beingcompared.
Mihalcea et al (2006) use both corpus-based and knowledge-based measures of the seman-tic similarity between words.
Both string similarityand semantic similarity might be combined: for in-stance, Islam and Inkpen (2007) combine semanticsimilarity with longest common subsequence stringsimilarity, while Li et al (2006) make additional useof word order similarity.2.2 Query ParaphrasingIn Information Retrieval, research on paraphrasingis dedicated to query paraphrasing which consists inidentifying semantically similar queries.
The over-all objective is to discover frequently asked ques-tions and popular topics (Wen et al, 2002) or sug-gest related queries to users (Sahami and Heilman,2006).
Traditional string similarity metrics are usu-ally deemed inefficient for such short text snip-pets and alternative similarity metrics have thereforebeen proposed.
For instance, Wen et al (2002) relyon user click logs, based on the idea that queries andquestions which result in identical document clicksare bound to be similar.2.3 Question ParaphrasingFollowing previous research in this domain, we de-fine question paraphrases as questions which haveall the following properties: (a) they have identi-cal meanings, (b) they have the same answers, and(c) they present alternate wordings.
Question para-45phrases differ from sentence paraphrases by the ad-ditional condition (b).
This definition encompassesthe following questions, taken from the WikiAn-swers web site: How many ounces are there in apound?, What?s the number of ounces per pound?,How many oz.
in a lb.
?Question paraphrases share some properties bothwith declarative sentence paraphrases and queryparaphrases.
On the one hand, questions are com-plete sentences which differ from declarative sen-tences by their specific word order and the presenceof question words and a question focus.
On the otherhand, questions are usually associated with answers,which makes them similar to queries associated withdocuments.
Accordingly, research on the identifi-cation of question paraphrases in Q&A repositoriesbuilds upon both sentence and query paraphrasing.Zhao et al (2007) propose to utilise user clicklogs from the Encarta web site to identify questionparaphrases.
Jeon et al (2005) employ a relatedmethod, in that they identify similar answers in theNaver Question and Answer database to retrieve se-mantically similar questions, while Jijkoun and deRijke (2005) include the answer in the retrieval pro-cess to return a ranked list of QA pairs in responseto a user?s question.
Lytinen and Tomuro (2002)suggest yet another feature to identify question para-phrases, namely question type similarity, which con-sists in determining a question?s category in order tomatch questions only if they belong to the same cat-egory.Our focus is on question paraphrase identificationin social Q&A sites.
Previous research was mostlybased on question paraphrase identification in FAQs(Lytinen and Tomuro, 2002; Tomuro and Lytinen,2004; Jijkoun and de Rijke, 2005).
In FAQs, ques-tions and answers are edited by expert informationsuppliers, which guarantees stricter conformance toconventional writing rules.
In social Q&A sites,questions and answers are written by users and mayhence be error-prone.
Question paraphrase identi-fication in social Q&A sites has been little investi-gated.
To our knowledge, only Jeon et al (2005)have used data from a Q&A site, namely the KoreanNaver portal, to find semantically similar questions.Our work is related to the latter since it employs asimilar dataset, yet in English and from a differentsocial Q&A site.3 Question and Answer Repositories3.1 Properties of Q&A RepositoriesQuestion and answer repositories have existed for along time on the Internet.
Their form has evolvedfrom Frequently Asked Questions (FAQs) to Ask-an-expert services (Baram-Tsabari et al, 2006) and,even more recently, social Q&A sites.
The latest,which include web sites such as Yahoo!
Answersand AnswerBag, provide portals where users canask their own questions as well as answer ques-tions from other users.
Social Q&A sites are in-creasingly popular.
For instance, in December 2006Yahoo!
Answers was the second-most visited edu-cation/reference site on the Internet after Wikipediaaccording to the Hitwise company (Prescott, 2006).Even more strikingly, the Q&A portal Naver is theleader of Internet search in South Korea, well aheadof Google (Sang-Hun, 2007).Several factors might explain the success of socialQ&A sites:?
they provide answers to questions which aredifficult to answer with a traditionalWeb searchor using static reference sites like Wikipedia,for instance opinions or advice about a specificfamily situation or a relationship problem;?
questions can be asked anonymously;?
users do not have to browse a list of documentsbut rather obtain a complete answer;?
the answers are almost instantaneous and nu-merous, due to the large number of users.Social Q&A sites record the questions and theiranswers online, and thus constitute a formidablerepository of collective intelligence, including an-swers to complex questions.
Moreover, they makeit possible for learners to reach other people world-wide.
The relevance of social Q&A sites for learninghas been little investigated.
To our knowledge, therehas been only one study which has shown that Ko-rean users of the Naver Question and Answer plat-form consider that social Q&A sites can satisfacto-rily and reliably support learning (Lee, 2006).3.2 WikiAnswersFor our experiments we collected a dataset of ques-tions and their paraphrases from the WikiAnswers46web site.
WikiAnswers1 is a social Q&A site similarto Yahoo!
Answers and AnswerBag.
As of Febru-ary 2008, it contained 1,807,600 questions, sorted in2,404 categories (Answers Corporation, 2008).Compared with its competitors, the main origi-nality of WikiAnswers is that it relies on the wikitechnology used in Wikipedia, which means that an-swers can be edited and improved over time by allcontributors.
Moreover, the Answers Corporation,which owns the WikiAnswers site, explicitly tar-gets educational uses and even provides an educatortoolkit.2 Another interesting property of WikiAn-swers is that users might manually tag question re-formulations in order to prevent the duplication ofquestions asking the same thing in a different way.When a user enters a question which is not alreadypart of the question repository, the web site dis-plays a list of questions already existing on the siteand similar to the one just asked by the user.
Theuser may then freely select the question which para-phrases her question, if available, or choose to viewone of the proposed alternatives without labelling itas a paraphrase.
The user-labelled question refor-mulations are stored in order to retrieve the sameanswer when the question rephrasing is asked again.The wiki principle holds for the stored reformula-tions too, since they can subsequently be edited byother users if they consider that they correspond toanother existing question or actually ask an entirelynew question.
It should be noted that contributorsget not reward in terms of trust points for providingor editing alternate wordings for questions.We use the wealth of question paraphrases avail-able on the WikiAnswers website as the so calleduser generated gold standard in our question para-phrasing experiments.
User generated gold stan-dards have been increasingly used in recent yearsfor research evaluation purposes, since they can beeasily created from user annotated content.
Forinstance, Mihalcea and Csomai (2007) use manu-ally annotated keywords (links to other articles) inWikipedia articles to evaluate their automatic key-word extraction and word sense disambiguation al-gorithms.
Similarly, quality assessments providedby users in social media have been used as gold1http://wiki.answers.com/2http://educator.answers.com/standards for the automatic assessment of post qual-ity in forum discussions (Weimer et al, 2007).
Itshould however be kept in mind that user generatedgold standards are not perfect, as already noticed by(Mihalcea and Csomai, 2007), and thus constitute atrade-off solution.For the experiments described hereafter, we ran-domly extracted a collection of 1,000 questionsalong with their paraphrases (totalling 7,434 ques-tion paraphrases) from 100 randomly selected FAQfiles in the Education category of the WikiAnswersweb site.
In what follows, the corpus of 1,000 ques-tions is called the target questions collection, whilethe 7,434 question paraphrases constitute the inputquestions collection.
The objective of the task is toretrieve the corresponding target question for eachinput question.
The target question selected is theone which maximises the question similarity value(see section 4.2).4 MethodIn order to rate the similarity of input and targetquestions, we have first pre-processed both the in-put and target questions and then experimented withseveral question similarity measures.4.1 Pre-processingWe employ the following steps in pre-processing thequestions:Stop words elimination however, we keep ques-tion words such as how, why, what, etc.
since thesemake it possible to implicitly identify the questiontype (Lytinen and Tomuro, 2002; Jijkoun and de Ri-jke, 2005)Stemming using the Porter Stemmer3Lemmatisation using the TreeTagger4Spelling correction using a statistical systembased on language modelling (Norvig, 2007).53http://snowball.tartarus.org/4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/5We used a Java implementation of the system, jSpell-Correct available at http://developer.gauner.org/jspellcorrect/, trained with the default English trainingdata, to which we appended the myspell English dictionaries.47Stop words were eliminated in all the experi-mental settings, while stemming and lemmatisationwere optionally performed to evaluate the effectsof these pre-processing steps on the identificationof question paraphrases.
We added spelling correc-tion to the conventional pre-processing steps, sincewe target paraphrasing of questions which oftencontain spelling errors, such as When was indoorpluming invented?
or What is the largest counteryin the western Hemipher?
Other related endeav-ours at retrieving question paraphrases have identi-fied spelling mistakes in questions as a significantsource of errors in the retrieval process, but have notattempted to solve this problem (Jijkoun and de Ri-jke, 2005; Zhao et al, 2007).4.2 Question Similarity MeasuresWe have experimented with several kinds of ques-tion similarity measures, belonging to two differentfamilies of measures: string similarity measures andvector space measures.4.3 String Similarity MeasuresBasic string similarity measures compare the wordscontained in the questions without taking word fre-quency into account.Matching coefficient The matching coefficient oftwo questions q1 and q2 represented by the set ofdistinct words Q1 and Q2 they contain is computedas follows (Manning and Schu?tze, 1999):matching coefficient =| Q1 ?
Q2 |Overlap coefficient The overlap coefficient iscomputed according to the following formula (Man-ning and Schu?tze, 1999):overlap coefficient = | Q1 ?
Q2 |min(| Q1 |, | Q2 |)Normalised Edit Distance The edit distance oftwo questions is the number of words that need to besubstituted, inserted, or deleted, to transform q1 intoq2.
In order to be able to compare the edit distancewith the other metrics, we have used the follow-ing formula (Wen et al, 2002) which normalises theminimum edit distance by the length of the longestquestion and transforms it into a similarity metric:normalised edit distance = 1?
edit dist(q1, q2)max(| q1 |, | q2 |)Word Ngram Overlap This metric compares theword n-grams in both questions:ngram overlap = 1NN?n=1| Gn(q1) ?
Gn(q2) |min(| Gn(q1) |, | Gn(q2) |)where Gn(q) is the set of n-grams of length n inquestion q and N usually equals 4 (Barzilay andLee, 2003; Cordeiro et al, 2007).4.4 Vector Space Based MeasuresVector space measures represent questions as real-valued vectors by taking word frequency into ac-count.Term Vector Similarity Questions are repre-sented as term vectors V1 and V2.
The feature val-ues of the vectors are the tf.idf scores of the corre-sponding terms:tf.idf = (1 + log(tf)) ?
log N + 1dfwhere tf is equal to the frequency of the term inthe question, N is the number of target questionsand df is the number of target questions in whichthe term occurs, computed by considering the in-put question as part of the target questions collection(Lytinen and Tomuro, 2002).The similarity of an input question vector and atarget question vector is determined by the cosinecoefficient:cosine coefficient = V1 ?
V2| V1 | ?
| V2 |Lucene?s Extended Boolean Model The prob-lem of question paraphrase identification can becast as an Information Retrieval problem, since inreal-world applications the user posts a questionand the system returns the best matching questionsfrom its database.
We have therefore tested the re-sults obtained using an Information Retrieval sys-tem, namely Lucene6, which combines the VectorSpace Model and the Boolean model.
Lucene hasalready been successfully used by Jijkoun and de Ri-jke (2005) to retrieve answers from FAQ web pagesby combining several fields: question text, answertext and the whole FAQ page.
The target questionsare indexed as documents and retrieved by trans-forming the input questions into queries.6http://lucene.apache.org/java/docs/48T-SWT-SW+SCS-SWS-SW+SCL-SWL-SWPreprocessing5060708090100AccuracyT-SWT-SW+SCS-SWS-SW+SCL-SWL-SWPreprocessing0.50.60.70.80.91.0MRRMatching coefficientOverlap coefficientNormalised edit distanceNgram OverlapTerm vector similarityLuceneFigure 1: Accuracy (%) and Mean Reciprocal Rank obtained for different question similarity measures and pre-processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction(+SC).5 Evaluation and Experimental Results5.1 Evaluation MeasuresWe use the following evaluation measures for evalu-ating the results:Mean Reciprocal Rank For a question, the recip-rocal rank RR is 1r where r is the rank of the correcttarget question, or zero if the target question was notfound.
The Mean Reciprocal Rank (MRR) is themean of the reciprocal ranks over all the input ques-tions.Accuracy We define accuracy as Success@1,which is the percentage of input questions for whichthe correct target question has been retrieved at rank1.5.2 Experimental ResultsFigure 1 displays the accuracy and the mean recip-rocal ranks obtained with the different question sim-ilarity measures and pre-processing strategies.
Ascould be expected, vector space based similaritymeasures are consistently more accurate than sim-ple string similarity measures.
Moreover, both theaccuracy and the MRR are rather high for vectorspace metrics (accuracy around 80-85% and MRRaround 0.85-0.9), which shows that good results canbe obtained with these retrieval mechanisms.
Addi-tional pre-processing, i.e.
stemming, lemmatisationand spelling correction, does not ameliorate the to-kens minus stop words (T -SW) baseline.5.3 Detailed Error AnalysisStemming and lemmatisation Morphologicalpre-processing brings about mitigated improve-ments over the tokens-only baseline.
On the onehand, it improves paraphrase retrieval for ques-tions containing morphological variants of the samewords such asWhat are analogies for mitochondria?and What is an analogy for mitochondrion?
On theother hand, it also leads to false positives, such hasHow was calculus started?, stemmed as How wascalculus start?
and lemmatised as How be calculusstart?, which is mapped by Lucene to the questionHow could you start your MA English studies?instead of Who developed calculus?.
The negativeeffect of stemming has already been identified by(Jijkoun and de Rijke, 2005) and our results areconsistent with this previous finding.Spelling correction We expected that spellingcorrection would have a positive impact on the re-sults.
There are indeed cases when spelling correc-tion helps.
For instance, given the question How doyou become an anestesiologist?, it is impossible toretrieve the target question How many years of med-ical school do you need to be an anesthesiolgist?without spelling correction since anesthesiologist isill-spelled both in the paraphrase and the target ques-tion.49Lemma + Stop words + Spelling correctionLemma + Stop wordsStem + Stop words + Spelling correctionStem + Stop wordsToken + Stop words + Spelling correctionToken + Stop words(a)LuceneTerm Vector similarityWord Ngram overlapOverlap coefficientMatching coefficientEdit distance(b)Figure 2: Comparison of the different pre-processing strategies 2(a) and methods 2(b) for 50 input questions.
For thepre-processing comparison, the Lucene retrieval method has been used, while the methods have been compared usingbaseline pre-processing (tokens minus stop words).
A filled square indicates that the target question has been retrievedat rank 1, while a blank square indicates that the target question has not been retrieved at rank 1.There are however cases when spelling correctioninduces worse results, since it is accurate in only ap-proximately 70% of the cases (Norvig, 2007).
Amajor source of errors lies in named entities and ab-breviations, which are recognised as spelling errorswhen they are not part of the training lexicon.
Forinstance, the question What are the GRE score re-quired to get into top100 US universities?
(whereGRE stands for Graduate Record Examination) isbadly corrected as What are the are score requiredto get into top100 US universities?.Spelling correction also induces an unexpectedside effect, when the spelling error does not affectthe question?s focus.
For instance, consider the fol-lowing question, with a spelling error: What eventsoccured in 1919?, which gets correctly mapped tothe target question What important events happenedin 1919?
by Lucene; however, after spelling correc-tion (What events occurred in 1919?
), it has a big-ger overlap with an entirely different question: Whatevents occurred in colonial South Carolina 1674-1775?.The latter example also points at another limita-tion of the evaluated methods, which do not identifysemantically similar words, such as occurred andhappened.Errors in the gold standard Some errors can ac-tually be traced back to inaccuracies in the gold stan-dard: some question pairs which have been flaggedas paraphrases by the WikiAnswers contributors areactually distantly related.
For instance, the questionsWhen was the first painting made?
and Where didleanardo da vinci live?
are marked as reformula-tions of the question What is the secret about monalisa?
Though these questions all share a commonbroad topic, they cannot be considered as relevantparaphrases.We can deduce several possible improvementsfrom what precedes.
First, named entities and ab-breviations play an important role in questions andshould therefore be identified and treated differentlyfrom other kinds of tokens.
This could be achievedby using a named entity recognition componentduring pre-processing and then assigning a higherweight to named entities in the retrieval process.This should also improve the results of spelling cor-rection since named entities and abbreviations couldbe excluded from the correction.
Second, seman-tic errors could be dealt with by using a semanticsimilarity metric similar to those used in declarativesentence paraphrase identification (Li et al, 2006;Mihalcea et al, 2006; Islam and Inkpen, 2007).5.4 Comparison and Combination of theMethodsIn a second part of the experiment, we investigatedwhether the evaluated methods display independent50error patterns, as suggested by our detailed resultsanalysis.
Figure 2 confirms that the pre-processingtechniques as well as the methods employed resultin dissimilar error patterns.
We therefore combinedseveral methods and pre-processing techniques inorder to verify if we could improve accuracy.We obtained the best results by performing a ma-jority vote combination of the following methodsand pre-processing strategies: Lucene, Term VectorSimilarity with stemming and Ngram Overlap withspelling correction.
The combination yielded an ac-curacy of 88.3%, that is 0.9% over the best Luceneresults with an accuracy of 87.4%.6 Conclusion and OutlookIn this paper, we have shown that it is feasible to an-swer learners?
questions by retrieving question para-phrases from social Q&A sites.
As a first step to-wards this objective, we investigated several ques-tion similarity metrics and pre-processing strategies,using WikiAnswers as input data and user generatedgold standard.
The approach is however not limitedto this dataset and can be easily applied to retrievequestion paraphrases from other social Q&A sites.We also performed an extended failure analysiswhich provided useful insights on how results couldbe further improved by performing named entityanalysis and using semantic similarity metrics.Another important challenge in using social Q&Asites for educational purposes lies in the quality ofthe answers retrieved from such sites.
Previous re-search on the identification of high quality content insocial Q&A sites has defined answer quality in termsof correctness, well-formedness, readability, objec-tivity, relevance, utility and interestingness (Jeon etal., 2006; Agichtein et al, 2008).
It is obvious thatall these elements play an important role in the ac-ceptance of the answers by learners.
We thereforeplan to integrate quality measures in the retrievalprocess and to perform evaluations in a real educa-tional setting.AcknowledgmentsThis work was supported by the Emmy Noether Pro-gramme of the German Research Foundation (DFG)under grant No.
GU 798/3-1.ReferencesEugene Agichtein, Carlos Castillo, Debora Donato, Aris-tides Gionis, and Gilad Mishne.
2008.
Findinghigh-quality content in social media.
In WSDM ?08:Proceedings of the international conference on Websearch and web data mining, pages 183?194.Answers Corporation.
2008.
WikiAnswers Jour-nalist Quick Guide.
[Online; visited March4, 2008].
http://site.wikianswers.com/resources/WikiAnswers_1-pager.pdf.Ayelet Baram-Tsabari, Ricky J. Sethi, Lynn Bry, andAnat Yarden.
2006.
Using questions sent to an Ask-A-Scientist site to identify children?s interests in science.Science Education, 90(6):1050?1072.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: an unsupervised approach using multiple-sequence alignment.
In Proceedings of NAACL-HLT2003, pages 16?23.
Association for ComputationalLinguistics.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In ACL?01: Proceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics, pages 50?57.
Association for Computational Linguistics.Joa?o Cordeiro, Gae?l Dias, and Pavel Brazdil.
2007.Learning Paraphrases from WNS Corpora.
In DavidWilson and Geoff Sutcliffe, editors, Proceedings ofthe Twentieth International Florida Artificial Intelli-gence Research Society Conference (FLAIRS), pages193?198, Key West, Florida, USA, May 7-9.
AAAIPress.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In COL-ING ?04: Proceedings of the 20th international con-ference on Computational Linguistics, pages 350?356.Association for Computational Linguistics.Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.2006.
An Intelligent Discussion-Bot for AnsweringStudent Queries in Threaded Discussions.
In Proceed-ings of the 11th international conference on Intelligentuser interfaces (IUI?06), pages 171?177.Aminul Islam and Diana Inkpen.
2007.
Semantic Sim-ilarity of Short Texts.
In Proceedings of the Interna-tional Conference on Recent Advances in Natural Lan-guage Processing (RANLP 2007), Borovets, Bulgaria,September.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and answerarchives.
In CIKM ?05: Proceedings of the 14th ACMinternational conference on Information and knowl-edge management, pages 84?90.51Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and SoyeonPark.
2006.
A framework to predict the quality ofanswers with non-textual features.
In SIGIR ?06: Pro-ceedings of the 29th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 228?235.Valentin Jijkoun and Maarten de Rijke.
2005.
Retrievinganswers from frequently asked questions pages on theweb.
In CIKM ?05: Proceedings of the 14th ACM in-ternational conference on Information and knowledgemanagement, pages 76?83.Yu Sun Lee.
2006.
Toward a New Knowledge Shar-ing Community: Collective Intelligence and Learn-ing through Web-Portal-Based Question-Answer Ser-vices.
Masters of arts in communication, culture &technology, Faculty of the Graduate School of Artsand Sciences of Georgetown University, May.
[On-line; visited February 15, 2008], http://hdl.handle.net/1961/3701.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence Simi-larity Based on Semantic Nets and Corpus Statistics.IEEE Transactions on Knowledge and Data Engineer-ing, 18(8):1138?1150.Steven L. Lytinen and Noriko Tomuro.
2002.
The Useof Question Types to Match Questions in FAQFinder.In Proceedings of the 2002 AAAI Spring Symposiumon Mining Answers from Texts and Knowledge Bases,pages 46?53.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press, Cambridge, Massachusetts.RadaMihalcea and Andras Csomai.
2007.
Wikify!
: link-ing documents to encyclopedic knowledge.
In CIKM?07: Proceedings of the sixteenth ACM conference oninformation and knowledge management, pages 233?242.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and Knowledge-based Measuresof Text Semantic Similarity.
In Proceedings of AAAI2006, Boston, July.Peter Norvig.
2007.
How to Write a Spelling Correc-tor.
[Online; visited February 22, 2008].
http://norvig.com/spell-correct.html.Lee Ann Prescott.
2006.
Yahoo!
Answers Cap-tures 96% of Q and A Market Share.
Hit-wise Intelligence [Online; visited February26, 2008].
http://weblogs.hitwise.com/leeann-prescott/2006/12/yahoo_answers_captures_96_of_q.html.Mehran Sahami and Timothy D. Heilman.
2006.
A web-based kernel function for measuring the similarity ofshort text snippets.
In WWW ?06: Proceedings ofthe 15th international conference on World Wide Web,pages 377?386.Choe Sang-Hun.
2007.
To outdo Google,Naver taps into Korea?s collective wis-dom.
International Herald Tribune, July 4.http://www.iht.com/articles/2007/07/04/technology/naver.php.Noriko Tomuro and Steven Lytinen.
2004.
RetrievalModels and Q&A Learning with FAQ Files.
InMark T. Maybury, editor, New Directions in QuestionAnswering, pages 183?194.
AAAI Press.Markus Weimer, Iryna Gurevych, and Max Mu?hlha?user.2007.
Automatically Assessing the Post Quality inOnline Discussions on Software.
In Proceedings of theDemo and Poster Sessions of the 45th Annual Meet-ing of the Association for Computational Linguistics,pages 125?128, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.2002.
Query clustering using user logs.
ACM Trans.Inf.
Syst., 20(1):59?81.Shiqi Zhao, Ming Zhou, and Ting Liu.
2007.
Learn-ing Question Paraphrases for QA from Encarta Logs.In Proceedings of the 20th International Joint Confer-ence on Artificial Intelligence, pages 1795?1801, Hy-derabad, India, January 6-12.52
