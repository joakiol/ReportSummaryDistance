Proceedings of the SIGDIAL 2013 Conference, pages 462?466,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsEngineering Statistical Dialog State Trackers: A Case Study on DSTCDaejoong Kim, Jaedeug Choi, Kee-Eung KimDepartment of Computer Science, KAISTSouth Korea{djkim, jdchoi, kekim}@ai.kaist.ac.krJungsu Lee, Jinho SohnLG ElectronicsSouth Korea{jungsu.lee, jinho.sohn}@lge.comAbstractWe describe our experience with engineer-ing the dialog state tracker for the firstDialog State Tracking Challenge (DSTC).Dialog trackers are one of the essentialcomponents of dialog systems which areused to infer the true user goal from thespeech processing results.
We explain themain parts of our tracker: the observationmodel, the belief refinement model, andthe belief transformation model.
We alsoreport experimental results on a numberof approaches to the models, and comparethe overall performance of our tracker toother submitted trackers.
An extended ver-sion of this paper is available as a technicalreport (Kim et al 2013).1 IntroductionIn spoken dialog systems (SDSs), one of the mainchallenges is to identify the user goal from her ut-terances.
The significance of accurately identify-ing the user goal, referred to as dialog state track-ing, has emerged from the need for SDSs to berobust to inevitable errors in the spoken languageunderstanding (SLU).A number of studies have been conducted totrack the dialog state through multiple dialog turnsusing a probabilistic framework, treating SLU re-sults as noisy observations and maintaining prob-ability distribution (i.e., belief) on user goals (Bo-hus and Rudnicky, 2006; Mehta et al 2010; Royet al 2000; Williams and Young, 2007; Thomsonand Young, 2010; Kim et al 2011).In this paper, we share our experience andlessons learned from developing the dialog statetracker that participated in the first Dialog StateTracking Challenge (DSTC) (Williams et al2013).
Our tracker is based on the belief up-date in the POMDP framework (Kaelbling et al1998), particularly the hidden information state(HIS) model (Young et al 2010) and the partitionrecombination method (Williams, 2010).2 Dialog State TrackingOur tracker mainly follows the belief update inHIS-POMDP (Young et al 2010).
The SDS ex-ecutes system action a, and the user with goalg responds to the system with utterance u. TheSLU processes the utterance and generates the re-sult as anN -best list o = [?u?1, f1?, .
.
.
, ?u?N , fN ?
]of the hypothesized user utterance u?i and its as-sociated confidence score fi.
Because the SLUis not perfect, the system maintains a probabilitydistribution over user goals, called a belief.
In ad-dition, the system groups user goals into equiva-lence classes and assigns a single probability foreach equivalence class since the number of usergoals is often too large to perform individual be-lief updates for all possible user goals.
The equiv-alence classes are called partitions and denoted as?.
Hence, given the current belief b, system actiona, and recognized N -best list o, the dialog statetracker updates the belief b?
over partitions as fol-lows:b?(??)
?
?uPr(o|u) Pr(u|?
?, a) Pr(??|?)b(?
)(1)where Pr(o|u) is the observation model,Pr(u|?, a) is the user utterance model, Pr(??|?
)is the belief refinement model.2.1 Observation ModelThe observation model Pr(o|u) is the probabilitythat the SLU produces the N -best list o when theuser utterance is u.
We experimented with the fol-lowing three models for the observation model.Confidence score model: as in HIS-POMDP,this model assumes that the confidence score fiobtained from the SLU is exactly the probability462of generating the hypothesized user utterance u?i.Hence, fi = Pr(u?i, fi|u).Histogrammodel: this model estimates a func-tion that maps the confidence score to the proba-bility of correctness.
We constructed a histogramof confidence scores from the training datasetsto obtain the empirical probability Pr(cor(fi)) ofwhether the entry associated with confidence scorefi is a correct hypothesis or not.Generative model: this model is a simplifiedversion of a generative model in (Williams, 2008)that only uses confidence score: Pr(u?i, fi|u) =Pr(cor(i)) Pr(fi|cor(i)) where Pr(cor(i)) is theprobability of the i-th entry being a correct hy-pothesis and Pr(fi|cor(i)) is the probability of thei-th entry having confidence score fi when it is acorrect hypothesis.2.2 User Utterance ModelThe user utterance model Pr(u|?, a) indicateshow the user responds to the system action awhen the user goal is in ?.
We adopted the HIS-POMDP user utterance model, consisting of a bi-gram model and an item model.
The details aredescribed in (Kim et al 2013).2.3 Belief Refinement ModelGiven the SLU result u?i and the system actiona, the partition ?
is split into ?
?i with probabil-ity Pr(??i|?)
and ?
?
?
?i with probability Pr(?
???i|?).
The belief refinement model Pr(??i|?)
canbe seen as the proportion of the belief that is car-ried from ?
to ??i.
This probability can be definedby the following models:Empirical model: we count n(?)
from thetraining datasets, which is the number of usergoals that are consistent with partition ?.
Theprobability is then modeled as Pr(??i|?)
=n(??i)n(?
)if n(?)
> 0 and Pr(??i|?)
= 0 otherwise.Word-match model: this model extends theempirical model by using the domain knowledgewhen the SLU result u?i does not appear in thetraining datasets.
We calculated how many wordsw ?
W in the user utterance u?i were included ina bus timetable D. The model is thus defined asPr(??i|?)
=n(??i)n(?)
if n(?
?i) > 0 and Pr(??i|?)
=?|W |?w?W ?
(w ?
D) otherwise.
?
is the indica-tor function (?
(x) = 1 if x holds and ?
(x) = 0otherwise) and ?
is the parameter estimated viacross-validation.Mixture model: this model mixes the empiri-cal model with a uniform probability, defined asPr(??i|?)
= ?
1nG + (1 ?
?)n(??i)n(?)
if n(?
?i) > 0 andPr(??i|?)
= 1nG otherwise.
nG is the number of allpossible user goals which is treated as the param-eter of the model and found via cross-validation,together with the mixing parameter ?
?
[0, 1].2.4 Belief Transformation ModelThe belief update described above pro-duces the M -best hypotheses of user goals[?g?1, b(g?1)?, .
.
.
, ?g?M , b(g?M )?]
in each dialog turn,which consists of M most likely user goal hy-potheses g?i and their associated beliefs b(g?i).
Thelast hypothesis g?M is reserved as the null hypoth-esis ?
with the belief b(?)
= 1 ?
?M?1i=1 b(g?i),which represents that the user goal is not knownup to the current dialog turn.One of the problems with the belief update isthat the null hypothesis often remains as the mostprobable hypothesis even when the SLU resultcontains the correct user utterance with a high con-fidence score.
This is because an atomic hypothe-sis has a very small prior probability.To overcome this problem, we added a post-processing step which transforms each belief b(hi)to the final confidence score si.Threshold model: this model ensures that thetop hypothesis has confidence score ?
when a be-lief of the hypothesis is greater than a threshold ?.The final output list is [?h?, s?
?, ?
?, 1?s??]
whereh?
= argmaxh?
{g?1,...,g?M?1} b(h) ands?
={?, if b(h?)
> ?b(h?
), otherwise.
(2)Full-list regression model: this model esti-mates the probability that each hypothesis is cor-rect via casting the task as regression.
Themodel uses two logistic regression functions F?and Fh.
F?
predicts the probability of correct-ness for the null hypothesis ?
using the sin-gle input feature ??
= b(?).
Likewise, Fhpredicts the probability of correctness for non-null hypotheses hi using the input feature ?i =b(hi).
The model generates the final outputlist [?h1, s1?, .
.
.
, ?hM?1, sM?1?, ?
?, sM ?]
wherehi = g?i andsi =???F?
(?i)PM?1j=1 Fh(?j)+F?(??
), if i = MFh(?i)PM?1j=1 Fh(?j)+F?(??
), otherwise.
(3)463Rank regression model: this model works ina similar way as in the full-link regression model,except that it uses a single logistic regression func-tion Fr for both the non-null and null hypothe-ses, and takes the rank value of the hypothesesas an additional input feature.
The final out-put list is [?h1, s1?, .
.
.
, ?hM?1, sM?1?, ?
?, sM ?
]where hi = g?i andsi = Fr(?i)PMj=1 Fr(?j).
(4)3 Experimental SetupIn the experiments, we used three labeled train-ing datasets (train1a, train2, train3) and three testdatasets (test1, test2, test3) used in DSTC.
Therewas an additional test dataset (test4), which wedecided not to include in the experiments sincewe found that a significant number of labels weremissing or incorrect.We measured the tracker performance accord-ing to the following evaluation metrics used inDSTC1: accuracy (acc) measures the rate of themost likely hypothesis h1 being correct, averagescore (avgp) measures the average of scores as-signed to the correct hypotheses, l2 norm mea-sures the Euclidean distance between the vectorof scores from the tracker and the binary vectorwith 1 in the position of the correct hypotheses,and 0 elsewhere, mean reciprocal rank (mrr)measures the average of 1/R, where R is theminimum rank of the correct hypothesis, ROCequal error rate (eer) is the sum of false accept(FA) and false reject (FR) rates when FA rate=FRrate, andROC.
{v1,v2}.P measures correct accept(CA) rate when there are at most P% false accept(FA) rate2.4 Results and AnalysesSince there are multiple slots to track in the dialogdomain, we report the average performance overthe ?marginal?
slots including the ?joint?
slot thatassigns the values to all slots.4.1 Observation ModelTbl.
1 shows the cross-validation results of thethree observation models.
In train1a and train2, nomodel had a clear advantage to others, whereas in1http://research.microsoft.com/apps/pubs/?id=1690242There are two types of ROC measured in DSTC depend-ing on how CA and FA rates are calculated.
The detailed dis-cussion is provided in the longer version of the paper (Kim etal., 2013).Table 1: Evaluation of observation models.Train1a Train2 Train3Conf Hist Gen Conf Hist Gen Conf Hist Genaccuracy 0.81 0.82 0.82 0.84 0.86 0.85 0.90 0.89 0.88avgp 0.77 0.78 0.78 0.81 0.82 0.82 0.81 0.79 0.77l2 0.31 0.30 0.30 0.26 0.25 0.25 0.25 0.27 0.30mrr 0.87 0.87 0.88 0.89 0.89 0.89 0.94 0.93 0.92roc.v1.05 0.69 0.70 0.70 0.73 0.74 0.74 0.82 0.80 0.79roc.v1.10 0.74 0.75 0.75 0.78 0.80 0.80 0.87 0.85 0.83roc.v1.20 0.78 0.79 0.79 0.83 0.84 0.84 0.89 0.87 0.85roc.v1.eer 0.14 0.14 0.14 0.12 0.13 0.13 0.10 0.11 0.12roc.v2.05 0.34 0.34 0.34 0.24 0.15 0.23 0.52 0.54 0.52roc.v2.10 0.54 0.46 0.46 0.33 0.26 0.25 0.71 0.67 0.70roc.v2.20 0.70 0.70 0.69 0.43 0.41 0.41 0.83 0.78 0.80Table 2: Evaluation of belief refinement models.Train1a Train2 Train3Emp WordMix Emp WordMix Emp WordMixaccuracy 0.75 0.77 0.81 0.80 0.84 0.84 0.71 0.88 0.90avgp 0.75 0.76 0.77 0.78 0.80 0.81 0.68 0.80 0.81l2 0.34 0.34 0.31 0.31 0.27 0.26 0.42 0.26 0.25mrr 0.83 0.85 0.87 0.86 0.89 0.89 0.82 0.93 0.94roc.v1.05 0.66 0.68 0.69 0.64 0.68 0.73 0.58 0.78 0.82roc.v1.10 0.69 0.71 0.74 0.73 0.78 0.78 0.65 0.83 0.87roc.v1.20 0.73 0.74 0.78 0.77 0.82 0.83 0.68 0.86 0.89roc.v1.eer 0.22 0.13 0.14 0.13 0.13 0.12 0.13 0.11 0.10roc.v2.05 0.34 0.24 0.34 0.30 0.24 0.24 0.61 0.51 0.52roc.v2.10 0.47 0.38 0.54 0.42 0.26 0.33 0.64 0.67 0.71roc.v2.20 0.72 0.60 0.70 0.56 0.37 0.43 0.72 0.77 0.83train3, the confidence score model outperformedothers.
Further analyses revealed that the confi-dence scores from the SLU results were not suf-ficiently indicative of the SLU accuracy in train1aand train2.
The histogram and the generative mod-els are expected to perform at least as well as theconfidence score model in train3, but they didn?tin the experiments.
We suspect that this is due tothe naive binning strategy we used to model theprobability distribution.4.2 Belief Refinement ModelAs shown in Tbl.
2, the mixture model outper-formed others throughout the metrics.
It evenoutperforms the word-match model which tries toleverage the domain knowledge to handle noveluser goals.
This implies that, unless the domainknowledge is used properly, simply taking themixture with the uniform distribution yields a suf-ficient level of performance.4.3 Belief Transformation ModelTbl.
3 summarizes the performances of the belieftransformation models.
All three models outper-formed the pure belief update, although not shown464Table 3: Evaluation of belief transform models.Train1a Train2 Train3Thre Full Rank Thre Full Rank Thre Full Rankaccuracy 0.81 0.81 0.81 0.83 0.84 0.85 0.89 0.90 0.90avgp 0.80 0.77 0.77 0.82 0.81 0.81 0.85 0.81 0.78l2 0.28 0.31 0.32 0.25 0.26 0.26 0.22 0.25 0.28mrr 0.84 0.87 0.87 0.86 0.89 0.89 0.91 0.94 0.92roc.v1.05 0.66 0.69 0.69 0.65 0.73 0.72 0.45 0.82 0.80roc.v1.10 0.71 0.74 0.75 0.69 0.78 0.79 0.68 0.87 0.86roc.v1.20 0.71 0.78 0.78 0.74 0.83 0.83 0.79 0.89 0.89roc.v1.eer 0.18 0.14 0.14 0.21 0.12 0.12 0.49 0.10 0.09roc.v2.05 0.22 0.34 0.34 0.20 0.24 0.24 0.42 0.52 0.48roc.v2.10 0.41 0.54 0.52 0.22 0.33 0.33 0.42 0.71 0.56roc.v2.20 0.64 0.70 0.71 0.30 0.43 0.49 0.43 0.83 0.75in the table.
The full-list and the rank regres-sion models show a similar level of performanceimprovement.
This is a naturally expected resultsince they use regression to convert the beliefs tofinal confidence scores, as an attempt to compen-sate for the errors incurred by approximations andassumptions made in the observation and belief re-finement models.4.4 DSTC ResultIn order to compare our tracker with others par-ticipated in DSTC, we chose tracker43 as the mosteffective one among our 5 submitted trackers sinceit achieved the top scores in the largest num-ber of evaluation metrics.
In the same way, weselected tracker2 for team3, tracker3 for team6,tracker3 for team8, and tracker1 for the rest of theteams.
The results of each team are presented inTbl.
4.
The baseline tracker is included as a ref-erence, which simply outputs the hypothesis withthe largest SLU confidence score in the N -bestlist.Compared to other teams, our tracker showedstrong performance in acc, avgp, l2 and mrr.
Adetailed discussion on the results is provided in thelonger version of the paper (Kim et al 2013).5 ConclusionIn this paper, we described our experience withengineering a statistical dialog state tracker whileparticipating in DSTC.
Our engineering effort wasfocused on improving three important models inthe tracker: the observation, the belief refine-ment, and the belief transformation models.
Us-ing standard statistical techniques, we were able3The tracker4 used the confidence score model, the mix-ture model and the rank regression model.Table 4: Results of the trackers.
The bold facedenotes top 3 scores in each evaluation metric.
T9is our tracker.BaseT1 T2 T3 T4 T5 T6 T7 T8 T9Test 1accuracy 0.71 0.83 0.81 0.81 0.74 0.80 0.87 0.78 0.51 0.82avgp 0.73 0.77 0.77 0.81 0.74 0.79 0.82 0.76 0.49 0.79l2 0.38 0.32 0.32 0.27 0.37 0.30 0.25 0.34 0.72 0.29mrr 0.80 0.88 0.86 0.85 0.81 0.85 0.90 0.84 0.59 0.88roc.v1.05 0.62 0.72 0.67 0.60 0.20 0.71 0.76 0.65 0.20 0.72roc.v1.10 0.63 0.78 0.75 0.77 0.29 0.75 0.82 0.70 0.33 0.76roc.v1.20 0.67 0.82 0.79 0.79 0.53 0.78 0.85 0.76 0.35 0.79roc.v1.eer 0.24 0.13 0.25 0.24 0.74 0.12 0.12 0.15 0.52 0.14roc.v2.05 0.49 0.64 0.01 0.02 0.00 0.55 0.16 0.19 0.04 0.26roc.v2.10 0.69 0.71 0.14 0.03 0.00 0.68 0.39 0.35 0.05 0.47roc.v2.20 0.71 0.80 0.48 0.29 0.00 0.74 0.59 0.58 0.27 0.62Test 2accuracy 0.55 0.65 0.71 0.68 0.63 0.62 0.79 0.65 0.34 0.71avgp 0.57 0.55 0.63 0.68 0.63 0.62 0.71 0.65 0.29 0.65l2 0.60 0.63 0.50 0.45 0.52 0.54 0.39 0.49 1.00 0.48mrr 0.65 0.72 0.79 0.76 0.71 0.72 0.84 0.74 0.46 0.80roc.v1.05 0.43 0.49 0.52 0.45 0.16 0.48 0.66 0.48 0.04 0.49roc.v1.10 0.45 0.54 0.57 0.63 0.16 0.51 0.71 0.54 0.11 0.57roc.v1.20 0.48 0.59 0.64 0.64 0.27 0.54 0.76 0.60 0.26 0.63roc.v1.eer 0.19 0.20 0.39 0.14 0.63 0.21 0.16 0.19 0.36 0.22roc.v2.05 0.43 0.52 0.24 0.27 0.00 0.40 0.46 0.41 0.05 0.38roc.v2.10 0.47 0.60 0.40 0.37 0.00 0.62 0.53 0.47 0.17 0.41roc.v2.20 0.50 0.70 0.48 0.56 0.00 0.70 0.62 0.55 0.44 0.47Test 3accuracy 0.79 0.79 0.84 0.82 0.82 0.78 0.84 0.79 0.79 0.85avgp 0.75 0.72 0.76 0.79 0.78 0.70 0.75 0.75 0.76 0.74l2 0.35 0.37 0.32 0.29 0.30 0.40 0.33 0.34 0.32 0.34mrr 0.83 0.85 0.88 0.85 0.85 0.83 0.89 0.84 0.80 0.89roc.v1.05 0.56 0.65 0.68 0.72 0.70 0.62 0.69 0.70 0.33 0.74roc.v1.10 0.66 0.70 0.77 0.77 0.76 0.69 0.76 0.74 0.47 0.78roc.v1.20 0.74 0.76 0.82 0.80 0.80 0.74 0.81 0.77 0.61 0.82roc.v1.eer 0.19 0.16 0.15 0.27 0.12 0.17 0.15 0.12 0.34 0.13roc.v2.05 0.56 0.62 0.34 0.28 0.21 0.62 0.61 0.14 0.00 0.56roc.v2.10 0.59 0.71 0.48 0.37 0.52 0.66 0.66 0.42 0.00 0.67roc.v2.20 0.66 0.78 0.73 0.52 0.82 0.71 0.78 0.87 0.00 0.79to produce a tracker that performed competitivelyamong the participants.As for the future work, we plan to refine theuser utterance model for improving the perfor-mance of the tracker since there are a number ofuser utterances that are not handled by the cur-rent model.
We also plan to re-evaluate our trackerwith properly handling the joint slot, since the cur-rent tracker constructs models independently foreach marginal slot and then combines the resultsby simply multiplying the predicted scores.AcknowledgementThis work was supported by NRF of Korea(Grant# 2012-007881), and MKE/KEIT of Korea(IT R&D Program Contract# 10041678)465ReferencesDan Bohus and Alex Rudnicky.
2006.
A ?k hypothe-ses + other?
belief updating model.
In Proceedingsof the AAAI Workshop on Statistical and EmpiricalApproaches for Spoken Dialogue Systems.Leslie Pack Kaelbling, Michael L. Littman, and An-thony R. Cassandra.
1998.
Planning and acting inpartially observable stochastic domains.
ArtificialIntelligence, 101(1?2):99?134.Dongho Kim, Jin Hyung Kim, and Kee-Eung Kim.2011.
Robust performance evaluation of POMDP-based dialogue systems.
IEEE Transactions on Au-dio, Speech, and Language Processing, 19(4):1029?1040.Daejoong Kim, Jaedeug Choi, Kee-Eung Kim, JungsuLee, and Jinho Sohn.
2013.
Engineering statisticaldialog state trackers:a case study on DSTC.
Techni-cal Report CS-TR-2013-379, Department of Com-puter Science, KAIST.Neville Mehta, Rakesh Gupta, Antoine Raux, DeepakRamachandran, and Stefan Krawczyk.
2010.
Prob-abilistic ontology trees for belief tracking in dialogsystems.
In Proceedings of the 11th Annual Meet-ing of the Special Interest Group on Discourse andDialogue (SIGDIAL), pages 37?46.Nicholas Roy, Joelle Pineau, and Sebastian Thrun.2000.
Spoken dialogue management using proba-bilistic reasoning.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics (ACL), pages 93?100.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.Jason D. Williams and Steve Young.
2007.
Partiallyobservable Markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Jason Williams, Antoine Raux, Deepak Ramachan-dran, and Alan Black.
2013.
The dialog state track-ing challenge.
In Proceedings of the 14th AnnualMeeting of the Special Interest Group on Discourseand Dialogue (SIGDIAL).Jason D. Williams.
2008.
Exploiting the ASR N-bestby tracking multiple dialog state hypotheses.
In Pro-ceedings of the 9th Annual Conference of the In-ternational Speech Communication Association (IN-TERSPEECH), pages 191?194.Jason D. Williams.
2010.
Incremental partition re-combination for efficient tracking of multiple dia-log states.
In Proceedings of the IEEE InternationalConference on Acoustics Speech and Signal Pro-cessing (ICASSP), pages 5382?5385.Steve Young, Milica Gas?ic?, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2010.
The hidden information state model:A practical framework for POMDP-based spoken di-alogue management.
Computer Speech and Lan-guage, 24(2):150?174.466
