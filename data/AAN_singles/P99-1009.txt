Man* vs. Machine: A Case Study  in Base Noun Phrase LearningEr ic  Br i l l  and  Grace  Nga iDepartment of Computer ScienceThe Johns Hopkins UniversityBaltimore, MD 21218, USAEmail: (b r i l l ,gyn}~cs .
jhu .
eduAbst ractA great deal of work has been done demonstrat-ing the ability of machine learning algorithms toautomatically extract linguistic knowledge fromannotated corpora.
Very little work has goneinto quantifying the difference in ability at thistask between a person and a machine.
This pa-per is a first step in that direction.1 In t roduct ionMachine learning has been very successful atsolving many problems in the field of naturallanguage processing.
It has been amply demon-strated that a wide assortment ofmachine learn-ing algorithms are quite effective at extractinglinguistic information from manually annotatedcorpora.Among the machine learning algorithms stud-ied, rule based systems have proven effectiveon many natural language processing tasks,including part-of-speech tagging (Brill, 1995;Ramshaw and Marcus, 1994), spelling correc-tion (Mangu and Brill, 1997), word-sense dis-ambiguation (Gale et al, 1992), message un-derstanding (Day et al, 1997), discourse tag-ging (Samuel et al, 1998), accent restoration(Yarowsky, 1994), prepositional-phrase attach-ment (Brill and Resnik, 1994) and base nounphrase identification (Ramshaw and Marcus, InPress; Cardie and Pierce, 1998; Veenstra, 1998;Argamon et al, 1998).
Many of these rule basedsystems learn a short list of simple rules (typ-ically on the order of 50-300) which are easilyunderstood by humans.Since these rule-based systems achieve goodperformance while learning a small list of sim-ple rules, it raises the question of whether peo-*and Woman.65ple could also derive an effective rule list man-ually from an annotated corpus.
In this pa-per we explore how quickly and effectively rel-atively untrained people can extract linguisticgeneralities from a corpus as compared to a ma-chine.
There are a number of reasons for doingthis.
We would like to understand the relativestrengths and weaknesses ofhumans versus ma-chines in hopes of marrying their con~plemen-tary strengths to create ven more accurate sys-tems.
Also, since people can use their meta-knowledge to generalize from a small number ofexamples, it is possible that a person could de-rive effective linguistic knowledge from a muchsmaller training corpus than that needed by amachine.
A person could also potentially learnmore powerful representations than a machine,thereby achieving higher accuracy.In this paper we describe xperiments we per-formed to ascertain how well humans, givenan annotated training set, can generate rulesfor base noun phrase chunking.
Much previouswork has been done on this problem and manydifferent methods have been used: Church'sPARTS (1988) program uses a Markov model;Bourigault (1992) uses heuristics along with agrammar; Voutilainen's NPTool (1993) uses alexicon combined with a constraint grammar;Juteson and Katz (1995) use repeated phrases;Veenstra (1998), Argamon, Dagan & Kry-molowski(1998) and Daelemaus, van den Bosch& Zavrel (1999) use memory-based systems;Ramshaw & Marcus (In Press) and Cardie &Pierce (1998) use rule-based systems.2 Learn ing  Base  Noun Phrases  byMach ineWe used the base noun phrase system ofRamshaw and Marcus (R&M) as the machinelearning system with which to compare the hu-man learners.
It is difficult o compare differentmachine learning approaches to base NP anno-tation, since different definitions of base NP areused in many of the papers, but the R&M sys-tem is the best of those that have been testedon the Penn Treebank.
1To train their system, R&M used a 200k-wordchunk of the Penn Treebank Parsed Wall StreetJournal (Marcus et al, 1993) tagged using atransformation-based tagger (Brill, 1995) andextracted base noun phrases from its parses byselecting noun phrases that contained no nestednoun phrases and further processing the datawith some heuristics (like treating the posses-sive marker as the first word of a new basenoun phrase) to flatten the recursive struc-ture of the parse.
They cast the problem asa transformation-based tagging problem, whereeach word is to be labelled with a chunk struc-ture tag from the set {I, O, B}, where wordsmarked 'T' are inside some base NP chunk,those marked "O" are not part of any base NP,and those marked "B" denote the first wordof a base NP which immediately succeeds an-other base NP.
The training corpus is first runthrough a part-of-speech tagger.
Then, as abaseline annotation, each word is labelled withthe most common chunk structure tag for itspart-of-speech tag.After the baseline is achieved, transforma-tion rules fitting a set of rule templates arethen learned to improve the "tagging accuracy"of the training set.
These templates take intoconsideration the word, part-of-speech tag andchunk structure tag of the current word and allwords within a window of 3 to either side of it.Applying a rule to a word changes the chunkstructure tag of a word and in effect alters theboundaries of the base NP chunks in the sen-tence.An example of a rule learned by the R&M sys-tem is: change a chunk structure tag of a wordfrom I to B if the word is a determiner, the nextword ks a noun, and the two previous words bothhave chunk structure tags of I.
In other words,a determiner in this context is likely to begin anoun phrase.
The R&M system learns a total1We would like to thank Lance Ramshaw for pro-viding us with the base-NP-annotated training and testcorpora that were used in the R&M system, as well asthe rules learned by this system.of 500 rules.3 Manua l  Ru le  Acquisit ionR&M framed the base NP annotation problemas a word tagging problem.
We chose insteadto use regular expressions on words and part ofspeech tags to characterize the NPs, as well asthe context surrounding the NPs, because thisis both a more powerful representational l n-guage and more intuitive to a person.
A personcan more easily consider potential phrases as asequence of words and tags, rather than lookingat each individual word and deciding whether itis part of a phrase or not.
The rule actions weallow are: 2Add Add a base NP (bracket a se-quence of words as a base NP)Kill Delete a base NP (remove a pairof parentheses)Transform Transform a base NP (moveone or both parentheses to ex-tend/contract a base NP)Merge Merge two base NPsAs an example, we consider an actual rulefrom our experiments:Bracket al sequences of words of: onedeterminer (DT), zero or more adjec-tives (JJ, JJR, JJS), and one or morenouns (NN, NNP, NNS, NNPS), ifthey are followed by a verb (VB, VBD,VBG, VBN, VBP, VBZ).In our language, the rule is written thus: 3A(* .
)({i} t=DT) (* t=JJ\[RS\]?)
(+ t=NNP?S?
)({i} t=VB \[DGNPZ\] ?
)The first line denotes the action, in this case,Add a bracketing.
The second line defines thecontext preceding the sequence we want to havebracketed - -  in this case, we do not care whatthis sequence is.
The third line defines the se-quence which we want bracketed, and the last2The rule types we have chosen are similar to thoseused by Vilain and Day (1996) in transformation-basedparsing, but are more powerful.SA full description of the rule language can be foundat http://nlp, cs.
jhu.
edu/,~baseNP/manual.6Bline defines the context following the bracketedsequence.Internally, the software then translates thisrule into the more unwieldy Perl regular expres-sion:s( ( ( \['\s_\] +__DT\s+) ( \['\s_\] +__JJ \[RS\] \s+)*The actual system is located ath t tp : / /n lp ,  cs.
jhu.
edu/~basenp/chunking.A screenshot of this system is shown in figure4.
The correct base NPs are enclosed in paren-theses and those annotated by the human'srules in brackets.
( \['\s_\] +__NNPFS?\s+) +) ( \[" \s_\] +__VB \[DGNPZ\] \s+)} 4{ ( $1 ) $5 \]'gThe base NP annotation system created bythe humans is essentially a transformation-based system with hand-written rules.
The usermanually creates an ordered list of rules.
Arule list can be edited by adding a rule at anyposition, deleting a rule, or modifying a rule.The user begins with an empty rule list.
Rulesare derived by studying the training corpusand NPs that the rules have not yet bracketed,as well as NPs that the rules have incorrectlybracketed.
Whenever the rule list is edited, theefficacy of the changes can be checked by run-ning the new rule list on the training set andseeing how the modified rule list compares tothe unmodified list.
Based on this feedback,the user decides whether, to accept or rejectthe changes that were made.
One nice prop-erty of transformation-based learning is that inappending a rule to the end of a rule list, theuser need not be concerned about how that rulemay interact with other rules on the list.
Thisis much easier than writing a CFG, for instance,where rules interact in a way that may not bereadily apparent to a human rule writer.To make it easy for people to study the train-ing set, word sequences are presented in one offour colors indicating that they:1. are not part of an NP either in the truth orin the output of the person's rule set2.
consist of an NP both in the truth and inthe output of the person's rule set (i.e.
theyconstitute a base NP that the person's rulescorrectly annotated)3. consist of an NP in the truth but not in theoutput of the person's rule set (i.e.
theyconstitute a recall error)4. consist of an NP in the output of the per-son's rule set but not in the truth (i.e.
theyconstitute a precision error)Exper imenta l  Set -Up  and  Resu l tsThe experiment ofwriting rule lists for base NPannotation was assigned as a homework set toa group of 11 undergraduate and graduate stu-dents in an introductory natural anguage pro-cessing course.
4The corpus that the students were given fromwhich to derive and validate rules is a 25k wordsubset of the R&M training set, approximately!
the size of the full R&M training set.
The 8reason we used a downsized training set wasthat we believed humans could generalize betterfrom less data, and we thought hat it might bepossible to meet or surpass R&M's results witha much smaller training set.Figure 1 shows the final precision, recall, F-measure and precision+recall numbers on thetraining and test corpora for the students.There was very little difference in performanceon the training set compared to the test set.This indicates that people, unlike machines,seem immune to overtraining.
The time thestudents pent on the problem ranged from lessthan 3 hours to almost 10 hours, with an av-erage of about 6 hours.
While it was certainlythe case that the students with the worst resultsspent the least amount of time on the prob-lem, it was not true that those with the bestresults spent the most time - -  indeed, the av-erage amount of time spent by the top threestudents was a little less than the overall aver-age - -  slightly over 5 hours.
On average, peo-ple achieved 90% of their final performance afterhalf of the total time they spent in rule writing.The number of rules in the final rule lists alsovaried, from as few as 16 rules to as many as 61rules, with an average of 35.6 rules.
Again, theaverage number for the top three subjects wasa little under the average for everybody: 30.3rules.4These 11 students were a subset of the entire class.Students were given an option of participating in this ex-periment or doing a much more challenging final project.Thus, as a population, they tended to be the less moti-vated students.67TRAINING SET (25K Words)Precision Recall87.8% 88.6%88.1% 88.2%88.6% 87.6%88.0% 87.2%86.2% 86.8%86.0% 87.1%84.9% 86.7%83.6% 86.0%83.9% 85.0%82.8% 84.5%84.8% 78.8%Student 1Student 2Student 3Student 4Student 5Student 6Student 7Student 8Student 9Student 10Student 11F-Measure P+n Precision 288.2 88.2 88.0%88.2 88.2 88.2%88.1 88.2 88.3%87.6 87.6 86.9%86.5 86.5 85.8%86.6 86.6 85.8%85.8 85.8 85.3%84.8 84.8 83.1%84.4 84.5 83.5%83.6 83.7 83.3%81.7 81.8 84.0%TEST SETRecall F-Measure88.8% 88.487.9% 88.087.8% 88.085.9% 86.485.8% 85.887.1% 86.487.3% 86.385.7% 84.484.8% 84.184.4% 83.877.4% 80.6288.488.188.186.485.886.586.384.484.283.880.7Figure 1: P /R  results of test subjects on training and test corporaIn the beginning, we believed that the stu-dents would be able to match or better theR&M system's results, which are shown in fig-ure 2.
It can be seen that when the same train-ing corpus is used, the best students do achieveperformances which are close to the R&M sys-tem's - -  on average, the top 3 students' per-formances come within 0.5% precision and 1.1%recall of the machine's.
In the following section,we will examine the output of both the manualand automatic systems for differences.5 Ana lys i sBefore we started the analysis of the test set,we hypothesized that the manually derived sys-tems would have more difficulty with potentialrifles that are effective, but fix only a very smallnumber of mistakes in the training set.The distribution of noun phrase types, iden-tified by their part of speech sequence, roughlyobeys Zipf's Law (Zipf, 1935): there is a largetail of noun phrase types that occur very infre-quently in the corpus.
Assuming there is not arule that can generalize across a large numberof these low-frequency noun phrases, the onlyway noun phrases in the tail of the distributioncan be learned is by learning low-count rules: inother words, rules that will only positively af-fect a small number of instances in the trainingcorpus.Van der Dosch and Daelemans (1998) showthat not ignoring the low count instances i  of-ten crucial to performance in machine learningsystems for natural anguage.
Do the human-written rules suffer from failing to learn theseinfrequent phrases?To explore the hypothesis that a primary dif-ference between the accuracy of human and ma-chine is the machine's ability to capture the lowfrequency noun phrases, we observed how theaccuracy of noun phrase annotation of both hu-man and machine derived rules is affected bythe frequency of occurrence of the noun phrasesin the training corpus.
We reduced each baseNP in the test set to its POS tag sequence asassigned by the POS tagger.
For each POS tagsequence, we then counted the number of timesit appeared in the training set and the recallachieved on the test set.The plot of the test set recall vs. the numberof appearances in the training set of each tagsequence for the machine and the mean of thetop 3 students i shown in figure 3.
For instance,for base NPs in the test set with tag sequencesthat appeared 5 times in the training corpus,the students achieved an average recall of 63.6%while the machine achieved a recall of 83.5%.For base NPs with tag sequences that appearless than 6 times in the training set, the machineoutperforms the students by a recall of 62.8%vs.
54.8%.
However, for the rest of the baseNPs - -  those that appear 6 or more times - -the performances of the machine and studentsare almost identical: 93.7% for the machine vs.93.5% for the 3 students, a difference that is notstatistically significant.The recall graph clearly shows that for thetop 3 students, performance is comparable tothe machine's on all but the low frequency con-stituents.
This can be explained by the human's68Recall F-Measure89.3% 89.092.3% 92.0289.092.10.9Figure 2: P /R  results of the R&M system on test corpus..." " " .
.
.
?o .
"0.80.7~0.6-0.5-0.4-0.3oTraining set size(words) Precision25k 88.7%200k 91.8%Number of Appearances in Training Set?
?
4- - ?
MachineStudentsFigure 3: Test Set Recall vs.
Frequency of Appearances in Training Set.reluctance or inability to write a rule that willonly capture a small number of new base NPs inthe training set.
Whereas a machine can easilylearn a few hundred rules, each of which makesa very small improvement to accuracy, this is atedious task for a person, and a task which ap-parently none of our human subjects was willingor able to take on.There is one anomalous point in figure 3.
Forbase NPs with POS tag sequences that appear3 times in the training set, there is a large de-crease in recall for the machine, but a largeincrease in recall for the students.
When welooked at the POS tag sequences in question andtheir corresponding base NPs, we found thatthis was caused by one single POS tag sequence- -  that of two successive numbers (CD).
The69test set happened to include many sentencescontaining sequences of the type:.
.
.
(  CD CD ) TO ( CD CD ) .
.
.as in:( International/NNP Paper/NNP )fell/VBD ( 1/CD 3/CD ) to/TO (51/CD ?/CD ) .
.
.while the training set had none.
The machineended up bracketing the entire sequenceI/CD -~/CD to/T0 51/CD ?/CDas a base NP.
None of the students, however,made this mistake.6 Conc lus ions  and  Future  WorkIn this paper we have described research we un-dertook in an attempt o ascertain how peoplecan perform compared to a machine at learninglinguistic information from an annotated cor-pus, and more importantly to begin to explorethe differences in learning behavior between hu-man and machine.
Although people did notmatch the performance of the machine-learnedannotator, it is interesting that these "languagenovices", with almost no training, were able tocome fairly close, learning a small number ofpowerful rules in a short amount of time on asmall training set.
This challenges the claimthat machine learning offers portability advan-tages over manual rule writing, seeing that rel-atively unmotivated people can near-match thebest machine performance on this task in so lit-tle time at a labor cost of approximately US$40.We plan to take this work in a number of di-rections.
First, we will further explore whetherpeople can meet or beat the machine's accuracyat this task.
We have identified one major weak-ness of human rule writers: capturing informa-tion about low frequency events.
It is possiblethat by providing the person with sufficientlypowerful corpus analysis tools to aide in rulewriting, we could overcome this problem.We ran all of our human experiments on afixed training corpus size.
It would be interest-ing to compare how human performance variesas a function of training corpus size with howmachine performance varies.There are many ways to combine humancorpus-based knowledge extraction with ma-chine learning.
One possibility would be to com-bine the human and machine outputs.
Anotherwould be to have the human start with the out-put of the machine and then learn rules to cor-rect the machine's mistakes.
We could also havea hybrid system where the person writes ruleswith the help of machine learning.
For instance,the machine could propose a set of rules andthe person could choose the best one.
We hopethat by further studying both human and ma-chine knowledge acquisition from corpora, wecan devise learning strategies that successfullycombine the two approaches, and by doing so,further improve our ability to extract useful in-guistic information from online resources.70AcknowledgementsThe authors would like to thank Ryan Brown,Mike Harmon, John Henderson and DavidYarowsky for their valuable feedback regardingthis work.
This work was partly funded by NSFgrant IRI-9502312.Re ferencesS.
Argamon, I. Dagan, and Y. Krymolowski.1998.
A memory-based approach to learningshallow language patterns.
In Proceedings ofthe ITth International Conference on Compu-tational Linguistics, pages 67-73.
COLING-ACL.D.
Bourigault.
1992.
Surface grammatical nal-ysis for the extraction of terminological nounphrases.
In Proceedings of the 30th AnnualMeeting of the Association of ComputationalLinguistics, pages 977-981.
Association ofComputational Linguistics.E.
Brill and P. Resnik.
1994.
A rule-basedapproach to prepositional phrase attachmentdisambiguation.
In Proceedings of the fif-teenth International Conference on Compu-tational Linguistics (COLING-1994).E.
Brill.
1995.
Transformation-based rror-driven learning and natural anguage process-ing: A case study in part of speech tagging.Computational Linguistics, December.C.
Cardie and D. Pierce.
1998.
Error-drivenpruning of treebank gramars for base nounphrase identification.
In Proceedings of the36th Annual Meeting of the Association ofComputational Linguistics, pages 218-224.Association of Computational Linguistics.K.
Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.In Proceedings of the Second Conference onApplied Natural Language Processing, pages136-143.
Association of Computational Lin-guistics.W.
Daelemans, A. van den Bosch, and J. Zavrel.1999.
Forgetting exceptions i harmful in lan-guage learning.
In Machine Learning, spe-cial issue on natural language learning, vol-ume 11, pages 11-43. to appear.D.
Day, J. Aberdeen, L. Hirschman,R.
Kozierok, P. Robinson, and M. Vi-lain.
1997.
Mixed-initiative developmentof language processing systems.
In FifthConference on Applied Natural Language~nUre corpus ~mSed lines only ~l'recision a'ro.
only ~ errors only~3rep on re~cRules so far:(Reload frame ON EVERY ITERATION to make=urethatcontents rare up to date)1~e in yore mla in thebox bdow,Tlmn~ for your im~dpation a d good luck~existential/pronoun Pule(e ,)({1 } t=(EX I PRP IWP It~T)) (* .
)# dete rm~ ne r+adjecti re+nounA,(-({1})t=(DT)) (* t=(CDt33[RS]?IVBG)) (+ t=NNS?)
(* .
)# POS+adjecti ves+nounsA (* .
)({1} t=PO5) (?
t=(JJ[RS]?IVBNIVBG)) (+ t=NNS?)
(* .
)([-~T-t~ird-lar~st ,3 thriftNN i~titutionNN D hi m ([PtlcrtONN PRiCONNp]) ahoRB ==Jdv~ ([itpap]) exlmCtSv~ ([aljT retnrnNs])tOTo ([profitabilitysN ] ) in m ([theft hird;~ quartersN])Wltc~wl ~([itpRl~]) rePOr~vBZ (opc~tingvB G rcsultsvl ~ ([thiZDT weekNN]) ..Sem~ce 499:([POneeNN P Federalt, iNp] ) Illddv~ ([th%T divid~dNN])WltSv~IRl=FatdedvBN inlN ([.anticipationN NI)OliN (m0reRBR [|tzhlgl~tjj~Pimlss r~u~u~nsss] )und=m [ (~r  Financi~ssPinstitotiomNN p Pd~OIIlINNP] ,, [I~d~C~NNP] ,,'ndcc[FmforeememtNN P AetNN P] ) ofm ([1989cD]) .;$mtcnc?
.~0:([%~ labor-..~,~=tn ~o~PNN])~'~ -~o ([~rcvisedvB Nbuy-otltNn bidNN] ) for m [ (Onited~Np Aklin=NsPS~-,-t,N] [UALNNp CO~' N.p] ) ([t~tw~r]),,,~d~ m,~([=~Jo~N.
~'~l '~  N])=~o ( [~mp~s] )  ~Figure 4: Screenshot of base NP chunking systemProcessing, pages 348-355.
Association forComputational Linguistics, March.W.
Gale, K. Church, and D. Yarowsky.
1992.One sense per discourse.
In Proceedings ofthe 4th DARPA Speech and Natural LanguageWorkship, pages 233-237.J.
Juteson and S. Katz.
1995.
Technical ter-minology: Some linguistic properties and analgorithm for identification in text.
NaturalLanguage Engineering, 1:9-27.L.
Mangu and E. Brill.
1997.
Automatic ruleacquisition for spelling correction.
In Pro-ceedings of the Fourteenth International Con-ference on Machine Learning, Nashville, Ten-nessee.M.
Marcus, M. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus ofEnglish: The Penn Treebank.
ComputationalLinguistics, 19(2):313-330.L.
Ramshaw and M. Marcus.
1994.
Exploringthe statistical derivation of transformational71rule sequences for part-of-speech tagging.
InThe Balancing Act: Proceedings of the A CLWorkshop on Combining Symbolic and Sta-tistical Approaches to Language, New MexicoState University, July.L.
Ramshaw and M. Marcus.
In Press.
Textchunking using transformation-based l arn-ing.
In Natural Language Processing UsingVery large Corpora.
Kluwer.K.
Samuel, S. Carberry, and K. Vijay-Shanker.
1998.
Dialogue act tagging withtransformation-based l arning.
In Proceed-ings of the 36th Annual Meeting of the As-sociation for Computational Linguistics, vol-ume 2.
Association of Computational Linguis-tics.A.
van der Dosch and W. Daelemans.
1998.Do not forget: Full memory in memory-based learning of word pronunciation.
In NewMethods in Language Processing, pages 195-204.
Computational Natural Language Learn-ing.J.
Veenstra.
1998.
Fast NP chunkingusing memory-based learning techniques.In BENELEARN-98: Proceedings of theEighth Belgian-Dutch Conference on Ma-chine Learning, Wageningen, the Nether-lands.M.
Vilain and D. Day.
1996.
Finite-stateparsing by rule sequences.
In InternationalConference on Computational Linguistics,Copenhagen, Denmark, August.
The Interna-tional Committee on Computational Linguis-tics.A Voutilainen.
1993.
NPTool, a detector ofEnglish noun phrases.
In Proceedings of theWorkshop on Very Large Corpora, pages 48-57.
Association for Computational Linguis-tics.D.
Yarowsky.
1994.
Decision lists for lexi-cal ambiguity resolution: Application to ac-cent restoration in Spanish and French.
InProceedings of the 32nd Annual Meeting ofthe Association for Computational Linguis-tics, pages 88-95, Las Cruces, NM.G.
Zipf.
1935.
The Psycho-Biology of Language.Houghton Mifflin.72
