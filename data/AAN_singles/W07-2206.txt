Proceedings of the 10th Conference on Parsing Technologies, pages 39?47,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsImproving the Efficiency of a Wide-Coverage CCG ParserBojan Djordjevic and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{bojan,james}@it.usyd.edu.auStephen ClarkComputing LaboratoryOxford UniversityWolfson Building, Parks RoadOxford, OX1 3QD, UKstephen.clark@comlab.ox.ac.ukAbstractThe C&C CCG parser is a highly efficientlinguistically motivated parser.
The effi-ciency is achieved using a tightly-integratedsupertagger, which assigns CCG lexical cat-egories to words in a sentence.
The integra-tion allows the parser to request more cat-egories if it cannot find a spanning anal-ysis.
We present several enhancements tothe CKY chart parsing algorithm used by theparser.
The first proposal is chart repair,which allows the chart to be efficiently up-dated by adding lexical categories individu-ally, and we evaluate several strategies foradding these categories.
The second pro-posal is to add constraints to the chart whichrequire certain spans to be constituents.
Fi-nally, we propose partial beam search to fur-ther reduce the search space.
Overall, theparsing speed is improved by over 35% withnegligible loss of accuracy or coverage.1 IntroductionA recent theme in parsing research has been theapplication of statistical methods to linguisticallymotivated grammars, for example LFG (Kaplan etal., 2004; Cahill et al, 2004), HPSG (Toutanovaet al, 2002; Malouf and van Noord, 2004), TAG(Sarkar and Joshi, 2003) and CCG (Hockenmaierand Steedman, 2002; Clark and Curran, 2004b).
Theattraction of linguistically motivated parsers is thepotential to produce rich output, in particular thepredicate-argument structure representing the under-lying meaning of a sentence.
The disadvantage ofsuch parsers is that they are typically not very effi-cient, parsing a few sentences per second on com-modity hardware (Kaplan et al, 2004).
The C&CCCG parser (Clark and Curran, 2004b) is an orderof magnitude faster, but is still limited to around 25sentences per second.The key to efficient CCG parsing is a finite-statesupertagger which performs much of the parsingwork (Bangalore and Joshi, 1999).
CCG is a lex-icalised grammar formalism, in which elementarysyntactic structures ?
in CCG?s case lexical cate-gories expressing subcategorisation information ?are assigned to the words in a sentence.
CCG su-pertagging can be performed accurately and effi-ciently by a Maximum Entropy tagger (Clark andCurran, 2004a).
Since the lexical categories containso much grammatical information, assigning themwith low average ambiguity leaves the parser, whichcombines them together, with much less work to doat parse time.
Hence Bangalore and Joshi (1999), inthe context of LTAG parsing, refer to supertagging asalmost parsing.Clark and Curran (2004a) presents a novelmethod of integrating the supertagger and parser:initially only a small number of categories, on av-erage, is assigned to each word, and the parser at-tempts to find a spanning analysis using the CKYchart-parsing algorithm.
If one cannot be found, theparser requests more categories from the supertaggerand builds the chart again from scratch.
This processrepeats until the parser is able to build a chart con-taining a spanning analysis.11Tsuruoka and Tsujii (2004) investigate a similar idea in thecontext of the CKY algorithm for a PCFG.39The supertagging accuracy is high enough thatthe parser fails to find a spanning analysis using theinitial category assignment in approximately 4% ofWall Street Journal sentences (?).
However, parsingthis 4%, which largely consists of the longer sen-tences, is disproportionately expensive.This paper describes several modifications to theC&C parser which improve parsing efficiency with-out reducing accuracy or coverage by reducing theimpact of the longer sentences.
The first involveschart repair, where the CKY chart is repaired whenextra lexical categories are added (according to thescheme described above), instead of being rebuiltfrom scratch.
This allows an even tighter integra-tion of the supertagger, in that the parser is able torequest individual categories.
We explore methodsfor choosing which individual categories to add, re-sulting in an 11% speed improvement.The next modification involves parsing with con-straints, so that certain spans are required to be con-stituents.
This reduces the search space consider-ably by eliminating a large number of constituentswhich cross the boundaries of these spans.
The bestset of constraints results in a 10% speed improve-ment over the original parser.
These constraints aregeneral enough that they could be applied to anyconstituency-based parser.
Finally, we experimentwith several beam strategies to reduce the searchspace, finding that a partial beam which operates onpart of the chart is most effective, giving a further6.1% efficiency improvement.The chart repair and constraints interact in an in-teresting, and unexpected, manner when combined,giving a 35.7% speed improvement overall withoutany loss in accuracy or coverage.
This speed im-provement is particularly impressive because it in-volves techniques which only apply to 4% of WallStreet Journal sentences.2 The CCG ParserClark and Curran (2004b) describes the CCG parser.The grammar used by the parser is extracted fromCCGbank, a CCG version of the Penn Treebank(Hockenmaier, 2003).
The grammar consists of 425lexical categories plus a small number of combi-natory rules which combine the categories (Steed-man, 2000).
A Maximum Entropy supertagger firstassigns lexical categories to the words in a sen-tence, which are then combined by the parser usingthe combinatory rules.
A log-linear model scoresthe alternative parses.
We use the normal-formmodel, which assigns probabilities to single deriva-tions based on the normal-form derivations in CCG-bank.
The features in the model are defined overlocal parts of the derivation and include word-worddependencies.
A packed chart representation allowsefficient decoding, with the Viterbi algorithm find-ing the most probable derivation.The supertagger uses a log-linear model to de-fine a distribution over the lexical category set foreach word and the previous two categories (Ratna-parkhi, 1996) and the forward backward algorithmefficiently sums over all histories to give a distribu-tion for each word.
These distributions are then usedto assign a set of lexical categories to each word (?
).The number of categories in each set is determinedby a parameter ?
: all categories are assigned whoseforward-backward probabilities are within ?
of thehighest probability category (?).
If the parser can-not then find a spanning analysis, the value of ?
isreduced ?
so that more lexical categories are as-signed ?
and the parser tries again.
This process re-peats until an analysis spanning the whole sentenceis found.In our previous work, when the parser was unableto find a spanning analysis, the chart was destroyedand then rebuilt from scratch with more lexical cate-gories assigned to each word.
However, this rebuild-ing process is wasteful because the new chart is al-ways a superset of the old one and could be createdby just updating the previous chart.
We describe thechart repair process in Section 3 which allows addi-tional categories to be assigned to an existing chartand the CKY algorithm run over just those parts ofthe chart which require modification.2.1 Chart ParsingThe parser uses the CKY chart parsing algorithm(Kasami, 1965; Younger, 1967) described in Steed-man (2000).
The CKY algorithm applies naturally toCCG since the grammar is binary.
It builds the chartbottom-up, starting with the lexical categories span-ning single words, incrementally increasing the spanuntil the whole sentence is covered.
Since the con-stituents are built in order of span size, at every stage40all the sub-constituents which could be used to cre-ate a particular new constituent are already presentin the chart.The charts are packed by grouping together equiv-alent chart entries, which allows a large number ofderivations to be represented efficiently.
Entries areequivalent when they interact in the same mannerwith both the generation of subsequent parse struc-ture and the statistical parse selection.
In practice,this means that equivalent entries have the samespan; form the same structures, i.e.
the remain-ing derivation plus dependencies, in any subsequentparsing; and generate the same features in any sub-sequent parsing.The Viterbi algorithm is used to find the mostprobable derivation from a packed chart.
For eachequivalence class of individual entries, we record theentry at the root of the subderivation which has thehighest score for the class.
The equivalence classesare defined so that any other individual entry can-not be part of the highest scoring derivation for thesentence.
The highest-scoring subderivations canbe calculated recursively using the highest-scoringequivalence classes that were combined to create theindividual entry.Given a sentence of n words, we define pos ?
{0, .
.
.
, n ?
1} to be the starting position of an en-try in the chart (represented by a CCG category) andspan ?
{1, .
.
.
, n} its length.
Let cell(pos, span)be the set of categories which span the sentence frompos to pos + span.
These will be combinations ofcategories in cell(pos, k) and cell(pos+k, span?k)for all k ?
{1, .
.
.
, span?
1}.
The chart is a two di-mensional array indexed by pos and span.
The valid(pos, span) pairs correspond to pos + span ?
n,that is, to spans that do not extend beyond the endof the sentence.
The squares represent valid cells inFigure 1.
The span from position 3 with length 4,i.e.
cell(3, 4), is marked with a diamond in Figure 2.3 Chart RepairThe parser interacts with the supertagger by decreas-ing the value of the ?
parameter when a spanninganalysis cannot be found for a sentence.
This hasthe effect of adding more lexical categories to thechart.
Instead of rebuilding the chart from scratchwhen new categories are added, it can be repairedaffected cellscell with a newcategory added1050 1 2 3 4 5 6 7 8 91spanpos2346789Figure 1: Cells affected by chart repair.by modifying cells that are affected by the new cat-egories.
Considering the case where a single lexicalcategory is added to the ith word in an n word sen-tence, the new category can only affect the cells thatsatisfy pos ?
i and pos+ span > i.
These cells areshown in Figure 1 for the word at position 3.The number of affected cells is (n?pos)(pos+1),and so the average over the sentence is approxi-mately 1n?
n?10 (n ?
p)(p + 1) dp ?n26 cells.
Thetotal number of cells in the chart is n(n+1)2 .
The chartcan therefore be repaired bottom up, in CKY order,by updating a third of the cells on average.Additional lexical categories for a word are in-serted into the corresponding cell in the bottom row,with the additional categories being marked as new.For each cell C in the second row, each pair of cellsA and B is considered whose spans combine to cre-ate the span of C. In the original CKY, all categoriesfrom A are combined with all categories from B. Inchart repair, categories are only combined if at leastone of them is new, because otherwise the result isalready in C. The categories added to C are marked,and the process is repeated for all affected cells inCKY order.Chart repair speeds up parsing for two reasons.First, it reuses previous computations and eliminateswasteful rebuilding of the chart.
Second, it allowslexical categories to be added to the chart one at a41affected cellsinvalid cellsrequired cell6 7 8 91spanpos23467891050 1 2 3 4 5Figure 2: Cells affected by adding a constraint.time until a spanning derivation is found.
In the orig-inal approach extra categories were added in bulk bychanging the ?
level, which significantly increasedthe average ambiguity.
Chart repair allows the min-imum amount of ambiguity to be added for a span-ning derivation to be found.The C&C parser has a predefined limit on the num-ber of categories in the chart.
If this is exceededbefore a spanning analysis is found then the parserfails on the sentence.
Our new strategy allows achart containing a spanning analysis to be built withthe minimum number of categories possible.
Thismeans that some sentences can now be parsed thatwould have previously exceeded the limit, slightlyincreasing coverage.3.1 Category selectionThe order in which lexical categories are added tothe chart will impact on parsing speed and accu-racy, and so we evaluate several alternatives.
Thefirst ordering (?
VALUE) is by decreasing ?
value,where the ?
value is the ratio between the probabil-ity of the most likely category and the probability ofthe given category for that word.2 The second or-dering (PROB) is by decreasing category probability2We are overloading the use of ?
for convenience.
Here, ?refers to the variable ratio dependent on the particular category,whereas the ?
value used in supertagging is a cutoff applied tothe variable ratio.as assigned by the supertagger using the forward-backward algorithm.We also investigated ordering categories using in-formation from the chart.
Examining the sentenceswhich required chart repair showed that, when aword is missing the correct category, the cells af-fected (as defined in Section 3) by the cell are oftenempty.
The CHART ordering uses this observation toselect the next lexical category to assign.
It selectsthe word corresponding to the cell with the high-est number of empty affected cells, and then addsthe highest probability category not in the chart forthat word.
Finally, we included a RANDOM orderingbaseline for comparison purposes.4 ConstraintsThe set of possible derivations can be constrainedif we know in advance that a particular span is re-quired to be the yield of a single constituent in thecorrect parse.
A constraint on span p reduces thesearch space because p must be the yield of a singlecell.
This means that cells with yields that cross theboundary of p cannot be part of a correct derivation,and do not need to be considered (the grey cells inFigure 2).
In addition, if a cell yields p as a prefix orsuffix (the hashed cells in Figure 2) then it also hasconstraints on how it can be created.Figure 2 shows an example constraint requiringwords 3?6 to be a constituent, which corresponds top = cell(3, 4).
Consider cell(3, 7): it yields words3?9 and so contains p as the prefix.
Normally it canbe created by combining cell(3, 1) with cell(4, 6),or cell(3, 2)with cell(5, 5), and so on up to cell(3, 6)with cell(9, 1).
However the first three combinationsare not allowed because the second child crosses theboundary of p. This gives a lower limit for the spanof the left child.
Similarly, if p is the suffix of thespan of a cell then there is a lower limit on the spanof the right child.As the example demonstrates, a single constraintcan eliminate many combinations, reducing thesearch space significantly, and thus improving pars-ing efficiency.4.1 Creating ConstraintsHow can we know in advance that the correct deriva-tion must yield specific spans, since this appears torequire knowledge of the parse itself?
We have ex-42plored constraints derived from shallow parsing andfrom the raw sentence.
Our results demonstrate thatsimple constraints can reduce parsing time signifi-cantly without loss of coverage or accuracy.Chunk tags were used to create constraints.
Weexperimented with both gold standard chunks fromthe Penn Treebank and also chunker output from theC&C chunk tagger.
The tagger is very similar to theMaximum Entropy POS tagger described in Curranand Clark (2003).
Only NP chunks were used be-cause the accuracy of the tagger for other chunks islower.
The Penn Treebank chunks required modi-fication because CCGbank analyses some construc-tions differently.
We also created longer NPs by con-catenating adjacent base NPs, for example in the caseof possessives.A number of punctuation constraints were usedand had a significant impact especially for longersentences.
There are a number of punctuation rulesin CCGbank which absorb a punctuation mark bycombining it with a category and returning a cate-gory of the same type.
These rules are very produc-tive, combining with many constituent types.
How-ever, in CCGbank the sentence final punctuation isalways attached at the root.
A constraint on the firstn ?
1 words was added to force the parser to onlyattach the sentence final punctuation once the rest ofthe sentence has been parsed.Constraints are placed around parenthesised andquoted phrases that usually form constituents be-fore attaching elsewhere.
Constraints are also placedaround phrases bound by colons, semicolons, or hy-phens.
These constraints are especially effectivefor long sentences with many clauses separated bysemicolons, reducing the sentence to a number ofsmaller units which significantly improves parsingefficiency.In some instances, adding constraints can beharmful to parsing efficiency and/or accuracy.
Lackof precision in the constraints can come from noisyoutput from NLP components, e.g.
the chunker, orfrom rules which are not always applicable, e.g.punctuation constraints.
We find that the punctua-tion constraints are particularly effective while thegold standard chunks are required to gain any ben-efit for the NP constraints.
Adding constraints alsohas the potential to increase coverage because the re-duced search space means that longer sentences canbe parsed without exceeding the pre-defined limitson chart size.5 Selective Beam SearchBeam search involves greedy elimination of lowprobability partial derivations before they can formcomplete derivations.
It is used in many parsers toreduce the search space, for example Collins (2003).We use a variable width beam where all categoriesc in a particular cell C that satisfy score(c) <max{score(x)|x ?
C} ?
B, for some beam cut-off B, are removed.
The category scores score(c)are log probabilities.In the C&C parser, the entire packed chart is con-structed first and then the spanning derivations aremarked.
Only the partial derivations that form partof spanning derivations are scored to select the bestparse, which is a small fraction of the categories inthe chart.
Because the categories are scored witha complex statistical model with a large number offeatures, the time spent calculating scores is signif-icant.
We found that applying a beam to every cellduring the construction of the chart was more expen-sive than not using the beam at all.
When the beamwas made harsh enough to be worthwhile, it reducedaccuracy and coverage significantly.We propose selective beam search where thebeam is only applied to spans of particular lengths.The shorter spans are most important to cull becausethere are many more of them and removing them hasthe largest impact in terms of reducing the searchspace.
However, the supertagger already acts likea beam at the lexical category level and the parsermodel has fewer features at this level, so the beammay be more accurate for longer spans.
We there-fore expect the beam to be most effective for spansof intermediate length.6 ExperimentsThe parser was trained on CCGbank sections 02-21and section 00 was used for development.
The per-formance is measured in terms of coverage, F-scoreand parsing time.
The F-score is for labelled depen-dencies compared against the predicate-argumentdependencies in CCGbank.
The time reported in-cludes loading the grammar and statistical model,which takes around 5 seconds, and parsing the 191343sentences in section 00.The failure rate (opposite of coverage) is brokendown into sentences with length ?
40 and > 40because longer sentences are more difficult to parseand the C&C parser already has very high coverageon shorter sentences.
There are 1784 1-40 word sen-tences and 129 41+ word sentences.
The averagelength and standard deviation in the 41+ set are 50.8and 31.5 respectively.All experiments used gold standard POS tags.Original and REPAIR do not use constraints.
TheNP(GOLD) experiments use Penn Treebank goldstandard NP chunks to determine an upper boundon the utility of chunk constraints.
The times re-ported for NP(C&C) using the C&C chunker includethe time to load the chunker model and run the chun-ker (around 1.3 seconds).
PUNCT adds all of thepunctuation constraints.Finally the best system was compared against theoriginal parser on section 23, which has 2257 sen-tences of length 1-40 and 153 of length 41+.
Themaximum length is only 65, which explains the highcoverage for the 41+ section.6.1 Chart Repair ResultsThe results in Table 1 show that chart repair givesan immediate 11.1% improvement in speed and asmall 0.21% improvement in accuracy.
96.1% ofsentences do not require chart repair because theyare successfully parsed using the initial set of lexi-cal categories supplied by the supertagger.
Hence,11% is a significant improvement for less than 4%of the sentences.We believe the accuracy was improved (on top ofthe efficiency) because of the way the repair processadds new categories.
Adding categories individuallyallows the parser to be influenced by the probabil-ities which the supertagger assigns, which are notdirectly modelled in the parser.
If we were to addthis information from the supertagger into the parserstatistical model directly we would expect almostno accuracy difference between the original methodand chart repair.Table 2 shows the impact of different categoryordering approaches for chart repair (with PUNCTconstraints).
The most effective approach is to usethe information from the chart about the proportionof empty cells, which adds half as many categoriesMETHOD secs % F-SCORE CATSRANDOM 70.2 -16.2 86.57 23.1?
VALUE 60.4 ?
86.66 15.7PROB 60.1 0.5 86.65 14.3CHART 57.2 5.3 86.61 7.0Table 2: Category ordering for chart repair.on average as the ?
value and probability based ap-proaches.
All of our approaches significantly out-perform randomly selecting extra categories.
TheCHART category ordering is used for the remainingexperiments.6.2 Constraints ResultsThe results in Table 1 show that, without chart re-pair, using gold standard noun phrases does not im-prove efficiency, while using noun phrases identi-fied by the C&C chunker decreases speed by 10.8%.They both also slightly reduce parsing accuracy.The number of times the parsing process had to berestarted with the constraints removed, was morecostly than the reduction of the search space.
Thisis unsurprising because the chunk data was not ob-tained from CCGbank and the chunker is not ac-curate enough for the constraints to improve pars-ing efficiency.
The most frequent inconsistenciesbetween CCGbank and chunks extracted from thePenn Treebank were fixed in a preprocessing step asexplained in Section 4.1, but the less frequent con-structions are still problematic.The best results for parsing with constraints (with-out repair) were with both punctuation and goldstandard noun phrase constraints, with 20.5% im-provement in speed and 0.42% in coverage, but anF-score penalty of 0.3%.
This demonstrates the pos-sible efficiency gain with a perfect chunker ?
thecorresponding results with the C&C chunker are stillworse than without constraints.
The best resultswithout a decrease in accuracy use only punctuationconstraints, with 10.4% increase in speed and 0.37%in coverage.
The punctuation constraints also havethe advantage of being simple to implement.The best overall efficiency gain was obtainedwhen punctuation and gold standard noun phraseswere used with chart repair, with a 45.4% improve-ment in speed and 0.63% in coverage, and a 0.4%drop in accuracy.
The best results without a drop in44METHOD secs % F-SCORE COVER n ?
40 n > 40Original 88.3 ?
86.54 98.85 0.392 11.63REPAIR 78.5 11.1 86.75 99.01 0.336 10.08NP(GOLD) 88.4 -0.1 86.27 99.06 0.224 10.85NP(C&C) 97.8 -10.8 86.31 99.16 0.224 9.30PUNCT 79.1 10.4 86.56 99.22 0.168 9.30NP(GOLD) + PUNCT 69.8 20.5 86.24 99.27 0.168 8.53NP(C&C) + PUNCT 97.0 -9.9 86.31 99.16 0.168 10.08NP(GOLD) + REPAIR 65.0 26.4 86.04 99.37 0.224 6.20NP(C&C) + REPAIR 77.5 12.2 86.35 99.37 0.224 6.20PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43NP(GOLD) + PUNCT + REPAIR 48.2 45.4 86.14 99.48 0.168 5.43NP(C&C) + PUNCT + REPAIR 63.2 28.4 86.43 99.53 0.163 3.88Table 1: Parsing performance on section 00 with constraints and chart repairMETHOD secs % F-SCORE COVER n ?
40 n > 40Original 88.3 ?
86.54 98.85 0.392 11.63PUNCT 79.1 10.4 86.56 99.22 0.168 9.30REPAIR 78.5 11.1 86.75 99.01 0.336 10.08PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43PUNCT + REPAIR + BEAM 52.4 40.7 86.56 99.48 0.168 5.43Table 3: Best performance on Section 00accuracy were with only punctuation constraints andchart repair, with improvements of 35.2% speed and0.63% coverage.
Coverage on both short and longsentences is improved ?
the best results show a 43%and 67% decrease in failure rate for sentence lengthsin the ranges 1-40 and 41+ respectively.6.3 Partial Beam ResultsWe found that using the selective beam on 1?2 wordspans had negligible impact on speed and accuracy.Using the beam on 3?4 word spans had the most im-pact without accuracy penalty, improving efficiencyby another ?5%.
Experiments with the selectivebeam on longer spans continued to improve effi-ciency, but with a much greater penalty in F-score,e.g.
a further ?5% at a cost of 0.5% F-score for 3?6word spans.
However, we are interested in efficiencyimprovements with negligible cost to accuracy.6.4 Overall ResultsTable 3 summarises the results for section 00.
Thechart repair and punctuation constraints individuallyincrease parsing efficiency by around 10%.
How-ever, the most interesting result is that in combina-tion they increase efficiency by over 35%.
This isbecause the cost of rebuilding the chart when theconstraints are incorrect has been significantly re-duced by chart repair.
Finally, the use of the selec-tive beam gives modest improvement of 5.5%.
Theoverall efficiency gain on section 00 is 40.7% withan additional 0.5% coverage, halving both the num-ber of short and long sentences that fail to be parsed.Table 4 shows the performance of the punctuationconstraints, chart repair and selective beam systemon section 23.
The results are consistent with sec-tion 00, showing a 30.9% improvement in speed and0.29% in coverage, with accuracy staying at roughlythe same level.
The results show a consistent 35-40% reduction in parsing time and a 40-65% reduc-tion in parse failure rate.7 ConclusionWe have introduced several modifications to CKYparsing for CCG that significantly increase parsingefficiency without an accuracy or coverage penalty.45METHOD secs % F-SCORE COVER n ?
40 n > 40Original 91.3 ?
86.92 99.29 0.621 1.961PUNCT + REPAIR + BEAM 58.7 35.7 86.82 99.58 0.399 0.654Table 4: Best performance on Section 23Chart repair improves efficiency by reusing thechart from the previous parse attempts.
This allowsus to further tighten the parser-supertagger integra-tion by adding one lexical category at a time until aspanning derivation is found.
We have also exploredseveral approaches to selecting which category toadd next.
We intend to further explore strategiesfor determining which category to add next when aparse fails.
This includes combining chart and prob-ability based orderings.
Chart repair alone gives an11.1% efficiency improvement.Constraints improve efficiency by avoiding theconstruction of sub-derivations that will not be used.They have a significant impact on parsing speed andcoverage without reducing the accuracy, providedthe constraints are identified with sufficient preci-sion.
Punctuation constraints give a 10.4% improve-ment, but NP constraints require higher precision NPchunking than is currently available for CCGbank.Constraints and chart repair both manipulate thechart for more efficient parsing.
Adding categoriesone at a time using chart repair is almost a form ofagenda-based parsing.
We intend to explore othermethods for pruning the space and agenda-basedparsing, in particular A* parsing (Klein and Man-ning, 2003), which will allow only the most proba-ble parts of the chart to be built, improving efficiencywhile still ensuring the optimal derivation is found.When all of our modifications are used parsingspeed increases by 35-40% and the failure rate de-creases by 40-65%, both for sentences of length 1-40and 41+, with a negligible accuracy penalty.
The re-sult is an even faster state-of-the-art wide-coverageCCG parser.AcknowledgementsWe would like to thank the anonymous reviewersfor their feedback.
James Curran was funded underARCDiscovery grants DP0453131 and DP0665973.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?265.A.
Cahill, M. Burke, R. O?Donovan, J. van Genabith,and A.
Way.
2004.
Long-distance dependency resolu-tion in automatically acquired wide-coverage PCFG-based LFG approximations.
In Proceedings of the42nd Meeting of the ACL, pages 320?327, Barcelona,Spain.Stephen Clark and James R. Curran.
2004a.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of COLING-04, pages 282?288,Geneva, Switzerland.Stephen Clark and James R. Curran.
2004b.
Parsing theWSJ using CCG and log-linear models.
In Proceed-ings of ACL-04, pages 104?111, Barcelona, Spain.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.James R. Curran and Stephen Clark.
2003.
InvestigatingGIS and smoothing for maximum entropy taggers.
InProceedings of the 10th Meeting of the EACL, pages91?98, Budapest, Hungary.James R. Curran, Stephen Clark, and David Vadas.2006.
Multi-tagging for lexicalized-grammar parsing.In Proceedings of COLING/ACL-06, pages 697?704,Sydney, Austrailia.Julia Hockenmaier and Mark Steedman.
2002.
Gener-ative models for statistical parsing with CombinatoryCategorial Grammar.
In Proceedings of the 40th Meet-ing of the ACL, pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003.
Data and Models for Statis-tical Parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Ron Kaplan, Stefan Riezler, Tracy H. King, JohnT.
Maxwell III, Alexander Vasserman, and RichardCrouch.
2004.
Speed and accuracy in shallow anddeep stochastic parsing.
In Proceedings of the HumanLanguage Technology Conference and the 4th Meetingof the North American Chapter of the Association forComputational Linguistics (HLT-NAACL?04), Boston,MA.46J.
Kasami.
1965.
An efficient recognition and syntaxanalysis algorithm for context-free languages.
Techni-cal Report AFCRL-65-758, Air Force Cambridge Re-search Laboratory, Bedford, MA.Dan Klein and Christopher D. Manning.
2003.
A* pars-ing: Fast exact Viterbi parse selection.
In Proceed-ings of Human Language Technology and the NorthAmerican Chapter of the Association for Computa-tional Linguistics Conference, pages 119?126, Ed-mond, Canada.Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute value gram-mars.
In Proceedings of the IJCNLP-04 Workshop:Beyond shallow analyses - Formalisms and statisticalmodeling for deep analyses, Hainan Island, China.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLP Con-ference, pages 133?142, Philadelphia, PA.Anoop Sarkar and Aravind Joshi.
2003.
Tree-adjoininggrammars and its application to statistical parsing.
InRens Bod, Remko Scha, and Khalil Sima?an, editors,Data-oriented parsing.
CSLI.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.Kristina Toutanova, Christopher Manning, StuartShieber, Dan Flickinger, and Stephan Oepen.
2002.Parse disambiguation for a rich HPSG grammar.
InProceedings of the First Workshop on Treebanksand Linguistic Theories, pages 253?263, Sozopol,Bulgaria.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2004.
Iterativecky parsing for probabilistic context-free grammars.In Proceedings of the IJCNLP conference, pages 52?60, Hainan Island, China.D.
Younger.
1967.
Recognition and parsing of context-free languages in time n3.
Information and Control,10(2):189?208.47
