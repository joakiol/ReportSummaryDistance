Automatic Acquisition of Language Modelbased on Head-Dependent Relation between WordsSeungmi  Lee and Key-Sun  Cho iDepartment  of Computer  ScienceCenter for Artificial Intell igence ResearchKorea Advanced Inst i tute of Science and Technologye-mail: {leesm, kschoi}@world, kaist, ac.
krAbst ractLanguage modeling is to associate a sequenceof words with a priori probability, which is akey part of many natural anguage applicationssuch as speech recognition and statistical ma-chine translation.
In this paper, we present alanguage modeling based on a kind of simpledependency grammar.
The grammar consistsof head-dependent relations between words andcan be learned automatically from a raw corpususing the reestimation algorithm which is alsointroduced in this paper.
Our experiments showthat the proposed model performs better thann-gram models at 11% to 11.5~ reductions intest corpus entropy.1 In t roduct ionLanguage modeling is to associate a priori prob-ability to a sentence.
It is a key part of manynatural language applications uch as speechrecognition and statistical machine translation.Previous works for language modeling can bebroadly divided into two approaches; one is n-gram-based and the other is grammar-based.N-gram model estimates the probability of asentence as the product of the probability ofeach word in the sentence.
It assumes thatprobability of the nth word is dependent onthe previous n -  1 words.
The n-gram prob-abilities are estimated by simply counting then-gram frequencies in a training corpus.
Insome cases, class (or part of speech) n-gramsare used instead of word n-grams(Brown et al,1992; Chang and Chen, 1996).
N-gram modelhas been widely used so far, but it has alwaysbeen clear that n-gram can not represent longdistance dependencies.In contrast with n-gram model, grammar-based approach assigns syntactic structures toa sentence and computes the probability of thesentence using the probabilities of the struc-tures.
Long distance dependencies can be rep-resented well by means of the structures.
Theapproach usually makes use of phrase struc-ture grammars such as probabilistic ontext-freegrammar and recursive transition network(Lariand Young, 1991; Sneff, 1992; Chen, 1996).
Inthe approach, however, a sentence which is notaccepted by the grammar is assigned zero prob-ability.
Thus, the grammar must have broad-coverage so that any sentence will get non-zeroprobability.
But acquisition of such a robustgrammar has been known to be very difficult.Due to the difficulty, some works try to use anintegrated model of grammar and n-gram com-pensating each other(McCandless, 1994; Meteerand Rohlicek, 1993).
Given a robust grammar,grammar-based language modeling is expectedto be more powerful and compact in model sizethan n-gram-based one.In this paper we present a language modelingbased on a kind of simple dependency gram-mar.
The grammar consists of head-dependentrelations between words and can be learned au-tomatically from a raw corpus using the rees-timation algorithm which is also introduced inthis paper.
Based on the dependencies, a sen-tence is analyzed and assigned syntactic struc-tures by which long distance dependences arerepresented.
Because the model can be thoughtof as a linguistic bi-gram model, the smoothingfunctions of n-gram models can be applied to it.Thus, the model can be robust, adapt easily tonew domains, and be effective.The paper is organized as follows.
We intro-duce some definitions and notations for the de-pendency grammar and the reestimation algo-rithm in section 2, and explain the algorithm insection 3.
In section 4, we show the experimen-tal results for the suggested model compared ton-gram models.
Finally, section 5 concludes thispaper.2 A S imple  Dependency  GrammarIn this paper, we assume a kind of simple de-pendency grammar which describes a language723by a set of head-dependent relations betweenwords.
A sentence is analyzed by establishingdependency links between individual words inthe sentence.
A dependency analysis, :D, of asentence can be represented with arrows point-ing from head to dependent as depicted in Fig-ure 1.
For structural generality, we assume thatthere is always a marking tag, "EOS"(End ofSentence), at the end of a sentence and it hasthe head word of the sentence as its own depen-dent("gave" in Figure 1).I gave him a book EOSFigure 1: An example dependency analysisA / )  is a set of inter-word ependencies whichsatisfy the following conditions: (1) every wordin the sentence has its head in the sentence x-cept the head word of the sentence.
(2) everyword can have only one head.
(3) there is nei-ther crossing nor cycle of dependencies.The probabilistic model of the simple depen-dency grammar is given byp(sentence) = ~-'~ p(D)2)= }2 II2) x.-.+y6Dwhere p(x--+ y) = p(yl x)freq(x --+ y)E, z)"Complete-Link and Complete-SequenceHere, we define complete-link and complete-sequence which represent partial :Ds for sub-strings.
They are used to construct overall79s and used as the basic structures for the rees-timation algorithm in section 3.A set of dependency relations on a word se-quence, wij  l, is a complete-link when the fol-lowing conditions are satisfied:?
there is (wi -+ wi) or (wi e-- wj) exclu-sively.?
Every inner word has a head in the wordsequence.?
Neither crossing nor cycle of dependencyrelations is allowed.tWe use wi for ith word in a sentence and wi,j for theword sequence from wl to wj(i < j).k her second child the busFigure 2: Example complete-linksA complete-link has direction.
A complete-linkon wij is said to be "rightward" if the outermostrelation is (wi --+ wj), and "leftward" if the rela-tion is (wi e-- wj).
Unit complete-link is definedon a string of two adjacent words, wi,;+l.
InFigure 2, (a) is a rightward complete-link, andboth of (b) and (c) are leftward ones.bird in the cage the bus bookFigure 3: Example complete-sequencesA complete-sequence is a sequence of 0 ormore adjacent complete-links that have thesame direction.
A unit complete-sequence is de-fined on a string of one word.
It is 0 sequenceof complete-links.
The direction of a complete-sequence is determined by the direction of thecomponent complete-links.
In Figure 3, (a) is arightward complete-sequence composed of twocomplete-links, and (b) is a leftward one.
(c) is acomplete-sequence composed of zero complete-links, and it can be both leftward and rightward.The word of "complete" means that the de-pendency relations on the inner words are com-pleted and that consequently there is no needto process further on them.
From now on,we use Lr( i , j ) /Lt( i , j )  for rightward/leftwardcomplete-links and Sr(i , j ) /St( i , j )  for right-ward/leftward complete-sequences on wi, j.Any complete-link on wi, j can be viewed asthe following combination.?
L~(i,j): {(wi --+ wj), S~(i,m), St (m+l, j )}?
Ll(i,j): {(wi e-- wj), St(i, m), St (m+l , j )}fo ram( i<m<j) .Otherwise, the set of dependencies does not sat-isfy the conditions of no crossing, no cycle andno multiple heads and is not a complete-link anymore.Similarly, any complete-sequence on wi,j canbe viewed as the following combination.?
S~(i,j): {Sr(i,m), L~(m,j)}?
St(i,j): {Lt(i,m), St(m,j)}fo ram( i<m<j) .In the case of complete-sequence, we canprevent multiple constructions of the same724complete-sequence by the above combinationalrestriction.Figure 4: Abstract representation f / )Figure 4 shows an abstract representation fa / )  of an n-word sentence.
When wk(1 < k <_n) is the head of the sentence, any D of thesentence can be represented by a St(l, EOS)uniquely by the assumption that there is alwaysthe dependency relation, (wk +-- wEos).3 Reest imat ion  A lgor i thmThe reestimation algorithm is a variation ofInside-Outside algorithm(Jelinek et al, 1990)adapted to dependency grammar.
In this sec-tion we first define the inside-outside probabili-ties of complete-links and complete-sequences,and then describe the reestimation algorithmbased on them 2.In the followings, ~ indicates inside probabil-ity and a, is for outside probability.
The su-perscripts, l and s, are used for "complete-link"and "complete-sequence" respectively.
The sub-scripts indicate direction: r for "rightward" andI for "leftward".The inside probabilities of complete-links(n~(i,j), Lt(i,j)) and complete-sequences(Sr(i,j), Sl(i,j)) are as follows.j -1/3t~(i,j) = ~ p(wi --+ wj)/3~(i, m)t3~(m + 1,j).rn=ij - - I/3\[(i,j) = E p(wi 6.- wj)t3~(i,m)13?
(m + 1,j).rn=ij--1fl~(i,j) = ~ /3~(i,m)~t~(m,j).miniJ/3?
(i,j) = ~ /3\[(i,m)t3?
(m,j).m=i+lThe basis probabilities are:/31r(i,i + 1) = p(wi "~ wi+l)/3\[(i,i + 1) = p(wi (-" wi+l)/3~(i, i) = fl?
(i, i) = 1/37(1, EO S) = p( wL, )~A little more detailed explanation of the expressionscan be found in (Lee and Choi, 1997)./3~(i,i+ 1) = p(L~(i,i+ 1)) = p(wi ~ wi+t)/37 (i, i + 1) = p(Lt(i, i + 1)) = p(wi +-- wi+t)./37(1, EOS) is the sentence probability be-cause every dependency analysis, D, is repre-sented by a St(l, EOS) and/37(1 , EOS) is sumof the probability of every St(l, EOS).probabilities for complete-(i, j)) and complete-sequencesare as follows.The outsidelinks (L,.
(i,j), Lt(S~(i,j), St(i,j))iat~(i,j) =nc~ (v, j)/3i~(v, i).a~ (i, h)/3?
(j, h).h=ja~(i,j) = ~ a~(i,h)/3tr(j,h)h=j+l+atr(i ,h)/3i~(j + 1, h)p(wi -+ Wh)+al(i, h)/3?
(j + 1, h)p(wi ~ wh).i - Ia~(i,j) = ~ a~(v,j)fl~(v,i)v----I+dr(v , j )Z ; (v ,  i - t)p(wv wA+al(v,j)t3;(v , i -  1)p(wv e- wj).The basis probability is~(1, EOS) = 1.Given a training corpus, the initial grammaris just a list of all pairs of unique words inthe corpus.
The initial pairs represent the ten-tative head-dependent relations of the words.And the initial probabilities of the pairs canbe given randomly.
The training starts withthe initial grammar.
The train corpus is an-alyzed with the grammar and the occurrencefrequency of each dependency relation is cal-culated.
Based on the frequencies, probabili-ties of dependency relations are recalculated byC(wp --+ w~) The process w,) = C(wcontinues until the entropy of the training cor-pus becomes the minimum.
The frequency ofoccurrence, C(wi --+ wj), is calculated byw)  = -+1 t ?
?
t= p(wt,.)a.
(,,3)/3~(i,j)where O~(wi ~ wj, D, wl,n) is 1 if the depen-dency relation, (wi --+ wj), is used in the D,725and 0 otherwise.
Similarly, the occurrence fre-quency of the dependency relation, (wi +- wj),is computed by ~----L---o~l(i,j)~\[(i,j ).4 Pre l iminary  exper imentsWe have experimented with three languagemodels, tri-gram model (TRI), bi-gram model(BI), and the proposed model (DEP) on a rawcorpus extracted from KAIST corpus 3.
The rawcorpus consists of 1,589 sentences with 13,139words, describing animal life in nature.
Werandomly divided the corpus into two parts: atraining set of 1,445 sentences and a test set of144 sentences.
And we made 15 partial trainingsets which include the first s sentences in thewhole training set, for s ranging from 100 to1,445 sentences.
We trained the three languagemodels for each partial training set, and testedthe training and the test corpus entropies.TRI and BI was trained by counting the oc-currence of tri-grams and bi-grams respectively.DEP was trained by running the reestimationalgorithm iteratively until it converges to an op-timal dependency grammar.
On the average, 26iterations were done for the training sets.Smoothing is needed for language modelingdue to the sparse data problem.
It is to com-pensate for the overestimated and the under-estimated probabilities.
Smoothing method it-self is an important factor.
But our goal is notto find out a better smoothing method.
So wefixed on an interpolation method and applied itfor the three models.
It can be represented as(McCandless, 1994)..., w , -x )  = ,\P,(wilw,-,+l, ..., wi_l)+(1 - ...,where = C(wl,  ..., w,-1)C(w, ,  ..., + K,"The Ks is the global smoothing factor.
The big-ger the Ks, the larger the degree of smoothing.For the experiments we used 2 for Ks.We take the performance of a language modelto be its cross-entropy on test corpus,1 sIVl E-l?g2Pm(Si)i=13KAIST (Korean Advanced Institute of Science andTechnology) corpus has been under construction since1994.
It consists of raw text collection(45,000,000words), POS-tagged collection(6,750,000 words), andtree-tagged collection(30,000 sentences) at present.where the test corpus contains a total of IV\]words and is composed of S sentences.3.4 i | | i | !
I3.232.8>" 2.6 O.2.4u~ 2.2 ~ (DEP model) o2 a (TRI model) i1.81.61.40 200 400 600 800 1000 1200 1400 600No.
of training sentencesFigure 5: Training corpus entropiesFigure 5 shows the training corpus entropiesof the three models.
It is not surprising thatDEP performs better than BI.
DEP can bethought of as a kind of linguistic bi-gram modelin which long distance dependencies can be rep-resented through the head-dependent relationsbetween words.
TRI shows better performancethan both BI and DEP.
We think it is becauseTRI overfits the training corpus, judging fromthe experimental results for the test corpus.9.5 i I I I I I I8.5uJ 7.5.=( (TRI model)7 / (DEP model) o6.5  a i I I I I I0 200 400 600 800 1000 1200 1400 1600No.
o f  t ra in ing  sentencesFigure 6: Test corpus entropiesFor the test corpus, BI shows slightly bet-ter performance than TRI as depicted in Fig-ure 6.
Increase in the order of n-gram fromtwo to three shows no gains in entropy reduc-tion.
DEP, however, Shows still better per-formance than the n-gram models.
It showsabout 11.5% entropy reduction to BI and about11% entropy reduction to TRI.
Figure 7 showsthe entropies for the mixed corpus of trainingand test sets.
From the results, we can seethat head-dependent relations between wordsare more useful information than the naive n-gram sequences, for language modeling.
We cansee also that the reestimation algorithm can findout properly the hidden head-dependent rela-tions between words, from a raw corpus.726,r,f -u J(noZ109876i i | i !
i i(B I  model)(TRI model)(DEP model)530 200 400 600 800 1000 1200 1400No.
of training sentencesFigure 7: Mixed corpus entropies6000050000400003000020000100000600i !
| i i i !
(DEP model) o(TRI model) "*'--r T  I I I I I I200 400 600 800 1000 1200 1400 1600No.
of training sentencesFigure 8: Model sizeRelated to the size of model, however, DEPhas much more parameters than TRI and BIas depicted in Figure 8.
This can be a seriousproblem when we create a language model froma large body of text.
In the experiments, how-ever, DEP used the grammar acquired automat-ically as it is.
In the grammar, many inter-worddependencies have probabilities near 0.
If weexclude such dependencies a was experimentedfor n-grams by Seymore and Rosenfeld (1996),we may get much more compact DEP modelwith very slight increase in entropy.5 Conc lus ionsIn this paper, we presented a language modelbased on a kind of simple dependency gram-mar.
The grammar consists of head-dependentrelations between words and can be learned au-tomatically from a raw corpus by the reestima-tion algorithm which is also introduced in thispaper.
By the preliminary experiments, it wasshown that the proposed language model per-forms better than n-gram models in test cor-pus entropy.
This means that the reestimationalgorithm can find out the hidden informationof head-dependent relation between words in araw corpus, and the information is more usefulthan the naive word sequences of n-gram, forlanguage modeling.We are planning to experiment the perfor-mance of the proposed language model for largecorpus, for various domains, and with varioussmoothing methods.
For the size of the model,we are planning to test the effects of excludingthe dependency relations with near zero proba-bilities.ReferencesP.
F. Brown, V. J. Della Pietra, P. V. deSouza,J.
C. Lai, and R. L. Mercer.
1992.
"Class-Based n-gram Models of Natural Language".Computational Linguistics, 18(4):467-480.C.
Chang and C. Chen.
1996.
"Application Is-sues of SA-class Bigram Language Models".Computer Processing of Oriental Languages,io(1):i-i5.S.
F. Chen.
1996.
"Building ProbabilisticModels for Natural Language".
Ph.D. the-sis, Havard University, Cambridge, Mas-sachusetts.F.
Jelinek, J. D. Lafferty, and R. L. Mercer.1990.
"Basic Methods of Probabilistic Con-text Free Grammars".
Technical report, IBM- T.J. Watson Research Center.K.
Lari and S. J.
Young.
1991.
"Applicationsof stochastic ontext-free grammars using theinside-outside algorithm".
Computer Speechand Language, 5:237-257.S.
Lee and K. Choi.
1997.
"Reestimation andBest-First Parsing Algorithm for Probabilis-tic Dependency Grammar".
In WVLC-5,pages 11-21.M.
K. McCandless.
1994.
"Automatic Acquisi-tion of Language Models for Speech Recog-nition".
Master's thesis, Massachusetts Insti-tute of Technology.M.
Meteer and J.R. Rohlicek.
1993.
"Statis-tical Language Modeling Combining N-gramand Context-free Grammars".
In ICASSP-93, volume II, pages 37-40, January.K.
Seymore and R. Rosenfeld.
1996.
"ScalableTrigram Backoff Language Models".
Techni-cal Report CMU-CS-96-139, Carnegie MellonUniversity.S.
Sneff.
1992.
"TINA: A natural anguage sys-tem for spoken language applications".
Com-putational Linguistics, 18(1):61-86.727
