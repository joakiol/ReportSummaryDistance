Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 578?587,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTowards a standard evaluation methodfor grammatical error detection and correctionMariano Felice and Ted BriscoeALTA Institute, Computer Laboratory, University of Cambridge15 JJ Thomson Avenue, Cambridge CB3 0FD, United Kingdom{mf501,ejb}@cl.cam.ac.ukAbstractWe present a novel evaluation method forgrammatical error correction that addressesproblems with previous approaches and scoressystems in terms of improvement on the orig-inal text.
Our method evaluates corrections atthe token level using a globally optimal align-ment between the source, a system hypothesis,and a reference.
Unlike the M2Scorer, ourmethod provides scores for both detection andcorrection and is sensitive to different types ofedit operations.1 IntroductionA range of methods have been applied to evaluationof grammatical error correction, but no entirely sat-isfactory method has emerged as yet.
Standard met-rics (such as accuracy, precision, recall and F -score)have been used, but they can lead to different resultsdepending on the criteria used for their computation(Leacock et al, 2014; Chodorow et al, 2012).Accuracy, for example, can only be computedin cases where we can enumerate all true nega-tives, which is why it has been mostly used forarticle and preposition errors (De Felice and Pul-man, 2008; Rozovskaya and Roth, 2010).
Extend-ing this approach to other error types involves theidentification of all relevant instances or positionswhere an error can occur, which is not always easyand renders the evaluation process costly, language-dependent, and possibly inexact.
Accuracy has alsobeen criticised as being a poor indicator of predictivepower, especially on unbalanced datasets (Manningand Sch?utze, 1999).Source: You have missed word.System hypothesis: You have missed a word.System edits: (?
a)Gold edits: (word?
a word) or(word?
words)Figure 1: Mismatch between system and gold standardedits producing the same corrected sentence.Alternatively, we can compute precision (P ), re-call (R) and F -score by comparing system edits togold-standard edits and thus circumvent the problemof counting true negatives.
This was the official eval-uation scheme adopted for the HOO 2011 (Dale andKilgarriff, 2011) and HOO 2012 (Dale et al, 2012)shared tasks.
However, these metrics can fail whenedits are not identical and therefore underestimatesystem performance (see Figure 1).This problem was later addressed by the Max-Match or M2Scorer (Dahlmeier and Ng, 2012),which is able to identify equivalent edits by apply-ing a transitive rule (e.g.
(?
a) + (word?
word)?
(word ?
a word)).
The scorer also allows formultiple gold standard annotations of each sentence,choosing the ones that maximise overall F -score.
Sofar, the M2Scorer has been the most reliable toolfor evaluating error correction systems and has beenused as the official scorer in the subsequent CoNLL2013 (Ng et al, 2013), CoNLL 2014 (Ng et al,2014) and EMNLP 2014 (Mohit et al, 2014) sharedtasks.
In 2014, system ranking was based on F0.5-score, weighting precision twice as highly as recall.Nevertheless, this method also suffers from anumber of limitations:578Source Annotator 1 Annotator 2This machines is designed for help people .
(This?
These), (is?
are),(help?
helping)(machines?
machine),(for?
to)System hypothesis System edits P R F0.5These machines are designed to help people .
(This?
These), (is?
are),(for?
to)0.67 0.67 0.67Table 1: The M2Scorer is unable to mix and match corrections from different annotators.Source Gold editsMachine is design to help people .
(Machine?Machines), (is design?
are designed)System hypothesis System edits P R F0.5Machine is designed to help people .
(design?
designed) 0.00 0.00 0.00Table 2: Partial matches are ignored by the M2Scorer.
(a) There is a limit to the number of unchangedwords allowed in an edit (2 by default), whosevalue affects final results.
(b) Given that the computed metrics rely on truepositive counts, a baseline system that does notpropose any correct edits will not produce in-formative results (P = 1 by definition, R = 0and F = 0).
The actual error rate and con-sequent potential for text improvement are nottaken into account.
(c) It is not possible to discriminate between a ?do-nothing?
baseline system and other systems thatonly propose wrong corrections, as they will allyield F = 0.
(d) System performance is underestimated whenusing multiple annotations for a sentence, sincethe scorer will choose the one that maximisesF -score instead of mixing and matching all theavailable annotations (see Table 1).
(e) Partial matches are ignored (see Table 2).
(f) Phrase-level edits can produce misleading re-sults, as they may not always reflect effectiveimprovements (see Table 3).
(g) The lack of a true negative count (i.e.
the num-ber of non-errors) precludes the computation ofaccuracy, which is useful for discriminating be-tween systems with F = 0.
(h) There is no clear indicator of improvement onthe original text after applying the suggestedcorrections, since an increase in P, R or F doesnot imply a reduction in the error rate (see Sec-tion 2.3.3).
(i) It is not clear how values of F should be in-terpreted (especially for F0.5), as there is noknown threshold that would signal improve-ment.
Ranking by F -score does not guaranteethat the top systems make the source text better.
(j) Detection scores are not computed.In addition, Leacock et al (2014) discuss key is-sues concerning system evaluation, such as the es-timation of true negatives and good practices for re-porting results, which are currently not addressed bythe M2scorer.2 Designing a new evaluation methodA better evaluation method should address the issuesdescribed above and use a metric that is meaningfuland easy to interpret.
We examine these and otherrelated problems, showing how they can be resolved.The proposed method uses tokens as the unit ofevaluation (instead of phrase-level edits), which pro-vides a stable unit of comparison and facilitates thecomputation of true negatives.
In turn, this providesa solution for problems 1.
(a), 1.
(e), 1.
(f) and 1.
(g).579Source Gold editsMachine is design to help people .
(Machine?Machines), (is?
are), (design?
designed)System hypothesis System edits P R F0.5The machine is designed for helpingpeople .
(Machine is?
The machine is),(design?
designed),(to help people?
for helping people)0.33 0.33 0.33Machines is a design on the helping ofthe people .
(Machine?Machines),(is design to help?
is a design on thehelping of the)0.50 0.33 0.45Table 3: The M2Scorer evaluates systems based on the number of edits, regardless of their length and their effect onthe final corrected sentence.
The first hypothesis is better than the second despite having a lower F0.5-score.The following sections describe the three pillarsof our method: a new annotation scheme, sentencealignment and metrics.2.1 AnnotationWe define a gold standard format where each sen-tence is annotated with a set of errors and their possi-ble corrections.
A sentence can contain zero or moreerrors, each of which includes information such astype, a flag indicating whether a correction is re-quired, and a list of alternative corrections corre-sponding to each of the annotators.
An error is re-quired to be corrected when all annotators provide acorrection for it.Unlike in other annotation schemes, each error isdefined by its locus (regardless of the position of theincorrect tokens in the sentence) and all its alterna-tive corrections must be mutually exclusive.
In otherwords, corrections are grouped whenever they referto the same underlying error, even if the tokens in-volved are not contiguous.
Listing 1 shows a sampleXML annotation for the sentence in Table 1.Because all the correction alternatives are mutu-ally exclusive, we can directly combine them to gen-erate all possible valid gold standard references.
Theannotation in Listing 1 would produce the followinglist of references:These machines are designed for helping people .These machines are designed to help people .This machine is designed for helping people .This machine is designed to help people .<sentence id="1" numann="2"><text>This machines is designed for helppeople .</text><error-list><error id="1" req="yes" type="SVA"><alt ann="0"><c start="0" end="1">These</c><c start="2" end="3">are</c></alt><alt ann="1"><c start="1" end="2">machine</c></alt></error><error id="2" req="yes" type="Vform"><alt ann="0"><c start="5" end="6">helping</c></alt><alt ann="1"><c start="4" end="5">to</c></alt></error></error-list></sentence>Listing 1: An example annotated sentence.By mixing and matching corrections from differ-ent annotators, we avoid the performance underesti-mation described in 1.
(d).2.2 AlignmentIn order to compute matches for detection and cor-rection, we generate a token-level alignment be-tween a source sentence, a system?s hypothesis, anda gold standard reference.
Three-way alignments580are a special case of multiple sequence alignment,a well-known string matching problem in computa-tional biology (Mount, 2004).We generate an exact (globally optimal) align-ment using a dynamic programming implementa-tion of the Sum of Pairs (SP) alignment (Carrilloand Lipman, 1988), shown in Listing 2.
Under thismodel, the score of a multiple alignment is the sumof the scores of each pairwise alignment, so that aglobally optimal alignment has minimum SP score.Time and space complexity of the dynamic program-ming implementation for k strings of length n isO(nk), which is acceptable for three average-lengthsentences but can quickly become impractical for alarger number of sequences.In computational biology, edit costs are defined interms of mutation probabilities, which are irrelevantto our task.
However, we can find new optimal costsby defining a set of constraints that are meaningfulfor error correction:(a) Matches have zero cost (cmatch= 0).
(b) Gaps (insertions or deletions) are more costlythan matches (cgap> cmatch).
(c) Mismatches (substitutions) are set to be morecostly than gaps (insertions or deletions) so asto maximise matches (cmis> cgap).Given these constraints, we can set cgap= 1and cmis= 2; however, they will not necessarilykeep gaps aligned (see Table 4).
To ensure this, wemust place a new constraint on the SP algorithm sothat a gap-aligned version (desired alignment) hasa lower cost than a gap-unaligned version (initialalignment):cost(A,?)
+ ...+ cost(B,?)
> cost(A,C) + ...+ cost(B,?
)cgap+ ...+ cgap> cmis+ ...+ cgap4cgap+ cmis> 2cmis+ 2cgap2cgap> cmisTherefore 2cgap> cmis> cgap> cmatch.
For ourimplementation, we adopted cgap= 2 and cmis= 3.There can be more than one optimal alignment fora given set of strings.
Some of these alignments willlook more intuitive than others (see Table 5) but theyare equally optimal for our evaluation method andwill produce the same final results.Initial alignment Desired alignmentA B A B?
C C ?A ?
A ?Table 4: Initial and desired alignments showing differ-ences in the distribution of gaps./*Initialisation*/cmatch:= cost of matchcmis:= cost of mismatchcgap:= cost of gapD[0, 0, 0] := 0D1,2[i, j] := edit_distance(S1[1..i], S2[1..j])D1,3[i, k] := edit_distance(S1[1..i], S3[1..k])D2,3[j, k] := edit_distance(S2[1..j], S3[1..k])/*Recurrences for boundary cells*/D[i, j, 0] := D1,2[i, j] + (i + j)*cgap,D[i, 0, k] := D1,3[i, k] + (i + k)*cgap,D[0, j, k] := D2,3[j, k] + (j + k)*cgap,/*Recurrences for non-boundary cells*/for i := 1 to n1dofor j := 1 to n2dofor k := 1 to n3dobeginif (S1[i] = S2[j]) then cij := cmatchelse cij := cmis;if (S1[i] = S3[k]) then cik := cmatchelse cik := cmis;if (S2[j] = S3[k]) then cjk := cmatchelse cjk := cmis;d1 := D[i-1, j-1, k-1] + cij + cik + cjk;d2 := D[i-1, j-1, k] + cij + 2*cgap;d3 := D[i-1, j, k-1] + cik + 2*cgap;d4 := D[i, j-1, k-1] + cjk + 2*cgap;d5 := D[i-1, j, k] + 2*cgap;d6 := D[i, j-1, k] + 2*cgap;d7 := D[i, j, k-1] + 2*cgap;D[i, j, k] := Min(d1,d2,d3,d4,d5,d6,d7);end;Listing 2: The Sum of Pairs dynamic programming algo-rithm for the alignment of three sequences, S1, S2and S3(adapted from Gusfield (1997)).2.3 MetricsOnce we have an optimal alignment between asource, a hypothesis and a reference, we compute anumber of metrics that measure different aspects ofperformance and can be used for ranking systems.581Their is wide spread usage of technology .
A A A A A A A AThere is widespread use of technology .
?
B A B B - A A AThere is widespread use of technology .
B A B B - A A ATheir is wide spread usage of technology .
A A A A A A A AThere is widespread use of technology .
?
B A B - B A A AThere is widespread use of technology .
B A B - B A A ATable 5: Two equally optimal alignments under the SP alignment model.Tokens ClassificationSource Hypothesis Reference Detection Correctiona a a TN TNa a b FN FNa a - FN FNa b a FP FPa b b TP TPa b c TP FP, FN, FPNa b - TP FP, FN, FPNa - a FP FPa - b TP FP, FN, FPNa - - TP TP- a a TP TP- a b TP FP, FN, FPN- a - FP FP- - a FN FNTable 6: Our extended WAS evaluation scheme.The limitation in 1.
(j) is addressed by computingthese metrics for both detection and correction.We adopt an extended version of the Writer-Annotator-System (WAS) evaluation scheme(Chodorow et al, 2012) where each token align-ment is classified as a true positive (TP), truenegative (TN), false positive (FP) or false negative(FN).
As noted by Chodorow et al (2012), caseswhere source 6= hypothesis 6= reference1are botha FP and a FN for correction,2so we introduce anew FPN class to count such cases and adjust ourmetrics accordingly.
Our extended WAS scheme isshown in Table 6.With these counts, we can compute P , R and F?using their standard definitions:P =TPTP + FPR =TPTP + FNF?= (1 + ?2) ?P ?R(?2?
P ) +R1Note that we use different terminology where source =writer, hypothesis = system and reference = annotator.2From a correction perspective, an alignment where a 6=b 6= c generates a FP for the b class and a FN for the c class.As mentioned in Section 1, the F measure doesnot shed light on the error rates in the data and is un-able to discriminate between a ?do-nothing?
baselineand other systems unless TP > 0.
However, becausewe now have a TN count, we can address problems1.
(b) and 1.
(c) by computing accuracy (Acc) as fol-lows:Acc =TP + TNTP + TN + FP + FN?
FPNUnlike in information retrieval, for example,where the whole document collection is usually un-known to the user so TNs are perhaps less relevant,the sentences fed into an error correction system willbe provided by users.
In this context, TNs are rele-vant because they indicate what parts of the text arealready correct, allowing users to focus on problem-atic regions.
For this reason, accuracy seems a moreappropriate measure of text quality than F -score.2.3.1 Weighted accuracyAccuracy treats all counts equally, which has twomain side effects.
A system that introduces the samenumber of TPs and FPs will have the same accuracyas the ?do-nothing?
baseline, in which case we wouldprefer to keep the original text and rank the systemlower, in accord with the choice of F0.5for evaluat-ing the 2014 shared task.
Accuracy is also unable todiscriminate between systems with different TP andTN counts if their sum is the same.It is clear that for error correction these countsshould be weighted differently.
In particular, wewould like to:?
Reward correction more than preservation (i.e.weightTP> weightTN).?
Penalise unnecessary corrections more than un-corrected errors (i.e.
weightFP> weightFN).5820.00.10.20.30.40.50.60.70.80.91.01 2 3 4 5 6 7 8 9 10 11 12 13 14 15WAccwAll correctAbove baselineBaselineBelow baselineAll incorrectFigure 2: Effect of w on weighted accuracy (WAcc).We can reformulate accuracy to satisfy these con-ditions by including a weight factor w > 1:WAcc =w ?
TP + TNw ?
TP + TN + w ?
(FP?FPN2)+(FN?FPN2)=w ?
TP + TNw ?
TP + TN + w ?
FP?
w ?FPN2+ FN?FPN2=w ?
TP + TNw ?
(TP + FP) + TN + FN?
(w + 1) ?FPN2Higher values of w will reward and penalise sys-tems more heavily, bringing those below the base-line closer to the lower bound and those above thebaseline closer to the upper bound (see Figure 2).As w increases, differences between WAccsysand itsbounds become less pronounced, which is why weadopt w = 2.
Regardless of w, WAcc will alwaysreduce to Acc for the ?do-nothing?
baseline.2.3.2 Metric behaviourBefore we set out to evaluate and compare sys-tems, we must understand how metrics behave andto what extent they are comparable.Table 6 indicates that the metrics will always pro-duce the same results for detection and correctionunless source 6= hypothesis 6= reference for at leastone position in the alignment.
A ?do-nothing?
base-line will always produce the same results for bothaspects, since source = hypothesis for all positions.Whenever a gold standard allows for alternativecorrections, references that maximise the target met-ric should be chosen.
Nevertheless, we note that the(maximum) score obtained by a system only appliesto a given set of chosen references and is thereforeonly directly comparable to results on the same ref-erence set.System Chosen references P R F0.5S11.2, 2.1, 3.1 0.60 0.20 0.43S21.2, 2.1, 3.1 0.80 0.05 0.20S11.1, 2.1, 3.2 0.30 0.30 0.30S21.1, 2.1, 3.2 0.30 0.40 0.32Table 7: S1outperforms S2in terms of overall F0.5but S2outperforms S1when evaluated on different references.To illustrate this, consider two systems (S1andS2) evaluated on a gold standard containing 3 sen-tences with 2 correction alternatives each (i.e.
sixpossible references: 1.1, 1.2, 2.1, 2.2, 3.1 and 3.2respectively).
Table 7 shows that, while S1achievesa higher maximum score than S2, comparing theirF0.5scores directly is not possible as they are com-puted on a different set of references.
In fact, S2could outperform S1on other reference sets.2.3.3 Measuring improvementWe know that whenever P > 0.5, the error ratedecreases (and therefore Acc increases) so the textis improved.3However, an increase in P , R or Falone does not necessarily imply an increase in Accor WAcc, as illustrated in Table 8.In order to determine whether a system improveson the source text, we must compare its performance(WAccsys) with that of the baseline (WAccbase).
Be-cause each WAccsysis computed from a different setof references, we must compute WAccbaseindivid-ually for each system using its chosen references.This is done by using the source sentence as the hy-pothesis in the existing alignment.
Once we haveWAccsysand WAccbasefor each system, we can com-pare them to determine if the text has improved.When these two values are equal, there is no ben-efit to deploying the system.If we want to compare and rank systems, we needto measure how much the text has been improvedor degraded.
This can be done using a baseline-normalised metric that measures relative coverage ofthe area between the baseline and WAcc bounds (seeFigure 3).
This metric, henceforth Improvement or3In theory, applying more correct edits than incorrect editswill yield a positive balance.
However, in practice, this dependson the edits, especially if they are variable-length phrases.
TheP > 0.5 criterion also only holds for Acc and not WAcc, as thelatter modifies the original proportions by introducing weights.583System TP FP TN FN P R F0.5Acc WAccBaseline 0 0 6 4 1.00 0.00 0.00 0.60 0.60S14 1 5 0 0.80 1.00 0.83 0.90 0.87S21 0 6 3 1.00 0.25 0.62 0.70 0.73S31 1 5 3 0.50 0.25 0.42 0.60 0.58S44 6 0 0 0.40 1.00 0.45 0.40 0.40Table 8: An increase in P , R or F does not necessarily translate into an increase in Acc, assuming all systems areevaluated on the same set of references.0 1WAccbaseWAcca WAccb{{IMPROVEMENT AREADEGRADATION AREA{1.00 +1.000Ia IbFigure 3: Graphical representation of improvement fortwo hypothetical systems, a and b.
Values of I are shownat the top while values of WAcc are shown at the bottom.Value Interpretation1 100% improvement (100% correct text).> 0 Relative improvement.0 Baseline performance (no change).< 0 Relative degradation.-1 100% degradation (100% incorrect text).Table 9: Interpretation of I values.I , is defined as:I =????????????????
?bWAccsysc if WAccsys= WAccbaseWAccsys?WAccbase1?WAccbaseif WAccsys> WAccbaseWAccsysWAccbase?
1 otherwiseValues of I lie in the [?1; 1] interval and shouldbe interpreted as per Table 9.
The use of this metricprovides a solution to problems 1.
(h) and 1.
(i).The I-measure should be computed after max-imising system WAcc at the sentence level, so as toensure all the evaluated hypotheses are paired withtheir highest scoring references.3 Experiments and resultsWe tested our evaluation method by re-ranking sys-tems in the CoNLL 2014 shared task on grammaticalerror correction.
Re-ranking was limited to the 12participating teams that made their system?s outputpublicly available.For the gold standard, we used the shared task testset containing corrections from the two official an-notators as well as alternative corrections providedby three participating teams.
This version allowedus to generate many more references than the origi-nal test set and thus reduce annotator bias.The corrections extracted from the gold standardwere automatically clustered into groups of inde-pendent errors based on token overlap.
This meansthat overlapping corrections from different annota-tors are considered to be mutually exclusive (i.e.alternative) corrections of the same error and aretherefore grouped together (the error elements inListing 1).
Provided the original annotations are cor-rect, the combination of alternatives will generate allpossible valid references.
Sentences containing cor-rections that could not be automatically clustered be-cause they require human knowledge were excluded,leaving a subset of 711 sentences (out of 1,312).We restrict our analysis to correction, since that isthe only aspect reported by the M2Scorer.
Table 10shows the results of the M2Scorer using the originalannotations as well as a modified version containingmixed-and-matched corrections.
Results of our pro-posed evaluation method are included in Table 11.As expected, rankings are clearly distinct betweenthe two methods, as they use different units of evalu-ation (phrase-level edits vs tokens) and maximisingmetrics (F0.5vs WAcc).
Results show that only theUFC system is able to beat the baseline (by a smallbut statistically significant margin), being also theone with consistently highest P (much higher thanthe rest).These rankings are affected by the fact that sys-tems were probably optimised for F0.5during de-velopment, as it was the official evaluation metric584System TP TN FP FN FPN P R F0.5Acc WAcc WAccbaseI ?UFC 19 13062 7 665 2 73.08 2.78 12.06 95.13 95.09 95.03 1.35BASELINE 0 13078 0 673 0 100.00 0.00 0.00 95.11 95.11 95.11 0.00IITB 11 13057 26 668 4 29.73 1.62 6.65 94.98 94.82 95.06 -0.25SJTU 54 12947 114 649 8 32.14 7.68 19.64 94.51 93.79 94.89 -1.16CUUI 290 12697 337 553 34 46.25 34.40 43.27 93.82 91.86 93.91 -2.18PKU 128 12800 283 625 66 31.14 17.00 26.70 93.89 92.28 94.53 -2.38AMU 219 12761 322 556 41 40.48 28.26 37.26 93.94 92.06 94.39 -2.47UMC 179 12761 314 603 26 36.31 22.89 32.50 93.56 91.67 94.35 -2.84IPN 25 12848 251 680 40 9.06 3.55 6.91 93.53 92.00 94.88 -3.04POST 231 12588 454 574 46 33.72 28.70 32.58 92.88 90.23 94.17 -4.18RAC 147 12723 426 623 49 25.65 19.09 24.00 92.79 90.28 94.45 -4.41CAMB 386 12402 641 502 78 37.59 43.47 38.63 92.31 88.77 93.59 -5.15NTHU 196 12620 521 575 54 27.34 25.42 26.93 92.48 89.44 94.44 -5.29Table 11: Results of our new evaluation method (in percentages).
All values of I are statistically significant (two-tailedpaired T-test, p < 0.01).SystemOriginal annotations Mixed annotationsP R F0.5?
P R F0.5?CUUI 47.66 33.87 44.07 47.57 39.60 45.73AMU 44.68 29.44 40.48 44.56 33.49 41.80CAMB 39.22 41.65 39.69 39.04 48.72 40.66POST 36.39 29.13 34.67 36.39 33.79 35.84NTHU 33.56 28.10 32.31 33.62 31.52 33.18UMC 34.86 20.86 30.73 34.86 23.31 31.71RAC 33.67 19.08 29.21 33.67 21.59 30.28PKU 32.17 19.60 28.51 32.42 21.63 29.48SJTU 28.00 7.08 17.60 28.00 7.46 18.06UFC 73.08 3.26 13.83 73.08 3.39 14.31IPN 9.16 3.87 7.20 9.16 4.09 7.34IITB 30.30 1.74 7.07 30.30 1.81 7.31BASELINE 100.00 0.00 0.00 100.00 0.00 0.00Table 10: M2Scorer results (in percentages).for the shared task.
Rankings by F0.5are almostidentical for the two methods (Spearman?s rank cor-relation is 0.9835 with p < 0.01), suggesting thatthere is a statistically significant difference betweenphrase-level edits and tokens, despite phrases beingonly 1.12 tokens on average in this dataset.Spearman?s ?
between both scorers (F0.5vs I)is ?0.5330, which suggests they generally produceinverse rankings.
Pearson?s correlation betweentoken-level F0.5and I is ?0.5942, confirming therelationship between rankings and our intuition thatF0.5is not a good indicator of overall correctionquality.
While the I-measure reflects improvement,F0.5indicates error manipulation.
We argue that I isbetter suited to the needs of end-users (as it indicateswhether the output of the system is better than theoriginal text) whereas F0.5is more relevant to sys-tem developers (since they need to analyse P and Rin order to tune their systems).Lastly, we verify that mixing and matching cor-rections from different annotators improves R (seeTable 10) and ensures systems are always assignedthe maximum possible score.4 DiscussionAutomatic evaluation metrics that are based on com-parisons with a gold standard are inherently limitedby the number of available references.
Although thisdoes not pose much problem for tasks such as part-of-speech tagging, it does constrain evaluation fortext generation tasks (such as error correction, ma-chine translation or summarisation), where the num-ber of ?correct answers?
goes beyond a few collectedreferences.Sentences can be corrected in many differentways and the fact that a given correction is notmatched by any of the references does not neces-sarily mean that it is not valid.
Therefore, we mustaccept that any metric used in such scenarios willnot be perfect.
However, it is worth noting that thislimitation does not extend to evaluation of error de-tection per se using such metrics.Finding independent evidence to support one cor-rection over another is also difficult, since the notionof sentence quality is somewhat subjective.
Eval-uation metrics that rely on a gold standard are es-585System hypothesesBestF0.5Ia.
The son was died after one year ?s treatment and a couple got divorced later after that .?b.
The son had died after one year ?s and the couple got divorced later after that .?a.
Although there might be a lot of challenges along the way in seeking medical attention ,such as a financial issues , everyone should be given right of knowing their family ?sinherented medical conditions .?b.
Although there might be a lot of challenges along the way in seeking medical attention ,such as finance , everyone should be given the right of knowing their family ?sinherented medical conditions .?a.
Taking Angeline Jolie , for example , she is famous but she still reveal the truth abouther genetic testing to the development of her breast cancer risk .?b.
Taking Angeline Jolie for example , she is famous but she still revealed the truth abouther genetic testing on the development of her breast cancer risk .
?Table 12: Example hypotheses produced by two error correction systems (a and b).
The last two columns indicate thehighest-scoring hypothesis from each pair according to each evaluation metric.sentially distance metrics, but judging between hy-potheses without looking at the source or referencesentences is a distinct task, which is more similar tosentence quality estimation for machine translationoutput.Our evaluation method overcomes many of thelimitations of previous approaches by using a stableunit of evaluation, weighting edit operations in linewith the goals of grammatical error correction andmaking the most of the available annotations.
Val-ues of F are always positive, with no clear interpre-tation or threshold that would indicate improvementof the original text whereas the I-measure providesmeaningful indicators (I < 0 for degradation, I = 0for no change and I > 0 for improvement).
Table12 shows a few examples where the M2Scorer dif-fers from our method, revealing how the I-measureis able to pick hypotheses in accord with (at leastour) intuitions.5 ConclusionWe have presented a new evaluation method forgrammatical error detection and correction thatovercomes many of the limitations of previous ap-proaches and provides more meaningful indicatorsof system performance.The method is designed to evaluate improvementin correction of the input text by analysing post-system error rate.
Improvement is measured usinga reformulation of accuracy where TPs and FPs areweighted higher than TNs and FNs, in an attemptto model desirable aspects of correction.
We alsocombine individual corrections from different anno-tators, as this improves R and ensures systems getthe maximum possible score from the available an-notations.Experiments show I and F0.5are inversely corre-lated and account for different aspects of system per-formance.
Choosing one metric over the other posesa fundamental question about the aims of error cor-rection, whether we prefer a system that tackles fewerrors but improves the original text or one that han-dles many more errors but degrades the original.
Webelieve that, from a user perspective, a system thatreliably improves text is more desirable.Future work might usefully explore automatedsentence quality estimation, as a component bothof grammatical error correction systems and of theirevaluation, in order to ameliorate the issue that anyset of gold standard references will underspecify theset of possible corrections.An open-source implementation of our evalua-tion method is available for download at https://github.com/mfelice/imeasure.586AcknowledgmentsWe would like to thank ?istein Andersen and ZhengYuan for their constructive feedback, as well as theanonymous reviewers for their comments and sug-gestions.
We are also grateful to Cambridge EnglishLanguage Assessment for supporting this researchvia the ALTA Institute.ReferencesHumberto Carrillo and David Lipman.
1988.
The mul-tiple sequence alignment problem in biology.
SIAM J.Appl.
Math., 48(5):1073?1082, October.Martin Chodorow, Markus Dickinson, Ross Israel, andJoel Tetreault.
2012.
Problems in evaluating gram-matical error detection systems.
In Proceedings ofCOLING 2012, pages 611?628, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Better evalu-ation for grammatical error correction.
In Proceedingsof the 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, NAACL 2012, pages568 ?
572, Montreal, Canada.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 Pilot Shared Task.
In Pro-ceedings of the Generation Challenges Session at the13th EuropeanWorkshop on Natural Language Gener-ation, pages 242?249, Nancy, France, September.
As-sociation for Computational Linguistics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Prepositionand Determiner Error Correction Shared Task.
InProceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pages 54?62,Montr?eal, Canada, June.
Association for Computa-tional Linguistics.Rachele De Felice and Stephen G. Pulman.
2008.
AClassifier-Based Approach to Preposition and Deter-miner Error Correction in L2 English.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics (COLING 2008), pages 169?176,Manchester, UK, August.
COLING 2008 OrganizingCommittee.Dan Gusfield.
1997.
Algorithms on Strings, Trees andSequences.
Cambridge University Press.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2014.
Automated GrammaticalError Detection for Language Learners, Second edi-tion.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Christopher D. Manning and Hinrich Sch?utze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press, Cambridge, Massachusetts.Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-jdi Zaghouani, and Ossama Obeid.
2014.
The FirstQALB Shared Task on Automatic Text Correction forArabic.
In Proceedings of the EMNLP 2014 Work-shop on Arabic Natural Language Processing (ANLP),pages 39?47, Doha, Qatar, October.
Association forComputational Linguistics.David W. Mount.
2004.
Bioinformatics: Sequence andGenome Analysis, Second edition.
Cold Spring HarborLaboratory Press.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-tian Hadiwinoto, and Joel Tetreault.
2013.
TheCoNLL-2013 Shared Task on Grammatical Error Cor-rection.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 1?12, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 Shared Taskon Grammatical Error Correction.
In Proceedings ofthe Eighteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 1?14, Balti-more, Maryland, June.
Association for ComputationalLinguistics.Alla Rozovskaya and Dan Roth.
2010.
Trainingparadigms for correcting errors in grammar and us-age.
In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 154?162, Los Angeles, California.
Associationfor Computational Linguistics.587
