Semantic Parsing Based on FrameNetCosmin Adrian Bejan, Alessandro Moschitti, Paul Mora?rescu,Gabriel Nicolae and Sanda HarabagiuUniversity of Texas at DallasHuman Language Technology Research InstituteRichardson, TX 75083-0688, USAAbstractThis paper describes our method based on SupportVector Machines for automatically assigning seman-tic roles to constituents of English sentences.
Thismethod employs four different feature sets, one ofwhich being first reported herein.
The combinationof features as well as the extended training data weconsidered have produced in the Senseval-3 experi-ments an F1-score of 92.5% for the unrestricted caseand of 76.3% for the restricted case.1 IntroductionThe evaluation of the Senseval-3 task for AutomaticLabeling of Semantic Roles is based on the annota-tions made available by the FrameNet Project (Bakeret al, 1998).
The idea of automatically identifyingand labeling frame-specific roles, as defined by thesemantic frames, was first introduced by (Gildea andJurasfky, 2002).
Each semantic frame is character-ized by a set of target words which can be nouns,verbs or adjectives.
This helps abstracting the the-matic roles and adding semantics to the given frame,highlighting the characteristic semantic features.Frames are characterized by (1) target words orlexical predicates whose meaning includes aspects ofthe frame; (2) frame elements (FEs) which representthe semantic roles of the frame and (3) examples ofannotations performed on the British National Cor-pus (BNC) for instances of each target word.
ThusFrameNet frames are schematic representations ofsituations lexicalized by the target words (predicates)in which various participants and conceptual rolesare related (the frame elements), exemplified by sen-tences from the BNC in which the target words andthe frame elements are annotated.In Senseval-3 two different cases of automatic la-beling of the semantic roles were considered.
TheUnrestricted Case requires systems to assign FE la-bels to the test sentences for which (a) the bound-aries of each frame element were given and the tar-get words identified.
The Restricted Case requiressystems to (i) recognize the boundaries of the FEsfor each evaluated frame as well as to (ii) assign alabel to it.
Both cases can be cast as two differentclassifications: (1) a classification of the role whenits boundaries are known and (2) a classification ofthe sentence words as either belonging to a role ornot1.A similar approach was used for automati-cally identifying predicate-argument structures inEnglish sentences.
The PropBank annotations(www.cis.upenn.edu/?ace) enable training for twodistinct learning techniques: (1) decision trees (Sur-deanu et al, 2003) and (2) Support Vector Machines(SVMs) (Pradhan et al, 2004).
The SVMs producedthe best results, therefore we decided to use the samelearning framework for the Senseval-3 task for Auto-matic Labeling of Semantic Roles.
Additionally, wehave performed the following enhancements:?
we created a multi-class classifier for each frame,thus achieving improved accuracy and efficiency;?
we combined some new features with features from(Gildea and Jurasfky, 2002; Surdeanu et al, 2003;Pradhan et al, 2004);?
we resolved the data sparsity problem generatedby limited training data for each frame, when usingthe examples associated with any other frame fromFrameNet that had at least one FE shared with eachframe that was evaluated;?
we crafted heuristics that improved mappings fromthe syntactic constituents to the semantic roles.We believe that the combination of these four exten-sions are responsible for our results in Senseval-3.The remainder of this paper is organized as fol-lows.
Section 2 describes our methods of classify-ing semantic roles whereas Section 3 describes ourmethod of identifying role boundaries.
Section 4 de-tails our heuristics and Section 5 details the exper-imental results.
Section 6 summarizes the conclu-sions.1The second classification represents the detection of roleboundaries.
The semantic parsing defined as two different clas-sification tasks was introduced in (Gildea and Jurasfky, 2002).Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systems2 Semantic role classificationThe result of the role classifier on a sentence, as il-lustrated in Figure 1, is the identification of semanticroles of the FEs when the boundaries of each FE areknown.
To be able to assign the labels of each FE, weused three sets of features.
Feature Set 1, illustratedin Figure 2 was used in the work reported in (Gildeaand Jurasfky, 2002).VPSVPNPPeople were fasteningTARGETAgentNPa ropePPto the ringNPGoalItemFigure 1: Sentence with annotated frame elements.?
POSITION (pos) ?
Indicates if the constituent appearsbefore or after the the predicate in the sentence.?
VOICE (voice) ?
This feature distinguishes betweenactive or passive voice for the predicate phrase.are preserved.of the evaluated phrase.
Case and morphological information?
HEAD WORD (hw) ?
This feature contains the head word?
PARSE TREE PATH (path): This feature contains the pathin the parse tree between the predicate phrase and thelabels linked by direction symbols (up or down), e.g.?
PHRASE TYPE (pt): This feature indicates the syntacticnoun phrases only, and it indicates if the NP is dominatedby a sentence phrase (typical for subject arguments withactive?voice predicates), or by a verb phrase (typicalfor object arguments).?
GOVERNING CATEGORY (gov) ?
This feature applies totype of the phrase labeled as a frame element, e.g.target word, expressed as a sequence of nonterminal?
TARGET WORD ?
In our implementation this feature(2) LEMMA which represents the target normalized to lowerthe case and morphological information preserved; andconsists of two components: (1) WORD: the word itself withcase and infinitive form for the verbs or singular for nouns.NP for Agent in Figure 1.NP    S    VP    VP for Agent in Figure 1.Figure 2: Feature Set 1 (FS1)Feature Set 2 was introduced in (Surdeanu et al,2003) and it is illustrated in Figure 3.
The CON-TENT WORD (cw) feature illustrated in Figure 3applies to PPs, SBARs and VPs, as it was reportedin (Surdeanu et al, 2003).
For example, if the PP is?in the past month?, instead of using ?in?, the headof the PP, as a feature, ?month?, the head of the NPis selected since it is more informative.
Similarly, ifthe SBAR is ?that occurred yesterday?, instead of us-ing the head ?that?
we select ?occurred?, the head ofthe VP.
When the VP ?to be declared?
is considered,?declared?
is selected over ?to?.Feature set 3 is a novel set of features introducedin this paper and illustrated in Figure 4.
Someof the new features characterize the frame, e.g.the frame name (FRAME-NAME); the frame FEs,(NUMBER-FEs); or the target word associated withthe frame (TAGET-TYPE).
Additional characteriza-tion of the FEs are provided by by the GRAMMATI-CAL FUNCTION feature and by the list of grammat-ical functions of all FEs recognized in each sentence(LIST Grammatical Function feature).BOOLEAN NAMED ENTITY FLAGS ?
A feature set comprising:?
neOrganization: set to 1 if an organization is recognized in the phrase?
neLocation: set to 1 a location is recognized in the phrase?
nePerson: set to 1 if a person name is recognized in the phrase?
neMoney: set to 1 if a currency expression is recognized in the phrase?
nePercent: set to 1 if a percentage expression is recognized in the phrase?
neTime: set to 1 if a time of day expression is recognized in the phrase?
neDate: set to 1 if a date temporal expression is recognized in the phraseword from the constituent, different from the head word.?
CONTENT WORD (cw) ?
Lexicalized feature that selects an informativePART OF SPEECH OF HEAD WORD (hPos) ?
The part of speech tag ofthe head word.PART OF SPEECH OF CONTENT WORD (cPos) ?The part of speechtag of the content word.NAMED ENTITY CLASS OF CONTENT WORD (cNE) ?
The class ofthe named entity that includes the content wordFigure 3: Feature Set 2 (FS2)In FrameNet, sentences are annotated with thename of the sub-corpus.
There are 12,456 possi-ble names of sub-corpus.
For the 40 frames eval-uated in Senseval-3, there were 1442 names asso-ciated with the example sentences in the trainingdata and 2723 names in the test data.
Three ofthe most frequent sub-corpus names are: ?V-trans-other?
(frequency=613), ?N-all?
(frequency=562)and ?V-trans-simple?(frequency=560).
The nameof the sub-corpus indicates the relations betweenthe target word and some of its FEs.
For ex-ample, the ?V-trans-other?
name indicated that thetarget word is a transitive verb, and thus its FEsare likely to have other roles than object or indi-rect object.
A sentence annotated with this sub-corpus name is: ?Night?s coming, you can seethe black shadow on [Self?mover the stones] that[TARGET rush] [Pathpast] and [Pathbetween yourfeet.?].
For this sentence both FEs with the role ofPath are neither objects or indirect objects of thetransitive verb.Feature SUPPORT VERBS considers the usage ofsupport expressions in FrameNet.
We have foundthat whenever adjectives are target words, their se-mantic interpretation depends on their co-occurrencewith verbs like ?take?, ?become?
or ?is?.
Supportverbs are defined as those verbs that combine with astate-noun, event-noun or state-adjective to create averbal predicate, allowing arguments of the verb toserve as FEs of the frame evoked by the noun or theadjective.The CORENESS feature takes advantage of amore recent implementation concept of core FEs(vs. non-core FEs) in FrameNet.
More specifi-cally, the FrameNet developers classify frame ele-ments in terms of how central they are to a particularframe, distinguishing three levels: core, peripheraland extra-thematic.The features were used to produce two types ofexamples: positive and negative examples.
For eachFE of a frame, aside from the positive examples ren-dered by the annotations, we considered as negativeexamples all the annotations of the other FEs for thesame frame.
The positive and the negative exampleswere used for training the multi-class classifiers.SUPPORT_VERBS that are recognized for adjective or noun target wordstarget word.
The values of this feature are either (1) The POS of the headof the VP containing the target word or (2) NULL if the target word doesnot belong to a VPor ADJECTIVELIST_CONSTITUENT (FEs): This feature represents a list of the syntacticGrammatical Function: This feature indicates whether the FE is:?
an External Argument (Ext)?
an Object (Obj)?
a Complement (Comp)?
a Modifier (Mod)?
Head noun modified by attributive adjective (Head)?
Genitive determiner (Gen)?
Appositive (Appos)LIST_Grammatical_Function: This feature represents a list of thegrammatical functions of the FEs recognized in the sentence.in each sentence.FRAME_NAME: This feature indicates the name of the semantic framefor which FEs are labeledCOVERAGE: This feature indicates whether there is a syntactic structurein the parse tree that perfectly covers the FEa conceptually necessary participant of a frame.
For example, in theare: (1) core; (2) peripheral and (3) extrathemathic.
FEs that mark notionssuch as Time, Place, Manner and Degree are peripheral.
ExtrathematicFEs situate an event against a backdrop of another event, by evokinga larger frame for which the target event fills a role.SUB_CORPUS: In FrameNet, sentences are annotated with the nameof the subcorpus they belong to.
For example, for a verb target word,to a FE included in a relative clause headed by a wh?word.
(2) a hyponym of sense 1 of PERSON in WordNet(1) a personal pronoun orHUMAN: This feature indicates whether the syntactic phrase is eitherTARGET?TYPE: the lexical class of the target word, e.g.
VERB, NOUNconsituents covering each FE of the frame recognized in a sentence.For the example illustrated in Figure 1, the list is: [NP, NP, PP]NUMBER_FEs: This feature indicates how many FEs were recognizedhave the role of predicate for the FEs.
For example, if the target word is"clever" in the sentence "Smith is very clever, but he?s no Einstein", thethe FE "Smith" is an argument of the support verb "is"?
rather than of theCORENESS: This feature indicates whether the FE instantiatesREVENGE frame, Punishment is a core element.
The valuesV?swh represents a subcorpus in which the target word is a predicateFigure 4: Feature Set 3 (FS3)Our multi-class classification allows each FE to beinitially labeled with more than one role when sev-eral classifiers decide so.
For example, for the AT-TACHING frame, an FE may be labeled both as Goaland as Item if the classifiers for the Goal and Itemselect it as a possible role.
To choose the final label,we select the classification which was assigned thelargest score by the SVMs.PARSE TREE PATH WITH UNIQUE DELIMITER ?
This feature removesthe direction in the path, e.g.
VBN?VP?ADVPPARTIAL PATH ?
This feature uses only the path from the constituent tothe lowest common ancestor of the predicate and the constituentFIRST WORD ?
First word covered by constituentFIRST POS ?
POS of first word covered by constituentLEFT CONSTITUENT ?
Left sibling constituent labelRIGHT HEAD ?
Right sibling head wordRIGHT POS HEAD ?
Right sibling POS of head wordLAST POS ?
POS of last word covered by the constituentLEFT HEAD ?
Left sibling head wordLEFT POS HEAD ?
Left sibling POS of head wordRIGHT CONSTITUENT ?
Right sibling constituent labelPP PREP ?
If constituent is labeled PP get first word in PPDISTANCE ?
Distance in the tree from constituent to the target wordLAST WORD ?
Last word covered by the constituentFigure 5: Feature Set 4 (FS4)3 Boundary DetectionThe boundary detection of each FE was requiredin the Restricted Case of the Senseval-3 evalua-tion.
To classify a word as belonging to an FEor not, we used all the entire Feature Set 1 and2.
From the Feature Set 3 we have used onlyfour features: the Support- Verbs feature; theTarget-Type feature, the Frame-Name featureand the Sub Corpus feature.
For this task we havealso used Feature Set 4, which were first introducedin (Pradhan et al, 2004).
The Feature Set 4 is il-lustrated in Figure 5.
After the boundary detec-tion was performed, the semantic roles of each FEwere assigned using the role classifier trained for theRestricted Case4 HeuristicsFrequently, syntactic constituents do not cover ex-actly FEs.
For the Unrestricted Case we imple-mented a very simple heuristic: when there is noparse-tree node that exactly covers the target role rbut a subset of adjacent nodes perfectly match r,we merge them in a new NPmerge node.
For theRestricted Case, a heuristic for adjectival and nomi-nal target words w adjoins consecutive nouns that arein the same noun phrase as w.5 Experimental ResultsIn the Senseval-3 task for Automatic Labeling ofSemantic Roles 24,558 sentences from FrameNetwere assigned for training while 8,002 for testing.We used 30% of the training set (7367 sentences)as a validation-set for selecting SVM parametersthat optimize accuracy.
The number of FEs forwhich labels had to be assigned were: 51,010 forthe training set; 15,924 for the validation set and16,279 for the test set.
We used an additional setof 66,687 sentences (hereafter extended data) as ex-tended data produced when using the examples as-sociated with any other frame from FrameNet thathad at least one FE shared with any of the 40frames evaluated in Senseval-3.
These sentenceswere parsed with the Collins?
parser (Collins, 1997).The classifier experiments were carried out using theSVM-light software (Joachims, 1999) available athttp://svmlight.joachims.org/with a poly-nomial kernel2 (degree=3).5.1 Unrestricted Task ExperimentsFor this task we devised four different experimentsthat used four different combination of features: (1)FS1 indicates using only Feature Set 1; (2) +H in-dicates that we added the heuristics; (3) +FS2+FS3indicates that we add the feature Set 2 and 3; and(4) +E indicates that the extended data has also beenused.
For each of the four experiments we trained 40multi-class classifiers, (one for each frame) for a totalof 385 binary role classifiers.
The following Table il-lustrates the overall performance over the validation-set.
To evaluate the results we measure the F1-scoreby combining the precision P with the recall R in theformula F1 = 2?P?RP+R .FS1 +H +H+FS2+FS3 +H+FS2+FS3+E84.4 84.9 91.7 93.15.2 Restricted Task ExperimentsIn order to find the best feature combination for thistask we carried out some preliminary experimentsover five frames.
In Table 1, the row labeled B liststhe F1-score of boundary detection over 4 differentfeature sets: FS1, +H, +FS4 and +E, the extendeddata.
The row labeled R lists the same results for thewhole Restricted Case.Table 1: Restrictive experiments on validation-set.+FS1 +H +H+FS2+FS3 +H+FS4+EB 80.29 80.48 84.76 84.88R 74.9 75.4 78 78.9Table 1 illustrates the overall performance (bound-ary detection and role classification) of automatic se-mantic role labeling.
The results listed in Tables 1and 2 were obtained by comparing the FE bound-aries identified by our parser with those annotated inFrameNet.
We believe that these results are more2In all experiments and for any classifier, we used the defaultSVM-light regularization parameter (e.g., C = 1 for normalizedkernels) and a cost-factor j = 100 to adjust the rate betweenPrecision and Recall.indicative of the performance of our systems thanthose obtained when using the scorer provided bySenseval-3.
When using this scorer, our results havea precision of 89.9%, recall of 77.2% and an F1-score of 83.07% for the Restricted Case.Table 2: Results on the test-set.Precision Recall F1Unrestricted Case 94.5 90.6 92.5Boundary Detection 87.3 75.1 80.7Restricted Case 82.4 71.1 76.3To generate the final Senseval-3 submissions weselected the most accurate models (for unrestrictedand restricted tasks) of the validation experiments.Then we re-trained such models with all training data(i.e.
our training plus validation data) and the set-ting (parameters, heuristics and extended data) de-rived over the validation-set.
Finally, we run all clas-sifiers on the test-set of the task.
Table 2 illustratesthe final results for both sub-tasks.6 ConclusionsIn this paper we describe a method for automaticallylabeling semantic roles based on support vector ma-chines (SVMs).
The training benefits from an ex-tended data set on which multi-class classifiers werederived.
The polynomial kernel of the SVMs en-able the combination of four feature sets that pro-duced very good results both for the Restricted Caseand the Unrestricted Case.
The paper also describessome heuristics for mapping syntactic constituentsonto FEs.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceedingsof the COLING-ACL, Montreal, Canada.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings of theACL-97, pages 16?23.,Daniel Gildea and Daniel Jurasfky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistic,28(3):496?530.T.
Joachims.
1999.
Making Large-Scale SVM LearningPractical.
In B. Schlkopf, C. Burges, and MIT-Press.A.
Smola (ed.
), editors, Advances in Kernel Methods -Support Vector Learning.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, WayneWard, James H. Martin, and Daniel Jurafsky.
2004.Support vector learning for semantic argument classifi-cation.
Journal of Machine Learning Research.Mihai Surdeanu, Sanda M. Harabagiu, John Williams,and John Aarseth.
2003.
Using predicate-argumentstructures for information extraction.
In Proceedingsof (ACL-03).
