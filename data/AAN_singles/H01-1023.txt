Evaluation Results for the Talk?n?Travel SystemDavid StallardBBN Technologies, Verizon70 Fawcett.
St.Cambridge, MA, 02140Stallard@bbn.comABSTRACTWe describe and present evaluation results for Talk?n?Travel, aspoken dialogue language system for making air travel plans overthe telephone.
Talk?n?Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints onhis travel plan in arbitrary order, ask questions, etc., in generalspoken English.
The system was independently evaluated as partof the DARPA Communicator program and achieved a highsuccess rate..1.
INTRODUCTIONThis paper describes and presents evaluation results forTalk?n?Travel, a spoken language dialogue system for makingcomplex air travel plans over the telephone.
Talk?n?Travel is aresearch prototype system sponsored under the DARPACommunicator program (MITRE, 1999).
Some other systems inthe program are Ward and Pellom (1999), Seneff and Polifroni(2000) and Rudnicky et al(1999).
The common task of thisprogram is a mixed-initiative dialogue over the telephone, inwhich the user plans a multi-city trip by air, including all flights,hotels, and rental cars, all in conversational English over thetelephone.
A similar research program is the European ARISEproject (Den Os et al 1999).An earlier version of Talk?n?Travel was presented in (Stallard,2000).
The present paper presents and discusses results of anindependent evaluation of Talk?n?Travel, recently conducted aspart of the DARPA Communicator program.The next section gives a brief overview of the system.2.
SYSTEM OVERVIEWThe figure shows a block diagram of Talk?n?Travel.
Spokenlanguage understanding is provided by statistical N-gram speechrecognition and a robust language understanding component.
Aplan-based dialogue manager coordinates interaction with theuser, handling unexpected user input more flexibly thanconventional finite-state dialogue control networks.
It works intandem with a state management component that adjusts thecurrent model of user intention based on the user?s last utterancein context.Meaning and task state are represented by the path constraintrepresentation (Stallard, 2000).
An inference component isincluded which allows the system to deduce implicit requirementsfrom explicit statements by the user, and to retract them if thepremises change.The system is interfaced to the Yahoo/Travelocity flight schedulewebsite, for access to live flight schedule information.
Queries tothe website are spawned off in a separate thread, which thedialogue manager monitors ands reports on to the user.3.
DIALOGUE STRATEGYTalk?n?Travel employs both open-ended and directed prompts.Sessions begin with open prompts like "What trip would you totake?".
The system then goes to directed prompts to get anyinformation the user did not provide ("What day are youleaving?
", etc).
The user may give arbitrary information at anyprompt, however.
The system provides implicit confirmation ofthe change in task state caused by the user?s last utterance("Flying from Boston to Denver tomorrow ") to ensure mutualunderstanding.The system seeks explicit confirmation in certain cases, forexample where the user appears to be making a change in date oftravel.
Once sufficient information is obtained, the system offers aset of candidate flights, one at a time, for the user to accept orreject.4.
EVALUATION4.1 Evaluation DesignThe 9 groups funded by the Communicator program (ATT, BBN,CMU, Lucent, MIT, MITRE, SRI, and University of Colorado)BYBLOSRecognizerSpeechSynthesizerDiscourseStateManagerDialogManagerFlightDatabaseLanguageGeneratorGEM NLUnderstanderPhoneFigure 1 : System Architecturetook part in an experimental common evaluation conducted by theNational Institute of Standards and Technology (NIST) in Juneand July of 2000.
A pool of approximately 80 subjects wasrecruited from around the United States.
The only requirementswere that the subjects be native speakers of American English andhave Internet access.
Only wireline or home cordless phones wereallowed.The subjects were given a set of travel planning scenarios toattempt.
There were 7 such prescribed scenarios and 2 open ones,in which the subject was allowed to propose his own task.Prescribed scenarios were given in a tabular format.
An examplescenario would be a round-trip flight between two cities,departing and returning on given dates, with specific arrival ordeparture time preferences.Each subject called each system once and attempted to workthrough a single scenario; the design of the experiment attemptedto balance the distributions of scenarios and users across thesystems.Following each scenario attempt, subjects filled out a Web-basedquestionnaire to determine whether subjects thought they hadcompleted their task, how satisfied they were with using thesystem, and so forth.
The overall form of this evaluation was thussimilar to that conducted under the ARISE program (Den Os, et al1999).4.2 ResultsTable 1 shows the result of these user surveys for Talk?n?Travel.The columns represent specific questions on the user survey.
Thefirst column represents the user?s judgement as to whether or nothe completed his task.
The remaining columns, labeled Q1-Q5,are Likert scale items, for which a value of 1 signifies completeagreement, and 5 signifies complete disagreement.
Lowernumbers for these columns are thus better scores.
The legendbelow the table identifies the questions.The first row gives the mean value for the measurements over all78 sessions with Talk?n?Travel.
The second row gives the meanvalue of the same measurements for all 9 systems participating.Talk?n?Travel?s task completion score of 80.5% was the highestfor all 9 participating systems.
Its score on question Q5,representing user satisfaction, was the second highest.An independent analysis of task completion was also performedby comparing the logs of the session with the scenario given.Table 2 shows Talk?n?Travel?s results for this metric, which areclose to that seen for the user questionnaire.Table 2: Objective AnalysisCompletion of required scenario 70.5%Completion of different scenario 11.5%Total completed scenarios 82.0%Besides task completion, other measurements were made ofsystem operation.
These included time to completion, word errorrate, and interpretation accuracy.
The values of thesemeasurements are given in Table 3.Table 3: Other MetricsAverage time to completion 246 secsAverage word error rate 21%Semantic error rate/utterance 10%4.3 Analysis and DiscussionWe analyzed the log files of the 29.5% of the sessions that did notresult in the completion of the required scenario.
Table 4 gives abreakdown of the causes.Table 4: Causes of FailureCity not in lexicon 39% (9)Unrepaired recognition error 22% (5)User error 17% (4)System diversion 13% (3)Other  9% (2)The largest cause (39%) was the inability of the system torecognize a city referred to by the user, simply because that citywas absent from the recognizer language model or languageunderstander?s lexicon.
These cases were generally trivial to fix.The second, and most serious, cause (22%) was recognition errorsthat the user either did not attempt to repair or did not succeed inrepairing.
Dates proved troublesome in this regard, in which onedate would be misrecognized for another, e.g.
?October twentythird?
for ?October twenty firstAnother class of errors were caused by the user, in that he eithergave the system different information than was prescribed by thescenario, or failed to supply the information he was supposed to.A handful of sessions failed because of additional causes,including system crashes and backend failure.Both time to completion and semantic error rate were affected byscenarios that failed because because of a missing city.
In suchscenarios, users would frequently repeat themselves many times ina vain attempt to be understood, thus increasing total utterancecount and utterance error.Q1  It was easy to get the information I wantedQ2  I found it easy to understand what the system saidQ3  I knew what I could do or say at each point in the dialogQ4  The system worked the way I expected it toQ5  I would use this system regularly to get travel informationComp%     Q1 Q2  Q3  Q4 Q5BBN 80.5% 2.23 2.09 2.10 2.36 2.84Mean 62.0% 2.88 2.23 2.54 2.95 3.36TaskScale: 1 = strongly agree, 5 = strongly disagreeTable 1 : Survey ResultsAn interesting result is that task success did not depend toostrongly on word error rate.
Even successful scenarios had anaverage WER of 18%, while failed scenarios had average WER ofonly 22%.A key issue in this experiment was whether users would actuallyinteract with the system conversationally, or would respond onlyto directive prompts.
For the first three sessions, we experimentedwith a highly general open prompt ("How can I help you??
), butquickly found that it tended to elicit overly general anduninformative responses (e.g.
"I want to plan a trip").
Wetherefore switched to the more purposeful "What trip would youlike to take?"
for the remainder of the evaluation.
Fully 70% ofthe time, users replied informatively to this prompt, supplyingutterances "I would like an American flight from Miami toSydney" that moved the dialogue forward.In spite of the generally high rate of success with open prompts,there was a pronounced reluctance by some users to take theinitiative, leading them to not state all the constraints they had inmind.
Examples included requirements on airline or arrival time.In fully 20% of all sessions, users refused multiple flights in arow, holding out for one that met a particular unstatedrequirement.
The user could have stated this requirementexplicitly, but chose not to, perhaps underestimating what thesystem could do.
This had the effect of lengthening totalinteraction time with the system.4.4 Possible ImprovementsSeveral possible reasons for this behavior on the part of userscome to mind, and point the way to future improvements.
Thesynthesized speech was fairly robotic in quality, which naturallytended to make the system sound less capable.
The promptsthemselves were not sufficiently variable, and were often repeatedverbatim when a reprompt was necessary.
Finally, the system?sdialogue strategy needs be modified to detect when more initiativeis needed from the user, and cajole him with open promptsaccordingly.5.
ACKNOWLEDGMENTSThis work was sponsored by DARPA and monitored bySPAWAR Systems Center under Contract No.
N66001-99-D-8615.6.
REFERENCES[1] MITRE (1999)  DARPA Communicator  homepagehttp://fofoca.mitre.org/[2] Ward W., and Pellom, B.
(1999) The CUCommunicator System.
In 1999 IEEE Workshop onAutomatic Speech Recognition and Understanding,Keystone, Colorado.
[3] Den Os,  E, Boves, L., Lamel,  L, and Baggia, P.(1999) Overview of the ARISE Project.
Proceedings ofEurospeech, 1999, Vol 4,   pp.
1527-1530.
[4] Miller S. (1998) The Generative Extraction Model.Unpublished manuscript.
[5] Constantinides P., Hansma S., Tchou C. and Rudnicky,A.
(1999) A schema-based approach to dialog control.Proceedings of ICSLP, Paper 637.
[6] Rudnicky A., Thayer, E., Constantinides P., Tchou C.,Shern, R., Lenzo K., Xu W., Oh A.
(1999) Creatingnatural dialogs in the Carnegie Mellon Communicatorsystem.
Proceedings of Eurospeech, 1999, Vol 4,   pp.1531-1534[7] Rudnicky A., and Xu W. (1999)  An agenda-baseddialog management  architecture for soken languagesystems.
In 1999 IEEE Workshop on Automatic SpeechRecognition and Understanding, Keystone, Colorado.
[8] Seneff S., and Polifroni, J.
(2000) DialogueManagement in the Mercury Flight ReservationSystem.
ANLP Conversational Systems Workshop.
