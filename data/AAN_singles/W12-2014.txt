The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 122?126,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsScoring Spoken Responses Based on Content AccuracyFei HuangCS Dept.
Temple Univ.Philadelphia, PA, 19122tub58431@temple.eduLei ChenEducational Testing Service (ETS)Princeton, NJ, 08541lchen@ets.orgJana SukkariehETSJSukkarieh@ets.orgAbstractAccuracy of content have not been fully uti-lized in the previous studies on automatedspeaking assessment.
Compared to writingtests, responses in speaking tests are noisy(due to recognition errors), full of incompletesentences, and short.
To handle these chal-lenges for doing content-scoring in speakingtests, we propose two new methods basedon information extraction (IE) and machinelearning.
Compared to using an ordinarycontent-scoring method based on vector anal-ysis, which is widely used for scoring writtenessays, our proposed methods provided con-tent features with higher correlations to humanholistic scores.1 IntroductionIn recent years, there is an increasing interest ofusing speech processing and natural language pro-cessing (NLP) technologies to automatically scorespeaking tests (Eskenazi, 2009).
A set of featuresrelated to speech delivery, such as fluency, pronun-ciation, and intonation, has been utilized in thesestudies.
However, accuracy of an answer?s contentto the question being asked, important factors to beconsidered during the scoring process, have not beenfully utilized.
In this paper, we will report our ini-tial efforts exploring content scoring in an automatedspeaking assessment task.
To start, we will brieflydescribe the speaking test questions in our research.In the test we used for evaluation, there weretwo types of questions.
The first type, survey,requires a test-taker to provide answers specificto one or several key points in a survey ques-tion without any background reading/listening re-lated to the topic of the survey.
Typical questionscould be ?how frequently do you go shopping??
or?what kind of products did you purchase recently?
?In contrast, the second type, opinion, requires a test-taker to speak as long as 60 seconds to present hisor her opinions about some topic.
An example ofsuch questions could be, ?Do you agree with thestatement that online shopping will be dominant infuture or not??
Compared to the essays in writingtests, these spoken responses could just be incom-plete sentences.
For example, for the survey ques-tions, test-takers could just say several words.
Forthe questions described above, some test-takers mayjust use phrases like ?once a week?
or ?books?.
Inaddition, given short responding durations, the num-ber of words in test-takers?
responses is limited.
Fur-thermore, since scoring speech responses requiresspeech recognition, more noisy inputs are expected.To tackle these challenges, we propose two novelcontent scoring methods in this paper.The remainder of the paper is organized as fol-lows: Section 2 reviews the related previous re-search efforts; Section 3 proposes the two content-scoring methods we designed for two types of ques-tions described above; Section 4 reports the experi-mental results of applying the proposed methods; fi-nally, Section 5 concludes our reported research anddescribes our plans for future research.2 Related WorkFor writing tests, previous content scoring investiga-tions can be divided into the following three groups.The first group relies on obtaining and matching pat-terns associated with the correct answers (Leacockand Chodorow, 2003; Sukkarieh and Blackmore,2009).The second group of methods, also mostly used122for content-scoring, is to rely on a variety of textsimilarity measurements to compare a response witheither pre-defined correct answers or a group of re-sponses rated with a high score (Mohler and Mihal-cea, 2009).
Compared to the first group, such meth-ods can bypass a labor intensive pattern-buildingstep.
A widely used approach to measuring textsimilarity between two text strings is to converteach text string into a word vector and then usethe angle between these two vectors as a similar-ity metric.
For example, Content Vector Analy-sis (CVA) has been successfully utilized to detectoff-topic essays (Higgins et al, 2006) and to pro-vide content-related features for essay scoring (At-tali and Burstein, 2004).
For this group of meth-ods, measuring the semantics similarity between twoterms is a key question.
A number of metrics havebeen proposed, including metrics (Courley and Mi-halcea, 2005) derived from WordNet, a semanticsknowledge database (Fellbaum, 1998), and metricsrelated to terms?
co-occurrence in corpora or on theWeb (Turney, 2001).The third group of methods treats content scor-ing as a Text Categorization (TC) task, which treatsthe responses being scored on different score levelsas different categories.
Therefore, a large amountof previous TC research, such as the many machinelearning approaches proposed for the TC task, canbe utilized.
For example, Furnkranz et al (1998)compared the performance of applying two machinelearning methods on a web-page categorization taskand found that the Repeated Incremental Pruning toProduce Error Reduction algorithm (RIPPER) (Co-hen, 1995) shows an advantage concerning the fea-ture sparsity issue.3 MethodologyAs described in Section 1, for the two types of ques-tions considered, the number of words appearingin a response is quite limited given the short re-sponse time.
Therefore, compared to written es-says, when applying the content-scoring methodsbased on vector analysis, e.g., CVA, feature sparsitybecomes a major factor negatively influencing theperformance of these methods.
Furthermore, thereare more challenges when applying vector analysison survey questions because test-takers could justuse words/phrases rather than completed sentences.Also, some survey questions could have a very largerange of correct answers.
For example, if a questionis about the name of a book, millions of book ti-tles could be potential answers.
Therefore, a simplephrase-matching solution cannot work.3.1 Semi-Automatic Information ExtractionFor survey responses, the answers should be relatedto the key points mentioned in the questions.
Forexample, for the question, ?What kind of TV pro-grams do you like to watch?
?, possible correct an-swers should be related to TV programs.
Moreover,it should be the instances of specific TV programs,like news, comedy, talk shows, etc.
Note that the ac-ceptable answers may be infinite, so it is not realis-tic to enumerate all possible answers.
Therefore, weproposed a method to extract the potential answercandidates and then measure their semantic similar-ities to the answer keys that could be determinedmanually.
In particular, the answer keys were deter-mined by the first author based on her analysis of thetest prompts.
For example, for the question ?Whatkind of books do you like to read?
?, two answer keys,?book?
and ?reading?
were selected.
After a fur-ther analysis of the questions, we found that most ofthe survey questions are about ?when?
?where?
and?what?, and the answers in the responses were usu-ally nouns or noun phrases.
Therefore, we decidedto extract the noun phrases from each response anduse them as potential candidates.We use two semantic similarity metrics (SSMs)to evaluate how each candidate relates to an answerkey, including PMI-IR (Turney, 2001) and a word-to-word similarity metric from WordNet (Courleyand Mihalcea, 2005).
The PMI-IR is a measurebased on web query analysis using Pointwise MutualInformation (PMI) and Information Retrieval (IR).For an answer candidate (c) and an answer key (k),their PMI-IR is computed as:SSMPMI-IR(c, k) =hits(cNEARk)hits(c)where the hits(x) function obtains the count of termx returned by a web search engine and NEAR is aquery operator for proximity search, searching thepages on which both k and c appear within a spec-ified distance.
Among many WordNet (WN) basedSSMs summarized in Courley and Mihalcea (2005),123we found that the Wu-Palmer metric proposed byWu and Palmer (1994) worked the best in our pilotstudy.
This metric is a score denoting how similartwo word senses are, based on the depth of the twoword senses in the taxonomy and their Least Com-mon Subsumer 1 (LCS):SSMWN(c, k) =2 ?
depth(LCS)depth(c) + depth(k)For each answer key, we calculated two sets ofSSMs (SSMPMI-IR and SSMWN , respectively)from all candidates.
Then, we selected the largestSSMPMI-IR and SSMWN as the final SSMs for thisparticular answer key.
For each test question, usingthe corresponding responses in the training set, webuilt a linear regression model between these SSMsfor all answer keys and the human judged scores.The learned regression model was applied to the re-sponses to this particular testing question in the test-ing set to convert a set of SSMs to predictions ofhuman scores.
The predicted scores were then usedas a content feature.
Since answer keys were deter-mined manually, we refer to this method as semi-automatic information extraction (Semi-IE).3.2 Machine Learning Using Smoothed InputsFor the opinion responses, inspired by Furnkranzet al (1998), we decided to try sophisticated ma-chine learning methods instead of the simple vector-distance computation used in CVA.
Due to shortresponse-time in the speaking test being considered,the ordinary vector analysis may face a problem thatthe obtained vectors are too short to be reliably used.In addition, using other non-CVA machine learningmethods can enable us to try other types of linguis-tic features.
To address the feature sparsity issue, asmoothing method, which converts word-based textfeatures into features based on other entities witha much smaller vocabulary size, is used.
We usea Hidden Markov Model (HMM) based smooth-ing method (Huang and Yates, 2009), which in-duces classes, corresponding to hidden states in theHMM model, from the observed word strings.
Thissmoothing method can use contextual informationof the word sequences due to the nature of HMM.Then, we convert word-entity vectors to the vec-tors based on the induced classes.
TF-IDF (term1Most specific ancestor nodefrequency and inverse document frequency) weight-ing is applied on the new class vectors.
Finally,the processed class vectors are used as input fea-tures (smoothed) to a machine learning method.
Inthis research, after comparing several widely usedmachine learning approaches, such as Naive Bayes,CART, etc., we decided to use RIPPER proposed byCohen (1995), a rule induction method, similar toFurnkranz et al (1998).4 ExperimentsOur experimental data was from a test for interna-tional workplace English.
Six testing papers wereused in our study and each individual test containsthree survey questions (1, 2, and 3) and two opin-ion questions (4 and 5).
Table 1 lists examplesfor these question types.
From the real test, wecollected spoken responses from a total of 1, 838test-takers.
1, 470 test-takers were used for trainingand 368 were used for testing.
Following scoringrubrics developed for this test by considering speak-ers?
various language skill aspects, such as fluency,pronunciation, vocabulary, as well as content accu-racy, the survey and opinion responses were scoredby a group of experienced human raters by using a3-point scale and a 5-point scale respectively.
Forthe survey responses, the human judged scores werecentered on 2; for the opinion responses, the humanjudged scores were centered on 3 and 4.Qs.
Example1 How frequently do you go shopping?2 What kinds of products do you buy often?3 How should retailers improve their services?4 Make a purchase decision based on the chartprovided and justify your decision.5 Do you agree with the statement that onlineshopping will be dominant in the future ornot?
Please justify your point.Table 1: Examples of the five kinds of questions investi-gated in the studyAll of these non-native speech responses weremanually transcribed.
A state-of-the-art HMM Au-tomatic Speech Recognition (ASR) system whichwas trained from a large set of non-native speechdata was used.
For each type of test question, acous-tic and language model adaptations were appliedto further lower the recognition error rate.
Finally,124a word error rate around 30% to 40% could beachieved on the held-out speech data.
In our exper-iments, we used speech transcriptions in the modeltraining stage and used ASR outputs in the testingstage.
Note that we decided to use speech transcrip-tions, instead of noisy ASR outputs that match tothe testing condition, to make sure that the learnedcontent-scoring model are based on correct word en-tities related to content accuracy.For the survey responses, we manually selectedthe key points from the testing questions.
Then,using a Part-Of-Speech (POS) tagger and a sen-tence chunker implemented by using the OpenNLP 2toolkit, we found all possible nouns and noun-phrases that could serve as answer candidates andapplied the Semi-IE method described in Sec-tion 3.1.
For opinion questions, based on Huang andYates (2009), we used 80 hidden states and appliedthe method described in Section 3.2 for content scor-ing.
We used JRip, a Java implementation of theRIPPER (Cohen, 1995) algorithm in the Weka (Hallet al, 2009) machine learning toolkit, in our experi-ments.When measuring performance of content-relatedfeatures, following many automated assessmentstudies (Attali and Burstein, 2004; Leacock andChodorow, 2003; Sukkarieh and Blackmore, 2009),we used the Pearson correlation r between the con-tent features and human scores as an evaluation met-ric.
We compared the proposed methods with a base-line method, CVA.
It works as follows: it first groupsall the training responses by scores, then it calculatesa TF vector from all the responses under a scorelevel.
Also, an IDF matrix is generated from allthe training responses.
After that, for each testingresponse, CVA first converts it into a TF-IDF vec-tor and then calculates the cosine similarity betweenthis vector with each score-level vector respectivelyand uses the largest cosine similarity as the contentfeature for that response.
The experimental results,including content-features?
correlations r to humanscores from each proposed method and the correla-tion increases measured on CVA results, are shownin Table 2.
First, we find that CVA, which is de-signed for scoring lengthy written essays, does notwork well for the survey questions, especially on2http://opennlp.sourceforge.netQuestion rCV A rSemi?IE r ?1 0.12 0.30 150%2 0.15 0.27 80%3 0.21 0.26 23.8%Question rCV A rRipperHMM r ?4 0.47 0.54 14.89%5 0.33 0.39 18.18%Table 2: Comparisons of the proposed content-scoringmethods with CVA on survey and opinion responsesfirst two questions, which are mostly phrases (notcompleted sentences).
By contrast, our proposedSemi-IE method can provide more informative con-tent measurements, indicated by substantially in-creased r. Second, CVA works better on opinionquestions than on survey questions.
This is becausethat opinion questions can be treated as short spo-ken essays and therefore are closer to the data onwhich the CVA method was originally designed towork.
However, even on such a well-performingCVA baseline, the HMM smoothing method allowsthe Ripper algorithm to outperform the CVA methodin content-features?
correlations to human scores.For example, on question 4, on which either a tableor a chart has been provided to test-takers, the CVAachieves a r of 0.47.
The proposed method can stillimprove the r by about 15%.5 Conclusions and Future WorksIn this paper, we proposed two content-scoringmethods for the two types of test questions in anautomated speaking assessment task.
For particu-lar properties of these two question types, we uti-lized information extraction (IE) and machine learn-ing technologies to better score them on contentaccuracy.
In our experiments, we compared thesetwo methods, Semi-IE and machine learning us-ing smoothed inputs, with an ordinary word-basedvector analysis method, CVA.
The content featurescomputed using the proposed methods show highercorrelations to human scores than what was obtainedby using the CVA method.For the Semi-IE method, one direction of investi-gation will be how to find the expected answer keysautomatically from testing questions.
In addition,we will investigate better ways to integrate many se-125mantic similarly measurements (SSMs) into a singlecontent feature.
For the machine learning approach,inspired by Furnkranz et al (1998), we will inves-tigate how to use some linguistic features related toresponse structures rather than just TF-IDF weights.ReferencesY.
Attali and J. Burstein.
2004.
Automated essay scoringwith e-rater v.2.0.
In Presented at the Annual Meet-ing of the International Association for EducationalAssessment.W.
Cohen.
1995.
Text categorization and relationallearning.
In In Proceedings of the 12th InternationalConference on Machine Learning.C.
Courley and R. Mihalcea.
2005.
Measuring the se-mantic similarity of texts.
In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equiv-alence and Entailment, pages 13?18.M.
Eskenazi.
2009.
An overview of spoken languagetechnology for education.
Speech Communication,51(10):832?844.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Bradford Books.J.
Furnkranz, T. Mitchell, and E. Riloff.
1998.
A casestudy in using linguistic phrases for text categorizationon the WWW.
In Proceedings from the AAAI/ICMLWorkshop on Learning for Text Categorization, page512.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H Witten.
2009.
The WEKA data min-ing software: An update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.D.
Higgins, J. Burstein, and Y. Attali.
2006.
Identifyingoff-topic student essays without topic-specific trainingdata.
Natural Language Engineering, 12.F.
Huang and A. Yates.
2009.
Distributional represen-tations for handling sparsity in supervised sequence-labeling.
In Proceedings of ACL.C.
Leacock and M. Chodorow.
2003.
C-rater: Auto-mated scoring of short-answer questions.
Computersand the Humanities, 37(4):385?405.M.
Mohler and R. Mihalcea.
2009.
Text-to-text seman-tic similarity for automatic short answer grading.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 567?575.J.
Z. Sukkarieh and J. Blackmore.
2009. c-rater: Auto-matic content scoring for short constructed responses.In Paper presented at the Florida Artificial Intelli-gence Research Society (FLAIRS) Conference, Sani-bel, FL.P.
D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Procs.
of theTwelfth European Conference on Machine Learning(ECML), pages 491?502, Freiburg, Germany.Z.
Wu and M. Palmer.
1994.
Verbs semantics and lexi-cal selection.
In Proceeding ACL ?94 Proceedings ofthe 32nd annual meeting on Association for Computa-tional Linguistics.126
