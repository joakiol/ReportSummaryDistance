Automatic Acquisition of English Topic Signatures Based ona Second LanguageXinglong WangDepartment of InformaticsUniversity of SussexBrighton, BN1 9QH, UKxw20@sussex.ac.ukAbstractWe present a novel approach for auto-matically acquiring English topic sig-natures.
Given a particular concept,or word sense, a topic signature is aset of words that tend to co-occur withit.
Topic signatures can be useful in anumber of Natural Language Process-ing (NLP) applications, such as WordSense Disambiguation (WSD) and TextSummarisation.
Our method takes ad-vantage of the different way in whichword senses are lexicalised in Englishand Chinese, and also exploits the largeamount of Chinese text available in cor-pora and on the Web.
We evaluated thetopic signatures on a WSD task, wherewe trained a second-order vector co-occurrence algorithm on standard WSDdatasets, with promising results.1 IntroductionLexical knowledge is crucial for many NLP tasks.Huge efforts and investments have been made tobuild repositories with different types of knowl-edge.
Many of them have proved useful, such asWordNet (Miller et al, 1990).
However, in someareas, such as WSD, manually created knowledgebases seem never to satisfy the huge requirementby supervised machine learning systems.
Thisis the so-called knowledge acquisition bottleneck.As an alternative, automatic or semi-automatic ac-quisition methods have been proposed to tacklethe bottleneck.
For example, Agirre et al (2001)tried to automatically extract topic signatures byquerying a search engine using monosemous syn-onyms or other knowledge associated with a con-cept defined in WordNet.The Web provides further ways of overcomingthe bottleneck.
Mihalcea et al (1999) presenteda method enabling automatic acquisition of sense-tagged corpora, based on WordNet and an Inter-net search engine.
Chklovski and Mihalcea (2002)presented another interesting proposal which turnsto Web users to produce sense-tagged corpora.Another type of method, which exploits dif-ferences between languages, has shown greatpromise.
For example, some work has been donebased on the assumption that mappings of wordsand meanings are different in different languages.Gale et al (1992) proposed a method which au-tomatically produces sense-tagged data using par-allel bilingual corpora.
Diab and Resnik (2002)presented an unsupervised method for WSD us-ing the same type of resource.
One problem withrelying on bilingual corpora for data collection isthat bilingual corpora are rare, and aligned bilin-gual corpora are even rarer.
Mining the Web forbilingual text (Resnik, 1999) is not likely to pro-vide sufficient quantities of high quality data.
An-other problem is that if two languages are closelyrelated, data for some words cannot be collectedbecause different senses of polysemous words inone language often translate to the same word inthe other.In this paper, we present a novel approach forautomatically acquiring topic signatures (see Ta-ble 1 for an example of topic signatures), whichalso adopts the cross-lingual paradigm.
To solvethe problem of different senses not being distin-guishable mentioned in the previous paragraph,we chose a language very distant to English ?Chinese, since the more distant two languagesare, the more likely that senses are lexicaliseddifferently (Resnik and Yarowsky, 1999).
Be-cause our approach only uses Chinese monolin-gual text, we also avoid the problem of shortageof aligned bilingual corpora.
We build the topicsignatures by using Chinese-English and English-Chinese bilingual lexicons and a large amount ofChinese text, which can be collected either fromthe Web or from Chinese corpora.
Since topic sig-natures are potentially good training data for WSDalgorithms, we set up a task to disambiguate 6words using a WSD algorithm similar to Schu?tze?s(1998) context-group discrimination.
The resultsshow that our topic signatures are useful for WSD.The remainder of the paper is organised as fol-lows.
Section 2 describes the process of acqui-sition of the topic signatures.
Section 3 demon-strates the application of this resource on WSD,and presents the results of our experiments.
Sec-tion 4 discusses factors that could affect the acqui-sition process and then we conclude in Section 5.2 Acquisition of Topic SignaturesA topic signature is defined as: TS ={(t1, w1), ..., (ti, wi), ...}, where ti is a termhighly correlated to a target topic (or concept) withassociation weight wi, which can be omitted.
Thesteps we perform to produce the topic signaturesare described below, and illustrated in Figure 1.1.
Translate an English ambiguous word w to Chinese,using an English-Chinese lexicon.
Given the assump-tion we mentioned, each sense si of w maps to a dis-tinct Chinese word1.
At the end of this step, we haveproduced a set C, which consists of Chinese words{c1, c2, ..., cn}, where ci is the translation correspond-ing to sense si of w, and n is the number of senses thatw has.2.
Query large Chinese corpora or/and a search enginethat supports Chinese using each element in C. Then,for each ci in C, we collect the text snippets retrievedand construct a Chinese corpus.1It is also possible that the English sense maps to a set ofChinese synonyms that realise the same concept.English ambiguous wordwSense 1 ofwSense 2 ofwChinese translation ofsense 2Chinese translation ofsense 1English-ChineseLexicon1.
Chinese document 12.
Chinese document 2... ...ChineseSearchEngineChinesesegmentationand POStagging;Chinese-EnglishLexicon1.
Chinese document 12.
Chinese document 2... ...1.
{English topic signature 1}2.
{English topic signature 2}... ...1.
{English topic signature 1}2.
{English topic signature 2}...
...Figure 1:Process of automatic acquisition of topic signatures.For simplicity, we assume here that w has two senses.3.
Shallow process these Chinese corpora.
Text segmen-tation and POS tagging are done in this step.4.
Either use an electronic Chinese-English lexicon totranslate the Chinese corpora word by word to En-glish, or use machine translation software to translatethe whole text.
In our experiments, we did the former.The complete process is automatic, and unsu-pervised.
At the end of this process, for each sensesi of an ambiguous word w, we have a large setof English contexts.
Each context is a topic sig-nature, which represents topical information thattends to co-occur with sense si.
Note that an el-ement in our topic signatures is not necessarily asingle English word.
It can be a set of Englishwords which are translations of a Chinese word c.For example, the component of a topic signature,{vesture, clothing, clothes}, is translated from theChinese word??.
Under the assumption that themajority of c?s are unambiguous, which we dis-cuss later, we refer to elements in a topic signatureas concepts in this paper.Choosing an appropriate English-Chinese dic-tionary is the first problem we faced.
The onewe decided to use is the Yahoo!
Student English-Chinese On-line Dictionary2.
As this dictionaryis designed for English learners, its sense gran-ularity is far coarser-grained than that of Word-Net.
However, researchers argue that the granular-ity of WordNet is too fine for many applications,and some also proposed new evaluation standards.For example, Resnik and Yarowsky (1999) sug-2See: http://cn.yahoo.com/dictionary/gested that for the purpose of WSD, the differentsenses of a word could be determined by consid-ering only sense distinctions that are lexicalisedcross-linguistically.
Our approach is in accordwith their proposal, since bilingual dictionaries in-terpret sense distinctions crossing two languages.For efficiency purposes, we extract our topicsignatures mainly from the Mandarin portion ofthe Chinese Gigaword Corpus (CGC), producedby the LDC3, which contains 1.3GB of newswiretext drawn from Xinhua newspaper.
Some Chi-nese translations of English word senses could besparse, making it impossible to extract sufficienttraining data simply relying on CGC.
In this sit-uation, we can turn to the large amount of Chi-nese text on the Web.
There are many good searchengines and on-line databases supporting the Chi-nese language.
After investigation, we chose Peo-ple?s Daily On-line4, which is the website for Peo-ple?s Daily, one of the most influential newspaperin mainland China.
It maintains a vast databaseof news stories, available to search by the public.Among other reasons, we chose this website be-cause its articles have similar quality and cover-age to those in the CGC, so that we could com-bine texts from these two resources to get a largeramount of topic signatures.
Note that we can al-ways turn to other sources on the Web to retrieveeven more data, if needed.For Chinese text segmentation and POS tag-ging5 we adopted the freely-available softwarepackage ?
ICTCLAS6.
This system includes aword segmenter, a POS tagger and an unknown-word recogniser.
The claimed precision of seg-mentation is 97.58%, evaluated on a 1.2M wordportion of the People?s Daily Corpus.To automatically translate the Chinese text backto English, we used the electronic LDC Chinese-English Translation Lexicon Version 3.0.
An al-ternative was to use machine translation software,which would yield a rather different type of re-source, but this is beyond the scope of this pa-per.
Then, we filtered the topic signatures with3Available at: http://www.ldc.upenn.edu/Catalog/4See: http://www.people.com.cn5POS tagging can be omitted.
We did it in our experi-ments purely for convenience for error analysis in the future.6See: http://mtgroup.ict.ac.cn/?zhp/ICTCLAS/index.htmla stop-word list, to ensure only content words areincluded in our final results.One might argue that, since many Chinesewords are also ambiguous, a Chinese word mayhave more than one English translation and thustranslated concepts in topic signatures would stillbe ambiguous.
This happens for some Chinesewords, and will inevitably affect the performanceof our system to some extent.
A practical solu-tion is to expand the queries with different descrip-tions associated with each sense of w, normallyprovided in a bilingual dictionary, when retriev-ing the Chinese text.
To get an idea of the baselineperformance, we did not follow this solution in ourexperiments.1.rate;2.bond;3.payment; 4.market;5.debt;6. dollar;7.bank;8. year; 9. loan; 10.income;11.company;12. inflation; 13. reserve; 14. government; 15.economy;16.stock;17.fund;18. week; 19. security; 20. level;AM1.
{bank}; 2.
{loan}; 3.
{company, firm, corporation};4.
{rate}; 5.
{deposit}; 6.
{income, revenue}; 7.
{fund};8.
{bonus, divident}; 9.
{investment}; 10.
{market};11.
{tax, duty}; 12.
{economy}; 13.
{debt}; 14.
{money};15.
{saving}; 16.
{profit}; 17.
{bond}; 18.
{income, earning};19.
{share,stock}; 20.
{finance, banking};Topic signatures for the "financial" sense of "interest"Table 1:A sample of our topic signatures.
Signature M wasextracted from a manually-sense-tagged corpus and A wasproduced by our algorithm.
Words occurring in both A andM are marked in bold.The topic signatures we acquired contain richtopical information.
But they do not provide anyother types of linguistic knowledge.
Since theywere created by word to word translation, syntac-tic analysis of them is not possible.
Even the dis-tances between the target ambiguous word and itscontext words are not reliable because of differ-ences in word order between Chinese and English.Table 1 lists two sets of topic signatures, each con-taining the 20 most frequent nouns, ranked by oc-currence count, that surround instances of the fi-nancial sense of interest.
One set was extractedfrom a hand-tagged corpus (Bruce and Wiebe,1994) and the other by our algorithm.3 Application on WSDTo evaluate the usefulness of the topic signaturesacquired, we applied them in a WSD task.
Weadopted an algorithm similar to Schu?tze?s (1998)context-group discrimination, which determines aword sense according to the semantic similarityof contexts, computed using a second-order co-occurrence vector model.
In this section, we firstlyintroduce our adaptation of this algorithm, andthen describe the disambiguation experiments on6 words for which a gold standard is available.3.1 Context-Group DiscriminationWe chose the so-called context-group discrimina-tion algorithm because it disambiguates instancesonly relying on topical information, which hap-pens to be what our topic signatures specialisein7.
The original context-group discriminationis a disambiguation algorithm based on cluster-ing.
Words, contexts and senses are representedin Word Space, a high-dimensional, real-valuedspace in which closeness corresponds to semanticsimilarity.
Similarity in Word Space is based onsecond-order co-occurrence: two tokens (or con-texts) of the ambiguous word are assigned to thesame sense cluster if the words they co-occur withthemselves occur with similar words in a trainingcorpus.
The number of sense clusters determinessense granularity.In our adaptation of this algorithm, we omittedthe clustering step, because our data has alreadybeen sense classified according to the senses de-fined in the English-Chinese dictionary.
In otherwords, our algorithm performs sense classifica-tion by using a bilingual lexicon and the levelof sense granularity of the lexicon determines thesense distinctions that our system can handle: afiner-grained lexicon would enable our system toidentify finer-grained senses.
Also, our adapta-tion represents senses in Concept Space, in con-trast to Word Space in the original algorithm.
Thisis because our topic signatures are not realised inthe form of words, but concepts.
For example, atopic signature may consist of {duty, tariff, cus-toms duty}, which represents a concept of ?a gov-ernment tax on imports or exports?.A vector for concept c is derived from all theclose neighbours of c, where close neighbours re-fer to all concepts that co-occur with c in a contextwindow.
The size of the window is around 1007Using our topic signatures as training data, other classi-fication algorithms would also work on this WSD task.words.
The entry for concept c?
in the vector forc records the number of times that c?
occurs closeto c in the corpus.
It is this representational vectorspace that we refer to as Concept Space.In our experiments, we chose concepts thatserve as dimensions of Concept Space using afrequency cut-off.
We count the number of oc-currences of any concepts that co-occur with theambiguous word within a context window.
The2, 500 most frequent concepts are chosen as thedimensions of the space.
Thus, the Concept Spacewas formed by collecting a n-by-2, 500 matrix M ,such that element mij records the number of timesthat concept i and j co-occur in a window, wheren is the number of concept vectors that occur inthe corpus.
Row l of matrix M represents conceptvector l.We measure the similarity of two vectors by thecosine score:corr(~v, ~w) =?Ni=1 ~vi ~wi?
?Ni=1 ~vi2?Ni=1 ~wi2where ~v and ~w are vectors and N is the dimen-sion of the vector space.
The more overlap thereis between the neighbours of the two words whosevectors are compared, the higher the score.Contexts are represented as context vectors inConcept Space.
A context vector is the sum of thevectors of concepts that occur in a context win-dow.
If many of the concepts in a window have astrong component for one of the topics, then thesum of the vectors, the context vector, will alsohave a strong component for the topic.
Hence, thecontext vector indicates the strength of differenttopical or semantic components in a context.Senses are represented as sense vectors in Con-cept Space.
A vector of sense si is the sum of thevectors of contexts in which the ambiguous wordrealises si.
Since our topic signatures are classi-fied naturally according to definitions in a bilin-gual dictionary, calculation of the vector for sensesi is fairly straightforward: simply sum all the vec-tors of the contexts associated with sense si.After the training phase, we have obtained asense vector ~vi for each sense si of an ambiguousword w. Then, we perform the following steps totag an occurrence t of w:1.
Compute the context vector ~c for t in Concept Spaceby summing the vectors of the concepts in t?s context.Since the basic units of the test data are words ratherthan concepts, we have to convert all words in the testdata into concepts.
A simple way to achieve this is toreplace a word v with all the concepts that contain v.2.
Compute the cosine scores between all sense vectors ofw and ~c, and then assign t to the sense si whose sensevector ~sj is closest to ~c.3.2 Experiments and ResultsWe tested our system on 6 nouns, as shown in Ta-ble 2, which also shows information on the train-ing and test data we used in the experiments.
Thetraining sets for motion, plant and tank are topicsignatures extracted from the CGC; whereas thosefor bass, crane and palm are obtained from bothCGC and the People?s Daily On-line.
This is be-cause the Chinese translation equivalents of sensesof the latter 3 words don?t occur frequently inCGC, and we had to seek more data from the Web.Where applicable, we also limited the training dataof each sense to a maximum of 6, 000 instances forefficiency purposes.76.6%Precision93.5%bass120390.7%'Supervised'BaselineTestTrainingSenseWord2.
music1.
fish8254189710crane230174.7%2.
machine1.
bird147282971241079569.7%motion926570.1%2.
legal1.
physical326560006014120176.1%palm124871.1%2.
tree1.
hand3968525814320170.2%plant1200054.3%2.
factory1.
living600060001028618870.1%tank934662.7%2.
vehicle1.
container3346600075126201Table 2:Sizes of the training data and the test data, baselineperformance, and the results.The test data is a binary sense-tagged corpus,the TWA Sense Tagged Data Set, manually pro-duced by Rada Mihalcea and Li Yang (Mihalcea,2003), from text drawn from the British NationalCorpus.
We calculated a ?supervised?
baselinefrom the annotated data by assigning the most fre-quent sense in the test data to all instances, al-though it could be argued that the baseline for un-supervised disambiguation should be computed byrandomly assigning one of the senses to instances(e.g.
it would be 50% for words with two senses).According to our previous description, the2, 500 most frequent concepts were selected as di-mensions.
The number of features in a ConceptSpace depends on how many unique concepts ac-tually occur in the training sets.
Larger amountsof training data tend to yield a larger set of fea-tures.
At the end of the training stage, for eachsense, a sense vector was produced.
Then we lem-matised the test data and extracted a set of contextvectors for all instances in the same way.
For eachinstance in the test data, the cosine scores betweenits context vector and all possible sense vectors ac-quired through training were calculated and com-pared, and then the sense scoring the highest wasallocated to the instance.The results of the experiments are also givenin Table 2 (last column).
Using our topic sig-natures, we obtained good results: the accuracyfor all words exceeds the supervised baseline, ex-cept for motion which approaches it.
The Chi-nese translations for motion are also ambiguous,which might be the reason that our WSD systemperformed less well on this word.
However, aswe mentioned, to avoid this problem, we couldhave expanded motion?s Chinese translations, us-ing their Chinese monosemous synonyms, whenwe query the Chinese corpus or the Web.
Consid-ering our system is unsupervised, the results arevery promising.
An indicative comparison mightbe with the work of Mihalcea (2003), who witha very different approach achieved similar perfor-mance on the same test data.4 DiscussionAlthough these results are promising, higher qual-ity topic signatures would probably yield better re-sults in our WSD experiments.
There are a num-ber of factors that could affect the acquisition pro-cess, which determines the quality of this resource.Firstly, since the translation was achieved by look-ing up in a bilingual dictionary, the deficienciesof the dictionary could cause problems.
For ex-ample, the LDC Chinese-English Lexicon we usedis not up to date, for example, lacking entries forwords such as??
(mobile phone),p?
(theInternet), etc.
This defect makes our WSD algo-rithm unable to use the possibly strong topical in-formation contained in those words.
Secondly, er-rors generated during Chinese segmentation couldaffect the distributions of words.
For example, aChinese string ABC may be segmented as eitherA+BC or AB + C; assuming the former is cor-rect whereas AB + C was produced by the seg-menter, distributions of words A, AB, BC, and Care all affected accordingly.
Other factors such ascultural differences reflected in the different lan-guages could also affect the results of this knowl-edge acquisition process.In our experiments, we adopted Chinese as asource language to retrieve English topic signa-tures.
Nevertheless, our technique should alsowork on other distant language pairs, as longas there are existing bilingual lexicons and largemonolingual corpora for the languages used.
Forexample, one should be able to build French topicsignatures using Chinese text, or Spanish topicsignatures from Japanese text.
In particular cases,where one only cares about translation ambiguity,this technique can work on any language pair.5 Conclusion and Future WorkWe presented a novel method for acquiring En-glish topic signatures from large quantities ofChinese text and English-Chinese and Chinese-English bilingual dictionaries.
The topic signa-tures we acquired are a new type of resource,which can be useful in a number of NLP applica-tions.
Experimental results have shown its appli-cation to WSD is promising and the performanceis competitive with other unsupervised algorithms.We intend to carry out more extensive evaluationto further explore this new resource?s propertiesand potential.AcknowledgementsThis research is funded by EU IST-2001-34460 project MEANING: Developing Multilin-gual Web-Scale Language Technologies, and bythe Department of Informatics at Sussex Univer-sity.
I am very grateful to Dr John Carroll, mysupervisor, for his continual help and encourage-ment.ReferencesEneko Agirre, Olatz Ansa, David Martinez, and Ed-uard Hovy.
2001.
Enriching WordNet conceptswith topic signatures.
In Proceedings of the NAACLworkshop on WordNet and Other Lexical Resources:Applications, Extensions and Customizations.
Pitts-burgh, USA.Rebecca Bruce and Janyce Wiebe.
1994.
Word-sensedisambiguation using decomposable models.
InProceedings of the 32nd Annual Meeting of the As-sociation for Computational Linguistics, pages 139?146.Timothy Chklovski and Rada Mihalcea.
2002.
Build-ing a sense tagged corpus with open mind word ex-pert.
In Proceedings of the ACL 2002 Workshop on?Word Sense Disambiguation Recent Successes andFuture Directions?.
Philadelphia, USA.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel cor-pora.
In Proceedings of the 40th Anniversary Meet-ing of the Association for Computational Linguistics(ACL-02).
Philadelphia, USA.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
Using bilingual materials todevelop word sense disambiguation methods.
InProceedings of the International Conference onTheoretical and Methodological Issues in MachineTranslation, pages 101?112.Rada Mihalcea and Dan I. Moldovan.
1999.
An auto-matic method for generating sense tagged corpora.In Proceedings of the 16th Conference of the Amer-ican Association of Artificial Intelligence.Rada Mihalcea.
2003.
The role of non-ambiguouswords in natural language disambiguation.
In Pro-ceedings of the Conference on Recent Advancesin Natural Language Processing, RANLP 2003.Borovetz, Bulgaria.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.1990.
Introduction to WordNet: An on-line lexicaldatabase.
Journal of Lexicography, 3(4):235?244.Philip Resnik and David Yarowsky.
1999.
Distinguish-ing systems and distinguishing senses: New evalua-tion methods for word sense disambiguation.
Natu-ral Language Engineering, 5(2):113?133.Philip Resnik.
1999.
Mining the Web for bilingualtext.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.
