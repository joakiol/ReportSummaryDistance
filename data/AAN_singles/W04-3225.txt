Adaptive Language and Translation Modelsfor Interactive Machine TranslationLaurent Nepveu, Guy LapalmePhilippe LanglaisRALI/DIRO - Universite?
de Montre?al,C.P.
6128, succursale Centre-villeMontre?al, Que?bec, Canada H3C 3J7{nepveul,lapalme,felipe}@iro.umontreal.caGeorge FosterLanguage Technologies Research CentreNational Research Council CanadaA-1330, 101 rue Saint-Jean Bosco,Gatineau, Que?bec, Canada K1A 0R6George.Foster@nrc-cnrc.gc.caAbstractWe describe experiments carried out with adaptivelanguage and translation models in the context of aninteractive computer-assisted translation program.We developed cache-based language models whichwere then extended to the bilingual case for a cache-based translation model.
We present the improve-ments we obtained in two contexts: in a theoreticalsetting, we achieved a drop in perplexity for the newmodels and, in a more practical situation simulat-ing a user working with the system, we showed thatfewer keystrokes would be needed to enter a trans-lation.1 IntroductionCache-based language models were introduced byKuhn and de Mori (1990) for the dynamic adap-tation of speech language models.
These models,inspired by the memory caches on modern com-puter architectures, are motivated by the principleof locality which states that a program tends to re-peatedly use memory cells that are physically close.Similarly, when speaking or writing, humans tendto use the same words and phrase constructs fromparagraph to paragraph and from sentence to sen-tence.
This leads us to believe that, when processinga document, the part of a document that is alreadyprocessed (e.g.
for speech recognition, translationor text prediction) gives us very useful informationfor future processing in the same document or inother related documents.A cache-based language model is a languagemodel to which is added a smaller model trainedonly on the history of the document being pro-cessed.
The history is usually the last N words orsentences seen in the document.Kuhn and de Mori (1990) obtained a drop in per-plexity of nearly 68% when adding an unigram POS(part-of-speech) cache on a 3g-gram model.
Martinand al.
(1997) obtained a drop of nearly 21% whenadding a bigram cache to a trigram model.
Clarksonand Robertson (1997) also obtained similar resultswith an exponentially decaying unigram cache.The major problem with these theoretical resultsis that they assume the correctness of the materialentering the cache.
In practice, this assumption doesnot always hold, and so a cache can sometimes domore harm than good.1.1 Interactive translation contextOver the last few years, an interactive machinetranslation (IMT) system (Foster et al, 2002) hasbeen developed which, as the translator is typing,suggests word and phrase completions that the usercan accept or ignore.
The system uses a transla-tion engine to propose the words or phrases whichit judges the most probable to be immediately typed.This engine includes a translation model (TM) anda language model (LM) used jointly to produce pro-posals that are appropriate translations of sourcewords and plausible completions of the current textin the target language.
The translator remains incontrol of the translation because what is typed bythe user is taken as a constraint to which the modelmust continually adapt its completions.
Experi-ments have shown that the use of this system cansave about 50% of the keystrokes needed for enter-ing a translation.
As the translation and languagemodels are built only once, before the user starts towork with the system, the translator is often forcedto repeatedly correct similar suggestions from thesystem.The interactive nature of this setup made us be-lieve that it is a good prospect for dynamic adaptivemodeling.
If the dynamic nature of the system canbe disadvantageous for static language and transla-tion models, it is an incomparable advantage for acache based approach because human correction in-tervenes before words go in the cache.
As the trans-lator is using the system to correctly enter his trans-lation progressively, we can expect the theoreticalresults presented in the literature to be obtainable inpractice in the IMT context.The first advantage of dynamic adaptation wouldbe to help the translation engine make better predic-tions, but it has a further psychological advantage:as the translator works and potentially corrects theproposals of the engine, the user would feel that thesoftware is learning from its errors.The next section describes the models currentlyembedded within our IMT prototype.
Section 3 de-scribes the cache-based adaptation we performed onthe target language model.
In section 4, we presentthe different types of adaptations we performed onthe translation model.
Section 5 then puts the resultsin the context of our IMT application.
Section 6 dis-cusses the implications of our experiments and sug-gests some improvements that could be made to thesystem.2 Current IMT modelsThe word-based translation model embedded withinthe IMT system has been designed by Foster (2000).It is a Maximum Entropy/Minimum Divergence(MEMD) translation model (Berger et al, 1996),which mimics the parameters of the IBM model 2(Brown et al, 1993) within a log-linear setting.The resulting model (named MDI2B) is of thefollowing form, where h is the current target text,s the source sentence being translated, s a particularword in s and w the next word to be predicted:p(w|h, s) =q(w|h) exp(?s?s ?sw + ?AB)Z(h, s)(1)The q distribution represents the prior knowledgethat we have about the true distribution and is mod-eled by an interpolated trigram in this study.
The?
coefficients are the familiar transfer or lexical pa-rameters, and the ?
ones can be understood as theirposition dependent correction.
Z is a normalizingfactor, the sum of the numerator for every w in thetarget vocabulary.Our baseline model used an interpolated trigramof the following form as the q distribution:p(w|h) = ?1(wi?2wi?1) ?
ptri(wi|wi?2wi?1)+ ?2(wi?2wi?1) ?
pbi(wi|wi?1)+ ?3(wi?2wi?1) ?
puni(wi)+ ?4(wi?2wi?1) ?
1|V |+1where ?1(wi?2wi?1) + ?2(wi?2wi?1) +?3(wi?2wi?1) + ?4(wi?2wi?1) = 1 and |V | + 1is the size of the event space (including a specialunknown word).As mentioned above, the MDI2B model is closelyrelated to the IBM2 model (Brown et al, 1988).
Itcontains two classes of features: word pair featuresand positional features.
The word pair feature func-tions are defined as follows:fst(w,h, s) ={1 if s ?
s and t = w0 otherwiseThis function is on if the predicted word is t and sis in the current source sentence.
Each feature fsthas a corresponding weight ?st (for brevity, this isdefined to be 0 in equation 1 if the pair s, t is notincluded in the model).The positional feature functions are defined asfollows:fA,B(w, i, s) =J?j=1?
[(i, j, J) ?
A ?
(sj , w) ?
B ?
j = ?
?sj ]where ?
[X] is 1 if X is true, otherwise 0; and ?
?sjis the position of the occurrence of sj that is clos-est to i according to an IBM2 model.
A is a classthat groups positional (i, j, J) configurations havingsimilar IBM2 alignment probabilities, in order to re-duce data sparseness.
B is a class of word pairshaving similar weights ?st.
Its purpose is to simu-late the way IBM2 alignment probabilities modulateIBM1 word-pair probabilities, by allowing the valueof the positional feature weight to depend on themagnitude of the corresponding word-pair weight.As with the word pair features, each fA,B has a cor-responding weight ?AB .Since feature selection is applied at training timein order to improve speed, avoid overfitting, andkeep the model compact, the summation in the ex-ponential term in (1) is only carried out over the setof active pairs maintained by the model and not overall pairs as might be inferred from the formulation.To give an example of how the model works, ifthe source sentence is the fruit I am eating is a ba-nana and we are predicting the word banane follow-ing the target words: Le fruit que je mange est une,the active pairs involving banana would be (fruit,banana) and (banane, banana) since, of all the pairs(s, t) they would be the only ones kept by the fea-ture selection algorithm1.
The probability of bananewould therefore depend on the weights of those twopairs, along with position weights which capture therelative proximity of the words involved.3 Language model adaptationWe implemented a first monolingual dynamic adap-tation of this model by inserting a cache compo-nent in its reference distribution, thus only affect-ing the q distribution.
We obtained similar results1See (Foster, 2000) for the description of this algorithm.as for classical ngram models: the unigram cachemodel proved to be less efficient than the bigramone, and the trigram cache suffered from sparsity.We also tested a model where we interpolated thethree cache models to gain information from eachof the unigram, bigram, and trigram cache mod-els.
For completeness, this generalized model is de-scribed in equation 2 under the usual constraints that?i ?i(h) = 1 for all h.p(w|h) = ?1(h) ?
ptri(wi|wi?2wi?1)+ ?2(h) ?
pbi(wi|wi?1)+ ?3(h) ?
puni(wi)+ ?4(h) ?
1|V |+1+ ?5(h) ?
ptric(wi|wi?2wi?1)+ ?6(h) ?
pbic(wi|wi?1)+ ?7(h) ?
punic(wi)(2)Those models were trained from splits of theCanadian Hansard corpus.
The base ngram modelwas estimated with a 30M word split of the corpus.The weighting coefficients of both the base trigramand the cache models were estimated with an EMalgorithm trained with 1M words.We tested our models, translating from Englishto French, on two corpora of different types: thefirst one hansard is a document taken from thesame large corpus that was used for training (thetesting and training corpora were exclusive splits).The second one sniper, which describes the jobof a sniper, is from another domain characterizedby lexical and phrasal constructions very differentfrom those used to estimate the probabilities of ourmodels.Table 1 shows the perplexity on the hansardand the sniper corpora.
Preliminary experimentsled us to two sizes of cache which seemed promis-ing: 2000 and 5000 corresponding to the last 2000and 5000 words seen during the processing of a doc-ument.
The BI column gives the results of the bi-gram cache model and the 1+2+3 gives the resultsof the interpolated cache model which included theunigram, bigram and trigram cache.The results show that our models improve thebase static model by 5% on documents supposedlywell known by the models and by more that 52%on documents that are unknown to the model.
Sec-tion 5 puts these results in the perspective of ouractual IMT system.
Note that he addition of a cachecomponent to a language model involves negligibleextra training time.Taille BI ?
1+2+3 ?base hansard=17.65842000 16.937 -4.1% 16.840 -4.6%5000 16.903 -4.3% 16.777 -5.0%base sniper=135.8082000 73.936 -45.6% 67.780 -50.1%5000 70.514 -48.1% 64.204 -52.7%Table 1: Perplexities of the MDI2B model with acache component included in the reference distribu-tion on the hansard and sniper corpora.4 Translation model adaptationWith those excellent results in mind, we extendedthe idea of dynamic adaptation to the bilingual casewhich, to our knowledge, has never been tried be-fore.We developed a model called MDI2BCachewhich is a MDI2B model to which we added a cachecomponent based on word pairs.
Recall that, whenpredicting a word w at a certain point in a document,the probability depends on the weights of the pairs(s, w) for each active word s in the current sourcesentence.
As the prediction of the words of the doc-ument goes on, our model keeps in a cache eachactive pair used for the prediction of each word.
Inthe example above, if the translator accepts the wordbanane, then the two pairs (fruit, banana) and (ba-nane, banana) will be added to the cache.We added a new feature to the MEMD model totake into account the presence of a certain pair inthe recent history of the processed document:fcache st(w,h, s) =????????
?1 if????
?s ?
s,t = w,(s, t) ?
cache?st > p0 otherwiseWe added a threshold value p to the feature func-tion because while analyzing the pair weights, wediscovered that low weight pairs are usually pairs ofutility words such as conjunctions and punctuation.We also came to the conclusion that they are not thekind of words we want to have in the cache, sincetheir presence in a sentence implies little about theirpresence in the next.The resulting model is of the form:p(w|h, s) =q(w|h)exp(?s?s ?sw + ?AB + ?sw)Z(h, s)Thus, every fcache sw has a corresponding weight?sw for the calculation of the probability of w.Size 0.3 ?
0.5 ?
0.7 ?base One feature weight, no Viterbi orig perp=17.65841000 17.5676 -0.51% 17.5756 -0.47% 17.5983 -0.34%2000 17.5698 -0.50% 17.5766 -0.46% 17.5976 -0.34%5000 17.5743 -0.48% 17.5776 -0.46% 17.5965 -0.35%10000 17.5777 -0.46% 17.5791 -0.45% 17.5962 -0.35%base One feature weight per pair, no Viterbi orig perp=17.65841000 17.5817 -0.43% 17.5858 -0.41% 17.6065 -0.29%2000 17.5933 -0.37% 17.5918 -0.38% 17.6061 -0.30%5000 17.5849 -0.42% 17.5874 -0.40% 17.6076 -0.29%10000 17.5890 -0.39% 17.5891 -0.39% 17.6069 -0.29%base One feature weight, Viterbi orig perp=17.65841000 17.5602 -0.56% 17.5697 -0.50% 17.5940 -0.36%2000 17.5676 -0.51% 17.5695 -0.50% 17.5896 -0.39%5000 17.5614 -0.55% 17.5687 -0.51% 17.5925 -0.37%10000 17.5650 -0.53% 17.5687 -0.51% 17.5906 -0.38%Table 2: MDI2BCache test perplexities.
One feature weight, Viterbi alignment version.4.1 Number of cache featuresWe implemented two versions of the model, one inwhich we estimated only one cache feature weightfor the whole model and another in which we esti-mated one cache feature weight for every word pairin the model.The first model is simpler and is easier to esti-mate.
The assumption is made that every pair in themodel has the same tendency to repeat itself.The second model doubles the number of word-pair parameters compared to MDI2B, and thus leadsto a linear increase in training time.
Extra trainingtime is negligible in the first model.4.2 Word alignmentOne of the main difficulties of automatic MT is de-termining which source word(s) translate to whichtarget word(s).
It is very difficult to do this taskautomatically, in part because it is also very diffi-cult manually.
If a pair of sentences are given to10 translators for alignment, the results would likelynot be identical in all cases.
As it is nearly impossi-ble to determine such an alignment, most translationmodels consider every source word to have an effecton the translation of every target word.This difficulty shows up in our cache-basedmodel.
When adding word pairs to the cache, weideally would like to add only word pairs that werereally in a translation relation in the given sentence.This is why we also implemented a version of ourmodel in which a word alignment is first carried outin order to select good pairs to be added to the cache.For this purpose, we computed a Viterbi alignmentbased on an IBM model 2.
This results in a subset ofthe good active pairs to be added to the cache.
TheViterbi algorithm gives us a higher confidence levelthat the pair of words added to the cache were reallyin a translation relation.
But it can also lead to wordpairs not added to the cache that should have beenadded.4.3 ResultsTable 2 shows the results of the different configura-tions of the MDI2BCache model.
For every config-uration we trained and tested on splits of the Cana-dian Hansard with threshold values of 0.3, 0.5, and0.7 and cache sizes of 1000, 2000, 5000, and 10000.The top of the table is the version of the model withonly one feature weight without Viterbi alignment.The middle of the table is the version with one fea-ture weight per word pair without Viterbi alignment.Finally, the bottom is for the version with only onefeature weight and a Viterbi alignment made priorto adding pairs to the cache.Threshold values of 0.3, 0.5, and 0.7 led to 75%,50%, and 25% of the pairs considered for additionto the cache respectively.
The results show that thethreshold values of 0.5 and 0.7 are removing toomany pairs.
The best results are obtained with athreshold of 0.3 in all tests.
Since the number ofpairs kept in the model appears to vary in proportionto the threshold value, we did not consider it neces-sary to use an automatic search algorithm to find anoptimal threshold value.
The gain in performancewould have been negligible.The results also show that having one featureweight per word pair leads to lower results.
Thiscan be explained by the fact that it is much moreSize 0.3 ?
0.5 ?base MDI2B=135.8081000 132.865 -2.17% 132.751 -2.25%2000 132.771 -2.23% 132.752 -2.25%5000 132.733 -2.26% 132.628 -2.34%10000 132.997 -2.07% 132.674 -2.31%Table 3: MDI2BCache test perplexities.
One fea-ture weight, Viterbi alignment version.
Sniper testdifficult to estimate a weight for every pair that oneweight for all pairs.
Since we use only thousands ofwords in the cache, the training process suffers froma poor data representation.The Viterbi alignment seems to be helping themodels.
The best results are obtained with the ver-sion of our model with Viterbi alignment.
However,this gives only a 0.56% percent drop in perplexity.We then tested our best configuration on thesniper corpus.
Table 3 shows the results.
Wedropped threshold value 0.7 and tested only themodel with only one feature weight and a Viterbialignment.Results show that our bilingual cache modelshows improvement (four times higher) in drop ofperplexity when used on documents very differentfrom the training corpus.
In general, results givelower perplexity than our base model showing thatthe bilingual cache is helpful to the model, but theresults are not as good as that the ones obtained inthe unilingual case.
Section 6 discusses these resultsfurther.5 Evaluation of IMTAs stated earlier, drops in perplexity are theoreti-cal results that have been obtained previously in thecase of unilingual dynamic adaptation but for whicha corresponding level of practical success was rarelyattained because of the cache correctness problem.To show that the interactive nature of our assisted-translation application can really benefit from dy-namic adaptation, we tested our models in a morerealistic translation context.
This test consists ofsimulating a translator using the IMT system as itproposes words and phrases and accepting, correct-ing or rejecting the proposals by trying to reproducea given target translation (Foster et al, 2002).
Themetric used is the percentage of keystrokes savedby the use of the system instead of having to typedirectly all the target text.For these simulations, we used only a 10K wordsplit of the hansard and of the sniper cor-pus.
The reason is that the IMT application poten-Taille BI ?
1+2+3 ?base hansard=27.4352000 27.784 +1.3% 27.719 +1.0%5000 27.837 +1.5% 27.821 +1.4%base sniper=9.6862000 11.404 +15.1% 11.294 +14.2%5000 11.498 +15.8% 11.623 +16.7%Table 4: Saved keystrokes raises for the MDI2Bmodel with cache component in the reference dis-tribution on the hansard and sniper corpora.0.3 ?base hansard=27.43581000 27.557 +0.44%2000 27.531 +0.35%5000 27.488 +0.18%10000 27.468 +0.12%base sniper=9.6861000 9.896 +2.17%2000 10.023 +3.48%5000 9.983 +3.07%10000 9.957 +2.80%Table 5: Saved keystrokes raises for theMDI2BCache model with only one featureweight and Viterbi alignment on the hansard andsniper corpora.tially proposes new completions after every charac-ter typed by the user.
For a 10K word document, itneeds to search about 1 million times for high prob-ability words and phrases.
This leads to relativelylong simulation times, even though predictions aremade at real time speeds.Table 4 shows the results obtained with theMDI2B model to which we added a cache compo-nent for the reference interpolated trigram distribu-tion.We can see that the saved keystroke percentagesare proportional to the perplexity drops reported insection 3.
The use of our models raises the savedkeystrokes by nearly 1.5% in the case of well knowndocuments and by nearly 17% in the case of verydifferent documents.
These are very interesting re-sults for a potential professional use of TransType.Table 5 shows an increase in the number of savedkeystrokes: 0.44% on the hansard and 3.5% onthe sniper corpora.
Once again, the results arenot as impressive as the ones obtained for the mono-lingual dynamic adaptation case.6 DiscussionThe results presented in section 3 on languagemodel adaptation confirmed what had been reportedin the literature: adding a cache component to a lan-guage model leads to a drop in perplexity.
More-over, we were able to demonstrate that using acache-based language model inside a translationmodel leads to better performance for the wholetranslation model.
We obtained drops in perplexityof 5% on a corpus of the same type as the trainingcorpus and of 50% on a different one.
These theo-retical results lead to very good practical results.
Wewere able to increase the saved keystroke percent-age by 1.5% on the similar corpus as the trainingand by nearly 17% on the different corpus.
Theseresults confirm our hypothesis that dynamic adapta-tion with cache-based language model can be usefulin the context of IMT, particularly for new types oftexts.Results presented in section 4 on translationmodel adaptation show that our approach has ledto drops in perplexity although not as high as wewould have hoped.
To understand these disappoint-ing results, we analyzed the content of the cache fordifferent configurations of our MDI2BCache model.base 0.3 viterbi + 0.3(is,qu?)
(to,afin) (offence,crime)(.,sa) (was,a) (was,e?te?
)(this,,) (UNK,UNK) (very,tre`s)(all,toutes) (piece,le?gislative) (today,aujourd?hui)(have,du) (this,ce) (jobs,emploi)(the,pour) (per,100) (concern,inquie?tude)(on,du) (that,soient) (skin,peau)(of,un) (,,,) (there,y)(we,nous) (?,il) (government,le)(the,du) (any,tout) (an,un)18 68 86Table 6: Cache sampling of different configurationsof MDI2BCache model.Table 6 shows the results of our sampling.
Wetested three model configurations.
The first one, inthe first column, was the base MDI2BCache modelwhich adds all active pairs to the cache.
The secondconfiguration, in the second column, was a thresh-old value of 0.3 that brings about 75% of the pairsbeing added to the cache.
The last configuration wasa model with threshold value of 0.3 and a Viterbialignment made prior to the addition of pairs in thecache.
The three model configuration were withonly one feature weight.
For all three configura-tions, we took a sample of 10 pairs (shown in table6) and a sample of 100 pairs.
With the second sam-ple, we manually analyzed each pair and countedthe number of pairs (shown in the last row of the ta-ble) we believed were useful for the model (wordsthat are occasionally translations of one another).The results obtained in section 4 seem to agreewith the current analysis.
From left to right in the ta-ble, the pairs seem to contain more information andto be more appropriate additions to the cache.
Theconfiguration with Viterbi alignment which contains86 good pairs clearly seems to be the configurationwith the most interesting pairs.The problem with such a cache-based translationmodel seem to be similar to the balance betweenprecision and recall in information retrieval.
On onehand, we want to add in the cache every word pairin which the two words are in translation relation inthe text.
We further want to add only the pairs inwhich the two words are really in translation rela-tion in the text.
It seems that with our base model,we add most of the good pairs, but also a lot of badones.
With the Viterbi alignment and a thresholdvalue of 0.3, most of the pairs added are good ones,but we are probably missing a number of other ap-propriate ones.
This comes back to the task of wordalignment, which is a very difficult task for comput-ers (Mihalcea and Pedersen, 2003).Moreover, we would want to add in the cacheonly those words for which more than one transla-tion is possible.
For example, the pair (today, au-jourd?hui), though it is a very useful pair for thebase model, is unlikely to help when added to thecache.
The reason is simple: they are two wordsthat are always translations of one another, so themodel will have no problem predicting them.
Thisideal of precision and recall and of useful pairs inthe cache is obtained by our model with thresholdof 0.3, a Viterbi alignment and a cache size of 1000.One disadvantage of our bilingual adaptive modelis the way it handles unknown words.
In the cache-based language model, the unknown words weredealt with normally, i.e.
they were added to thecache and given a certain probability afterwards.So, if an unknown word was seen in a certain sen-tence and then later on, it would receive a proba-bility mass of its own but not the one given to anyunknown word.
By having its own probability massdue to its presence in the cache, such previously un-known word can be predicted by the model.
In thecase of our MDI2BCache model, because we havenot yet implemented an algorithm for guessing thetranslations of unknown words, they are simply rep-resented within the model as UNK words, whichmeans that the model never learns them.The results obtained with the sniper corpusshows us that dynamic adaptation is also more help-ful for documents that are little known to the modelin the bilingual context.
The results are four timesbetter on the sniper corpus than on the Hansardtesting corpus.Once again for the bilingual case, the practicaltest results in the number of saved keystrokes agreewith the theoretical results of drops in perplexity.This result shows that bilingual dynamic adaptationalso can be implemented in a practical context andobtain results similar to the theoretical results.All things considered, we believe that a cache-based translation model shows a great potentialfor bilingual adaptation and that greater perplexitydrops and keystroke savings could be obtained byeither reengineering the model or by improving theMDI2BCache model.6.1 Key improvements to the modelFollowing the analysis of the results obtained by ourmodel, we have pointed out some key improvementsthat the model would need in order to get better re-sults.
In this list we focus on ways of improvingadaptation strategies for the current model, omittingother obvious enhancements such as adding phrasetranslations.Unknown word processing Learning new wordswould be a very important feature to add tothe model and would lead to better results.
Wedid not incorporate the processing of unknownwords in the MDI2BCache because the struc-ture of model did not lend itself to this addi-tion.
Especially with documents such as thesniper corpus, we believe that this couldbe a key improvement for a dynamic adaptivemodel.Better alignment As mentioned before, the ulti-mate goal for our cache is that it contains onlythe pairs present in the perfect alignment.
Bet-ter performance from the alignment would leadto pairs in the cache closer to this ideal.
In thisstudy we computed Viterbi alignments from anIBM model 2, because it is very efficient tocompute and also because for training MDI2B,we do use the IBM model 2.
We could consideralso more advanced word alignment models(Och and Ney, 2000; Lin and Cherry, 2003;Moore, 2001).
To keep the alignment modelsimple, we could still use an IBM model 2, butwith the compositionality constraint that hasbeen shown to give better word alignment thanthe Viterbi one (Simard and Langlais, 2003).Feature weights We implemented two versions ofour model: one with only one feature weightand another with one feature weight for eachword pair.
The second model suffered frompoor data representation and our training algo-rithm wasn?t able to estimate good cache fea-ture weights.
We think that creating classesof word pairs, such as it was done for posi-tional alignment features, would lead to betterresults.
It would enable the model to take intoaccount the tendency that a pair has to repeatitself in a document.Relative weighting Another key improvement isthat changes to word-pair weights should berelative to each source word.
For example,if (house, maison) is a pair in the cache, wewould like to favour maison over possible al-ternatives such as chambre as a translation ofhouse.
In the existing model this is done byboosting the weight on (house,maison), whichhas the undesirable side-effect of making mai-son more important in the model than transla-tions of other source words in the current sen-tence which have not appeared in the cache.One way of eliminating this behaviour wouldbe to learn negative weights on alternatives like(house,chambre) which do not appear in thecache.We believe these improvements would better showthe potential of bilingual dynamic adaptation.7 ConclusionWe have presented dynamic adaptive translationmodels using cache-based implementations.
Wehave shown that monolingual dynamic adaptivemodels exhibit good theoretical performance in abilingual translation context.
We observed thatthese theoretical results carry over to practical gainsin the context of an IMT application.We have developed bilingual dynamic adaptationthrough a cache-based translation model.
Our re-sults show the potential of bilingual dynamic adap-tation.
We have given explanations about why theresults obtained are not as high as hoped and pre-sented some key improvements that should be madeto our model or should be taken into account in thedevelopment of a new model.We believe that this study reveals the potential foradaptive interactive machine translation system andwe hope to read similar reports for other implemen-tations of the same interactive scenario e.g.
(Och etal., 2003).ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Maximum Entropyapproach to Natural Language Processing.
Com-putational Linguistics, 22(1):39?71.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Fredrick Jelinek,Robert L. Mercer, and Paul Roossin.
1988.
Astatistical approach to language translation.
InProceedings of the International Conference onComputational Linguistics (COLING), pages 71?76, Budapest, Hungary, August.Peter F. Brown, Stephen A. Della Pietra, VincentDella J. Pietra, and Robert L. Mercer.
1993.The mathematics of Machine Translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?312, June.P.R.
Clarkson and P.R.
Robertson.
1997.
Languagemodel adaptation using mixtures and an expo-nentially decaying cache.
In IEEE Int.
Confer-ence on Acoustics, Speech, and Signal Process-ing, Munich.George Foster, Philippe Langlais, and Guy La-palme.
2002.
User-friendly text predictionfor translators.
In 2002 Conference on Em-pirical Methods in Natural Language Process-ing (EMNLP 2002), Philadelphia, July.
TT2TransType2.George Foster.
2000.
A Maximum Entropy / Mini-mum Divergence translation model.
In Proceed-ings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pages37?42, Hong Kong, October.Roland Kuhn and Renato De Mori.
1990.
A cache-based natural language model for speech recog-nition.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence (PAMI), 12(6):570?583, June.Dekang Lin and Colin Cherry.
2003.
Proalign:Shared task system description.
In NAACL 2003Workshop on Building and Using Parallel Texts:Data Driven Machine Translation and Beyond,pages 11?14, Edmonton Canada, May 31.
TT2.S.C.
Martin, J. Liermann, and H. Ney.
1997.
Adap-tative topic-dependent language modelling usingword-based varigrams.
In Eurospeech.Rada Mihalcea and Ted Pedersen.
2003.
An evalua-tion exercise for word alignment.
In Rada Mihal-cea and Ted Pedersen, editors, HLT-NAACL 2003Workshop: Building and Using Parallel Texts:Data Driven Machine Translation and Beyond,pages 1?10, Edmonton, Alberta, Canada, May31.
Association for Computational Linguistics.Robert C. Moore.
2001.
Towards a simple andaccurate statistical approach to learning transla-tion relationships among words.
In Workshop onData-driven Machine Translation, 39th AnnualMeeting and 10th Conference of the EuropeanChapter, pages 79?86, Toulouse, France.
Asso-ciation for Computational Linguistics.Franz Josef Och and Hermann Ney.
2000.
Im-proved statistical alignment models.
In Proceed-ings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pages440?447, Hong Kong, October.F.J.
Och, R. Zens, and H. Ney.
2003.
Efficientsearch for interactive statistical machine trans-lation.
In Proceedings of the 10th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL), pages 387?393, Budapest, Hungary, April.
TT2.Michel Simard and Philippe Langlais.
2003.
Statis-tical translation alignment with compositionalityconstraints.
In NAACL 2003 Workshop on Build-ing and Using Parallel Texts: Data Driven Ma-chine Translation and Beyond, pages 19?22, Ed-monton Canada, May 31.
TT2.
