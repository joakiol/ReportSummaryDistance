Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 169?176, Vancouver, October 2005. c?2005 Association for Computational LinguisticsHMM Word and Phrase Alignment for Statistical Machine TranslationYonggang Deng1 , William Byrne1,2Center for Language and Speech Processing, Johns Hopkins UniversityBaltimore, MD 21210, USA 1Machine Intelligence Lab, Cambridge University Engineering DepartmentTrumpington Street, Cambridge CB2 1PZ, UK 2dengyg@jhu.edu , wjb31@cam.ac.ukAbstractHMM-based models are developed for thealignment of words and phrases in bitext.The models are formulated so that align-ment and parameter estimation can be per-formed efficiently.
We find that Chinese-English word alignment performance iscomparable to that of IBM Model-4 evenover large training bitexts.
Phrase pairsextracted from word alignments generatedunder the model can also be used forphrase-based translation, and in Chineseto English and Arabic to English transla-tion, performance is comparable to sys-tems based on Model-4 alignments.
Di-rect phrase pair induction under the modelis described and shown to improve trans-lation performance.1 IntroductionDescribing word alignment is one of the fundamen-tal goals of Statistical Machine Translation (SMT).Alignment specifies how word order changes whena sentence is translated into another language, andgiven a sentence and its translation, alignment spec-ifies translation at the word level.
It is straightfor-ward to extend word alignment to phrase alignment:two phrases align if their words align.Deriving phrase pairs from word alignments isnow widely used in phrase-based SMT.
Parametersof a statistical word alignment model are estimatedfrom bitext, and the model is used to generate wordalignments over the same bitext.
Phrase pairs are ex-tracted from the aligned bitext and used in the SMTsystem.
With this approach the quality of the under-lying word alignments can have a strong influenceon phrase-based SMT system performance.
Thecommon practice therefore is to extract phrase pairsfrom the best attainable word alignments.
Currently,Model-4 alignments (Brown and others, 1993) asproduced by GIZA++ (Och and Ney, 2000) are oftenthe best that can be obtained, especially with largebitexts.Despite its modeling power and widespread use,Model-4 has shortcomings.
Its formulation is suchthat maximum likelihood parameter estimation andbitext alignment are implemented by approximate,hill-climbing, methods.
Consequently parameter es-timation can be slow, memory intensive, and diffi-cult to parallelize.
It is also difficult to computestatistics under Model-4.
This limits its usefulnessfor modeling tasks other than the generation of wordalignments.We describe an HMM alignment model devel-oped as an alternative to Model-4.
In the word align-ment and phrase-based translation experiments tobe presented, its performance is comparable or im-proved relative to Model-4.
Practically, we can trainthe model by the Forward-Backward algorithm, andby parallelizing estimation, we can control memoryusage, reduce the time needed for training, and in-crease the bitext used for training.
We can also com-pute statistics under the model in ways not practicalwith Model-4, and we show the value of this in theextraction of phrase pairs from bitext.2 HMM Word and Phrase AlignmentOur goal is to develop a generative probabilisticmodel of Word-to-Phrase (WtoP) alignment.
Westart with an l-word source sentence e = el1, and an169m-word target sentence f = fm1 , which is realizedas a sequence of K phrases: f = vK1 .Each phrase is generated as a translation of onesource word, which is determined by the alignmentsequence aK1 : eak ?
vk .
The length of each phraseis specified by the process ?K1 , which is constrainedso that?Kk=1 ?k = m.We also allow target phrases to be inserted, i.e.
tobe generated by a NULL source word.
For this, wedefine a binary hallucination sequence hK1 : if hk =0, then NULL ?
vk ; if hk = 1 then eak ?
vk.With all these quantities gathered into an align-ment a = (?K1 , aK1 , hK1 ,K), the modeling objectiveis to realize the conditional distribution P (f ,a|e).With the assumption that P (f ,a|e) = 0 if f 6= vK1 ,we write P (f ,a|e) = P (vK1 ,K, aK1 , hK1 , ?K1 |e) andP (vK1 ,K, aK1 , hK1 , ?K1 |e)= ?
(m|l) ?
P (K|m, e)?
P (aK1 , ?K1 , hK1 |K,m, e)?
P (vK1 |aK1 , hK1 , ?K1 ,K,m, e)We now describe the component distributions.Sentence Length ?
(m|l) determines the targetsentence length.
It is not needed during alignment,where sentence lengths are known, and is ignored.Phrase Count P (K|m, e) specifies the number oftarget phrases.
We use a simple, single parameterdistribution, with ?
= 8.0 throughoutP (K|m, e) = P (K|m, l) ?
?KWord-to-Phrase Alignment Alignment is aMarkov process that specifies the lengths of phrasesand their alignment with source wordsP (aK1 , hK1 , ?K1 |K,m, e)=K?k=1P (ak, hk, ?k|ak?1, ?k?1, e)=K?k=1p(ak|ak?1, hk; l) d(hk)n(?k; eak )The actual word-to-phrase alignment (ak) is a first-order Markov process, as in HMM-based word-to-word alignment (Vogel et al, 1996).
It necessarilydepends on the hallucination variablep(aj |aj?1, hj ; l)=??
?1 aj = aj?1, hj = 00 aj 6= aj?1, hj = 0a(aj |aj?1; l) hj = 1This formulation allows target phrases to be in-serted without disrupting the Markov dependenciesof phrases aligned to actual source words.The phrase length model n(?
; e) gives the proba-bility that a word e produces a phrase with ?
wordsin the target language; n(?
; e) is defined for ?
=1, ?
?
?
, N .
The hallucination process is a simplei.i.d.
process, where d(0) = p0, and d(1) = 1 ?
p0.Word-to-Phrase Translation The translation ofwords to phrases is given asP (vK1 |aK1 , hK1 , ?K1 ,K,m, e) =K?k=1p(vk|eak , hk, ?k)We introduce the notation vk = vk[1], .
.
.
, vk[?k]and a dummy variable xk (for phrase insertion) :xk ={eak hk = 1NULL hk = 0We define two models of word-to-phrase translation.This simplest is based on context-independent word-to-word translationp(vk|eak , hk, ?k) =?k?j=1t(vk[j] |xk)We also define a model that captures foreign wordcontext with bigram translation probabilitiesp(vk|eak , hk, ?k)= t(vk[1] |xk)?k?j=2t2(vk[j] | vk [j ?
1], xk)Here, t(f |e) is the usual context independent word-to-word translation probability.
The bigram trans-lation probability t2(f |f ?, e) specifies the likelihoodthat target word f is to follow f ?
in a phrase gener-ated by source word e.1702.1 Properties of the Model and Prior WorkThe formulation of the WtoP alignment modelwas motivated by both the HMM word alignmentmodel (Vogel et al, 1996) and IBM Model-4 withthe goal of building on the strengths of each.The relationship with the word-to-word HMMalignment model is straightforward.
For example,constraining the phrase length component n(?
; e)to permit only phrases of one word would give aword-to-word HMM alignment model.
The exten-sions introduced are the phrase count, and the phraselength models, and the bigram translation distribu-tion.
The hallucination process is motivated by theuse of NULL alignments into Markov alignmentmodels as done by (Och and Ney, 2003).The phrase length model is motivated byToutanova et al (2002) who introduced ?stay?
prob-abilities in HMM alignment as an alternative to wordfertility.
By comparison, Word-to-Phrase HMMalignment models contain detailed models of stateoccupancy, motivated by the IBM fertility model,which are more powerful than a single staying pa-rameter.
In fact, the WtoP model is a segmentalHidden Markov Model (Ostendorf et al, 1996), inwhich states emit observation sequences.Comparison with Model-4 is less straightforward.The main features of Model-4 are NULL sourcewords, source word fertility, and the distortionmodel.
The WtoP alignment model includes thefirst two of these.
However distortion, which al-lows hypothesized words to be distributed through-out the target sentence, is difficult to incorporate intoa model that supports efficient DP-based search.
Wepreserve efficiency in the WtoP model by insistingthat target words form connected phrases; this is notas general as Model-4 distortion.
This weaknessis somewhat offset by a more powerful (Markov)alignment process as well as by the phrase countdistribution.
Despite these differences, the WtoPalignment model and Model-4 allow similar align-ments.
For example, in Fig.
1, Model-4 would allowfefef f1 21 2 3 4Figure 1: Word-to-Word and Word-to-Phrase Linksf1, f3, and f4 to be generated by e1 with a fertilityof 3.
Under the WtoP model, e1 could generate f1and f3f4 with phrase lengths 1 and 2, respectively:source words can generate more than one phrase.This alignment could also be generated via foursingle word foreign phrases.
The balance betweenword-to-word and word-to-phrase alignments is setby the phrase count distribution parameter ?.
As?
increases, alignments with shorter phrases arefavored, and for very large ?
the model allowsonly word-to-word alignments (see Fig.
2).
Al-though the WtoP alignment model is more com-plex than the word-to-word HMM alignment model,the Baum-Welch and Viterbi algorithms can still beused.
Word-to-word alignments are generated bythe Viterbi algorithm: a?
= argmaxa P (f ,a|e); ifeak ?
vk , eak is linked to all the words in vk.The bigram translation probability relies on wordcontext, known to be helpful in translation (Bergeret al, 1996), to improve the identification of tar-get phrases.
As an example, f is the Chinese wordfor ?world trade center?.
Table 1 shows how thelikelihood of the correct English phrase is improvedwith bigram translation probabilities; this exampleis from the C?E, N=4 system of Table 2.Model unigram bigramP (world|f) 0.06 0.06P (trade|world, f) 0.06 0.99P (center|trade, f) 0.06 0.99P (world trade center|f, 3) 0.0002 0.0588Table 1: Context in Bigram Phrase Translation.There are of course much prior work in translationthat incorporates phrases.
Sumita et al (2004) de-velop a model of phrase-to-phrase alignment, whichwhile based on HMM alignment process, appearsto be deficient.
Marcu and Wong (2002) propose amodel to learn lexical correspondences at the phraselevel.
To our knowledge, ours is the first non-syntactic model of bitext alignment (as opposed totranslation) that links words and phrases.3 Embedded Alignment Model EstimationWe now discuss estimation of the WtoP model pa-rameters by the EM algorithm.
Since the WtoPmodel can be treated as an HMM with a very com-plex state space, it is straightforward to apply Baum-171Welch parameter estimation.
We show the forwardrecursion as an example.Given a sentence pair (el1, fm1 ), the forward prob-ability ?j(i, ?)
is defined as the probability of gen-erating the first j target words with the added con-dition that the target words f jj?
?+1 form a phrasealigned to source word ei.
It can be calculated recur-sively (omitting the hallucination process, for sim-plicity) as?j(i, ?)
={?i?,???j??
(i?, ??
)a(i|i?, l)}?
??
n(?
; ei) ?
t(fj?
?+1|ei) ?j?j?=j??+2t2(fj?
|ei)This recursion is over a trellis of l(N + 1)m nodes.Models are trained from a flat-start.
We beginwith 10 iterations of EM to train Model-1, followedby 5 EM iterations to train Model-2 (Brown and oth-ers, 1993).
We initialize the parameters of the word-to-word HMM alignment model by collecting wordalignment counts from the Model-2 Viterbi align-ments, and refine the word-to-word HMM alignmentmodel by 5 iterations of the Baum-Welch algorithm.We increase the order of the WtoP model (N ) from2 to the final value in increments of 1, by perform-ing 5 Baum Welch iterations at each step.
At the fi-nal value of N , we introduce the bigram translationprobability; we use Witten-Bell smoothing (1991)as a backoff strategy for t2, and other strategies arepossible.4 Bitext Word AlignmentWe now investigate bitext word alignment perfor-mance.
We start with the FBIS Chinese/Englishparallel corpus which consists of approx.
10M En-glish/7.5M Chinese words.
The Chinese side of thecorpus is segmented into words by the LDC seg-menter1.
The alignment test set consists of 124 sen-tences from the NIST 2001 dry-run MT-eval2 set thatare manually word aligned.We first analyze the distribution of word linkswithin these manual alignments.
Of the Chinesewords which are aligned to more than one Englishwords, 82% of these words align with consecutive1http://www.ldc.upenn.edu/Projects/Chinese2http://www.nist.gov/speech/tests/mtModel AER1?1 AER1?N AERC?
?EModel-4 37.9 68.3 37.3HMM, N=1 42.8 72.9 42.0HMM, N=2 38.3 71.2 38.1HMM, N=3 37.4 69.5 37.8HMM, N=4 37.1 69.1 37.8+ bigram t-table 37.5 65.8 37.1E?
?CModel-4 42.3 87.2 45.0HMM, N=1 45.0 90.6 47.2HMM, N=2 42.7 87.5 44.5+ bigram t-table 44.2 85.5 45.1Table 2: FBIS Bitext Alignment Error Rate.2 4 6 8 10 1215001850220025502900325036003950?# of hypothesized links0 142628303234363840Overall AER1?1 Links1?N LinksTotal LinksOverall AERFigure 2: Balancing Word and Phrase AlignmentsEnglish words (phrases).
In the other direction,among all English words which are aligned to mul-tiple Chinese words, 88% of these align to Chinesephrases.
In this collection, at least, word-to-phrasealignments are plentiful.Alignment performance is measured by theAlignment Error Rate (AER) (Och and Ney, 2003)AER(B;B?)
= 1?
2 ?
|B ?B?|/(|B?| + |B|)where B is a set reference word links, and B?
are theword links generated automatically.AER gives a general measure of word alignmentquality.
We are also interested in how the modelperforms over the word-to-word and word-to-phrasealignments it supports.
We split the reference align-ments into two subsets: B1?1 contains word-to-word reference links (e.g.
1?1 in Fig 1); andB1?N contains word-to-phrase reference links (e.g.1?3, 1?4 in Fig 1); The automatic alignment B?is partitioned similarly.
We define additional AERs:AER1?1 = AER(B1?1, B?1?1), and AER1?N =AER(B1?N , B?1?N ), which measure word-to-wordand word-to-phrase alignment, separately.Table 2 presents the three AER measurements for172the WtoP alignment models trained as described inSection 3.
GIZA++ Model 4 alignment performanceis also presented for comparison.
We note first thatthe word-to-word HMM (N=1) alignment model isworse than Model 4, as expected.
For the WtoPmodels in the C?E direction, we see reduced AERfor phrases lengths up to 4, although in the E?C di-rection, AER is reduced only for phrases of length2; performance for N > 2 is not reported.In introducing the bigram phrase translation (thebigram t-table), there is a tradeoff between word-to-word and word-to-phrase alignment quality.
Asmentioned, the bigram t-table increases the likeli-hood of word-to-phrase alignments.
In both transla-tion directions, this reduces the AER1?N .
However,it also causes increases in AER1?1, primarily due toa drop in recall: fewer word-to-word alignments areproduced.
For C?E, this is not severe enough tocause an overall AER increase; however, in E?C,AER does increase.Fig.
2 (C?E, N=4) shows how the 1-1 and 1-N alignment behavior is balanced by the phrasecount parameter.
As ?
increases, the model favorsalignments with more word-to-word links and fewerword-to-phrase links; the overall Alignment ErrorRate (AER) suggests a good balance at ?
= 8.0.After observing that the WtoP model performs aswell as Model-4 over the FBIS C-E bitext, we inves-tigated performance over these large bitexts :- ?NEWS?
containing non-UN parallel Chi-nese/English corpora from LDC (mainly FBIS, Xin-hua, Hong Kong, Sinorama, and Chinese Treebank).- ?NEWS+UN01-02?
also including UN parallelcorpora from the years 2001 and 2002.- ?ALL C-E?
refers to all the C-E bitext availablefrom LDC as of his submission; this consists of theNEWS corpora with the UN bitext from all years.Over all these collections, WtoP alignment per-formance (Table 3) is comparable to that of Model-4.
We do note a small degradation in the E?C WtoPalignments.
It is quite possible that this one-to-manymodel suffers slightly with English as the source andChinese as the target, since English sentences tend tobe longer.
Notably, simply increasing the amount ofbitext used in training need not improve AER.
How-ever, larger aligned bitexts can give improved phrasepair coverage of the test set.One of the desirable features of HMMs is that theBitext English Words Model C?E E?CM-4 37.1 45.3NEWS 71MWtoP 36.1 44.8NEWS+ M-4 36.1 43.4UN01-02 96M WtoP 36.4 44.2ALL C-E 200M WtoP 36.8 44.7Table 3: AER Over Large C-E Bitexts.Forward-Backward steps can be run in parallel: bi-text is partitioned; the Forward-Backward algorithmis run over the subsets on different CPUs; statisticsare merged to reestimate model parameters.
Parti-tioning the bitext also reduces the memory usage,since different cooccurrence tables can be kept foreach partition.
With the ?ALL C-E?
bitext collec-tion, a single set of WtoP models (C?E, N=4, bi-gram t-table) can be trained over 200M words ofChinese-English bitext by splitting training over 40CPUs; each Forward-Backward process takes lessthan 2GB of memory and the training run finishesin five days.
By contrast, the 96M English wordNEWS+UN01-02 is about the largest C-E bitextover which we can train Model-4 with our GIZA++configuration and computing infrastructure.Based on these and other experiments, in this pa-per we set a maximum value of N = 4 for F?E; inE?F, we set N=2 and omit the bigram phrase trans-lation probability; ?
is set to 8.0.
We do not claimthat this is optimal, however.5 Phrase Pair InductionA common approach to phrase-based translation isto extract an inventory of phrase pairs (PPI) from bi-text (Koehn et al, 2003), For example, in the phrase-extract algorithm (Och, 2002), a word alignmenta?m1 is generated over the bitext, and all word sub-sequences ei2i1 and fj2j1 are found that satisfy :a?m1 : a?j ?
[i1, i2] iff j ?
[j1, j2] .
(1)The PPI comprises all such phrase pairs (ei2i1 , fj2j1 ).The process can be stated slightly differently.First, we define a set of alignments :A(i1, i2; j1, j2) = {am1 : aj ?
[i1, i2] iff j ?
[j1, j2]} .If a?m1 ?
A(i1, i2; j1, j2) then (ei2i1 , fj2j1 ) form aphrase pair.Viewed in this way, there are many possible align-ments under which phrases might be paired, and173the selection of phrase pairs need not be based ona single alignment.
Rather than simply accepting aphrase pair (ei2i1 , fj2j1 ) if the unique MAP alignmentsatisfies Equation 1, we can assign a probability tophrases occurring as translation pairs :P (f , A(i1, i2; j1, j2 ) | e) =?a : am1 ?A(i1,i2;j1,j2 )P (f ,a|e)For a fixed set of indices i1, i2, j1, j2, the quan-tity P (f , A(i1, i2; j1, j2 ) | e) can be computed effi-ciently using a modified Forward algorithm.
SinceP (f |e) can also be computed by the Forward al-gorithm, the phrase-to-phrase posterior distributionP (A(i1, i2; j1, j2 ) | f , e) is easily found.PPI Induction Strategies In the phrase-extractalgorithm (Och, 2002), the alignment a?
is gener-ated as follows: Model-4 is trained in both directions(e.g.
F?E and E?F); two sets of word alignmentsare generated by the Viterbi algorithm for each setof models; and the two alignments are merged.
Thisforms a static aligned bitext.
Next, all foreign wordsequences up to a given length (here, 5 words) areextracted from the test set.
For each of these, aphrase pair is added to the PPI if the foreign phrasecan be found aligned to an English phrase underEq 1.
We refer to the result as the Model-4 ViterbiPhrase-Extract PPI.Constructed in this way, the PPI is limited tophrase pairs which can be found in the Viterbi align-ments.
Some foreign phrases which do appear inthe training bitext will not be included in the PPIbecause suitable English phrases cannot be found.To add these to the PPI we can use the phrase-to-phrase posterior distribution to find English phrasesas candidate translations.
This adds phrases to theViterbi Phrase-Extract PPI and increase the test setcoverage.
A somewhat ad hoc PPI Augmentationalgorithm is given to the right.Condition (A) extracts phrase pairs based on thegeometric mean of the E?F and F?E posteriors(Tg = 0.01 throughout).
The threshold Tp selectsadditional phrase pairs under a more forgiving crite-rion: as Tp decreases, more phrase pairs are addedand PPI coverage increases.
Note that this algorithmis constructed specifically to improve a Viterbi PPI;it is certainly not the only way to extract phrase pairsunder the phrase-to-phrase posterior distribution.Once the PPI phrase pairs are set, the phrase trans-lation probabilities are set based on the number oftimes each phrase pair is extracted from a sentencepair, i.e.
from relative frequencies.For each foreign phrase v not in the Viterbi PPI :For all pairs (fm1 , el1) and j1, j2 s.t.
fj2j1 = v :For 1 ?
i1 ?
i2 ?
l, findf(i1, i2) = PF?E(A(i1, i2; j1, j2) | el1, fm1 )b(i1, i2) = PE?F (A(i1, i2; j1, j2) | el1, fm1 )g(i1, i2) =?f(11, i2) b(i1, i2)(?i1, i?2) = argmax1?i1,i2?lg(i1, i2) , and set u = ei?2i?1Add (u, v) to the PPI if any of A, B, or C hold :b(?i1, i?2) ?
Tg and f (?i1, i?2) ?
Tg (A)b(?i1, i?2) < Tg and f (?i1, i?2) > Tp (B)f (?i1, i?2) < Tg and b(?i1, i?2) > Tp (C)PPI Augmentation via Phrase-Posterior InductionHMM-based models are often used if posteriordistributions are needed.
Model-1 can also be usedin this way (Venugopal et al, 2003), although it isa relatively weak alignment model.
By comparison,finding posterior distributions under Model-4 is dif-ficult.
The Word-to-Phrase alignment model appearsnot to suffer this tradeoff: it is a good model of wordalignment under which statistics such as the phrase-to-phrase posterior can be calculated.6 Translation ExperimentsWe evaluate the quality of phrase pairs extractedfrom the bitext through the translation performanceof the Translation Template Model (TTM) (Kumaret al, 2005), which is a phrase-based translation sys-tem implemented using weighted finite state trans-ducers.
Performance is measured by BLEU (Pap-ineni and others, 2001).Chinese?English Translation We report perfor-mance on the NIST Chinese/English 2002, 2003 and2004 (News only) MT evaluation sets.
These consistof 878, 919, and 901 sentences, respectively.
EachChinese sentence has 4 reference translations.We evaluate two C?E translation systems.
Thesmaller system is built on the FBIS C-E bitext col-lection.
The language model used for this system isa trigram word language model estimated with 21M174V-PE WtoP eval02 eval03 eval04 eval02 eval03 eval04Model Tp cvg BLEU cvg BLEU cvg BLEU cvg BLEU cvg BLEU cvg BLEUFBIS C?E System News A?E System1 M-4 - 20.1 23.8 17.7 22.8 20.2 23.0 19.5 36.9 21.5 39.1 18.5 40.02 0.7 24.6 24.6 21.4 23.7 24.6 23.7 23.8 37.6 26.6 40.2 22.4 40.33 WtoP - 19.7 23.9 17.4 23.3 19.8 23.3 18.4 36.2 20.6 38.6 17.4 39.24 1.0 23.1 24.0 20.0 23.7 23.2 23.5 21.8 36.7 24.3 39.3 20.4 39.75 0.9 24.0 24.8 20.9 23.9 24.0 23.8 23.2 37.2 25.8 39.7 21.8 40.16 0.7 24.6 24.9 21.3 24.0 24.7 23.9 23.7 37.2 26.5 39.7 22.4 39.97 0.5 24.9 24.9 21.6 24.1 24.8 23.9 24.0 37.2 26.9 39.7 22.7 39.8Large C?E System Large A?E System8 M-4 - 32.5 27.7 29.3 27.1 32.5 26.6 26.4 38.1 28.1 40.1 28.2 39.99 WtoP - 30.6 27.9 27.5 27.0 30.6 26.4 24.8 38.1 26.6 40.1 26.7 40.610 0.7 38.2 28.2 32.3 27.3 37.1 26.8 30.7 39.3 32.9 41.6 32.5 41.9Table 4: Translation Analysis and Performance of PPI Extraction Procedureswords taken from the English side of the bitext; alllanguage models are built with the SRILM toolkitusing Kneser-Ney smoothing (Stolcke, 2002).The larger system is based on alignments gener-ated over all available C-E bitext (the ?ALL C-E?collection of Section 4).
The language model isan equal-weight interpolated trigram model trainedover 373M English words taken from the Englishside of the bitext and the LDC Gigaword corpus.Arabic?English Translation We also evaluate ourWtoP alignment models in Arabic-English transla-tion.
We report results on a small and a large system.In each, Arabic text is tokenized by the Buckwalteranalyzer provided by LDC.
We test our models onNIST Arabic/English 2002, 2003 and 2004 (Newsonly) MT evaluation sets that consists of 1043, 663and 707 Arabic sentences, respectively.
Each Arabicsentence has 4 reference translations.In the small system, the training bitext is fromA-E News parallel text, with ?3.5M words on theEnglish side.
We follow the same training proce-dure and configurations as in Chinese/English sys-tem in both translation directions.
The languagemodel is an equal-weight interpolated trigram builtover ?400M words from the English side of the bi-text, including UN text, and the LDC English Gi-gaword collection.
The large Arabic/English systememploys the same language model.
Alignments aregenerated over all A-E bitext available from LDC asof this submission; this consists of approx.
130Mwords on the English side.WtoP Model and Model-4 Comparison We firstlook at translation performance of the small A?Eand C?E systems, where alignment models aretrained over the smaller bitext collections.
The base-line systems (Table 4, line 1) are based on Model-4Viterbi Phrase-Extract PPIs.We compare WtoP alignments directly to Model-4 alignments by extracting PPIs from the WtoPalignments using the Viterbi Phrase-Extract proce-dure (Table 4, line 3).
In C?E translation, perfor-mance is comparable to that of Model-4; in A?Etranslation, performance lags slightly.
As we addphrase pairs to the WtoP Viterbi Phrase-Extract PPIvia the Phrase-Posterior Augmentation procedure(Table 4, lines 4-7), we obtain a ?1% improvementin BLEU; the value of Tp = 0.7 gives improvementsacross all sets.
In C?E translation, this yields goodgains relative to Model-4, while in A?E we matchor improve the Model-4 performance.The performance gains through PPI augmentationare consistent with increased PPI coverage of the testset.
We tabulate the percentage of test set phrasesthat appear in each of the PPIs (the ?cvg?
valuesin Table 4).
The augmentation scheme is designedspecifically to increase coverage, and we find thatBLEU score improvements track the phrase cover-age of the test set.
This is further confirmed by theexperiment of Table 4, line 2 in which we take thePPI extracted from Model-4 Viterbi alignments, andadd phrase pairs to it using the Phrase-Posterior aug-mentation scheme with Tp = 0.7.
We find that theaugmentation scheme under the WtoP models canbe used to improve the Model-4 PPI itself.We also investigate C?E and A?E translationperformance with PPIs extracted from large bitexts.175Performance of systems based on Model-4 ViterbiPhrase-Extract PPIs is shown in Table 4, line 8.To train Model-4 using GIZA++, we split the bi-texts into two (A-E) or three (C-E) partitions, andtrain models for each division separately; we findthat memory usage is otherwise too great.
Theseserve as a single set of alignments for the bitext,as if they had been generated under a single align-ment model.
When we translate with Viterbi Phrase-Extract PPIs taken from WtoP alignments createdover all available bitext, we find comparable perfor-mance to the Model-4 baseline (Table 4, line 9).
Us-ing the Phrase-Posterior augmentation scheme withTp = 0.7 yields further improvement (Table 4, line10).
Pooling the sets to form two large C?E andA?E test sets, the A?E system improvements aresignificant at a 95% level (Och, 2003); the C?E sys-tems are only equivalent.7 ConclusionWe have described word-to-phrase alignment mod-els capable of good quality bitext word alignment.In Arabic-English and Chinese-English translationand alignment they compare well to Model-4, evenwith large bitexts.
The model architecture was in-spired by features of Model-4, such as fertility anddistortion, but care was taken to ensure that dy-namic programming procedures, such as EM andViterbi alignment, could still be performed.
Thereis practical value in this: training and alignmentare easily parallelized.
Working with HMMs alsomakes it straightforward to explore new modelingapproaches.
We show an augmentation scheme thatadds to phrases extracted from Viterbi alignments;this improves translation with both the WtoP and theModel-4 phrase pairs, even though it would be infea-sible to implement the scheme under Model-4 itself.We note that these models are still relatively simple,and we anticipate further alignment and translationimprovement as the models are refined.Acknowledgments The TTM translation system was providedby Shankar Kumar.
This work was funded by ONR MURIGrant N00014-01-1-0685.ReferencesA.
L. Berger, S. Della Pietra, and V. J. Della Pietra.
1996.A maximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.P.
F. Brown et al 1993.
The mathematics of machinetranslation: Parameter estimation.
Computational Lin-guistics, 19:263?312.P.
Koehn, F. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proc.
of HLT-NAACL.S.
Kumar, Y. Deng, and W. Byrne.
2005.
A weighted fi-nite state transducer translation template model for sta-tistical machine translation.
Journal of Natural Lan-guage Engineering, 11(3).D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proc.
of EMNLP.F.
Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proc.
of ACL, Hong Kong, China.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.
Och.
2002.
Statistical Machine Translation: FromSingle Word Models to Alignment Templates.
Ph.D.thesis, RWTH Aachen, Germany.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL.M.
Ostendorf, V. Digalakis, and O. Kimball.
1996.
FromHMMs to segment models: a unified view of stochas-tic modeling for speech recognition.
IEEE Trans.Acoustics, Speech and Signal Processing, 4:360?378.K.
Papineni et al 2001.
BLEU: a method for automaticevaluation of machine translation.
Technical ReportRC22176 (W0109-022), IBM Research Division.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
ICSLP.E.
Sumita et al 2004.
EBMT, SMT, Hybrid and More:ATR spoken language translation system.
In Proc.of the International Workshop on Spoken LanguageTranslation, Kyoto, Japan.K.
Toutanova, H. T. Ilhan, and C. Manning.
2002.
Exten-tions to HMM-based statistical word alignment mod-els.
In Proc.
of EMNLP.A.
Venugopal, S. Vogel, and A. Waibel.
2003.
Effectivephrase translation extraction from alignment models.In Proc.
of ACL.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM basedword alignment in statistical translation.
In Proc.
ofthe COLING.I.
H. Witten and T. C. Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novel eventsin adaptive text compression.
In IEEE Trans.
InformTheory, volume 37, pages 1085?1094, July.176
