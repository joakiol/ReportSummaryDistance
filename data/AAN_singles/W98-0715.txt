II|IIIIIIIIIIISemi-automatic Induction of Systematic Polysemyfrom WordNetNor iko  TomuroDePau l  Un ivers i tySchool of Computer  Science, Te lecommunicat ions  and  In fo rmat ion  Systems243 S. Wabash  Ave.Ch icago IL 60604cphdnt  ~ted .cs .depau l .eduAbstractThis paper describes a semi-automaticmethod of inducing underspecified seman-tic classes from WordNet verbs and nouns.An underspecified semantic lass is an ab-stract semantic lass which encodes sys-tematic polysem~f, a set of word sensesthat are related in systematic and pre-dictable ways.
We show the usefulnessof the induced classes in the semantic in-terpretations and contextual inferences ofreal-word texts by applying them to thepredicate-argument structures in Browncorpus.1 In t roduct ionWordNet (Miller, 1990) has been used as a gen-eral resource of broad-coverage lexical informationin many Natural Language Processing (NLP) tasks,including sense tagging, text summarization a d ma-chine translation.
However, like other large-scaleknowledge-base systems or machine readable dictio-naries (MRDs), WordNet contains massive ambigu-ity and redundancy.
In particular, since WordNetsenses are more fine-grained than most other MRDssuch as LDOCE (Procter, 1978), each word entry ismore ambiguous.
For example, WordNet 1.6 (re-leased December 1997) lists the following 9 sensesfor the verb write:1. write, compose, pen, indite - produce aliterary work2.
write - communicate or express by writing3.
publish, write - have (one's written work)issued for publication4.
write, drop a line - communicate (with) inwriting5.
write - communicate by letter6.
compose, wr i te -  write music7.
write - mark or trace on a surface8.
write - record data on a computer9.
spell, write - write or name the lettersThese fine sense distinctions may not be desired insome applications.
Consequently any system whichincorporates WordNet without customization mustpresume this redundancy, and may need to controlthe ambiguities in order to make the computationtractable.Although the redundancy in WordNet could bea drawback, it can be an ideal resource for abroad-coverage domain-independent semantic lexi-con based on underspecified semantic lasses (Buite-laar, 1997, 1998).
An underspecified semantic lassis an abstract semantic type which encodes sys-tematic polysemy (or regular polysemy (Apresjan,1973)): 1 a set of word senses that are related in sys-tematic and predictable ways (eg.
INSTITUTIONand BUILDING meanings of the word school).These related word senses are grouped together, andassigned an abstract semantic lass that generalizesthe relation.
This way, we do not need to distinguishor disambiguate word senses that encompass severalsemantic "axes", and we can regard azt underspec-ified class as a multi-dimensional semantic entity.This abstract class is underspecified because it doesnot specify either one of the member senses.
Here,in building a lexicon based on such underspecifiedsemantic lasses, redundancy in WordNet is a desir-able property since the amount of information lostby abstraction is minimized.
Also, since WordNetsense entries are taken from general but wide rangeof domains, systematic polysemy can be extractedfrom the dictionary rather than from a sense-taggedcorpus.
Therefore, data sparseness problems becomeless significant.
Then, the resulting lexicon can ef-fectively compact he redundancy and ambiguity inWordNet by two dimensions: abstraction and sys-tematic polysemy.The use of underspecified semantic lasses is oneof the underspecification techniques being investi-gated in recent years (van Deemter and Peters,I Note that systematic polysemy should be con-trasted with homonymy which refers to words whichhave more than one unrelated sense (eg.
FINAN-CIAL_INSTITUTION and SLOPING_LAND mean-ings of the word bank).108IIIIiIIIIIIIIIIIIII1996).
This underspecified class has several advan-tages.
First, it can compactly represent the am-biguity which arises from multiple related senses.Thus it is more expressive and computationaUy ef-ficient than single sense representations.
Second, itcan facilitate abductive inference through the sys-tematicity between senses: given a word with nrelated senses, the identification of one sense in acontext can imply maximally all n senses, some ofwhich may only be implicit in the context.
In addi-tion, when two systematically polysemous words areused together, the combination enables even morepowerful inferences through a complex matching be-tween the two sets of systematic relations.
Then, adomain-independent broad-coverage lexicon definedby such abstract underspecified classes can be usedas a background lexicon in domain-specific reason-ing tasks such as Information Extraction (Kilgarriff,1997), or as a general semantic lexicon for parsing,as well as for many other NLP tasks that requirecontextual inferences.However, automatic acquisition of systematic pol-ysemy has been a difficult ask.
In fact, in most pre-vious work in lexical semantics it is done manually(Buitelaar, 1997, 1998).
In this paper, we presenta semi-automatic method of inducing underspeci-fled semantic classes from WordNet verbs and nouns.The method first applies a statistical analysis to ob-tain a rough approximation f the sense dependen-cies found in WordNet.
Incorrect dependencies arethen manually filtered out.
Although the approachis not fully automated, it provides a principled wayof acquiring systematic polysemy from a large-scalelexical resource, and greatly reduces the amount ofmanual effort hat was previously required.
Further-more, by having a manual intervention, the resultswill be able to reflect our prior knowledge aboutWordNet that was not assumed in the statisticalanalysis.
To see the usefulness of the induced se-mantic classes in the contextual inferences of real-world texts, predicate-argument structures are ex-tracted from Brown corpus, and the occurrences ofsuch classes are observed.2 Sys temat ic  Po lysemyBefore presenting the induction method, we firstclarify what we consider a systematic polysemy inthe work described in this paper, and explain theassumptions we made for such polysemy.Our systematic polysemy is analogous to logicalpolysemy in (Pustejovsky, 1995): word senses inwhich there is no change in lexical category, and themultiple senses of the word have overlapping, depen-dent, or shared meanings.
This definition excludesmeanings obtained by cross-categorical lternations(eg.
denominals) or morphological ternations (eg.suffixing with -ify), or homonyms or metaphors, andincludes only the senses of the word of the same cat-109egory and form that have some systematic relations.For example, INSTITUTION and BUILDING mean-ings of the word school are systematically polyse-mons because BUILDING relates to INSTITUTIONby the location of the institution.For nouns, each polysemous sense often refers toa different object.
In the above example, school asINSTITUTION refers to an organization, whereasschool as BUILDING refers to a physical object.On the other hand, for verbs, polysemous senses re-fer to different aspects of the same action.
For ex-ample, a word write in the sentence "John wrotethe book" is ambiguous between CREATION (of thebook) and COMMUNICATION (through the con-tent of the book) meanings.
But they both de-scribe the same action of John writing the partic-ular book.
Here, these two meanings are system-atically related by referring to the causation aspect(CREATION) or the purpose aspect (COMMUNI-CATION) of the write action.
This view is largelyconsistent with the entailment relations (temporalinclusion and causation) used to organize WordNetverb taxonomies (Fellbaum, 1990}.Another assumption we made is the dependencybetween related senses.
In the work in this pa-per, sense dependency is viewed as sense exten-sion, similar to (Copestake and Briscoe.
1995), inwhich a primary sense causes the existence of sec-ondary senses.
This assumption is in accord withlexical rules (Copestake and Briscoe, 1995; Ostlerand Atkins, 1992), where meaning extension is ex-pressed by if-then implication rules.
In the aboveexample of the noun school, INSTITUTION mean-ing is considered as the primary and BUILDING asthe secondary, since institutions are likely to haveoffice space but building may be occupied by otherentities besides institutions.
Similarly for the verbwrite, CREATION is considered as the primary andCOMMUNICATION as the secondary, since commu-nication takes place through the object that is justproduced but communication can take place withoutproducing an object.3 Induct ion  MethodOur induction method is semi-automatic, requiringa manual filtering step between the phased auto-matic processing.
The basic scheme of our methodis to first identify the prominent pair-wise cooccur-fence between any two basic types (abstract senses),and then build more complex types (underspeci-fled classes) by the composition of those cooccur-fences.
But instead of generating/composing all pos-sible types statically, we only maintain the pair-wiserelations in a graph representation called type de-pendency graph, and dynamically form/induce theunderspecified classes during the phase when eachWordNet entry is assigned the class label(s).IIIIBased on the definitions and assumptions de-scribed in the previous ection 2, underspecified se-mantic classes are induced from WordNet 1.6 (re-leased December 1997) by the following steps:.
Select a set of abstract (coarse-grained) sensesfrom WordNet taxonomies as basic semantictypes.
This step is done manually, to deter-mine the right level of abstraction to capturesystematic polysemy..
Create a type dependency graph from ambigu-ous words in WordNet.
This step is done bytwo phased analyses: an automatic analysis fol-lowed by a manual filtering..
Generate a set of underspecified semanticclasses by partitioning the senses of each wordinto a set of basic types.
Each set becomes anunderspecified semantic lass.
This step is fullyautomatic.Each step is described in detail below.3.1 Coarse-grained Basic TypesAs has been pointed out previously, there are manyregularities between polysemous senses, and theseregularities eem to hold across words.
For ex'am-pie, words such as chicken and duck which haveANIMAL sense often have MEAT meaning also (i.e.,animal-grinding lexical rule (Copestake and Briscoe,1992)).
This generalization holds at an abstractlevel rather than the word sense level.
Therefore,-the first step in the induction is to select a set ofabstract senses that are useful in capturing the sys-tematicity.
To this end, WordNet is a good resourcebecause word senses (or synsets) are organized intaxonomies.Ideally, basic types should be semantically or-thogonal, to function essentially as the "axes" in ahigh-dimensional semantic space.
Good candidateswould be the top abstract nodes in the WordNet ax-enemies or lexicographers' file names listed in thesense entries.
However, both of them fall short offorming a set of orthogonal axes because of severalreasons.
First, domain categories are mixed in withontological categories (eg.
co , ,pet i t ion  and bodyverb categories).
Second, some categories are onto-logically more general than others (eg.
change cat-egory in verbs).
Third, particularly for the verbs,senses that seem to take different argument nountypes are found under the same category (eg.
"in-gest" and "use" in consumption category).
There-fore, some WordNet categories are broken into morespecific types.For the verbs, the following 18 abstract basictypes are selected.110ehange(CHA) communication(COMM)cognition(COG) competition(COMP)contact(CeNT) motion(MOT)emoeion(ENO) perception(PER)possession(POSS) stat ive(STA)~eather(WEA) ingestion(ING)use(USE) social(SOC) body(BOD)phy_creation(PCR) mental_creation(MCR)verbal_creagion (VCR)These are mostly taken from the classificationsmade by lexicographers.Two classes ("consumption" and "creation" are sub-divided into finer categories ( ingest ion,  use andphys ical /ment a l /verba l_creat  ion, respectively)according to the different predicate-argument struc-tures they take.For the nouns, 31 basic types are selected fromWordNet top categories (unique beginners): 2entity(ENT) life~orm(LIF)causal_agent(AGT) human(HUN)animal(ANI) plan~(PLA) object(OBJ)natural_object(NOBJ) substance(SUB)food(FOOD) artifact(AFT) article(ART)location(LOC) psych_feature(PSY)cognition(COG) feeling(FEEL)motivation(MOT) abstraction(ABS)time(TIME) space(SPA) attribute(ATT)relation(REL) social_relation(SREL)communication(C0MN) shape(SHA)measure(NEA) event(EVE) action(ACT)possession(POSS) state(STA)phenomena(PHE)Senses under the lexicographers' class "group" areredirected to other classes, assuming a collection ofa type has the same basic semantic properties as theindividual type.3.2 Type Dependency  GraphAfter the basic types are selected, the next step is tocreate a type dependency graph: a directed graph inwhich nodes represent the basic types, and directededges correspond to the systematic relations betweentwo basic types.The type dependency graph is constructed by anautomatic statistical analysis followed by a manualfiltering process, as described below.
The premisehere is that, if there is a systematic relation be-tween two types, and if the regularity is prominent,it can be captured by the type cooccurrence statis-tics.
In machine learning, several statistical tech-niques have been developed which discover depen-dencies among features (or causal structures), such2Noun top categories in WordNet do not match ex-actly with lexicographers" file names, in our experi-ment, noun types are determined by actually travers-ing the hierarchies, therefore they correspond to the topcategories.IIIII Figure 1: Part of type dependency graph for Word-Net verbsIIIIIIIIIIIIIIas Bayesian network learning (eg.
Spirtes et al,1993).
Those techniques use sophisticated meth-ods that take into consideration of multiple an-tecedents/causations and so on, and build a com-plex and precise model with probabilities associatedwith edges.
In our present work however, Word-Net is compiled from human lexicographers' entries,thus the data has a fair amount of arbitrariness (i.e.,noisy data).
Therefore, we chose a simple techniquewhich yields a simpler network, and used the resultas a rough approximation f the type dependenciesto be corrected manually at the next phase.The advantage of this automatic analysis here istwo fold: not only it discovers/reveals the semantictype associations with respect o the basic types se-lected from the previous tep, it also helps the man-ual filtering to become more informed and consistentthan by judging with mere intuition, since the resultis based on the actual content of WordNet.The type dependency graph is constructed in thefollowing way.
First, for all type-pairs extractedfrom the ambiguous words in WordNet, mutual in-formation is computed to obtain the association byusing the standard formula: for type tl, t2, a mutualinformation I(tl, t2) isf(tt^t.-)l ( t l , t2 ) - lg  l ( t t lx  l(tt)N Nwhere f(t) is the number of occurrence of the typet, and N is the size of the data.
The associationbetween two types are considered prominent whenthe mutual information value was greater than somethreshold (in our current implementation, it is 0).At this point, type associations are undirected be-cause mutual information is symmetric (i.e., commu-tative).
Then, these associations are manually in-spected to create a directed type dependency graphin the next phase.
The manual filtering does twothings: to filter out the spurious relations (i.e.,false positives) and add back the missing ones (i.e.,false negatives), and to determine the direction ofthe correct associations.
Detected false positivesare mostly homonyms (including metaphors) (eg.111WEA-EM0 (weather and emotion) verb type pair forwords such as the word ignite).
False negatives aremostly the ones that we know exist, but were not sig-nificant according to the cooccurrence statistics (eg.ANI-F00D in nouns).
As a heuristic to detect thefalse negatives, we used the cross-categorical inheri-tance in the taxonomies in which category switchesas the hierarchy is traversed up.The direction of the associations are determinedby sense extension described in section 2.
In addi-tion, we used "the ontological generality of the ba-sic types as another criteria.
This is because atransitive inference through a ontologically generaltype may result in a relation where unrelated (spe-cific) types are combined, particularly when the spe-cific types are domain categories.
For instance, theverb category Cl~ (change) is ontologically gen-eral, and may occur with specific types in entail-ment relation.
But the transitive inference is donethrough this general type does not necessarily guar-antee the systematicity between the associated spe-cific types.
In order to prevent his kind of im-plausible inference, we restricted the direction ofa systematic relation to be from the specific typeto the general type, if one of the member typesis the generalization of the other.
Note for someassociations which involve equally general/specifictypes ontologically (such as COG (cognit ion) andC0MH (co~tmicat ion)) ,  the direction was consid-ered bidirectional (unless sens~ extension stronglysuggests the dependency).
A part of the type depen-dency graph for WordNet verbs is shown in Figure1.3.3 Underspecified Semantic ClassesUnderspecified semantic lasses are automaticallyformed by partitioning the ambiguous senses of eachword according to the type dependency graph.Using the type dependency graph, all words inWordNet verb and noun categories are assigned oneor more type partitions.
A partition is an orderedset of basic types (abstracted from the fine-grainedword senses in the first step) keyed by the primarytype emcompassing the secondary types.
From alist of frequency-ordered s nses of a WordNet word,a partition is created by taking one of the three mostfrequent types (listed as the first three senses in theWordNet entry) as the primary and collecting thesecondary types from the remaining list accordingto the type dependency graph.
3 Here, the secondarytypes are taken only from the nodes/types that aredirectly connected to the primary type.
That is be-3The reason we look at the first three senses i becauseprimary types are not always listed as the most frequentsense in the WordNec sense lists (or in actual usage forthat matter).
We chose the first three senses becausethe average degree of polysemy isaround 3 for WordNet(version 1.6) verbs and nouns.IIIIIIIIIIIIIITable 1: Example verbs in CONT classesVerb Class VerbscONT-CHA :" blend, crush, enclose, fasten,fold, puncture, tie, weldCON'r-HOT beat, chop, fumble, jerk,kick, press, spread, whipCONT-POSS pluck, release, seize, spongeC0NT-MOT-CHA "' dip, gather, mount, take_outC0HT-HOT-POSS carry, cover, fling, tosscause we assumed if an indirect transitive depen-dency of t l  on t3 through t2 is strong enough, it willbe captured as a direct dependency.
This schemealso ensures the existence of a core concept in ev-ery partition (thus more implausible than transitivecomposition ).
This procedure is applied recursivelyif the sense list of a word was not covered by one par-tition (note in this case, the word is a homonym).As an example, for the verb wr i te  whose senselist is (VCR C0g, H PCR Cl~t),4 the first 3 types VCR,COI~ and PCR are picked in turn as the primary typeto see whether a partition can be created that en-compasses all other member types.
In this case, apartition keyed by PCR can cover all member types(see the type dependency graph in Figure i), thusa class VCR-C0~-PCR-CBA is created.
The system-atic relation of this class would be "a change orcreation action which involves words (and resultedsome object), performed for communication purpose(through the object)".For the verbs and nouns in WordNet 1.6, 136underspecified verb classes and 325 underspecifiednoun classes are formed.
Some verbs of the classesinvolving ?ontacl; (coN'r) areshown in Table I.We can observe from the words assigned to eachclass that member types are indeed systematicallyrelated.
For example, CONT-MOT class representsan action which involves physical contact resultingfrom motion (MOT).
Words assigned to this class doseem to have tool;ion flavor.
On  the other hand,CONT-POSS class represents a transfer of posses-sion (P0SS) which involves physical contact.
Again,words in this class do seem to be used in a contextin which possession of something is changed.
Forthe more polysemous class CONT-HOT-POSS, wordsin this class, for instance toss, do seem to cover allthree member types.By using the underspecified classes, the degree ofambiguity in WordNet has substantially decreased.Table 2 shows the summary of our results (indicatedby Und) compared to the original WordNet statis-tics.
There, the advantage of our underspecifiedclasses for reducing ambiguity seems very effective4The original 9 senses listed in WordNet were com-pressed own to these 4 basic types.112Table 2: Average degree of ambiguity in WordNetCategory \[ All words Polysemy onlyr WordNet I Und WordNet I Undverb 2.13 I 1.37 I 3.57\[ 2.39"noun 1.23 1.06 2.73 2.2,1 .for polysemous verbs (from 3.57 to 2.39, 33 % de-crease).
This is an encouraging result because manyfamiliar (frequently used) verbs are polysemous inactual usage.4 App l i ca t ionTo observe how the induced underspecified classesfacilitates abductive inference in the contextual un-derstanding of real-world texts, predicate-argumentstructures were extracted from the Brown corpus.
?Table 3 shows some examples of the extractedverb-object relations involving the verb class VCR(verbal_creal; ion).Abductive inference facilitated by underspecifiedclasses is most significant when both the predicateand the argument are systematically polysemous.We call this a multi-facet matching.
6 As an example,the verb wr i te (VCR-COMM-PCR-CHA) takes an objectnoun paper  (AFT-COHM) in a sentence in Brown cor-pusIn 19,J8, Afranio Do Amaral, the notedBrazilian herpetologist, wrote a technicalpaper on the giant snakes.In this sentence, by matching the two systematicallypolysemous words wr i te and paper,  multiple in-terpretations are simultaneously possible.
The mostpreferred reading, according to the hand-tagged cor-pus WNSEMCOR, would be the match between VCRof the verb (sense # 3 of wr i te  - to have somethingpublished, as shown in section 1) and C0MM of thenoun (sense ~ 2 of paper  - an essay), giving risethe reading "to publish an essay".
However in thiscontext, other readings are possible as well.
For in-stance, the match between verb gca and noun AFT(a printed media), which gives rise the reading "tohave a written material printed for publishing".
Oranother eading is possible from the match betweenverb C0HH (sense # 2 of wr i te  - to communicate(thoughts) by writing) and noun AFT, which givesSPredicate~argument structures (verb-object andsubject-verb relations in this experiment) are extractedby syntactic pattern matching, similar to the cascadedfufite-state processing used in FASTUS (Hobbs, et al,1997)).
In the preliminary performance analysis, recallwas around 50 % and precision was around 80 %.6 By taking the first sense for both predicate verb andargument noun, 78 % of the verb-object relations and66 ?70 of the subject-verb relations were systematicallypolysemous for at least one constituent.III\[\[IIIIIIL Verb ClassVCRVCR-~CRVCR-COMKVCR-COMM-PCR-CIIATable 3: Examples of verb-object relations extracted from Brown corpusVerb Object Nounspen note (COI~-ATT-POSS), dispatch (C0/~ILACT-ATT)draft  agreement COI~I-ATT..-COG-REL-ACT), ordinance (C0MM)write_out number (ATT-COMII), question (ACT-COMM-ATT)dramatize comment (COIIM-ACT), fact (COG-C01fl4-STA), scene (LOC)write article (AFT-COI~I-ART-RF, L), book (AFT-COYd~),description (C0/IN-ACT-C0G), fiction (COMH),letter (C0/ei-ACT), paper (AFT-COMM), song (AFT-ACT-COMM)rise the reading "to communicate through a printedmedia".
This reading implies the purpose and en-tailment of the write action (as COMH): a paper waswritten to communicate some thoughts, and thosethoughts were very likely understood by the readers.Also from those readings, we can infer the paper isan artifacl;,  that is, a physical object rather thanan intangible mental object such as "idea" for in-stance.
Those secondary readings can be used laterin the discourse to make further inferences on thewrite action, and to resolve references to the pa-per either from the content of the paper (i.e., essay)or from the physical object itself (i.e., a printed ar-tifact).One interesting observation on multi-facet match-ing is the polysemous degrees of matched classes.Table 4 shows the predicate verbs of different sys-tematically polysemous classes and the average pol-ysemous degree of argument nouns observed in verb-object and subject-verb relations, r The result indi-cates, as the verb becomes more polysemous, thepolysemous degree of the argument stays about thesame for both subject and object nouns.
This sug-gests a complex multi-facet matching between verband noun basic types, since the polysemous degreeof nouns does not monotonically increase.5 DiscussionThe induction method described above should beconsidered as an initial attempt to automatically ac-quiring systematic polysemy from a broad-coveragelexical resource.
The task is essentially to mapour semantic/ontological knowledge about the sys-tematicity ofword meanings to some computationalterms for a given iexical resource.
In our presentwork, we mapped the systematicity o the cooccur-fence of word senses.
But the mapping only bycomputational/automatic means (mutual informa-r'I'he predicate-argument structures in this table rep-resettt the ones in which both verb and noun entries axefound in WordNet.
The total numbers of structures ex-tracted from Brown corpus were 47287 for verb-objectand 39266 for sub j-verb.
Discrepancies were mostly dueto proper nouns and pronouns which WordNet does notencode.113tion) was not possible: manual filtering was furtherneeded to enhance the mapping.Also, there was a difficulty with type dependencygraph.
In the current scheme, systematicity amongpolysemous senses are represented by binary rela-tions between a primary and a secondary sense inthe graph.
A partition, and eventually an under-specified class, is formed by taking all the secondarysenses from the primary sense listed in each Word-Net entry.
The difficulty is that some combinationsdo not seem correct collectively.
For example, aclass PKR-COG-CONT consists of two binary relations:PER-COG (to reason about what is perceived, eg.
de-tect), and PER-CONT (to perceive through physicalcontact, eg.
hide).
Although each one correctlyrepresents a systematic relation, PF_,R-COG-CONT doesnot seem correct as a collection.
In the Word-Net entries, a verb bury is assigned to this classPER-COG-CONT.
Here, CONT sense seems to select fora physical object (as in '"they buried the stolengoods"), whereas COG sense (to dismiss from themind) seems to select for a mental non-physical ob-ject.
Therefore the construction of type partitionsneeds more careful considerations.
Also the appli-cability of the induced classes must be evaluated inthe further analysis.6 Future WorkThe work described in this paper is still preliminary.Our current induction method is semi-automatic,requiring some manual intervention.
The first twosteps, which selects basic types and creates type de-pendency graph, could be improved to further de-crease the amount of manual effort, possibly to fullyautomated processes.
The issues, then, will be howto detect he right level of abstraction and how toincorporate our linguistic knowledge as a prior do-main knowledge in the induction algorithm for thegiven resource (WordNet).Our next plan is to further analyze the result ofthe experiment and extract the selectional prefer-ences, which will help disambiguate arid refine thepolysemous senses to a more restricted set of sensesused in the context.
However, as pointed out in(Resnik.
1997), strong selectional preferences maynot be observed for broad-coverage t xts, particu-IIIIIiIIIIIIIIIIIIITable 4: Systematically polysemous verbs and average polysemous degree of argument nounsVerb Object SubjectAverage AverageVerb Class #of  ~:of Noun Class # of Noun ClassPolyDeg Verbs Nouns Poly peg Nouns Poly Deg1 2729 9104 2.00 8969 1.712 714 5934 2.02' 3884 1.653 169 2948 1.96' 2402 1.724 34 1958 1.98 1640 1.715 1 279 1.95 87 1.37Total Jlady at the abstract level which our underspecifiedclasses are defined.Another important extension is to define a repre-sentation.for each underspecified class that explic-itly encodes how the senses relate to one another.Such information, which captures the implicit, com-plicated interactions between different aspects of anaction which may involve implied objects, can beencoded in a structured lexical representation thatis along the same line of some recent research inlexical semantics (eg.
Pustejovsky, 1995; Verspoor,1997) and knowledge representation.
Then, it willbe interesting to see such representation defined atthe abstract polysemous class level can be combinedwith micro (word sense) level representation (eg.
~(Harabagiu and Moldovan, 1997}}.AcknowledgmentsThe author would like to thank Paul Buitelaar forhelpful discussions, insights and encouragement.Re ferencesApresjan, J.
(1973).
Regular Polysemy.
Linguistics,(142).Buitelaar, P. (1997}.
A Lexicon for Underspeci-fled Semantic Tagging, In Proceedings of the A CLSIGLEX Workshop on Tagging Text with LexicalSemantics.
pp.
25-33.Buitelaat, P. (1998).
CORELEX: Systematic Poly-semy and Underspecification.
Ph.D. dissertation,Department ofComputer Science, Brandeis Uni-versity.Copestake, A. and Briscoe, T. (1992).
Lexical Oper-ations in a Unification-based Framework, In Lex-ical Semantics and Knowledge Representation, J.Pustejovsky (eds.
), pp.
101-119, Springer-Verlag.Copestake, A. and Briscoe, T. (1995).
Semi-productive Polysemy and Sense Extension.
Jour-nal of Semantics, 12.Fellbaum, C. (1990).
English Verbs as a SemanticNet.
International Journal of Lexicography, 3(4), pp.
278-301.114Harabagiu, S. and Moldovan, D. (1997).
TextNet- A Text-based Intelligent System.
Natural Lan-guage Engineering, 3.Hobbs, J., Appelt, D. Bear, J., Israel, D., Ka-mayama, M., Stickel, M. and Tyson, M.(1997).
FASTUS: A Cascaded Finite-stateTransducer for Extracting Information fromNatural-language T xt, In Finite-state LanguageProcessing, E. Roche and Y. Schabes (eds.
), pp.383-406, The MIT Press.Kilgarriff, A.
(1997).
Foreground and BackgroundLexicons and Word Sense Disambiguation for In-formation Extraction, In Proceedings of the In-ternational Workshop on Lexically Driven Infor-mation Extraction.Miller, G.
(eds.)
(1990).
WORDNET.
An OnlineLexical Database.
International Journal of Lex-icography, 3 (4),Ostler, N. and Atkins, B.
(1992).
Predictable Mean-ing Shift: Some Linguistic Properties of Lexi-cal Implication Rules, In Lexical Semantics andKnowledge Representation, J. Pustejovsky (eds.),pp.
87-100, Springer-Verlag.Procter, P. (1978).
Longman dictionary of Contem-porary English, Longman Group.Pustejovsky, J.
(1995}.
The Generative Lexicon,The MIT Press.Resnik, P. (1997}.
Selectional Preference andSense Disambiguation, I  Proceedings of the A CLSIGLEX Workshop on Tagging Text with LexicalSemantics.
pp.
52-57.Spirtes, P., Glymour, C. and Scheines, R. (1993).Causation, Prediction and Search, Springer-Verlag.van Deemter.
K. and Peters, S. (1996).
SemanticAmbiguity and b'nderspecification, CSLI LectureNotes 55, Cambridge University Press.Verspoor, C. (1997).
Contextually-dependent Lexi-cal Semantics.
Ph.D. dissertation, University ofEdinburgh.
