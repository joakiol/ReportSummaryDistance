Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1266?1276,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsBilingual Segmented Topic ModelAkihiro Tamura and Eiichiro SumitaNational Institute of Information and Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN{akihiro.tamura, eiichiro.sumita}@nict.go.jpAbstractThis study proposes the bilingual seg-mented topic model (BiSTM), which hi-erarchically models documents by treat-ing each document as a set of segments,e.g., sections.
While previous bilingualtopic models, such as bilingual latentDirichlet alocation (BiLDA) (Mimno etal., 2009; Ni et al, 2009), consider onlycross-lingual alignments between entiredocuments, the proposed model consid-ers cross-lingual alignments between seg-ments in addition to document-level align-ments and assigns the same topic distri-bution to aligned segments.
This studyalso presents a method for simultane-ously inferring latent topics and segmen-tation boundaries, incorporating unsuper-vised topic segmentation (Du et al, 2013)into BiSTM.
Experimental results showthat the proposed model significantly out-performs BiLDA in terms of perplexityand demonstrates improved performancein translation pair extraction (up to +0.083extraction accuracy).1 IntroductionProbabilistic topic models, such as probabilis-tic latent semantic analysis (PLSA) (Hofmann,1999) and latent Dirichlet alocation (LDA) (Bleiet al, 2003), are generative models for documentsthat have been used as unsupervised frameworksto discover latent topics in document collectionswithout prior knowledge.
These topic modelswere originally applied to monolingual data; how-ever, various recent studies have proposed the useof probabilistic topic models in multilingual set-			 				  					  																																			 					 (football): soccer 1112   !
"#$ %&'()( *+ciatiall ; ,-./+ 012&'()( 34./+1 2  (name)3  (game)4  (history)5Figure 1: Wikipedia Article Exampletings1, where latent topics are shared across multi-ple languages.
These models have improved sev-eral multilingual tasks, such as translation pair ex-traction and cross-lingual text classification (seethe survey paper by Vuli?c et al (2015) for details).Most multilingual topic models, including bilin-gual LDA (BiLDA) (Mimno et al, 2009; Ni etal., 2009), model a document-aligned comparablecorpus, such as a collection of Wikipedia articles,where aligned documents are topically similar butare not direct translations2.
In particular, thesemodels assume that the documents in each tupleshare the same topic distribution and that eachcross-lingual topic has a language-specific worddistribution.Existing multilingual topic models consideronly document-level alignments.
However, mostdocuments are hierarchically structured, i.e., adocument comprises segments (e.g., sections andparagraphs) that can be aligned across languages.Figure 1 shows a Wikipedia article example,which contains a set of sections.
Sections 1, 2,and 3 in the English article correspond topically tosections 4, 2, and 3 in the Japanese counterpart, re-1In this work, we deal with a bilingual setting, but ourapproach can be extended straightforwardly to apply to morethan two languages.2In this study, we focus on models for a document-alignedcomparable corpus.
We describe other types of multilingualtopic models and their limitations in Section 7.1266spectively.
To date, such segment-level alignmentshave been ignored; however, we consider that suchcorresponding segments must share the same topicdistribution.Du et al (2010) have shown that segment-leveltopics and their dependencies can improve model-ing accuracy in a monolingual setting.
Based onthat research, we expect that segment-level topicscan also be useful for modeling multilingual data.This study proposes a bilingual segmented topicmodel (BiSTM) that extends BiLDA to capturesegment-level alignments through a hierarchicalstructure.
In particular, BiSTM considers eachdocument as a set of segments and models a docu-ment as a document-segment-word structure.
Thetopic distribution of each segment (per-segmenttopic distribution) is generated using a Pitman?Yor process (PYP) (Pitman and Yor, 1997), inwhich the base measure is the topic distribution ofthe related document (per-document topic distri-bution).
In addition, BiSTM introduces a binaryvariable that indicates whether two segments indifferent languages are aligned.
If two segmentsare aligned, their per-segment topic distributionsare shared; if they are not aligned, they are inde-pendently generated.BiSTM leverages existing segments from agiven segmentation.
However, a segmentation isnot always given, and a given segmentation mightnot be optimal for statistical modeling.
Therefore,this study also presents a model, BiSTM+TS, thatincorporates unsupervised topic segmentation intoBiSTM.
BiSTM+TS integrates point-wise bound-ary sampling into BiSTM in a manner similar tothat proposed by Du et al (2013) and infers seg-mentation boundaries and latent topics jointly.Experiments using an English?Japanese andEnglish?French Wikipedia corpus show that theproposed models (BiSTM and BiSTM+TS) sig-nificantly outperform the standard bilingual topicmodel (BiLDA) in terms of perplexity, and thatthey improve performance in translation extrac-tion (up to +0.083 top 1 accuracy).
The exper-iments also reveal that BiSTM+TS is comparableto BiSTM, which uses manually provided segmen-tation, i.e., section boundaries in Wikipedia arti-cles.2 Bilingual LDAThis section describes the BiLDA model (Mimnoet al, 2009; Ni et al, 2009), which we take as          Figure 2: Graphical Model of BiLDAAlgorithm 1 Generative Process of BiLDA1: for each topic k ?
{1, ...,K} do2: for each language l ?
{e, f} do3: choose ?lk ?
Dirichlet(?l)4: end for5: end for6: for each document pair di(i ?
{1, ..., D}) do7: choose ?i ?
Dirichlet(?
)8: for each language l ?
{e, f} do9: for each word wlim(m ?
{1, ..., Nli}) do10: choose zlim?
Multinomial(?i)11: choose wlim?
p(wlim|zlim,?l)12: end for13: end for14: end forour baseline.
BiLDA is a bilingual extension ofbasic monolingual LDA (Blei et al, 2003) fora document-aligned comparable corpus.
Whilemonolingual LDA assumes that each documenthas its own topic distribution, BiLDA assumes thataligned documents share the same topic distribu-tion and discovers latent cross-lingual topics.Algorithm 1 and Figure 2 show the genera-tive process and graphical model, respectively, ofBiLDA.
BiLDA models a document-aligned com-parable corpus, i.e., a set of D document pairsin two languages, e and f .
Each document pairdi(i ?
{1, ..., D}) comprises aligned documentsin the language e and f : di=(dei, dfi).
BiLDA as-sumes that each topic k ?
{1, ...,K} comprisesthe set of a discrete distribution over words foreach language.
Each language-specific per-topicword distribution ?lk (l ?
{e, f}) is drawn froma Dirichlet distribution with the prior ?l(Steps1-5).
To generate a document pair di, the per-document topic distribution ?i is first drawn froma Dirichlet distribution with the prior ?
(Step 7).Thus, aligned documents deiand dfishare the sametopic distribution.
Then, for each word at m ?
{1, ..., Nli} in document dliin language l, a latenttopic assignment zlimis drawn from a multinomial1267         Figure 3: Graphical Model of BiSTMAlgorithm 2 Generative Process of BiSTM1: for each topic k ?
{1, ...,K} do2: for each language l ?
{e, f} do3: choose ?lk ?
Dirichlet(?l)4: end for5: end for6: for each document pair di(i ?
{1, ..., D}) do7: choose ?i ?
Dirichlet(?
)8: if yi are not given then9: choose ?i?
Beta(?0, ?1)10: choose yi ?
Bernoulli(?i)11: end if12: generate aligned segment sets ASi = genAS(yi)13: for each set ASig (g ?
{1, ..., |ASi|}) do14: choose ?ig ?
PYP(a, b,?i)15: end for16: for each language l ?
{e, f} do17: for each segment slij(j ?
{1, ..., Sli}) do18: get index of slijin ASi: g =get idx(ASi,slij)19: for each word wlijm(m ?
{1, ..., Nlij}) do20: choose zlijm?
Multinomial(?ig)21: choose wlijm?
p(wlijm|zlijm,?l)22: end for23: end for24: end for25: end fordistribution with the prior ?i (Step 10).
Later, aword wlimis drawn from a probability distributionp(wlim|zlim,?l) given the topic zlim(Step 11).3 Bilingual Segmented Topic ModelHere, we describe BiSTM, which extends BiLDAto capture segment-level alignments.
Algorithm2 and Figure 3 show the generative process andgraphical model, respectively, of BiSTM.
As canbe seen in Figure 3, BiSTM introduces a segment-level layer between the document- and word-levellayers in both languages.
In other words, per-segment topic distributions for each language, ?eand ?f, are introduced between per-documenttopic distributions ?
and topic assignments forwords, zeand zf.
In addition, BiSTM incorpo-rates binary variables y to represent segment-levelalignments.Each document dliin a pair of aligned doc-uments diis divided into Slisegments: dli=?Slij=1slij.
BiSTM makes the same assumption forper-topic word distributions as BiLDA, i.e.,?lk arelanguage-specific and drawn from Dirichlet distri-butions (Steps 1-5).In the generative process for a document pairdi, the per-document topic distribution ?i is firstdrawn in the same way as in BiLDA (Step 7).Thus, in BiSTM, each document pair shares thesame topic distribution.Then, if segment-level alignments are not given,yi are generated (Steps 8-11).
We assume thateach document pair dihas a probability ?ithatindicates comparability between segments acrosslanguages.
?iis drawn from a Beta distributionwith the priors ?0and ?1(Step 9).
Then, each ofyi is drawn from a Bernoulli distribution with theprior ?i(Step 10).
Here, yijj?= 1 if and only ifseijand sfij?are aligned; otherwise, yijj?= 0.
Notethat if segment-level alignments are observed, thenSteps 8-11 are skipped.
Later, a set of alignedsegment sets ASi is generated based on yi (Step12).
For example, given dei= {sei1, sei2}, dfi={sfi1, sfi2, sfi3}, yi11and yi12are 1, and the other y?sare 0, ASi = {ASi1 = {sei1, sfi1, sfi2},ASi2 ={sei2},ASi3 = {sfi3}} is generated in Step 12.Then, for each aligned segment set ASig (g ?
{1, ..., |ASi|}), the per-segment topic distribution?ig is obtained from a Pitman?Yor process withthe base measure ?i, the concentration parame-ter a, and the discount parameter b (Step 14).Through Steps 12-15, aligned segments indicatedby y share the same per-segment topic distribu-tion.
For instance, sei1, sfi1, and sfi2have the sametopic distribution ?i1 ?
PYP(a, b, ?i) in the aboveexample.Then, for each word at m ?
{1, ..., Nlij} insegment slijin document dliin language l, a la-tent topic assignment zlijmis drawn from a multi-nomial distribution with the prior ?ig (Step 20),where g denotes the index of the element set ofASi that includes the segment slij, e.g., g for sfi2is 1.
Subsequently, a word wlijmis drawn based onthe assigned topic zlijmand the language-specificper-topic word distribution ?lin the same manneras in BiLDA (Step 21).1268tigkTable count of topic k in the CRP for ali-gned segment set g in document pair i.tig K-dimensional vector, where k-th valueis tigk.tig?Total table count in aligned segment setg in document pair i, i.e.,?ktigk.nigkTotal number of words with topic k in al-igned segment set g in document pair i.nig?Total number of words in aligned segme-nt set g in document pair i, i.e.,?knigk.MlkwTotal number of word w with topic k inlanguage l.Mlk |Wl|-dimensional vector, where w-thvalue is Mlkw.Table 1: Statistics used in our Inference3.1 Inference for BiSTMIn inference, we find the set of latent variables?, ?, z, and ?
that maximizes their posteriorprobability given the model parameters ?, ?
andobservations w, y, i.e., p(?,?, z,?|?,?,w,y).Here, a language-dependent variable without a su-perscript denotes both of the variable in languagee and that in f , e.g., z = {ze, zf}.
Unfortu-nately, as in other probabilistic topic models, suchas LDA and BiLDA, we cannot compute this pos-terior using an exact inference method.
This sec-tion presents an approximation method for BiSTMbased on blocked Gibbs sampling, inspired by Duet al (2013).In our inference, the hierarchy in BiSTM, i.e.,the generation of ?
and z, is explained by theChinese restaurant process (CRP), through whichthe parameters ?, ?, and ?
are integrated out,and the statistics on table counts in the CRP, t,are introduced.
Table 1 lists all statistics used inour inference, where Wldenotes a vocabularyset in language l. Moreover, to accelerate con-vergence, we introduce an auxiliary binary vari-able ?lijmfor wlijm, indicating whether wlijmisthe first customer on a table (?lijm= 1) or not(?lijm= 0), and tigkis computed based on ?in the same manner as in Chen et al (2011):tigk=?slij?ASigNlij?m=1?lijmI(zlijm= k), where I(x)is a function that returns 1 if the condition x is trueand 0 otherwise.Our inference groups zlijmand ?lijm(each groupis called a ?block?)
and jointly samples them.Moreover, if y is not observed, our inference al-ternates two different kinds of blocks, (zlijm, ?lijm)and yijj?.
In each sampling, individual variablesare resampled, conditioned on all other variables.In the following, we describe each sampling stage.Sampling (z, ?
):The joint posterior distribution of z, w, and ?
isinduced in a manner similar to that in Du et al(2010; 2013): p(z,w, ?|?,?, a, b,y)=D?i=1(BetaK(?+?ASi tig)BetaK(?)?ASi((b|a)tig?
(b)nig?K?k=1S(nigk, tigk, a)(nigktigk)?1))K?k=1(BetaWe(?e+Mek )BetaWe(?e)BetaWf(?f+Mfk )BetaWf(?f)),where BetaK(?)
and BetaWl(?)
are K- and |Wl|-dimensional beta functions, respectively, (b|a)nisthe Pochhammer symbol3, and (b)nis given by(b|1)n. S(n,m, a) is a generalized Stirling num-ber of the second kind (Hsu and Shiue, 1998),which is given by the linear recursion S(n +1,m, a) = S(n,m?
1, a)+ (n?ma)S(n,m, a).To reduce computational cost, the Stirling num-bers are preliminarily calculated in a logarithmformat (Buntine and Hutter, 2012).
Then, thecached values are used in our sampling.The joint conditional distributions ofzlijmand ?lijmare obtained from theabove joint distribution using Bayes?
rule:p(zlijm= k, ?lijm= 1|z?zlijm,w, ??
?lijm,?,?, a, b,y)=?lwlijm+ Mlkwlijm?w?W l(?lw+ Mlkw)?k+?ASi tigk?Kk=1(?k+?ASi tigk)b + atig?
?b + nig?
?S(nig?k+ 1, tig?k+ 1, a)S(nig?k, tig?k, a)tig?k+ 1nig?k+ 1,p(zlijm= k, ?lijm= 0|z?zlijm,w, ??
?lijm,?,?, a, b,y)=?lwlijm+ Mlkwlijm?w?W l(?lw+ Mlkw)1b + nig?
?S(nig?k+ 1, tig?k, a)S(nig?k, tig?k, a)nig?k+ 1 ?
tig?knig?k+ 1,where slijis included in ASig?
.Sampling y:In our inference, each aligned segment set cor-responds to a restaurant in the CRP.
We regardthe sampling of yijj?as the choice of splitting ormerging restaurant(s) in a manner similar to that3(b|a)n=?n?1t=0(b + ta).1269in the sampling of segmentation boundaries in Duet al (2013).
In particular, if yijj?= 0, thenone aligned segment set ASm is split into twoaligned segment sets ASl and ASr, where ASl,ASr, and ASm include seij, sfij?, and both, re-spectively.
If yijj?= 1, then ASl and ASr aremerged to ASm.
For simplicity, our inferencespecifies ASl and ASr based on the current y asfollows: if ASi(seij) = ASi(sfij?
), then ASl ={seij} ?
ASfi(seij) \ {sfij?}
and ASr = {sfij?}
?ASei(sfij?
)\{seij}; otherwise,ASl = ASi(seij) andASr = ASi(sfij?).
Here,ASi(j) is the element setof ASi that includes the segment j, and ASli(j)is the set of segments in language l included inASi(j).
For example, in the example in Section 3,ASi(sfi1) = ASi1 = {sei1, sfi1, sfi2}, ASei(sfi1) ={sei1}, and ASfi(sfi1) = {sfi1, sfi2}.
In addition, ifyi11= 0, then ASm = {sei1, sfi1, sfi2} is split intoASl = {sei1} ?
ASfi(sei1) \ {sfi1} = {sei1, sfi2}and ASr = {sfi1} ?
ASei(sfi1) \ {sei1} = {sfi1}.If yi23= 1, then ASl = ASi(sei2) = {sei2}and ASr = ASi(sfi3) = {sfi3} are merged toASm = {sei2, sfi3}.The conditional distributions of yijj?are asfollows:p(yijj?= 0|y?yijj?, z,w, ?,?, a, b, ?0, ?1)?
?0+ ci0?0+ ?1+ ci0+ ci1BetaK(?+?ASitig)?g?{ASl,ASr}(b|a)tig?
(b)nig?K?k=1S(nigk, tigk, a),p(yijj?= 1|y?yijj?, z,w, t \ T,?, a, b, ?0, ?1)?
?T(?1+ ci1?0+ ?1+ ci0+ ci1BetaK(?+?ASitig)(b|a)ti,ASm,?
(b)ni,ASm,?K?k=1S(ni,ASm,k, ti,ASm,k, a)),where T is the set of tigksuch that for either orboth of ASl and ASr, tigk = 1. ci0 and ci1 arethe total number of yi?s whose values are 0 andthat of yi?s whose values are 1, respectively.
Notethat we change yi?s that relate to the selectedaction (merging or splitting), in addition to yijj?tomaintain consistency between y and the alignedsegment sets.Inference of ?, ?, ?
:Although our inference does not directly estimate?, ?, and ?, these variables can be inferredfrom the following posterior expected values viaAlgorithm 3 Generative Process for Segments1: for each document dli(i ?
{1, ..., D}) do2: choose pili?
Beta(?0,?1)3: for each passage ulih(h ?
{1, ..., Uli}) do4: choose ?lih?
Bernoulli(pili)5: end for6: sli = concatenate(uli, ?li)7: end forsampling:?
?ik= Ezi,ti|wi,?,?,a,b,y[?k+?ASi tigk?Kk=1(?k+?ASi tigk)],?
?igk= Ezi,ti|wi,?,?,a,b,y[nigk?
atigkb + nig?+ ?ikatig?+ bb + nig?],?
?lkw= Ez,t|w,?,?,a,b,y[?lw+ Mlkw?w?
?W l(?lw?+ Mlkw?
)].4 Integration of Topic Segmentation intoBiSTM (BiSTM+TS)To infer segmentation boundaries simultaneouslywith cross-lingual topics, we integrate the unsu-pervised Bayesian topic segmentation method pro-posed by Du et al (2013) into the proposedBiSTM (BiSTM+TS).We assume that each segment is a sequence oftopically-related passages.
In particular, we con-sider a sentence as a passage.
Our segmenta-tion model defines a segment in document dlibya boundary indicator variable ?lihfor each pas-sage ulih(h ?
{1, ..., Uli}); ?lihis 1 if there is aboundary after passage ulih(otherwise 0).
For ex-ample, ?li = (0, 1, 0, 0, 1) indicates that the doc-ument dlicomprises the two segments {uli1, uli2}and {uli3, uli4, uli5}.Algorithm 3 shows the generative process forsegments.
The generative process of BiSTM+TSinserts Algorithm 3 between Steps 7 and 8 of Al-gorithm 2.
Note that two documents (dei, dfi) ?diare segmented independently.
BiSTM+TS as-sumes that each document dlihas its own topicshift probability pili.
For each document dli, piliis first drawn from a Beta distribution with thepriors ?0and ?1(Step 2).
Then, for each pas-sage ulih(h ?
{1, ..., Uli}), ?lihis drawn from aBernoulli distribution with the prior pili(Step 4).Finally, segments sli are generated by concatenat-ing passages based on ?li (Step 6).12704.1 Inference for BiSTM+TSOur inference for BiSTM+TS alternates three dif-ferent kinds of blocks, sampling of ?
and sam-plings for BiSTM ((z, ?)
and y).
The conditionaldistribution of ?
comprises the Gibbs probabilityfor splitting one segment sminto two segments srand slby placing the boundary after ulih(?lih= 1)and that for merging srand slto smby removingthe boundary after ulih(?lih= 0).These probabilities are estimated in the samemanner as the conditional probabilities of yijj?,where y (yijj?= 0, 1),ASl,ASr,ASm, ?0, and?1are replaced with ?
(?lih= 1, 0), sl, sr, sm, ?1,and ?0, respectively, and the statistics t and n aresummed for every segment rather than for everyaligned segment set (see Equation (6) and (9) inDu et al (2013)).Our inference assumes that sampling ?
doesnot depend on aligned segments in the other lan-guage, i.e., y4.
After splitting or merging, weset the y?s of sm, sl, and sras follows: if smissplit into sland sr, then AS(sl) = AS(sm) andAS(sr) = AS(sm); if sland srare merged to sm,then AS(sm) = AS(sl) ?AS(sr).5 ExperimentWe evaluated the proposed models in terms ofperplexity and performance in translation pairextraction, which is a well-known applicationthat uses a bilingual topic model.
We used adocument-aligned comparable corpus comprising3,995 document pairs, each of which is a JapaneseWikipedia article in the Kyoto Wiki Corpus5andits corresponding EnglishWikipedia article6.
Notethat the English articles were collected from theEnglish Wikipedia database dump (2 June 2015)7based on inter-language links, even though theoriginal Kyoto Wiki corpus is a parallel corpus,in which each sentence in the Japanese articles ismanually translated into English.
Thus, our ex-perimental data is not a parallel corpus.
We ex-tracted texts from the collected English articlesusing an open-source script8.
All Japanese and4We leave a bilingual extension of the topic segmentation,i.e., incorporation of y, for future work.5http://alaginrc.nict.go.jp/WikiCorpus/index_E.html6We filtered out the Japanese articles that do not have cor-responding English articles.7http://dumps.wikimedia.org/enwiki/8https://github.com/attardi/wikiextractor/English texts were segmented using MeCab9andTreeTagger10(Schmid, 1994), respectively.
Then,function words were removed, and the remainingwords were lemmatized to reduce data sparsity.For translation extraction experiments, we au-tomatically created a gold-standard translation setaccording to Liu et al (2013).
We first com-puted p(we|wf) and p(wf|we) by running IBMModel 4 on the original Kyoto Wiki corpus,which is a parallel corpus, using GIZA++ (Ochand Ney, 2003), and then extracted word pairs(?we,?wf) that satisfy both of the following con-ditions:?we= argmaxwep(we|wf=?wf) and?wf= argmaxwfp(wf|we=?we).
Finally, weeliminated word pairs that do not appear in thedocument pairs in the document-aligned compa-rable corpus.
We used all 7,930 Japanese wordsin the resulting gold-standard set as the evaluationinput.5.1 Competing MethodsWe compared the proposed models (BiSTMand BiSTM+TS) with a standard bilingual topicmodel (BiLDA).
BiSTM considers each section inWikipedia articles as a segment.
Note that align-ments between sections are not given in our exper-imental data.
Thus, y is inferred in both BiSTMand BiSTM+TS.As in the proposed models, BiLDA was trainedusing Gibbs sampling (Mimno et al, 2009; Niet al, 2009; Vuli?c et al, 2015).
In the trainingof each model, each variable was first initialized.Here, zlijmis randomly initialized to an integer be-tween 1 and K, and each of ?lijm, yijj?, and ?lihisrandomly initialized to 0 or 1.
We then performed10,000 Gibbs iterations.
We used the symmetricprior ?k= 50/K and ?lw= 0.01 over ?
and?l, respectively, in accordance with Vuli?c et al(2011).
The hyperparameters a, b, ?0, and ?1wereset to 0.2, 10, 0.1, and 0.1, respectively, in accor-dance with Du et al (2010; 2013).
Both ?0and?1were set to 0.2 as a result of preliminary exper-iments.
We used several values of K to measurethe impact of topic size: we used K = 100 andK = 400 in accordance with Liu et al (2013)in addition to the suggested value K = 2, 000 inVuli?c et al (2011).In the translation extraction experiments,9http://taku910.github.io/mecab/10http://www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/1271Model K=100 K=400 K=2,000BiLDA 693.6 530.7 479.9BiSTM 520.1 429.3 394.6BiSTM+TS 537.5 445.3 411.8Table 2: Test Set Perplexitywe used two translation extraction methods,i.e., Cue (Vuli?c et al, 2011) and Liu (Liu etal., 2013).
Both methods first infer cross-lingual topics for words using a bilingualtopic model (BiLDA/BiSTM/BiSTM+TS)and then extract word pairs (we, wf) with ahigh value of the probability p(we|wf) de-fined by the inferred topics.
Cue calculatesp(we|wf) =?Kk=1p(we|k)p(k|wf), wherep(k|w) ?p(w|k)?Kk=1p(w|k)and p(w|k) = ?kw.Liu first converts a document-aligned com-parable corpus into a topic-aligned parallelcorpus according to the topics of words andcomputes p(we|wf, k) by running IBM Model1 on the parallel corpus.
Liu then calcu-lates p(we|wf) =?Kk=1p(we|wf, k)p(k|wf).Hereafter, a bilingual topic model used in anextraction method is shown in parentheses, e.g.,Cue(BiLDA) denotes Cue with BiLDA.5.2 Experimental ResultsWe evaluated the predictive performance of eachmodel by computing the test set perplexity basedon 5-fold cross validation.
A lower perplexity in-dicates better generalization performance.
Table2 shows the perplexity of each model.
As canbe seen, BiSTM and BiSTM+TS are better thanBiLDA in terms of perplexity.We measured the performance of translation ex-traction with top N accuracy (ACCN), the numberof test words whose top N translation candidatescontain a correct translation over the total num-ber of test words (7,930).
Table 3 summarizesACC1and ACC10for each model.
As can beseen, Cue/Liu(BiSTM) and Cue/Liu(BiSTM+TS)significantly outperform Cue/Liu(BiLDA) (p <0.01 in the sign test).
This indicates that BiSTMand BiSTM+TS improve the performance of trans-lation extraction for both the Cue and Liu methodsby assigning more suitable topics.Both experiments prove that capturing segment-level alignments is effective for modeling bilin-gual data.
In addition, these experiments show thatBiSTM+TS is comparable with BiSTM, indicat-ACC1Method K=100 K=400 K=2,000Cue(BiLDA) 0.024 0.056 0.101Cue(BiSTM) 0.055 0.112 0.184Cue(BiSTM+TS) 0.052 0.107 0.176Liu(BiLDA) 0.206 0.345 0.426Liu(BiSTM) 0.287 0.414 0.479Liu(BiSTM+TS) 0.283 0.406 0.467ACC10Method K=100 K=400 K=2,000Cue(BiLDA) 0.093 0.170 0.281Cue(BiSTM) 0.218 0.286 0.410Cue(BiSTM+TS) 0.196 0.274 0.398Liu(BiLDA) 0.463 0.550 0.603Liu(BiSTM) 0.531 0.625 0.671Liu(BiSTM+TS) 0.536 0.612 0.667Table 3: Performance of Translation ExtractionReference y = 1 Reference y = 0Inference y = 1 195 174Inference y = 0 43 1132Table 4: Distribution of Segment-level Align-mentsing that the proposed model could yield a signifi-cant benefit even if the boundaries of segments areunknown.Tables 2 and 3 show that a larger topic sizeyields better performance for each model.
Fur-thermore, Liu outperforms Cue regardless of thechoice of bilingual topic models, which is con-sistent with previously reported results (Liu et al,2013).
The results of our experiments demonstratethat the proposed models have the same tendenciesas BiLDA.6 Discussion6.1 Inferred Segment-level AlignmentsWe created a reference set to evaluate segment-level alignments y inferred by BiSTM (K=2,000).We randomly selected 100 document pairs fromthe comparable corpus and then manually iden-tified cross-lingual alignments between sections.Table 4 shows the distribution of inferred y valuesand that of y values in the reference set.
As can beseen, the accuracy of y is 0.859 (1,327/1,544).The majority of false negatives (121/174) aresections that are not parallel but correspond par-tially.
An example is the alignment between the1272Model Japanese article English articleBiSTM 4.8 2.9BiSTM+TS 10.6 4.1Table 5: Average Number of SegmentsJapanese section ?history?
and the English sec-tion ?Bujutsu (old type of Budo)?
in the ?Budo (aJapanese martial art)?
article pair, where a part ofthe English section ?Bujutsu?
is described in theJapanese section ?history.?
Such errors might notnecessarily have a negative effect, because partialalignments can be useful.6.2 Inferred Segmentation BoundariesThis section compares segment boundaries in-ferred by BiSTM+TS (K=2,000) with sectionboundaries in the original articles, which havebeen referred to by BiSTM.
The recall ofBiSTM+TS for the original section boundariesis 0.727.
This indicates that the unsupervisedsegmentation in BiSTM+TS finds drastic topicalchanges, i.e., section boundaries, with high recall.Table 5 shows the average number of seg-ments per article for each model.
As can beseen, BiSTM+TS divides an article into segmentssmaller than the original sections.
This seems tobe reasonable, because some original sections in-clude multiple topics.
However, Tables 2 and 3show that inferred boundaries do not work betterthan section boundaries.
One reason for that isthat some errors are caused by a sparseness prob-lem, when BiSTM+TS separates an article into ex-tremely fine-grained segments.
In addition, Table5 reveals that BiSTM+TS increases the gap be-tween languages.
Thus, segmentation with a com-parable granularity between languages might befavorable for the proposed models.6.3 Effectiveness for an English?FrenchWikipedia CorpusWe evaluated BiLDA, BiSTM, and BiSTM+TS interms of perplexity and performance in translationextraction on an English?FrenchWikipedia corpusto verify the effectiveness of the proposed modelsfor language pairs other than English?Japanese.The settings, e.g., parameters, for each model arethe same as in Section 5.
Note that we report onlythe performances of each model with K = 2, 000,because all models achieved the best performanceswhen K = 2, 000.Model Test Set PerplexityBiLDA 439.1BiSTM 379.4BiSTM+TS 396.6Model ACC1ACC10Cue(BiLDA) 0.219 0.556Cue(BiSTM) 0.275 0.580Cue(BiSTM+TS) 0.257 0.582Liu(BiLDA) 0.715 0.838Liu(BiSTM) 0.742 0.859Liu(BiSTM+TS) 0.732 0.852Table 6: Performance on an English?FrenchWikipedia Corpus (K = 2, 000)We collected French articles that correspond tothe English articles used in the experiments inSection 5, from the French Wikipedia databasedump (2 June 2015) based on inter-language links.As a result, our English?French corpus comprises3,159 document pairs.
The French articles werepreprocessed in the samemanner as the English ar-ticles: text extraction using the open-source script,segmentation using TreeTagger, removal of func-tion words, and lemmatization.We created a gold-standard translation set fortranslation extraction experiments using GoogleTranslate service11in a manner similar to that inGouws et al (2015) and Coulmance et al (2015),translating the French words in our corpus us-ing Google Translate, and then eliminating wordpairs that do not appear in the document pairs inour corpus.
We used the top 1,000 most frequentFrench words in the resulting gold-standard set asthe evaluation input.Table 6 summarizes ACC1, ACC10, and per-plexity.
It shows that the proposed models are ef-fective also for the English?French Wikipedia cor-pus.
BiSTM and BiSTM+TS outperform BiLDAin terms of perplexity and performance of transla-tion extraction, and BiSTM+TS works well evenif the boundaries of segments are unknown.7 Related WorkMultilingual topic models other than BiLDA (Sec-tion 2) have been proposed for document-alignedcomparable corpora.
Fukumasu et al (2012) ap-plied SwitchLDA (Newman et al, 2006) and Cor-respondence LDA (Blei and Jordan, 2003), which11http://translate.google.com/1273were originally intended to work with multimodaldata, such as annotated image data, to modelingmultilingual text data.
They also proposed a sym-metric version of Correspondence LDA.
Platt etal.
(2010) projected monolingual models basedon PLSA or Principal Component Analysis into ashared multilingual space with the constraint thatdocument pairs must map to similar locations.
Huet al (2014) proposed a multilingual tree-basedtopic model that uses a hierarchical bilingual dic-tionary in addition to document alignments.
Notethat these models do not consider segment-levelalignments.There are several multilingual topic models tai-lored for data other than a document-aligned com-parable corpus, including bilingual topic mod-els for word alignment and machine translationon parallel sentence pairs (Zhao and Xing, 2006;Zhao and Xing, 2008).
Some models havemined multilingual topics from unaligned textdata by bridging the gap between different lan-guages using a bilingual dictionary (Jagarlamudiand Daum?e III, 2010; Zhang et al, 2010; Negi,2011).
Boyd-Graber and Blei (2009) used parallelsentences in combination with a bilingual dictio-nary.
However, these models have the drawbackthat they require a parallel corpus or a bilingualdictionary in advance, which cannot be obtainedfor some language pairs or domains.In a monolingual setting, some topic modelsthat consider segment-level topics have been pro-posed.
Du et al (2010) considered a document asa set of segments and generated each per-segmenttopic distribution from the topic distribution of therelated document through a Pitman?Yor process.Others have considered a document as a sequenceof segments.
Cheng et al (2009) reflected the un-derlying sequences of segments?
topics by posit-ing a permutation distribution over a document.Wang et al (2011) modeled topical sequences indocuments with a latent first-order Markov chain,and Du et al (2012) generated each per-segmenttopic distribution from the topic distribution of itsdocument and that of its previous segment.
Notethat none of these models have been extended to amultilingual setting.8 ConclusionsIn this paper, we proposed BiSTM, which modelsa document hierarchically and deals with segment-level alignments.
BiSTM assigns the same topicdistribution to both aligned documents and alignedsegments.
We also presented an extended model,BiSTM+TS, that infers segmentation boundariesin addition to latent topics by incorporating unsu-pervised topic segmentation (Du et al, 2013).
Ourexperimental results show that capturing segment-level alignments improves perplexity and transla-tion extraction performance, and that BiSTM+TSyields a significant benefit even if the boundariesof segments are not given.This paper presented an extension to BiLDA,but hierarchical structures can also be incorporatedinto other bilingual topic models (Section 7).
Asfuture work, we would like to verify the effec-tiveness of the proposed models for other datasetsor other cross-lingual tasks, such as cross-lingualdocument classification (Ni et al, 2009; Platt etal., 2010; Ni et al, 2011; Smet et al, 2011) andcross-lingual information retrieval (Vuli?c et al,2013).AcknowledgmentsWe thank Atsushi Fujita for valuable comments onearlier versions of this manuscript.ReferencesDavid M. Blei and Michael I. Jordan.
2003.
Model-ing Annotated Data.
In Proceedings of the 26th An-nual International ACM SIGIR Conference on Re-search and Development in Informaion Retrieval,pages 127?134.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2009.
Mul-tilingual Topic Models for Unaligned Text.
In Pro-ceedings of the Twenty-Fifth Conference on Uncer-tainty in Artificial Intelligence, pages 75?82.Wray Buntine and Marcus Hutter.
2012.
A BayesianView of the Poisson-Dirichlet Process.
http://arxiv.org/pdf/1007.0296.pdf.Harr Chen, S.R.K.
Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Global Models of Docu-ment Structure using Latent Permutations.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 371?379.Changyou Chen, Lan Du, and Wray Buntine.
2011.Sampling Table Configurations for the Hierarchi-cal Poisson-Dirichlet Process.
In Proceedings ofthe European Conference on Machine Learning and1274Principles and Practice of Knowledge Discovery inDatabases 2011, pages 296?311.Jocelyn Coulmance, Jean-Marc Marty, GuillaumeWenzek, and Amine Benhalloum.
2015.
Trans-gram, Fast Cross-lingual Word-embeddings.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1109?1113.Lan Du, Wray Buntine, and Huidong Jin.
2010.A Segmented Topic Model Based on the Two-parameter Poisson-Dirichlet Process.
MachineLearning, 81(1):5?19.Lan Du, Wray Buntine, and Huidong Jin.
2012.Modelling Sequential Text with an Adaptive TopicModel.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 535?545.Lan Du, Wray Buntine, and Mark Johnson.
2013.Topic Segmentation with a Structured Topic Model.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 190?200.Kosuke Fukumasu, Koji Eguchi, and Eric P. Xing.2012.
Symmetric Correspondence Topic Models forMultilingual Text Analysis.
In Advances in Neu-ral Information Processing Systems 25, pages 1286?1294.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
BilBOWA: Fast Bilingual Distributed Rep-resentations without Word Alignments.
In Proceed-ings of the 32nd International Conference on Ma-chine Learning, pages 748?756.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Indexing.
In Proceedings of the 22nd AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages50?57.Leetsch C. Hsu and Peter Jau-Shyong Shiue.
1998.
AUnified Approach to Generalized Stirling Numbers.Advances in Applied Mathematics, 20(3):366?384.Yuening Hu, Ke Zhai, Vladimir Eidelman, and JordanBoyd-Graber.
2014.
Polylingual Tree-Based TopicModels for Translation Domain Adaptation.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1166?1176.Jagadeesh Jagarlamudi and Hal Daum?e III.
2010.
Ex-tracting Multilingual Topics from Unaligned Com-parable Corpora.
In Proceedings of the 32nd Eu-ropean Conference on Advances in Information Re-trieval, pages 444?456.Xiaodong Liu, Kevin Duh, and Yuji Matsumoto.
2013.Topic Models + Word Alignment = A FlexibleFramework for Extracting Bilingual Dictionary fromComparable Corpus.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning, pages 212?221.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual Topic Models.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 880?889.Sumit Negi.
2011.
Mining Bilingual Topic Hierarchiesfrom Unaligned Text.
In Proceedings of 5th Interna-tional Joint Conference on Natural Language Pro-cessing, pages 992?1000.David Newman, Chaitanya Chemudugunta, PadhraicSmyth, and Mark Steyvers.
2006.
Statistical Entity-topic Models.
In Proceedings of the 12th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 680?686.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining Multilingual Topics from Wikipedia.In Proceedings of the 18th International World WideWeb Conference, pages 1155?1156.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2011.
Cross Lingual Text Classification by MiningMultilingual Topics from Wikipedia.
In Proceed-ings of the Fourth ACM International Conference onWeb Search and Data Mining, pages 375?384.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29:19?51.Jim Pitman and Marc Yor.
1997.
The Two-ParameterPoisson-Dirichlet Distribution Derived from a Sta-ble Subordinator.
The Annals of Probability,25(2):855?900.John Platt, Kristina Toutanova, and Wen tau Yih.
2010.Translingual Document Representations from Dis-criminative Projections.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 251?261.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofthe International Conference on New Methods inLanguage Processing, pages 44?49.Wim De Smet, Jie Tang, and Marie-Francine Moens.2011.
Knowledge Transfer Across MultilingualCorpora via Latent Topics.
In Proceedings ofthe 15th Pacific-Asia Conference on Advances inKnowledge Discovery and Data Mining, pages 549?560.Ivan Vuli?c, Wim De Smet, and Marie-Francine Moens.2011.
Identifying Word Translations from Compa-rable Corpora Using Latent Topic Models.
In Pro-ceedings of the 49th Annual Meeting of the Associ-1275ation for Computational Linguistics: Human Lan-guage Technologies, pages 479?484.Ivan Vuli?c, Wim De Smet, and Marie-Francine Moens.2013.
Cross-Language Information Retrieval Mod-els Based on Latent Topic Models Trained withDocument-Aligned Comparable Corpora.
Informa-tion Retrieval, 16(3):331?368.Ivan Vuli?c, Wim De Smet, Jie Tang, and Marie-Francine Moens.
2015.
Probabilistic Topic Mod-eling in Multilingual Settings: An Overview of ItsMethodology and Applications.
Information Pro-cessing & Management, 51(1):111?147.Hongning Wang, Duo Zhang, and ChengXiang Zhai.2011.
Structural Topic Model for Latent TopicalStructure Analysis.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1526?1535.Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai.2010.
Cross-Lingual Latent Topic Extraction.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1128?1137.Bing Zhao and Eric P. Xing.
2006.
BiTAM: BilingualTopic AdMixture Models for Word Alignment.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 969?976.Bing Zhao and Eric P. Xing.
2008.
HM-BiTAM:Bilingual Topic Exploration, Word Alignment, andTranslation.
In Advances in Neural InformationProcessing Systems 20, pages 1689?1696.1276
