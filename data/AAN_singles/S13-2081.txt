Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 490?494, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsREACTION: A naive machine learning approach for sentimentclassificationSilvio MoreiraIST/INESC-IDRua Alves Redol, 91000-029 LisboaPortugalsamir@inesc-id.ptJoa?o FilgueirasINESC-IDRua Alves Redol, 91000-029 LisboaPortugaljfilgueiras@inesc-id.ptBruno MartinsIST/INESC-IDRua Alves Redol, 91000-029 LisboaPortugalbruno.g.martins@ist.utl.ptFrancisco CoutoLASIGE - FCULEdif?
?cio C6 Piso 3Campo Grande1749 - 016 LisboaPortugalfcouto@di.fc.ul.ptMa?rio J. SilvaIST/INESC-IDRua Alves Redol, 91000-029 LisboaPortugalmjs@inesc-id.ptAbstractWe evaluate a naive machine learning ap-proach to sentiment classification focusedon Twitter in the context of the sentimentanalysis task of SemEval-2013.
We employa classifier based on the Random Forests al-gorithm to determine whether a tweet ex-presses overall positive, negative or neu-tral sentiment.
The classifier was trainedonly with the provided dataset and uses asmain features word vectors and lexicon wordcounts.
Our average F-score for all threeclasses on the Twitter evaluation datasetwas 51.55%.
The average F-score of bothpositive and negative classes was 45.01%.For the optional SMS evaluation dataset ouroverall average F-score was 58.82%.
Theaverage between positive and negative F-scores was 50.11%.1 IntroductionSentiment Analysis is a growing research field, es-pecially on web social networks.
In this setting,users share very diverse messages such as real-time reactions to news, events and daily experi-ences.
The ability to tap on a vast repository ofopinions, such as Twitter, where there is great di-versity of topics, has become an important goalfor many different applications.
However, due tothe nature of the text, NLP systems face additionalchallenges in this context.
Shared messages, suchas tweets, are very short and users tend to resort tohighly informal an noisy speech.Following this trend, the 2013 edition of Se-mEval1 included a sentiment analysis on Twittertask (SemEval-2013 Task 2).
Participants wereasked to implement a system capable of determin-ing whether a given tweet expresses positive, neg-ative or neutral sentiment.
To help in the develop-ment of the system, an annotated training corpuswas released.
Systems that used only the givencorpus for training were considered constrained,while others were considered unconstrained.
Thesubmitted prototypes were evaluated in a datasetconsisting of around 3700 tweets of several topics.The metric used was the average F-score betweenthe positive and negative classes.Our goal with this participation was to create abaseline system from which we can build upon andperform experiments to compare new approacheswith the state-of-the-art.2 Related WorkThe last decade saw a growing interest in systemsto automatically process sentiment in text.
Manyapproaches to detect subjectivity and determine1Proceedings of the 7th International Workshop on Se-mantic Evaluation (SemEval 2013), in conjunction with theSecond Joint Conference on Lexical and Computational Se-mantics (*SEM 2013)490polarity of opinions in news articles, weblogs andproduct reviews have been proposed (Pang et al2002; Pang et al 2004; Wiebe et al 2005; Wil-son et al 2005).
This sub-field of NLP, known asSentiment Analysis is presented in great depth in(Liu, 2012).The emergence and proliferation of microblogplatforms created a medium where people expressand convey all kinds of information.
In particu-lar, these platforms are a rich source of subjec-tive and opinionated text, which has motivatedthe application of similar techniques to this do-main.
However, in this context, messages tendto be very short and highly informal, full of ty-pos, slang and unconventional spelling, posing ad-ditional challenges to NLP systems.
In fact, earlyexperiments in Sentiment Analysis in the contextof Twitter (Barbosa et al 2010; Davidov et al2010; Koulompis et al 2011; Pak et al 2010;Bifet et al 2010) show that the techniques thatproved effective in other domains are not sufficientin the microblog setting.
In the spirit of these ap-proaches, we included a preprocessing step, fol-lowed by feature extraction focusing on word,lexical and Twitter-specific features.
Finally, weuse annotated data to train an automatic classifierbased on the Random Forests (Breiman, 2001) andBESTrees (Sun et al 2011) learning algorithms.3 ResourcesTwo annotated datasets were made available toparticipants of SemEval-2013 Task 2: one fortraining purposes which was to contain 8000 to12000 tweets; and another, for development, con-taining 2000.
The combined datasets ended upamounting to a little over 7500 tweets.
The distri-bution of positives, negatives and neutrals for thecombined datasets can be found in Table 1.
Nearlyhalf of all tweets belonged to the neutral class, andnegatives represent just 15% of these datasets.Class NumberPositive 37%Negative 15%Neutral 48%Table 1: Class distribution of annotated data.Random examples of each class drawn from thedatasets are shown in Table 2.Positive:1 Louis inspired outfit on Monday and Zayninspired outfit today..4/5 done just need Harry2 waking up to a Niners win, makes Tuesdayget off to a great start!
21-3 over the cardsand 2 games clear in the NFC West.Negative:3 Sitting at home on a Saturday night doingabsolutely nothing...
Guess I?ll just watchGreys Anatomy all night.
#lonerproblems#greysanatomy4 Life just isn?t the same when there is noPretty Little Liars on Tuesday nights.Neutral:5 Won the match #getin .
Plus,tomorrow is a very busy day, withAwareness Day?s and debates.
Gulp.
Debates6 @ Nenaah oh cause my friend got somethingfrom china and they said it will take at least 6to 8 weeks and it came in the 2nd week :PTable 2: Random examples of annotated tweets.4 ApproachGiven our goal of creating a baseline system, weexperimented with a common set of features usedin sentiment analysis.
The messages were mod-elled as a combination of binary (or presence) uni-grams, lexical features and Twitter-specific fea-tures.
We decided to follow a supervised approachby learning a Random Forests classifier from theannotated data provided by the organisers of theworkshop (see Section 3).
In summary, the devel-opment of our system consisted of four steps: 1)preprocessing of the data, 2) feature extraction, 3)learning the classifier, and 4) applying the classi-fier to the test set.4.1 PreprocessingThe lexical variation introduced by typos, ab-breviations, slang and unconventional spelling,leads to very large vocabularies.
The resulting491sparse vector representations with few non-zerovalues hamper the learning process.
In order totackle this problem, we replaced user mentions(@<username>) with a fixed tag <USER> andURLs with the tag <URL>.
Then, each sentencewas normalised by converting to lower-case andreducing character repetitions to at most 3 charac-ters (e.g.
?heelloooooo!?
would be normalised to?heellooo!?).
Finally, we performed the lemma-tisation of the sentence using the Morphadorner2software.4.2 Feature ExtractionAfter the preprocessing step, we extract a vectorconsisting of the top uni-grams present in the train-ing set and represent individual messages in termsof this vector.
For each message we also computethe frequency of smileys and words with prior sen-timent polarity using a sentiment lexicon.
Finally,we include the harmonic mean of positive and neg-ative words.
Next we explain each feature in moredetail.Word vector: a sparse word vector containingthe top 25.000 most frequent words that occur inthe training set.
This feature aims at capturing re-lations between certain words and overall messagepolarity.
The vector was extracted using the Wekatoolkit (Hall et al 2009) with the stop word listoption.Lexicon word count: positive and negative sen-timent word counts.
When the word is preceded bya negation particle we invert the polarity.
We usedBing Liu?s Opinion Lexicon3 that includes 2006positive and 4783 negative words and is especiallytailored for social media because it considers mis-spellings, slang and other domain specific varia-tions.Smileys count: a count of positive and negativesmileys that appear in the tweet.
We take advan-tage of these constructs being especially indicativeof the overall expressed sentiment in a text (Davi-dov et al 2010).
Although there are smiley lexi-cons, such as the one used on SentiStrength4, weused regular expressions to capture most common2http://morphadorner.northwestern.edu/3http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html4http://sentistrength.wlv.ac.uksmileys in a flexible way.Hashtag count: a count of positive and negativehashtags.
This feature also uses Bing Liu?s lexiconto determine wether a word contained in an hash-tag is positive or negative.
The rationale behindthis feature is that positive or negative words in theform of hashtags can have a stronger meaning thanregular words (Davidov et al 2010).Positive/negative harmonic mean: harmonicmean between positive and negative token counts,including words and hashtags.In an attempt to further reduce the dimensional-ity of the feature space we computed the principalcomponents of the word vector using the PrincipalComponents Analysis filter in Weka but observedthat this yielded worse results.4.3 Learning the classifierTo implement our classifier we used the Weka ma-chine learning framework and experimented withtwo ensemble algorithms: Random Forests andBESTrees.
We eventually dropped the use of BE-STrees as initial results were worse.We attempted to use most of the data while be-ing able to effectively measure the performance ofthe classifier.
Therefore we used the totality ofboth sets for training and evaluated using 10 foldcross-validation.Since we used only the annotated dataset thatwas provided for this task, our approach is consid-ered constrained.5 ResultsOur results with 10 fold cross-validation using thesubmitted classifier, are presented in Table 3.Class Precision Recall F-scorepositive 61.0% 63.9% 62.4%negative 54.1% 26.8% 35.8%neutral 64.7% 72.4% 68.3%average F-score (pos/neg) 49.1%Table 3: Cross-validation results using the training set.Task evaluation results are presented in Table 4for tweets.
Our approach ranked 44th out of 48participants.
The evaluation dataset had a sim-ilar class distribution to the annotated datasets,492with almost half being neutral, and just 14% neg-ative.
Preliminary results with cross-validationwere similar to those of the final evaluation forTwitter.Class Precision Recall F-scorepositive 62.52% 55.28% 58.68%negative 55.74% 21.80% 31.34%neutral 56.54% 75.43% 64.63%average F-score (pos/neg) 45.01%Table 4: Task evaluation results for Tweets.Also included in SemEval-2013 Task 2 was anevaluation using a SMS dataset to understand if aclassifier trained using tweets could be applied toSMS messages.
SMS results are shown in Table 5.In this case our approach ranked 23th out of 42 par-ticipants.
The SMS evaluation dataset was com-posed of more than half neutral messages (58%),and similarly distributed positives (23%) and neg-atives (19%).Class Precision Recall F-scorepositive 53.66% 59.50% 56.45%negative 60.54% 34.26% 43.76%neutral 72.91% 79.90% 76.27%average F-score (pos/neg) 50.11%Table 5: Task evaluation results for SMS.6 Discussion and ConclusionsAs expected, our naive approach performs poorlyin the context of Twitter messages.
The obtainedresults are in line with similar approaches de-scribed in the literature and we found that Ran-dom Forests achieve the same performance asother learning algorithms tried for the same task(Koulompis et al 2011).The uneven distribution of classes in the datamay have also contributed to the low performanceof the classifier.
Although the neutral class wasnot considered in the evaluation, the datasets hada great predominance of neutral messages whereasthe negative examples only accounted for 15% ofthe corpus.
This suggests that it could be useful touse a minority class over-sampling method, suchas SMOTE (Chawla, 2002), to reduce the effectof this imbalance on the data.
We used n-gramsto model the words that compose each message.However, this approach leads to very sparse rep-resentations, thus becoming important to considertechniques that reduce feature space.
We experi-mented with PCA, without success, but we still be-lieve that applying feature selection algorithms ordenser word representations (Turian et al 2010)could improve performance in this task.We find that our classifier performs better on theSMS dataset.
This might be explained by the factthat SMS messages tend to be more direct, whereasthe same tweet can express, or show signs of, con-tradictory sentiments.
In fact, our naive approachoutperforms other systems that had better resultsin the Twitter dataset, but it is difficult to say why,given that we do not have access to the SMS testset annotations.Despite the poor ranking results, we achievedour goal of performing basic experiments in thetask of sentiment analysis in Twitter and developeda baseline system that will serve as a starting pointfor future research.AcknowledgmentsThis work was partially supported by FCT(Portuguese research funding agency) underproject grants UTA-Est/MAI/0006/2009 (RE-ACTION) and PTDC/CPJ-CPO/116888/2010(POPSTAR).
FCT also supported scholarshipSFRH/BD/89020/2012.
This research was alsofunded by the PIDDAC Program funds (INESC-ID multi annual funding) and the LASIGE multiannual support.ReferencesBarbosa, L., and Feng, J.
2010.
Robust sentiment de-tection on twitter from biased and noisy data.
Pro-ceedings of the 23rd International Conference onComputational Linguistics: Posters, pp.
36-44.Bifet, A., and Frank, E. 2010.
Sentiment knowledgediscovery in twitter streaming data.
Discovery Sci-ence.Breiman, L. 2001.
Random forests.
Machine learning,45(1), 5-32.Chawla, N. V., Bowyer, K. W., Hall, L. O., andKegelmeyer, W. P. 2002 SMOTE: synthetic minority493over-sampling technique.
Journal of Artificial Intel-ligence Research, 16, 321-357.Davidov, D., Tsur, O., and Rappoport, A.
2010 En-hanced sentiment learning using twitter hashtagsand smileys.
Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters.Pages 241-249.
Association for Computational Lin-guistics.Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reute-mann, P., and Witten, I. H. 2009.
The WEKAData Mining Software: An Update SIGKDD Ex-plorations, Volume 11, Issue 1.Kouloumpis, E., Wilson, T., and Moore, J.
2011.
Twit-ter sentiment analysis: The good the bad and theomg.
Proceedings of the Fifth International AAAIConference on Weblogs and Social Media, 538541.Liu, B.
2012.
Sentiment Analysis and Opinion Mining.Synthesis Lectures on Human Language Technolo-gies, 5(1), 1167.Pak, A., and Paroubek, P. 2010.
Twitter as a corpus forsentiment analysis and opinion mining.
Proceedingsof LREC.Pang, B., Lee, L., and Vaithyanathan, S. 2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
Proceedings of the ACL-02 conferenceon Empirical methods in natural language process-ing.
Volume 10, pp.
79-86.
Association for Compu-tational Linguistics.Pang, B. and Lee, L. 2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
Proceedings of the 42nd an-nual meeting on Association for Computational Lin-guistics.Sun, Q. and Pfahringer, B.
2011.
Bagging EnsembleSelection.
Proceedings of the 24th Australasian JointConference on Artificial Intelligence (AI?11), Perth,Australia, pages 251-260.
Springer.Turian, J., Ratinov, L., and Bengio, Y.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (pp.
384-394).
Association for Computa-tional Linguistics.Wiebe, J. and Riloff, E. 2005.
Creating subjective andobjective sentence classifiers from unannotated texts.Computational Linguistics and Intelligent Text Pro-cessing, pages 486-497, Springer.Wilson, T., Wiebe, J., and Hoffmann, P. 2005.
Rec-ognizing contextual polarity in phrase-level senti-ment analysis.
Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pp.
347-354.
Asso-ciation for Computational Linguistics.494
