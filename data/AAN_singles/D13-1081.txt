Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 839?844,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsWhat is Hidden among Translation RulesLibin ShenPersado50 West 17th StreetNew York, NY 10011libin.shen@persado.comBowen ZhouIBM T. J. Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598zhou@us.ibm.comAbstractMost of the machine translation systems relyon a large set of translation rules.
These rulesare treated as discrete and independent events.In this short paper, we propose a novel methodto model rules as observed generation outputof a compact hidden model, which leads tobetter generalization capability.
We present apreliminary generative model to test this idea.Experimental results show about one point im-provement on TER-BLEU over a strong base-line in Chinese-to-English translation.1 IntroductionMost of the modern Statistical Machine Transla-tion (SMT) systems, for example (Koehn et al2003; Och and Ney, 2004; Chiang, 2005; Marcu etal., 2006; Shen et al 2008), employ a large ruleset that may contain tens of millions of translationrules or even more.
In these systems, each transla-tion rule has about 20 dense features, which repre-sent key statistics collected from the training data,such as word translation probability, phrase transla-tion probability etc.
Except for these common fea-tures, there is no connection among the translationrules.
The translation rules are treated as indepen-dent events.The use of sparse features as in (Arun and Koehn,2007; Watanabe et al 2007; Chiang et al 2009) tosome extent mitigated this problem.
In their work,there are as many as 10,000 features defined on theappearance of certain frequent words and Part ofSpeech (POS) tags in rules.
They provide signifi-cant improvement in automatic evaluation metrics.However, these sparse features fire quite randomlyand infrequently on each rule.
Thus, there is stillplenty of space to better model translation rules.In this paper, we will explore the relationshipamong translation rules.
We no longer view rulesas discrete or unrelated events.
Instead, we viewrules, which are observed from training data, as ran-dom variables generated by a hidden model.
Thisgenerative process itself is also hidden.
All possiblegenerative processes can be represented with factor-ized structures such as weighted hypergraphs and fi-nite state machines.
This approach leads to a com-pact model that has better generalization capabilityand allows translation rules not explicitly observedin training date.This paper reports work-in-progress to exploithidden relations among rules.
Preliminary experi-ments show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-Englishtranslation.2 Hidden ModelsLet G = {(r, f)} be a grammar observed from paral-lel training data, where f is the frequency of a bilin-gual translation rule r.Let M be a hidden model that generates everytranslation rule r. For example, M could be mod-eled with a weighted hypergraph or finite state ma-chine.
For the sake of convenience, in this sectionwe assumeM is a meta-grammar M = {m}, whereeach m represents a meta-rule.
For each translationr, there exists a hypergraph Hr that represents allpossible derivations Dr = {d} that can generate ruler.
Here, each derivation d is a hyperpath using meta-rules Md, where Md ?
M. Thus, we can use hy-pergraph Hr to characterize r. Translation rules in G839can share nodes and meta-rules in their hypergraphs,so that M is more compact model than G.In the rest of this section, we will introduce threemethods to quantify Hr as features of rule r. Itshould be noted that there are more ways to exploitthe compact model of M than these three.2.1 Type 1 : A Generative ModelLet ?
be the parameters of a statistical modelPr(m; ?)
for meta-rules m in meta-grammar M es-timated from the observed translation grammar G.The probability of a translation rule r can be calcu-lated as follows.Pr(r; ?)
?
Pr(Hr; ?
)=?d?DrPr(d; ?)
(1)By assuming separability,Pr(d; ?)
=?m?MdPr(m; ?)
(2)we can further decompose rule probability Pr(r; ?
)as below.Pr(r; ?)
=?d?Dr?m?MdPr(m; ?)
(3)In practice, Pr(r; ?)
in (3) can be calculatedthrough bottom-up dynamic programming on hyper-graph Hr.
Hypergraphs of different rules can sharenodes and meta-rules.
This reveals the underlyingrelationship among translation rules.As a by-product of this generative model, we usethe log-likelihood of a translation rule, log Pr(r; ?
),as a new dense feature.
We call it Type 1 in experi-ments.2.2 Type 2 : Meta-Rules as Sparse FeaturesAs given in (3), likelihood of a translation rule isa function over Pr(m; ?
), in which ?
is estimatedfrom the training data with a generative model.
Pre-vious work in (Chiang et al 2009) showed the ad-vantage of using a discriminative model to optimizeindividual weights for these factors towards a betterautomatic score.Following this practice, we treat each meta-rulem as a sparse feature.
Feature value f(m) = 1 ifand only if m is used in hypergraph Hr.
Otherwise,its default value is 0.
We call these features Type2 in experiments.
The Type 2 system contains thelog-likelihood feature in Type 1.2.3 Type 3 : Posterior as Feature ValuesA natural question on the binary sparse features de-fined above is why all the active features have thesame value of 1.
We use these meta-rules to repre-sent a translation rule in feature space.
Intuitively,for meta-rules with closer connection to the trans-lation rules, we hope to use relatively larger featurevalues to increase their effect.We formalize this intuition with the posteriorprobability that a meta-rule m is used to generater, as below.f(m) ?
Pr(m|r; ?)
(4)= Pr(m, r; ?
)Pr(r; ?
)=?d?Dr ,m?Md Pr(d; ?
)Pr(r; ?
)The posterior in (4) could be too sharp.
Follow-ing the common practice, we smooth the posteriorfeatures with a scaling factor ?.f(m) ?
Pr(m|r)?We use Type 3(?)
to represent the posterior modelwith a scaling factor of ?
in experiments.
The Type3 systems also contain the log-likelihood feature inType 1.2.4 Parameter EstimationNow we explain how to obtain parameter ?.
Withproper definition of the underlying model M, wecan estimate ?
with the traditional EM algorithm orBayesian methods.In the next section, we will present an exampleof the hidden model.
We will employ the EM algo-rithm to estimate the parameters in ?.
Here, trans-lation rules and their frequencies in G are observeddata, and derivation d for each rule r is hidden.
Atthe Expectation step, we search all derivations d inDr of each rule r and calculate their probabilitiesaccording to equation (2).
At the Maximization step,we re-estimate ?
on all derivations in proportion totheir posterior probability.8403 Case StudyIn Section 2, we explored the use of meta-grammarsas the underlying model M and developed threemethods to define features.
Similar techniques canbe applied to finite state machines and other underly-ing models.
Now, we introduce a POS-based under-lying model to illustrate the generic model proposedin Section 2.
We will show experimental results inSection 4.3.1 Meta-rules on POS tagsLet r ?
G be a translation rule composed of a pair ofsource and target word strings (Fw, Ew).
Let Fp andEp be the POS tags for the source and target sidesrespectively.
For the sake of simplicity as the firstattempt, we treat non-terminal as a special word Xwith POS tag X.Suppose we have a Chinese-to-English translationrule as below.yuehan qu zhijiage ?
john leaves for chicagoWe callNR VV NR ?
NNP VBZ IN NNP (5)a translation rule in POS tags.We will propose an underlying model M to gen-erate translation rules in POS tags instead of trans-lation rules themselves.
For the rest of this section,we take translation rules in POS tags as the target ofour generative model.
We define meta-rules on pairsof POS tag strings, e.g.
NR VV ?
NNP VBZ .We can decompose the probability of translationrule in (5) into a product on meta-rule probabilitiesvia various derivations, such as?
Pr(NR VV , NNP VBZ ) ?Pr(NR, IN NNP), and?
Pr(NR, NNP) ?
Pr(VV , VBZ IN ) ?Pr(NR, NNP).3.2 The Underlying Model and FeaturesNow, we introduce a generative model M for trans-lation rules in POS tags.
We still use the example in(5) as shown in Figure 1, where the top box repre-sents the source side and the bottom box representsthe target side.
Dotted lines represent word align-ments on three pairs of words.Figure 1: An exampleWe first generate the number of source tokens ofa translation rule with a uniform distribution for upto, for example, 7 tokens.Then we split the source side into chunks witha binomial distribution with a Bernoulli variable atthe gap between each two continuous words, whichsplits the two words into two chunks with a proba-bility of p. For example, the probability of obtainingtwo chunks NR VV and NR is (1 ?
p)p, as shown inFigure 1.Suppose we split the target side into two parts,NNP VBZ and IN NNP, which respects the wordalignments.
It generates two meta-rules NR VV ?NNP VBZ and NR ?
IN NNP, as shown in Figure 1.The probability for the first meta-rule isPr(|E| = 2 | |F | = 2) ?Pr(NR VV ,NNP VBZ | |F | = 2, |E| = 2),where |F | represents the number of source tokens,and |E| the number of target tokens.
Similarly, theprobability of the second one is as follows.Pr(|E| = 2 | |F | = 1) ?Pr(NR, IN NNP | |F | = 1, |E| = 2).To sum up, the probability of a derivation d for atranslation rule r : F ?
E isPr(d) ?
Pr?1(|F |)?
Pr?2(Fs)?
?m?MdPr?3(|Em| | |Fm|)?
?m?MdPr?4(m | |Fm|, |Em|) (6)where Fm and Em are source and target sides of ameta-rule m used in derivation d, and Fs is a split-ting of the source side.
As for the distributions, we841have?1 ?
Uniform?2 ?
Binomial?3 ?
Categorical?4 ?
Categoricalwhere ?1 and ?2 have pre-selected hyperparameters,and ?3 and ?4 are estimated with the EM algorithm.As for sparse features, we will obtain 7 meta-rulefeatures as below.?
NR ?
NNP?
VV ?
VBZ?
VV ?
VBZ IN?
NR VV ?
NNP VBZ?
NR VV ?
NNP VBZ IN?
VV NR ?
VBZ IN NNP?
NR VV NR ?
NNP VBZ IN NNPAll of them respect the word alignment, whichmeans that?
there is no alignment that aligns one word in ameta-rule with the other out of the same meta-rule, and?
there is at least one alignment within a meta-rule.3.3 Implementation DetailsEven though the size of all possible meta-rules ismuch smaller than the space of translation rules,it is still too large to work with existing optimiza-tion methods for sparse features in MT, i.e.
MIRA(Chiang et al 2009) or L-BFGS (Matsoukas et al2009).
In practice, we have to limit the feature spaceto around 20,000 dimensions.For this purpose, we first use a frequency basedmethod to filter meta-rule features.
Specifically,we first divide all the meta-rules into 100 bins,(|F |, |E|), where |F | is the number of words on thesource side, and |E| the target side, 0 < |F |, |E| ?10.
For each bin, we keep the same top k-percentileof the meta-rules such that we obtain a total of20,000 meta-rules as features.System BLEU% TER% T-BBaseline 30.35 55.32 24.97Type 1 30.74 55.48 24.74Type 2 31.07 55.07 24.00Type 3 (1) 30.93 55.34 24.41Type 3 (0.1) 31.05 55.02 23.97Type 3 (0.01) 31.09 54.96 23.87Table 1: scores on test-1A shortcoming of this filtering method is that allthese features are positive indicators, while low-frequency negative indicators are discarded.
In orderto keep the features of various level of frequency, wedefine class features with a 3-tuple C(|F |, |E|, q),where |F | and |E| are numbers of source and targetwords as defined above, and q is the integer part ofthe log2 value of the feature frequency in the trainingdata.In this way, each meta-rule feature can be mappedto one of these classes.
The value of a class featureequals the sum of the meta-rule features that mappedinto this class.
We have about 2,000 class featuresdefined in this way.
They are applied on both Type2 and Type 3 features.4 ExperimentsWe carry out our experiments on web genre ofChinese-to-English translation.
The training setcontains about 10 million parallel sentences avail-able to Phase 1 of the DARPA BOLT MT task.
Thetune set contains 1275 sentences.
Each has four ref-erences.
There are two test sets.
Test-1 is from asimilar source of the tune set, and it contains 1239sentences.
Test-2 is the web part of the MT08 eval-uation data.Our baseline system is a home-made Hiero (Chi-ang, 2005) style system.
The baseline rule set con-tains about 17 million rules.
It contains about 40dense features, including a 6-gram LM.The sparse feature optimization algorithm is sim-ilar to the MIRA recipe described in (Chiang et al2009).
We optimize on TER-BLEU (Snover et al2006; Papineni et al 2001).The BLEU, TER and T-B scores on the two testsare shown in Tables 1 and 2.
It should be noted that,even though our metric of tuning is T-B, the baseline842System BLEU% TER% T-BBaseline 25.80 56.96 31.16Type 1 26.18 57.09 30.91Type 2 26.63 56.64 30.01Type 3 (1) 26.30 57.00 30.70Type 3 (0.1) 26.34 56.73 30.39Type 3 (0.01) 26.50 56.73 30.23Table 2: scores on test-2 (MT08-WB)system already provides a very competitive BLEUscore on MT08-WB as compared the best system inthe evaluation1, thanks to comprehensive features inthe baseline system and more data in training.All the three types of systems provide consis-tent improvement on both test sets in terms of T-B,our optimization metric.
Type 1 gives marginal im-provement of 0.2.
This shows the limitation of thegenerative feature.
When we use meta-rules as bi-nary sparse features in Type 2, we obtain about onepoint improvement on T-B on both sets.
This showsthe advantage of tuning individual meta-rule weightsover a generative model.
Type 3 (0.01) and Type 2are at the same level.
Proper smoothing is importantto Type 3.5 DiscussionIn the case study of Section 3, we use POS-basedrules as hidden states.
However, it should be notedthat the hidden structures surely do not have to bePOS tags.
For example, an alternative could beunsupervised NT splitting similar to (Huang et al2010).The meta-grammar based approach was also mo-tivated by the insight acquired on mono-lingual lin-guistic grammar generation, especially in the TAGrelated research (Xia, 2001; Prolo, 2002).
Meta-grammar was viewed as an effective way to removeredundancy in grammars.The link between Tree Adjoining Grammar(TAG) (Joshi et al 1975; Joshi and Schabes, 1997)and MT was first introduced in (Shieber and Sch-abes, 1990), a pioneer work in tree-to-tree transla-tion.
(DeNeefe and Knight, 2009) re-visited the useof adjoining operation in the context of StatisticalMT, and reported encouraging results.
On the other1http://www.itl.nist.gov/iad/mig/tests/mt/2008/hand, (Dras, 1999) showed how a meta-level gram-mar could help in modeling parallel operations in(Shieber and Schabes, 1990).
Our work is anothereffort of statistical modeling of well-recognized lin-guistic insight in NLP and MT.6 Conclusions and Future WorkIn this paper, we introduced a novel method to modeltranslation rules as observed generation output of acompact hidden model.
As a case study to capital-ize this model, we presented three methods to en-rich rule modeling with features defined on a hid-den model.
Preliminary experiments verified gain ofone point on TER-BLEU over a strong baseline inChinese-to-English translation.As for future work, we plan to extend this work inthe following aspects.?
To try other prior distributions to generate thenumber of source tokens.?
Unsupervised and semi-supervised learning ofhidden models.?
To incorporate rich models into the generativeprocess, e.g.
reordering, non-terminals, struc-tural information and lexical models.?
To improve the posterior model with better pa-rameter estimation, e.g.
Bayesian methods.?
To replace the exhaustive translation rule setwith a compact meta grammar that can createand parameterize new translation rules dynam-ically, which is the ultimate goal of this line ofwork.AcknowledgmentsWe would like thank the anonymous reviewers fortheir valuable comments.
Haitao Mi and MartinCmejrek kindly helped on data preparation.This work was done when the first author wasat IBM.
The work was supported by DARPA un-der Grant HR0011-12-C-0015 for funding part ofthis work.
The views, opinions, and/or findings con-tained in this article/presentation are those of the au-thor/presenter and should not be interpreted as repre-senting the ofcial views or policies, either expressedor implied, of the DARPA.843ReferencesAbhishek Arun and Philipp Koehn.
2007.
Onlinelearning methods for discriminative training of phrasebased statistical machine translation.
In Proceedingsof MT Summit XI.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001new features for statistical machine translation.
InProceedings of the 2009 Human Language Technol-ogy Conference of the North American Chapter of theAssociation for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 263?270, Ann Ar-bor, MI.Steve DeNeefe and Kevin Knight.
2009.
Synchronoustree adjoining machine translation.
In Proceedings ofthe 2009 Conference of Empirical Methods in NaturalLanguage Processing, pages 727?736, Singapore.Mark Dras.
1999.
A meta-level grammar: redefiningsynchronous tag for translation and paraphrase.
InProceedings of the 37th Annual Meeting of the Associ-ation for Computational Linguistics (ACL).Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of the 2010 Conference ofEmpirical Methods in Natural Language Processing.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages, vol-ume 3, pages 69?124.
Springer-Verlag.Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.1975.
Tree adjunct grammars.
Journal of Computerand System Sciences, 10(1):136?163.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase based translation.
In Proceedingsof the 2003 Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 48?54, Edmonton,Canada.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proceedings of the 2006 Conference of EmpiricalMethods in Natural Language Processing, pages 44?52, Sydney, Australia.Spyros Matsoukas, Antti-Veikko Rosti, and Bing Zhang.2009.
Discriminative corpus weight estimation formachine translation.
In Proceedings of the 2009 Con-ference of Empirical Methods in Natural LanguageProcessing.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4).Kishore Papineni, Salim Roukos, and Todd Ward.
2001.Bleu: a method for automatic evaluation of machinetranslation.
IBM Research Report, RC22176.Carlos Prolo.
2002.
Generating the xtag english gram-mar using metarules.
In Proceedings of the 19th in-ternational conference on Computational linguistics(COLING).Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics (ACL).Stuart Shieber and Yves Schabes.
1990.
Synchronoustree adjoining grammars.
In Proceedings of COLING?90: The 13th Int.
Conf.
on Computational Linguistics,pages 253?258, Helsinki, Finland.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas, pages 223?231, Cambridge, MA.T.
Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
In Proceedings of the 2007 Confer-ence of Empirical Methods in Natural Language Pro-cessing.F.
Xia.
2001.
Automatic Grammar Generation FromTwo Different Perspectives.
Ph.D. thesis, Universityof Pennsylvania.844
