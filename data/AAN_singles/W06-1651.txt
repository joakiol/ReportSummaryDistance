Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 431?439,Sydney, July 2006. c?2006 Association for Computational LinguisticsJoint Extraction of Entities and Relations for Opinion RecognitionYejin Choi and Eric Breck and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853{ychoi,ebreck,cardie}@cs.cornell.eduAbstractWe present an approach for the joint ex-traction of entities and relations in the con-text of opinion recognition and analysis.We identify two types of opinion-relatedentities ?
expressions of opinions andsources of opinions ?
along with the link-ing relation that exists between them.
In-spired by Roth and Yih (2004), we employan integer linear programming approachto solve the joint opinion recognition task,and show that global, constraint-based in-ference can significantly boost the perfor-mance of both relation extraction and theextraction of opinion-related entities.
Per-formance further improves when a seman-tic role labeling system is incorporated.The resulting system achieves F-measuresof 79 and 69 for entity and relation extrac-tion, respectively, improving substantiallyover prior results in the area.1 IntroductionInformation extraction tasks such as recognizingentities and relations have long been consideredcritical to many domain-specific NLP tasks (e.g.Mooney and Bunescu (2005), Prager et al (2000),White et al (2001)).
Researchers have furthershown that opinion-oriented information extrac-tion can provide analogous benefits to a variety ofpractical applications including product reputationtracking (Morinaga et al, 2002), opinion-orientedquestion answering (Stoyanov et al, 2005), andopinion-oriented summarization (e.g.
Cardie etal.
(2004), Liu et al (2005)).
Moreover, muchprogress has been made in the area of opinion ex-traction: it is possible to identify sources of opin-ions (i.e.
the opinion holders) (e.g.
Choi et al(2005) and Kim and Hovy (2005b)), to determinethe polarity and strength of opinion expressions(e.g.
Wilson et al (2005)), and to recognize propo-sitional opinions and their sources (e.g.
Bethardet al (2004)) with reasonable accuracy.
To date,however, there has been no effort to simultane-ously identify arbitrary opinion expressions, theirsources, and the relations between them.
Withoutprogress on the joint extraction of opinion enti-ties and their relations, the capabilities of opinion-based applications will remain limited.Fortunately, research in machine learning hasproduced methods for global inference and jointclassification that can help to address this defi-ciency (e.g.
Bunescu and Mooney (2004), Rothand Yih (2004)).
Moreover, it has been shown thatexploiting dependencies among entities and/or re-lations via global inference not only solves thejoint extraction task, but often boosts performanceon the individual tasks when compared to clas-sifiers that handle the tasks independently ?
forsemantic role labeling (e.g.
Punyakanok et al(2004)), information extraction (e.g.
Roth and Yih(2004)), and sequence tagging (e.g.
Sutton et al(2004)).In this paper, we present a global inference ap-proach (Roth and Yih, 2004) to the extractionof opinion-related entities and relations.
In par-ticular, we aim to identify two types of entities(i.e.
spans of text): entities that express opin-ions and entities that denote sources of opinions.More specifically, we use the term opinion expres-sion to denote all direct expressions of subjectiv-ity including opinions, emotions, beliefs, senti-ment, etc., as well as all speech expressions thatintroduce subjective propositions; and use the termsource to denote the person or entity (e.g.
a re-431port) that holds the opinion.1 In addition, weaim to identify the relations between opinion ex-pression entities and source entities.
That is, fora given opinion expression Oi and source entitySj , we determine whether the relation Li,j def=(Sj expresses Oi) obtains, i.e.
whether Sj is thesource of opinion expression Oi.
We refer to thisparticular relation as the link relation in the restof the paper.
Consider, for example, the followingsentences:S1.
[Bush](1) intends(1) to curb the increase inharmful gas emissions and is counting on(1)the good will(2) of [US industrialists](2) .S2.
By questioning(3) [the Imam](4)?s edict(4) [theIslamic Republic of Iran](3) made [the peopleof the world](5) understand(5)...The underlined phrases above are opinion expres-sions and phrases marked with square brackets aresource entities.
The numeric superscripts on en-tities indicate link relations: a source entity andan opinion expression with the same number sat-isfy the link relation.
For instance, the source en-tity ?Bush?
and the opinion expression ?intends?satisfy the link relation, and so do ?Bush?
and?counting on.?
Notice that a sentence may con-tain more than one link relation, and link relationsare not one-to-one mappings between sources andopinions.
Also, the pair of entities in a link rela-tion may not be the closest entities to each other, asis the case in the second sentence, between ?ques-tioning?
and ?the Islamic Republic of Iran.
?We expect the extraction of opinion relations tobe critical for many opinion-oriented NLP appli-cations.
For instance, consider the following ques-tion that might be given to a question-answeringsystem:?
What is the Imam?s opinion toward the IslamicRepublic of Iran?Without in-depth opinion analysis, the question-answering system might mistake example S2 asrelevant to the query, even though S2 exhibits theopinion of the Islamic Republic of Iran towardImam, not the other way around.Inspired by Roth and Yih (2004), we modelour task as global, constraint-based inference overseparately trained entity and relation classifiers.In particular, we develop three base classifiers:two sequence-tagging classifiers for the extraction1See Wiebe et al (2005) for additional details.of opinion expressions and sources, and a binaryclassifier to identify the link relation.
The globalinference procedure is implemented via integerlinear programming (ILP) to produce an optimaland coherent extraction of entities and relations.Because many (60%) opinion-source relationsappear as predicate-argument relations, where thepredicate is a verb, we also hypothesize that se-mantic role labeling (SRL) will be very useful forour task.
We present two baseline methods forthe joint opinion-source recognition task that usea state-of-the-art SRL system (Punyakanok et al,2005), and describe two additional methods for in-corporating SRL into our ILP-based system.Our experiments show that the global inferenceapproach not only improves relation extractionover the base classifier, but does the same for in-dividual entity extractions.
For source extractionin particular, our system achieves an F-measure of78.1, significantly outperforming previous resultsin this area (Choi et al, 2005), which obtained anF-measure of 69.4 on the same corpus.
In addition,we achieve an F-measure of 68.9 for link relationidentification and 82.0 for opinion expression ex-traction; for the latter task, our system achieveshuman-level performance.22 High-Level Approach and RelatedWorkOur system operates in three phases.Opinion and Source Entity Extraction Webegin by developing two separate token-levelsequence-tagging classifiers for opinion expres-sion extraction and source extraction, using linear-chain Conditional Random Fields (CRFs) (Laf-ferty et al, 2001).
The sequence-tagging classi-fiers are trained using only local syntactic and lex-ical information to extract each type of entity with-out knowledge of any nearby or neighboring enti-ties or relations.
We collect n-best sequences fromeach sequence tagger in order to boost the recall ofthe final system.Link Relation Classification We also developa relation classifier that is trained and tested onall pairs of opinion and source entities extractedfrom the aforementioned n-best opinion expres-sion and source sequences.
The relation classifieris modeled using Markov order-0 CRFs(Lafferty2Wiebe et al (2005) reports human annotation agreementfor opinion expression as 82.0 by F1 measure.432et al, 2001), which are equivalent to maximum en-tropy models.
It is trained using only local syntac-tic information potentially useful for connecting apair of entities, but has no knowledge of nearby orneighboring extracted entities and link relations.Integer Linear Programming Finally, we for-mulate an integer linear programming problem foreach sentence using the results from the previoustwo phases.
In particular, we specify a numberof soft and hard constraints among relations andentities that take into account the confidence val-ues provided by the supporting entity and relationclassifiers, and that encode a number of heuristicsto ensure coherent output.
Given these constraints,global inference via ILP finds the optimal, coher-ent set of opinion-source pairs by exploiting mu-tual dependencies among the entities and relations.While good performance in entity or relationextraction can contribute to better performance ofthe final system, this is not always the case.
Pun-yakanok et al (2004) notes that, in general, it isbetter to have high recall from the classifiers in-cluded in the ILP formulation.
For this reason, it isnot our goal to directly optimize the performanceof our opinion and source entity extraction modelsor our relation classifier.The rest of the paper is organized as follows.Related work is outlined below.
Section 3 de-scribes the components of the first phase of oursystem, the opinion and source extraction classi-fiers.
Section 4 describes the construction of thelink relation classifier for phase two.
Section 5describes the ILP formulation to perform globalinference over the results from the previous twophases.
Experimental results that compare our ILPapproach to a number of baselines are presented inSection 6.
Section 7 describes how SRL can be in-corporated into our global inference system to fur-ther improve the performance.
Final experimentalresults and discussion comprise Section 8.Related Work The definition of our source-expresses-opinion task is similar to that of Bethardet al (2004); however, our definition of opin-ion and source entities are much more extensive,going beyond single sentences and propositionalopinion expressions.
In particular, we evaluateour approach with respect to (1) a wide varietyof opinion expressions, (2) explicit and implicit3sources, (3) multiple opinion-source link relations3Implicit sources are those that are not explicitly men-tioned.
See Section 8 for more details.per sentence, and (4) link relations that span morethan one sentence.
In addition, the link rela-tion model explicitly exploits mutual dependen-cies among entities and relations, while Bethardet al (2004) does not directly capture the potentialinfluence among entities.Kim and Hovy (2005b) and Choi et al (2005)focus only on the extraction of sources ofopinions, without extracting opinion expressions.Specifically, Kim and Hovy (2005b) assume a pri-ori existence of the opinion expressions and ex-tract a single source for each, while Choi et al(2005) do not explicitly extract opinion expres-sions nor link an opinion expression to a sourceeven though their model implicitly learns approxi-mations of opinion expressions in order to identifyopinion sources.
Other previous research focusesonly on the extraction of opinion expressions (e.g.Kim and Hovy (2005a), Munson et al (2005) andWilson et al (2005)), omitting source identifica-tion altogether.There have also been previous efforts to si-multaneously extract entities and relations by ex-ploiting their mutual dependencies.
Roth andYih (2002) formulated global inference using aBayesian network, where they captured the influ-ence between a relation and a pair of entities viathe conditional probability of a relation, given apair of entities.
This approach however, could notexploit dependencies between relations.
Roth andYih (2004) later formulated global inference usinginteger linear programming, which is the approachthat we apply here.
In contrast to our work, Rothand Yih (2004) operated in the domain of factualinformation extraction rather than opinion extrac-tion, and assumed that the exact boundaries of en-tities from the gold standard are known a priori,which may not be available in practice.3 Extraction of Opinion and SourceEntitiesWe develop two separate sequence tagging classi-fiers for opinion extraction and source extraction,using linear-chain Conditional Random Fields(CRFs) (Lafferty et al, 2001).
The sequence tag-ging is encoded as the typical ?BIO?
scheme.4Each training or test instance represents a sen-tence, encoded as a linear chain of tokens and their4?B?
is for the token that begins an entity, ?I?
is for to-kens that are inside an entity, and ?O?
is for tokens outside anentity.433associated features.
Our feature set is based onthat of Choi et al (2005) for source extraction5,but we include additional lexical and WordNet-based features.
For simplicity, we use the samefeatures for opinion entity extraction and sourceextraction, and let the CRFs learn appropriate fea-ture weights for each task.3.1 Entity extraction featuresFor each token xi, we include the following fea-tures.
For details, see Choi et al (2005).word: words in a [-4, +4] window centered on xi.part-of-speech: POS tags in a [-2, +2] window.6grammatical role: grammatical role (subject, ob-ject, prepositional phrase types) of xi derived froma dependency parse.7dictionary: whether xi is in the opinion expres-sion dictionary culled from the training data andaugmented by approximately 500 opinion wordsfrom the MPQA Final Report8.
Also computedfor tokens in a [-1, +1] window and for xi?s parent?chunk?
in the dependency parse.semantic class: xi?s semantic class.9WordNet: the WordNet hypernym of xi.104 Relation ClassificationWe also develop a maximum entropy binary clas-sifier for opinion-source link relation classifica-tion.
Given an opinion-source pair, Oi-Sj , the re-lation classifier decides whether the pair exhibitsa valid link relation, Li,j .
The relation classifierfocuses only on the syntactic structure and lexicalproperties between the two entities of a given pair,without knowing whether the proposed entities arecorrect.
Opinion and source entities are taken fromthe n-best sequences of the entity extraction mod-els; therefore, some are invariably incorrect.From each sentence, we create training and testinstances for all possible opinion-source pairingsthat do not overlap: we create an instance for Li,jonly if the span of Oi and Sj do not overlap.For training, we also filter out instances forwhich neither the proposed opinion nor source en-5We omit only the extraction pattern features.6Using GATE: http://gate.ac.uk/7Provided by Rebecca Hwa, based on the Collins parser:ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz8https://rrc.mitre.org/pubs/mpqaFinalReport.pdf9Using SUNDANCE: (http://www.cs.utah.edu/r?iloff/publications.html#sundance)10http://wordnet.princeton.edu/tity overlaps with a correct opinion or source en-tity per the gold standard.
This training instancefiltering helps to avoid confusion between exam-ples like the following (where entities marked inbold are the gold standard entities, and entitiesin square brackets represent the n-best output se-quences from the entity extraction classifiers):(1) [The president] s1 walked away from [themeeting] o1, [ [revealing] o2 his disap-pointment] o3 with the deal.
(2) [The monster] s2 walked away, [revealing] o4a little box hidden underneath.For these sentences, we construct training in-stances for L1,1, L1,2, and L1,3, but not L2,4,which in fact has very similar sentential structureas L1,2, and hence could confuse the learning al-gorithm.4.1 Relation extraction featuresThe training and test instances for each (potential)link Li,j (with opinion candidate entity Oi andsource candidate entity Sj) include the followingfeatures.opinion entity word: the words contained in Oi.phrase type: the syntactic category of the con-stituent in which the entity is embedded, e.g.
NPor VP.
We encode separate features for Oi and Sj .grammatical role: the grammatical role of theconstituent in which the entity is embedded.Grammatical roles are derived from dependencyparse trees, as done for the entity extraction classi-fiers.
We encode separate features for Oi and Sj .position: a boolean value indicating whether Sjprecedes Oi.distance: the distance between Oi and Sj in num-bers of tokens.
We use four coarse categories: ad-jacent, very near, near, far.dependency path: the path through the depen-dency tree from the head of Sj to the head of Oi.For instance, ?subj?verb?
or ?subj?verb?obj?.voice: whether the voice of Oi is passive or active.syntactic frame: key intra-sentential relations be-tween Oi and Sj .
The syntactic frames that we useare:?
[E1:role] [distance] [E2:role], where distance?
{adjacent, very near, near, far}, and Ei:roleis the grammatical role of Ei.
Either E1 is anopinion entity and E2 is a source, or vice versa.?
[E1:phrase] [distance] [E2:phrase], whereEi:phrase is the phrasal type of entity Ei.434?
[E1:phrase] [E2:headword], where E2 must bethe opinion entity, and E1 must be the source en-tity (i.e.
no lexicalized frames for sources).
E1and E2 can be contiguous.?
[E1:role] [E2:headword], where E2 must be theopinion entity, and E1 must be the source entity.?
[E1:phrase] NP [E2:phrase] indicates thepresence of specific syntactic patterns, e.g.
?VP NP VP?
depending on the possible phrasetypes of opinion and source entities.
The threephrases do not need to be contiguous.?
[E1:phrase] VP [E2:phrase] (See above.)?
[E1:phrase] [wh-word] [E2:phrase] (Seeabove.)?
Src [distance] [x] [distance] Op, where x ?
{by, of, from, for, between, among, and, have,be, will, not, ], ?, .
.
.
}.When a syntactic frame is matched to a sen-tence, the bracketed items should be instantiatedwith particular values corresponding to the sen-tence.
Pattern elements without square bracketsare constants.
For instance, the syntactic frame?
[E1:phrase] NP [E2:phrase]?
may be instantiatedas ?VP NP VP?.
Some frames are lexicalized withrespect to the head of an opinion entity to reflectthe fact that different verbs expect source enti-ties in different argument positions (e.g.
SOURCEblamed TARGET vs. TARGET angered SOURCE).5 Integer Linear ProgrammingApproachAs noted in the introduction, we model our taskas global, constraint-based inference over the sep-arately trained entity and relation classifiers, andimplement the inference procedure as binary in-teger linear programming (ILP) ((Roth and Yih,2004), (Punyakanok et al, 2004)).
ILP consistsof an objective function which is a dot productbetween a vector of variables and a vector ofweights, and a set of equality and inequality con-straints among variables.
Given an objective func-tion and a set of constraints, LP finds the opti-mal assignment of values to variables, i.e.
one thatminimizes the objective function.
In binary ILP,the assignments to variables must be either 0 or 1.The variables and constraints defined for the opin-ion recognition task are summarized in Table 1 andexplained below.Entity variables and weights For each opinionentity, we add two variables, Oi and O?i, whereOi = 1 means to extract the opinion entity, andObjective function f=?i(woiOi) +?i(w?oiO?i)+?j(wsjSj) +?j(w?sj S?j)+?i,j(wli,jLi,j) +?i(w?li,j L?i,j)?i, Oi + O?i = 1?j, Sj + S?j = 1?i, j, Li,j + L?i,j = 1?i, Oi =?j Li,j?j, Sj + Aj =?i Li,j?j, Aj ?
Sj ?
0?i, j, i < j, Xi + Xj = 1,X ?
{S,O}Table 1: Binary ILP formulationO?i = 1 means to discard the opinion entity.
Toensure coherent assignments, we add equality con-straints ?i, Oi + O?i = 1.
The weights woi andw?oi for Oi and O?i respectively, are computed asa negative conditional probability of the span ofan entity to be extracted (or suppressed) given thelabelings of the adjacent variables of the CRFs:woidef= ?P (xk, xk+1, ..., xl|xk?1, xl+1)where xk = ?B?& xm = ?I?
for m ?
[k + 1, l]w?oidef= ?P (xk, xk+1, ..., xl|xk?1, xl+1)where xm = ?O?
for m ?
[k, l]where xi is the value assigned to the random vari-able of the CRF corresponding to an entity Oi.Likewise, for each source entity, we add two vari-ables Sj and S?j and a constraint Sj + S?j = 1.
Theweights for source variables are computed in thesame way as opinion entities.Relation variables and weights For each linkrelation, we add two variables Li,j and L?i,j , anda constraint Li,j + L?i,j = 1.
By the definition ofa link, if Li,j = 1, then it is implied that Oi = 1and Sj = 1.
That is, if a link is extracted, then thepair of entities for the link must be also extracted.Constraints to ensure this coherency are explainedin the following subsection.
The weights for linkvariables are based on probabilities from the bi-nary link classifier.Constraints for link coherency In our corpus, asource entity can be linked to more than one opin-ion entity, but an opinion entity is linked to only435one source.
Nonetheless, the majority of opinion-source pairs involve one-to-one mappings, whichwe encode as hard and soft constraints as follows:For each opinion entity, we add an equality con-straint Oi =?j Li,j to enforce that only onelink can emanate from an opinion entity.
For eachsource entity, we add an equality constraint and aninequality constraint that together allow a sourceto link to at most two opinions: Sj +Aj =?i Li,jand Aj ?
Sj ?
0, where Aj is an auxiliary vari-able, such that its weight is some positive constantvalue that suppresses Aj from being assigned to 1.And Aj can be assigned to 1 only if Sj is alreadyassigned to 1.
It is possible to add more auxiliaryvariables to allow more than two opinions to linkto a source, but for our experiments two seemed tobe a reasonable limit.Constraints for entity coherency When we usen-best sequences where n > 1, proposed entitiescan overlap.
Because this should not be the casein the final result, we add an equality constraintXi + Xj = 1, X ?
{S,O} for all pairs of entitieswith overlapping spans.Adjustments to weights To balance the preci-sion and recall, and to take into account the per-formance of different base classifiers, we apply ad-justments to weights as follows.1) We define six coefficients cx and c?x, wherex ?
{O,S,L} to modify a group of weightsas follows.
?i, x, wxi := wxi ?
cx;?i, x, w?xi := w?xi ?
c?x;In general, increasing cx will promote recall,while increasing c?x will promote precision.Also, setting co > cs will put higher confi-dence on the opinion extraction classifier thanthe source extraction classifier.2) We also define one constant cA to set theweights for auxiliary variable Ai.
That is,?i, wAi := cA.3) Finally, we adjust the confidence of the linkvariable based on n-th-best sequences of the en-tity extraction classifiers as follows.
?i, wLi,j := wLi,j ?
dwhere d def= 4/(3 + min(m,n)), when Oi isfrom an m-th sequence and Sj is from a n-thsequence.1111This will smoothly degrade the confidence of a linkbased on the entities from higher n-th sequences.
Values of ddecrease as 4/4, 4/5, 4/6, 4/7....6 Experiments?IWe evaluate our system using the NRRC Multi-Perspective Question Answering (MPQA) corpusthat contains 535 newswire articles that are man-ually annotated for opinion-related information.In particular, our gold standard opinion entitiescorrespond to direct subjective expression anno-tations and subjective speech event annotations(i.e.
speech events that introduce opinions) in theMPQA corpus (Wiebe et al, 2005).
Gold stan-dard source entities and link relations can be ex-tracted from the agent attribute associated witheach opinion entity.
We use 135 documents as adevelopment set and report 10-fold cross valida-tion results on the remaining 400 documents in allexperiments below.We evaluate entity and link extraction usingboth an overlap and exact matching scheme.12 Be-cause the exact start and endpoints of the man-ual annotations are somewhat arbitrary, the over-lap scheme is more reasonable for our task (Wiebeet al, 2005).
We report results according to bothmatching schemes, but focus our discussion on re-sults obtained using overlap matching.13We use the Mallet14 implementation of CRFs.For brevity, we will refer to the opinion extractionclassifier as CRF-OP, the source extraction classi-fier as CRF-SRC, and the link relation classifier asCRF-LINK.
For ILP, we use Matlab, which pro-duced the optimal assignment in a matter of fewseconds for each sentence.
The weight adjustmentconstants defined for ILP are based on the devel-opment data.15The link-nearest baselines For baselines, wefirst consider a link-nearest heuristic: for eachopinion entity extracted by CRF-OP, the link-nearest heuristic creates a link relation with theclosest source entity extracted by CRF-SRC.
Re-call that CRF-SRC and CRF-OP extract entitiesfrom n-best sequences.
We test the link-nearestheuristic with n = {1, 2, 10} where larger n willboost recall at the cost of precision.
Results for the12Given two links L1,1 = (O1, S1) and L2,2 = (O2, S2),exact matching requires the spans of O1 and O2, and thespans of S1 and S2, to match exactly, while overlap matchingrequires the spans to overlap.13Wiebe et al (2005) also reports the human annotationagreement study via the overlap scheme.14Available at http://mallet.cs.umass.edu15co = 2.5, c?o = 1.0, cs = 1.5, c?s = 1.0, cL = 2.5, c?L =2.5, cA = 0.2.
Values are picked so as to boost recall whilereasonably suppressing incorrect links.436Overlap Match Exact Matchr(%) p(%) f(%) r(%) p(%) f(%)NEAREST-1 51.6 71.4 59.9 26.2 36.9 30.7NEAREST-2 60.7 45.8 52.2 29.7 19.0 23.1NEAREST-10 66.3 20.9 31.7 28.2 00.0 00.0SRL 59.7 36.3 45.2 32.6 19.3 24.2SRL+CRF-OP 45.6 83.2 58.9 27.6 49.7 35.5ILP-1 51.6 80.8 63.0 26.4 42.0 32.4ILP-10 64.0 72.4 68.0 31.0 34.8 32.8Table 2: Relation extraction performanceNEAREST-n : link-nearest heuristic w/ n-bestSRL : all V-A0 frames from SRLSRL+CRF-OP : all V-A0 filtered by CRF-OPILP-n : ILP applied to n-best sequenceslink-nearest heuristic on the full source-expresses-opinion relation extraction task are shown in thefirst three rows of table 2.
NEAREST-1 performsthe best in overlap-match F-measure, reaching59.9.
NEAREST-10 has higher recall (66.3%), butthe precision is really low (20.9%).
Performanceof the opinion and source entity classifiers will bediscussed in Section 8.SRL baselines Next, we consider two base-lines that use a state-of-the-art SRL system (Pun-yakanok et al, 2005).
In many link relations,the opinion expression entity is a verb phrase andthe source entity is in an agent argument posi-tion.
Hence our second baseline, SRL, extractsall verb(V)-agent(A0) frames from the output ofthe SRL system and provides an upper bound onrecall (59.7%) for systems that use SRL in isola-tion for our task.
A more sophisticated baseline,SRL+CRF-OP, extracts only those V-A0 frameswhose verb overlaps with entities extracted by theopinion expression extractor, CRF-OP.
As shownin table 2, filtering out V-A0 frames that are in-compatible with the opinion extractor boosts pre-cision to 83.2%, but the F-measure (58.9) is lowerthan that of NEAREST-1.ILP results The ILP-n system in table 2 de-notes the results of the ILP approach applied to then-best sequences.
ILP-10 reaches an F-measureof 68.0, a significant improvement over the high-est performing baseline16 , and also a substantialimprovement over ILP-1.
Note that the perfor-mance of NEAREST-10 was much worse than that16Statistically significant by paired-t test, where p <0.001.Overlap Match Exact Matchr(%) p(%) f(%) r(%) p(%) f(%)ILP-1 51.6 80.8 63.0 26.4 42.0 32.4ILP-10 64.0 72.4 68.0 31.0 34.8 32.8ILP+SRL-f -1 51.7 81.5 63.3 26.6 42.5 32.7ILP+SRL-f -10 65.7 72.4 68.9 31.5 34.3 32.9ILP+SRL-fc-10 64.0 73.5 68.4 28.4 31.3 29.8Table 3: Relation extraction with ILP and SRLILP-n : ILP applied to n-best sequencesILP+SRL-f -n : ILP w/ SRL features, n-bestILP+SRL-fc-n : ILP w/ SRL features,and SRL constraints, n-bestof NEAREST-1, because the 10-best sequences in-clude many incorrect entities whereas the corre-sponding ILP formulation can discard the bad en-tities by considering dependencies among entitiesand relations.177 Additional SRL IncorporationWe next explore two approaches for more directlyincorporating SRL into our system.Extra SRL Features for the Link classifier Weincorporate SRL into the link classifier by addingextra features based on SRL.
We add boolean fea-tures to check whether the span of an SRL argu-ment and an entity matches exactly.
In addition,we include syntactic frame features as follows:?
[E1:srl-arg] [E2:srl-arg], where Ei:srl-arg indi-cates the SRL argument type of entity Ei.?
[E1.srl-arg] [E1:headword] [E2:srl-arg], whereE1 must be an opinion entity, and E2 must be asource entity.Extra SRL Constraints for the ILP phase Wealso incorporate SRL into the ILP phase of oursystem by adding extra constraints based on SRL.In particular, we assign very high weights for linksthat match V-A0 frames generated by SRL, in or-der to force the extraction of V-A0 frames.17A potential issue with overlap precision and recall is thatthe measures may drastically overestimate the system?s per-formance as follows: a system predicting a single link rela-tion whose source and opinion expression both overlap withevery token of a document would achieve 100% overlap pre-cision and recall.
We can ensure this does not happen by mea-suring the average number of (source, opinion) pairs to whicheach correct or predicted pair is aligned (excluding pairs notaligned at all).
In our data, this does not exceed 1.08, (exceptfor baselines), so we can conclude these evaluation measuresare behaving reasonably.437Opinion Source Linkr(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)Before ILP CRF-OP/SRC/LINK with 1 best 76.4 88.4 81.9 67.3 81.9 73.9 60.5 50.5 55.0merged 10 best 95.7 31.2 47.0 95.3 24.5 38.9 N/AAfter ILP ILP-SRL-f -10 75.1 82.9 78.8 80.6 75.7 78.1 65.7 72.4 68.9ILP-SRL-f -10 ?
CRF-OP/SRC with 1 best 82.3 81.7 82.0 81.5 73.4 77.3 N/ATable 4: Entity extraction performance (by overlap-matching)8 Experiments?IIResults using SRL are shown in Table 3 (on theprevious page).
In the table, ILP+SRL-f denotesthe ILP approach using the link classifier withthe extra SRL ?f ?eatures, and ILP+SRL-fc de-notes the ILP approach using both the extra SRL?f ?eatures and the SRL ?c?onstraints.
For compar-ison, the ILP-1 and ILP-10 results from Table 2are shown in rows 1 and 2.The F-measure score of ILP+SRL-f -10 is 68.9,about a 1 point increase from that of ILP-10,which shows that extra SRL features for the linkclassifier further improve the performance overour previous best results.18 ILP+SRL-fc-10 alsoperforms better than ILP-10 in F-measure, al-though it is slightly worse than ILP+SRL-f -10.This indicates that the link classifier with extraSRL features already makes good use of the V-A0frames from the SRL system, so that forcing theextraction of such frames via extra ILP constraintsonly hurts performance by not allowing the extrac-tion of non-V-A0 pairs in the neighborhood thatcould have been better choices.Contribution of the ILP phase In order tohighlight the contribution of the ILP phase for ourtask, we present ?before?
and ?after?
performancein Table 4.
The first row shows the performanceof the individual CRF-OP, CRF-SRC, and CRF-LINK classifiers before the ILP phase.
Without theILP phase, the 1-best sequence generates the bestscores.
However, we also present the performancewith merged 10-best entity sequences19 in orderto demonstrate that using 10-best sequences with-out ILP will only hurt performance.
The precisionof the merged 10-best sequences system is verylow, however the recall level is above 95% for both18Statistically significant by paired-t test, where p <0.001.19If an entity Ei extracted by the ith-best sequence over-laps with an entity Ej extracted by the jth-best sequence,where i < j, then we discard Ej .
If Ei and Ej do not over-lap, then we extract both entities.CRF-OP and CRF-SRC, giving an upper bound forrecall for our approach.
The third row presentsresults after the ILP phase is applied for the 10-best sequences, and we see that, in addition to theimproved link extraction described in Section 7,the performance on source extraction is substan-tially improved, from F-measure of 73.9 to 78.1.Performance on opinion expression extraction de-creases from F-measure of 81.9 to 78.8.
This de-crease is largely due to implicit links, which wewill explain below.
The fourth row takes the unionof the entities from ILP-SRL-f -10 and the entitiesfrom the best sequences from CRF-OP and CRF-SRC.
This process brings the F-measure of CRF-OP up to 82.0, with a different precision-recallbreak down from those of 1-best sequences with-out ILP phase.
In particular, the recall on opinionexpressions now reaches 82.3%, while maintain-ing a high precision of 81.7%.Overlap Match Exact Matchr(%) p(%) f(%) r(%) p(%) f(%)DEV.CONF 65.7 72.4 68.9 31.5 34.3 32.9NO.CONF 63.7 76.2 69.4 30.9 36.7 33.5Table 5: Relation extraction with ILP weight ad-justment.
(All cases using ILP+SRL-f -10)Effects of ILP weight adjustment Finally, weshow the effect of weight adjustment in the ILPformulation in Table 5.
The DEV.CONF row showsrelation extraction performance using a weightconfiguration based from the development data.In order to see the effect of weight adjustment,we ran an experiment, NO.CONF, using fixed de-fault weights.20 Not surprisingly, our weight ad-justment tuned from the development set is not theoptimal choice for cross-validation set.
Neverthe-less, the weight adjustment helps to balance theprecision and recall, i.e.
it improves recall at the20To be precise, cx = 1.0, c?x = 1.0 for x ?
{O, S, L},but cA = 0.2 is the same as before.438cost of precision.
The weight adjustment is moreeffective when the gap between precision and re-call is large, as was the case with the developmentdata.Implicit links A good portion of errors stemfrom the implicit link relation, which our systemdid not model directly.
An implicit link relationholds for an opinion entity without an associatedsource entity.
In this case, the opinion entity islinked to an implicit source.
Consider the follow-ing example.?
Anti-Soviet hysteria was firmly oppressed.Notice that opinion expressions such as ?Anti-Soviet hysteria?
and ?firmly oppressed?
do nothave associated source entities, because sources ofthese opinion expressions are not explicitly men-tioned in the text.
Because our system forceseach opinion to be linked with an explicit sourceentity, opinion expressions that do not have ex-plicit source entities will be dropped during theglobal inference phase of our system.
Implicitlinks amount to 7% of the link relations in ourcorpus, so the upper bound for recall for our ILPsystem is 93%.
In the future we will extend oursystem to handle implicit links as well.
Note thatwe report results against a gold standard that in-cludes implicit links.
Excluding them from thegold standard, the performance of our final sys-tem ILP+SRL-f -10 is 72.6% in recall, 72.4% inprecision, and 72.5 in F-measure.9 ConclusionThis paper presented a global inference approachto jointly extract entities and relations in the con-text of opinion oriented information extraction.The final system achieves performance levels thatare potentially good enough for many practicalNLP applications.Acknowledgments We thank the reviewers for theirmany helpful comments and Vasin Punyakanok for runningour data through his SRL system.
This work was sup-ported by the Advanced Research and Development Activity(ARDA), by NSF Grants IIS-0535099 and IIS-0208028, andby gifts from Google and the Xerox Foundation.ReferencesS.
Bethard, H. Yu, A. Thornton, V. Hativassiloglou andD.
Jurafsky 2004.
Automatic Extraction of Opin-ion Propositions and their Holders.
In AAAI SpringSymposium on Exploring Attitude and Affect in Text.R.
Bunescu and R. J. Mooney 2004.
Collective In-formation Extraction with Relational Markov Net-works.
In ACL.C.
Cardie, J. Wiebe, T. Wilson and D. Litman 2004.Low-Level Annotations and Summary Representa-tions of Opinions for Multi-Perspective QuestionAnswering.
New Directions in Question Answering.Y.
Choi, C. Cardie, E. Riloff and S. Patwardhan 2005.Identifying Sources of Opinions with ConditionalRandom Fields and Extraction Patterns.
In HLT-EMNLP.S.
Kim and E. Hovy 2005.
Automatic Detection ofOpinion Bearing Words and Sentences.
In IJCNLP.S.
Kim and E. Hovy 2005.
Identifying OpinionHolders for Question Answering in Opinion Texts.In AAAI Workshop on Question Answering in Re-stricted Domains.J.
Lafferty, A. K. McCallum and F. Pereira 2001 Con-ditional Random Fields: Probabilistic Models forSegmenting and Labeling Sequence Data.
In ICML.B.
Liu, M. Hu and J. Cheng 2005 Opinion Observer:Analyzing and Comparing Opinions on the Web.
InWWW.R.
J. Mooney and R. Bunescu 2005 Mining Knowl-edge from Text Using Information Extraction.
InSIGKDD Explorations.S.
Morinaga, K. Yamanishi, K. Tateishi and T.Fukushima 2002.
Mining product reputations onthe Web.
In KDD.M.
A. Munson, C. Cardie and R. Caruana.
2005.
Opti-mizing to arbitrary NLP metrics using ensemble se-lection.
In HLT-EMNLP.J.
Prager, E. Brown, A. Coden and D. Radev 2000.Question-answering by predictive annotation.
In SI-GIR.V.
Punyakanok, D. Roth and W. Yih 2005.
General-ized Inference with Multiple Semantic Role Label-ing Systems (Shared Task Paper).
In CoNLL.V.
Punyakanok, D. Roth, W. Yih and D. Zimak 2004.Semantic Role Labeling via Integer Linear Program-ming Inference.
In COLING.D.
Roth and W. Yih 2004.
A Linear Programming For-mulation for Global Inference in Natural LanguageTasks.
In CoNLL.D.
Roth and W. Yih 2002.
Probabilistic Reasoning forEntity and Relation Recognition.
In COLING.V.
Stoyanov, C. Cardie and J. Wiebe 2005.
Multi-Perspective Question Answering Using the OpQACorpus.
In HLT-EMNLP.C.
Sutton, K. Rohanimanesh and A. K. McCallum2004.
Dynamic Conditional Random Fields: Fac-torized Probabilistic Models for Labeling and Seg-menting Sequence Data.
In ICML.M.
White, T. Korelsky, C. Cardie, V. Ng, D. Pierce andK.
Wagstaff 2001.
Multi-document Summarizationvia Information Extraction In HLT.J.
Wiebe and T. Wilson and C. Cardie 2005.
Annotat-ing Expressions of Opinions and Emotions in Lan-guage.
In Language Resources and Evaluation, vol-ume 39, issue 2-3.T.
Wilson, J. Wiebe and P. Hoffmann 2005.
Recogniz-ing Contextual Polarity in Phrase-Level SentimentAnalysis.
In HLT-EMNLP.439
