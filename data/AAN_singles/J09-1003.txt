Evaluating Centering for InformationOrdering Using CorporaNikiforos Karamanis?University of CambridgeChris Mellish?
?University of AberdeenMassimo Poesio?University of EssexJon Oberlander?University of EdinburghIn this article we discuss several metrics of coherence defined using centering theory andinvestigate the usefulness of such metrics for information ordering in automatic text generation.We estimate empirically which is the most promising metric and how useful this metric is usinga general methodology applied on several corpora.
Our main result is that the simplest metric(which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformedby other metrics which make use of additional centering-based features.
This baseline can be usedfor the development of both text-to-text and concept-to-text generation systems.1.
IntroductionInformation ordering (Barzilay and Lee 2004), that is, deciding in which sequence topresent a set of preselected information-bearing items, has received much attention inrecent work in automatic text generation.
This is because text generation systems needto organize the content in a way that makes the output text coherent, that is, easy to readand understand.
The easiest way to exemplify coherence is by arbitrarily reordering thesentences of a comprehensible text.
This process very often gives rise to documents thatdo not make sense although the information content is the same before and after thereordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).Entity coherence, which is based on the way the referents of noun phrases (NPs)relate subsequent clauses in the text, is an important aspect of textual organization.Since the early 1980s, when it was first introduced, centering theory has been aninfluential framework for modelling entity coherence.
Seminal papers on centering suchas Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, andWeinstein (1995, page 215) suggest that centering may provide solutions for informationordering.Indeed, following the pioneering work of McKeown (1985), recent work on textgeneration exploits constraints on entity coherence to organize information (Mellishet al 1998; Kibble and Power 2000, 2004; O?Donnell et al 2001; Cheng 2002; Lapata?
Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK.Nikiforos.Karamanis@cl.cam.ac.uk.??
Department of Computing Science, King?s College, Aberdeen AB24 3UE, UK.?
Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK.?
School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication:7 January 2008.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 12003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others).
Although theseapproaches often make use of heuristics related to centering, the features of entitycoherence they employ are usually defined informally.
Additionally, centering-relatedfeatures are combined with other coherence-inducing factors in ways that are basedmainly on intuition, leaving many equally plausible options unexplored.Thus, the answers to the following questions remain unclear: (i) How appropriateis centering for information ordering in text generation?
(ii) Which aspects of centering aremost useful for this purpose?
These are the issues we investigate in this paper, whichpresents the first systematic evaluation of centering for information ordering.
To do this,we define centering-based metrics of coherence which are compatible with several extantinformation ordering approaches.
An important insight of our work is that centeringcan give rise to many such metrics of coherence.
Hence, a general methodologyfor identifying which of these metrics represent the most promising candidates forinformation ordering is required.We adopt a corpus-based approach to compare the metrics empirically anddemonstrate the portability and generality of our evaluation methods by experimentingwith several corpora.
Our main result is that the simplest metric (which reliesexclusively on NOCB transitions) sets a baseline that cannot be outperformed byother metrics that make use of additional centering-related features.
Thus, we providesubstantial insight into the role of centering as an information ordering constraint andoffer researchers working on text generation a simple, yet robust, baseline to use againsttheir own information ordering approaches during system development.The article is structured as follows: In Section 2 we discuss our information orderingapproach in relation to other work on text generation.
After a brief introductionto centering in Section 3, Section 4 demonstrates how we derived centering datastructures from existing corpora.
Section 5 discusses how centering can be used todefine various metrics of coherence suitable for information ordering.
Then, Section 6outlines a corpus-based methodology for choosing among these metrics.
Section 7reports on the results of our experiments and Section 8 discusses their implications.We conclude the paper with directions for future work and a summary of our maincontributions.12.
Information OrderingInformation ordering has been investigated by substantial recent work in text-to-text generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay andLee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji andPulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al 2007,among others) as well as concept-to-text generation (particularly Kan and McKeown[2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this workby presenting approaches to information ordering based on a genetic algorithm(Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, andKoller 2004) which can be applied to both concept-to-text and text-to-text generation.These approaches use a metric of coherence defined using features derived from1 Earlier versions of this work were presented in Karamanis et al (2004) and Karamanis (2006).2 Concept-to-text generation is concerned with the automatic generation of text from some underlyingnon-linguistic representation.
By contrast, the input to text-to-text generation applications is text.30Karamanis et al Centering for Information Orderingcentering and will serve as the premises of our investigation of centering in thisarticle.Metrics of coherence are used in other work on text generation, too (Mellish et al1998; Kibble and Power 2000, 2004; Cheng 2002).
With the exception of Kibble andPower?s work, the features of entity coherence used in these metrics are informallydefined using heuristics related to centering.
Additionally, the metrics are furtherspecified by combining these features with other coherence-inducing factors suchas rhetorical relations (Mann and Thompson 1987).
However, as acknowledged inmost of this work, these are preliminary computational investigations of the complexinteractions between different types of coherence which leave many other equallyplausible combinations unexplored.Clearly, one would like to know what centering can achieve on its own beforedevising more complicated metrics.
To address this question, we define metrics whichare purely centering-based, placing any attempt to specify a more elaborate model ofcoherence beyond the scope of this article.
This strategy is similar to most work oncentering for text interpretation in which additional constraints on coherence are nottaken into account (the papers in Walker, Joshi, and Prince [1998] are characteristicexamples).
This simplification makes it possible to assess for the first time how usefulthe employed centering features are for information ordering.Work on text generation which is solely based on rhetorical relations (Hovy 1988;Marcu 1997, among others) typically masks entity coherence under the ELABORATIONrelation.
However, ELABORATION has been characterized as ?the weakest of allrhetorical relations?
(Scott and de Souza 1990, page 60).
Knott et al (2001) identifiedseveral theoretical problems all related to ELABORATION and suggested that this relationbe replaced by a theory of entity coherence for text generation.
Our work builds on thissuggestion by investigating how appropriate centering is as a theory of entity coherencefor information ordering.McKeown (1985, pages 60?75) also deployed features of entity coherence toorganize information for text generation.
McKeown?s ?constraints on immediate focus?
(which are based on the model of entity coherence that was introduced by Sidner[1979] and precedes centering) are embedded within the schema-driven approach togeneration which is rather domain-specific (Reiter and Dale 2000).
By contrast, ourmetrics are general and portable across domains and can be applied within informationordering approaches which are applicable to both concept-to-text and text-to-textgeneration.3.
Centering OverviewThis section provides an overview of centering, focusing on the aspects which are mostclosely related to our work.
Poesio et al (2004) and Walker, Joshi, and Prince (1998)discuss centering and its relation to other theories of coherence in more detail.According to Grosz, Joshi, and Weinstein (1995), each utterance Un is assigned aranked list of forward looking centers (i.e., discourse entities) denoted as CF(Un).
Themembers of CF(Un) must be realized by the NPs in Un (Brennan, Friedman [Walker],and Pollard 1987).
The first member of CF(Un) is called the preferred centerCP(Un).The backward looking center CB(Un) links Un to the previous utterance Un?1.CB(Un) is defined as the most highly ranked member of CF(Un?1) which also belongsto CF(Un).
CF lists prior to CF(Un?1) are not taken into account for the computation31Computational Linguistics Volume 35, Number 1Table 1Centering transitions are defined according to whether the backward looking center, CB, isthe same in two subsequent utterances, Un?1 and Un, and whether the CB of the currentutterance, CB(Un), is the same as its preferred center, CP(Un).
These identity checks are alsoknown as the principles of COHERENCE and SALIENCE, the violations of which are denotedwith an asterisk.COHERENCE: COHERENCE?
:CB(Un)=CB(Un?1) CB(Un) =CB(Un?1)or CB(Un?1) undef.SALIENCE: CB(Un)=CP(Un) CONTINUE SMOOTH-SHIFTSALIENCE?
: CB(Un) =CP(Un) RETAIN ROUGH-SHIFTof CB(Un).
The original formulations of centering by Brennan, Friedman [Walker], andPollard (1987) and Grosz, Joshi, and Weinstein (1995) lay emphasis on the uniquenessand the locality of the CB and will serve as the foundations of our work.The CB and the CP are combined to define transitions across pairs ofadjacent utterances (Table 1).
This definition of transitions is based on Brennan,Friedman [Walker], and Pollard (1987) and has been popular with subsequent work.There exist several variations, however, the most important of which comes from Grosz,Joshi, and Weinstein (1995), who define only one SHIFT transition.3Centering makes two major claims about textual coherence, the first of whichis known as Rule 2.
Rule 2 states that CONTINUE is preferred to RETAIN, whichis preferred to SMOOTH-SHIFT, which is preferred to ROUGH-SHIFT.
Although theRule was introduced within an algorithm for anaphora resolution, Brennan, Friedman[Walker], and Pollard (1987, page 160) consider it to be relevant to text generationtoo.
Grosz, Joshi, and Weinstein (1995, page 215) also take Rule 2 to suggest thattext generation systems should attempt to avoid unfavorable transitions such asSHIFTs.The second claim, which is implied by the definition of the CB (Poesio et al 2004),is that CF(Un) should contain at least one member of CF(Un?1).
This became knownas the principle of CONTINUITY (Karamanis and Manurung 2002).
Although Grosz,Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discussthe effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) define theadditional transition NOCB to account for this case.
Different types of NOCB transitionsare introduced by Passoneau (1998) and Poesio et al (2004), among others.
Otherresearchers, however, consider the NOCB transition to be a type of ROUGH-SHIFT(Miltsakaki and Kukich 2004).Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE andSALIENCE, which correspond to the identity checks used to define the transitions(see Table 1).
To improve the way centering resolves pronominal anaphora, Strubeand Hahn (1999) introduced a fourth principle called CHEAPNESS and defined it asCB(Un)=CP(Un?1).
They also redefined Rule 2 to favor transition pairs which satisfy3 ?CB(Un?1) undef.?
in Table 1 stands for the cases where Un?1 does not have a CB.
Instead of classifyingthe transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENTis sometimes used (Kameyama 1998; Poesio et al 2004).32Karamanis et al Centering for Information OrderingCHEAPNESS over those which violate it.
This means that CHEAPNESS is given priorityover every other centering principle in Strube and Hahn?s model.In addition to the variability caused by the numerous definitions of transitions andthe introduction of the various principles, parameters such as ?utterance,?
?ranking,?and ?realization?
can also be specified in several ways giving rise to differentinstantiations of centering (Poesio et al 2004).
The following section discusses how theseparameters were defined in the corpora we deploy.4.
Experimental DataWe made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOMEcorpus (Poesio et al 2004), and the two corpora that Barzilay and Lapata (2005)experimented with.
In this section, we discuss how the centering representations weutilize were derived from each corpus.4.1 The MPIRO-CF CorpusDimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from thedatabase of the MPIRO concept-to-text generation system (Isard et al 2003), realizedthem as sentences, and organized them in sets.
Each set consisted of six facts whichwere ordered by a domain expert.
The orderings produced by this expert were shownto be very close to those produced by two other archeologists (Karamanis and Mellish2005b).Our first corpus, MPIRO-CF, consists of 122 orderings that were made availableto us by D&A.
We computed a CF list for each fact in each ordering by applying theinstantiation of centering introduced by Kibble and Power (2000, 2004) for concept-to-text generation.
That is, we took each database fact to correspond to an ?utterance?and specified the ?realization?
parameter using the arguments of each fact as themembers of the corresponding CF list.
Table 2 shows the CF lists, the CBs, thecentering transitions, and the violations of CHEAPNESS for the following example fromMPIRO-CF:(1) (a) This exhibit is an amphora.
(b) This exhibit was decorated by the Painter of Kleofrades.
(c) The Painter of Kleofrades used to decorate big vases.
(d) This exhibit depicts a warrior performing splachnoscopy before leaving for thebattle.
(e) This exhibit is currently displayed in the Martin von Wagner Museum.
(f) The Martin von Wagner Museum is in Germany.MPIRO facts consist of two arguments, the first of which was specified as the CPfollowing the definition of ?CF ranking?
in O?Donnell et al (2001).4 Notice that thesecond argument can often be an entity such as en914 that is realized by a canned phraseof significant syntactic complexity (a warrior performing splachnoscopy before leaving forthe battle).
Moreover, the deployed definition of ?realization?
is similar to what Grosz,4 This is the main difference between our approach and that of Kibble and Power, who allow for more thanone potential CP in their CF lists.33Computational Linguistics Volume 35, Number 1Table 2The CF list, the CB, NOCB, or centering transition (see Table 1) and violations of CHEAPNESS(denoted with an asterisk) for each fact in Example (1) from the MPIRO-CF corpus.Fact CF list: next referent} CB Transition CHEAPNESS{CP, CBn=CPn?1(1a) {ex1, amphora} n.a.
n.a.
n.a.
(1b) {ex1, paint-of-kleofr} ex1 CONTINUE ?
(1c) {paint-of-kleofr, en404} paint-of-kleofr SMOOTH-SHIFT ?
(1d) {ex1, en914} ?
NOCB n.a.
(1e) {ex1, wagner-mus} ex1 CONTINUE ?
(1f) {wagner-mus, germany} wagner-mus SMOOTH-SHIFT ?Joshi, and Weinstein (1995) call ?direct realization,?
which ignores potential bridgingrelations (Clark 1977) between the members of two subsequent CF lists.
These relationsare typically not taken into account for information ordering and were not consideredin any of the deployed corpora.4.2 The GNOME-LAB CorpusWe also made use of the GNOME corpus (Poesio et al 2004), which contains objectdescriptions (museum labels) reliably annotated with features relevant to centering.The motivation for this study was to examine whether the phenomena observed inMPIRO-CF (which is arguably somewhat artificial) also manifest in texts from thesame genre written by humans without the constraints imposed by a text generationsystem.Based on the definition of museum labels in Cheng (2002, page 65), we identified20 such texts in GNOME, which were published in a book and a museum Web site (andwere thus taken to be coherent).
The following example is a characteristic text from thissubcorpus (referred to here as GNOME-LAB):(2) (a) Item 144 is a torc.
(b) Its present arrangement, twisted into three rings, may be a modern alteration;(c) it should probably be a single ring, worn around the neck.
(d) The terminals are in the form of goats?
heads.The GNOME corpus provides us with reliable annotation of discourse units (i.e.,clauses and sentences) that can be used for the computation of ?utterance?
and ofNPs which introduce entities to the CF list.
Each feature was marked up by atleast two annotators and agreement was checked using the ?
statistic on part of thecorpus.In order to avoid deviating too much from the MPIRO application domain, wecomputed the CF lists from the units that seemed to correspond more closely to MPIROfacts.
So instead of using sentence for the definition of ?utterance,?
we followed mostwork on centering for English and computed CF lists from GNOME?s finite units.5 The5 This definition includes titles which do not always have finite verbs, but excludes finite relative clauses,the second element of coordinated VPs and clause complements which are often taken as not having theirown CF lists in the centering literature.34Karamanis et al Centering for Information OrderingTable 3First two members of the CF list, the CB, NOCB, or centering transition (see Table 1) andviolations of CHEAPNESS (denoted with an asterisk) for each finite unit in Example (2) from theGNOME-LAB corpus.Unit CF list: next referent} CB Transition CHEAPNESS{CP, CBn=CPn?1(2a) {de374, de375} n.a.
n.a.
n.a.
(2b) {de376, de374, ... } de374 RETAIN ?
(2c) {de374, de379, ... } de374 CONTINUE ?
(2d) {de380, de381, ... } ?
NOCB n.a.text spans with the indexes (a) to (d) in Example (2) are examples of such units.
Unitssuch as (2a) are as simple as the MPIRO-generated sentence (1a), whereas others appearto be of similar syntactic complexity to (1d).
On the other hand, the second sentence inExample (2) consists of two finite units, namely (b) and (c), and appears to correspondto higher degrees of aggregation than is typically seen in an MPIRO fact.
The texts inGNOME-LAB consist of 8.35 finite units on average.Table 3 shows the first two members of the CF list, the CB, the transitions, and theviolations of CHEAPNESS for Example (2).
Note that the same entity (i.e., de374) is usedto denote the referent of the NP Item 144 in (2a) and its in (2b), which is annotated ascoreferring with Item 144.
All annotated NPs introduce referents to the CF list (whichoften contains more entities than in MPIRO), but only direct realization is used for thecomputation of the list.
This means that, similarly to the MPIRO domain, bridgingrelations between, for example, it in (2c) and the terminals in (2d), are not taken intoaccount.The members of the CF list were ranked by combining grammatical function withlinear order, which is a robust way of estimating ?CF ranking?
in English (Poesio et al2004).
In this instantiation, the CP corresponds to the referent of the first NP within theunit that is annotated as a subject or as the post-copular NP in a there-clause.4.3 The NEWS and ACCS CorporaBarzilay and Lapata (2005) presented a probabilistic approach for information orderingwhich is particularly suitable for text-to-text generation and is based on a newrepresentation called the entity grid.
A collection of 200 articles from the North AmericanNews Corpus (NEWS) and 200 narratives of accidents from the National TransportationSafety Board database (ACCS) was used for training and evaluation.
Example (3)presents a characteristic text from the NEWS corpus:(3) (a) [The Justice Department]S is conducting [an anti-trust trial]O against [MicrosoftCorp.
]X with [evidence]X that [the company]S is increasingly attempting to crush[competitors]O.
(b) [Microsoft]O is accused of trying to forcefully buy into [markets]X where [itsown products]S are not competitive enough to unseat [established brands]O.
(c) [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring[Netscape]O into merging [browser software]O.
(d) [Microsoft]S claims [its tactics]S are commonplace and good economically.35Computational Linguistics Volume 35, Number 1Table 4Fragment of the entity grid for Example (3).
The grammatical function of the referents in eachsentence is reported using S, O, and X (for subject, object, and other).
The symbol ???
is used forreferents which do not occur in the sentence.ReferentsSentences department trial microsoft evidence ... products brands ...(3a) S O S X ... ?
?
...(3b) ?
?
O ?
... S O ...(3c) ?
?
S O ... ?
?
...(3d) ?
?
S ?
... ?
?
...(3e) ?
?
?
?
... ?
?
...(3f) ?
X S ?
... ?
?
...(e) [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb[competition]O through [collusion]X is [a violation]O of [the Sherman Act]X.
(f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X.Barzilay and Lapata automatically annotated their corpora for the grammatical functionof the NPs in each sentence (denoted in the example by the subscripts S, O, andX for subject, object, and other, respectively) as well as their coreferential relations(which do not include bridging references).
More specifically, they used a parser(Collins 1997) to determine the constituent structure of the sentences from which thegrammatical function for each NP was derived.6 Coreferential NPs such as MicrosoftCorp.
and the company in (3a) were identified using the system of Ng and Cardie(2002).The entity grid is a two-dimensional array that captures the distribution of NPreferents across sentences in the text using the aforementioned symbols for theirgrammatical role and the symbol ???
for a referent that does not occur in a sentence.Table 4 illustrates a fragment of the grid for the sentences in Example (3).7Barzilay and Lapata use the grid to compute models of coherence that areconsiderably more elaborate than centering.
To derive an appropriate instantiation ofcentering for our investigation, we compute a CF list for each grid row using thereferents with the symbols S, O, and X.
These referents are ranked according to theirgrammatical function and their position in the text.
This definition of ?CF ranking?
issimilar to the one we use in GNOME-LAB.
For instance, department is ranked higherthan microsoft in CF(3a) because the Justice Department is mentioned before MicrosoftCorp.
in the text.
The derived sequence of CF lists is used to compute the additionalcentering data structures shown in Table 5.The average number of sentences per text is 10.4 in NEWS and 11.5 in ACCS.As we explain in the next section, our centering-based metrics of coherence can be6 They also used a small set of patterns to recognize passive verbs and annotate arguments involved inpassive constructions with their underlying grammatical function.
This is why Microsoft is marked withthe role O in sentence (3b).7 If a referent such as microsoft is attested by several NPs in the same sentence, for example, MicrosoftCorp.
and the company in (3a), the role with the highest priority (in this case S) is used to represent it.36Karamanis et al Centering for Information OrderingTable 5First two members of the CF list, the CB, NOCB, or centering transitions (see Table 1) andviolations of CHEAPNESS (denoted with an asterisk) for Example (3) from the NEWS corpus.Sentence CF list: next referent} CB Transition CHEAPNESS{CP, CBn=CPn?1(3a) {department, microsoft, ...} n.a.
n.a.
n.a.
(3b) {products, microsoft, ...} microsoft RETAIN ?
(3c) {microsoft, case, ...} microsoft CONTINUE ?
(3d) {microsoft, tactics} microsoft CONTINUE ?
(3e) {government, conspiracy, ...} ?
NOCB n.a.
(3f) {microsoft, earnings, ... } ?
NOCB n.a.deployed directly on unseen texts, so we treated all texts in NEWS and ACCS as testdata.85.
Computing Centering-Based Metrics of CoherenceFollowing our previous work (Karamanis and Manurung 2002; Althaus, Karamanis,and Koller 2004), the input to information ordering is an unordered set of information-bearing items represented as CF lists.
A set of candidate orderings is produced bycreating different permutations of these lists.
A metric of coherence uses features fromcentering to compute a score for each candidate ordering and select the highest scoringordering as the output.9A wide range of metrics of coherence can be defined in centering?s terms, simplyon the basis of the work we reviewed in Section 3.
To exemplify this, let us first assumethat the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5,is a candidate ordering.
Table 6 summarizes the NOCBs, the violations of COHERENCE,SALIENCE, and CHEAPNESS, and the centering transitions for this ordering.10The candidate ordering contains two NOCBs in sentences (3e) and (3f).
Its scoreaccording to M.NOCB, the metric used by Karamanis and Manurung (2002) andAlthaus, Karamanis, and Koller (2004), is 2.
Another ordering with fewer NOCBs (shouldsuch an ordering exist) will be preferred over this candidate as the selected output ofinformation ordering if M.NOCB is used to guide this process.
M.NOCB relies only onCONTINUITY.
Because satisfying this principle is a prerequisite for the computation ofevery other centering feature, M.NOCB is the simplest possible centering-based metricand will be used as the baseline in our experiments.According to Strube and Hahn (1999) the principle of CHEAPNESS is the mostimportant centering feature for anaphora resolution.
We are interested in assessing howsuitable M.CHEAP, a metric which utilizes CHEAPNESS, is for information ordering.CHEAPNESS is violated twice according to Table 6 so the score of the candidate ordering8 By contrast, Barzilay and Lapata used 100 texts in each domain to train their models and reserved theother 100 for testing them.9 If the best coherence score is assigned to several candidate orderings, then the information orderingalgorithm will choose randomly between them.10 Principles and transitions will be collectively referred to as ?features?
from now on.37Computational Linguistics Volume 35, Number 1Table 6Violations of CONTINUITY (NOCB), COHERENCE, SALIENCE, and CHEAPNESS and centeringtransitions for Example (3), based on the analysis in Table 5.
The table reports the sentencesmarked with each centering feature: That is, sentences (3e) and (3f) are classified as NOCBs, andso on.CONTINUITY?
COHERENCE?
SALIENCE?
CHEAPNESS?NOCB: CBn = CBn?1: CBn = CPn: CBn = CPn?1:(3e), (3f) ?
(3b) (3b), (3c)CONTINUE: RETAIN: SMOOTH-SHIFT: ROUGH-SHIFT:(3c), (3d) (3b) ?
?according to M.CHEAP is 2.11 If another candidate ordering with fewer violations ofCHEAPNESS exists, it will be chosen as a preferred output according to M.CHEAP.M.BFP employs the transition preferences of Rule 2 as specified by Brennan,Friedman [Walker], and Pollard (1987).
The first score to be computed by M.BFP isthe sum of CONTINUE transitions, which is 2 for the candidate ordering according toTable 6.
If this ordering is found to score higher than every other candidate ordering forthe number of CONTINUEs, it is selected as the output.
If another ordering is found tohave the same number of CONTINUEs, the sum of RETAINs is examined, and so forth forthe other two types of centering transitions.12M.KP, the metric deployed by Kibble and Power (2000) in their text generationsystem, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE,and SALIENCE, preferring the ordering with the lowest total cost.
In addition tothe violations of CONTINUITY and CHEAPNESS, the candidate ordering also violatesSALIENCE once, so its score according to M.KP is 5.
An alternative ordering with alower score (if any) will be preferred by this metric.
Although Kibble and Power (2004)introduced a weighted version of M.KP, the exact weighting of centering?s principlesremains an open question, as argued by Kibble (2001).
This is why we decided toexperiment with M.KP instead of its weighted variant.In the remainder of the paper, we take forward the four metrics motivated in thissection as the most appropriate starting point for experimentation.
We would like toemphasize, however, that these are not the only possible options.
Indeed, similarly tothe various ways in which centering?s parameters can be specified, there exist manyother ways of using centering to define metrics of entity coherence for informationordering.
These possibilities arise from the numerous other definitions of centering?stransitions and the various ways in which transitions and principles can be combined.These are explored in more detail in Karamanis (2003, Chapter 3), which also providesa formal definition of the metrics discussed previously.6.
Evaluation MethodologyBecause using naturally occurring discourse in psycholinguistic studies to investigatecoherence effects is almost infeasible, computational corpus-based experiments are11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS.12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for thedefinition of transitions in M.BFP.38Karamanis et al Centering for Information Orderingoften the most viable alternative (Poesio et al 2004; Barzilay and Lee 2004).
Corpus-based evaluation can be usefully employed during system development and maybe later supplemented by less extended evaluation based on human judgments assuggested by Lapata (2006).The corpus-based methodology of Karamanis (2003) served as our experimentalframework.
This methodology is based on the premise that the original sentence order(OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any otherordering.
If a metric takes an alternative ordering to be more coherent than the OSO, ithas to be penalized.Karamanis (2003) introduced a performance measure called the classification errorrate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2.Better(M,OSO) stands for the percentage of orderings that score better than the OSOaccording to a metric M, and Equal(M,OSO) is the percentage of orderings that scoreequal to the OSO.13 This measure provides an indication of how likely a metric is to leadto an ordering different from the OSO.
When comparing several metrics with each other,the one with the lowest classification error rate is the most appropriate for orderingthe sentences that the OSO consists of.
In other words, the smaller the classificationerror rate, the better a metric is expected to perform for information ordering.
Theaverage classification error rate is used to summarize the performance of each metric ina corpus.To compute the classification error rate we permute the CF lists of the OSO andclassify each alternative ordering as scoring better, equal, or worse than the OSOaccording to M. When the number of CF lists in the OSO is fairly small, it is feasibleto search through all possible orderings.
For OSOs consisting of more than 10 CFlists, the classification error rate for the entire population of orderings can be reliablyestimated using a random sample of one million permutations (Karamanis 2003,Chapter 5).7.
ResultsTable 7 shows the average performance of each metric in the corpora employed in ourexperiments.
The smallest?that is, best?score in each corpus is printed in boldface.The table indicates that the baseline M.NOCB performs best in three out of four corpora.The experimental results of the pairwise comparisons of M.NOCB with each ofM.CHEAP, M.KP, and M.BFP in each corpus are reported in Table 8.
The exact numberof texts for which the classification error rate of M.NOCB is lower than its competitor foreach comparison is reported in the columns headed by ?lower.?
For instance, M.NOCBhas a lower classification error rate than M.CHEAP for 110 (out of 122) texts fromMPIRO-CF.
M.CHEAP achieves a lower classification error rate for just 12 texts, andthere do not exist any ties, that is, cases in which the classification error rate of the twometrics is the same.The p value returned by the two-tailed Sign Test for the difference in the numberof texts in each corpus, rounded to the third decimal place, is also reported.14 With13 Weighting Equal(M,OSO) by 0.5 is based on the assumption that, similarly to tossing a coin, the OSO willon average do better than half of the orderings that score the same as it does when other coherenceconstraints are considered.14 The Sign Test was chosen over its parametric alternatives to test significance because it does not carryspecific assumptions about population distributions and variance.39Computational Linguistics Volume 35, Number 1Table 7Average classification error rate for the centering-based metrics in each corpus.CorpusMetric MPIRO-CF GNOME-LAB NEWS ACCS MeanM.NOCB 20.42 19.95 30.90 15.51 21.70M.BFP 19.91 33.01 37.90 21.20 28.01M.KP 53.15 58.22 57.70 55.60 56.12M.CHEAP 81.04 57.23 64.60 76.29 69.79No.
of texts 122 20 200 200Table 8Comparing M.NOCB with M.CHEAP, M.KP, and M.BFP in each corpus.MPIRO-CF GNOME-LABM.NOCB M.NOCBlower greater ties p lower greater ties pM.CHEAP 110 12 0 <0.001 18 2 0 <0.001M.KP 103 16 3 <0.001 16 2 2 0.002M.BFP 42 31 49 0.242 12 3 5 0.036No.
of texts 122 20NEWS ACCSM.NOCB M.NOCBlower greater ties p lower greater ties pM.CHEAP 155 44 1 <0.001 183 17 0 <0.001M.KP 131 68 1 <0.001 167 33 0 <0.001M.BFP 121 71 8 <0.001 100 100 0 1.000No.
of texts 200 200respect to the exemplified comparison of M.NOCB against M.CHEAP in MPIRO-CF,the p value is lower than 0.001 after rounding.
This in turn means that M.NOCBreturns a better classification error rate for significantly more texts in MPIRO-CFthan M.CHEAP.
In other words, M.NOCB outperforms M.CHEAP significantly in thiscorpus.Notably, M.NOCB performs significantly better than its competitor in 10 out of12 cases.15 In the remaining two comparisons, the difference in performance betweenM.NOCB and M.BFP is not significant (p > 0.05).
However, this does not constituteevidence against M.NOCB, the simplest of the investigated metrics.
In fact, becauseM.BFP fails to outperform the baseline, the latter may be considered as the mostpromising solution for information ordering in these cases too by applying Occam?srazor.
Thus, M.NOCB is shown to be the best performing metric across all fourcorpora.15 This result is significant too according to the two-tailed Sign Test (p < 0.05).40Karamanis et al Centering for Information Ordering8.
DiscussionOur experiments show that M.NOCB is the most suitable metric for informationordering among the metrics we experimented with.
Despite the differences between ourcorpora (in genre, average length, syntactic complexity, number of referents in the CFlist, etc.
), M.NOCB proves robust across all four of them.
It is also the most appropriatemetric to use in both application areas we relate our corpora to, namely concept-to-text(MPIRO-CF and GNOME-LAB) as well as text-to-text (NEWS and ACCS) generation.These results indicate that when purely centering-based metrics are used, simplyavoiding NOCBs is more relevant to information ordering than the combinations ofadditional centering features that the other metrics make use of.In this section, we compare our work with other recent evaluation studies,including the corpus-based investigation of centering by Poesio et al (2004); discussthe implications of our findings for text generation; and summarize our contributions.8.1 Recent Evaluation Studies in Information OrderingThere has been significant recent work on the corpus-based evaluation for informationordering.
In this section, we discuss the methodological differences between our workand the studies which are most closely related to it.Barzilay and Lee (2004) introduce a stochastic model for information orderingwhich computes the probability of generating the OSO and every alternative ordering.Then, all orderings are ranked according to this probability and the rank given to theOSO is retrieved.
Several evaluation measures are discussed, the most important ofwhich is the average OSO rank, that is, the average rank of the OSOs in their corpora.This measure does not take into account that the OSOs differ in length.
However, thisinformation is necessary to estimate reliably the performance of an information orderingapproach, as we discuss in Karamanis and Mellish (2005a) in more detail.Barzilay and Lapata (2005) overcome this difficulty by introducing a performancemeasure called ranking accuracywhich expresses the percentage of alternative orderingsthat are ranked lower than the OSO.
In Karamanis?s (2003) terms, ranking accuracyequals 100% ?
Better(M,OSO), assuming that no equally ranking orderings exist.16Barzilay and Lapata (2005) compare the OSO with just 20 alternative orderings,often sampled out of several millions.
On the other hand, Barzilay and Lee (2004)enumerate exhaustively each possible ordering, which might become impractical as thesearch space grows factorially.
We overcame these problems by using a large randomsample for the texts which consist of more than 10 sentences as suggested in Karamanis(2003, Chapter 5).
Equally important is the emphasis we placed on the use of statisticaltests, which were not deployed by either Barzilay and Lee or Barzilay and Lapata.Lapata (2003) presented a methodology for automatically evaluating generatedorderings on the basis of their distance from observed sentence orderings in a corpus.A measure of rank correlation (called Kendall?s ?
), which was subsequently shown tocorrelate reliably with human ratings and reading times (Lapata 2006), was used toestimate the distance between orderings.16 Neither Barzilay and Lapata (2005) nor Barzilay and Lee (2004) appear to consider the possibility that twoorderings may be equally ranked.41Computational Linguistics Volume 35, Number 1Whereas ?
estimates how close the predictions of a metric are to several originalorderings, we measure how likely a metric is to lead to an ordering different than theOSO.
Taking into account more than one OSO for information ordering is the mainstrength of Lapata?s method, but to do this one needs to ask several humans to order thesame set of sentences (Madnani et al 2007).
Karamanis and Mellish (2005b) conductedan experiment in the MPIRO domain using Lapata?s methodology which supplementsthe work reported in this article.
However, such an approach is less practical for muchlarger collections of texts such as NEWS and ACCS.
This is presumably the reason whyBarzilay and Lapata (2005) use ranking accuracy instead of ?
in their evaluation.8.2 Previous Corpus-Based Evaluations of CenteringOur work investigates how the coherence score of the OSO compares to the scoresof alternative orderings of the sentences that the OSO consists of.
As Kibble (2001,page 582) noticed, this question is crucial from an information ordering viewpoint, butwas not taken into account by any previous corpus-based study of centering.
Grosz,Joshi, and Weinstein (1995, page 215) also suggested that Rule 2 should be tested byexamining ?alternative multi-utterance sequences that differentially realize the samecontent.?
We are the first to have pursued this research objective in the evaluation ofcentering for information ordering.Poesio et al (2004) observed that there remained a large number of NOCBs underevery instantiation of centering they tested and concluded that centering is inadequateas a coherence model.17 However, the frequency of NOCBs does not necessarily provideadequate indication of how appropriate NOCBs (and centering in general) are forinformation ordering.
Although over 50% of the transitions in GNOME-LAB are NOCBs,the average classification error rate of approximately 20% for M.NOCB suggests that theOSO tends to be in greater agreement with the preference to avoid NOCBs than 80% ofthe alternative orderings.
Thus, it appears that the observed ordering in the corpus doesoptimize with respect to the number of potential NOCBs to a great extent.8.3 A Simple and Robust Baseline for Text GenerationHow likely is M.NOCB to come up with the attested ordering in the corpus (the OSO)if it is actually used to guide an algorithm that orders the CF lists in our corpora?The average classification error rates (Table 7) estimate exactly this variable.
Theperformance of M.NOCB varies across the corpora from about 15.5% (ACCS) to 30.9%(NEWS).
We attribute this variation to the aforesaid differences between the corpora.Notice, however, that these differences affect all metrics in a similar way, not allowingfor another metric to significantly outperform M.NOCB.Noticeably, even in ACCS, for which M.NOCB achieves its best performance,approximately one out of six alternative orderings on average are taken to be morecoherent than the OSO.
Given the average number of sentences per text in this corpus17 We viewed the definition of the centering instantiation as being related to the application domain, as weexplained in Section 4.
This is why, unlike Poesio et al, we did not experiment with differentinstantiations of centering on the same data.42Karamanis et al Centering for Information Ordering(11.5), this means that several millions of alternative orderings are often taken to bemore coherent than the gold standard.Barzilay and Lapata (2005) report an average ranking accuracy of 87.3% for theirbest sentence ordering method in ACCS.
This corresponds to an average classificationerror rate of 12.7% (assuming that there are no equally scoring orderings in theirevaluation; see Section 8.1).
This is equal to an improvement of just 2.8% overthe performance of our baseline metric (15.5%) using a coherence model which issubstantially more elaborate than centering.
However, it is in NEWS (for whichM.NOCB returns its worst performance of 30.9%) that this model shows its real strength,approximating an average classification error rate of 9.6%, which corresponds to animprovement of 21.3% over our baseline.
We believe that the experiments reported inthis article put the studies of our colleagues in better perspective by providing a reliablebaseline to compare their metrics against.8.4 Moving Beyond Centering-Based MetricsFollowing McKeown (1985), Kibble and Power argue in favor of an integrated approachfor concept-to-text generation in which the same centering features are used at differentstages in the generation pipeline.
However, our study suggests that features such asCHEAPNESS and the centering transitions are not particularly relevant to informationordering.
The poor performance of these features can be explained by the fact that theywere originally introduced to account for pronoun resolution rather than informationordering.
CONTINUITY, on the other hand, captures a fundamental intuition about entitycoherence which constitutes part of several other discourse theories.18CONTINUITY, however, captures just one aspect of coherence.
This explains therelatively high classification error rates for M.NOCB, which needs to be supplementedwith other coherence-inducing factors in order to be used in practice.
This verifies thepremises of researchers such as Kibble and Power who a priori use features derivedfrom centering in combination with other factors in the definition of their metrics.
Ourwork should be quite helpful for that effort too, suggesting that M.NOCB is a betterstarting point for defining such metrics than M.CHEAP or M.KP.9.
ConclusionIn conclusion, our analysis sheds more light on two previously unaddressed questionsin the corpus-based evaluation of centering: (i) which aspects of centering are mostrelevant to information ordering and (ii) to what extent centering on its own can beuseful for this purpose.
We have shown that the metric which relies exclusively on NOCBtransitions (M.NOCB) sets a baseline that cannot be outperformed by other coherencemetrics which make use of additional centering features.
Although this metric does notperform well enough to be used on its own, it constitutes a simple, yet robust, baselineagainst which more elaborate information ordering approaches can be tested duringsystem development in both text-to-text and concept-to-text generation.This work can be extended in numerous ways.
For instance, given the abundanceof possible centering-based metrics one may investigate whether a different metric can18 We thank one anonymous reviewer for suggesting this explanation of our results.43Computational Linguistics Volume 35, Number 1outperform M.NOCB in any corpus or application domain.
M.NOCB can also serve asthe starting point for the definition of more informed metrics which will incorporateadditional coherence-inducing factors.
Finally, given that we used the instantiationof centering which seemed to correspond more closely to the targeted applicationdomains, the extent to which computing the CF list in a different way may affect theperformance of the metrics is another question to explore in future work.AcknowledgmentsMany thanks to Aggeliki Dimitromanolaki,Mirella Lapata, and Regina Barzilay for theirdata; to David Schlangen, Ruli Manurung,James Soutter, and Le An Ha forprogramming solutions; and to Ruth Sealand two anonymous reviewers for theircomments.
Nikiforos Karamanis receivedsupport from the Greek State ScholarshipsFoundation (IKY) as a PhD student inEdinburgh as well as the Rapid ItemGeneration project and the BBSRC-fundedFlySlip grant (No 38688) as a postdoc inWolverhampton and Cambridge,respectively.ReferencesAlthaus, Ernst, Nikiforos Karamanis, andAlexander Koller.
2004.
Computing locallycoherent discourses.
In Proceedings of ACL2004, pages 399?406, Barcelona.Barzilay, Regina, Noemie Elhadad, andKathleen McKeown.
2002.
Inferringstrategies for sentence ordering inmultidocument news summarization.Journal of Artificial Intelligence Research,17:35?55.Barzilay, Regina and Mirella Lapata.
2005.Modeling local coherence: An entity-basedapproach.
In Proceedings of ACL 2005,pages 141?148, Ann Arbor, MI.Barzilay, Regina and Lillian Lee.
2004.Catching the drift: Probabilistic contentmodels with applications to generationand summarization.
In Proceedings ofHLT-NAACL 2004, pages 113?120,Boston, MA.Beaver, David.
2004.
The optimization ofdiscourse anaphora.
Linguistics andPhilosophy, 27(1):3?56.Bollegala, Danushka, Naoaki Okazaki, andMitsuru Ishizuka.
2006.
A bottom-upapproach to sentence ordering formulti-document summarization.
InProceedings of ACL-COLING 2006,pages 385?392, Sydney.Brennan, Susan E., Marilyn A.Friedman [Walker], and Carl J. Pollard.1987.
A centering approach to pronouns.In Proceedings of ACL 1987, pages 155?162,Stanford, CA.Cheng, Hua.
2002.
Modelling AggregationMotivated Interactions in Descriptive TextGeneration.
Ph.D. thesis, Division ofInformatics, University of Edinburgh.Clark, Herbert.
H. 1977.
Bridging.
In P. N.Johnson-Laird and P. C. Wason, editors,Thinking: Readings in Cognitive Science.Cambridge University Press, Cambridge,pages 9?27.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of ACL-EACL 1997,pages 16?23, Madrid.Dimitromanolaki, Aggeliki and IonAndroutsopoulos.
2003.
Learning to orderfacts for discourse planning in naturallanguage generation.
In Proceedings ofENLG 2003, pages 23?30, Budapest.Grosz, Barbara J., Aravind K. Joshi, and ScottWeinstein.
1995.
Centering: A frameworkfor modeling the local coherence ofdiscourse.
Computational Linguistics,21(2):203?225.Hovy, Eduard.
1988.
Planning coherentmultisentential text.
In Proceedings of ACL1988, pages 163?169, Buffalo, NY.Isard, Amy, Jon Oberlander, IonAndroutsopoulos, and Colin Matheson.2003.
Speaking the users?
languages.
IEEEIntelligent Systems Magazine, 18(1):40?45.Ji, Paul and Stephen Pulman.
2006.
Sentenceordering with manifold-basedclassification in multi-documentsummarization.
In Proceedings of EMNLP2006, pages 526?533, Sydney.Kameyama, Megumi.
1998.
Intrasententialcentering: A case study.
In Walker, Joshi,and Prince 1998, pages 89?122.Kan, Min-Yen and Kathleen McKeown.
2002.Corpus-trained text generation forsummarization.
In Proceedings of INLG2002, pages 1?8, Harriman, NY.Karamanis, N. 2006.
Evaluating centering forinformation ordering in two new domains.In Proceedings of NAACL 2006, CompanionVolume, pages 65?68, New York.Karamanis, N., M. Poesio, C. Mellish, andJ.
Oberlander.
2004.
Evaluatingcentering-based metrics of coherence using44Karamanis et al Centering for Information Orderinga reliably annotated corpus.
In Proceedingsof ACL 2004, pages 391?398, Barcelona.Karamanis, Nikiforos.
2003.
Entity Coherencefor Descriptive Text Structuring.
Ph.D.thesis, Division of Informatics, Universityof Edinburgh.Karamanis, Nikiforos and Hisar MaruliManurung.
2002.
Stochastic textstructuring using the principle ofcontinuity.
In Proceedings of INLG 2002,pages 81?88, Harriman, NY.Karamanis, Nikiforos and Chris Mellish.2005a.
A review of recent corpus-basedmethods for evaluating informationordering in text production.
In Proceedingsof Corpus Linguistics 2005 Workshop onUsing Corpora for NLG, pages 13?18,Birmingham.Karamanis, Nikiforos and Chris Mellish.2005b.
Using a corpus of sentenceorderings defined by many experts toevaluate metrics of coherence for textstructuring.
In Proceedings of ENLG 2005,pages 174?179, Aberdeen.Kibble, Rodger.
2001.
A reformulation of rule2 of centering theory.
ComputationalLinguistics, 27(4):579?587.Kibble, Rodger and Richard Power.
2000.An integrated framework for textplanning and pronominalisation.
InProceedings of INLG 2000, pages 77?84,Mitzpe Ramon.Kibble, Rodger and Richard Power.
2004.Optimizing referential coherence in textgeneration.
Computational Linguistics,30(4):401?416.Knott, Alistair, Jon Oberlander, MickO?Donnell, and Chris Mellish.
2001.Beyond elaboration: The interaction ofrelations and focus in coherent text.
InT.
Sanders, J. Schilperoord, andW.
Spooren, editors, Text Representation:Linguistic and Psycholinguistic Aspects.John Benjamins, Amsterdam, chapter 7,pages 181?196.Lapata, Mirella.
2003.
Probabilistic textstructuring: Experiments with sentenceordering.
In Proceedings of ACL 2003,pages 545?552, Sapporo.Lapata, Mirella.
2006.
Automatic evaluationof information ordering: Kendall?s tau.Computational Linguistics, 32(4):1?14.Madnani, Nitin, Rebecca Passonneau,Necip Fazil Ayan, John Conroy, BonnieDorr, Judith Klavans, Dianne O?Leary, andJudith Schlesinger.
2007.
Measuringvariability in sentence ordering for newssummarization.
In Proceedings of ENLG2007, pages 81?88, Schloss Dagstuhl.Mann, William C. and Sandra A. Thompson.1987.
Rhetorical structure theory: A theoryof text organisation.
Technical ReportRR-87-190, University of SouthernCalifornia / Information Sciences Institute.Marcu, Daniel.
1997.
The Rhetorical Parsing,Summarization and Generation of NaturalLanguage Texts.
Ph.D. thesis, University ofToronto.McKeown, Kathleen.
1985.
Text Generation:Using Discourse Strategies and FocusConstraints to Generate Natural LanguageText.
Studies in Natural LanguageProcessing.
Cambridge University Press,Cambridge.Mellish, Chris, Alistair Knott, JonOberlander, and Mick O?Donnell.
1998.Experiments using stochastic search fortext planning.
In Proceedings of INLG 1998,pages 98?107, Niagara-on-the-Lake.Miltsakaki, Eleni and Karen Kukich.
2004.Evaluation of text coherence for electronicessay scoring systems.
Natural LanguageEngineering, 10(1):25?55.Ng, Vincent and Claire Cardie.
2002.Improving machine learning approachesto coreference resolution.
In Proceedings ofACL 2002, pages 104?111, Philadelphia,PA.O?Donnell, Mick, Chris Mellish, JonOberlander, and Alistair Knott.
2001.
ILEX:An architecture for a dynamic hypertextgeneration system.
Natural LanguageEngineering, 7(3):225?250.Passoneau, Rebecca J.
1998.
Interactionof discourse structure with explicitnessof discourse anaphoric phrases.
InWalker, Joshi, and Prince 1998,pages 327?358.Poesio, Massimo, Rosemary Stevenson,Barbara Di Eugenio, and Janet Hitzeman.2004.
Centering: a parametric theory andits instantiations.
Technical ReportCSM-369, Department of ComputerScience, University of Essex.
Extendedversion of the paper that appeared inComputational Linguistics 30(3):309?363,2004.Reiter, Ehud and Robert Dale.
2000.Building Natural Language GenerationSystems.
Cambridge University Press,Cambridge.Scott, Donia and Clarisse Sieckeniusde Souza.
1990.
Getting the message acrossin RST-based text generation.
In RobertDale, Chris Mellish, and Michael Zock,editors, Current Research in NaturalLanguage Generation.
Academic Press, SanDiego, CA, pages 47?74.45Computational Linguistics Volume 35, Number 1Siddharthan, Advaith.
2006.
Syntacticsimplification and text cohesion.Research on Language and Computation,4(1):77?109.Sidner, Candace L. 1979.
Towards aComputational Theory of Definite AnaphoraComprehension in English.
Ph.D. thesis, AILaboratory/MIT, Cambridge, MA.
Alsoavailable as Technical Report No.AI-TR-537.Soricut, Radu and Daniel Marcu.
2006.Discourse generation using utility-trainedcoherence models.
In Proceedings ofACL-COLING 2006 Poster Session,pages 803?810, Sydney.Strube, Michael and Udo Hahn.
1999.Functional centering: Groundingreferential coherence in informationstructure.
Computational Linguistics,25(3):309?344.Walker, Marilyn A., Aravind K. Joshi, andEllen F. Prince, editors.
1998.
CenteringTheory in Discourse.
Clarendon Press,Oxford.46
