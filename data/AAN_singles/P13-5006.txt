Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 9?10,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsVariational Inference for Structured NLP ModelsDavid Burkett and Dan KleinComputer Science DivisionUniversity of California, Berkeley{dburkett,klein}@cs.berkeley.eduDescriptionHistorically, key breakthroughs in structured NLPmodels, such as chain CRFs or PCFGs, have re-lied on imposing careful constraints on the local-ity of features in order to permit efficient dynamicprogramming for computing expectations or find-ing the highest-scoring structures.
However, asmodern structured models become more complexand seek to incorporate longer-range features, it ismore and more often the case that performing ex-act inference is impossible (or at least impractical)and it is necessary to resort to some sort of approx-imation technique, such as beam search, pruning,or sampling.
In the NLP community, one increas-ingly popular approach is the use of variationalmethods for computing approximate distributions.The goal of the tutorial is to provide an intro-duction to variational methods for approximate in-ference, particularly mean field approximation andbelief propagation.
The intuition behind the math-ematical derivation of variational methods is fairlysimple: instead of trying to directly compute thedistribution of interest, first consider some effi-ciently computable approximation of the originalinference problem, then find the solution of the ap-proximate inference problem that minimizes thedistance to the true distribution.
Though the fullderivations can be somewhat tedious, the resultingprocedures are quite straightforward, and typicallyconsist of an iterative process of individually up-dating specific components of the model, condi-tioned on the rest.
Although we will provide sometheoretical background, the main goal of the tu-torial is to provide a concrete procedural guide tousing these approximate inference techniques, il-lustrated with detailed walkthroughs of examplesfrom recent NLP literature.Once both variational inference procedureshave been described in detail, we?ll provide a sum-mary comparison of the two, along with some in-tuition about which approach is appropriate when.We?ll also provide a guide to further exploration ofthe topic, briefly discussing other variational tech-niques, such as expectation propagation and con-vex relaxations, but concentrating mainly on pro-viding pointers to additional resources for thosewho wish to learn more.Outline1.
Structured Models and Factor Graphs?
Factor graph notation?
Example structured NLP models?
Inference2.
Mean Field?
Warmup (iterated conditional modes)?
Mean field procedure?
Derivation of mean field update?
Example3.
Structured Mean Field?
Structured approximation?
Computing structured updates?
Example: Joint parsing and alignment4.
Belief Propagation?
Intro?
Messages and beliefs?
Loopy BP5.
Structured Belief Propagation?
Warmup (efficient products for mes-sages)?
Example: Word alignment?
Example: Dependency parsing6.
Wrap-Up?
Mean field vs BP?
Other approximation techniques9Presenter BiosDavid Burkett is a postdoctoral researcher in theComputer Science Division at the University ofCalifornia, Berkeley.
The main focus of his re-search is on modeling syntactic agreement in bilin-gual corpora.
His interests are diverse, though, andhe has worked on parsing, phrase alignment, lan-guage evolution, coreference resolution, and evenvideo game AI.
He has worked as an instructionalassistant for multiple AI courses at Berkeley andwon multiple Outstanding Graduate Student In-structor awards.Dan Klein is an Associate Professor of Com-puter Science at the University of California,Berkeley.
His research includes many areas ofstatistical natural language processing, includ-ing grammar induction, parsing, machine trans-lation, information extraction, document summa-rization, historical linguistics, and speech recog-nition.
His academic awards include a Sloan Fel-lowship, a Microsoft Faculty Fellowship, an NSFCAREER Award, the ACM Grace Murray Hop-per Award, Best Paper Awards at ACL, EMNLPand NAACL, and the UC Berkeley DistinguishedTeaching Award.10
