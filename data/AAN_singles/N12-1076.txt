2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 611?615,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsA comparison of models of word meaning in contextGeorgiana DinuUniversit?t des SaarlandesSaarbr?cken, Germanydinu@coli.uni-saarland.deStefan ThaterUniversit?t des SaarlandesSaarbr?cken, Germanystth@coli.uni-saarland.deS?ren LaueFriedrich-Schiller Universit?tJena, Germanysoeren.laue@uni-jena.deAbstractThis paper compares a number of recently pro-posed models for computing context sensitiveword similarity.
We clarify the connectionsbetween these models, simplify their formula-tion and evaluate them in a unified setting.
Weshow that the models are essentially equivalentif syntactic information is ignored, and that thesubstantial performance differences previouslyreported disappear to a large extent when thesesimplified variants are evaluated under identi-cal conditions.
Furthermore, our reformulationallows for the design of a straightforward andfast implementation.1 IntroductionThe computation of semantic similarity scores be-tween words is an important sub-task for a varietyof NLP applications (Turney and Pantel, 2010).
Onestandard approach is to exploit the so-called distribu-tional hypothesis that similar words tend to appearin similar contexts: Word meaning is represented bythe contexts in which a word occurs, and semanticsimilarity is computed by comparing these contextsin a high-dimensional vector space.Such distributional models of word meaning areattractive because they are simple, have wide cover-age, and can be easily acquired in an unsupervisedway.
Ambiguity, however, is a fundamental problem:when encountering a word in context, we want a dis-tributional representation which reflects its meaningin this specific context.
For instance, while buy andacquire are similar when we consider them in iso-lation, they do not convey the same meaning whenacquire occurs in students acquire knowledge.
Thisis particularly difficult for vector space models whichcompute a single type vector summing up over alloccurrences of a word.
This vector mixes all of aword?s usages and makes no distinctions betweenits?potentially very diverse?senses.Several proposals have been made in the recentliterature to address this problem.
Type-based meth-ods combine the (type) vector of the target with thevectors of the surrounding context words to obtaina disambiguated representation.
In recent work, thishas been proposed by Mitchell and Lapata (2008),Erk and Pad?
(2008) and Thater et al (2010; 2011),which differ in the choice of input vector representa-tion and in the combination operation they propose.A different approach has been taken by Erk andPad?
(2010), Reisinger and Mooney (2010) andReddy et al (2011), who make use of token vectorsfor individual occurrences of a word, rather than us-ing the already mixed type vectors.
Generally speak-ing, these methods ?select?
a set of token vectorsof the target, which are similar to the current con-text, and use only these to obtain a disambiguatedrepresentation.Yet another approach has been taken by Dinu andLapata (2010), ?
S?aghdha and Korhonen (2011)and Van de Cruys et al (2011), who propose to uselatent variable models.
Conceptually, this comesclose to token-based models, however their approachis more unitary as they attempt to recover a hiddenlayer which best explains the observation data.In this paper, we focus on the first group of ap-proaches and investigate the precise differences be-tween the three models of Erk and Pad?
and Thater etal., out of which (Thater et al, 2011) achieves state ofthe art results on a standard data set.
Despite the factthat these models exploit similar intuitions, both theirformal presentations and the results obtained vary toa great extent.
The answer given in this paper is sur-prising: the three models are essentially equivalent ifsyntactic information is ignored; in a syntactic spacethe three methods implement only slightly different611intuitions.
We clarify these connections, simplifythe syntactic variants originally proposed and reducethem to straightforward matrix operations, and evalu-ate them in a unified experimental setting.
We obtainsignificantly better results than originally reported inthe literature.
Our reformulation also also supportsefficient implementations for these methods.2 Models for meaning in contextWe consider the following problem: we are givenan occurrence of a target word and want to obtain avector that reflects its meaning in the given context.To simplify the presentation, we restrict ourselves tocontexts consisting of a single word, and use acquirein context knowledge as a running example.EP08.
Erk and Pad?
(2008) compute a contextu-alized vector for acquire by combining its type vec-tor (~w) with the inverse selectional preference vectorof knowledge (c).
This is simply the centroid of thevectors of all words that take knowledge as directobject (r):v(w,r,c) =(1n?w?f (w?,r,c) ?
~w?
)?~w (1)where f (w?,r,c) denotes the co-occurrence associa-tion between the context word c and words w?
relatedto c by grammatical relation r in a training corpus;n is the number of words w?
and ?
denotes a vectorcomposition operation.
In this paper, we take?
to bepoint-wise multiplication, which is reported to workbest in many studies in the literature.TFP10.
Thater et al (2010) also compute contex-tualized vectors by combing the vectors of the targetword and of its context.
In contrast to EP08, however,they use second order vectors as basic representationfor the target word.~w = ?r,r?,w??
(?w?f (w,r,w?)
?
f (w?,r?,w??))~er,r?,w??
(2)That is, the vector for a target word w has componentsfor all combinations of two grammatical roles r,r?
anda context word w?
; the inner sum gives the value foreach component.The contextualized vector for acquire is obtainedthrough pointwise multiplication with the (1st-order)vector for knowledge (~c), which has to be ?lifted?
firstto make the two vectors comparable:v(w,r,c) = ~w?Lr(~c) (3)~c = ?r?,w?
f (c,r?,w?)~e(r?,w?)
is a first order vectorfor the context word; the ?lifting map" Lr(~c) mapsthis vector to ?r?,w?
f (c,r?,w?)~e(r,r?,w?)
to make it com-patible with ~w.TFP11.
Thater et al (2011) take a slightly differentperspective on contextualization.
Instead of comb-ing vector representations for the target word and itscontext directly, they propose to re-weight the vectorcomponents of the target word, based on distribu-tional similarity with the context word:v(w,r,c) = ?r?,w??(r,c,r?,w?)
?
f (w,r?,w?)
?~e(r?,w?)
(4)where ?(r,c,r?,w?)
is simply cos(~c,~w?)
if r and r?denote the same grammatical function, else 0.3 ComparisonThe models presented above have a number of thingsin common: they all use syntactic information and?second order?
vectors to represent word meaning incontext.
Yet, their formal presentations differ substan-tially.
We now show that the models are essentiallyequivalent if we ignore syntax: they component-wisemultiply the second order vector of one word (targetor context) with the first order vector of the otherword.
Specifically, we obtain the following deriva-tions, where W = {w1, ...,wn} denotes the vocabu-lary, and V the symmetric n?n input matrix, whereVi j = f (wi,w j) gives the co-occurrence associationbetween words wi and w j:vEP08(w,c) =1n?w?
(f (w?,c) ?
~w?)?~w=1n?w?
(f (w?,c) ?
?
f (w?,w1), .
.
.?)?~w=1n?
?w?f (w?,c) ?
f (w?,w1), .
.
.?
?~w=1n?<~c, ~w1>,.
.
.
,<~c, ~wn>?
?~w=1n~c V ?~w612vTFP10(w,c) = ?w???W(?w?
?Wf (w,w?)
?
f (w?,w??))~ew??
?~c= ?
?w?
?Wf (w,w?)
f (w?,w1), .
.
.?
?~c= ?<~w, ~w1>,...,<~w, ~wn>?
?~c= ~w V ?~cvTFP11(w,c) = ?w??W?(c,w?)
?
f (w,w?)
?~ew?= ??
(w1,c) ?
f (w,w1), .
.
.
?= ??
(w1,c), .
.
.?
?~w (*)= ?<~w1,~c>,.
.
.
,<~wn,~c>?
?~w=~c V ?~wwhere <~v,~w> denotes scalar product.
In step (*), weassume that ?
(w,c) denotes the scalar product of ~wand~c, instead of cosine similarity, as TFP11.
This isjustified if we assume that all vectors are normalized,in which case the two are identical.As it can be observed the syntax-free variants ofEP08 and TFP11 are identical up to the choice innormalization.
TFP10 proposes an identical model tothat of TFP11, however with a different interpretation,in which the roles of the context word and of thetarget word are swapped.4 EvaluationWe have just shown that EP08, TFP10 and TFP11are essentially equivalent to each other if syntacticinformation is ignored, hence it is a bit surprising thatperformance results reported in the literature varyto such a great extent.
In this section we considersyntactic variants of these methods and we show thatperformance differences previously reported can onlypartly be explained by the different ways syntacticinformation is used: when we simplify these modelsand evaluate them under identical conditions, thedifferences between them disappear to a large extent.To evaluate the three models, we reimplementedthem using matrix operations similar to the ones usedin Section 3, where we made few simplificationsto the TFP10 and EP08 models: we follow TFP11and we use component-wise multiplication to com-bine the target with one context word, and add theresulting composed vectors when given more con-text words1.
Furthermore for TFP10, we change the1Note that some of the parameters in the EP08 method (omit-Model GAP ?
LiteratureEP08 46.6 + 14.4 (32.2)?TFP10 48.3 + 3.9 (44.4)TFP11 51.8 ?0.0TFP10+11 52.1 N/ATable 1: GAP scores LST data.?
The best available GAP score for this model (from Erk andPad?
(2010)) is reported only on a subset of the data - this subsetis however judged by the authors to be ?easier?
than the entiredata; all other methods are tested on the entire dataset.treatment of syntax in the line of the much simplerproposal of TFP11.
Specifically:v(w,r,c) = Lr?1(VVT )w,:?Vc,: (TFP10)v(w,r,c) = Vw,:?Lr(VVT )c,: (TFP11)where V is a I?
J syntactic input matrix, i.e.
thecolumns are (word, relation) pairs.
For simplification,the columns of V are reordered such that syntacticrelations form continuous regions.
Lr is a lifting mapsimilar to that of Equation (3) as it maps I- into J-dimensional vectors: the resulting vector is equal tothe original one in the column region of relation r,while everything else is 0.
In the above equations weuse the standard Matlab notation, Vw,: denoting a rowvector in matrix V .We evaluate these models on a paraphrase rankingtask, using the SemEval 2007 Lexical SubstitutionTask (LST) dataset: the models are given a targetword in context plus a list of potential synonyms(substitution candidates) ranging over all senses ofthe target word.
The models have to decide to whatextent each substitution candidate is a synonym ofthe target in the given context.
We omit the precise de-scription of the evaluation setting here, as we followthe methodology described in Thater et al (2011).Results are shown in Table 1, where the first col-umn gives the GAP (Generalized Average Precision)score of the model and the second column givesthe difference to the result reported in the literature.TFP10 and EP08 perform much better than the origi-nal proposals, as we obtain very significant gains of4 and 14 GAP points.ted in the brief presentation in Section 2), which are difficult totune (Erk and Pad?
(2009)), disappear this way.613We can observe that the differences between thethree methods, when simplified and tested in an uni-fied setting, largely disappear.
This is to be expectedas all three methods implement very similar, all moti-vated intuitions: TFP11 reweights the vector of thetarget acquire with the second order vector of thecontext knowledge, i.e.
with the vector of similaritiesof knowledge to all other words in the vocabulary.TFP10 takes a complementary approach: it reweightsthe vector of knowledge with the second order vectorof acquire.
In both these methods, anything outsidethe object (object?1 respectively) region of the space,is set to 0.
The variant of EP08 that we implement isvery similar to TFP11, however it compares knowl-edge to all other words in the vocabulary only usingoccurrences as objects while TFP11 takes all syntac-tic relations into account.Note that TFP10 and TFP11 operate on comple-mentary syntactic regions of the vectors.
For thisreason the two models can be trivially combined.The combined model (TFP10+11) achieves even bet-ter results: the difference to TFP11 is small, howeverstatistically significant at level p < 0.05.Implementation details.
Straightforward imple-mentations of the three models are computationallyexpensive, as they all use ?second order?
vectors toimplement contextualization of a target word.
Our re-formulation in terms of matrix operations allows forefficient implementations, which take advantage ofthe sparsity of the input matrix V : contextualizationof a target word runs in O(nnz(V )), where nnz is thenumber of non-zero entries.
Note that ranking notonly a small set of predefined substitution candidates,as in the experiment above, but also ranking the en-tire vocabulary runs in O(nnz(V )).
On this task, thisoverall running time is in fact identical to that of sim-pler methods such as those of Mitchell and Lapata(2008).In our experiments, we use GigaWord to extracta syntactic input matrix V of size ?
2M?7M.
V isonly 4.5?10?06 dense.
Note that because of the sim-ple operations involved, we do not need to computeor store the entire VV T matrix, which is much denserthan V (we have estimated order of 1010 entries).
Thesparsity of V allows for very efficient computationsin practice: the best single model, TFP11, runs inless than 0.2s/0.4s per LST instance, for ranking thecandidate list/entire vocabulary in a Python imple-mentation using scipy.sparse, on a standard 1GHzprocessor.5 ConclusionsIn this paper, we have compared three related vec-tor space models of word meaning in context.
Wehave reformulated the models and showed that theyare in fact very similar.
We also showed that thedifferent performances reported in the literature areonly to some extent due to the differences in themodels: We evaluated simplified variants of theseand obtained results which are (much) better thanpreviously reported, bringing the three models muchcloser together in terms of performance.
Aside fromclarifying the precise relationship between the threemodels under consideration, our reformulation hasthe additional benefit of allowing the design of astraightforward and efficient implementation.Finally, our focus on these methods is justified bytheir clear advantages over other classes of models:unlike token-based or latent variable methods, theyare much simpler and require no parameter tuning.Furthermore, they also obtain state of the art resultson the paraphrase ranking task, outperforming othersimple type-based methods (see (Van de Cruys etal., 2011) and (?
S?aghdha and Korhonen, 2011) forresults of other methods on this data).Acknowledgments.
This work was partially sup-ported by the Cluster of Excellence ?MultimodalComputing and Interaction", funded by the GermanExcellence Initiative.ReferencesGeorgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofEMNLP 2010, Cambridge, MA.Katrin Erk and Sebastian Pad?.
2008.
A structured vectorspace model for word meaning in context.
In Proceed-ings of EMNLP 2008, Honolulu, HI, USA.Katrin Erk and Sebastian Pad?.
2009.
Paraphrase assess-ment in structured vector space: Exploring parametersand datasets.
In Proceedings of the Workshop on Geo-metrical Models of Natural Language Semantics.Katrin Erk and Sebastian Pad?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof ACL 2010 Short Papers, Uppsala, Sweden.614Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, Columbus, OH, USA.Diarmuid ?
S?aghdha and Anna Korhonen.
2011.
Prob-abilistic models of similarity in syntactic context.
InProceedings of EMNLP 2011.Siva Reddy, Ioannis Klapaftis, Diana McCarthy, andSuresh Manandhar.
2011.
Dynamic and static pro-totype vectors for semantic composition.
In Proc.
ofIJCNLP 2011.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InProceedings of NAACL 2010, Los Angeles, California.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedingsof ACL 2010, Uppsala, Sweden.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effectivevector model.
In Proceedings of IJCNLP 2011.Peter D. Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space modes of semantics.
Journalof Artificial Intelligence Research, 37:141?188.Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.2011.
Latent vector weighting for word meaning incontext.
In Proceedings of EMNLP 2011.615
