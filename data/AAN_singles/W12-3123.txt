Proceedings of the 7th Workshop on Statistical Machine Translation, pages 181?190,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsComparing human perceptions of post-editing effort with post-editingoperationsMaarit KoponenUniversity of Helsinki, Dept of Modern LanguagesPO Box 2400014 University of Helsinki, Finlandmaarit.koponen@helsinki.fiAbstractPost-editing performed by translators is anincreasingly common use of machine trans-lated texts.
While high quality MT may in-crease productivity, post-editing poor transla-tions can be a frustrating task which requiresmore effort than translating from scratch.
Forthis reason, estimating whether machine trans-lations are of sufficient quality to be usedfor post-editing and finding means to reducepost-editing effort are an important field ofstudy.
Post-editing effort consists of differentaspects, of which temporal effort, or the timespent on post-editing, is the most visible andinvolves not only the technical effort neededto perform the editing, but also the cognitiveeffort required to detect and plan necessarycorrections.
Cognitive effort is difficult to ex-amine directly, but ways to reduce the cogni-tive effort in particular may prove valuable inreducing the frustration associated with post-editing work.
In this paper, we describe anexperiment aimed at studying the relationshipbetween technical post-editing effort and cog-nitive post-editing effort by comparing caseswhere the edit distance and a manual score re-flecting perceived effort differ.
We present re-sults of an error analysis performed on suchsentences and discuss the clues they may pro-vide about edits requiring great cognitive ef-fort compared to the technical effort, on onehand, or little cognitive effort, on the other.1 IntroductionAn increasingly common use for machine transla-tion is producing texts to be post-edited by transla-tors.
While sufficiently high-quality MT has beenshown to produce benefits for productivity, a well-known problem is that post-editing poor machinetranslation can require more effort than translatingfrom scratch.
Measuring and estimating post-editingeffort is therefore a growing concern addressed byConfidence Estimation (CE) (Specia, 2011).Time spent on post-editing can be seen as themost visible and economically most important as-pect of post-editing effort (Krings, 2001); however,post-editing effort can be defined and approached indifferent ways.
Krings (2001) divides post-editingeffort into three types: 1. temporal, 2. cognitiveand 3. technical.
Temporal effort refers to post-editing time.
Cognitive effort involves identifyingthe errors in the MT and the necessary steps to cor-rect the output.
Technical effort then consists of thekeystrokes and cut-and-paste operations needed toproduce the post-edited version after the errors havebeen detected and corrections planned.
These dif-ferent aspects of effort are not necessarily equal invarious situations.
In some cases, the errors may beeasy to detect but involve several technical opera-tions to be corrected.
In other cases, parsing the sen-tence and detecting the errors may require consid-erable cognitive effort, although the actual technicaloperations required are quick and easy.
Accordingto Krings (2001), temporal effort is a combinationof both cognitive and technical effort, with cogni-tive effort being the decisive factor.
Assessing andreducing the cognitive effort involved in MT post-editing would therefore be important but the taskis far from simple.
Past experiments have involvedcognitive approaches such as think-aloud protocols(Krings, 2001; O?Brien, 2005; Carl et al, 2011) and181post-editing effort scores assigned by human evalua-tors (Specia et al, 2009; Specia, 2011; Specia et al,2011).While edit operations reflect the amount of tech-nical effort needed, subjective assessments of per-ceived post-editing effort needed can serve as a mea-sure of cognitive post-editing effort: in order to givesuch an estimate, the evaluator needs to cognitivelyprocess the segment in order to detect the errorsand plan the necessary corrections.
Using these twomeasures, a comparison of technical effort and per-ceived amount of post-editing effort can serve as away to evaluate cognitive post-editing effort.
Wepropose that studying cases where the perceived ef-fort necessary is greater or smaller than the num-ber of actual edit operations performed may provideclues to situations where the cognitive and technicaleffort differ.
Cases where the human editor overes-timates the need for editing (as compared to num-ber of edit operations performed) could indicate thatthese segments contain errors requiring considerablecognitive effort.
On the other hand, cases wherethe manual score underestimates the amount of edit-ing needed could indicate errors that require rela-tively little cognitive effort compared to the numberof technical operations.To examine the question of differences in techni-cal and cognitive post-editing effort, we present ananalysis of MT segments that have different levelsof post-editing indicated by the manual effort scoreand actual number of post-edit operations indicatedby the edit distance.
By analyzing cases where thesetwo measures of post-editing effort differ, it may bepossible to isolate cases that require more cognitiveeffort than technical effort and vice versa.
Section 3describes the material and method used in the exper-iment, and the results of the analysis are presentedin Section 4.2 Related workAs the temporal aspect of post-editing effort is im-portant for the practice of machine translation post-editing, post-editing time has been a commonlyused measure of post-editing effort (Krings, 2001;O?Brien, 2005; Specia et al, 2009; Tatsumi, 2009;Tatsumi and Roturier, 2010; Specia, 2011; Carl etal., 2011).
The technical aspect of post-editing efforthas been approached by following keystrokes andcut-and-paste operations (Krings, 2001; O?Brien,2005; Carl et al, 2011) or using automatic met-rics for edit distance between the raw MT and post-edited version (Tatsumi, 2009; Temnikova, 2010;Tatsumi and Roturier, 2010; Specia and Farzindar,2010; Specia, 2011; Blain et al, 2011).
Several editoperations may also be incorporated in one ?post-edit action (PEA)?, introduced by Blain et al (2011).For example, changing the number of a noun propa-gates changes to other words, such as the determin-ers and adjectives modifying it.
Tatsumi and Ro-turier (2010) also explore the relationship betweentemporal and technical aspects of post-editing effort.Cognitive aspects of post-editing effort have beenapproached with the help of keystroke logging(Krings, 2001; O?Brien, 2005; Carl et al, 2011)and gaze data (Carl et al, 2011), attempting to mea-sure cognitive effort in terms of pauses and fixations.O?Brien (2005) also experiments with the use ofchoice network analysis (CNA) and think-aloud pro-tocols (TAP).
Human scores for post-editing efforthave involved assessing the amount of post-editingneeded (Specia et al, 2009; Specia, 2011) or ade-quacy of the MT (Specia et al, 2011).Temnikova (2010) proposes the analysis of thetypes of changes and comparison to post-editingtime as a way to explore cognitive effort.
For thispurpose, Temnikova (2010) builds upon the MT er-ror classification by Vilar et al (2006) and their ownpost-editing experiments using controlled languageto draft a classification for the cognitive effort re-quired for correcting different types of MT errors.This classification defines ten types of errors andranks them from 1 to 10 with 1 indicating the eas-iest and 10 the hardest error type to correct.
Theeasiest errors are considered to be connected to themorphological level, or correct words with incorrectform, followed by the lexical level, involving incor-rect style synonyms, incorrect words, extra words,missing words and erroneously translated idiomaticexpressions.
The hardest errors in the classificationrelate to syntactic level and include wrong punctua-tion, missing punctuation, then word order at wordlevel and finally word order at phrase level.
Theranking is based on studies in written language com-prehension and error detection.
Results reported inTemnikova (2010) suggest that pre-edited machine182translations that had previously been found to re-quire less post-editing effort measured by post-edittime and edit distance contain less errors that arecognitively more difficult compared to MT that hadnot been pre-edited.In this study, we aim to investigate the relation-ship between the cognitive effort and the technicaleffort involved in post-editing.
Edit distance be-tween MT segments and their post-edited versionsis used as a measure of technical effort and humaneffort scores as a measure of cognitive effort.3 Material and methodThe data used in this study consists of English toSpanish MT segments from the evaluation task train-ing set provided for the quality estimation task atthe NAACL 2012 Seventh Workshop on Statisti-cal Machine Translation WMT12.
1 The train-ing set consists English to Spanish machine trans-lations of news texts, produced by a phrase-basedSMT system.
The data available for each segmentincludes the English source segment, Spanish ref-erence translation produced by a human translator,machine translation into Spanish, post-edited ver-sion of the machine translation and a manual scoreindicating how much editing would be required totransform the MT segment into a useful translation.The manual score included is the average of scor-ing conducted by three professional translators us-ing a 5-point scale where (1) indicates the segmentis incomprehensible and needs to be translated fromscratch, (2) significant editing is required (50-70%of the output), (3) about 25-50% of the output needsto be edited, (4) about 10-25% needs to be edited,and (5) little to no editing is required.Additional information includes the SMT align-ment tables.
The alignments were not part of theoriginal set, and in some cases differed slightly fromthe segments that had been used for the manual scor-ing.
As we intended to make use of the alignmentsfrom source to MT, we included only segments thatwere identical in the original evaluated set.To measure the amount of editing performed onthe segments, the translation edit rate (TER) (Snoveret al, 2006) was calculated using the post-edited1http://www.statmt.org/wmt12/quality-estimation-task.htmlversions as reference.
TER measures the minimumnumber of edits that are needed to transform the ma-chine translation into the post-edited segment usedas reference.
Edits can be insertion, deletion, substi-tution or reordering and the score is calculated as thenumber of edits divided by the number of tokens inthe reference.
The higher the TER score, the moreedits have been performed.As our aim was to focus on cases where the per-ceived effort score and the amount of editing dif-fered, we looked for two types of sentences at theopposite ends of the manual effort scoring scale: (1)Cases where the manual score indicated more edit-ing was needed than had actually been performed.
(2) Cases where the manual score indicated less edit-ing was needed than had actually been performed.For Case (1), we selected segments with a manualscore of 2.5 or lower, meaning that at least 50% ofthe segment needed editing according to the evalu-ators.
We looked for the ones with the lowest TERscores, trying to find at least 30 sentences.
The setselected for analysis consists of 37 sentences with amanual effort score of 2.5 or lower and TER score0.33 or lower.
For comparison, we also selected thesame number of sentences with similar TER scoresbut with manual scores of 4 or above.
These sets arereferred to as the low TER set.For Case (2), we selected segments with a manualscore of 4 or above, meaning that no more than 25%of the segment needed editing according to the eval-uator.
Again, we looked for about 30 sentences withthe highest TER scores.
The set selected consistsof 35 sentences with a manual effort score of 4 orhigher, and TER score 0.45 or higher.
For compar-ison, we also selected sentences with similar TERscores but low manual scores.
These sets are re-ferred to as the high TER set.The selected MT segments and post-edited ver-sions were then tagged with the FreeLing Spanishtagger (Padro?
et al, 2010).
The tagged versionscontain the surface form of the word, lemma and atag with part-of-speech (POS) and grammatical in-formation.
Other tools such as dependency parsingwere considered, but within the scope of this study,we decided to experiment what changes can be ob-served using only the basic lemma, POS and forminformation.The tagged versions were aligned manually, first183matching identical tokens (words and punctuation)in the sentence, then matching words with the samelemma but different surface form.
The alignmenttable was consulted to match substitutions that in-volved a different word and even different POS.Each matched pair of words in the MT and post-edited versions was then labeled to indicate whetherthe match was identical or involved editing the wordform, substituting with a different word of the samePOS or a word of different POS.
Words appearingin the post-edited version but not in the MT were la-beled as insertions and words appearing in the MTbut not in the post-edited version as deletions.
Incases where several MT words were replaced withone in the post-edited version or one MT word wasreplaced with many in the post-edited, a match wasmade between words of the same POS and form, ifsuch was found, or the first word in the sequence ifnone matched.
The remaining words were labeledas inserted/deleted.The positions of the matched words were alsocompared.
For matching the word order, changescaused only by insertion or deletion of other wordswere ignored, and words that had remained in thesame order after post-editing were labeled as same.In cases where the word order did not match, theword was labeled with the distance it had beenmoved and whether it had been moved alone or asa part of a larger group.The totals of changes within a sentence were thencalculated and the patterns of changes made by ed-itors were examined.
In addition to the total num-ber of edit operations, we considered the possibil-ity that editing certain parts-of-speech might requiremore effort than others.
In particular, editing con-tent words such as verbs or nouns might requiremore effort than editing function words such as de-terminers, because they are more central to convey-ing the content of the sentence.
Further, as Blain etal.
(2011) argue, changes to these words may prop-agate changes to other words in the sentence.
Punc-tuation was also treated separately to follow Tem-nikova?s (2010) classification of punctuation errorsas a class of their own.The patterns found in the sample sentences werecompared to the comparison sets of sentences withsimilar TER scores.
Additionally, Spearman rankcorrelations between the manual effort score and thevarious edit categories were calculated for all tokensand specific POS classes.
The next section presentsthe results of these comparisons.4 ResultsThis section presents the results from the analysisof post-editing changes.
The total number of seg-ments and tokens and the percentages of edited andreordered tokens in each set are shown in Table 1.Comparisons of the edit patterns between segmentswith similar TER scores but different manual scoresare shown in Figures 1 to 4.
Figure 1 presents thedistributions of edit categories in the low TER setsand Figure 3 in the high TER sets.
Figure 2 presentsthe percentages of changed tokens and reordered to-kens by POS class in the low TER set and Figure4 in the high TER sets.
In Figures 2 and 4, nouns,verbs, adjectives and determiners are shown sepa-rately, while other parts-of-speech are combined into?Other?.
Punctuation is also presented separately.Tables 2 and 3 present Spearman rank correla-tions between the manual score and different editcategories.
Overall correlations regardless of POSare given for all edit categories.
For specific POSclasses, only the edit categories with strongest cor-relations are listed in each case.4.1 Case 1: Low TER setThese sentences represent a case where the humanevaluators indicated that significant post-editingwould be needed but the low TER score indicatedthat relatively little editing had been performed.
Themost noticeable difference between segments withhigh and low manual scores is the number of tokens:low-scored segments have about twice as many to-kens on average than the high-scored ones (see Ta-ble 1) and the number of tokens in the post-editedsegment has a strong negative correlation (Table 2).Besides segment length, other strong correlations in-volve different types of reordering.
Reorderings in-volving a distance of one step show weaker corre-lation than changes involving a longer distance.
Nocorrelation was found for any of the word changecategories in this case.Broken down by the POS class, results are simi-lar to the overall result in that reordering categorieshave the strongest (negative) correlations with the184TER Manual Number of Number of Edited Reorderedscore score segments tokens tokens tokensLow Low 37 1480 23% 24%Low High 37 695 21% 15%High Low 35 943 45% 45%High High 35 556 42% 33%Table 1: Total number of sentences and tokens per set, percentage of tokens edited and percentage of tokens reordered.Figure 1: Distribution of edit categories - Low TER.Figure 2: Edited and reordered tokens by POS - Low TEReffort score.
Strongest correlations also mostly in-volve nouns, adjectives or verbs.
As shown in Fig-ure 2, the differences in percentage of edited tokensare largest for verbs and adjectives.
In high-scoredsentences, 72% of verbs were unchanged by the ed-itor compared to 55% in the low-scored ones.
Inboth cases, most edits to verbs involved changingthe form of the verb, (23% in low-scored vs 11% inhigh-scored).
Adjectives have a similar pattern with18% of edited adjective forms in low-scored vs 7%in high-scored sentences.Sentences with high manual scores actually havemore cases of edited determiners and nouns, al-though for nouns the difference is only 1%.
Mostedits to determiners involved deletion (15% of deter-miners) or changed form (11%) in the case of high-scored sentences.
In low-scored sentences, insertionwas most common (10% of determiners).
Within185Overall correlationsnumber of tokens -0.51 ***word match 0.11form changed -0.10word changed -0.15pos changed -0.15deleted 0.08inserted -0.15order same 0.51 ***group moved -0.48 ***1 word moved -0.47 ***dist.
1 -0.37 **dist ?
2 -0.53 ***Strongest correlations by POSNoun, order same 0.49 ***Adj, order same 0.47 ***Noun, group moved -0.46 ***Adj, dist.
?
2 -0.46 ***Noun, dist.
?
2 -0.45 ***Other, group moved -0.44 ***Verb, 1 word moved -0.44 ***Verb, dist.
?
2 -0.43 ***Other, order same 0.41 ***Det, group moved -0.40 ***Verb, word match 0.39 ***Adj, 1 word moved -0.38 ****** p < 0.001, ** p < 0.01, * p < 0.05Table 2: Spearman rank correlations between effort scoreand edit categories - Low TER.the class ?Other?
combining numbers, adverbs, con-junctions, pronouns and prepositions, adverbs werean similar case in that there were more unchangedadverbs in the low-rated sentences (86%) than in thehigh-rated (72%).
However, the total number of ad-verbs in either set was very small.4.2 Case 2: High TER setThese sentences represent a case where the humanevaluators indicated only a little editing was neededbut the high TER score indicated much more editinghad been performed.
Again one noticeable differ-ence between the sentences with low and high man-ual scores is the number of tokens (see Table 1), al-though the negative correlation shown in Table 3 wasnot as strong as for the low TER set.For these sentences, word changes have strongercorrelations with the manual effort score (Table 3).While the shares of fully matched words are fairlyequal between the sentences, differences appear insome of the edit categories.
Sentences with highmanual scores have more cases where the word formhas been edited (Figure 3), and changed form hasthe strongest (positive) correlation after number oftokens.
High-scored segments also appear to havemore deletions, but essentially no correlation wasfound between the manual score and deletions onthe segment level.
As shown in Figure 3, low-scoredsegments have more cases of substitution with dif-ferent word.
Reordering is again more common inlow-scored segments, but correlations for reorderingare weaker than in the low TER set.
Cases whereone word has been moved alone rather than as a partof a group has the strongest correlation among thereordering categories.Overall correlationsnumber of tokens -0.43 ***word match 0.14form changed 0.36 **word changed -0.25 *pos changed -0.28 *deleted 0.14inserted -0.22order same 0.21group moved -0.121 word moved -0.34 **dist.
1 -0.22dist.
?
2 -0.25 *Strongest correlations by POSOther, inserted -0.38 **Noun, 1 word moved -0.36 **Noun, pos changed -0.35 **Noun, word changed -0.30 *Adj, order same 0.28 *Det, inserted -0.27 *Adj, dist.
?
2 -0.25 *Noun, word match 0.24 **** p < 0.001, ** p < 0.01, * p < 0.05Table 3: Spearman rank correlations between effort scoreand edit categories - High TER.For specific POS classes, the strongest correlation186Figure 3: Distribution of edit types - High TER.Figure 4: Edited and reordered tokens by POS - High TERin Table 3 involves insertion of words in the com-bined class ?Other?
(numbers, adverbs, conjunc-tions, pronouns and prepositions).
Within this class,pronouns actually required most edits: in low-scoredsegments, 50% of pronouns were inserted by the ed-itor (32% in high-scored segments).
The largest dif-ference in the percentage of edited tokens is seenwith nouns (41% edited in low-scored segments vs32% in high-scored, and edits related to nouns arealso among the strongest correlations for this set.
Inthe case of adjectives, the segments with low man-ual score actually have more cases where no edit-ing of the word has been required (61% vs 53%),but high-scored sentences contain a larger share ofcases (32% vs 16%) where only the form of the ad-jective has been edited.
However, these correlationsremained weak.
Reordering involving nouns and ad-jectives, on the other hand, again appears among thestrongest correlations.5 DiscussionPerhaps the most obvious difference between seg-ments with high and low manual scores is segmentlength: long segments tend to get low scores evenwhen the amount of editing turns out to be less thanestimated.
The effect of sentence length has alsobeen observed in other studies, e.g.
(Tatsumi, 2009).One simple explanation would be that a high totalnumber of words leads to a high total number ofchanges to be made and therefore involves consid-erable technical post-editing effort.
However, as thecase of segments with low manual scores but lowTER show, sometimes these long sentences do not,in fact, require a large number of edit operations.187This suggests also increased cognitive effort, as thesheer length may make it difficult for the evalua-tor/editor to perceive what needs to be changed andplan the edits.We also noticed during the analysis that some ofthe very long segments actually consisted of twosentences.
Furthermore, in some these cases, oneof the sentences contained few changes while mostof the changes were confined to the other.
Sim-ilarly, long segments consisting of only one sen-tence sometimes contained long unchanged pas-sages while some other part of the sentence wasedited significantly.
In these cases, such unchangedpassages could be useful to the post-editor in real lifesituations, but the error-dense passage affects per-ception of the segment as a whole.
Perhaps this sug-gests that assessing MT for post-editing and post-editing itself could benefit from presenting longersegments in shorter units, allowing the evaluator oreditor to choose and discard different units within alonger segment.Tatsumi (2009) also found that very short sen-tences increased post-editing time.
In this study, allextremely short sentences found had received highscores from the human evaluators.
Some are foundin the low TER/high manual score set used for com-parison purposes, but there are also some in theset of sentences with high TER/high manual score,meaning that there were relatively many edits com-pared to the length of the segment but the evaluatorshad indicated that little editing was needed.
At leastfor the segments analyzed here, it appears that theevaluators did not consider short sentences to requiremuch effort regardless of the actual number of editsperformed.
In Tatsumi?s (2009) results, also otheraspects, such as source sentence structure and de-pendency errors in the MT were discovered to havean effect on post-editing time.
In this study, sentencestructure and dependency errors were not explicitlyexamined, but these aspects would be of interest infuture work.Edits related to reordering also appear to be con-nected to low manual scores, as low-scored sen-tences involved more reordering than high-scoredones in both cases.
This reflects Temnikova?s (2010)error ranking where errors involving word order,particularly at phrase level, are considered the mostdifficult to correct.
Besides the number of reorder-ings necessary, the results of this study may suggestsome differences in whether reordering involves iso-lated words or groups of words and distances of onestep (word level order) or longer distances.Examining the results by parts-of-speech maysuggest that overall, edits related to nouns, verbsor adjectives take more effort than other POS, be-cause in both sets, strongest correlations mainly in-volved nouns, verbs and adjectives.
In both sets,sentences with low manual scores contained morecases of edited verbs, and verb matches had one ofthe strongest correlations in the low TER set.
Onthe other hand, edits related to nouns appeared tohave particularly strong correlations in the high TERset.
In this set, however, the strongest negative cor-relation was found for insertion of the other POS(mainly pronouns), so at least some of the other POSmay also be difficult to edit.Some cases where relatively little cognitive effortis required may be suggested by the situations wherethe high-scored sentences in fact contain more ed-its than the low-scored ones.
In the high TERset, sentences with high manual scores containedmore cases where only the form of a word has beenedited, whereas sentences with low manual scorescontained more cases of substitution with a differ-ent word or even different POS.
This reflects theranking of such errors in (Temnikova, 2010), whereword form errors are considered cognitively easiest.This particularly appears to be the case for adjectivesin this set.
Although segments with a high manualscore actually have a smaller number of fully cor-rect adjectives than low-scored ones, they contain alarger share of instances where only the form of theadjective has been edited.
Another example of editsinvolving less cognitive effort might be determinersin the low TER set, where again sentences with highmanual scores contain more edited determiners thanthose with low scores.
In this case, deletion of de-terminers was common in addition to changing theform.Overall, deletion and insertion or extra words andmissing words appeared to have little effect.
Whilesentences with high manual scores have a slightlyhigher percentage of deleted words in both sets, thecorrelation was weak.
Most of the deletions of con-tent words seemed to involve auxiliary verbs, but insome instances it is difficult to say whether the ed-188itor has, in fact, considered something ?extra?
in-formation and why, whether there has been a de-liberate choice to implicitate certain information orwhether the deletion has been at least partly uninten-tional.
During the alignment process of the MT andpost-edited version, it appeared that some source el-ements, in some cases entire clauses and in otherscertain words, were completely missing in the post-edited version.
On the other hand, some of the in-sertions were also difficult to map onto anything inthe source segment and the editor appeared to havebrought in something extra.
One clear example in-volved adding a conversion from miles per hour tokm per hour that did not appear in the MT or sourcetext.
Such deletions and insertions concerned only afew isolated cases which were not examined in detailwithin the scope of this work.
Some error classifica-tions, such as Blain et al (2011), do also take errorsmade by post-editors into account, and one interest-ing aspect of post-editing would be to study the cor-rectness of post-edits.
If it would turn out that post-editors are more prone to make errors or to fail tocorrect errors, (particularly errors related to contentas opposed to typographical errors etc.)
in certainsituations, this might suggest situations that involveparticular cognitive effort or mislead the editor.6 Conclusion and Future WorkWe have presented an experiment aimed at explor-ing the difference between cognitive and technicalaspects of MT post-editing effort by comparing hu-man scores of perceived effort necessary to actualedits made by post-editors.
We examined caseswhere considerably more or considerably less post-editing was done than predicted by the evaluators?estimate of post-editing needed.
The results showthat one of the factors most affecting the perceptionof post-editing necessary involves segment length:long segments are perceived to involve much effortand therefore receive low scores even when the ac-tual number of edits turns out to be small.
This sug-gests that sentence length affects the cognitive effortrequired in identifying errors and planning the cor-rections, and presenting MT for this type of evalu-ation and post-editing may benefit from displayingsegments to the evaluator or editor in smaller units.The results also suggest other features affectingcognitive effort.
Sentences with low manual scoreswere found to involve more reordering, indicatingincreased cognitive effort, while sentences with highmanual scores were found to involve more cases ofcorrect words with incorrect form, suggesting thatthese errors are cognitively easier.
Examining edittype distributions in different POS classes suggeststhat edits related to certain parts-of-speech, namelynouns, verbs and adjectives, may also be associatedwith perception of more effort.
On the other hand,sentences with high scores in some cases containedeven more editing of some other POS and types,such as editing forms of adjectives or deleting deter-miners, which may indicate that these errors affectperception of effort to a lesser extent.
As the num-ber of sentences used was relatively low, however,such effects would require more study.In future work, we aim to more explicitly exam-ine combinations of edit operations, (e.g.
changingthe form and reordering, moving a group and substi-tuting one word within the group) and features suchas dependency errors (Tatsumi, 2009).
Further ex-periments with data on other language pairs wouldalso be needed.
Another interesting aspect for futurework would be trying to distinguish between editsmade for reasons of incorrect language and edits forreasons of incorrect content.
Further, examining thesuccess of post-editing and exploring whether post-editors themselves are prone to make errors or fail tocorrect errors in certain situations could be an inter-esting avenue for discovering situations that involvesignificant cognitive effort.AcknowledgmentsThis work has been supported by LANGNETFinnish Graduate School in Language Studies.ReferencesFre?de?ric Blain, Jean Senellart, Holger Schwenk, MirkoPlitt and Johann Roturier 2011.
Qualitative analy-sis of post-editing for high quality machine translation.In MT Summit XIII: the Thirteenth Machine Transla-tion Summit [organized by the] Asia-Pacific Associa-tion for Machine Translation (AAMT), pages 164-171.19-23 September 2011, Xiamen, China.Michael Carl, Barbara Dragsted, Jakob Elming, DanielHardt and Arnt Lykke Jakobsen.
2011.
The pro-cess of post-editing: a pilot study.
In Proceedings of189the 8th international NLPSCworkshop.
Special theme:Human-machine interaction in translation, pages 131-142.
Copenhagen Business School, 20-21 August2011.
(Copenhagen Studies in Language 41), Fred-eriksberg: Samfundslitteratur.Hans P. Krings.
2001.
Repairing texts: Empirical inves-tigations of machine translation post-editing process.The Kent State University Press, Kent, OH.Sharon O?Brien 2005.
Methodologies for Measuring theCorrelations between Post-Editing Effort and MachineTranslatability.
Machine Translation, 19(1):37-58.Llu?
?s Padro?, Miquel Collado, Samuel Reese, MarinaLloberes and Irene Castello?n.
2010.
FreeLing2.1: Five Years of Open-Source Language Process-ing Tools.
In LREC 2010: proceedings of the seventhinternational conference on Language Resources andEvaluation, pages 3485-3490.
17-23 May 2010, Val-letta, Malta.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla and John Makhoul.
2006.
A Study of Trans-lation Edit Rate with Targeted Human Annotation.
InProceedings of Association for Machine Translationin the Americas, pages 223-231.
August 8-12, 2006,Cambridge, Massachusetts, USA.Lucia Specia, Marco Turchi, Nicola Cancedda, MarcDymetman and Nello Cristianini 2009.
Estimating theSentence-Level Quality of Machine Translation Sys-tems.
In Proceedings of the 13th Annual Conferenceof the EAMT, pages 28-35.
Barcelona, May 2009.Lucia Specia and Atefeh Farzindar.
2010.
EstimatingMachine Translation Post-Editing Effort with HTER.In Proceedings of the Second Joint EM+/CNGL Work-shop Bringing MT to the User: Research on Integrat-ing MT in the Translation Industry (JEC 10), pages33-41.
Denver, CO, 4 November 2010.Lucia Specia.
2011.
Exploiting Objective Annotationsfor Measuring Translation Post-Editing Effort.
In Pro-ceedings of the 15th Conference of the European As-sociation for Machine Translation, pages 73-80.
Leu-ven, Belgium, May 2011.Lucia Specia, Najeh Hajlaoui, Catalina Hallett andWilker Aziz.
2011.
Predicting Machine Transla-tion Adequacy.
In MT Summit XIII: the ThirteenthMachine Translation Summit [organized by the] Asia-Pacific Association for Machine Translation (AAMT),pages 513-520.
19-23 September 2011, Xiamen,China.Midori Tatsumi.
2009.
Correlation between AutomaticEvaluation Metric Scores, Post-Editing Speed, andSome Other Factors.
In MT Summit XII: proceedingsof the twelfth Machine Translation Summit, pages 332-339 August 26-30, 2009, Ottawa, Ontario, Canada.Midori Tatsumi and Johann Roturier.
2010.
SourceText Characteristics and Technical and Temporal Post-Editing Effort: What is Their Relationship?.
In Pro-ceedings of the Second Joint EM+/CNGL WorkshopBringing MT to the User: Research on IntegratingMT in the Translation Industry (JEC 10), pages 43-51.Denver, CO, 4 November 2010.Irina Temnikova.
2010.
Cognitive Evaluation Approachfor a Controlled Language Post-Editing Experiment.In LREC 2010: proceedings of the seventh interna-tional conference on Language Resources and Eval-uation, pages 3485-3490.
17-23 May 2010, Valletta,Malta.David Vilar, Jia Xu, Luis Fernando D?Haro and HermannNey.
2006.
Error analysis of statistical machine trans-lation output.
In LREC-2006: Fifth International Con-ference on Language Resources and Evaluation.
Pro-ceedings, pages 697-702.
Genoa, Italy, 22-28 May2006.190
