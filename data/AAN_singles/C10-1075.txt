Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 662?670,Beijing, August 2010Adaptive Development Data Selection for Log-linear Modelin Statistical Machine TranslationMu LiMicrosoft Research Asiamuli@microsoft.comYinggong Zhao?Nanjing Universityzhaoyg@nlp.nju.edu.cnDongdong ZhangMicrosoft Research Asiadozhang@microsoft.comMing ZhouMicrosoft Research Asiamingzhou@microsoft.comAbstractThis paper addresses the problem of dy-namic model parameter selection for log-linear model based statistical machinetranslation (SMT) systems.
In this work,we propose a principled method for thistask by transforming it to a test data de-pendent development set selection prob-lem.
We present two algorithms for au-tomatic development set construction, andevaluated our method on several NISTdata sets for the Chinese-English trans-lation task.
Experimental results showthat our method can effectively adaptlog-linear model parameters to differenttest data, and consistently achieves goodtranslation performance compared withconventional methods that use a fixedmodel parameter setting across differentdata sets.1 IntroductionIn recent years, log-linear model (Och and Ney,2002) has been a mainstream method to formu-late statistical models for machine translation.
Us-ing this formulation, various kinds of relevantproperties and data statistics used in the transla-tion process, either on the monolingual-side or onthe bilingual-side, are encoded and used as real-valued feature functions, thus it provides an ef-fective mathematical framework to accommodatea large variety of SMT formalisms with differentcomputational linguistic motivations.
?This work was done while the author was visiting Mi-crosoft Research Asia.Formally, in a log-linear SMT model, given asource sentence f , we are to find a translation e?with largest posterior probability among all possi-ble translations:e?
= argmaxePr(e|f)and the posterior probability distribution Pr(e|f)is directly approximated by a log-linear formula-tion:Pr(e|f) = p?
(e|f)= exp(?Mm=1 ?mhm(e, f))?e?
exp(?Mm=1 ?mhm(e?, f))(1)in which hm?s are feature functions and ?
=(?1, .
.
.
, ?M ) are model parameters (featureweights).For a successful practical log-linear SMTmodel, it is usually a combined result of the sev-eral efforts:?
Construction of well-motivated SMT models?
Accurate estimation of feature functions?
Appropriate scaling of log-linear model fea-tures (feature weight tuning).In this paper, we focus on the last mentionedissue ?
parameter tuning for log-linear model.In general, log-linear model parameters are opti-mized on a held-out development data set.
Us-ing this method, similarly to many machine learn-ing tasks, the model parameters are solely tunedbased on the development data, and the optimal-ity of obtained model on unseen test data relieson the assumption that both development and testdata observe identical probabilistic distribution,662which often does not hold for real-world data.
Thegoal of this paper is to investigate novel meth-ods for test data dependent model parameter se-lection.
We begin with discussing the principleof parameter learning for log-linear SMT models,and explain the rationale of task transformationfrom parameter selection to development data se-lection.
We describe two algorithms for automaticdevelopment set construction, and evaluated ourmethod on several NIST MT evaluation data sets.Experimental results show that our method can ef-fectively adapt log-linear model parameters to dif-ferent test data and achieves consistent good trans-lation performance compared with conventionalmethods that use a group of fixed model param-eters across different data sets.2 Model Learning for SMT withLog-linear ModelsModel learning refers to the task to estimate agroup of suitable log-linear model parameters?
= (?1, .
.
.
, ?M ) for use in Equation 1, which isoften formulated as an optimization problem thatfinds the parameters maximizing certain goodnessof the translations generated by the learnt modelon a development corpus D. The goodness can bemeasured with either the translations?
likelihoodor specific machine translation evaluation metricssuch as TER or BLEU.More specifically, let e?
be the most probabletranslation of D with respect to model parameters?, and E(e?,?, D) be a score function indicatingthe goodness of translation e?, then a parameterestimation algorithm will try to find the ?
whichsatisfies:??
= argmax?E(e?,?, D) (2)Note when the goodness scoring function E(?
)is specified, the parameter learning criterion inEquation 2 indicates that the derivation of modelparameters ??
only depends on development dataD, and does not require any knowledge of testdata T .
The underlying rationale for this rule isthat if the test data T observes the same distribu-tion as D, ??
will be optimal for both of them.On the other side, however, when there are mis-matches between development and test data, thetranslation performance on test data will be sub-optimal, which is very common for real-worlddata.
Due to the difference between data sets, gen-erally there is no such ??
that is optimal for multi-ple data sets at the same time.
Table 1 shows someempirical evidences when two data sets are mutu-ally used as development and test data.
In this set-ting, we used a hierarchical phrase based decoderand 2 years?
evaluation data of NIST Chinese-to-English machine translation task (for the year2008 only the newswire subset was used becausewe want to limit both data sets within the same do-main to show that data mismatch also exists evenif there is no domain difference), and report re-sults using BLEU scores.
Model parameters weretuned using the MERT algorithm (Och, 2003) op-timized for BLEU metric.Dev data MT05 MT08-nwMT05 0.402 0.306MT08-nw 0.372 0.343Table 1: Translation performance of cross devel-opment/test on two NIST evaluation data sets.In our work, we present a solution to this prob-lem by using test data dependent model parame-ters for test data translation.
As discussed above,since model parameters are solely determined bydevelopment dataD, selection of log-linear modelparameters is basically equivalent to selecting aset of development data D.However, automatic development data selectionin current SMT research remains a relatively openissue.
Manual selection based on human experi-ence and observation is still a common practice.3 Adaptive Model Parameter SelectionAn important heuristic behind manual develop-ment data selection is to use the dataset which isas similar to test set as possible in order to workaround the data mismatch problem to maximalextent.
There are also empirical evidences sup-porting this heuristics.
For instance, it is gener-ally perceived that data set MT03 is more similarto MT05, while MT06-nw is closer to MT08-nw.Table 2 shows experimental results using modelparameters induced from MT03 and MT06-nw as663development sets with the same settings as in Ta-ble 1.
As expected, MT06-nw is far more suitablethan MT03 as the development data for MT08-nw; yet for test set MT05, the situation is just theopposite.Dev data MT05 MT08-nwMT03 0.397 0.306MT06-nw 0.381 0.337Table 2: Translation performance on different testsets of using different development sets.In this work, this heuristic is further exploitedfor automatic development data selection whenthere is no prior knowledge of the test data avail-able.
In the following discussion, we assume theavailability of a set of candidate source sentencestogether with translation references that are qual-ified for the log-linear model parameter learningtask.
Let DF be the full candidate set, given a testset T , the task of selecting a set of developmentdata which can optimize the translation quality onT can be transformed to searching for a suitablesubset of DF which is most similar to T :D?
= argmaxD?DFSim(D,T )To achieve this goal, we need to address the fol-lowing key issues:?
How to define and compute Sim(D,T ), thesimilarity between different data sets;?
How to extract development data sets from afull candidate set for unseen test data.3.1 Dataset SimilarityComputing document similarity is a classical taskin many research areas such as information re-trieval and document classification.
However, typ-ical methods for computing document similaritymay not be suitable for our purpose.
The reasonsare two-fold:1.
The sizes of both development and test dataare small in usual circumstances, and usingsimilarity measures such as cosine or dicecoefficient based on term vectors will sufferfrom severe data sparseness problems.
As aresult, the obtained similarity measure willnot be statistically reliable.2.
More importantly, what we care about hereis not the surface string similarity.
Instead,we need a method to measure how similartwo data sets are from the view of a log-linearSMT model.Next we start with discussing the similaritybetween sentences.
Given a source sentencef , we denote its possible translation space withH(f).
In a log-linear SMT model, every trans-lation e ?
H(f) is essentially a feature vectorh(e) = (h1, .
.
.
, hM ).
Accordingly, the similar-ity between two sentences f1 and f2 should be de-fined on the feature space of the model in use.
LetV (f) = {h(e) : e ?
H(f)} be the set of featurevectors for all translations inH(f), we haveSim(f1, f2) = Sim(V (f1),V (f2))(3)Because it is not practical to compute Equation3 directly by enumerating all translations inH(f1)and H(f2) due to the huge search space in SMTtasks, we need to resort to some approximations.A viable solution to this is that if we can use asingle feature vector h?
(f) to represent V (f), thenEquation 3 can be simply computed using existingvector similarity measures.One reasonable method to derive h?
(f) is to usea feature vector based on the average principle ?each dimension of the vector is set to the expec-tation of its corresponding feature value over alltranslations:h?
(f) =?e?H(f)P (e|f)h(e) (4)An alternative and much simpler way to com-pute h?
(f) is to employ the max principle in whichwe just use the feature vector of the best transla-tion inH(f):h?
(f) = h(e?)
(5)where e?
= argmaxe P (e|f).Note that in both Equation 4 and Equation 5we make use of e?s posterior probability P (e|f).664Since the true distribution is unknown, a pre-learnt modelM has to be used to assign approxi-mate probabilities to translations, which indicatesthat the obtained similarity depends on a specificmodel.
As a convention, we use SimM(f1, f2) todenote the similarity between f1 and f2 based onM, and callM the reference model of the com-puted similarity.
To avoid unexpected bias causedby a single reference model, multiple referencemodels can be simultaneously used, and the simi-larity is defined to be the maximum of all model-dependent similarity values:Sim(f1, f2) = maxM SimM(f1, f2) (6)where M belongs to {M1, .
.
.
,Mn}, which isthe set of reference models under consideration.To generalize this method to data set level, wecompute the vector h?
(S) for a data set S =(f1, .
.
.
, f|S|) as follows:h?
(S) =|S|?i=1h?
(fi) (7)3.2 Development Sets Pre-constructionIn the following, we sketch a method for automat-ically building a set of development data based onthe full candidate set DF before seeing any testdata.Theoretically, a subset of DF containing ran-domly sampled sentences from DF will not meetour requirement well because it is very probablethat it will observe a distribution similar to DF .What we expect is that the pre-built developmentsets can approximate as many as possible typi-cal data distributions that can be estimated fromsubsets of DF .
Our solution is based on the as-sumption that DF can be depicted by some mix-ture models, hence we can use classical cluster-ing methods such as k-means to partition DF intosubsets with different distributions.Let SF be the set of extracted development datafrom DF .
The construction of SDF proceeds asfollowing:1.
Train a log-linear model MF using DF asdevelopment data;2.
Compute a feature vector h?
(d)1 for each sen-tence d ?
DF usingMF as reference model;3.
Cluster sentences in DF using h?
(d)/|d| asfeature vectors;4.
Add obtained sentence clusters to SDF ascandidate development sets.In the third step, since the feature vector h?
(d)is defined at sentence level, it is averaged by thenumber of words in d so that it is irrelevant to thelength of a sentence.
Considering the outputs ofunsupervised data clustering methods are usuallysensitive to initial conditions, we include in SDFsentence clusters based on different initializationconfigurations to remove related random effects.An initialization configuration for sentence clus-tering in our work includes starting point for eachcluster and total number of clusters.
In fact, theinclusion of more sentence clusters increases thediversity of the resulted SDF as well.At decoding time, when a test set T is pre-sented, we compute the similarity between T andeach development set D ?
SDF , and choose theone with largest similarity score as the develop-ment set for T :D?
= argmaxD?SDFSim(T,D) (8)When a single reference model is used to com-pute Sim(T,D), MF is a natural choice.
In themulti-model setting as shown in Equation 6, mod-els learnt from the development sets in SDF canserve this purpose.Note in this method model learning is not re-quired for every new test set because the modelparameters for each development set in SDF canalso be pre-learnt and ready to be used for decod-ing.3.3 Dynamic Development Set ConstructionIn the previous method, test data T is only in-volved in the process of choosing a developmentset from a list of candidates but not in process ofdevelopment set construction.
Next we present a1Throughout this paper, a development sentence d gener-ally refers to the source part of it if there is no extra explana-tion.665method for building a development set on demandbased on test data T .Let DF = (d1, .
.
.
, dn) be the data set con-taining all candidate sentences for developmentdata selection.
The method is iterative process inwhich development data and learnt model are al-ternatively updated.
Detailed steps are illustratedas follows:1.
Let i = 0, D0 = DF ;2.
Train a modelMi based on Di;3.
For each dk ?
DF , compute the similarityscore SimMi(T, dk) between T and dk basedon modelMi;4.
Select top n candidate sentences with highestsimilarity scores from DF to form Di+1;5.
Repeat step 2 to step 4 until the similarity be-tween T and latest selected development dataconverges (the increase in similarity measureis less than a specified threshold compared tolast round) or the specified iteration limit isreached.In step 4, Di+1 is greedily extracted from DF ,and there is no guarantee that SimMi(T,Di+1)will increase or decrease after a new sentence isadded to Di+1.
Thereby the number of selectedsentences n needs to be empirically determined.If n is too small, neither the selected data nor thelearnt model parameters will be statistically reli-able; while if n is too large, we may have to in-clude some sentences that are not suitable for testdata in the development data, and miss the oppor-tunity to extract the most desirable developmentset.One drawback of this method is the relativelyhigh computational cost because it requires multi-ple parameter training passes when any test set ispresented to the system for translation.4 Experiments4.1 DataExperiments were conducted on the data setsused for NIST Chinese-English machine transla-tion evaluation tasks.
MT03 and MT06 data sets,which contain 919 and 1,664 sentences respec-tively, were used for development data in vari-ous settings.
MT04, MT05 and MT08 data setswere used for test purpose.
In some settings, wealso used a test set MT0x, which containing 1,000sentences randomly sampled from the above 3data sets.
All the translation performance resultswere measured in terms of case-insensitive BLEUscores.For all experiments, all parallel corpora avail-able to the constrained track of NIST 2008Chinese-English MT evaluation task were usedfor translation model training, which consist ofaround 5.1M bilingual sentence pairs.
GIZA++was used for word alignment in both directions,which was further refined with the intersec-diag-grow heuristics.We used a 5-gram language model which wastrained from the Xinhua portion of English Giga-word corpus version 3.0 from LDC and the En-glish part of parallel corpora.4.2 Machine Translation SystemWe used an in-house implementation of the hierar-chical phrase-based decoder as described in Chi-ang (2005).
In addtion to the standard featuresused in Chiang (2005), we also used a lexicon fea-ture indicating how many word paris in the trans-lation found in a conventional Chinese-Englishlexicon.
Phrasal rules were extracted from all theparallel data, but hierarchical rules were only ex-tracted from the FBIS part of the parallel datawhich contains around 128,000 sentence pairs.For all the development data, feature weights ofthe decoder were tuned using the MERT algorithm(Och, 2003).4.3 Results of Development DataPre-constructionIn the following we first present some overall re-sults using the method of development data pre-construction, then dive into more detailed settingsof the experiments.Table 3 shows the results using 3 different datasets for log-linear model parameter tuning.
El-ements in the first column indicate the data setsused for parameter tuning, and other columns con-tain evaluation results on different test sets.
In the666Tuning set MT04 MT05 MT08 MT0xMT03 0.399 / 0.392 0.395 / 0.390 0.241 / 0.258 0.319 / 0.322MT06 0.381 / 0.388 0.382 / 0.391 0.275 / 0.283 0.343 / 0.342MT03+MT06 0.391 / 0.401 0.392 / 0.397 0.265 / 0.281 0.336 / 0.345Oracle cluster 0.401 0.398 0.293 0.345Self-training 0.406 0.402 0.298 0.351Table 3: Translation performance using different methods and data sets for parameter tuning.third row of the table, MT03+MT06 means com-bining the data sets of MT03 and MT06 togetherto form a larger tuning set.
The first number ineach cell denotes the BLEU score using the tuningset as standard development setD, and the secondfor using the tuning set as a candidate set DF .For all experiment settings in the table, weused cosine value between feature vectors to mea-sure similarity between data sets, and feature vec-tors were computed according to Equation 5 andEquation 7 using a reference model which istrained on the corresponding candidate set DF asdevelopment set.2 We adopted the k-means algo-rithm for data clustering with the number of clus-ters iterating from 2 to 5.
In each iteration, we ran4 passes of clustering using different initial values.Therefore, in total there are 56 sentence clustersgenerated in each SDF .3From the table it can be seen that giventhe same set of sentences (MT03, MT06 andMT03+MT06), when they are used as the can-didate set DF for the development set pre-construction method, the translation performanceis generally better than when they are just used asdevelopment sets as a whole.
Using MT03 dataset as DF is an exception: there is slight perfor-mance drop on test sets MT04 and MT05, but italso helps reduce the performance see-saw prob-lem on different test sets as shown in Table 1.Meanwhile, in the other two settings of DF , weobserved significant BLEU score increase on alltest sets but MT0x (on which the performance al-most kept unchanged).
In addition, the fact thatusing MT03+MT06 as DF achieves best (or al-2For example, in all the experiments in the row of MT03as DF , we use the same reference model trained with MT03as development set.3Sometimes some clusters are empty or contain too fewsentences, so the actual number may be smaller.most best) performance on all test sets implies thatit should be a better choice to include as diversedata as possible in DF .We also appended two oracle BLEU numbersfor each test set in Table 3 for reference.
One isdenoted with oracle cluster, which is the high-est possible BLEU that can be achieved on thetest set when the development set must be cho-sen from the sentence clusters in SMT03+MT06.The other is labeled as self-training, which is theBLEU score that can be obtained when the testdata itself is used as development data.
This num-ber can serve as actual performance upper boundon the test set.Next we investigated the impact of using dif-ferent ways to compute feature vectors presentedin Section 3.1.
We re-ran some previous exper-iments on test sets MT04, MT05 and MT08 us-ing MT03+MT06 as DF .
Most settings were keptunchanged except that the feature vector of eachsentence was computed according to Equation 4.A 20-best translation list was used to approximateH(f).
The results are shown in Table 4.Test set average maxMT04 0.397 0.401MT05 0.393 0.397MT08 0.286 0.281Table 4: Translation performance when using av-eraged feature values for similarity computation.The numbers in the second column are basedon Equation 4.
Numbers based on Equation 5 arealso listed in the third column for comparison.
Inall the experiment settings we did not observe con-sistent or significant advantage when using Equa-tion 4 over using Equation 5.
Since Equation 5667is much simpler, it is a good decision to use it inpractice.
So did we conduct all following experi-ments based on Equation 5.We are also interested in the correlation be-tween two measures: the similarity between de-velopment and test data and the actual translationperformance on test data.First we would like to echo the motivating ex-periment presented in Section 3.
Table 5 showsthe similarity between the data sets used in the ex-periment withMMT03+MT06 as reference model.Obviously the results in Table 2 and Table 5 fiteach other very well.Dev data MT05 MT08-nwMT03 0.99988 0.99012MT06-nw 0.99004 0.99728Table 5: Similarity between NIST data sets.Figure 1 shows the results of a set of more com-prehensive experiments on MT05 data set con-cerning the similarity between development andtest sets.0.280.30.320.340.360.380.410  20  30  40  50  60BLEURank of development setMultipleMT03+MT06MT06Figure 1: Correlation between similarity andBLEU on MT05 data setIn the figure, every data line shows how BLEUscore changes when different pre-built develop-ment set in SMT03+MT06 is used for model learn-ing.
The data points in each line are sorted by therank of similarity between the development set inuse and the MT05 data set.
We also compared re-sults based on 3 reference model settings.
In thefirst one (multiple), the similarity was computedusing Equation 6, and the reference model set con-tains all models learnt from the development setsin SMT03+MT06.
The other two settings use refer-ence models learnt from MT06 and MT03+MT06data sets respectively.We can observe from the figure that the corre-lation between BLEU scores and data set similar-ity can only be identified on macro scales for allthe three similarity settings.
Although using datasimilarity may not be able to select the perfect de-velopment data set from SDF , by picking a devel-opment set with highest similarity score, we canusually (almost always) get good enough BLEUscores in our experiments.4.4 Results of Development Data DynamicGenerationWe ran two sets of experiments for the method ofdevelopment data dynamic construction.The first one was designed to investigate howthe size of extracted development data affects thetranslation performance.
Using MT05 and MT08as test sets and MT03+MT06 as DF , we ran ex-periments for the algorithm presented in Section3.3 with n = 200 to n = 1, 000.
In this ex-periment we did not observe significant enoughchanges in BLEU scores ?
the difference betweenthe highest and lowest numbers is generally lessthan 0.005.The second one aimed at examining how BLEUnumbers changes when the extracted developmentdata were iteratively updated.
Figure 2 shows oneset of results on test sets MT05 and MT08 usingMT03+MT06 data set as DF and n set to 400.0.3650.370.3750.380.3850.390.3950.40.4051  2  3  4  5  6  70.2650.270.2750.280.2850.290.2950.30.305MT05BLEUMT08BLEUIterationMT05MT08Figure 2: BLEU score as function of iteration indynamic development data extraction.The similarity usually converged after 2 to 3 it-668erations, which is consistent with trend of BLEUscores on test sets.
However, in all our experimen-tal settings, we did not observe any results signif-icantly better than using the development set pre-construction method.5 DiscussionsSome of the previous work related to buildingadaptive SMT systems were discussed in the do-main adaptation context, in which one fundamen-tal idea is to estimate a more suitable domain-specific translation model or language model.When the target domain is already known, addinga small amount of domain data (both monolingualand bilingual) to the existing training corpora hasbeen shown to be very effective in practice.
Butmodel adaptation is required in more scenariosother than explicitly defined domains.
As shownby the results in Table 2, even for the data fromthe same domain, distribution mismatch can alsobe a problem.There are also considerable efforts made to dealwith the unknown distribution of text to be trans-lated, and the research topics were still focused ontranslation and language model adaptation.
Typ-ical methods used in this direction include dy-namic data selection (Lu?
et al, 2007; Zhao et al,2004; Hildebrand et al, 1995) and data weighting(Foster and Kuhn, 2007; Matsoukas et al, 2009).All the mentioned methods use information re-trieval techniques to identify relevant training datafrom the entire training corpora.Our work presented here also makes no as-sumption about the distribution of test data, butit differs from the previous methods significantlyfrom a log-linear model?s perspective.
Adjust-ing translation and language models based on testdata can be viewed as adaptation of feature val-ues, while our method is essentially adaptation offeature weights.
This difference makes these twokinds of methods complementary to each other ?it is possible to make further improvement by us-ing both of them in one task.To our knowledge, there is no dedicated discus-sion on principled methods to perform develop-ment data selection in previous research.
In Lu?et al (2007), log-linear model parameters canalso be adjusted at decoding time.
But in theirapproach, the adjustment was based on heuristicrules and re-weighted training data distribution.In addition, compared with training data selection,the computational cost of development data selec-tion is much smaller.From machine learning perspective, both pro-posed methods can be viewed as certain formof transductive learning applied to the SMT task(Ueffing et al, 2007).
But our methods donot rely on surface similarities between trainingand training/development sentences, and develop-ment/test sentences are not used to re-train SMTsub-models.6 Conclusions and Future WorkIn this paper, we addressed the data mismatch is-sue between training and decoding time of log-linear SMT models, and presented principledmethods for dynamically inferring test data de-pendent model parameters with development setselection.
We describe two algorithms for thistask, development set pre-construction and dy-namic construction, and evaluated our methodon the NIST data sets for the Chinese-Englishtranslation task.
Experimental results show thatour methods are capable of consistently achiev-ing good translation performance on multipletest sets with different data distributions withoutmanual tweaking of log-linear model parameters.Though theoretically using the dynamic construc-tion method could bring better results, the pre-construction method performs comparably well inour experimental settings.
Considering the factthat the pre-consruction method is computation-ally cheaper, it should be a better choice in prac-tice.In the future, we are interested in two direc-tions.
One is to explore the possibility to performdata clustering on test set as well and choosingsuitable model parameters for each cluster sepa-rately.
The other involves dynamic SMT modelselection ?
for example, some parts of the testdata fit the phrase-based model better while otherparts can be better translated using a syntax-basedmodel.669ReferencesDavid Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.of the 43th Annual Meeting of the Associationfor Computational Linguistic (ACL).
Ann Arbor,Michigan.George Foster and Roland Kuhn.
2007.
Mixture-Model Adaptation for SMT.
In Proc.
of the SecondACL Workshop on Statistical Machine Translation..Prague, Czech Republic.Michel Galley, Mark Hopkins, Kevin Knight andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of the Human Language Technology Conf.(HLT-NAACL).
Boston, Massachusetts.Almut Hildebrand, Matthias Eck, Stephan Vogel, andAlex Waibel.
1995.
Adaptation of the Transla-tion Model for Statistical Machine translation Basedon Information Retrieval.
In Proc.
of EAMT.
Bu-dapest, Hungary.Philipp Koehn, Franz Och and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proc.
of theHuman Language Technology Conf.
(HLT-NAACL).Edmonton, Canada.Yang Liu, Qun Liu, and Shouxun Lin.
2007.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of the 45th Annual Meet-ing of the Association for Computational Linguistic(ACL).
Prague, Czech Republic.Yajuan Lu?, Jin Huang and Qun Liu.
2007.
Improv-ing Statistical Machine Translation Performance byTraining Data Selection and Optimization.
In Proc.of the Conference on Empirical Methods in NaturalLanguage Processing.
Prague, Czech Republic.Spyros Matsoukas, Antti-Veikko I. Rosti and BingZhang.
2009.
Discriminative Corpus Weight Es-timation for Machine Translation.
In Proc.
of theConference on Empirical Methods in Natural Lan-guage Processing.
Singapore.Franz Och and Hermann Ney.
2002.
DiscriminativeTraining and Maximum Entropy Models for Statis-tical Machine Translation.
In Proc.
of the 40th An-nual Meeting of the Association for ComputationalLinguistic (ACL).
Philadelphia, PA.Franz Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of the 41thAnnual Meeting of the Association for Computa-tional Linguistic (ACL).
Sapporo, Japan.Nicola Ueffing, Gholamreza Haffari and AnoopSarkar.
2007.
Transductive learning for statisticalmachine translation.
In Proc.
of the Annual Meet-ing of the Association for Computational Linguis-tics.
Prague, Czech Republic.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language Model Adaptation for Statistical MachineTranslation with Structured Query Models.
In Proc.of COLING.
Geneva, Switzerland.670
