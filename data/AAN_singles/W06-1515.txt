Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 109?114,Sydney, July 2006. c?2006 Association for Computational LinguisticsConstraint-based Computational Semantics:A Comparison between LTAG and LRSLaura KallmeyerUniversity of T?bingenCollaborative Research Center 441lk@sfs.uni-tuebingen.deFrank RichterUniversity of T?bingenCollaborative Research Center 441fr@sfs.uni-tuebingen.deAbstractThis paper compares two approaches tocomputational semantics, namely seman-tic unification in Lexicalized Tree Ad-joining Grammars (LTAG) and LexicalResource Semantics (LRS) in HPSG.There are striking similarities between theframeworks that make them comparable inmany respects.
We will exemplify the dif-ferences and similarities by looking at sev-eral phenomena.
We will show, first of all,that many intuitions about the mechanismsof semantic computations can be imple-mented in similar ways in both frame-works.
Secondly, we will identify someaspects in which the frameworks intrin-sically differ due to more general differ-ences between the approaches to formalgrammar adopted by LTAG and HPSG.1 IntroductionThis paper contrasts two frameworks for compu-tational semantics, the proposal for semantics inLTAG described in (Kallmeyer and Romero, 2005)and LRS (Richter and Sailer, 2004), a computa-tional semantics framework formulated in Head-Driven Phrase Structure Grammar (HPSG).There are significant differences between LTAGand HPSG.
LTAG is a mildly context-sensitivelexicalized formalism characterized by an ex-tended domain of locality.
HPSG is based on theidea of a separation of the lexicon and syntacticstructure and on the strict locality of general gram-mar principles that are formulated in an expres-sive and very flexible logical description language.These fundamental differences are reflected in therespective architectures for semantics: LTAG as-sumes a separate level of underspecified semanticrepresentations; LRS uses the description logic ofsyntax for semantic specifications.However, despite the different mathematicalstructures, we find striking similarities betweenLTAG semantics with unification and LRS.
Theyboth show similar intuitions underlying specificanalyses, use the same higher order type-theoreticlanguage (Ty2, (Gallin, 1975)) as a means forspecifying the truth conditions of sentences, andemploy a feature logic in the combinatorial seman-tics instead of the lambda calculus.
Because ofthese similarities, analyses using both approachesare closely related and can benefit from each other.The paper is structured as follows: Sections 2and 3 will introduce the two frameworks.
Thenext three sections (4?6) will sketch analyses ofsome phenomena in both frameworks that will re-veal relevant relations between them.
Section 7presents a summary and conclusion.2 LTAG semanticsIn (Kallmeyer and Romero, 2005), each elemen-tary tree is linked to a semantic representation (aset of Ty2 formulas and scope constraints).
Ty2formulas (Gallin, 1975) are typed ?-terms with in-dividuals and situations as basic types.
The scopeconstraints of the form x ?
y specify subordina-tion relations between Ty2 terms.
In other words,x ?
y indicates that y is a component of x.A semantic representation is equipped with asemantic feature structure description.
Semanticcomputation is done on the derivation tree andconsists of certain feature value equations betweenmother and daughter nodes in the derivation tree.
(1) John always laughs.As an example, see Fig.
1 showing the deriva-tion tree for (1) with semantic representations and109l1 : laugh( 1 )??
?NP[GLOBAL[I 1]]VP[B[P l1]]??
?np vpjohn(x) l2 : always( 3 ),3 ?
4[GLOBAL[I x]] ??
?VPr[B[P l2]]VPf[B[P 4]]??
?Figure 1: LTAG semantics of (1)semantic feature structure descriptions as nodelabels.
The additional feature equations in thisexample are depicted using dotted lines.
Theyarise from top-bottom feature identifications par-allel to the unifications performed in FTAG (Vijay-Shanker and Joshi, 1988) and from identificationsof global features.
They yield 1 = x and 4 = l1.Applying these identities to the semantic represen-tations after having built their union leads to (2).The constraint 3 ?
l1 states that l1 : laugh(x) isa component of 3 .
(2) john(x), l2 : always( 3 ), l1 : laugh(x),3 ?
l1Note that the feature structure descriptions donot encode the semantic expressions one is inter-ested in.
They only encode their contributions tofunctional applications by restricting the argumentslots of certain predicates in the semantic repre-sentations: They state which elements are con-tributed as possible arguments for other seman-tic expressions and which arguments need to befilled.
They thereby simulate lambda abstractionand functional application while assembling thesemantic representations.
To achieve this, a re-stricted first order logic is sufficient.Semantic computation is local on the derivationtree: The new feature equations that are added de-pend only on single edges in the derivation tree.Because of this, even with the extension to seman-tics, the formalism is still mildly context-sensitive.3 LRSIn LRS the feature logic specifies the entire gram-mar, including well-formed Ty2 terms as seman-tic representations, and their mode of composi-tion.
Instead of the lambda calculus of tradi-tional Montague Grammar, LRS crucially uses anovel distinction between three aspects of the log-ical representations of signs (external content, in-ternal content, and parts).
LRS constraints es-tablish sub-term relationships between pieces ofsemantic representations within and across signs,thereby specifying the combinatorial properties ofthe semantics.
The subterm or component-of con-ditions (symbolized as /) are imposed by gram-mar principles.
Since these principles are descrip-tions of object-language expressions, they permitthe application of various underspecification tech-niques of computational semantics, although anLRS grammar does not employ underspecified se-mantic representations, in contrast to LTAG se-mantics.Fig.
2 shows an HPSG description of the syn-tactic tree and the LRS specifications of (1).
Thesyntactic trees in HPSG correspond to the derivedtrees of LTAG.
Since HPSG does not have deriva-tion trees, the LRS principles refer to derived trees.NP?
?exc 1inc 1p ?
1 john???JohnA?
?exc 5inc 5 always( 3 )p ?
5 , 5a always???alwaysV?
?exc 4inc 2 laugh( 1 )p ?
2 , 2a laugh??
?laughsadj headVP?
?exc 4inc 2p ?
2 , 2a , 5 , 5a ??
?& 2 / 3 & 5 / 4comp headS?
?exc 4 always(laugh(john))inc 2p ?
2 , 2a , 5 , 5a , 1 ??
?Figure 2: LRS analysis of (1)Each word lexically specifies its contribution tothe overall meaning of the sentence (P(ARTS)), thepart of its semantics which is outscoped by allsigns the word combines with (INC(ONT)), andthe overall semantic contribution of its maximalprojection (EXC(ONT)).
Feature percolation prin-ciples identify INC and EXC, respectively, alonghead projections and collect the elements of thePARTS lists of the daughters at each phrase.
Thecombination of the adjunct with a verbal pro-jection introduces two component-of constraints:The EXC of always must be within the EXC oflaughs, and the INC of laughs must be in thescope of always.
The semantic argument of110laughs (john) is identified by subcategorization(not shown in Fig.
2).
A closure condition requiresthat the semantic representation of an utteranceuse up all and only the PARTS contributions of allsigns, which yields 4 = always(laugh(john)).4 Quantifier scope4.1 Specifying a scope window(3) Exactly one student admires every professor:?
> ?,?
> ?
(4) John seems to have visited everybody:seem > ?,?
> seemQuantificational NPs in English can in princi-ple scope freely (see (3) and (4)).
An analysis ofquantifier scope must guarantee only two things:1. the proposition to which a quantifier attachesmust be in its nuclear scope, and 2. a quantifiercannot scope higher than the next finite clause.One way to model this is to define a scope win-dow delimited by a maximal scope and a minimalscope for a quantifier.
Both LTAG and LRS, spec-ify such scope windows for quantifiers.
We willnow outline the two analyses.
(5) Everybody laughs.
(Kallmeyer and Romero, 2005) use global fea-tures MAXS and MINS for the limits of the scopewindow.
Fig.
3 shows the LTAG analysis of (5).The feature identifications (indicated by dottedlines) lead to the constraints 2 ?
5 , 5 ?
l1.These constraints specify an upper and a lowerboundary for the nuclear scope 5 .
With the as-signments following from the feature identifica-tions we obtain the semantic representation (6):(6)l1 : laugh(x),l2 : every(x, 4 , 5 ), l3 : person(x)2 ?
l1,4 ?
l3, 2 ?
5 , 5 ?
l1There is one possible disambiguation consis-tent with the scope constraints, namely 2 ?
l2,4 ?
l3, 5 ?
l1.
This leads to the semanticsevery(x, person(x), laugh(x)).In LRS, the EXCONT value of the utterance isthe upper boundary while the INCONT value of thesyntactic head a quantifier depends on is the lowerboundary for scope, as illustrated in Fig.
4.
Theupper boundary is obtained through the interactionof 1) a PROJECTION PRINCIPLE stating that thel1 : laugh( 1 ),2 ?
3npl2 : every(x, 4 , 5 ),l3 : person(x),4 ?
l3,6 ?
5 , 5 ?
7???
?GLOBAL[MINS l1MAXS 2]NP[GLOBAL[I 1]]???????
?GLOBAL[I x]NP[GLOBAL[MINS 7MAXS 6]]???
?Figure 3: LTAG analysis of (5) Everybody laughsPARTS list of a phrase contains all elements on thePARTS lists of its daughters, and 2) the EXCONTPRINCIPLE which states that a) the PARTS list ofeach non-head contains its own EXCONT, and b)in an utterance, everything on the PARTS list is acomponent of the EXCONT.
This leads to the con-straint 4  6 in Fig.
4, among others.
The lowerboundary is obtained from the SEMANTICS PRIN-CIPLE which states that if the non-head of a headedphrase is a quantifier, then the INCONT of the headis a component of its nuclear scope.
This yields1  ?
in Fig.
4.S?
?EXC 6 ?x(person(x)?
laugh(x))INC 1P ?x, 1 , 1a , 2 , 2a , 4 , 4a ??
?NP VP??
?EXC 4 ?x (?
?
?
)INC 2 person(x)P ?x, 2 , 2a person,4 , 4a ?
?
??????
?EXC 6INC 1 laugh(x)P ?
1 , 1a laugh??
?everybody laughsRelevant subterm constraints: 2  ?
(from the lexical entryof everybody), 1  ?, 4  6Figure 4: LRS analysis of (5) Everybody laughsThe striking similarity between the two anal-yses shows that, despite the fundamental differ-ences between the frameworks, central insightscan be modelled in parallel.4.2 Nested quantifiersThe use of the upper limit of the scope windows is,however, slightly different: EXCONT contains thequantifier itself as a component while MAXS limitsonly the nuclear scope, not the quantifier.
Conse-quently, in LTAG the quantifier can scope higher111than the MAXS limiting its nuclear scope but inthis case it takes immediate scope over the MAXS.
(7) Two policemen spy on someone from everycity: ?
> ?
> 2 (among others)The LTAG analysis is motivated by nested quan-tifiers.
In sentences such as (7), the embeddedquantifier can take scope over the embedding onebut if so, this must be immediate scope.
In otherwords, other quantifiers cannot intervene.
In (7),the scope order ?
> 2 > ?
is therefore not pos-sible.1 The LTAG analysis is such that the max-imal nuclear scope of the embedded quantifier isthe propositional label of the embedding quanti-fier.2In LRS, the way the scope window is speci-fied, a corresponding constraint using the EXCONTof the embedded quantifier cannot be obtained.The LRS principle governing the distribution ofembedded quantifiers in complex NPs states di-rectly that in this syntactic environment, the em-bedded quantifier may only take direct scope overthe quantifier of the matrix NP.
This principledoes not refer to the notion of external content atall.
At this point it is an open question whetherLRS could learn from LTAG here and adapt thescope window so that an analogous treatment ofnested quantifiers would be possible.5 LTAG?s extended domain of localityWhereas the treatment of quantification sketchedin the preceding section highlights the similaritiesbetween LTAG semantics and LRS, this and thefollowing section will illustrate some fundamentaldifferences between the frameworks.In spite of the parallels mentioned above, evenINCONT and MINS differ sometimes, namely insentences containing bridge verbs.
This is relatedto the fact that LTAG has an extended domain oflocality whereas HPSG does not.
Let us illustratethe difference with the example (8).
(8) Mary thinks John will come.1(Joshi et al, 2003) propose an extra mechanism thatgroups quantifiers into sets in order to derive these con-straints.
(Kallmeyer and Romero, 2005) however show thatthese constraints can be derived even if the upper limit MAXSfor nuclear scope is used as sketched above.2Note that this approach requires constraints of the forml ?
n with l being a label, n a variable.
This goesbeyond the polynomially solvable normal dominance con-straints (Althaus et al, 2003).
This extension, though, isprobably still polynomially solvable (Alexander Koller, per-sonal communication).In LTAG, the two elementary verb trees (forthinks and will come) have different global MINSfeatures.
The one for thinks is the label of the thinkproposition while the one for will come is the labelof the embedded proposition.
As a consequence, aquantifier which attaches to the matrix verb cannotscope into the embedded clause.
This distinctionof different MINS values for different verb trees isnatural in LTAG because of the extended domainof locality.In LRS, all verbal nodes in the constituent struc-ture of (8) carry the same INCONT value, namelythe proposition of the embedded verb.
Conse-quently, the minimal scope of quantifiers attachingeither to the embedding or to the embedded verbis always the proposition of the embedded verb.However, due to the requirement that variables bebound, a quantifier binding an argument of the em-bedding verb cannot have narrow scope over theembedded proposition.How to implement the LTAG idea of differentINCONT values for the embedding and the embed-ded verb in LRS is not obvious.
One might intro-duce a new principle changing the INCONT valueat a bridge verb, whereby the new INCONT wouldget passed up, and the embedded INCONT wouldno longer be available.
This would be problem-atic: Take a raising verb as in (9) (adjoining to theVP node in LTAG) instead of a bridge verb:(9) Most people seem to everybody to like thefilm.Here the minimal scope of most people shouldbe the like proposition while the minimal scopeof everybody is the seem proposition.
In LTAGthis does not pose a problem since, due to the ex-tended domain of locality, most people attaches tothe elementary tree of like even though the seemtree is adjoined in between.
If the INCONT treat-ment of LRS were modified as outlined above andseem had an INCONT value that differed from theINCONT value of the embedded like proposition,then the new INCONT value would be passed upand incorrectly provide the minimal scope of mostpeople.
LRS must identify the two INCONTs.The difference between the two analyses illus-trates the relevance of LTAG?s extended domain oflocality not only for syntax but also for semantics.6 Negative ConcordThe analysis of negative concord in Polish de-scribed in this section highlights the differences112in the respective implementation of underspeci-fication techniques in LTAG and LRS.
Recallthat both LTAG and LRS use component-of con-straints.
But in LTAG, these constraints link ac-tual Ty2-terms (i.e., objects) to each other, whilein LRS, these constraints are part of a descriptionof Ty2-terms.
(10) Janek nie pomaga ojcu.Janek NM helps father?Janek doesn?t help his father.?
(11) a. Janek nie pomaga nikomu.Janek NM helps nobody?Janek doesn?t help anybody.?b.
?Janek pomaga nikomu.
(12) Nikt nie przyszed?.nobody NM came?Nobody came.
?The basic facts of sentential negation and nega-tive concord in Polish are illustrated in (10)?
(12):The verbal prefix nie is obligatory for sententialnegation, and it can co-occur with any numberof n-words (such as nikt, ?anybody?)
without everleading to a double negation reading.
As a conse-quence, (12) expresses only one logical sententialnegation, although the negation prefix nie on theverb and the n-word nikt can carry logical nega-tion alone in other contexts.
LRS takes advantageof the fact that its specifications of semantic repre-sentations are descriptions of logical expressionswhich can, in principle, mention the same partsof the expressions several times.
Fig.
5 showsthat both nikt and the verb nie przyszed?
introducedescriptions of negations ( 4 and 2 , respectively).The constraints of negative concord in Polish willthen conspire to force the negations contributed bythe two words to be the same in the overall logicalrepresentation 6 of the sentence.Such an analysis is not possible in LTAG.
Eachnegation in the interpretation corresponds to ex-actly one negated term introduced in the seman-tic representations.
Therefore, the negative parti-cle nie necessarily introduces the negation whilethe n-word nikt requires a negation in the proposi-tion it attaches to.
An analysis along these lines issketched in Fig.
6 (?GL?
stands for ?GLOBAL?
).The requirement of a negation is checked witha feature NEG indicating the presence of a nega-tion.
The scope of the negation (feature N-SCOPE)?
?EXC 6 ??e?x(person(x)?
come(e, x))INC 1P ?e, x, 0 , 1 , 1a , 1b , 2 , 3 , 3a , 4 , 5 , 5a ??
?nikt nie przyszed???
?EXC 5 ?x (?
?
?
)INC 3 person(x)P ?x, 3 , 3a person,4?
?, 5 , 5a ?
?
?????????
?EXC 6INC 1 come(e, x)P ?e, 1 , 1a come e,1b come, 2?
?,0 ?e??????
?1  ?, 2  6 , 5  ?, 3  ?, 1  ?, 1  ?, 1  ?Figure 5: LRS analysis of (12) Nikt nie przyszed?marks the maximal scope of the existential quan-tifier of the n-word nikt (constraint 7 ?
6 ).3SNP VPVNP nie Vnikt przyszed?l1 : ?
1 ,l2 : come( 2 , 3 )1 ?
l2, 4 ?
l1npl3 : some(x, 5 , 6 ),l4 : person(x)5 ?
l4,7 ?
6 , 6 ?
8???????GL??
?MAXS 4N-SCOPE 1MINS l2NEG yes??
?NP[GL[I 2]]???????????
?GL[I x]NP?
?GL[N-SCOPE 7MINS 8NEG yes]??????
?Figure 6: LTAG analysis of (12) Nikt nie przyszed?This example illustrates that the two frame-works differ substantially in their treatment of un-derspecification: 1.
LRS employs partial descrip-tions of fully specified models, whereas LTAGgenerates underspecified representations in thestyle of (Bos, 1995) that require the definition ofa disambiguation (a ?plugging?
in the terminol-ogy of Bos).
2.
LRS constraints contain not Ty2terms but descriptions of Ty2 terms.
Therefore, incontrast to LTAG, two descriptions can denote thesame formula.
Here, LTAG is more limited com-pared to LRS.
On the other hand, the way seman-tic representations are defined in LTAG guarantees3See (Lichte and Kallmeyer, 2006) for a discussion ofNEG and N-SCOPE in the context of NPI-licensing.113that they almost correspond to normal dominanceconstraints, which are known to be polynomiallyparsable.
The difference in the use of underspecifi-cation techniques reflects the more general differ-ence between a generative rewriting system suchas LTAG, in which the elements of the grammarare objects, and a purely description-based for-malism such as HPSG, in which token identitiesbetween different components of linguistic struc-tures are natural and frequently employed.7 Summary and ConclusionLTAG and LRS have several common characteris-tics: They both 1. use a Ty2 language for seman-tics; 2. allow underspecification (LTAG scope con-straints ?
versus LRS component-of constraints); 3. use logical descriptions for semantic com-putation; 4. are designed for computational appli-cations.
Due to these similarities, some analysescan be modelled in almost identical ways (e.g., thequantifier scope analyses, and the identification ofarguments using attribute values rather than func-tional application in the lambda calculus).
We takethe existence of this clear correspondence as in-dicative of deeper underlying insight into the func-tioning of semantic composition in natural lan-guages.Additionally, the differences between theframeworks that can be observed on the level ofsyntax carry over to semantics: 1.
LTAG?s ex-tended domain of locality allows the localizationwithin elementary trees of syntactic and seman-tic relations between elements far apart from eachother on the level of constituent structure.
2.
LTAG(both syntax and semantics) is a formalism withrestricted expressive power that guarantees goodformal properties.
The restrictions, however, canbe problematic.
Some phenomena can be moreeasily described in a system such as HPSG andLRS while their description is less straightfor-ward, perhaps more difficult or even impossiblewithin LTAG.
The concord phenomena describedin section 7 are an example of this.A further noticable difference is that within the(Kallmeyer and Romero, 2005) framework, thederivation tree uniquely determines both syntac-tic and semantic composition in a context-freeway.
Therefore LTAG semantics is mildly context-sensitive and can be said to be compositional.As far as LRS is concerned, it is not yet knownwhether it is compositional or not; compositional-ity (if it holds at all) is at least less straightforwardto show than in LTAG.In conclusion, we would like to say that the sim-ilarities between these two frameworks permit adetailed and direct comparison.
Our comparativestudy has shed some light on the impact of the dif-ferent characteristic properties of our frameworkson concrete semantic analyses.AcknowledgmentsFor many long and fruitful discussions of variousaspects of LTAG semantics and LRS, we wouldlike to thank Timm Lichte, Wolfgang Maier, Mari-bel Romero, Manfred Sailer and Jan-Philipp S?hn.Furthermore, we are grateful to three anonymousreviewers for helpful comments.ReferencesErnst Althaus, Denys Duchier, Alexander Koller, KurtMehlhorn, Joachim Niehren, and Sven Thiel.
2003.An efficient graph algorithm for dominance con-straints.
Journal of Algorithms, 48(1):194?219.Johan Bos.
1995.
Predicate logic unplugged.
In PaulDekker and Martin Stokhof, editors, Proceedings ofthe 10th Amsterdam Colloquium, pages 133?142.Daniel Gallin.
1975.
Intensional and Higher-OrderModal Logic with Applications to Montague Seman-tics.
North Holland mathematics studies 19.
North-Holland Publ.
Co., Amsterdam.Aravind K. Joshi, Laura Kallmeyer, and MaribelRomero.
2003.
Flexible Composition in LTAG:Quantifier Scope and Inverse Linking.
In HarryBunt, Ielka van der Sluis, and Roser Morante, ed-itors, Proceedings of the Fifth International Work-shop on Computational Semantics IWCS-5, pages179?194, Tilburg.Laura Kallmeyer and Maribel Romero.
2005.
Scopeand Situation Binding in LTAG using Semantic Uni-fication.
Submitted to Research on Language andComputation.
57 pages., December.Timm Lichte and Laura Kallmeyer.
2006.
LicensingGerman Negative Polarity Items in LTAG.
In Pro-ceedings of The Eighth International Workshop onTree Adjoining Grammar and Related Formalisms(TAG+8), Sydney, Australia, July.Frank Richter and Manfred Sailer.
2004.
Basic con-cepts of lexical resource semantics.
In Arnold Beck-mann and Norbert Preining, editors, ESSLLI 2003 ?Course Material I, (= Collegium Logicum, 5), pages87?143.
Kurt G?del Society, Wien.K.
Vijay-Shanker and Aravind K. Joshi.
1988.
Featurestructures based tree adjoining grammar.
In Pro-ceedings of COLING, pages 714?719, Budapest.114
