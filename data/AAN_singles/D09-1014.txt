Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 131?140,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPGeneralized Expectation Criteria forBootstrapping Extractors using Record-Text AlignmentKedar BellareDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003kedarb@cs.umass.eduAndrew McCallumDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003mccallum@cs.umass.eduAbstractTraditionally, machine learning ap-proaches for information extractionrequire human annotated data that can becostly and time-consuming to produce.However, in many cases, there alreadyexists a database (DB) with schemarelated to the desired output, and recordsrelated to the expected input text.
Wepresent a conditional random field (CRF)that aligns tokens of a given DB recordand its realization in text.
The CRF modelis trained using only the available DB andunlabeled text with generalized expecta-tion criteria.
An annotation of the textinduced from inferred alignments is usedto train an information extractor.
We eval-uate our method on a citation extractiontask in which alignments between DBLPdatabase records and citation texts areused to train an extractor.
Experimentalresults demonstrate an error reductionof 35% over a previous state-of-the-artmethod that uses heuristic alignments.1 IntroductionA substantial portion of information on the Webconsists of unstructured and semi-structured text.Information extraction (IE) systems segment andlabel such text to populate a structured databasethat can then be queried and mined efficiently.In this paper, we mainly deal with informationextraction from text fragments that closely resem-ble structured records.
Examples of such textsinclude citation strings in research papers, con-tact addresses on person homepages and apart-ment listings in classified ads.
Pattern match-ing and rule-based approaches for IE (Brin, 1998;Agichtein and Gravano, 2000; Etzioni et al, 2005)that only use specific patterns, and delimiter andfont-based cues for segmentation are prone to fail-ure on such data because these cues are gen-erally not broadly reliable.
Statistical machinelearning methods such as hidden Markov models(HMMs) (Rabiner, 1989; Seymore et al, 1999;Freitag and McCallum, 1999) and conditional ran-dom fields (CRFs) (Lafferty et al, 2001; Pengand McCallum, 2004; Sarawagi and Cohen, 2005)have become popular approaches to address thetext extraction problem.
However, these methodsrequire labeled training data, such as annotatedtext, which is often scarce and expensive to pro-duce.In many cases, however, there already exists adatabase with schema related to the desired out-put, and records that are imperfectly rendered inthe available unlabeled text.
This database canserve as a source of significant supervised guid-ance to machine learning methods.
Previous workon using databases to train information extrac-tors has taken one of three simpler approaches.In the first, a separate language model is trainedon each column of the database and these mod-els are then used to segment and label a giventext sequence (Agichtein and Ganti, 2004; Can-isius and Sporleder, 2007).
However, this ap-proach does not model context, errors or differ-ent formats of fields in text, and requires largenumber of database entries to learn an accuratelanguage model.
The second approach (Sarawagiand Cohen, 2004; Michelson and Knoblock, 2005;Mansuri and Sarawagi, 2006) uses database ordictionary lookups in combination with similaritymeasures to add features to the text sequence.
Al-though these features are very informative, learn-ing algorithms still require annotated data to makeuse of them.
The final approach heuristicallylabels texts using matching records and learnsextractors from these annotations (Ramakrishnanand Mukherjee, 2004; Bellare and McCallum,2007; Michelson and Knoblock, 2008).
Heuris-131tic labeling decisions, however, are made indepen-dently without regard for the Markov dependen-cies among labels in text and are sensitive to subtlechanges in text.Here we propose a method that automaticallyinduces a labeling of an input text sequence us-ing a word alignment with a matching databaserecord.
This induced labeling is then used to traina text extractor.
Our approach has several advan-tages over previous methods.
First, we are ableto model field ordering and context around fieldsby learning an extractor from annotations of thetext itself.
Second, a probabilistic model for wordalignment can exploit dependencies among align-ments, and is also robust to errors, formatting dif-ferences, and missing fields in text and the record.Our word alignment model is a conditional ran-dom field (CRF) (Lafferty et al, 2001) that gen-erates alignments between tokens of a text se-quence and a matching database record.
Thestructure of the graphical model resembles IBMModel 1 (Brown et al, 1993) in which each tar-get (record) word is assigned one or more source(text) words.
The alignment is generated con-ditioned on both the record and text sequence,and therefore supports large sets of rich and non-independent features of the sequence pairs.
Ourmodel is trained without the need for labeled wordalignments by using generalized expectation (GE)criteria (Mann and McCallum, 2008) that penal-ize the divergence of specific model expectationsfrom target expectations.
Model parameters areestimated by minimizing this divergence.
To limitover-fitting we include a L2-regularization term inthe objective.
The model expectations in GE cri-teria are taken with respect to a set of alignmentlatent variables that are either specific to each se-quence pair (local) or summarizing the entire dataset (global).
This set is constructed by includingall alignment variables a that satisfy a certain bi-nary feature (e.g., f(a,x1,y1,x2) = 1, for la-beled record (x1,y1), and text sequence x2).
Oneexample global criterion is that ?an alignment ex-ists between two orthographically similar1words95% of the time.?
Here the criterion has a targetexpectation of 95% and is defined over alignments{a = ?i, j?
| x1[i] ?
x2[j],?x1,x2}.
Another cri-terion for extraction can be ?the word ?EMNLP?is always aligned with the record label booktitle?.1Two words are orthographically similar if they have lowedit distance.This criterion has a target of 100% and definedfor {a = ?i, j?
| y1[i] = booktitle ?
x2[j] =?EMNLP?,?y1,x2}.
One-to-one correspondencebetween words in the sequence pair can be speci-fied as collection of local expectation constraints.Since we directly encode prior knowledge of howalignments behave in our criteria, we obtain suffi-ciently accurate alignments with little supervision.We apply our method to the task of citationextraction.
The input to our training algorithmis a set of matching DBLP2-record/citation-textpairs and global GE criteria3of the following twotypes: (1) alignment criteria that consider fea-tures of mapping between record and text words,and, (2) extraction criteria that consider featuresof the schema label assigned to a text word.
Inour experiments, the parallel record-text pairs arecollected manually but this process can be auto-mated using systems that match text sequencesto records in the DB (Michelson and Knoblock,2005; Michelson and Knoblock, 2008).
Such sys-tems achieve very high accuracy close to 90% F1on semi-structured domains similar to ours.4Ourtrained alignment model can be used to directlyalign new record-text pairs to create a labeling ofthe texts.
Empirical results demonstrate a 20.6%error reduction in token labeling accuracy com-pared to a strong baseline method that employs aset of high-precision alignments.
Furthermore, weprovide a 63.8% error reduction compared to IBMModel 4 (Brown et al, 1993).
Alignments learnedby our model are used to train a linear-chain CRFextractor.
We obtain an error reduction of 35.1%over a previous state-of-the-art extraction methodthat uses heuristically generated alignments.2 Record-Text AlignmentHere we provide a brief description of the record-text alignment task.
For the sake of clarity andspace, we describe our approach on a fictionalrestaurant address data set.
The input to our sys-tem is a database (DB) consisting of records (pos-sibly containing errors) and corresponding textsthat are realizations of these DB records.
An ex-ample of a matching record-text pair is shown in2http://www.informatik.uni-trier.de/?ley/db/3Expectation criteria used in our experiments are listed athttp://www.cs.umass.edu/?kedarb/dbie expts.txt.4To obtain more accurate record-text pairs, matchingmethods can be tuned for high precision at the expenseof recall.
Alternatively, humans can cheaply providematch/mismatch labels on automatically matched pairs.132Recordname address city state phonerestaurant katsu n. hillhurst avenue los angeles 665-1891Textkatzu, 1972 hillhurst ave., los feliz, californiaTable 1: An example of a matching record-text pair for restaurant addresses.Table 1.
This example displays the differencesbetween the record and text: (1) spelling errors:katsu ?
katzu, (2) word insertions (restaurant),deletions (1972), substitutions (angeles ?
feliz),(3) abbreviations (avenue ?
ave.), (4) missingfields in text (phone=665-1891), and (5) extrafields in text (state=california).
These discrep-ancies plus the unknown ordering of fields withintext can be addressed through word alignment.restaurant [name]       katsu [name]       *null* [name]       n.
[address]       hillhurst [address]       avenue [address]       *null* [address]       los [city]       angeles [city]       *null* [city]       *null* [state]       665-1891 [phone]       *null* [phone]       katzu,1972hillhurstave.,losfeliz,californiaTable 2: Example of a word alignment.
 repre-sents aligned tokens.
Vertical text at the bottomare the text tokens.
Horizontal text on the left aretokens from the DB record with labels shown inbraces.An example word alignment between the recordand text is shown in Table 2.
Tokenization ofrecord/text string is based on whitespace charac-ters.
We add a special *null* token at the fieldboundaries for each label in the schema to modelword insertions.
The record sequence is obtainedby concatenating individual fields according to theDB schema order.
As in statistical word align-ment, we assume the DB record to be our sourceand the text to be our target.
The induced labelingof the text is given by (name, address, address,address, city, city, state) which can be used totrain an information extractor.
In the next section,we present our approach to address this task.3 ApproachWe first define notation that will be usedthroughout this section.
Let (x1,y1) be adatabase record with token sequence x1=?x1[1], x1[2], .
.
.
, x1[m]?
and label sequence y1=?y1[1], y1[2], .
.
.
, y1[m]?
with y1[?]
?
Y whereY is the database schema.
Let x2=?x2[1], x2[2], .
.
.
, x2[n]?
be the text sequence.
Leta = ?a1, a2, .
.
.
, an?
be an alignment sequenceof same length as the target text sequence.
Thealignment ai= j assigns the DB token-label pair(x1[j], y1[j]) to the text token x2[i].3.1 Conditional Random Field for AlignmentOur conditional random field (CRF) for alignmenthas a graphical model structure that resembles thatof IBM Model 1 (Brown et al, 1993).
The CRFis an undirected graphical model that defines aprobability distribution over alignment sequencesa conditioned on the inputs (x1,y1,x2) as:p?
(a|x1,y1,x2) =exp(Pnt=1?>~f(at,x1,y1,x2,t))Z?
(x1,y1,x2), (1)where~f(at,x1,y1,x2, t) are feature functionsdefined over the alignments and inputs, ?
arethe model parameters and Z?
(x1,y1,x2) =?a?exp(?nt=1?>~f(a?t,x1,y1,x2, t)) is the par-tition function.The feature vector~f(at,x1,y1,x2, t) is theconcatenation of two types of feature functions:(1) alignment features falign(at,x1,x2, t) definedon source-target tokens, and, (2) extraction fea-tures fextr(at,y1,x2, t) defined on source labelsand target text.
To obtain the probability of analignment in a particular position t we marginal-ize out the alignments over the rest of the positions{1, .
.
.
, n}\{t},p?
(at|x1,y1,x2) =?{a[1...n]}\{at}p?
(a|x1,y1,x2)133=exp(?>~f(at,x1,y1,x2, t))exp(?a?
?>~f(a?,x1,y1,x2, t))(2)Furthermore, the marginal over label ytassignedto the text token x2[t] at time step t during align-ment is given byp?
(yt|x2) =?{at|y1[at]=yt}p?
(at|x1,y1,x2),(3)where {at| y1[at] = yt} is the set of alignmentsthat result in a labeling ytfor token x2[t].
Hence-forth, we abbreviate p?
(at|x1,y1,x2) to p?
(at).The gradient of p?
(at) with respect to parameters?
is given by?p?(at)?
?= p?
(at)[~f(at,x1,y1,x2, t)?Ep?
(a)(~f(a,x1,y1,x2, t))],(4)where the expectation term in the above equationsums over all alignments a at position t. We usethe Baum-Welch and Viterbi algorithms to com-pute marginal probabilities and best alignment se-quences respectively.3.2 Expectation Criteria and ParameterEstimationLetD = ?
(x(1)1,y(1)1,x(1)2), .
.
.
, (x(K)1,y(K)1,x(K)2)?be a data set of K record-text pairs gathered man-ually or automatically through matching (Michel-son and Knoblock, 2005; Michelson andKnoblock, 2008).
A global expectation criterionis defined on the set of alignment latent variablesAf= {a|f(a,x(i)1,y(i)1,x(i)2) = 1,?i = 1 .
.
.K}on the entire data set that satisfy a given bi-nary feature f(a,x1,y1,x2).
Similarly a localexpectation criterion is defined only for aspecific instance (x(i)1,y(i)1,x(i)2) with the setAf= {a|f(a,x(i)1,y(i)1,x(i)2) = 1}.
For a featurefunction f , a target expectation p, and, a weightw, our criterion minimizes the squared divergence?
(f, p, w,?)
= w(Ep?(Af)|Af|?
p)2, (5)where Ep?
(Af) =?a?Afp?
(a) is the sum ofmarginal probabilities given by Equation (2) and|Af| is the size of the variable set.
The weightw influences the importance of satisfying a givenexpectation criterion.
Equation (5) is an instanceof generalized expectation criteria (Mann and Mc-Callum, 2008) that penalizes the divergence ofa specific model expectation from a given targetvalue.
The gradient of the divergence with respectto ?
is given by,??
(f, p, w,?)?
?= 2w(Ep?(Af)|Af|?
p)???1|Af|?a?Af?p?(a)???
p?
?, (6)where the gradient?p?(a)?
?is given by Eq.
(4).Given expectation criteria C = ?F,P,W?
witha set of binary feature functions F = ?f1, .
.
.
, fl?,target expectations P = ?p1, .
.
.
, pl?
and weightsW = ?w1, .
.
.
, wl?, we maximize the objectiveO(?
;D, C) = max??l?i=1?
(fi, pi, wi,?
)?||?||22,(7)where ||?||2/2 is the regularization term added tolimit over-fitting.
Hence the gradient of the objec-tive is?O(?
;D, C)?
?= ?l?i=1??
(fi, pi, wi,?)???
?.We maximize our objective (Equation 7) using theL-BFGS algorithm.
It is sometimes necessary torestart maximization after resetting the Hessiancalculation in L-BFGS due to non-convexity ofour objective.5Also, non-convexity may lead toa local instead of a global maximum.
Our experi-ments show that local maxima do not adversely af-fect performance since our accuracy is within 4%of a model trained with gold-standard labels.3.3 Linear-chain CRF for ExtractionThe alignment CRF (AlignCRF) model describedin Section 3.1 is able to predict labels for a textsequence given a matching DB record.
However,without corresponding records for texts the modeldoes not perform well as an extractor because ithas learned to rely on the DB record and alignmentfeatures (Sutton et al, 2006).
Hence, we traina separate linear-chain CRF on the alignment-induced labels for evaluation as an extractor.The extraction CRF (ExtrCRF) employs afully-connected state machine with a unique state5Our L-BFGS optimization procedure checks whether theapproximate Hessian computed from cached gradient vectorsis positive semi-definite.
The optimization is restarted if thischeck fails.134per label y ?
Y in the database schema.
The CRFinduces a conditional probability distribution overlabel sequences y = ?y1, .
.
.
, yn?
and input textsequences x = ?x1, .
.
.
, xn?
asp?
(y|x) =exp(?nt=1?>~g(yt?1, yt,x, t))Z?(x).
(8)In comparison to our earlier zero-order AlignCRFmodel, our ExtrCRF is a first-order model.
Allthe feature functions in this model g(yt?1, yt,x, t)are a conjunction of the label pair (yt?1, yt) andinput observational features.
Z?
(x) in the equa-tion above is the partition function.
Inference inthe model is performed using the Viterbi algo-rithm.Given expectation criteria C and data setD = ?
(x(1)1,y(1)1,x(1)2), .
.
.
, (x(K)1,y(K)1,x(K)2)?,we first estimate the parameters ?
of AlignCRFmodel as described in Section 3.2.
Next, for alltext sequences x(i)2, i = 1 .
.
.K we compute themarginal probabilities of the labels p?
(yt|x(i)2),?tusing Equation (3).
To estimate parameters ?
weminimize the KL-divergence between p?
(y|x) =?nt=1p?
(yt|x) and p?
(y|x) for all sequences x,KL(p??p?)
=?yp?
(y|x) log(p?(y|x)p?
(y|x))= H(p?(y|x))??t,yt?1,ytEp?
(yt?1,yt)[?>~g(yt?1, yt,x, t)]+ log(Z?(x)).
(9)The gradient of the above equation is given by?KL??=?t,yt?1,ytEp?
(yt?1,yt|x)[~g(yt?1, yt,x, t)]?Ep?
(yt?1,yt|x)[~g(yt?1, yt,x, t)].
(10)Both the expectations can be computed using theBaum-Welch algorithm.
The parameters ?
are es-timated for a given data set D and learned param-eters ?
by optimizing the objectiveO(?;D,?)
= min?K?i=1KL(p?(y|x(i)2)?p?(y|x(i)2)+??
?2/2.The objective is minimized using L-BFGS.
Sincethe objective is convex we are guaranteed to obtaina global minima.4 ExperimentsIn this section, we present details about the appli-cation of our method to citation extraction task.Data set.
We collected a set of 260 randomrecords from the DBLP bibliographic database.The schema of DBLP has the following labels{author, editor, address, title, booktitle, pages,year, journal, volume, number, month, url, ee,cdrom, school, publisher, note, isbn, chapter, se-ries}.
The complexity of our alignment model de-pends on the number of schema labels and numberof tokens in the DB record.
We reduced the num-ber of schema labels by: (1) mapping the labelsaddress, booktitle, journal and school to venue, (2)mapping month and year to date, and (3) droppingthe fields url, ee, cdrom, note, isbn and chapter,since they never appeared in citation texts.
Wealso added the other label O for fields in text thatare not represented in the database.
Therefore, ourfinal DB schema is {author, title, date, venue, vol-ume, number, pages, editor, publisher, series, O}.For each DBLP record we searched on the webfor matching citation texts using the first author?slast name and words in the title.
Each citation textfound is manually labeled for evaluation purposes.An example of a matching DBLP record-citationtext pair is shown in Table 3.
Our data set6con-tains 522 record-text pairs for 260 DBLP entries.Features and Constraints.
We use a variety ofrich, non-independent features in our models tooptimize system performance.
The input featuresin our models are of the following two types:(a) Extraction features in the AlignCRFmodel (f(at,y1,x2, t)) and ExtrCRF model(g(yt?1, yt,x, t)) are conjunctions of assigned la-bels and observational tests on text sequence attime step t. The following observational testsare used: (1) regular expressions to detect to-kens containing all characters (ALLCHAR), all dig-its (ALLDIGITS) or both digits and characters (AL-PHADIGITS), (2) number of characters or digitsin the token (NUMCHAR=3, NUMDIGITS=1), (3)domain-specific patterns for date and pages, (4)token identity, suffixes, prefixes and character n-grams, (5) presence of a token in lexicons such as?last names,?
?publisher names,?
?cities,?
(6) lex-icon features within a window of 10, (7) regular6The data set can be found athttp://www.cs.umass.edu/?kedarb/dbie cite data.sgml.135DBLP record Citation text[Chengzhi Li]author[Edward W. Knightly]author[Coordinated Net-work Scheduling: A Framework for End-to-End Services.]title[69-]pages[2000]date[ICNP]venue[C.
Li]author[and]O[E.
Knightly.
]author[Coordinated network schedul-ing: A framework for end-to-end services.
]title[In Proceedings of IEEEICNP]venue[?00,]date[Osaka, Japan,]venue[November 2000.
]dateTable 3: Example of matching record-text pair found on the web.expression features within a window of 10, and (8)token identity features within a window of 3.
(b) Alignment features in the AlignCRF model(f(at,x1,x2, t)) that operate on the alignedsource token x1[at] and target token x2[t].
Againthe observational tests used for alignment are: (1)exact token match tests whether the source-targettokens are string identical, (2) approximate tokenmatch produces a binary feature after binning theJaro-Winkler edit distance (Cohen et al, 2003) be-tween the tokens, (3) substring token match testswhether one token is a substring of the other,(4) prefix token match returns true if the pre-fixes match for lengths {1, 2, 3, 4}, (5) suffix to-ken match returns true if the prefixes match forlengths {1, 2, 3, 4}, and (6) exact and approximatetoken matches at offsets {?1,?1} and {+1,+1}around the alignment.Thus, a conditional model lets us use these ar-bitrary helpful features that cannot be exploitedtractably in a generative model.As is common practice (Haghighi and Klein,2006; Mann and McCallum, 2008), we simulateuser-specified expectation criteria through statis-tics on manually labeled citation texts.
For ex-traction criteria, we select for each label, the topN extraction features ordered by mutual informa-tion (MI) with that label.
Also, we aggregate thealignment features of record tokens whose align-ment with a target text token results in a correctlabel assignment.
The top N alignment featuresthat have maximum MI with this correct label-ing are selected as alignment criteria.
We bin tar-get expectations of these criteria into 11 bins as[0.05, 0.1, 0.2, 0.3, .
.
.
, 0.9, 0.95].7In our exper-iments, we set N = 10 and use a fixed weightw = 10.0 for all expectation criteria (no tuningof parameters was performed).
Table 4 shows asample of GE criteria used in our experiments.87Mann and McCallum (2008) note that GE criteria are ro-bust to deviation of specified targets from actual expectations.8A complete list of expectation criteria is available athttp://www.cs.umass.edu/?kedarb/dbie expts.txt.Label Feature Prioralignment PREFIX MATCH4 0.95author LEXICON LASTNAME 0.6title WINDOW WORD=Maintenance 0.95venue WINDOW WORD=Conference 0.95date YEAR PATTERN 0.95volume NUMDIGITS=2 0.6number NUMDIGITS=1 0.6pages PAGES PATTERN 0.95editor WORD PREFIX[2]=ed 0.95publisher WORD=Press 0.95series WORD=Notes 0.95O WORD=and 0.7Table 4: Sample of expectation criteria used byour model.Experimental Setup.
Our experiments use a 3:1split of the data for training and testing.
We re-peat the experiment 20 times with different ran-dom splits of the data.
We train the AlignCRFmodel using the training data and the automati-cally created expectation criteria (Section 3.2).
Weevaluate our alignment model indirectly in termsof token labeling accuracy (i.e., percentage of cor-rectly labeled tokens in test citation data) since wedo not have annotated alignments.
The alignmentmodel is then used to train a ExtrCRF model asdescribed in Section 3.3.
Again, we use token la-beling accuracy for evaluation.
We also measureF1 performance as the harmonic mean of precisionand recall for each label.4.1 Alternate approachesWe compare our method against alternate ap-proaches that either learn alignment or extractionmodels from training data.Alignment approaches.
We use GIZA++ (Ochand Ney, 2003) to train generative directed align-ment models: HMM and IBM Model4 (Brown etal., 1993) from training record-text pairs.
Thesemodels are currently being used in state-of-the-artmachine translation systems.
Alignments betweenmatching DB records and text sequences are thenused for labeling at test time.136Extraction approaches.
The first alternative(DB-CRF) trains a linear-chain CRF for extrac-tion on fields of the database entries only.
Eachfield of the record is treated as a separate labeledtext sequence.
Given an unlabeled text sequence,it is segmented and labeled using the Viterbi algo-rithm.
This method is an enhanced representativefor (Agichtein and Ganti, 2004) in which a lan-guage model is trained for each column of the DB.Another alternative technique constructs par-tially annotated text data using the matchingrecords and a labeling function.
The labeling func-tion employs high-precision alignment rules to as-sign labels to text tokens using labeled record to-kens.
We use exact and approximate token match-ing rules to create a partially labeled sequence,skipping tokens that cannot be unambiguously la-beled.
In our experiments, we achieve a pre-cision of 97% and a recall of 70% using theserules.
Given a partially annotated citation text,we train a linear-chain CRF by maximizing themarginal likelihood of the observed labels.
Thismarginal CRF training method (Bellare and Mc-Callum, 2007) (M-CRF) was the previous state-of-the-art on this data set.
Additionally, if a match-ing record is available for a test citation text,we can partially label tokens and use constrainedViterbi decoding with labeled positions fixed attheir observed values (M+R-CRF approach).Our third approach is similar to (Mann and Mc-Callum, 2008).
We create extraction expectationcriteria from labeled text sequences in the trainingdata and uses these criteria to learn a linear-chainCRF for extraction (MM08).
The performanceachieved by this approach is an upper bound onmethods that: (1) use labeled training records tocreate extraction criteria, and, (2) only use extrac-tion criteria without any alignment criteria.Finally, we train a supervised linear-chain CRF(GS-CRF) using the labeled text sequences fromthe training set.
This represents an upper bound onthe performance that can be achieved on our task.All the extraction methods have access to the samefeatures as the ExtrCRF model.4.2 ResultsTable 5 shows the results of various alignmentalgorithms applied to the record-text data set.Alignment methods use the matching record toperform labeling of a test citation text.
The Align-CRF model outperforms the best generative align-HMM Model4 AlignCRFaccuracy 78.5% 79.8% 92.7%author 92.7 94.9 99.0title 93.3 95.1 97.3date 69.5 66.3 81.9venue 73.3 73.1 91.2volume 50.0 49.2 78.5number 53.5 66.3 68.0pages 38.2 44.1 88.2editor 22.8 21.5 78.1publisher 29.7 31.0 72.6series 77.4 77.3 74.6O 49.6 58.8 85.7Table 5: Token-labeling accuracy and per-label F1for different alignment methods.
These methodsall use matching DB records at test time.
Bold-faced numbers indicate the best performing model.HMM, Model4: generative alignment modelsfrom GIZA++, AlignCRF: alignment model fromthis paper.ment model Model4 (IBM Model 4) with an er-ror reduction of 63.8%.
Our conjecture is thatModel4 is getting stuck in sub-optimal local max-ima during EM training since our training set onlycontains hundreds of parallel record-text pairs.This problem may be alleviated by training on alarge parallel corpus.
Additionally, our alignmentmodel is superior toModel4 since it leverages richnon-independent features of input sequence pairs.Table 6 shows the performance of various ex-traction methods.
Except M+R-CRF, all extrac-tion approaches, do not use any record informationat test time.
In comparison to the previous state-of-the-artM-CRF, theExtrCRFmethod providesan error reduction of 35.1%.
ExtrCRF also pro-duces an error reduction of 21.7% compared toM+R-CRF without the use of matching records.These reductions are significant at level p = 0.005using the two-tailed t-test.
Training only on DBrecords is not helpful for extraction as we do notlearn the transition structure9and additional con-text information10in text.
This explains the lowaccuracy of the DB-CRF method.
Furthermore,theMM08 approach (Mann and McCallum, 2008)achieves low accuracy since it does not use any9In general, the editor field follows the title field while theauthor field precedes it.10The token ?Vol.?
generally precedes the volume field intext.
Similarly, tokens ?pp?
and ?pages?
occur before thepages field.137DB-CRF M-CRF M+R-CRF?MM08 ExtrCRF GS-CRFaccuracy 70.4% 88.9% 90.8% 73.5% 92.8% 96.5%author 72.4 93.7 94.1 85.4 98.5 99.0title 79.4 96.7 98.4 83.1 94.6 98.1date 60.1 74.5 76.2 57.8 81.7 93.5venue 67.3 89.4 91.5 73.2 89.8 95.9volume 20.3 69.4 74.2 27.7 78.9 90.5number 30.1 72.8 80.8 47.8 75.1 91.4pages 41.4 80.9 84.5 49.6 92.1 94.1editor 7.1 71.1 79.3 75.3 73.3 93.7publisher 62.1 67.5 77.2 40.2 58.5 82.2series 65.2 74.9 76.3 65.9 73.8 85.8O 54.1 7.0 8.3 57.7 91.9 94.5Table 6: Token-labeling accuracy and per-label F1 for different extraction methods.
Except M+R-CRF?,all other approaches do not use any records at test time.
Bold-faced numbers indicate the best performingmodel.
DB-CRF: CRF trained on DB fields.
M+R-CRF, M-CRF: CRFs trained from heuristic align-ments.
ExtrCRF: Extraction model presented in this paper.
GS-CRF: CRF trained on human annotatedcitation texts.alignment criteria during training.
Hence, align-ment information is crucial for obtaining high ac-curacy.Note that we do not observe a decrease in per-formance of ExtrCRF over AlignCRF althoughwe are not using the test records during decoding.This is because: (1) a first-order model in Extr-CRF improves performance compared to a zero-order model in AlignCRF and (2) the use of noisyDB records in the test set for alignment often in-creases extraction error.Both our models have a high F1 value for theother label O because we provide our algorithmwith constraints for the label O.
In contrast, sincethere is no realization of the O field in the DBrecords, both M-CRF and M+R-CRF methodsfail to label such tokens correctly.
Our alignmentmodel trained using expectation criteria achievesa performance of 92.7% close to gold-standardtraining (GS-CRF) (96.5%).
Furthermore, Ex-trCRF obtains an accuracy of 92.8% similar toAlignCRF without access to DB records due tobetter modeling of transition structure and context.5 Related WorkRecent research in information extraction (IE) hasfocused on reducing the labeling effort neededto train supervised IE systems.
For instance,Grenager et al (2005) perform unsupervisedHMM learning for field segmentation, and biasthe model to prefer self-transitions and transi-tions on boundary tokens.
Unfortunately, suchunsupervised IE approaches do not attain perfor-mance close to state-of-the-art supervised meth-ods.
Semi-supervised approaches that learn amodel with only a few constraints specifyingprior knowledge have generated much interest.Haghighi and Klein (2006) assign each label inthe model certain prototypical features and train aMarkov random field for sequence tagging fromthese labeled features.
In contrast, our methoduses GE criteria (Mann and McCallum, 2008) ?allowing soft-labeling of features with target ex-pectation values ?
to train conditional models withcomplex and non-independent input features.
Ad-ditionally, in comparison to previous methods, aninformation extractor trained from our record-textalignments achieves accuracy of 93% making ituseful for real-world applications.
Chang et al(2007) use beam search for decoding unlabeledtext with soft and hard constraints, and train amodel with top-K decoded label sequences.
How-ever, this model requires large number of labeledexamples (e.g., 300 annotated citations) to boot-strap itself.
Active learning is another popular ap-proach for reducing annotation effort.
Settles andCraven (2008) provide a comparison of various ac-tive learning strategies for sequence labeling tasks.We have shown, however, that in domains where adatabase can provide significant supervision, onecan bootstrap accurate extractors with very littlehuman effort.138Another area of research, related to the taskdescribed in our paper, is learning extractorsfrom database records.
These records are alsoknown as field books and reference sets in liter-ature (Canisius and Sporleder, 2007; Michelsonand Knoblock, 2008).
Both Agichtein and Ganti(2004) and Canisius and Sporleder (2007) train alanguage model for each database column.
Thelanguage modeling approach is sensitive to wordre-orderings in text and other variability presentin real-world text (e.g., abbreviation).
We allowfor word and field re-orderings through alignmentsand model complex transformations through fea-ture functions.
Michelson and Knoblock (2008)extract information from unstructured texts using arule-based approach to align segments of text withfields in a DB record.
Our probabilistic alignmentapproach is more robust and uses rich features ofthe alignment to obtain high performance.Recently, Snyder and Barzilay (2007) and Lianget al (2009) have explored record-text matching indomains with unstructured texts.
Unlike a semi-structured text sequence obtained by noisily con-catenating fields from a single record, an unstruc-tured sequence may contain fields from multiplerecords embedded in large amounts of extraneoustext.
Hence, the problems of record-text matchingand word alignment are significantly harder in un-structured domains.
Snyder and Barzilay (2007)achieve a state-of-the-art performance of 80% F1on matching multiple NFL database records tosentences in the news summary of a football game.Their algorithm is trained using supervised ma-chine learning and learns alignments at the level ofsentences and DB records.
In contrast, this paperpresents a semi-supervised learning algorithm forlearning token-level alignments between recordsand texts.
Liang et al (2009) describe a model thatsimultaneously performs record-text matching andword alignment in unstructured domains.
Theirmodel is trained in an unsupervised fashion usingEM.
It may be possible to further improve theirmodel performance by incorporating prior knowl-edge in the form of expectation criteria.Traditionally, generative word alignment mod-els have been trained on massive parallel cor-pora (Brown et al, 1993).
Recently, discrim-inative alignment methods trained using anno-tated alignments on small parallel corpora haveachieved superior performance.
Taskar et al(2005) train a discriminative alignment modelfrom annotated alignments using a large-marginmethod.
Labeled alignments are also used byBlunsom and Cohn (2006) to train a CRF wordalignment model.
Our method is trained using asmall number of easily specified expectation cri-teria thus avoiding tedious and expensive humanlabeling of alignments.
An alternate method oflearning alignment models is proposed by McCal-lum et al (2005) in which the training set consistsof sequence pairs classified as match or mismatch.Alignments are learned to identify the class of agiven sequence pair.
However, this method relieson carefully selected negative examples to pro-duce high-accuracy alignments.
Our method pro-duces good alignments as we directly encode priorknowledge about alignments.6 Conclusion and Future WorkInformation extraction is an important first step indata mining applications.
Earlier approaches forlearning reliable extractors have relied on manu-ally annotated text corpora.
This paper presents anovel approach for training extractors using align-ments between texts and existing database records.Our approach achieves performance close to su-pervised training with very little supervision.In the future, we plan to surpass supervised ac-curacy by applying our method to millions of par-allel record-text pairs collected automatically us-ing matching.
We also want to explore the addi-tion of Markov dependencies into our alignmentmodel and other constraints such as monotonicityand one-to-one correspondence.AcknowledgmentsThis work was supported in part by the Centerfor Intelligent Information Retrieval and in part byThe Central Intelligence Agency, the National Se-curity Agency and National Science Foundationunder NSF grant #IIS-0326249.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are the authors?
and do notnecessarily reflect those of the sponsor.ReferencesEugene Agichtein and Venkatesh Ganti.
2004.
Miningreference tables for automatic text segmentation.
InKDD.Eugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-lections.
In ICDL.139Kedar Bellare and Andrew McCallum.
2007.
Learn-ing extractors from unlabeled text using relevantdatabases.
In IIWeb workshop at AAAI 2007.Phil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InACL.Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In EDBT Workshop,pages 172?183.Peter Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert Mercer.
1993.
The mathematicsof statistical machine translation: parameter estima-tion.
Computational Linguistics, 19:263?311.Sander Canisius and Caroline Sporleder.
2007.
Boot-strapping information extraction from field books.In EMNLP-CoNLL.M.
Chang, L. Ratinov, and D. Roth.
2007.
Guidingsemi-supervision with constraint-driven learning.
InACL, pages 280?287.William Cohen, Pradeep Ravikumar, and Stephen Fien-berg.
2003.
A comparison of string distance metricsfor name-matching tasks.
In IJCAI.O.
Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe Web: An experimental study.
Artificial Intelli-gence, 165.D.
Freitag and A. McCallum.
1999.
Information ex-traction with HMM and shrinkage.
In AAAI.T.
Grenager, D. Klein, and C. D. Manning.
2005.
Un-supervised learning of field segmentation models forinformation extraction.
In ACL.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In HLT-NAACL.John Lafferty, Andrew McCallum, and Fernando C NPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In ICML, page 282.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InAssociation for Computational Linguistics (ACL).Gideon S. Mann and Andrew McCallum.
2008.Generalized expectation criteria for semi-supervisedlearning of conditional random fields.
In Proceed-ings of ACL?08, pages 870?878.I.
R. Mansuri and S. Sarawagi.
2006.
Integrating un-structured data into relational databases.
In ICDE.Andrew McCallum, Kedar Bellare, and FernandoPereira.
2005.
A conditional random field fordiscriminatively-trained finite-state string edit dis-tance.
In UAI.MatthewMichelson and Craig A. Knoblock.
2005.
Se-mantic annotation of unstructured and ungrammati-cal text.
In IJCAI, pages 1091?1098.Matthew Michelson and Craig A. Knoblock.
2008.Creating relational data from unstructured and un-grammatical data sources.
JAIR, 31:543?590.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29.Fuchun Peng and A. McCallum.
2004.
Accurate infor-mation extraction from research papers using condi-tional random fields.
In HLT-NAACL.Lawrence R. Rabiner.
1989.
A tutorial on hiddenmarkov models and selected applications in speechprocessing.
IEEE, 17:257?286.Sridhar Ramakrishnan and Sarit Mukherjee.
2004.Taming the unstructured: Creating structured con-tent from partially labeled schematic text sequences.In CoopIS/DOA/ODBASE, volume 2, page 909.Sunita Sarawagi and William W. Cohen.
2004.
Ex-ploiting dictionaries in named entity extraction:combining semi-markov extraction processes anddata integration methods.
In KDD, page 89.Sunita Sarawagi and William W. Cohen.
2005.
Semi-Markov conditional random fields for informationextraction.
In NIPS.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In EMNLP, pages 1070?1079.K.
Seymore, A. McCallum, and R. Rosenfeld.
1999.Learning hidden markov model structure for infor-mation extraction.
In Proceedings of the AAAIWorkshop on ML for IE.Benjamin Snyder and Regina Barzilay.
2007.Database-text alignment via structured multi-labelclassification.
In IJCAI.Charles Sutton, Michael Sindelar, and Andrew McCal-lum.
2006.
Reducing weight undertraining in struc-tured discriminative learning.
In HLT-NAACL.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In HLT-EMNLP, pages 73?80.140
