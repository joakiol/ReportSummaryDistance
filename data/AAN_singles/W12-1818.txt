NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 45?48,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsOne Year of Contender: What Have We Learned about Assessing andTuning Industrial Spoken Dialog Systems?David SuendermannSpeechCycle, New York, USAdavid@suendermann.comRoberto PieracciniICSI, Berkeley, USAroberto@icsi.berkeley.eduAbstractA lot.
Since inception of Contender, a ma-chine learning method tailored for computer-assisted decision making in industrial spo-ken dialog systems, it was rolled out in over200 instances throughout our applications pro-cessing nearly 40 million calls.
The net ef-fect of this data-driven method is a signifi-cantly increased system performance gainingabout 100,000 additional automated calls ev-ery month.1 From the unwieldiness of data to theContender processAcademic institutions involved in the research onspoken dialog systems often lack access to data fortraining, tuning, and testing their systems.
This issimply because the majority of systems only live inlaboratory environments and hardly get deployed tothe live user1.
The lack of data can result in sys-tems not sufficiently tested, models trained on non-representative or artificial data, and systems of lim-ited domains (usually restaurant or flight informa-tion).On the other hand, in industrial settings, spokendialog systems are often deployed to take over tasksof call center agents associated with potentially verylarge amounts of traffic.
Here, we are speaking ofapplications which may process more than one mil-lion calls per week.
Having applications log every1One of the few exceptions to this rule is the Let?s Go bus in-formation system maintained at the Carnegie Mellon Universityin Pittsburgh (Raux et al, 2005).action they take during the course of a call can pro-vide developers with valuable data to tune and testthe systems they maintain.
As opposed to the aca-demic world, often, there appears to be too muchdata to capture, permanently store, mine, and re-trieve.
Harddisks on application servers run full,log processing scripts demand too much comput-ing capacity, database queues get stuck, queries slowdown, and so on and so forth.
Even if these billionsand billions of log entries are eventually availablefor random access from a highly indexed databasecluster, it is not clear what one should search forin an attempt to improve a dialog system?s perfor-mance.About a year and a half ago, we proposed amethod we called Contender playing the role of alive experiment in a deployed spoken dialog sys-tem (Suendermann et al, 2010a).
Conceptually, aContender is an activity in a call flow which has aninput transition and multiple output transitions (al-ternatives).
When a call hits a Contender?s inputtransition, a randomization is carried out to deter-mine which alternative the call will continue with(see Figure 1).
The Contender itself does not do any-thing else but performing the random decision dur-ing runtime.
The different call flow activities andprocesses the individual alternatives get routed tomake calls depend on the Contenders?
decisions.Say, one wants to find out which of ten possibletime-out settings in an activity is optimal.
This couldbe achieved by duplicating the activity in questionten times and setting each copy?s time-out to a dif-ferent value.
Now, a Contender is placed whose tenalternatives get connected to the ten competing ac-45randomizerAlternative 1 Alternative 2 Alternative 3randomizationweightsFigure 1: Contender with three alternatives.tivities.
Finally, the outbound transitions of the com-peting activities have to be bundled to make the restof the application be independent of the Contender.A Contender can be used for all sorts of exper-iments in dialog systems.
For instance, if systemdesigners are unsure about which of a number ofprompts has more expressive power, they can imple-ment all of them in the application and have the Con-tender decide at runtime which one to play.
Or if it isunclear which actions to perform in which order, dif-ferent strategies can be compared using a Contender.The same applies to certain parameter settings, errorhandling approaches, confirmation strategies, and soon.
Every design aspect with one or more alterna-tives can be implemented by means of a Contender.Once an application featuring Contenders startstaking live production traffic, an analysis has to becarried out, to determine which alternative resultsin the highest average performance.
In doing so,it is crucial to implement some measure of statisti-cal significance as, otherwise, conclusions may bemisleading.
If no statistical significance measurewas in place, processing two calls in a two-wayContender, one routed to Alternative 1 and endingup automated and one routed to Alternative 2 end-ing up non-automated, could lead to the conclu-sion that Alternative 1?s automation rate is 100%and Alternative 2?s is 0.
To avoid such potentiallyerroneous conclusions, we are using two-sample t-tests for Contenders with two alternatives and pair-wise two-sample t-tests with probability normaliza-tion for more alternatives as measures of statisticalsignificance.
A more exact but computationally veryexpensive method was explained in (Suendermannet al, 2010a), but for the sake of performing statis-tical analysis with acceptable delays given the vastamount of data, we primarily use the former in pro-duction deployments.If an alternative is found to statistically signifi-cantly outperform the other alternatives, it is deemedthe winner, and it would be advisable routing most(if not all) calls to that alternative.
While this hardreset maximizes performance induced by this Con-tender going forward, it sometimes takes quite awhile before the required statistical significance isactually reached.
Hence, in the time span beforethis hard reset, the Contender may perform subop-timally.
Furthermore, even though statistical mea-sures could indicate which alternative the likely win-ner is, this fact is potentially subject to change overtime depending upon alterations in the caller popu-lation, the distribution of call reasons, or the appli-cation itself.
For this reason, it is recommendable tokeep exploring seemingly underperforming alterna-tives by routing a very small portion of calls to them.The statistical model we discussed in (Suender-mann et al, 2010a) presents a solution to the abovelisted issues.
The model associates each alternativeof a Contender with a weight controlling which per-centage of traffic is routed down this alternative onaverage.
As derived in (Suendermann et al, 2010a),the weight for an alternative is generated based onthe probability that this alternative is the actual win-ner of the Contender given the available historicdata.
The weights are subject to regular updatescomputed by a statistical analysis engine that con-tinuously analyzes the behavior of all Contenders inproduction deployment.
In order to do so, the en-gine accesses the entirety of available applicationlogs associating performance metrics, such as au-tomation rate (the fraction of processed calls thatsatisfied the call reason) or average handling time(average call duration), with Contenders and theiralternatives.
This is relatively straightforward sincethe application can log call category (to tell whethera call was automated or not), call duration, the Con-tenders visited and the results of the randomizationat each of the Contender.
In Figure 2, a high-leveldiagram of the Contender process is shown.46w1 wnapplication logsApplicationstatistical analysisAnalysis Enginew2randomizerAlternative 1 Alternative 2 Alternative nw1,w2,...,wnFigure 2: Contender process.Since statistical analysis of Contenders involvesdata points of hundreds of thousands of calls, per-formance measurement needs to be based on autom-ically derivable, i.e.
objective, metrics.
Popular ob-jective metrics are automation rate, average handlingtime, ?speech errors?, retry rate, number of hang-upsor opt-outs (Suendermann et al, 2010c).
There arealso techniques correlating objective metrics to sub-jective ones in an attempt to predict user or caller ex-perience, i.e., to evaluate interaction quality as per-ceived by the caller (Walker et al, 1997; Evanini etal., 2008; Mo?ller et al, 2008).
Despite the impor-tance of making interactions as smooth and pleasantas possible, stakeholders of industrial systems ofteninsist on using metrics directly tied to the savingsgenerated by the deployed spoken dialog system.
Aswe introduced in (Suendermann et al, 2010b), sav-ings mainly depend on automation rate (A) and av-erage handling time (T ) and can be expressed by therewardR = TAA?
Twhere TA is a trade-off factor that depends on aver-age agent salary and hosting and telecommunicationfees.2 A snapshot of our last year?s experiencesShortly after setting the mathematical foundations ofthe Contender process and establishing the involvedsoftware and hardware pieces, the first Contenderswere implemented in production applications.
Un-der the close look of operations, quality assurance,engineering, speech science, as well as technical ac-count management departments, the process under-went a number of refinement cycles.
In the mean-time, more and more Contenders were implementedinto a variety of applications and released into pro-duction traffic.
Until to date, 233 Contenders werereleased into production systems processing an totalcall volume of 39 million calls.
Table 1 shows somestatistics of a number of example Contenders per ap-plication.
These statistics are drawn from spoken di-alog systems for technical troubleshooting of cableservices as discussed e.g.
in (Acomb et al, 2007).Such applications assist callers fixing problems withtheir cable TV or Internet (such as no, slow, or inter-mittent connection, e-mail issues).
In addition to theapplication and a short description of the Contender,the table shows three quantities:?
the number of calls processed by the Contendersince its establishment (# calls),?
the reward difference between the highest- andlowest-performing alternative of a Contender?R (a high value indicates that the best-performing alternative is substantially betterthan the worst-performing one, that is, the Con-tender is very effective), and?
an estimate of the number of automated callsgained or saved per month by running the Con-tender ?At [mo?1] (this value indicates thenet effect of having all calls route throughthe best-performing alternative vs. the worst-performing one, that is, the upper bound of howmany calls were gained or saved).
This metric47Table 1: Statistics of example Contenders.application Contender # calls ?At [mo?1] ?RTV problem capture 13,477,810 40,362 0.05TV cable box reboot order 4,322,428 28,975 0.11TV outage prediction 2,758,963 08,198 0.04TV on demand 485,300 08,123 0.17TV input source troubleshooting 1,162,445 03,487 0.05TV account lookup 9,627 03,201 0.02Internet troubleshooting paths I 275,248 05,568 0.02Internet troubleshooting paths II 1,389,489 03,530 0.01Internet computer monitor instruction 1,500,010 03,271 0.01TV/Internet opt in 6,865,929 31,764 0.05is calculated by multiplying the observed dif-ference in automation rate?Awith the numberof monthly calls hitting the Contender (t).3 ConclusionWe have seen that the use of Contenders (a methodto assess and tune arbitrary components of indus-trial spoken dialog systems) can be very benefi-cial in multiple respects.
Applications can self-correct as soon as reliable data becomes availablewithout additional manual analysis and intervention.Moreover, performance can increase substantiallyin applications implementing Contenders.
Lookingat only the 10 best-performing Contenders out of233 running in our applications to-date, the numberof automated calls increased by about 100,000 permonth.However, multiple Contenders that are active inthe same call flow cannot always be regarded inde-pendent of each other.
A routing decision made inContender 1 earlier in the call can potentially havean impact on which decision is optimal in Contender2 further down the call.
In this respect, reward gainsof Contenders installed in the same application arenot necessarily additive.
Not only can optimal deci-sions in a Contender depend on other Contenders butalso on other runtime parameters such as time of theday, day of the week, geographic origin of the callerpopulation, or the equipment used by the caller.
Ourcurrent research focuses on evaluating these depen-dencies and accordingly optimize the way decisionsare made in Contenders.ReferencesK.
Acomb, J. Bloom, K. Dayanidhi, P. Hunter, P. Krogh,E.
Levin, and R. Pieraccini.
2007.
Technical SupportDialog Systems: Issues, Problems, and Solutions.
InProc.
of the HLT-NAACL, Rochester, USA.K.
Evanini, P. Hunter, J. Liscombe, D. Suendermann,K.
Dayanidhi, and R. Pieraccini:.
2008.
Caller Expe-rience: A Method for Evaluating Dialog Systems andIts Automatic Prediction.
In Proc.
of the SLT, Goa,India.S.
Mo?ller, K. Engelbrecht, and R. Schleicher.
2008.
Pre-dicting the Quality and Usability of Spoken DialogueServices.
Speech Communication, 50(8-9).A.
Raux, B. Langner, D. Bohus, A.
Black, and M. Eske-nazi.
2005.
Let?s Go Public!
Taking a Spoken DialogSystem to the Real World.
In Proc.
of the Interspeech,Lisbon, Portugal.D.
Suendermann, J. Liscombe, and R. Pieraccini.
2010a.Contender.
In Proc.
of the SLT, Berkeley, USA.D.
Suendermann, J. Liscombe, and R. Pieraccini.
2010b.Minimally Invasive Surgery for Spoken Dialog Sys-tems.
In Proc.
of the Interspeech, Makuhari, Japan.D.
Suendermann, J. Liscombe, R. Pieraccini, andK.
Evanini.
2010c.
?How am I Doing??
A NewFramework to Effectively Measure the Performanceof Automated Customer Care Contact Centers.
InA.
Neustein, editor, Advances in Speech Recogni-tion: Mobile Environments, Call Centers and Clinics.Springer, New York, USA.M.
Walker, D. Litman, C. Kamm, and A. Abella.
1997.PARADISE: A Framework for Evaluating Spoken Di-alogue Agents.
In Proc.
of the ACL, Madrid, Spain.48
