Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496?504,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPMulti-Class Confidence Weighted AlgorithmsKoby Crammer?
?Department of Computerand Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104{crammer,kulesza}@cis.upenn.eduMark Dredze?Alex Kulesza?
?Human Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211mdredze@cs.jhu.eduAbstractThe recently introduced onlineconfidence-weighted (CW) learningalgorithm for binary classification per-forms well on many binary NLP tasks.However, for multi-class problems CWlearning updates and inference cannotbe computed analytically or solved asconvex optimization problems as they arein the binary case.
We derive learningalgorithms for the multi-class CW settingand provide extensive evaluation usingnine NLP datasets, including three derivedfrom the recently released New YorkTimes corpus.
Our best algorithm out-performs state-of-the-art online and batchmethods on eight of the nine tasks.
Wealso show that the confidence informationmaintained during learning yields usefulprobabilistic information at test time.1 IntroductionOnline learning algorithms such as the Perceptronprocess one example at a time, yielding simple andfast updates.
They generally make few statisti-cal assumptions about the data and are often usedfor natural language problems, where high dimen-sional feature representations, e.g., bags-of-words,demand efficiency.
Most online algorithms, how-ever, do not take into account the unique propertiesof such data, where many features are extremelyrare and a few are very frequent.Dredze, Crammer and Pereira (Dredze et al,2008; Crammer et al, 2008) recently introducedconfidence weighted (CW) online learning for bi-nary prediction problems.
CW learning explicitlymodels classifier weight uncertainty using a multi-variate Gaussian distribution over weight vectors.The learner makes online updates based on its con-fidence in the current parameters, making largerchanges in the weights of infrequently observedfeatures.
Empirical evaluation has demonstratedthe advantages of this approach for a number of bi-nary natural language processing (NLP) problems.In this work, we develop and test multi-classconfidence weighted online learning algorithms.For binary problems, the update rule is a sim-ple convex optimization problem and inferenceis analytically computable.
However, neither istrue in the multi-class setting.
We discuss sev-eral efficient online learning updates.
These up-date rules can involve one, some, or all of thecompeting (incorrect) labels.
We then perform anextensive evaluation of our algorithms using ninemulti-class NLP classification problems, includ-ing three derived from the recently released NewYork Times corpus (Sandhaus, 2008).
To the bestof our knowledge, this is the first learning evalua-tion on these data.
Our best algorithm outperformsstate-of-the-art online algorithms and batch algo-rithms on eight of the nine datasets.Surprisingly, we find that a simple algorithm inwhich updates consider only a single competinglabel often performs as well as or better than multi-constraint variants if it makes multiple passes overthe data.
This is especially promising for largedatasets, where the efficiency of the update canbe important.
In the true online setting, whereonly one iteration is possible, multi-constraint al-gorithms yield better performance.Finally, we demonstrate that the label distribu-tions induced by the Gaussian parameter distribu-tions resulting from our methods have interestingproperties, such as higher entropy, compared tothose from maximum entropy models.
Improvedlabel distributions may be useful in a variety oflearning settings.2 Problem SettingIn the multi-class setting, instances from an inputspace X take labels from a finite set Y , |Y| = K.496We use a standard approach (Collins, 2002) forgeneralizing binary classification and assume afeature function f(x, y) ?
Rdmapping instancesx ?
X and labels y ?
Y into a common space.We work in the online framework, where learn-ing is performed in rounds.
On each round thelearner receives an input xi, makes a prediction y?iaccording to its current rule, and then learns thetrue label yi.
The learner uses the new example(xi, yi) to modify its prediction rule.
Its goal is tominimize the total number of rounds with incor-rect predictions, |{i : yi6= y?i}|.In this work we focus on linear models parame-terized by weightsw and utilizing prediction func-tions of the form hw(x) = arg maxzw ?
f(x, z).Note that since we can choose f(x, y) to be thevectorized Cartesian product of an input featurefunction g(x) and y, this setup generalizes the useof unique weight vectors for each element of Y .3 Confidence Weighted LearningDredze, Crammer, and Pereira (2008) introducedonline confidence weighted (CW) learning for bi-nary classification, where X = Rdand Y ={?1}.
Rather than using a single parameter vec-tor w, CW maintains a distribution over param-eters N (?,?
), where N (?,?)
the multivariatenormal distribution with mean ?
?
Rdand co-variance matrix ?
?
Rd?d.
Given an input in-stance x, a Gibbs classifier draws a weight vectorw from the distribution and then makes a predic-tion according to the sign of w ?
x.This prediction rule is robust if the exampleis classified correctly with high-probability, thatis, for some confidence parameter .5 ?
?
< 1,Prw[y (w ?
x) ?
0] ?
?.
To learn a binary CWclassifier in the online framework, the robustnessproperty is enforced at each iteration while mak-ing a minimal update to the parameter distributionin the KL sense:(?i+1,?i+1) =arg min?,?DKL(N (?,?)
?N (?i,?i))s.t.
Prw[yi(w ?
xi) ?
0] ?
?
(1)Dredze et al (2008) showed that this optimizationcan be solved in closed form, yielding the updates?i+1= ?i+ ?i?ixi(2)?i+1=(?
?1i+ ?ixixTi)?1(3)for appropriate ?iand ?i.For prediction, they use the Bayesian ruley?
= arg maxz?
{?1}Prw?N (?,?
)[z (x ?w) ?
0] ,which for binary labels is equivalent to using themean parameters directly, y?
= sign (?
?
x).4 Multi-Class Confidence WeightedLearningAs in the binary case, we maintain a distributionover weight vectors w ?
N (?,?).
Given an in-put instance x, a Gibbs classifier draws a weightvector w ?
N (?,?)
and then predicts the labelwith the maximal score, arg maxz(w ?
f(x, z)).As in the binary case, we use this prediction ruleto define a robustness condition and correspondinglearning updates.We generalize the robustness condition used inCrammer et al (2008).
Following the update onround i, we require that the ith instance is correctlylabeled with probability at least ?
< 1.
Among thedistributions that satisfy this condition, we choosethe one that has the minimal KL distance from thecurrent distribution.
This yields the update(?i+1,?i+1) = (4)arg min?,?DKL(N (?,?)
?N (?i,?i))s.t.
Pr [yi|xi,?,?]
?
?
,wherePr [y |x,?,?]
=Prw?N (?,?
)[y = arg maxz?Y(w ?
f(x, z))].Due to the max operator in the constraint, this op-timization is not convex when K > 2, and it doesnot permit a closed form solution.
We thereforedevelop approximations that can be solved effi-ciently.
We define the following set of events for ageneral input x:Ar,s(x)def= {w : w ?
f(x, r) ?
w ?
f(x, s)}Br(x)def= {w : w ?
f(x, r) ?
w ?
f(x, s) ?s}=?s 6=rAr,s(x)We assume the probability that w ?
f(x, r) =w ?
f(x, s) for some s 6= r is zero, which497holds for non-trivial distribution parameters andfeature vectors.
We rewrite the prediction y?
=arg maxrPr [Br(x)], and the constraint fromEq.
(4) becomesPr [Byi(x)] ?
?
.
(5)We focus now on approximating the event Byi(x)in terms of events Ayi,r.
We rely on the fact thatthe level sets of Pr [Ayi,r] are convex in ?
and?.
This leads to convex constraints of the formPr [Ayi,r] ?
?.Outer Bound: Since Br(x) ?
Ar,s(x), it holdstrivially that Pr [Byi(x)] ?
?
?
Pr [Ayi,r] ?
?,?r 6= yi.
Thus we can replace the constraintPr [Byi(x)] ?
?
with Pr [Ayi,r] ?
?
to achieve anouter bound.
We can simultaneously apply all ofthe pairwise constraints to achieve a tighter bound:Pr [Ayi,r] ?
?
?r 6= yiThis yields a convex approximation to Eq.
(4) thatmay improve the objective value at the cost ofviolating the constraint.
In the context of learn-ing, this means that the new parameter distribu-tion will be close to the previous one, but may notachieve the desired confidence on the current ex-ample.
This makes the updates more conservative.Inner Bound: We can also consider an innerbound.
Note that Byi(x)c= (?rAyi,r(x))c=?rAyi,r(x)c, thus the constraint Pr [Byi(x)] ?
?is equivalent toPr [?rAyi,r(x)c] ?
1?
?
,and by the union bound, this follows whenever?rPr [Ayi,r(x)c] ?
1?
?
.We can achieve this by choosing non-negative?r?
0,?r?r= 1, and constrainingPr [Ayi,r(x)] ?
1?
(1?
?)
?rfor r 6= yi.This formulation yields an inner bound on theoriginal constraint, guaranteeing its satisfactionwhile possibly increasing the objective.
In thecontext of learning, this is a more aggressive up-date, ensuring that the current example is robustlyclassified even if doing so requires a larger changeto the parameter distribution.Algorithm 1 Multi-Class CW Online AlgorithmInput: Confidence parameter ?Feature function f(x, y) ?
RdInitialize: ?1= 0 , ?1= Ifor i = 1, 2 .
.
.
doReceive xi?
XPredict ranking of labels y?1, y?2, .
.
.Receive yi?
YSet ?i+1,?i+1by approximately solvingEq.
(4) using one of the following:Single-constraint update (Sec.
5.1)Exact many-constraint update (Sec.
5.2)Seq.
many-constraint approx.
(Sec.
5.2)Parallel many-constraint approx.
(Sec.
5.2)end forOutput: Final ?
and ?Discussion: The two approximations are quitesimilar in form.
Both replace the constraintPr [Byi(x)] ?
?
with one or more constraints ofthe formPr [Ayi,r(x)] ?
?r.
(6)To achieve an outer bound we choose ?r= ?
forany set of r 6= yi.
To achieve an inner bound weuse all K ?
1 possible constraints, setting ?r=1 ?
(1?
?)
?rfor suitable ?r.
A simple choice is?r= 1/(K ?
1).In practice, ?
is a learning parameter whosevalue will be optimized for each task.
In this case,the outer bound (when all constraints are included)and inner bound (when ?r= 1/(K ?
1)) can beseen as equivalent, since for any fixed value of?
(in)for the inner bound we can choose?
(out)= 1?1?
?
(in)K ?
1,for the outer bound and the resulting ?rwill beequal.
By optimizing ?
we automatically tune theapproximation to achieve the best compromise be-tween the inner and outer bounds.
In the follow-ing, we will therefore assume ?r= ?.5 Online UpdatesOur algorithms are online and process examplesone at a time.
Pseudo-code for our approach isgiven in algorithm 1.
We approximate the pre-diction step by ranking each label y accordingto the score given by the mean weight vector,?
?
f(xi, y).
Although this approach is Bayes op-timal for binary problems (Dredze et al, 2008),498it is an approximation in general.
We note thatmore accurate inference can be performed in themulti-class case by sampling weight vectors fromthe distribution N (?,?)
or selecting labels sen-sitive to the variance of prediction; however, inour experiments this did not improve performanceand required significantly more computation.
Wetherefore proceed with this simple and effectiveapproximation.The update rule is given by an approximationof the type described in Sec.
4.
All that remainsis to choose the constraint set and solve the opti-mization efficiently.
We discuss several schemesfor minimizing KL divergence subject to one ormore constraints of the form Pr [Ayi,r(x)] ?
?.We start with a single constraint.5.1 Single-Constraint UpdatesThe simplest approach is to select the single con-straint Pr [Ayi,r(x)] ?
?
corresponding to thehighest-ranking label r 6= yi.
This ensures that,following the update, the true label is more likelyto be predicted than the label that was its closestcompetitor.
We refer to this as the k = 1 update.Whenever we have only a single constraint, wecan reduce the optimization to one of the closed-form CW updates used for binary classification.Several have been proposed, based on linear ap-proximations (Dredze et al, 2008) and exact for-mulations (Crammer et al, 2008).
For simplicity,we use the Variance method from Dredze et al(2008), which did well in our initial evaluations.This method leads to the following update rules.Note that in practice ?
is projected to a diagonalmatrix as part of the update; this is necessary dueto the large number of features that we use.
?i+1= ?i+ ?i?igi,yi,r(7)?i+1=(?
?1i+ 2?i?gi,yi,rg>i,yi,r)?1(8)gi,yi,r= f(xi, yi)?
f (xi, r) ?
= ??1(?
)The scale ?iis given by max(?i, 0), where ?iisequal to?
(1 + 2?mi) +?
(1 + 2?mi)2?
8?(mi?
?vi)4?viandmi= ?i?
gi,yi,rvi= g>i,yi,r?igi,yi,r.These rules derive directly from Dredze et al(2008) or Figure 1 in Crammer et al (2008); wesimply substitute yi= 1 and xi= gi,yi,r.5.2 Many-Constraints UpdatesA more accurate approximation can be obtainedby selecting multiple constraints.
Analogously, wechoose the k ?
K?1 constraints corresponding tothe labels r1, .
.
.
, rk6= yithat achieve the highestpredicted ranks.
The resulting optimization is con-vex and can be solved by a standard Hildreth-likealgorithm (Censor & Zenios, 1997).
We refer tothis update as Exact.
However, Exact is expen-sive to compute, and tends to over-fit in practice(Sec.
6.2).
We propose several approximate alter-natives.Sequential Update: The Hildreth algorithm it-erates over the constraints, updating with respectto each until convergence is reached.
We approxi-mate this solution by making only a single pass:?
Set ?i,0= ?iand ?i,0= ?i.?
For j = 1, .
.
.
, k, set (?i,j,?i,j) to the solu-tion of the following optimization:min?,?DKL(N (?,?)
?N(?i,j?1,?i,j?1))s.t.
Pr[Ayi,rj(x)]?
??
Set ?i+1= ?i,kand ?i+1= ?i,k.Parallel Update: As an alternative to the Hil-dreth algorithm, we consider the simultaneous al-gorithm of Iusem and Pierro (1987), which findsan exact solution by iterating over the constraintsin parallel.
As above, we approximate the exactsolution by performing only one iteration.
Theprocess is as follows.?
For j = 1, .
.
.
, k, set (?i,j,?i,j) to the solu-tion of the following optimization:min?,?DKL(N (?,?)
?N (?i,?i))s.t.
Pr[Ayi,rj(x)]?
??
Let ?
be a vector, ?j?0 ,?j?j=1.?
Set ?i+1=?j?j?i,j, ??1i+1=?j?j?
?1i,j.In practice we set ?j= 1/k for all j.6 Experiments6.1 DatasetsFollowing the approach of Dredze et al (2008),we evaluate using five natural language classifica-tion tasks over nine datasets that vary in difficulty,size, and label/feature counts.
See Table 1 for anoverview.
Brief descriptions follow.499Task Instances Features Labels Bal.20 News 18,828 252,115 20 YAmazon 7 13,580 686,724 7 YAmazon 3 7,000 494,481 3 YEnron A 3,000 13,559 10 NEnron B 3,000 18,065 10 NNYTD 10,000 108,671 26 NNYTO 10,000 108,671 34 NNYTS 10,000 114,316 20 NReuters 4,000 23,699 4 NTable 1: A summary of the nine datasets, includ-ing the number of instances, features, and labels,and whether the numbers of examples in each classare balanced.Amazon Amazon product reviews.
Using thedata of Dredze et al (2008), we created two do-main classification datasets from seven producttypes (apparel, books, dvds, electronics, kitchen,music, video).
Amazon 7 includes all seven prod-uct types and Amazon 3 includes books, dvds, andmusic.
Feature extraction follows Blitzer et al(2007) (bigram features and counts).20 Newsgroups Approximately 20,000 news-group messages, partitioned across 20 differentnewsgroups.1This dataset is a popular choice forbinary and multi-class text classification as well asunsupervised clustering.
We represent each mes-sage as a binary bag-of-words.Enron Automatic sorting of emails into fold-ers.2We selected two users with many emailfolders and messages: farmer-d (Enron A) andkaminski-v (Enron B).
We used the ten largestfolders for each user, excluding non-archival emailfolders such as ?inbox,?
?deleted items,?
and ?dis-cussion threads.?
Emails were represented as bi-nary bags-of-words with stop-words removed.NY Times To the best of our knowledge we arethe first to evaluate machine learning methods onthe New York Times corpus.
The corpus con-tains 1.8 million articles that appeared from 1987to 2007 (Sandhaus, 2008).
In addition to beingone of the largest collections of raw news text,it is possibly the largest collection of publicly re-leased annotated news text, and therefore an idealcorpus for large scale NLP tasks.
Among otherannotations, each article is labeled with the deskthat produced the story (Financial, Sports, etc.
)(NYTD), the online section to which the article was1http://people.csail.mit.edu/jrennie/20Newsgroups/2http://www.cs.cmu.edu/?enron/Task Sequential Parallel Exact20 News 92.16 91.41 88.08Amazon 7 77.98 78.35 77.92Amazon 3 93.54 93.81 93.00Enron A 82.40 81.30 77.07Enron B 71.80 72.13 68.00NYTD 83.43 81.43 80.92NYTO 82.02 78.67 80.60NYTS 52.96 54.78 51.62Reuters 93.60 93.97 93.47Table 2: A comparison of k = ?
updates.
Whilethe two approximations (sequential and parallel)are roughly the same, the exact solution over-fits.posted (NYTO), and the section in which the arti-cle was printed (NYTS).
Articles were representedas bags-of-words with feature counts (stop-wordsremoved).Reuters Over 800,000 manually categorizednewswire stories (RCV1-v2/ LYRL2004).
Eacharticle contains one or more labels describing itsgeneral topic, industry, and region.
We performedtopic classification with the four general topics:corporate, economic, government, and markets.Details on document preparation and feature ex-traction are given by Lewis et al (2004).6.2 EvaluationsWe first set out to compare the three update ap-proaches proposed in Sec.
5.2: an exact solutionand two approximations (sequential and parallel).Results (Table 2) show that the two approxima-tions perform similarly.
For every experiment theCW parameter ?
and the number of iterations (upto 10) were optimized using a single randomizediteration.
However, sequential converges faster,needing an average of 4.33 iterations compared to7.56 for parallel across all datasets.
Therefore, weselect sequential for our subsequent experiments.The exact method performs poorly, displayingthe lowest performance on almost every dataset.This is unsurprising given similar results for bi-nary CW learning Dredze et al (2008), where ex-act updates were shown to over-fit but convergedafter a single iteration of training.
Similarly, ourexact implementation converges after an averageof 1.25 iterations, much faster than either of theapproximations.
However, this rapid convergenceappears to come at the expense of accuracy.
Fig.
1shows the accuracy on Amazon 7 test data aftereach training iteration.
While both sequential andparallel improve with several iterations, exact de-5001 2 3 4 5Training Iterations77.077.578.078.5Test AccuracyK=1Sequential K=5Sequential K=AllParallel K=AllExact K=AllFigure 1: Accuracy on test data after each iterationon the Amazon 7 dataset.grades after the first iteration, suggesting that itmay over-fit to the training data.
The approxima-tions appear to smooth learning and produce betterperformance in the long run.6.3 Relaxing Many-ConstraintsWhile enforcing many constraints may seem op-timal, there are advantages to pruning the con-straints as well.
It may be time consuming to en-force dozens or hundreds of constraints for taskswith many labels.
Structured prediction tasks of-ten involve exponentially many constraints, mak-ing pruning mandatory.
Furthermore, many realworld datasets, especially in NLP, are noisy, andenforcing too many constraints can lead to over-fitting.
Therefore, we consider the impact of re-ducing the constraint set in terms of both reducingrun-time and improving accuracy.We compared using all constraints (k = ?
)with using 5 constraints (k = 5) for the sequentialupdate method (Table 3).
First, we observe thatk = 5 performs better than k =?
on nearly everydataset: fewer constraints help avoid over-fittingand once again, simpler is better.
Additionally,k = 5 converges faster than k = ?
in an averageof 2.22 iterations compared with 4.33 iterations.Therefore, reducing the number of constraints im-proves both speed and accuracy.
In comparingk = 5 with the further reduced k = 1 results, weobserve the latter improves on seven of the ninemethods.
This surprising result suggests that CWlearning can perform well even without consid-ering more than a single constraint per example.However, k = 1 exceeds the performance of mul-tiple constraints only through repeated training it-erations.
k = 5 CW learning converges faster ?2.22 iterations compared with 6.67 for k = 1 ?
adesirable property in many resource restricted set-tings.
(In the true online setting, only a single it-eration may be possible.)
Fig.
1 plots the perfor-mance of k = 1 and k = 5 CW on test data aftereach training iteration.
While k = 1 does betterin the long run, it lags behind k = 5 for severaliterations.
In fact, after a single training iteration,k = 5 outperforms k = 1 on eight out of ninedatasets.
Thus, there is again a tradeoff betweenfaster convergence (k = 5) and increased accuracy(k = 1).
While the k = 5 update takes longer periteration, the time required for the approximate so-lutions grows only linearly in the number of con-straints.
The evaluation in Fig.
1 required 3 sec-onds for the first iteration of k = 1, 10 secondsfor k = 5 and 11 seconds for one iteration of all7 constraints.
These differences are insignificantcompared to the cost of performing multiple itera-tions over a large dataset.
We note that, while bothapproximate methods took about the same amountof time, the exact solution took over 4 minutes forits first iteration.Finally, we compare CW methods with sev-eral baselines in Table 3.
Online baselines in-clude Top-1 Perceptron (Collins, 2002), Top-1Passive-Aggressive (PA), and k-best PA (Cram-mer & Singer, 2003; McDonald et al, 2004).Batch algorithms include Maximum Entropy (de-fault configuration in McCallum (2002)) and sup-port vector machines (LibSVM (Chang & Lin,2001) for one-against-one classification and multi-class (MC) (Crammer & Singer, 2001)).
Classifierparameters (C for PA/SVM and maxent?s Gaus-sian prior) and number of iterations (up to 10) forthe online methods were optimized using a sin-gle randomized iteration.
On eight of the ninedatasets, CW improves over all baselines.
In gen-eral, CW provides faster and more accurate multi-class predictions.7 Error and Probabilistic OutputOur focus so far has been on accuracy and speed.However, there are other important considerationsfor selecting learning algorithms.
Maximum en-tropy and other probabilistic classification algo-rithms are sometimes favored for their probabil-ity scores, which can be useful for integrationwith other learning systems.
However, practition-501PA CW SVMTask Perceptron K=1 K=5 K=1 K=5 K=?
1 vs. 1 MC Maxent20 News 81.07 88.59 88.60 ?
?92.90 ?
?92.78 ?
?92.16 85.18 90.33 88.94Amazon 7 74.93 76.55 76.72 ?
?78.70 ?
?78.04 ?
?77.98 75.11 76.60 76.40Amazon 3 92.26 92.47 93.29 ?94.01 ?
?94.29 93.54 92.83 93.60 93.60Enron A 74.23 79.27 80.77 ?
?83.83 ?82.23 ?82.40 80.23 82.60 82.80Enron B 66.30 69.93 68.90 ?
?73.57 ?
?72.27 ?
?71.80 65.97 71.87 69.47NYTD 80.67 83.12 81.31 ?
?84.57 ?83.94 83.43 82.95 82.00 83.54NYTO 78.47 81.93 81.22 ?82.72 ?82.55 82.02 82.13 81.01 82.53NYTS 50.80 56.19 55.04 54.67 54.26 52.96 55.81 56.74 53.82Reuters 92.10 93.12 93.30 93.60 93.67 93.60 92.97 93.32 93.40Table 3: A comparison of CW learning (k = 1, 5,?
with sequential updates) with several baselinealgorithms.
CW learning achieves the best performance eight out of nine times.
Statistical significance(McNemar) is measured against all baselines (?
indicates 0.05 and ??
0.001) or against online baselines(?
indicates 0.05 and ??
0.001).0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75282930313233entropyerrorMC CWMaxEnt0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9020040060080010001200Bin lower thresholdNumberof examples perbinMaxEntMC CW0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9024681012Bin lower thresholdTest error in binMaxEntMC CW0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.900.10.20.30.40.50.60.70.8Bin lower thresholdTest error givenbinMaxEntMC CWFigure 2: First panel: Error versus prediction entropy on Enron B.
As CW converges (right to left) errorand entropy are reduced.
Second panel: Number of test examples per prediction probability bin.
Thered bars correspond to maxent and the blue bars to CW, with increasing numbers of epochs from leftto right.
Third panel: The contribution of each bin to the total test error.
Fourth panel: Test errorconditioned on prediction probability.ers have observed that maxent probabilities canhave low entropy and be unreliable for estimatingprediction confidence (Malkin & Bilmes, 2008).Since CW also produces label probabilities ?
anddoes so in a conceptually distinct way ?
we in-vestigate in this section some empirical propertiesof the label distributions induced by CW?s param-eter distributions and compare them with those ofmaxent.We trained maxent and CW k = 1 classi-fiers on the Enron B dataset, optimizing parame-ters as before (maxent?s Gaussian prior and CW?s?).
We estimated the label distributions from ourCW classifiers after each iteration and on everytest example x by Gibbs sampling weight vec-tors w ?
N (?,?
), and for each label y count-ing the fraction of weight vectors for which y =arg maxzw ?
f(x, z).
Normalizing these countsyields the label distributions Pr [y|x].
We denoteby y?
the predicted label for a given x, and refer toPr [y?|x] as the prediction probability.The leftmost panel of Fig.
2 plots eachmethod?s prediction error against the nor-malized entropy of the label distribution?
(1m?i?zPr [z|xi] log (Pr [z|xi]))/ log(K).Each CW iteration (moving from right to left inthe plot) reduces both error and entropy.
From ourmaxent results we make the common observationthat maxent distributions have (ironically) lowentropy.
In contrast, while CW accuracy exceedsmaxent after its second iteration, normalizedentropy remains high.
Higher entropy suggestsa distribution over labels that is less peaked andpotentially more informative than those frommaxent.
We found that the average probabilityassigned to a correct prediction was 0.75 forCW versus 0.83 for maxent and for an incorrectprediction was 0.44 for CW versus 0.56 formaxent.Next, we investigate how these probabilitiesrelate to label accuracy.
In the remaining pan-els, we binned examples according to their pre-diction probabilities Pr [y?|x] = maxyPr [y|x].The second panel of Fig.
2 shows the numbersof test examples with Pr [y?|x] ?
[?, ?
+ 0.1) for?
= 0.1, 0.2 .
.
.
0.9.
(Note that since there are 10502classes in this problem, we must have Pr [y?|x] ?0.1.)
The red (leftmost) bar corresponds to themaximum entropy classifier, and the blue bars cor-respond, from left to right, to CW after each suc-cessive training epoch.From the plot we observe that the maxent classi-fier assigns prediction probability greater than 0.9to more than 1,200 test examples out of 3,000.Only 50 examples predicted by maxent fall in thelowest bin, and the rest of examples are distributednearly uniformly across the remaining bins.
Thelarge number of examples with very high predic-tion probability explains the low entropy observedfor the maximum entropy classifier.In contrast, the CW classifier shows the oppo-site behavior after one epoch of training (the left-most blue bar), assigning low prediction probabil-ity (less than 0.3) to more than 1,200 examplesand prediction probability of at least 0.9 to only100 examples.
As CW makes additional passesover the training data, its prediction confidenceincreases and shifts toward more peaked distribu-tions.
After seven epochs fewer than 100 exampleshave low prediction probability and almost 1,000have high prediction probability.
Nonetheless, wenote that this distribution is still less skewed thanthat of the maximum entropy classifier.Given the frequency of high probability maxentpredictions, it seems likely that many of the highprobability maxent labels will be wrong.
This isdemonstrated in the third panel, which shows thecontribution of each bin to the total test error.
Eachbar reflects the number of mistakes per bin dividedby the size of the complete test set (3,000).
Thus,the sum of the heights of the corresponding barsin each bin is proportional to test error.
Much ofthe error of the maxent classifier comes not onlyfrom the low-probability bins, due to their inac-curacy, but also from the highest bin, due to itsvery high population.
In contrast, the CW clas-sifiers see very little error contribution from thehigh-probability bins.
As training progresses, wesee again that the CW classifiers move in the direc-tion of the maxent classifier but remain essentiallyunimodal.Finally, the rightmost panel shows the condi-tional test error given bin identity, or the fractionof test examples from each bin where the predic-tion was incorrect.
This is the pointwise ratio be-tween corresponding values of the previous twohistograms.
For both methods, there is a monoton-ically decreasing trend in error as prediction prob-ability increases; that is, the higher the value ofthe prediction probability, the more likely that theprediction it provides is correct.
As CW is trained,we see an increase in the conditional test error, yetthe overall error decreases (not shown).
This sug-gests that as CW is trained and its overall accuracyimproves, there are more examples with high pre-diction probability, and the cost for this is a rela-tive increase in the conditional test error per bin.The maxent classifier produces an extremely largenumber of test examples with very high predictionprobabilities, which yields relatively high condi-tional test error.
In nearly all cases, the conditionalerror values for the CW classifiers are smaller thanthe corresponding values for maximum entropy.These observations suggest that CW assigns prob-abilities more conservatively than maxent does,and that the (fewer) high confidence predictions itmakes are of a higher quality.
This is a potentiallyvaluable property, e.g., for system combination.8 ConclusionWe have proposed a series of approximations formulti-class confidence weighted learning, wherethe simple analytical solutions of binary CWlearning do not apply.
Our best CW method out-performs online and batch baselines on eight ofnine NLP tasks, and is highly scalable due to theuse of a single optimization constraint.
Alterna-tively, our multi-constraint algorithms provide im-proved performance for systems that can affordonly a single pass through the training data, as inthe true online setting.
This result stands in con-trast to previously observed behaviors in non-CWsettings (McDonald et al, 2004).
Additionally, wefound improvements in both label entropy and ac-curacy as compared to a maximum entropy clas-sifier.
We plan to extend these ideas to structuredproblems with exponentially many labels and de-velop methods that efficiently model label correla-tions.
An implementation of CW multi-class algo-rithms is available upon request from the authors.ReferencesBlitzer, J., Dredze, M., & Pereira, F. (2007).Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentimentclassification.
Association for ComputationalLinguistics (ACL).503Censor, Y., & Zenios, S. (1997).
Parallel opti-mization: Theory, algorithms, and applications.Oxford University Press, New York, NY, USA.Chang, C.-C., & Lin, C.-J.
(2001).
LIBSVM: alibrary for support vector machines.
Softwareavailable at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Collins, M. (2002).
Discriminative training meth-ods for hidden markov models: Theory and ex-periments with perceptron algorithms.
Empir-ical Methods in Natural Language Processing(EMNLP).Crammer, K., Dredze, M., & Pereira, F. (2008).Exact confidence-weighted learning.
Advancesin Neural Information Processing Systems 22.Crammer, K., & Singer, Y.
(2001).
On the al-gorithmic implementation of multiclass kernel-based vector machines.
Jornal of MachineLearning Research, 2, 265?292.Crammer, K., & Singer, Y.
(2003).
Ultraconserva-tive online algorithms for multiclass problems.Jornal of Machine Learning Research (JMLR),3, 951?991.Dredze, M., Crammer, K., & Pereira, F. (2008).Confidence-weighted linear classification.
In-ternational Conference on Machine Learning(ICML).Iusem, A., & Pierro, A. D. (1987).
A simultaneousiterative method for computing projections onpolyhedra.
SIAM J.
Control and Optimization,25.Lewis, D. D., Yang, Y., Rose, T. G., & Li, F.(2004).
Rcv1: A new benchmark collection fortext categorization research.
Journal of MachineLearning Research (JMLR), 5, 361?397.Malkin, J., & Bilmes, J.
(2008).
Ratio semi-definite classifiers.
IEEE Int.
Conf.
on Acous-tics, Speech, and Signal Processing.McCallum, A.
(2002).
MALLET: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.McDonald, R., Crammer, K., & Pereira, F. (2004).Large margin online learning algorithms forscalable structured classification.
NIPS Work-shop on Structured Outputs.Sandhaus, E. (2008).
The new york times an-notated corpus.
Linguistic Data Consortium,Philadelphia.504
