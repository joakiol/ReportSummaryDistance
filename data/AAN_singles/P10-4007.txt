Proceedings of the ACL 2010 System Demonstrations, pages 36?41,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsTalking NPCs in a Virtual Game WorldTina Klu?wer, Peter Adolphs, Feiyu Xu, Hans Uszkoreit, Xiwen ChengDeutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)Projektbu?ro BerlinAlt-Moabit 91c10559 BerlinGermany{tina.kluewer,peter.adolphs,feiyu,uszkoreit,xiwen.cheng}@dfki.deAbstractThis paper describes the KomParse sys-tem, a natural-language dialog systemin the three-dimensional virtual worldTwinity.
In order to fulfill the variouscommunication demands between non-player characters (NPCs) and users insuch an online virtual world, the systemrealizes a flexible and hybrid approachcombining knowledge-intensive domain-specific question answering, task-specificand domain-specific dialog with robustchatbot-like chitchat.1 IntroductionIn recent years multi-user online games in virtualworlds such as Second Life or World of Warcrafthave attracted large user communities.
Such vir-tual online game worlds provide new social andeconomic platforms for people to work and inter-act in.
Furthermore, virtual worlds open new per-spectives for research in the social, behavioral, andeconomic sciences, as well as in human-centeredcomputer science (Bainbridge, 2007).
Dependingon the game type, non-player characters (NPCs)are often essential for supporting the game plot,for making the artificial world more vivid and ulti-mately for making it more immersive.
In addition,NPCs are useful to populate new worlds by carry-ing out jobs the user-led characters come in touchwith.
The range of functions to be filled by NPCsis currently still strongly restricted by their limitedcapabilities in autonomous acting and communi-cation.
This shortcoming creates a strong need forprogress in areas such as AI and NLP, especiallytheir planning and dialog systems.The KomParse system, described in this paper,provides NPCs for a virtual online world namedTwinity, a product of the Berlin startup companyMetaversum1.
The KomParse NPCs offer vari-ous services through conversation with game usersusing question-answering and dialog functional-ity.
The utilization of Semantic Web technologywith RDF-encoded generic and domain-specificontologies furthermore enables semantic searchand inference.This paper is organized as follows: Section 2presents the NPC modelling and explains the ap-plication scenarios.
Section 3 details the knowl-edge representation and semantic inference in oursystem.
Section 4 explains the system architectureand its key components.
Section 5 describes theKomParse dialog system.
Section 7 gives a con-clusion and closes off with our future work.2 Application Scenario and NPCModellingThe online game Twinity extends the Second Lifeidea by mirroring an urban part of the real world.At the time of this writing, the simulated section ofreality already contains 3D models of the cities ofBerlin, Singapore and London and it keeps grow-ing.
Users can log into the virtual world, wherethey can meet other users and communicate withthem using the integrated chat function or talkto each other via Voice-over-IP.
They can styletheir virtual appearance, can rent or buy their ownflats and decorate them as to their preferences andtastes.Out of many types of NPCs useful for this appli-cation such as pedestrians, city guides and person-nel in stores, restaurants and bars, we start withtwo specific characters: a female furniture salesagent and a male bartender.
The furniture selleris designed for helping users furnish their virtualapartments.
Users can buy pieces of furniture androom decoration from the NPC by describing theirdemands and wishes in a text chat.
During the di-1http://www.metaversum.com/36Figure 1: The furniture sales NPC selling a sofaalog with the NPC, the preferred objects are thenselected and directly put into a location in theapartment, which can be further refined with theuser interfaces that Twinity provides.In the second scenario, the bartender sells vir-tual drinks.
He can talk about cocktails with users,but moreover, he can also entertain his guests byproviding trivia-type information about popularcelebrities and various relations among them.We chose these two characters not only becauseof their value for the Twinity application but alsofor our research goals.
They differ in many in-teresting aspects.
First of all, the furniture salesagent is controlled by a complex task model in-cluding ontology-driven and data-driven compo-nents to guide the conversation.
This agent alsopossesses a much more fine-grained action model,which allows several different actions to coverthe potential conversation situations for the sell-ing task.
The bartender agent on the other hand isdesigned not to fulfill one strict task because hisclients do not follow a specific goal except order-ing drinks.
Our bartender has the role of a conver-sation companion and is able to entertain clientswith his broad knowledge.
Thus, he is allowed toaccess to several knowledge bases and is able tohandle questions (and later conversations) abouta much larger domain called the ?gossip domain?which enables conversation about pop stars, movieactors and other celebrities as well as the relationsbetween these people.
In order to achieve a highrobustness, we integrate a chatbot into the bar-tender agent to catch chitchat utterances we cannothandle.Figure 2: Our bartender NPC in his bar in Twinity3 Knowledge Representation andSemantic InferenceSemantic Web technology is employed for mod-elling the knowledge of the NPCs.
The ResourceDescription Format (RDF) serves as the base forthe actual encoding.
An RDF statement is a binaryrelation instance between two individuals, that is atriple of a predicate and two arguments, called thesubject and the object, and written as subj pred obj(e.g.
f:Sofa Alatea f:hasMainColourf:Burgundy).All objects and properties the NPC can talkabout are modelled in this way.
Therefore theknowledge base has to reflect the physical prop-erties of the virtual objects in Twinity as faithfullyas possible.
For instance, specific pieces of furni-ture are described by their main color, material orstyle, whereas cocktails are characterized by theiringredients, color, consistence and taste.
Further-more, references to the 3D models of the objectsare stored in order to create, find and remove suchobjects in the virtual world.The concepts and individuals of the particulardomain are structured and organized in domain-specific ontologies.
These ontologies are mod-elled in the Web Ontology Language (OWL).OWL allows us to define concept hierarchies, re-lations between concepts, domains and ranges ofthese relations, as well as specific relation in-stances between instances of a concept.
Our on-tologies are defined by the freely available ontol-ogy editor Prote?ge?
4.02.
The advantage of using anontology for structuring the domain knowledge is2http://protege.stanford.edu/, as accessed27 Oct 200937TwinityServerKomParseServerTwinityClientConversationalAgentConversationalAgentConversationalAgentTwinityClientTwinityClientFigure 3: Overall System Architecture ?
Server/Client Architecture for NPC Controlthe modular non-redundant encoding.
When com-bined with a reasoner, only a few statements aboutan individual have to be asserted explicitely, whilethe rest can be inferred from the ontology.
We em-ploy several ontologies, among which the follow-ing are relevant for modelling the specific domainsof our NPCs:?
An extensive furniture ontology, created byour project partner ZAS Berlin, definingkinds of furniture, room parts, colors andstyles as well as the specific instances of fur-niture in Twinity.
This knowledge base con-tains 95,258 triples, 123 furniture classes, 20color classes, 243 color instances and variousclasses defining styles and similar concepts.?
A cocktail ontology, defining 13 cocktailclasses with ingredients and tastes in 21,880triples.?
A biographical ontology, the ?gossip on-tology?, defining biographical and career-specific concepts for people.
This ontology isaccompanied by a huge database of celebri-ties, which has been automatically acquiredfrom the Web and covers nearly 600,000 per-sons and relations between these people likefamily relationships, marriages and profes-sional relations.
(Adolphs et al, 2010)The furniture ontology is the only knowledgebase for the furniture sales agent, whereas the bar-tender NPC has access to both the cocktail as wellas the gossip knowledge base.We use SwiftOwlim3 for storing and queryingthe data.
SwiftOwlim is a ?triple store?, a kindof database which is specifically built for storingand querying RDF data.
It provides a forward-chaining inference engine which evaluates thedomain definitions when loading the knowledgerepository, and makes implicit knowledge explicitby asserting triples that must also hold true accord-ing to the ontology.
Once the reasoner is finished,the triple store can be queried directly using theRDF query language SPARQL.3http://www.ontotext.com/owlim/4 Overall System ArchitectureFigure 3 shows the overall system architecture.Twinity is a server/client application, in which theserver hosts the virtual world and coordinates theuser interactions.
In order to use Twinity, usershave to download the Twinity client.
The clientallows the user to control the physical represen-tation of the user?s character in the virtual world,also called the ?avatar?.
Thus the client is respon-sible for displaying the graphics, calculating theeffects of physical interactions, handling the user?sinput and synchronizing the 3D data and user ac-tions with the Twinity server.Each NPC comprises two major parts: whereasits avatar is the physical appearance of the NPC inthe virtual world, the ?conversational agent?
pro-vides the actual control logic which controls theavatar autonomously.
It is in particular able to holda conversation with Twinity users in that it reactsto a user?s presence, interprets user?s utterances indialog context and generates adequate responses.The KomParse server is a multi-client, multi-threaded server written in Java that hosts the con-versational agents for the NPCs (section 5).
TheNPC?s avatar, on the other hand, is realized by amodified Twinity client.
We utilize the Python in-terface provided by the Twinity client to call ourown plugin which opens a bidirectional socketconnection to the KomParse server.
The plugin isstarted together with the Twinity client and servesas a mediator between the Twinity server and theKomParse server from then on (fig.
3).
It sends allin-game events relevant to our system to the serverand translates the commands sent by the serverinto Twinity-specific actions.The integration architecture allows us to bemaximally independent of the specific game plat-form.
Rather than using the particular program-ming language and development environment ofthe platform for realizing the conversational agentor reimplementing a whole client/server proto-col for connecting the avatar to the correspondingagent, we use an interface tailored to the specificneeds of our system.
Thus the KomParse system38can be naturally extended to other platforms sinceonly the avatar interfaces have to be adapted.The integration architecture also has the advan-tage that the necessary services can be easily dis-tributed in a networked multi-platform environ-ment.
The Twinity clients require aMicrosoftWin-dows machine with a 3D graphics card supportingDirectX 9.0c or higher, 1 GB RAM and a CPUcore per instance.
The KomParse server requiresroughly 1 GB RAM.
The triple store is run asa separate server process and is accessed by anXML-RPC interface.
Roughly 1.2 GB RAM arerequired for loading our current knowledge base.5 Conversational Agent: KomParseDialog SystemFigure 4: Dialog System: Conversational AgentThe KomParse dialog system, the main func-tionality of the conversational agent, consists ofthe following three major components: input ana-lyzer, dialog manager and output generator (fig.4).The input analyzer is responsible for the lin-guistic analysis of the user?s textual input includ-ing preprocessing such as string cleaning, part-of-speech tagging, named entity recognition, parsingand semantic interpretation.
It yields a semanticrepresentation which is the input for the dialogmanager.The dialog manager takes the result of the inputanalyzer and delivers an interpretation based onthe dialog context and the available knowledge.
Italso controls the task conversation chain and han-dles user requests.
The dialog manager determinesthe next system action based on the interpreted pa-rameters.The output generator realizes the action definedby the dialog manager with its multimodal gener-ation competence.
The generated results can beverbal, gestural or a combination of both.As mentioned above, our dialog system has todeal with two different scenarios.
While the fo-cal point of the bartender agent lies in the questionanswering functionality, the furniture sales agentis driven by a complex dialog task model based ona dialog graph.
Thus, the bartender agent reliesmainly on question answering technology, in thatit needs to understand questions and extract theright answer from our knowledge bases, whereasthe sales agent has to accommodate various dialogsituations with respect to a sales scenario.
It there-fore has to understand the dialog acts intendedby the user and trigger the corresponding reac-tions, such as presenting an object, memorizinguser preferences, negotiating further sales goals,etc.The task model for sales conversations is in-spired by a corpus resulting from the annotation ofa Wizard-of-Oz experiment in the furniture salesagent scenario carried out by our project partner atZAS (Bertomeu and Benz, 2009).
In these exper-iments, 18 users spent one hour each on furnish-ing a virtual living room in a Twinity apartment bytalking to a human wizard controlling the virtualsales agent.
The final corpus consists of 18 di-alogs containing 3,171 turns with 4,313 utterancesand 23,015 alpha-numerical strings (words).
Thefollowing example shows a typical part of such aconversation:USR.1: And do we have a little side table for the TV?NPC.1: I could offer you another small table or a sideboard.USR.2: Then I?ll take a sideboard thats similar to my shelf.NPC.2: Let me check if we have something like that.Table 1: Example Conversation from the Wizard-of-Oz ExperimentThe flow of the task-based conversation is con-trolled by a data-driven finite-state model, whichis the backbone of the dialog manager.
Duringa sales conversation, objects and features of ob-jects mentioned by the NPC and the user are ex-tracted from the knowledge bases and added intothe underspecified graph nodes and egdes at run-time.
This strategy keeps the finite-state graph assmall as possible.
Discussed objects and their fea-tures are stored in a frame-based sub-componentnamed ?form?.
The form contains entries whichcorrespond to ontological concepts in the furni-39ture ontology.
During conversation, these entrieswill be specified with the values of the propertiesof the discussed objects.
This frame-based ap-proach increases the flexibility of the dialog man-ager (McTear, 2002) and is particularly useful fora task-driven dialog system.
As long as the negoti-ated object is not yet fully specified, the form rep-resents the underspecified object description ac-cording to the ontology concept.
Every time theuser states a new preference or request, the formis enriched with additional features until the set ofobjects is small enough to be presented to the userfor final selection.
Thus the actual flow of dia-log according to the task model does not have tobe expressed by the graph but can be derived ondemand from the knowledge and handled by theform which in turn activates the appropriate dia-log subgraphs.
This combination of graph-baseddialog models and form-based task modelling ef-fectively accounts for the interaction of sequentialdialog strategies and the non-sequential nature ofcomplex dialog goals.Given a resolved semantic representation, thedialog manager triggers either a semantic searchin the knowledge bases to deliver factual answersas needed in a gossip conversation or a further di-alog response for example providing choices forthe user in a sales domain.
The semantic search isneeded in both domains.
In case that the semanticrepresentation can neither be resolved in the taskdomain nor in the gossip domain, it gets passed tothe embedded A.L.I.C.E.
chatbot that uses its ownunderstanding and generation components (Wal-lace and Bush, 2001).5.1 Semantic RepresentationThe input understanding of the system is imple-mented as one single understanding pipeline.Theunderstanding pipeline delivers a semantic repre-sentation which is the basis for the decision of thedialog manager which action to perform next.This semantic representation can be extractedfrom the user input by our understanding com-ponent via a robust hybrid approach: either via anumber of surface patterns containing regular ex-pressions or via patterns reflecting the syntacticanalysis of a dependency parser (de Marneffe andManning, 2008).The representation?s structure is inspired by ourknowledge representation design described in sec-tion 3 as well as by predicate logic.
The core of therepresentation is a predicate-argument structurelimited to two arguments including message typeand the whole syntactic information found by theanalysis pipeline.
The field ?Message Type?
canhave one of the following values: wh-question,yes/no-question, declarative.
Predicates can oftenbe instantiated with the lemmatized matrix verb ofthe successfully analysed piece of the input.
If theinput contains a wh-question, the questioned factis marked as an unfilled argument slot.
The gen-eral structure can be simplified described as:<PREDICATE, ARG1, ARG2, [message-type]>The following examples show the structure usedfor different input:?
?Who is the boyfriend of Madonna?
?<hasBoyfriend, Madonna, ?, [wh]>?
?I want to buy a sofa.
?<buy, I, "a sofa", [declarative]>5.2 Information ExtractionBoth scenarios make use of state-of-the-art infor-mation extraction approaches to extract the impor-tant pieces from the user input.
While the bar-tender depends on relation extraction to detect thefact or relation questioned by the user (Xu et al,2007), the sales agent uses information extractionmethods to recognize user wishes and demands.As a result, the questioned fact or the demandedobject feature equals the ontology structure con-taining the knowledge needed to handle the userinput.
The input ?Do you have any red couches?
?for example needs to get processed by the systemin such a way that the information regarding thesofa with red color is extracted.This is done by the system in a data-driven way.The input analysis first tries to find a demandedobject in the input via asking the ontology: Everyobject which can be discussed in the scenario isencoded in the sales agents knowledge base.
Thiscan be seen as a Named Entity Recognition step.In case of success, the system tries to detect oneof the possible relations of the object found in theinput.
This is achieved by querying the ontologyabout what kind of relations the identified objectcan satisfy.
Possible relations are encoded in theclass description of the given object.
As a resultthe system can detect a relation ?hasColour?
forthe found object ?sofa?
and the color value ?red?.The found information gets inserted into the formwhich gets more and more similar or if possibleequal to a search query via RDF.40Figure 5: Comparison of Input, Extracted Information and Knowledge Base6 Conclusion and Future WorkThe KomParse system demonstrates an attractiveapplication area for dialog systems that bears greatfuture potential.
Natural language dialog withNPCs is an important factor in making virtualworlds more interesting, interactive and immer-sive.
Virtual worlds with conversing characterswill also find many additional applications in edu-cation, marketing, and entertainment.KomParse is an ambitious and neverthelesspragmatic attempt to bring NLP into the world ofvirtual games.
We develop a new strategy to inte-grate task models and domain ontologies into dia-log models.
This strategy is useful for task-drivenNPCs such as furniture sellers.
With the chattybartender, a combination of task-specific dialogand domain-specific question answering enables asmart wide-domain off-task conversation.
Sincethe online game employs bubble-chat as a modeof communication in addition to Voice-over-IP, weare able to test our dialog system in a real-timeapplication without being hindered by imperfectspeech recognition.The system presented here is still work inprogress.
The next goals will include various eval-uation steps.
On the one hand we will focus onsingle components like hybrid parsing of input ut-terances and dialog interpretation in terms of pre-cision and recall.
On the other hand an evaluationof the two different scenarios regarding the us-ability are planned in experiments with end users.Moreover we will integrate some opinion miningand sentiment analysis functionality which can behelpful to better detect and understand the user?spreferences in the furniture sales agents scenario.AcknowledgementsThe project KomParse is funded by the ProFITprogramme of the Federal State of Berlin, co-funded by the EFRE programme of the Euro-pean Union.
The research presented here is ad-ditionally supported through a grant to the projectTAKE, funded by the German Ministry for Edu-cation and Research (BMBF, FKZ: 01IW08003).Many thanks go to our project partners at the Cen-tre for General Linguistics (ZAS) in Berlin as wellas to the supporting company Metaversum.ReferencesPeter Adolphs, Xiwen Cheng, Tina Klu?wer, HansUszkoreit, and Feiyu Xu.
2010.
Question answeringbiographic information and social network poweredby the semantic web.
In Proceedings of LREC 2010,Valletta, Malta.William Sims Bainbridge.
2007.
The scientific re-search potential of virtual worlds.
Science, 317.Nuria Bertomeu and Anton Benz.
2009.
Annotation ofjoint projects and information states in human-npcdialogues.
In Proceedings of the First InternationalConference on Corpus Linguistics (CILC-09), Mur-cia, Spain.Marie C. de Marneffe and Christopher D. Manning.2008.
The Stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, Manchester, UK.Michael F. McTear.
2002.
Spoken dialogue tech-nology: enabling the conversational user interface.ACM Comput.
Surv., 34(1).Richard Wallace and Noel Bush.
2001.
Artificialintelligence markup language (aiml) version 1.0.1(2001).
Unpublished A.L.I.C.E.
AI FoundationWorking Draft (rev 006).Feiyu Xu, Hans Uszkoreit, and Hong Li.
2007.
Aseed-driven bottom-up machine learning frameworkfor extracting relations of various complexity.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 584?591, Prague, Czech Republic, June.
Association forComputational Linguistics.41
