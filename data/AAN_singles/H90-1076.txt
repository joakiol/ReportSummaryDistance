An 86,000-Word Recognizer Based on Phonemic ModelsM.
Lennig, V. Gupta, P. Kenny, P. Mermelstein, D. O'ShaughnessyIN RS-T414communications3 Place du CommerceMontreal, Canada H3E 1H6(514) 765-7772AbstractWe have developed an algorithm for the automaticconversion of dictated English sentences to written text,with essentially no restriction on the nature of the mate-rial dictated.
We require that speakers undergo a shorttraining session so that the system can adapt to theirindividual speaking characteristics and that they leavebrief pauses between words.
We have tested our algo-rithm extensively on an 86,000 word vocabulary (thelargest of any such system in the world) using ninespeakers and obtained word recognition rates on the or-der of 93Uo.IntroductionMost speech recognition systems, research and com-mercial, impose severe restrictions on the vocabularythat may be used.
For a system that aims to do speech-to-text conversion, this is a serious limitation since thespeaker may be unable to express himself in his ownwords without leaving the vocabulary.
From the outsetwe have worked with a very large vocabulary, based onthe 60,000 words in Merriam Webster's Seventh NewCollegiate Dictionary.
We have augmented this num-ber by 26,000 so that at present he probability of en-countering a word not in the vocabulary in a text cho-sen at random from a newspaper, magazine or novel isless than 2% \[25\].
(More than 80% of out-of-vocabularywords are proper names.
)Our vocabulary is thus larger than that of any otherEnglish language speech-to-text system.
IBM has areal-time isolated word recognizer with a vocabulary of20,000 words \[1\] giving over 95% word recognition on anoffice correspondence task.
The perplexity \[16\] of thistask is about 200; the corresponding figure in our case is700.
There is only one speech recognition project in theworld having a larger vocabulary than ours; it is beingdeveloped by IBM France \[20\] and it requires that theuser speak in isolated syllable mode, a constraint whichmay be reasonable in French but which would be veryunnatural in English.Briefly, our approach to the problem of speech recog-nition is to apply the principle of naaximum a poste-riori probability (MAP) using a stochastic model forthe speech data associated with an arbitrary string ofwords.
The model has three components: (i) a languagemodel which assigns prior probabilities to word strings,(ii) a phonological component which assigns phonetictranscriptions to words in the dictionary and (iii) anacoustic-phonetic model which calculates the likelihoodof speech data for an arbitrary phonetic transcription.Language ModelingWe have trained a trigram language model, whichassigns a prior probability distribution to words in thevocabulary based on the previous two words uttered,on 60 million words of text consisting of 1 million wordsfrom the Brown Corpus \[11\], 14 million from Hansard(the record of House of Commons debates), 21 millionfrom the Globe and Mail and 24 million from the Mon-treal Gazette.
1 Reliable estimation of trigram statisticsfor our vocabulary would require a corpus which is sev-eral orders of magnitude larger and drawn from muchmore heterogeneous sources but such a corpus is notavailable today.
Nonetheless we have found that thetrigram model is capable of correcting over 60% of theerrors made by the acoustic component of our recog-nizer; in the case of words for which trigram statisticscan be compiled from the training corpus, 90% of theerrors are corrected.Perhaps the simplest way of increasing recognitionperformance would be to increase the amount of train-ing data for the language model.
Although we are for-tunate to have had access to a very large amount ofdata, we are still a long way from having a representa-tive sample of contemporary written English.
IBM hastrained their language model using 200 million wordsof text.
It seems that at least one billion words drawnfrom diverse sources are needed.We have found that it is possible to compensate tosome extent for the lack of training data by training1 We take this opportunity to acknowledge our debtto the Globe and Mail, to the Gazette, to G. & C.Merriam Co., to InfoGlobe, and to Infomart.
Also,this work was supported by the Natural Sciences andEngineering Research Council of Canada.391parts-of-speech trigrams rather than word trigrams \[10\].One of our graduate students has produced a Master'sthesis which uses Markov modeling and the very de-tailed parts-of-speech tags with which the Brown Cor-pus is annotated to annotate new text automatically.We have also developed a syntactic parser which is ca-pable of identifying over 30% of the recognition errorswhich occur after the trigram model \[22\].The Phonological ComponentIn most cases Merriam Webster's Seventh New Col-legiate Dictionary indicates only one pronunciation foreach word.
The transcriptions do not provide forphenomena such as consonant cluster reduction orepenthetic stops.
Guided by acoustic recognition 2errors, we have devised a comprehensive collection ofcontext-dependent production rules which we use to de-rive a set of possible pronunciations for each word.
Thiswork is described in \[26\].Acoustic-Phonetic ModelingWith the exception of /1/ and / r / ,  we representeach phoneme by a single hidden Markov model.
Theoutstanding advantage of Markov modeling over othermethods of speech recognition is that it provides a sim-ple means of matching an arbitrary phonetic transcrip-tion with an utterance.
However it suffers from severalwell-known drawbacks: HMMs fail to represent the dy-namics of speech adequately since they treat successiveframes as being essentially independent of each other;they cannot be made sensitive to context-dependentphonetic variation without greatly increasing the num-ber of parameters to be estimated; they do not modelphoneme durations in a realistic way.
We have madesubstantial contributions to the literature on each ofthese problems.
Our approach as been to increase thespeech knowledge incorporated in our models withoutincreasing the training requirements unduly.
This hasgenerally paid off in significant improvements in recog-nition performance.We were one of the first groups to advocate the useof dynamic parameters, calculated by taking differencesbetween feature vectors eparated by a fixed time inter-val, and we have patented this idea.
In \[12\] we intro-duced the idea of multiple codebooks, which enables vec-tor quantization HMMs using both static and dynamicparameters to be trained using reasonable amounts ofdata.
This idea has been adopted by several other re-searchers, notably Lee and Hon \[19\] and BBN.
(We noThat is, recognition performed without the benefit ofthe language modellonger use it ourselves ince we found early on that mul-tivariate Gaussian HMMs outperform vector quantiza-tion HMMs on our task and that the problem of under-training is much less severe in the Gaussian case \[6\]).An unfortunate consequence of using both static anddynamic parameters in a HMM is that the resultingmodel is a probability distribution on 'data' which sat-isfy no constraints relating static and dynamic parame-ters.
(The model does not know how the dynamic pa-rameters are calculated from the static parameters.)
Inthe multivariate Gaussian case, it follows that the modelis inconsistent in the sense that the totality of the datait can be presented with in training or recognition is as-signed zero probability.
This inconsistency led us to con-struct a new type of linear predictive HMM \[18\] whichcontains the static parameters HMM, the dynamic pa-rameters ttMM and the Poritz hidden filter model \[23\]as special cases.In recognition tasks with a medium sized vocabulary(on the order of 1,000 words), the method of triphonemodeling \[24\] has been found to be successful in address-ing the problem of context-dependent phonetic varia-tion.
In its present form, this method cannot be scaledup to a recognition task as large as ours.
(The num-ber of triphones in our dictionary is more than 17,000;when triphones panning word boundaries are countedas well, the number is much larger \[15\].)
However wefound that by constructing a collection of twenty fivegeneralized-triphone models for each phoneme we wereable to get a substantial improvement in recognitionperformance over unimodal phonemic HMMs (bench-mark results) \[5\].
The generalized-triphone units weredefined by means of a five way classification of left andright contexts for each phoneme s .
We use the preced-ing phoneme class for a vowel and the following phonemeclass for a consonant to construct one-sided HMMs (alsocalled L/R-allophonic HMMs).
In constructing two-sided allophonic HMMs (LR-allophonic HMMs) for eachphoneme, a combination of the above five contexts inboth left and right gives rise to 25 two-sided allophoniccontexts.
The first conclusion we can draw from TableI is that allophonic HMM's (columns 5-8) consistentlyoutperform unimodal phonemic HMM's (columns 3-4).The difference in recognition accuracy is particularly no-ticeable with a large amount of training data (e.g., over2,500 words).
In this case, averaged over speakers CAand AM, L/R-allophonic HMM's reduce recognition er-rors by 18% when we use the uniform language modelFor vowels, neighboring phonemes were classified as:(1) word boundary, breath noise, or /h / ,  (2) labialconsonants, (3) apical consonants, (4) velar con-sonants, (5) vowels.
For consonants, neighboringphonemes were classified as (1) word boundary orbreath noise, (2) palatal vowels (including / j / ) ,  (3)rounded vowels ( including/w/),  (4) plain vowels, (5)consonants.392Speaker(test size)CA(female)(1090words)AM(male)(698 wds)MA(fem.
)(586 wds)Train Benchmarksize unif.
3-gram717 32.1 19.41532 29.8 15.02347 29.4 13.23098 29.2 13.23880 29.3 13.31100 45.5 22.02039 31.8 19.02742 31.3 18.11600 21.0 9.6Table I.L /R-a l loph.unif.
3-gram30.0 16.525.0 11.524.5 10.924.0 11.023.9 10.946.0 22.027.0 16.025.8 11.916.9 8.4LR-alloph.uniL 3-gram30.3 17.521.9 12.019.2 9.617.3 8.717.0 8.146.6 23.028.0 17.023.9 13.616.2 10.4Mixturesunif.
3-gram30.3 19.519.0 10.013.9 5.814.0 6.014.1 6.044.6 21.026.1 13.023.3 10.313.8 7.5Comparison of recognition error rates (in %) for thecontext-dependent allophonic HMM's (L/R-allophone andLR-allophone models) and the context-independent phonemicHMM's (unimodal (benchmark) and mixture models).
Results forthe uniform (unif.)
and trigram (3-gram)language models aregiven separately.and by 26% when we use the trigram language model.LR-allophonic HMM's reduce the error rate further, by35% and 33%, respectively, for the two language models.One of our most interesting discoveries was that wecould obtain still better performance by training Gaus-sian mixture HMMs for each phoneme with 25 mixturecomponents per state, using the mean vectors of thegeneralized triphone models as an initialization.
Sincethe forward-backward calculations are notoriously coma-putationally expensive for mixture models having largenumbers of components, we had to devise a new vari-ant of the Baum-Welch algorithm in order to train oursystem.
We call it the semi-relaxed training algorithm\[8\].
It uses knowledge of the approximate location ofsegment boundaries to reduce the computation eededfor training by 70% without sacrificing optimality.
(Forcontinuous peech, the computational savings will belarger still.)
As can be seen from Table I (comparecolumns 7-8 to columns 9-10), the mixture HMMs out-perform the LR-allophonic HMMs in almost every in-stance both with the uniform and the trigram languagemodels.The acoustic realization of stop consonants i highlyvariable, making them the most difficult phonemes torecognize.
In general, they may be decomposed intoquasi-stationary subsegments (microsegments) whichcan be classified crudely as silence, voice-bar, stop-burst and aspiration; the microsegments hat actuallyoccur in the realization of a given stop depend largelyon its phonological context.
We performed an experi-ment where we trained HMMs for several different ypesof microsegment (15 in all) and formulated context-dependent rules governing their incidence.
We obtaineda dramatic improvement in the acoustic recognition ratefor CVC words.
When tested on two speakers (see Ta-ble II), the error rate improved from 32.4% to 22.1% inone case and from 31.4% to 19.6% in the other \[4\].Much of the information for recognizing stops (andother consonants) is contained in the formant transi-tions of adjacent vowels.
It is not possible for us to takeadvantage of this fact directly since there are far moreCV and VC pairs in our dictionary than can be coveredin a training set of reasonable size.
However, we haveconstructed a model for these transitional regions whichwe call a state interpolation HMM \[17\] and which canbe trained using data that contains instances of everyvowel and every consonant but not necessarily of everyCV and VC pair.
The state interpolation HMM modelsthe signal in the transitional region by assuming that itcan be fitted to a line segment in the feature parameterspace joining a vowel steady-state vector to a consonantlocus vector (the terminology is motivated by \[3\]); theremainder of the signal is modeled by consonant andvowel HMMs in the usual way.
One steady-state vectoris trained for each vowel and one locus vector for eachconsonant, so the model is quite robust.
When tested393Table II.speakerspkr lspkr2Percent  Er ror  (no language model)one HMMper stop32.4%31.4%stop microsegmentscontext - indep.26.0%24.4%context-  dependent22.1%19.6%Compar ison  of recognit ion error rates for 312 CVC(V)  wordsusing one model  per stop, context - independent  mic rosegmentmodels,  and context -dependent  microsegment  models.
Nolanguage model  is used.on five speakers we found that this model gave improve-ments in acoustic recognition performance in every case;it also gives consistent improvements across a variety offeature parameter sets.We have observed marked differences in the distribu-tion of vowel durations in certain environments and wehave found that this can be used to improve recogni-tion performance by conditioning the transition proba-bilities (but not the output distributions) of the vowelHMMs on these environments \[7\].
We have performedrecognition experiments where we distinguish three en-vironments for each vowel: monosyllabic words with avoiceless coda, monosyllabic words having a voiceless orabsent coda and polysyllabic words.
This gave a 2%increase in acoustic recognition accuracy for both thespeakers tested.Many acoustic misrecognitions in our recognizer aredue to phonemic hidden Markov models mapping toshort segments of speech.
When we force these mod-els to map to larger segments corresponding to the ob-served minimum durations for the phonemes \[14\], thenthe likelihood of the incorrect phoneme sequences dropsdramatically.
This drop in the likelihood of the incor-rect words results in significant reduction in the acous-tic recognition error rate.
Even in cases where acous-tic recognition performance is unchanged, the likelihoodof the correct word choice improves relative to the in-correct word choices, resulting in significant reductionin recognition error rate with the language model.
Onnine speakers, the error rate for acoustic recognition re-duces from 18.6% to 17.3%, while the error rate withthe language model reduces from 9.2% to 7.2%.Overv iew of the Recogn izerSpeech is sampled at 16 kHz and a 15-dimensionalfeature vector is computed every 10 ms using a 25 mswindow.
The feature vector consists of 7 reel-based cep-stral coefficents \[2\] and 8 dynamic parameters calcu-lated by taking cepstral differences over a 40 ms inter-val.
(The zeroth order cepstral coefficent which containsthe loudness information is not included in the staticparameters but it is used in calculating the dynamicparameters.
)The first step in recognizing a word is to find its end-points, which we do using a weighted spectral energymeasure.
In order to avoid searching the entire dic-tionary, we then attempt o 'recognize' the number ofsyllables in the word using a vector quantization HMMtrained for this purpose, generating up to three hypothe-ses for the syllable count.
The correct count is found inthe hypothesis list 99.5% of the time.For each of the hypothetical syllable counts we gen-erate a list of up to 100 candidate phonetic transcrip-tions using crude forward-backward calculations and ourgraph search algorithm \[13\] to search a syllable networkfor transcriptions which are permitted by the lexicon.The exact likelihood of the speech data is then calcu-lated for each of the candidate transcriptions using theacoustic-phonetic model.
We thus obtain the acousticmatch of the data with up to 300 words in the vocab-ulary (the number of words depends on the number ofhypotheses for the syllable count).
This list of candidatewords is found to contain the correct word 96.5% of thetime when phonemic duration constraints are not im-posed on the search.
In this case the search takes abouttwo minutes to perform on a Mars-432 array processor.The percentage increases to 98% when the search is con-strained to respect minimum durations.
We have alsofound that the number of search errors can be reducedby using the language model to generate additional wordhypotheses, but this increases recognition time by a fac-tor of two so we do not use it.At this point we have a lattice of acoustic matchesfor each of the words uttered by the speaker.
The finalstep is to find the MAP word string by using the acous-tic matches to perform an A* search \[21\] through thelanguage model.Word recognition rates on data collected from ninespeakers are presented in Table III.
For each speaker,394acoustic recognitiontotal wordsspeaker(sex)DS (m)AM (m)ML (m)JM (m)FS (m)NM (f)CM (f)MM (f)LM (f)Ave/Tottraining test1900 4512742 5652000 5961664 5872322 10141299 9672343 10902338 5862353 8632107 6719search  errorsno dur  dur4.9% 3.1%3.6% 1.8%1.7% 1.2%3.0% 3.0%1.7% 1.3%4.0% 1.7%3.5% 2.1%2.2% 1.4%3.8% 1.7%3.1% 1.8%er rors  af terlang  mode lno dur dur24.0% 21.9%30.6% 31.0%14.5% 12.6%23.9% 22.7%8.4% 7.5%19.4% 15.0%16.9% 16.5%14.3% 14.3%23.7% 22.6%18.6% 17.3%recog  er rorsno dur  dur14.4%14.2%6.7%8.2%5.0%11.0%8.9%5.0%12.1%9.2%10.4%12.2%5.4%7.8%3.7%6.0%7.8%3.6%9.8%7.2%Table I I I .
Recognition error rates for nine speakers with and withoutduration constraints.the number of word tokens used in training and test-ing are listed in the first two columns.
The test datacomprise 6,719 word tokens in all; recall that there are86,000 words in the vocabulary.Conc lus ionsOur objective was to develop an algorithm for speech-to-text conversion of English sentences spol~en as iso-lated words from a very large vocabulary.
We startedwith a vocabulary of 60,000 words but we found it nec-essary to increase this number to 86,000.
Our initialrecognizer used VQ-based HMMs, but we have sincethen switched to Gaussian mixture HMMs resulting indramatic reduction in acoustic recognition errors.
Im-posing duration constraints on these HMMs has resultedin further eductions in acoustic recognition errors.
Wehave shown that the trigram language model can beused effectively in our 86,000-word vocabulary recog-nizer, reducing the recognition errors by another 60%.The recognition results show that we have acquiredthe capability to recognize words drawn from this muchlarger vocabulary with a degree of accuracy which issufficient o warrant the commercial development ofthis technology once real-time implementation problemshave been solved.
Professor Jack Dennis of MIT hasproposed aparallel architecture for HMM-based contin-uous speech recognition \[9\].
He estimates that decodingtime can be decreased by a factor of at least 100 usinga 'parallel priority queue'.
We have recently begun toexplore this avenue.References1..3.4.5.6.Averbuch, A. et al, "Experiments with theTangora 20,000 word speech recognizer," Proc.1987 IEEE International Conference on Acous-tics, Speech, and Signal Processing, 701-704.Davis, S.B., and Mermelstein, P., "Compari-son of parametric representations for monosyl-labic word recognition in continuously spokensentences," IEEE Transactions on Acoustics,Speech, and Signal Processing, ASSP-28 (4),357-365, 1980.Delattre, P., Liberman, A.M., and Cooper, F.S.,"Acoustic loci and transitional cues for conso-nants," J. Acoust.
Soc.
Am.
27,769-774, 1955.Deng, L., Lennig, M., and Mermelstein, P.,"Modeling microsegments of stop consonants ina hidden Markov model based word recognizer."J.
Acoust.
Soc.
Am.
87 (6), 2738-2747, 1990.Deng, L., Lennig, M., Seitz, F. and Mer-melstein, P., "Large vocabulary word recogni-tion using context-dependent allophonic hiddenMarkov models," Computer Speech and Lan-guage, in press, 1990.Deng, L., Kenny, P., Lennig, M., and Mer-melstein, P., "Modeling acoustic transitions in395speech by state-interpolation hidden Markovmodels," IEEE Trans.
on Acoustics, Speech,and Signal Processing, in press, 1990..8.Deng, L., Lennig, M., and Mermelstein, P.,"Use of vowel duration information in a largevocabulary word recognizer," J. Acoust.
Soc.Am.
86 (2), 540-548, 1989.Deng, L., Kenny, P., Lennig, M., Gupta, V.,Seitz, F., and Mermelstein, P., "Phonemichidden Markov models with continuous mix-ture output densities for large vocabulary wordrecognition," correspondence it m, IEEE Trans.on Acoustics, Speech, and Signal Processing, inpress, 1990.9.
Dennis, J., "Dataflow computation for artificialintelligence," Parallel processing for supercom-puters and artificial intelligence, Edited by KaiHwang and Doug DeGroot, McGraw Hill.10.
Dumouchel, P., Gupta, V., Lennig, M., andMermelstein, P., "Three probabilistic languagemodels for a large-vocabulary speech recog-nizer," Proc.
1988 IEEE International Confer-ence on Acoustics, Speech and Signal Process-ing, 513-516.11.
Francis, W.N., and Kucera, H., "Manual of in-formation to accompany a standard sample ofpresent-day edited American English, for usewith digital computers," Department of Lin-guistics, Brown University, 1979.12.
Gupta, V., Lennig, M., and Mermelstein, P.,"Integration of acoustic information in a largevocabulary word recognizer," Proc.
1987 IEEEInternational Conference on Acoustics, Speechand Signal Processing, 697-700.13.
Gupta, V., Lennig, M., and Mermelstein, P.,"Fast search strategy in a large vocabulary wordrecognizer," J. Acoust.
Soc.
Am.
84(6), 2007-2017, 1988.14.
Gupta, V., Lennig, M., Mermelstein, P., KennyP., Seitz, F., and O'Shaughnessy, D., "The useof minimum durations and energy contours forphonemes to improve large vocabulary isolatedword recognition," submitted to IEEE Trans.on Acoustics, Speech, and Signal Processing,1990.15.
Harrington, J., Watson, G., and Cooper,M., "Word boundary detection in broad classphoneme strings," Computer, Speech and Lan-guage, 3 (4), 367-382, 1989.39616.17.18.19.20.21.22.23.24.25.26.Jelinek, F., "The development of an experimen-tal discrete dictation recognizer," Proc.
IEEE,73 (11), 1616-1624, 1985.Kenny, P., Lennig, M., and Mermelstein,P., "Speaker adaptation in a large-vocabularyHMM recognizer," letter to the editor, IEEETrans.
Pattern Analysis and Machine Intelli-gence, August 1990.Kenny, P., Lennig, M., and Mermelstein, P., "Alinear predictive HMM for vector-valued obser-vations with applications to speech recognition"IEEE Trans.
on Acoustics, Speech, and SignalProcessing, 38 (3), 220-225, 1990.Lee, K.-F., Hon, H.-W., "Large-vocabularyspeaker-independent continuous peech recog-nition using HMM," Proc.
1988 IEEE Interna-tional Conference on Acoustics, Speech and Sig-nal Processing, 123-126.Merialdo, B., "Speech Recognition using verylarge size dictionary," Proc.
1987 IEEE Inter-national Conference on Acoustics, Speech andSignal Processing, 364-367.Nilsson, N., Principles of arlificial intelligence,Tioga Publishing Company, 1982.O'Shaughnessy, D., "Using syntactic informa-tion to improve large-vocabulary word recogni-tion," Proc.
1989 IEEE International Confer-ence on Acoustics, Speech and Signal Process-ing, 44S13.6.Poritz, A., "Hidden Markov models: a guidedtour," Proc.
1988 IEEE International Confer-ence on Acoustics, Speech and Signal Process-ing, 7-13.Schwartz, R.M., Chow Y.L., Roucos S., Kras-net M., and Makhoul 3., "Improved hiddenMarkov modeling of phonemes for continuousspeech recognition," Proc.
1984 IEEE Interna-tional Conference on Acoustics, Speech, andSignal Processing, 35.6.1-35.6.4.Seitz, F., Gupta, V., Lennig, M., Kenny, P.,Deng, L., and Mermelstein, P., "A dictionaryfor a very large vocabulary word recognitionsystem," Computer, Speech and Language, 4,193-202, 1990.Seitz, F., Gupta, V., Lennig, M., Deng, L.,Kenny, P.,and Mermelstein, P., "Phonologicalrules and representations in a phoneme-basedvery large vocabulary word recognition sys-tem," J. Acoust.
Soc.
Am.
87 (S1), S108, 1990.
