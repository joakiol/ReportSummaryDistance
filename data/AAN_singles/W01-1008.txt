Document Fusion for Comprehensive Event DescriptionChristof MonzInstitute for Logic, Language and ComputationUniversity of Amsterdam1018 TV Amsterdam, The Netherlandschristof@science.uva.nlwww.science.uva.nl/?christofAbstractThis paper describes a fully imple-mented system for fusing related newsstories into a single comprehensive de-scription of an event.
The basic compo-nents and the underlying algorithm areexplained.
The system uses a compu-tationally feasible and robust notion ofentailment for comparing informationstemming from different documents.We discuss the issue of evaluating doc-ument fusion and provide some prelim-inary results.1 IntroductionConventional text retrieval systems respond to auser?s query by providing a (ranked) list of doc-uments which potentially satisfy the informationneed.
After having identified a number of docu-ments which are actually relevant, the user readssome of those documents to get the informationrequested.
To be sure to get a comprehensive ac-count of a particular topic, the list of documentsone has to read may be rather long, including a se-vere amount of redundancy; i.e., documents par-tially conveying the same information.Although this problems basically holds for anytext retrieval situation, where comprehensivenessis relevant, it becomes particularly evident in theretrieval of news texts.News agencies, such as AP, BBC, CNN, orReuters, often describe the same event differ-ently.
For instance, they provide different back-ground information, helping the reader to situatethe story, they interview different people to com-ment on an event, and they provide additional,conflicting or more accurate information, depend-ing on their sources.To get a description of an event which is ascomprehensive as possible and also as short aspossible, a user has to compile his or her owndescription by taking parts of the original newsstories, ignoring duplicate information.
Typicalusers include journalists and intelligence analysts,for whom compiling and fusing information is anintegral part of their work (Carbonell et al, 2000).Obviously, if done manually, this process can berather laborious as it involves numerous compar-isons, depending on the number and length of thedocuments.The aim of this paper is to describe an approachautomatizing this process by fusing informationstemming from different documents to generatea single comprehensive document, containing theinformation of all original documents without re-peating information which is conveyed by two ormore documents.The work described in this paper is closely re-lated to the area of multi-document summariza-tion (Barzilay et al, 1999; Mani and Bloedorn,1999; McKeown and Radev, 1995; Radev, 2000),where related documents are analyzed to use fre-quently occurring segments for identifying rele-vant information that has to be included in thesummary.
Our work differs from the work onmulti-document summarization as we focus ondocument fusion disregarding summarization.
Onthe contrary, we are not aiming for the shortestdescription containing the most relevant informa-tion, but for the shortest description containingall information.
For instance, even historic back-ground information is included, as long as it al-lows the reader to get a more comprehensive de-scription of an event.Although the techniques that are used formulti-document fusion and multi-document sum-marization are similar, the task of fusion is com-plementary to the summarization task.
They dif-fer in the way that, roughly speaking, multi-document summarization is the intersection ofinformation within a topic, whereas multi-document fusion is the union of information.They are similar to the extent that in both casesnearly equivalent information stemming from dif-ferent documents within the topic has to be iden-tified as such.The remainder of this paper is structured as fol-lows: Section 2 introduces the main componentsand challenges of implementing a document fu-sion system.
Issues of evaluating document fu-sion and some preliminary evaluation of our sys-tem are presented in Section 3.
In Section 4,some conclusions and prospects on future workare given.2 Fusing DocumentsBefore developing a document fusion system,some basic issues have to be considered.1.
On which level of granularity are the docu-ments fused (i.e., word or phrase level, sen-tence level, or paragraph level?2.
How to decide whether news fragments fromdifferent sources convey the same informa-tion?3.
How to ensure readability of the fused docu-ment?
I.e., where should information stem-ming from different documents be placed inthe fused document, retaining a natural flowof information.Each of the these issues is addressed in the fol-lowing subsections.2.1 SegmentationIn the current implementation, we decided tofuse documents on paragraph level for two rea-sons: First, paragraphs are less context-dependentthan sentences and are therefore easier to com-pare.
Second, compiling paragraphs yields a bet-ter readability of the fused document.
It shouldbe noted that paragraphs are rather short in newsstories, rarely being longer than three sentences.When putting together (fusing) pieces of textfrom different sources in a way that was not an-ticipated by the writers of the news stories, it canintroduce information gaps.
For instance, if aparagraph containing a pronoun is taken out of itsoriginal context and placed in a new context (thefused document), this can lead to dangling pro-nouns, which cannot be correctly resolved any-more.
In general, this problem does not only holdfor pronouns but for all kind of anaphoric ex-pressions such as pronouns, definite noun phrases(e.g., the negotiations) and anaphoric adverbials(e.g., later).
To cope with this problem simplesegmentation is applied as a pre-processing stepwhere paragraphs that contain pronouns or simpledefinite noun phrases are attached to the preced-ing paragraph.
A more sophisticated approach totext segmentation is described in (Hearst, 1997).Obviously, it would be better to use an au-tomatic anaphora resolution component to copewith this problem, see, e.g., (Kennedy and Bogu-raev, 1996; Kameyama, 1997), where anaphoricexpressions are replaced by their antecedents, butat the moment, the integration of such a compo-nent remains future work.2.2 Informativity(Radev, 2000) describes 24 cross-document rela-tions that can hold between their segments, one ofwhich is the subsumption (or entailment) relation.In the context of document fusion, we focus onthe entailment relation and how it can be formallydefined; unfortunately, (Radev, 2000) provides noformal definition for any of the relations.Computing the informativity of a segmentcompared to another segment is an essential taskduring document fusion.
Here, we say that the i-th segment of document d (si,d) is more informa-tive than the j-th segment of document d?
(sj,d?)
ifsi,d entails sj,d?
.
In theory, this should be provenlogically, but in practice this is far beyond the cur-rent state of the art in natural language processing.Additionally, a binary logical decision might alsobe too strict for simulating the human understand-ing of entailment.A simple but nevertheless quite effective solu-tion is based on one of the simpler similarity mea-sures in information retrieval (IR), where texts aresimply represented as bags of (weighted) words.The definition of the entailment score (es) is givenin (1).
es(si,d, sj,d?)
compares the sum of theweights of terms that appear in both segments tothe total sum weights of sj,d?
.es(si,d, sj,d?)
=?tk?si,d?sj,d?idf k?tk?sj,d?idf k(1)The weight of a term ti is its inverse documentfrequency (idf i), as defined in (2), where N is thenumber of all segments in the set of related doc-uments (the topic) and ni is the number of seg-ments in which the term ti occurs.idf i = log(Nni)(2)Terms which occur in many segments (i.e., forwhich ni is rather large), such as the, some, etc.,receive a lower idf -score than terms that occuronly in a few segments.
The underlying intuitionof the idf -score is that terms with a higher idf -score are better suited for discriminating the con-tent of a particular segment from the other seg-ments in the topic, or to put it differently, they aremore content-bearing.
Note, that the logarithm in(2) is only used for dampening the differences.The entailment score es(si,d, sj,d?)
measureshow many of the words of the segment si,d oc-cur in sj,d?
, and how important those words are.This is obviously a very shallow approach to en-tailment computation, but nevertheless it provedto be effective, see (Monz and de Rijke, 2001).2.3 ImplementationIn this subsection, we present the general algo-rithm underlying the implementation, given a setof documents belonging to topic T .
The imple-mentation has to tackle two basic tasks.
First,identify segments that are entailed by other seg-ments and use the more informative one.
Second,place the remaining segments at positions withsimilar content.
The fusion algorithm depicted inFigure 1 consists of five steps.1.
is basically a pre-processing step as ex-plained above.
2. computes pairwise the cross-document entailment scores for all segments in T .Although the pairwise computation of es and simis exponential in the number of documents in T ,it still remains computationally tractable in prac-tice.
For instance, for a topic containing 4 docu-ments (the average case) it takes 10 CPU secondsto compute all entailment and similarity relations.For a topic containing 8 documents (an artificiallyconstructed extreme case) it takes 66 CPU sec-onds; both on a 600 MHz Pentium III PC.In 3., one of the documents is taken as base forthe fusion process.
Starting with a ?real?
docu-ment improves the readability of the final fuseddocuments as it imposes some structure on thefusion process.
There are several ways to selectthe base document.
For instance, take the docu-ment with the most unique terms, or the documentwith the highest document weight (sum of all idf -scores).
In the current implementation we simplytook the longest document within the topic, whichensures a good base coverage of an event.4.
and 5. are the actual fusion steps.
Step 4. re-places a segment si,dF in the fused document bya segment sj,d?
from another document if sj,d?
isthe segment maximally entailing si,dF and if it issignificantly (above the threshold ?es ) more infor-mative than si,dF .
Choosing an optimal value for?es is essential for the effectiveness of the fusionsystem.
Section 3 discusses some of our experi-ments to determine ?es .Step 5. is kind of complementary to step 4.,where related but more informative segments areidentified.
Step 5. identifies segments that addnew information to dF , where a segment sj,d?
isnew if it has low similarity to all segments in dF ,i.e., if the the similarity score is below the thresh-old ?sim .
If a segment sj,d?
is new, it is placedright after the segment in dF to which it is mostsimilar.Similarity is implemented as the traditional co-sine similarity in information retrieval, as definedin (3).
This similarity measure is also knownas the tfc.tfc measure, see (Salton and Buckley,1988).sim(si,d, sj,d?)
=?tk?si,d?sj,d?wk,si,d ?
wk,sj,d??
?tk?si,dw2si,d ??tk?sj,d?w2sj,d?
(3)Where wk,si,d is the weight associated with theterm tk in segment si,d.
In the nominator of (3),1. segmentize all documents in T2.
for all si,d, sj,d?
s.t.
d, d?
?
T and d 6= d?
: compute es(si,d, sj,d?)3.
select a document d ?
T as fusion base document: dF4.
for all si,dF : find sj,d?
s.t.
dF 6= d?
and sj,d?
= arg maxsk,d?
: es(sk,d?
, si,dF ) > es(si,dF , sk,d?
)if es(sj,d?
, si,dF ) > ?es then replace si,dF by sj,d?
in the fused document5.
for all sj,d?
s.t.
sj,d?
6?
dF :if for all si,dF : sim(si,dF , sj,d?)
< ?sim ,then find the most similar si,dF : si,dF = arg maxsk,dF: sim(sj,d?
, sk,dF )and place sj,d?
between si,dF and si+1,dFFigure 1: Sketch of the document fusion algorithm.the weights of the terms that occur in si,d and sj,d?are summed up.
The denominator is used for nor-malization.
Otherwise, longer documents tend toresult in a higher similarity score.
In the currentimplementation wk,si,d = idf k for all si,d.
Thereader is referred to (Salton and Buckley, 1988;Zobel and Moffat, 1998) for a broad spectrum ofsimilarity measures for information retrieval.3 Evaluation IssuesThe document fusion system is evaluated in twosteps.
First, the effectiveness of entailment detec-tion is evaluated, which is the key component ofour system.
Then we present some preliminaryevaluation of the whole system focusing on thequality of the fused documents.3.1 Evaluating EntailmentRecently, we have started to build a small test col-lection for evaluating entailment relations.
Thereader is referred to (Monz and de Rijke, 2001)for more details on the results presented in thissubsection.For each of the 21 topics in our test corpus twodocuments in the topic were randomly selected,and given to a human assessor to determine allsubsumption relations between segments in dif-ferent documents (within the same topic).
Judg-ments were made on a scale 0?2, according to theextent to which one segment was found to entailanother.Out of the 12083 possible subsumption rela-tions between the text segments, 501 (4.15%) re-ceived a score of 1, and 89 (0.73%) received ascore of 2.Let a subsumption pair be an ordered pair ofsegments (si,d, sj,d?)
that may or may not standin the subsumption relation, and let a correct sub-sumption pair be a subsumption pair (si,d, sj,d?
)for which si,d does indeed entail sj,d?
.
Further, acomputed subsumption pair is a subsumption pairfor which our subsumption method has produceda score above the subsumption threshold.Then, precision is the fraction of computedsubsumption pairs that is correct:Precision = number of correct subsumption pairs computedtotal number of subsumption pairs computed .And recall is the proportion of the total number ofcorrect subsumption pairs that were computed:Recall = number of correct subsumption pairs computedtotal number of correct subsumption pairs .Observe that precision and recall depend on thesubsumption threshold that we use.We computed average recall and precision at 11different subsumption thresholds, ranging from 0to 1, with .1 increments; the average was com-puted over all topics.
The results are summarizedin Figures 2 (a) and (b).Since precision and recall suggest two differ-ent optimal subsumption thresholds, we use the F-Score, or harmonic mean, which has a high valueonly when both recall and precision are high.F =21Recall +1Precision0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Subsumption ThresholdPrecisionHuman Judgm.
> 0Human Judgm.
> 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Subsumption ThresholdRecallHuman Judgm.
> 0Human Judgm.
> 1(a) (b)Figure 2: (a) Average precision with human judgments > 0 and > 1.
(b) Average recall with humanjudgments > 0 and > 1.The average F -scores are given in Figure 3.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Subsumption ThresholdF?ScoreHuman Judgm.
> 0Human Judgm.
> 1Figure 3: Average F -scores with human judg-ments > 0 and > 1.The optimal subsumption threshold for humanjudgments > 0 is around 0.18, while it is approx-imately 0.4 for human judgments > 1.
This con-firms the intuition that a higher threshold is moreeffective when human judgments are stricter.3.2 Evaluating FusionIn the introduction, it was pointed out that docu-ment fusion by hand can be rather laborious, andthe same holds for the evaluation of automaticdocument fusion.
Similar to automatic summa-rization, there are no standard document collec-tions or clear evaluation criteria aiding to autom-atize the process of evaluation.
One approachcould be to focus on news stories which mentiontheir sources.
For instance CNN?s new stories of-ten say that ?AP and Reuters contributed to thisstory?.
On the other hand one has to be cautiousto take those news stories as gold standard as therespective contributions of the journalist and hisor her sources are not made explicit.In the area of multi-document summarization,there is a distinction between intrinsic and extrin-sic evaluation, see (Mani et al, 1998).
Intrin-sic evaluation judges the quality directly based onanalysis of the summary.
Usually, a human judgeassesses the quality of a summary based on somestandardized evaluation criteria.In extrinsic evaluation, the usefulness of a sum-mary is judged based on how it affects the com-pletion of some other task.
A typical task usedfor extrinsic evaluation is ad-hoc retrieval, wherethe relevance of a retrieved document is assessedby a human judge based on the document?s sum-mary.
Then, those judgments are compared tojudgments based on original documents, see, e.g.,(Brandow et al, 1995; Mani and Bloedorn, 1999).At this stage we have just carried out some pre-liminary evaluation.
The test collection consistsof 69 news stories categorized into 21 topics.
Cat-egorization was done by hand, but it is also pos-sible to have information filtering, see (Robertsonand Hull, 2001), or topic detection and tracking(TDT) tools carrying out this task (Allan et al,1998).
All documents belonging to the same topicwere released on the same day and describe thesame event.
Table 1 provides further details onthe collection.avg.
per topicno.
of docs.
3.3 docs.length of a doc.
612 wordslength of all docs.
together 2115 wordslength of longest doc.
783 wordslength of shortest doc.
444 wordsTable 1: Test collection (21 topics, 69 docu-ments).In addition to the aforementioned news agen-cies, the collection includes texts from the L.A.Times, Washington Post and Washington Times.In general, a segment should be included in thefused document if it did not occur before to avoidredundancy (False Alarm), and if it adds informa-tion, so no information is left out (Miss).
As in IRor TDT, Miss and False Alarm tend to be inverselyrelated; i.e., a decrease of Miss often results in anincrease of False Alarm and vice versa.Table 2 illustrates the different possibilitieshow the system responds as to whether a seg-ment should be included in the fused documentand how a human reader judges.system reader: reader:judgement include excludeinclude a bexclude c dTable 2: Contingency table.Then, Miss and False Alarm can be defined asin (4) and (5), respectively.Miss =ca+ cif a+ c > 0 (4)False Alarm =bb+ dif b+ d > 0 (5)The fusion impact factor (fif) describes to whatextent the different sources actually contributed tothe fused document.
For instance if the fused doc-ument solely contains segments from one source,fif equals 0, and if all sources equally contributedit equals 1.
This can be formalized as follows:fif = 1?
?d?T| 1/nT ?
nseg,d/nseg |2 ?
(1?
1/nT )(6)Where S is a set of related documents, and nTis its size.
nseg is the number of segments in thefused document and nseg,d is the number of seg-ments stemming from document d.For our test collection, the average fusion im-pact factor was 0.56.
Of course the fif -score de-pends on the choice of ?es and ?sim , in a way thata lower value of ?es or a higher value of ?sim in-creases the fif -score.
In this case, ?es = 0.2 and?sim = 0.05.Table 3 shows the length of the fused docu-ments in average compared to the longest, short-est, and all documents in a topic, for ?es = 0.2and ?sim = 0.05.avg.
compressionratio per topicall docs.
together 0.55longest doc.
1.36shortest doc.
2.55Table 3: Compression ratios.Measuring Miss intrinsically is extremely labo-rious; especially comparing the effectiveness ofdifferent values for the thresholds ?es and ?sim isinfeasible in practice.
Therefore, we decided tomeasure Miss extrinsically.
We used ad-hoc re-trieval as the extrinsic evaluation task.
The eval-uation criterion is stated as follows: Using thefused document of each topic as a query, what isthe average (non-interpolated) precision?As baseline, we concatenated all documentsof each topic.
This would constitute an eventdescription that does not miss any informationwithin the topic.
This document is then used toquery a collection of 242,996 documents, con-taining the 69 documents from our test collection.Since the baseline is simply the concatenation ofall documents within the topic, one can expectthat all documents from that topic receive a highrank in the set of retrieved documents.
This aver-age precision forms the optimal performance forthat topic.
For instance, if a topic contains threedocuments, and the ad-hoc retrieval ranks thosedocuments as 1, 3, and 6, there are three recalllevels: 33.3?%, 66.6?%, and 100%.
The precisionat these levels is 1/1, 2/3, and 3/6 respectively,which averages to 0.72?.The next step is to compare the actually fuseddocuments to the baseline.
It is to be expected thatthe performance is worse, because the fused docu-ments do not contain segments which are entailedby other segments in the topic.
For instance, ifthe fused document for the aforementioned topicis used as a query and the original documents ofthe topic are ranked as 2, 4, and 9, the averageprecision is (1/2 + 2/4 + 3/9)/3 = 0.4?.Compared to 0.72?
for the baseline, fusion leadsto a decrease of effectiveness of approximately38.5%.
Figure 4, gives the averaged precision forthe different values for ?es .0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.60.70.80.91Entailment ThresholdAvgeragePrecisionSystemBaselineFigure 4: Average precision for ad-hoc retrieval.It is not obvious how to interpret the numericalvalue of the ad-hoc retrieval precision in terms ofMiss, but the degree of deviation from the base-line gives a rough estimate of the completenessof a fused document.
At least, this allows for anordinally scaled ranking of the different methods(in our case different values for ?es ), that are usedfor generating the fused documents.
Figure 4 il-lustrates that in the context of the ad-hoc retrievalevaluation an optimal entailment threshold (?es )lies around 0.2.
Table 4 shows the decrease in re-trieval effectiveness in percent, compared to thebaseline.
The average precision at 0.2 is 0.8614,which is just ?
11.5% below the baseline.For all ad-hoc retrieval experiments, the Lnu.ltuweighting scheme, see (Singhal et al, 1996), hasbeen used, which is one of the best-performingweighting schemes in ad-hoc retrieval.
In addi-tion to the 69 documents from our collection, theretrieval collection contains articles from Associ-Decrease in Decrease in?es precision ?es precision0.0 20.9% 0.6 23.8%0.1 14.6% 0.7 23.8%0.2 11.5% 0.8 23.8%0.3 13.7% 0.9 23.8%0.4 22.0% 1.0 24.4%0.5 22.2%Table 4: Differences to baseline retrieval.ated Press 1988?1990 (from the TREC distribu-tion), which also belong to the newswire or news-paper domain.
Any meta information such as thename of the journalist or news agency is removedto avoid matches based on that information.In the context of multi-document summariza-tion, (Stein et al, 2000) use topic clustering forextrinsic evaluation.
Although we did not carryout any evaluation based on topic clustering, itseems that it could also be applied to multi-document fusion, given the close relationship be-tween fusion and summarization on the one handand retrieval and clustering on the other hand.4 ConclusionsThe document fusion system described is just pro-totype and there is much more space for improve-ment.
Although detecting redundancies by usinga shallow notion of entailment works reasonablywell, it is still far from perfect.In the current implementation, text analysis isvery shallow.
Pattern matching is used to avoiddangling anaphora and lemmatization is used tomake the entailment and similarity scores unsus-ceptible to morphological variations such as num-ber and tense.
A question for future research is towhat extent shallow parsing techniques can im-prove the entailment scores.
In particular, doesconsidering the relational structure of a sentenceimprove computing entailment relations?
Thishas shown to be successful in inference-based ap-proaches to question-answering, see (Harabagiuet al, 2000), and document fusion might also ben-efit from representations that are a bit deeper thanthe one discussed in this paper.Another open issue at this point is the need forstandards for evaluating the quality of documentfusion.
We think that this can be done by usingstandard IR measures like Miss and False Alarm.Although Miss can be approximated extrinsically,it is unclear whether this also possible for FalseAlarm.
Obviously, intrinsic evaluation is more re-liable, but it remains an extremely laborious pro-cess, where inter-judge disagreement is still an is-sue, see (Radev et al, 2000).AcknowledgmentsThe author would like to thank Maarten de Rijkefor providing the entailment judgments.
Thiswork was supported by the Physical Sci-ences Council with financial support from theNetherlands Organization for Scientific Research(NWO), project 612-13-001.ReferencesJ.
Allan, J. Carbonell, G. Doddington, J. Yamron, andY.
Yang.
1998.
Topic detection and tracking pilotstudy final report.
In Proceedings of the BroadcastNews Transcription and Understranding Workshop(Sponsored by DARPA).R.
Barzilay, K. McKeown, and M. Elhadad.
1999.
In-formation fusion in the context of multi-documentsummarization.
In Proceedings of the 37th AnnualMeeting of the Association of Computational Lin-guistics (ACL?99).R.
Brandow, K. Mitze, and L. Rau.
1995.
Automaticcondensation of electronic publications by sentenceselection.
Information Processing & Management,31(5):675?685.J.
Carbonell, D. Harman, E. Hovy, S. Maiorano,J.
Prange, and Sparck-Jones.
K. 2000.
Visionstatement to guide research in question answering(Q&A) and text summarization.
NIST Draft Publi-cation.S.
Harabagiu, M. Pasca, and S. Maiorano.
2000.
Ex-periments with open-domain textual question an-swering.
In Proceedings of COLING-2000.M.
Hearst.
1997.
TextTiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.M.
Kameyama.
1997.
Recognizing referentiallinks: An information extraction perspective.
InR.
Mitkov and B. Boguraev, editors, Proceedingsof ACL/EACL-97 Workshop on Operational Factorsin Practical, Robust Anaphora Resolution for Unre-stricted Texts, pages 46?53.C.
Kennedy and B. Boguraev.
1996.
Anaphora foreveryone: Pronominal anaphora resolution withouta parser.
In Proceedings of the 16th InternationalConference on Computational Linguistics (COL-ING?96).
Association for Computational Linguis-tics.I.
Mani and E. Bloedorn.
1999.
Summarizing sim-ilarities and differences among related documents.Information Retrieval, 1(1?2):35?67.I.
Mani, D. House, G. Klein, L. Hirschman, L. Obrst,T.
Firmin, M. Chrzanowski, and B. Sundheim.1998.
The Tipster SUMMAC text summariza-tion evaluation, final report.
Technical Report98W0000138, Mitre.K.
McKeown and D. Radev.
1995.
Generating sum-maries of multiple news articles.
In Proceedings ofthe 18th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 74?82.C.
Monz and M. de Rijke.
2001.
Light-weight sub-sumption checking for computational semantics.
InP.
Blackburn and M. Kohlhase, editors, Proceedingsof the 3rd Workshop on Inference in ComputationalSemantics (ICoS-3).D.
Radev, H. Jing, and M. Budzikowska.
2000.Centroid-based summarization of multiple docu-ments: Clustering, sentence extraction, and evalu-ation.
In Proceedings of the ANLP/NAACL-2000Workshop on Summarization.D.
Radev.
2000.
A common theory of informationfusion from multiple text sources, step one: Cross-document structure.
In In Proceedings of the 1stACL SIGDIAL Workshop on Discourse and Dia-logue.S.
Robertson and D. Hull.
2001.
The TREC-9 fil-tering track final report.
In Proceedings of The 9thText Retrieval Conference (TREC-9).
NIST SpecialPublication.G.
Salton and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
InformationProcessing & Management, 24(5):513?523.A.
Singhal, G. Salton, M. Mitra, and C. Buckley.1996.
Document length normalization.
Informa-tion Processing & Management, 32(5):619?633.G.
Stein, G. Wise, T. Strzalkowski, and A. Bagga.2000.
Evaluating summaries for multiple docu-ments in an interactive environment.
In Proceed-ings of the Second International Conference onLanguage Resources and Evaluation (LREC?00),pages 1651?1657.J.
Zobel and A. Moffat.
1998.
Exploring the similarityspace.
ACM SIGIR Forum, 32(1):18?34.
