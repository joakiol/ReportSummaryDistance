Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 430?440,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Bilingual Sentiment Word Embeddings for Cross-languageSentiment ClassificationHuiwei Zhou, Long Chen, Fulin Shi, and Degen HuangSchool of Computer Science and TechnologyDalian University of Technology, Dalian, P.R.
China{zhouhuiwei,huangdg}@dlut.edu.cn{chenlong.415,shi fl}@mail.dlut.edu.cnAbstractThe sentiment classification performancerelies on high-quality sentiment resources.However, these resources are imbalancedin different languages.
Cross-languagesentiment classification (CLSC) can lever-age the rich resources in one language(source language) for sentiment classifica-tion in a resource-scarce language (targetlanguage).
Bilingual embeddings couldeliminate the semantic gap between twolanguages for CLSC, but ignore the senti-ment information of text.
This paper pro-poses an approach to learning bilingualsentiment word embeddings (BSWE) forEnglish-Chinese CLSC.
The proposed B-SWE incorporate sentiment information oftext into bilingual embeddings.
Further-more, we can learn high-quality BSWEby simply employing labeled corpora andtheir translations, without relying on large-scale parallel corpora.
Experiments onNLP&CC 2013 CLSC dataset show thatour approach outperforms the state-of-the-art systems.1 IntroductionSentiment classification is a task of predicting sen-timent polarity of text, which has attracted consid-erable interest in the NLP field.
To date, a num-ber of corpus-based approaches (Pang et al, 2002;Pang and Lee, 2004; Kennedy and Inkpen, 2006)have been developed for sentiment classification.The approaches heavily rely on quality and quan-tity of the labeled corpora, which are consideredas the most valuable resources in sentiment classi-fication task.
However, such sentiment resourcesare imbalanced in different languages.
To leverageresources in the source language to improve thesentiment classification performance in the targetlanguage, cross-language sentiment classification(CLSC) approaches have been investigated.The traditional CLSC approaches employ ma-chine translation (MT) systems to translate corpo-ra in the source language into the target language,and train the sentiment classifiers in the target lan-guage (Banea et al, 2008).
Directly employingthe translated resources for sentiment classifica-tion in the target language is simple and couldget acceptable results.
However, the gap betweenthe source language and target language inevitablyimpacts the performance of sentiment classifica-tion.
To improve the classification accuracy, multi-view approaches have been proposed.
In these ap-proaches, the resources in the source language andtheir translations in the target language are bothused to train sentiment classifiers in two indepen-dent views (Wan, 2009; Gui et al, 2013; Zhou etal., 2014a).
The final results are determined by en-semble classifiers in these two views to overcomethe weakness of monolingual classifiers.
However,learning language-specific classifiers in each viewfails to capture the common sentiment informationof two languages during training process.With the revival of interest in deep learning(Hinton and Salakhutdinov, 2006), shared deeprepresentations (or embeddings) (Bengio et al,2013) are employed for CLSC (Chandar A P etal., 2013).
Usually, paired sentences from par-allel corpora are used to learn word embeddingsacross languages (Chandar A P et al, 2013; Chan-dar A P et al, 2014), eliminating the need of MTsystems.
The learned bilingual embeddings couldeasily project the training data and test data into acommon space, where training and testing are per-formed.
However, high-quality bilingual embed-dings rely on the large-scale task-related parallelcorpora, which are not always readily available.Meanwhile, though semantic similarities acrosslanguages are captured during bilingual embed-ding learning process, sentiment information of430text is ignored.
That is, bilingual embeddingslearned from unlabeled parallel corpora are noteffective enough for CLSC because of a lack ofexplicit sentiment information.
Tang and Wan(2014) first proposed a bilingual sentiment embed-ding model using the original training data and thecorresponding translations through a linear map-ping rather than deep learning technique.This paper proposes a denoising autoencoderbased approach to learning bilingual sentimen-t word embeddings (BSWE) for CLSC, whichincorporates sentiment polarities of text into thebilingual embeddings.
The proposed approachlearns BSWE with the original labeled documentsand their translations instead of parallel corpo-ra.
The BSWE learning process consists of twophases: the unsupervised phase of semantic learn-ing and the supervised phase of sentiment learn-ing.
In the unsupervised phase, sentiment wordsand their negation features are extracted from thesource training data and their translations to rep-resent paired documents.
These features are usedas inputs for a denoising autoencoder to learn thebilingual embeddings.
In the supervised phase,sentiment polarity labels of documents are used toguide BSWE learning for incorporating sentimentinformation into the bilingual embeddings.The learned BSWE are applied to project En-glish training data and Chinese test data into acommon space.
In this space, a linear support vec-tor machine (SVM) is used to perform training andtesting.
The experiments are carried on NLP&CC2013 CLSC dataset, including book, DVD andmusic categories.
Experimental results show thatour approach achieves 80.68% average accuracy,which outperforms the state-of-the-art systems onthis dataset.
Although the BSWE are only evaluat-ed on English-Chinese CLSC here, it can be pop-ularized to many other languages.The major contributions of this work can besummarized as follows:?
We propose bilingual sentiment word em-beddings (BSWE) for CLSC based on deeplearning technique.
Experimental resultsshow that the proposed BSWE significantlyoutperform the bilingual embeddings by in-corporating sentiment information.?
Instead of large-scale parallel corpora, on-ly the labeled English corpora and English-to-Chinese translations are required for B-SWE learning.
It is proved that in spite ofthe small-scale of training set, our approachoutperforms the state-of-the-art systems inNLP&CC 2013 CLSC share task.?
We employ sentiment words and their nega-tion features rather than all words in doc-uments to learn sentiment-specific embed-dings, which significantly reduces the dimen-sion of input vectors as well as improves sen-timent classification performance.2 Related WorkIn this section, we review the literature related tothis paper from two perspectives: cross-languagesentiment classification and embedding learningfor sentiment classification.2.1 Cross-language Sentiment Classification(CLSC)The critical problem of CLSC is how to bridge thegap between the source language and target lan-guage.
Machine translations or parallel corporaare usually employed to solve this problem.
Wepresent a brief review of CLSC from two aspects:machine translation based approaches and parallelcorpora based approaches.Machine translation based approaches use MTsystems to project training data into the target lan-guage or test data into the source language.
Wan(2009) proposed a co-training approach for CLSC.The approach first translated Chinese test data in-to English, and English training data into Chinese.Then, they performed training and testing in t-wo independent views: English view and Chineseview.
Gui et al (2013) combined self-trainingapproach with co-training approach by estimatingthe confidence of each monolingual system.
Liet al (2013) selected the samples in the sourcelanguage that were similar to those in the targetlanguage to decrease the gap between two lan-guages.
Zhou et al (2014a) proposed a combi-nation CLSC model, which adopted denoising au-toencoders (Vincent et al, 2008) to enhance therobustness to translation errors of the input.Most recently, a number of studies adopt deeplearning technique to learn bilingual representa-tions with parallel corpora.
Bilingual represen-tations have been successfully applied in manyNLP tasks, such as machine translation (Zouet al, 2013), sentiment classification (Chan-dar A P et al, 2013; Zhou et al, 2014b), tex-t classification (Chandar A P et al, 2014), etc.431Chandar A P et al (2013) learned bilingualrepresentations with aligned sentences through-out two phases: the language-specific represen-tation learning phase and the shared representa-tion learning phase.
In the language-specific rep-resentation learning phase, they applied autoen-coders to obtain a language-specific representa-tion for each entity in two languages respective-ly.
In shared representation learning phase, pairsof parallel language-specific representations werepassed to an autoencoder to learn bilingual repre-sentations.
To joint language-specific representa-tions and bilingual representations, Chandar A Pet al (2014) integrated the two learning phases in-to a unified process to learn bilingual embeddings.Zhou et al (2014b) employed bilingual represen-tations for English-Chinese CLSC.
The work men-tioned above employed aligned sentences in bilin-gual embedding learning process.
However, in thesentiment classification process, only representa-tions in the source language are used for training,and representations in the target language are usedfor testing, which ignores the interactions of se-mantic information between the source languageand target language.2.2 Embedding Learning for SentimentClassificationBilingual embedding learning algorithms focuson capturing syntactic and semantic similaritiesacross languages, but ignore sentiment informa-tion.
To date, many embedding learning algo-rithms have been developed for sentiment classi-fication problem by incorporating sentiment in-formation into word embeddings.
Maas et al(2011) presented a probabilistic model that com-bined unsupervised and supervised techniques tolearn word vectors, capturing semantic informa-tion as well as sentiment information.
Wang etal.
(2014) introduced sentiment labels into NeuralNetwork Language Models (Bengio et al, 2003)to enhance sentiment expression ability of wordvectors.
Tang et al (2014) theoretically and em-pirically analyzed the effects of the syntactic con-text and sentiment information in word vectors,and showed that the syntactic context and senti-ment information were equally important to senti-ment classification.Recent years have seen a surge of interest inword embeddings with deep learning technique(Bespalov et al, 2011; Glorot et al, 2011; Socheret al, 2011; Socher et al, 2012), which have beenempirically shown to preserve linguistic regulari-ties (Mikolov et al, 2013).
Our work focuses onlearning bilingual sentiment word embeddings (B-SWE) with deep learning technique.
Unlike thework of Chandar A P et al (2014) that adopt-ed parallel corpora to learn bilingual embeddings,we only use training data and their translations tolearn BSWE.
More importantly, sentiment infor-mation is integrated into bilingual embeddings toimprove their performance in CLSC.3 Bilingual Sentiment Word Embeddings(BSWE) for Cross-language SentimentClassification3.1 Denoising AutoencoderIt has been demonstrated that the denoising au-toencoder could decrease the effects of translationerrors on the performance of CLSC (Zhou et al,2014a).
This paper proposes a deep learning basedapproach, which employs the denoising autoen-coder to learn the bilingual embeddings for CLSC.A denoising autoencoder is the modification ofan autoencoder.
The autoencoder (Bengio et al,2007) includes an encoder f?and a decoder g?
?.The encoder maps a d-dimensional input vectorx ?
[0, 1]dto a hidden representation y ?
[0, 1]d?through a deterministic mapping y = f?
(x) =?
(Wx + b), parameterized by ?
= {W,b}.
Wis a weight matrix, b is a bias term, and ?
(x) is theactivation function.
The decoder maps y back to areconstructed vector?x = g??
(y) = ?
(WTy + c),parameterized by ?
?= {WT, c}, where c is thebias term for reconstruction.Through the process of encoding and decod-ing, the parameters ?
and ?
?of the autoencoderwill be trained by gradient descent to minimize theloss function.
The sum of reconstruction cross-entropies across the training set is usually used asthe loss function:l(x) = ?d?i=1[xilog?xi+(1?xi) log(1?
?xi)] (1)A denoising autoencoder enhances robustnessto noises by corrupting the input x to a partiallydestroyed version?x.
The desired noise level of theinput x can be changed by adjusting the destruc-tion fraction ?.
For each input x, a fixed number?d (d is the dimension of x) of components areselected randomly, and their values are set to 0,432while the others are left untouched.
Like an au-toencoder, the destroyed input?x is mapped to alatent representation y = f?
(?x) = ?
(W?x + b).Then y is mapped back to a reconstructed vector?x through?x = g??
(y) = ?
(WTy + c).
The lossfunction of a denoising autoencoder is the same asthat of an autoencoder.
Minimizing the loss makes?x close to the input x rather than?x.Our BSWE learning process can be divided in-to two phases: the unsupervised phase of seman-tic learning and the supervised phase of sentimentlearning.
In the unsupervised phase, a denoisingautoencoder is employed to learn the bilingual em-beddings.
In the supervised phase, the sentimentinformation is incorporated into the bilingual em-beddings based on sentiment labels of documentsto obtain BSWE.3.2 Unsupervised Phase of the BilingualEmbedding LearningIn the unsupervised phase, the English trainingdocuments and their Chinese translations are em-ployed to learn the bilingual embeddings (Sen-timent polarity labels of documents are not em-ployed in this phase).
Based on the English docu-ments, 2,000 English sentiment words in MPQAsubjectivity lexicon1are extracted by the Chi-square method (Galavotti et al, 2000).
Their cor-responding Chinese translations are used as Chi-nese sentiment words.
Besides, some sentimen-t words are often modified by negation words,which lead to inversion of their polarities.
There-fore, negation features are introduced to each sen-timent word to represent its negative form.We take into account 14 frequently-used nega-tion words in English such as not and none; 5negation words in Chinese such as?
(no/not) andvk (without).
A sentiment word modified bythese negation words in the window [-2, 2] is con-sidered as its negative form in this paper, whilesentiment word features remain the initial mean-ing.
Negation features use binary expressions.
If asentiment word is not modified by negation words,the value of its negation features is set to 0.
Thus,the sentiment words and their corresponding nega-tion features in English and Chinese are adopted torepresent the document pairs (xE,xC).We expect that pairs of documents could beforced to capture the common semantic informa-tion of two languages.
To achieve this, a denoising1http://mpqa.cs.pitt.edu/lexicons/subj lexiconautoencoder is used to perform the reconstructionsof paired documents in both English and Chinese.Figure 1 shows the framework of bilingual embed-ding learning.EWEx?EyCx?
[ ]TE CW W?Cx),( CEl xxEx?Ex)( El xCWCx?CyCx?Cx),( ECl xxEx?Ex )( Cl x[ ]TE CW W?Ex Cx(a) reconstruction from xE (b) reconstruction from xCFigure 1: The framework of bilingual embeddinglearning.For the corrupted versions?xE(?xC) of the initialinput vector xE(xC), we use the sigmoid functionas the activation function to extract latent repre-sentations:yE= f?
(?xE) = ?
(WE?xE+ b) (2)yC= f?
(?xC) = ?
(WC?xC+ b) (3)where WEand WCare the language-specificword representation matrices, corresponding toEnglish and Chinese respectively.
Notice that thebias b is shared to ensure that the produced repre-sentations in two languages are on the same scale.For the latent representations in either language,we would like two decoders to perform recon-structions in English and Chinese respectively.
Asshown in Figure 1(a), for the latent representationyEin English, one decoder is used to map yEback to a reconstruction?xEin English, and theother is used to map yEback to a reconstruction?xCin Chinese such that:?xE= g??
(yE) = ?
(WTEyE+ cE) (4)?xC= g??
(yE) = ?
(WTCyE+ cC) (5)where cEand cCare the biases of the decoders inEnglish and Chinese, respectively.
Similarly, thesame steps repeat for the latent representation yCin Chinese, which are shown in Figure 1(b).The encoder and decoder structures allow usto learn a mapping within and across languages.Specifically, for a given document pair (xE,xC),we can learn bilingual embeddings to recon-struct xEfrom itself (loss l(xE)), reconstruc-t xCfrom itself (loss l(xC)), construct xCfrom433xE(loss l(xE,xC)), construct xEfrom xC(loss l(xC,xE)) and reconstruct the concatena-tion of xEand xC([xE, xC]) from itself (lossl([xE,xC], [?xE,?xC])).
The sum of 5 losses isused as the loss function of bilingual embeddings:L =l(xE) + l(xC) + l(xE,xC) + l(xC,xE)+ l([xE,xC], [?xE,?xC])(6)3.3 Supervised Phase of Sentiment LearningIn the unsupervised phase, we have learned thebilingual embeddings, which could capture the se-mantic information within and across languages.However, the sentiment polarities of text are ig-nored in the unsupervised phase.
Bilingual em-beddings without sentiment information are noteffective enough for sentiment classification task.This paper proposes an approach to learning B-SWE for CLSC, which introduces a supervisedlearning phase to incorporate sentiment informa-tion into the bilingual embeddings.
The process ofsupervised phase is shown in Figure 2.
[ , ]E CW Wby ?labelmax ( | ; )p s d?
?sentiment( | ; )p s d ?
],[ CE xxFigure 2: The supervised learning process.For paired documents [xE,xC], the sigmoidfunction is adopted as the activation functionto extract latent bilingual representations yb=?
([WE,WC][xE,xC]+b), where [WE,WC] isthe concatenation of WEand WC.The latent bilingual representation ybis usedto obtain the positive polarity probability p(s =1|d; ?)
of a document through a sigmoid function:p(s = 1|d; ?)
= ?
(?Tyb+ bl) (7)where ?
is the logistic regression weight vectorand blis the bias of logistic regression.
The senti-ment label s is a Boolean value representing sen-timent polarity of a document: s = 0 representsnegative polarity and s = 1 represents positive po-larity.
Parameter ?
?= {[WE,WC]?,b?, ?
?, b?l}is learned by maximizing the objective functionaccording to the sentiment polarity label siof doc-ument di:?
?= argmax?
?i=1log p(si|di; ?)
(8)Through the supervised learning phase,[WE,WC] is optimized by maximizing senti-ment polarity probability.
Thus, rich sentimentinformation is encoded into the bilingual embed-dings.The following experiments will prove that theproposed BSWE outperform the traditional bilin-gual embeddings significantly in CLSC.3.4 Bilingual Document RepresentationMethod (BDR)Once we have learned BSWE [WE,WC], whosecolumns are representations for sentiment words,we can use them to represent documents in twolanguages.Given an English training document dEcontaining 2,000 sentiment word featuress1, s2, ?
?
?
, s2,000and 2,000 corresponding nega-tion features, we represent it as the TF-IDFweighted sum of BSWE:?dE=4,000?i=1TF ?
IDF (si)WE.,si(9)Similarly, for its Chinese translation dCcontaining2,000 sentiment word features t1, t2, ?
?
?
, t2,000and 2,000 corresponding negation features, werepresent it as:?dC=4,000?j=1TF ?
IDF (tj)WC.,tj(10)We propose a bilingual document representa-tion method (BDR) in this paper, which representseach document diwith the concatenation of its En-glish and Chinese representations [?dE, ?dC].
B-DR is expected to enhance the ability of sentimentexpression for further improving the classificationperformance.
Such bilingual document represen-tations are fed to a linear SVM to perform senti-ment classification.4 Experiment4.1 Experimental SettingsData Set.
The proposed approach is evaluated onNLP&CC 2013 CLSC dataset2 3.
The dataset con-2http://tcci.ccf.org.cn/conference/2013/dldoc/evsam03.zip3http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip434sists of product reviews on three categories: book,DVD, and music.
Each category contains 4,000English labeled data as training data (the ratio ofthe number of positive and negative samples is1:1) and 4,000 Chinese unlabeled data as test data.Tools.
In our experiments, Google Translate4isadopted for both English-to-Chinese and Chinese-to-English translation.
ICTCLAS (Zhang et al,2003) is used as Chinese word segmentation tool.A denoising autoencoder is developed based onTheano system (Bergstra et al, 2010).
BSWEare trained for 50 and 30 epochs in unsuper-vised phase and supervised phases respectively.SVMlight(Joachims, 1999) is used to train lin-ear SVM sentiment classifiersEvaluation Metric.
The performance is evalu-ated by the classification accuracy for each cate-gory, and the average accuracy of three categories,respectively.
The category accuracy is defined as:Accuracyc=#system correctc#system totalc(11)where c is one of the three categories, and#system correctcand #system totalcstandfor the number of being correctly classified re-views and the number of total reviews in the cate-gory c, respectively.The average accuracy is shown as:Average =13?cAccuracyc(12)4.2 Evaluations on BSWEIn this section, we evaluate the quality of BSWEfor CLSC.
The dimension of bilingual embeddingsd is set to 50, and destruction fraction ?
is set to0.2.Effects of Bilingual Embedding LearningMethodsWe first compare our unsupervised bilingual em-bedding learning method with the parallel cor-pora based method.
The parallel corpora basedmethod uses the paired documents in the parallelcorpus5to learn bilingual embeddings, while ourmethod only uses the English training documentsand their Chinese translations (Sentiment polari-ty labels of documents are not employed here).The Boolean feature weight calculation method is4http://translate.google.cn/5http://www.datatang.com/data/45485adopted to represent documents for bilingual em-bedding learning and BDR is employed to rep-resent training data and test data for sentimen-t classification.
To represent the paired docu-ments in the parallel corpus, 27,597 English word-s and 31,786 Chinese words are extracted forbilingual embedding learning.
Our method onlyneeds 2,000 English sentiment words, 2,000 Chi-nese sentiment words, and their negation features,which significantly reduces the dimension of inputvectors.Our methodParallel corpora based methodAverage0.50.550.60.650.70.750.8Corpus Scale1042?1043?1044?1045?1046?1047?104Figure 3: Our unsupervised bilingual embed-ding learning method vs.
Parallel corpora basedmethod.The average accuracies on NLP&CC 2013 testdata of the two bilingual embedding learningmethods are shown in Figure 3.
As can be seenfrom Figure 3, when the corpus scales of the twomethods are the same (4,000 paired documents),our method (75.09% average accuracy) surpassesthe parallel corpora method (54.82% average ac-curacy) by about 20%.
With the scale of the par-allel corpora increasing, the performance of par-allel corpora based method is steadily improved.However, the performance is not as good as ourbilingual embedding learning method.
Though thedocument number of the parallel corpus is up to70,000 , the average accuracy is only 70.05%.
It isproved that our method is more suitable for learn-ing bilingual embeddings for cross-language senti-ment classification than the parallel corpora basedmethod.Effects of Feature Weight in BilingualEmbeddingsIn this part, we compare the Boolean and TF-IDF feature weight calculation methods in bilin-gual embedding learning process.Table 1 shows the classification accuracy with435Category book DVD music AverageBoolean 76.22% 74.30% 74.75% 75.09%TF-IDF 76.65% 77.60% 74.50% 76.25%Table 1: The classification accuracy with theBoolean and TF-IDF methods.the Boolean and TF-IDF methods.
Generally, theTF-IDF method performs better than the Booleanmethod.
The average accuracy of the TF-IDFmethod is 1.16% higher than the Boolean method,which illustrates that the TF-IDF method could re-flect the latent contribution of sentiment words toeach document effectively.
The TF-IDF weightcalculation method is exploited in the followingexperiments.
Notice that sentiment informationis not yet introduced in the bilingual embeddingshere.Effects of Sentiment Information in BSWEIncorporating sentiment information in the bilin-gual embeddings, the performance of bilingualembeddings (without sentiment information) andBSWE (with sentiment information) is comparedin Figure 4.Ouru met rhod Poaau mlOcp bsAAevtvg0.560.570.580.550.5C0.5S0.CP114 2 ?2 d eluA s 3ovtmoFigure 4: Performance comparison of the bilingualembeddings and BSWE.As can be seen from Figure 4, by encoding sen-timent information in the bilingual embeddings,the performance in book, DVD and music cate-gories significantly improves to 79.47%, 78.72%and 76.58% respectively (2.82% increase in book,1.12% in DVD, and 2.08% in music).
The av-erage accuracy reaches 78.26%, which is 2.01%higher than that of the bilingual embeddings.
Theexperimental results indicate the effectiveness ofsentiment information in the bilingual embeddinglearning.
The BSWE learning approach is em-ployed for CLSC in the following experiments.Effects of Bilingual Document RepresentationMethodIn this experiment, our bilingual document rep-resentation method (BDR) is compared with thefollowing monolingual document representationmethods.En-En: This method represents training andtest documents in English only with WE.
Englishtraining documents and Chinese-to-English trans-lations of test documents are both represented withWE.Cn-Cn: This method represents training andtest documents in Chinese only with WC.English-to-Chinese translations of training docu-ments and Chinese test documents are both repre-sented with WC.En-Cn: This method represents English train-ing documents with WE, while represents Chi-nese test documents with WC.
Chandar A P etal.
(2014) employed this method in their work.BDR: This method adopts our bilingual doc-ument representation method, which representstraining and test documents with both WEandWC.OurOuur uOur ume thodPaldcpbscpbAcpbvcpbgcpbbcpb0cpb.cp0?c cp5 cpA cpg cp0Figure 5: Effects of bilingual document represen-tation method (BDR).Figure 5 shows the average accuracy curves ofdifferent document representation methods with d-ifferent destruction fraction ?.
We vary ?
from 0to 0.9 with an interval of 0.1.From Figure 5 we can see that En-En, Cn-Cn,and En-Cn get similar results.
BDR performs con-stantly better than the other representation meth-ods throughout the interval [0, 0.9].
The absolutesuperiority of BDR benefits from the enhanced a-bility of sentiment expression.Meanwhile, when the input x is partially de-436stroyed (?
varies from 0.1 to 0.9), the perfor-mance of En-En, Cn-Cn and En-Cn remains sta-ble, which illustrates the robustness of the denois-ing autoencoder to corrupting noises.
In addi-tion, the average accuracies of BDR in the inter-val ?
?
[0.1, 0.9] are all higher than the averageaccuracy under the condition ?
= 0 (78.23%).Therefore, adding noises properly to the trainingdata could improve the performance of BSWE forCLSC.4.3 Influences of Dimension d andDestruction Fraction ?Figure 6 shows the relationship between accura-cies and dimension d of BSWE as well as that be-tween accuracies and destruction fraction ?
in au-toencoders in different categories.
Dimension ofembeddings d varies from 50 to 500, and destruc-tion fraction ?
varies from 0.1 to 0.9.As shown in Figure 6, the average accuraciesgenerally move upward as dimension of BSWE in-creasing.
Generally, the average accuracies keephigher than 80% with ?
varying from 0.1 to 0.5as well as dimension varying from 300 to 500.When ?
= 0.1 and d = 400, the average accu-racy reaches the peak value 80.68% (category ac-curacy of 81.05% in book, 81.60% in DVD, and79.40% in music).
The experimental results showthat in BSWE learning process, increasing the di-mension of embeddings or properly adding noisesto the training data helps improve the performanceof CLSC.
In this paper, we only evaluate BSWEwhen dimension d varies from 50 to 500.
Howev-er, there is still space for further improvement if dcontinues to increase.4.4 Comparison with Related WorkTable 2 shows comparisons of the performancebetween our approach and some state-of-the-artsystems on NLP&CC 2013 CLSC dataset.
Ourapproach achieves the best performance with an80.68% average accuracy.
Compared with the re-cent related work, our approach is more effectiveand suitable for eliminating the language gap.Chen et al (2014) translated Chinese test da-ta into English and then gave different weight-s to sentiment words according to the subject-predicate component of sentiment words.
Theygot 77.09% accuracy and took the 2nd place inNLP&CC 2013 CLSC share task.
The machinetranslation based approach was limited by thetranslation errors.System book DVD music AverageChen et al(2014)77.00% 78.33% 75.95% 77.09%Gui et al(2013)78.70% 79.65% 78.30% 78.89%Gui et al(2014)80.10% 81.60% 78.60% 80.10%Zhou et al(2014a)80.63% 80.95% 78.48% 80.02%Our approach81.05% 81.60% 79.40% 80.68%Table 2: Performance comparisons on theNLP&CC 2013 CLSC dataset.Gui et al (2013; 2014) and Zhou et al (2014a)adopted the multi-view approach to bridge the lan-guage gap.
Gui et al (2013) proposed a mixedCLSC model by combining co-training and trans-fer learning strategies.
They achieved the high-est accuracy of 78.89% in NLP&CC CLSC sharetask.
Gui et al (2014) further improved the accu-racy to 80.10% by removing noise from the trans-ferred samples to avoid negative transfers.
Zhouet al (2014a) built denoising autoencoders in t-wo independent views to enhance the robustnessto translation errors in the inputs and achieved80.02% accuracy.
The multi-view approach learn-s language-specific classifiers in each view dur-ing training process, which is difficult to capturethe common sentiment information of the two lan-guages.
Our approach integrates the bilingual em-bedding learning into a unified process, and out-performs Chen et al (2014), Gui et al (2013), Guiet al (2014) and Zhou et al (2014a) by 3.59%,1.79%, 0.58%, and 0.66% respectively.
The su-periority of our approach benefits from the unifiedbilingual embedding learning process and the in-tegration of semantic and sentiment information.5 Conclusion and Future WorkThis paper proposes an approach to learning B-SWE by incorporating sentiment information in-to the bilingual embeddings for CLSC.
The pro-posed approach learns BSWE with the labeleddocuments and their translations rather than par-allel corpora.
In addition, BDR is proposed to en-hance the sentiment expression ability which com-bines English and Chinese representations.
Exper-iments on the NLP&CC 2013 CLSC dataset showthat our approach outperforms the previous state-of-the-art systems as well as traditional bilingualembedding systems.
The proposed BSWE are on-ly evaluated on English-Chinese CLSC in this pa-per, but it can be popularized to other languages.437Figure 6: The relationship between accuracies and dimension d as well as that between accuracies anddestruction fraction ?.Both semantic and sentiment information playan important role in sentiment classification.
Inthe following work, we will further investigate therelationship between semantic and sentiment in-formation for CLSC, and balance their functionsto optimize their combination for CLSC.AcknowledgmentsWe wish to thank the anonymous reviewers fortheir valuable comments.
This research is support-ed by National Natural Science Foundation of Chi-na (Grant No.
61272375).ReferencesCarmen Banea, Rada Mihalcea, Janyce Wiebe andSamer Hassan.
2008.
Multilingual SubjectivityAnalysis Using Machine Translation.
In Proceed-ings of the 2008 Conference on Empirical Method-s in Natural Language Processing, pages 127-135.Association for Computational Linguistics.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A Neural Probabilistic Lan-guage Model.
The Journal of Machine Learning Re-search, vol 3: 1137-1155.Yoshua Bengio, Pascal Lamblin, Dan Popovici, andHugo Larochelle.
2007.
Greedy layer-wise train-ing of deep networks.
In Proceedings of Advancesin Neural Information Processing Systems 19 (NIPS06), pages 153-160.
MIT Press.Yoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and newperspectives.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence 35(8): 1798-1828.IEEE.James Bergstra, Olivier Breuleux, Frederic Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, Yoshua Bengio.
2010.Theano: a CPU and GPU math expression compiler.In Proceedings of the Python for scientific comput-ing conference (SciPy).Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-oufandeh.
2011.
Sentiment classification based onsupervised latent n-gram analysis.
In Proceedingsof the Conference on Information and KnowledgeManagement, pages 375-382.
ACM.Sarath Chandar A P, Mitesh M. Khapra, Balara-man Ravindran, Vikas Raykar and Amrita Saha.2013.
Multilingual deep learning.
In Deep LearningWorkshop at NIPS 2013.Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,Mitesh M Khapra, Balaraman Ravindran,Vikas Raykar, and Amrita Saha.
2014.
Anautoencoder approach to learning bilingual wordrepresentations.
In Advances in Neural InformationProcessing Systems, pages 1853-1861.438Qiang Chen, Yanxiang He, Xule Liu, Songtao Sun,Min Peng, and Fei Li.
2014.
Cross-Language Sen-timent Analysis Based on Parser (in Chinese).
ActaScientiarum Naturalium Universitatis Pekinensis, 50(1): 55-60.G.
E. Hinton and R. R. Salakhutdinov.
2006.
Reducingthe Dimensionality of Data with Neural Networks.Science, vol 313: 504-507.Luigi Galavotti, Fabrizio Sebastiani, and Maria Sim-i.
2000.
Feature Selection and Negative Evidencein Automated Text Categorization.
In Proceedingsof ECDL-00, 4th European Conference on Researchand Advanced Technology for Digital Libraries.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Pro-ceedings of 28th International Conference on Ma-chine Learning, pages 513-520.Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao,Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam-Fai Wong, and Ricky Cheung.
2013.
A mixed mod-el for cross lingual opinion analysis.
In Proceedingsof Natural Language Processing and Chinese Com-puting, pages 93-104.
Springer Verlag.Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu,and Xiaolong Wang.
2014.
Cross-lingual OpinionAnalysis via Negative Transfer Detection.
In Pro-ceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (Short Papers),pages 860-865.
Association for Computational Lin-guistics.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
Universit?at Dortmund.Alistair Kennedy and Diana Inkpen.
2006.
Sentimentclassification of movie reviews using contextual va-lence shifters.
Computational intelligence, 22(2):110-125.Shoushan Li, Rong Wang, Huanhuan Liu, and Chu-Ren Huang.
2013.
Active learning for cross-lingualsentiment classification.
In Proceedings of Natu-ral Language Processing and Chinese Computing,pages 236-246.
Springer Verlag.Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning Word Vectors for Sentiment Anal-ysis.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics,pages 142-150.
Association for Computational Lin-guistics.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL-HLT, pages 746-751.
Association for Computation-al Linguistics.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 79-86.
ACM.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Com-putational Linguistics, pages 271-278.
Associationfor Computational Linguistics.Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,and Christopher D. Manning.
2011.
Parsing natu-ral scenes and natural language with recursive neu-ral networks.
In Proceedings of the InternationalConference on Machine Learning, pages 129-136.Bellevue.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Com-positionality through Recursive Matrix-Vector S-paces.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1201-1211.
Association for Computational Linguis-tics.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, T-ing Liu, and Bing Qin.
2014.
Learning Sentiment-Specific Word Embedding for Twitter Sentimen-t Classification.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistic, pages 1555-1565.
Association for Compu-tational Linguistics.Xuewei Tang and Xiaojun Wan.
2014.
Learn-ing Bilingual Embedding Model for Cross-languageSentiment Classification.
In Proceedings of 2014IEEE/WIC/ACM International Joint Conferences onWeb Intelligence (WI) and Intelligent Agent Tech-nologies (IAT), pages 134-141.
IEEE.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, andPierre-Antoine Manzagol.
2008.
Extracting andcomposing robust features with denoising autoen-coders.
In Proceedings of the 25th internationalconference on Machine learning, pages 1096-1103.ACM.Xiaojun Wan.
2009.
Co-training for cross-lingual sen-timent classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 235-243.
Association for Computational Linguistics.Yuan Wang, Zhaohui Li, Jie Liu, Zhicheng He,Yalou Huang, and Dong Li.
2014.
Word Vec-tor Modeling for Sentiment Analysis of Product Re-views.
In Proceedings of Natural Language Pro-cessing and Chinese Computing, pages 168-180.Springer Verlag.439Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Li-u.
2003.
HHMM-based Chinese Lexical AnalyzerICTCLAS.
In 2nd SIGHAN workshop affiliated with41th ACL, pages 184-187.
Association for Compu-tational Linguistics.Guangyou Zhou, Tingting He, and Jun Zhao.
2014b.Bridging the Language Gap: Learning Distribut-ed Semantics for Cross-Lingual Sentiment Classifi-cation.
In Proceedings of Natural Language Pro-cessing and Chinese Computing, pages 138-149.Springer Verlag.Huiwei Zhou, Long Chen, and Degen Huang.
2014a.Cross-lingual sentiment classification based on de-noising autoencoder.
In Proceedings of Natu-ral Language Processing and Chinese Computing,pages 181-192.
Springer Verlag.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual Word Embed-ding for Phrase-Based Machine Translation.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1393-1398.440
