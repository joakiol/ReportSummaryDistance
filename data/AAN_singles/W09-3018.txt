Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 112?115,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPTimed Annotations ?
Enhancing MUC7 Metadataby the Time It Takes to Annotate Named EntitiesKatrin Tomanek and Udo HahnJena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t Jena, Germany{katrin.tomanek|udo.hahn}@uni-jena.deAbstractWe report on the re-annotation of selectedtypes of named entities from the MUC7corpus where our focus lies on record-ing the time it takes to annotate theseentities given two basic annotation units?
sentences vs. complex noun phrases.Such information may be helpful to laythe empirical foundations for the develop-ment of cost measures for annotation pro-cesses based on the investment in time fordecision-making per entity mention.1 IntroductionManually supplied annotation metadata is at theheart of (semi)supervised machine learning tech-niques which have become very popular in NLPresearch.
At their flipside, they create an enor-mous bottleneck because major shifts in the do-main of discourse, the basic entities of interest, orthe text genre often require new annotation efforts.But annotations are costly in terms of getting well-trained and intelligible human resources involved.Surprisingly, cost awareness has not been a pri-mary concern in most of the past linguistic anno-tation initiatives.
Only recently, annotation strate-gies (such as Active Learning (Cohn et al, 1996))which strive for minimizing the annotation loadhave gained increasing attention.
Still, when itcomes to the empirically plausible assessment ofannotation costs even proponents of Active Learn-ing make overly simplistic and empirically ques-tionable assumptions, e.g., the uniformity of an-notation costs over the number of linguistic units(e.g., tokens) to be annotated.We here consider the time it takes to annotatea particular entity mention as a natural indicatorof effort for named entity annotations.
In order tolay the empirical foundations for experimentallygrounded annotation cost models we couple com-mon named entity annotation metadata with a timestamp reflecting the time measured for decisionmaking.1Previously, two studies ?
one dealing with POSannotation (Haertel et al, 2008), the other withnamed entity and relation annotation (Settles et al,2008) ?
have measured the time needed to anno-tate sentences on small data sets and attempted tolearn predictive models of annotation cost.
How-ever, these data sets do not meet our requirementsas we envisage a large, coherent, and also well-known newspaper entity corpus extended by an-notation costs on a fine-grained level.
Especiallysize and coherence of such a corpus are not onlyessential for building accurate cost models but alsoas a reference baseline for cost-sensitive annota-tion strategies.
Moreover, the annotation level forwhich cost information is available is crucial be-cause document- or sentence-level data might betoo coarse for several applications.
Accordingly,this paper introduces MUC7T , our extension to theentity annotations of the MUC7 corpus (Linguis-tic Data Consortium, 2001) where time stamps areadded to two levels of annotation granularity, viz.sentences and complex noun phrases.2 Corpus Annotation2.1 Annotation TaskOur annotation initiative constitutes an extensionto the named entity annotations (ENAMEX) of theEnglish part of the MUC7 corpus covering NewYork Times articles from 1996.
ENAMEX annota-tions cover three types of named entities, viz.
PER-SONS, LOCATIONS, and ORGANIZATIONS.
We in-structed two human annotators, both advanced stu-dents of linguistics with good English languageskills, to re-annotate the MUC7 corpus for theENAMEX subtask.
To be as consistent as possi-1These time stamps should not be confounded with the an-notation of temporal expressions (TIMEX in MUC7, or evenmore advanced metadata using TIMEML for the creation ofthe TIMEBANK (Pustejovsky et al, 2003)).112ble with the existing MUC7 annotations, the an-notators had to follow the original guidelines ofthe MUC7 named entity task.
For ease of re-annotation, we intentionally ignored temporal andnumber expressions (TIMEX and NUMEX).MUC7 covers three distinct document sets forthe named entity task.
We used one of these setsto train the annotators and develop the annotationdesign, and another one for our actual annotationinitiative which consists of 100 articles reportingon airplane crashes.
We split lengthy documents(27 out of 100) into halves to fit on the annota-tion screen without the need for scrolling.
Further-more, we excluded two documents due to over-length which would have required overly manysplits.
Our final corpus contains 3,113 sentences(76,900 tokens) (see Section 3.1 for more details).Time-stamped ENAMEX annotation of this cor-pus constitutes MUC7T , our extension of MUC7.Annotation time measurements were taken on twosyntactically different annotation units of singledocuments: (a) complete sentences and (b) com-plex noun phrases.
The annotation task was de-fined such as to assign an entity type label to eachtoken of an annotation unit.
Sentence-level anno-tation units where derived by the OPENNLP2 sen-tence splitter.
The use of complex noun phrases(CNPs) as an alternative annotation unit is mo-tivated by the fact that in MUC7 the syntacticencoding of named entity mentions basically oc-curs through nominal phrases.
CNPs were derivedfrom the sentences?
constituency structure usingthe OPENNLP parser (trained on PENNTREE-BANK data) to determine top-level noun phrases.To avoid overly long phrases, CNPs dominatingspecial syntactic structures, such as coordinations,appositions, or relative clauses, were split up atdiscriminative functional elements (e.g., a relativepronoun) and these elements were eliminated.
Anevaluation of our CNP extractor on ENAMEX an-notations in MUC7 showed that 98.95% of all en-tities where completely covered by automaticallyidentified CNPs.
For the remaining 1.05% of theentity mentions, parsing errors were the most com-mon source of incomplete coverage.2.2 Annotation and Time MeasurementWhile the annotation task itself was ?officially?declared to yield only annotations of named en-tity mentions within the different annotation units,2http://opennlp.sourceforge.netwe were primarily interested in the time neededfor these annotations.
For precise time measure-ments, single annotation examples were shown tothe annotators, one at a time.
An annotation exam-ple consists of the chosen MUC7 document withone annotation unit (sentence or CNP) selectedand highlighted.
Only the highlighted part of thedocument could be annotated and the annotatorswere asked to read only as much of the context sur-rounding the annotation unit as necessary to makea proper annotation decision.
To present the an-notation examples to annotators and allow for an-notation without extra time overhead for the ?me-chanical?
assignment of entity types, our annota-tion GUI is controlled by keyboard shortcuts.
Thisminimizes annotation time compared to mouse-controlled annotation such that the measured timereflects only the amount of time needed for takingan annotation decision.In order to avoid learning effects at the annota-tors?
side on originally consecutive syntactic sub-units, we randomly shuffled all annotation exam-ples so that subsequent annotation examples werenot drawn from the same document.
Hence, an-notation times were not biased by the order of ap-pearance of the annotation examples.Annotators were given blocks of either 500CNP- or 100 sentence-level annotation examples.They were asked to annotate each block in a singlerun under noise-free conditions, without breaksand disruptions.
They were also instructed not toannotate for too long stretches of time to avoid tir-ing effects making time measurements unreliable.All documents were first annotated with respectto CNP-level examples within 2-3 weeks, withonly very few hours per day of concrete annota-tion work.
After completion of the CNP-level an-notation, the same documents had to be annotatedon the sentence level as well.
Due to randomiza-tion and rare access to surrounding context duringthe CNP-level annotation, annotators credibly re-ported that they had indeed not remembered thesentences from the CNP-level round.
Thus, thetime measurements taken on the sentence level donot seem to exhibit any human memory bias.Both annotators went through all annotation ex-amples so that we have double annotations of thecomplete data set.
Prior to coding, they indepen-dently got used to the annotation guidelines andwere trained on several hundred examples.
For theannotators?
performance see Section 3.2.1133 Analysis3.1 Corpus StatisticsTable 1 summarizes statistics on the time-stampedMUC7 corpus.
About 60% of all tokens are cov-ered by CNPs (45,097 out of 76,900 tokens) show-ing that sentences are made up from CNPs to alarge extent.
Still, removing the non-CNP to-kens markedly reduces the amount of tokens tobe considered for entity annotation.
CNPs coverslightly less entities (3,937) than complete sen-tences (3,971), a marginal loss only.sentences 3,113sentence tokens 76,900chunks 15,203chunk tokens 45,097entity mentions in sentences 3,971entity mentions in CNPs 3,937sentences with entity mentions 63%CNPs with entity mentions 23%Table 1: Descriptive statistics of time-stamped MUC7 corpusOn the average, sentences have a length of24.7 tokens, while CNPs are rather short with3.0 tokens, on the average.
However, CNPs varytremendously in their length, with the shortestones having only one token and the longest ones(mostly due to parsing errors) spanning over 30(and more) tokens.
Figure 1 depicts the lengthdistribution of sentences and CNPs showing thata reasonable portion of CNPs have less than fivetokens, while the distribution of sentence lengthsalmost follows a normal distribution in the interval[0, 50].
While 63% of all sentences contain at leastone entity mention, only 23% of CNPs contain en-tity mentions.
These statistics show that CNPs aregenerally rather short and a large fraction of CNPsdoes not contain entity mentions at all.
We mayhypothesize that this observation will be reflectedby annotation times.sentence lengthtokensfrequency0 20 40 60 800100300500CNP lengthtokensfrequency0 5 10 15 20020006000Figure 1: Length distribution of sentences and CNPs3.2 Annotation PerformanceTo test the validity of the guidelines and the gen-eral performance of our annotators A and B, wecompared their annotation results on 5 blocks ofsentence-level annotation examples created dur-ing training.
Annotation performance was mea-sured in terms of Cohen?s kappa coefficient ?
onthe token level and entity-segment F -score againstMUC7 annotations.
The annotators achieved?A = 0.95 and ?B = 0.96, and FA = 0.92and FB = 0.94, respectively.3 Moreover, they ex-hibit an inter-annotator agreement of ?A,B = 0.94and an averaged mutual F-score of FA,B = 0.90.These numbers reveal that the task is well-definedand the annotators have sufficiently internalizedthe annotation guidelines to produce valid results.Figure 2 shows the annotators?
scores againstthe original MUC7 annotations for the 31 blocksof sentence-level annotations (3,113 sentences)which range from ?
= 0.89 to ?
= 0.98.
Largely,annotation performance is similar for both anno-tators and shows that they consistently found ablock either rather hard or easy to annotate.
More-over, annotation performance seems stationary ?no general trend in annotation performance overtime can be observed.l l l llllllllllll lll lll lll ll ll ll l0 5 10 15 20 25 300.800.850.900.951.00sentence?level annotationblockskappa ll l l l l lll l llllllll l lllllllllllll annotator Aannotator BFigure 2: Average kappa coefficient per block3.3 Time MeasurementsFigure 3 shows the average annotation time perblock (CNPs and sentences).
Considering theCNP-level annotations, there is a learning effectfor annotator B during the first 9 blocks.
Af-ter that, both annotators are approximately on apar regarding the annotation time.
For sentence-level annotations, both annotators again yield sim-ilar annotation times per block, without any learn-ing effects.
Similar to annotation performance,3Entity-specific F-scores against MUC7 annotations forA and B are 0.90 and 0.92 for LOCATION, 0.92 and 0.93for ORGANIZATION, and 0.96 and 0.98 for PERSON, respec-tively.114l l l l ll l ll lll l l l l lll ll l l lll l lll0 5 10 15 20 25 301.01.21.41.61.82.0CNP?level annotationblockssecondsll lll lll ll ll l l l l ll l l lllllll annotator Aannotator Bllll l lllllllllllllllll l ll lll lll0 5 10 15 20 25 304.04.55.05.56.0sentence?level annotationblockssecondsll l llllllllll llll l l llll ll llllll annotator Aannotator BFigure 3: Average annotation times per blockanalysis of annotation time shows that the annota-tion behavior is largely stationary (excluding firstrounds of CNP-level annotation) which allows sin-gle time measurements to be interpreted indepen-dently of previous time measurements.
Both, timeand performance plots exhibit that there are blockswhich were generally harder or easier than otherones because both annotators operated in tandem.3.4 Easy and Hard Annotation ExamplesAs we have shown, inter-annotator variation ofannotation performance is moderate.
Intra-blockperformance, in contrast, is subject to high vari-ance.
Figure 4 shows the distribution of annota-tor A?s CNP-level annotation times for block 20.A?s average annotation time on this block amountsto 1.37 seconds per CNP, the shortest time be-ing 0.54, the longest one amounting 10.2 seconds.The figure provides ample evidence for an ex-tremely skewed time investment for coding CNPs.A preliminary manual analysis revealed CNPswith very low annotation times are mostly shortand consist of stop words and pronouns only, orCNP?level annotationannotation timefrequency2 4 6 8 10050150250Figure 4: Distribution of annotation times in one blockare otherwise simple noun phrases with a sur-face structure incompatible with entity mentions(e.g., all tokens are lower-cased).
Here, humanscan quickly exclude the occurrence of entity men-tions which results in low annotation times.
CNPswhich took desparately long (more than 6 seconds)were outliers indicating distraction or loss of con-centration.
Times between 3 and 5 seconds werebasically caused by semantically complex CNPs.4 ConclusionsWe have created a time-stamped version of MUC7entity annotations, MUC7T , on two levels of anno-tation granularity ?
sentences and complex nounphrases.
Especially the phrase-level annotationsallow for fine-grained time measurement.
We willuse this corpus for studies on (time) cost-sensitiveActive Learning.
MUC7T can also be used to de-rive or learn accurate annotation cost models al-lowing to predict annotation time on new data.
Weare currently investigating causal factors of anno-tation complexity for named entity annotation onthe basis of MUC7T .AcknowledgementsThis work was funded by the EC within theBOOTStrep (FP6-028099) and CALBC (FP7-231727) projects.
We want to thank Oleg Lichten-wald (JULIE Lab) for implementing the nounphrase extractor for our experiments.ReferencesDavid A. Cohn, Zoubin Ghahramani, and Michael I.Jordan.
1996.
Active learning with statistical mod-els.
Journal of Artificial Intelligence Research,4:129?145.Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-roll, and Peter McClanahan.
2008.
Assessing thecosts of sampling methods in active learning for an-notation.
In Proceedings of the ACL-08: HLT, ShortPapers, pages 65?68.Linguistic Data Consortium.
2001.
Message Under-standing Conference 7.
LDC2001T02.
FTP file.James Pustejovsky, Patrick Hanks, Roser Saur?
?, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, and Marcia Lazo.
2003.
The TIMEBANKcorpus.
In Proceedings of the Corpus Linguistics2003 Conference, pages 647?656.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active learning with real annotation costs.
In Pro-ceedings of the NIPS?08 Workshop on Cost SensitiveLearning, pages 1?10.115
