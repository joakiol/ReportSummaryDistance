Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 33?40,Sydney, July 2006. c?2006 Association for Computational LinguisticsCoupling a Linguistic Formalism and a Script LanguageAbstractThis article presents a novel syntacticparser architecture, in which a linguisticformalism can be enriched with all sortsof constraints, included extra-linguisticones, thanks to the seamless coupling ofthe formalism with a programming lan-guage.1 IntroductionThe utilization of constraints in natural languageparsers (see Blache and Balfourier,2001 orTapanainen and J?rvinen , 1994) is central tomost systems today.
However, these constraintsare often limited to purely linguistic features,such as linearity or dependency relations be-tween categories within a given syntactic tree.Most linguistic formalisms have been createdwith the sole purpose of extracting linguistic in-formation from bits and pieces of text.
They usu-ally use a launch and forget strategy, where a textis analyzed according to local constraints, dis-played and then discarded to make room for thenext block of text.
These parsers take each sen-tence as an independent input, on which gram-mar rules are applied together with constraints.However, no sentence is independent of a text,and no text is really independent of extra-linguistic information.
In order to assess cor-rectly the phrase President Bush, we need toknow that Bush is a proper name, whose functionis ?President?.
Washington can be a town, astate, the name of a famous president, but alsothe name of an actor.
Moreover, the analysis of asentence is never an independent process, ifPresident Bush is found in a text, the reference topresident, later in the document will be related tothis phrase.These problems are certainly not new and adense literature has been written about how tobetter deal with these issues.
However, mostsolutions rely on formalism enrichments withsolutions ?engraved in stone?, that makes it diffi-cult to adapt a grammar to new domains (see De-clerck, 2002, or Roux, 2004), even though theyuse XML representation or database to storehuge amounts of extra-linguistic information.The interpretation of these data is intertwinedinto the very fabric of the parser and requiresdeep modifications to use new sources with acomplete different DTD.Of course, there are many other ways to solvethese problems.
For instance, in the case of lan-guages such as Prolog or Lisp, the grammar for-malism is often indistinguishable from the pro-gramming language itself.
For these parsers, thequerying of external information is easily solvedas grammar rules can be naturally augmentedwith non-linguistic procedures that are written inthe same language.
In other cases, when theparser is independent from any specific pro-gramming languages, the problem can prove dif-ficult to solve.
The formalism can of course beaugmented with new instructions to tackle thequerying of external information.
However thetime required to enrich the parser language maynot be worth the effort, as the development of acomplete new instruction set is a heavy andcomplex task that is loosely related to linguisticparser programming.We propose in this article a new way of buildingnatural language parsers, with the coupling of ascript language with an already rich linguisticformalism.2 ScriptingThe system we describe in this article mix anatural language parser, namely Xerox Incre-mental Parser (XIP hereafter, see A?t-Mohktar etal., 2002, Roux, 1999) with a scripting language,in our case Python.
The interaction of a grammarwith scripting instructions is almost as old asClaude RouxXerox Research Centre Europe/ 6,chemin de Maupertuis, 38240 Meylan,FranceClaude.roux@xrce.xerox.com33computational linguistics.
Pereira and Shieber forinstance, in their book: Prolog and Natural Lan-guage Processing (see Pereira and Shieber, 1987)already suggested mixing grammar rules withextra-linguistic information.
This m?lange wasmade possible thanks to the homogeneity be-tween grammar rules and the external code, writ-ten in both cases in the same programming lan-guage.
However, these programming languagesare not exactly tuned to do linguistic analyses;they are often slow and cumbersome.
Moreover,they blur the boundary between program andgrammar rules, as the programming language isboth the algorithm language and the rule lan-guage.
Allen (see Allen, 1994) proposes a differ-ent approach in his TRAINS Parsing system.
Thegrammar formalism is independent to a certainextent from the implementation language, whichis LISP in this case.
However, since the grammaris translated into a LISP program, it is easy for alinguist to specialize the generated rules withexternal LISP procedures.
Nevertheless, thegrammar formalism remains very close to LISPdata description, which makes the grammar rulessomewhat difficult to read.The other solution, which is usually favored bycomputational linguists, is to store the externalinformation in databases, which are accessedwith some pre-defined instructions and translatedinto linguistic features.
For instance (see De-clerk, 2002 or Roux, 2004), the external informa-tion is presented as an XML document whoseDTD is defined once and for all.
This DTD isthen enriched with extra-linguistic informationthat a parser can exploit to guide rule application.This method alleviates the necessity of a com-plex interaction mechanism between the parserand its external data sources.
The XPath lan-guage is used to query this document in order toretrieve salient information at parsing time,which is then translated into local linguistic fea-tures.
However, only static information can beexploited, as these XML databases must be builtbeforehand.Similar mechanisms have also been proposed inother architectures to help heterogeneous linguis-tic modules to communicate through a commonXML interface (see Cunningham et al,2002,Blache and Gu?not , 2003).
These architecturesare very powerful as they connect together toolsthat only need to comply with a common in-put/output DTD.
Specialized Java modules canthen be written which are applied to intermediaterepresentations to add their own touch of extra-linguistic data.
Since, the intermediate represen-tation is an XML document, the number of pos-sible enrichments is almost limitless, as eachmodule will only extract from this document theXML markup tags that it is designed to handle.However, since XML is by nature furiously ver-bose, the overall system might be very slow as itmight spend a large amount of time translatingexternal XML representation into internal repre-sentations.Furthermore, applications that require naturallanguage processing also have different pur-poses, different needs.
They may require a shal-low output, such as a simple tokenization with awhiff of tagging, or a much deeper analysis.
Syn-tactic parsing is usually integrated as a black boxinto these architectures, with little control leftover the grammar execution, control which nev-ertheless might prove very important in manycases.
An XML document, for instance, oftencontains some specific markup tags to identify atitle, a section or author name.
If the parser isgiven some indications about the input, it couldbe guided through the grammar maze to favorthese rules that are better suited to analyze a titlefor example.Finally, syntactic parsing, when it is limited tolexical information, often fails to assess correctlysome ambiguous relations.
Thus, the only way todeal with PP-attachment or anaphoric pronounantecedents is to use both previous analyses andexternal information.
However, most syntacticparsers are often ill geared to link with externalmodules.
The formalism is engraved into a Cprogram as in Link Grammar (see Grinberg etal.,1995) or as in Sylex (see Constant, 1995)which offers little or no opening to the rest of theworld, as it is mainly designed to accomplish oneunique task.
We will show how the seamless in-tegration of a script language into the very fabricof the formalism simplifies the task of keepingtrack of previous analyses together with the useof external sources of data.3 Xerox Incremental Parser (XIP)The XIP engine has been developed by a re-search team in computational linguistics at theXerox Research Centre Europe (see A?t-Mokhtaret al, 2001).
It has been designed from the be-ginning to follow a strictly incremental strategy,where rules apply one after the other.
There isonly one analysis path that is followed for agiven linguistic unit (phrase, sentence or evenparagraph): the failure of a rule does not preventthe whole analysis from continuing to comple-34tion.
Since the system never backtracks on anyrules, XIP cannot propel itself into a combinato-rial explosion.XIP can be divided into two main components:?
A component that builds a chunk tree onthe basis of lexical nodes.?
A component that creates functions ordependencies that connect together distantnodes from the chunk tree.The central goal of this parser is the extraction ofdependencies.
A dependency is a function thatconnects together distant nodes within a chunktree.
The system constructs a dependency be-tween two nodes, if these two nodes are in a spe-cific configuration within the chunk tree or if aspecific set of dependencies has already beenextracted for some of these nodes (see Hagegeand Roux, 2002).
The notion of constraint em-bedded in XIP is both configurational and Boo-lean.
The configuration part is based on treeregular rules which express constraints over nodeconfiguration, while the Boolean constraints areexpressed over dependencies.3.1 Three Level of AnalysisThe parsing is done in three different stages:?
Part-of-speech disambiguation andchunking.?
Dependency Extraction between wordson the basis of sub-tree patterns over thechunk sequence.?
Combination of those dependencies withBoolean operators to generate new de-pendencies, or to modify or delete existingdependencies.3.2 The Different Steps of AnalysisBelow is an example of how a sentence is parsed.We present a little grammar, written in the XIPformalism, together with the output yielded bythese rules.ExampleThe chunking rules produce a chunk tree.In a first stage, chunking rules are applied andthe following chunk tree is built for this sen-tence.Below is a small XIP grammar that can analyzethe above example:1> AP = Adj.2> NP @= Det,(AP),(Noun),Noun.3> FV= verb.4> SC= NP,FV.Each rule is associated with a layer number,which defines the order in which the rules mustbe executed.If this grammar is applied to the above sentence,the result is the following:TOP{SC{NP{The AP{chunking} rules}FV{produce}}NP{a chunk tree}.
}TOP is a node that is automatically created, onceall chunking rules have applied, to transform thissequence of chunks into a tree.
(The ?@?
denotes a longest match strategy.
Therule is then applied to the longest sequence ofcategories found in the linguistic unit)The next step consists of extracting some basicdependencies from this tree.
These dependenciesare obtained with some very basic rules that onlyconnect nodes that occur in a specific sub-treeconfiguration.SUBJ(produce,rules)OBJ(produce,tree)SUBJ is a subject relation, which has been ex-tracted with the following rule:| NP{?
*, noun#1}, FV{?
*,verb#2}|SUBJ(#2,#1).This rule links together the noun and the verbrespectively the sub-nodes of a NP and a VP thatare next to each other.
The ?{?}?
denotes a pat-tern over sub-nodes.Other rules may then be applied to this output,to add or modify existing dependencies.if (SUBJ(#1,#2) & OBJ(#1,#3))TRIPLET(#2,#1,#3).For instance, the above rule will generate a threeslot dependency TRIPLET with the nodes ex-tracted from the subject and object dependencies.If we apply this rule to our previous example, wewill create: TRIPLET(rules,produce,tree).353.3 Script LanguageThe utilization of a script language deeply in-grained into the parser fabric might sound like apure technical gadget with very little influenceon parsing theories.
However, the developmentof a parser often poses some very trivial prob-lems, which we can sum up in the three questionsbelow:?
How can we use previous analyses??
How do we access external information??
How do we control the grammar from anembedding application?Usually, the answer for each of these questionsleads to three different implementations, as noneof these problems seem to have any connectionswhatsoever.
Their only common point seems tobe some extra-programming into the parser en-gine.
If a grammar and a parser are both writtenin the same programming language, the problemis relatively simple to solve.
However, if thegrammar is written in a formalism specificallydesigned for linguistic analysis interpreted with alinguistic compiler (as it is the case for XIP),then any new features that would implementsome of these instructions translate into a modi-fication of the parsing engine itself.
However,one cannot expand the parser engine forever.
Thesolution that has been chosen in XIP is to de-velop a script language, which linguists can useto enrich the original grammatical formalismwith new instructions.3.4 First attemptsThe first attempts to add scripting instructions toXIP consisted in enriching the grammar withnumerical and string variables together withsome instructions to handle these values.
Forinstance, it is possible in XIP to declare a stringvariable, to instantiate it with the lemma value ofa syntactic node and to apply some string modi-fications upon it.
However, the development ofsuch a script language, however useful it proved,became closer and closer to a general-purposeprogramming language, which XIP was not de-signed to be.
The task of developing a full-fledged programming language with a large in-struction set is a complex ongoing process,which has little connection with parsing theories.Nevertheless, there was a need for such an ad-dendum, which led the development team to linkXIP with Python, whose own ongoing develop-ment is backed up by thousands of dedicatedcomputer scientists.3.5 PythonScripting languages have been around for a verylong time.
Thus Perl and Awk have been part ofthe Unix OS for at least twenty years.
Python isalready an old language, in computational timescale.
It has been central to the Linux environ-ment for more than ten years.
Most of the basicinstallation procedures are written in that lan-guage.
It has also been ported to a variety of plat-forms such as Windows or Mac OS.
The lan-guage syntax is close to C, but lacks type verifi-cation.
However, the language is thoroughlydocumented and a large quantity of specializedlibraries is available.
Python has also been cho-sen because of the simplicity of its API, whichallows programmers to link easily a Python en-gine to their own application or to enlarge thelanguage with new libraries.
The other reason ofthis choice, over for instance a more conven-tional language such as C or Java is the fact thatit is an interpreted language.
A XIP grammar is aset of text files, which are all compiled on the flyin memory every time the parser is run.
It stemsfrom this choice that any addenda to this gram-mar should be written in a language that is alsocompiled on the fly.
In this way, the new instruc-tions can be developed in parallel with thegrammar and immediately put in test.
It alsosimplifies the non-trivial task of debugging acomplete grammar as any modifications on anyparts of the grammar can be immediately ex-perimented together with the python script.We have produced two different versions of theXIP-python parsing engine.3.6 Python Embedded within XIPWe have linked the python engine to XIP, whichallows us to call and execute python scripts fromwithin the parsing engine.
In this case, a gram-mar rule can call a python script to verify spe-cific conditions.
The python scripts are then ap-pended to the grammar itself.
These scripts havefull access to all linguistic objects constructed sofar.
XIP is the master program with pythonscripts being triggered by grammar rules.3.7 XIP as a Python LibraryWe have created a specific XIP library which canbe freely imported in python.
In this case, theXIP library exports a basic API, compliant withthe python programming interface, which allowspython developers to benefit from the XIP en-36gine.
The XIP results are then returned as pythonobjects.
Since the purpose in this article is toshow how a grammar formalism can be enrichedwith new instructions, we will mainly concen-trate on the first point.3.8 Interfacing Python and a XIP grammarA XIP grammar mainly handles syntactic nodes,features, categories, and dependencies.
In orderto be efficient, a Python script, called from a XIPgrammar, should have access to all this informa-tion in a simple and natural way.
The notion ofprocedure has already been added to the XIPformalism.
They can be used in any sort of rule.Exampleif (subject(#1,#2) & TestNode(#1))ambiguous(#1).The above rule tests the existence of a subjectdependency and will use the TestNode procedureto check some properties of the #1 node.
If allthese conditions are true, then a new depend-ency: ambiguous is created with #1 as parameter.3.9 InterfaceThe TestNode procedure is declared in a XIPgrammar in the following way:ExamplePython: //XIP field nameTestNode(#1).
//the XIP procedure name, withXIP parameter style.//All that follows is in Pythondef TestNode(node):?The only constraint is that the XIP procedurename (TestNode) should also have been imple-mented as a Python procedure.
If this Pythonprocedure is missing, then the grammar compila-tion fails.The system works as a very simple linker, wherethe code integrity is verified to the presence ofcommon names in XIP and Python.However, the next step, which consists in trans-lating XIP data into Python data, is done at run-time.XIP recognizes many different sorts of data,which can all be transmitted to a Python script,such as syntactic nodes, dependencies, integervariables, string variables, or even vector vari-ables.
Each of these data is then translated intosimple Python variables.
However, the syntacticnodes and the dependencies are not directlytransformed into Python objects; we simplypropagate them into the Python code as integers.Each node and each dependency has a uniqueindex, which simplifies the task of sharing pa-rameters between XIP and Python.3.10 XIP APIPython procedures have access to all internalparsing data through a specific API.
This APIconsists of a dozen instructions, which can becalled anywhere in the Python code.
For in-stance, XIP provides Python instructions to re-turn a node or a dependency object on the basisof its index.
We have implemented the PythonXipNode class, with the following fields:class XipNodeindex #the unique index of the nodePOS #the part of speechLemma #a vector of possible lemmasfor the nodeSurface #the surface form as it appears in the sentencefeatures  #a vector of attribute-valuefeaturesleftoffset,rightoffset  #the text offsetsnext,previous,parent,child # indexesA XipNode object is automatically created whenthe object creator is called with the node index asparameter.
We can also travel through the syn-tactic tree, thanks to the next, previous, parent,child indexes that are provided by this class.There is a big difference between using this APIand exploiting the regular output of a syntacticparser.
Since the Python procedures are called atruntime from the grammar, they have full accessto the on-going linguistic data.
Second, the selec-tion of syntactic nodes on which to apply Pythonprocedures is done at the grammar level, whichmeans that the access of specific nodes is donethrough the parsing engine itself, without anyneed to duplicate any sorts of tree operators,which would be mandatory in the case of a Java,XML or C++ object.
Finally, the memory foot-print is only limited to the nodes that are re-quested by the application, there is no need toreduplicate the whole linguistic data structure.The memory footprint reduction also has the ef-fect of speeding up the execution.373.11 Other Basic InstructionsXIP provides the following Python instructions:?
XipDependency(index) builds a Xip-Dependency object.?
nodeset(POS) returns a vector of nodeindices corresponding to a POS: node-set(?noun?)?
dependencyset(POS) returns a vector ofdependency indices corresponding to adependency name: dependen-cyset(?SUBJECT?)?
dependencyonfirstnode(n) returns avector of dependency indices, whose firstparameter is the node index n: depend-encyonfirstnode(12)These basic instructions make it possible for aPython script to access all internal XIP data atany stages.3.12 An ExampleLet us define the Python code of TestNbSenses,which checks whether a verbal node is highlyambiguous according to WordNet.
As a demon-stration, a verb will be said to be highly ambigu-ous if the number of its senses is larger than 10.def TestNbSenses(i):n=XipNode(i)senses=N[n.lemma].getSenses()if len(senses)>=10:return 1return 0We can now use this procedure in a syntacticrule to test the ambiguity of a verb in order toguide the grammar:if (subject(#1,#2) & TestNbSenses(#1))ambiguous(#1).The dependency ambiguous will be created for averbal node, if this verb is highly ambiguous.4 Back to the Initial QuestionsThe questions we wish to answer are the follow-ing:?
How can we use previous analyses??
How do we access external information??
How do we control the grammar from anembedding application?We have shown in the previous section how newinstructions could be easily defined and thus be-come part of the XIP formalism.
These instruc-tions are mapped to a Python program whichoffers all we need to answer the above questions.4.1 How can we use previous analyses?Since, we have a full access to the internal lin-guistic representation of XIP, we can store what-ever data we might find useful for a given task.For instance, we could decide to count the num-ber of time a word has been detected in thecourse of parsing.
This could be implementedwith a Python dictionary variable.Python:countword(#1).getcount(#1).
?The first procedure countword receives a nodeindex as input.
It translates it into a XipNode,and it uses the lemma as an entry for the Pythondictionary wordcounter.
At the end of the proc-ess, wordcounter contains a list of words withtheir number of occurrences.
The second proce-dure implements a simple test which returns thenumber of time a word has been found.
It returns0, if it is an unknown word.The grammar rule below is used to count words:|Noun#1| {countword(#1);}The instruction |noun#1| automatically loopsbetween all noun nodes.The rule below is used to test if a word has al-ready been found:if (subject(#1,#2) & getcount(#2)) ?4.2 How do we access external information?We have already given an example with Word-Net.
Thanks to the large number of librariesavailable, a Python script can benefit fromWordNet information.
It can also connect to avariety of databases such as MySQL, which alsoallows a grammar to query a database for spe-cific data.For instance, we could store in a database verb-noun couples that have been extracted from a38large corpus.
Then, at runtime, a grammar couldcheck whether a certain verb and a certain nounhave already been found together in anotherdocument.ExamplePython:TestCouple(#1,#2).def TestCouple(v,n):noun=XipNode(n)verb=XipNode(v)cmd=?select * from couples where ?cmd+=?verb=?+verb.lemma+"cmd+=?
and noun=?+noun.lemma+?
;?nb=mysql.execute(cmd)return nbIn the XIP grammar:|FV{verb#1},PP{prep,NP{noun#2}}|if (TestCouple(#1,#2))Complement(#1,#2).If we have a verb followed by a PP, then if wehave already found in a previous analysis a linkbetween the verb and the noun embedded in thePP, we create a dependency Complement overthe verb and the noun.4.3 How do we control the grammar froman embedding application?Since a Python script can exploit any sort of in-put, from text files to databases; it becomes rela-tively simple to implement a simple Python pro-cedure that blocks the execution of certaingrammar rules.
If we examine the above exam-ple, we can see how the grammar execution canbe modified by an external calling program.
Forinstance, the selection of a different database willhave a strong influence on how dependencies areconstructed.5 Expression PowerThe main goal of this article is to describe a wayto articulate no-linguistic constraints with a dedi-cated linguistic formalism.
The notion of con-straint in this perspective does not only apply topurely linguistic properties such as category or-der or dependency building constraints; it isenlarged to encompass properties that are rarelytaken into account in syntactic theories.
It shouldbe noted, however, that if most theories are de-signed to apply to a single sentence, nothing pre-vents these formalisms to benefit from extra-linguistic data through a complex feature systemthat would encode the sentence context.
Howthese features are instantiated is nevertheless outthe realm of these theories.
The originality of oursystem lies in the fact that we intertwine from thebeginning these constraints into the fabric of theformalism.
Since any rules can be governed by aBoolean expression, which in turn can accept anyBoolean python functions, it becomes feasible todefine a formalism in which a constraint is nolonger reduced to only linguistic data, but to anyproperties that a full-fledged programming lan-guage can allow.
Thus, any rule can be con-strained during its application with complex con-straints which are implemented as a python script.Examplepythontest is a generic Boolean python function,which  any XIP rules can embed within its ownset of constraints.Below are some examples of XIP rules, whichare constrained with this generic python function.A constraint in XIP is introduced with the key-word ?if?.?
A chunking rule:PP = prep, NP#1, if (pythontest(#1)).?
A dependency rule:if (subject(#1,#2) & pythontest(#1)) ?However, since any rule might be constrainedwith an external process it should be noted thatthis system can no longer be described as a purelinguistic parser.
Its expression power largelyexceeds what is usually expected from a syntac-tic formalism.6 Implementation ExamplesWe have successfully used Python in our gram-mars in two different applications so far.
Thefirst implementation consists of a script that iscalled at the end of any sentence analysis to storethe results in a MySQL database.
Since the sav-ing is done with a Python program, it is verysimple to modify this script to store only infor-mation that is salient to a particular application.In this respect, the maintenance of such a scriptis much simpler and much flexible than its C++or Java counterpart.
The storage is also done atruntime which limits the amount of data kept inmemory.39The second example is the implementation of aco-reference system (Salah A?t-Mohktar to ap-pear), which uses Python as a backup languageto keep a specific representation of linguistic in-formation that is used at the end of the analysisto link together pronouns and their antecedents.Once again, this program could have been cre-ated in C++ or Java, using the C++ or the JavaXIP API, however, the development of such asystem in python benefits from the simplicity ofthe language itself and its direct bridge to inter-nal XIP representation.7 ConclusionThe integration of a linguistic parser into an ap-plication has always posed some tricky prob-lems.
First, the grammar, whether it has beencompiled into an external library or run throughan interpreter, often works as a black box, whichallows little or no possibility of interfering withthe internal execution.
Second, the output is usu-ally frozen into one single object which forcesthe calling applications to perform format trans-lation afterward.
In many systems (Cunninghamet al,2002, Grinberg et al, 1995), the output isoften a large, complex object, or a large XMLdocument.
This has an impact on both memoryfootprint (these objects might be very large) andthe analysis speed as the system must re-implement some tree operators to traverse theseobjects.
Thereby, the automatic extraction of allnodes that share a common property on the basisof these objects requires some cumbersome pro-gramming, when this could be more elegantlyhandled through the linguistic formalism.
Third,the use of extra-linguistic information often im-poses a modification of the parsing engine itself,which prevents developers from switchingquickly between heterogeneous data sources.
Fora long time, linguistic formalisms have beenconceived as specialized theoretical languageswith little if no algorithmic possibilities.
How-ever, today, the use of syntactic parsers in largeapplications triggers the need for more than justpure linguistic description.
For all these reasons,the integration of a script language as part of theformalism seems a reasonable solution, as it willtransform dedicated linguistic formalisms to lin-guistically driven programming languages.ReferenceGazdar G., Klein E., Pullum G., Sag A. I., 1985.
Gen-eralized Phrase Structure Grammar, Blackwell,Cambridge Mass., Harvard University Press.Pereira F. and S. Shieber, 1987.
Prolog and NaturalLanguage Analysis, CSLI, Chicago UniversityPress.Allen J. F, 1994.
TRAINS Parsing System, NaturalLanguage Understanding, Second Ed., chapters3,4,5.Tapanainen P., J?rvinen T. 1994.
Syntactic analysisof natural language using linguistic rules and cor-pus-based patterns, Proceedings of the 15th con-ference on Computational linguistics, Kyoto, Japan,pages: 629-634.Constant P. 1995.
L'analyseur Linguistique SYLEX, 5?me ?cole d'?t?
du CNET.Grinberg D., Lafferty John, Sleator D., 1995.
A robustparsing algorithm for link grammars, CarnegieMellon University Computer Science technical re-port CMU-CS-95-125, also Proceedings of theFourth International Workshop on Parsing Tech-nologies, Prague, September, 1995.Fellbaum C., 1998.
WordNet: An Electronic LexicalDatabase, Rider University and Princeton Univer-sity, Cambridge, MA: The MIT Press (Language,speech, and communication series), 1998, xxii+423pp; hardbound, ISBN 0-262-06197-X.Roux  C. 1999.
Phrase-Driven Parser,Proceedings ofVEXTALL 99, Venezia, San Servolo, V.I.U.
- 22-24.Blache P., Balfourier J.-M., 2001.
Property Gram-mars: a Flexible Constraint-Based Approach toParsing, in proceedings of IWPT-2001.A?t-Mokhtar S., Chanod J-P., Roux C., 2002.
Robust-ness beyond shallowness incremental dependencyparsing, NLE Journal, 2002.Hag?ge C., Roux C.,2002.
A Robust And FlexiblePlatform for Dependency Extraction, in proceed-ings of LREC 2002.Declerck T. 2002, A set of tools for integrating lin-guistic and non-linguistic information, Proceedingsof SAAKM.H.
Cunningham, D. Maynard, K. Bontcheva, V. Tab-lan.,2002.
GATE: A Framework and GraphicalDevelopment Environment for Robust NLP Toolsand Applications, Proceedings of the 40th Anni-versary Meeting of the Association for Computa-tional Linguistics (ACL'02), Philadelphia, July2002.Blache P., Gu?not M-L. 2003.
Flexible Corpus Anno-tation with Property Grammars, BulTreeBank Pro-jectRoux C., 2004.
Une Grammaire XML, TALN Confe-rence, Fez, Morocco, April, 19-22, 2004.
[Python] http://www.python.org/40
