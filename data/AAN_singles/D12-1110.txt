Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1201?1211, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsSemantic Compositionality through Recursive Matrix-Vector SpacesRichard Socher Brody Huval Christopher D. Manning Andrew Y. Ngrichard@socher.org, {brodyh,manning,ang}@stanford.eduComputer Science Department, Stanford UniversityAbstractSingle-word vector space models have beenvery successful at learning lexical informa-tion.
However, they cannot capture the com-positional meaning of longer phrases, prevent-ing them from a deeper understanding of lan-guage.
We introduce a recursive neural net-work (RNN) model that learns compositionalvector representations for phrases and sen-tences of arbitrary syntactic type and length.Our model assigns a vector and a matrix to ev-ery node in a parse tree: the vector capturesthe inherent meaning of the constituent, whilethe matrix captures how it changes the mean-ing of neighboring words or phrases.
Thismatrix-vector RNN can learn the meaning ofoperators in propositional logic and naturallanguage.
The model obtains state of the artperformance on three different experiments:predicting fine-grained sentiment distributionsof adverb-adjective pairs; classifying senti-ment labels of movie reviews and classifyingsemantic relationships such as cause-effect ortopic-message between nouns using the syn-tactic path between them.1 IntroductionSemantic word vector spaces are at the core of manyuseful natural language applications such as searchquery expansions (Jones et al 2006), fact extrac-tion for information retrieval (Pas?ca et al 2006)and automatic annotation of text with disambiguatedWikipedia links (Ratinov et al 2011), among manyothers (Turney and Pantel, 2010).
In these mod-els the meaning of a word is encoded as a vectorcomputed from co-occurrence statistics of a wordand its neighboring words.
Such vectors have beenshown to correlate well with human judgments ofword similarity (Griffiths et al 2007).?
very                        good                           movie          ...       (  a  ,  A  )                (  b  ,  B  )                     (  c  ,  C  )Recursive Matrix-Vector Modelf(Ba, Ab)=Ba=                        Ab=- vector- matrix...?Figure 1: A recursive neural network which learns se-mantic vector representations of phrases in a tree struc-ture.
Each word and phrase is represented by a vectorand a matrix, e.g., very = (a,A).
The matrix is appliedto neighboring vectors.
The same function is repeated tocombine the phrase very good with movie.Despite their success, single word vector modelsare severely limited since they do not capture com-positionality, the important quality of natural lan-guage that allows speakers to determine the meaningof a longer expression based on the meanings of itswords and the rules used to combine them (Frege,1892).
This prevents them from gaining a deeperunderstanding of the semantics of longer phrases orsentences.
Recently, there has been much progressin capturing compositionality in vector spaces, e.g.,(Mitchell and Lapata, 2010; Baroni and Zamparelli,2010; Zanzotto et al 2010; Yessenalina and Cardie,2011; Socher et al 2011c) (see related work).
Weextend these approaches with a more general andpowerful model of semantic composition.We present a novel recursive neural networkmodel for semantic compositionality.
In our context,compositionality is the ability to learn compositionalvector representations for various types of phrasesand sentences of arbitrary length.
Fig.
1 shows anillustration of the model in which each constituent(a word or longer phrase) has a matrix-vector (MV)1201representation.
The vector captures the meaning ofthat constituent.
The matrix captures how it modifiesthe meaning of the other word that it combines with.A representation for a longer phrase is computedbottom-up by recursively combining the words ac-cording to the syntactic structure of a parse tree.Since the model uses the MV representation with aneural network as the final merging function, we callour model a matrix-vector recursive neural network(MV-RNN).We show that the ability to capture semantic com-positionality in a syntactically plausible way trans-lates into state of the art performance on varioustasks.
The first experiment demonstrates that ourmodel can learn fine-grained semantic composition-ality.
The task is to predict a sentiment distributionover movie reviews of adverb-adjective pairs such asunbelievably sad or really awesome.
The MV-RNNis the only model that is able to properly negate sen-timent when adjectives are combined with not.
TheMV-RNN outperforms previous state of the art mod-els on full sentence sentiment prediction of moviereviews.
The last experiment shows that the MV-RNN can also be used to find relationships betweenwords using the learned phrase vectors.
The rela-tionship between words is recursively constructedand composed by words of arbitrary type in thevariable length syntactic path between them.
Onthe associated task of classifying relationships be-tween nouns in arbitrary positions of a sentence themodel outperforms all previous approaches on theSemEval-2010 Task 8 competition (Hendrickx et al2010).
It outperforms all but one of the previous ap-proaches without using any hand-designed semanticresources such as WordNet or FrameNet.
By addingWordNet hypernyms, POS and NER tags our modeloutperforms the state of the art that uses significantlymore resources.
The code for our model is availableat www.socher.org.2 MV-RNN: A Recursive Matrix-VectorModelThe dominant approach for building representationsof multi-word units from single word vector repre-sentations has been to form a linear combination ofthe single word representations, such as a sum orweighted average.
This happens in information re-trieval and in various text similarity functions basedon lexical similarity.
These approaches can workwell when the meaning of a text is literally ?the sumof its parts?, but fails when words function as oper-ators that modify the meaning of another word: themeaning of ?extremely strong?
cannot be capturedas the sum of word representations for ?extremely?and ?strong.
?The model of Socher et al(2011c) provided anew possibility for moving beyond a linear combi-nation, through use of a matrix W that multipliedthe word vectors (a, b), and a nonlinearity functiong (such as a sigmoid or tanh).
They compute theparent vector p that describes both words asp = g(W[ab])(1)and apply this function recursively inside a binarizedparse tree so that it can compute vectors for multi-word sequences.
Even though the nonlinearity al-lows to express a wider range of functions, it is al-most certainly too much to expect a single fixed Wmatrix to be able to capture the meaning combina-tion effects of all natural language operators.
Afterall, inside the function g, we have the same lineartransformation for all possible pairs of word vectors.Recent work has started to capture the behaviorof natural language operators inside semantic vec-tor spaces by modeling them as matrices, whichwould allow a matrix for ?extremely?
to appropri-ately modify vectors for ?smelly?
or ?strong?
(Ba-roni and Zamparelli, 2010; Zanzotto et al 2010).These approaches are along the right lines but sofar have been restricted to capture linear functionsof pairs of words whereas we would like nonlinearfunctions to compute compositional meaning repre-sentations for multi-word phrases or full sentences.The MV-RNN combines the strengths of both ofthese ideas by (i) assigning a vector and a matrix toevery word and (ii) learning an input-specific, non-linear, compositional function for computing vectorand matrix representations for multi-word sequencesof any syntactic type.
Assigning vector-matrix rep-resentations to all words instead of only to words ofone part of speech category allows for greater flex-ibility which benefits performance.
If a word lacksoperator semantics, its matrix can be an identity ma-trix.
However, if a word acts mainly as an operator,1202such as ?extremely?, its vector can become close tozero, while its matrix gains a clear operator mean-ing, here magnifying the meaning of the modifiedword in both positive and negative directions.In this section we describe the initial word rep-resentations, the details of combining two words aswell as the multi-word extensions.
This is followedby an explanation of our training procedure.2.1 Matrix-Vector Neural Word RepresentationWe represent a word as both a continuous vectorand a matrix of parameters.
We initialize all wordvectors x ?
Rn with pre-trained 50-dimensionalword vectors from the unsupervised model of Col-lobert and Weston (2008).
Using Wikipedia text,their model learns word vectors by predicting howlikely it is for each word to occur in its context.
Sim-ilar to other local co-occurrence based vector spacemodels, the resulting word vectors capture syntacticand semantic information.
Every word is also asso-ciated with a matrix X .
In all experiments, we ini-tialize matrices as X = I+ , i.e., the identity plus asmall amount of Gaussian noise.
If the vectors havedimensionality n, then each word?s matrix has di-mensionality X ?
Rn?n.
While the initialization israndom, the vectors and matrices will subsequentlybe modified to enable a sequence of words to com-pose a vector that can predict a distribution over se-mantic labels.
Henceforth, we represent any phraseor sentence of length m as an ordered list of vector-matrix pairs ((a,A), .
.
.
, (m,M)), where each pairis retrieved based on the word at that position.2.2 Composition Models for Two WordsWe first review composition functions for twowords.
In order to compute a parent vector p fromtwo consecutive words and their respective vectorsa and b, Mitchell and Lapata (2010) give as theirmost general function: p = f(a, b, R,K),where Ris the a-priori known syntactic relation and K isbackground knowledge.There are many possible functions f .
For ourmodels, there is a constraint on p which is that ithas the same dimensionality as each of the inputvectors.
This way, we can compare p easily withits children and p can be the input to a compositionwith another word.
The latter is a requirement thatwill become clear in the next section.
This excludestensor products which were outperformed by sim-pler weighted addition and multiplication methodsin (Mitchell and Lapata, 2010).We will explore methods that do not requireany manually designed semantic resources as back-ground knowledge K. No explicit knowledge aboutthe type of relation R is used.
Instead we want themodel to capture this implicitly via the learned ma-trices.
We propose the following combination func-tion which is input dependent:p = fA,B(a, b) = f(Ba,Ab) = g(W[BaAb]),(2)whereA,B are matrices for single words, the globalW ?
Rn?2n is a matrix that maps both transformedwords back into the same n-dimensional space.
Theelement-wise function g could be simply the identityfunction but we use instead a nonlinearity such asthe sigmoid or hyperbolic tangent tanh.
Such a non-linearity will allow us to approximate a wider rangeof functions beyond purely linear functions.
We canalso add a bias term before applying g but omit thisfor clarity.
Rewriting the two transformed vectors asone vector z, we get p = g(Wz) which is a singlelayer neural network.
In this model, the word ma-trices can capture compositional effects specific toeach word, whereas W captures a general composi-tion function.This function builds upon and generalizes severalrecent models in the literature.
The most relatedwork is that of (Mitchell and Lapata, 2010; Zan-zotto et al 2010) who introduced and explored thecomposition function p = Ba + Ab for word pairs.This model is a special case of Eq.
2 when we setW = [II] (i.e.
two concatenated identity matri-ces) and g(x) = x (the identity function).
Baroniand Zamparelli (2010) computed the parent vectorof adjective-noun pairs by p = Ab, where A is anadjective matrix and b is a vector for a noun.
Thiscannot capture nouns modifying other nouns, e.g.,disk drive.
This model too is a special case of theabove model with B = 0n?n.
Lastly, the models of(Socher et al 2011b; Socher et al 2011c; Socher etal., 2011a) as described above are also special caseswith bothA andB set to the identity matrix.
We willcompare to these special cases in our experiments.1203???????????????
(a , A)     (b , B)    (c , C)Matrix-Vector Recursive Neural Network(p1 , P1)( p2, P2  ) p2  = g(W                )P2 =  WMCp1 P1c[   ]P1C[   ]Figure 2: Example of how the MV-RNN merges a phrasewith another word at a nonterminal node of a parse tree.2.3 Recursive Compositions of Multiple Wordsand PhrasesThis section describes how we extend a word-pairmatrix-vector-based compositional model to learnvectors and matrices for longer sequences of words.The main idea is to apply the same function f topairs of constituents in a parse tree.
For this towork, we need to take as input a binary parse treeof a phrase or sentence and also compute matrices ateach nonterminal parent node.
The function f canbe readily used for phrase vectors since it is recur-sively compatible (p has the same dimensionality asits children).
For computing nonterminal phrase ma-trices, we define the functionP = fM (A,B) = WM[AB], (3)where WM ?
Rn?2n, so P ?
Rn?n just like eachinput matrix.After two words form a constituent in the parsetree, this constituent can now be merged with an-other one by applying the same functions f andfM .
For instance, to compute the vectors and ma-trices depicted in Fig.
2, we first merge words aand b and their matrices: p1 = f(Ba,Ab), P1 =fM (A,B).
The resulting vector-matrix pair (p1, P1)can now be used to compute the full phrase whencombining it with word c and computing p2 =f(Cp1, P1c), P2 = fM (P1, C).
The model com-putes vectors and matrices in a bottom-up fashion,applying the functions f, fM to its own previous out-put (i.e.
recursively) until it reaches the top node ofthe tree which represents the entire sentence.For experiments with longer sequences we willcompare to standard RNNs and the special case ofthe MV-RNN that computes the parent by p = Ab+Ba, which we name the linear Matrix-Vector Re-cursion model (linear MVR).
Previously, this modelhad not been trained for multi-word sequences.
Sec.6 talks about alternatives for compositionality.2.4 Objective Functions for TrainingOne of the advantages of RNN-based models is thateach node of a tree has associated with it a dis-tributed vector representation (the parent vector p)which can also be seen as features describing thatphrase.
We train these representations by adding ontop of each parent node a simple softmax classifierto predict a class distribution over, e.g., sentiment orrelationship classes: d(p) = softmax(W labelp).
Ifthere are K labels, then d ?
RK is a K-dimensionalmultinomial distribution.
For the applications below(excluding logic), the corresponding error functionE(s, t, ?)
that we minimize for a sentence s and itstree t is the sum of cross-entropy errors at all nodes.The only other methods that use this type of ob-jective function are (Socher et al 2011b; Socheret al 2011c), who also combine it with either ascore or reconstruction error.
Hence, for compar-isons to other related work, we need to merge vari-ations of computing the parent vector p with thisclassifier.
The main difference is that the MV-RNNhas more flexibility since it has an input specific re-cursive function fA,B to compute each parent.
Inthe following applications, we will use the softmaxclassifier to predict both sentiment distributions andnoun-noun relationships.2.5 LearningLet ?
= (W,WM ,W label, L, LM ) be our model pa-rameters and ?
a vector with regularization hyperpa-rameters for all model parameters.
L andLM are thesets of all word vectors and word matrices.
The gra-dient of the overall objective function J becomes:?J??=1N?
(x,t)?E(x, t; ?)?
?+ ??.
(4)To compute this gradient, we first compute all treenodes (pi, Pi) from the bottom-up and then takederivatives of the softmax classifiers at each nodein the tree from the top down.
Derivatives are com-puted efficiently via backpropagation through struc-ture (Goller and Ku?chler, 1996).
Even though the1204objective is not convex, we found that L-BFGS runover the complete training data (batch mode) mini-mizes the objective well in practice and convergenceis smooth.
For more information see (Socher et al2010).2.6 Low-Rank Matrix ApproximationsIf every word is represented by an n-dimensionalvector and additionally by an n ?
n matrix, the di-mensionality of the whole model may become toolarge with commonly used vector sizes of n = 100.In order to reduce the number of parameters, we rep-resent word matrices by the following low-rank plusdiagonal approximation:A = UV + diag(a), (5)where U ?
Rn?r, V ?
Rr?n, a ?
Rn and we setthe rank for all experiments to r = 3.2.7 Discussion: Evaluation and GeneralityEvaluation of compositional vector spaces is a com-plex task.
Most related work compares similarityjudgments of unsupervised models to those of hu-man judgments and aims at high correlation.
Theseevaluations can give important insights.
However,even with good correlation the question remainshow these models would perform on downstreamNLP tasks such as sentiment detection.
We ex-perimented with unsupervised learning of generalvector-matrix representations by having the MV-RNN predict words in their correct context.
Ini-tializing the models with these general representa-tions, did not improve the performance on the taskswe consider.
For sentiment analysis, this is not sur-prising since antonyms often get similar vectors dur-ing unsupervised learning from co-occurrences dueto high similarity of local syntactic contexts.
In ourexperiments, the high prediction performance camefrom supervised learning of meaning representationsusing labeled data.
While these representations aretask-specific, they could be used across tasks in amulti-task learning setup.
However, in order to fairlycompare to related work, we use only the super-vised data of each task.
Before we describe our full-scale experiments, we analyze the model?s expres-sive powers.3 Model AnalysisThis section analyzes the model with two proof-of-concept studies.
First, we examine its ability to learnoperator semantics for adverb-adjective pairs.
If amodel cannot correctly capture how an adverb op-erates on the meaning of adjectives, then there?s lit-tle chance it can learn operators for more complexrelationships.
The second study analyzes whetherthe MV-RNN can learn simple boolean operators ofpropositional logic such as conjunctives or negationfrom truth values.
Again, if a model did not have thisability, then there?s little chance it could learn thesefrequently occurring phenomena from the noisy lan-guage of real texts such as movie reviews.3.1 Predicting Sentiment Distributions ofAdverb-Adjective PairsThe first study considers the prediction of fine-grained sentiment distributions of adverb-adjectivepairs and analyzes different possibilities for com-puting the parent vector p. The results show thatthe MV-RNN operators are powerful enough to cap-ture the operational meanings of various types of ad-verbs.
For example, very is an intensifier, pretty is anattenuator, and not can negate or strongly attenuatethe positivity of an adjective.
For instance not greatis still pretty good and not terrible; see Potts (2010)for details.We use a publicly available IMDB dataset of ex-tracted adverb-adjective pairs from movie reviews.1The dataset provides the distribution over star rat-ings: Each consecutive word pair appears a certainnumber of times in reviews that have also associ-ated with them an overall rating of the movie.
Afternormalizing by the total number of occurrences, onegets a multinomial distribution over ratings.
Onlyword pairs that appear at least 50 times are kept.
Ofthe remaining pairs, we use 4211 randomly sampledones for training and a separate set of 1804 for test-ing.
We never give the algorithm sentiment distribu-tions for single words, and, while single words over-lap between training and testing, the test set consistsof never before seen word pairs.The softmax classifier is trained to minimize thecross entropy error.
Hence, an evaluation in terms ofKL-divergence is the most reasonable choice.
It is1http://compprag.christopherpotts.net/reviews.html1205Method Avg KLUniform 0.327Mean train 0.193p = 12(a+ b) 0.103p = a?
b 0.103p = [a; b] 0.101p = Ab 0.103RNN 0.093Linear MVR 0.092MV-RNN 0.0911 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 fairly annoyingMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 fairly awesomeMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 fairly sadMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 not annoyingMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 not awesomeMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 not sadTraining Pair1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 unbelievably annoyingMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 unbelievably awesomeMV?RNNRNN1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5 unbelievably sadMV?RNNRNNFigure 3: Left: Average KL-divergence for predicting sentiment distributions of unseen adverb-adjective pairs of thetest set.
See text for p descriptions.
Lower is better.
The main difference in the KL divergence comes from the fewnegation pairs in the test set.
Right: Predicting sentiment distributions (over 1-10 stars on the x-axis) of adverb-adjective pairs.
Each row has the same adverb and each column the same adjective.
Many predictions are similarbetween the two models.
The RNN and linear MVR are not able to modify the sentiment correctly: not awesome ismore positive than fairly awesome and not annoying has a similar shape as unbelievably annoying.
Predictions of thelinear MVR model are almost identical to the standard RNN for these examples.defined as KL(g||p) =?i gi log(gi/pi), where g isthe gold distribution and p is the predicted one.We compare to several baselines and ablations ofthe MV-RNN model.
An (adverb,adjective) pair isdescribed by its vectors (a, b) and matrices (A,B).1 p = 0.5(a+ b), vector average2.
p = a?
b, element-wise vector multiplication3.
p = [a; b], vector concatenation4.
p = Ab, similar to (Baroni and Lenci, 2010)5. p = g(W [a; b]), RNN, similar to Socher et al6.
p = Ab+Ba, Linear MVR, similar to (Mitchelland Lapata, 2010; Zanzotto et al 2010)7. p = g(W [Ba;Ab]), MV-RNNThe final distribution is always predicted by asoftmax classifier whose inputs p vary for each ofthe models.
This objective function (see Sec.
2.4)is different to all previously published work exceptthat of (Socher et al 2011c).We cross-validated all models over regulariza-tion parameters for word vectors, the softmax clas-sifier, the RNN parameter W and the word op-erators (10?4, 10?3) and word vector sizes (n =6, 8, 10, 12, 15, 20).
All models performed best atvector sizes of below 12.
Hence, it is the model?spower and not the number of parameters that deter-mines the performance.
The table in Fig.
3 showsthe average KL-divergence on the test set.
It showsthat the idea of matrix-vector representations for allwords and having a nonlinearity are both impor-tant.
The MV-RNN which combines these two ideasis best able to learn the various compositional ef-fects.
The main difference in KL divergence comesfrom the few negation cases in the test set.
Fig.
3shows examples of predicted distributions.
Manyof the predictions are accurate and similar betweenthe top models.
However, only the MV-RNN hasenough expressive power to allow negation to com-pletely shift the sentiment with respect to an adjec-tive.
A negated adjective carrying negative senti-ment becomes slightly positive, whereas not awe-some is correctly attenuated.
All three top modelscorrectly capture the U-shape of unbelievably sad.This pair peaks at both the negative and positivespectrum because it is ambiguous.
When referringto the performance of actors, it is very negative, but,when talking about the plot, many people enjoy sadand thought-provoking movies.
The p = Ab modeldoes not perform well because it cannot model thefact that for an adjective like ?sad,?
the operator of?unbelievably?
behaves differently.1206falsefalse?
falsefalsetrue?
falsefalsefalse?
truetruetrue?
truetrue?
falsefalse?
trueFigure 4: Training trees for the MV-RNN to learn propositional operators.
The model learns vectors and operators for?
(and) and ?
(negation).
The model outputs the exact representations of false and true respectively at the top node.Hence, the operators can be combined recursively an arbitrary number of times for more complex logical functions.3.2 Logic- and Vector-based CompositionalityAnother natural question is whether the MV-RNNcan, in general, capture some of the simple booleanlogic that is sometimes found in language.
In otherwords, can it learn some of the propositional logicoperators such as and, or, not in terms of vectors andmatrices from a few examples.
Answering this ques-tion can also be seen as a first step towards bridg-ing the gap between logic-based, formal semantics(Montague, 1974) and vector space models.The logic-based view of language accounts nicelyfor compositionality by directly mapping syntac-tic constituents to lambda calculus expressions.
Atthe word level, the focus is on function words, andnouns and adjectives are often defined only in termsof the sets of entities they denote in the world.
Mostwords are treated as atomic symbols with no rela-tion to each other.
There have been many attemptsat automatically parsing natural language to a logi-cal form using recursive compositional rules.Conversely, vector space models have the attrac-tive property that they can automatically extractknowledge from large corpora without supervision.Unlike logic-based approaches, these models allowus to make fine-grained statements about the seman-tic similarity of words which correlate well with hu-man judgments (Griffiths et al 2007).
Logic-basedapproaches are often seen as orthogonal to distribu-tional vector-based approaches.
However, Garretteet al(2011) recently introduced a combination of avector space model inside a Markov Logic Network.One open question is whether vector-based mod-els can learn some of the simple logic encounteredin language such as negation or conjunctives.
Tothis end, we illustrate in a simple example that ourMV-RNN model and its learned word matrices (op-erators) have the ability to learn propositional logicoperators such as ?,?,?
(and, or, not).
This is anecessary (though not sufficient) condition for theability to pick up these phenomena in real datasetsand tasks such as sentiment detection which we fo-cus on in the subsequent sections.Our setup is as follows.
We train on 6 strictlyright-branching trees as in Fig.
4.
We consider the 1-dimensional case and fix the representation for trueto (t = 1, T = 1) and false to (f = 0, F = 1).Fixing the operators to the 1 ?
1 identity matrix 1is essentially ignoring them.
The objective is thento create a perfect reconstruction of (t, T ) or (f, F )(depending on the formula), which we achieve bythe least squares error between the top vector?s rep-resentation and the corresponding truth value, e.g.for ?false: min ||ptop ?
t||2 + ||Ptop ?
T ||2.As our function g (see Eq.
2), we use a linearthreshold unit: g(x) = max(min(x, 1), 0).
Givingthe derivatives computed for the objective functionfor the examples in Fig.
4 to a standard L-BFGS op-timizer quickly yields a training error of 0.
Hence,the output of these 6 examples has exactly one of thetruth representations, making it recursively compati-ble with further combinations of operators.
Thus, wecan combine these operators to construct any propo-sitional logic function of any number of inputs (in-cluding xor).
Hence, this MV-RNN is complete interms of propositional logic.4 Predicting Movie Review RatingsIn this section, we analyze the model?s performanceon full length sentences.
We compare to previousstate of the art methods on a standard benchmarkdataset of movie reviews (Pang and Lee, 2005; Nak-agawa et al 2010; Socher et al 2011c).
Thisdataset consists of 10,000 positive and negative sin-gle sentences describing movie sentiment.
In thisand the next experiment we use binarized trees fromthe Stanford Parser (Klein and Manning, 2003).
Weuse the exact same setup and parameters (regulariza-tion, word vector size, etc.)
as the published code ofSocher et al(2011c).22www.socher.org1207Method Acc.Tree-CRF (Nakagawa et al 2010) 77.3RAE (Socher et al 2011c) 77.7Linear MVR 77.1MV-RNN 79.0Table 1: Accuracy of classification on full length moviereview polarity (MR).S.
C. Review sentence1?The film is bright and flashy in all the right ways.0?Not always too whimsical for its own good thisstrange hybrid of crime thriller, quirky characterstudy, third-rate romance and female empowermentfantasy never really finds the tonal or thematic glueit needs.0?Doesn?t come close to justifying the hype that sur-rounded its debut at the Sundance film festival twoyears ago.0 x Director Hoffman, his writer and Kline?s agentshould serve detention.1 x A bodice-ripper for intellectuals.Table 2: Hard movie review examples of positive (1) andnegative (0) sentiment (S.) that of all methods only theMV-RNN predicted correctly (C:?)
or could not classifyas correct either (C: x).Table 1 shows comparisons to the system of (Nak-agawa et al 2010), a dependency tree based classifi-cation method that uses CRFs with hidden variables.The state of the art recursive autoencoder model ofSocher et al(2011c) obtained 77.7% accuracy.
Ournew MV-RNN gives the highest performance, out-performing also the linear MVR (Sec.
2.2).Table 2 shows several hard examples that only theMV-RNN was able to classify correctly.
None of themethods correctly classified the last two exampleswhich require more world knowledge.5 Classification of Semantic RelationshipsThe previous task considered global classification ofan entire phrase or sentence.
In our last experimentwe show that the MV-RNN can also learn how a syn-tactic context composes an aggregate meaning of thesemantic relationships between words.
In particular,the task is finding semantic relationships betweenpairs of nominals.
For instance, in the sentence?My [apartment]e1 has a pretty large [kitchen]e2.
?,we want to predict that the kitchen and apartment arein a component-whole relationship.
Predicting such???
?[ m o v i e ]  s h o w e d  [ w ar s ]     ?MV - R N N  f o r  R e l at i o n s h i p Cl as s i f i cat i o n?
?Cl as s i f i e r :  Me s s age - T o pi cFigure 5: The MV-RNN learns vectors in the path con-necting two words (dotted lines) to determine their se-mantic relationship.
It takes into consideration a variablelength sequence of various word types in that path.semantic relations is useful for information extrac-tion and thesaurus construction applications.
Manyapproaches use features for all words on the pathbetween the two words of interest.
We show thatby building a single compositional semantics for theminimal constituent including both terms one canachieve a higher performance.This task requires the ability to deal with se-quences of words of arbitrary type and length in be-tween the two nouns in question.Fig.
5 explains ourmethod for classifying nominal relationships.
Wefirst find the path in the parse tree between the twowords whose relation we want to classify.
We thenselect the highest node of the path and classify therelationship using that node?s vector as features.
Weapply the same type of MV-RNN model as in senti-ment to the subtree spanned by the two words.We use the dataset and evaluation frameworkof SemEval-2010 Task 8 (Hendrickx et al 2010).There are 9 ordered relationships (with two direc-tions) and an undirected other class, resulting in19 classes.
Among the relationships are: message-topic, cause-effect, instrument-agency (etc.
see Ta-ble 3 for list).
A pair is counted as correct if theorder of the words in the relationship is correct.Table 4 lists results for several competing meth-ods together with the resources and features usedby each method.
We compare to the systems ofthe competition which are described in Hendrickxet al(2010) as well as the RNN and linear MVR.Most systems used a considerable amount of hand-designed semantic resources.
In contrast to thesemethods, the MV-RNN only needs a parser for thetree structure and learns all semantics from unla-beled corpora and the training data.
Only the Se-mEval training dataset is specific to this task, the re-1208Relationship Sentence with labeled nouns for which to predict relationshipsCause-Effect(e2,e1) Avian [influenza]e1 is an infectious disease caused by type a strains of the influenza [virus]e2.Entity-Origin(e1,e2) The [mother]e1 left her native [land]e2 about the same time and they were married in that city.Message-Topic(e2,e1) Roadside [attractions]e1 are frequently advertised with [billboards]e2 to attract tourists.Product-Producer(e1,e2) A child is told a [lie]e1 for several years by their [parents]e2 before he/she realizes that ...Entity-Destination(e1,e2) The accident has spread [oil]e1 into the [ocean]e2.Member-Collection(e2,e1) The siege started, with a [regiment]e1 of lightly armored [swordsmen]e2 ramming down the gate.Instrument-Agency(e2,e1) The core of the [analyzer]e1 identifies the paths using the constraint propagation [method]e2.Component-Whole(e2,e1) The size of a [tree]e1 [crown]e2 is strongly correlated with the growth of the tree.Content-Container(e1,e2) The hidden [camera]e1, found by a security guard, was hidden in a business card-sized [leafletbox]e2 placed at an unmanned ATM in Tokyo?s Minato ward in early September.Table 3: Examples of correct classifications of ordered, semantic relations between nouns by the MV-RNN.
Note thatthe final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketedwords.
The paths vary in length and the words vary in type.Classifier Feature Sets F1SVM POS, stemming, syntactic patterns 60.1SVM word pair, words in between 72.5SVM POS, WordNet, stemming, syntacticpatterns74.8SVM POS, WordNet, morphological fea-tures, thesauri, Google n-grams77.6MaxEnt POS, WordNet, morphological fea-tures, noun compound system, the-sauri, Google n-grams77.6SVM POS, WordNet, prefixes and othermorphological features, POS, depen-dency parse features, Levin classes,PropBank, FrameNet, NomLex-Plus,Google n-grams, paraphrases, Tex-tRunner82.2RNN - 74.8Lin.MVR - 73.0MV-RNN - 79.1RNN POS,WordNet,NER 77.6Lin.MVR POS,WordNet,NER 78.7MV-RNN POS,WordNet,NER 82.4Table 4: Learning methods, their feature sets and F1results for predicting semantic relations between nouns.The MV-RNN outperforms all but one method withoutany additional feature sets.
By adding three such features,it obtains state of the art performance.maining inputs and the training setup are the sameas in previous sentiment experiments.The best method on this dataset (Rink andHarabagiu, 2010) obtains 82.2% F1.
In order tosee whether our system can improve over this sys-tem, we added three features to the MV-RNN vec-tor and trained another softmax classifier.
The fea-tures and their performance increases were POS tags(+0.9); WordNet hypernyms (+1.3) and named en-tity tags (NER) of the two words (+0.6).
Featureswere computed using the code of Ciaramita and Al-tun (2006).3 With these features, the performanceimproved over the state of the art system.
Table 3shows random correct classification examples.6 Related workDistributional approaches have become omnipresentfor the recognition of semantic similarity betweenwords and the treatment of compositionality hasseen much progress in recent years.
Hence, we can-not do justice to the large amount of literature.
Com-monly, single words are represented as vectors ofdistributional characteristics ?
e.g., their frequenciesin specific syntactic relations or their co-occurrenceswith given context words (Pado and Lapata, 2007;Baroni and Lenci, 2010; Turney and Pantel, 2010).These representations have proven very effective insense discrimination and disambiguation (Schu?tze,1998), automatic thesaurus extraction (Lin, 1998;Curran, 2004) and selectional preferences.There are several sophisticated ideas for com-positionality in vector spaces.
Mitchell and Lap-ata (2010) present an overview of the most impor-tant compositional models, from simple vector ad-dition and component-wise multiplication to tensorproducts, and convolution (Metcalfe, 1990).
Theymeasured the similarity between word pairs suchas compound nouns or verb-object pairs and com-pared these with human similarity judgments.
Sim-ple vector averaging or multiplication performedbest, hence our focus on related baselines above.3sourceforge.net/projects/supersensetag/1209Other important models are tensor products (Clarkand Pulman, 2007), quantum logic (Widdows,2008), holographic reduced representations (Plate,1995) and the Compositional Matrix Space model(Rudolph and Giesbrecht, 2010).
RNNs are relatedto autoencoder models such as the recursive autoas-sociative memory (RAAM) (Pollack, 1990) or recur-rent neural networks (Elman, 1991).
Bottou (2011)and Hinton (1990) discussed related models such asrecursive autoencoders for text understanding.Our model builds upon and generalizes the mod-els of (Mitchell and Lapata, 2010; Baroni and Zam-parelli, 2010; Zanzotto et al 2010; Socher et al2011c) (see Sec.
2.2).
We compare to them inour experiments.
Yessenalina and Cardie (2011) in-troduce a sentiment analysis model that describeswords as matrices and composition as matrix mul-tiplication.
Since matrix multiplication is associa-tive, this cannot capture different scopes of nega-tion or syntactic differences.
Their model, is a spe-cial case of our encoding model (when you ignorevectors, fix the tree to be strictly branching in onedirection and use as the matrix composition func-tion P = AB).
Since our classifiers are trained onthe vectors, we cannot compare to this approach di-rectly.
Grefenstette and Sadrzadeh (2011) learn ma-trices for verbs in a categorical model.
The trainedmatrices improve correlation with human judgmentson the task of identifying relatedness of subject-verb-object triplets.7 ConclusionWe introduced a new model towards a completetreatment of compositionality in word vector spaces.Our model builds on a syntactically plausible parsetree and can handle compositional phenomena.
Themain novelty of our model is the combination ofmatrix-vector representations with a recursive neu-ral network.
It can learn both the meaning vectors ofa word and how that word modifies its neighbors (viaits matrix).
The MV-RNN combines attractive the-oretical properties with good performance on large,noisy datasets.
It generalizes several models in theliterature, can learn propositional logic, accuratelypredicts sentiment and can be used to classify se-mantic relationships between nouns in a sentence.AcknowledgmentsWe thank for great discussions about the paper:John Platt, Chris Potts, Josh Tenenbaum, Mihai Sur-deanu, Quoc Le and Kevin Miller.
The authorsgratefully acknowledges the support of the DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181, and the DARPA Deep Learning programunder contract number FA8650-10-C-7020.
Anyopinions, findings, and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the view ofDARPA, AFRL, or the US government.ReferencesM.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36(4):673?721.M.
Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InEMNLP.L.
Bottou.
2011.
From machine learning to machinereasoning.
CoRR, abs/1102.1808.M.
Ciaramita and Y. Altun.
2006.
Broad-coverage sensedisambiguation and information extraction with a su-persense sequence tagger.
In EMNLP.S.
Clark and S. Pulman.
2007.
Combining symbolic anddistributional models of meaning.
In Proceedings ofthe AAAI Spring Symposium on Quantum Interaction,pages 52?55.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: deep neural networkswith multitask learning.
In ICML.J.
Curran.
2004.
From Distributional to Semantic Simi-larity.
Ph.D. thesis, University of Edinburgh.J.
L. Elman.
1991.
Distributed representations, simplerecurrent networks, and grammatical structure.
Ma-chine Learning, 7(2-3).G.
Frege.
1892.
U?ber Sinn und Bedeutung.
In Zeitschriftfu?r Philosophie und philosophische Kritik, 100.D.
Garrette, K. Erk, and R. Mooney.
2011.
Integrat-ing Logical Representations with Probabilistic Infor-mation using Markov Logic.
In Proceedings of the In-ternational Conference on Computational Semantics.C.
Goller and A. Ku?chler.
1996.
Learning task-dependent distributed representations by backpropaga-tion through structure.
In Proceedings of the Interna-tional Conference on Neural Networks (ICNN-96).1210E.
Grefenstette and M. Sadrzadeh.
2011.
Experimentalsupport for a categorical compositional distributionalmodel of meaning.
In EMNLP.T.
L. Griffiths, J.
B. Tenenbaum, and M. Steyvers.
2007.Topics in semantic representation.
Psychological Re-view, 114.I.
Hendrickx, S.N.
Kim, Z. Kozareva, P. Nakov,D.
O?
Se?aghdha, S.
Pado?, M. Pennacchiotti, L. Ro-mano, and S. Szpakowicz.
2010.
Semeval-2010 task8: Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation.G.
E. Hinton.
1990.
Mapping part-whole hierarchies intoconnectionist networks.
Artificial Intelligence, 46(1-2).R.
Jones, B. Rey, O. Madani, and W. Greiner.
2006.
Gen-erating query substitutions.
In Proceedings of the 15thinternational conference on World Wide Web.D.
Klein and C. D. Manning.
2003.
Accurate unlexical-ized parsing.
In ACL.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of COLING-ACL, pages768?774.E.
J. Metcalfe.
1990.
A compositive holographic asso-ciative recall model.
Psychological Review, 88:627?661.J.
Mitchell and M. Lapata.
2010.
Composition in dis-tributional models of semantics.
Cognitive Science,34(8):1388?1429.R.
Montague.
1974.
English as a formal language.
Lin-guaggi nella Societa e nella Tecnica, pages 189?224.T.
Nakagawa, K. Inui, and S. Kurohashi.
2010.
Depen-dency tree-based sentiment classification using CRFswith hidden variables.
In NAACL, HLT.M.
Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Names and similarities on the web: fact extrac-tion in the fast lane.
In ACL.S.
Pado and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.B.
Pang and L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respectto rating scales.
In ACL, pages 115?124.T.
A.
Plate.
1995.
Holographic reduced representations.IEEE Transactions on Neural Networks, 6(3):623?641.J.
B. Pollack.
1990.
Recursive distributed representa-tions.
Artificial Intelligence, 46, November.C.
Potts.
2010.
On the negativity of negation.
In DavidLutz and Nan Li, editors, Proceedings of Semanticsand Linguistic Theory 20.
CLC Publications, Ithaca,NY.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In ACL.B.
Rink and S. Harabagiu.
2010.
UTD: Classifying se-mantic relations by combining lexical and semantic re-sources.
In Proceedings of the 5th International Work-shop on Semantic Evaluation.S.
Rudolph and E. Giesbrecht.
2010.
Compositionalmatrix-space models of language.
In ACL.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24:97?124.R.
Socher, C. D. Manning, and A. Y. Ng.
2010.
Learningcontinuous phrase representations and syntactic pars-ing with recursive neural networks.
In Proceedings ofthe NIPS-2010 Deep Learning and Unsupervised Fea-ture Learning Workshop.R.
Socher, E. H. Huang, J. Pennington, A. Y. Ng, andC.
D. Manning.
2011a.
Dynamic Pooling and Unfold-ing Recursive Autoencoders for Paraphrase Detection.In NIPS.
MIT Press.R.
Socher, C. Lin, A. Y. Ng, and C.D.
Manning.
2011b.Parsing Natural Scenes and Natural Language withRecursive Neural Networks.
In ICML.R.
Socher, J. Pennington, E. H. Huang, A. Y. Ng, andC.
D. Manning.
2011c.
Semi-Supervised RecursiveAutoencoders for Predicting Sentiment Distributions.In EMNLP.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.D.
Widdows.
2008.
Semantic vector products: Some ini-tial investigations.
In Proceedings of the Second AAAISymposium on Quantum Interaction.A.
Yessenalina and C. Cardie.
2011.
Composi-tional matrix-space models for sentiment analysis.
InEMNLP.F.M.
Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-andhar.
2010.
Estimating linear models for composi-tional distributional semantics.
COLING.1211
