Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
620?629, Prague, June 2007. c?2007 Association for Computational LinguisticsRecovery of Empty Nodes in Parse StructuresDenis Filimonov11University of MarylandCollege Park, MD 20742den@cs.umd.eduMary P. Harper1,22Purdue UniversityWest Lafayette, IN 47907mharper@casl.umd.eduAbstractIn this paper, we describe a new algorithmfor recovering WH-trace empty nodes.
Ourapproach combines a set of hand-writtenpatterns together with a probabilistic model.Because the patterns heavily utilize regu-lar expressions, the pertinent tree structuresare covered using a limited number of pat-terns.
The probabilistic model is essen-tially a probabilistic context-free grammar(PCFG) approach with the patterns acting asthe terminals in production rules.
We eval-uate the algorithm?s performance on goldtrees and parser output using three differ-ent metrics.
Our method compares favorablywith state-of-the-art algorithms that recoverWH-traces.1 IntroductionIn this paper, we describe a new algorithm for re-covering WH-trace empty nodes in gold parse treesin the Penn Treebank and, more importantly, inautomatically generated parses.
This problem hasonly been investigated by a handful of researchersand yet it is important for a variety of applications,e.g., mapping parse trees to logical representationsand structured representations for language mod-eling.
For example, SuperARV language models(LMs) (Wang and Harper, 2002; Wang et al, 2003),which tightly integrate lexical features and syntacticconstraints, have been found to significantly reduceword error in English speech recognition tasks.
Inorder to generate SuperARV LM training, a state-of-the-art parser is used to parse training material andthen a rule-based transformer converts the parses tothe SuperARV representation.
The transformer isquite accurate when operating on treebank parses;however, trees produced by the parser lack one im-portant type of information ?
gaps, particularly WH-traces, which are important for more accurate ex-traction of the SuperARVs.Approaches applied to the problem of emptynode recovery fall into three categories.
Dienesand Dubey (2003) recover empty nodes as a pre-processing step and pass strings with gaps to theirparser.
Their performance was comparable to(Johnson, 2002); however, they did not evaluatethe impact of the gaps on parser performance.Collins (1999) directly incorporated wh-traces intohis Model 3 parser, but he did not evaluate gap in-sertion accuracy directly.
Most of the research be-longs to the third category, i.e., post-processing ofparser output.
Johnson (2002) used corpus-inducedpatterns to insert gaps into both gold standard treesand parser output.
Campbell (2004) developed aset of linguistically motivated hand-written rules forgap insertion.
Machine learning methods were em-ployed by (Higgins, 2003; Levy and Manning, 2004;Gabbard et al, 2006).In this paper, we develop a probabilistic modelthat uses a set of patterns and tree matching to guidethe insertion of WH-traces.
We only insert traces ofnon-null WH-phrases, as they are most relevant forour goals.
Our effort differs from the previous ap-proaches in that we have developed an algorithm forthe insertion of gaps that combines a small set of ex-pressive patterns with a probabilistic grammar-basedmodel.6202 The ModelWe have developed a set of tree-matching patternsthat are applied to propagate a gap down a path ina parse tree.
Pattern examples appear in Figure 1.Each pattern is designed to match a subtree (a rootand one or more levels below that root) and used toguide the propagation of the trace into one or morenodes at the terminal level of the pattern (indicatedusing directed edges).
Since tree-matching patternsare applied in a top-down fashion, multiple patternscan match the same subtree and allow alternativeways to propagate a gap.
Hence, we have developeda probabilistic model to select among the alterna-tive paths.
We have created 24 patterns for WHNPtraces, 16 for WHADVP, 18 for WHPP, and 11 forWHADJP.Figure 1: Examples of tree-matching patternsBefore describing our model, we first introducesome notation.?
TNij is a tree dominating the string of words be-tween positions i and j with N being the label ofthe root.
We assume there are no unary chains likeN?X?
...?Y ?N (which could be collapsed toa single node N ) in the tree, so that TNij uniquelydescribes the subtree.?
A gap location gab,Ncd is represented as a tuple(gaptype, ancstr(a, b,N), c, d), where gaptypeis the type of the gap, (e.g., whnp for a WHNPtrace), ancstr(a, b,N) is the gap?s nearest ances-tor, with a and b being its span and N being itslabel, and c and d indicating where the gap canbe inserted.
Note that a gap?s location is specifiedprecisely when c = d. If the gap is yet to be in-serted into its final location but will be insertedsomewhere inside ancstr(a, b,N), then we setc = a and d = b.?
ancstr(a, b,N) in the tuple for gab,Nxy is the treeTNab .?
p(gab,Nxy |gaptype, TNij ) is the probability that agap of gaptype is located between x and y, with aand b being the span of its ancestor, and i ?
a ?x ?
y ?
b ?
j.Given this notation, our model is tasked to identifythe best location for the gap in a parse tree amongthe alternatives, i.e.,argmaxx,a,b,NPr(gab,Nxx |T, gaptype)where gab,Nxx represents a gap location in a tree, andT = TNij is the subtree of the parse tree whoseroot node is the nearest ancestor node dominatingthe WH-phrase, excluding the WH-node itself, andgaptype is the type of the gap.
In order to simplifythe notation, we will omit the root labels N in TNijand gab,Nxy , implying that they match where appropri-ate.To guide this model, we utilize tree-matching pat-terns (see Figure 1), which are formally defined asfunctions:ptrn : T ?
G ?
?
?
{none}where T is the space of parse trees, G is the spaceof gap types, and ?
is the space of gaps gabcd ,and none is a special value representing failure tomatch1.
The application of a pattern is defined as:app(ptrn, ?, gaptype) = ptrn(?, gaptype), where?
?
T and gaptype ?
G. We define application ofpatterns as follows:app(ptrn, Tij , gaptype) ?
gabxy : i ?
a ?
x < y ?
b ?
japp(ptrn, Tij , gaptype) ?
gabxx : i ?
a ?
x ?
b ?
japp(ptrn, Tij , gaptype) ?
noneBecause patterns are uniquely associated with spe-cific gap types, we will omit gaptype to simplify thenotation.
Application is a function defined for everypair (ptrn, Tij) with fixed gaptype.
Patterns are ap-plied to the root of Tij , not to an arbitrary subtree.Consider an example of pattern application shownin Figure 2.
The tree contains a relative clause suchthat the WHNP-phrase that was moved from somelocation inside the subtree of its sister node S.2viewers3will4tune5in6to7see81Modeling conjunction requires an alternative definition forpatterns: ptrn : T ?
G ?
Powerset(?)
?
{none}.
For thesake of simplicity, we ignore conjunctions in the following dis-cussion, except for in the few places where it matters, since thishas little impact on the development of our model.621Figure 2: A pattern application exampleNow suppose there is a pattern P1 that matchesthe tree T28 indicating that the gap is some-where in its subtree T38 (will tune in to see), i.e.,app(P1, T28) ?
g3838 .
The process of applying pat-terns continues until the pattern P4 proposes an ex-act location for the gap: app(P4, T78) = g7888 .Figure 3: Another pattern application exampleSuppose that, in addition to the pattern applica-tions shown in Figure 2, there is one more, namely:app(P5, T48) ?
g4866 .
The sequence of patternsP1, P2, P5 proposes an alternative grammaticallyplausible location for the gap, as shown in Figure3.
Notice that the combination of the two sequencesproduces a tree of patterns, as shown in Figure 4,and this pattern tree covers much of the structure ofthe T28 subtree.2.1 Tree ClassesThe number of unique subtrees that contain WH-phrases is essentially infinite; hence, modeling themdirectly is infeasible.
However, trees with varyingdetails, e.g., optional adverbials, often can be char-P1P2P3CD P4,$EABFP5,$Figure 4: Pattern treeacterized by the same tree of patterns.
Hence, wecan represent the space of trees by utilizing a rela-tively small set of classes of trees that are determinedby their tree of pattern applications.Let ?
be the set of all patterns.
We define the setof patterns matching tree Tij as follows:M(Tij) = {P | P ?
?
?
app(P, Tij) 6= none}To enable recursive application:app(ptrn, gabxy) ={ app(ptrn, Tab) if x < ynone if x = yA Pattern Chain PC is a sequence of pairsof patterns and sets of pattern sets, terminated by$, i.e., ( p1M1 ,p2M2 , ...pnMn , $), where ?i pi ?
Mi ??.
Mi = M(Tab), where Tab is the result ofconsequent application of the first i ?
1 patterns:app(pi?1, app(pi?2, ..., app(p1, T??)))
= gabxy, andwhere T??
is the subtree we started with, (T28 in theexample above).
We define the application of a pat-tern chain PC = ( p1M1 ,p2M2 , ...pnMn , $) to a tree Tijas:app(PC, Tij) = app(pn, ...app(p2, app(p1, Tij)))It is important to also define a function to mapa tree to the set of pattern chains applicable to aparticular tree.
The pseudocode for this functioncalled FindPCs appears in Figure 52.
When ap-plied to Tij , this function returns the set of all pat-tern chains, applications of which would result inconcrete gap locations.
The algorithm is guaranteedto terminate as long as trees are of finite depth andeach pattern moves the gap location down at leastone level in the tree at each iteration.
Using thisfunction, we define Tree Class (TC) of a tree Tijas TC(Tij) = FindPCs(Tij).2list ?
element means ?append element to list?.622function FindPCs?
(Tij , PC, allPCs) {Mij ?
{P | P ?
?
?
app(P, Tij) 6= none}forall P ?
Mijgabxy ?
app(P, Tij)PC ?
PC ?
PMijif x = y then // gabxy is a concrete locationallPCs ?
allPCs ?
{PC ?
$}elseallPCs ?
FindPCs?
(Tab, PC, allPCs)return allPCs }function FindPCs(Tij) { return FindPCs?
(Tij , [ ], ?)
}Figure 5: Pseudocode for FindPCsIn the case of a conjunction, the function Find-PCs is slightly more complex.
Recall that in thiscase app(P, Tij) produces a set of gaps or none.
Thepseudocode for this case appears in Figure 6.2.2 A Gap AutomatonThe set of pattern chains constructed by the functionFindPCs can be represented as a pattern tree withpatterns being the edges.
For example, the patterntree in Figure 4 corresponds to the tree displayed inFigures 2 and 3.This pattern tree captures the history of gap prop-agations beginning at A.
Assuming at that point onlypattern P1 is applicable, subtree B is produced.
If P2yields subtree C, and at that point patterns P3 andP5 can be applied, this yields subtree D and exactlocation F (which is expressed by the terminationsymbol $), respectively.
Finally, pattern P4 matchessubtree D and proposes exact gap location E. It isimportant to note that this pattern tree can be thoughtof as an automaton, with A,B,C,D,E, and F be-ing the states and the pattern applications being thetransitions.Now, let us assign meaning of the statesA,B,C, and D to be the set of matching patterns,i.e., A = {P1}, B = {P2}, C = {P3, P5}, D = {P4}, andE = F = ?.
Given this representation, the patternchains for the insertion of the gaps in our examplewould be as follows:({P1}) P1?
({P2}) P2?
({P3, P5}) P3?
({P4}) P4,$??
(?
)({P1}) P1?
({P2}) P2?
({P3, P5}) P5,$??
(?
)With this representation, we can create a regulargrammar using patterns as the terminals and theirfunction CrossProd(PC1, PC2) {prod ?
?forall pci ?
PC1forall pcj ?
PC2 : prod ?
prod?
{pci?pcj}return prod }function FindPCs(Tij) {Mij ?
{P | P ?
?
?
app(P, Tij) 6= none}newPCs ?
?forall P ?
MijPCs ?
{[ ]}forall gabxy ?
app(P, Tij)if x = y thenforall pc ?
PCs : pc ?
pc ?
$elsePCs ?
CrossProd(PCs,FindPCs(Tab))forall pc ?
PCs : pc ?
PMij ?
pcnewPCs ?
newPCs ?
PCsreturn newPCs }The set app(P, Tij) must be ordered, so thatbranches of conjunction are concatenated in a well de-fined order.Figure 6: Pseudocode for FindPCs in the case ofconjunctionpowerset as the non-terminals (adding a few moredetails like the start symbol) and production rulessuch as {P2} ?
P2 {P3, P5}.
However, for our exam-ple the chain of patterns applied P1, P2, P3, P4, $ couldgenerate a pattern tree that is incompatible with theoriginal tree.
For example:({P1}) P1?
({P2}) P2?
({P3, P5}) P3?
({P3, P4}) P4,$??
(?
)which might correspond to something like ?thatviewers will tune in to expect to see.?
Note that thispattern chain belongs to a different tree class, whichincidentally would have inserted the gap at a differ-ent location (VP see gap).To overcome this problem we add additional con-straints to the grammar to ensure that all parses thegrammar generates belong to the same tree class.One way to do this is to include the start state ofa transition as an element of the terminal, e.g., P2{P2} ,P3{P3,P5} .
That is, we extend the terminals to includethe left-hand side of the productions they are emittedfrom, e.g.,{P2} ?
P2{P2} {P3, P5}623{P3, P5} ?
P3{P3, P5} {P4}and the sequence of terminals becomes:P1{P1}P2{P2}P3{P3,P5}P4{P4} $.Note that the grammar is unambiguous.
For sucha grammar, the question ?what is the probability of aparse tree given a string and grammar?
doesn?t makesense; however, the question ?what is the probabilityof a string given the grammar?
is still valid, and thisis essentially what we require to develop a genera-tive model for gap insertion.2.3 The Pattern GrammarLet us define the pattern grammar more rigorously.Let ?
be the set of patterns, and ??
?
?
be the setof terminal patterns3.
Let pset(P ) be the set of allsubsets of patterns which include the pattern P , i.e.,pset(P ) = {?
?
{P} | ?
?
powerset(?)}?
Let T = { Ppset(P ) | P ?
?}?
{$} be the set ofterminals, where $ is a special symbol4.?
Let N = {S}?
powerset(?)
be the set of non-terminals with S being the start symbol.?
Let P be the set of productions, defined as theunion of the following sets:1.
{S ?
?
| ?
?
powerset(?)}.2.
{?
?
P?
?
| P ?
????
, ?
?
pset(P ) and ?
?powerset(?)}.
These are nonterminal transi-tions, note that they emit only non-terminal pat-terns.3.
{?
?
P?
$ | P ?
??
and ?
?
pset(P )}.
Theseare the terminal transitions, they emit a termi-nal pattern and the symbol $.4.
{?
?
P?
?1 .
.
.
?n | P ?
?
?
??
, ?
?pset(P ) and ?i?
[1..n] ?i ?
powerset(?
)}.This rule models conjunction with n branches.2.4 Our Gap ModelGiven the grammar defined in the previous subsec-tion, we will define a probabilistic model for gap in-sertion.
Recall that our goal is to find:argmaxx,a,bPr(gabxx|T )Just like the probability of a sentence is obtained bysumming up the probabilities of its parses, the prob-ability of the gap being at gabxx is the sum of proba-bilities of all pattern chains that yield gabxx.3Patterns that generate exact position for a gap.4Symbol $ helps to separate branches in strings with con-junction.Pr(gabxx|T ) =?pci?
?Pr(pci|T )where ?
= {pc | app(pc, T ) = gabxx}.
Note thatpci ?
TC(T ) by definition.For our model, we use two approximations.
First,we collapse a tree T into its Tree Class TC(T ), ef-fectively ignoring details irrelevant to gap insertion:Pr(pci|T ) ?
Pr(pci|TC(T ))Figure 7: A pattern tree with the pattern chainABDGM marked using bold linesConsider the pattern tree shown in Figure 7.
Theprobability of the pattern chain ABDGM given thepattern tree can be computed as:Pr(ABDGM |TC(T )) = Pr(ABDGM,TC(T ))Pr(TC(T ))= NR(ABDGM,TC(T ))NR(TC(T ))where NR(TC(T )) is the number of occurrencesof the tree class TC(T ) in the training corpus andNR(ABDGM,TC(T )) is the number cases whenthe pattern chain ABDGM leads to a correct gap intrees corresponding to the tree class TC(T ).
Formany tree classes, NR(TC(T )) may be a smallnumber or even zero, thus this direct approach can-not be applied to the estimation of Pr(pci|TC(T )).Further approximation is required to tackle the spar-sity issue.In the following discussion, XY will denotean edge (pattern) between vertices X and Y in624the pattern tree shown in Figure 7.
Note thatPr(ABDGM |TC(T )) can be represented as:Pr(AB|TC(T ), A)?
Pr(BD|TC(T ), AB)?
?Pr(DG|TC(T ), ABD)?
Pr(GM |TC(T ), ABDG)We make an independence assumption, specifi-cally, that Pr(BD|TC(T ), AB) depends only onstates B, D, and the edge between them, not onthe whole pattern tree or the edges above B, i.e.,Pr(BD|TC(T ), AB) ?
Pr(BD,D|B).
Note thatthis probability is equivalent to the probability of aproduction Pr(B BD?
D) of a PCFG.Recall that the meaning assigned to a statein pattern grammar in Section 2.2 is the set ofpatterns matching at that state.
Thus, accord-ing to that semantics, only the edges displayedbold in Figure 8 are involved in computation ofPr(B BD?
D).
Written in the style we used forour grammar, the production is {BD,BE,BF} ?BD{BD,BE,BF}{DG,DH}.Figure 8: The context considered for estimation ofthe probability of transition from B to DPattern trees are fairly shallow (partly becausemany patterns cover several layers in a parse treeas can be seen in Figures 1 and 2); therefore, thecontext associated with a production covers a goodpart of a pattern tree.
Another important observa-tion is that the local configuration of a node, whichis described by the set of matching patterns, is themost relevant to the decision of where the gap is tobe propagated5.
This is the reason why the states arerepresented this way.Formally, the second approximation we make is5We have evaluated a model that only usesPr(BD|{BD,BE,BF}) for the probability of takingBD and found it performs only slightly worse than the modelpresented here.as follows:Pr(pci|TC(T )) ?
Pr(pci|G)where G is a PCFG model based on the grammardescribed above.Pr(pci|G) =?prodj?P(pci)Pr(prodj |G)where P(pci) is the parse of the pattern chain pciwhich is a string of terminals of G. Combining theformulae:Pr(gabxx|T ) ??pci?
?Pr(pci|G)Finally, since Pr(TC(T )|G) is a constant for T ,argmaxx,a,bPr(gabxx|T ) ?
argmaxx,a,b?pci?
?Pr(pci|G)To handle conjunction, we must express the factthat pattern chains yield sets of gaps.
Thus, the goalbecomes:argmax(x1,a1,b1),...,(xn,an,bn)Pr({ga1b1x1x1 , .
.
.
, ganbnxnxn}|T )Pr({ga1b1x1x1 , .
.
.
, ganbnxnxn}|T ) =?pci?
?Pr(pci|T )where ?
= {pc | app(pc, T ) ={ga1b1x1x1 , .
.
.
, ganbnxnxn}}.
The remaining equationsare unaffected.2.5 SmoothingEven for the relatively small number of patterns,the number of non-terminals in the grammar canpotentially be large (2|?|).
This does not happenin practice since most patterns are mutually exclu-sive.
Nonetheless, productions, unseen in the train-ing data, do occur and their probabilities have to beestimated.
Rewriting the probability of a transitionPr(A ?
aA B) as P(A, a,B), we use the following in-terpolation:P?
(A, a,B) = ?1P(A, a,B) + ?2P(A, a)+?3P(A,B) + ?4P(a,B) + ?5P(a)We estimate the parameters on the held out data(section 24 of WSJ) using a hill-climbing algorithm.6253 Evaluation3.1 SetupWe compare our algorithm under a variety of condi-tions to the work of (Johnson, 2002) and (Gabbardet al, 2006).
We selected these two approaches be-cause of their availability6.
In addition, (Gabbard etal., 2006) provides state-of-the-art results.
Since weonly model the insertion of WH-traces, all metricsinclude co-indexation with the correct WH phrasesidentified by their type and word span.We evaluate on three metrics.
The first metric,which was introduced by Johnson (2002), has beenwidely reported by researchers investigating gap in-sertion.
A gap is scored as correct only when it hasthe correct type and string position.
The metric hasthe shortcoming that it does not require correct at-tachment into the tree.The second metric, which was developed byCampbell (2004), scores a gap as correct only whenit has the correct gap type and its mother node hasthe correct nonterminal label and word span.
AsCampbell points out, this metric does not restrict theposition of the gap among its siblings, which in mostcases is desirable; however, in some cases (e.g., dou-ble object constructions), it does not correctly detecterrors in object order.
This metric is also adverselyaffected by incorrect attachments of optional con-stituents, such as PPs, due to the span requirement.To overcome the latter issue with Campbell?s met-ric, we propose to use a third metric that evaluatesgaps with respect to correctness of their lexical head,type of the mother node, and the type of the co-indexed wh-phrase.
This metric differs from thatused by Levy and Manning (2004) in that it countsonly the dependencies involving gaps, and so it rep-resents performance of the gap insertion algorithmmore directly.We evaluate gap insertion on gold trees from sec-tion 23 of the Wall Street Journal Penn Treebank(WSJ) and parse trees automatically produced usingthe Charniak (2000) and Bikel (2004) parsers.
Theseparsers were trained using sections 00 through 22 ofthe WSJ with section 24 as the development set.Because our algorithm inserts only traces of non-empty WH phrases, to fairly compare to Johnson?sand Gabbard?s performance on WH-traces alone, we6Johnson?s source code is publicly available, and Ryan Gab-bard kindly provided us with output trees produced by his sys-tem.remove the other gap types from both the gold treesand the output of their algorithms.
Note that Gab-bard et al?s algorithm requires the use of functiontags, which are produced using a modified versionof the Bikel parser (Gabbard et al, 2006) and a sep-arate software tool (Blaheta, 2003) for the Charniakparser output.For our algorithm, we do not utilize function tags,but we automatically replace the tags of auxiliaryverbs in tensed constructions with AUX prior to in-serting gaps using tree surgeon (Levy and Andrew,2006).
We found that Johnson?s algorithm moreaccurately inserts gaps when operating on auxifiedtrees, and so we evaluate his algorithm using thesemodified trees.In order to assess robustness of our algorithm, weevaluate it on a corpus of a different genre ?
Broad-cast News Penn Treebank (BN), and compare the re-sult with Johnson?s and Gabbard?s algorithms.
TheBN corpus uses a modified version of annotationguidelines, with some of the modifications affectinggap placement.Treebank 2 guidelines (WSJ style):(SBAR (WHNP-2 (WP whom))(S (NP-SBJ (PRP they))(VP (VBD called)(S (NP-SBJ (-NONE- *T*-2))(NP-PRD (NNS exploiters))))))Treebank 2a guidelines (BN style):(SBAR-NOM (WHNP-1 (WP what))(S (NP-SBJ (PRP they))(VP (VBP call)(NP-2 (-NONE- *T*-1))(S-CLR (NP-SBJ (-NONE- *PRO*-2))(NP-PRD (DT an) (NN epidemic))))))Since our algorithms were trained on WSJ, we ap-ply tree transformations to the BN corpus to convertthese trees to WSJ style.
We also auxify the trees asdescribed previously.3.2 ResultsTable 1 presents gap insertion F measure for John-son?s (2002) (denoted J), Gabbard?s (2006) (denotedG), and our (denoted Pres) algorithms on section 23gold trees, as well as on parses generated by theCharniak and Bikel parsers.
In addition to WHNPand WHADVP results that are reported in the liter-ature, we also present results for WHPP gaps eventhough there is a small number of them in section23 (i.e., 22 gaps total).
Since there are only 3 non-empty WHADJP phrases in section 23, we omitthem in our evaluation.626Gold Trees Charniak Parser Bikel ParserMetric J G Pres J G Pres J G PresWHNP Johnson 94.8 90.7 97.9 89.8 86.3 91.5 90.2 86.8 92.6Campbell 94.8 97.0 99.1 81.9 83.8 83.5 80.7 81.5 82.2Head dep 94.8 97.0 99.1 88.8 90.6 91.0 89.1 91.4 92.3WHADVP Johnson 75.5 91.4 96.5 61.4 78.0 80.0 61.0 77.9 77.2Campbell 74.5 89.1 95.0 61.4 71.7 78.4 60.0 71.5 74.8Head dep 75.5 89.8 95.8 64.4 78.0 84.7 63.0 77.1 80.3WHPP Johnson 58.1 N/R 72.7 35.7 N/R 55.0 42.9 N/R 53.7Campbell 51.6 N/R 86.4 28.6 N/R 60.0 35.7 N/R 63.4Head dep 51.6 N/R 86.4 35.7 N/R 70.0 35.7 N/R 73.2Table 1: F1 performance on section 23 of WSJ (N/R indicates not reported)Compared to Johnson?s and Gabbard?s algorithm,our algorithm significantly reduces the error ongold trees (table 1).
Operating on automaticallyparsed trees, our system compares favorably onall WH traces, using all metrics, except for twoinstances: Gabbard?s algorithm has better perfor-mance on WHNP, using Cambpell?s metric and treesgenerated by the Charniak parser by 0.3% and onWHADVP, using Johnson?s metric and trees pro-duces by the Bikel parser by 0.7%.
However, webelieve that the dependency metric is more appropri-ate for evaluation on automatically parsed trees be-cause it enforces the most important aspects of treestructure for evaluating gap insertion.
The relativelypoor performance of Johnson?s and our algorithmson WHPP gaps compared that on WHADVP gapsis probably due, at least in part, to the significantlysmaller number of WHPP gaps in the training corpusand the relatively wider range of possible attachmentsites for the prepositional phrases.Table 2 displays how well the algorithms trainedon WSJ perform on BN.
A large number of the er-rors are due to FRAGs which are far more com-mon in the speech corpus than in WSJ.
WHPP andWHADJP, although more rare than the other types,are presented for reference.3.3 Error AnalysisIt is clear from the contrast between the results basedon gold standard trees and the automatically pro-duced parses in Table 1 that parse error is a majorsource of error.
Parse error impacts all of the met-rics, but the patterns of errors are different.
For WH-NPs, Campbell?s metric is lower than the other twoacross all three algorithms, suggesting that this met-ric is adversely affected by factors that do not im-pact the other metrics (most likely the span of thegap?s mother node).
For WHADVPs, the metricsshow a similar degradation due to parse error acrossthe board.
We are reluctant to draw conclusions forthe metrics on WHPPs; however, it should be notedthat the position of the PP should be less critical forevaluating these gaps than their correct attachment,suggesting that the head dependency metric wouldmore accurately reflect the performance of the sys-tem for these gaps.Campbell?s metric has an interesting property: inparse trees, we can compute the upper bound on re-call by simply checking whether the correct WH-phrase and gap?s mother node exist in the parse tree.We present recall results and upper bounds in Table3.
Clearly the algorithms are performing close to theupper bound for WHNPs when we take into accountthe impact of parse errors on this metric.
Clearlythere is room for improvement for the WHPPs.Metric J G PresWHNP Johnson 88.0 90.3 92.0Campbell 88.2 94.0 95.3Head dep 88.3 94.0 95.3WHADVP Johnson 76.4 92.0 94.3Campbell 76.3 88.2 92.4Head dep 76.3 88.5 92.5WHPP Johnson 56.6 N/R 75.7Campbell 60.4 N/R 91.9Head dep 60.4 N/R 91.9WHADJP Johnson N/R N/R 89.8Campbell N/R N/R 85.7Head dep N/R N/R 85.7Table 2: F1 performance on gold trees of BNIn addition to parser errors, which naturally havethe most profound impact on the performance, wefound the following sources of errors to have impacton our results:?
Annotation errors and inconsistency in PTB,which impact not only the training of our system,but also its evaluation.627Charniak Parser J G Pres UBWHNP 81.9 82.8 83.5 84.0WHADVP 61.4 71.7 78.4 81.1WHPP 28.6 N/R 60.0 86.4Bikel Parser J G Pres UBWHNP 77.0 80.5 81.5 82.0WHADVP 47.2 70.1 74.8 78.0WHPP 22.7 N/R 59.1 81.8Table 3: Recall on trees produced by the Charniakand Bikel parsers and their upper bounds (UB)1.
There are some POS labeling errors that con-fuse our patterns, e.g.,(SBAR (WHNP-3 (IN that))(S (NP-SBJ (NNP Canada))(VP (NNS exports)(NP (-NONE- *T*-3))(PP ...))))2.
Some WHADVPs have gaps attached in thewrong places or do not have gaps at all, e.g.,(SBAR (WHADVP (WRB when))(S (NP (PRP he))(VP (VBD arrived)(PP (IN at)(NP ...))(ADVP (NP (CD two)(NNS days))(JJ later)))))3.
PTB annotation guidelines leave it to annota-tors to decide whether the gap should be at-tached at the conjunction level or inside itsbranches (Bies et al, 1995) leading to incon-sistency in attachment decisions for adverbialgaps.?
Lack of coverage: Even though the patterns weuse are very expressive, due to their small numbersome rare cases are left uncovered.?
Model errors: Sometimes despite one of the appli-cable pattern chains proposes the correct gap, theprobabilistic model chooses otherwise.
We be-lieve that a lexicalized model can eliminate mostof these errors.4 Conclusions and Future WorkThe main contribution of this paper is the de-velopment of a generative probabilistic model forgap insertion that operates on subtree structures.Our model achieves state-of-the-art performance,demonstrating results very close to the upper boundon WHNP using Campbell?s metric.
Performancefor WHADVPs and especially WHPPs, however,has room for improvement.We believe that lexicalizing the model by addinginformation about lexical heads of the gaps may re-solve some of the errors.
For example:(SBAR (WHADVP-3 (WRB when))(S (NP (NNP Congress))(VP (VBD wanted)(S (VP (TO to)(VP (VB know) ...)))(ADVP (-NONE- *T*-3)))))(SBAR (WHADVP-1 (WRB when))(S (NP (PRP it))(VP (AUX is)(VP (VBN expected)(S (VP (TO to)(VP (VB deliver) ...(ADVP (-NONE- *T*-1)))))))))These sentences have very similar structure, withtwo potential places to insert gaps (ignoring re-ordering with siblings).
The current model insertsthe gaps as follows: when Congress (VP wanted (Sto know) gap) and when it is (VP expected (S todeliver) gap), making an error in the second case(partly due to the bias towards shorter pattern chains,typical for a PCFG).
However, deliver is more likelyto take a temporal modifier than know.In future work, we will investigate methods foradding lexical information to our model in order toimprove the performance on WHADVPs and WH-PPs.
In addition, we will investigate methods forautomatically inferring patterns from a treebank cor-pus to support fast porting of our approach to otherlanguages with treebanks.5 AcknowledgementsWe would like to thank Ryan Gabbard for provid-ing us output from his algorithm for evaluation.
Wewould also like to thank the anonymous reviewersfor invaluable comments.
This material is basedupon work supported by the Defense Advanced Re-search Projects Agency (DARPA) under ContractNo.
HR0011-06-C-0023.
Any opinions, findingsand conclusions or recommendations expressed inthis material are those of the authors and do not nec-essarily reflect the views of DARPA.ReferencesA.
Bies, M. Ferguson, K. Katz, and R. MacIntyre.
1995.Bracketing guidelines for treebank II style Penn Tree-bank project.
Technical report.D.
M. Bikel.
2004.
On the Parameter Space of Gen-628erative Lexicalized Statistical Parsing Models.
Ph.D.thesis, University of Pennsylvania.D.
Blaheta.
2003.
Function Tagging.
Ph.D. thesis,Brown University.R.
Campbell.
2004.
Using linguistic principles to re-cover empty categories.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the North American Chapter of theAssociation for Computational Linguistics.M.
Collins.
1999.
Head-driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.P.
Dienes and A. Dubey.
2003.
Antecedent recovery:Experiments with a trace tagger.
In Proceedings ofthe 2003 Conference on Empirical Methods in NaturalLanguage Processing.R.
Gabbard, S. Kulick, and M. Marcus.
2006.
Fully pars-ing the Penn Treebank.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.D.
Higgins.
2003.
A machine-learning approach to theidentification of WH gaps.
In Proceedings of the An-nual Meeting of the European Chapter of the Associa-tion for Computational Linguistics.M.
Johnson.
2002.
A simple pattern-matching algorithmfor recovering empty nodes and their antecedents.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics.R.
Levy and G Andrew.
2006.
Tregex and Tsurgeon:Tools for querying and manipulating tree data struc-tures.
In Proceedings of LREC.R.
Levy and C. Manning.
2004.
Deep dependenciesfrom context-free statistical parsers: Correcting thesurface dependency approximation.
In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics.W.
Wang and M. P. Harper.
2002.
The SuperARV lan-guage model: Investigating the effectiveness of tightlyintegrating multiple knowledge sources in languagemodeling.
In Proceedings of the Empirical Methodsin Natural Language Processing.W.
Wang, M. P. Harper, and A. Stolcke.
2003.
The ro-bustness of an almost-parsing language model givenerrorful training data.
In Proceedings of the IEEE In-ternational Conference on Acoustics, Speech, and Sig-nal Processing.629
