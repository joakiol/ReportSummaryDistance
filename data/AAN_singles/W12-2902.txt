NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 5?8,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsWinkTalk: a demonstration of a multimodal speech synthesis platformlinking facial expressions to expressive synthetic voicesE?va Sze?kely, Zeeshan Ahmed, Joa?o P. Cabral, Julie Carson-BerndsenCNGL, School of Computer Science and Informatics, University College DublinBelfield, D4, Dublin, Ireland{eva.szekely|zeeshan.ahmed}@ucdconnect.ie,{joao.cabral|julie.berndsen}@ucd.ieAbstractThis paper describes a demonstration of theWinkTalk system, which is a speech synthe-sis platform using expressive synthetic voices.With the help of a webcamera and facial ex-pression analysis, the system allows the userto control the expressive features of the syn-thetic speech for a particular utterance withtheir facial expressions.
Based on a person-alised mapping between three expressive syn-thetic voices and the users facial expressions,the system selects a voice that matches theirface at the moment of sending a message.The WinkTalk system is an early research pro-totype that aims to demonstrate that facialexpressions can be used as a more intuitivecontrol over expressive speech synthesis thanmanual selection of voice types, thereby con-tributing to an improved communication expe-rience for users of speech generating devices.1 IntroductionDuring a human verbal communication process, ex-pressive features of face and speech are congru-ent, operating in a synchronised manner (Campbell,2008), (Graf et al, 2002).
Facial expressions andexpressive speech styles often help to convey theemotional intent of the speaker that is only partiallycontained in the words.
The application describedin this paper aims to make use of this synchronyand applies facial expressions as a real time voli-tional control over the expressive features of syn-thetic utterance productions of augmented speakers.The WinkTalk system is currently a research proto-type in progress, operating on a personal computerequipped with a webcamera.
The goal of the systemis to respond to the need of integrated multimodalityin speech generating devices of users of augmenta-tive and alternative communication1 (AAC) applica-tions (Higginbotham, 2010).
Being able to correctlylink facial expression to synthetic speech output isa step forward to a more intuitive way of control-ling the expressiveness of synthetic speech.
The ap-proach can be considered novel, as the authors arenot aware of another system using facial expressionsto control expressive TTS.2 WinkTalk system architectureThe WinkTalk system is a web based application de-veloped using AJAX and PHP technologies.
Theweb application provides a flexible interface and al-lows for easy integration of new components suchas synthetic voices or gesture recognisers runningon a web server.
The internal architecture of thesystem is shown in figure 1.
The system operatesbased on a configurable workflow defining the threemodes of the system: a personalisation mode, an au-tomatic voice selection mode based on facial expres-sion, which is the core functionality of the system,and a control mode of manual voice selection, thatwas included for evaluation purposes.
In the man-ual voice selection application the user is presentedwith the three options and selects the voice style that1Augmentative and alternative communication (AAC) refersto an area of research, clinical, and educational practice.
AACinvolves attempts to study and when necessary compensate fortemporary or permanent impairments, activity limitations, andparticipation restrictions of individuals with severe disorders ofspeech-language production and/or comprehension, includingspoken and written modes of communication.
(ASHA, 2005)5Figure 1: Architecture and working modes of the Wink-Talk systemmatches the emotional or expressive intent of themessage.
It has previously been shown that after ashort familiarisation with the voices, it is possiblefor the user to make a fairly good prediction of howa particular utterance will sound when synthesisedwith one of the voices (Sze?kely et al, 2012).
Thismakes it possible to use the system in a conversationsituation, in which the user does not have the oppor-tunity to listen to the three possible speech samplesbut needs to make a choice ahead of the time of thesynthesis.
The automatic voice choice mode and thepersonalisation mode will be described in sections 3and 4, respectively.3 Facial expression based voice selection3.1 Expressive synthetic voicesThe synthesiser component of the application usesthree expressive HMM-based synthetic voices ofa middle aged American male.
The voices havebeen built using the HTS speech engine 2.1., froman audiobook corpus made available for BlizzardChallenge 2012 by Toshiba Research Europe Ltd,Cambridge Research Laboratory.
Each syntheticvoice was trained from different subcorpora of theaudiobook obtained using an unsupervised cluster-ing technique based on glottal source parameters(Sze?kely et al, 2011).
Perceptual experiments haveshown (Hennig et al, 2012) that the three voices canbe characterised on an expressiveness gradient: fromcalm (A voice), through intense (B voice) to very in-tense (C voice).
This expressiveness gradient canbe described with characteristics such as with risingpitch, greater prosodic variation, increased powerand voice quality changing from lax to tense.3.2 Facial expression analysisFor facial expression recognition, the system usesthe Sophisticated Highspeed Object RecognitionEngine (SHORE) library by Fraunhofer.
To detectfaces and expressions, SHORE analyses local struc-ture features in images and outputs scores for fourdistinct facial expressions: happy, sad, angry andsurprised, with an indication of the intensity of theexpression (Kueblbeck and Ernst, 2006).
The inten-sity ranges from 0-100, a higher value meaning amore intense expression in that category.3.3 Mapping between facial expressions andvoicesThe system uses the facial expression categories andintensity scores outputted by SHORE to select fromthe three synthetic voices.
The initial mapping be-tween facial expression categories and ranges of in-tensity values and voices are shown in Table 1.
Forexample, an image analysed as containing the fa-cial expression suprised with an intensity of 25, thesystem will synthesise the corresponding utterancewith the C voice.
The system always uses the fa-cial expression category with the highest value fora particular image.
These initial values have beenFigure 2: Initial thresholds for mapping different inten-sity values of the facial expressions to the synthetic voices6Figure 3: Interface of the dialogue simulation with Wink-Talk.chosen based on considerations about arousal lev-els of the underlying basic emotion of the facial ex-pression categories, for example with surprise be-ing a high arousal emotion, the intensity scores ofit result sooner in a higher intensity voice choice.The values have also been supported by the resultsa perceptual test carried out by 25 participants on adataset that was balanced to contain equal amountof stimuli from all facial expression categories.
Par-ticipants were asked to select from three synthesisedutterances the one best matching the facial expres-sion of a person on a picture.
The perceptual testhas shown that 90% of all majority votings (above66% agreement among participants) fell within theinitial threshold values.
When a message is beingsent to the synthesiser, the system makes a snapshotof the user?s face.
Based on the image scores andthreshold table, the system decides which voice bestsuits the current facial expression and returns the re-sults accordingly.
The system also provides an op-tion to take streaming video input from the camerarather than a single image, and calculate the featurevalues over an interval of the video around the timeof sending a message.
To take into consideration thecases where individual preferences of voice choicediffer greatly, as well as to account for individualdifferences in facial characteristics, a personalisa-tion component has been integrated in the system,which will be introduced in section 4.4 Personalisation componentIn order to optimise the performance of the Wink-Talk system, a personalisation session needs to becompleted by each user.
The objective of the per-sonalisation is to adjust the voice selection thresh-old according to users?
facial characteristics and in-dividual preferences.
In the personalisation phase,Figure 4: Interface of the personalisation component ofWinkTalk.the user is presented with a sentence and makes anappropriate facial expression to accompany the ut-terance.
The facial expression is captured and anal-ysed by the system and the user is presented withthe three options of synthetic speech samples, alongwith an indication of which sample the system choseto match their facial expression.
If the user doesnot agree with the selection provided by the sys-tem, a preference can be indicated by choosing fromthe other two options.
The system then adjusts thethreshold by moving it by a standard factor towardsthe outlying training example.
The new threshold isapplied in next trial.
The thresholds for each facialexpression-voice pair are normalised so that there isno overlap between the different voices for the samefeature.5 Conclusions and future workThe WinkTalk system has been evaluated within aninteractive evaluation session involving 10 subjects,each of them acting out pre-scripted dialogues witha conversation partner.
The evaluation has shownthat while there is a general preference to manual se-lection of expressive voices, 90% of the participantsdescribed facial expression control as a valuable ad-dition to the simulated augmented communicationprocess.
A strong learning effect in the ease of usingthe system has also been observed.
Future work isplanned to research further input strategies of ges-tures as well as to integrate a female expressive syn-thetic voice.
An essential next step is to extend the7personalisation component to include the possibilityof fully personalised training of the facial expressionanalysis to fit individual needs of users who are re-stricted with respect to their gestural expressiveness.6 Demonstration6.1 OverviewThe demonstration will give participants an oppor-tunity to use the WinkTalk system by conductingthe personalisation phase and using the system withpre-scripted dialogues.
It is intended for those in-terested in using multimodal tools and expressivespeech to improve the communication experienceof individuals with complex communication needs.The demonstration will give participants a chanceto experience the facial expression control over thevoice choice of the system as well as get an im-pression of how the range of expressive voices canbe used in an acted dialogue situation.
A 3 minutevideo of the system in use will also be available forviewing.6.2 Familiarisation/Personalisation phaseFirst, a short introduction will be given to the systemand its aims, then the participants will be introducedto the synthetic voices by listening to a few sam-ples receiving a brief description of their character-istics.
Subsequently, the participants will be asked toconduct a personalisation session including 20 iter-ations, that will help optimise the system to adapt tothe participants?
preferences, as described in section4.
It will also familiarise the users with the char-acteristics of the voices and the mapping of facialexpressions and voices.6.3 Dialogue simulation with synthetic voicesAfter the users are familiarised with the system, theycan choose from a set of 8 dialogues representing arange of social interactions and emotional sentimentand intensity.
Participants will act out some of thedialogues with a conversation partner, using facialexpressions to control the selection of the syntheticvoices instead of speaking with their own voice.They will also have the option to compare the facialexpression control of the WinkTalk system with asimple manual selection of synthetic voices for eachutterance.
At the end of the dialogue session therewill be a chance to fill out a feedback form to helpthe further development of the system.AcknowledgmentsThis research is supported by the Science Foundation Ireland(Grant 07/CE/I1142) as part of the Centre for Next Genera-tion Localisation (www.cngl.ie) at University College Dublin(UCD).
The opinions, findings, and conclusions or recommen-dations expressed in this material are those of the authors and donot necessarily reflect the views of Science Foundation Ireland.The authors would also like to thank Shannon Hennig (IIT),Nick Campbell and the Speech Communication Lab (TCD), fortheir invaluable help with the interactive evaluation.ReferencesAmerican Speech-Language-Hearing Association.
2005Roles and Responsibilities of Speech-LanguagePathologists With Respect to Augmentative and Alter-native Communication: Position Statement.
Availablefrom www.asha.org/policy.Campbell, N. 2008.
Multimodal processing of discourseinformation; the effect of synchrony Proc.
of Interna-tional Symposium on Universal Communication, Os-aka.Graf, H.P., Cosatto, E., Strom, V., and Huang, F.J.2002.
Visual prosody: Facial movements accompany-ing speech.
Proc.
of the 5th International Conferenceon Automatic Face and Gesture Recognition.Hennig, S., Sze?kely, E?., Carson-Berndsen, J. and Chel-lali, R. 2012.
Listener evaluation of an expressivenessscale in speech synthesis for conversational phrases:implications for AAC.
to appear in: Proc.
of ISAAC,Pittsburgh.Higginbotham, D. J.
2010.
Humanizing Vox Artificialis:The Role of Speech Synthesis in Augmentative andAlternative Communication Computer SynthesizedSpeech Technologies: Tools for Aiding Impairment,J.
Mullennix and S. Stern, Eds.
IGI Global, pp.
50-70.HTS-2.1 toolkit, HMM-based speech synthesis systemversion 2.1.
http://hts.sp.nitech.ac.jp.Kueblbeck., C. and Ernst, A.
2006.
Face detection andtracking in video sequences using the modified censustransformation.
Journal on Image and Vision Comput-ing, vol.
24, issue 6, pp.
564-572.SHORE face detection engine, Fraunhofer Institutehttp://www.iis.fraunhofer.de/en/bf/bsy/fue/isystSze?kely, E?., Cabral, J., Abou-Zleikha, M., Cahill, P. andCarson-Berndsen, J.
2012.
Evaluating expressivespeech synthesis from audiobook corpora for conver-sational phrases.
Proc.
of LREC, Istanbul.Sze?kely, E?., Cabral, J. P., Cahill, P. and Carson-Berndsen,J.
2011.
Clustering expressive speech styles in audio-books using glottal source parameters.
Proc.
of Inter-speech, Florence.8
