Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 705?713,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsCost Optimization for Crowdsourcing TranslationMingkun Gao, Wei Xu and Chris Callison-BurchComputer and Information Science DepartmentUniversity of Pennsylvania, Philadelphia, PA, USA{gmingkun, xwe, ccb}@seas.upenn.eduAbstractCrowdsourcing makes it possible to createtranslations at much lower cost than hiringprofessional translators.
However, it is stillexpensive to obtain the millions of transla-tions that are needed to train statistical ma-chine translation systems.
We propose twomechanisms to reduce the cost of crowdsourc-ing while maintaining high translation quality.First, we develop a method to reduce redun-dant translations.
We train a linear model toevaluate the translation quality on a sentence-by-sentence basis, and fit a threshold betweenacceptable and unacceptable translations.
Un-like past work, which always paid for a fixednumber of translations for each source sen-tence and then chose the best from them, wecan stop earlier and pay less when we receivea translation that is good enough.
Second,we introduce a method to reduce the pool oftranslators by quickly identifying bad transla-tors after they have translated only a few sen-tences.
This also allows us to rank translators,so that we re-hire only good translators to re-duce cost.1 IntroductionCrowdsourcing is a promising new mechanism forcollecting large volumes of annotated data at lowcost.
Many NLP researchers have started creatingspeech and language data through crowdsourcing(for example, Snow et al (2008), Callison-Burchand Dredze (2010) and others).
One NLP applica-tion that has been the focus of crowdsourced datacollection is statistical machine translation (SMT)which requires large bilingual sentence-aligned par-allel corpora to train translation models.
Crowd-sourcing?s low cost has made it possible to hire peo-ple to create sufficient volumes of translation in or-der to train SMT systems (for example, Ambati andVogel (2010), Zbib et al (2012), Post et al (2012),Zbib et al (2013)).However, crowdsourcing is not perfect, and oneof its most pressing challenges is how to ensure thequality of the data that is created by it.
Unlike inmore traditional employment scenarios, where an-notators are pre-vetted and their skills are clear, incrowdsourcing very little is known about the annota-tors.
They are not professional translators, and thereare no built-in mechanisms for testing their languageskills.
They complete tasks without any oversight.Thus, translations produced via crowdousrcing maybe low quality.
Previous work has addressed thisproblem, showing that non-professional translatorshired on Amazon Mechanical Turk (MTurk) canachieve professional-level quality, by soliciting mul-tiple translations of each source sentence and thenchoosing the best translation (Zaidan and Callison-Burch, 2011).In this paper we focus on a different aspectof crowdsourcing than Zaidan and Callison-Burch(2011).
We attempt to achieve the same high qual-ity while minimizing the associated costs.
We pro-pose two complementary methods: (1) We reducethe number of translations that we solicit for eachsource sentence.
Instead of soliciting a fixed numberof translations for each foreign sentence, we stop so-liciting translations after we get an acceptable one.We do so by building models to distinguish between705acceptable translations and unacceptable ones.
(2)We reduce the number of workers we hire, and re-tain only high quality translators by quickly identify-ing and filtering out workers who produce low qual-ity translations.
Our work stands in contrast withZaidan and Callison-Burch (2011) who always so-licited and paid for a fixed number of translationsfor each source sentence, and who had no model ofannotator quality.In this paper we demonstrate that:?
Our model can predict whether a given transla-tion is acceptable with high accuracy, substan-tially reducing the number of redundant trans-lations needed for every source segment.?
Translators can be ranked well even when ob-serving only small amounts of data.
Comparedwith a gold standard ranking, we achieve a cor-relation of 0.94 after seeing the translations ofonly 20 sentences from each worker.
There-fore, bad workers can be filtered out quickly.?
We can achieve a similar BLEU score as Zaidanand Callison-Burch (2011) at half the cost us-ing our cost optimizing methods.2 Problem SetupWe start with a corpus of source sentences to betranslated, and we may solicit one or more transla-tion for every sentence in the corpus.
Our targetedtask is to assemble a single high quality translationfor each source sentence while minimizing the asso-ciated cost.
This process can be repeated to obtainmultiple high quality translations.We study the data collected by Zaidan andCallison-Burch (2011) through Amazon?s Mechani-cal Turk.
They hired Turkers to translate 1792 Urdusentences from the 2009 NIST Urdu-English OpenMachine Translation Evaluation set1.
A total of 52Turkers contributed translations.
Turkers also filledout a survey about their language skills and theircountries of origin.
Each Urdu sentence was trans-lated by 4 non-professional translators (the Turkers)and 4 professional translators hired by the LDC.
Thecost of non-professional translation was $0.10 persentence and we estimate the cost of professional1LDC Catalog number LDC2010T23translation to be approximately $0.30 per word (or$6 per sentence, with 20 words on average).Following Zaidan and Callison-Burch (2011), weuse BLEU (Papineni et al, 2002) to gauge the qual-ity of human translations.
We can compute the ex-pected quality of professional translation by com-paring each of the professional translators againstthe other 3.
This results in an average BLEUscore of 42.38.
In comparison, the average Turkertranslations score only 28.13 without quality con-trol.
Zaidan and Callison-Burch trained a MERT(Och, 2003; Zaidan, 2009) model to select one non-professional translation out of the four and pushedthe quality of crowdsourcing translation to 38.99,closer to the expected quality of professional trans-lation.
They used a small amount of professionaltranslations (10%) as calibration data to estimate thegoodness of the non-professional translation.
Thecomponent costs of their approach are the 4 non-professional translations for each source sentence,and the professional translations for the calibrationdata.Although Zaidan and Callison-Burch demon-strated that non-professional translation was signif-icantly cheaper than professionals, we are inter-ested in further reducing the costs.
Cost reductionplays an important role if we want to assemble alarge enough parallel corpus to train a statistical ma-chine translation system which typically require mil-lions of translated sentences.
Here, we introduceseveral methods for reducing the number of non-professional translations while still maintaining highquality.3 Estimating Translation QualityWe use a linear regression model2to predict a qual-ity score (score(t) ?
R) for an input translation t.score(t) = ~w ?~f(t)where ~w is the associated weight vector and~f(t) isthe feature vector of the translation t.We replicate the feature set used by Zaidan andCallison-Burch (2011) in their MERT model:?
Sentence-level features: 9 features based on2We used WEKA package: http://www.cs.waikato.ac.nz/ml/weka/706Figure 1: Example bilingual features for two crowd-sourced translations of an Urdu sentence.
The numbersare alignment probabilities for each aligned word.
Thebilingual feature is the average of these probabilities, thus0.240 for the good translation and 0.043 for the bad trans-lation.
Some words are not aligned if potential word pairsdon?t exist in bilingual training corpus.language model, sentence length and edit dis-tance to other translations.?
Worker-level features: 15 features based onworker?s language ability, location and averagesentence-level scores.?
Ranking features: 3 features based on the judg-ments of monolingual English speakers?
rank-ing the translations from best to worst.?
Calibration features: 1 feature based on the av-erage BLEU score of a worker?s translationsprovided is computed against professional ref-erences.We additionally introduce a new bilingual fea-ture based on IBM Model 1.
We align words be-tween each candidate translation and its correspond-ing source sentence.
The bilingual feature is the av-erage of its alignment probabilities between wordsin the source sentence and words in the Turker?stranslation.
In Figure 1, we show how the bilin-gual feature allows us to distinguish between a validtranslation (top) and an invalid/spammy translation(bottom).4 Reducing the Number of TranslationsThe first way that we optimize cost is to solicit fewerredundant translations.
The strategy is to recognizewhen we have got a good translation of a sourcesentence and to immediately stop purchasing addi-tional translations of that sentence.
The crux of thismethod is to decide whether a translation is ?goodAlgorithm 1 How good is good enoughInput: ?, the allowable deviation from the expectedupper bound on BLEU score (using all redundanttranslations); ?, the upper bound BLEU score; atraining set S = {~fsi,j, ysi,j)j=1..mi=1..n} and a validationset V = {(~fvi,j, yvi,j)j=1..mi=1..n} where~fi,jis the fea-ture vector for ti,jwhich is the jth translation of thesource sentence siand yi,jis the label for~fi,j.Output: ?, the threshold between acceptable andunacceptable translations; ~w, a linear regressionmodel parameter.1: initialize ?
?
0,~w ?
?2:~w??
train a linear regression model on S3: maxbleu ?
select best translations for eachsi?
S based on the model parameter~w?andrecord the highest model predicted BLEU score4: while ?
6= maxbleu do5: for i?
1 to n do6: for j ?
1 to m do7: if~w?
?~fvi,j> ?
?
j < m then selecttvi,jfor siand break8: if j == m then select tvi,mfor si9: q ?
calculate translation quality for V10: if q > ?
?
?
then break11: else ?
= ?
+ stepsize12: ~w ?
train a linear regression model on S ?
V13: Return: ?
and model parameter ~wenough,?
in which case we do not gain any benefitfrom paying for another redundant translation.Our translation reduction method allows us to setan empirical definition of ?good enough?.
We definean Oracle upper bound ?
to be the estimated BLEUscore using the full set of non-professional transla-tions.
We introduce a parameter ?
to set the allow-able degradation in translation quality.
We train amodel to search for a threshold ?
between acceptableand unacceptable translations for a specific value of?.
For instance, we may fix ?
at 95%, meaningthat the resulting BLEU score should not drop below95% of the ?
after reducing the number of transla-tions.For a new translation, our model scores it, andif its score is higher than ?, then we do not solicitanother translation.
Otherwise, we continue to so-707?
(%) BLEU Score # Trans.90 36.26 1.6391 36.66 1.6992 36.93 1.7893 37.23 1.8594 37.48 1.9395 38.05 2.2196 38.16 2.3097 38.48 2.4798 38.67 2.5999 38.95 2.78100 39.54 3.18Table 1: The relationship between ?
(the allowable devia-tion from the expected upper bound on BLEU score), theBLEU score for translations selected by models from par-tial sets and the average number of translation candidatesset for each source sentence (# Trans).licit translations.
Algorithm 1 details the process ofmodel training and searching for ?.4.1 ExperimentsWe divide data into a training set (10%), a validationset (10%) and a test set (80%).
Each source sen-tence has four translations in total.
We use the val-idation set to search for ?.
The Oracle upper boundon BLEU is set to be 40.13 empirically.
We thenvary the value of ?
from 90% to 100%, and sweepvalues of ?
by incrementing it in step sizes of 0.01.We report results based on a five-fold cross valida-tion, rotating the training, validation and test sets.4.1.1 Baseline and upper boundThe baseline selection method of randomly pick-ing one translation for each source sentence achievesa BLEU score of 29.56.
To establish an upper boundon translation quality, we perform an oracle exper-iment to select best translation for each source seg-ment from full sets of candidates.
It reaches a BLEUscore of 40.13.4.1.2 Translation reducing methodTable 1 shows the results for translation reducingmethod.
The ?
variable correctly predicts the devia-tion in BLEU score when compared to using the fullset of translations.
If we set ?
< 0.95 then we lose 2BLEU points, but we cut the cost of translations in0 1 2 3 4 5 6 7Time (days)a143bvgouf83jea3dd3acpmvdvcaa2yc779twnpohqa1wyssw33m2fz2a3b84pq645okwba132zmwemnnusaa3sw1e5d0b9v9aa1es9zcdrlgxlsa2xknsbfsj3hsoa4x4g5ttibjera28z6a8uc4er3xa1hb5veh552cysa39gcdog0zj64oa2llfcd7di80k3a28e6z78qj2yz6a3u16uhguaktzsa8v7wa74iohz9a31n8vegvccz9aa2aktvoca80377a2qlm59qc9g1ufa2jtc8u7z5z9tfa21xirv18up71ha1is07hajk7bzra1fij2sbw160xta1u0z1mafqeh9ya7o9tyb0xcikga2yfc3l62fkzfra3fq8i38xt2b4za33mu4sfa9v8eia3bz8b0jpubzqqa1aczgd5azz3r7a1vbzioywe4osha2de039cxxjugaa237ydzvlsvdzwa1sanjgoj47idfa2u20xxn0ob88ealzgu09bjzsiwa353ocl6lm6m4oa2i57ww1b3evwxalrghxunh1uv7amwxjmcv94h5sa2pwmdzucikw4ca3hs2e871iw2fiayowrg5s0py3fa3kwcqj39dxkt4az9utcfpk0udea2dsltew8ffmbva172x4w90uost1a34ce07kjic192a1kpcqmdzmxxzwa2iouac3vzbks6Figure 2: A time-series plot of all of the translationsproduced by Turkers (identified by their WorkerID serialnumber).
Turkers are sorted with the best translator at thetop of the y-axis.
Each tick represent a single translationand black means better than average quality.half, since we pay for only two translations of eachsource segment on average.5 Choosing Better TranslatorsThe second mechanism that we use to optimize costis to reduce the number of non-professional trans-lators that we hire.
Our goal is to quickly identifywhether Turkers are good or bad translators, so thatwe can continue to hire only the good translators andstop hiring the bad translators after they are identi-fied as such.
Before presenting our method, we firstdemonstrate that Turkers produce consistent qualitytranslations over time.5.1 Turkers?
behavior in translating sentencesDo Turkers produce good (or bad) translations con-sistently or not?
Are some Turkers consistent andothers not?
We used the professional translationsas a gold-standard to analyze the individual Turk-ers, and we found that most Turkers?
performancestayed surprisingly consistent over time.Figure 2 illustrates the consistency of workers?quality by plotting quality of their individual trans-lations on a timeline.
The translation quality is com-708puted based on the BLEU against professional trans-lations.
Each tick represent a single translation anddepicts the BLEU score using two colors.
The tickis black if its BLEU score is higher than the medianand it is red otherwise.
Good translators tend to pro-duce consistently good translations and bad transla-tors rarely produce good translations.5.2 Evaluating RankingsWe use weighted Pearson correlation (Pozzi et al,2012) to evaluate our ranking of workers againstgold standard ranking.
Since workers translated dif-ferent number of sentences, it is more important torank the workers who translated more sentences cor-rectly.
Taking the importance of workers into con-sideration, we set a weight to each worker using thenumber of translations he or she submitted when cal-culating the correlation.
Given two lists of workerscores x and y and the weight vector w, the weightedPearson correlation ?
can be calculated as:?
(x, y;w) =cov(x, y;w)?cov(x, x;w)cov(y, y;w)(1)where cov is weighted covariance:cov(x, y;w) =?iwi(xi?m(x;w))(yi?m(y;w))?iwi(2)and m is weighted mean:m(x;w) =?iwixi?iwi(3)5.3 Automatically Ranking TranslatorsWe introduce two approaches to rank workers usinga small portion of the work that they submitted.
Thestrategy is to filter out bad workers, and to select thebest translation from translations provided by the re-maining workers.
We propose two different rankingmethods:Ranking workers using their first k translationsWe rank the Turkers using their first few transla-tions by comparing their translations against the pro-fessional translations of those sentences.
Rankingworkers on gold standard data would allow us to dis-card bad workers.
This is similar to the idea of aqualification test in MTurk.Ranking workers using a model In addition toranking workers by comparing them against a goldstandard, we also attempt to automatically predicttheir ranks with a model.
We use the linear re-gression model to score each translation and rankworkers by their model predicted performance.
Themodel predicted performance of the worker w is:performance(w) =?t?Twscore(t)|Tw|(4)where Twis the set of translations completed by theworker w and score(t) is the model predicted scorefor translation t.5.4 ExperimentsAfter we rank workers, we keep top-ranked workersand select the best translation only from their trans-lations.
For both ranking approaches, we vary thenumber of good workers that we retain.We report both rankings?
correlation with the goldstandard ranking.
Since the top worker threshold isvaried and since we change the value of k in firstk sentence ranking, we have a different test set indifferent settings.
Each test set excludes any itemswhich were used to rank the workers, or which didnot have any translations from the top workers ac-cording to our rankings.5.4.1 Gold standard and BaselineWe evaluate ranking quality using the weightedPearson correlation (?)
compared with the gold stan-dard ranking of workers.
To establish the gold stan-dard ranking, we score each Turker based on theBLEU score comparing all of his or her translationsto the corresponding professional references.We use the ranking by the MERT model devel-oped by Zaidan and Callison-Burch (2011) as base-line.
It achieves a correlation of 0.73 against the goldstandard ranking.5.4.2 Ranking workers using their first ktranslationsWithout using any model, we rank workers usingtheir first k translations.
We select best translationof each source sentence from the top ranked workerwho translated that sentence.Table 2 shows the results of Pearson correlationsfor different value of k. As k increases, our rankings7090 10 20 30 40 5001020304050Ranking Turkers: Gold Ranking vs. First 20 Sentences RankingGold RankingFirst 20Sentences Ranking lllllllllllll llllllllllllllllllllllllllllllllllFigure 3: Correlation between gold standard rankingand ranking computed using the first 20 sentences as cal-ibration.
Each bubble represents a worker.
The radiusof each bubble shows the relative volume of translationscompleted by the worker.
The weighted correlation is0.94.0 10 20 30 40 5001020304050Ranking Turkers: Gold Ranking vs. Model RankingGold RankingModel RankingllllllllllllllllllllllllllllllllllllllllllllFigure 4: Correlation between gold standard rankingand our model?s ranking.
The corresponding weightedcorrelation is 0.95.fit the gold ranking better.
Consequently, we candecide whether to continue to hire a worker in a veryshort time after analyzing the first k sentences (k ?20) provided by each worker.
Figure 3 shows thecorrelation of gold ranking and the ranking based onworkers?
first 20 sentences.5.4.3 Ranking workers using a modelWe train a linear regression model on 10% of thedata to rank workers.
We use the model to select thebest translation in one of two ways:?
Using the model?s prediction of workers?
rank,and selecting the translation from the bestworker.?
Using the model?s score for each translationand selecting the highest scoring translation ofeach source sentence.Table 3 shows that the model trained on all fea-tures achieves a very high correlation with the goldstandard ranking (Pearson?s ?
= 0.95), and a BLEUscore of 39.80.Figure 4 presents a visualization of the gold rank-ing and model ranking.
The workers who producethe largest number of translations (large bubbles inthe figure) are ranked extremely well.5.5 Filtering out bad workersRanking translators would allow us to reduce costsby only re-hiring top workers.
Table 4 shows whathappens when we vary the percentage of top rankedworkers we retain.
In general, the model does agood job of picking the best translations from theremaining good translators.
Compared to actuallyknowing the gold ranking, the model loses only 0.55BLEU when we filter out 75% of the workers.
In thiscase we only need to solicit two translations for eachsource sentence on average.6 Cost AnalysisWe have introduced several ways of significantlylowering the costs associated with crowdsourcingtranslations when a large amount of data are so-licited (on the order of millions of samples):?
We show that after we have collected one trans-lation of a source sentence, we can consult amodel that predicts whether its quality is suffi-ciently high or whether we should pay to havethe sentence re-translated.
The cost savingsfor non-professionals here comes from reduc-ing the number of redundant translations.
We710Proportion ofCalibration Data ?First k sentences Percentage1 0.7% 0.212 1.3% 0.383 2.0% 0.414 2.7% 0.565 3.3% 0.7010 6.6% 0.8120 13.3% 0.9430 19.9% 0.9640 26.6% 0.9850 33.2% 0.9860 39.8% 0.98Table 2: Pearson Correlations for calibration data in dif-ferent proportion.
The percentage column shows whatproportion of the whole data set is used for calibration.BLEUFeature Set ?
rank score(S)entence features 0.80 36.66 37.84(W)orker features 0.78 36.92 36.92(R)anking features 0.81 36.94 35.69Calibration features 0.93 38.27 38.27S+W+R features 0.86 37.39 38.69S+W+R+Bilingual features 0.88 37.59 39.23All features 0.95 38.37 39.80Baseline (MERT) 0.73 - 38.99Table 3: Correlation (?)
and translation quality for thevarious features used by our model.
Translation quality iscomputed by selecting best translations based on model-predicted ranking for workers (rank) and model-predictedscores for translations (score).
Here we do not filter outbad workers when selecting the best translation.can save almost half of the cost associated withnon-professional translations to get 95% of thetranslation quality using the full set of redun-dant translations.?
We show that we can quickly identify badtranslators, either by having them first trans-late a small number of sentences to be testedagainst professional translations, or by estimat-ing their performance using a feature-based lin-ear regression model.
The cost savings fornon-professionals here comes from not hiringTop BLEU(%) random model gold ?
# Trans25 29.85 38.53 39.08 0.55 1.9550 29.80 38.40 39.00 0.60 2.7375 29.76 38.37 38.98 0.61 3.48100 29.83 38.37 38.99 0.62 4.00Table 4: A comparison of the translation quality when weretain the top translators under different rankings.
Therankings shown are random, the model?s ranking (usingall features from Table 3) and the gold ranking.
?
is thedifference between the BLEU scores for the gold rankingand the model ranking.
# Trans is the average number oftranslations needed for each source sentence.bad workers.
Similarly, we reduce the non-professional translation cost to the half of theoriginal cost.?
In both cases we need some amount of profes-sionally translated materials to use as a goldstandard for calibration.
Although the unit costfor each reference is much higher than the unitcost for each non-professional translation, thecost associated with non-professional transla-tions can dominate the total cost since the largeamount of data need to be collected.
Thus,we focus on reducing cost associated with non-professional translations.7 Related WorkSheng et al (2008) focused on training a machinelearning model from noisy labels.
We cannot al-ways get high-quality labeled data from crowdsourc-ing, but we can still ensure that a model trainedon the data is accurate by redundantly labeling thedata.
Sheng et al (2008) proposed a framework forrepeated-labeling that resolves the uncertainty in la-beling via majority voting.
The experimental resultsshow that a model?s accuracy is improved even if la-bels in its training data are noisy and imperfect.
Aslong as the integrated quality (the probability of theintegrated labeling being correct) is higher than 0.5,repeated labeling benefits model training.Passonneau and Carpenter (2013) created aBayesian model of annotation.
They applied it tothe problem of word sense annotation.
Passonneauand Carpenter (2013) also proposed an approach todetect and avoid spam workers.
They measured the711performance of worker by comparing worker?s la-bels to the current majority labels.
Workers with badperformance can be identified and blocked.Lin et al (2014) examined the relationship be-tween worker accuracy and budget in the contextof using crowdsourcing to train a machine learningclassifier.
They show that if the goal is to train a clas-sifier on the labels, that the properties of the clas-sifier will determine whether it is better to re-labeldata (resulting in higher quality labels) or get moresingle labeled items (of lower quality).
They showedthat classifiers with weak inductive bias benefit morefrom relabeling, and that relabeling is more impor-tant when worker accuracy is low.Novotney and Callison-Burch (2010) showed asimilar result for training an automatic speech recog-nition (ASR) system.
When creating training datafor an ASR system, given a fixed budget, their sys-tem?s accuracy was higher when it is trained on morelow quality transcription data compared to when itwas trained on fewer high quality transcriptions.8 ConclusionIn this paper, we propose two mechanisms to op-timize cost: a translation reducing method and atranslator reducing method.
They have differentapplicable scenarios for large corpus construction.The translation reducing method works if there ex-ists a specific requirement that the quality mustreach a certain threshold.
This model is most effec-tive when reasonable amounts of pre-existing pro-fessional translations are available for setting themodel?s threshold.
The translator reducing methodis very simple and easy to implement.
This approachis inspired by the intuition that workers?
perfor-mance is consistent.
The translator reducing methodis suitable for crowdsourcing tasks which do nothave specific requirements about the quality of thetranslations, or when only very limited amounts ofgold standard data is available.AcknowledgementsThis material is based on research sponsored bya DARPA Computer Science Study Panel phase 3award entitled ?Crowdsourcing Translation?
(con-tract D12PC00368).
The views and conclusionscontained in this publication are those of the authorsand should not be interpreted as representing offi-cial policies or endorsements by DARPA or the U.S.Government.
This research was supported by theJohns Hopkins University Human Language Tech-nology Center of Excellence and through gifts fromMicrosoft, Google and Facebook.ReferencesVamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation systems?In Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, pages 62?65.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 1?12.Christopher H Lin, Mausam, and Daniel S Weld.
2014.To re (label), or not to re (label).
In Proceedings of the2014 AAAI Conference on Human Computation andCrowdsourcing.Scott Novotney and Chris Callison-Burch.
2010.
Cheap,fast and good enough: Automatic speech recognitionwith non-expert transcription.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-HLT), pages 207?215.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1(ACL), pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics (ACL), pages 311?318.Rebecca J Passonneau and Bob Carpenter.
2013.
Thebenefits of a model of annotation.
In Proceedings ofthe 7th Linguistic Annotation Workshop and Interop-erability with Discourse, pages 187?195.Matt Post, Chris Callison-Burch, and Miles Osborne.2012.
Constructing parallel corpora for six indianlanguages via crowdsourcing.
In Proceedings of theSeventh Workshop on Statistical Machine Translation,pages 401?409.F Pozzi, T Di Matteo, and T Aste.
2012.
Exponen-tial smoothing weighted correlations.
The EuropeanPhysical Journal B-Condensed Matter and ComplexSystems, 85(6):1?21.712Victor S Sheng, Foster Provost, and Panagiotis G Ipeiro-tis.
2008.
Get another label?
Improving data qual-ity and data mining using multiple, noisy labelers.
InProceedings of the 14th ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 614?622.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 254?263.Omar F. Zaidan and Chris Callison-Burch.
2011.
Crowd-sourcing translation: Professional quality from non-professionals.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies (ACL-HLT),pages 1220?1229.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F Zaidan, and Chris Callison-Burch.2012.
Machine translation of arabic dialects.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (NAACL-HLT), pages 49?59.Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,Richard M Schwartz, and John Makhoul.
2013.
Sys-tematic comparison of professional and crowdsourcedreference translations for machine translation.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (NAACL-HLT), pages 612?616.713
