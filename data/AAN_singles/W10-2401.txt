Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 1?11,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsReport of NEWS 2010 Transliteration Generation Shared TaskHaizhou Li?, A Kumaran?, Min Zhang?
and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg?Multilingual Systems Research, Microsoft Research IndiaA.Kumaran@microsoft.comAbstractThis report documents the Translitera-tion Generation Shared Task conducted asa part of the Named Entities Workshop(NEWS 2010), an ACL 2010 workshop.The shared task features machine translit-eration of proper names from English to9 languages and from 3 languages to En-glish.
In total, 12 tasks are provided.
7teams from 5 different countries partici-pated in the evaluations.
Finally, 33 stan-dard and 8 non-standard runs are submit-ted, where diverse transliteration method-ologies are explored and reported on theevaluation data.
We report the results with4 performance metrics.
We believe that theshared task has successfully achieved itsobjective by providing a common bench-marking platform for the research commu-nity to evaluate the state-of-the-art tech-nologies that benefit the future researchand development.1 IntroductionNames play a significant role in many NaturalLanguage Processing (NLP) and Information Re-trieval (IR) systems.
They are important in CrossLingual Information Retrieval (CLIR) and Ma-chine Translation (MT) as the system performancehas been shown to positively correlate with thecorrect conversion of names between the lan-guages in several studies (Demner-Fushman andOard, 2002; Mandl and Womser-Hacker, 2005;Hermjakob et al, 2008; Udupa et al, 2009).
Thetraditional source for name equivalence, the bilin-gual dictionaries ?
whether handcrafted or sta-tistical ?
offer only limited support because newnames always emerge.All of the above point to the critical need for ro-bust Machine Transliteration technology and sys-tems.
Much research effort has been made to ad-dress the transliteration issue in the research com-munity (Knight and Graehl, 1998; Meng et al,2001; Li et al, 2004; Zelenko and Aone, 2006;Sproat et al, 2006; Sherif and Kondrak, 2007;Hermjakob et al, 2008; Al-Onaizan and Knight,2002; Goldwasser and Roth, 2008; Goldberg andElhadad, 2008; Klementiev and Roth, 2006; Ohand Choi, 2002; Virga and Khudanpur, 2003; Wanand Verspoor, 1998; Kang and Choi, 2000; Gaoet al, 2004; Zelenko and Aone, 2006; Li et al,2009b; Li et al, 2009a).
These previous workfall into three categories, i.e., grapheme-based,phoneme-based and hybrid methods.
Grapheme-based method (Li et al, 2004) treats translitera-tion as a direct orthographic mapping and onlyuses orthography-related features while phoneme-based method (Knight and Graehl, 1998) makesuse of phonetic correspondence to generate thetransliteration.
Hybrid method refers to the com-bination of several different models or knowledgesources to support the transliteration generation.The first machine transliteration shared task (Liet al, 2009b; Li et al, 2009a) was held in NEWS2009 at ACL-IJCNLP 2009.
It was the first timeto provide common benchmarking data in diverselanguage pairs for evaluation of state-of-the-arttechniques.
NEWS 2010 is a continued effort ofNEWS 2009.
It builds on the foundations estab-lished in the first transliteration shared task andextends the scope to include new language pairs.The rest of the report is organised as follows.Section 2 outlines the machine transliteration taskand the corpora used and Section 3 discusses themetrics chosen for evaluation, along with the ratio-nale for choosing them.
Sections 4 and 5 presentthe participation in the shared task and the resultswith their analysis, respectively.
Section 6 con-cludes the report.12 Transliteration Shared TaskIn this section, we outline the definition and thedescription of the shared task.2.1 ?Transliteration?
: A definitionThere exists several terms that are used inter-changeably in the contemporary research litera-ture for the conversion of names between twolanguages, such as, transliteration, transcription,and sometimes Romanisation, especially if Latinscripts are used for target strings (Halpern, 2007).Our aim is not only at capturing the name con-version process from a source to a target lan-guage, but also at its practical utility for down-stream applications, such as CLIR and MT.
There-fore, we adopted the same definition of translit-eration as during the NEWS 2009 workshop (Liet al, 2009a) to narrow down ?transliteration?
tothree specific requirements for the task, as fol-lows:?Transliteration is the conversion of a givenname in the source language (a text string in thesource writing system or orthography) to a namein the target language (another text string in thetarget writing system or orthography), such thatthe target language name is: (i) phonemicallyequivalent to the source name (ii) conforms to thephonology of the target language and (iii) matchesthe user intuition of the equivalent of the sourcelanguage name in the target language, consider-ing the culture and orthographic character usagein the target language.
?In NEWS 2010, we introduce threeback-transliteration tasks.
We define back-transliteration as a process of restoring translit-erated words to their original languages.
Forexample, NEWS 2010 offers the tasks to convertwestern names written in Chinese and Thai intotheir original English spellings, or romanizedJapanese names into their original Kanji writings.2.2 Shared Task DescriptionFollowing the tradition in NEWS 2009, the sharedtask at NEWS 2010 is specified as development ofmachine transliteration systems in one or more ofthe specified language pairs.
Each language pairof the shared task consists of a source and a targetlanguage, implicitly specifying the transliterationdirection.
Training and development data in eachof the language pairs have been made available toall registered participants for developing a translit-eration system for that specific language pair usingany approach that they find appropriate.At the evaluation time, a standard hand-craftedtest set consisting of between 1,000 and 3,000source names (approximately 10% of the train-ing data size) have been released, on which theparticipants are required to produce a ranked listof transliteration candidates in the target languagefor each source name.
The system output istested against a reference set (which may includemultiple correct transliterations for some sourcenames), and the performance of a system is cap-tured in multiple metrics (defined in Section 3),each designed to capture a specific performancedimension.For every language pair every participant is re-quired to submit at least one run (designated as a?standard?
run) that uses only the data provided bythe NEWS workshop organisers in that languagepair, and no other data or linguistic resources.
Thisstandard run ensures parity between systems andenables meaningful comparison of performanceof various algorithmic approaches in a given lan-guage pair.
Participants are allowed to submitmore ?standard?
runs, up to 4 in total.
If more thanone ?standard?
runs is submitted, it is required toname one of them as a ?primary?
run, which isused to compare results across different systems.In addition, up to 4 ?non-standard?
runs could besubmitted for every language pair using either databeyond that provided by the shared task organisersor linguistic resources in a specific language, orboth.
This essentially may enable any participantto demonstrate the limits of performance of theirsystem in a given language pair.The shared task timelines provide adequate timefor development, testing (approximately 1 monthafter the release of the training data) and the finalresult submission (7 days after the release of thetest data).2.3 Shared Task CorporaWe considered two specific constraints in select-ing languages for the shared task: language diver-sity and data availability.
To make the shared taskinteresting and to attract wider participation, it isimportant to ensure a reasonable variety amongthe languages in terms of linguistic diversity, or-thography and geography.
Clearly, the ability ofprocuring and distributing a reasonably large (ap-proximately 10K paired names for training andtesting together) hand-crafted corpora consisting2primarily of paired names is critical for this pro-cess.
At the end of the planning stage and afterdiscussion with the data providers, we have cho-sen the set of 12 tasks shown in Table 1 (Li et al,2004; Kumaran and Kellner, 2007; MSRI, 2009;CJKI, 2010).NEWS 2010 leverages on the success of NEWS2009 by utilizing the training and dev data ofNEWS 2009 as the training data of NEWS 2010and the test data of NEWS 2009 as the dev dataof NEWS 2010.
NEWS 2010 provides totally newtest data across all 12 tasks for evaluation.
In ad-dition to the 7 tasks inherited from NEWS 2009,NEWS 2010 is enhanced with 5 new tasks, threenew languages (Arabic, Bangla and Thai) and twoback-transliteration (Chinese to English and Thaito English).The names given in the training sets for Chi-nese, Japanese, Korean and Thai languages areWestern names and their respective translitera-tions; the Japanese Name (in English)?
JapaneseKanji data set consists only of native Japanesenames; the Arabic data set consists only of nativeArabic names.
The Indic data set (Hindi, Tamil,Kannada, Bangla) consists of a mix of Indian andWestern names.For all of the tasks chosen, we have beenable to procure paired names data between thesource and the target scripts and were able tomake them available to the participants.
Forsome language pairs, such as English-Chinese andEnglish-Thai, there are both transliteration andback-transliteration tasks.
Most of the task are justone-way transliteration, although Indian data setscontained mixture of names of both Indian andWestern origins.
The language of origin of thenames for each task is indicated in the first columnof Table 1.Finally, it should be noted here that the corporaprocured and released for NEWS 2010 representperhaps the most diverse and largest corpora to beused for any common transliteration tasks today.3 Evaluation Metrics and RationaleThe participants have been asked to submit resultsof up to four standard and four non-standard runs.One standard run must be named as the primarysubmission and is used for the performance sum-mary.
Each run contains a ranked list of up to10 candidate transliterations for each source name.The submitted results are compared to the groundtruth (reference transliterations) using 4 evaluationmetrics capturing different aspects of translitera-tion performance.
We have dropped two MAPmetrics used in NEWS 2009 because they don?toffer additional information to MAPref .
Since aname may have multiple correct transliterations,all these alternatives are treated equally in the eval-uation, that is, any of these alternatives is consid-ered as a correct transliteration, and all candidatesmatching any of the reference transliterations areaccepted as correct ones.The following notation is further assumed:N : Total number of names (sourcewords) in the test setni : Number of reference transliterationsfor i-th name in the test set (ni ?
1)ri,j : j-th reference transliteration for i-thname in the test setci,k : k-th candidate transliteration (systemoutput) for i-th name in the test set(1 ?
k ?
10)Ki : Number of candidate transliterationsproduced by a transliteration system3.1 Word Accuracy in Top-1 (ACC)Also known as Word Error Rate, it measures cor-rectness of the first transliteration candidate in thecandidate list produced by a transliteration system.ACC = 1 means that all top candidates are cor-rect transliterations i.e.
they match one of the ref-erences, and ACC = 0 means that none of the topcandidates are correct.ACC = 1NN?i=1{1 if ?ri,j : ri,j = ci,1;0 otherwise}(1)3.2 Fuzziness in Top-1 (Mean F-score)The mean F-score measures how different, on av-erage, the top transliteration candidate is from itsclosest reference.
F-score for each source wordis a function of Precision and Recall and equals 1when the top candidate matches one of the refer-ences, and 0 when there are no common charactersbetween the candidate and any of the references.Precision and Recall are calculated based onthe length of the Longest Common Subsequence(LCS) between a candidate and a reference:LCS(c, r) = 12(|c|+ |r| ?
ED(c, r)) (2)3Name origin Source script Target script Data Owner Data Size Task IDTrain Dev TestWestern English Chinese Institute for Infocomm Research 32K 6K 2K EnChWestern Chinese English Institute for Infocomm Research 25K 5K 2K ChEnWestern English Korean Hangul CJK Institute 5K 2K 2K EnKoWestern English Japanese Katakana CJK Institute 23K 3K 3K EnJaJapanese English Japanese Kanji CJK Institute 7K 3K 3K JnJkArabic Arabic English CJK Institute 25K 2.5K 2.5K ArAeMixed English Hindi Microsoft Research India 10K 2K 2K EnHiMixed English Tamil Microsoft Research India 8K 2K 2K EnTaMixed English Kannada Microsoft Research India 8K 2K 2K EnKaMixed English Bangla Microsoft Research India 10K 2K 2K EnBaWestern English Thai NECTEC 26K 2K 2K EnThWestern Thai English NECTEC 24K 2K 2K ThEnTable 1: Source and target languages for the shared task on transliteration.where ED is the edit distance and |x| is the lengthof x.
For example, the longest common subse-quence between ?abcd?
and ?afcde?
is ?acd?
andits length is 3.
The best matching reference, thatis, the reference for which the edit distance hasthe minimum, is taken for calculation.
If the bestmatching reference is given byri,m = argminj(ED(ci,1, ri,j)) (3)then Recall, Precision and F-score for i-th wordare calculated asRi =LCS(ci,1, ri,m)|ri,m|(4)Pi =LCS(ci,1, ri,m)|ci,1|(5)Fi = 2Ri ?
PiRi + Pi(6)?
The length is computed in distinct Unicodecharacters.?
No distinction is made on different charactertypes of a language (e.g., vowel vs. conso-nants vs. combining diereses etc.
)3.3 Mean Reciprocal Rank (MRR)Measures traditional MRR for any right answerproduced by the system, from among the candi-dates.
1/MRR tells approximately the averagerank of the correct transliteration.
MRR closer to 1implies that the correct answer is mostly producedclose to the top of the n-best lists.RRi ={minj 1j if ?ri,j , ci,k : ri,j = ci,k;0 otherwise}(7)MRR = 1NN?i=1RRi (8)3.4 MAPrefMeasures tightly the precision in the n-best can-didates for i-th source name, for which referencetransliterations are available.
If all of the refer-ences are produced, then the MAP is 1.
Let?s de-note the number of correct candidates for the i-thsource word in k-best list as num(i, k).
MAPrefis then given byMAPref =1NN?i1ni( ni?k=1num(i, k))(9)4 Participation in Shared Task7 teams from 5 countries and regions (Canada,Hong Kong, India, Japan, Thailand) submittedtheir transliteration results.Two teams have participated in all or almost alltasks while others participated in 1 to 4 tasks.
Eachlanguage pair has attracted on average around 4teams.
The details are shown in Table 3.Teams are required to submit at least one stan-dard run for every task they participated in.
Intotal, we receive 33 standard and 8.
Table 2shows the number of standard and non-standardruns submitted for each task.
It is clear that themost ?popular?
task is transliteration from Englishto Hindi attempted by 5 participants.
The nextmost popular are other Indic scripts (Tamil, Kan-nada, Bangla) and Thai, attempted by 3 partici-pants.
This is somewhat different from NEWS2009, where the two most popular tasks were En-glish to Hindi and English to Chinese translitera-tion.4English toChineseChinese toEnglishEnglish toThaiThai to En-glishEnglish toHindiEnglish toTamilLanguage pair code EnCh ChEn EnTh ThEn EnHi EnTaStandard runs 5 2 2 2 7 3Non-standard runs 0 0 1 1 2 1English toKannadaEnglish toJapaneseKatakanaEnglishto KoreanHangulEnglish toJapaneseKanjiArabic toEnglishEnglish toBengali(Bangla)Language pair code EnKa EnJa EnKo JnJk ArAe EnBaStandard runs 3 2 1 1 2 3Non-standard runs 1 0 0 0 0 2Table 2: Number of runs submitted for each task.
Number of participants coincides with the number ofstandard runs submitted.TeamIDOrganisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArAe EnBa1?
IIT, Bombay x2 University of Alberta x x x x x x x x x x x x3 x4 City University ofHong Kongx x5 NICT x x x x x x x x6 x x7 Jadavpur University x x x xTable 3: Participation of teams in different tasks.
?Participation without a system paper.5 Task Results and Analysis5.1 Standard runsAll the results are presented numerically in Ta-bles 4?15, for all evaluation metrics.
These are theofficial evaluation results published for this editionof the transliteration shared task.Among the four submitted system papers1,Song et al (2010) and Finch and Sumita (2010)adopt the approach of phrase-based statistical ma-chine transliteration (Finch and Sumita, 2008),an approach initially developed for machine trans-lation (Koehn et al, 2003) while Das et al(2010) adopts the approach of Conditional Ran-dom Fields (CRF) (Lafferty et al, 2001).
Jiampo-jamarn et al (2010) further develop DirectTL ap-proach presented at the previous NEWS work-shop (Jiampojamarn et al, 2009), achieving verygood performance in the NEWS 2010.An example of a completely language-1To maintain anonymity, papers of the teams that submit-ted anonymous results are not cited in this report.independent approach is (Finch and Sumita,2010).
Other participants used language-independent approach but added language-specific pre- or post-processing (Jiampojamarnet al, 2010; Das et al, 2010; Song et al, 2010),including name origin recognition for English toHindi task (Jiampojamarn et al, 2010).Combination of different models via re-rankingof their outputs has been used in most of the sys-tems (Das et al, 2010; Song et al, 2010; Finch andSumita, 2010).
In fact, one system (Song et al,2010) is mostly devoted to re-ranking of the sys-tem output to achieve significant improvement ofthe ACC (accuracy in top-1) results compared tothe same system in NEWS 2009 workshop (Song,2009).Compared the same seven tasks among theNEWS 2009 and the NEWS 2010 (almost sametraining sets, but different test sets), we can seethat the performance in the NEWS 2010 drops ex-cept the English to Korean task.
This could be dueto the fact that NEWS 2010 introduces a entirely5new test set, which come from different sourcesthan the train and dev sets, while NEWS 2009have all train, dev and test sets from the samesources.As far as back-transliteration is concerned, wecan see that English-to-Thai and Thai-to-Englishhave the similar performance.
However, Chinese-to-English back transliteration performs muchworse than English-to-Chinese forward transliter-ation.
This could be due to the fact that Thaiand English are alphabet languages in nature whileChinese is not.
As a result, Chinese have muchfewer transliteration units than English and Thai.In other words, Chinese to English translitera-tion is a one-to-many mapping while English-to-Chinese is a many-to-one mapping.
The later onehas fewer mapping ambiguities.5.2 Non-standard runsFor the non-standard runs there exist no restric-tions on the use of data or other linguistic re-sources.
The purpose of non-standard runs is tosee how best personal name transliteration can be,for a given language pair.
In NEWS 2010, the ap-proaches used in non-standard runs are typical andmay be summarised as follows:?
Pronunciation dictionaries to convert wordsto their phonetic transcription (Jiampojamarnet al, 2010).?
Web search.
First, transliteration candidatesare generated.
A Web search is then per-formed to re-affirm or re-rank the candi-dacy (Das et al, 2010).Unfortunately, these additional knowledge usedin the non-standard runs is not helpful since allnon-standard runs perform worse than their cor-responding standard runs.
This would be an inter-esting issue to look into.6 Conclusions and Future PlansThe Transliteration Generation Shared Task inNEWS 2010 shows that the community has acontinued interest in this area.
This report sum-marizes the results of the shared task.
Again,we are pleased to report a comprehensive cal-ibration and baselining of machine translitera-tion approaches as most state-of-the-art machinetransliteration techniques are represented in theshared task.
The most popular techniques suchas Phrase-Based Machine Transliteration (Koehnet al, 2003), system combination and re-ranking,are inspired by recent progress in statistical ma-chine translation.
As the standard runs are lim-ited by the use of corpus, most of the systems areimplemented under the direct orthographic map-ping (DOM) framework (Li et al, 2004).
Whilethe standard runs allow us to conduct meaningfulcomparison across different algorithms, we recog-nise that the non-standard runs open up more op-portunities for exploiting larger linguistic corpora.It is also noted that two systems have reportedsignificant performance improvement over theirNEWS 2009 systems.NEWS 2010 Shared Task represents a success-ful debut of a community effort in driving machinetransliteration techniques forward.
We would liketo continue this event in the future conference topromote the machine transliteration research anddevelopment.AcknowledgementsThe organisers of the NEWS 2010 Shared Taskwould like to thank the Institute for InfocommResearch (Singapore), Microsoft Research India,CJK Institute (Japan) and National Electronics andComputer Technology Center (Thailand) for pro-viding the corpora and technical support.
Withoutthose, the Shared Task would not be possible.
Wethank those participants who identified errors inthe data and sent us the errata.
We also want tothank the members of programme committee fortheir invaluable comments that improve the qual-ity of the shared task papers.
Finally, we wish tothank all the participants for their active participa-tion that have made this first machine translitera-tion shared task a comprehensive one.6ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proc.ACL-2002Workshop: Computational Apporaches toSemitic Languages, Philadelphia, PA, USA.CJKI.
2010.
CJK Institute.
http://www.cjk.org/.Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ek-bal, and Sivaji Bandyopadhyay.
2010.
English toIndian languages machine transliteration system atNEWS 2010.
In Proc.
ACL Named Entities Work-shop Shared Task.D.
Demner-Fushman and D. W. Oard.
2002.
The ef-fect of bilingual term list size on dictionary-basedcross-language information retrieval.
In Proc.
36-thHawaii Int?l.
Conf.
System Sciences, volume 4, page108.2.Andrew Finch and Eiichiro Sumita.
2008.
Phrase-based machine transliteration.
In Proc.
3rd Int?l.Joint Conf NLP, volume 1, Hyderabad, India, Jan-uary.Andrew Finch and Eiichiro Sumita.
2010.
Transliter-ation using a phrase-based statistical machine trans-lation system to re-score the output of a joint multi-gram model.
In Proc.
ACL Named Entities Work-shop Shared Task.Wei Gao, Kam-Fai Wong, and Wai Lam.
2004.Phoneme-based transliteration of foreign names forOOV problem.
In Proc.
IJCNLP, pages 374?381,Sanya, Hainan, China.Yoav Goldberg andMichael Elhadad.
2008.
Identifica-tion of transliterated foreign words in Hebrew script.In Proc.
CICLing, volume LNCS 4919, pages 466?477.Dan Goldwasser and Dan Roth.
2008.
Translitera-tion as constrained optimization.
In Proc.
EMNLP,pages 353?362.Jack Halpern.
2007.
The challenges and pitfallsof Arabic romanization and arabization.
In Proc.Workshop on Comp.
Approaches to Arabic Script-based Lang.Ulf Hermjakob, Kevin Knight, and Hal Daume?.
2008.Name translation in statistical machine translation:Learning when to transliterate.
In Proc.
ACL,Columbus, OH, USA, June.Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,Kenneth Dwyer, and Grzegorz Kondrak.
2009.
Di-recTL: a language-independent approach to translit-eration.
In Proc.
ACL/IJCNLP Named EntitiesWorkshop Shared Task.Sittichai Jiampojamarn, Kenneth Dwyer, ShaneBergsma, Aditya Bhargava, Qing Dou, Mi-YoungKim, and Grzegorz Kondrak.
2010.
Translitera-tion generation and mining with limited training re-sources.
In Proc.
ACL Named Entities WorkshopShared Task.Byung-Ju Kang and Key-Sun Choi.
2000.English-Korean automatic transliteration/back-transliteration system and character alignment.
InProc.
ACL, pages 17?18, Hong Kong.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discov-ery from multilingual comparable corpora.
In Proc.21st Int?l Conf Computational Linguistics and 44thAnnual Meeting of ACL, pages 817?824, Sydney,Australia, July.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4).P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
HLT-NAACL.A Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proc.
SIGIR,pages 721?722.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
Int?l.Conf.
Machine Learning, pages 282?289.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In Proc.
42nd ACL Annual Meeting, pages 159?166,Barcelona, Spain.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009a.
Report of NEWS 2009 machinetransliteration shared task.
In Proc.
Named EntitiesWorkshop at ACL 2009.Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2009b.
ACL-IJCNLP 2009 NamedEntities Workshop ?
Shared Task on Translitera-tion.
In Proc.
Named Entities Workshop at ACL2009.T.
Mandl and C. Womser-Hacker.
2005.
The effect ofnamed entities on effectiveness in cross-language in-formation retrieval evaluation.
In Proc.
ACM Symp.Applied Comp., pages 1059?1064.Helen M. Meng, Wai-Kit Lo, Berlin Chen, and KarenTang.
2001.
Generate phonetic cognates to han-dle name entities in English-Chinese cross-languagespoken document retrieval.
In Proc.
ASRU.MSRI.
2009.
Microsoft Research India.http://research.microsoft.com/india.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciationand contextual rules.
In Proc.
COLING 2002,Taipei, Taiwan.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proc.
45th Annual Meetingof the ACL, pages 944?951, Prague, Czech Repub-lic, June.7Yan Song, Chunyu Kit, and Hai Zhao.
2010.
Rerank-ing with multiple features for better transliteration.In Proc.
ACL Named Entities Workshop SharedTask.Yan Song.
2009.
Name entities transliteration viaimproved statistical translation on character-levelchunks.
In Proc.
ACL/IJCNLP Named EntitiesWorkshop Shared Task.Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006.Named entity transliteration with comparable cor-pora.
In Proc.
21st Int?l Conf Computational Lin-guistics and 44th Annual Meeting of ACL, pages 73?80, Sydney, Australia.Raghavendra Udupa, K. Saravanan, Anton Bakalov,and Abhijit Bhole.
2009.
?They are out there, ifyou know where to look?
: Mining transliterationsof OOV query terms for cross-language informa-tion retrieval.
In LNCS: Advances in InformationRetrieval, volume 5478, pages 437?448.
SpringerBerlin / Heidelberg.Paola Virga and Sanjeev Khudanpur.
2003.
Translit-eration of proper names in cross-lingual informationretrieval.
In Proc.
ACL MLNER, Sapporo, Japan.Stephen Wan and Cornelia Maria Verspoor.
1998.
Au-tomatic English-Chinese name transliteration for de-velopment of multilingual resources.
In Proc.
COL-ING, pages 1352?1356.Dmitry Zelenko and Chinatsu Aone.
2006.
Discrimi-native methods for transliteration.
In Proc.
EMNLP,pages 612?617, Sydney, Australia, July.8Team ID ACC F -score MRR MAPref OrganisationPrimary runs4 0.477333 0.740494 0.506209 0.455491 City University of Hong Kong2 0.363333 0.707435 0.430168 0.347701 University of AlbertaNon-primary standard runs2 0.362667 0.704284 0.428854 0.347500 University of Alberta2 0.360333 0.706765 0.428990 0.345215 University of Alberta2 0.357000 0.702902 0.419415 0.341567 University of AlbertaTable 4: Runs submitted for English to Chinese task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs4 0.226766 0.749237 0.268557 0.226090 City University of Hong Kong2 0.137209 0.740364 0.197665 0.136702 University of AlbertaTable 5: Runs submitted for Chinese to English back-transliteration task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs5 0.391000 0.872526 0.505264 0.391000 NICT2 0.377500 0.866254 0.467328 0.377500 University of AlbertaNon-standard runs6 0.247000 0.842063 0.366959 0.247000Table 6: Runs submitted for English to Thai task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs5 0.396690 0.872642 0.524511 0.396690 NICT2 0.352056 0.861207 0.450472 0.352056 University of AlbertaNon-standard runs6 0.092778 0.706995 0.131779 0.092778Table 7: Runs submitted for Thai to English back-transliteration task.9Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.456456 0.884199 0.559212 0.456456 University of Alberta5 0.445445 0.883841 0.574195 0.445445 NICT3 0.381381 0.860320 0.403172 0.3813811 0.158158 0.810309 0.231594 0.158158 IIT, Bombay7 0.150150 0.714490 0.307674 0.150150 Jadavpur UniversityNon-primary standard runs2 0.456456 0.885122 0.558203 0.456456 University of Alberta1 0.142142 0.799092 0.205945 0.142142 IIT, BombayNon-standard runs7 0.254254 0.751766 0.369072 0.254254 Jadavpur University7 0.170170 0.738777 0.314335 0.170170 Jadavpur UniversityTable 8: Runs submitted for English to Hindi task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.390000 0.890692 0.515298 0.390000 University of Alberta5 0.390000 0.886560 0.522088 0.390000 NICT7 0.013000 0.562917 0.121233 0.013000 Jadavpur UniversityNon-standard runs7 0.082000 0.759856 0.142317 0.082000 Jadavpur UniversityTable 9: Runs submitted for English to Tamil task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs5 0.371000 0.871131 0.506010 0.371000 NICT2 0.341000 0.867133 0.460189 0.341000 University of Alberta7 0.056000 0.663196 0.111500 0.056000 Jadavpur UniversityNon-standard runs7 0.055000 0.662106 0.168750 0.055000 Jadavpur UniversityTable 10: Runs submitted for English to Kannada task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.397933 0.791233 0.507828 0.398062 University of Alberta5 0.378295 0.782682 0.510096 0.377778 NICTTable 11: Runs submitted for English to Japanese Katakana task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.553604 0.770168 0.672665 0.553835 University of AlbertaTable 12: Runs submitted for English to Korean task.10Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.125937 0.426349 0.201497 0.127339 University of AlbertaTable 13: Runs submitted for English to Japanese Kanji back-transliteration task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.463679 0.923826 0.535097 0.265379 University of Alberta5 0.403014 0.891443 0.512337 0.327418 NICTTable 14: Runs submitted for Arabic to English task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs5 0.411705 0.882858 0.549913 0.411705 NICT2 0.394551 0.876947 0.511876 0.394551 University of Alberta7 0.232089 0.818470 0.325345 0.232089 Jadavpur UniversityNon-standard runs7 0.429869 0.875349 0.526152 0.429869 Jadavpur University7 0.369324 0.845273 0.450589 0.369324 Jadavpur UniversityTable 15: Runs submitted for English to Bengali (Bangla) task.11
