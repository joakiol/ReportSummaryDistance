NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 105?110,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsCombining the Sparsity and Unambiguity Biases for Grammar InductionKewei TuDepartments of Statistics and Computer ScienceUniversity of California, Los AngelesLos Angeles, CA 90095, USAtukw@ucla.eduAbstractIn this paper we describe our participating sys-tem for the dependency induction track of thePASCAL Challenge on Grammar Induction.Our system incorporates two types of induc-tive biases: the sparsity bias and the unambi-guity bias.
The sparsity bias favors a gram-mar with fewer grammar rules.
The unambi-guity bias favors a grammar that leads to un-ambiguous parses, which is motivated by theobservation that natural language is remark-ably unambiguous in the sense that the num-ber of plausible parses of a natural languagesentence is very small.
We introduce our ap-proach to combining these two types of biasesand discuss the system implementation.
Ourexperiments show that both types of inductivebiases are beneficial to grammar induction.1 IntroductionGrammar induction refers to the induction of a for-mal grammar from a corpus of unannotated sen-tences.
There has been significant progress overthe past decade in the research of natural languagegrammar induction.
A variety of approaches andtechniques have been proposed, most of which aredesigned to induce probabilistic dependency gram-mars.
The PASCAL Challenge on Grammar In-duction aims to provide a thorough evaluation ofapproaches to natural language grammar induction.The challenge includes three tracks: inducing de-pendency structures using the gold standard part-of-speech tags, inducing both dependency structuresand part-of-speech tags directly from text, and anopen-resource track which allows other external re-sources to be used.
Ten corpora of nine differentlanguages are used in the challenge: Arabic (Hajic?et al, 2004), Basque (Aduriz et al, 2003), Czech(Hajic?
et al, 2000), Danish (Buch-Kromann et al,2007), Dutch (Beek et al, 2002), EnglishWSJ (Mar-cus et al, 1993), English CHILDES (Sagae et al,2007), Portuguese (Afonso et al, 2002), Slovene(Erjavec et al, 2010), and Swedish (Nivre et al,2006).
For each corpus, a large set of unannotatedsentences are provided as the training data, alongwith a small set of annotated sentences as the devel-opment data; the predictions on the unannotated testdata submitted by challenge participants are evalu-ated against the gold standard annotations.We participate in the track of inducing depen-dency structures from gold standard part-of-speechtags.
Our system incorporates two types of inductivebiases in learning dependency grammars: the spar-sity bias and the unambiguity bias.
The sparsity biasfavors a grammar with fewer grammar rules.
Weemploy two different approaches to inducing spar-sity: Dirichlet priors over grammar rule probabili-ties and an approach based on posterior regulariza-tion (Gillenwater et al, 2010).
The unambiguitybias favors a grammar that leads to unambiguousparses, which is motivated by the observation thatnatural language is remarkably unambiguous in thesense that the number of plausible parses of a naturallanguage sentence is very small.
To induce unam-biguity in the learned grammar we propose an ap-proach named unambiguity regularization based onthe posterior regularization framework (Ganchev etal., 2010).
To combine Dirichlet priors with unam-105biguity regularization, we derive a mean-field varia-tional inference algorithm.
To combine the sparsity-inducing posterior regularization approach with un-ambiguity regularization, we employ a simplistic ap-proach that optimizes the two regularization termsseparately.The rest of the paper is organized as follows.
Sec-tion 2 introduces the two approaches that we employto induce sparsity.
Section 3 introduces the unam-biguity bias and the unambiguity regularization ap-proach.
Section 4 discusses how we combine thesparsity bias with the unambiguity bias.
Section 5provides details of our implementation and trainingprocedure.
Section 6 concludes the paper.2 Sparsity BiasA sparsity bias in grammar induction favors a gram-mar that has fewer grammar rules.
We employ twodifferent approaches to inducing sparsity: Dirichletpriors over grammar rule probabilities and an ap-proach based on posterior regularization (Gillenwa-ter et al, 2010).A probabilistic grammar consists of a set of prob-abilistic grammar rules.
A discrete distribution is de-fined over each set of grammar rules with the sameleft-hand side, and a Dirichlet distribution can beused as the prior of the discrete distribution.
De-note vector ?
of dimension K as the parameter of adiscrete distribution.
Then a Dirichlet prior over ?
isdefined as:P (?
;?1, .
.
.
, ?K) =1B(?)K?i=1?
?i?1iwhere ?
= (?1, .
.
.
, ?K) are the hyperparameters,and B(?)
is the normalization constant.
Typically,all the hyperparameters are set to the same value.It can be shown that if the hyperparameters are lessthan 1, then the Dirichlet prior assigns larger prob-abilities to vectors that have more elements closeto zero.
Therefore, Dirichlet priors can be used toencourage parameter sparsity.
It has been foundthat when applied to dependency grammar induc-tion, Dirichlet priors with hyperparamters set to val-ues less than 1 can slightly improve the accuracy ofthe learned grammar over the maximum-likelihoodestimation (Cohen et al, 2008; Gillenwater et al,2010).Gillenwater et al (2010) proposed a differnt ap-proach to inducing sparsity in dependency gram-mar induction based on the posterior regularizationframework (Ganchev et al, 2010).
They added aregularization term to the posterior of the gram-mar that penalizes the number of unique dependencytypes in the parses of the training data.
More specif-ically, their objective function is:J(?)
= log p(?|X)?minq(KL(q(Z)||p?
(Z|X))+ ?s?cpmaxiEq[?cpi(X,Z)])where ?
is the parameter of the grammar, X is thetraining data, Z is the dependency parses of thetraining data X, ?s is a constant that controls thestrength of the regularization term, c and p rangeover all the tags of the dependency grammar, iranges over all the occurrences of tag c in the train-ing data X, and ?cpi(X,Z) is an indicator func-tion of whether tag p is the dependency head of thei-th occurrence of tag c in the dependency parsesZ.
This objective function is optimized using avariant of the expectation-maximization algorithm(EM), which contains an E-step that optimizes theauxiliary distribution q using the projected subgra-dient method.
It has been shown that this approachachieves higher degree of sparsity than Dirichlet pri-ors and leads to significant improvement in accuracyof the learned grammars.3 Unambiguity BiasThe unambiguity bias favors a grammar that leads tounambiguous parses on natural language sentences(Tu and Honavar, 2012).
This bias is motivated bythe observation that natural language is remarkablyunambiguous in the sense that the number of plau-sible parses of a natural language sentence is verysmall in comparison with the total number of pos-sible parses.
To illustrate this, we randomly samplean English sentence from theWall Street Journal andparse the sentence using the Berkeley parser (Petrovet al, 2006), one of the state-of-the-art English lan-guage parsers.
The estimated total number of pos-sible parses of this sentence is 2 ?
1020 (by assum-ing a complete Chomsky normal form grammar with1060 20 40 60 80 10000.050.10.150.20.25100 Best ParsesProbabilityFigure 1: The probabilities of the 100 best parses of thesample sentence.the same number of nonterminals as in the Berke-ley parser).
However, as shown in Figure 1, mostof the parses have probabilities that are negligiblecompared with the probability of the best parse.To induce unambiguity in the learned grammar,we derive an approach named unambiguity regu-larization (Tu and Honavar, 2012) based on theposterior regularization framework (Ganchev et al,2010).
Specifically, we add into the objective func-tion a regularization term that penalizes the entropyof the parses given the training sentences.
Let Xdenote the set of training sentences, Z denote theset of parses of the training sentences, and ?
denotethe rule probabilities of the grammar.
Our objectivefunction isJ(?)
= log p(?|X)?minq(KL(q(Z)||p?
(Z|X)) + ?uH(q))where ?u is a nonnegative constant that controls thestrength of the regularization term; q is an auxiliarydistribution.
The first term in the objective functionis the log posterior probability of the grammar pa-rameters given the training corpus, and the secondterm minimizes the KL-divergence between the aux-iliary distribution q and the posterior distribution ofZwhile also minimizes the entropy of q.
This objec-tive function is optimized using coordinate ascent inour approach.
It can be shown that the behavior ofour approach is controlled by the value of the pa-rameter ?u.
When ?u = 0, our approach reduces tothe standard EM algorithm.
When ?u ?
1, our ap-proach reduces to the Viterbi EM algorithm, whichconsiders only the best parses of the training sen-tences in the E-step.
When 0 < ?u < 1, our ap-proach falls between standard EM and Viterbi EM:it applies a softmax function to the distribution of theparse zi of each training sentence xi in the E-step:q(zi) = ?ip?(zi|xi)11?
?uwhere ?i is the normalization factor.
To compute q,note that p?
(zi|xi) is the product of a set of grammarrule probabilities, so we can raise all the rule prob-abilities of the grammar to the power of 11?
?u andthen run the normal E-step of the EM algorithm.
Thenormalization of q is included in the normal E-step.We refer to the algorithm in the case of 0 < ?u < 1as the softmax-EM algorithm.The choice of the value of ?u is important in un-ambiguity regularization.
Considering that in gram-mar induction the initial grammar is typically veryambiguous, the value of ?u should be set largeenough to induce unambiguity.
On the other hand,natural language grammars do contain some degreeof ambiguity, so the value of ?u should not be settoo large.
One way to avoid choosing a fixed valueof ?u is to anneal its value.
We start learning witha large value of ?u (e.g., ?u = 1) to strongly pushthe learner away from the highly ambiguous initialgrammar; then we gradually reduce the value of ?u,possibly ending with ?u = 0, to avoid inducing ex-cessive unambiguity in the learned grammar.4 Combining Sparsity and UnambiguityBiasesTo incorporate Dirichlet priors over grammar ruleprobabilities into our unambiguity regularization ap-proach, we derive a mean-field variational inferencealgorithm (Tu and Honavar, 2012).
The algorithmalternately optimizes q(?)
and q(Z).
The optimiza-tion of q(?)
is exactly the same as in the standardmean-field variational inference with Dirichlet pri-ors, in which we obtain a set of weights that are sum-marized from q(?)
(Kurihara and Sato, 2004).
Theoptimization of q(Z) is similar to the E-step of ourapproach discussed in section 3: when 0 < ?u < 1,we raise all the weights to the power of 11?
?u beforerunning the normal step of computing q(Z) in thestandard mean-field variational inference; and when?u ?
1, we use the weights to find the best parse ofeach training sentence and assign probability 1 to it.107The sparsity-inducing posterior regularization ap-proach and our unambiguity regularization approachare based on the same posterior regularizationframework.
To combine these two approaches, thestandard method is to optimize a linear combina-tion of the sparsity and unambiguity regularizationterms in the E-step of the posterior regularization al-gorithm.
Here we employ a simplistic approach in-stead which optimizes the two regularization termsseparately in the E-step.
Specifically, we first ignorethe sparsity regularization term and optimize q(Z)with respect to the unambiguity regularization termusing the approach discussed in section 3.
The opti-mization result is an intermediate distribution q?
(Z).Then we ignore the unambiguity regularization termand optimize q(Z) to minimize the sparsity regular-ization term as well as the KL-divergence betweenq(Z) and q?
(Z).5 Implementation and ExperimentsOur system was built on top of the PR-Dep-Parsingpackage1.
We implemented both approaches intro-duced in section 4, i.e., unambiguity regularizationwith Dirichlet priors and combined posterior regu-larization of sparsity and unambiguity.
For the lat-ter, we did not implement the ?u ?
1 case and theannealing of ?u because of time constraint.We preprocessed the corpora to remove all thepunctuations as denoted by the universal POS tags.One exception is that for the English WSJ corpuswe did not remove the $ symbol because we foundthat removing it significantly decreased the accuracyof the learned grammar.
We combined the providedtraining, development and test set as our training set.We trained our system on the fine POS tags exceptfor the Dutch corpus.
In the Dutch corpus, the finePOS tags are the same as the coarse POS tags ex-cept that each multi-word unit is annotated with theconcatenation of the POS tags of all the componentwords, making the training data for such tags ex-tremely sparse.
So we chose to use the coarse POStags for the Dutch corpus.We employed the informed initialization pro-posed in (Klein and Manning, 2004) and ran our twoapproaches on the training set.
We tuned the param-1Available at http://code.google.com/p/pr-toolkit/eters by coordinate ascent on the development set.The parameters that we tuned include the maximallength of sentences used in training, the valence andback-off strength of the E-DMVmodel, the hyperpa-rameter ?
of Dirichlet priors, the type (PR-S or PR-AS) and strength ?s of sparsity-inducing posteriorregularization, and the strength ?u of unambiguityregularization.
Sparsity-inducing posterior regular-ization has a high computational cost.
Consequently,we were not able to run our second approach onthe English CHILDES corpus and the Czech corpus,and performed relatively limited parameter tuning ofthe second approach on the other eight corpora.Table 1 shows, for each corpus, the approach andthe parameters that we found to perform the best onthe development set and were hence used to learnthe final grammar that produced the submitted pre-dictions on the test set.
Each of our two approacheswas found to be the better approach for five of theten corpora.
The sparsity bias was found to be ben-eficial (i.e., ?
< 1 if Dirichlet priors were used, or?s > 0 if sparsity-inducing posterior regularizationwas used) for six of the ten corpora.
The unambi-guity bias was found to be beneficial (i.e., ?u > 0)for seven of the ten corpora.
This implies the use-fulness of both types of inductive biases in gram-mar induction.
For only one corpus, the EnglishCHILDES corpus, neither the sparsity bias nor theunambiguity bias was found to be beneficial, proba-bly because this corpus is a collection of child lan-guage and the corresponding grammar might be lesssparse and more ambiguous than adult grammars.6 ConclusionIn this paper we have described our participatingsystem for the dependency induction track of thePASCAL Challenge on Grammar Induction.
Oursystem incorporates two types of inductive biases:the sparsity bias and the unambiguity bias.
Thesparsity bias favors a grammar with fewer gram-mar rules.
We employ two types of sparsity biases:Dirichlet priors over grammar rule probabilities andthe sparsity-inducing posterior regularization.
Theunambiguity bias favors a grammar that leads to un-ambiguous parses, which is motivated by the obser-vation that natural language is remarkably unam-biguous in the sense that the number of plausible108Corpus Approach ParametersArabic Dir+UR maxlen = 20, valence = 4/4, back-off = 0.1, ?
= 10?5, ?u = 0.75Basque PR+UR maxlen = 10, valence = 3/3, back-off = 0.1, PR-AS, ?s = 100, ?u = 0Czech Dir+UR maxlen = 10, valence = 3/3, back-off = 0.1, ?
= 1, ?u = 1?
0.1?
iterDanish PR+UR maxlen = 20, valence = 2/1, back-off = 0.33, PR-AS, ?s = 100, ?u = 0.5Dutch PR+UR maxlen = 10, valence = 3/3, back-off = 0, PR-S, ?s = 140, ?u = 0English WSJ Dir+UR maxlen = 10, valence = 2/2, back-off = 0.33, ?
= 1, ?u = 1?
0.01?
iterEnglish CHILDES Dir+UR maxlen = 15, valence = 4/4, back-off = 0.1, ?
= 10, ?u = 0Portuguese PR+UR maxlen = 15, valence = 2/1, back-off = 0, PR-AS, ?s = 140, ?u = 0.5Slovene PR+UR maxlen = 10, valence = 4/4, back-off = 0.1, PR-AS, ?s = 140, ?u = 0Swedish Dir+UR maxlen = 10, valence = 4/4, back-off = 0.1, ?
= 1, ?u = 1?
0.5?
iterTable 1: For each corpus, the approach and the parameters that we found to perform the best on the development setand were hence used to learn the final grammar that produced the submitted predictions on the test set.
In the secondcolumn, ?Dir+UR?
denotes our approach of unambiguity regularization with Dirichlet priors, and ?PR+UR?
denotesour approach of combined posterior regularization of sparsity and unambiguity.
The parameters in the third columnare explained in the main text.parses of a natural language sentence is very small.We propose an approach named unambiguity regu-larization to induce unambiguity based on the poste-rior regularization framework.
To combine Dirich-let priors with unambiguity regularization, we de-rive a mean-field variational inference algorithm.
Tocombine the sparsity-inducing posterior regulariza-tion approach with unambiguity regularization, weemploy a simplistic approach that optimizes the tworegularization terms separately.
We have also in-troduced our implementation and training procedurefor the challenge.
Our experimental results showthat both types of inductive biases are beneficial togrammar induction.ReferencesI.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, , and M. Oronoz.2003.
Construction of a basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT).Susana Afonso, Eckhard Bick, Renato Haber, and DianaSantos.
2002.
?floresta sinta?(c)tica?
: a treebank forPortuguese.
In Proceedings of the 3rd Intern.
Conf.
onLanguage Resources and Evaluation (LREC), pages1968?1703.Van Der Beek, G. Bouma, R. Malouf, G. Van Noord, andRijksuniversiteit Groningen.
2002.
The alpino depen-dency treebank.
In In Computational Linguistics in theNetherlands (CLIN, pages 1686?1691.Matthias Buch-Kromann, Ju?rgen Wedekind, , andJakob Elming.
2007.
The copenhagen danish-english dependency treebank v. 2.0. http://www.buch-kromann.dk/matthias/cdt2.0/.Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.2008.
Logistic normal priors for unsupervised prob-abilistic grammar induction.
In NIPS, pages 321?328.Tomaz Erjavec, Darja Fiser, Simon Krek, and NinaLedinek.
2010.
The jos linguistically tagged corpusof slovene.
In LREC.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 11:2001?2049.Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-nando Pereira, and Ben Taskar.
2010.
Sparsity in de-pendency grammar induction.
In ACL ?10: Proceed-ings of the ACL 2010 Conference Short Papers, pages194?199, Morristown, NJ, USA.
Association for Com-putational Linguistics.Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and BarboraVidova?-Hladka?.
2000.
The Prague DependencyTreebank: A Three-Level Annotation Scenario.
InA.
Abeille?, editor, Treebanks: Building and UsingParsed Corpora, pages 103?127.
Amsterdam:Kluwer.Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, andEmanuel Bes?ka.
2004.
Prague arabic dependencytreebank: Development in data and tools.
In In Proc.of the NEMLAR Intern.
Conf.
on Arabic Language Re-sources and Tools, pages 110?117.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for pcfgs via markovchain monte carlo.
In HLT-NAACL, pages 139?146.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL.109Kenichi Kurihara and Taisuke Sato.
2004.
An appli-cation of the variational Bayesian approach to prob-abilistic contextfree grammars.
In IJCNLP-04 Work-shop beyond shallow analyses.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
COMPUTA-TIONAL LINGUISTICS, 19(2):313?330.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish Treebank with Phrase Struc-ture and Dependency Annotation.
In Proceedings ofthe fifth international conference on Language Re-sources and Evaluation (LREC2006), May 24-26,2006, Genoa, Italy, pages 1392?1395.
European Lan-guage Resource Association, Paris.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In ACL-44: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, pages 433?440,Morristown, NJ, USA.
Association for ComputationalLinguistics.Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-ney, and Shuly Wintner.
2007.
High-accuracy anno-tation and parsing of childes transcripts.
In Proceed-ings of the Workshop on Cognitive Aspects of Com-putational Language Acquisition, CACLA ?07, pages25?32, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kewei Tu and Vasant Honavar.
2012.
Unambiguity reg-ularization for unsupervised learning of probabilisticgrammars.
Technical report, Computer Science, IowaState University.110
