The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 216?224,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsNUS at the HOO 2012 Shared TaskDaniel Dahlmeier1, Hwee Tou Ng1,2, and Eric Jun Feng Ng21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore{danielhe,nght,eng}@comp.nus.edu.sgAbstractThis paper describes the submission of the Na-tional University of Singapore (NUS) to theHOO 2012 shared task.
Our system uses apipeline of confidence-weighted linear classi-fiers to correct determiner and preposition er-rors.
Our system achieves the highest correc-tion F1 score on the official test set among all14 participating teams, based on gold-standardedits both before and after revision.1 IntroductionGrammatical error correction is the task of automati-cally detecting and correcting erroneous word usageand ill-formed grammatical constructions in text.Determiner and preposition errors are the two mostprominent types of errors made by non-native speak-ers of English.
Although there has been much workon automatic correction of determiner and preposi-tion errors over the last few years, it has so far beenimpossible to directly compare results because dif-ferent teams have evaluated on different data sets.The HOO 2012 shared task evaluates grammaticalerror correction systems for determiner and prepo-sition errors.
Participants are provided with a setof documents written by non-native speakers of En-glish.
The task is to automatically detect and cor-rect determiner and preposition errors and produce aset of corrections (called edits).
Evaluation is doneby computing precision, recall, and F1 score be-tween the system edits and a manually created setof gold-standard edits.
The details of the HOO 2012shared task are described in the official overview pa-per (Dale et al, 2012).In this paper, we describe the system submissionfrom the National University of Singapore (NUS).Our system treats determiner and preposition correc-tion as classification problems.
We use confidence-weighted linear classifiers to predict the correctword from a confusion set of possible correction op-tions.
Separate classifiers are built for determinererrors, preposition replacement errors, and preposi-tion insertion and deletion errors.
The classifiers arecombined into a pipeline of correction steps to forman end-to-end error correction system.
Our systemachieves the highest correction F1 score on the offi-cial test set among all 14 participating teams, basedon gold-standard edits both before and after revision.The remainder of this paper is organized as fol-lows.
The next section presents our error correctionsystem.
Section 3 describes the features.
Section 4presents experimental results.
Section 5 containsfurther discussion.
Section 6 concludes the paper.2 System ArchitectureOur system consists of a pipeline of sequential stepswhere the output of one step serves as the input tothe next step.
The steps in sequence are:1.
Pre-processing2.
Determiner correction (Det)3.
Replacement preposition correction (RT)4.
Missing and unwanted preposition correction(MT, UT)The final output after the last step forms our submis-sion to the shared task.
Each correction step (i.e.,steps 2, 3, 4) involves three internal steps:1.
Feature extraction2162.
Classification3.
Language model filterFeature extraction first analyzes the syntactic struc-ture of the input sentences (part-of-speech (POS)tagging, chunking, and parsing) and identifiesrelevant instances for correction (e.g., all nounphrases (NP) for determiner correction).
Each in-stance is mapped to a real-valued feature vector.Next, a classifier predicts the most likely correctionfor each feature vector.
Finally, the proposed correc-tions are filtered using a language model and onlycorrections that strictly increase the language modelscore are kept.2.1 Confidence-Weighted LearningAs the learning algorithm for all classifiers, wechoose confidence-weighted (CW) learning (Dredzeet al, 2008; Crammer et al, 2009), which has beenshown to perform well for natural language pro-cessing (NLP) problems with high dimensional andsparse feature spaces.
Instead of keeping a singleweight vector, CW learning maintains a distribu-tion over weight vectors, parametrized by a multi-variate normal distribution N (?,?)
with mean ?and covariance matrix ?.
In practice, ?
is of-ten approximated by a diagonal matrix (Dredze etal., 2008).
CW is an online learning algorithmthat proceeds in rounds over a labeled training set((y1,x1), (y2,x2), .
.
.
, (yn,xn)), one example at atime.
After the i-th round, CW learning updates thedistribution over weight vectors such that the i-th ex-ample is predicted correctly with probability at least0 < ?
< 1 while choosing the update step that min-imizes the Kullback-Leibler (KL) distance from thecurrent distribution.
The CW update rule is:(?i+1,?i+1) = (1)arg min?,?DKL (N (?,?
)||N (?i,?i))s.t.
Pr[yi|xi,?,?]
?
?.Dredze et al (2008) show that in the binary case, theCW update rule has a closed-form solution.
In themulti-class case, there exists no closed-form solu-tion but the solution can be efficiently approximated.2.2 Pre-processingPre-processing involves sentence splitting, tokeniza-tion, re-casing, and spelling correction.
We noticedthat the HOO 2012 training data contained a largenumber of spelling mistakes and that some docu-ments are written in all upper case.
Both have a neg-ative effect on tagging and classification accuracy.We automatically identify and re-case upper-casedocuments using a standard re-casing model fromstatistical machine translation (SMT).
Re-casing ismodeled as monotone decoding (without reorder-ing) involving translation of an un-cased sentenceto a mixed-case sentence.
Next, we automaticallycorrect spelling mistakes using an open-source spellchecker.
Words are excluded from spelling correc-tion if they are shorter than a threshold (set to 4 char-acters in our work), or if they include hyphens or up-per case characters inside the word.
We apply a lan-guage model filter (described in the next subsection)to filter the proposed spelling corrections.
Note thatspelling correction is only performed to improve theaccuracy of subsequent correction steps.
Spellingcorrections themselves are not part of the edits sub-mitted for evaluation.2.3 Determiner CorrectionDeterminer errors include three error types: replace-ment determiner (RD), missing determiner (MD),and unwanted determiner (UD).
Although determin-ers are not limited to articles (a, an, the, empty arti-cle ), article errors account for the majority of de-terminer errors.
We therefore focus our efforts onerrors involving only articles.2.3.1 Correction as ClassificationWe treat determiner error correction as a multi-class classification problem.
A classifier is trainedto predict the correct article from a confusion set ofpossible article choices {a, the, }, given the sen-tence context.
The article an is normalized as a andrestored later using a rule-based heuristic.
Duringtraining, every NP in the training data generates onetraining example.
The class y ?
{a, the, } is thecorrect article as annotated by the gold standard orthe observed article used by the writer if the arti-cle is not annotated (i.e., the article is correct).
Thesurrounding context is represented as a real-valuedfeature vector x ?
X .
The features of our classifiersare described in Section 3.One challenge in training classifiers for grammat-ical error correction is that the data is highly skewed.217Training examples without any error (i.e., the ob-served article equals the correct article) greatly out-number those examples with an error (i.e., the ob-served article is different from the correct article).As the observed article is highly correlated with thecorrect article, the observed article is a valuable fea-ture (Rozovskaya and Roth, 2010; Dahlmeier andNg, 2011).
However, the high correlation can havethe undesirable effect that the classifier always pre-dicts the observed article and never proposes anycorrections.
To mitigate this problem, we re-samplethe training data, either by oversampling exampleswith an error or undersampling examples without anerror.
The sampling parameter is chosen through agrid search so as to maximize the F1 score on the de-velopment data.
After training, the classifier can beused to predict the correct article for NPs from newunseen sentences.During testing, every NP in the test data generatesone test example.
If the article predicted by the clas-sifier differs from the observed article and the differ-ence between the classifier?s confidence score for itsfirst choice and the classifier?s confidence score forthe observed article is higher than some thresholdparameter t, the observed article is replaced by theproposed correction.
The threshold parameter t istuned through a grid search so as to maximize the F1score on the development data.
We found that usinga separate threshold parameter value for each classworked better than using a single threshold value.2.3.2 Language Model FilterAll corrections are filtered using a large languagemodel.
Only corrections that strictly increase thenormalized language model score of a sentence arekept.
The normalized language model score is de-fined asscorelm =1|s|logPr(s), (2)where s is the corrected sentence and |s| is the sen-tence length in tokens.
The final set of article correc-tions is applied to an input sentence (i.e., replacingthe observed article with the predicted article).2.4 Replacement Preposition CorrectionReplacement preposition correction follows thesame strategy as determiner correction, but with adifferent confusion set and different features.
Theconfusion set consists of 36 frequent prepositionswhich we adopt from our previous work (Dahlmeierand Ng, 2011).1 These prepositions account forthe majority of preposition replacement errors in theHOO 2012 training data.
During training, everyprepositional phrase (PP) in the training data whichis headed by a preposition from the confusion setgenerates one training example.
The class y is thecorrect preposition.
During testing, every PP in thetest data which is headed by a preposition from theconfusion set generates one test example.2.5 Missing Preposition CorrectionOur system corrects missing and unwanted prepo-sition errors for the seven most frequently missedor wrongly inserted prepositions in the HOO 2012training data.
These preposition are about, at, for,in, of, on, and to.
While developing our system, wefound that adding more prepositions did not increaseperformance in our experiments.We treat missing preposition (MT) correction as abinary classification problem.2 For each prepositionp, we train a binary classifier that predicts the pres-ence or absence of that preposition.
Thus, the con-fusion set consists only of the preposition p and the?empty preposition?.
During training, we requireexamples of contexts where p should be used andwhere it should be omitted.
As prepositions typi-cally appear before NPs, we take every NP in thetraining data as one training example.
If the prepo-sition p appears right in front of the NP (i.e., thepreposition p and the NP form a PP), the exampleis a positive example, otherwise (i.e., another prepo-sition or no preposition appears before the NP) itis a negative example.
During testing, every NPwhich does not directly follow a preposition gener-ates one test example.
If the classifier predicts thatthe preposition p should have been used in this con-text with sufficiently high confidence and insertingp increases the normalized language model score, pis inserted before the NP.1about, along, among, around, as, at, beside, besides, be-tween, by, down, during, except, for, from, in, inside, into, of,off, on, onto, outside, over, through, to, toward, towards, under,underneath, until, up, upon, with, within, without2Alternatively, missing preposition error correction could betreated as a multi-class problem, but we found that binary clas-sifiers gave better performance in initial experiments.2182.6 Unwanted Preposition CorrectionUnwanted preposition correction is treated as a bi-nary classification problem similar to missing prepo-sition correction but with different training and testexamples.
When training the classifier for preposi-tion p, every PP where the writer used the preposi-tion p is one training example.
If the gold-standardannotation labels p as unwanted, the example is apositive example for deleting p, otherwise it is anegative example.
During testing, every PP withthe preposition p generates one test example.
If theclassifier predicts that p should be deleted with suffi-ciently high confidence and deleting p increases thenormalized language model score, p is deleted.We found that separate classifiers for missing andunwanted preposition correction gave slightly bet-ter results compared to using a single classifier forboth tasks.
As the test examples for missing andunwanted preposition correction of a preposition pare disjoint, both steps can be performed in paral-lel.
This also prevents the case of the system ?con-tradicting?
itself by first inserting a preposition andlater deleting it.
We perform missing prepositioncorrection and unwanted preposition correction foreach preposition in turn, before moving to the nextpreposition.3 FeaturesIn this section, we describe the features used in oursystem.
The choice of features can have an impor-tant effect on classification performance.
The exactfeatures used for determiner, replacement preposi-tion, and missing and unwanted preposition correc-tion are listed in Tables 1, 2, 3, and 4, respectively.The features were chosen empirically through exper-iments on the development data.The most commonly used features for grammat-ical error correction are lexical and POS N-grams,and chunk features.
We adopt the features fromprevious work by Han et al (2006), Tetreault andChodorow (2008), and Rozovskaya et al (2011) forour system.
Tetreault et al (2010) show that parsefeatures can further increase performance, and weuse the dependency parse features based on theirwork.
For all the above features, the observed ar-ticle or preposition used by the writer is ?blankedout?
when computing the features.
However, we addthe observed article or preposition as an additionalfeature for determiner and replacement prepositioncorrection.The features described so far are all binary-valued, i.e., they indicate whether some feature ispresent in the input or not.
Additionally, we canconstruct real-valued features by counting the logfrequency of surface N-grams on the web or in aweb-scale corpus (Bergsma et al, 2009).
Web-scaleN-gram count features can harness the power of theweb in connection with supervised classification andhave successfully been used for a number of NLPgeneration and disambiguation problems (Bergsmaet al, 2009; Bergsma et al, 2010), although weare not aware of any previous application in gram-matical error correction.
Web-scale N-gram countfeatures usually use N-grams of consecutive tokens.The release of web-scale parsed corpora like theWaCky project (Baroni et al, 2009) makes it pos-sible to extend the idea to dependency N-grams ofchild-parent tuples over the dependency arcs in thedependency parse tree, e.g., {(child, node), (node,parent)} for bigrams, {(child?s child, child, node),(child, node, parent), (node, parent, parent?s par-ent)} for trigrams.
We collect log frequency countsfor dependency N-grams from a large dependency-parsed web corpus and use the log frequency countas a feature.
We normalize all real-valued featurevalues to a unit interval [0, 1] to avoid features withlarger values dominating features with smaller val-ues.4 ExperimentsIn this section, we report experimental results of oursystem on two different data sets: a held-out testsplit of the HOO 2012 training data, and the officialHOO 2012 test set.4.1 Data SetsThe HOO 2012 training data consists of 1,000 doc-uments together with gold-standard annotation.
Thedocuments are a subset of the 1,244 documentsin the Cambridge Learner Corpus FCE (First Cer-tificate in English) data set (Yannakoudakis et al,2011).
The HOO 2012 gold-standard annotationonly contains edits for six determiner and prepo-sition error types and discards all other gold edits219Feature ExampleLexical featuresObserved article?
theFirst word in NP?
blackWord i before (i = 1, 2, 3)?
{on, sat, ..}Word i before NP (i = 1, 2) {on, sat, ..}Word + POS i before (i = 1, 2, 3)?
{on+IN, sat+VBD, ..}Word i after (i = 1, 2, 3)?
{black, door, ..}Word after NP periodWord + POS i after (N = 1, 2)?
{period+period, .. }Bag of words in NP?
{black, door, mat}N-grams (N = 2, .., 5)?
{on X, X black, .. }Word before + NP?
on+black door matNP + N-gram after NP { black door mat+period, ..}(N = 1, 2, 3)?Noun compound (NC)?
door matAdj + NC?
black+door matAdj POS + NC?
JJ+door matNP POS + NC?
JJ NN NN+door matPOS featuresFirst POS in NP JJPOS i before (i = 1, 2, 3) {IN, VBD, ..}POS i before NP (i = 1, 2) {IN, VBD, ..}POS i after (i = 1, 2, 3) {JJ, NN, ..}POS after NP periodBag of POS in NP {JJ, NN, NN}POS N-grams (N = 2, .., 4) {IN X, X JJ, .. }Head word featuresHead of NP?
matHead POS NNHead word + POS?
mat+NNHead number singularHead countable yesNP POS + head?
JJ NN NN+matWord before + head?
on+matHead + N-gram after NP ?
mat+period, ..(N = 1, 2, 3)Adjective + head?
black+matAdjective POS + head?
JJ+matWord before + adj + head?
on+black+matWord before + adj POS + head?
on+JJ+matWord before + NP POS + head?
on+JJ NN NN+matWeb N-gram count featuresWeb N-gram log counts {log freq(on a black),N = 3, .., 5 log freq(on the black),log freq(on black),..}Dependency featuresDep NP head-child?
{mat-black-amod, ..}Dep NP head-parent?
mat-on-pobjDep child-NP head-parent?
{black-mat-on-amod-pobj, ..}Preposition featuresPrep before + head on+matPrep before + NC on+door matPrep before + NP on+black door matPrep before + adj + head on+black+matPrep before + adj POS + head on+JJ+matPrep before + adj + NC on+black+door matPrep before + adj POS + NC on+JJ+door matPrep before + NP POS + head on+JJ NN NN+matPrep before + NP POS + NC on+JJ NN NN+door matTable 1: Features for determiner correction.
Exam-ple: ?The cat sat on the black door mat.?
?
: lexicaltokens in lower case, ?
: lexical tokens in both origi-nal and lower caseFeature ExampleVerb object featuresVerb obj?
sat onVerb obj + head?
sat on+matVerb obj + NC?
sat on+door matVerb obj + NP?
sat on+black door matVerb obj + adj + head?
sat on+black+matVerb obj + adj POS + head?
sat on+JJ+matVerb obj + adj + NC?
sat on+black+door matVerb obj + adj POS + NC?
sat on+JJ+door matVerb obj + NP POS + head?
sat on+JJ NN NN+matVerb obj + NP POS + NC?
sat on+JJ NN NN+door matTable 1: (continued)from the original FCE data set.
This can lead to?wrong?
gold edits that produce ungrammatical sen-tences, like the following sentenceThere are a lot of possibilities ( ?
of) toearn some money ...where the preposition of is inserted before to earn.The FCE data set contains another edit (to earn ?earning) but this edit is not included in the HOO2012 gold annotation.
This necessarily introducesnoise into the training data as a classifier trained onthis data will learn that inserting of before to earnis correct.
We sidestep this problem by directly us-ing the FCE data set for training, and applying allgold edits except the six determiner and prepositionerror types.
This gives us training data that onlycontains those types of grammatical errors that weare interested in.
Note that this only applies to thetraining data.
For our development and develop-ment test data, we use the HOO 2012 released datawhere the texts contain all types of errors and donot make use of the annotations in the FCE dataset.
For system development, we randomly select100 documents from the HOO 2012 training dataas our development set (HOO-DEV) and another100 disjoint documents as our held-out developmenttest set (HOO-DEVTEST).
We train classifiers onthe remaining 1,044 documents of the FCE data set(FCE(1044)), tune parameters on HOO-DEV, andtest on HOO-DEVTEST.
For our final submission,we train classifiers on all FCE documents, exceptthose 100 documents in HOO-DEV which are usedfor parameter tuning.
Finally, we fix all parametersand re-train the classifiers on the complete FCE cor-pus (FCE(1244)).
This allows us to make maxi-mum use of the FCE corpus as training data.
The220Features ExampleLexical and POS featuresObserved preposition?
onWord i before (i = 1, 2, 3)?
{sitting, cat, ..}Word i after (i = 1, 2, 3)?
{the, mat, ..}N-grams (N = 2, .., 5)?
{sitting X, X the, .. }POS N-grams (N = 2, 3) {VBG X, X DT, .. }Head word featuresHead of prev VP?
sittingPOS head of prev VP VBGHead of prev NP?
catPOS head of prev NP NNHead of next NP?
matPOS head of next NP NNHead prev NP + head next NP?
cat+matPOS head prev NP NN+NN+POS head next NPHead prev VP + head prev NP sitting+cat+mat+ head next NP?POS head prev VP VBG+NN+NN+ POS head prev NP+ POS head next NPN-gram before + {sitting+mat}head of next NP (N = 1, 2)?Web N-gram count featuresWeb N-gram log counts {log freq(sitting at),N = 2, .., 5 log freq(sitting in),.., log freq(sitting on),.., log freq(sitting with), ..}Web dep N-gram log counts {log freq(sitting-at),N = 2, 3 log freq(sitting-in),.., log freq(sitting-on),.., log freq(sitting-with),.., log freq(at-mat),.., log freq(on-mat),.., log freq(with-mat),.., log freq(sitting-at-mat), ...., log freq(sitting-on-mat), ..}Dependency featuresDep parent?
sittingDep parent POS VBGDep parent relation prepDep child?
{mat}Dep child POS {NN}Dep child relation {pobj}Dep parent+child?
sitting+matDep parent POS+child POS?
VBG+NNDep parent+child POS?
sitting+NNDep parent POS+child?
VBG+matDep parent+relation?
sitting+prepDep child+relation?
mat+pobjDep parent+child+relation?
sitting+mat+prep+pobjTable 2: Features for replacement preposition cor-rection.
Example: ?He saw a cat sitting on the mat.??
: lexical tokens in lower case, ?
: lexical tokens inboth original and lower caseFeatures ExampleLexical and POS featuresWord i before (i = 1, 2, 3)?
{sitting, cat, ..}Word i after (i = 1, 2, 3)?
{the, mat, ..}N-grams (N = 2, .., 5)?
{sitting X, X the, .. }POS N-grams (N = 2, 3) {VBG X, X DT, .. }Head word featuresHead of prev VP?
sittingPOS head of prev VP VBGHead of prev NP?
catPOS head of prev NP NNHead of next NP?
matPOS head of next NP NNHead prev NP + head next NP?
cat+matPOS head prev NP NN+NN+ POS head next NPHead prev VP + head prev NP sitting+cat+mat+ head next NP?POS head prev VP VBG+NN+NN+ POS head prev NP+ POS head next NPN-gram before + {sitting+mat, ..}head of next NP (N = 1, 2)?Web N-gram count featuresWeb N-gram log counts {log freq(sitting on the),N = 3, .., 5 log freq(sitting the),.. ,log freq(sitting on the mat),.., log freq(sitting the mat), ..}Table 3: Features for missing preposition correction.Example: ?He saw a cat sitting the mat.??
: lexicaltokens in lower case, ?
: lexical tokens in both origi-nal and lower caseFeatures ExampleWeb N-gram count featuresWeb N-gram log counts {log freq(went to home),N = 3, .., 5 log freq(went home),.. ,log freq(cat went to home),.., log freq(cat went home), ..}Table 4: Features for unwanted preposition correc-tion.
Example: ?The cat went to home.
?Data set # Documents # Sentences # TokensFCE(1044) 1,044 22,434 339,902FCE(1244) 1,244 28,033 423,850HOO-DEV 100 2,798 42,347HOO-DEVTEST 100 2,674 41,518HOO-TEST 100 1,393 20,563Table 5: Overview of the data sets.221official HOO 2012 test data (HOO-TEST), whichis not part of the FCE corpus, is completely unob-served during system development.
Table 5 gives anoverview of the data.
Besides the FCE and HOO2012 data sets, we use the following corpora.
TheGoogle Web 1T 5-gram corpus (Brants and Franz,2006) is used for language modeling and collect-ing N-gram counts, the PukWaC corpus from theWaCky project (Baroni et al, 2009) is used for col-lecting web-scale dependency N-gram counts, andthe New York Times section of the Gigaword cor-pus3 is used for training the re-casing model.
Alldata sets used in our system are publicly available.4.2 ResourcesWe use the following NLP resources in our sys-tem.
Sentence splitting is performed with the NLTKtoolkit.4 For spelling correction, we use the freesoftware Aspell.5 All words that appear at least tentimes in the HOO 2012 training data are added to thespelling dictionary.
We use the OpenNLP tools (ver-sion 1.5.2)6 for POS tagging, YamCha (version0.33) (Kudo and Matsumoto, 2003) for chunk-ing, and the MaltParser (version 1.6.1) (Nivre etal., 2007) for dependency parsing.
We use Ran-dLM (Talbot and Osborne, 2007) for language mod-eling.
The re-casing model is built with the MosesSMT system (Koehn et al, 2007) from the GigawordNew York Times section and all normal-cased docu-ments in the HOO 2012 training data.
The CuVPlusEnglish dictionary (Mitton, 1992) is used to deter-mine the countability of nouns.
The CW learningalgorithm is implemented by our group.
The sourcecode is available from our website.7 All resourcesused in our system are publicly available.4.3 EvaluationEvaluation is performed by computing detection,recognition, and correction F1 score between the setof system edits and the set of gold-standard editsas defined in the HOO 2012 overview paper (Daleet al, 2012).
Detection scores are very similar torecognition scores (about 1?2% higher).
We omit3LDC2009T134http://www.nltk.org5http://aspell.net6http://opennlp.apache.org7http://nlp.comp.nus.edu.sg/softwareStep Recognition CorrectionP R F1 P R F1Det 62.26 12.68 21.06 54.09 11.01 18.30+ RT 64.34 22.41 33.24 57.35 19.97 29.63+ MT/UT 60.75 28.94 39.20 54.84 26.12 35.39Table 6: Overall precision, recall, and F1 score onthe HOO-DEVTEST data after determiner correc-tion (Det), replacement preposition correction (RT),and missing and unwanted preposition correction(MT/UT).detection scores due to space limitations.
Evaluationon the official test set is performed with respect totwo different gold standards: the original gold stan-dard from Cambridge University Press and a revisedversion which was created in the HOO 2012 sharedtask in response to change requests from participat-ing teams.
All scores are computed with the officialscorer.
The official gold-standard edits are given incharacter offsets, while our system internally workswith token offsets.
Therefore, all token offsets areautomatically mapped back to character offsets be-fore we submit our system edits.
We only submittedone run of our system.Type Recognition CorrectionP R F1 P R F1RD 30.00 5.66 9.52 30.00 5.66 9.52MD 69.67 41.67 52.15 59.02 35.29 44.17UD 40.74 11.00 17.32 40.74 11.00 17.32Det 62.26 27.73 38.37 54.09 24.09 33.33RT 69.09 33.63 45.24 63.64 30.97 41.67MT 53.25 35.34 42.49 49.35 32.76 39.38UT 38.46 12.20 18.52 38.46 12.20 18.52Prep 59.62 29.95 39.87 55.40 27.83 37.05Table 7: Individual scores for each error type on theHOO-DEVTEST data.4.4 ResultsTables 6 and 8 show the overall precision, recall andF1 score of our system after each processing step onthe held-out HOO-DEVTEST set and the official testset, respectively.
All numbers are shown in percent-ages.
We note that each processing step improvesthe overall performance.
The final F1 correctionscore on the official test set is 28.70% before revi-sion and 37.83% after revision, which are the highestscores achieved by any participating team.
Tables 7and 9 show individual precision, recall, and F1 score222Step Recognition CorrectionP R F1 P R F1Det 57.76 14.79 23.55 48.28 12.36 19.68+ RT 58.93 21.85 31.88 47.02 17.44 25.44+ MT/UT 55.98 25.83 35.35 45.45 20.97 28.70(a) Before revisionsStep Recognition CorrectionP R F1 P R F1Det 68.10 16.70 26.83 62.93 15.43 24.79+ RT 71.43 25.37 37.44 63.10 22.41 33.07+ MT/UT 69.38 30.66 42.52 61.72 27.27 37.83(b) After revisionsTable 8: Overall precision, recall, and F1 score on the HOO-TEST data after determiner correction (Det),replacement preposition correction (RT), and missing and unwanted preposition correction (MT/UT).Type Recognition CorrectionP R F1 P R F1RD 33.33 2.56 4.76 33.33 2.56 4.76MD 62.24 48.80 54.71 51.02 40.00 44.84UD 33.33 9.43 14.71 33.33 9.43 14.71Det 57.76 30.88 40.24 48.28 25.81 33.63RT 61.54 23.53 34.04 44.23 16.91 24.47MT 46.15 21.05 28.92 38.46 17.54 24.10UT 40.00 13.95 20.69 40.00 13.95 20.69Prep 53.76 21.19 30.40 41.94 16.53 23.71(a) Before revisionsType Recognition CorrectionP R F1 P R F1RD 100.00 8.33 15.38 66.67 5.56 10.26MD 70.41 52.67 60.26 65.31 48.85 55.90UD 46.67 11.29 18.18 46.67 11.29 18.18Det 68.10 34.50 45.80 62.93 31.88 42.32RT 78.85 27.52 40.80 63.46 22.15 32.84MT 61.54 28.57 39.02 53.85 25.00 34.15UT 60.00 23.08 33.33 60.00 23.08 33.33Prep 70.97 27.05 39.17 60.22 22.95 33.23(b) After revisionsTable 9: Individual scores for each error type on the HOO-TEST data.for each of the six error types, and for determiners(Det: aggregate of RD, MD, UD) and prepositions(Prep: aggregate of RT, MT, UT) on the held-outHOO-DEVTEST set and the official test set HOO-TEST, respectively.5 DiscussionThe main differences between our submission to theHOO 2011 shared task (Dahlmeier et al, 2011) andto this year?s shared task are the use of the CW learn-ing algorithm, the use of web-scale N-gram countfeatures, and the use of the observed article or prepo-sition as a feature.
The CW learning algorithm per-formed slightly better than the empirical risk mini-mization batch learning algorithm that we have usedpreviously while being significantly faster duringtraining.
Adding the web-scale N-gram count fea-tures showed significant improvements in initial ex-periments.
Using the observed article or prepositionfeature allows the classifier to learn a bias againstunnecessary corrections.
We believe that our goodprecision scores are a result of using this feature.In our experiments, we tried adding additionaltraining data from other text corpora: the NUS Cor-pus of Learner English (NUCLE) (Dahlmeier andNg, 2011) and the Gigaword corpus.
Unfortunately,we did not see any consistent improvements oversimply using the FCE corpus.
The general rule ofthumb that ?more data is better data?
did not seem tohold true in this case.
After the evaluation had com-pleted, we also tried training on additional trainingdata and tested the resulting system on the officialtest set but did not see improvements either.
We be-lieve that no improvements were obtained due to thesimilarity between the training and test data, sinceall of them are student essays written in response toquestion prompts from the Cambridge FCE exam.6 ConclusionWe have presented the system from the NationalUniversity of Singapore that participated in the HOO2012 shared task.
Our system achieves the highestcorrection F1 score on the official test set among all14 participating teams, based on gold-standard editsboth before and after revision.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.223ReferencesM.
Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.2009.
The WaCky wide web: A collection of verylarge linguistically processed web-crawled corpora.Language Resources and Evaluation, 43(3):209?226.S.
Bergsma, D. Lin, and R. Goebel.
2009.
Web-scaleN-gram models for lexical disambiguation.
In Pro-ceedings of the Twenty-First International Joint Con-ference on Artificial Intelligence, pages 1507?1512,Pasadena, California, USA.S.
Bergsma, E. Pitler, and D. Lin.
2010.
Creating robustsupervised classifiers via web-scale N-gram data.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, pages 865?874,Uppsala, Sweden.T.
Brants and A. Franz.
2006.
Web 1T 5-gram corpusversion 1.1.
Technical report, Google Research.K.
Crammer, M. Dredze, and A. Kulesza.
2009.
Multi-class confidence weighted algorithms.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 496?504, Singapore.D.
Dahlmeier and H.T.
Ng.
2011.
Grammatical error cor-rection with alternating structure optimization.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 915?923, Portland, Oregon, USA.D.
Dahlmeier, H.T.
Ng, and T.P.
Tran.
2011.
NUS atthe HOO 2011 pilot shared task.
In Proceedings ofthe Generation Challenges Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 257?259, Nancy, France.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
HOO2012: A report on the preposition and determiner errorcorrection shared task.
In Proceedings of the SeventhWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, Montre?al, Que?bec, Canada.M.
Dredze, K. Crammer, and F. Pereira.
2008.Confidence-weighted linear classification.
In Pro-ceedings of the 25th International Conference on Ma-chine Learning, pages 184?191, Helsinki, Finland.N.-R. Han, M. Chodorow, and C. Leacock.
2006.
De-tecting errors in English article usage by non-nativespeakers.
Natural Language Engineering, 12(2):115?129.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of theACL 2007 Demo and Poster Sessions, pages 177?180,Prague, Czech Republic.T.
Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 24?31, Sapporo, Japan.R.
Mitton.
1992.
A description of a computer-usable dic-tionary file based on the Oxford Advanced Learner?sDictionary of Current English.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and M. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13(2):95?135.A.
Rozovskaya and D. Roth.
2010.
Training paradigmsfor correcting errors in grammar and usage.
In Pro-ceedings of Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe ACL, pages 154?162, Los Angeles, California.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of Illinois system in HOO text cor-rection shared task.
In Proceedings of the GenerationChallenges Session at the 13th European Workshop onNatural Language Generation, pages 263?266, Nancy,France.D.
Talbot and M. Osborne.
2007.
Randomised languagemodelling for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 512?519, Prague,Czech Republic.J.
Tetreault and M. Chodorow.
2008.
The ups and downsof preposition error detection in ESL writing.
InProceedings of the 22nd International Conference onComputational Linguistics, pages 865?872, Manch-ester, UK.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Usingparse features for preposition selection and error de-tection.
In Proceedings of the ACL 2010 ConferenceShort Papers, pages 353?358, Uppsala, Sweden.H.
Yannakoudakis, T. Briscoe, and B. Medlock.
2011.A new dataset and method for automatically gradingESOL texts.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 180?189, Port-land, Oregon, USA.224
