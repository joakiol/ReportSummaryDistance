Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1018?1024,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsThe Gun Violence Database: A new task and data set for NLPEllie Pavlick1 Heng Ji2 Xiaoman Pan2 Chris Callison-Burch11Computer and Information Science Department, University of Pennsylvania2Computer Science Department, Rensselaer Polytechnic InstituteAbstractWe argue that NLP researchers are especiallywell-positioned to contribute to the nationaldiscussion about gun violence.
Reasoningabout the causes and outcomes of gun violenceis typically dominated by politics and emo-tion, and data-driven research on the topic isstymied by a shortage of data and a lack offederal funding.
However, data abounds in theform of unstructured text from news articlesacross the country.
This is an ideal applicationof NLP technologies, such as relation extrac-tion, coreference resolution, and event detec-tion.
We introduce a new and growing dataset,the Gun Violence Database, in order to facil-itate the adaptation of current NLP technolo-gies to the domain of gun violence, thus en-abling better social science research on thisimportant and under-resourced problem.1 IntroductionThe field of natural language processing often toutsits mission as harnessing the information containedin human language: taking unstructured data in theform of speech and text, and transforming it intoinformation that can be searched, categorized, andreasoned about.
This is an ambitious goal, andthe current state-of-the-art of language technologyhas made impressive strides towards understanding?who did what to whom, when, where, how, andwhy?
(Kao and Poteet, 2007).
Advances in NLPhave enabled us to read news in real time (Petrovic?
etal., 2010), identify the key players (Ruppenhofer etal., 2009), recognize the relationships between them(Riedel et al, 2013), summarize the new informa-tion (Wang et al, 2016), update central databases(Singhal, 2012), and use those databases to answerquestions about the world (Berant et al, 2013).Although these technological achievements areprofound, often times we as researchers apply themto somewhat trivial settings like learning about thelatest Hollywood divorces (Wijaya et al, 2015) orlearning silly facts about the world, like that ?whitesuites, will never go out of, style?
(Fader et al,2011).
In this paper, we call the attention of the NLPcommunity to one particularly good use case of ourcurrent technology, which could have profound pol-icy implications: gun violence research.Gun violence is an undeniable problem in theUnited States, but its causes are poorly understood,and attempts to reason about solutions are oftenmarred by emotions and political bias.
Research intothe factors that cause and prevent gun violence islimited by the fact that data collection is expensive,and political agendas have all but eliminated fund-ing on the topic.
However, in the form of unstruc-tured natural language published daily by newspa-pers across the country, data abounds.
We argue thatthis is the exact type of information that NLP is de-signed to organize, and the positive social impact ofdoing so would be substantial.
We introduce the GunViolence Database (GVDB), a new dataset of gunviolence articles paired with NLP annotations.
Ourhope is that the GVDB will facilitate the adaptationof core NLP technologies to the domain of gun vi-olence.
In turn, we believe these NLP technologiescan help overcome the data vacuum that is currentlypreventing productive discussion about gun violenceand its possible solutions.1018What we have: Daily reports of gun violence, published as free text by local newspapers and TV stations.What we need: Structured, queryable database with one record per incident.Information Retrieval: Find articles about gun violence.Event Detection: Identify precise incident being reported.Temporal Annotation: Pinpoint precise time of the event.NER: Extract key locations and participants from the event.Semantic Role Labeling: Relate participants to their rolein the incident (e.g.
shooter, victim).With-document Coref: Resolve mentions to consistentlymodel each participant throughout the event.Semantic Parsing: Extract precise, detailed informationabout participants, e.g.
race, age, and gender.Cross-document Coref: Recognize mentions of the sameshooter or victim appearing in different articles.Event Coref: Identify articles reporting the same event,and resolve to a single database entry.Three seconds.
On a dashcam video clock, that's the amount of time between the moment when two officers have their guns drawn and the point when Laquan McDonald falls to the ground.
The video, released to the public for the first time late Tuesday, is a key piece of evidence in a case that's sparked protests in Chicago and has landed an officer behind bars.
The 17-year-old McDonald was shot 16 times on that day the video shows in October 2014.
Chicago police Officer Jason Van Dyke was charged Tuesday with first-degree murder?.Chicago Police release Laquan McDonald shooting video | National NewsProtesters took to the streets of Chicago late Tuesday after police released a video showing an officer shooting 17-year-old Laquan McDonald.
McDonald was killed last October.
The city's mayor has called for peace.
"I believe this is a moment that can build bridges of understanding rather than become a barrier of misunderstanding."
Mayor Rahm Emanuel said.
Chicago has been preparing for protests in advance of the video's release.
McDonald was a black teenager.
The officer who shot him, Jason Van Dyke, is white.
He was charged Tuesday with first-degree murder in McDonald's death?Police release video of officer shooting teen | Oklahoma CityEvent Co-RefCr ss docCo-RefWithin docCo-RefIncident #1053City ChicagoDate October 2014Shooter Jason Van DykeVictim Laquan McDonaldShooter #1009Name Jason Van DykeGender MaleAge UnkRace WhiteVictim #1014Name Laquan McDonaldGender MaleAge 17Race BlackKilled TRUEEvent DetectionSemantic RolesSemantic ParsingTemporal ResolutionFigure 1: Turning daily news reports into usable data for public health and social science researchers is a textbook application ofNLP technologies, and one that can have meaningful social impact.2 Gun Violence?s Data ProblemIt is not difficult to motivate why gun violence isan important problem for research.
Gun violencecauses approximately 34,000 deaths in the US everyyear and more than twice as many injuries (FICAP,2006), with violence especially high among youngpeople and racial minorities (CDC, 2013).The magnitude of the gun violence problem, theinherent gravity of the topic, and that fact thatit inevitably leads to discussion of race, personalsafety, and constitutional rights, makes the topichighly emotional and politically charged.
Researchinto such hot-blooded topics stands to benefit im-mensely from data.
In the past decade, machinelearning researchers have championed data-drivendecision making in place of oft-fallible human in-tuition.
This approach has revolutionized the waywe design and evaluate the effectiveness of busi-ness practices (Brynjolfsson et al, 2011; Kohaviet al, 2009), advertisements (Breese et al, 1998),and political campaigns (Issenberg, 2013).
Gun vi-olence policy should be no different.
The problemis that researchers lack the data they need to an-swer the questions they want to ask.
There is nosingle database1 of gun violence incidents in the1There are 13 national data systems in the U.S., managed byUS, and the data that is available is mostly aggre-gated at the state level.
Without locally-aggregateddata, it is impossible to conduct meaningful studiesof how firearm injury varies by community, a keystep toward designing good policies for prevention(FICAP, 2006).
However, for the past 25 years, re-search in this area has been, in the best case, mas-sively underfunded (Roth et al, 1993) and in theworst case, actively blocked by federal legislation(Kassirer, 1995; Frankel, 2015; Bertrand, 2015).As a result, federal resources for gun violence re-search are orders of magnitude lower than is war-ranted (Branas et al, 2005), and there is no near-term likelihood of a federally-funded effort to collectdetailed datasets to facilitate gun violence research.Why NLP?
Local newspapers and television sta-tions report daily on gun injuries and fatalities.Many of these stories never make national news, butthey represent precisely the kind of high-resolutiondata that epidemiologists need.
The details of thesereports could transform gun violence research if theywere in a structured database, rather than spreadseparate federal agencies.
The National Violent Death RegistrySystem, arguably the most organized effort, receives data fromonly 16 states.
Most large-scale epidemiological studies sampleinformation from only 100 Emergency Departments.1019across the text of thousands of web pages.Replacing expensive, manual data entry with au-tomated processing is exactly the type of problemthat NLP is made to solve.
In fact, the recentapplication of NLP tools to social science prob-lems has generated a flurry of exciting and en-couraging results.
NLP has made novel contribu-tions to the way scientists measure everything fromincome (Preoctiuc-Pietro et al, 2015b) to mentalhealth (Preoctiuc-Pietro et al, 2015a; Schwartz etal., 2016; Choudhury et al, 2016), disease (Santil-lana et al, 2015; Ireland et al, 2015; Eichstaedt etal., 2015), and the quality of patient care (Nakhasi etal., 2016; Ranard et al, 2016).Text mining has promise for the study of gunviolence, too (Bushman et al, 2016).
However,most questions about gun violence are not easilyanswered using shallow analyses like topic modelsor word clusters.
Epidemiologists want to know,for example, does gun ownership lead to increasesin gun violence?
Or, is there evidence of conta-gion in suicides, and if so, does the style of report-ing on suicides affect the likelihood that others willcommit suicide after the initial event?
Answeringthese questions requires extracting precise informa-tion from text: identifying entities, their actions, andtheir attributes specifically and reliably.We believe this level of depth is well within thereach of current NLP technology.
The state-of-the-art tools that NLP researchers have been buildingand fine-tuning for decades are an ideal fit for theproblem described.
Nearly every step of this pro-cess, from retrieving articles about gun violence tocorrectly determining whether the phrase 14 yearold girl describes the victim or the shooter, has beenstudied as a core NLP problem in its own right (Fig-ure 1).
These NLP tools have the potential to makea marked difference for gun violence researchers.3 The Gun Violence DatabaseIn order to facilitate the adaptation of NLP tools foruse in gun violence research, we introduce the GunViolence Database2 (GVDB), a dataset for trainingand evaluating the performance of NLP systems inthe domain of gun violence.
The GVDB is the resultof a large crowdsourced annotation effort.
This an-2http://gun-violence.org/notation is ongoing, and the GVDB will be regularlyupdated with new data and new layers of annotation,making it an interesting and challenging data set onwhich to evaluate state-of-the-art NLP tools.Crowdsourced Annotation The GVDB is builtand updated through a continuously running crowd-sourced annotation pipeline.
The pipeline consistsof daily crawls of local newspapers and televisionwebsites from across the US.
The crawled articlesare automatically classified using a high-recall textclassifier, and then manually vetted by humans tofilter out false positives.
So far, the GVDB contains60K articles (?49M words) describing incidents ofgun violence, and is (sadly) growing at a rate ofnearly 1,000 per day.Crowdsourced annotators then mark up the textof the articles with the key information we expectautomated NLP systems to extract.
In addition toclassifying articles according to multiple binary di-mensions (e.g.
whether or not the shooting was in-tentional), annotators mark specific spans of the textwhich populate the database schema.
For exam-ple, workers highlight the shooters, the victims, andthe location.3 These precise spans are stored in thedatabase so that automated systems can be trained toreproduce the extracted information.
Our annotationinterface is shown in Figure 2.At the time of writing, the GVDB contains 7,366fully annotated articles (Table 1) coming from 1,512US cities, and the database is continuing to grow.The latest version of the database will be main-tained and available for download at http://gun-violence.org/.60,443 Articles reporting incidents of gun violence7,366 Articles fully-annotated for IE6,804 w/ location information5,394 w/ shooter/victim information4,143 w/ temporal information1,666 w/ weapon informationTable 1: Current contents of the GVDB.
Size and level of an-notation is continually growing.
See Forthcoming Extensions.Current Baselines To establish a baseline levelof performance, we run an off-the-shelf information3See supplementary material for all extracted informationand screenshots.1020Figure 2: Annotation interface associates structured information (e.g.
the time of day when the shooting occurred) with a specificspan of text in the article.extraction system on the 7,366 articles and measureprecision and recall for identifying key informationabout the incidents.
We use the Li et al (2013) sys-tems, which identifies a range of entities and events.We focus on the those events identified by the sys-tem which are relevant to the main fields in theGVDB schema.4 We map the arguments of theseevents onto the corresponding database fields, e.g.the agent of the event corresponds to the GVDB?sshooter name.
Since the system identifies multiplesuch events per article, we count it as correct as longas one argument correctly matches the correspond-ing value in the GVDB (e.g.
the system is correctas long as one extracted event has an agent whichmatches the GVDB?s shooter name for that article).In addition, we run the Stanford CoreNLP TimeExsystem (Chang and Manning, 2012) over the articlesin order to identify the time of the reported incident.We report the system?s performance using bothexact match against the gold annotation (?strict?)
aswell as an approximate match, in which the systemis correct if it is either a substring or a superstringof the gold annotation.
E.g.
if the victim name isSean Bolton, the approximate metric will count bothBolton and Officer Sean Bolton as correct.While performance is high for certain structuredtypes of information, like dates and times, fields likevictim and shooter name are much less reliably iden-tified.
Furthermore, many key pieces of informationin the GVDB, such as age and race, are not sup-4Specifically, we focus on Attack, Injure, and Die eventsStrict Approx.Prec.
Rec.
Prec.
Rec.Date/Time 69.3% 66.9% 70.5% 68.1%Location 19.9% 8.8% 30.8% 13.6%Victim 10.2% 8.5% 59.5% 49.6%Shooter 5.8% 3.9% 30.2% 20.1%Weapon 2.1% 0.7% 36.8% 11.8%Table 2: Performance of an off-the-shelf IE system on identi-fying key information about gun violence incidents from newsarticles.
For ?strict?
vs.
?approximate?, see text.ported by the off-the-shelf system.
These baselinesare evidence that NLP systems have potential, butrequire some effort to make their output usable fordownstream research.
Our hope is that the GVDBwill serve as the impetus for undertaking this effort.Forthcoming Extensions The building of theGVDB is an ongoing effort, with new articles anddeeper annotation being continuously added.
Weare currently adding approximately 300 new fully-annotated articles per day, while simultaneously en-riching the annotation pipeline.
The GVDB is soonto include annotation for event coreference, whichwill link articles describing the same incident, andcross-document coreference, which will link men-tions of the same shooter/victim appearing in sep-arate documents.
In the future, the database willalso include full within-document coreference anno-tation, with all mentions of a shooter/victim beingflagged as such, and will incorporate visual data, sothat within-article images are tagged with relevant1021information which may not be communicated by thetext alone (e.g.
race/approximate age).4 Related EffortsSeveral projects collect data about gun violence vianewspaper teams (Boyle, 2013; Swaine et al, 2015)or volunteer crowds (Burghart, 2014; Wagner, 2014;Kirk and Kois, 2013).
Perhaps the largest such ef-fort is the Gun Violence Archive5.
However, noneare aimed at the eventual automation of the process.We believe that automating this data collection iskey to keeping it scalable, consistent, and unbiased.Our focus is therefore on collecting data that is well-suited for training and evaluating NLP systems.5 ConclusionWe believe that NLP researchers have the potentialto significantly advance gun violence research.
Theshortage of data and funding for studying gun vi-olence in America has severely limited the abilityof scientists to have productive conversations aboutpractical solutions.
Applying core NLP technolo-gies to local news reports of gun violence couldtransform raw text into structured, queryable datathat public health researchers can use.
We have in-troduced the Gun Violence Database, a new datasetof gun violence articles with rich NLP annotationswhich will support efforts on this new NLP task.AcknowledgmentsWe would like to thank Douglas Wiebe for his ad-vice and insight on building a useful resource forpublic health researchers.
We also thank the studentsof the University of Pennsylvania?s crowdsourcingclass (NETS 213) for their involvement in buildingand testing a useful crowdsourcing pipeline for in-formation extraction.
Finally, we thank the devel-opers at 10clouds for the excellent engineering anddesign of http://gun-violence.org/.ReferencesJonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 20135http://www.gunviolencearchive.orgConference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544, Seattle, Wash-ington, USA, October.
Association for ComputationalLinguistics.Natasha Bertrand.
2015.
Congress quietly renewed a banon gun-violence research.
Business Insider (July 7).Andy Boyle.
2013.
Mapping Chicago?s shooting victims(Chicago Tribune), July.Charles C Branas, Douglas J Wiebe, CW Schwab, andTS Richmond.
2005.
Getting past the f word in feder-ally funded public health research.
Injury prevention,11(3):191?191.John S Breese, David Heckerman, and Carl Kadie.
1998.Empirical analysis of predictive algorithms for collab-orative filtering.
In Proceedings of the Fourteenth con-ference on Uncertainty in artificial intelligence, pages43?52.Erik Brynjolfsson, Lorin M Hitt, and Heekyung HellenKim.
2011.
Strength in numbers: How doesdata-driven decisionmaking affect firm performance?Available at SSRN 1819486.Brian D. Burghart.
2014.
What I?ve learned from twoyears collecting data on police killings, August.Brad J. Bushman, Katherine Newman, Sandra L. Calvert,Geraldine Downey, Mark Dredze, Michael Gottfred-son, Nina G. Jablonski, Ann S. Masten, Calvin Morrill,Daniel B. Neill, Daniel Romer, and Daniel W. Webster.2016.
Youth violence: What we know and what weneed to know.
American Psychologist, 71(1):17?39,Jan.CDC.
2013.
Deaths: Final data for 2013.
National vitalstatistics reports: from the Centers for Disease Con-trol and Prevention, National Center for Health Statis-tics, National Vital Statistics System, 64(2).Angel X Chang and Christopher D Manning.
2012.
Su-time: A library for recognizing and normalizing timeexpressions.
In LREC, pages 3735?3740.Munmun De Choudhury, Emre Kiciman, Mark Dredze,Glen Coppersmith, and Mrinal Kumar.
2016.
Dis-covering shifts to suicidal ideation from mental healthcontent in social media.
In Conference on Human Fac-tors in Computing Systems (CHI).Johannes C Eichstaedt, Hansen Andrew Schwartz, Mar-garet L Kern, Gregory Park, Darwin R Labarthe,Raina M Merchant, Sneha Jha, Megha Agrawal,Lukasz A Dziurzynski, Maarten Sap, et al 2015.Psychological language on Twitter predicts county-level heart disease mortality.
Psychological science,26(2):159?169.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,1022pages 1535?1545, Edinburgh, Scotland, UK., July.
As-sociation for Computational Linguistics.FICAP.
2006.
Firearm injury in the US.
Online Re-source Book from The Firearm and Injury Center atPenn.Todd C Frankel.
2015.
Why the CDC still isn?t research-ing gun violence, despite the ban being lifted two yearsago.
The Washington Post (January 14).Molly E Ireland, Qijia Chen, H Andrew Schwartz, Lyle HUngar, and Dolores Albarracin.
2015.
Action tweetslinked to reduced county-level HIV prevalence in theUnited States: Online messages and structural deter-minants.
AIDS and Behavior, pages 1?9.Sasha Issenberg.
2013.
How president Obama?s cam-paign used big data to rally individual voters.
Technol-ogy Review, 116(1):38?49.Anne Kao and Steve R Poteet.
2007.
Natural languageprocessing and text mining.
Springer Science & Busi-ness Media.Jerome P Kassirer.
1995.
A partisan assault on science?the threat to the CDC.
New England journal ofmedicine, 333(12):793?794.Chris Kirk and Dan Kois.
2013.
How many people havebeen killed by guns since Newtown?, December.Ron Kohavi, Roger Longbotham, Dan Sommerfield, andRandal M Henne.
2009.
Controlled experiments onthe web: survey and practical guide.
Data mining andknowledge discovery, 18(1):140?181.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint event ex-traction via structured prediction with global features.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 73?82, Sofia, Bulgaria, August.Association for Computational Linguistics.Atul Nakhasi, Sarah G Bell, Ralph J Passarella, Michael JPaul, Mark Dredze, and Peter J Pronovost.
2016.
Thepotential of Twitter as a data source for patient safety.Journal of Patient Safety, Jan.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applicationto Twitter.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 181?189, Los Angeles, California, June.
Asso-ciation for Computational Linguistics.Daniel Preoctiuc-Pietro, Maarten Sap, H AndrewSchwartz, and Lyle H Ungar.
2015a.
Mental ill-ness detection at the World Well-Being Project for theCLPsych 2015 Shared Task.
In Proceedings of theWorkshop on Computational Linguistics and ClinicalPsychology: From Linguistic Signal to Clinical Real-ity, NAACL.Daniel Preoctiuc-Pietro, Svitlana Volkova, VasileiosLampos, Yoram Bachrach, and Nikolaos Aletras.2015b.
Studying User Income through Language, Be-haviour and Affect in Social Media.
PLoS ONE, 10(9),09.Benjamin L Ranard, Rachel M Werner, Tadas Antanavi-cius, H Andrew Schwartz, Robert J Smith, Zachary FMeisel, David A Asch, Lyle H Ungar, and Raina MMerchant.
2016.
Yelp reviews of hospital care cansupplement and inform traditional surveys of the pa-tient experience of care.
Health Affairs, 35(4):697?705.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.
In Pro-ceedings of the 2013 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 74?84, Atlanta, Georgia, June.
Association for Computa-tional Linguistics.Jeffrey A Roth, Albert J Reiss Jr, et al 1993.
Under-standing and preventing violence, volume 1.
NationalAcademies Press.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2009.
Semeval-2010 task 10: Linking events and their participants indiscourse.
In Proceedings of the Workshop on Seman-tic Evaluations: Recent Achievements and Future Di-rections (SEW-2009), pages 106?111, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Mauricio Santillana, Andre T. Nguyen, Mark Dredze,Michael J. Paul, Elaine Nsoesie, and John S. Brown-stein.
2015.
Combining search, social media, andtraditional data sources to improve influenza surveil-lance.
PLOS Computational Biology.H Andrew Schwartz, Maarten Sap, Margaret L Kern,Johannes C Eichstaedt, Adam Kapelner, MeghaAgrawal, Eduardo Blanco, Lukasz Dziurzynski, Gre-gory Park, David Stillwell, Michal Kosinski, Mar-tin EP Seligman, and Lyle H. Ungar.
2016.
Predict-ing Individual Well-Being Through the Language ofSocial Media.
Pacific Symposium on Biocomputing,21:516?527.Amit Singhal.
2012.
Introducing the knowledge graph:things, not strings.
Official Google Blog, May.Jon Swaine, Oliver Laughland, Jamiles Lartey, and CiaraMcCarthy.
2015.
The counted: People killed by po-lice in the US (The Guardian), June.Kyle Wagner.
2014.
We?re compiling every police-involved shooting in America.
Help us., August.William Yang Wang, Yashar Medhad, Dragomir Radev,and Amanda Stent.
2016.
A low-rank approximation1023approach to learning joint embeddings of news storiesand images for timeline summarization.
In Proceed-ings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, San Diego, CA,USA.
ACL.Derry Tanti Wijaya, Ndapandula Nakashole, and TomMitchell.
2015.
A spousal relation begins with a dele-tion of engage and ends with an addition of divorce:Learning state changing verbs from Wikipedia revi-sion history.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing, pages 518?523, Lisbon, Portugal, September.
As-sociation for Computational Linguistics.1024
