Abductive Explanation of Dialogue MisunderstandingsSusan  McRoy  and Graeme Hi rs tDepartment  of Computer  ScienceUniversity of TorontoToronto, Canada M5S 1A4Abst rac tTo respond to an utterance, a listener mustinterpret what others have said and whythey have said it.
Misunderstandings oc-cur when agents differ in their beliefs aboutwhat has been said or why.
Our work com-bines intentional nd social accounts of dis-course, unifying theories of speech act pro-duction, interpretation, and the repair ofmisunderstandings.
A unified theory hasbeen developed by characterizing the gen-eration of utterances as default reasoningand using abduction to characterize inter-pretation and repair.1 In t roduct ionWhen agents participate in a dialogue, they bringto it different beliefs and goals.
These differencescan lead them to make different assumptions aboutone another's actions, construct different interpre-tations of discourse objects, or produce utterancesthat are either too specific or too vague for othersto interpret as intended.
As a result, agents mayfail to understand some part of the dialogue--orunknowingly diverge in their understanding of it--making a breakdown in communication likely.
Onestrategy an agent might use to address the prob-lem of breakdowns i to try to circumvent them,for example, by trying to identify and correct appar-ent confusions about objects or concepts mentionedin the discourse \[Goodman, 1985; McCoy, 1985;Calistri-Yeh, 1991; Eller and Carberry, 1992\].
Thework reported here takes a different, but complemen-tary, approach: it models how an agent can use whatshe or he knows about the discourse to recognizewhether either participant has misunderstood someprevious utterance to repair the misunderstanding.This strategy handles cases that the preventive ap-proaches cannot anticipate.
It is also more general,because our system can generate repairs on the basisof the relatively few types of manifestations of mis-understanding, rather than the much broader (andhence more difficult o anticipate) range of sources.In this paper, we shall describe an abduetive ac-count of interpreting speech acts and recognizingmisunderstandings (wediscuss the generation of re-pairs of misunderstandings in McRoy and Hirst,1992).
This account is part of a unified theoryof speech act production, interpretation, and re-pair \[McRoy, 1993\].
According to the theory, speak-ers use their beliefs about the discourse context andwhich speech acts are expected to follow from agiven speech act in order to select one that accom-plishes their goals and then to produce an utter-ance that performs the chosen speech act.
Interpre-tation and repair attempt o retrace this selectionprocess abductively--when a hearer attempts to in-terpret an observed utterance, he tries to identify thegoals, expectations, or misunderstandings that mighthave led the to produce it.
Previous plan-based ap-proaches \[Allen, 1979; Allen, 1983; Litman, 1985;Carberry, 1985\] have had difficulty constraining thisinference---from only a germ of content, potentially atremendous number of goals could be inferred.
A keyassumption of our approach, which follows from in-sights provided by Conversation Analysis \[Garfinkel,1967; Schegloff and Sacks, 1973\], is that participantscan rely primarily on expectations derived from so-cial conventions about language use.
These expec-tations enable participants to determine whetherthe conversation is proceeding smoothly: if noth-ing unusual is detected, then understanding is pre-sumed to occur.
Conversely, when a hearer finds277that a speaker's utterance is inconsistent with hisexpectations, he may change his interpretation ofan earlier turn and generate a repair \[Fox, 1987;Suchman, 1987\].
Our approach differs from stan-dard CA accounts in that it treats Gricean inten-tions \[Grice, 1957\] as part of these conventions anduses them to constrain an agent's expectations; thework thus represents a synthesis of intentional andstructural accounts.Recognizing misunderstanding is like abductionbecause hearers must explain why, given their knowl-edge of how differences in understanding are mani-fested, a speaker might have said what she did.
At-tributions of misunderstanding are assumptions thatmight be abduced in constructing such an explana-tion.
Recognizing misunderstanding also resembles adiagnosis in which utterances play the role of "symp-toms" and misunderstandings are "faults".
Previ-ous work on diagnosis has shown abduction to bea useful characterization \[Ahuja and Reggia, 1986;Poole, 1986\].An alternative approach to diagnosing discoursemisunderstandings is to reason deductively from aspeaker's utterances to his or her goals on the basisof (default) prior beliefs and then rely on belief revi-sion to retract inconsistent interpretations \[Cawsey,1991\]; however, this approach as a number of disad-vantages.
First, any set of rules of this form will beunable to specify all the conditions (such as insincer-ity) that might also influence the agent's interpreta-tion; a reasoner will need also to assume that thereare no "abnormalities" relevant o the participantsor the speech event \[Poole, 1989\].
This approachalso ignores the many other possible interpretationsthat participants might achieve through negotiation,independent of their actual beliefs.
For example, anagent's response to a yes-no question might treat itas a question, a request, a warning, a test, an insult,a challenge, or just a vacuous tatement intended tokeep the conversation going.
If conversational par-ticipants can negotiate such ambiguities, then utter-ances are at most a reason for attributing a certaingoal to an agent.
That is, they are a symptom, not acause.
Any deductive account would thus be counter-intuitive, and very likely false as well.2 The  abduct ive  f rameworkWe have chosen to develop the proposed accountof dialogue using the Prioritized Theorist frame-work \[Poole l ai., 1987; Brewka, 1989; van Arragon,1990\].
Theorist typifies what is known as a "proof-based approach" to abduction because it relies on atheorem prover to collect he assumptions that wouldbe needed to prove a given set of observations and toverify their consistency.
This framework was selectedbecause of its first-order syntax and its support forboth default and abductive reasoning.
Within The-orist, we represent linguistic knowledge and the dis-course context, and also model how speakers reasonabout their actions and misunderstandings.We have used Poole's implementation of Theo-rist, extended to incorporate preferences among de-faults as suggested by Van Arragon \[1990\].
Poole'sTheorist implements a full first-order clausal theo-rem prover in Prolog.
It extends Prolog with a truenegation symbol and the contrapositive forms of eachclause.
Thus, a Theorist clause a D/3 is interpretedas {/3 *-- a,-~a 4-- -~/3}.
A Prioritized Theorist rea-soner can also assume any default d that the pro-grammer has designated as a potential hypothesis,unless it can prove -~d from some fact or overridinghypothesis.The reasoning algorithm uses model elimina-tion \[Loveland, 1978; Stickel, 1989; Umrigar andPitchumani, 1985\] as its proof strategy.
Like Pro-log, it is a resolution-based procedure that chainsbackward from goals to subgoals, using rules of theform goal 4-- subgoall A .
.
.
A subgoaln, to reduce thegoals to their subgoals.
However, unlike Prolog, itrecords each subgoal that occurs in the proof treeleading to the current one and checks this list beforesearching the knowledge base for a relevant clause;this permits it to reason by cases.3 The  fo rmal  languageThe model is based on a sorted first-order lan-guage, ?, comprising a denumerable set of predi-cates, variables, constants, and functions, along withthe boolean connectives V, A,-,, D, and --, and thepredicate =.
The terms of ?
come in six sorts:agents, turns, sequences of turns, actions, descrip-tions, and suppositions 1. ?
includes an infinite num-ber of variables and function symbols of every sortand arity.
We also define a number of special ones:do, mistake, intend, knowif, knowref, knows-BetterRef,  not, and and.
Each of of these func-tions takes an agent as its first argument and an ac-tion, supposition, or description for each of its otherarguments; each of them returns a supposition.
Thefunction symbols that return speech acts each taketwo agents as their first two argument and an action,supposition, or description for each of their other ar-guments.For the abductive model, we define a correspond-ing language/~Th in the Prioritized Theorist frame-work.
/:Th includes all the sorts, terms, functions,and predicates of /:; however, /:Tit lacks explicitquantification, distinguishes facts from defaults, andassociates with each default a priority value.
Vari-able names are understood to be universally quan-tified in facts and defaults (but existentially quan-tified in an explanation).
Facts are given by "FACTw.
", where w is a wff.
A default can be given ei-ther by "DEFAULT (p, d)."
or "DEFAULT (p, d) : w.",1Suppositions represent he propositions that speak-ers express in a conversation, independent of the truthvalues that those propositions might have.278where p is a priority value, d is an atomic symbolwith only free variables as arguments, and w is awtf.
For example, we can express the default thatbirds normally fly, as:DEFAULT (2, birdsFly(b)) : bird(b) D .fly(b).If Y: is the set of facts and AP is the set of defaultswith priority p, then an expression DEFAULT(p, d) : wasserts that d E A p and (d D w) E .~'.4 The  arch i tec ture  o f  the  mode lIn the architecture that we have formulated, pro-ducing an utterance is a default, deductive processof choosing both a speech act that meets an agent'scommunicative and interactional goals and a utter-ance that will be interpretable as this act in the cur-rent context.
Utterance interpretation is the com-plementary (abductive) process of attributing to thespeaker communicative and interactional goals by at-tributing to him or her a discourse-level form thatprovides a reasonable explanation for an observed ut-terance in the current context.
Social norms delimitthe range of responses that a participant may pro-duce without becoming accountable for additionalexplanation.
2 The attitudes that speakers expressprovide additional constraints, because speakers areexpected not to contradict themselves.
We thereforeattribute to each agent:?
A theory T describing his or her linguisticknowledge, including principles of interactionand facts relating linguistic acts.?
A set B of prior assumptions about the beliefsand goals expressed by the speakers (includingassumptions about misunderstanding).?
A set Ad of potential assumptions about misun-derstandings and meta-planning 3 decisions thatagents can make to select among coherent alter-natives.To interpret an utterance u, by speaker s, the hearerh will attempt o solve:T O B U M t- utter(s, h, u, ts)for some set M C AJ, where ts refers to the currentcontext.In addition, acts of interpretation and generationupdate the set of beliefs and goals assumed to beexpressed uring the discourse.
Our current formal-ization focuses on the problems of identifying howan utterance relates to a context and whether it hasbeen understood.
The update of expressed beliefs2These norms include guidelines uch as "If someoneasks you a question, you should answer it" or "If someoneoffers their opinion and you disagree, you should let themknow".3Our notion of "meta-planning ~ is similar to Lit-man's \[1985\] use of meta-plans, but we prefer to treatmeta-planning as a pattern of inference that is part ofthe task specification rather than as an action.is handled in the implementation, but outside theformal language.
44.1 Speech actsFor simplicity, we represent utterances as surface-level speech acts in the manner first used by Perraultand Allen \[1980\].
For example, if speaker m asksspeaker the question "Do you know who's goingto that meeting?"
we would represent his as: s-request (m,  r, in formi f ( r ,  m,  knowref ( r ,  w))).Following Cohen and Levesque \[1985\], we limit thesurface language to the acts s-request,  s- inform, s-informref ,  and s- informif.
Discourse-level acts in-clude in form,  informif ,  in formref ,  askref,  askif,request ,  preteH 5, testref ,  tes t i f  and warn,  andare represented using a similar notation.4.2 Expressed a t t i tudesWe distinguish the beliefs that speakers act as if theyhave during a course of a conversation from thosethey might actually have.
Most models of discourseincorporate notions of belief and mutual belief to de-scribe what happens when a speaker talks about aproposition, without distinguishing the expressing ofbelief from believing (see Cohen et al 1990).
How-ever, real belief involves notions of evidence, trust-worthiness, and expertise, not accounted for in thesemodels; it is not automatic.
Moreover, the beliefsthat speakers as if they have need not match theirreal ones.
For example, a speaker might simplifyor ignore certain facts that could interfere with theaccomplishment of a primary goal \[Gutwin and Mc-Calla, 1992\].
Speakers need to keep track of whatothers say, in addition to whether they believe them,because ven insincere attitudes can affect the inter-pretation and production of utterances.
Althoughspeakers normally choose to be consistent in the at-titudes they express, they can recant if it appearsthat doing so will lead (or has led) to conversationalbreakdown.Following Thomason \[1990\], we call the contents ofthe attitudes that speakers express during a dialoguesuppositions and the attitude itself simply active.
6Thus, when a speaker performs a particular speechact, she activates the linguistic intentions associatedwith the act, along with a belief that the act hasbeen done.
These attitudes do not depend on the4A related concern is how an agent's beliefs mightchange after an utterance has been understood as an actof a particulax type.
Although we have nothing new toadd here, Perrault \[1990\] shows how Default Logic mightbe used to address this problem.5A pretellingis apreannouncement that says, in effect,"I'm going to tell you something that will surprise you.You might think you know, but you don't.
"eSupposition differs from belief in that speakers neednot distinguish their own suppositions from those of an-other \[Stalnaker, 1972; Thomason, 1990\].279speakers' real beliefs.
7The following expressions are used to denote sup-positions:?
do(s, a) expresses that agent s has performedthe action a;?
mistake(s,  at, az) expresses that agent s hasmistaken an act al for act a2;?
in tend(s ,p)  expresses that agent s intends toachieve a situation described by supposition p;?
knowif(s,p)expresses that the agent s knowswhether the proposition amed by suppositionp is true;?
knowref(s ,  d) expresses that the agent s knowsthe referent of description d;?
knowsBetterP~ef(st ,  2  d) expresses thatagent sl has "expert" knowledge about the ref-erent of description d, so that if s2 has a differentbelief about the referent, then sz is likely to bewrong; s and?
and(pl ,p2) expresses the conjunction of suppo-sitions Pl and P2;?
not(p) expresses the negation of supposition p.94.3 L ingu is t ic  knowledge re la t ionsWe represent agents' linguistic knowledge with threerelations: decomp, a binary relation on utteranceforms and speech acts; lintention, a binary rela-tion on speech acts and suppositions; lezpectation, athree-place relation on speech acts, suppositions, andspeech acts.
The decomp relation specifies the speechacts that each utterance form might accomplish.
Thelintention relation specifies the beliefs and intentionsthat each speech act conventionally expresses.
Thelexpectation relation specifies, for each speech act,which speech acts an agent believing the given con-dition can expect o follow.4.4 Bel iefs and  goalsWe assume that an agent's beliefs and goals are givenexplicitly by statements ofthe form believe(S, P) andhasGoal(S, P, TS), respectively, where S is an agent,P is a supposition and TS is a turn sequence.4.5 Act ivat ionTo represent the dialogue as a whole, including re-pairs, we introduce the notion of a turn sequence andtit is essential that these suppositions name proposi-tions independent of their truth values, so that we mayrepresent agents talking about knowing and intendingwithout fully analyzing these concepts.8This specialization is needed to capture the prag-matic force of pretelling.9The function ot is distinct from boolean connective-~.
It is used to capture the supposition expressed by anagent who says something negative, e.g., "I do not w~ntto go.
"the activation of a supposition with respect o a se-quence.
A turn sequence represents the interpreta-tions of the discourse that a speaker has considered.Turn sequences are characterized by the followingthree relations:?
tumOr(is, t) holds if and only if t is a turn inthe sequence ts;?
succ(tj, tl, ts) holds if and only if turnO\](ts, ti),turnOf(ts, tj), tj follows ti in ts, and there is not~ such that turnOf(ts, tk), suce(tk,ti,ts), andsucc(tj, tk, ts);?
focus(ts, t) holds i ft  is a distinguished turn uponwhich the sequence is focused; normally this isthe last turn of ts.We also define a successor relation on turn sequences.A turn sequence TS2 is a successor to turn sequenceTS1 if TS2 is identical to TS1 except hat TS2 hasan additional turn t that is not a turn of TS1 andthat is the successor to the focused turn of TS1.The set of prior assumptions about the beliefs andgoals expressed by the participants in a dialogue isrepresented as the activation of suppositions.
For ex-ample, an agent nan  performing an in fo rmref (nan ,bob ,  theT ime)  expresses the supposition do(nan,i n fo rmref (nan ,  bob ,  theT ime) )  and the Griceanintention,and(knowref (nan ,  theT ime) ,in tend(nan ,  knowref (bob ,  theT i rne) ) )given by the lintention relation.
We assumethat an agent will maintain a record of both par-ticipants' suppositions, indexed by the turns inwhich they were expressed.
It is represented asa set of statements of the form expressed(P, T) orexpressedNot(P, T) where P is a simple suppositionand T is a turn.Beliefs and intentions that participants expressduring a turn of a sequence tSl become and remainactive in all sequences that are successors to tsl, un-less they are explicitly refuted.DEFINITION 1: If, according to the interpretation ofthe conversation represented by turn sequenceTS with focused turn T, the supposition P wasexpressed uring turn T, we say that P becomesactive with respect to that interpretation andthe predicate active(P, TS) is derivable:FACT expressed(p, t) A focus (ts, t)D active(p, ts).FACT ezpressedNot(p, t) A focus(ts, t)aaiveCnot(p), t ).FACT -,(active(p, ts) A active(not(p), ts)).If formula P is active within a sequence TS, itwill remain active until not (P )  is expressed:280FACT expressed(p, t) A focns(ts, t)D -~aetivationPersists(not (p), t).FACT ezpressedNot(p, t) A focns( ts, t)D -.aetivationPersists(p, t).DEFAULT (1, aetivationp ersists(p, t) ) :active(p, tsi )A sueeessorTS(tsnow, tsi)A foeus(tsno~, t)D adive(p, ts.o~).4.6 Expectat ionThe following definition captures the notion of "ex-pectation".DEFINITION 2: A discourse-level action R is ez-pected by speaker S in turn sequence TS when:?
An action of type A has occurred;?
There is a planning rule corresponding toan adjacency pair A-R  with condition C;?
S believes that C;?
The linguistic intentions expressed by R axeconsistent with TS; and?
R has not occurred yet in TS.DEFAULT (2, ezpectedReply(Pdo, p, do(Sl,  a2), ts)):active(pdo , is)A lezpectation(pdo, p, dO(Sl, a2))A believe(sx, p)A iintentionsOk(sl, az, ts)D expected(s1, a2, ts).FACT active(pdo, ts)D ",ezpectedReply(pdo, p, preply, ts).The predicate xpectedReply is a default.
Althoughactivation might depend on default persistence, acti-vation always takes precedence over expectation be-cause it has a higher priority (on the assumption thatmemory for suppositions i stronger than expecta-tion).The predicate l intentionsOk(S, A, TS) is true ifspeaker S expresses the linguistic intentions of theact A in turn sequence TS, and these intentions areconsistent with TS.We also introduce a subjunctive form of expecta-tion, which depends only on a speaker's real beliefs:FACT lezpectation(do(sl, al), p, do(s2, a2))A believe(s1, p)D wouldEz(sl, al, a2).4.7 Recognizing misunderstandingsWhen a dialogue proceeds normally, a speaker's ut-terance can be explained by abducing that a dis-course action has been planned using one of a knownrange of discourse strategies: plan adoption, accep-tance, challenge, repair, or closing.
(Figure 1 in-cludes ome examples in Theorist.)
In cases of appax-ent misunderstanding, the same explanation processsuggests a misunderstanding, rather than a plannedact, as the reason for the utterance.
To handle thesecases, the model needs a theory of the symptoms ofa failure to understand \[Poole, 1989\].
For example,a speaker $2 might explain an otherwise unexpectedresponse by a speaker $1 by hypothesizing that $2has mistaken some speech act by $1 for another witha similar decomposition r $2 might hypothesize that$1 has misunderstood (see Figure 2).
We shall nowconsider some applications.5 Some app l i ca t ionsThis first example (from \[Sehegloff, 1992\]) illustratesboth normal interpretation and the recognition of anagent's own misunderstanding:T1 Mother:  Do you know who's going to thatmeeting?T2 Russ: Who?T3 Mother:  I don't know.T4 Russ: Oh.
Probably Mrs. McOwen andprobably Mrs. Cadry and some ofthe teachers.The surface-level representation f this conversationis given as the following:T1 m: s-request(m, rinformif(r, m,knowref(r ,  w)))T2 r: s-request(r ,  m, informref(m, r, w))T3 m: s- inform(m, r, not(knowref(m,  w)))T4 r: s- informref(r,  m, w)5.1 Russ's interpretation of T1 in themeet ing example~,From Russ's perspective, T1 can be explained as apretelling, an attempt by Mother to get him to askher who is going.
Russ's rules about the relationshipbetween surface forms and speech acts (decomp) in-clude that:FACT decomp( s-request ( s l , s2,informif(s2, sl, knowref(s2, p))),pretell(sl,  s2, p)).FACT decomp( s-request ( s l , s2 ,informif(s2, sl, knowref(s2, p))),askref(sl, s2, p)).FACT decomp( s-request ( s l , s2 ,informit~s2, sl, knowref(s2, p))),askif(sx, s2, knowref(s2, p))).Russ has linguistic expectation rules for the ad-jacency pairs pretell-askref, askref-inforraref, andaskif-informif (as well as for pairs of other types).Russ also has believes that he knows who's going tothe meeting, that he knows he knows this, and thatMother's knowledge about the meeting is likely to be281Utterance ExplanationFACT decomp( u, al )^ try(s l ,s2,al , ts)D utter(s1, s2, u, ts).Planned ActionsDEFAULT (2, intendact(sl, s2, al , ts) ) :shouldTry(sl, s2, al, ts):D try(sl ,s2,al ,ts) .Plan Adopt ionDEFAULT (3, adopt(a1, s2, al ,  a2, ts)):hasGoal(sl, do(s2, a2 ), ts)^ wouldEx(sl, do(s1, aa), do(s2, a2))^ iintentionsOk(sl, al, ts)D shouldTry(sl, s2, al, ts).AcceptanceDEFAULT (2, ts)):expected(s1, a, ts)D shouldTry(sl, s2, a, is).
"If agent $1 intends that agent S$ perform the action A~and A2 is the expected reply to the action A1, and itwould be coherent for SI to perform A1, then $1 shoulddo so.
""If agent $1 believes that act A is the expected nextaction, then $1 should perform A.
"Figure 1: Theorist rules for producing and interpreting utterancesFailure to understandDEFAULT (3, seafMis(s~, s2,p, a2, is)) :aai  (do(s , aM),^ ambiguous(aM, al)^ lintention(a2,pli)^ lintention(aM, pli2)^ inconsistentLl(ptl, P i2)^ p = mistake(s2, at, aM))D try(s1, s2, a2, ts).Failure to be understoodDEFAULT (3, otherMis(sl, s2, p, a~, ts)) :active(do(s2, at), ts)A ambiguous(at, aM)^  o ZdE (sl, do(s2, aM), do(s1, a2))A p = mlstake(sl, ai, aM))D try(s1, s2, a2, ts).
"Speaker S might be attempting action A in discourse TSif: S was thought o have performed action AM; but, thelinguistic intentions of AM are inconsistent with those ofA; acts A1 and AM have a similar surface form (and hencecould be mistaken); and, H may have made this mistake.
""Speaker S might be attempting action A in discourseTS if: speaker H was thought o have performed ac-tion At; but, acts AI and AM have a similar surfaceform; if H had performed AM, A would be expected;S may express the linguistic intentions of A; and, Smay have made the mistake.
"Figure 2: Rules for diagnosing misunderstandingbetter than his own.
We assume that he can makedefault assumptions about what Mother believes andwants:FACT believe(r, knowref ( r ,  w)).FACT believe(r, knowi f ( r ,knowref ( r ,w) ) ) .FACT believe(r, knowsBet terRef (m, r ,w) ) .DEFAULT (1, credulousB(p)) : believe(in, p).DEFAULT (1, credulousg(p, ts)) : hasGoal(in, p, ts).Russ's interpretation of T1 as a pretelling is pos-sible using the meta-plan for plan adoption and therule for planned action.1.
The propositionhasGoal(in, do(r ,  askref ( r ,  In, w)), ts(0))may be explained by abducingcredulousH(do(r,askref(r,  m, w)) , ts (0) ) .2.
An askre f  by Russ would be the expected replyto a prete l l  by Mother:wouldEz( in,do( in,pretel l (m, r, w)) ,do(r,askref(r, In, w)))It would be expected by Mother because:?
The lezpectation relation suggests that shemight try to pretell in order to get him toproduce an askref:lezpec~ation( do( in,prete l l ( in , r ,w ) ),knowsBet  e rRef ( in , r ,w) ,do( r ,askre f ( r ,m,w) ) )?
Russ may abducecred aousB(knowsnetterRef(in, r, w ) )to explainbelieve ( in ,knowsBet terRef ( in ,  r w)) .3.
The discourse context is empty at this point,so the linguistic intentions of pretelling satisfyl intentionsOk.2824.
Lastly, Russ may assume 1?adopt(m, r, pretell(m, r, w),askref(r, m, w), ts(0))Thus, the conditions of the plan-adoptionmeta~rule are satisfied, and Russ can explainshouldTry(m, r pretell(m, r, w), ts(0)).
Thisenables him to explaintry(m, r, pretell(m, r, w), ts(0))as a planned action.
Once Russ explains thepretelling, his decomp relation and utterance expla-nation rule allow him to explain the utterance.5.2 Russ's detection of his ownmisunderstanding i  the meetingexample~From Russ's perspective, the inform-not-knowrefthat Mother performs in T3 signals a misunderstand-ing.
Assuming T1 is a pretelling, just prior to T3,Russ's model of the discourse corresponds to the fol-lowing:expressed(do(m, pretell(m, r, w)), 1)expressed(knowref(m, w), 1)expressed(knowsBetterItef(m, r, w), 1)expressed(intend(m,do(m, informref(m, r w))), 1)expressed(intend(m, knowref(r, w)), 1)expressed(do(r, askref(r, m, w)), 2)expressedNot(knowref(r, w), 2)expressed(intend(r, knowref(r, w)), 2)expressed(intend(r,do(m, informref(m, r w))), 2)T3 does not demonstrate acceptance b cause in-form(m, r, not(knowref(m, w))) is not coherentwith this interpretation f the discourse.
This act isincoherent because not(knowref(m, w)) is amongthe linguistic intentions of this inform, while accord-ing to the model active(knowref(m, w),ts(2)).Thus, it is not the case that:lintentionsOk (m,inform(m, r, not(knowref(m, w))),ts(2))As a result, Russ cannot attribute to Mother anyexpected act, and must attribute a misunderstandingto himself or to her.Russ may attribute T3 to a self-misunderstandingusing the rule for detecting failure to understand.We sketch the proof below.1.
According to the Context,expressed( o(m,pretell(m,r,w) ),O).And, Russ may assume that the activation of1?The only constraint on adopting aplan, is that theresult not yet be achieved:FACT active(do(a, az), ts)D -~adopt(sl, 2, al, a2, ts).this supposition persists:activationPersists(do(m,pretell(m,r,w) ),O)activationPersists( do m,pretell(m,r,w) ),lThus,active(do(m, pretell(m, r, w)), ts(2)).2.
The acts pretell and askrefhave a surface formthat is similar,s-request (m,r,informif(r,m,knowref(r,w)))So,ambiguous(pretell(m,r,w), askref(m,r,w)).3.
The linguistic intentions of the pretelling are:and(knowref(m, w),and(knowsBetterRef(m, r, w),and(intend(m,do(m, informref(m, r w))),intend(m, knowref(r, w)))))The linguistic intentions of inform-not-knowrefareand(not (knowref(m, w)),intend(m,knowif(r,not (knowref(m, w))))).But these intentions are inconsistent.4.
Russ may assumeselfMis(m,r,mistake(r,askref(m, r, w),prete|l(m, r w)),inform(m, r, not(knowref(m, w))),ts(2)).Once Russ explains the inform-not-knowref, hisdeeomp relation and utterance explanation rule al-low him to explain the utterance.5.3 A case of other-misunderstanding:Speaker A finds that speaker B hasmisunderstoodWe now consider a new example (from McLaugh-lin \[1984\]), in which a participant A recognizes thata another participant, B, has mistaken a request inT1 for a test:T1 A: When is the dinner for Alfred?T2 B: Is it at seven-thirty?T3 A: No, I'm asking you.T4 B: Oh.
I don't know.The surface-level representation f this conversationis given as the following:T1 a: s-request(a, b informref(b, a, d))T2 b: s-request(b, a informif(a, b, p))T3 a: s-lnform(a, b,intend(a, do(a, askref(a, b, d))))T4 b: s-inform(b, a, not(knowref(b, d)))283A has linguistic expectation rules for the adjacencypairs pretell-askref, askref-informref, askif-informif,and testref-askif.
A also believes that she does notknow the time of the dinner, that B does know thetime of the dinner.
11 We assume that A can make de-fault assumptions about what B believes and wants:FACT believe(a, not(knowref(a,d))) .FACT believe(a, knowref(b,d)).FACT hasGoal( a,do(b,informref(b,a,d ) ),ts( O ) ).DEFAULT (1, credulousB(p) ) : believe(b, p).DEFAULT (1, credulousH(p, ts)) : hasGoal(b, p, ts)./,From A's perspective, after generating T1, hermodel of the discourse is the following:ezpressed(do(a, skref(a, b, d)), 1)e p,e,sedgot(knowref(a, d), 1)expressed(intend(a, knowref(a, d)), 1)expressed(intend(a,do(b, informref(b, a, d))), 1)According to the decomp relation, T2 might be in-terpretable as askif(b, a, p).
However, T2 does notdemonstrate acceptance, because there is no askref-askif adjacency-pair from which to derive an expec-tation.
T2 is not a plan adoption because A does notbelieve that B believes that A knows whether the din-ner is at seven-thirty.
However, there is evidence formisunderstanding, because both information-seekingquestions and tests can be formulated as surface re-quests.
Also, T2 is interpretable as a guess and re-quest for confirmation (represented asaskif), whichwould be expected after a test.
We sketch the proofbelow.1.
According to the context:ezpressed(do(a, skref(a, b, d)), 0).A may assume that the activation of this sup-position persists:activationPersists(do(a, askref(a, b, d)), 0).Thus, aaive( do( a,askref( a,b,d )),ts(1) .2.
The acts askref and testrefhave a surface formthat is similar, namelys-request (a,b, lnformref(b,a,knowref(b,d))).So,ambiguous( askref( a,b,d ), testref(a,b,d)).3.
An askif by B would be the expected reply to atestref  by A:wouldEx(b,do(a,testref(a, b, d)),do(b,asklf(b, a, p)))From A's perspective, it would be expected byB because:?
The iezpectation relation suggests that Amight try to produce a testref  in order toget him to produce an askif:11A must believe that B knows when the dinner is forher to have adopted a plan in T1 to produce an askrefget B to perform the desired informref.lexpectation( do( a,testref( a,b,d )),and(knowref(b,d) ,and(knowlf(b,p),and(pred(p,X) ,pred(d,X))) ,do(b,asklf(b,a,p)))The condition of this rule requires that Bbelieve he knows the referent of descrip-tion d and that p asserts that the de-scribed property holds of the referent hathe knows.
For example, if we represent "Bknows when the dinner is" as the descrip-tionknowref(b, the(X, t ime(dinner ,  X))),then the condition requires thatknowif(b, t ime(dlnner ,  q)) for some q.This is a gross simplification, but the bestthat the notation allows.A may assume that B believes the conditionof this lezpecta~ion by default.6 ConclusionThe primary contribution of this work is that ittreats misunderstanding and repair as intrinsic toconversants' core language abilities, accounting forthem with the same processing mechanisms that un-derlie normal speech.
In particular, it formulatesboth interpretation and the detection of misunder-standings as explanation problems and models themas abduction.We have implemented our model in Prolog andthe Theorist framework for abduction with Priori-tized defaults.
Program executions on a Sun-4 forfour-turn dialogues take 2 cpu seconds per turn onaverage.Directions for future work include extending themodel to handle more than one communicative actper turn, misunderstood reference \[Heeman andHirst, 1992\], and integrating the account with sen-tence processing and domain planning.AcknowledgementsThis work was supported by the University ofToronto and the Natural Sciences and EngineeringResearch Council of Canada.
We thank Ray Reiterfor his suggestions regarding abduction; James Allenfor his advice; Paul van Arragon and Randy Goebelfor their help on using Theorist; Hector Levesque,Mike Gruninger, Sheila McIlraith, Javier Pinto, andSteven Shapiro for their comments on many of theformal aspects of this work; Phil Edmonds, StephenGreen, Diane ttorton, Linda Peto, and the othermembers of the natural language group for their com-ments; and Suzanne Stevenson for her comments onearlier drafts of this paper.284References\[Ahuja nd Reggia, 1986\] Sanjiev B. Ahuja andJames A. Reggia.
The parsimonious coveringmodel for inexact abductive reasoning in diagnos-tic systems.
In Recent Developments in the The.ory and Applications of Fuzzy Sets.
Proceedingsof NAFIPS '86 - 1986 Conference of the NorthAmerican Fuzzy Information Processing Society,pages 1-20, 1986.\[Allen, 1979\] James F. Allen.
A Plan-Based Ap-proach to Speech Act Recognition.
PhD thesis,Department of Computer Science, University ofToronto, Toronto, Canada, 1979.
Published asUniversity of Toronto, Department of ComputerScience Technical Report No.
131.\[Allen, 1983\] James F. Allen.
Recognizing inten-tions from natural language utterances.
In MichaelBrady, Robert C. Berwick, and James F. Allen, ed-itors, Computational Models of Discourse, pages107-166.
The MIT Press, 1983.\[Brewka, 1989\] Gerhard Brewka.
Preferred subthe-ories: An extended logical framework for defaultreasoning.
In Proceedings of the 11th InternationalJoint Conference on Artificial Intelligence, pages1043-1048, Detroit, MI, 1989.\[Calistri-Yeh, 1991\] Randall J. Calistri-Yeh.
Utiliz-ing user models to handle ambiguity and miscon-ceptions in robust plan recognition.
User Mod-elling and User Adapted Interaction, 1(4):289-322,1991.\[Carberry, 1985\] Sandra Carberry.
Pragmatics Mod-eling in Information Systems Interfaces.
PhD the-sis, University of Delaware, Newark, Delaware,1985.\[Cawsey, 1991\] Alison J. Cawsey.
A belief revisionmodel of repair sequences in dialogue.
In ErnestoCosta, editor, New Directions in Intelligent Tutor-ing Systems.
Springer Verlag, 1991.\[Cohen and Levesque, 1985\] Philip R. Cohen andHector J. Levesque.
Speech acts and rationality.
In23th Annual Meeting of the Association for Com-putational Linguistics, Proceedings of the Confer-ence, pages 49-60, 1985.\[Cohen et aL, 1990\] Philip R. Cohen, Jerry Morgan,and Martha Pollack, editors.
Intentions in Com-munication.
The MIT Press, 1990.\[Eller and Carberry, 1992\] Rhonda Eller and SandraCarberry.
A meta-rule approach to flexible planrecognition i  dialogue.
User Modelling and UserAdapted Interaction, 2(1-2):27-53, 1992.\[Fox, 1987\] Barbara Fox.
Interactional reconstruc-tion in real-time language processing.
CognitiveScience, 11:365-387, 1987.\[Garfinkel, 1967\] Harold Garfinkel.
Studies in Eth-nomethodology.
Prentice Hall, Englewood Cliffs,NJ, 1967.
(Reprinted: Cambridge, England:Polity Press, in association with Basil Blackwell,1984.
).\[Goodman, 1985\] Bradley Goodman.
Repairing ref-erence identification failures by relaxation.
In The23rd Annual Meeting of the Association for Com-putational Linguistics: Proceedings of the Confer-ence, pages 204-217, Chicago, 1985.\[(\]rice, 1957\] H. P. Grice.
Meaning.
The Philosoph-ical Review, 66:377-388, 1957.\[Gutwin and McCalla, 1992\] Carl Gutwin and Gor-don McCalla.
Would I lie to you?
Modelling con-text and pedagogic misrepresentation n tutorialdialogue.
In 30th Annual Meeting of the Associa-tion for Computational Linguistics, Proceedings ofthe Conference, pages 152-158, Newark, DE, 1992.\[Heeman and Hirst, 1992\] Peter Heemanand Graeme Hirst.
Collaborating on referring ex-pressions.
Technical Report 435, Department ofComputer Science, University of Rochester, 1992.\[Litman, 1985\] Diane J. Litman.
Plan Recogni-tion and Discourse Analysis: An Integrated Ap-proach for Understanding Dialogues.
PhD the-sis, Department of Computer Science, Universityof P~chester, Rochester, NY, 1985.
Published asUniversity of Rochester Computer Science Techni-cal Report 170.\[Loveland, 1978\] D. W. Loveland.
Automated The-orem Proving: A Logical Basis.
North-Holland,Amsterdam, The Netherlands, 1978.\[McCoy, 1985\] Kathleen F. McCoy.
The role of per-spective in responding to property misconceptions.In Proceedings of the Ninth International JointConference on Artificial Intelligence, volume 2,pages 791-793, 1985.\[McLanghlin, 1984\] Margaret L. McLaughlin.
Con-versation: How Talk is Organized.
Sage Publica-tions, Beverly Hills, 1984.\[McRoyandHirst, 1992\] Susan W. McRoy andGraeme Hirst.
The repair of speech act misunder-standings by abductive inference.
1992.
Submittedfor publication.\[McRoy, 1993\] Susan W. McRoy.
Abductive Inter-pretation and Reinterpretation of Natural Lan-guage Utterances.
PhD thesis, Departmentof Computer Science, University of Toronto,Toronto, Canada, 1993.
In preparation.\[Perrault and Allen, 1980\] C. Raymond Per-fault and :lames F. Allen.
A plan-based analysisof indirect speech acts.
Computational Linguistics,6:167-183, 1980.\[Perrault, 1990\] C. Raymond Perrault.
An appli-cation of default logic to speech act theory.
InPhilip R. Cohen, Jerry Morgan, and Martha Pol-lack, editors, Intentions in Communication, pages285161-186.
The MIT Press, 1990.
An earlier versionof this paper was published as Technical ReportCSLI-87-90 by the Center for the Study of Lan-guage and Information.\[Poole l al., 1987\] David Poole, Randy Goebel, andRomas Aleliunas.
Theorist: A logical reasoningsystem for defaults and diagnosis.
In Nick Cer-cone and Gordon McCalla, editors, The Knowl-edge Frontier: Essays in the Representation ofKnowledge, pages 331-352.
Springer-Verlag, NewYork, 1987.
Also published as Research ReportCS-86-06, Faculty of Mathematics, University ofWaterloo, February, 1986.\[Poole, 1986\] David Poole.
Default reasoning anddiagnosis as theory formation.
Technical ReportCS-86-08, Department ofComputer Science, Uni-versity of Waterloo, Waterloo, Ontario, 1986.\[Poole, 1989\] David Poole.
Normality and faults inlogic-based iagnosis.
In Proceedings of the 11thInternational Joint Conference on Artificial Intel-ligence, pages 1304-1310, 1989.\[Schegloff and Sacks, 1973\] Emanuel A. Schegloffand Harvey Sacks.
Opening up closings.
Semi-otica, 7:289-327, 1973.\[Schegloff, 1992\] Emanuel A. Schegloff.
Repair af-ter next turn: The last structurally provided e-fense of intersubjectivity in conversation.
Ameri-can Journal of Sociology, 97(5):1295-1345, 1992.\[Stalnaker, 1972\] Robert C. Stalnaker.
Pragmatics.In Semantics of Natural Language, pages 380-397.D.
Reidel Publishing Company, Dordrecht, 1972.\[Stickel, 1989\] M. E. Stickel.
A Prolog technologytheorem prover.
Journal of Automated Reasoning,4:353-360, 1989.\[Suchman, 1987\] Lucy A. Suchman.
Plans and Sit-uated Actions.
Cambridge University Press, Cam-bridge, UK, 1987.\[Thomason, 1990\] Pdchmond H. Thomason.
Propa-gating epistemic coordination through mutual de-faults I.
In Rohit Parikh, editor, Proceedings,Third Conference on Theoretical Aspects of Rea-soning about Knowledge (TARK 1990), pages 29-39, Pacific Grove, CA, 1990.\[Umrigar and Pitchumani, 1985\] Zerksis D. Umri-gar and Vijay Pitchumani.
An experiment in pro-gramming with full first-order logic.
In Symposiumof Logic Programming, Boston, MA, 1985.
IEEEComputer Society Press.\[van Arragon, 1990\] Paul van Arragon.
Nested De-fault Reasoning for User Modeling.
PhD thesis,Department of Computer Science, University ofWaterloo, Waterloo, Ontario, 1990.
Published bythe department as Research Report CS-90-25.286
