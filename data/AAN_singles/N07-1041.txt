Proceedings of NAACL HLT 2007, pages 324?331,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsCombining Probability-Based Rankers for Action-Item DetectionPaul N. BennettMicrosoft Research?One Microsoft WayRedmond, WA 98052paul.n.bennett@microsoft.comJaime G. CarbonellLanguage Technologies Institute, Carnegie Mellon5000 Forbes AvePittsburgh, PA 15213jgc@cs.cmu.eduAbstractThis paper studies methods that automat-ically detect action-items in e-mail, animportant category for assisting users inidentifying new tasks, tracking ongoingones, and searching for completed ones.Since action-items consist of a short spanof text, classifiers that detect action-itemscan be built from a document-level or asentence-level view.
Rather than com-mit to either view, we adapt a context-sensitive metaclassification framework tothis problem to combine the rankings pro-duced by different algorithms as well asdifferent views.
While this framework isknown to work well for standard classi-fication, its suitability for fusing rankershas not been studied.
In an empirical eval-uation, the resulting approach yields im-proved rankings that are less sensitive totraining set variation, and furthermore, thetheoretically-motivated reliability indica-tors we introduce enable the metaclassi-fier to now be applicable in any problemwhere the base classifiers are used.1 IntroductionFrom business people to the everyday person, e-mail plays an increasingly central role in a modernlifestyle.
With this shift, e-mail users desire im-proved tools to help process, search, and organizethe information present in their ever-expanding in-boxes.
A system that ranks e-mails according to the?This work was performed primarily while the first authorwas supported by Carnegie Mellon University.From: Henry Hutchins <hhutchins@innovative.company.com>To: Sara Smith; Joe Johnson; William WoolingsSubject: meeting with prospective customersHi All,I?d like to remind all of you that the group from GRTY willbe visiting us next Friday at 4:30 p.m.
The schedule is:+ 9:30 a.m.
Informal Breakfast and Discussion in Cafeteria+ 10:30 a.m. Company Overview+ 11:00 a.m.
Individual Meetings (Continue Over Lunch)+ 2:00 p.m. Tour of Facilities+ 3:00 p.m.
Sales PitchIn order to have this go off smoothly, I would like to practicethe presentation well in advance.
As a result, I will need eachof your parts by Wednesday.Keep up the good work!
?HenryFigure 1: An E-mail with Action-Item (italics added).likelihood of containing ?to-do?
or action-items canalleviate a user?s time burden and is the subject ofongoing research throughout the literature.In particular, an e-mail user may not always pro-cess all e-mails, but even when one does, someemails are likely to be of greater response urgencythan others.
These messages often contain action-items.
Thus, while importance and urgency are notequal to action-item content, an effective action-itemdetection system can form one prominent subcom-ponent in a larger prioritization system.Action-item detection differs from standard textclassification in two important ways.
First, the useris interested both in detecting whether an emailcontains action-items and in locating exactly wherethese action-item requests are contained within theemail body.
Second, action-item detection attempts324to recover the sender?s intent ?
whether she meansto elicit response or action on the part of the receiver.In this paper, we focus on the primary problemof presenting e-mails in a ranked order according totheir likelihood of containing an action-item.
Sinceaction-items typically consist of a short text span ?a phrase, sentence, or small passage ?
supervisedinput to a learning system can either come at thedocument-level where an e-mail is labeled yes/noas to whether it contains an action-item or at thesentence-level where each span that is an action-item is explicitly identified.
Then, a correspondingdocument-level classifier or aggregated predictionsfrom a sentence-level classifier can be used to esti-mate the overall likelihood for the e-mail.Rather than commit to either view, we use a com-bination technique to capture the information eachviewpoint has to offer on the current example.
TheSTRIVE approach (Bennett et al, 2005) has beenshown to provide robust combinations of heteroge-neous models for standard topic classification bycapturing areas of high and low reliability via theuse of reliability indicators.However, using STRIVE in order to produce im-proved rankings has not been previously studied.Furthermore, while they introduce some reliabil-ity indicators that are general for text classificationproblems as well as ones specifically tied to na?
?veBayes models, they do not address other classifica-tion models.
We introduce a series of reliability in-dicators connected to areas of high/low reliability inkNN, SVMs, and decision trees to allow the combi-nation model to include such factors as the sparse-ness of training example neighbors around the cur-rent example being classified.
In addition, we pro-vide a more formal motivation for the role these vari-ables play in the resulting metaclassification model.Empirical evidence demonstrates that the result-ing approach yields a context-sensitive combinationmodel that improves the quality of rankings gener-ated as well as reducing the variance of the rankingquality across training splits.2 Problem ApproachIn contrast to related combination work, we focus onimproving rankings through the use of a metaclass-ification framework.
In addition, rather than sim-ply focusing on combining models from differentclassification algorithms, we also examine combin-ing models that have different views, in that both thequalitative nature of the labeled data and the applica-tion of the learned base models differ.
Furthermore,we improve upon work on context-sensitive com-bination by introducing reliability indicators whichmodel the sensitivity of a classifier?s output aroundthe current prediction point.
Finally, we focus on theapplication of these methods to action-item data ?a growing area of interest which has been demon-strated to behave differently than more standard textclassification problems (e.g.
topic) in the literature(Bennett and Carbonell, 2005).2.1 Action-Item DetectionThere are three basic problems for action-item de-tection.
(1) Document detection: Classify an e-mailas to whether or not it contains an action-item.
(2)Document ranking: Rank the e-mails such that alle-mail containing action-items occur as high as pos-sible in the ranking.
(3) Sentence detection: Classifyeach sentence in an e-mail as to whether or not it isan action-item.Here we focus on the document ranking problem.Improving the overall ranking not only helps usersfind e-mails with action-items quicker (Bennett andCarbonell, 2005) but can decrease response timesand help ensure that key e-mails are not overlooked.Since a typical user will eventually process allreceived mail, we assume that producing a qualityranking will more directly measure the impact onthe user than accuracy or F1.
Therefore, we focus onROC curves and area under the curve (AUC) sinceboth reflect the quality of the ranking produced.2.2 Combining Classifiers with MetaclassifiersOne of the most common approaches to classi-fier combination is stacking (Wolpert, 1992).
Inthis approach, a metaclassifier observes a past his-tory of classifier predictions to learn how to weightthe classifiers according to their demonstrated ac-curacies and interactions.
To build the history,cross-validation over the training set is used to ob-tain predictions from each base classifier.
Next, ametalevel representation of the training set is con-structed where each example consists of the classlabel and the predictions of the base classifiers.
Fi-nally, a metaclassifier is trained on the metalevel rep-resentation to learn a model of how to combine thebase classifiers.However, it might be useful to augment the his-tory with information other than the predicted prob-abilities.
For example, during peer review, reviewers325class class     MetaclassifierReliabilityIndicatorsSVMUnigram    w1w2w3wn?
?
?
?
?
?r1r2rnFigure 2: Architecture of STRIVE.
In STRIVE, an additional layer of learning is added where the metaclassifier can use the contextestablished by the reliability indicators and the output of the base classifiers to make an improved decision.typically provide both a 1-5 acceptance rating and a1-5 confidence.
The first of these is related to an es-timate of class membership, P (?accept??
| paper),but the second is closer to a measure of expertise ora self-assessment of the reviewer?s reliability on anexample-by-example basis.Automatically deriving such self-assessments forclassification algorithms is non-trivial.
The StackedReliability Indicator Variable Ensemble framework,or STRIVE, demonstrates how to extend stacking byincorporating such self-assessments as a layer of re-liability indicators and introduces a candidate set offunctions (Bennett et al, 2005).The STRIVE architecture is depicted in Figure 2.From left to right: (1) a bag-of-words representationof the document is extracted and used by the baseclassifiers to predict class probabilities; (2) reliabil-ity indicator functions use the predicted probabili-ties and the features of the document to characterizewhether this document falls within the ?expertise?of the classifiers; (3) a metalevel classifier uses thebase classifier predictions and the reliability indica-tors to make a more reliable combined prediction.From the perspective of improving action-itemrankings, we are interested in whether stacking orstriving can improve the quality of rankings.
How-ever, we hypothesize that striving will perform bettersince it can learn a model that varies the combinationrule based on the current example and thus, bettercapture when a particular classifier at the document-level or sentence-level, bag-of-words or n-gram rep-resentation, etc.
will produce a reliable prediction.2.3 Formally Motivating Reliability IndicatorsWhile STRIVE has been shown to provide robustcombination for topic classification, a formal moti-vation is lacking for the type of reliability indicatorsthat are the most useful in classifier combination.Assume we restrict our choice of metaclassifier toa linear model.
One natural choice is to rank thee-mails according to the estimated posterior proba-bility, P?
(class = action item | x), but in a linearcombination framework it is actually more conve-nient to work with the estimated log-odds or logittransform which is monotone in the posterior, ??
=log P?
(class=action item|x)1?P?
(class=action item|x) (Kahn, 2004).Now, consider applying a metaclassifier to a sin-gle base classifier.
Given only a classifier?s probabil-ity estimates, a metaclassifier cannot improve on theestimates if they are well-calibrated (DeGroot andFienberg, 1986).
Thus a metaclassifier applied toa single base classifier corresponds to recalibration(Kahn, 2004).Assume each of the n base models gives an un-calibrated log-odds estimate ??i.
Then the com-bination model would have the form ???
(x) =W0(x)+?ni=1 Wi(x)?
?i(x) where the Wi are exam-ple dependent weight functions that the combinationmodel learns.
The obvious implication is that ourreliability indicators can be informed by the optimalvalues for the weighting functions.We can determine the optimal weights in a sim-plified case with a single base classifier by assumingwe are given ?true?
log-odds values, ?, and a familyof distributions ?x such that ?x = p(z | x)encodes what is local to x by giving the probabilityof drawing a point z near to x.
We use ?
instead of?x for notational simplicity.
Since ?
encodes theexample dependent nature of the weights, we candrop x from the weight functions.
To find weightsthat minimize the squared difference between thetrue log-odds and the estimated log-odds in the ?vicinity of x, we can solve a standard regressionproblem, argminw0,w1 E?
[(w1 ??
+ w0 ?
?
)2].Under the assumption VAR?[??
]6= 0, this yields:326w0 = E?[?]
?
w1E?[??
](1)w1 =COV?[?
?, ?]VAR?[??]
= ???????,??
(2)where ?
and ?
are the stdev and correlation co-efficient under ?, respectively.
The first parame-ter is a measure of calibration that addresses thequestion, ?How far off on average is the estimatedlog-odds from the true log-odds in the local con-text??
The second is a measure of correlation, ?Howclosely does the estimated log-odds vary with thetrue log-odds??
Note that the second parameter de-pends on the local sensitivity of the base classifier,VAR1/2?[??
]= ???.
Although we do not have truelog-odds, we can introduce local density models toestimate the local sensitivity of the model.In particular, we introduce a series of relia-bility indicators by first defining a ?
distribu-tion and either computing VAR?[??
], E?[??
]orthe closely related terms VAR?[??
(z) ?
??(x)],E?[??
(z) ?
??(x)].
We use the resulting values foran example as features for a linear metaclassifier.Thus we use a context-dependent bias term but leavethe more general model for future work.2.4 Model-Based Reliability IndicatorsAs discussed in Section 2.3, we wish to define localdistributions in order to compute the local sensitivityand similar terms for the base classification models.To do so, we define local distributions that have thesame ?flavor?
as the base classification model.First, consider the kNN classifier.
Since we areconcerned with how the decision function wouldchange as we move locally around the current pre-diction point, it is natural to consider a set of shiftsdefined by the k neighbors.
In particular, let di de-note the document that has been shifted by a factor?i toward the ith neighbor, i.e.
di = d+?i(ni?d).We use the largest ?i such that the closest neighborto the new point is the original document, i.e.
theboundary of the Voronoi cell (see Figure 3).
Clearly,?i will not exceed 0.5, and we can find it efficientlyusing a simple bisection algorithm.
Now, let ?
bea uniform point-mass distribution over the shiftedpoints and ?
?kNN, the output score of the kNN model.
?1.5 ?1 ?0.5 0 0.5 1 1.5 2?1.5?1?0.500.511.5212 3456xFigure 3: Illustration of the kNN shifts produced for a predic-tion point x using the numbered points as its neighborhood.Given this definition of ?, it is now straight-forward to compute the kNN based reliabil-ity indicators: E?[?
?kNN(z) ?
?
?kNN(x)] andVar1/2?
[?
?kNN(z) ?
?
?kNN(x)].Similarly, we define variables for the SVM class-ifier by considering a document?s locality in termsof nearby support vectors from the set of supportvectors, V .
To determine ?i, we define it in termsof the closest support vector in V to d. Let  behalf the distance to the nearest point in V , i.e.
 =12 minv?V ?v ?
d?.
Then ?i = ?vi?d?
.1 Thus, theshift vectors are all rescaled to have the same length.Now, we must define a probability for the shift.
Weuse a simple exponential based on  and the rela-tive distance from the document to the support vec-tor defining this shift.
Let di ?
?
where P?
(di) ?exp(?
?vi ?
d?
+ 2) and?Vi=1 P?
(di) = 1.2Given this definition of ?, we compute theSVM based reliability indicators: E?[?
?SVM(z) ??
?SVM(x)] and Var1/2?
[?
?SVM(z) ?
?
?SVM(x)].Space prevents us from presenting all the deriva-tions here.
However, we also define decision-treebased variables where the locality distribution giveshigh probability to documents that would land innearby leaves.
For a multinomial na?
?ve Bayes model(NB), we define a distribution of documents iden-tical to the prediction document except having anoccurrence of a single feature deleted.
For themultivariate Bernoulli na?
?ve Bayes (MBNB) model1We assume that the minimum distance is not zero.
If it iszero, then we return zero for all of the variables.2As is standard to handle different document lengths, wetake the distance between documents after they have been nor-malized to the unit sphere.327that models feature presence/absence, we use adistribution over all documents that has one pres-ence/absence bit flipped from the prediction docu-ment.
It is interesting to note that the variables fromthe na?
?ve Bayes models can be shown to be equiva-lent to variables introduced by Bennett et al (2005)?
although those were derived in a different fashionby analyzing the weight a single feature carries withrespect to the overall prediction.Furthermore, from this starting point, we go on todefine similar variables of possible interest.
Includ-ing the two for each model described here, we define10 kNN variables, 5 SVM variables, 2 decision-treevariables, 6 NB model based variables, and 6 MBNBvariables.
We describe these variables as well as im-plementation details and computational complexityresults in (Bennett, 2006).3 Experimental Analysis3.1 DataOur corpus consists of e-mails obtained from vol-unteers at an educational institution and coverssubjects such as: organizing a research work-shop, arranging for job-candidate interviews, pub-lishing proceedings, and talk announcements.
Af-ter eliminating duplicate e-mails, the corpus con-tains 744 messages with a total of 6301 automat-ically segmented sentences.
A human panel la-beled each phrase or sentence that contained anexplicit request for information or action.
416 e-mails have no action-items and 328 e-mails con-tain action-items.
Additional information suchas annotator agreement, distribution of messagelength, etc.
can be found in (Bennett and Car-bonell, 2005).
An anonymized corpus is availableat http://www.cs.cmu.edu/?pbennett/action-item-dataset.html.3.2 Feature RepresentationWe use two types of feature representation: a bag-of-words representation which uses all unigram to-kens as the feature pool; and a bag-of-n-gramswhere n includes all n-grams where n ?
4.
Forboth representations at both the document-level andsentence-level, we used only the top 300 features bythe chi-squared statistic.3.3 Document-Level ClassifierskNNWe used a s-cut variant of kNN common in textclassification (Yang, 1999) and a tfidf-weightingof the terms with a distance-weighted vote of theneighbors to compute the output.
k was set to be2(dlog2 Ne + 1) where N is the number of trainingpoints.
3 The score used as the uncalibrated log-odds estimate of being an action-item is:?
?kNN(x) =?n?kNN(x)|c(n)=actionitemcos(x,n) ?
?n?kNN(x)|c(n) 6=actionitemcos(x,n).SVMWe used a linear SVM as implemented in theSVMlight package v6.01 (Joachims, 1999) with atfidf feature representation and L2-norm.
All de-fault settings were used.
SVM?s margin score,?
?iyi K(xi,xj), has been shown to empiricallybehave like an uncalibrated log-odds estimate (Platt,1999).Decision TreesFor the decision-tree implementation, we used theWinMine toolkit and refer to this as Dnet below (Mi-crosoft Corporation, 2001).
Dnet builds decisiontrees using a Bayesian machine learning algorithm(Chickering et al, 1997; Heckerman et al, 2000).The estimated log-odds is computed from a Laplacecorrection to the empirical probability at a leaf node.Na?
?ve BayesWe use a multinomial na?
?ve Bayes (NB) and a mul-tivariate Bernoulli na?
?ve Bayes classifier (MBNB)(McCallum and Nigam, 1998).
For these classifi-ers, we smoothed word and class probabilities us-ing a Bayesian estimate (with the word prior) anda Laplace m-estimate, respectively.
Since these areprobabilistic, they issue log-odds estimates directly.3.4 Sentence-Level ClassifiersEach e-mail is automatically segmented into sen-tences using RASP (Carroll, 2002).
Since the cor-pus has fine grained labels, we can train classifiersto classify a sentence.
Each classifier in Section 3.3is also used to learn a sentence classifier.
However,we then must make a document-level prediction.In order to produce a ranking score, the con-fidence that the document contains an action-item is:??
(d) ={ 1n(d)?s?d|pi(s)=1 ??
(s), ?s?d|pi(s) = 11n(d) maxs?d ??
(s) o.w.3This rule is not guaranteed be optimal for a particular valueof N but is motivated by theoretical results which show such arule converges to the optimal classifier as the number of trainingpoints increases (Devroye et al, 1996).328where s is a sentence in document d, pi is the class-ifier?s 1/0 prediction, ??
is the score the classifier as-signs as its confidence that pi(s) = 1, and n(d) isthe greater of 1 and the number of (unigram) to-kens in the document.
In other words, when anysentence is predicted positive, the document scoreis the length normalized sum of the sentence scoresabove threshold.
When no sentence is predicted pos-itive, the document score is the maximum sentencescore normalized by length.
The length normaliza-tion compensates for the fact that we are more likelyto emit a false positive the longer a document is.3.5 StackingTo examine the hypothesis that the reliability in-dicators provide utility beyond the informationpresent in the output of the 20 base classifiers(2 representations?2 views?5 classifiers), we con-struct a linear stacking model which uses only thebase classifier outputs and no reliability indicators asa baseline.
For the implementation, we use SVMlightwith default settings.
The inputs to this classifier arenormalized to have zero mean and a scaled variance.3.6 StrivingSince we are constructing base classifiers for boththe bag-of-words and bag-of-n-grams representa-tions, this gives 58 reliability indicators from com-puting the variables in Section 2.4 for the document-level classifiers (58 = 2 ?
[6 + 6 + 10 + 5 + 2]).Although the model-based indicators are definedfor each sentence prediction, to use them at thedocument-level we must somehow combine the re-liability indicators over each sentence.
The simplestmethod is to average each classifier-based indicatoracross the sentences in the document.
We do so andthus obtain another 58 reliability indicators.Furthermore, our model might benefit from someof the structure a sentence-level classifier offerswhen combining document predictions.
Analogousto the sensitivity of each base model, we can con-sider such indicators as the mean and standard de-viation of the classifier confidences across the sen-tences within a document.
For each sentence-levelbase classifier, these become two more indicatorswhich we can benefit from when combining docu-ment predictions.
This introduces 20 more variables(20 = 2 representations ?
2 ?
5 classifiers).Finally, we include the 2 basic voting statisticreliability-indicators (PercentPredictingPositive andPercentAgreeWBest) that Bennett et al (2005) founduseful for topic classification.
This yields a total of138 reliability-indicators (138 = 58+ 20+ 58+ 2).With the 20 classifier outputs, there are a total of 158input features for striving to handle.As with stacking, we use SVMlight with defaultsettings and normalize the inputs to this classifier tohave zero mean and a scaled variance.3.7 Performance MeasuresWe wish to improve the rankings of the e-mails inthe inbox such that action-item e-mails occur higherin the inbox.
Therefore, we use the area under thecurve (AUC) of an ROC curve as a measure of rank-ing performance.
AUC is a measure of overall modeland ranking quality that has gained wider adoptionrecently and is equivalent to the Mann-Whitney-Wilcoxon sum of ranks test (Hanley and McNeil,1982).
To put improvement in perspective, we canwrite our relative reduction in residual area (RRA)as 1?AUC1?AUCbaseline .
We present gains relative to thebest AUC performer (bRRA), and relative to perfectdynamic selection performance, (dRRA), which as-sumes we could accurately dynamically choose thebest classifier per cross-validation run.The F1 measure is the harmonic mean of preci-sion and recall and is common throughout text class-ification (Yang and Liu, 1999).
Although we are notconcerned with F1 performance here, some users ofthe system might be interested in improving rank-ing while having negligible negative effect on F1.Therefore, we examine F1 to ensure that an improve-ment in ranking will not come at the cost of a statis-tically significant decrease in F1.3.8 Experimental MethodologyTo evaluate performance of the combination sys-tems, we perform 10-fold cross-validation and com-pute the average performance.
For significance tests,we use a two-tailed t-test (Yang and Liu, 1999)to compare the values obtained during each cross-validation fold with a p-value of 0.05.We examine two hypotheses: Stacking will out-perform all of the base classifiers; Striving will out-perform all the base classifiers and stacking.3.9 Results & DiscussionTable 1 presents the summary of results.
The bestperformer in each column is in bold.
If a combi-nation method statistically significantly outperformsall base classifiers, it is underlined.329F1 AUC bRRA dRRADocument-Level, Bag-of-Words RepresentationDnet 0.7398 0.8423 1.41 1.78NB 0.6905 0.7537 2.27 2.91MBNB 0.6729 0.7745 2.00 2.49SVM 0.6918 0.8367 1.48 1.87kNN 0.6695 0.7669 2.17 2.74Document-Level, Ngram RepresentationDnet 0.7412 0.8473 1.38 1.77NB 0.7361 0.8114 1.75 2.23MBNB 0.7534 0.8537 1.30 1.61SVM 0.7392 0.8640 1.24 1.59kNN 0.7021 0.8244 1.62 2.01Sentence-Level, Bag-of-Words RepresentationDnet 0.7793 0.8885 1.00 1.27NB 0.7731 0.8645 1.21 1.50MBNB 0.7888 0.8699 1.14 1.42SVM 0.6985 0.8548 1.34 1.70kNN 0.6328 0.6823 2.98 3.88Sentence-Level, Ngram RepresentationDnet 0.7521 0.8723 1.13 1.42NB 0.8012 0.8723 1.15 1.46MBNB 0.8010 0.8777 1.10 1.38SVM 0.7842 0.8620 1.23 1.58kNN 0.6811 0.8078 1.76 2.29MetaclassifiersStacking 0.7765 0.8996 0.88 1.12STRIVE 0.7813 0.9145 0.76 0.94Table 1: Base classifier and combiner performanceNow, we turn to the issue of whether combinationimproves the ranking of the documents.
Examiningthe results in Table 1, we see that STRIVE statisticallysignificantly beats every other classifier according toAUC.
Stacking outperforms the base classifiers withrespect to AUC but not statistically significantly.Examining F1, we see that neither combinationmethod outperforms the best base classifier, NB(sent,ngram).
If we examine the hypothesis ofwhether this base classifier significantly outperformseither combination method, the hypothesis is re-jected.
Thus, STRIVE improves the overall rankingwith a negligible effect on F1.Finally, we compare the ROC curves of striving,stacking, and two of the most competitive base class-ifiers in Figure 4.
We see that striving loses by aslight amount to stacking early in the curve but still0.50.60.70.80.9  100.20.40.60.81True Positive RateFalse Positive RateMBNB(sent,ngram)SVM(sent,ngram)StackingSTRIVEFigure 4: ROC curves (rotated).beats the base classifiers.
Later in the curve, it dom-inates all the classifiers.
If we examine the curvesusing error bars, we see that the variance of STRIVEdrops faster than the other classifiers as we move fur-ther along the x-axis.
Thus, STRIVE?s ranking qualityvaries less with changes to the training set.4 Related WorkSeveral researchers have considered text classifi-cation tasks similar to action-item detection.
Co-hen et al (2004) describe an ontology of ?speechacts?, such as ?Propose a Meeting?, and attemptto predict when an e-mail contains one of thesespeech acts.
Corston-Oliver et al (2004) con-sider detecting items in e-mail to ?Put on a To-DoList?
using a sentence-level classifier.
In earlierwork (Bennett and Carbonell, 2005), we demon-strated that sentence-level classifiers typically out-perform document-level classifiers on this problemand examined the underlying reasons why this was330the case.
Furthermore, we presented user studiesdemonstrating that users identify action-items morerapidly when using the system.In terms of classifier combination, a wide varietyof work has been done in the arena.
The STRIVEmetaclassification approach (Bennett et al, 2005)extended Wolpert?s stacking framework (Wolpert,1992) to use reliability indicators.
In recent work,Lee et al (2006) derive variance estimates for na?
?veBayes and tree-augmented na?
?ve Bayes and usethem in the combination model.
Our work comple-ments theirs by laying groundwork for how to com-pute variance estimates for models such as kNN thathave no obvious probabilistic component.5 Future Work and ConclusionWhile there are many interesting directions for fu-ture work, the most interesting is to directly integratethe sensitivity and calibration quantities derived intothe more general model discussed in Section 2.3.In this paper, we took an existing approach tocontext-dependent combination, STRIVE, that usedmany ad hoc reliability indicators and derived aformal motivation for classifier model-based localsensitivity indicators.
These new reliability indi-cators are efficiently computable, and the resultingcombination outperformed a vast array of alterna-tive base classifiers for ranking in an action-item de-tection task.
Furthermore, the combination resultsyielded a more robust performance relative to varia-tion in the training sets.
Finally, we demonstratedthat the STRIVE method could be successfully ap-plied to ranking.AcknowledgmentsThis work was supported by the Defense Advanced Re-search Projects Agency (DARPA) under Contract No.NBCHD030010.
Any opinions, findings and conclusions orrecommendations expressed in this material are those of the au-thor(s) and do not necessarily reflect the views of the DefenseAdvanced Research Projects Agency (DARPA), or the Depart-ment of Interior-National Business Center (DOI-NBC).ReferencesPaul N. Bennett and Jaime Carbonell.
2005.
Feature repre-sentation for effective action-item detection.
In SIGIR ?05,Beyond Bag-of-Words Workshop.Paul N. Bennett, Susan T. Dumais, and Eric Horvitz.
2005.The combination of text classifiers using reliability indica-tors.
Information Retrieval, 8:67?100.Paul N. Bennett.
2006.
Building Reliable Metaclassifiers forText Learning.
Ph.D. thesis, CMU.
CMU-CS-06-121.John Carroll.
2002.
High precision extraction of grammaticalrelations.
In COLING ?02.D.M.
Chickering, D. Heckerman, and C. Meek.
1997.
ABayesian approach to learning Bayesian networks with lo-cal structure.
In UAI ?97.William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell.2004.
Learning to classify email into ?speech acts?.
InEMNLP ?04.Simon Corston-Oliver, Eric Ringger, Michael Gamon, andRichard Campbell.
2004.
Task-focused summarization ofemail.
In Text Summarization Branches Out: Proceedings ofthe ACL ?04 Workshop.Morris H. DeGroot and Stephen E. Fienberg.
1986.
Comparingprobability forecasters: Basic binary concepts and multivari-ate extensions.
In P. Goel and A. Zellner, editors, BayesianInference and Decision Techniques.
Elsevier.Luc Devroye, La?szlo?
Gyo?rfi, and Ga?bor Lugosi.
1996.
A Prob-abilistic Theory of Pattern Recognition.
Springer-Verlag,New York, NY.James A. Hanley and Barbara J. McNeil.
1982.
The meaningand use of the area under a recever operating characteristic(roc) curve.
Radiology, 143(1):29?36.D.
Heckerman, D.M.
Chickering, C. Meek, R. Rounthwaite,and C. Kadie.
2000.
Dependency networks for inference,collaborative filtering, and data visualization.
JMLR, 1:49?75.Thorsten Joachims.
1999.
Making large-scale svm learningpractical.
In Bernhard Scho?lkopf, Christopher J. Burges, andAlexander J. Smola, editors, Advances in Kernel Methods -Support Vector Learning.
MIT Press.Joseph M. Kahn.
2004.
Bayesian Aggregation of Probabil-ity Forecasts on Categorical Events.
Ph.D. thesis, StanfordUniversity, June.Chi-Hoon Lee, Russ Greiner, and Shaojun Wang.
2006.
Usingquery-specific variance estimates to combine bayesian class-ifiers.
In ICML ?06.Andrew McCallum and Kamal Nigam.
1998.
A comparisonof event models for naive bayes text classification.
In AAAI?98, Workshops.
TR WS-98-05.Microsoft Corporation.
2001.
WinMineToolkit v1.0.
http://research.microsoft.com/?dmax/WinMine/ContactInfo.html.John C. Platt.
1999.
Probabilistic outputs for support vec-tor machines and comparisons to regularized likelihoodmethods.
In Alexander J. Smola, Peter Bartlett, Bern-hard Scholkopf, and Dale Schuurmans, editors, Advances inLarge Margin Classifiers.
MIT Press.David H. Wolpert.
1992.
Stacked generalization.
Neural Net-works, 5:241?259.Yiming Yang and Xin Liu.
1999.
A re-examination of textcategorization methods.
In SIGIR ?99.Yiming Yang.
1999.
An evaluation of statistical approaches totext categorization.
Information Retrieval, 1(1/2):67?88.331
