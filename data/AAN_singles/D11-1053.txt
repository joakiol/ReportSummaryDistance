Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 571?582,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsPersonalized Recommendation of User Comments via Factor ModelsDeepak Agarwal Bee-Chung Chen Bo PangYahoo!
Research701 First AveSunnyvale, CA 94089{dagarwal,beechun,bopang}@yahoo-inc.comAbstractIn recent years, the amount of user-generatedopinionated texts (e.g., reviews, user com-ments) continues to grow at a rapid speed: fea-tured news stories on a major event easily at-tract thousands of user comments on a popularonline News service.
How to consume subjec-tive information of this volume becomes an in-teresting and important research question.
Incontrast to previous work on review analysisthat tried to filter or summarize informationfor a generic average user, we explore a dif-ferent direction of enabling personalized rec-ommendation of such information.For each user, our task is to rank the commentsassociated with a given article according topersonalized user preference (i.e., whether theuser is likely to like or dislike the comment).To this end, we propose a factor model thatincorporates rater-comment and rater-authorinteractions simultaneously in a principledway.
Our full model significantly outperformsstrong baselines as well as related models thathave been considered in previous work.1 IntroductionRecent years have seen rapid growth in user-generated opinions online.
Many of them are userreviews: a best-seller or a popular restaurant canget over 1000 reviews on top review sites like Ama-zon or Yelp.
A large quantity of them also come inthe form of user comments on blogs or news arti-cles.
Most notably, during the short period of timefor which a major event is active, news stories onone single event can easily attract over ten thousandcomments on a popular online news site like Yahoo!News.
One question becomes immediate: how canwe help people consume such gigantic amount ofopinionated information?One possibility is to take the summarization route.Briefly speaking (see Section 2 for a more detaileddiscussion), previous work has largely formulatedreview summarization as automatically or manuallyidentify ratable aspects, and present overall senti-ment polarity for each aspect (Hu and Liu, 2004;Popescu and Etzioni, 2005; Snyder and Barzilay,2007; Titov and McDonald, 2008).
A related lineof research looked into predicting helpfulness of re-views in the hope of promoting those with betterquality, where helpfulness is usually defined as somefunction over the percentage of users who found thereview to be helpful (Kim et al, 2006; Liu et al,2007; Danescu-Niculescu-Mizil et al, 2009).
Inshort, the focus of previous work has been on dis-tilling subjective information for an average user.Whether opinion consumers are looking for qual-ity information or just wondering what other peoplethink, each may have different purposes or prefer-ences that is not well represented by a generic av-erage user.
If we think about how we deal with in-formation content overflow on the Web, there havebeen two main frameworks to identify relevant infor-mation for each person.
One is search.
Indeed manytop review sites allow users to search within reviewsfor a given entity.
But this is only useful when usershave explicit information needs that can be formu-lated as queries.
The other paradigm is recommen-dation: based on what users have liked or disliked inthe past, the system will automatically recommend571new items.Can we provide similar recommendation mech-anisms to help users consume large quantities ofsubjective information?
Many commenting environ-ments allow users to mark ?like?
or ?dislike?
overexisting comments (e.g., Yahoo!
News comments,Facebook posts, or review sites that allow helpful-ness votes).
Can we learn from users?
past prefer-ences, so that when a user is reading a new article,we have a system that automatically ranks its com-ments according to their likelihood of being liked bythe user?
This can be used directly to create person-alized presentation of comments (e.g., into a ?like?column and a ?dislike?
column), as well as enablingdown-stream applications such as personalized sum-marization.Recommending textual information has recentlyattracted more attention.
So far, the focus has beenmainly on recommending news articles (Ahn et al,2007; Das et al, 2007).
Our task differs in severalaspects.
Intuitively, recommending news articles islargely about identifying the topics of interest to agiven user, and it is conceivable that unigram repre-sentation of full-length articles can reasonably cap-ture that information.
In our case, most commentsfor an article a user is reading are already of interestto that user topically.
Which ones the user ends upliking may depend on several non-topical aspects ofthe text: whether the user agrees with the viewpointexpressed in the comment, whether the comment isconvincing and well-written, etc.
Previous work hasshown that such analysis can be more difficult thantopic-based analysis (Pang and Lee, 2008), and wehave the additional challenge that comments are typ-ically much shorter than full-length articles.
How-ever, the difficulty in analyzing the textual infor-mation in comments can be alleviated by additionalcontextual information such as author identities.
Ifbetween a pair of users one consistently likes or dis-likes the other, then at least for the heavy users, thisauthorship information alone could be highly infor-mative.
Indeed, previous work in collaborative filter-ing has usually found no additional gain from lever-aging content information when entity-level prefer-ence information is abundant.In this paper, we present a principled way of uti-lizing multiple sources of information for the task ofrecommending user comments, which significantlyoutperforms strong baseline methods, as well as pre-vious methods proposed for text recommendation.While using authorship information alone tends toprovide stronger signal than using textual informa-tion alone, to our surprise, even for heavy users,adding textual information to the authorship infor-mation yields additional improvements.2 Related WorkThere are two main bodies of related work: ourproblem formulation is closer to collaborative filter-ing, while the nature of the text we are dealing withhas more in common with opinion mining and sen-timent analysis.Our approach is related to a large body of workin collaborative filtering.
While a proper surveyis not possible here, we describe some of the ap-proaches that are germane.
Classical approaches incollaborative filtering are based on item-item/user-user similarity, these are nearest-neighbor methodswhere the response for a user-item pair is predictedbased on a local neighborhood mean (Sarwar et al,2001; Wang et al, 2006).
In general, neighbor-hoods are defined by measuring similarities betweenusers/items through correlation measures like Pear-son, cosine similarities, etc.
Better approaches toestimate similarities have also been proposed in Ko-ren (2010).
However, modern methods based onmatrix factorization have been shown to outperformnearest neighbor methods (Salakhutdinov and Mnih,2008a,b; Bell et al, 2007).
Generalizations of ma-trix factorization to include both features and pastratings have been proposed (Agarwal and Chen,2009; Stern et al, 2009).
The approach in this pa-per is an extension where in addition to interactionsamong users and items (comments in our case), wealso consider the authorship information.
Three-wayinteractions were recently studied for personalizedtag recommendation (Rendle and Lars, 2010).
Theirmodel was based on the sum of two-way interac-tions, and was trained by using pairwise tag pref-erences for each (user, item) pair.
However, no fea-tures were considered, which is an important consid-eration for us.
We show using both text and author-ship provides the best performance.Our work is also related to news personalizationthat has received increasing attention in the last few572years.
For instance, Billsus and Pazanni (2007) de-scribes an approach to build user profile models foradaptive personalization in the context of mobilecontent access.
Their approach is based on a hybridmodel that combines content-based approaches withsimilarity methods used in recommender systems.This is further exemplified in the work by Ahn et al(2007) where text processing techniques are used tobuild content profiles for users to recommend per-sonalized news.
In our experiments, we show thatsuch approaches are inferior to our method.
A con-tent agnostic approach based on collaborative filter-ing techniques was proposed by Das et al (2007);cold-start for new items/users was not their focus,but is important for our task ?
candidate commentsfor recommendation are often not in training data.As discussed in Section 1, previous work in opin-ion mining and sentiment analysis has addressed theinformation consumption challenge via review sum-marization.
Discussion of early work in that di-rection can be found in Pang and Lee (2008).
Inthis line of work, opinions for each given aspect areusually summarized as the average sentiment po-larity associated with that aspect.
Related to that,people have looked into predicting review helpful-ness given the textual information in reviews, wherehelpfulness is either defined as the percentage ofusers who have voted the review to be helpful (Kimet al, 2006), or labeled by annotators according toa set of criteria (Liu et al, 2007).
Our goal dif-fers in that we look for personalized ranking (whata specific user might like) rather than generic qual-ity (what an average user might like).
Subsequently,there has been work that tried to predict similarlydefined helpfulness scores using meta-informationover the reviewer.
For instance, whether the au-thor has used his/her true name or where the useris from (Danescu-Niculescu-Mizil et al, 2009), aswell as graph structure in the social network be-tween reviewers (Lu et al, 2010).
In this work, wesimply use author identity to provide more contextto the short text; in future work, additional meta-information over users can easily be incorporatedvia our model.As discussed in Section 1, whether a rater likes acomment or not may depend on whether they agreewith the viewpoint expressed in the text and qualityof the text.
While previous work has not looked intothe reader-comments relationship, there has been re-lated work on identifying political orientations orviewpoints (Lin and Hauptmann, 2006; Lin et al,2006; Mullen and Malouf, 2006, 2008; Laver et al,2003); whether a piece of text expresses support oropposition in congressional debates (Thomas et al,2006) or online debates (Somasundaran and Wiebe,2009, 2010); as well as identifying contrastive re-lationship (Kawahara et al, 2010).
Note that it isnot trivial to use previous work along this line to di-rectly serve as sub-components in our setting.
Forinstance, for work on identifying political orienta-tions or viewpoints, the training data consists of textwith the desired labels.
In our setting, our labelscome in the form of whether users liked or dislikeda previous comment.
In the simplest case, we mighthave pair-wise constraints on whether two pieces oftext have the same viewpoints (i.e., liked or dis-liked by the same rater), which would yield a dif-ferent learning problem akin to the metric learningproblem; note, however, the complication that twopieces of text receiving different labels from a givenuser might not necessarily contain contrasting view-points.
Consequently, rather than trying to reducethis problem to a set of known text classificationtasks, we address this task via a collaborative filter-ing framework that incorporates textual features.3 MethodIn this section, we describe our model that predictsrater affinity to comments.
A key strength of ourmodel is the ability to incorporate rater-commentand rater-author interactions simultaneously in aprincipled fashion.
Our model also provides a seam-less mechanism to transition from cold-start (whererecommendations need to be made for users or itemswith no or few past ratings) to warm-start scenarios?
with a large amount of data, it fits a per-rater (au-thor) model; with increase in data sparsity, the modelapplies a small sample size correction through fea-tures (in our case, textual features).
The exact for-mula for such corrections in the presence of sparsityis based on parameter estimates that are obtained byapplying an EM algorithm to the training data.5733.1 ModelNotation: Let yij denote the rating that user i, calledthe rater, gives to comment j.
Since throughout, weuse suffix i to denote a rater and suffix j to denote acomment, we slightly abuse notation and let xi (ofdimension pu) and xj (of dimension pc) denote fea-ture vectors of user i and comment j respectively.For example, xi can be the bag of words represen-tation (a sparse vector) inferred through text anal-ysis on comments voted positively by user i in thepast, and xj can be the bag of words representationfor comment j.
We use a(j) to denote the author ofcomment j, and use ?ij to denote the mean rating byrater i on comment j, i.e., ?ij = E(yij).
Of course itis impossible to estimate ?ij empirically since eachuser i usually rates a comment j at most once.Model specification: We work in a generalizedlinear model framework (McCullagh and Nelder,1989) that assumes ?ij (or some monotone functionh of ?ij) is an additive function of (1) the rater bias?i of user i since some users may have a tendencyof rating comments more positively or negativelythan others, (2) popularity ?j of comment j, whichcould reflect the quality of the comment in this set-ting, and (3) the author reputation ?a(j) of user a(j)since comments by a reputed author may in generalget more positive ratings.
Thus, the overall bias is?i + ?j + ?a(j).In addition to the bias, we include terms thatcapture interactions among entities (raters, authors,comments).
Indeed, capturing such interactions is anon-trivial part of our modeling procedure.
In ourapproach, we take recourse to factor models thathave been widely used in collaborative filtering ap-plications in recent times.
The basic idea is to at-tach latent factors to each rater, author and comment.These latent factors are finite dimensional Euclideanvectors that are unknown and estimated from thedata.
They provide a succinct representation of vari-ous aspects that are important to explain interactionamong entities.
In our case, we use the followingfactors ?
(a) user factor vi of dimension rv(?
1)to model rater-author affinity, (b) user factor ui andcomment factor cj of dimension ru(?
1) to modelrater-comment affinity.
Intuitively, each could repre-sent viewpoints of users or comments along differenti index for ratersj index for commentsa(j) author of comment jyij rating given by rater i to comment j?ij mean rating given by rater i to comment jxj feature vector of comment j(e.g., textual features in comment j)xi feature vector of user i(e.g., comments voted positively by user i)bias terms:?i rater bias of user i?j popularity of comment j(e.g., quality of the comment)?a(j) reputation of the author of comment jinteraction terms:vi user factor for rater-author affinityui, cj factors for rater-comment affinityTable 1: Table of Notations.dimensions.Affinity of rater i to comment j by author a(j)is captured by (1) similarity between viewpoints ofusers i and a(j), measured by v?iva(j); and (2) simi-larity between the preferences of user i and the per-spectives reflected in comment j, measured by u?icj .The overall interaction is v?iva(j) + u?icj .
Then, themean rating ?ij , or more precisely h(?ij), is mod-eled as the sum of bias and interaction terms.
Math-ematically, we assume:yij ?
N(?ij , ?2y) or Bernoulli(?ij)h(?ij) = ?i + ?j + ?a(j) + v?iva(j) + u?icj(1)For numeric ratings, we use the Gaussian distri-bution denoted by N (mean,var); for binary rat-ings, we use the Bernoulli distribution.
For Gaus-sian, h(?ij) = ?ij , and for Bernoulli, we assumeh(?ij) = log ?ij1?
?ij , which is the commonly usedlogistic transformation.Table 1 summarizes the notations for easy refer-ences.
We denote the full model specified above asvv+uc since both user-user interaction v?iva(j) anduser-comment interaction u?icj are modeled at thesame time.Latent factors: A natural approach to estimate la-tent factors in Equation 1 is through a maximumlikelihood estimation (MLE) approach.
This does574not work in our scenario since a large fraction ofentities have small sample size.
For instance, if acomment is rated only by one user and ru > 1,the model is clearly overparametrized and the MLEof the comment factor would tend to learn idiosyn-crasies in the training data.
Hence, it is imperativeto impose constraints on the factors to obtain esti-mates that generalize well on unseen data.
We workin a Bayesian framework where such constraints areimposed through prior distributions.
The crucial is-sue is the selection of appropriate priors.
In our sce-nario, we need priors that provide a good backoffestimate when interacting entities have small sam-ple sizes.
For instance, to estimate latent factors ofa user with little data, we provide a backoff estimatethat is obtained by pooling data across users withthe same user features.
We perform such a poolingthrough regression, the mathematical specification isgiven below.
?i ?
N(g?xi, ?2?
), ui ?
N(Gxi, ?2u),?j ?
N(d?xj , ?2?
), cj ?
N(Dxj , ?2c ),?a(j) ?
N(0, ?2?
), vi ?
N(0, ?2v),where gpu?1 and dpc?1 are regression weight vec-tors, and Gru?pu and Dru?pc are regression weightmatrices.
These regression weights are learnt fromdata and provide the backoff estimate.
Take the priordistribution of ui for example.
We can rewrite theprior as ui = Gxi + ?i, where ?i ?
N(0, ?2u).If user i has no rating in the training data, ui willbe predicted as the prior mean (backoff) Gxi, a lin-ear projection from the feature vector xi throughmatrix G learnt from data.
This projection can bethought of as a multivariate linear regression prob-lem with weight matrix G, one weight vector perdimension of ui.
However, if user i has many rat-ings in the training data, we will precisely estimatethe per-user residual ?i that is not captured by the re-gressionGxi.
For sample sizes in between these twoextremes, the per user residual estimate is ?shrunk?toward zero ?
amount of shrinkage depends on thesample size, past user ratings, variability in ratingson comments rated by the user, and the value of vari-ance components ?2?
s.3.2 Special Cases of Our ModelOur full model (vv+uc) includes several existingmodels explored in collaborative filtering and socialnetworks as special cases.The matrix factorization model: This model as-sumes the mean rating of user i on item j is givenby h(?ij) = ?i + ?j + u?icj , and the mean ofthe prior distributions on ?i, ?j ,ui, cj are zero, i.e.,g = d = G = D = 0.
Recent work clearly illus-trates that this method obtains better predictive accu-racy than classical collaborative filtering techniquesbased on item-item similarity (Bell et al (2007)).The uc model: This is also a matrix factorizationmodel but with priors based on regressions (i.e.,non-zero g, d,G,D).
It provides a mechanism todeal with both cold and warm-start scenarios in rec-ommender applications (Agarwal and Chen (2009)).The vv model: This model assumes h(?ij) = ?i +?a(j) + v?iva(j).
It was first proposed by Hoff (2005)to model interactions in social networks.
The modelwas fitted to small datasets (at most a few hundrednodes) and the goal was to test certain hypotheseson social behavior, out-of-sample prediction was notconsidered.The low-rank bilinear regression model: Here,h(?ij) = g?xi + d?xj + x?iG?Dxj .
This is a re-gression model purely based on features with no per-user or per-comment latent factors.
In a more gen-eral form, x?iG?Dxj can be written as x?iAxj , whereApu?pc is the matrix of regression weights (Chu andPark, 2009).
However, since xi and xj are typicallyhigh dimensional, A can be a large matrix that needsto be learnt from data.
To reduce dimensionality, onecan decompose A as A = G?D, where the numberof rows inD andG are small.
Thus, instead of learn-ingA, we learn a low-rank approximation ofA.
Thisensures scalability and provides an attractive methodto avoid over-fitting.3.3 Model FittingModel fitting for our model is based on theexpectation-maximization (EM) algorithm (Demp-ster et al, 1977).
For ease of exposition and spaceconstraints, we only provide a sketch of the algo-rithm for the Gaussian case, the logistic model canbe fitted along the same lines by using a variationalapproximation (see Agarwal and Chen (2009)).Let Y = {yij} denote the set of the observedratings.
In the EM parlance, this is ?incomplete?575data that gets augmented with the latent factors?
= {ui,vi, cj} to obtain the ?complete?
data.The goal of the EM algorithm is to find the param-eter ?
= (g, d,G,D, ?2?, ?2?, ?2u, ?2v , ?2y) that maxi-mizes the ?incomplete?
data likelihood Pr(Y |?)
=?Pr(Y ,?|?)d?
that is obtained after marginaliza-tion (taking expectation) over the distribution of ?.Since such marginalization is not available in closedform for our model, we use the EM algorithm.EM algorithm: The complete data log-likelihoodl(?
;Y ,?)
for the full model in the Gaussian case(where h(?ij) = ?ij) is given by l(?
;Y ,?)
=?
12?ij((yij ?
?ij)2/?2y + log ?2y)?
12?i((?i ?
g?xi)2/?2?
+ log ?2?)?
12?j((?j ?
d?xj)2/?2?
+ log ?2?)?
12?i(?ui ?Gxi?2/?2u + ru log ?2u)?
12?j(?cj ?Dxj?2/?2c + ru log ?2c),?
12?i(v?ivi/?2v + rv log ?2v + ?2i /?2?
+ log ?2?
),where ru is the dimension of factors ui and cj , andrv is the dimension of vi.
Let ?
(t) denote the esti-mated parameter setting at the tth iteration.
The EMalgorithm iterates through the following two stepsuntil convergence.?
E-step: Compute ft(?)
= E?[l(?
;Y ,?)
|?
(t)]as a function of ?, where the expectation is takenover the posterior distribution of (?
|?
(t),Y ).Note that here ?
is the input variable of functionft, but ?
(t) consists of known quantities (deter-mined in the previous iteration).?
M-step: Find the ?
that maximizes the expecta-tion computed in the E-step.?
(t+1) = argmax?ft(?
)Since the expectation in the E-step is not available ina closed form, we use a Gibbs sampler to computethe Monte Carlo expectation (Booth and Hobert,1999).
The Gibbs sampler repeats the followingprocedure L times.
It samples ?i, ?i, ?j , ui, vj ,and cj sequentially one at a time by sampling fromthe corresponding full conditional distributions.
Thefull conditional distributions are all Gaussian, hencethey are easy to sample.
Once a Monte Carlo ex-pectation is calculated from the samples, an updatedestimate of ?
is obtained in the M-step.
The opti-mization of variance components ?2?
s in the M-stepis available in closed form, the regression param-eters are estimated through off-the-shelf linear re-gression routines.
We note that the posterior distri-bution of latent factors for known ?
is multi-modal,we have found the Monte Carlo based EM method tooutperform other optimization methods like gradientdescent in terms of predictive accuracy.4 Experiments4.1 DataWe obtained comment rating data between Marchand May, 2010 from Yahoo!
News, with all user IDsanonymized.
On this site, users can post commentson news article pages and rate the comments madeby others through thumb-up (positive) or thumb-down (negative) votes.
Clearly, for articles with veryfew comments, there is no need to recommend com-ments.
Also, we do not expect deep personalizedrecommendations for users who have rated very fewcomments in the past.
To focus on instances of in-terest to us, we restricted ourselves to a subset of therating data associated with relatively heavy raters.In particular, we formed the experimental datasetby randomly selecting 9,003 raters who provided atleast 200 ratings (of which at least 10 were posi-tive and 10 were negative), 189,291 authors who re-ceived at least 20 ratings, and 5,088 news articlesthat received at least 40 comments in the raw datasetduring the three-month period.
Note that the per en-tity sample size in the experimental dataset can besmaller than the thresholds specified above.
For in-stance, a rater with more than 200 ratings in the rawdataset can have fewer than 200 in the experimentaldataset due to the removal of certain authors or newsarticles.
(See Figure 2 for a distribution of users withdifferent activity levels.)
In total, we have 4,440,222ratings on 1,197,098 comments.The 5,088 news articles were split into trainingarticles (the earliest 50%), tuning articles (next 5%),and test articles (the last 45%) based on their pub-lication time.
The ratings and comments were splitinto training, tuning, and test sets according to thearticle they were associated with.
All tuning param-eters are determined using the tuning set, and per-formances are reported over the test set.
Note that576this training-test split ensures that performance onthe test data best simulates our application scenar-ios.
It also creates a completely cold-start situationfor comments ?
no comment in the test set has anypast rating in the training set.4.2 Experimental SetupFeatures: All comments were tokenized, lower-cased, with stopwords and punctuations removed.We limited the vocabulary to the 10K most frequenttokens in all comments associated with the trainingarticles.
(See Section 4.3.3 for a discussion on theeffect of the vocabulary size.)
For a given commentj, xj is its bag of words representation, L2 normal-ized.
For term weighting, we experimented withboth presence value and tf-idf weighting.
The lattergives slight better performance.
Rater feature vectorxi is created by summing over the feature vectorsof all comments rated positively by rater i, which isthen L2 normalized.Methods: We compare the following methodsbased on our model: The full model vv+uc, as wellas the three main special cases, vv, uc, and bilin-ear, as defined in Section 3.
The dimensions of vi,ui and cj (i.e., rv and ru), and the rank of bilin-ear are selected to obtain the best AUC on the tun-ing set.
In our experiments, rv = 2, ru = 3 andrank of bilinear is 3.
In addition, we also evaluatethe following baseline methods that predict per-userpreferences in isolation, primarily based on textualinformation.?
Cosine similarity (cos): x?ixj .
This is simplybased on how similar a new comment j is to thecomments rater i has liked in the past.?
Per-user SVM (svm): For each rater, train a sup-port vector machine (SVM) classifier using onlycomments (xj) rated by that user.?
Per-user Naive Bayes (nb): For each rater, traina Naive Bayes classifier using only comments(xj) rated by that user.1Note that SVMs typically yield the best performanceon text classification tasks; a Naive Bayes classifier1As we mentioned in Section 4.1, not all users have trainingdata of both classes in the experimental dataset.
For svm andnb, we use the following backoff: for users with training datafrom only ci, we predict ci; for users with no training data atall, we predict the majority class, in this case, the positive class.can be more robust over shorter text spans commonin user comments given the high variance.
For faircomparisons, for the three baseline methods, we usea simple way of utilizing author information: thefeature space is augmented with author IDs and eachxj is augmented with a(j)2.
In Section 4.3, we onlyreport results using the augmented feature vectorssince they yield better performance (though the dif-ference is fairly small).Performance metrics: We use two types of met-rics to measure the performance of a method: (1)A global metric based on Receiver Operating Char-acteristic (ROC) and (2) Precision at rank k (P@k).The former measures the overall correlation of pre-dicted scores for a method with the observed rat-ings in the test set, while the latter measures theperformance of a hypothetical top-k recommenda-tion scenario using the method.
To summarize anROC curve into a single number, we use the AreaUnder the ROC Curve (AUC).
Since random guessyields AUC score of 0.5, regardless of the class dis-tribution, using this measure makes it convenient forus to compare the performance over different sub-sets of the data (where class distributions could bedifferent).
The P@k of a method is computed asfollows: (1) For each rater, rank comments that therater rated in the test set according to the scores pre-dicted by the method, and compute the precision atrank k for that rater; and then (2) average the per-rater precision numbers over all raters.
To reportP@k, for k = 5, 10, 20, we only use raters who haveat least 50 ratings in the test set.
Statistical signif-icance based on a two-sample t-test across raters isalso reported.4.3 Results4.3.1 Main comparisonsWe first show the ROC curves of different meth-ods on the test set in Figure 1, and the AUCs andprecisions in Table 2.
Results from significance testsare in Table 3.First, note that while svm significantly outper-forms random guesses and nb, it is worse than bi-linear, which is also using (mostly) textual infor-mation, but learns the model for all users together,2We assign weight 1 to a(j), so that the author informationhave the same impact as the textual features.577False positive rateTruepositiverate0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Truepositiverate0.00.20.40.60.81.0Truepositiverate0.00.20.40.60.81.0Truepositiverate0.00.20.40.60.81.0Truepositiverate0.00.20.40.60.81.0Truepositiverate0.00.20.40.60.81.0Truepositiverate0.00.20.40.60.81.0vv+ucvvucbilinearsvmnbcosrandomFigure 1: ROC curves of different modelsMethod AUC P@5 P@10 P@20vv+uc 0.8360 0.9152 0.9079 0.8942vv 0.8090 0.8810 0.8807 0.8727uc 0.7857 0.9046 0.8921 0.8694bilinear 0.7701 0.9028 0.8894 0.8668svm 0.6768 0.7814 0.7678 0.7497nb 0.6465 0.7660 0.7486 0.7309cos 0.5382 0.6834 0.6813 0.6754Table 2: AUCs and precisions of different models.rather than in isolation.
Next, uc outperforms bilin-ear (significantly in AUC, P@10 and P@20), show-ing per-user and per-comment latent factors help.Note that vv outperforms uc in ROC, AUC andP@20, but is worse than uc in P@5 and P@10; wewill take a closer look at this later.
Finally, the fullmodel vv+uc significantly outperforms both vv anduc, achieving 0.83 in AUC, and close to 90% in pre-cision at rank 20.4.3.2 Break-down by user activity levelNext, we investigate model performance in differ-ent subsets of the test set.
For succinctness, we useAUC as our performance metric.
In Figure 2(a), webreakdown model performance by different authoractivity levels.
In Figure 2(b), we breakdown modelperformance by different voter activity levels.
Wealso generated similar plots with the y-axis replacedby P@5, P@10 and P@20, and observed the sametrend except that vv starts to outperform uc at differ-ent user activity thresholds for different metrics.Comparison Metrics p-valuevv+uc > vv All < 10?7vv+uc > uc All < 10?20uc > bilinear All except P@5 < 0.006bilinear > svm All < 10?20vv > svm All < 10?20svm > nb All < 10?8nb > cos All < 10?20Table 3: Paired t-test results.
Note that uc is better thanbilinear in P@5, but not significant.
The orders of ucand vv are not consistent across different metrics.Not surprisingly, vv performs poorly for raters orauthors with no ratings observed in the training data.However, once we have a small amount of ratings, itstarts to outperform uc, even though intuitively, thetextual information in the comment should be moreinformative than the authorship information alone.Using paired t-tests with significance level 0.05, wereport when vv starts to significantly outperform ucin the following table, which is interpreted as fol-lows ?
vv is not significantly worse than uc in met-ric M if the author of a test comment received atleast Neq ratings in the training set, and vv signifi-cantly outperforms uc in metric M if the author re-ceived at least Ngt ratings in the training set.Metric M P@5 P@10 P@20 AUC# Ratings Neq 50 5 5 5Ngt 1000 50 5 5Recall that our training/test split is by article.
Sincewe have never observed a rater?s preference over thetest articles before, it is rather surprising that authorinformation alone can yield 0.8 in AUC score, evenfor very light authors who have received between 3and 5 votes in total in the training data.
This suggeststhat users?
viewpoints are quite consistent: a largeportion of the ratings can be adequately explainedby the pair of user identities.
One interesting obser-vation is that the number of ratings required for vv tooutperform uc in P@5 is quite high.
This suggeststhat to obtain high precision at the top of a recom-mended list, comment features are important.Nonetheless, modeling textual information in ad-dition to author information provides additional im-provements.
Based on paired t-tests with signifi-5780.600.650.700.750.800.850.90AUC0?01?23?56?1011?2021?5051?100101?200201?500501?2000409K168K147K143K170K231K184K131K120K82Kvv+ucvvucbilinearsvmnb(a) AUC by author activity levels0.50.60.70.8AUC0?01?56?1011?2021?5051?100101?200201?500501?200025K22K15K31K96K202K404K597K403Kvv+ucvvucbilinearsvmnb(b) AUC by rater activity levels0.700.750.800.850.90AUC50?100101?200201?500501?200021K29K41K19Kvv+ucvvucbilinearsvmnb(c) AUC by user activity levelsFigure 2: AUC of different models as a function of the activity level of authors or raters.
The x-axis (bottom) has theform m-n, meaning the subset of the test data in which the number of ratings that each author received (as in (a)) oreach rater gave (as in (b)) in the training set is between m and n. In (c), we select both authors and raters based on them-n criterion.
The x-axis (top) denotes the number of ratings in the subsetcance level 0.05, vv+uc significantly outperforms vvin all metrics if the author received < 500 ratingsin the training set.
Except for the very heavy au-thors, even for cases where both raters and authorsare heavy users (Figure 2(c)), adding the commentfeature information still yields additional improve-ment over the already impressive performance of us-ing vv alone.
In spite of the simple representation weadopted for the textual information, the full model isstill capable of accounting for part of the residualerrors from vv model (that uses authorship informa-tion alone) by using comment features ?
what wasactually written does matter.Finally, if we breakdown the comparison be-tween vv+uc and uc for different user activity lev-els, vv+uc significantly outperforms uc (with level0.05) in all metrics if the author received at least 5ratings in the training set.4.3.3 Analysis of textual featuresRecall that we limited the vocabulary size to the10K most frequent terms for efficiency reasons.
Isthis limitation likely to affect our model perfor-mance significantly?
We examined the effect of dif-ferent numbers of features.
In the following table,#features = n means that both xi and xj are bagsof n words3.
Since the vv model does not utilizerater or comment features, we examine AUC of theuc model.#features 1K 3K 5K 10KAUC 0.7713 0.7855 0.7872 0.7876As can be seen, the performance improvement is inthe 4th decimal place when we increase from 5Kfeatures to 10K features.
Thus, we do not furtherincrease the number of features in our experiments.Note that our full model does not require rater fea-tures and comment features to be in the same featurespace.
Each is projected into the hidden ?viewpoint?space, via G and D, separately.
For simplicity andeasy comparison to other methods, we used all com-ments liked by a rater in the past to build the featurevector of the rater.
But since the full model alreadyhas information of the textual content of commentsfrom the comment features, and which commentswere liked by the users from the ratings, rater fea-tures constructed this way do not provide any newinformation.
Indeed, if we model ui ?
N(1, ?2u),instead of ui ?
N(Gxi, ?2u), this omission of xidoes not hurt the performance of the model.
In fu-ture work, other meta-information about the rater3Note that we used n most useful features in each case.579can easily be incorporated into xi to enrich rater rep-resentation.Recall that comment featuresxj were projected tocomment factors cj via D. We envisioned that thecomment factors could be representing viewpoints.Does our model conform to this intuition?
Let?s con-sider the simplest case, where we restrict ui and cjto be one-dimensional vectors.
In this case, each canbe represented by scalars ui and cj .
If ui and cjare of the same sign, then the rater is likely to likethe comment.
Words assigned high positive weightsor low negative weights via D will have significantcontributions to the overall sign of cj .
Now if we ex-amine such words, will we see any meaningful dif-ferences in the underlying viewpoints of these twogroups of words?To address this question qualitatively, we manu-ally sampled words with heavy weights, focusing onpolitics-related ones (so that viewpoints are likelyto be polarized and easier to interpret).
At one ex-treme, we observe words like repukes, repugs, whichseemed to be derogatory mentions of Republica-tions, and likely to represent an anti-Republicationpoint of view.
At the other end, we observe termslike libtards, nobama, obummer.
While terms likenobama may appear to be typos at first sight, aquick search online reveals that these are at leastintentional typos expressing anti-Obama sentiments,which clearly represents an opposite underlying per-spective from terms like repukes.These examples also illustrate the importance tolearn directly from the data of interest to us.
Suchindicative words would never have appeared in moreformal writings.
While we do not have direct labelsfor perspectives, our model seems to be capturingthe underlying perspectives (as much as a unigram-based model could) by learning from user preferencelabels across different users.
This allows us to learnthe text features most relevant to our dataset, whichis particularly important given the time-sensitive andever-evolving nature of news-related comments.5 ConclusionsIn this paper, we promote personalized recommen-dation as a novel way of helping users to consumelarge quantities of subjective information.
We pro-pose using a principled way of incorporating bothrater-comment and rater-author interactions simul-taneously.
Our full model significantly outperformsstrong baseline methods, as well as previous meth-ods proposed for text recommendation.
In particu-lar, learning weights over textual features across allusers outperforms learning for each user individu-ally, which holds true even for heavy raters.
Further-more, while using authorship information alone pro-vides stronger signal than using textual informationalone, to our surprise, even for heavy users, addingtextual information yields additional improvements.It is difficult to comprehensively capture useraffinity to comments using a finite number of rat-ings observed during a certain time interval.
Newsand comments on news articles are dynamic in na-ture, novel aspects may emerge over time.
To cap-ture such dynamic behavior, comment factors haveto be allowed to evolve over time and such an evolu-tion would also necessitate the re-estimation of userfactors.
Incorporating such temporal dynamics intoour modeling framework is a challenging researchproblem and requires significant elaboration of ourcurrent approach.AcknowledgmentsWe thank the anonymous reviewers for useful sug-gestions.ReferencesD.
Agarwal and B.C.
Chen.
Regression-based latentfactor models.
In Proceedings of the 15th ACMSIGKDD international conference on Knowledgediscovery and data mining, pages 19?28.
ACM,2009.Jae-Wook Ahn, Peter Brusilovsky, Jonathan Grady,Daqing He, and Sue Y. Syn.
Open user profilesfor adaptive news systems: help or harm?
In Pro-ceedings of the 16th international conference onWorld Wide Web (WWW), 2007.Robert Bell, Yehuda Koren, and Chris Volinsky.Modeling relationships at multiple scales to im-prove accuracy of large recommender systems.
InKDD, 2007.D.
Billsus and M. Pazanni.
Adaptive news access.Springer, Berlin, 2007.J.G.
Booth and J.P Hobert.
Maximizing generalizedlinear mixed model likelihoods with an automated580monte carlo EM algorithm.
J.R.Statist.
Soc.
B,1999.Wei Chu and Seung T. Park.
Personalized recom-mendation on dynamic content using predictivebilinear models.
In Proceedings of the 18th inter-national conference on World Wide Web (WWW),2009.Cristian Danescu-Niculescu-Mizil, GueorgiKossinets, Jon Kleinberg, and Lillian Lee.How opinions are received by online communi-ties: A case study on Amazon.com helpfulnessvotes.
In Proceedings of WWW, pages 141?150,2009.Abhinandan S. Das, Mayur Datar, Ashutosh Garg,and Shyam Rajaram.
Google news personaliza-tion: scalable online collaborative filtering.
InProceedings of the 16th international conferenceon World Wide Web (WWW), 2007.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
Max-imum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Soci-ety, Series B, 39:1?38, 1977.Peter D. Hoff.
Bilinear mixed-effects models fordyadic data.
Journal of the American StatisticalAssociation, 100(469):286?295, 2005.Minqing Hu and Bing Liu.
Mining and summa-rizing customer reviews.
In Proceedings of theACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD), pages 168?177,2004.Daisuke Kawahara, Kentaro Inui, and Sadao Kuro-hashi.
Identifying contradictory and contrastiverelations between statements to outline web infor-mation on a given topic.
In Proceedings of the23rd International Conference on ComputationalLinguistics (COLING): Posters, 2010.Soo-Min Kim, Patrick Pantel, Tim Chklovski, andMarco Pennacchiotti.
Automatically assessing re-view helpfulness.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 423?430, Sydney,Australia, July 2006.
Association for Computa-tional Linguistics.Y.
Koren.
Factor in the neighbors: Scalable and ac-curate collaborative filtering.
ACM Transactionson Knowledge Discovery from Data (TKDD), 4(1):1?24, 2010.
ISSN 1556-4681.Michael Laver, Kenneth Benoit, and John Garry.
Ex-tracting policy positions from political texts usingwords as data.
American Political Science Review,97(2):311?331, 2003.Wei-Hao Lin and Alexander Hauptmann.
Are thesedocuments written from different perspectives?
Atest of different perspectives based on sta tisti-cal distribution divergence.
In Proceedings of theInternational Conference on Computational Lin-guistics (COLING)/Proceedings of the Associa-tion for Computational Linguistics (ACL), pages1057?1064, Sydney, Australia, July 2006.
Asso-ciation for Computational Linguistics.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
Which side are you on?identifying perspectives at the document and sen-tence levels.
In Proceedings of the Conference onNatural Language Learning (CoNLL), 2006.Jingjing Liu, Yunbo Cao, Chin-Yew Lin, YalouHuang, and Ming Zhou.
Low-quality productreview detection in opinion summarization.
InProceedings of the Joint Conference on Empir-ical Methods in Natural Language Processingand Computational Natural Language Learning(EMNLP-CoNLL), pages 334?342, 2007.
Posterpaper.Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas,and Livia Polanyi.
Exploiting social context forreview quality prediction.
In Proceedings of the19th International World Wide Web Conference(WWW), 2010.P.
McCullagh and J.
A. Nelder.
Generalized LinearModels.
Chapman & Hall/CRC, 1989.Tony Mullen and Robert Malouf.
A preliminary in-vestigation into sentiment analysis of informal po-litical discourse.
In AAAI Symposium on Compu-tational Approaches to Analysing Weblogs (AAAI-CAAW), pages 159?162, 2006.Tony Mullen and Robert Malouf.
Taking sides:User classification for informal online politicaldiscourse.
Internet Research, 18:177?190, 2008.Bo Pang and Lillian Lee.
Opinion mining and sen-timent analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135, 2008.581Ana-Maria Popescu and Oren Etzioni.
Extractingproduct features and opinions from reviews.
InProceedings of the Human Language Technol-ogy Conference and the Conference on Empir-ical Methods in Natural Language Pro cessing(HLT/EMNLP), 2005.Steffen Rendle and Schmidt-Thie Lars.
Pairwiseinteraction tensor factorization for personalizedtag recommendation.
In Proceedings of the thirdACM international conference on Web search anddata mining, WSDM ?10, pages 81?90, NewYork, NY, USA, 2010.
ACM.R.
Salakhutdinov and A. Mnih.
Bayesian proba-bilistic matrix factorization using Markov chainMonte Carlo.
In Proceedings of the 25th inter-national conference on Machine learning, pages880?887.
ACM, 2008a.R.
Salakhutdinov and A. Mnih.
Probabilistic ma-trix factorization.
Advances in neural informationprocessing systems, 20:1257?1264, 2008b.Badrul Sarwar, George Karypis, Joseph Konstan,and John Reidl.
Item-based collaborative filter-ing recommendation algorithms.
In WWW ?01:Proceedings of the 10th international conferenceon World Wide Web, pages 285?295.
ACM, 2001.Benjamin Snyder and Regina Barzilay.
Multiple as-pect ranking using the Good Grief algorithm.
InProceedings of the Joint Human Language Tech-nology/North American Chapter of the ACL Con-ference (HLT- NAACL), pages 300?307, 2007.Swapna Somasundaran and Janyce Wiebe.
Recog-nizing stances in online debates.
In Proceedingsof the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International JointConference on Natural Language Processing ofthe AFNLP, 2009.Swapna Somasundaran and Janyce Wiebe.
Recog-nizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshopon Computational Approaches to Analysis andGeneration of Emotion in Text, 2010.David H. Stern, Ralf Herbrich, and Thore Grae-pel.
Matchbox: large scale online bayesian rec-ommendations.
In Proceedings of the 18th inter-national conference on World Wide Web (WWW),2009.Matt Thomas, Bo Pang, and Lillian Lee.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages327?335, 2006.Ivan Titov and Ryan McDonald.
A joint model oftext and aspect ratings for sentiment summariza-tion.
In Proceedings of the Association for Com-putational Linguistics (ACL), 2008.Jun Wang, Arjen P. de Vries, and Marcel J. T. Rein-ders.
Unifying user-based and item-based collab-orative filtering approaches by similarity fusion.In SIGIR ?06: Proceedings of the 29th annual in-ternational ACM SIGIR conference on Researchand development in information retrieval, pages501?508, New York, NY, USA, 2006.
ACM.582
