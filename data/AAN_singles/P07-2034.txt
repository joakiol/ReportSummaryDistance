Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133?136,Prague, June 2007. c?2007 Association for Computational LinguisticsBuilding Emotion Lexicon from Weblog CorporaChanghua Yang        Kevin Hsin-Yih Lin        Hsin-Hsi ChenDepartment of Computer Science and Information EngineeringNational Taiwan University#1 Roosevelt Rd.
Sec.
4, Taipei, Taiwan 106{d91013, f93141, hhchen}@csie.ntu.edu.twAbstractAn emotion lexicon is an indispensable re-source for emotion analysis.
This paperaims to mine the relationships betweenwords and emotions using weblog corpora.A collocation model is proposed to learnemotion lexicons from weblog articles.Emotion classification at sentence level isexperimented by using the mined lexiconsto demonstrate their usefulness.1 IntroductionWeblog (blog) is one of the most widely used cy-bermedia in our internet lives that captures andshares moments of our day-to-day experiences,anytime and anywhere.
Blogs are web sites thattimestamp posts from an individual or a group ofpeople, called bloggers.
Bloggers may not followformal writing styles to express emotional states.In some cases, they must post in pure text, so theyadd printable characters, such as ?:-)?
(happy) and?:-(?
(sad), to express their feelings.
In other cases,they type sentences with an internet messenger-style interface, where they can attach a special setof graphic icons, or emoticons.
Different kinds ofemoticons are introduced into text expressions toconvey bloggers?
emotions.Since thousands of blog articles are created eve-ryday, emotional expressions can be collected toform a large-scale corpus which guides us to buildvocabularies that are more emotionally expressive.Our approach can create an emotion lexicon free oflaborious efforts of the experts who must be famil-iar with both linguistic and psychological knowl-edge.2 Related WorksSome previous works considered emoticons fromweblogs as categories for text classification.Mishne (2005), and Yang and Chen (2006) usedemoticons as tags to train SVM (Cortes and Vap-nik, 1995) classifiers at document or sentence level.In their studies, emoticons were taken as moods oremotion tags, and textual keywords were taken asfeatures.
Wu et al (2006) proposed a sentence-level emotion recognition method using dialogs astheir corpus.
?Happy, ?Unhappy?, or ?Neutral?was assigned to each sentence as its emotion cate-gory.
Yang et al (2006) adopted Thayer?s model(1989) to classify music emotions.
Each musicsegment can be classified into four classes ofmoods.
In sentiment analysis research, Read (2005)used emoticons in newsgroup articles to extractinstances relevant for training polarity classifiers.3 Training and Testing Blog CorporaWe select Yahoo!
Kimo Blog1 posts as our sourceof emotional expressions.
Yahoo!
Kimo Blogservice has 40 emoticons which are shown in Table1.
When an editing article, a blogger can insert anemoticon by either choosing it or typing in thecorresponding codes.
However, not all articlescontain emoticons.
That is, users can decidewhether to insert emoticons into articles/sentencesor not.
In this paper, we treat these icons asemotion categories and taggings on thecorresponding text expressions.The dataset we adopt consists of 5,422,420 blogarticles published at Yahoo!
Kimo Blog fromJanuary to July, 2006, spanning a period of 212days.
In total, 336,161 bloggers?
articles were col-lected.
Each blogger posts 16 articles on average.We used the articles from January to June as thetraining set and the articles in July as the testing set.Table 2 shows the statistics of each set.
On aver-age, 14.10% of the articles contain emotion-taggedexpressions.
The average length of articles withtagged emotions, i.e., 272.58 characters, is shorter1http://tw.blog.yahoo.com/133than that of articles without tagging, i.e., 465.37characters.
It seems that people tend to use emoti-cons to replace certain amount of text expressionsto make their articles more succinct.Figure 1 shows the three phases for the con-struction and evaluation of emotion lexicons.
Inphase 1, 1,185,131 sentences containing only oneemoticon are extracted to form a training set tobuild emotion lexicons.
In phase 2, sentence-levelemotion classifiers are constructed using the minedlexicons.
In phase 3, a testing set consisting of307,751 sentences is used to evaluate the classifi-ers.4 Emotion Lexicon ConstructionThe blog corpus contains a collection of bloggers?emotional expressions which can be analyzed toconstruct an emotion lexicon consisting of wordsthat collocate with emoticons.
We adopt a variationof pointwise mutual information (Manning andSch?tze, 1999) to measure the collocation strengthco(e,w) between an emotion e and a word w:)()(),(log),(),o(wPePwePwecwec ?=  (1)where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N,c(e)and c(w) are the total occurrences of emoticone and word w in a tagged corpus, respectively,c(e,w) is total co-occurrences of e and w, and Ndenotes the total word occurrences.A word entry of a lexicon may contain severalemotion senses.
They are ordered by the colloca-tion strength co.
Figure 2 shows two Chinese ex-ample words, ???
?
(ha1ha1) and ???
?(ke3wu4).
The former collocates with ?laughing?and ?big grin?
emoticons with collocation strength25154.50 and 2667.11, respectively.
Similarly, thelatter collocates with ?angry?
and ?phbbbbt?.When all collocations (i.e., word-emotion pairs)are listed in a descending order of co, we canchoose top n collocations to build an emotion lexi-con.
In this paper, two lexicons (Lexicons A and B)are extracted by setting n to 25k and 50k.
LexiconA contains 4,776 entries with 25,000 sense pairsand Lexicon B contains 11,243 entries and 50,000sense pairs.5 Emotion ClassificationSuppose a sentence S to be classified consists of nemotion words.
The emotion of S is derived by amapping from a set of n emotion words to m emo-tion categories as follows:},...,{?
},...,{ 11 mtionclassificaneeeewewS ??
?Table 1.
Yahoo!
Kimo Blog Emoticon Set.ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description1:) happy 11:O surprise 210:) angel 31(:| yawn2:( sad 12X-( angry 22:-B nerd 32=P~ drooling3;) winking 13:> smug 23=;talk tothe hand 33  :-?
thinking4:D big grin 14B-) cool 24I-) asleep 34;)) hee hee5;;) batting  eyelashes 15  :-S worried 25  8-) rolling eyes 35  =D> applause6:-/ confused 16>:) devil 26:-& sick 36[-o< praying7:x love struck 17:(( crying 27:-$ don't tell  anyone 37  :-< sigh8:?> blushing 18:)) laughing 28[-( not talking 38>:P phbbbbt9:p tongue 19:| straight face 29:o) clown 39@};- rose10:* kiss 20/:) raised  eyebrow 30  @-) hypnotized 40  :@) pigTable 2.
Statistics of the Weblog Dataset.Dataset Article # Tagged # Percentage Tagged Len.
Untagged L.Training 4,187,737 575,009 13.86% 269.77 chrs.
468.14 chrs.Testing 1,234,683 182,999 14.92% 281.42 chrs.
455.82 chrs.Total 5,422,420 764,788 14.10% 272.58 chrs.
465.37 chrs.Testing SetFigure 1.
Emotion Lexicon Construction and Evaluation.ExtractionBlogArticlesFeaturesClassifiersEvaluationLexiconConstructionTraining SetPhase 2Phase 3EmotionLexiconPhase 1134For each emotion word ewi, we may find severalemotion senses with the corresponding collocationstrength co by looking up the lexicon.
Three alter-natives are proposed as follows to label a sentenceS with an emotion:(a) Method 1(1)  Consider all senses of ewi as votes.
Label Swith the emotion that receives the most votes.
(2)  If more than two emotions get the same num-ber of votes, then label S with the emotion thathas the maximum co.(b) Method 2Collect emotion senses from all ewi.
Label Swith the emotion that has the maximum co.(c) Method 3The same as Method 1 except that each ewi v-otes only one sense that has the maximum co.In past research, the approach used by Yang etal.
(2006) was based on the Thayer?s model (1989),which divided emotions into 4 categories.
In sen-timent analysis research, such as Read?s study(2006), a polarity classifier separated instances intopositive and negative classes.
In our experiments,we not only adopt fine-grain classification, but alsocoarse-grain classification.
We first select 40emoticons as a category set, and also adopt theThayer?s model to divide the emoticons into 4quadrants of the emotion space.
As shown in Fig-ure 3, the top-right side collects the emotions thatare more positive and energetic and the bottom-leftside is more negative and silent.
A polarity classi-fier uses the right side as positive and the left sideas negative.6 EvaluationTable 3 shows the performance under variouscombinations of lexicons, emotion categories andclassification methods.
?Hit #?
stands for thenumber of correctly-answered instances.
The base-line represents the precision of predicting the ma-jority category, such as ?happy?
or ?positive?, asthe answer.
The baseline method?s precision in-creases as the number of emotion classes decreases.The upper bound recall indicates the upper limit onthe fraction of the 307,751 instances solvable bythe corresponding method and thus reflects thelimitation of the method.
The closer a method?sactual recall is to the upper bound recall, the betterthe method.
For example, at most 40,855 instances(14.90%) can be answered using Method 1 incombination with Lexicon A.
But the actual recallis 4.55% only, meaning that Method 1?s recall ismore than 10% behind its upper bound.
Methodswhich have a larger set of candidate answers havehigher upper bound recalls, because the probabilitythat the correct answer is in their set of candidateanswers is greater.Experiment results show that all methods utiliz-ing Lexicon A have performance figures lowerthan the baseline, so Lexicon A is not useful.
Incontrast, Lexicon B, which provides a larger col-lection of vocabularies and emotion senses, outper-forms Lexicon A and the baseline.
AlthoughMethod 3 has the smallest candidate answer setand thus has the smallest upper bound recall, itoutperforms the other two methods in most cases.Method 2 achieves better precisions when using??
(ha1ha1) ?hah hah?Sense 1.
(laughing) ?
co: 25154.50e.g., ??...
??????
?~?hah hah?
I am getting lucky~?Sense 2.
(big grin) ?
co: 2667.11e.g., ??????????~??
?I only memorized vowels today~ haha ???
(ke3wu4) ?darn?Sense 1.
(angry) ?
co: 2797.82e.g., ??????...??
?What's the hacker doing... darn it ?Sense 2.
(phbbbbt) ?
co: 619.24e.g., ???????
?Damn those aliens ?Figure 2.
Some Example Words in a Lexicon.Arousal (energetic)Valence(negative)                                            (positive)(silent)unassigned:Figure 3.
Emoticons on Thayer?s model.135Thayer?s emotion categories.
Method 1 treats thevote to every sense equally.
Hence, it loses somedifferentiation abilities.
Method 1 performs thebest in the first case (Lexicon A, 40 classes).We can also apply machine learning to the data-set to train a high-precision classification model.To experiment with this idea, we adopt LIBSVM(Fan et al, 2005) as the SVM kernel to deal withthe binary polarity classification problem.
TheSVM classifier chooses top k (k = 25, 50, 75, and100) emotion words as features.
Since the SVMclassifier uses a small feature set, there are testinginstances which do not contain any features seenpreviously by the SVM classifier.
To deal withthis problem, we use the class prediction fromMethod 3 for any testing instances without anyfeatures that the SVM classifier can recognize.
InTable 4, the SVM classifier employing 25 featureshas the highest precision.
On the other hand, theSVM classifier employing 50 features has thehighest F measure when used in conjunction withMethod 3.7 Conclusion and Future WorkOur methods for building an emotional lexiconutilize emoticons from blog articles collaborativelycontributed by bloggers.
Since thousands of blogarticles are created everyday, we expect the set ofemotional expressions to keep expanding.
In theexperiments, the method of employing each emo-tion word to vote only one emotion categoryachieves the best performance in both fine-grainand coarse-grain classification.AcknowledgmentResearch of this paper was partially supported byExcellent Research Projects of National TaiwanUniversity, under the contract of 95R0062-AE00-02.
We thank Yahoo!
Taiwan Inc. for providingthe dataset for researches.ReferencesCorinna Cortes and V. Vapnik.
1995.
Support-VectorNetwork.
Machine Learning, 20:273?297.Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin.
2005.Working Set Selection Using Second Order Informa-tion for Training Support Vector Machines.
Journalof Machine Learning Research, 6:1889?1918.Gilad Mishne.
2005.
Experiments with Mood Classifi-cation in Blog Posts.
Proceedings of 1st Workshop onStylistic Analysis of Text for Information Access.Jonathon Read.
2005.
Using Emotions to Reduce De-pendency in Machine Learning Techniques for Sen-timent Classification.
Proceedings of the ACL Stu-dent Research Workshop, 43-48.Robert E. Thayer.
1989.
The Biopsychology of Moodand Arousal, Oxford University Press.Changhua Yang and Hsin-Hsi Chen.
2006.
A Study ofEmotion Classification Using Blog Articles.
Pro-ceedings of Conference on Computational Linguisticsand Speech Processing, 253-269.Yi-Hsuan Yang, Chia-Chu Liu, and Homer H. Chen.2006.
Music Emotion Classification: A Fuzzy Ap-proach.
Proceedings of ACM Multimedia, 81-84.Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin.2006.
Emotion Recognition from Text Using Seman-tic Labels and Separable Mixture Models.
ACMTransactions on Asian Language Information Proc-essing, 5(2):165-182.Table 3.
Evaluation Results.Method 1 (M1) Method 2 (M2) Method 3 (M3)Baseline Upp.
R. Hit # Prec.
Reca.
Upp.
R. Hit # Prec.
Reca.
Upp.
R. Hit # Prec.
Reca.Lexicon A40 classes 8.04% 14.90% 14,009 4.86% 4.55% 14.90% 9,392 3.26% 3.05% 6.49% 13,929 4.83% 4.52%Lexicon AThayer 38.38% 48.70% 90,332 32.46% 29.35% 48.70% 64,689 23.25% 21.02% 35.94% 93,285 33.53% 30.31%Lexicon APolarity 63.49% 60.74% 150,946 54.25% 49.05% 60.74% 120,237 43.21% 39.07% 54.97% 153,292 55.09% 49.81%Lexicon B40 classes 8.04% 73.18% 45,075 15.65% 14.65% 73.18% 43,637 15.15% 14.18% 27.89% 45,604 15.83% 14.81%Lexicon BThayer 38.38% 89.11% 104,094 37.40% 33.82% 89.11% 118,392 42.55% 38.47% 63.74% 110,904 39.86% 36.04%Lexicon BPolarity 63.49% 91.12% 192,653 69.24% 62.60% 91.12% 188,434 67.72% 61.23% 81.92% 195,190 70.15% 63.42%Upp.
R. ?
upper bound recall; Prec.
?
precision; Reca.
?
recallTable 4.
SVM  Performance.Method Upp.
R. Hit # Prec.
Reca.
FLexicon B M3 81.92% 195,190 70.15% 63.42% 66.62%SVM 25 features 15.80% 38,651 79.49% 12.56% 21.69%SVM 50 features 26.27% 62,999 77.93% 20.47% 32.42%SVM 75 features 36.74% 84,638 74.86% 27.50% 40.23%SVM 100 features 45.49% 101,934 72.81% 33.12% 45.53%(Svm-25 + M3) 90.41% 196,147 70.05% 63.73% 66.74%(Svm-50 + M3) 90.41% 195,835 70.37% 63.64% 66.83%(Svm-75 + M3) 90.41% 195,229 70.16% 63.44% 66.63%(Svm-100 + M3) 90.41% 195,054 70.01% 63.38% 66.53%F = 2?
(Precision?Recall)/(Precision+Recall)136
