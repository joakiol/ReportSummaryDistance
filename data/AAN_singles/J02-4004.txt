c?
2002 Association for Computational LinguisticsEfficiently Computed Lexical Chains as anIntermediate Representation forAutomatic Text SummarizationH.
Gregory Silber?
Kathleen F. McCoy?University of Delaware University of DelawareWhile automatic text summarization is an area that has received a great deal of attention in recentresearch, the problem of efficiency in this task has not been frequently addressed.
When the sizeand quantity of documents available on the Internet and from other sources are considered, theneed for a highly efficient tool that produces usable summaries is clear.
We present a linear-timealgorithm for lexical chain computation.
The algorithm makes lexical chains a computationallyfeasible candidate as an intermediate representation for automatic text summarization.
A methodfor evaluating lexical chains as an intermediate step in summarization is also presented and carriedout.
Such an evaluation was heretofore not possible because of the computational complexity ofprevious lexical chains algorithms.1.
IntroductionThe overall motivation for the research presented in this article is the development ofa computationally efficient system to create summaries automatically.
Summarizationhas been viewed as a two-step process.
The first step is the extraction of importantconcepts from the source text by building an intermediate representation of some sort.The second step uses this intermediate representation to generate a summary (SparckJones 1993).In the research presented here, we concentrate on the first step of the summariza-tion process and follow Barzilay and Elhadad (1997) in employing lexical chains toextract important concepts from a document.
We present a linear-time algorithm forlexical chain computation and offer an evaluation that indicates that such chains area promising avenue of study as an intermediate representation in the summarizationprocess.Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step inthe text summarization process.
Attempts to determine the benefit of this proposalhave been faced with a number of difficulties.
First, previous methods for computinglexical chains have either been manual (Morris and Hirst 1991) or automated, butwith exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997).Because of this, computing lexical chains for documents of any reasonable size hasbeen impossible.
We present here an algorithm for computing lexical chains that islinear in space and time.
This algorithm makes the computation of lexical chainscomputationally feasible even for large documents.?
Department of Computer and Information Sciences, Newark, DE 19711.
E-mail: silber@udel.edu?
Department of Computer and Information Sciences, Newark, DE 19711.
E-mail: mccoy@mail.eecis.udel.edu488Computational Linguistics Volume 28, Number 4A second difficulty faced in evaluating Barzilay and Elhadad?s proposal is that itis a proposal for the first stage of the summarization process, and it is not clear howto evaluate this stage independent of the second stage of summarization.
A secondcontribution of this article is a method for evaluating lexical chains as an intermedi-ate representation.
The intuition behind the method is as follows.
The (strong) lexicalchains in a document are intended to identify important (noun) concepts in the docu-ment.
Our evaluation requires access to documents that have corresponding human-generated summaries.
We run our lexical chain algorithm both on the document andon the summary and examine (1) how many of the concepts from strong lexical chainsin the document also occur in the summary and (2) how many of the (noun) conceptsappearing in the summary are represented in strong lexical chains in the document.Essentially, if lexical chains are a good intermediate representation for text sum-marization, we expect that concepts identified as important according to the lexicalchains will be the concepts found in the summary.
Our evaluation of 24 documentswith summaries indicates that indeed lexical chains do appear to be a promising av-enue of future research in text summarization.1.1 Description of Lexical ChainsThe concept of lexical chains was first introduced by Morris and Hirst.
Basically, lex-ical chains exploit the cohesion among an arbitrary number of related words (Morrisand Hirst 1991).
Lexical chains can be computed in a source document by grouping(chaining) sets of words that are semantically related (i.e., have a sense flow).
Iden-tities, synonyms, and hypernyms/hyponyms (which together define a tree of ?is a?relations between words) are the relations among words that might cause them to begrouped into the same lexical chain.
Specifically, words may be grouped when:?
Two noun instances are identical, and are used in the same sense.
(The house on the hill is large.
The house is made of wood.)?
Two noun instances are used in the same sense (i.e., are synonyms).
(The car is fast.
My automobile is faster.)?
The senses of two noun instances have a hypernym/hyponym relationbetween them.
(John owns a car.
It is a Toyota.)?
The senses of two noun instances are siblings in the hypernym/hyponyntree.
(The truck is fast.
The car is faster.
)In computing lexical chains, the noun instances must be grouped according to theabove relations, but each noun instance must belong to exactly one lexical chain.
Thereare several difficulties in determining which lexical chain a particular word instanceshould join.
For instance, a particular noun instance may correspond to several differ-ent word senses, and thus the system must determine which sense to use (e.g., shoulda particular instance of ?house?
be interpreted as sense 1, dwelling, or sense 2, legisla-ture).
In addition, even if the word sense of an instance can be determined, it may bepossible to group that instance into several different lexical chains because it may berelated to words in different chains.
For example, the word?s sense may be identicalto that of a word instance in one grouping while having a hypernym/hyponym rela-tionship with that of a word instance in another.
What must happen is that the wordsmust be grouped in such a way that the overall grouping is optimal in that it createsthe longest/strongest lexical chains.
It is our contention that words are grouped intoa single chain when they are ?about?
the same underlying concept.489Silber and McCoy Efficient Lexical Chains for Summarization2.
Algorithm DefinitionWe wish to extract lexical chains from a source document using the complete methodthat Barzilay and Elhadad implemented in exponential time, but to do so in lineartime.
Barzilay and Elhadad define an interpretation as a mapping of noun instancesto specific senses, and further, of these senses to specific lexical chains.
Each uniquemapping is a particular ?way of interpreting?
the document, and the collection ofall possible mappings defines all of the interpretations possible.
In order to computelexical chains in linear time, instead of computing every interpretation of a sourcedocument as Barzilay and Elhadad did, we create a structure that implicitly storesevery interpretation without actually creating it, thus keeping both the space and timeusage of the program linear.
We then provide a method for finding that interpretationwhich is best from within this representation.
As was the case with Barzilay andElhadad, we rely on WordNet1 to provide sense possibilities for, and semantic relationsamong, word instances in the document.Before we could actually compute the interpretations, one issue we had to tacklewas the organization and speed of the WordNet dictionary.
In order to provide expe-dient access to WordNet, we recompiled the noun database into a binary format andmemory-mapped it so that it could be accessed as a large array, changing the WordNetsense numbers to match the array indexes.2.1 Chain ComputationBefore computation can begin, the system uses a part-of-speech tagger2 to find thenoun instances within the document.
Processing a document involves creating a largearray of ?metachains,?
the size of which is the number of noun senses in WordNetplus the number of nouns in the document, to handle the possibility of words notfound in WordNet.
(This is the maximum size that could possibly be needed.)
Ametachain represents all possible chains that can contain the sense whose number isthe index of the array.
When a noun is encountered, for every sense of the noun inWordNet, the noun sense is placed into every metachain for which it has an iden-tity, synonym, or hyperonym relation with that sense.
These metachains representevery possible interpretation of the text.
Note that each metachain has an index thatis a WordNet sense number, so in a very real way, we can say that a chain has an?overriding sense.?
Table 1 shows the structure of such an array, with each row beinga metachain based on the sense listed in the first column.
In each node of a givenmetachain, appropriate pointers are kept to allow fast lookup.
In addition, in associa-tion with each word, a list of the metachains to which it belongs is also kept (not shownin table).The second step, finding the best interpretation, is accomplished by making asecond pass through the source document.
For each noun, each chain to which thenoun belongs is examined and a determination is made, based on the type of relationand distance factors, as to which metachain the noun contributes to most.
In theevent of a tie, the higher sense number is used, since WordNet is organized with morespecific concepts indexed with higher numbers.
The noun is then deleted from all othermetachains.
Once all of the nouns have been processed, what is left is the interpretationwhose score is maximum.
From this interpretation, the best (highest-scoring) chainscan be selected.
The algorithm in its entirety is outlined in Table 2.1 WordNet is available at ?http://www.cogsci.princeton.edu/?wn?.2 The part-of-speech tagger we used is available from ?http://www.rt66.com/gcooke/SemanTag?.490Computational Linguistics Volume 28, Number 4Table 1Example of metachains.Index Meaning Chain0 person John Machine1 unit Computer IBM2 device Computer Machine IBM3 organization Machine IBM4 unknown IBM...NNote: Assume the sentences ?John has a computer.The machine is an IBM.?
and that the nouns have thefollowing senses (meanings): John (0), computer (1,2),machine (0,2,3), IBM (1,2,3,4), and that words are putin a chain if they have an identity relation.
This tablethen depicts the metachains after the first step.Table 2Basic linear-time lexical chains algorithm.Step 1 For each noun instanceFor each sense of the noun instanceCompute all scored metachainsStep 2 For each noun instanceFor each metachain to which the noun belongsKeep word instance in the metachain to which it contributes mostUpdate the scores of each other metachain2.2 Scoring SystemOur scoring system allows different types of relations within a lexical chain to con-tribute to that chain differently.
Further, our scoring system allows the distance be-tween word instances in the original document to affect the word?s contribution toa chain.
Table 3 shows the scoring values used by our algorithm.
These values wereobtained through empirical testing, and although not optimal, appear to give goodresults.Each noun instance is included in a chain because either it is the first noun instanceto be inserted or it is related to some word that is already in the chain.
If it is the firstword, then the ?identical word?
relation score is used.
If not, then the type of relation isdetermined, and the closest noun in the chain to which it is related is found.
Using thedistance between these two words and the relation type, we look up the contributionof the word instance to the overall chain score.Once chains are computed, some of the high-scoring ones must be picked asrepresenting the important concepts from the original document.
To select these, weuse the idea of ?strong chains?
introduced by Barzilay and Elhadad (1997).
They definea strong chain as any chain whose score is more than two standard deviations abovethe mean of the scores computed for every chain in the document.491Silber and McCoy Efficient Lexical Chains for SummarizationTable 3Scoring system tuned by empirical methods.One Sentence Three Sentences Same Paragraph DefaultIdentical word 1 1 1 1Synonym 1 1 1 1Hypernym 1 .5 .5 .5Sibling 1 .3 .2 0Table 4Constants from WordNet 1.6.Value Worst Case Average CaseC1 = # of senses for a given word 30 2C2 = parent/child ?is a?
relations of a word sense 45,147 14C3 = # of nouns in WordNet 94,474 94,474C4 = # of synsets in WordNet 66,025 66,025C5 = # of siblings of a word sense 397 39C6 = # of chains to which a word instance can belong 45,474 553.
Linear Runtime ProofIn this analysis, we will not consider the computational complexity of part-of-speechtagging, since it is quite fast.
The runtime of the full algorithm will be O(pos tagging)+ O(our algorithm).
Also, as it does not change from execution to execution of thealgorithm, we shall take the size and structure of WordNet to be constant.
We willexamine each phase of our algorithm to show that the extraction of these lexical chainscan indeed be performed in linear time.
Table 4 defines constants for this analysis.3.1 Collection of WordNet InformationFor each noun in the source document that appears in WordNet, each sense that theword can take must be examined.
Additionally, for each sense, we must walk up anddown the hypernym/hyponym graph collecting all parent and child information.
Itis important to note that we are interested not only in direct parents and children,but in all parents and children in the graph from most specific to most general.
Lastlywe must collect all of the senses in WordNet that are siblings (i.e., share immediateparents) with the word being processed.
All of the complexity in this step is relatedto the size of WordNet, which is constant.
Lookups in WordNet use a binary search;hence a search in WordNet is O(log(C3)).
The runtime is given byn ?
(log2(C3) + C1 ?
C2 + C1 ?
C5).3.2 Building the GraphThe graph of all possible interpretations is nothing more than an array of sense values(66, 025 + n in size) that we will call the sense array.
For each word, we examine eachrelation computed as above from WordNet.
For each of these relations, we modifythe list that is indexed in the sense array by the sense number of the noun?s senseinvolved in the relation.
This list is then modified by adding the word to the list andupdating the list?s associated score.
Additionally, we add the chain?s pointer (storedin the array) to a list of such pointers in the word object.
Lastly, we add the value of492Computational Linguistics Volume 28, Number 4how this word affects the score of the chain based on the scoring system to an arraystored within the word structure.
The runtime for this phase of the algorithm isn ?
C6 ?
4,which is also clearly O(n).3.3 Extracting the Best InterpretationFor each word in the source document, we look at each chain to which the word canbelong.
A list of pointers to these chains is stored within the word object, so lookingthem up takes O(1) time.
For each of these, we simply look at the maximal scorecomponent value in all of these chains.
We then set the scores of all of the nodes thatdid not contain the maximum to zero and update all the chain scores appropriately.The operation takesn ?
C6 ?
4,which is also O(n).3.4 Overall Runtime PerformanceThe overall runtime performance of this algorithm is given by the sum of the stepslisted above, for an overall runtime ofn ?
(1, 548, 216 + log2(94, 474) + 45, 474 ?
4) [worst case]n ?
(326 + log2(94, 474) + 55 ?
4) [average case].Initially, we may be greatly concerned with the size of these constants; upon furtheranalysis, however, we see that most synsets have very few parent-child relations.Thus the worst-case values may not reflect the actual performance of our application.In addition, the synsets with many parent-child relations tend to represent extremelygeneral concepts such as ?thing?
and ?object.?
These synsets will most likely notappear very often in a document.Whereas in the worst case these constants are quite large, in the average casethey are reasonable.
This algorithm is O(n) in the number of nouns within the sourcedocument.
Considering the size of most documents, the linear nature of this algorithmmakes it usable for generalized summarization of large documents (Silber and McCoy2000).
For example, in a test, our algorithm calculated a lexical chain interpretation of a40,000-word document in 11 seconds on a Sparc Ultra 10 Creator.
It was impossible tocompute lexical chains for such a document under previous implementations becauseof computational complexity.
Thus documents tested by Barzilay and Elhadad weresignificantly smaller in size.
Our method affords a considerable speedup for thesesmaller documents.
For instance, a document that takes 300 seconds using Barzilayand Elhadad?s method takes only 4 seconds using ours (Silber and McCoy 2000).4.
Evaluation DesignOur algorithm now makes it feasible to use lexical chains as the method for identifyingimportant concepts in a document, and thus they may now form the basis of anintermediate representation for summary generation, as proposed by Barzilay andElhadad.
An important consequence of this is that Barzilay and Elhadad?s proposalcan now be evaluated on documents of substantial size.
We propose an evaluation ofthis intermediate stage that is independent of the generation phase of summarization.493Silber and McCoy Efficient Lexical Chains for SummarizationThis said, we make no attempt to claim that a summary can actually be generatedfrom this representation; we do attempt, however, to show that the concepts found ina human-generated summary are indeed the concepts identified by our lexical chainsalgorithm.The basis of our evaluation is the premise that if lexical chains are a good in-termediate representation for summary generation, then we would expect that eachnoun in a given summary should be used in the same sense as some word instancegrouped into a strong chain in the original document on which the summary is based.Moreover, we would expect that all (most) strong chains in the document should berepresented in the summary.For this analysis, a corpus of documents with their human-generated summariesare required.
Although there are many examples of document and summary types,for the purposes of this experiment, we focus on two general categories of summariesthat are readily available.
The first, scientific documents with abstracts, represents areadily available class of summaries often discussed in the literature (Marcu 1999).The second class of document selected was chapters from university level textbooksthat contain chapter summaries.
To prevent bias, textbooks from several fields werechosen.In this analysis, we use the term concept to denote a noun in a particular sense (agiven sense number in the WordNet database).
It is important to note that differentnouns with the same sense number3 are considered to be the same concept.
It is alsoimportant to note that for the purposes of this analysis, when we refer to the ?sense?
ofa word, we mean the sense as determined by our lexical chain analysis.
The basic ideaof our experiment is to try to determine whether the concepts represented by (strong)lexical chains in an original document appear in the summary of that document andwhether the concepts appearing in the summary (as determined by the lexical chainanalysis of the summary) come from strong chains in the document.
If both of thesegive 100% coverage, this would mean that all and only the concepts identified bystrong lexical chains in the document occur in the summary.
Thus the higher thesenumbers turn out to be, the more likely it is that lexical chains are a good intermediaterepresentation of the text summarization task.A corpus was compiled containing the two specific types of documents, rangingin length from 2,247 to 26,320 words each.
These documents were selected at random,with no screening by the authors.
The scientific corpus consisted of 10 scientific articles(5 computer science, 3 anthropology, and 2 biology) along with their abstracts.
Thetextbook corpus consisted of 14 chapters from 10 university level textbooks in varioussubjects (4 computer science, 6 anthropology, 2 history, and 2 economics), includingchapter summaries.For each document in the corpus, the document and its summary were analyzedseparately to produce lexical chains.
In both cases we output the sense numbers spec-ified for each word instance as well as the overriding sense number for each chain.By comparing the sense numbers of (words in) each chain in the document with thecomputed sense of each noun instance in the summary, we can determine whether thesummary indeed contains the same ?concepts?
as indicated by the lexical chains.
Forthe analysis, the specific metrics we are interested in are?
The number and percentage of strong chains from the original text thatare represented in the summary.
Here we say a chain is represented if a3 Recall that synonyms in the WordNet database are identified by a synset (sense) number.494Computational Linguistics Volume 28, Number 4word occurs in the summary in the same sense as in the documentstrong chain.
(Analogous to recall)?
The number and percentage of noun instances in the summary thatrepresent strong chains in the document.
(Analogous to precision)By analyzing these two metrics, we can determine how well lexical chains representthe information that appears in these types of human-generated summaries.
We willloosely use the terms recall and precision to describe these two metrics.4.1 Experimental ResultsEach document in the corpus was analyzed by running our lexical chain algorithm andcollecting the overriding sense number of each strong lexical chain computed.
Eachsummary in the corpus was analyzed by our algorithm, and the disambiguated sense(i.e., the sense of the noun instance that was selected in order to insert it into a chain)of each noun was collected.
Table 5 shows the results of this analysis.
The number ofstrong chains computed for the document is shown in column 2.
Column 3 shows theTable 5Evaluation results.Total Total Strong Chains Noun InstancesNumber Number with withof Strong of Noun Corresponding CorrespondingChains in Instances Noun Instances Strong Chains inDocument Document in Summary in Summary DocumentCS Paper 1 10 22 7 (70.00%) 19 (86.36%)CS Paper 2 7 19 6 (71.43%) 17 (89.47%)CS Paper 3 5 31 4 (80.00%) 27 (87.19%)CS Paper 4 6 25 5 (83.33%) 24 (96.00%)CS Paper 5 8 16 6 (75.00%) 12 (75.00%)ANTH Paper 1 7 20 7 (100.00%) 17 (85.00%)ANTH Paper 2 5 17 4 (80.00%) 13 (76.47%)ANTH Paper 3 7 21 6 (28.57%) 7 (33.33%)BIO Paper 1 4 19 4 (100.00%) 17 (89.47%)BIO Paper 2 5 31 5 (80.00%) 28 (90.32%)CS Chapter 1 9 55 8 (88.89%) 49 (89.09%)CS Chapter 2 7 49 6 (85.71%) 42 (85.71%)CS Chapter 3 11 31 9 (81.82%) 25 (80.65%)CS Chapter 4 14 47 5 (35.71%) 21 (44.68%)ANTH Chapter 1 5 61 4 (80.00%) 47 (77.05%)ANTH Chapter 2 8 74 7 (87.50%) 59 (79.73%)ANTH Chapter 3 12 58 11 (91.67%) 48 (82.76%)ANTH Chapter 4 13 49 11 (84.62%) 42 (85.71%)ANTH Chapter 5 7 68 5 (71.43%) 60 (88.24%)ANTH Chapter 6 9 59 8 (88.89%) 48 (81.36%)HIST Chapter 1 12 71 10 (83.33%) 67 (94.37%)HIST Chapter 2 8 65 7 (87.50%) 55 (84.62%)ECON Chapter 1 14 68 12 (85.71%) 63 (92.65%)ECON Chapter 2 9 51 7 (77.78%) 33 (64.71%)Mean 79.12% 80.83%Median 82.58% 85.35%Note: ANTH?anthropology, BIO?biology, CS?computer science, ECON?economics,HIST?history.495Silber and McCoy Efficient Lexical Chains for Summarizationtotal number of noun instances found in the summary.
Column 4 shows the number,and percentage overall, of strong chains from the document that are represented bynoun instances in the summary (recall).
The number, and the percentage overall, ofnouns of a given sense from the summary that have a corresponding strong chainwith the same overriding sense number (representing the chain) in the original textare presented in column 5 (precision).
Summary statistics are also presented.In 79.12% of the cases, lexical chains appropriately represent the nouns in the sum-mary.
In 80.83% of the cases, nouns in the summary would have been predicted by thelexical chains.
The algorithm performs badly on two documents, anthropology paper3 and computer science chapter 4, under this analysis.
Possible reasons for this willbe discussed below, but our preliminary analysis of these documents leads us to be-lieve that they contain a greater number of pronouns and other anaphoric expressions(which need to be resolved to compute lexical chains properly).
These potential rea-sons need to be examined further to determine why our algorithm performs so poorlyon these documents.
Excluding these two documents, our algorithm has a recall of83.39% and a precision of 84.63% on average.
It is important to note that strong chainsrepresent only between 5% and 15% of the total chains computed for any document.The evaluation presented here would be enhanced by having a baseline for com-parison.
It is not clear, however, what this baseline should be.
One possibility wouldbe to use straight frequency counts as an indicator and use these frequency counts forcomparison.5.
Discussion and Future WorkSome problems that cause our algorithm to have difficulty, specifically proper nounsand anaphora resolution, need to be addressed.
Proper nouns (people, organization,company, etc.)
are often used in naturally occurring text, but since we have no in-formation about them, we can only perform frequency counts on them.
Anaphoraresolution, especially in certain domains, is a bigger issue.
Much better results areanticipated with the addition of anaphora resolution to the system.Other issues that may affect the results we obtained stem from WordNet?s coverageand the semantic information it captures.
Clearly, no semantically annotated lexiconcan be complete.
Proper nouns and domain-specific terms, as well as a number ofother words likely to be in a document, are not found in the WordNet database.
Thesystem defaults to word frequency counts for terms not found.
Semantic distance inthe ?is a?
graph, a problem in WordNet, does not affect our implementation, sincewe don?t use this information.
It is important to note that although our system usesWordNet, there is nothing specific to the algorithm about WordNet per se, and anyother appropriate lexicon could be ?plugged in?
and used.Issues regarding generation of a summary based on lexical chains need to be ad-dressed and are the subject of our current work.
Recent research has begun to look atthe difficult problem of generating a summary text from an intermediate representa-tion.
Hybrid approaches such as extracting phrases instead of sentences and recom-bining these phrases into salient text have been proposed (Barzilay, McKeown, andElhadad 1999).
Other recent work looks at summarization as a process of revision; inthis work, the source text is revised until a summary of the desired length is achieved(Mani, Gates, and Bloedorn 1999).
Additionally, some research has explored cuttingand pasting segments of text from the full document to generate a summary (Jing andMcKeown 2000).
It is our intention to use lexical chains as part of the input to a moreclassical text generation algorithm to produce new text that captures the concepts fromthe extracted chains.
The lexical chains identify noun (or argument) concepts for the496Computational Linguistics Volume 28, Number 4summary.
We are examining ways for predicates to be identified and are concentratingon situations in which strong lexical chains intersect in the text.6.
ConclusionsIn this article, we have outlined an efficient, linear-time algorithm for computing lexicalchains as an intermediate representation for automatic machine text summarization.This algorithm is robust in that it uses the method proposed by Barzilay and Elhadad,but it is clearly O(n) in the number of nouns in the source document.The benefit of this linear-time algorithm is its ability to compute lexical chainsin documents significantly larger than could be handled by Barzilay and Elhadad?simplementation.
Thus, our algorithm makes lexical chains a computationally feasi-ble intermediate representation for summarization.
In addition, we have presented amethod for evaluating lexical chains as an intermediate representation and have eval-uated the method using 24 documents that contain human-generated summaries.
Theresults of these evaluations are promising.An operational sample of our algorithm is available on the Web; a search enginethat uses our algorithm can be accessed there as well (available at ?http://www.eecis.udel.edu/?silber/research.htm?
).AcknowledgmentsThe authors wish to thank the KoreanGovernment, Ministry of Science andTechnology, whose funding, as part of theBilingual Internet Search Machine Project,has made this research possible.Additionally, special thanks to MichaelElhadad and Regina Barzilay for theiradvice and for generously making their dataand results available, and to the anonymousreviewers for their helpful comments.ReferencesBarzilay, Regina and Michael Elhadad.
1997.Using lexical chains for textsummarization.
In Proceedings of theIntelligent Scalable Text SummarizationWorkshop (ISTS-97), Madrid, Spain.Barzilay, Regina, Kathleen R. McKeown,and Michael Elhadad.
1999.
Informationfusion in the context of multi-documentsummarization.
In Proceedings of the 37thAnnual Conference of the Association forComputational Linguistics, College Park,MD.
Association for ComputationalLinguistics, New Brunswick, NJ.Hirst, Graeme and David St.-Onge.
1997.Lexical chains as representation of contextfor the detection and correction ofmalapropisms.
In Christiane Fellbaum,editor, Wordnet: An electronic lexical databaseand some of its applications.
MIT Press,Cambridge, pages 305?332.Jing, H. and K. McKeown.
2000.
Cut andpaste based text summarization.
InProceedings of NAACL-00, Seattle.Mani, Inderjeet, Barbara Gates, and EricBloedorn.
1999.
Improving summaries byrevising them.
In Proceedings of the 37thAnnual Conference of the Association forComputational Linguistics, College Park,MD.
Association for ComputationalLinguistics, New Brunswick, NJ.Marcu, Daniel.
1999.
The automatic creationof large scale corpora for summarizationresearch.
In The 22nd International ACMSIGIR Conference on Research andDevelopment in Information Retrieval,Berkeley.
ACM Press, New York.Morris, J. and G. Hirst.
1991.
Lexicalcohesion computed by thesaural relationsas an indecator of the structure of text.Computational Linguistics, 18(1):21?45.Silber, H. Gregory and Kathleen F. McCoy.2000.
Efficient text summarization usinglexical chains.
In 2000 InternationalConference on Intelligent User Interfaces,New Orleans, January.Sparck Jones, Karen.
1993.
What might be insummary?
Information Retrieval ?93,Regensburg, Germany, September, pages9?26.
