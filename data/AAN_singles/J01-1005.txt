Squibs and DiscussionsUnsupervised Named Entity RecognitionUsing Syntactic and Semantic ContextualEvidenceAlessandro Cucchiarelli*Universita di AnconaPaola Velardi tUniversit~i di Roma 'La Sapienza'Proper nouns form an open class, making the incompleteness of manually or automatically earnedclassification rules an obvious problem.
The purpose of this paper is twofold:first, o suggest the useof a complementary "backup" method to increase the robustness of any hand-crafted ormachine-learning-based NE tagger; and second, to explore the effectiveness of using more fine-grainedevidence--namely, s ntactic and semantic ontextual knowledge--in classifying NEs.1.
Proper Noun ClassificationIn this paper we present a corpus-driven statistical technique that uses a learningcorpus to acquire contextual classification cues, and then uses the results of thisphase to classify unrecognized proper nouns (PN) in an unlabeled corpus.
Trainingexamples of proper nouns are obtained using any available named entity (NE) recog-nizer (in our experiments we used a rule-based recognizer and a machine-learning-based recognizer).
The contextual model of PN categories i learned without supervi-sion.The approach described in this paper is complementary to current methods forNE recognition: our objective is to improve, without additional manual effort, therobustness of any available NE system through the use of more "fine-grained" con-textual knowledge, best exploited at a relatively late stage of analysis.
The method isparticularly useful when an available NE system must be rapidly adapted to anotherlanguage or to another domain, provided the shift is not dramatic.Furthermore, our study provides experimental evidence relating to two issuesstill under debate: i) the effectiveness, in practical NLP applications, of using syntacticrelations (most systems use plain collocations and morphological features), and ii)context expansion based on thesauri.
While we do not provide a definitive argumentin favor of syntactic ontexts and semantic expansion for word sense disambiguationtasks in general, we do show that they can be successfully used for unknown propernoun classification.
Proper nouns have particular characteristics, such as low or zeroambiguity, which makes it easier to characterize their contexts.2.
Description of the U_PN Classification MethodIn this section we briefly summarize the corpus-based tagging technique for the classi-fication of unknown proper nouns (for more details, see Cucchiarelli, Luzi, and Velardi\[1998\]).
* Istituto di Informatica, Via Brecce Bianche 1-60131 Ancona, Italy.
E-mail: alex@inform.unian.itt Dipartimento di Scienze dell'Informazione, Via Salaria 113, 1-00198 Roma, Italy.
E-mail: velardi@dsi.uniromal.itComputational Linguistics Volume 27, Number 12.1 Learning Contextual Sense IndicatorsOur method proceeds as follows: first, by means of any available NE recognitiontechnique (which we will call an early NE classifier), at least some examples of PNs ineach category are detected.
Second, through an unsupervised corpus-based technique,typical PN syntactic and semantic contexts are learned.
Syntactic and semantic cues canthen be used to extend the coverage of the early NE classifier, increasing its robustnessto the limitations of the gazetteers (PN dictionaries) and domain shifts.In phase one, a learning corpus in the application domain is morphologicallyprocessed.
The gazetteer lookup and the early NE classifier are then used to detectPNs.
At the end of this phase, "some" PNs are recognized and classified, dependingupon the size of the gazetteer and the actual performance (in the domain) of the NEclassifier.In phase two, the objective is to learn a contextual model of each PN category,augmented with syntactic and semantic features.
Since the algorithm is unsupervised,statistical techniques are applied to smooth the weight of acquired examples as afunction of semantic and syntactic ambiguity.
1Syntactic processing is applied over the corpus.
A shallow parser (see details inBasili, Pazienza, and Velardi \[1994\]) extracts from the learning corpus elementary syn-tactic relations such as Subject-Object, Noun-Preposition-Noun, etc.
2 An elementarysyntactic link (esl) is represented as:esl(wi, mod( typei, Wk ) )where wj is the headword, Wk is the modifier, and type i is the type of syntactic relation(e.g.
Prepositional Phrase, Subject-Verb, Verb-Direct-Object, e c.).
For example, esl(closemod(G_N_V_Act Xerox)) reads: Xerox is the modifier of the head close in a Subject-Verb(G_N_V_Act) syntactic relation.In our study, the context of a word w in a sentence S is represented by the eslsincluding w as one of its arguments (wj or Wk).
The esls that include semanticallyclassified PNs as one of their arguments are grouped in a database, called PN_esl.This database provides contextual evidence for assigning a category to unknown PNs.2.2 Tagging Unknown PNsA corpus-driven algorithm is used to classify unknown proper nouns recognized assuch, but not semantically classified by the early NE recognizer.
3?
Let U_PN be an unknown proper noun, i.e., a single word or a complexnominal.
Let Cpn = (Cp~l, Cpn2 .
.
.
.
.
CpnN) be the set of semantic ategoriesfor proper nouns (e.g.
Person, Organization, Product, etc.).
Finally, letESL be the set of esls (often more than one in a text) that include U_PNas one of their arguments.?
For each esli in ESL let:esli( wj, mod( typei, Wk )) = esli( x, U_PN)1 We say the algorithm is unsupervised because neither the NE items detected by the early recognizernor the extracted syntactic ontexts are inspected for correctness.2 Shallow, or partial parsers are a well-established technique for corpus parsing.
Several partial parsersare readily available---for example, the freely downloadable LINK parser.3 A standard POS tagger augmented with simple heuristics is used to detect possible instances of PNs.Errors are originated only by ambiguous entence beginners, as "Owens Illinois" or "Boots Plc"causing partial recognition.124Cucchiarelli and Velardi Unsupervised Named Entity Recognitionwhere x = w\] or x = Wk and U-PN=wk or wj (the unknown PN can beeither the head or the modifier), type i is the syntactic type of esl (e.g.N-of-N, NAN, V-for-N, etc.
), and furthermore l t:pl(esli(x, U_PN) )be the plausibi l i ty of a detected esl.
Plausibility is a measure of thestatistical evidence of a detected syntactic relation (Basili, Marziali, andPazienza 1994; Grishman and Sterling 1994) that depends upon local(i.e., sentence-level) syntactic ambiguity and global corpus evidence.
Theplausibility accounts for the uncertainty arising from syntactic ambiguity.,.
Finally, let:- -  ESLA be a set of esls in PN_esl (the previously learnedcontextual model) defined as follows: for each esli(x, Uff)N) inESL, put in ESLA the set of eslj(x, PNj) with typej = type i, x inthe same position as esli, and PNj a known proper noun, inthe same position as U_PN in esli.ESLB be a set of esls in PN_esl defined as follows: for eachesli(x, U_PN) in ESL put in ESLB the set of eslj(w, PNj) withtype\] -- type i, w in the same position as x in esli, Sim(w,x) > 6,and PNj a known proper noun, in the same position as U_PNin esli.
Sim(w, x) is a similarity measure between x and w. Inour experiments, Sim(w,x) > ~ iff w and x have a commonhyperonym H in WordNet.
The generality of H (i.e., thenumber of levels from x to H) is made parametric, to analyzethe effect of generalization.?
For each semantic ategory Cp,j compute vidence(Cp,j) as:E weightq (x)D(x, C(PNj))esliC ESLA,C( PNj)=Cpn jevidence(Cp~j) = a +E weight~j (x)D(x, C(PNj))esliEESLAE weightq (x)D(x, C(PNj))esli E ESLB,C( PNj) =Cpn j flE weightiy(x)D(x'C(PNJ ))esli6 ESLBwhere:weightq(x) = weight q( esli(x, PNj) ) = pl( esli(x, PNj) ) ?
(1 - ~(~)-1~_1 ,u weightij(w ) = weightij(esli(w, PNj) ) = pl(esli(w, PNj)).
(1 - amb(w)-l~k_\] 2pl(esli(x, PNj)) is the plausibility and arab(x) is the ambiguityof x in eslik is a constant factor used to incrementally reduce the influenceof ambiguous words.
The smoothing is tuned to be higher inESLBa and fl are parametric, and can be used to study the evidenceprovided by ESLA and ESLB125Computational Linguistics Volume 27, Number 1D(x, C(PNj)) is a discrimination factor used to determine thesaliency (Yarowsky 1992) of a context esli(x, _) for a categoryC(PNj), i.e., how good a context is at discriminating betweenC(PNj)and the other categories.
4The selected category for U~N isC = argmax(evidence(Cp~k))When grouping all the evidence of a U_PN in a text, the underlying hypothesis ithat, in a given linguistic domain (finance, medicine, etc.
), a PN has a unique sense.
Thisis a reasonable restriction for Proper Nouns, supported by empirical evidence, thoughwe would be more skeptical about the applicability of the one-sense-per-discourseparadigm (Gale, Church, and Yarowsky 1992) to generic words.
We believe that it isprecisely this restriction that makes the use of syntactic and semantic ontexts effectivefor PNs.Notice that the formula of the evidence has several smoothing factors that work to-gether to reduce the influence of unreliable or uninformative contexts.
The formula alsohas parameters (k, ~, fl), estimated by running systematic experiments.
Standard sta-tistical techniques have been used to balance xperimental conditions and the sourcesof variance.3.
Using WordNet for Context GeneralizationOne of the stated objectives of this paper is to investigate the effect of context gen-eralization (the addend ESLB in the formula of the evidence) on our sense taggingtask.The use of on-line thesauri for context generalization has already been investigatedwith limited success (Hearst and Schuetze 1993; Brill and Resnik 1994; Resnik 1997;Agirre and Rigau 1996).
Though the idea of using thesauri for context expansion isquite common, there are no clear indications that this is actually useful in terms ofperformance.
However, studying the effect of context expansion for a PN tagging taskin particular is relevant because:PNs may be hypothesized to have a unique sense in a text, and even in adomain corpus.
Therefore, we can reliably consider as potential senseindicators all the contexts in which a PN appears.
The only source ofambiguity is then the word wi co-occurring in a syntactic ontext with aPN, esli(wi, U_PN), but since in ESLB we group several contexts,hopefully spurious hyperonyms of wi will gain lower evidence.
Forexample, consider the context "division of Americand3randsdnc".
Divisionis a highly ambiguous word, but, when generalizing it, the majority ofits senses appearing in the same type of syntactic relation with a ProperNoun (e.g.
branch of Drexel_ Burnhamd,ambert_Group dnc, part of Nationale_Nederlanden_Group) are indeed pertinent senses.4 For example, a Subject_Verb phrase with the verb make (e.g., Ace made acontract) isfound with almostequal probability with Person and Organization names.
We used a simple conditional probabilitymodel for D(x, c(PNj)), but we believe that more refined measures could improve performance.126Cucchiarelli and Velardi Unsupervised Named Entity Recognition?
PN categories (e.g., Person, Location, Product) exhibit a more stable andless ambiguous contextual behavior than other more vague categories,such as psychological_feature.
5?
We can study the degree of generalization at which an opt imumperformance is achieved.4.
Experimental DiscussionThe purpose of experimental evaluation is twofold:To test the improvement in robustness of a state-of-the-art NE recognizer.To study the effectiveness of syntactic ontexts and of a "cautious"context generalization on the performance of the U_PN tagger, analyzedin isolation.
The effect of generalization is studied by gradually relaxingthe notion of similarity in the formula of evidence and by tuning,through the factors a and fl, the contribution of generalized contexts tothe formula of evidence.In our experiment, we used the Italian Sole24Ore half-million-word corpus onfinancial news, the one-mill ion-word Wall Street Journal corpus, and WordNet, as stan-dard on-line available resources, as well as a series of computational tools made avail-able for our research:?
the VIE system (Humphreys et al 1996) for initial detection of ProperNouns from the learning corpus; for the same purpose we also used amachine learning method based on decision lists, described in Paliouras,Karkaletsis, and Spyropolous (1998).?
the SSA shallow syntactic analyzer (Basili, Pazienza, and Velardi 1994)for surface corpus parsing.
6?
the tool described in Cucchiarelli and Velardi (1998) for corpus-drivenWordNet pruning.
74.1 Experiment 1: Improving Robustness of NE RecognizersThe objective of Experiment 1 is to verify the improvement in robustness of existingNE recognizers, through the use of our tagger.
In Figure 1, three testing experimentsare shown.
The table measures the local performance of the NE tagging task achievedby the early NE recognizer, by our untrained tagger, and finally, the joint performanceof the two methods.In the first test, we used the Italian Sole24Ore corpus.
Due to the unavailability ofWordNet in Italian, we used a dictionary of strict synonyms for context expansion.
Inthis test, we "loosely" adapted the English VIE system (as used in MUC-6) to Italian.5 In Velardi and Cucchiarelli (2000) we formally studied the relation between category type andlearnability of contextual cues for WSD.6 We also used the GATE partial parser.
We were not as successful with this parser because it is notdesigned for high-performance VP3?P and NP-PP detection, but prepositional contexts are often themost informative indicators.7 This method produces a20-30% reduction of the initial WordNet ambiguity, depending on the specificcorpus.127Computational Linguistics Volume 27, Number 1A B C D E F G H I J K LTest 1 239 355 67.32% 339 70.50% 60 83 72.29% 75 80.00% 84.23% 88.20%Test 2 650 793 81.90% 759 85.63% 67 83 80.72% 80 83.75% 90.42% 94.47%Test 3 3,040 4,168 72.94% 3,233 94.03% 585 935 62.57% 810 72.22% 86.97% 89.66%LegendA: PNs correctly tagged by the early NE recognizerB: Total PNs in the Test CorpusC: Local Recall of the early NE recognizer (A/B)D: Total PNs detected by the early NE recognizer (D = A + A1 (errors) + G(unknown)E: Local Precision of the early NE recognizer (A/D)F: UPNs correctly tagged by the UPN tagger in the Test CorpusG: Total UPNs not detected by the early NE recognizerH: Local recall of UPN tagger (Phase2) (F/G)I: Total UPNs for which a decision was possible by the UPN tagger\]: Local precision of the UPN taggerK: Joint Recall of the two methods (A + F)/BL: Joint Precision of the two methods (A+F)/DFigure 1Outline of results on the Sole24Ore corpus.We used the English gazetteer as it was and we appl ied simple " language porting" tothe NE grammar  (e.g., replacing English words and preposit ions with correspondingItalian words, and little more), s This justifies the low performance of the rule-basedclassifier.
Note that our context-based tagger produces a considerable improvement  inperformance (around 18%), therefore the global performance (column K and L) turnsout to be comparable with state-of-the-art systems, without a significant readaptationeffort.In the second test, we used again VIE, on the English Wall Street Journal corpus.We used a version of VIE that was designed to detect NE in a management  successiondomain (we are testing the effect of a domain shift here).
Local performance wassomewhat  lower than in MUC-6.
Again, we measured a 9% improvement  using ourtagger, and very high global performance.The third test was the most demanding.
Here, we used only half of the namedentity gazetteer used in previous experiments.
The purpose of this test was also toverify the effect on performance of a poorly populated gazetteer.
In this test, rather thanusing LASIE, we used a machine learning method described in Paliouras, Karkaletsisand Spyropolous (1998).
This method uses as a training set the available half of thegazetteer to learn a context-based decision list for NE classification.As shown in Test 3, column B, the initial number  of PNs in the test corpus is nowconsiderably higher.
The decision-list classifier is tuned to classify with high precisionand lower recall.
Therefore, only the "hardest" cases are submitted to our untrainedclassifier.
In fact, local performance of our classifier is around 10% lower than for pre-vious tests, but nevertheless, global performance (in terms of joint precision and recall)shows an improvement.
Finally, we observe that the performance figures reported inFigure 1 say nothing about the various sources of errors.
Errors and misses occur bothduring the off-line learning phase (as we said, NE instances and syntactic contexts8 Most location and company names known worldwide (e.g., NewYork, IBM) are in fact mentioned ineconomic journals regardless of the language.128Cucchiarelli and Velardi Unsupervised Named Entity Recognitionare not inspected for correctness, therefore the contextual knowledge base is errorprone) and prior to the U_PN tagging phase: a compound PN may be incompletelyrecognized uring POS tagging, causing the generation of an uninformative syntacticcontext (e.g., "Owens Illinois" at the beginning of a sentence is recognized as "owensIllinois", causing a spurious NdN(owen,Illinois) context o be generated).Because all these "external" sources of noise are not filtered out, we may thenreliably conclude that our tagger is effective at improving the robustness of propernoun classification, though clearly the amount of improvement depends upon thebaseline performances of the early method used for PN classification.Although the classification evidence provided by syntactic ontexts is somewhatnoise prone, it proves to be useful as a "backup," when other "simpler" contextualevidence does not allow a reliable decision.4.2 Effectiveness of Syntactic and Semantic Cues for Semantic ClassificationIn a second experiment, we used the experimental set up of Test 2 (WSJ+VIE describedabove) to evaluate the effectiveness of context expansion on system performance.
Weapplied a pruning method on WordNet (Cucchiarelli and Velardi 1998) to reduce initialambiguity of contexts.
This pruning method allowed an average of 27% reduction inthe initial ambiguity of the total number of the 13,428 common nouns in the WallStreet Journal corpus.
The objective of this experiment was to allow a more detailedevaluation of our method, with respect o several parameters.We built four test sets with the same distribution of PN categories and frequencydistribution as in the application corpus.
We selected four frequency ranges (1, 2, 3-9,> 10) and in each range we selected 100 PNs, reflecting the frequency distributionin the corpus of the three main PN semantic ategories--Person, Organization, andLocation.
We then built another test set, called TSAll, with 400 PNs again reflecting thefrequency and category distribution of the corpus.
The 400 PNs were then removedfrom the set of 37,018 esls extracted by our parser and from the gazetteer (wheneverincluded).In this experiment, we wanted to measure the performance of the U_PN taggerover the 400 words in the test set, in terms of F-measure, according to several varyingfactors:?
the category type;?
the amount of initial contextual evidence (i.e., the frequency range,reflected by the different test sets);?
the factors oe and fl, i.e., the influence of local and generalized contexts;?
the level of generalization L.Figures 2 summarizes the results of the experiment.
Figure 2(a) shows the increasein performance as a function of the values of oe and fl and the generalization level.
Nmeans no generalization, only the evidence provided by ESLA is computed; 0 meansthat ESLB collects the evidence provided by contexts in which w is a strict synonym ofx according to WordNet; 1, 2, and 3 refer to incremental levels of generalization in the(pruned) WordNet hierarchy.
The figure shows that context generalization produces upto 7% improvement in performance.
Best results are obtained with L = 2 and ~ = 0.7,fl = 0.3.
Further generalization may cause a drop in performance.
High ambiguity isthe cause of this behavior, despite WordNet pruning (without WordNet pruning, weobserved a performance inversion at level 1; this experiment is not reported ue to129Computational Linguistics Volume 27, Number 1hi1% i .
.
, "  .
,"  "/ /( l=o ?, \]3 0 3~% 4 o 3 I~ 41 776%9S~,IN " Leve l  of  Gen~ra l i za t lon(a )Figure 2Evaluation of the effectiveness of context expansion.f:2t~(b)limitations of space).
Figure 2(b) illustrates the influence of initial contextual evidence.Recognition of singleton PNs remains almost constant as the contribution of gener-alized and nongeneralized contexts varies.
Looking more in detail, we observe thatrecall increases with fl -- (1 -  c~), but precision decreases.
Generalization on the basis ofa unique context does not allow any filtering of spurious senses, while when groupingseveral contexts, spurious senses gain lower evidence (as anticipated in Section 3).Finally, we designed an experiment to evaluate the influence of the test set com-position on the U_PN tagger performances.
We performed an analysis of variance(ANOVA test \[Hoel 1971\]) on the results obtained by processing nine different estsets of 400 PNs each, selected randomly.
In all our experiments the details of whichwe omit, for lack of space), we found that the U-PN tagging method performanceswere independent of the variations of the test set.ReferencesAgirre, Eneko and German Rigau.
1996.Word Sense Disambiguation usingConceptual Density.
In Proceedings ofthe16th International Conference onComputational Linguistics (COLING '96),Copenhagen, Denmark.Basili, Roberto, Alessandro Marziali, andMaria Teresa Pazienza.
1994.
Modellingsyntax uncertainty in lexical acquisitionfrom texts.
Journal of QuantitativeLinguistics, 1(1).Basili, Roberto, Maria Teresa Pazienza, andPaola Velardi.
1994.
A (not-so) shallowparser for collocational nalysis.
InProceedings ofthe 15th InternationalConference on Computational Linguistics(COLING '94), Kyoto, Japan.Brill, Erik and Philip Resnik.
1994.
Atransformation-based approach toprepositional phrase attachmentdisambiguation.
I  Proceedings ofthe 15thInternational Conference on ComputationalLinguistics (COLING '94), Kyoto, Japan.Cucchiarelli, Alessandro, Danilo Luzi, andPaola Velardi.
1998.
Automatic semantictagging of unknown proper names.
InCOLING-ACL "98: 36th Annual Meeting ofthe Association for Computational Linguisticsand I7th International Conference onComputational Linguistics, Montreal,Canada.Cucchiarelli, Alessandro and Paola Velardi.1998.
Finding a domain-appropriate senseinventory for semantically tagging acorpus.
International Journal on NaturalLanguage Engineering, December.Gale, William, Kenneth Church, and DavidYarowsky.
1992.
One sense per discourse.In Proceedings ofthe DARPA Speech andNatural Language Workshop.
Harriman, NY.Grishrnan, Ralph and John Sterling.
1994.Generalizing automatically generatedselectional patterns.
Proceedings ofthe 15thInternational Conference on ComputationalLinguistics (COLING "94), Kyoto, Japan.Hearst, Marti and Hinrich Schuetze.
1993.Customizing a lexicon to better suite acomputational task.
In Proceedings ofACL-SIGLEX Workshop on LexicalAcquisition from Text.
Columbus, OH.Hoel, Paul Gerhard.
1971.
Introduction to130Cucchiarelli and Velardi Unsupervised Named Entity RecognitionMathematical Statistics.
John Wiley & SonsInc., New York.Humphreys, Kevin, Robert Gaizauskas,Hamish Cunningam, and Sheila Azzan.1996.
Technical Specifications, 1996/10/1815.ILASH, University of Sheffield, UK.Paliouras, George, Vangelis Karkaletsis, andConstantine Spyropolous.
1998.
Resultsfrom the named entity recognition task.
InDeliverable 3.2.1 of the European projectECRAN LE 2110.
Available at: http://www2.echo.lu/langeng/en/lel/ecran/ecran.html.Resnik, Philip.
1997.
Selectional referenceand sense disambiguation.
I  Proceedingsof the ACL Workshop Tagging Text withLexical Semantics: Why, What, and How?Washington, DC.Velardi, Paola and Alessandro Cucchiarelli.2000.
A theoretical nalysis ofcontextual-based l arning algorithms forword sense disambiguation.
I  Proceedingsof ECA12000, Berlin, Germany.
(Toappear.
)Yarowsky, David.
1992 Word-sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofthe 14thInternational Conference on ComputationalLinguistics (COLING "92), Nantes, France.131
