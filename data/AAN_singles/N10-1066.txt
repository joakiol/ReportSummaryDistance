Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429?437,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsDiscriminative Learning over Constrained Latent RepresentationsMing-Wei Chang and Dan Goldwasser and Dan Roth and Vivek SrikumarUniversity of Illinois at Urbana ChampaignUrbana, IL 61801{mchang,goldwas1,danr,vsrikum2}@uiuc.eduAbstractThis paper proposes a general learning frame-work for a class of problems that require learn-ing over latent intermediate representations.Many natural language processing (NLP) de-cision problems are defined over an expressiveintermediate representation that is not explicitin the input, leaving the algorithm with boththe task of recovering a good intermediate rep-resentation and learning to classify correctly.Most current systems separate the learningproblem into two stages by solving the firststep of recovering the intermediate representa-tion heuristically and using it to learn the finalclassifier.
This paper develops a novel jointlearning algorithm for both tasks, that uses thefinal prediction to guide the selection of thebest intermediate representation.
We evalu-ate our algorithm on three different NLP tasks?
transliteration, paraphrase identification andtextual entailment ?
and show that our jointmethod significantly improves performance.1 IntroductionMany NLP tasks can be phrased as decision prob-lems over complex linguistic structures.
Successfullearning depends on correctly encoding these (of-ten latent) structures as features for the learning sys-tem.
Tasks such as transliteration discovery (Kle-mentiev and Roth, 2008), recognizing textual en-tailment (RTE) (Dagan et al, 2006) and paraphraseidentification (Dolan et al, 2004) are a few proto-typical examples.
However, the input to such prob-lems does not specify the latent structures and theproblem is defined in terms of surface forms only.Most current solutions transform the raw input intoa meaningful intermediate representation1, and thenencode its structural properties as features for thelearning algorithm.Consider the RTE task of identifying whether themeaning of a short text snippet (called the hypoth-esis) can be inferred from that of another snippet(called the text).
A common solution (MacCartneyet al, 2008; Roth et al, 2009) is to begin by definingan alignment over the corresponding entities, pred-icates and their arguments as an intermediate rep-resentation.
A classifier is then trained using fea-tures extracted from the intermediate representation.The idea of using a intermediate representation alsooccurs frequently in other NLP tasks (Bergsma andKondrak, 2007; Qiu et al, 2006).While the importance of finding a good inter-mediate representation is clear, emphasis is typi-cally placed on the later stage of extracting featuresover this intermediate representation, thus separat-ing learning into two stages ?
specifying the la-tent representation, and then extracting features forlearning.
The latent representation is obtained by aninference process using predefined models or well-designed heuristics.
While these approaches oftenperform well, they ignore a useful resource whengenerating the latent structure ?
the labeled data forthe final learning task.
As we will show in this pa-per, this results in degraded performance for the ac-tual classification task at hand.
Several works haveconsidered this issue (McCallum et al, 2005; Gold-wasser and Roth, 2008b; Chang et al, 2009; Dasand Smith, 2009); however, they provide solutions1In this paper, the phrases ?intermediate representation?
and?latent representation?
are used interchangeably.429that do not easily generalize to new tasks.In this paper, we propose a unified solution to theproblem of learning to make the classification deci-sion jointly with determining the intermediate rep-resentation.
Our Learning Constrained Latent Rep-resentations (LCLR) framework is guided by the in-tuition that there is no intrinsically good intermedi-ate representation, but rather that a representation isgood only to the extent to which it improves perfor-mance on the final classification task.
In the rest ofthis section we discuss the properties of our frame-work and highlight its contributions.Learning over Latent Representations This pa-per formulates the problem of learning over latentrepresentations and presents a novel and general so-lution applicable to a wide range of NLP applica-tions.
We analyze the properties of our learningsolution, thus allowing new research to take advan-tage of a well understood learning and optimizationframework rather than an ad-hoc solution.
We showthe generality of our framework by successfully ap-plying it to three domains: transliteration, RTE andparaphrase identification.Joint Learning Algorithm In contrast to mostexisting approaches that employ domain specificheuristics to construct intermediate representationsto learn the final classifier, our algorithm learns toconstruct the optimal intermediate representation tosupport the learning problem.
Learning to representis a difficult structured learning problem however,unlike other works that use labeled data at the in-termediate level, our algorithm only uses the binarysupervision supplied for the final learning problem.Flexible Inference Successful learning dependson constraining the intermediate representation withtask-specific knowledge.
Our framework uses thedeclarative Integer Linear Programming (ILP) infer-ence formulation, which makes it easy to define theintermediate representation and to inject knowledgein the form of constraints.
While ILP has been ap-plied to structured output learning, to the best of ourknowledge, this is the first work that makes use ofILP in formalizing the general problem of learningintermediate representations.2 PreliminariesWe introduce notation using the Paraphrase Iden-tification task as a running example.
This is the bi-nary classification task of identifying whether onesentence is a paraphrase of another.
A paraphrasepair from the MSR Paraphrase corpus (Quirk et al,2004) is shown in Figure 1.
In order to identifythat the sentences paraphrase each other , we needto align constituents of these sentences.
One possi-ble alignment is shown in the figure, in which thedotted edges correspond to the aligned constituents.An alignment can be specified using binary variablescorresponding to every edge between constituents,indicating whether the edge is included in the align-ment.
Different activations of these variables inducethe space of intermediate representations.The notification was first reported Friday by MSNBC.MSNBC.com first reported the CIA request on Friday.Figure 1: The dotted lines represent a possible intermediaterepresentation for the paraphrase identification task.
Since dif-ferent representation choices will impact the binary identifica-tion decision directly, our approach chooses the representationthat facilitates the binary learning task.To formalize this setting, let x denote the inputto a decision function, which maps x to {?1, 1}.We consider problems where this decision dependson an intermediate representation (for example, thecollection of all dotted edges in Figure 1), which canbe represented by a binary vector h.In the literature, a common approach is to sepa-rate the problem into two stages.
First, a genera-tion stage predicts h for each x using a pre-definedmodel or a heuristic.
This is followed by a learn-ing stage, in which the classifier is trained using h.In our example, if the generation stage predicts thealignment shown, then the learning stage would usethe features computed based on the alignments.
For-mally, the two-stage approach uses a pre-defined in-ference procedure that finds an intermediate repre-sentation h?.
Using features ?(x,h?)
and a learnedweight vector ?, the example is classified as positiveif ?T?(x,h?)
?
0.However, in the two stage approach, the latentrepresentation, which is provided to the learning al-gorithm, is determined before learning starts, andwithout any feedback from the final task.
It is dic-tated by the intuition of the developer.
This approachmakes two implicit assumptions: first, it assumes430the existence of a ?correct?
latent representation and,second, that the model or heuristic used to generateit is the correct one for the learning problem at hand.3 Joint Learning with an IntermediateRepresentationIn contrast to two-stage approaches, we use theannotated data for the final classification task tolearn a suitable intermediate representation which,in turn, helps the final classification.Choosing a good representation is an optimizationproblem that selects which of the elements (features)of the representation best contribute to success-ful classification given some legitimacy constraints;therefore, we (1) set up the optimization frameworkthat finds legitimate representations (Section 3.1),and (2) learn an objective function for this optimiza-tion problem, such that it makes the best final deci-sion (Section 3.2.
)3.1 InferenceOur goal is to correctly predict the final labelrather than matching a ?gold?
intermediate repre-sentation.
In our framework, attempting to learn thefinal decision drives both the selection of the inter-mediate representation and the final predictions.For each x, let ?
(x) be the set of all substructuresof all possible intermediate representations.
In Fig-ure 1, this could be the set of all alignment edgesconnecting the constituents of the sentences.
Givena vocabulary of such structures of sizeN , we denoteintermediate representations by h ?
{0, 1}N , which?select?
the components of the vocabulary that con-stitute the intermediate representation.
We define?s(x) to be a feature vector over the substructures, which is used to describe the characteristics of s,and define a weight vector u over these features.Let C denote the set of feasible intermediate repre-sentations h, specified by means of linear constraintsover h. While ?
(x) might be large, the set of thoseelements in h that are active can be constrained bycontrolling C. After we have learned a weight vec-tor u that scores intermediate representations for thefinal classification task, we define our decision func-tion asfu(x) = maxh?CuT?s??
(x)hs?s(x), (1)and classify the input as positive if fu(x) ?
0.In Eq.
(1), uT?s(x) is the score associated withthe substructure s, and fu(x) is the score for the en-tire intermediate representation.
Therefore, our de-cision function fu(x) ?
0 makes use of the interme-diate representation and its score to classify the in-put.
An input is labeled as positive if its underlyingintermediate structure allows it to cross the decisionthreshold.
The intermediate representation is cho-sen to maximize the overall score of the input.
Thisdesign is especially beneficial for many phenomenain NLP, where only positive examples have a mean-ingful underlying structure.
In our paraphrase iden-tification example, good alignments generally existonly for positive examples.One unique feature of our framework is that wetreat Eq.
(1) as an Integer Linear Programming(ILP) instance.
A concrete instantiation of this set-ting to the paraphrase identification problem, alongwith the actual ILP formulation is shown in Section4.3.2 LearningWe now present an algorithm that learns theweight vector u.
For a loss function ` : R ?
R,the goal of learning is to solve the following opti-mization problem:minu?2?u?2 +?i` (?yifu(xi)) (2)Here, ?
is the regularization parameter.
SubstitutingEq.
(1) into Eq.
(2), we getminu?2?u?2+?i`??
?yi maxh?CuT?s??(x)hs?s(xi)??
(3)Note that there is a maximization term inside theglobal minimization problem, making Eq.
(3) a non-convex problem.
The minimization drives u towardssmaller empirical loss while the maximization usesu to find the best representation for each example.The algorithm for Learning over Constrained La-tent Representations (LCLR) is listed in Algorithm1.
In each iteration, first, we find the best featurerepresentations for all positive examples (lines 3-5).This step can be solved with an off-the-shelf ILPsolver.
Having fixed the representations for the pos-itive examples, we update the u by solving Eq.
(4)at line 6 in the algorithm.
It is important to observe431Algorithm 1 LCLR :The algorithm that optimizes (3)1: initialize: u?
u02: repeat3: for all positive examples (xi, yi = 1) do4: Find h?i ?
arg maxh?C?shsuT?s(xi)5: end for6: Update u by solvingminu?2?u?2 +?i:yi=1`(?uT?sh?i,s?s(xi))+?i:yi=?1`(maxh?CuT?shs?s(xi)) (4)7: until convergence8: return uthat for positive examples in Eq.
(4), we use the in-termediate representations h?
from line 4.Algorithm 1 satisfies the following property:Theorem 1 If the loss function ` is a non-decreasing function, then the objective functionvalue of Eq.
(3) is guaranteed to decrease in everyiteration of Algorithm 1.
Moreover, if the loss func-tion is also convex, then Eq.
(4) in Algorithm 1 isconvex.Due to the space limitation, we omit the proof.Theoretically, we can use any loss function thatsatisfies the conditions of the theorem.
In the exper-iments in this paper, we use the squared-hinge lossfunction: `(?yfu(x)) = max(0, 1?
yfu(x))2.Recall that Eq.
(4) is not the traditional SVM orlogistic regression formulation.
This is because in-side the inner loop, the best representation for eachnegative example must be found.
Therefore, weneed to perform inference for every negative exam-ple when updating the weight vector solution.
In-stead of solving a difficult non-convex optimizationproblem (Eq.
(3)), LCLR iteratively solves a seriesof easier problems (Eq.
(4)).
This is especially truefor our loss function because Eq.
(4) is convex andcan be solved efficiently.We use a cutting plane algorithm to solve Eq.
(4).A similar idea has been proposed in (Joachims et al,2009).
The algorithm for solving Eq.
(4) is presentedas Algorithm 2.
This algorithm uses a ?cache?
Hjto store all intermediate representations for negativeexamples that have been seen in previous iterationsAlgorithm 2 Cutting plane algorithm to optimize Eq.
(4)1: for each negative example xj , Hj ?
?2: repeat3: for each negative example xj do4: Find h?j ?
arg maxh?C?s hsuT?s(xj)5: Hj ?
Hj ?
{h?j}6: end for7: Solveminu?2?u?2 +?i:yi=1`(?uT?sh?i,s?s(xi))+?i:yi=?1`(maxh?HjuT?shs?s(xi)) (5)8: until no new element is added to any Hj9: return u(lines 3-6) 2.
The difference between Eq.
(5) in line7 of Algorithm 2 and Eq.
(4) is that in Eq.
(5), we donot search over the entire space of intermediate rep-resentations.
The search space for the minimizationproblem Eq.
(5) is restricted to the cache Hj .
There-fore, instead of solving the minimization problemEq.
(4), we can now solve several simpler problemsshown in Eq.
(5).
The algorithm is guaranteed tostop (line 8) because the space of intermediate rep-resentations is finite.
Furthermore, in practice, thealgorithm needs to consider only a small subset of?hard?
examples before it converges.Inspired by (Hsieh et al, 2008), we apply an effi-cient coordinate descent algorithm for the dual for-mulation of (5) which is guaranteed to find its globalminimum.
Due to space considerations, we do notpresent the derivation of dual formulation and thedetails of the optimization algorithm.4 Encoding with ILP: A ParaphraseIdentification ExampleIn this section, we define the latent representationfor the paraphrase identification task.
Unlike the ear-lier example, where we considered the alignment oflexical items, we describe a more complex interme-diate representation by aligning graphs created usingsemantic resources.An input example is represented as two acyclic2In our implementation, we keep a global cache Hj for eachnegative example xj .
Therefore, in Algorithm 2, we start witha non-empty cache improving the speed significantly.432graphs, G1 and G2, corresponding to the firstand second input sentences.
Each vertex in thegraph contains word information (lemma and part-of-speech) and the edges denote dependency rela-tions, generated by the Stanford parser (Klein andManning, 2003).
The intermediate representationfor this task can now be defined as an alignment be-tween the graphs, which captures lexical and syntac-tic correlations between the sentences.We use V (G) and E(G) to denote the set of ver-tices and edges in G respectively, and define fourhidden variable types to encode vertex and edgemappings between G1 and G2.?
The word-mapping variables, denoted byhv1,v2 , define possible pairings of vertices,where v1 ?
V (G1) and v2 ?
V (G2).?
The edge-mapping variables, denoted byhe1,e2 , define possible pairings of the graphsedges, where e1 ?
E(G1) and e2 ?
E(G2).?
The word-deletion variables hv1,?
(or h?,v2) al-low for vertices v1 ?
V (G1) (or v2 ?
V (G2))to be deleted.
This accounts for omission ofwords (like function words).?
The edge-deletion variables, he1,?
(or h?,e2) al-low for deletion of edges from G1 (or G2).Our inference problem is to find the optimal set ofhidden variable activations, restricted according tothe following set of linear constraints?
Each vertex inG1 (orG2) can either be mappedto a single vertex in G2 (or G1) or marked asdeleted.
In terms of the word-mapping andword-deletion variables, we have?v1 ?
V (G1);hv1,?
+?v2?V (G2)hv1,v2 = 1 (6)?v2 ?
V (G2);h?,v2 +?v1?V (G1)hv1,v2 = 1 (7)?
Each edge in G1 (or G2) can either be mappedto a single edge in G2 (or G1) or marked asdeleted.
In terms of the edge-mapping andedge-deletion variables, we have?e1 ?
E(G1);he1,?
+?e2?E(G2)he1,e2 = 1 (8)?e2 ?
E(G2);h?,e2 +?e1?E(G1)he1,e2 = 1 (9)?
The edge mappings can be active if, and onlyif, the corresponding node mappings are ac-tive.
Suppose e1 = (v1, v?1) ?
E(G1) ande2 = (v2, v?2) ?
E(G2), where v1, v?1 ?
V (G1)and v2, v?2 ?
V (G2).
Then, we havehv1,v2 + hv?1,v?2 ?
he1,e2 ?
1 (10)hv1,v2 ?
he1,e2 ;hv?1,v?2 ?
he1,e2 (11)These constraints define the feasible set for the in-ference problem specified in Equation (1).
This in-ference problem can be formulated as an ILP prob-lem with the objective function from Equation (1):maxh?shsuT?s(x)subject to (6)-(11); ?s;hs ?
{0, 1} (12)This example demonstrates the use of integer linearprogramming to define intermediate representationsincorporating domain intuition.5 ExperimentsWe applied our framework to three different NLPtasks: transliteration discovery (Klementiev andRoth, 2008), RTE (Dagan et al, 2006), and para-phrase identification (Dolan et al, 2004).Our experiments are designed to answer the fol-lowing research question: ?Given a binary classifi-cation problem defined over latent representations,will the joint LCLR algorithm perform better than atwo-stage approach??
To ensure a fair comparison,both systems use the same feature functions and def-inition of intermediate representation.
We use thesame ILP formulation in both configurations, with asingle exception ?
the objective function parameters:the two stage approach uses a task-specific heuristic,while LCLR learns it iteratively.The ILP formulation results in very strong twostage systems.
For example, in the paraphrase iden-tification task, even our two stage system is the cur-rent state-of-the-art performance.
In these settings,the improvement obtained by our joint approach isnon-trivial and can be clearly attributed to the su-periority of the joint learning algorithm.
Interest-ingly, we find that our more general approach is bet-ter than specially designed joint approaches (Gold-wasser and Roth, 2008b; Das and Smith, 2009).Since the objective function (3) of the joint ap-proach is not convex, a good initialization is re-quired.
We use the weight vector learned by the two433stage approach as the starting point for the joint ap-proach.
The algorithm terminates when the relativeimprovement of the objective is smaller than 10?5.5.1 Transliteration DiscoveryTransliteration discovery is the problem of iden-tifying if a word pair, possibly written using twodifferent character sets, refers to the same underly-ing entity.
The intermediate representation consistsof all possible character mappings between the twocharacter sets.
Identifying this mapping is not easy,as most writing systems do not perfectly align pho-netically and orthographically; rather, this mappingcan be context-dependent and ambiguous.For an input pair of words (w1, w2), the interme-diate structure h is a mapping between their charac-ters, with the latent variable hij indicating if the ithcharacter in w1 is aligned to the jth character in w2.The feature vector associated with the variable hijcontains unigram character mapping, bigram char-acter mapping (by considering surrounding charac-ters).
We adopt the one-to-one mapping and non-crossing constraint used in (Chang et al, 2009).We evaluated our system using the English-Hebrew corpus (Goldwasser and Roth, 2008a),which consists of 250 positive transliteration pairsfor training, and 300 pairs for testing.
As negativeexamples for training, we sample 10% from randompairings of words from the positive data.
We reporttwo evaluation measurements ?
(1) the Mean Recip-rocal Rank (MRR), which is the average of the mul-tiplicative inverse of the rank of the correct answer,and (2) the accuracy (Acc), which is the percentageof the top rank candidates being correct.We initialized the two stage inference process asdetailed in (Chang et al, 2009) using a Romaniza-tion table to assign uniform weights to prominentcharacter mappings.
This initialization procedureresembles the approach used in (Bergsma and Kon-drak, 2007).
An alignment is first built by solvingthe constrained optimization problem.
Then, a sup-port vector machine with squared-hinge loss func-tion is used to train a classifier using features ex-tracted from the alignment.
We refer to this twostage approach as Alignment+Learning.The results summarized in Table 1 show the sig-nificant improvement obtained by the joint approach(95.4% MRR) compared to the two stage approachTransliteration System Acc MRR(Goldwasser and Roth,2008b)N/A 89.4Alignment + Learning 80.0 85.7LCLR 92.3 95.4Table 1: Experimental results for transliteration.
We comparea two-stage system: ?Alignment+Learning?
with LCLR, ourjoint algorithm.
Both ?Alignment+Learning?
and LCLR usethe same features and the same intermediate representation def-inition.(85.7%).
Moreover, LCLR outperforms the jointsystem introduced in (Goldwasser and Roth, 2008b).5.2 Textual EntailmentRecognizing Textual Entailment (RTE) is an im-portant textual inference task of predicting if a giventext snippet, entails the meaning of another (the hy-pothesis).
In many current RTE systems, the entail-ment decision depends on successfully aligning theconstituents of the text and hypothesis, accountingfor the internal linguistic structure of the input.The raw input ?
the text and hypothesis ?
arerepresented as directed acyclic graphs, where ver-tices correspond to words.
Directed edges link verbsto the head words of semantic role labeling argu-ments produced by (Punyakanok et al, 2008).
Allother words are connected by dependency edges.The intermediate representation is an alignment be-tween the nodes and edges of the graphs.
We usedthree hidden variable types from Section 4 ?
word-mapping, word-deletion and edge-mapping, alongwith the associated constraints as defined earlier.Since the text is typically much longer than the hy-pothesis, we create word-deletion latent variables(and features) only for the hypothesis.The second column of Table 2 lists the resourcesused to generate features corresponding to each hid-den variable type.
For word-mapping variables, thefeatures include a WordNet based metric (WNSim),indicators for the POS tags and negation identifiers.We used the state-of-the-art coreference resolutionsystem of (Bengtson and Roth, 2008) to identify thecanonical entities for pronouns and extract featuresaccordingly.
For word deletion, we use only the POStags of the corresponding tokens (generated by theLBJ POS tagger3) to generate features.
For edge3http://L2R.cs.uiuc.edu/?cogcomp/software.php434Hidden RTE ParaphraseVariable features featuresword-mapping WordNet, POS,Coref, NegWordNet, POS,NE, EDword-deletion POS POS, NEedge-mapping NODE-INFO NODE-INFO,DEPedge-deletion N/A DEPTable 2: Summary of latent variables and feature resources forthe entailment and paraphrase identification tasks.
See Section4 for an explanation of the hidden variable types.
The linguisticresources used to generate features are abbreviated as follows ?POS: Part of speech, Coref: Canonical coreferent entities; NE:Named Entity, ED: Edit distance, Neg: Negation markers, DEP:Dependency labels, NODE-INFO: corresponding node align-ment resources, N/A: Hidden variable not used.Entailment System AccMedian of TAC 2009 systems 61.5Alignment + Learning 65.0LCLR 66.8Table 3: Experimental results for recognizing textual entail-ment.
The first row is the median of best performing systems ofall teams that participated in the RTE5 challenge (Bentivogli etal., 2009).
Alignment + Learning is our two-stage system im-plementation, and LCLR is our joint learning algorithm.
Detailsabout these systems are provided in the text.mapping variables, we include the features of thecorresponding word mapping variables, scaled bythe word similarity of the words forming the edge.We evaluated our system using the RTE-5data (Bentivogli et al, 2009), consisting of 600 sen-tence pairs for training and testing respectively, inwhich positive and negative examples are equallydistributed.
In these experiments the joint LCLR al-gorithm converged after 5 iterations.For the two stage system, we used WN-Sim to score alignments during inference.
Theword-based scores influence the edge variablesvia the constraints.
This two-stage system (theAlignment+Learning system) is significantly betterthan the median performance of the RTE-5 submis-sions.
Using LCLR further improves the result by al-most 2%, a substantial improvement in this domain.5.3 Paraphrase IdentificationOur final task is Paraphrase Identification, dis-cussed in detail at Section 4.
We use all the fourhidden variable types described in that section.
Thefeatures used are similar to those described earlierParaphrase System AccExperiments using (Dolan et al, 2004)(Qiu et al, 2006) 72.00(Das and Smith, 2009) 73.86(Wan et al, 2006) 75.60Alignment + Learning 76.23LCLR 76.41Experiments using Extended data setAlignment + Learning 72.00LCLR 72.75Table 4: Experimental Result For Paraphrasing Identification.Our joint LCLR approach achieves the best results comparedto several previously published systems, and our own two stagesystem implementation (Alignment + Learning).
We evaluatedthe systems performance across two datasets: (Dolan et al,2004) dataset and the Extended dataset, see the text for details.Note that LCLR outperforms (Das and Smith, 2009), which is aspecifically designed joint approach for this task.for the RTE system and are summarized in Table 2.We used the MSR paraphrase dataset of (Dolanet al, 2004) for empirical evaluation.
Additionally,we generated a second corpus (called the Extendeddataset) by sampling 500 sentence pairs from theMSR dataset for training and using the entire testcollection of the original dataset.
In the Extendeddataset, for every sentence pair, we extended thelonger sentence by concatenating it with itself.
Thisresults in a more difficult inference problem becauseit allows more mappings between words.
Note thatthe performance on the original dataset sets the ceil-ing on the second one.The results are summarized in Table 4.
The firstpart of the table compares the LCLR system witha two stage system (Alignment + Learning) andthree published results that use the MSR dataset.
(We only list single systems in the table4) Inter-estingly, although still outperformed by our jointLCLR algorithm, the two stage system is able per-form significantly better than existing systems forthat dataset (Qiu et al, 2006; Das and Smith, 2009;Wan et al, 2006).
We attribute this improvement,consistent across both the ILP based systems, to theintermediate representation we defined.We hypothesize that the similarity in performancebetween the joint LCLR algorithm and the two stage4Previous work (Das and Smith, 2009) has shown that com-bining the results of several systems improves performance.435(Alignment + Learning) systems is due to the limitedintermediate representation space for input pairs inthis dataset.
We evaluated these systems on the moredifficult Extended dataset.
Results indeed show thatthe margin between the two systems increases as theinference problem becomes harder.6 Related WorkRecent NLP research has largely focused on two-stage approaches.
Examples include RTE (Zanzottoand Moschitti, 2006; MacCartney et al, 2008; Rothet al, 2009); string matching (Bergsma and Kon-drak, 2007); transliteration (Klementiev and Roth,2008); and paraphrase identification (Qiu et al,2006; Wan et al, 2006).
(MacCartney et al, 2008) considered construct-ing a latent representation to be an independent taskand used manually labeled alignment data (Brockett,2007) to tune the inference procedure parameters.While this method identifies alignments well, it doesnot improve entailment decisions.
This strengthensour intuition that the latent representation should beguided by the final task.There are several exceptions to the two-stage ap-proach in the NLP community (Haghighi et al,2005; McCallum et al, 2005; Goldwasser and Roth,2008b; Das and Smith, 2009); however, the interme-diate representation and the inference for construct-ing it are closely coupled with the application task.In contrast, LCLR provides a general formulationthat allows the use of expressive constraints, mak-ing it applicable to many NLP tasks.Unlike other latent variable SVM frameworks(Felzenszwalb et al, 2009; Yu and Joachims, 2009)which often use task-specific inference procedure,LCLR utilizes the declarative inference frameworkthat allows using constraints over intermediate rep-resentation and provides a general platform for awide range of NLP tasks.The optimization procedure in this work and(Felzenszwalb et al, 2009) are quite different.We use the coordinate descent and cutting-planemethods ensuring we have fewer parameters andthe inference procedure can be easily parallelized.Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-gorithm to define a language model.
Unfortunately,their implementation is not guaranteed to converge.In CRF-like models with latent variables (McCal-lum et al, 2005), the decision function marginal-izes over the all hidden states when presented withan input example.
Unfortunately, the computationalcost of applying their framework is prohibitive withconstrained latent representations.
In contrast, ourframework requires only the best hidden representa-tion instead of marginalizing over all possible repre-sentations, thus reducing the computational effort.7 ConclusionWe consider the problem of learning over an inter-mediate representation.
We assume the existence ofa latent structure in the input, relevant to the learn-ing problem, but not accessible to the learning algo-rithm.
Many NLP tasks fall into these settings andeach can consider a different hidden input structure.We propose a unifying thread for the different prob-lems and present a novel framework for Learningover Constrained Latent Representations (LCLR).Our framework can be applied to many different la-tent representations such as parse trees, orthographicmapping and tree alignments.
Our approach con-trasts with existing work in which learning is doneover a fixed representation, as we advocate jointlylearning it with the final task.We successfully apply the proposed framework tothree learning tasks ?
Transliteration, Textual En-tailment and Paraphrase Identification.
Our jointLCLR algorithm achieves superior performance inall three tasks.
We attribute the performance im-provement to our novel training algorithm and flex-ible inference procedure, allowing us to encode do-main knowledge.
This presents an interesting line offuture work in which more linguistic intuitions canbe encoded into the learning problem.
For these rea-sons, we believe that our framework provides an im-portant step forward in understanding the problemof learning over hidden structured inputs.Acknowledgment We thank James Clarke and Mark Sam-mons for their insightful comments.
This research was partly sponsoredby the Army Research Laboratory (ARL) (accomplished under Cooper-ative Agreement Number W911NF-09-2-0053) and by Air Force Re-search Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the author(s) and do not necessarilyreflect the view of the ARL or of AFRL.436ReferencesE.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP.L.
Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, andB.
Magnini.
2009.
The fifth PASCAL recognizingtextual entailment challenge.
In Proc.
of TAC Work-shop.S.
Bergsma and G. Kondrak.
2007.
Alignment-baseddiscriminative string similarity.
In ACL.C.
Brockett.
2007.
Aligning the RTE 2006 corpus.In Technical Report MSR-TR-2007-77, Microsoft Re-search.M.
Chang, D. Goldwasser, D. Roth, and Y. Tu.
2009.Unsupervised constraint driven learning for transliter-ation discovery.
In NAACL.C.
Cherry and C. Quirk.
2008.
Discriminative, syntacticlanguage modeling through latent svms.
In Proc.
ofthe Eighth Conference of AMTA.I.
Dagan, O. Glickman, and B. Magnini, editors.
2006.The PASCAL Recognising Textual Entailment Chal-lenge.D.
Das and N. A. Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.
InACL.W.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Ex-ploiting massively parallel news sources.
In COLING.P.
F. Felzenszwalb, R. B. Girshick, D. McAllester, andD.
Ramanan.
2009.
Object detection with discrimina-tively trained part based models.
IEEE Transactionson Pattern Analysis and Machine Intelligence.D.
Goldwasser and D. Roth.
2008a.
Active sample se-lection for named entity transliteration.
In ACL.
ShortPaper.D.
Goldwasser and D. Roth.
2008b.
Transliteration asconstrained optimization.
In EMNLP.A.
Haghighi, A. Ng, and C. Manning.
2005.
Robusttextual inference via graph matching.
In HLT-EMNLP.C.-J.
Hsieh, K.-W. Chang, C.-J.
Lin, S. S. Keerthi, andS.
Sundararajan.
2008.
A dual coordinate descentmethod for large-scale linear svm.
In ICML.T.
Joachims, T. Finley, and Chun-Nam Yu.
2009.Cutting-plane training of structural svms.
MachineLearning.D.
Klein and C. D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
InNIPS.A.
Klementiev and D. Roth.
2008.
Named entity translit-eration and discovery in multilingual corpora.
InCyril Goutte, Nicola Cancedda, Marc Dymetman, andGeorge Foster, editors, Learning Machine Translation.B.
MacCartney, M. Galley, and C. D. Manning.
2008.A phrase-based alignment model for natural languageinference.
In EMNLP.A.
McCallum, K. Bellare, and F. Pereira.
2005.
A condi-tional random field for discriminatively-trained finite-state string edit distance.
In UAI.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics.L.
Qiu, M.-Y.
Kan, and T.-S. Chua.
2006.
Paraphraserecognition via dissimilarity significance classifica-tion.
In EMNLP.C.
Quirk, C. Brockett, and W. Dolan.
2004.
Monolin-gual machine translation for paraphrase generation.
InEMNLP.D.
Roth, M. Sammons, and V.G.
Vydiswaran.
2009.
Aframework for entailed relation recognition.
In ACL.S.
Wan, M. Dras, R. Dale, and C. Paris.
2006.
Usingdependency-based features to take the p?ara-farceo?utof paraphrase.
In Proc.
of the Australasian LanguageTechnology Workshop (ALTW).C.
Yu and T. Joachims.
2009.
Learning structural svmswith latent variables.
In ICML.F.
M. Zanzotto and A. Moschitti.
2006.
Automatic learn-ing of textual entailments with cross-pair similarities.In ACL.437
