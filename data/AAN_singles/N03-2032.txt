Latent Semantic Analysis for dialogue act classificationRiccardo SerafinComputer ScienceUniversity of IllinoisChicago, IL, USArseraf1@uic.eduBarbara Di EugenioComputer ScienceUniversity of IllinoisChicago, IL, USAbdieugen@uic.eduMichael GlassMathematics and Computer ScienceValparaiso UniversityValparaiso, IN, USAMichael.Glass@valpo.eduAbstractThis paper presents our experiments in apply-ing Latent Semantic Analysis (LSA) to dia-logue act classification.
We employ both LSAproper and LSA augmented in two ways.
Wereport results on DIAG, our own corpus of tu-toring dialogues, and on the CallHome Spanishcorpus.
Our work has the theoretical goal of as-sessing whether LSA, an approach based onlyon raw text, can be improved by using addi-tional features of the text.1 IntroductionDialogue systems need to perform dialog act classifica-tion, in order to understand the role the user?s utteranceplays in the dialog (e.g., a question for information or arequest to perform an action), and to generate an appro-priate next turn.
In recent years, a variety of empiricaltechniques have been used to train the dialogue act clas-sifier (Reithinger and Maier, 1995; Stolcke et al, 2000;Walker et al, 2001).In this paper, we propose Latent Semantic Analysis(LSA) as a method to train the dialogue act classifier.LSA can be thought as representing the meaning of aword as a kind of average of the meanings of all the pas-sages in which it appears, and the meaning of a passageas a kind of average of the meaning of all the words itcontains (Landauer et al, 1998).
LSA learns from co-occurrence of words in collections of texts.
It builds a se-mantic space where words and passages are representedas vectors.
Their similarity is measured by the cosine oftheir contained angle in the semantic space.
LSA is basedon Single Value Decomposition (SVD), a mathematicaltechnique that causes the semantic space to be arrangedso as to reflect the major associative patterns in the data,and ignores the smaller, less important influences.LSA has been successfully applied to many tasks: e.g,to assess the quality of student essays (Foltz et al, 1999)and to interpret the student?s input in an Intelligent Tutor-ing system (Graesser et al, 2000).
However, there is noresearch on applying LSA to dialogue act classification.LSA is an attractive method because it is relativelystraightforward to train and use.
More importantly, al-though it is a statistical theory, it has been shown to mimica number of aspects of human competence / performance(Landauer et al, 1998).
Thus, it appears to somehow cap-ture and represent important components of meanings.We also have a theoretical goal in investigating LSA.A common criticism of LSA is that its ?bag of words?
ap-proach ignores any other linguistic information that maybe available, e.g.
order and syntactic information: toLSA, man bites dog is identical to dog bites man.
Wesuggest that an LSA semantic space can be built from theco-occurrence of arbitrary textual features.
We proposeto place in the bag of words other features that co-occurin the same text.
We are calling LSA augmented withfeatures ?FLSA?
(for ?feature LSA?).
The only relevantprior work is (Wiemer-Hastings, 2001), that adds part ofspeech tags and some syntactic information to LSA.This paper describes the corpora and the methods weused, and the results we obtained.
To summarize, plainLSA seems to perform well on large corpora and classi-fication tasks.
Augmented LSA seems to perform betteron smaller corpora and target classifications.2 CorporaWe report experiments on two corpora, DIAG and Span-ish CallHome.DIAG is a corpus of computer mediated tutoring dia-logues between a tutor and a student who is diagnosing afault in a mechanical system with the DIAG tutoring sys-tem (Towne, 1997).
The student?s input is via menu, thetutor is in a different room and answers via a text window.The DIAG corpus comprises 23 dialogues for a total of607 different words and 660 dialogue acts.
It has been an-notated for a variety of features, including four dialogueacts1 (Glass et al, 2002): problem solving, the tutor givesproblem solving directions; judgement, the tutor evalu-ates the student?s actions or diagnosis; domain knowl-edge, the tutor imparts domain knowledge; and other,when none of the previous three applies.The Spanish CallHome corpus (Levin et al, 1998;Ries, 1999) comprises 128 unrestricted phone calls inSpanish, for a total of 12066 different words and 44628dialogue acts.
The Spanish CallHome annotation aug-ments a basic tag such as statement along several dimen-sions, such as whether the statement describes a psycho-logical state of the speaker.
This results in 232 differ-ent dialogue act tags, many with very low frequencies.In this sort of situations, tag categories are often col-lapsed when running experiments so as to get meaningfulfrequencies (Stolcke et al, 2000).
In CallHome37, wecollapsed statements and backchannels, obtaining 37 dif-ferent tags.
CallHome37 maintains some subcategoriza-tions, e.g.
whether a question is yes/no or rhetorical.
InCallHome10, we further collapse these categories.
Call-Home10 is reduced to 8 dialogue acts proper (eg state-ment, question, answer) plus the two tags ??%??
forabandoned sentences and ??x??
for noise.3 MethodsWe have experimented with four methods: LSA proper,which we call plain LSA; two versions of clustered LSA,in which we ?cluster?
the document dimension in theWord-Document matrix; FLSA, in which we incorporatefeatures other than words to train LSA (specifically, weused the preceding n dialogue acts).Plain LSA.
The input to LSA is a Word-Document ma-trix with a row for each word, and a column for eachdocument (for us, a document is a unit such as a sen-tence or paragraph tagged with a dialogue act).
Cellc(i; j) contains the frequency with which wordiappearsin documentj.
Clearly, this w*d matrix will be verysparse.
Next, LSA applies SVD to the Word-Documentmatrix, obtaining a representation of each document in ak dimensional space: crucially, k is much smaller than thedimension of the original space.
As a result, words thatdid not appear in certain documents now appear, as anestimate of their correlation to the meaning of those doc-uments.
The number of dimensions k retained by LSAis an empirical question.
The results we report below arefor the best k we experimented with.To choose the best tag for a document in the test set, wecompare the vector representing the new document withthe vector of each document in the training set.
The tag of1They should be more appropriately termed tutor moves.the document which has the highest cosine with our testvector is assigned to the new document.Clustered LSA.
Instead of building the Word-Document matrix we build a Word-Tag matrix, where thecolumns refer to all the possible dialog act types (tags).The cell c(i; j) will tell us how many times wordiisused in documents that have tagj.
The Word-Tag matrixis w*t instead of w*d. We then apply Plain LSA to theWord-Tag matrix.SemiClustered LSA.
In Clustered LSA we lose thedistribution of words in the documents.
Moreover, if thenumber of tags is small, such as for DIAG, SVD loses itsmeaning.
SemiClustered LSA tries to remedy these prob-lems.
We still produce the k-dimensional space apply-ing SVD to the Word-Document matrix.
We then reducethe Word-Tag matrix to the k dimensional space using atransformation based on the SVD of the Word-Documentmatrix.
Note that both Clustered and SemiClustered LSAare much faster at test time than plain LSA, as the testdocument needs to be compared only with t tag vectors,rather than with d document vectors (t << d).Feature LSA (FLSA).
We add extra features to plainLSA.
Specifically, we have experimented with the se-quence of the previous n dialogue acts.
We computethe input WordTag-Document matrix by computing theWord-Document matrix, computing the Tag-Documentmatrix and then concatenating them vertically to get the(w+t)*d final matrix.
Otherwise, the method is the sameas Plain LSA.4 ResultsTable 1 reports the best results we obtained for each cor-pus and method.
In parentheses, we include the k di-mension, and, for FLSA, the number of previous tags weconsidered.In all cases, we can see that Plain LSA performs muchbetter than baseline, where baseline is computed as pick-ing the most frequent dialogue act in each corpus.
Asconcerns DIAG, we can also see that SemiClustered LSAimproves on Plain LSA by 3%, but no other method does.As regards CallHome, first, the results with plain LSAare comparable to published ones, even if the comparisonis not straightforward, because it is often unclear whatthe target classification and features used are.
For exam-ple, (Ries, 1999) reports 76.2% accuracy by using neuralnetworks augmented with the sequence of the n previousspeech acts.
However, (Ries, 1999) does not mention thetarget classification; the reported baseline appears com-patible with both CallHome37 and CallHome10.
Thetraining features in (Ries, 1999) include part-of-speech(POS) tags for words, which we do not have.
This mayCorpus Plain Clustered SemiClustered FLSADiag (43.64%) 75.73% (50) 71.91% (3) 78.78% (50) 74.26% (1,150)CallHome37 (42.69%) 65.36% (50) 22.08% (10) 31.39% (300) 62.59% (1, 50)CallHome10 (42.69%) 68.91% (25) 61.64% (5) 58.38% (300) 66.57% (1, 100)Table 1: Result Summaryexplain the higher performance.
Including POS tags intoour FLSA method is left for future work.No variation on LSA performs better than plain LSA inour CallHome experiments.
In fact, clustered and semi-clustered LSA perform vastly worse on the larger clas-sification problem in CallHome37.
It appears that, thesmaller the corpus and target classification are, the betterclustered and semiclustered LSA perform.
In fact, semi-clustered LSA outperforms plain LSA on DIAG.Our experiments with FLSA do not support the hy-pothesis that adding features different from words to LSAhelps with classification.
(Wiemer-Hastings, 2001) re-ports mixed results when augmenting LSA: adding POStags did not improve performance, but adding some syn-tactic information did.
Note that, in our experiments,adding more than one previous speech act did not help.5 Future workOur experiments show that LSA can be effectively usedto train a dialogue act classifier.
On the whole, plain LSAappears to perform well.
Even if our experiments withextensions to plain LSA were mostly unsuccessful, theyare not sufficient to conclude that plain LSA cannot beimproved.
Thus, we will pursue the following directions.1) Further investigate the correlation of the performanceof (semi)clustered LSA with the size of the corpus and /or of the target classification.
2) Include other features inFLSA, e.g.
syntactic roles.
3) Redo our experiments onother corpora, such as Map Task (Carletta et al, 1997).Map Task is appropriate because besides dialogue acts itis annotated for syntactic information, while CallHomeis not.
4) Experiment with FLSA on other tasks, such asassessing text coherence.AcknowledgementsThis work is supported by grant N00014-00-1-0640 from theOffice of Naval Research.ReferencesJ.
Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-Sneddon, and A. H. Anderson.
1997.
The reliabilityof a dialogue structure coding scheme.
ComputationalLinguistics, 23(1):13?31.P.
W. Foltz, D. Laham, and T. K. Landauer.
1999.
Theintelligent essay assessor: Applications to educationaltechnology.
Interactive Multimedia Electronic Journalof Computer-Enhanced Learning, 1(2).M.
Glass, H. Raval, B.
Di Eugenio, and M. Traat.
2002.The DIAG-NLP dialogues: coding manual.
TechnicalReport UIC-CS 02-03, University of Illinois - Chicago.A.
C. Graesser, K. Wiemer-Hastings, P. Wiemer-Hastings, R. Kreuz, and the Tutoring Research Group.2000.
Autotutor: A simulation of a human tutor.
Jour-nal of Cognitive Systems Research.T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.
Intro-duction to Latent Semantic Analysis.
Discourse Pro-cesses, 25:259?284.L.
Levin, A. Thyme?-Gobbel, A. Lavie, K. Ries, and K.Zechner.
1998.
A discourse coding scheme for con-versational Spanish.
In Proceedings ICSLP.N.
Reithinger and E. Maier.
1995.
Utilizing statisticaldialogue act processing in Verbmobil.
In ACL95, Pro-ceedings of the 33rd Annual Meeting of the Associationfor Computational Linguistics.K.
Ries.
1999.
HMM and Neural Network Based SpeechAct Detection.
In Proceedings of ICASSP 99.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema,and M. Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.D.
M. Towne.
1997.
Approximate reasoning techniquesfor intelligent diagnostic instruction.
InternationalJournal of Artificial Intelligence in Education,8.M.
A. Walker, R. Passonneau, and J. E. Boland.
2001.Qualitative and quantitative evaluation of DARPAcommunicator dialogue systems.
In ACL01, Proceed-ings of the 39th Annual Meeting of the Association forComputational Linguistics.P.
Wiemer-Hastings.
2001.
Rules for syntax, vectors forsemantics.
In CogSci01, Proceedings of the Twenty-Third Annual Meeting of the Cognitive Science Society.
