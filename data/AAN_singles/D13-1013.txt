Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 124?129,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsJoint Parsing and Disfluency Detection in Linear TimeMohammad Sadegh Rasooli?Department of Computer ScienceColumbia University, New York, NYrasooli@cs.columbia.eduJoel TetreaultNuance Communications, Inc.Sunnyvale, CAjoel.tetreault@nuance.comAbstractWe introduce a novel method to jointly parseand detect disfluencies in spoken utterances.Our model can use arbitrary features for pars-ing sentences and adapt itself with out-of-domain data.
We show that our method, basedon transition-based parsing, performs at a highlevel of accuracy for both the parsing anddisfluency detection tasks.
Additionally, ourmethod is the fastest for the joint task, runningin linear time.1 IntroductionDetecting disfluencies in spontaneous speech hasbeen widely studied by researchers in different com-munities including natural language processing (e.g.Qian and Liu (2013)), speech processing (e.g.
Wanget al(2013)) and psycholinguistics (e.g.
Finlaysonand Corley (2012)).
While the percentage of spo-ken words which are disfluent is typically not morethan ten percent (Bortfeld et al 2001), this addi-tional ?noise?
makes it much harder for spoken lan-guage systems to predict the correct structure of thesentence.Disfluencies can be filled pauses (e.g.
?uh?, ?um?,?huh?
), discourse markers (e.g.
?you know?, ?Imean?)
or edited words which are repeated or cor-rected by the speaker.
For example, in the follow-ing sentence, an edited phrase or reparandum inter-val (?to Boston?)
occurs with its repair (?to Den-ver?
), a filled pause (?uh?)
and discourse marker (?I?
The first author worked on this project while he was aresearch intern in CoreNL research group, NLU lab, NuanceCommunications, Sunnyvale, CA.mean?
).1I want a flight to Boston?
??
?ReparandumInterregnum?
??
?uh???
?FPI mean?
??
?DMto Denver?
??
?RepairFilled pauses and discourse markers are to someextent a fixed and closed set.
The main challengein finding disfluencies is the case where the editedphrase is neither a rough copy of its repair or has anyrepair phrase (i.e.
discarded edited phrase).
Hence,in previous work, researchers report their methodperformance on detecting edited phrases (reparan-dum) (Johnson and Charniak, 2004).In contrast to most previous work which focusessolely on either detection or on parsing, we intro-duce a novel framework for jointly parsing sentenceswith disfluencies.
To our knowledge, our work isthe first model that is based on joint dependency anddisfluency detection.
We show that our model is ro-bust enough to detect disfluencies with high accu-racy, while still maintaining a high level of depen-dency parsing accuracy that approaches the upperbound.
Additionally, our model outperforms priorwork on joint parsing and disfluency detection onthe disfluency detection task, and improves upon thisprior work by running in linear time complexity.The remainder of this paper is as follows.
In ?2,we overview some the previous work on disfluencydetection.
?3 describes our model.
Experiments aredescribed in ?4 and Conclusions are made in ?5.1In the literature, edited words are also known as ?reparan-dum?, and the fillers are known as ?interregnum?.
Filled pausesare also called ?Interjections?.1242 Related WorkDisfluency detection approaches can be divided intotwo different groups: text-first and speech first(Nakatani and Hirschberg, 1993).
In the first ap-proach, all prosodic and acoustic cues are ignoredwhile in the second approach both grammatical andacoustic features are considered.
For this paper, wefocus on developing a text-first approach but ourmodel is easily flexible with speech-first features be-cause there is no restriction on the number and typesof features in our model.Among text-first approaches, the work is splitbetween developing systems which focus specifi-cally on disfluency detection and those which coupledisfluency detection with parsing.
For the former,Charniak and Johnson (2001) employ a linear clas-sifier to predict the edited phrases in Switchboardcorpus (Godfrey et al 1992).
Johnson and Char-niak (2004) use a TAG-based noisy channel modelto detect disfluencies while parsing with getting n-best parses from each sentence and re-ranking witha language model.
The original TAG parser is notused for parsing itself and it is used just to findrough copies in the sentence.
Their method achievespromising results on detecting edited words but atthe expense of speed (the parser has a complexity ofO(N5).
Kahn et al(2005) use the same TAG modeland add semi-automatically extracted prosodic fea-tures.
Zwarts and Johnson (2011) improve the per-formance of TAG model by adding external lan-guage modeling information from data sets such asGigaword in addition to using minimal expected F-loss in n-best re-ranking.Georgila (2009) uses integer linear programmingcombined with CRF for learning disfluencies.
Thatwork shows that ILP can learn local and global con-straints to improve the performance significantly.Qian and Liu (2013) achieve the best performanceon the Switchboard corpus (Godfrey et al 1992)without any additional data.
They use three steps fordetecting disfluencies using weighted Max-MarginMarkov (M3) network: detecting fillers, detectingedited words, and refining errors in previous steps.Some text-first approaches treat parsing and dis-fluency detection jointly, though the models differin the type of parse formalism employed.
Lease andJohnson (2006) use a PCFG-based parser to parsesentences along with finding edited phrases.
Millerand Schuler (2008) use a right-corner transform ofbinary branching structures on bracketed sentencesbut their results are much worse than (Johnson andCharniak, 2004).
To date, none of the prior joint ap-proaches have used a dependency formalism.3 Joint Parsing ModelWe model the problem using a deterministictransition-based parser (Nivre, 2008).
These parsershave the advantage of being very accurate while be-ing able to parse a sentence in linear time.
An ad-ditional advantage is that they can use as many non-local and local features as needed.Arc-Eager Algorithm We use the arc-eager algo-rithm (Nivre, 2004) which is a bottom-up parsingstrategy that is used in greedy and k-beam transition-based parsers.
One advantage of this strategy is thatthe words can get a head from their left side, beforegetting right dependents.
This is particularly bene-ficial for our task, since we know that reparanda aresimilar to their repairs.
Hence, a reparandum mayget its head but whenever the parser faces a repair, itremoves the reparandum from the sentence and con-tinues its actions.The actions in an arc-eager parsing algorithm are:?
Left-arc (LA): The first word in the buffer be-comes the head of the top word in the stack.The top word is popped after this action.?
Right-arc (RA): The top word in the stack be-comes the head of the first word in the buffer.?
Reduce (R): The top word in the stack ispopped.?
Shift (SH): The first word in the buffer goes tothe top of the stack.Joint Parsing and Disfluency Detection We firstextend the arc-eager algorithm by augmenting theaction space with three new actions:?
Reparandum (Rp[i:j]): treat a phrase (words ito j) outside the look-ahead buffer as a reparan-dum.
Remove them from the sentence and cleartheir dependencies.?
Discourse Marker (Prn[i]): treat a phrase inthe look-ahead buffer (first i words) as a dis-course marker and remove them from the sen-tence.125Stack Buffer Act.flight to Boston uh I mean ... RAflight to Boston uh I mean to ... RAflight to Boston uh I mean to Denver Intj[1]flight to Boston I mean to Denver Prn[1]flight to Boston to Denver RP[2:3]flight to Denver RAflight to Denver RAflight to Denver Rflight to Rflight RFigure 1: A sample transition sequence for the sentence?flight to Boston uh I mean to Denver?.
In the third col-umn, only the underlined parse actions are learned by theparser (second classifier).
The first classifier uses all in-stances for training (learns fluent words with ?regular?label).?
Interjection (Intj[i]): treat a phrase in thelook-ahead buffer (first i words) as a filledpause and remove them from the sentence.2Our model has two classifiers.
The first classi-fier decides between four possible actions and pos-sible candidates in the current configuration of thesentence.
These actions are the three new onesfrom above and a new action Regular (Reg): whichmeans do one of the original arc-eager parser ac-tions.At each configuration, there might be several can-didates for being a prn, intj or reparandum, andone regular candidate.
The candidates for beinga reparandum are a set of words outside the look-ahead buffer and the candidates for being an intj orprn are a set of words beginning from the head ofthe look-ahead buffer.
If the parser decides regularas the correct action, the second classifier predictsthe best parsing transition, based on arc-eager pars-ing (Nivre, 2004).For example, in the 4th state in Figure 1, there aremultiple candidates for the first classifier: regular,?I?
as prn[1] or intj[1], ?I mean?
as prn[2] or intj[2],?I mean to?
as prn[3] or intj[3], ?I mean to Denver?as prn[4] or intj[4], ?Boston?
as rp[3:3], ?to Boston?as rp[2:3], and ?flight to Boston?
as rp[1:3].2In the bracketed version of Switchboard corpus, reparan-dum is tagged with EDITED and discourse markers and pausedfillers are tagged as PRN and INTJ respectively.Training A transition-based parser action (oursecond-level classifier) is sensitive to the words inthe buffer and stack.
The problem is that we do nothave gold dependencies for edited words in our data.Therefore, we need a parser to remove reparandumwords from the buffer and push them into the stack.Since our parser cannot be trained on disfluent sen-tences from scratch, the first step is to train it onclean treebank data.In the second step, we adapt parser weights bytraining it on disfluent sentences.
Our assumptionis that we do not know the correct dependencies be-tween disfluent words and other words in the sen-tence.
At each configuration, the parser updates it-self with new instances by traversing all configura-tions in the sentences.
In this case, if at the head ofthe buffer there is an intj or prn tag, the parser allowsthem to be removed from the buffer.
If a reparan-dum word is not completely outside the buffer (thefirst two states in Figure 1), the parser decides be-tween the four regular arc-eager actions (i.e.
left-arc, right-arc, shift, and reduce).
If the last wordpushed into the stack is a reparandum and the firstword in the buffer is a regular word, the parser re-moves all reparanda at the same level (in the case ofnested edited words), removes their dependencies toother words and push their dependents into the stack.Otherwise, the parser performs the oracle action andadds that action as its new instance.3With an adapted parser which is our second-levelclassifier, we can train our first-level classifier.
Thesame procedure repeats, except that instances fordisfluency detection are used for updating param-eter weights for the first classifier for deciding theactions.
In Figure 1, only the oracle actions (under-lined) are added to the instances for updating parserweights but all first-level actions are learned by thefirst level classifier.4 Experiments and EvaluationFor our experiments, we use the Switchboard corpus(Godfrey et al 1992) with the same train/dev/testsplit as Johnson and Charniak (2004).
As in that3The reason that we use a parser instead of expanding allpossible transitions for an edited word is that, the number of reg-ular actions will increase and the other actions become sparserthan natural.126work, incomplete words and punctuations are re-moved from data (except that we do not remove in-complete words that are not disfluent4) and all wordsare turned into lower-case.
The main difference withprevious work is that we use Switchboard mrg filesfor training and testing our model (since they con-tain parse trees) instead of the more commonly usedSwithboard dps text files.
Mrg files are a subset ofdps files with about more than half of their size.Unfortunately, the disfluencies marked in the dpsfiles are not exactly the same as those marked inthe corresponding mrg files.
Hence, our result is notcompletely comparable to previous work except for(Kahn et al 2005; Lease and Johnson, 2006; Millerand Schuler, 2008).We use Tsurgeon (Levy and Andrew, 2006) forextracting sentences from mrg files and use thePenn2Malt tool5 to convert them to dependencies.Afterwards, we provide dependency trees with dis-fluent words being the dependent of nothing.Learning For the first classifier, we use averagedstructured Perceptron (AP) (Collins, 2002) with aminor modification.
Since the first classifier data isheavily biased towards the ?regular label?, we mod-ify the weight updates in the original algorithm to 2(original is 1) for the cases where a ?reparandum?is wrongly recognized as another label.
We callthe modified version ?weighted averaged Perceptron(WAP)?.
We see that this simple modification im-proves the model accuracy.6 For the second classi-fier (parser), we use the original averaged structuredPerceptron algorithm.
We report results on both APand WAP versions of the parser.Features Since for every state in the parser config-uration, there are many candidates for being disflu-ent; we use local features as well as global featuresfor the first classifier.
Global features are mostlyuseful for discriminating between the four actionsand local features are mostly useful for choosing aphrase as a candidate for being a disfluent phrase.The features are described in Figure 2.
For the sec-ond classifier, we use the same features as (Zhangand Nivre, 2011, Table 1) except that we train our4E.g.
I want t- go to school.5http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html6This is similar to WM3N in (Qian and Liu, 2013).Global FeaturesFirst n words inside/outside buffer (n=1:4)First n POS i/o buffer (n=1:6)Are n words i/o buffer equal?
(n=1:4)Are n POS i/o buffer equal?
(n=1:4)n last FG transitions (n=1:5)n last transitions (n=1:5)n last FG transitions + first POS in the buffer (n=1:5)n last transitions + first POS in the buffer (n=1:5)(n+m)-gram of m/n POS i/o buffer (n,m=1:4)Refined (n+m)-gram of m/n POS i/o buffer (n,m=1:4)Are n first words of i/o buffer equal?
(n=1:4)Are n first POS of i/o buffer equal?
(n=1:4)Number of common words i/o buffer words (n=1:6)Local FeaturesFirst n words of the candidate phrase (n=1:4)First n POS of the candidate phrase (n=1:6)Distance between the candidate and first word in the bufferFigure 2: Features used for learning the first classifier.Refined n-gram is the n-gram without considering wordsthat are recognized as disfluent.
Fine-grained (FG) tran-sitions are enriched with parse actions (e.g.
?regular:left-arc?
).parser in a similar manner as the MaltParser (Nivreet al 2007) without k-beam training.Parser Evaluation We evaluate our parser withboth unlabeled attachment accuracy of correct wordsand precision and recall of finding the dependenciesof correct words.7 The second classifier is trainedwith 3 iterations in the first step and 3 iterations inthe second step.
We use the attachment accuracyof the parse tree of the correct sentences (withoutdisfluencies) as the upper-bound attachment scoreand parsed tree of the disfluent sentences (withoutdisfluency detection) as our lower-bound attachmentscore.
As we can see in Table 1, WAP does a slightlybetter job parsing sentences.
The upper-bound pars-ing accuracy shows that we do not lose too much in-formation while jointly detecting disfluencies.
Ourparser is not comparable to (Johnson and Charniak,2004) and (Miller and Schuler, 2008), since we usedependency relations for evaluation instead of con-stituencies.Disfluency Detection Evaluation We evaluateour model on detecting edited words in the sentences7The parser is actually trained to do labeled attachment andlabeled accuracy is about 1-1.5% lower than UAS.127UAS LB UB Pr.
Rec.
F2AP 88.6 70.7 90.2 86.8 88.0 87.4WAP 88.1 70.7 90.2 87.2 88.0 87.6Table 1: Parsing results.
UB = upperbound (parsing cleansentences), LB = lowerbound (parsing disfluent sentenceswithout disfluency correction).
UAS is unlabeled attach-ment score (accuracy), Pr.
is precision, Rec.
is recall andF1 is f-score.Pr.
Rec.
F1AP 92.9 71.6 80.9WAP 85.1 77.9 81.4KL (2005) ?
?
78.2LJ (2006) ?
?
62.4MS (2008) ?
?
30.6QL (2013) ?
Default ?
?
81.7QL (2013) ?
Optimized ?
?
82.1Table 2: Disfluency results.
Pr.
is precision, Rec.
is recalland F1 is f-score.
KL = (Kahn et al 2005), LJ = (Leaseand Johnson, 2006), MS = (Miller and Schuler, 2008) andQL = (Qian and Liu, 2013).
(words with ?EDITED?
tag in mrg files).
As wesee in Table 2, WAP works better than the originalmethod.
As mentioned before, the numbers are notcompletely comparable to others except for (Kahnet al 2005; Lease and Johnson, 2006; Miller andSchuler, 2008) which we outperform.
For the sakeof comparing to the state of the art, the best resultfor this task (Qian and Liu, 2013) is replicated fromtheir available software8 on the portion of dps filesthat have corresponding mrg files.
For a fairer com-parison, we also optimized the number of trainingiterations of (Qian and Liu, 2013) for the mrg setbased on dev data (10 iterations instead of 30 iter-ations).
As shown in the results, our model accu-racy is slightly less than the state-of-the-art (whichfocuses solely on the disfluency detection task anddoes no parsing), but we believe that the perfor-mance can be improved through better features andby changing the model.
Another characteristic ofour model is that it operates at a very high precision,though at the expense of some recall.8We use the second version of the code: http://code.google.com/p/disfluency-detection/.
Resultsfrom the first version are 81.4 and 82.1 for the default and opti-mized settings.5 ConclusionIn this paper, we have developed a fast, yet accurate,joint dependency parsing and disfluency detectionmodel.
Such a parser is useful for spoken dialoguesystems which typically encounter disfluent speechand require accurate syntactic structures.
The modelis completely flexible with adding other features (ei-ther text or speech features).There are still many ways of improving thisframework such as using k-beam training and decod-ing, using prosodic and acoustic features, using outof domain data for improving the language and pars-ing models, and merging the two classifiers into onethrough better feature engineering.
It is worth notingthat we put the dummy root word in the first positionof the sentence.
Ballesteros and Nivre (2013) showthat parser accuracy can improve by changing thatposition for English.One of the main challenges in this problem isthat most of the training instances are not disflu-ent and thus the sample space is very sparse.
Asseen in the experiments, we can get further improve-ments by modifying the weight updates in the Per-ceptron learner.
In future work, we will exploredifferent learning algorithms which can help us ad-dress the sparsity problem and improve the modelaccuracy.
Another challenge is related to the parserspeed, since the number of candidates and featuresare much greater than the number used in classicaldependency parsers.Acknowledgements We would like to thankanonymous reviewers for their helpful commentson the paper.
Additionally, we were aided by re-searchers by their prompt responses to our manyquestions: Mark Core, Luciana Ferrer, KallirroiGeorgila, Mark Johnson, Jeremy Kahn, Yang Liu,Xian Qian, Kenji Sagae, and Wen Wang.
Finally,this work was conducted during the first author?ssummer internship at the Nuance Sunnyvale Re-search Lab.
We would like to thank the researchersin the group for the helpful discussions and assis-tance on different aspects of the problem.
In particu-lar, we would like to thank Chris Brew, Ron Kaplan,Deepak Ramachandran and Adwait Ratnaparkhi.128ReferencesMiguel Ballesteros and Joakim Nivre.
2013.
Going tothe roots of dependency parsing.
Computational Lin-guistics, 39(1):5?13.Heather Bortfeld, Silvia D. Leon, Jonathan E. Bloom,Michael F. Schober, and Susan E. Brennan.
2001.Disfluency rates in conversation: Effects of age, re-lationship, topic, role, and gender.
Language andSpeech, 44(2):123?147.Eugene Charniak and Mark Johnson.
2001.
Edit detec-tion and parsing for transcribed speech.
In NAACL-HLT, pages 1?9.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In ACL, pages 1?8.Ian R. Finlayson and Martin Corley.
2012.
Disfluencyin dialogue: an intentional signal from the speaker?Psychonomic bulletin & review, 19(5):921?928.Kallirroi Georgila.
2009.
Using integer linear program-ming for detecting speech disfluencies.
In NAACL-HLT, pages 109?112.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech corpusfor research and development.
In ICASSP, volume 1,pages 517?520.Mark Johnson and Eugene Charniak.
2004.
A tag-basednoisy channel model of speech repairs.
In ACL, pages33?39.Jeremy G. Kahn, Matthew Lease, Eugene Charniak,Mark Johnson, and Mari Ostendorf.
2005.
Effectiveuse of prosody in parsing conversational speech.
InEMNLP, pages 233?240.Matthew Lease and Mark Johnson.
2006.
Early dele-tion of fillers in processing conversational speech.
InNAACL-HLT, pages 73?76.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In LREC, pages 2231?2234.Tim Miller and William Schuler.
2008.
A unified syn-tactic model for parsing fluent and disfluent speech.
InACL-HLT, pages 105?108.Christine Nakatani and Julia Hirschberg.
1993.
Aspeech-first model for repair detection and correction.In ACL, pages 46?53.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In the Workshop on Incremen-tal Parsing: Bringing Engineering and Cognition To-gether, pages 50?57.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4):513?553.Xian Qian and Yang Liu.
2013.
Disfluency detectionusing multi-step stacked learning.
In NAACL-HLT,pages 820?825.Wen Wang, Andreas Stolcke, Jiahong Yuan, and MarkLiberman.
2013.
A cross-language study on auto-matic speech disfluency detection.
In NAACL-HLT,pages 703?708.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InACL (Short Papers), pages 188?193.Simon Zwarts and Mark Johnson.
2011.
The impact oflanguage models and loss functions on repair disflu-ency detection.
In ACL, pages 703?711.129
