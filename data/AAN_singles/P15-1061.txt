Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 626?634,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsClassifying Relations by Ranking with Convolutional Neural NetworksC?
?cero Nogueira dos SantosIBM Research138/146 Av.
PasteurRio de Janeiro, RJ, Brazilcicerons@br.ibm.comBing XiangIBM Watson1101 KitchawanYorktown Heights, NY, USAbingxia@us.ibm.comBowen ZhouIBM Watson1101 KitchawanYorktown Heights, NY, USAzhou@us.ibm.comAbstractRelation classification is an important se-mantic processing task for which state-of-the-art systems still rely on costly hand-crafted features.
In this work we tackle therelation classification task using a convo-lutional neural network that performs clas-sification by ranking (CR-CNN).
We pro-pose a new pairwise ranking loss functionthat makes it easy to reduce the impactof artificial classes.
We perform experi-ments using the the SemEval-2010 Task8 dataset, which is designed for the taskof classifying the relationship between twonominals marked in a sentence.
Using CR-CNN, we outperform the state-of-the-artfor this dataset and achieve a F1 of 84.1without using any costly handcrafted fea-tures.
Additionally, our experimental re-sults show that: (1) our approach is moreeffective than CNN followed by a soft-max classifier; (2) omitting the representa-tion of the artificial class Other improvesboth precision and recall; and (3) usingonly word embeddings as input features isenough to achieve state-of-the-art results ifwe consider only the text between the twotarget nominals.1 IntroductionRelation classification is an important NaturalLanguage Processing (NLP) task which is nor-mally used as an intermediate step in many com-plex NLP applications such as question-answeringand automatic knowledge base construction.
Sincethe last decade there has been increasing interestin applying machine learning approaches to thistask (Zhang, 2004; Qian et al, 2009; Rink andHarabagiu, 2010).
One reason is the availabilityof benchmark datasets such as the SemEval-2010task 8 dataset (Hendrickx et al, 2010), which en-codes the task of classifying the relationship be-tween two nominals marked in a sentence.
Thefollowing sentence contains an example of theComponent-Whole relation between the nominals?introduction?
and ?book?.The [introduction]e1in the [book]e2is asummary of what is in the text.Some recent work on relation classification hasfocused on the use of deep neural networks withthe aim of reducing the number of handcrafted fea-tures (Socher et al, 2012; Zeng et al, 2014; Yu etal., 2014).
However, in order to achieve state-of-the-art results these approaches still use some fea-tures derived from lexical resources such as Word-Net or NLP tools such as dependency parsers andnamed entity recognizers (NER).In this work, we propose a new convolutionalneural network (CNN), which we name Classifi-cation by Ranking CNN (CR-CNN), to tackle therelation classification task.
The proposed networklearns a distributed vector representation for eachrelation class.
Given an input text segment, thenetwork uses a convolutional layer to produce adistributed vector representation of the text andcompares it to the class representations in orderto produce a score for each class.
We propose anew pairwise ranking loss function that makes iteasy to reduce the impact of artificial classes.
Weperform an extensive number of experiments usingthe the SemEval-2010 Task 8 dataset.
Using CR-CNN, and without the need for any costly hand-crafted feature, we outperform the state-of-the-artfor this dataset.
Our experimental results are ev-idence that: (1) CR-CNN is more effective thanCNN followed by a softmax classifier; (2) omit-ting the representation of the artificial class Otherimproves both precision and recall; and (3) usingonly word embeddings as input features is enoughto achieve state-of-the-art results if we consideronly the text between the two target nominals.626The remainder of the paper is structured as fol-lows.
Section 2 details the proposed neural net-work.
In Section 3, we present details about thesetup of experimental evaluation, and then de-scribe the results in Section 4.
In Section 5, wediscuss previous work in deep neural networksfor relation classification and for other NLP tasks.Section 6 presents our conclusions.2 The Proposed Neural NetworkGiven a sentence x and two target nouns, CR-CNNcomputes a score for each relation class c ?
C.For each class c ?
C, the network learns a dis-tributed vector representation which is encoded asa column in the class embedding matrix Wclasses.As detailed in Figure 1, the only input for the net-work is the tokenized text string of the sentence.
Inthe first step, CR-CNN transforms words into real-valued feature vectors.
Next, a convolutional layeris used to construct a distributed vector represen-tations of the sentence, rx.
Finally, CR-CNN com-putes a score for each relation class c ?
C by per-forming a dot product between r?xand Wclasses.2.1 Word EmbeddingsThe first layer of the network transforms wordsinto representations that capture syntactic andsemantic information about the words.
Givena sentence x consisting of N words x ={w1, w2, ..., wN}, every wordwnis converted intoa real-valued vector rwn.
Therefore, the input tothe next layer is a sequence of real-valued vectorsembx= {rw1, rw2, ..., rwN}Word representations are encoded by columnvectors in an embedding matrixWwrd?
Rdw?|V |,where V is a fixed-sized vocabulary.
Each columnWwrdi?
Rdwcorresponds to the word embeddingof the i-th word in the vocabulary.
We transform aword w into its word embedding rwby using thematrix-vector product:rw= Wwrdvwwhere vwis a vector of size |V | which has value1 at index w and zero in all other positions.
Thematrix Wwrdis a parameter to be learned, and thesize of the word embedding dwis a hyperparame-ter to be chosen by the user.2.2 Word Position EmbeddingsIn the task of relation classification, informationthat is needed to determine the class of a relationFigure 1: CR-CNN: a Neural Network for classi-fying by ranking.between two target nouns normally comes fromwords which are close to the target nouns.
Zenget al (2014) propose the use of word position em-beddings (position features) which help the CNNby keeping track of how close words are to the tar-get nouns.
These features are similar to the posi-tion features proposed by Collobert et al (2011)for the Semantic Role Labeling task.In this work we also experiment with the wordposition embeddings (WPE) proposed by Zeng etal.
(2014).
The WPE is derived from the relativedistances of the current word to the target noun1and noun2.
For instance, in the sentence shown inFigure 1, the relative distances of left to car andplant are -1 and 2, respectively.
As in (Collobertet al, 2011), each relative distance is mapped toa vector of dimension dwpe, which is initializedwith random numbers.
dwpeis a hyperparameterof the network.
Given the vectorswp1andwp2forthe word w with respect to the targets noun1andnoun2, the position embedding of w is given by627the concatenation of these two vectors, wpew=[wp1, wp2].In the experiments where word positionembeddings are used, the word embed-ding and the word position embedding ofeach word are concatenated to form theinput for the convolutional layer, embx={[rw1, wpew1], [rw2, wpew2], ..., [rwN, wpewN]}.2.3 Sentence RepresentationThe next step in the NN consists in creating thedistributed vector representation rxfor the inputsentence x.
The main challenges in this step arethe sentence size variability and the fact that im-portant information can appear at any position inthe sentence.
In recent work, convolutional ap-proaches have been used to tackle these issueswhen creating representations for text segmentsof different sizes (Zeng et al, 2014; Hu et al,2014; dos Santos and Gatti, 2014) and character-level representations of words of different sizes(dos Santos and Zadrozny, 2014).
Here, we usea convolutional layer to compute distributed vec-tor representations of the sentence.
The convo-lutional layer first produces local features aroundeach word in the sentence.
Then, it combines theselocal features using a max operation to create afixed-sized vector for the input sentence.Given a sentence x, the convolutional layer ap-plies a matrix-vector operation to each windowof size k of successive windows in embx={rw1, rw2, ..., rwN}.
Let us define the vector zn?Rdwkas the concatenation of a sequence of k wordembeddings, centralized in the n-th word:zn= (rwn?
(k?1)/2, ..., rwn+(k?1)/2)?In order to overcome the issue of referencingwords with indices outside of the sentence bound-aries, we augment the sentence with a specialpadding token replicatedk ?
12times at the be-ginning and the end.The convolutional layer computes the j-th ele-ment of the vector rx?
Rdcas follows:[rx]j= max1<n<N[f(W1zn+ b1)]jwhere W1?
Rdc?dwkis the weight matrix of theconvolutional layer and f is the hyperbolic tangentfunction.
The same matrix is used to extract localfeatures around each word window of the givensentence.
The fixed-sized distributed vector rep-resentation for the sentence is obtained by usingthe max over all word windows.
Matrix W1andvector b1are parameters to be learned.
The num-ber of convolutional units dc, and the size of theword context window k are hyperparameters to bechosen by the user.
It is important to note that dccorresponds to the size of the sentence representa-tion.2.4 Class embeddings and ScoringGiven the distributed vector representation of theinput sentence x, the network with parameter set?
computes the score for a class label c ?
C byusing the dot products?
(x)c= r?x[Wclasses]cwhere Wclassesis an embedding matrix whosecolumns encode the distributed vector representa-tions of the different class labels, and [Wclasses]cis the column vector that contains the embeddingof the class c. Note that the number of dimensionsin each class embedding must be equal to the sizeof the sentence representation, which is defined bydc.
The embedding matrix Wclassesis a parame-ter to be learned by the network.
It is initializedby randomly sampling each value from an uniformdistribution: U (?r, r), where r =?6|C|+ dc.2.5 Training ProcedureOur network is trained by minimizing a pairwiseranking loss function over the training set D. Theinput for each training round is a sentence x andtwo different class labels y+?
C and c??
C,where y+is a correct class label for x and c?isnot.
Let s?
(x)y+ and s?(x)c?
be respectively thescores for class labels y+and c?generated by thenetwork with parameter set ?.
We propose a newlogistic loss function over these scores in order totrain CR-CNN:L = log(1 + exp(?(m+?
s?
(x)y+))+ log(1 + exp(?
(m?+ s?(x)c?
))(1)where m+and m?are margins and ?
is a scal-ing factor that magnifies the difference betweenthe score and the margin and helps to penalizemore on the prediction errors.
The first term inthe right side of Equation 1 decreases as the scores?
(x)y+ increases.
The second term in the right628side decreases as the score s?(x)c?
decreases.Training CR-CNN by minimizing the loss func-tion in Equation 1 has the effect of training to givescores greater than m+for the correct class and(negative) scores smaller than ?m?for incorrectclasses.
In our experiments we set ?
to 2, m+to2.5 and m?to 0.5.
We use L2 regularization byadding the term ???
?2to Equation 1.
In our ex-periments we set ?
to 0.001.
We use stochasticgradient descent (SGD) to minimize the loss func-tion with respect to ?.Like some other ranking approaches that onlyupdate two classes/examples at every traininground (Weston et al, 2011; Gao et al, 2014), wecan efficiently train the network for tasks whichhave a very large number of classes.
This is anadvantage over softmax classifiers.On the other hand, sampling informative nega-tive classes/examples can have a significant impactin the effectiveness of the learned model.
In thecase of our loss function, more informative nega-tive classes are the ones with a score larger than?m?.
The number of classes in the relation clas-sification dataset that we use in our experiments issmall.
Therefore, in our experiments, given a sen-tence x with class label y+, the incorrect class c?that we choose to perform a SGD step is the onewith the highest score among all incorrect classesc?= argmaxc ?
C; c 6=y+s?
(x)c.For tasks where the number of classes is large,we can fix a number of negative classes to be con-sidered at each example and select the one withthe largest score to perform a gradient step.
Thisapproach is similar to the one used by Weston etal.
(2014) to select negative examples.We use the backpropagation algorithm to com-pute gradients of the network.
In our experi-ments, we implement the CR-CNN architectureand the backpropagation algorithm using Theano(Bergstra et al, 2010).2.6 Special Treatment of Artificial ClassesIn this work, we consider a class as artificial if it isused to group items that do not belong to any of theactual classes.
An example of artificial class is theclass Other in the SemEval 2010 relation classifi-cation task.
In this task, the artificial class Otheris used to indicate that the relation between twonominals does not belong to any of the nine rela-tion classes of interest.
Therefore, the class Otheris very noisy since it groups many different typesof relations that may not have much in common.An important characteristic of CR-CNN is thatit makes it easy to reduce the effect of artificialclasses by omitting their embeddings.
If the em-bedding of a class label c is omitted, it means thatthe embedding matrix Wclassesdoes not containa column vector for c. One of the main benefitsfrom this strategy is that the learning process fo-cuses on the ?natural?
classes only.
Since the em-bedding of the artificial class is omitted, it will notinfluence the prediction step, i.e., CR-CNN doesnot produce a score for the artificial class.In our experiments with the SemEval-2010 rela-tion classification task, when training with a sen-tence x whose class label y = Other, the firstterm in the right side of Equation 1 is set tozero.
During prediction time, a relation is clas-sified as Other only if all actual classes have neg-ative scores.
Otherwise, it is classified with theclass which has the largest score.3 Experimental Setup3.1 Dataset and Evaluation MetricWe use the SemEval-2010 Task 8 dataset to per-form our experiments.
This dataset contains10,717 examples annotated with 9 different rela-tion types and an artificial relation Other, whichis used to indicate that the relation in the exam-ple does not belong to any of the nine main rela-tion types.
The nine relations are Cause-Effect,Component-Whole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency,Member-Collection, Message-Topic and Product-Producer.
Each example contains a sentencemarked with two nominals e1and e2, and thetask consists of predicting the relation betweenthe two nominals taking into consideration the di-rectionality.
That means that the relation Cause-Effect(e1,e2) is different from the relation Cause-Effect(e2,e1), as shown in the examples below.More information about this dataset can be foundin (Hendrickx et al, 2010).The [war]e1resulted in other collateral imperial[conquests]e2as well.
?
Cause-Effect(e1,e2)The [burst]e1has been caused by water hammer[pressure]e2.
?
Cause-Effect(e2,e1)The SemEval-2010 Task 8 dataset is alreadypartitioned into 8,000 training instances and 2,717test instances.
We score our systems by using theSemEval-2010 Task 8 official scorer, which com-putes the macro-averaged F1-scores for the nine629actual relations (excluding Other) and takes the di-rectionality into consideration.3.2 Word Embeddings InitializationThe word embeddings used in our experiments areinitialized by means of unsupervised pre-training.We perform pre-training using the skip-gram NNarchitecture (Mikolov et al, 2013) available inthe word2vec tool.
We use the December 2013snapshot of the English Wikipedia corpus to trainword embeddings with word2vec.
We prepro-cess the Wikipedia text using the steps describedin (dos Santos and Gatti, 2014): (1) removal ofparagraphs that are not in English; (2) substitu-tion of non-western characters for a special char-acter; (3) tokenization of the text using the to-kenizer available with the Stanford POS Tagger(Toutanova et al, 2003); (4) removal of sentencesthat are less than 20 characters long (includingwhite spaces) or have less than 5 tokens.
(5) lower-case all words and substitute each numerical digitby a 0.
The resulting clean corpus contains about1.75 billion tokens.3.3 Neural Network Hyper-parameterWe use 4-fold cross-validation to tune the neu-ral network hyperparameters.
Learning rates inthe range of 0.03 and 0.01 give relatively simi-lar results.
Best results are achieved using be-tween 10 and 15 training epochs, depending onthe CR-CNN configuration.
In Table 1, we showthe selected hyperparameter values.
Additionally,we use a learning rate schedule that decreases thelearning rate ?
according to the training epoch t.The learning rate for epoch t, ?t, is computed us-ing the equation: ?t=?t.Parameter Parameter Name ValuedwWord Emb.
size 400dwpeWord Pos.
Emb.
size 70dcConvolutinal Units 1000k Context Window size 3?
Initial Learning Rate 0.025Table 1: CR-CNN Hyperparameters4 Experimental Results4.1 Word Position Embeddings and InputText SpanIn the experiments discussed in this section we as-sess the impact of using word position embeddings(WPE) and also propose a simpler alternative ap-proach that is almost as effective as WPEs.
Themain idea behind the use of WPEs in relation clas-sification task is to give some hint to the convo-lutional layer of how close a word is to the targetnouns, based on the assumption that closer wordshave more impact than distant words.Here we hypothesize that most of the informa-tion needed to classify the relation appear betweenthe two target nouns.
Based on this hypothesis,we perform an experiment where the input for theconvolutional layer consists of the word embed-dings of the word sequence {we1?
1, ..., we2+1}where e1and e2correspond to the positions of thefirst and the second target nouns, respectively.In Table 2 we compare the results of differentCR-CNN configurations.
The first column indi-cates whether the full sentence was used (Yes) orwhether the text span between the target nounswas used (No).
The second column informs ifthe WPEs were used or not.
It is clear that theuse of WPEs is essential when the full sentence isused, since F1 jumps from 74.3 to 84.1.
This ef-fect of WPEs is reported by (Zeng et al, 2014).
Onthe other hand, when using only the text span be-tween the target nouns, the impact of WPE is muchsmaller.
With this strategy, we achieve a F1 of 82.8using only word embeddings as input, which is aresult as good as the previous state-of-the-art F1 of83.0 reported in (Yu et al, 2014) for the SemEval-2010 Task 8 dataset.
This experimental result alsosuggests that, in this task, the CNN works betterfor short texts.All experiments reported in the next sectionsuse CR-CNN with full sentence and WPEs.Full WordPrec.
Rec.
F1Sentence PositionYes Yes 83.7 84.7 84.1No Yes 83.3 83.9 83.5No No 83.4 82.3 82.8Yes No 78.1 71.5 74.3Table 2: Comparison of different CR-CNN con-figurations.6304.2 Impact of Omitting the Embedding of theartificial class OtherIn this experiment we assess the impact of omit-ting the embedding of the class Other.
As wementioned above, this class is very noisy since itgroups many different infrequent relation types.Its embedding is difficult to define and thereforebrings noise into the classification process of thenatural classes.
In Table 3 we present the resultscomparing the use and omission of embeddingfor the class Other.
The two first lines of resultspresent the official F1, which does not take intoaccount the results for the class Other.
We can seethat by omitting the embedding of the class Otherboth precision and recall for the other classes im-prove, which results in an increase of 1.4 in theF1.
These results suggest that the strategy we usein CR-CNN to avoid the noise of artificial classesis effective.Use embeddingClass Prec.
Rec.
F1of class OtherNo All 83.7 84.7 84.1Yes All 81.3 84.3 82.7No Other 52.0 48.7 50.3Yes Other 60.1 48.7 53.8Table 3: Impact of not using an embedding for theartificial class Other.In the two last lines of Table 3 we present theresults for the class Other.
We can note thatwhile the recall for the cases classified as Otherremains 48.7, the precision significantly decreasesfrom 60.1 to 52.0 when the embedding of the classOther is not used.
That means that more casesfrom natural classes (all) are now been classifiedas Other.
However, as both the precision and therecall of the natural classes increase, the cases thatare now classified as Other must be cases that arealso wrongly classified when the embedding of theclass Other is used.4.3 CR-CNN versus CNN+SoftmaxIn this section we report experimental results com-paring CR-CNN with CNN+Softmax.
In orderto do a fair comparison, we?ve implemented aCNN+Softmax and trained it with the same data,word embeddings and WPEs used in CR-CNN.Concretely, our CNN+Softmax consists in gettingthe output of the convolutional layer, which is thevector rxin Figure 1, and giving it as input fora softmax classifier.
We tune the parameters ofCNN+Softmax by using a 4-fold cross-validationwith the training set.
Compared to the hyperpa-rameter values for CR-CNN presented in Table 1,the only difference for CNN+Softmax is the num-ber of convolutional units dc, which is set to 400.In Table 4 we compare the results of CR-CNN and CNN+Softmax.
CR-CNN outperformsCNN+Softmax in both precision and recall, andimproves the F1 by 1.6.
The third line in Ta-ble 4 shows the result reported by Zeng et al(2014) when only word embeddings and WPEsare used as input to the network (similar to ourCNN+Softmax).
We believe that the word embed-dings employed by them is the main reason theirresult is much worse than that of CNN+Softmax.We use word embeddings of size 400 while theyuse word embeddings of size 50, which weretrained using much less unlabeled data than wedid.Neural Net.
Prec.
Rec.
F1CR-CNN 83.7 84.7 84.1CNN+SoftMax 82.1 83.1 82.5CNN+SoftMax- - 78.9(Zeng et al, 2014)Table 4: Comparison of results of CR-CNN andCNN+Softmax.4.4 Comparison with the State-of-the-artIn Table 5 we compare CR-CNN results withresults recently published for the SemEval-2010Task 8 dataset.
Rink and Harabagiu (2010) presenta support vector machine (SVM) classifier that isfed with a rich (traditional) feature set.
It ob-tains an F1 of 82.2, which was the best resultat SemEval-2010 Task 8.
Socher et al (2012)present results for a recursive neural network(RNN) that employs a matrix-vector representa-tion to every node in a parse tree in order to com-pose the distributed vector representation for thecomplete sentence.
Their method is named thematrix-vector recursive neural network (MVRNN)and achieves a F1 of 82.4 when POS, NER andWordNet features are used.
In (Zeng et al, 2014),the authors present results for a CNN+Softmaxclassifier which employs lexical and sentence-level features.
Their classifier achieves a F1 of82.7 when adding a handcrafted feature based onthe WordNet.
Yu et al (2014) present the Factor-631based Compositional Embedding Model (FCM),which achieves a F1 of 83.0 by deriving sentence-level and substructure embeddings from word em-beddings utilizing dependency trees and namedentities.As we can see in the last line of Table 5, CR-CNN using the full sentence, word embeddingsand WPEs outperforms all previous reported re-sults and reaches a new state-of-the-art F1 of 84.1.This is a remarkable result since we do not useany complicated features that depend on externallexical resources such as WordNet and NLP toolssuch as named entity recognizers (NERs) and de-pendency parsers.We can see in Table 5 that CR-CNN1alsoachieves the best result among the systems thatuse word embeddings as the only input features.The closest result (80.6), which is produced by theFCM system of Yu et al (2014), is 2.2 F1 pointsbehind CR-CNN result (82.8).4.5 Most Representative Trigrams for eachRelationIn Table 6, for each relation type we present thefive trigrams in the test set which contributed themost for scoring correctly classified examples.Remember that in CR-CNN, given a sentence xthe score for the class c is computed by s?(x)c=r?x[Wclasses]c.
In order to compute the most rep-resentative trigram of a sentence x, we trace backeach position in rxto find the trigram responsiblefor it.
For each trigram t, we compute its particularcontribution for the score by summing the termsin score that use positions in rxthat trace back tot.
The most representative trigram in x is the onewith the largest contribution to the improvement ofthe score.
In order to create the results presentedin Table 6, we rank the trigrams which were se-lected as the most representative of any sentencein decreasing order of contribution value.
If a tri-gram appears as the largest contributor for morethan one sentence, its contribuition value becomesthe sum of its contribution for each sentence.We can see in Table 6 that for most classes, thetrigrams that contributed the most to increase thescore are indeed very informative regarding the re-lation type.
As expected, different trigrams playan important role depending on the direction ofthe relation.
For instance, the most informative tri-1This is the result using only the text span between thetarget nouns.gram for Entity-Origin(e1,e2) is ?away from the?,while reverse direction of the relation, Entity-Origin(e2,e1) or Origin-Entity, has ?the sourceof?
as the most informative trigram.
These re-sults are a step towards the extraction of meaning-ful knowledge from models produced by CNNs.5 Related WorkOver the years, various approaches have beenproposed for relation classification (Zhang, 2004;Qian et al, 2009; Hendrickx et al, 2010; Rink andHarabagiu, 2010).
Most of them treat it as a multi-class classification problem and apply a variety ofmachine learning techniques to the task in order toachieve a high accuracy.Recently, deep learning (Bengio, 2009) has be-come an attractive area for multiple applications,including computer vision, speech recognition andnatural language processing.
Among the differentdeep learning strategies, convolutional neural net-works have been successfully applied to differentNLP task such as part-of-speech tagging (dos San-tos and Zadrozny, 2014), sentiment analysis (Kim,2014; dos Santos and Gatti, 2014), question classi-fication (Kalchbrenner et al, 2014), semantic rolelabeling (Collobert et al, 2011), hashtag predic-tion (Weston et al, 2014), sentence completionand response matching (Hu et al, 2014).Some recent work on deep learning for relationclassification include Socher et al (2012), Zenget al (2014) and Yu et al (2014).
In (Socher etal., 2012), the authors tackle relation classificationusing a recursive neural network (RNN) that as-signs a matrix-vector representation to every nodein a parse tree.
The representation for the com-plete sentence is computed bottom-up by recur-sively combining the words according to the syn-tactic structure of the parse tree Their method isnamed the matrix-vector recursive neural network(MVRNN).Zeng et al (2014) propose an approach for re-lation classification where sentence-level featuresare learned through a CNN, which has word em-bedding and position features as its input.
In par-allel, lexical features are extracted according togiven nouns.
Then sentence-level and lexical fea-tures are concatenated into a single vector andfed into a softmax classifier for prediction.
Thisapproach achieves state-of-the-art performance onthe SemEval-2010 Task 8 dataset.Yu et al (2014) propose a Factor-based Com-632Classifier Feature Set F1SVM POS, prefixes, morphological, WordNet, dependency parse,82.2(Rink and Harabagiu, 2010) Levin classes, ProBank, FrameNet, NomLex-Plus,Google n-gram, paraphrases, TextRunnerRNN word embeddings 74.8(Socher et al, 2012) word embeddings, POS, NER, WordNet 77.6MVRNN word embeddings 79.1(Socher et al, 2012) word embeddings, POS, NER, WordNet 82.4word embeddings 69.7CNN+Softmax word embeddings, word position embeddings,82.7(Zeng et al, 2014) word pair, words around word pair, WordNetFCM word embeddings 80.6(Yu et al, 2014) word embeddings, dependency parse, NER 83.0CR-CNNword embeddings 82.8word embeddings, word position embeddings 84.1Table 5: Comparison with results published in the literature.Relation (e1,e2) (e2,e1)Cause-Effecte1 resulted in, e1 caused a, had caused e2 caused by, was caused by, arethe, poverty cause e2, caused a e2 caused by, been caused by, e2 from e1Component-Wholee1 of the, of the e2, part of the, e2 ?s e1, with its e1, e2 has a,in the e2, e1 on the e2 comprises the, e2 with e1Content-Containerwas in a, was hidden in, were in a, e2 full of, e2 with e1, e2 was full,was inside a, was contained in e2 contained a, e2 with coldEntity-Destinatione1 into the, e1 into a, e1 to the,-was put inside, imported into theEntity-Originaway from the, derived from a, had the source of, e2 grape e1,left the, derived from an, e1 from the e2 butter e1Instrument-Agencyare used by, e1 for e2, is used by, with a e1, by using e1, e2 finds a,trade for e2, with the e2 e2 with a, e2 , whoMember-Collectionof the e2, in the e2, of this e2, e2 of e1, of wild e1, of elven e1,the political e2, e1 collected in e2 of different, of 0000 e1Message-Topice1 is the, e1 asserts the, e1 that the, described in the, discussed in the,on the e2, e1 inform about featured in numerous, discussedin cabinet, documented in two,Product-Producere1 by the, by a e2, of the e2, e2 of the, e2 has constructed, e2 ?s e1,by the e2, from the e2 e2 came up, e2 who createdTable 6: List of most representative trigrams for each relation type.positional Embedding Model (FCM) by derivingsentence-level and substructure embeddings fromword embeddings, utilizing dependency trees andnamed entities.
It achieves slightly higher accu-racy on the same dataset than (Zeng et al, 2014),but only when syntactic information is used.There are two main differences between the ap-proach proposed in this paper and the ones pro-posed in (Socher et al, 2012; Zeng et al, 2014; Yuet al, 2014): (1) CR-CNN uses a pair-wise rank-ing method, while other approaches apply multi-class classification by using the softmax functionon the top of the CNN/RNN; and (2) CR-CNNemploys an effective method to deal with artificialclasses by omitting their embeddings, while otherapproaches treat all classes equally.6 ConclusionIn this work we tackle the relation classificationtask using a CNN that performs classification byranking.
The main contributions of this work are:(1) the definition of a new state-of-the-art for theSemEval-2010 Task 8 dataset without using anycostly handcrafted features; (2) the proposal of anew CNN for classification that uses class embed-dings and a new rank loss function; (3) an effectivemethod to deal with artificial classes by omittingtheir embeddings in CR-CNN; (4) the demonstra-tion that using only the text between target nomi-nals is almost as effective as using WPEs; and (5)a method to extract from the CR-CNN model themost representative contexts of each relation type.Although we apply CR-CNN to relation classifica-tion, this method can be used for any classificationtask.633AcknowledgmentsThe authors would like to thank Nina Wacholderfor her valuable suggestions to improve the finalversion of the paper.ReferencesYoshua Bengio.
2009.
Learning deep architecturesfor ai.
Foundations and Trends Machine Learning,2(1):1?127.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU and GPUmath expression compiler.
In Proceedings of thePython for Scientific Computing Conference.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.C?
?cero Nogueira dos Santos and Ma?
?ra Gatti.
2014.Deep convolutional neural networks for sentimentanalysis of short texts.
In Proceedings of the 25th In-ternational Conference on Computational Linguis-tics (COLING), Dublin, Ireland.C?
?cero Nogueira dos Santos and Bianca Zadrozny.2014.
Learning character-level representations forpart-of-speech tagging.
In Proceedings of the31st International Conference on Machine Learning(ICML), JMLR: W&CP volume 32, Beijing, China.Jianfeng Gao, Patrick Pantel, Michael Gamon, Xi-aodong He, and Li Deng.
2014.
Modeling interest-ingness with deep neural networks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid?O.
S?eaghdha, SebastianPad?o, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz.
2010.
Semeval-2010 task 8:Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 33?38.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.
InProceedings of the Conference on Neural Informa-tion Processing Systems, pages 2042?2050.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural netork for mod-elling sentences.
In Proceedings of the 52th AnnualMeeting of the Association for Computational Lin-guistics, pages 655?665, Baltimore, Maryland.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014Conference on Empirical Methods for Natural Lan-guage Processing, pages 1746?1751, Doha, Qatar.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In In Proceedings of Work-shop at ICLR.Longhua Qian, Guodong Zhou, Fang Kong, andQiaoming Zhu.
2009.
Semi-supervised learning forsemantic relation classification using stratified sam-pling strategy.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 1437?1445.Bryan Rink and Sanda Harabagiu.
2010.
Utd: Clas-sifying semantic relations by combining lexical andsemantic resources.
In Proceedings of InternationalWorkshop on Semantic Evaluation, pages 256?259.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of the Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, pages173?180.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
Wsabie: Scaling up to large vocabulary imageannotation.
In Proceedings of the Twenty-SecondInternational Joint Conference on Artificial Intelli-gence, pages 2764?2770.Jason Weston, Sumit Chopra, and Keith Adams.
2014.#tagspace: Semantic embeddings from hashtags.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 1822?1827.Mo Yu, Matthew Gormley, and Mark Dredze.
2014.Factor-based compositional embedding models.
InProceedings of the 2nd Workshop on Learning Se-mantics, Montreal, Canada.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation classification via con-volutional deep neural network.
In Proceedings ofthe 25th International Conference on ComputationalLinguistics (COLING), pages 2335?2344, Dublin,Ireland.Zhu Zhang.
2004.
Weakly-supervised relation classifi-cation for information extraction.
In Proceedings ofthe ACM International Conference on Informationand Knowledge Management, pages 581?588, NewYork, NY, USA.634
