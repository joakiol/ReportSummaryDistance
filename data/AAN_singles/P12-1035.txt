Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 330?338,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Joint Model for Discovery of Aspects in UtterancesAsli CelikyilmazMicrosoftMountain View, CA, USAasli@ieee.orgDilek Hakkani-TurMicrosoftMountain View, CA, USAdilek@ieee.orgAbstractWe describe a joint model for understandinguser actions in natural language utterances.Our multi-layer generative approach uses bothlabeled and unlabeled utterances to jointlylearn aspects regarding utterance?s target do-main (e.g.
movies), intention (e.g., finding amovie) along with other semantic units (e.g.,movie name).
We inject information extractedfrom unstructured web search query logs asprior information to enhance the generativeprocess of the natural language utterance un-derstanding model.
Using utterances from fivedomains, our approach shows up to 4.5% im-provement on domain and dialog act perfor-mance over cascaded approach in which eachsemantic component is learned sequentiallyand a supervised joint learning model (whichrequires fully labeled data).1 IntroductionVirtual personal assistance (VPA) is a human tomachine dialog system, which is designed to per-form tasks such as making reservations at restau-rants, checking flight statuses, or planning weekendactivities.
A typical spoken language understanding(SLU) module of a VPA (Bangalore, 2006; Tur andMori, 2011) defines a structured representation forutterances, in which the constituents correspond tomeaning representations in terms of slot/value pairs(see Table 1).
While target domain corresponds tothe context of an utterance in a dialog, the dialogact represents overall intent of an utterance.
Theslots are entities, which are semantic constituents atthe word or phrase level.
Learning each componentSample utterances on ?plan a night out?
scenario(I) Show me theaters in [Austin] playing [iron man 2].
(II)I?m in the mood for [indian] food tonight, show me theones [within 5 miles] that have [patios].Extracted Class and LabelsDomain Dialog Act Slots=Values(I) Movie find Location=Austintheater Movie-Name= iron man 2(II) Restaurant find Rest-Cusine=indianrestaurant Location=within 5 milesRest-Amenities= patiosTable 1: Examples of utterances with corresponding se-mantic components, i.e., domain, dialog act, and slots.is a challenging task not only because there are noa priori constraints on what a user might say, butalso systems must generalize from a tractably smallamount of labeled training data.
In this paper, weargue that each of these components are interdepen-dent and should be modeled simultaneously.
Webuild a joint understanding framework and introducea multi-layer context model for semantic representa-tion of utterances of multiple domains.Although different strategies can be applied,typically a cascaded approach is used whereeach semantic component is modeled sepa-rately/sequentially (Begeja et al, 2004), focusingless on interrelated aspects, i.e., dialog?s domain,user?s intentions, and semantic tags that can beshared across domains.
Recent work on SLU(Jeong and Lee, 2008; Wang, 2010) presents jointmodeling of two components, i.e., the domain andslot or dialog act and slot components together.Furthermore, most of these systems rely on labeledtraining utterances, focusing little on issues suchas information sharing between the discourse andword level components across different domains,or variations in use of language.
To deal with de-330pendency and language variability issues, a modelthat considers dependencies between semanticcomponents and utilizes information from largebodies of unlabeled text can be beneficial for SLU.In this paper, we present a novel generativeBayesian model that learns domain/dialog-act/slotsemantic components as latent aspects of text ut-terances.
Our approach can identify these semanticcomponents simultaneously in a hierarchical frame-work that enables the learning of dependencies.
Weincorporate prior knowledge that we observe in websearch query logs as constraints on these latent as-pects.
Our model can discover associations betweenwords within a multi-layered aspect model, in whichsome words are indicative of higher layer (meta) as-pects (domain or dialog act components), while oth-ers are indicative of lower layer specific entities.The contributions of this paper are as follows:(i) construction of a novel Bayesian framework forsemantic parsing of natural language (NL) utter-ances in a unifying framework in ?4,(ii) representation of seed labeled data and informa-tion from web queries as informative prior to designa novel utterance understanding model in ?3 & ?4,(iii) comparison of our results to supervised sequen-tial and joint learning methods on NL utterances in?5.
We conclude that our generative model achievesnoticeable improvement compared to discriminativemodels when labeled data is scarce.2 BackgroundLanguage understanding has been well studied inthe context of question/answering (Harabagiu andHickl, 2006; Liang et al, 2011), entailment (Sam-mons et al, 2010), summarization (Hovy et al,2005; Daume?-III and Marcu, 2006), spoken lan-guage understanding (Tur and Mori, 2011; Dinarelliet al, 2009), query understanding (Popescu et al,2010; Li, 2010; Reisinger and Pasca, 2011), etc.However data sources in VPA systems pose newchallenges, such as variability and ambiguities innatural language, or short utterances that rarely con-tain contextual information, etc.
Thus, SLU playsan important role in allowing any sophisticated spo-ken dialog system (e.g., DARPA Calo (Berry et al,2011), Siri, etc.)
to take the correct machine actions.A common approach to building SLU frameworkis to model its semantic components separately, as-suming that the context (domain) is given a pri-ori.
Earlier work takes dialog act identification asa classification task to capture the user?s intentions(Margolis et al, 2010) and slot filling as a sequencelearning task specific to a given domain class (Wanget al, 2009; Li, 2010).
Since these tasks are con-sidered as a pipeline, the errors of each componentare transfered to the next, causing robustness issues.Ideally, these components should be modeled si-multaneously considering the dependencies betweenthem.
For example, in a local domain application,users may require information about a sub-domain(movies, hotels, etc.
), and for each sub-domain, theymay want to take different actions (find a movie, calla restaurant or book a hotel) using domain specificattributes (e.g., cuisine type of a restaurant, titles formovies or star-rating of a hotel).
There?s been littleattention in the literature on modeling the dependen-cies of SLU?s correlated structures.Only recent research has focused on the jointmodeling of SLU (Jeong and Lee, 2008; Wang,2010) taking into account the dependencies at learn-ing time.
In (Jeong and Lee, 2008), a triangularchain conditional random fields (Tri-CRF) approachis presented to model two of the SLU?s componentsin a single-pass.
Their discriminative approach rep-resents semantic slots and discourse-level utterancelabels (domain or dialog act) in a single structureto encode dependencies.
However, their model re-quires fully labeled utterances for training, whichcan be time consuming and expensive to generate fordynamic systems.
Also, they can only learn depen-dencies between two components simultaneously.Our approach differs from the earlier work- inthat- we take the utterance understanding as a multi-layered learning problem, and build a hierarchicalclustering model.
Our joint model can discoverdomain D, and user?s act A as higher layer latentconcepts of utterances in relation to lower layer la-tent semantic topics (slots) S such as named-entities(?New York?)
or context bearing non-named enti-ties (?vegan?).
Our work resembles the earlier workof PAM models (Mimno et al, 2007), i.e., directedacyclic graphs representing mixtures of hierarchicaltopic structures, where upper level topics are multi-nomial over lower level topics in a hierarchy.
In ananalogical way to earlier work, the D and A in our331approach represent common co-occurrence patterns(dependencies) between semantic tags S (Fig.
2).Concretely, correlated topics eliminate assignmentof semantic tags to segments in an utterance thatbelong to other domains, e.g., we can discover that?Show me vegan restaurants in San Francisco?
hasa low probably of outputting a movie-actor slot.
Be-ing generative, our model can incorporate unlabeledutterances and encode prior information of concepts.3 Data and Approach OverviewHere we define several abstractions of our jointmodel as depicted in Fig.
1.
Our corpus mainlycontains NL utterances (?show me the nearest dim-sum places?)
and some keyword queries (?iron man2 trailers?).
We represent each utterance u as a vec-tor wu of Nu word n-grams (segments), wuj , eachof which are chosen from a vocabulary W of fixed-size V. We use entity lists obtained from web sources(explained next) to identify segments in the corpus.Our corpus contains utterances from KD=4 maindomains:?
{movies, hotels, restaurants, events},as well as out-of-domain other class.
Each utterancehas one dialog act (A) associated with it.
We assumea fixed number of possible dialog acts KA for eachdomain.
Semantic Tags, slots (S) are lexical units(segments) of an utterance, which we classify intotwo types: domain-independent slots that are sharedacross all domains, (e.g., location, time, year, etc.
),and domain-dependent slots, (e.g.
movie-name,actor-name, restaurant-name, etc.).
For tractability,we consider a fixed number of latent slot types KS .Our algorithm assigns domain/dialog-act/slot labelsto each topic at each layer in the hierarchy using la-beled data (explained in ?4.
)We represent domain and dialog act componentsas meta-variables of utterances.
This is similar toauthor-topic models (Rosen-Zvi et al, 2004), thatcapture author-topic relations across documents.
Inthat case, words are generated by first selecting anauthor uniformly from an observed author list andthen selecting a topic from a distribution over wordsthat is specific to that author.
In our model, eachutterance u is associated with domain and dialogact topics.
A word wuj in u is generated by firstselecting a domain and an act topic and then slottopic over words of u.
The domain-dependent slotsin utterances are usually not dependent on the di-alog act.
For instance, while ?find [hugo] trailer?and ?show me where [hugo] is playing?
have botha movie-name slot (?hugo?
), they have different di-alog acts, i.e., find-trailer and find-movie, respec-tively.
We predict posterior probabilities for domainP?
(d ?
D|u) dialog act P?
(a ?
A|ud) and slotsP?
(sj ?
S|wuj , d, sj?1) of words wuj in sequence.To handle language variability, and hence dis-cover correlation between hierarchical aspects of ut-terances1, we extract prior information from twoweb resources as follows:Web n-Grams (G).
Large-scale engines such asBing or Google log more than 100M search querieseach day.
Each query in the search logs has an as-sociated set of URLs that were clicked after usersentered a given query.
The click information canbe used to infer domain class labels, and there-fore, can provide (noisy) supervision in training do-main classifiers.
For example, two queries (?cheaphotels Las Vegas?
and ?wine resorts in Napa?
),which resulted in clicks on the same base URL (e.g.,www.hotels.com) probably belong to the same do-main (?hotels?
in this case).movie rest.
hotel event other?G= P(d=hotel|wj=?room?
)d|wjGiven query logs, wecompile sets of in-domainqueries based on theirbase URLs2.
Then, foreach vocabulary itemwj ?
W in our corpus, we calculate frequency ofwj in each set of in-domain queries and representeach word (e.g., ?room?)
as a discrete normalizedprobability distribution ?jG over KD domains{?d|jG }?
?jG.
We inject them as nonuniform priorsover domain and dialog act parameters in ?4.Entity Lists (E).
We limit our model to a setof named-entity slots (e.g., movie-name, restaurant-name) and non-named entity slots (e.g., restaurant-cuisine, hotel-rating).
For each entity slot, we ex-tract a large collection of entity lists through the url?son the web that correspond to our domains, suchas movie-names listed on IMDB, restaurant-nameson OpenTable, or hotel-ratings on tripadvisor.com.1Two utterances can be intrinsically related but contain nocommon terms, e.g., ?has open bar?
and ?serves free drinks?.2We focus on domain specific search engines such asIMDB.com, RottenTomatoes.com for movies, Hotels.com andExpedia.com for hotels, etc.332slottransitionparametersslot topicsdialog acttopics!Adomain specificact parametersn-grampriorfromweb query logsentitypriorfromweb documentsdomain topicsdomainparametersUtterancew w+1wujmovie restaurant hotelmenu 0.02 0.93 0.01rooms 0.001 0.001 0.98(?G) Web N-Gram Context Prior(?E) Entity List PriorV?Dwujmovienamerestaurantnamehotelnamehotel california 0.5 0.0 0.5zucca 0.0 1.0 0.0Sw-1S+1S-1DA!D!SKS?G"SKStopic-wordparameters?EMDMAMSFigure 1: Graphical model depiction of the MCM.
D,A,S aredomain, dialog act and slot in a hierarchy, each consisting ofKD,KA,KS components.
Shaded nodes indicate observedvariables.
Hyper-parameters are omitted.
Sample informativepriors over latent topics ?E and ?G are shown.
Blue arrowsindicate frequency of vocabulary terms sampled for each topic.We represent each entity list as observed nonuniformpriors ?E and inject them into our joint learning pro-cess as V sparse multinomial distributions over la-tent topics D, and S to ?guide?
the generation ofutterances (Fig.
1 top-left table), explained in ?4.4 Multi-Layer Context Model - MCMThe generative process of our multi-layer contextmodel (MCM) (Fig.
1) is shown in Algorithm 1.
Eachutterance u is associated with d = 1..KD multino-mial domain-topic distributions ?dD.
Each domain d,is represented as a distribution over a = 1, ..,KAdialog acts ?daA (?dD ?
?daA ).
In our MCM model, weassume that each utterance is represented as a hiddenMarkov model with KS slot states.
Each state gen-erates n-grams according to a multinomial n-gramdistribution.
Once domain Du and act Aud topicsare sampled for u, a slot state topic Sujd is drawnto generate each segment wuj of u by consideringthe word-tag sequence frequencies based on a sim-ple HMM assumption, similar to the content modelsof (Sauper et al, 2011).
Initial and transition prob-ability distributions over the HMM states are sam-pled from Dirichlet distribution over slots ?dsS .
Eachslot state s generates words according to multino-mial word distribution ?sS .
We also keep track of thefrequency of vocabulary termswj?s in a V ?KD ma-trixMD.
Every time awj is sampled for a domain d,we increment its count, a degree of domain bearingwords.
Similarly, we keep track of dialog act andslot bearing words in V ?KA and V ?KS matrices,MA and MS (shown as red arrows in Fig 1).
BeingBayesian, each distribution ?dD, ?adA , and ?dsS is sam-pled from a Dirichlet prior distribution with differentparameters, described next.Algorithm 1 Multi-Layer Context Model Generation1: for each domain d?
1, ...,KD2: draw domain dist.
?dD ?
Dir(?
?D)?,3: for each dialog-act a?
1, ...,KA4: draw dialog act dist.
?daA ?
Dir(?
?A),5: for each slot type s?
1, ...,KS6: draw slot dist.
?dsS ?
Dir(?
?S).7: endfor8: draw ?sS ?
Dir(?)
for each slot type s?
1, ...,KS .9: for each utterance u?
1, ..., |U | do10: Sample a domain Du?Multi(?dD) and,11: and act topic Aud?Multi(?daA ).12: for words wuj , j ?
1, ..., Nu do13: - Draw Sujd?Multi(?Du,Su(j?1)dS )?.14: - Sample wuj?Multi(?Sujd ).15: end for16: end for?
Dir(?
?D), Dir(?
?A), Dir(?
?S) are parameterized based on priorknowledge.?
Here HMM assumption over utterance words is used.In hierarchical topic models (Blei et al, 2003;Mimno et al, 2007), etc., topics are representedas distributions over words, and each document ex-presses an admixture of these topics, both of whichhave symmetric Dirichlet (Dir) prior distributions.Symmetric Dirichlet distributions are often used,since there is typically no prior knowledge favoringone component over another.
In the topic model lit-erature, such constraints are sometimes used to de-terministically allocate topic assignments to knownlabels (Labeled Topic Modeling (Ramage et al,2009)) or in terms of pre-learnt topics encoded asprior knowledge on topic distributions in documents(Reisinger and Pas?ca, 2009).
Similar to previouswork, we define a latent topic per each known se-mantic component label, e.g., five domain topics forfive defined domains.
Different from earlier workthough, we also inject knowledge that we extractfrom several resources including entity lists fromweb search query click logs as well as seed labeledtraining utterances as prior information.
We con-strain the generation of the semantic components ofour model by encoding prior knowledge in terms of333asymmetric Dirichlet topic priors ?=(?m1,...,?mK)where each kth topic has a prior weight ?k=?mk,with varying base measure m=(m1,...,mk) 3.We update parameter vectors of Dirichlet domainprior ?u?D ={(?D?
?u1D ),..., ?D?
?uKDD }, where ?D isthe concentration parameter for domain Dirichletdistribution and ?uD={?udD }KDd=1 is the base mea-sure which we obtain from various resources.
Be-cause base measure updates are dependent on priorknowledge of corpus words, each utterance u getsa different base measure.
Similarly, we updatethe parameter vector of the Dirichlet dialog actand slot priors ?u?A ={(?A?
?u1A ),...,(?A?
?uKAA )} and?u?S ={(?S ?
?u1S ),...,(?S ?
?uKSS )} using base measures?uA={?uaA }KAa=1 and ?Su={?usS }KSs=1 respectively.Before describing base measure update for do-main, act and slot Dirichlet priors, we explain theconstraining prior knowledge parameters below:?
Entity List Base Measure(?jE): Entity fea-tures are indicative of domain and slots and MCMutilizes these features while sampling topics.
Forinstance, entities hotel-name ?Hilton?
and location?New York?
are discriminative features in classi-fying ?find nice cheap double room in New YorkHilton?
into correct domain (hotel) and slot (hotel-name) clusters.
We represent entity lists correspond-ing to known domains as multinomial distributions?jE , where each ?d|jE is the probability of entity-word wj used in the domain d. Some entities maybelong to more than one domain, e.g., ?hotel Cali-fornia?
can either be a movie, or song or hotel name.?
Web n-Gram Context Base Measure (?jG):As explained in ?3, we use the web n-grams as ad-ditional information for calculating the base mea-sures of the Dirichlet topic distributions.
Normal-ized word distributions ?jG over domains were usedas weights for domain and dialog act base measure.?
Corpus n-Gram Base Measure (?jC): Sim-ilar to other measures, MCM also encodes n-gramconstraints as word-frequency features extractedfrom labeled utterances.
Concretely, we cal-culate the frequency of vocabulary items givendomain-act label pairs from the training labeled ut-terances and convert there into probability mea-sures over domain-acts.
We encode conditional3See (Wallach, 2008) Chapter 3 for analysis of hyper-priorson topic models.probabilities {?ad|jC }?
?jC as multinomial distribu-tions of words over domain-act pairs, e.g., ?ad|jC =P(d=?restaurant?, a=?make-reservation?|?table?
).Base measure update: The ?-base measures areused to shape Dirichlet priors ?u?D , ?u?A and ?u?S .
Weupdate the base measures of each sampled domainDu = d given each vocabulary wj as:?djD ={?d|jE , ?d|jE > 0?d|jG , otherwise(1)In (1) we assume that entities (E) are more indica-tive of the domain compared to other n-grams (G)and should be more dominant in sampling decisionfor domain topics.
Given an utterance u, we calcu-late its base measure ?udD =(?Nuj ?djD )/Nu.Once the domain is sampled, we update the priorweight of dialog acts Aud = a:?ajA = ?ad|jC ?
?d|jG (2)and slot components Sujd = s:?sjS = ?d|jE (3)Then we update their base measures for a given u as:?uaA =(?Nuj ?ajA )/Nu and ?usS =(?Nuj ?sjS )/Nu.4.1 Inference and LearningThe goal of inference is to predict the domain, user?sact and slot distributions over each segment givenan utterance.
The MCM has the following set of pa-rameters: domain-topic distributions ?dD for each u,the act-topic distributions ?daA for each domain topicd of u, local slot-topic distributions for each do-main ?S , and ?sS for slot-word distributions.
Pre-vious work (Asuncion et al, 2009; Wallach et al,2009) shows that the choice of inference method hasnegligible effect on the probability of testing doc-uments or inferred topics.
Thus, we use MarkovChain Monte Carlo (MCMC) method,specificallyGibbs sampling, to model the posterior distributionPMCM(Du, Aud, Sujd|?u?D , ?u?A , ?u?S , ?)
by obtainingsamples (Du, Aud, Sujd) drawn from this distribu-tion.
For each utterance u, we sample a domain Duand act Aud and hyper-parameters ?D and ?A andtheir base measures ?udD , ?uaA (from Eq.
1,2):?dD =Ndu + ?D?udDNu + ?u?D; ?daA =Na|ud + ?A?udDNud + ?u?A(4)The Ndu is the number of occurrences of domaintopic d in utterance u, Na|ud is the number of occur-rences of act a given d in u.
During sampling of a334slot state Sujd, we assume that utterance is generatedby the HMM model associated with the assigneddomain.
For each segment wuj in u, we sample aslot state Sujd given the remaining slots and hyper-parameters ?S , ?
and base measure ?usS (Eq.
3) by:p(Sujd = s|w,Du,S?
(ujd)?u?S , ?)
?Nkujd + ?Nk(.)
+ V ??
(NDu,Su(j?1)ds + ?S?usS )?NDu,sSu(j+1)d + I(Suj?1, s) + I(Suj+1, s) + ?S?usSNDu,s(.)
+ I(Suj?1, s) +KD?u?S(5)The Nkujd is the number of times segment wuj isgenerated from slot state s in all utterances as-signed to domain topic d, NDu,s1s2 is the num-ber of transitions from slot state s1 to s2, wheres1 ?
{Su(j?1)d,Su(j+1)d}, I(s1, s2)=1 if slot s1=s2.4.2 Semantic Structure Extraction with MCMDuring Gibbs sampling, we keep track of the fre-quency of draws of domain, dialog act and slot in-dicating n-grams wj , in MD, MA and MS matri-ces, respectively.
These n-grams are context bearingwords (examples are shown in Fig.1.).
For given uthe predicted domain d?u is determined by:d?u = arg maxd P?
(d|u) = arg maxd[?dD ?
?Nuj=1MjdDMD]and predicted dialog act by arg maxa P?
(a|ud?
):a?u = arg maxa[?d?aA ?
?Nuj=1MjaAMA] (6)For each segment wuj in u, its predicted slot are de-termined by arg maxs P (sj |wuj , d?, sj?1):s?uj = arg maxs[p(Sujd?
= s|.)
?
?Nuj=1ZjsSZS] (7)5 ExperimentsWe performed several experiments to evaluate ourproposed approach.
Before presenting our results,we describe our datasets as well as two baselines.5.1 Datasets, Labels and TagsOur dataset contains utterances obtained from di-alogs between human users and our personal assis-tant system.
We use the transcribed text forms ofDomain Sample Dialog Acts (DAs) & Slotsmovie DAs: find-movie/director/actor,buy-ticketSlots: name, mpaa-rating (g-rated), date,director/actor-name, award(oscar winning)...hotel DAs: find-hotel, book-hotel,Slots: name, room-type(double), amenities,smoking, reward-program(platinum elite)...restaurant DAs: find-restaurant, make-reservation,Slots: opening-hour, amenities, meal-type,...event DAs: find-event/ticket/performers, get-info..Slots: name, type(concert), performer....Table 2: List of domains, dialog acts and semantic slottags of utterance segments.
Examples for some slots val-ues are presented in parenthesis as italicized.the utterances obtained from (acoustic modeling en-gine) to train our models 4.
Thus, our dataset con-tains 18084 NL utterances, 5034 of which are usedfor measuring the performance of our models.
Thedataset consists of five domain classes, i.e, movie,restaurant, hotel, event, other, 42 unique dialog actsand 41 slot tags.
Each utterance is labeled with adomain, dialog act and a sequence of slot tags cor-responding to segments in utterance (see examplesin Table 1).
Table 2 shows sample dialog act andslot labels.
Annotation agreement, Kappa measure(Cohen, 1960), was around 85%.We pulled a month of web query logs and ex-tracted over 2 million search queries from the movie,hotel, event, and restaurant domains.
We also usedgeneric web queries to compile a set of ?other?
do-main queries.
Our vocabulary consists of n-gramsand segments (phrases) in utterances that are ex-tracted using web n-grams and entity lists of ?3.
Weextract distributions of n-grams and entities to injectas prior weights for entity list base (?jE) and webn-gram context base measures (?jG) (see ?4).5.2 Baselines and Experiment SetupWe evaluated two baselines and two variants of ourjoint SLU approach as follows:?
Sequence-SLU: A traditional approach to SLUextracts domain, dialog act and slots as seman-tic components of utterances using three sequentialmodels.
Typically, domain and dialog act detec-tion models are taken as query classification, wherea given NL query is assigned domain and act la-bels.
Among supervised query classification meth-4We submitted sample utterances used in our models as ad-ditional resource.
Due to licensing issues, we will reveal the fulltrain/test utterances upon acceptance of our paper.335movierestaurantmovie, theater,ticket, matinee,fandangomenu, table,dinner, togokids-friendlychinese, coffeeD1D2find-movieA1find-reviewA2reservationA3check-menuA4movie-nameS1actor-nameS2iron man 2,hugo, muppetsdescendantsrest-nameS3cuisineS4Sktom hanks,angelina jolie,cameronreviews, criticsratings, mpaa,breath-takingscary, ticketiron-man 2,oscar winnerkid-friendlyreserve, tablewait-timemenu, list,vine list,check, hotpotnearest,city center,Vancouver,New Yorkamici, zuccanew yorkbagelstarbuckschinese,vietnamese,italian,fast foodDOMAINDIALOGACTSlocationSLOTSdomainin-dependentslotsFigure 2: Sample topics discovered by Multi-Layer ContextModel (MCM).
Given samples of utterances, MCM is able to in-fer a meaningful set of dialog act (A) and slots (S), falling intobroad categories of domain classes (D).ods, we used the Adaboost, utterance classifica-tion method that starts from a set of weak classifiersand builds a strong classifier by boosting the weakclassifiers.
Slot discovery is taken as a sequence la-beling task in which segments in utterances are la-beled (Li, 2010).
For segment labeling we use Semi-Markov Conditional Random Fields (Semi-CRF)(Sarawagi and Cohen, 2004) method as a benchmarkin evaluating semantic tagging performance.?
Tri-CRF: We used Triangular Chain CRF (Jeongand Lee, 2008) as our supervised joint model base-line.
It is a state-of-the art method that learns thesequence labels and utterance class (domain or dia-log act) as meta-sequence in a joint framework.
Itencodes the inter-dependence between the slot se-quence s and meta-sequence label (d or a) using atriangular chain (dual-layer) structure.?
Base-MCM: Our first version injects an informa-tive prior for domain, dialog act and slot topic dis-tributions using information extracted from only la-beled training utterances and inject as prior con-straints (corpus n-gram base measure ?jC) duringtopic assignments.?
WebPrior-MCM: Our full model encodes distri-butions extracted from labeled training data as wellas structured web logs as asymmetric Dirichlet pri-ors.
We analyze performance gain by the informa-tion from web sources (?jG and ?jE) when injectedinto our approach compared to Base-MCM.We inject dictionary constraints as featuresto train supervised discriminative methods, i.e.,boosting and Semi-CRF in Sequence-SLU, andTri-CRF models.
For semantic tagging, dictionaryconstraints apply to the features between individualsegments and their labels, and for utterance classifi-cation (to predict domain and dialog acts) they applyto the features between utterance and its label.
Givena list of dictionaries, these constraints specify whichlabel is more likely.
For discriminative methods,we use several named entities, e.g., Movie-Name,Restaurant-Name, Hotel-Name, etc., non-named en-tities, e.g., Genre, Cuisine, etc., and domain inde-pendent dictionaries, e.g., Time, Location, etc.We train domain and dialog act classifiers viaIcsiboost (Favre et al, 2007) with 10K iterationsusing lexical features (up to 3-n-grams) and con-straining dictionary features (all dictionaries).
Forfeature templates of sequence learners, i.e., Semi-CRF and Tri-CRF, we use current word, bi-gramand dictionary features.
For Base-MCM andWebPrior-MCM, we run Gibbs sampler for 2000iterations with the first 500 samples as burn-in.5.3 Evaluations and DiscussionsWe evaluate the performance of our joint model ontwo experiments using two metrics.
For domain anddialog act detection performance we present resultsin accuracy, and for slot detection we use the F1 pair-wise measure.Experiment 1.
Encoding Prior Knowledge: Acommon evaluation method in SLU tasks is to mea-sure the performance of each individual semanticmodel, i.e., domain, dialog act and semantic tagging(slot filling).
Here, we not only want to demon-strate the performance of each component of MCMbut also their performance under limited amount oflabeled data.
We randomly select subsets of labeledtraining data U iL ?
UL with different samples sizes,niL ={?
?nL}, where nL represents the sample sizeof UL and ?={10%,25%,..} is the subset percentage.At each random selection, the rest of the utterancesare used as unlabeled data to boost the performanceof MCM.
The supervised baselines do not leverage theunlabeled utterances.The results reported in Figure 3 reveal boththe strengths and some shortcomings of our ap-proach.
When the number of labeled data issmall (niL ?25%*nL), our WebPrior-MCM hasa better performance on domain and act predic-tions compared to the two baselines.
Compared toSequence-SLU, we observe 4.5% and 3% perfor-mance improvement on the domain and dialog act33610 25 50 75 100919293949596% Labeled DataAccuracy%Utterance Domain Performance20 40 60 80 10082838485868788% Labeled DataAccuracy%Dialog Act Performance20 40 60 80 1006570758085% Labeled DataF-MeasureSemantic Tag (Slot) PerformanceSequence-SLU Tri-CRF Base-MCM WebPrior-MCMFigure 3: Semantic component extraction performance measures for various baselines as well as our approach with different priors.models, whereas our gain is 2.6% and 1.7% overTri-CRF models.
As the percentage of labeled ut-terances in training data increase, Tri-CRF perfor-mance increases, however WebPrior-MCM is stillcomparable with Sequence-SLU.
This is becausewe utilize domain priors obtained from the websources as supervision during generative process aswell as unlabeled utterances that enable handlinglanguage variability.
Adding labeled data improvesthe performance of all models however supervisedmodels benefit more compared to MCM models.Although WebPrior-MCM?s domain and dialogact performances are comparable (if not better than)the other baselines, it falls short on the semantictagging model.
This is partially due to the HMMassumption compared to the supervised conditionalmodel?s used in the other baselines, i.e., Semi-CRFin Sequence-SLU and Tri-CRF).
Our work canbe extended by replacing HMM assumption withCRF based sequence learner to enhance the capa-bility of the sequence tagging component of MCM.Experiment 2.
Less is More?
Being Bayesian,our model can incorporate unlabeled data at train-ing time.
Here, we evaluate the performance gain ondomain, act and slot predictions as more unlabeleddata is introduced at learning time.
We use only 10%of the utterances as labeled data in this experimentand incrementally add unlabeled data (90% of la-beled data are treated as unlabeled).The results are shown in Table 3. n% (n=10,25,..)unlabeled data indicates that the WebPrior-MCMis trained using n% of unlabeled utterances alongwith training utterances.
Adding unlabeled data hasa positive impact on the performance of all three se-Table 3: Performance evaluation results ofWebPrior-MCM using different sizes of unlabeledutterances at learning time.Unlabeled Domain Dialog Act Slot% Accuracy Accuracy F-Measure10% 94.69 84.17 52.6125% 94.89 84.29 54.2250% 95.08 84.39 56.5875% 95.19 84.44 57.45100% 95.28 84.52 58.18mantic components when WebPrior-MCM is used.The results show that our joint modeling approachhas an advantage over the other joint models (i.e.,Tri-CRF) in that it can leverage unlabeled NL ut-terances.
Our approach might be usefully extendedinto the area of understanding search queries, wherean abundance of unlabeled queries is observed.6 ConclusionsIn this work, we introduced a joint approach tospoken language understanding that integrates twoproperties (i) identifying user actions in multipledomains in relation to semantic units, (ii) utilizinglarge amounts of unlabeled web search queries thatsuggest the user?s hidden intentions.
We proposed asemi-supervised generative joint learning approachtailored for injecting prior knowledge to enhance thesemantic component extraction from utterances as aunifying framework.
Experimental results using thenew Bayesian model indicate that we can effectivelylearn and discover meta-aspects in natural languageutterances, outperforming the supervised baselines,especially when there are fewer labeled and moreunlabeled utterances.337ReferencesA.
Asuncion, M. Welling, P. Smyth, and Y. W. Teh.
2009.On smoothing and inference for topic models.
UAI.S.
Bangalore.
2006.
Introduction to special issue of spo-ken language understanding in conversational systems.In Speech Conversation, volume 48, pages 233?238.L.
Begeja, B. Renger, Z. Liu D. Gibbon, andB.
Shahraray.
2004.
Interactive machine learningtechniques for improving slu models.
In Proceedingsof the HLT-NAACL 2004 Workshop on Spoken Lan-guage Understanding for Conversational Systems andHigher Level Linguistic Information for Speech Pro-cessing.Pauline M. Berry, Melinda Gervasio, Bart Peintner, andNeil Yorke-Smith.
2011.
Ptime: Personalized assis-tance for calendaring.
In ACM Transactions on Intel-ligent Systems and Technology, volume 2, pages 1?40.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
In Educational and Psychological Measure-ment, volume 20, pages 37?46.H.
Daume?-III and D. Marcu.
2006.
Bayesian query fo-cused summarization.M.
Dinarelli, A. Moschitti, and G. Riccardi.
2009.
Re-ranking models for spoken language understanding.Proc.
European Chapter of the Annual Meeting of theAssociation of Computational Linguistics (EACL).B.
Favre, D. Hakkani-Tu?r, and Sebastien Cuendet.2007.
Icsiboost.
http://code.google.come/p/icsiboost.S.
Harabagiu and A. Hickl.
2006.
Methods for usingtextual entailment for question answering.
pages 905?912.E.
Hovy, C.Y.
Lin, and L. Zhou.
2005.
A be-based multi-document summarizer with query interpretation.
Proc.DUC.M.
Jeong and G. G. Lee.
2008.
Triangular-chain con-ditional random fields.
EEE Transactions on Audio,Speech and Language Processing (IEEE-TASLP).X.
Li.
2010.
Understanding semantic structure of nounphrase queries.
Proc.
of the Annual Meeting of theAssociation of Computational Linguistics (ACL).P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learningdependency based compositional semantics.A.
Margolis, K. Livescu, and M. Osterdorf.
2010.
Do-main adaptation with unlabeled data for dialog act tag-ging.
In Proc.
Workshop on Domain Adaptation forNatural Language Processing at the the Annual Meet-ing of the Association of Computational Linguistics(ACL).D.
Mimno, W. Li, and A. McCallum.
2007.
Mixturesof hierarchical topics with pachinko allocation.
Proc.ICML.A.
Popescu, P. Pantel, and G. Mishne.
2010.
Semanticlexicon adaptation for use in query interpretation.
19thWorld Wide Web Conference (WWW-10).D.
Ramage, D. Hall, R. Nallapati, and C. D. Man-ning.
2009.
Labeled lda: A supervised topic modelfor credit attribution in multi-labeled corpora.
Proc.EMNLP.J.
Reisinger and M. Pas?ca.
2009.
Latent variable modelsof concept-attribute attachement.
Proc.
of the AnnualMeeting of the Association of Computational Linguis-tics (ACL).J.
Reisinger and M. Pasca.
2011.
Fine-grained class la-bel markup of search queries.
In Proc.
of the AnnualMeeting of the Association of Computational Linguis-tics (ACL).M.
Sammons, V. Vydiswaran, and D. Roth.
2010.
Asknot what textual entailment can do for you...
In Proc.of the Annual Meeting of the Association of Computa-tional Linguistics (ACL), Uppsala, Sweden, 7.S.
Sarawagi and W. W. Cohen.
2004.
Semimarkovconditional random fields for information extraction.Proc.
NIPS.C.
Sauper, A. Haghighi, and R. Barzilay.
2011.
Contentmodels with attitude.
In Proc.
of the Annual Meet-ing of the Association of Computational Linguistics(ACL).G.
Tur and R. De Mori.
2011.
Spoken language under-standing: Systems for extracting semantic informationfrom speech.
Wiley.H.
Wallach, D. Mimno, and A. McCallum.
2009.
Re-thinking lda: Why priors matter.
NIPS.H.
Wallach.
2008.
Structured topic models for language.Ph.D.
Thesis, University of Cambridge.Y.Y.
Wang, R. Hoffman, X. Li, and J. Syzmanski.2009.
Semi-supervised learning of semantic classesfor query understanding from the web and for theweb.
In The 18th ACM Conference on Information andKnowledge Management.Y-Y.
Wang.
2010.
Strategies for statistical spoken lan-guage understanding with small amount of data - anemprical study.
Proc.
Interspeech 2010.338
