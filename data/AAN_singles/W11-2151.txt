Proceedings of the 6th Workshop on Statistical Machine Translation, pages 420?425,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsUPM system for the translation taskVer?nica L?pez-Lude?a Rub?n San-SegundoGrupo de Tecnolog?a del Habla Grupo de Tecnolog?a del HablaUniversidad Polit?cnica de Madrid Universidad Polit?cnica de Madridveronicalopez@die.upm.es lapiz@die.upm.esAbstractThis paper describes the UPM system fortranslation task at the EMNLP 2011 workshopon statistical machine translation(http://www.statmt.org/wmt11/), and it hasbeen used for both directions: Spanish-Englishand English-Spanish.
This system is based onMoses with two new modules for pre and postprocessing the sentences.
The maincontribution is the method proposed (based onthe similarity with the source language test set)for selecting the sentences for training themodels and adjusting the weights.
Withsystem, we have obtained a 23.2 BLEU forSpanish-English and 21.7 BLEU for English-Spanish.1 IntroductionThe Speech Technology Group of the UniversidadPolit?cnica de Madrid has participated in the sixthworkshop on statistical machine translation in theSpanish-English and English-Spanish translationtask.Our submission is based on the state-of-the-artSMT toolkit Moses (Koehn, 2010) adding a pre-processing and a post-processing module.
Themain contribution is a corpus selection method fortraining the translation models based on thesimilarity of each source corpus sentence with thelanguage model of the source language test set.There are several related works on filtering thetraining corpus by using a similarity measure basedon the alignment score or based on sentenceslength (Khadivi and Ney, 2005; Sanchis-Trilles etal, 2010).
However, these techniques are focusedon removing noisy data, i.e., their idea is toeliminate possible errors in the databases.The difference between these techniques and themethod that we propose is that we do not search?bad?
pairs of sentences, but we search thosesentences in source training corpus that are moresimilar with the language model generated with thesource test sentences and we select them fortraining.Other interesting technique of corpus selectionis based on transductive learning (Ueffing, 2007).In this work, authors use of transductive semi-supervised methods for the effective use ofmonolingual data from the source language inorder to improve translation quality.The method proposed in this paper is alsoapplied to the validation corpus.
There are otherworks related to select development set (Hui,2010) that they combine different development setsin order to find the more similar one with test set.2 Overall description of the systemThe translation system used is based on Moses,the software released to support the translation task(http://www.statmt.org/wmt11/) at the EMNLP2011 workshop on statistical machine translation.Figure 1: Moses translation system420The phrase model has been trained followingthese steps (Figure 1):?
Word alignment computation.
GIZA++ (Ochand Ney, 2003) is a statistical machinetranslation toolkit that is used to calculate thealignments between Spanish and English wordsin both direction (Spanish-English and English-Spanish).
To generate the translation model, theparameter ?alignment?
was fixed to ?grow-diag-final?
(default value), and the parameter?reordering?
was fixed to ?msd-bidirectional-fe?
as the best option, based on experiments onthe development set.?
Phrase extraction (Koehn et al2003).
All phrasepairs that are consistent with the wordalignment (grow-diag-final alignment in ourcase) are collected.
To extract the phrases, theparameter ?max-phrase-length?
was fixed to?7?
(default value), based on experiments onthe development set.?
Phrase scoring.
In this step, the translationprobabilities are computed for all phrase pairs.Both translation probabilities are calculated:forward and backward.The Moses decoder is used for the translationprocess (Koehn, 2010).
This program is a beamsearch decoder for phrase-based statistical machinetranslation models.
In order to obtain a 3-gramlanguage model, the SRI language modelingtoolkit has been used (Stolcke, 2002).In addition, a pre-processing module wasdeveloped for adapting the format of the corpusbefore training (pre-processing of training,development and test corpora).
And a post-processing for ordering punctuations, recasing, etc.is also applied to Moses output.3 Corpora used in these experimentsFor the system development, we have only usedthe free corpora distributed in the EMNLP 2011translation task.In particular, we have considered the union ofthe Europarl corpus, the United NationsOrganization (UNO) Corpus, the NewsCommentary Corpus and the test sets of 2000,2006, 2007 and 2008.For developing the system, we have developedand evaluated the system considering the union of2009 and 2010 test sets.All these files can be free downloaded fromhttp://www.statmt.org/wmt11/.A pre-processing of these databases is necessaryfor adapting the original format to our system.We have not used the complete union of allcorpora, but a corpus selection by filtering theunion of the training set and also filtering the unionof the development set.
This selection will beexplained in section 5.The main characteristics of the corpus are shownin Table 1: the previous corpora and the filteredcorpora.Table 1: Main characteristics of the corpus4 Preparing the corporaIn order to use the corpus described in section 3with the mentioned translation systems, it isnecessary a pre-processing.
This pre-processing,for training files, consists of:?
UTF-8 to Windows format conversion, becauseour software adapted to Windows had severalproblems with the UTF-8 format: it does notknow accent marks, ?
letter, etc.?
Deletion of blank lines and sentences that arecomments (for instance: ?<CHAPTER ID=1>?)?
Deletion of special characters (.,;:???
!-/\, etc.
),except those that are next to numbers (forinstance: ?1.4?, ?2,000?, ?1/3?).
We decided toremove these special characters to avoidincluding them in the translation model.
Duringtranslation, these characters will be consideredas phrase limits.Original sentencesFilteredsentencesTraining(TranslationModel (TM)/LanguageModel (LM))EuroparlTrainingCorpus1,650,152150,000(TM)3,000,000(LM)UNOCorpus 6,222,450Newscommentary 98,598Previous testsets 15,150Development news-test2009 2,525 1,000 news-test2010 2,489Test news-test2011 3,003 3,003421?
Words were kept in their natural case, but thefirst letter of each sentence was lowercased,because first words of sentences are used to belowercased as their most common form.?
Contracted words were separated for trainingeach word separately.
For instance, ?it?s?becomes ?it is?.
For the ambiguous cases, like?he?s?
that can be ?he is?
or ?he has?, we havenot done any further processing: we haveconsidered the most frequent situation.
For thecase of Saxon genitive, when proper names areused (instead of pronouns), ??s?
is a Saxongenitive most of the times.
But, when using apronoun, it is a contracted word.For development and test sets, the same actionswere carried out, but now, special characters werenot deleted, but separated in tokens, i.e., a blankspace was introduced between special charactersand adjacent words.
For instance, ?la bolsa dePraga , al principio del martes comercial ,reaccion?
inmediatamente a la ca?da del lunescuando descendi?
aproximadamente a un 6 %  .
?So, special characters are considered asindependent tokens in translation.
The main ideawas to force the system to consider specialcharacters as phrase limits during the translationprocess.5 Selecting the training corpusScattering of training data is a problem whenintegrating training material from different sourcesfor developing a statistical system.
In this case, wewant to use a big training corpus joining allavailable corpora obtaining about 8 millionssentences.But an excessive amount of data can produce animportant scattering that the statistical modelcannot learn properly.The technique proposed by the SpeechTechnology Group at UPM in the translation task(Spanish-English and English-Spanish) consists ofa filtering of the training data in order to obtainbetter results, without having memory problems.The first step is to compute a language model ofthe source language considering sentences totranslate (sentences from the 2011 source test set).Secondly, the system computes the similarity ofeach source sentence in the training to the languagemodel obtained in the first step.
This similarity iscomputed with the following formula:)1()log(10?==ninPnsimFor example, if one sentence is ?A B C D?
(where each letter is a word of the sentence):)2()(41BCDABCABA PPPPsim +++=Each probability is extracted from the languagemodel calculated in the first step.
This similarity isthe negative of the source sentence perplexitygiven the language model.With all the similarities, the mean and thestandard deviation values are computed and usedto define a threshold.
For example, calculating thesimilarity of all sentences in our train corpus(about 8,000,000 of sentences) a similarityhistogram is obtained (Figure 2).0200004000060000800001000001200001 11 21 31 41 51 61 71 81 91SentencesSimilarity?s?+sFigure 2: Similarity histogram of Spanish-EnglishsystemThis histogram indicates the number ofsentences inside each interval.
There are 100different intervals: the minimum similarity ismapped into 0 and the maximum one into 100.Finally, source training sentences with asimilarity lower than the threshold are eliminatedfrom the training set (the corresponding targetsentences are also removed).The whole process is shown in Figure 3.
Thisprocess takes 20 hours approximately for filtering422more than 8 million sentences in an Intel core 2quad computer.Source testsetPre-processTarget testsetPost-processN-gramprobabilities of thelanguage modelBigSource trainingsetBigTarget trainingsetSource trainingfiltered setTarget trainingfiltered setLanguageModelTranslationmodelTranslationTarget test setFeatures extractionClassificationFigure 3: Diagram of complete processFigure 4 shows the results of the experiments inSpanish-English system selecting the trainingcorpus with different similarity thresholds.
Theseresults were obtained before filtering thedevelopment corpus, with the same filteredtraining corpus for translation and language modelsand before post-processing.051015200 100000 200000 300000 400000 500000 600000BLEU?
(%)SentencesFigure 4: Translation results of baseline Spanish-English system with different number of trainingsentencesAs can be observed, with more than 400,000sentences there is a 12% BLEU (with anasymptotic tendency), but there is an importantimprovement filtering up to 100,000 (there isalready not scattering).
But results start to fall offwhen there are insufficient sentences (problem ofsparseness of data with less than 100,000sentences).6 Post processingAfter performing the statistical translation, wehave incorporated a post-processing module withthe following functions:?
To check the date format, detecting possibleorder errors and correcting them.?
To check the format of the numbers, numericaland ordinal ones: 1?
into 1st and so on.?
Detokenization and ordering the punctuationsmarks when there are several onesconsecutively (i.e.
??.?
or ?).?
), trying to follow,always, the same order.?
To put the first letter of the sentences in capitalletters.?
To use a backup dictionary for translatingisolated words.
This aspect has improved 2%(BLEU) but it has also introduced some errors.For example in the case of English-Spanish,there was a checking process for translatingEnglish words into Spanish.
But there wereseveral English words that also are Spanishwords.
For example, ?un?
is an article inSpanish but in English means ?United Nations?
(Naciones Unidas) so some ?un?
weretranslated as ?Naciones Unidas?
by error.7 Selecting the development corpusThe development corpus is used to adapt thedifferent weights used in the translation process forcombining the different sources of information.Weight computation is a sensible task.
In order tobetter adapt these weights, the development corpusis also filtered considering the same strategycommented in section 5.Our solution consists of using two differentcorpora (2009 and 2010 test sets) and ?choosing?the best sentences to use in development task with423the same filtering technique explained in section 5.Finally, we select the 1,000 sentences with thegreater similarity respect to the source languagemodel of the test set.Other action carried out in final experiments isusing different corpora for training translation andlanguage models.
In order to generate the languagemodel it is better to use a big corpus; so, we use3,000,000 sentences that it is the biggest modelthat we can generate without memory problems.But in order to generate the translation model,the final one is trained with 150,000 sentences.The final results are shown in Table 2.Spanish-English BLEU BLEU casedBaseline 12.57 12.15Best result 23.20 21.90English-Spanish BLEU BLEU casedBaseline 10.73 10.30Best result 21.70 20.90Table 2: Final results of the translation systemWith this work, we have demonstrated thatfiltering the corpus for training the translationmodule, can improve the translation results.
Butthere are still important problems that must beaddressed like the high number of out ofvocabulary words (OOVs) (more than 40% of thetest corpus vocabulary) that they have to beimproved in the selecting method.About the selection, it is important to commentthat this method more likely filters long sentencesout: the average number of words in the selectedcorpus is 14 while in the whole training set and inthe test set is higher than 25.Other interesting aspect to comment is that in theselected training corpus, more than 70% of thesentences come from the Europarl or the NewsCommentary corpus, being the UNO corpus thebiggest one.Anyway, although the improvement isinteresting, the system can not compete with otherwell-known translation systems until weincorporate additional modules for reordering or n-best post processing.8 ConclusionsThis paper has presented and described the UPMstatistical machine translation system for Spanish-English and English-Spanish.
This system is basedon Moses with pre-processing and post-processingmodules.
The main contribution has been theproposed method for selecting the sentences usedfor training and developing the system.
Thisselection is based on the similarity with the sourcelanguage test set.
The results have been 23.2BLEU for Spanish into English and 21.7 forEnglish into Spanish.9 Future workOne of the main problems we have observed in theselection proposed method has been the highnumber of OOVs during translation.
This problemhas been addressed by incorporating a backupvocabulary in the post-processing module.
Thissolution has solved some cases but it has not ableto deal with order problems.
Because of this, in thenear future, we will try to improve the corpusselection method for reducing the number ofOOVs.AcknowledgmentsThe authors would like to thank discussions andsuggestions from the colleagues at GTH-UPM.This work has been supported by Plan Avanza ExpN?
: TSI-020100-2010-489), INAPRA (MEC ref:DPI2010-21247-C02-02), and SD-TEAM (MECref: TIN2008-06856-C05-03) projects and FEDERprogram.ReferencesHui, C., Zhao, H., Song, Y., Lu, B., 2010 ?An EmpiricalStudy on Development Set Selection Strategy forMachine Translation Learning?
on Fifth Workshopon Statistical Machine Translation.Koehn P., F.J. Och D. Marcu.
2003.
?Statistical Phrase-based translation?.
Human Language TechnologyConference 2003 (HLT-NAACL 2003), Edmonton,Canada, pp.
127-133, May 2003.Koehn, Philipp.
2010.
?Statistical MachineTranslation?.
Cambridge University Press.Khadivi, S., Ney, H., 2005.
?Automatic filtering ofbilingual corpora for statistical machine translation.
?In Natural Language Processing and InformationSystems, 10th Int.
Conf.
on Applications of NaturalLanguage to Information Systems, volume 3513 ofLecture Notes in Computer Science, pages 263?274,Alicante, Spain, June.
Springer.424Och J., Ney.
H., 2003.
?A systematic comparison ofvarious alignment models?.
ComputationalLinguistics, Vol.
29, No.
1 pp.
19-51, 2003.Sanchis-Trilles, G., Andr?s-Ferrer, J., Gasc?, G.,Gonz?lez-Rubio, J., Mart?nez-G?mez, P., Rocha, M.,S?nchez, J., Casacuberta, F., 2010.
?UPV-PRHLTEnglish?Spanish System for WMT10?.
On ACLFifth Workshop on Statistical Machine Translation.Stolcke A., 2002.
?SRILM ?
An Extensible LanguageModelling Toolkit?.
Proc.
Intl.
Conf.
on SpokenLanguage Processing, vol.
2, pp.
901-904, Denver.Ueffing, N., Haffari, G., Sarkar, A., 2007.?Transductive learning for statistical machinetranslation?.
On ACL Second Workshop onStatistical Machine Translation.425
