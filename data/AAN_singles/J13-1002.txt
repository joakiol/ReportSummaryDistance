Going to the Roots of Dependency ParsingMiguel Ballesteros?Complutense University of MadridJoakim Nivre?
?Uppsala UniversityDependency trees used in syntactic parsing often include a root node representing a dummyword prefixed or suffixed to the sentence, a device that is generally considered a mere technicalconvenience and is tacitly assumed to have no impact on empirical results.
We demonstrate thatthis assumption is false and that the accuracy of data-driven dependency parsers can in fact besensitive to the existence and placement of the dummy root node.
In particular, we show thata greedy, left-to-right, arc-eager transition-based parser consistently performs worse when thedummy root node is placed at the beginning of the sentence (following the current conventionin data-driven dependency parsing) than when it is placed at the end or omitted completely.Control experiments with an arc-standard transition-based parser and an arc-factored graph-based parser reveal no consistent preferences but nevertheless exhibit considerable variation inresults depending on root placement.
We conclude that the treatment of dummy root nodes indata-driven dependency parsing is an underestimated source of variation in experiments andmay also be a parameter worth tuning for some parsers.1.
IntroductionIt is a lesson learned in many studies on natural language processing that choosing theright linguistic representation can be crucial for obtaining high accuracy on a given task.In constituency-based parsing, for example, adding or deleting nodes in syntactic treescan have a substantial impact on the performance of a statistical parser.
In dependencyparsing, the syntactic representations used offer less opportunity for transformation,given that the nodes of a dependency tree are basically determined by the tokens ofthe input sentence, except for the possible addition of a dummy word acting as theroot of the tree.
In this article, we show that even this seemingly trivial modificationcan make a difference, and that the exact placement of the dummy root node canhave a significant impact on the accuracy of a given parser.
This suggests that theplacement of the dummy root is a parameter worth tuning for certain parsing systemsas well as a source of variation to be taken into account when interpreting experimentalresults.?
Universidad Complutense de Madrid, Departamento de Ingenier?
?a del Software e Inteligencia Artificial,C/ Prof. Jose?
Garc?
?a Santesmases, s/n, 28040 Madrid, Spain.
E-mail: miballes@fdi.ucm.es.??
Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden.E-mail: joakim.nivre@lingfil.uu.se.Submission received: 25 July 2012; revised submission received: 13 October 2012; accepted for publication:19 October 2012.?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 12.
Dependency GraphsDependency-based approaches to syntactic parsing assume that the syntactic structureof a sentence can be analyzed in terms of binary dependency relations between lexicalunits, units that in the simplest case are taken to correspond directly to the tokensof the sentence.
It is very natural to represent this structure by a directed graph,with nodes representing input tokens and arcs representing dependency relations.In addition, we can add labels to arcs in order to distinguish different dependencytypes or grammatical functions (e.g., subject, object, adverbial).
We call such a graph adependency graph.Dependency graphs are normally assumed to satisfy certain formal constraints,such as the single-head constraint, which forbids more than one incoming arc to a node,and the acyclicity constraint, ruling out cyclic graphs.
Many dependency theories andannotation schemes further require that the graph should be a tree, with a unique roottoken on which all other tokens are transitively dependent, whereas other frameworksallow more than one token to be a root in the sense of not having any incoming arc.
Asimple and elegant way of reconciling such cross-framework differences and arrivingat a single formalization of dependency structures is to add a dummy root node, a specialnode that does not correspond to any input token, and to require that the dependencygraph is a tree rooted at this node.
The original tree constraint can then be enforcedby requiring that the special node has exactly one child, but not all frameworks need toenforce this constraint.
An additional advantage of adding a dummy root node is that itsoutgoing arcs can be labeled to indicate the functional status of what would otherwisesimply be unlabeled root nodes.
With a slight misuse of terminology, we call such labelsinformative root labels.1Because the dummy root node does not correspond to an input token, it has no well-defined position in the node sequence defined by the word order of a sentence and couldin principle be inserted anywhere (or nowhere at all).
One option that can be found inthe literature is to insert it at the end of this sequence, but the more common conventionin contemporary research on dependency parsing is to insert it at the beginning, hencetreating it as a dummy word prefixed to the sentence.
This is also the choice implicitlyassumed in the CoNLL data format, used in the CoNLL shared tasks on dependencyparsing in 2006 and 2007 (Buchholz and Marsi 2006; Nivre et al2007) and the currentde facto standard for exchange of dependency annotated data.The question that concerns us here is whether the use of a dummy root node is justa harmless technicality permitting us to treat different dependency theories uniformly,and whether its placement in the input sequence is purely arbitrary, or whether both ofthese choices may in fact have an impact on the parsing accuracy that can be achievedwith a given parsing model.
In order to investigate this question empirically, we definethree different types of dependency graphs that differ only with respect to the existenceand placement of the dummy root node:1.
None: Only nodes corresponding to tokens are included in the graph.2.
First: A dummy root node is added as the first token in the sentence.3.
Last: A dummy root node is added as the last token in the sentence.1 For example, in the Prague Dependency Treebank, which allows multiple children of the dummy rootnode, the label may indicate whether the child functions as a main predicate, as the head of a coordinatestructure, or as final punctuation.6Ballesteros and Nivre Going to the Roots of Dependency ParsingFigure 1 illustrates the three types of dependency graphs with examples taken fromthe Penn Treebank of English (Marcus, Santorini, and Marcinkiewicz 1993) convertedto dependency structure using the procedure described in Nivre (2006), and the PragueDependency Treebank of Czech (Hajic?
et al2001; Bo?hmova?
et al2003).
In the formercase, it is assumed that the dummy root node always has exactly one child, with adummy dependency label ROOT.
In the latter case, the dummy root node may haveseveral children and these children have informative root labels indicating their func-tion (Pred and AuxK in the example).
Note also that the Czech dependency graph oftype None is not a tree, but a forest, because it consists of two disjoint trees.3.
ExperimentsIn order to test the hypothesis that the existence and placement of the dummy root nodecan have an impact on parsing accuracy, we performed an experiment using two widelyused data-driven dependency parsers, MaltParser (Nivre, Hall, and Nilsson 2006) andMSTParser (McDonald 2006), and all the 13 data sets from the CoNLL 2006 sharedtask on multilingual dependency parsing (Buchholz and Marsi 2006) as well as theEnglish Penn Treebank converted to Stanford dependencies (de Marneffe, MacCartney,and Manning 2006).
We created three different versions of each data set, correspondingto the representation types None, First, and Last, and used them to evaluate MaltParserwith two different transition systems?arc-eager (Nivre 2003) and arc-standard (Nivre2004)?and MSTParser with the arc-factored non-projective algorithm (McDonaldet al2005).
The results are shown in Table 1.When creating the data sets, we took the original version from the CoNLL-X sharedtask as None, because it does not include the dummy root node as an explicit inputtoken.
In this representation, the tokens of a sentence are indexed from 1 to n and thedependency graph is specified by giving each word a head index ranging from 0 to n,where 0 signifies that the token is not a dependent on any other token in the sentence.The First version was created by adding an extra token at the beginning of the sentencewith index 1 and head index 0, increasing all other token and head indices by 1, meaningthat all tokens that previously had a head index of 0 would now be attached to the newEconomic1 NMODnews2 SBJhad3 little4 NMODeffect5 OBJon6 NMODfinancial7 NMODmarkets8 PMOD.9 PZ1 AuxPnich2 Atrje3 jen4 AuxZjedna5 Sbna6 AuxPkvalitu7 Adv.8ROOT1 Economic2 NMODnews3 SBJhad4 ROOTlittle5 NMODeffect6 OBJon7 NMODfinancial8 NMODmarkets9 PMOD.10 PROOT1 Z2 AuxPnich3 Atrje4 Predjen5 AuxZjedna6 Sbna7 AuxPkvalitu8 Adv.9 AuxKROOT10Economic1 NMODnews2 SBJhad3 ROOTlittle4 NMODeffect5 OBJon6 NMODfinancial7 NMODmarkets8 PMOD.9 PROOT9Z1 AuxPnich2 Atrje3 Predjen4 AuxZjedna5 Sbna6 AuxPkvalitu7 Adv.8 AuxKFigure 1Dependency graph types None (top), First (middle), and Last (bottom) for an Englishsentence from the Penn Treebank (left) and a Czech sentence taken from the PragueDependency Treebank (right).
(Gloss of Czech sentence: Z/Out-of nich/them je/is jen/only jedna/one-FEM-SG na/to kvalitu/quality ./.
= ?Only one of them concerns quality.?
)7Computational Linguistics Volume 39, Number 1Table 1Experimental results for arc-eager (AE), arc-standard (AS), and maximum spanning treeparsing (MST) on all the CoNLL-X data sets plus the English Penn Treebank converted toStanford dependencies with three different dependency graph types (None, First, Last).Evaluation metrics are labeled attachment score (LAS), unlabeled attachment score (UAS), rootattachment (or no attachment in the case of None) measured as recall (RR) and precision (RP).Scores in bold are best in their column (per language); scores in italic are not comparable to therest because of informative arc labels that cannot be predicted with the None representation.AE AS MSTLanguage Type LAS UAS RR RP LAS UAS RR RP LAS UAS RR RPArabicNone 60.00 75.17 74.24 69.52 60.48 77.29 81.69 80.60 66.73 78.96 84.07 83.78First 63.63 74.57 84.75 73.75 64.93 77.09 83.73 81.79 66.41 78.32 83.39 90.44Last 64.15 74.97 73.56 68.24 65.29 77.31 82.71 81.06 66.41 78.32 78.31 87.83BulgarianNone 85.76 90.80 94.22 90.80 85.06 90.33 91.96 91.96 86.30 91.64 98.24 98.24First 84.64 89.83 90.20 87.56 85.12 90.33 91.71 91.71 86.32 91.28 97.49 97.49Last 85.76 90.78 94.22 90.58 85.16 90.33 91.96 91.96 86.14 91.28 97.24 97.24ChineseNone 85.13 89.68 93.63 88.42 85.25 90.08 93.06 93.06 86.88 90.82 94.33 94.33First 84.59 89.09 92.25 89.55 85.23 90.10 92.82 92.82 86.54 90.68 94.33 94.33Last 85.15 89.70 93.63 88.42 85.17 90.00 92.82 92.82 86.36 90.52 93.87 93.97CzechNone 68.30 81.14 80.51 74.61 68.36 81.96 87.85 73.35 76.70 85.98 82.20 80.83First 72.98 79.96 83.33 72.66 74.88 82.52 86.44 88.95 77.04 86.34 85.88 84.92Last 73.96 81.16 81.07 75.53 74.28 81.78 87.01 73.16 77.68 86.70 89.55 89.55DanishNone 82.36 87.88 91.33 88.06 81.64 87.86 92.88 93.17 83.39 89.46 92.57 91.44First 80.60 86.59 86.69 82.11 81.66 87.86 92.88 93.17 83.97 89.84 94.74 94.94Last 82.38 87.94 91.64 88.36 81.52 87.74 92.88 92.31 83.43 89.42 92.26 92.26DutchNone 71.09 74.51 65.56 66.08 70.67 74.43 69.84 72.53 79.05 83.49 79.77 79.92First 70.81 74.41 72.18 57.88 71.07 75.23 64.20 82.09 78.91 83.43 74.32 83.59Last 71.05 74.51 65.76 66.67 70.65 74.45 69.84 72.38 78.25 82.95 75.10 85.21EnglishNone 88.63 90.47 88.70 81.75 88.15 90.07 87.83 86.42 87.55 89.91 87.70 87.70First 88.00 90.04 83.99 85.42 88.04 89.95 86.20 86.24 87.63 90.00 90.20 90.17Last 88.57 90.46 88.58 81.73 88.16 90.07 87.91 86.61 87.69 90.06 88.45 88.38GermanNone 83.85 86.64 94.68 85.14 84.31 87.22 93.84 93.84 85.64 89.54 97.76 97.76First 83.29 86.08 89.64 90.40 84.37 87.24 93.84 93.84 85.74 89.66 97.48 97.48Last 83.93 86.72 94.68 85.14 84.35 87.22 93.84 93.84 85.34 89.50 97.76 97.76JapaneseNone 89.85 92.10 92.74 85.20 90.15 92.30 92.53 85.42 90.45 93.02 93.38 88.47First 88.79 91.27 88.15 89.20 89.13 91.57 87.83 90.94 90.83 93.36 92.85 91.58Last 89.77 92.12 92.96 84.56 90.01 92.28 92.64 85.60 90.47 93.06 92.21 90.66PortugueseNone 79.12 88.60 92.01 85.76 78.32 87.78 90.28 90.28 84.87 89.74 89.58 89.58First 83.77 88.36 87.85 86.35 83.45 87.82 90.62 90.62 85.19 90.26 91.67 91.67Last 84.17 88.62 92.01 85.76 83.47 87.80 90.62 90.62 84.89 89.26 90.62 90.31SpanishNone 78.64 82.51 83.25 76.28 77.88 81.69 81.73 81.73 79.40 83.57 79.70 78.50First 78.14 82.15 79.70 73.36 77.64 81.51 81.22 81.22 79.20 83.41 83.76 84.18Last 78.64 82.49 83.76 76.39 77.72 81.55 80.71 81.12 79.48 83.53 84.77 83.50SwedishNone 83.49 89.60 93.32 90.07 82.65 89.42 92.03 91.56 81.36 88.29 89.97 90.21First 83.13 89.29 91.77 89.47 82.53 89.38 91.77 91.77 81.76 88.59 91.52 91.75Last 83.59 89.70 93.32 90.07 82.65 89.36 91.77 91.30 81.66 88.35 92.03 92.03SloveneNone 64.25 79.56 73.98 63.46 63.67 79.28 75.26 67.35 71.44 82.47 79.08 75.98First 67.73 77.84 71.94 64.83 69.40 79.42 73.47 78.69 71.72 82.67 79.34 79.34Last 69.98 79.62 75.00 64.05 69.42 79.28 75.26 67.66 71.64 82.33 76.79 79.21TurkishNone 56.66 72.18 90.29 92.25 57.00 72.06 90.59 92.13 58.49 74.55 93.47 86.88First 56.48 71.86 88.77 93.00 56.80 72.12 89.68 94.86 58.59 74.59 92.56 94.28Last 56.64 72.16 90.14 91.95 56.88 72.10 90.59 92.13 58.89 74.83 93.02 94.45AverageNone 76.94 84.35 86.32 81.24 76.69 84.41 87.24 85.24 79.88 86.53 88.70 87.40First 77.61 83.67 85.09 81.11 78.16 84.44 86.17 88.48 79.99 86.60 89.25 90.44Last 78.41 84.35 86.45 81.25 78.20 84.38 87.18 85.18 79.88 86.44 88.71 90.178Ballesteros and Nivre Going to the Roots of Dependency Parsingdummy root token.
The Last version was created by adding an extra token at the endof the sentence with index n+1, and changing every head index that previously was 0to n+1.
In both First and Last, we made sure that the new dummy token had a uniqueword form and unique values for all other features, so that it could not be mistaken forany real word.
For First and Last, we applied an inverse transformation to the parseroutput before evaluation.Both MaltParser and MSTParser by default add a dummy root node at the be-ginning of the sentence internally before parsing, so we had to modify the parsersso that they only considered arcs involving nodes corresponding to input tokens.
ForMaltParser this only required setting a flag that makes the parser start with an emptystack instead of a stack containing an extra dummy root node.2 For MSTParser, wemodified the parser implementation so that it extracts a maximum spanning tree thatis still rooted in an extra dummy root node but where the score of a tree is based onlyon the scores of arcs connecting real token nodes.
Finally, because MaltParser with thearc-eager and arc-standard transition systems can only construct projective dependencygraphs, we projectivized all training sets before training the MaltParser models usingthe baseline pseudo-projective transformation of Nivre and Nilsson (2005).3 Except forthese modifications, all parsers were run with out-of-the-box settings.3.1 Deterministic Arc-Eager ParsingThe arc-eager transition-based parser first described in Nivre (2003) parses a sentencein a single pass from left to right, using a stack to store partially processed tokensand greedily choosing the highest-scoring parsing action at each point.
The arc-eagerproperty entails that every arc in the output graph is added at the earliest possibleopportunity, which means that right-dependents are attached to their head before theyhave found their own right-dependents.
This can be an advantage because the earlyattachment neither implies nor precludes the later addition of right-dependents, butit can also be a drawback because it forces the parser to make an early commitmentabout right-dependents.
In this context, it is especially relevant that the addition of adummy root node at the beginning of the sentence (First) forces the parser to makean early commitment regarding dependents of this root node.
By contrast, if a dummyroot node is added at the end of a sentence (Last), decisions regarding root dependentswill be postponed until the end.
Similarly, if no root node is added (None), then thesedecisions will not be explicitly modeled at all, meaning that whatever nodes remainon the stack after parsing will be treated as root dependents.As can be seen from Table 1, the arc-eager parser performs consistently worse underthe First condition, with an average unlabeled attachment score (UAS) of 83.67 over the14 languages, to be compared with 84.35 for None and Last.
The difference in accuracybetween First and None/Last ranges from 0.10 for Dutch to 1.72/1.78 for Slovene, andthe difference in means is highly statistically significant (p < 0.001, Wilcoxon signed-rank test).
The difference between None and Last is never greater than 0.20 (and veryfar from statistically significant), indicating that either postponing or excluding rootattachment decisions leads to very similar performance for the arc-eager parser.
The2 The exact commandline flag is -allow root false.
A side effect of this flag is that all unattached tokensget the dummy label ROOT, meaning that informative root labels cannot be predicted for representationsof type None.
See http://maltparser.org for more information.3 This is done with the MaltParser flag -pp baseline.9Computational Linguistics Volume 39, Number 1same pattern is found for labeled attachment score (LAS), but here we can only directlycompare First and Last because the Arabic, Czech, Portuguese, and Slovene data setscontain informative root labels that cannot be predicted under the None condition (cf.footnote 2).
The difference in means between First and Last is 0.80 and again highlystatistically significant (p < 0.001, Wilcoxon signed-rank test).
A closer look at the rootaccuracy suggests that most of the difference stems from a lower recall on root depen-dents with the First representation, but this pattern is not completely consistent acrosslanguages and Arabic, Czech, and Dutch actually have higher recall.
For Czech andDutch this is accompanied by lower precision, but for Arabic the First representationactually gives the best recall and precision of root dependents (but nevertheless theworst overall attachment score).
It is probably significant that the Arabic data set hasthe longest sentences with the root word often appearing early in the sentence.
Hence,an early commitment to root attachment is more likely to be correct in this case, even ifit is more error prone in general.3.2 Deterministic Arc-Standard ParsingThe arc-standard transition-based parser first described in Nivre (2004) is similar to thearc-eager parser in that it parses a sentence in a single pass from left to right, usinga stack to store partially processed tokens and greedily choosing the highest-scoringparsing action at each point.
It differs by postponing the attachment of right-dependentsuntil the complete subtree under the dependent itself has been built.
As a consequence,the dependency tree is built strictly bottom?up, which means that attachment to adummy root node will always happen at the end, regardless of whether the dummyroot node is positioned at the beginning or at the end of the sentence.
There is thereforeno reason to expect that the placement of the dummy root node should have the sameimpact as for the arc-eager parser.Looking at the results for the arc-standard parser in Table 1 confirms this expecta-tion, with the three conditions giving very similar mean UAS (84.41 for None, 84.44 forFirst, 84.38 for Last) and none of the differences being statistically significant.
For LAS,we can again only directly compare First and Last, but there is practically no differencein the means here either (78.16 for First vs. 78.20 for Last).
Nevertheless, it is worthnoting that, for individual languages, differences in scores can be quite substantial.Thus, for Dutch, the First condition outperforms the None/Last condition by 0.80/0.78in UAS and 0.40/0.42 in LAS.
Conversely, for Japanese, the None/Last conditions arebetter than First by 0.73/0.71 in UAS and 1.02/0.88 in LAS.
Although the general trendis that None and Last give the most similar results, just as for the arc-eager parser,there are also cases like Chinese where None and First are both slightly better thanLast.
Zooming in on root accuracy, we see a clear gain in precision (and marginal dropin recall) for the First representation, which is probably an effect of the arc-standardstrategy where the attachment of right-dependents often have to be delayed whereasleft-dependents can be attached eagerly.3.3 Maximum Spanning Tree ParsingThe maximum spanning tree parser described in McDonald et al(2005) uses a verydifferent parsing model compared with the two transition-based parsers.
Instead ofscoring individual parsing actions, it scores all possible dependency arcs in the sentenceand then uses exact inference to extract the highest-scoring complete dependency tree10Ballesteros and Nivre Going to the Roots of Dependency Parsingunder an arc-factored model, where the score of each tree is the sum of the scores ofits component arcs.
Because the parsing algorithm does not impose any ordering at allon different attachments, we would expect even less impact from the placement of thedummy root node than for the deterministic arc-standard parser.The results in Table 1 do not quite confirm this expectation.
The mean UAS variesfrom 86.44 for the Last condition to 86.60 for the First condition, and the mean LASis 79.88 for None and Last but 79.99 for First.
Although none of these differences isstatistically significant on the aggregate level, differences can be quite substantial forindividual languages, with First outperforming Last by a whole percentage point inUAS for Portuguese (but only 0.30 in LAS) and None outperforming both First and Lastby 0.64 for Arabic (and 0.32 in LAS).
The fact that LAS differences tend to be smallerthan UAS differences can probably be explained by the fact that MSTParser uses a two-stage approach, where the second labeling stage is the same for all three conditions.With respect to root accuracy, the most interesting observation is that both First andLast seem to given higher precision than None, which suggests that it is an advantageto represent root attachments explicitly so that features over these arcs can contribute tothe overall score of a parse tree.
It is also worth noting that these features are differentfor First and Last, despite the arc-factored model, because of the so-called ?in-betweenfeatures?
that record the part-of-speech tags of words occurring between the headand the dependent of an arc, which in turn explains why these two conditions do notalways give the same results.4.
DiscussionThe main conclusion we draw from this experiment is that the addition of a dummyword prefixed or suffixed to a sentence is not a mere technical convenience withoutimpact on empirical results.
Whether we include a dummy word representing the rootof the dependency tree and, if so, where we place this word in the sequence of inputtokens, can have a non-negligible effect on parsing accuracy for different parsers?insome cases resulting in statistically significant differences with a magnitude of severalpercentage points according to standard evaluation metrics.The nature and magnitude of the impact definitely depends on the parsing modelused.
Whereas the deterministic arc-eager parser gives consistently worse results with adummy root node positioned at the beginning of the sentence, neither the deterministicarc-standard parser nor the maximum spanning tree parser has any clear preference inthis respect.
Although the overall patterns emerging when averaging over many datasets can largely be explained in this way, there is also considerable variation across datasets that we do not yet fully understand, however.
Zooming in on root accuracy hasallowed us to start forming hypotheses, such as the impact of long sentences in com-bination with predominantly head-initial structures for Arabic, but a full explorationof the interaction of parsing models and language-specific properties is clearly outsidethe scope of this article and has to be left for future research.
Another limitation ofthe current study is that it only examines three different parsers, and although thisis clearly sufficient to prove the existence of the phenomenon it will be interesting tosee whether the same patterns can be found if we examine more recent state-of-the-artmethods, going from deterministic parsing to beam search for transition-based parsingand from arc-factored to higher-order models for graph-based parsing.
In this context,it is also relevant to mention previous work, such as Hall et al(2007) and Attardiand Dell?Orletta (2009), which have tried to improve parsing accuracy by switching or11Computational Linguistics Volume 39, Number 1combining parsing directions, which implicitly has the effect of changing the positionof the root node (if present).In conclusion, we believe there may be two methodological lessons to learn fromour experiments.
The first is that, for certain parsing models, the existence and place-ment of the dummy root node is in fact a parameter worth tuning for best performance.Thus, for the deterministic arc-eager parser, it seems that we can obtain higher parsingaccuracy by placing the dummy root node at the end of the sentence (or omittingit completely) instead of placing it at the beginning in the sentence, as is currentlythe norm in data-driven dependency parsing.
The second lesson is that, because thedifferences observed between different conditions are sometimes at least as large asthe differences considered significant when comparing different parsing models, thestatus of the dummy root node may be an underestimated source of variation and avariable that needs to be controlled for in experimental evaluations.
The current practiceof consistently placing the root node at the beginning of the sentence is one way ofensuring comparability of results, but given the arbitrariness of this decision togetherwith our experimental results, it may be worth exploring other representations as well.AcknowledgmentsThanks to Ryan McDonald, Yoav Goldberg,and three anonymous reviewers for usefulcomments, and to Ryan also for help inmodifying MSTParser.
Miguel Ballesteros isfunded by the Spanish Ministry of Educationand Science (TIN2009-14659-C03-01 Project).ReferencesAttardi, Giuseppe and Felice Dell?Orletta.2009.
Reverse revision and linear treecombination for dependency parsing.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL HLT),pages 261?264, Boulder, CO.Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,and Barbora Hladka?.
2003.
The PragueDependency Treebank: A three-levelannotation scenario.
In Anne Abeille?,editor, Treebanks: Building and UsingParsed Corpora.
Kluwer, pages 103?127.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings ofthe 10th Conference on ComputationalNatural Language Learning (CoNLL),pages 149?164, New York, NY.de Marneffe, Marie-Catherine, BillMacCartney, and Christopher D.Manning.
2006.
Generating typeddependency parses from phrasestructure parses.
In Proceedings ofthe 5th International Conference onLanguage Resources and Evaluation(LREC), pages 449?454, Genoa.Hajic?, Jan, Barbora Vidova Hladka, JarmilaPanevova?, Eva Hajic?ova?, Petr Sgall, andPetr Pajas.
2001.
Prague DependencyTreebank 1.0.
LDC, 2001T10.Hall, Johan, Jens Nilsson, Joakim Nivre,Gu?lsen Eryig?it, Bea?ta Megyesi, MattiasNilsson, and Markus Saers.
2007.
Singlemalt or blended?
A study in multilingualparser optimization.
In Proceedings of theCoNLL Shared Task of EMNLP-CoNLL 2007,pages 933?939, Prague.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.McDonald, Ryan.
2006.
DiscriminativeLearning and Spanning Tree Algorithmsfor Dependency Parsing.
Ph.D. thesis,University of Pennsylvania.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic?.
2005.Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings ofthe Human Language Technology Conferenceand the Conference on Empirical Methods inNatural Language Processing (HLT/EMNLP),pages 523?530, Vancouver.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.In Proceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT),pages 149?160, Nancy.Nivre, Joakim.
2004.
Incrementality indeterministic dependency parsing.In Proceedings of the Workshop onIncremental Parsing: Bringing Engineeringand Cognition Together (ACL), pages 50?57,Barcelona.12Ballesteros and Nivre Going to the Roots of Dependency ParsingNivre, Joakim.
2006.
Inductive DependencyParsing.
Springer, Berlin.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.In Proceedings of the CoNLL Shared Task ofEMNLP-CoNLL 2007, pages 915?932,Prague.Nivre, Joakim, Johan Hall, and Jens Nilsson.2006.
Maltparser: A data-drivenparser-generator for dependency parsing.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC), pages 2216?2219,Genoa.Nivre, Joakim and Jens Nilsson.
2005.Pseudo-projective dependency parsing.In Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 99?106,Ann Arbor, MI.13
