Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 363?366,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDuluth-WSI: SenseClusters Applied to theSense Induction Task of SemEval-2Ted PedersenDepartment of Computer ScienceUniversity of Minnesota, DuluthDuluth, MN 55812tpederse@d.umn.eduhttp://senseclusters.sourceforge.netAbstractThe Duluth-WSI systems in SemEval-2built word co?occurrence matrices fromthe task test data to create a second orderco?occurrence representation of those testinstances.
The senses of words were in-duced by clustering these instances, wherethe number of clusters was automaticallypredicted.
The Duluth-Mix system was avariation of WSI that used the combina-tion of training and test data to create theco-occurrence matrix.
The Duluth-R sys-tem was a series of random baselines.1 IntroductionThe Duluth systems in the sense induction taskof SemEval-2 (Manandhar et al, 2010) werebased on SenseClusters (v1.01), a freely availableopen source software package which relies on thepremise that words with similar meanings will oc-cur in similar contexts (Purandare and Pedersen,2004).
The data for the sense induction task in-cluded 100 ambiguous words made up of 50 nounsand 50 verbs.
There were a total of 8,915 test in-stances and 879,807 training instances provided.Note that neither the training nor the test data wassense tagged.
The training data was made avail-able as a resource for participants, with the under-standing that system evaluation would be done onthe test instances only.
The organizers held back agold standard annotation of the test data that wasonly used for evaluation.Five Duluth-WSI systems participated in thistask, six Duluth-Mix systems, and five DuluthRandom systems.
The WSI and Mix systems al-most always represented the test instances usingsecond order co?occurrences, where each word ina test instance is replaced by a vector that showsthe words with which it co-occurs.
The word vec-tors that make up a test instance are averaged to-gether to make up a new representation for thatinstance.
All the test instances for a word are clus-tered, and the number of senses is automaticallypredicted by either the PK2 measure or AdaptedGap Statistic (Pedersen and Kulkarni, 2006).In the Duluth systems the co-occurrence matri-ces are either based on order-dependent bigramsor unordered pairs of words, both of which can beseparated by up to some given number of interven-ing words.
Bigrams are used to preserve distinc-tions between collocations such as cat house andhouse cat, whereas co?occurrences do not con-sider order and would treat these two as beingequivalent.2 Duluth-WSI systemsThe Duluth-WSI systems build co-occurrence ma-trices from the test data by identifying bigrams orco?occurrences that occur with up to eight inter-mediate words between them in instances of am-biguous nouns, and up to 23 intermediate wordsfor the verbs.
Any bigram (bi) or co?occurrence(co) that occurs more than 5 times with up to theallowed number of intervening words and has sta-tistical significance of 0.95 or above according tothe left-sided Fisher?s exact test was selected (Ped-ersen et al, 1996).
Some of the WSI systems re-duce the co?occurrence matrix to 300 dimensionsusing Singular Value Decomposition (SVD).The resulting co-occurrence matrix was used tocreate second order co?occurrence vectors to rep-resent the test instances, which were clustered us-ing the method of repeated bisections (rb), wheresimilarity was measured using the cosine.
Table1 summarizes the distinctions between the variousDuluth-WSI systems.3 Duluth-Mix systemsThe Duluth-Mix systems used the combination ofthe test and training data to identify features to rep-resent the test instances.
The goal of this combi-363Table 1: Duluth-WSI Distinctionsname optionsDuluth-WSI bigrams, no SVD, PK2Duluth-WSI-Gap bigrams, no SVD, GapDuluth-WSI-SVD bigrams, SVD, PK2Duluth-WSI-Co co-occur, no SVD, PK2Duluth-WSI-Co-Gap co-occur, no SVD, Gapnation was to increase the amount of data that wasavailable for feature identification.
Since therewas a larger amount of data, some parameter set-tings as used in Duluth-WSI were reduced.For example, the Duluth-Mix-PK2 and Duluth-Mix-Gap are identical to the Duluth-WSI andDuluth-WSI-Gap systems, except that they limitboth nouns and verbs to 8 intervening words.Duluth-Mix-Narrow-PK2 and Duluth-Mix-Narrow-Gap are identical to Duluth-Mix-PK2and Duluth-Mix-Gap except that bigrams andco?occurrences must be made up of adjacentwords, with no intermediate words allowed.Duluth-Mix-Uni-PK2 and Duluth-Mix-Uni-Gap are unique among the Duluth systems inthat they do not use second order co-occurrences,but instead rely on first order co-occurrences.These are simply individual words (unigrams) thatoccur more than 5 times in the combined test andtraining data.
These features are used to generateco-occurrence vectors for the test instances whichare then clustered (this is very similar to a bag ofwords model).4 Duluth-Random systemsDuluth-R12, Duluth-R13, Duluth-R15, andDuluth-R110 provide random baselines.
R12randomly assigns each instance to one of twosenses, R13 to one of three, R15 to one of five,and R110 to one of ten senses.
Random numbersare generated in the given range with equalprobability, so the distribution of assigned sensesis balanced.5 DiscussionThe evaluation of unsupervised sense discrimina-tion and induction systems is still not standard-ized, so an important part of any exercise likeSemEval-2 is to scrutinize the evaluation measuresused in order to determine to what degree they areproviding a useful and reasonable way of evaluat-ing system results.5.1 Evaluation MeasuresEach participating system was scored by three dif-ferent evaluation methods: the V-measure (Rosen-berg and Hirschberg, 2007), the supervised recallmeasure (Agirre and Soroa, 2007), and the pairedF-score (Artiles et al, 2009).
The results of theevaluation are in some sense confusing - a sys-tem that ranks near the top according to one mea-sure may rank at the bottom or middle of another.There was not any single system that did well ac-cording to all of the different measures.
The sit-uation is so extreme that in some cases a systemwould perform near the top in one measure, andthen below random baselines in another.
Thesestark differences suggest a real need for continueddevelopment of other methods for evaluating un-supervised sense induction.One minimum expectation of an evaluationmeasure is that it should expose and identify ran-dom baselines by giving them low scores thatclearly distinguish them from actual participatingsystems.
The scores of all the evaluation mea-sures used in this task when applied to differentrandom baseline systems are summarized in Table2.
These include a number of post-evaluation ran-dom clustering systems, which are referred to aspost-R1k, where k is the number of random clus-ters.5.1.1 V-measureThe V-measure appears to be quite easily misleadby random baselines.
As evidence of that, theDuluth-R (random) systems got increasingly bet-ter scores the more random they became, and infact the post-evaluation random systems reachedlevels of performance better than any of the partic-ipating systems.
Table 2 shows that the V-measurecontinues to improve (rather dramatically) as ran-domness increases.The average number of senses in the gold stan-dard data for all 100 words was 3.79.
The offi-cial random baseline assigned one of four randomsenses to each instance of a word, and achieveda V-measure of 4.40.
Duluth-R15 improved theV-measure to 5.30 by assigning one of five ran-dom senses, and Duluth-R110 improved it againto 8.60 by assigning one of ten random senses.The more random the result, the better the score.In fact Duluth-R110 placed sixth in the sense in-364duction task according to the V-measure.
In post-evaluation experiments a number of additionalrandom baselines were explored, where instanceswere assigned senses randomly from 20, 33, and50 possible values per word.
The V-measures forthese random systems were 13.9, 18.7, and 23.2respectively, where the latter two were better thanthe first place participating system (which scored16.2).
In a post-evaluation experiment, the taskorganizers found that assigning one sense per in-stance resulted in a V?measure of 31.7.5.1.2 Supervised RecallThe supervised recall measure takes the sense in-duction results (on the 8,915 test instances) as sub-mitted by a participating system and splits that intoa training and test portion for supervised learning.The recall attained on the test split by a classifierlearned on the training split becomes the measureof the unsupervised system.
Two different splitswere used, with 80% or 60% of the test instancesfor training, and the remainder for testing.This evaluation method was also used inSemEval-1, where (Pedersen, 2007) noted that itseemed to compress the results of all the systemsinto a narrow band that converged around the MostFrequent Sense result.
The same appears to havehappened in 2010.
The supervised recall of theMost Frequent Sense baseline (MFS) is 58 or .59(depending on the split), and the majority of par-ticipating systems (and even some of the randombaselines) fall in a range of scores from .56 to .62(a band of .06).
This blurs distinctions among par-ticipating systems with each other and with ran-dom baselines.The number of senses actually assigned by theclassifier learned from the training split to the in-stances in the test split is quite small, regardless ofthe number of senses discovered by the participat-ing system.
There were at most 2.06 senses identi-fied per word based on the 80-20 split, and at most2.27 senses per word based on the 60-40 split.For most systems, regardless of their underlyingmethodology, the number of senses the classifieractually assigns is approximately 1.5 per word.This shows that the supervised learning algorithmthat underlies this evaluation method gravitates to-wards a very small number of senses and there-fore tends to converge on the MFS baseline.
Thiscould be caused by noise in the induced senses,a small number of examples in the training splitfor a sense, or it may be that the supervised recallTable 2: Evaluation of Random Systemsname k V F 60-40 80-20MFS 1 0.0 63.4 58.3 58.7Duluth-R12 2 2.3 47.8 57.7 58.5Duluth-R13 3 3.6 38.4 57.6 58.0Random 4 4.4 31.9 56.5 57.3Duluth-R15 5 5.3 27.6 56.5 56.8Duluth-R110 10 8.6 16.1 53.6 54.8post-R120 20 13.9 7.5 46.2 48.6post-R133 33 18.7 4.0 38.3 42.5post-R150 50 23.2 2.3 30.0 34.2measure is making different distinctions than arefound by the unsupervised sense induction methodit seeks to evaluate.5.1.3 Paired F-scoreThe paired F-score was the only evaluation mea-sure that seemed able to identify and expose ran-dom baselines.
Duluth-R110 was by far the mostrandom of the officially participating systems, andit was by far the lowest ranked system accordingto the paired F-score, which assigned it a score of16.1.
All the Duluth-R systems ranked relativelylow (20th or below).
When presented with the 20,33, and 50 random sense post?evaluation systems,the F-score assigned those scores of 7.46, 4.00,and 2.33, which placed them far below any of theother systems.However, the paired F-score also showed thatthe Most Frequent Sense baseline outperformedall of the participating systems.
The systems thatscored close to the MFS tended to predict verysmall numbers of senses, and so were in effect act-ing much like the MFS baseline themselves.
TheF-score is not bounded by MFS and in fact it ispossible (theoretically) to reach a score of 1.00with a perfect assignment of instances to senses.The lesson learned in this task is that it would havebeen more effective to simply assume that therewas just one sense per word, rather than using thesenses induced by participating systems.
Whilethis may be a frustrating conclusion, in fact it isa reasonable observation given that in many do-mains a single sense for a given word can tend todominate.5.2 Duluth-WSI and Duluth-Mix ResultsThe Duluth-WSI systems used the test data tobuild co-occurrence matrices, while the Duluth-365Mix systems used both the training and testdata.
Within those frameworks bigrams or co?occurrences were used to represent features, thenumber of senses was automatically discoveredwith the PK2 measure or the Adapted Gap Statis-tic, and SVD was optionally used to reduce thedimensionality of the resulting matrix.
Previousstudies using SenseClusters have noted that theAdapted Gap Statistic tends to find a relativelysmall number of clusters, and that SVD typicallydoes not help to improve results of unsupervisedsense induction.
These findings were again con-firmed in this task.Mixing together all of the training and test datafor building the co?occurrence matrices was nomore effective than just using the test data.
How-ever, the Duluth-Mix systems did not finish be-fore the end of the evaluation period.
The Duluth-Mix-Narrow-Gap and PK2 systems were able tofinish 8,211 of the 8,915 test instances (92%),the Duluth-Mix-Gap and PK2 systems completed7,417 instances (83%), and Duluth-Mix-Uni-PK2and Gap systems completed 2,682 of these in-stances (30%).
While these are partial results theyseem sufficient to support this conclusion.To be usable in practical settings, an unsuper-vised sense induction system should discover thenumber of senses accurately and automatically.Duluth-WSI and Duluth-WSI-SVD were very suc-cessful in that regard, and predicted 4.15 senses onaverage per word (with the PK2 measure) whilethe actual number of senses was 3.79.The Duluth-WSI systems are direct descen-dents of UMND2 which participated in SemEval-1 (Pedersen, 2007), where Duluth-WSI-Gap isthe closest relative.
However, UMND2 usedPointwise Mutual Information (PMI) rather thanFisher?s left sided test, and it performed clusteringwith k-means rather than the method of repeatedbisections.
Both UMND2 and Duluth-WSI-Gapused the Adapted Gap Statistic, and interestinglyenough both discovered approximately 1.4 senseson average per word.6 ConclusionThe SemEval-2 sense induction task was an oppor-tunity to compare participating systems with eachother, and also to analyze evaluation measures.
Atthe very least, an evaluation measure should penal-ize random results in a fairly significant way.
Thistask showed that the paired F-score is able to iden-tify and expose random baselines, and that it drivesthem far down the rankings and places them wellbelow participating systems.
This seems prefer-able to the V-measure, which tends to rank randomsystems above all others, and to supervised recall,which provides little or no separation between ran-dom baselines and participating systems.ReferencesE.
Agirre and A. Soroa.
2007.
SemEval-2007 Task02: Evaluating word sense induction and discrim-ination systems.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 7?12, Prague, Czech Repub-lic, June.J.
Artiles, E.
Amigo?, and J. Gonzalo.
2009.
The role ofnamed entities in Web People Search.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing, pages 534?542,Singapore, August.S.
Manandhar, I. Klapaftis, D. Dligach, and S. Prad-han.
2010.
SemEval-2010 Task 14: Word senseinduction and disambiguation.
In Proceedings ofthe SemEval 2010 Workshop : the 5th InternationalWorkshop on Semantic Evaluations, Uppsala, Swe-den, July.T.
Pedersen and A. Kulkarni.
2006.
Automatic clusterstopping with criterion functions and the gap statis-tic.
In Proceedings of the Demonstration Session ofthe Human Language Technology Conference andthe Sixth Annual Meeting of the North AmericanChapter of the Association for Computational Lin-guistics, pages 276?279, New York City, June.T.
Pedersen, M. Kayaalp, and R. Bruce.
1996.
Sig-nificant lexical relationships.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 455?460, Portland, OR, August.T.
Pedersen.
2007.
UMND2 : SenseClusters appliedto the sense induction task of Senseval-4.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 394?397, Prague, Czech Republic, June.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and sim-ilarity spaces.
In Proceedings of the Conference onComputational Natural Language Learning, pages41?48, Boston, MA.A.
Rosenberg and J. Hirschberg.
2007.
V-measure:A conditional entropy-based external cluster eval-uation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 410?420, Prague, Czech Re-public, June.366
