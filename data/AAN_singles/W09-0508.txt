Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 58?65,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsAn Integrated Approach to Robust Processingof Situated Spoken DialoguePierre LisonLanguage Technology Lab,DFKI GmbH,Saarbru?cken, Germanypierre.lison@dfki.deGeert-Jan M. KruijffLanguage Technology Lab,DFKI GmbH,Saarbru?cken, Germanygj@dfki.deAbstractSpoken dialogue is notoriously hard toprocess with standard NLP technologies.Natural spoken dialogue is replete withdisfluent, partial, elided or ungrammaticalutterances, all of which are very hard toaccommodate in a dialogue system.
Fur-thermore, speech recognition is known tobe a highly error-prone task, especially forcomplex, open-ended discourse domains.The combination of these two problems?
ill-formed and/or misrecognised speechinputs ?
raises a major challenge to the de-velopment of robust dialogue systems.We present an integrated approach for ad-dressing these two issues, based on a in-cremental parser for Combinatory Cate-gorial Grammar.
The parser takes wordlattices as input and is able to handle ill-formed and misrecognised utterances byselectively relaxing its set of grammati-cal rules.
The choice of the most rele-vant interpretation is then realised via adiscriminative model augmented with con-textual information.
The approach is fullyimplemented in a dialogue system for au-tonomous robots.
Evaluation results on aWizard of Oz test suite demonstrate verysignificant improvements in accuracy androbustness compared to the baseline.1 IntroductionSpoken dialogue is often considered to be one ofthe most natural means of interaction between ahuman and a robot.
It is, however, notoriouslyhard to process with standard language process-ing technologies.
Dialogue utterances are often in-complete or ungrammatical, and may contain nu-merous disfluencies like fillers (err, uh, mm), rep-etitions, self-corrections, etc.
Rather than gettingcrisp-and-clear commands such as ?Put the redball inside the box!
?, it is more likely the robotwill hear such kind of utterance: ?right, now, couldyou, uh, put the red ball, yeah, inside the ba/ box!
?.This is natural behaviour in human-human interac-tion (Ferna?ndez and Ginzburg, 2002) and can alsobe observed in several domain-specific corpora forhuman-robot interaction (Topp et al, 2006).Moreover, even in the (rare) case where the ut-terance is perfectly well-formed and does not con-tain any kind of disfluencies, the dialogue sys-tem still needs to accomodate the various speechrecognition errors thay may arise.
This problemis particularly acute for robots operating in real-world noisy environments and deal with utterancespertaining to complex, open-ended domains.The paper presents a new approach to addressthese two difficult issues.
Our starting point is thework done by Zettlemoyer and Collins on parsingusing relaxed CCG grammars (Zettlemoyer andCollins, 2007) (ZC07).
In order to account fornatural spoken language phenomena (more flex-ible word order, missing words, etc.
), they aug-ment their grammar framework with a small setof non-standard combinatory rules, leading to arelaxation of the grammatical constraints.
A dis-criminative model over the parses is coupled withthe parser, and is responsible for selecting the mostlikely interpretation(s) among the possible ones.In this paper, we extend their approach in twoimportant ways.
First, ZC07 focused on the treat-ment of ill-formed input, and ignored the speechrecognition issues.
Our system, to the contrary,is able to deal with both ill-formed and misrec-ognized input, in an integrated fashion.
This isdone by augmenting the set of non-standard com-binators with new rules specifically tailored to dealwith speech recognition errors.Second, the only features used by ZC07 are syn-tactic features (see 3.4 for details).
We signifi-cantly extend the range of features included in the58discriminative model, by incorporating not onlysyntactic, but also acoustic, semantic and contex-tual information into the model.An overview of the paper is as follows.
We firstdescribe in Section 2 the cognitive architecture inwhich our system has been integrated.
We thendiscuss the approach in detail in Section 3.
Fi-nally, we present in Section 4 the quantitative eval-uations on a WOZ test suite, and conclude.2 ArchitectureThe approach we present in this paper is fully im-plemented and integrated into a cognitive architec-ture for autonomous robots.
A recent version ofthis system is described in (Hawes et al, 2007).
Itis capable of building up visuo-spatial models ofa dynamic local scene, continuously plan and exe-cute manipulation actions on objects within thatscene.
The robot can discuss objects and theirmaterial- and spatial properties for the purpose ofvisual learning and manipulation tasks.Figure 1: Architecture schema of the communica-tion subsystem (only for comprehension).Figure 2 illustrates the architecture schema forthe communication subsystem incorporated in thecognitive architecture (only the comprehensionpart is shown).Starting with ASR, we process the audio signalto establish a word lattice containing statisticallyranked hypotheses about word sequences.
Subse-quently, parsing constructs grammatical analysesfor the given word lattice.
A grammatical analy-sis constructs both a syntactic analysis of the ut-terance, and a representation of its meaning.
Theanalysis is based on an incremental chart parser1for Combinatory Categorial Grammar (Steedmanand Baldridge, 2009).
These meaning represen-tations are ontologically richly sorted, relational1Built on top of the OpenCCG NLP library:http://openccg.sf.netstructures, formulated in a (propositional) descrip-tion logic, more precisely in the HLDS formal-ism (Baldridge and Kruijff, 2002).
The parsercompacts all meaning representations into a sin-gle packed logical form (Carroll and Oepen, 2005;Kruijff et al, 2007).
A packed LF represents con-tent similar across the different analyses as a singlegraph, using over- and underspecification of howdifferent nodes can be connected to capture lexicaland syntactic forms of ambiguity.At the level of dialogue interpretation, a packedlogical form is resolved against a SDRS-like di-alogue model (Asher and Lascarides, 2003) toestablish contextual co-reference and dialoguemoves.Linguistic interpretations must finally be associ-ated with extra-linguistic knowledge about the en-vironment ?
dialogue comprehension hence needsto connect with other subarchitectures like vision,spatial reasoning or planning.
We realise thisinformation binding between different modalitiesvia a specific module, called the ?binder?, which isresponsible for the ontology-based mediation ac-cross modalities (Jacobsson et al, 2008).2.1 Context-sensitivityThe combinatorial nature of language providesvirtually unlimited ways in which we can commu-nicate meaning.
This, of course, raises the ques-tion of how precisely an utterance should then beunderstood as it is being heard.
Empirical stud-ies have investigated what information humans usewhen comprehending spoken utterances.
An im-portant observation is that interpretation in con-text plays a crucial role in the comprehension ofutterance as it unfolds (Knoeferle and Crocker,2006).
During utterance comprehension, humanscombine linguistic information with scene under-standing and ?world knowledge?.Figure 2: Context-sensitivity in processing situ-ated dialogue understandingSeveral approaches in situated dialogue forhuman-robot interaction have made similar obser-59vations (Roy, 2005; Roy and Mukherjee, 2005;Brick and Scheutz, 2007; Kruijff et al, 2007): Arobot?s understanding can be improved by relatingutterances to the situated context.
As we will seein the next section, by incorporating contextual in-formation into our model, our approach to robustprocessing of spoken dialogue seeks to exploit thisimportant insight.3 Approach3.1 Grammar relaxationOur approach to robust processing of spoken di-alogue rests on the idea of grammar relaxation:the grammatical constraints specified in the gram-mar are ?relaxed?
to handle slightly ill-formed ormisrecognised utterances.Practically, the grammar relaxation is donevia the introduction of non-standard CCG rules(Zettlemoyer and Collins, 2007).
In CombinatoryCategorial Grammar, the rules are used to assem-ble categories to form larger pieces of syntacticand semantic structure.
The standard rules are ap-plication (<,>), composition (B), and type rais-ing (T) (Steedman and Baldridge, 2009).Several types of non-standard rules have beenintroduced.
We describe here the two most impor-tant ones: the discourse-level composition rules,and the ASR correction rules.
We invite the readerto consult (Lison, 2008) for more details on thecomplete set of grammar relaxation rules.3.1.1 Discourse-level composition rulesIn natural spoken dialogue, we may encounter ut-terances containing several independent ?chunks?without any explicit separation (or only a shortpause or a slight change in intonation), such as(1) ?yes take the ball no the other one on yourleft right and now put it in the box.
?Even if retrieving a fully structured parse forthis utterance is difficult to achieve, it would beuseful to have access to a list of smaller ?discourseunits?.
Syntactically speaking, a discourse unitcan be any type of saturated atomic categories -from a simple discourse marker to a full sentence.The type raising rule Tdu allows the conversionof atomic categories into discourse units:A : @if ?
du : @if (Tdu)where A represents an arbitrary saturatedatomic category (s, np, pp, etc.
).The rule>C is responsible for the integration oftwo discourse units into a single structure:du : @if, du : @jg ?du : @{d:d-units}(list?(?FIRST?
i ?
f)?(?NEXT?
j ?
g)) (>C)3.1.2 ASR error correction rulesSpeech recognition is a highly error-prone task.
Itis however possible to partially alleviate this prob-lem by inserting new error-correction rules (moreprecisely, new lexical entries) for the most fre-quently misrecognised words.If we notice e.g.
that the ASR system frequentlysubstitutes the word ?wrong?
for the word ?round?during the recognition (because of their phonolog-ical proximity), we can introduce a new lexical en-try in the lexicon in order to correct this error:round ` adj : @attitude(wrong) (2)A set of thirteen new lexical entries of this typehave been added to our lexicon to account for themost frequent recognition errors.3.2 Parse selectionUsing more powerful grammar rules to relax thegrammatical analysis tends to increase the numberof parses.
We hence need a a mechanism to dis-criminate among the possible parses.
The task ofselecting the most likely interpretation among a setof possible ones is called parse selection.
Once allthe possible parses for a given utterance are com-puted, they are subsequently filtered or selectedin order to retain only the most likely interpreta-tion(s).
This is done via a (discriminative) statisti-cal model covering a large number of features.Formally, the task is defined as a function F :X ?
Y where the domain X is the set of possibleinputs (in our case, X is the set of possible wordlattices), and Y the set of parses.
We assume:1.
A function GEN(x) which enumerates allpossible parses for an input x.
In our case,this function simply represents the set ofparses of x which are admissible accordingto the CCG grammar.2.
A d-dimensional feature vector f(x, y) ?<d, representing specific features of the pair(x, y).
It can include various acoustic, syn-tactic, semantic or contextual features whichcan be relevant in discriminating the parses.603.
A parameter vector w ?
<d.The function F , mapping a word lattice to itsmost likely parse, is then defined as:F (x) = argmaxy?GEN(x)wT ?
f(x, y) (3)where wT ?
f(x, y) is the inner product?ds=1ws fs(x, y), and can be seen as a measureof the ?quality?
of the parse.
Given the parametersw, the optimal parse of a given utterance x can betherefore easily determined by enumerating all theparses generated by the grammar, extracting theirfeatures, computing the inner product wT ?f(x, y),and selecting the parse with the highest score.The task of parse selection is an example ofstructured classification problem, which is theproblem of predicting an output y from an inputx, where the output y has a rich internal structure.In the specific case of parse selection, x is a wordlattice, and y a logical form.3.3 Learning3.3.1 Training dataIn order to estimate the parameters w, we need aset of training examples.
Unfortunately, no corpusof situated dialogue adapted to our task domain isavailable to this day, let alne semantically anno-tated.
The collection of in-domain data via Wizardof Oz experiments being a very costly and time-consuming process, we followed the approach ad-vocated in (Weilhammer et al, 2006) and gener-ated a corpus from a hand-written task grammar.To this end, we first collected a small set ofWoZ data, totalling about a thousand utterances.This set is too small to be directly used as a cor-pus for statistical training, but sufficient to cap-ture the most frequent linguistic constructions inthis particular context.
Based on it, we designeda domain-specific CFG grammar covering most ofthe utterances.
Each rule is associated to a seman-tic HLDS representation.
Weights are automati-cally assigned to each grammar rule by parsing ourcorpus, hence leading to a small stochastic CFGgrammar augmented with semantic information.Once the grammar is specified, it is randomlytraversed a large number of times, resulting in alarger set (about 25.000) of utterances along withtheir semantic representations.
Since we are inter-ested in handling errors arising from speech recog-nition, we also need to ?simulate?
the most fre-quent recognition errors.
To this end, we synthe-sise each string generated by the domain-specificCFG grammar, using a text-to-speech engine2,feed the audio stream to the speech recogniser,and retrieve the recognition result.
Via this tech-nique, we are able to easily collect a large amountof training data3.3.3.2 Perceptron learningThe algorithm we use to estimate the parametersw using the training data is a perceptron.
The al-gorithm is fully online - it visits each example inturn and updates w if necessary.
Albeit simple,the algorithm has proven to be very efficient andaccurate for the task of parse selection (Collinsand Roark, 2004; Collins, 2004; Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007).The pseudo-code for the online learning algo-rithm is detailed in [Algorithm 1].It works as follows: the parameters w are firstinitialised to some arbitrary values.
Then, foreach pair (xi, zi) in the training set, the algorithmsearchs for the parse y?
with the highest score ac-cording to the current model.
If this parse happensto match the best parse which generates zi (whichwe shall denote y?
), we move to the next example.Else, we perform a simple perceptron update onthe parameters:w = w + f(xi, y?)?
f(xi, y?)
(4)The iteration on the training set is repeated Ttimes, or until convergence.The most expensive step in this algorithm isthe calculation of y?
= argmaxy?GEN(xi) wT ?f(xi, y) - this is the decoding problem.It is possible to prove that, provided the train-ing set (xi, zi) is separable with margin ?
> 0, thealgorithm is assured to converge after a finite num-ber of iterations to a model with zero training er-rors (Collins and Roark, 2004).
See also (Collins,2004) for convergence theorems and proofs.3.4 FeaturesAs we have seen, the parse selection operates byenumerating the possible parses and selecting the2We used MARY (http://mary.dfki.de) for thetext-to-speech engine.3Because of its relatively artificial character, the qualityof such training data is naturally lower than what could beobtained with a genuine corpus.
But, as the experimental re-sults will show, it remains sufficient to train the perceptronfor the parse selection task, and achieve significant improve-ments in accuracy and robustness.
In a near future, we planto progressively replace this generated training data by a realspoken dialogue corpus adapted to our task domain.61Algorithm 1 Online perceptron learningRequire: - set of n training examples {(xi, zi) : i = 1...n}- T : number of iterations over the training set- GEN(x): function enumerating possible parsesfor an input x, according to the CCG grammar.- GEN(x, z): function enumerating possible parsesfor an input x and which have semantics z,according to the CCG grammar.- L(y) maps a parse tree y to its logical form.- Initial parameter vector w0% Initialisew?
w0% Loop T times on the training examplesfor t = 1...T dofor i = 1...n do% Compute best parse according to current modelLet y?
= argmaxy?GEN(xi) wT ?
f(xi, y)% If the decoded parse 6= expected parse, update theparametersif L(y?)
6= zi then% Search the best parse for utterance xi with se-mantics ziLet y?
= argmaxy?GEN(xi,zi) wT ?
f(xi, y)% Update parameter vector wSet w = w + f(xi, y?)?
f(xi, y?
)end ifend forend forreturn parameter vector wone with the highest score according to the linearmodel parametrised by w.The accuracy of our method crucially relies onthe selection of ?good?
features f(x, y) for ourmodel - that is, features which help discriminat-ing the parses.
They must also be relatively cheapto compute.
In our model, the features are of fourtypes: semantic features, syntactic features, con-textual features, and speech recognition features.3.4.1 Semantic featuresWhat are the substructures of a logical form whichmay be relevant to discriminate the parses?
We de-fine features on the following information sources:1.
Nominals: for each possible pair?prop, sort?, we include a feature fi inf(x, y) counting the number of nominalswith ontological sort sort and propositionprop in the logical form.2.
Ontological sorts: occurrences of specificontological sorts in the logical form.Figure 3: graphical representation of the HLDSlogical form for ?I want you to take the mug?.3.
Dependency relations: following (Clark andCurran, 2003), we also model the depen-dency structure of the logical form.
Eachdependency relation is defined as a triple?sorta, sortb, label?, where sorta denotesthe sort of the incoming nominal, sortb thesort of the outgoing nominal, and label is therelation label.4.
Sequences of dependency relations: numberof occurrences of particular sequences (ie.
bi-gram counts) of dependency relations.The features on nominals and ontological sortsaim at modeling (aspects of) lexical semantics -e.g.
which meanings are the most frequent for agiven word -, whereas the features on relations andsequence of relations focus on sentential seman-tics - which dependencies are the most frequent.These features therefore help us handle lexical andsyntactic ambiguities.3.4.2 Syntactic featuresBy ?syntactic features?, we mean features associ-ated to the derivational history of a specific parse.The main use of these features is to penalise to acorrect extent the application of the non-standardrules introduced into the grammar.To this end, we include in the feature vectorf(x, y) a new feature for each non-standard rule,which counts the number of times the rule was ap-plied in the parse.62picks/particle/npcupup corrparticles/np>thenp/nballnnp >s >Figure 4: CCG derivation of ?pick cup the ball?.In the derivation shown in the figure 4, the rulecorr (correction of a speech recognition error) isapplied once, so the corresponding feature value isset to 1.
The feature values for the remaining rulesare set to 0, since they are absent from the parse.These syntactic features can be seen as a penaltygiven to the parses using these non-standard rules,thereby giving a preference to the ?normal?
parsesover them.
This mechanism ensures that the gram-mar relaxation is only applied ?as a last resort?when the usual grammatical analysis fails to pro-vide a full parse.
Of course, depending on therelative frequency of occurrence of these rules inthe training corpus, some of them will be morestrongly penalised than others.3.4.3 Contextual featuresAs we have already outlined in the backgroundsection, one striking characteristic of spoken dia-logue is the importance of context.
Understandingthe visual and discourse contexts is crucial to re-solve potential ambiguities and compute the mostlikely interpretation(s) of a given utterance.The feature vector f(x, y) therefore includesvarious features related to the context:1.
Activated words: our dialogue system main-tains in its working memory a list of contex-tually activated words (cfr.
(Lison and Krui-jff, 2008)).
This list is continuously updatedas the dialogue and the environment evolves.For each context-dependent word, we includeone feature counting the number of times itappears in the utterance string.2.
Expected dialogue moves: for each possibledialogue move, we include one feature indi-cating if the dialogue move is consistent withthe current discourse model.
These featuresensure for instance that the dialogue movefollowing a QuestionYN is a Accept, Re-ject or another question (e.g.
for clarificationrequests), but almost never an Opening.3.
Expected syntactic categories: for eachatomic syntactic category in the CCG gram-mar, we include one feature indicating if thecategory is consistent with the current dis-course model.
These features can be used tohandle sentence fragments.3.4.4 Speech recognition featuresFinally, the feature vector f(x, y) also includesfeatures related to the speech recognition.
TheASR module outputs a set of (partial) recognitionhypotheses, packed in a word lattice.
One exam-ple of such a structure is given in Figure 5.
Eachrecognition hypothesis is provided with an asso-ciated confidence score, and we want to favourthe hypotheses with high confidence scores, whichare, according to the statistical models incorpo-rated in the ASR, more likely to reflect what wasuttered.To this end, we introduce three features: theacoustic confidence score (confidence score pro-vided by the statistical models included in theASR), the semantic confidence score (based on a?concept model?
also provided by the ASR), andthe ASR ranking (hypothesis rank in the word lat-tice, from best to worst).Figure 5: Example of word lattice4 Experimental evaluationWe performed a quantitative evaluation of our ap-proach, using its implementation in a fully inte-grated system (cf.
Section 2).
To set up the ex-periments for the evaluation, we have gathered acorpus of human-robot spoken dialogue for ourtask-domain, which we segmented and annotatedmanually with their expected semantic interpreta-tion.
The data set contains 195 individual utter-ances along with their complete logical forms.4.1 ResultsThree types of quantitative results are extractedfrom the evaluation results: exact-match, partial-match, and word error rate.
Tables 1, 2 and 3 illus-trate the results, broken down by use of grammarrelaxation, use of parse selection, and number ofrecognition hypotheses considered.63Size of word lattice(number of NBests)GrammarrelaxationParseselection Precision Recall F1-value(Baseline) 1 No No 40.9 45.2 43.0.
1 No Yes 59.0 54.3 56.6.
1 Yes Yes 52.7 70.8 60.4.
3 Yes Yes 55.3 82.9 66.3.
5 Yes Yes 55.6 84.0 66.9(Full approach) 10 Yes Yes 55.6 84.9 67.2Table 1: Exact-match accuracy results (in percents).Size of word lattice(number of NBests)GrammarrelaxationParseselection Precision Recall F1-value(Baseline) 1 No No 86.2 56.2 68.0.
1 No Yes 87.4 56.6 68.7.
1 Yes Yes 88.1 76.2 81.7.
3 Yes Yes 87.6 85.2 86.4.
5 Yes Yes 87.6 86.0 86.8(Full approach) 10 Yes Yes 87.7 87.0 87.3Table 2: Partial-match accuracy results (in percents).Each line in the tables corresponds to a possibleconfiguration.
Tables 1 and 2 give the precision,recall and F1 value for each configuration (respec-tively for the exact- and partial-match), and Table3 gives the Word Error Rate [WER].The first line corresponds to the baseline: nogrammar relaxation, no parse selection, and use ofthe first NBest recognition hypothesis.
The lastline corresponds to the results with the full ap-proach: grammar relaxation, parse selection, anduse of 10 recognition hypotheses.Size of wordlattice (NBests)GrammarrelaxationParseselection WER1 No No 20.51 Yes Yes 19.43 Yes Yes 16.55 Yes Yes 15.710 Yes Yes 15.7Table 3: Word error rate (in percents).4.2 Comparison with baselineHere are the comparative results we obtained:?
Regarding the exact-match results betweenthe baseline and our approach (grammar re-laxation and parse selection with all fea-tures activated for NBest 10), the F1-measureclimbs from 43.0 % to 67.2 %, which meansa relative difference of 56.3 %.?
For the partial-match, the F1-measure goesfrom 68.0 % for the baseline to 87.3 % forour approach ?
a relative increase of 28.4 %.?
We obverse a significant decrease in WER:we go from 20.5 % for the baseline to 15.7 %with our approach.
The difference is statisti-cally significant (p-value for t-tests is 0.036),and the relative decrease of 23.4 %.5 ConclusionsWe presented an integrated approach to the pro-cessing of (situated) spoken dialogue, suited tothe specific needs and challenges encountered inhuman-robot interaction.In order to handle disfluent, partial, ill-formedor misrecognized utterances, the grammar used bythe parser is ?relaxed?
via the introduction of aset of non-standard combinators which allow forthe insertion/deletion of specific words, the com-bination of discourse fragments or the correctionof speech recognition errors.The relaxed parser yields a (potentially large)set of parses, which are then packed and retrievedby the parse selection module.
The parse selec-tion is based on a discriminative model exploring aset of relevant semantic, syntactic, contextual andacoustic features extracted for each parse.
The pa-rameters of this model are estimated against an au-tomatically generated corpus of ?utterance, logicalform?
pairs.
The learning algorithm is an percep-tron, a simple albeit efficient technique for param-eter estimation.As forthcoming work, we shall examine the po-tential extension of our approach in new direc-tions, such as the exploitation of parse selectionfor incremental scoring/pruning of the parse chart,64the introduction of more refined contextual fea-tures, or the use of more sophisticated learning al-gorithms, such as Support Vector Machines.ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofConversation.
Cambridge University Press.J.
Baldridge and G.-J.
M. Kruijff.
2002.
CouplingCCG and hybrid logic dependency semantics.
InACL?02: Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics,pages 319?326, Philadelphia, PA. Association forComputational Linguistics.T.
Brick and M. Scheutz.
2007.
Incremental natu-ral language processing for HRI.
In Proceeding ofthe ACM/IEEE international conference on Human-Robot Interaction (HRI?07), pages 263 ?
270.J.
Carroll and S. Oepen.
2005.
High efficiency re-alization for a wide-coverage unification grammar.In Proceedings of the International Joint Confer-ence on Natural Language Processing (IJCNLP?05),pages 165?176.Stephen Clark and James R. Curran.
2003.
Log-linearmodels for wide-coverage ccg parsing.
In Proceed-ings of the 2003 conference on Empirical methods innatural language processing, pages 97?104, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In ACL?04: Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics, page111, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Michael Collins.
2004.
Parameter estimation forstatistical parsing models: theory and practice ofdistribution-free methods.
In New developments inparsing technology, pages 19?55.
Kluwer AcademicPublishers.R.
Ferna?ndez and J. Ginzburg.
2002.
A corpus studyof non-sentential utterances in dialogue.
TraitementAutomatique des Langues, 43(2):12?43.N.
Hawes, A. Sloman, J. Wyatt, M. Zillich, H. Jacob-sson, G.J.
M. Kruijff, M. Brenner, G. Berginc, andD.
Skocaj.
2007.
Towards an integrated robot withmultiple cognitive functions.
In AAAI, pages 1548?1553.
AAAI Press.Henrik Jacobsson, Nick Hawes, Geert-Jan Kruijff, andJeremy Wyatt.
2008.
Crossmodal content bind-ing in information-processing architectures.
In Pro-ceedings of the 3rd ACM/IEEE International Con-ference on Human-Robot Interaction (HRI), Amster-dam, The Netherlands, March 12?15.P.
Knoeferle and M.C.
Crocker.
2006.
The coordinatedinterplay of scene, utterance, and world knowledge:evidence from eye tracking.
Cognitive Science.G.J.M.
Kruijff, P. Lison, T. Benjamin, H. Jacobsson,and N.A.
Hawes.
2007.
Incremental, multi-levelprocessing for comprehending situated dialogue inhuman-robot interaction.
In Language and Robots:Proceedings from the Symposium (LangRo?2007),pages 55?64, Aveiro, Portugal, December.Pierre Lison and Geert-Jan M. Kruijff.
2008.
Salience-driven contextual priming of speech recognition forhuman-robot interaction.
In Proceedings of the 18thEuropean Conference on Artificial Intelligence, Pa-tras (Greece).Pierre Lison.
2008.
Robust processing of situated spo-ken dialogue.
Master?s thesis, Universita?t des Saar-landes, Saarbru?cken.D.
Roy and N. Mukherjee.
2005.
Towards situatedspeech understanding: visual context priming oflanguage models.
Computer Speech & Language,19(2):227?248, April.D.K.
Roy.
2005.
Semiotic schemas: A framework forgrounding language in action and perception.
Artifi-cial Intelligence, 167(1-2):170?205.Mark Steedman and Jason Baldridge.
2009.
Combina-tory categorial grammar.
In Robert Borsley and Ker-sti Bo?rjars, editors, Nontransformational Syntax: AGuide to Current Models.
Blackwell, Oxford.E.
A. Topp, H. Hu?ttenrauch, H.I.
Christensen, andK.
Severinson Eklundh.
2006.
Bringing togetherhuman and robotic environment representations ?a pilot study.
In Proc.
of the IEEE/RSJ Interna-tional Conference on Intelligent Robots and Systems(IROS), Beijing, China, October.Karl Weilhammer, Matthew N. Stuttle, and SteveYoung.
2006.
Bootstrapping language modelsfor dialogue systems.
In Proceedings of INTER-SPEECH 2006, Pittsburgh, PA.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In UAI ?05, Proceedings of the 21st Con-ference in Uncertainty in Artificial Intelligence, July2005, pages 658?666.Luke Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 678?687.65
