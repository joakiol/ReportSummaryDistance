Coling 2010: Poster Volume, pages 910?918,Beijing, August 2010Opinion Summarization with Integer Linear Programming Formulationfor Sentence Extraction and OrderingHitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro KikuiNTT Cyber Space Laboratories, NTT Corporation{ nishikawa.hitoshi, hasegawa.takaakimatsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jpAbstractIn this paper we propose a novel algorithmfor opinion summarization that takes ac-count of content and coherence, simulta-neously.
We consider a summary as a se-quence of sentences and directly acquirethe optimum sequence from multiple re-view documents by extracting and order-ing the sentences.
We achieve this with anovel Integer Linear Programming (ILP)formulation.
Our proposed formulation isa powerful mixture of the Maximum Cov-erage Problem and the Traveling Sales-man Problem, and is widely applicable totext generation and summarization tasks.We score each candidate sequence accord-ing to its content and coherence.
Sinceour research goal is to summarize reviews,the content score is defined by opinionsand the coherence score is developed intraining against the review document cor-pus.
We evaluate our method using thereviews of commodities and restaurants.Our method outperforms existing opinionsummarizers as indicated by its ROUGEscore.
We also report the results of humanreadability experiments.1 IntroductionThe Web now holds a massive number of reviewsdescribing the opinions of customers about prod-ucts and services.
These reviews can help the cus-tomer to reach purchasing decisions and guide thebusiness activities of companies such as productimprovement.
It is, however, almost impossible toread all reviews given their sheer number.Automatic text summarization, particularlyopinion summarization, is expected to allow allpossible reviews to be efficiently utilized.
Givenmultiple review documents, our summarizer out-puts text consisting of ordered sentences.
A typ-This restaurant offers customers a delicious menu and arelaxing atmosphere.
The staff are very friendly but theprice is a little high.Table 1: A typical summary.ical summary is shown in Table 1.
This task isconsidered as multidocument summarization.Existing summarizers focus on organizing sen-tences so as to include important information inthe given document into a summary under somesize limitation.
A serious problem is that most ofthese summarizers completely ignore coherenceof the summary, which improves reader?s compre-hension as reported by Barzilay et al (2002).To make summaries coherent, the extractedsentences must be appropriately ordered.
How-ever, most summarization systems delink sentenceextraction from sentence ordering, so a sentencecan be extracted that can never be ordered natu-rally with the other extracted sentences.
More-over, due to recent advances in decoding tech-niques for text summarization, the summarizerstend to select shorter sentences to optimize sum-mary content.
It aggravates this problem.Although a preceding work tackles this prob-lem by performing sentence extraction and order-ing simultaneously (Nishikawa et al, 2010), theyadopt beam search and dynamic programming tosearch for the optimal solution, so their proposedmethod may fail to locate it.To overcome this weakness, this paper proposesa novel Integer Linear Programming (ILP) formu-lation for searching for the optimal solution effi-ciently.
We formulate the multidocument sum-marization task as an ILP problem that tries tooptimize the content and coherence of the sum-mary by extracting and ordering sentences simul-taneously.
We apply our method to opinion sum-marization and show that it outperforms state-of-the-art opinion summarizers in terms of ROUGEevaluations.
Although in this paper we challenge910our method with opinion summarization, it can bewidely applied to other text generation and sum-marization tasks.This paper is organized as follows: Section 2describes related work.
Section 3 describes ourproposal.
Section 4 reports our evaluation experi-ments.
We conclude this paper in Section 5.2 Related Work2.1 Sentence ExtractionAlthough a lot of summarization algorithms havebeen proposed, most of them solely extract sen-tences from a set of sentences in the source docu-ment set.
These methods perform extractive sum-marization and can be formalized as follows:S?
= argmaxS?TL(S) (1)s.t.
length(S) ?
KT stands for all sentences in the source docu-ment set and S is an arbitrary subset of T .
L(S)is a function indicating the score of S as deter-mined by one or more criteria.
length(S) indi-cates the length of S, K is the maximum size ofthe summary.
That is, most summarization algo-rithms search for, or decode, the set of sentences S?that maximizes function L under the given maxi-mum size of the summary K. Thus most stud-ies focus on the design of function L and efficientsearch algorithms (i.e.
argmax operation in Eq.1).Objective FunctionMany useful L functions have been proposedincluding the cosine similarity of given sentences(Carbonell and Goldstein, 1998) and centroid(Radev et al, 2004); some approaches directlylearn function L from references (Kupiec et al,1995; Hirao et al, 2002).There are two approaches to defining the scoreof the summary.
One defines the weight on eachsentence forming the summary.
The other definesa weight for a sub-sentence, concept, that the sum-mary contains.McDonald (2007) and Martins and Smith(2009) directly weight sentences and use MMRto avoid redundancy (Carbonell and Goldstein,1998).
In contrast to their approaches, we setweights on concepts, not sentences.
Gillickand Favre (2009) reported that the concept-basedmodel achieves better performance and scalabilitythan the sentence-based model when it is formu-lated as ILP.There is a wide range of choice with regardto the unit of the concept.
Concepts includewords and the relationship between named en-tities (Filatova and Hatzivassiloglou, 2004), bi-grams (Gillick and Favre, 2009), and word stems(Takamura and Okumura, 2009).Some summarization systems that target re-views, opinion summarizers, extract particularinformation, opinion, from the input sentencesand leverage them to select important sentences(Carenini et al, 2006; Lerman et al, 2009).
Inthis paper, since we aim to summarize reviews,the objective function is defined through opinionas the concept that the reviews contain.
We ex-plain our detailed objective function in Section 3.We describe features of above existing summariz-ers in Section 4 and compare our method to themas baselines.Decoding MethodThe algorithms proposed for argmax operationinclude the greedy method (Filatova and Hatzivas-siloglou, 2004), stack decoding (Yih et al, 2007;Takamura and Okumura, 2009) and Integer LinearProgramming (Clarke and Lapata, 2007; McDon-ald, 2007; Gillick and Favre, 2009; Martins andSmith, 2009).
Gillick and Favre (2009) and Taka-mura and Okumura (2009) formulate summariza-tion as a Maximum Coverage Problem.
We alsouse this formulation.
While these methods focuson extracting a set of sentences from the sourcedocument set, our method performs extraction andordering simultaneously.Some studies attempt to generate a single sen-tence (i.e.
headline) from the source document(Banko et al, 2000; Deshpande et al, 2007).While they extract and order words from thesource document as a unit, our model uses the unitof sentences.
This problem can be formulated asthe Traveling Salesman Problem and its variants.Banko et al (2000) uses beam search to identifyapproximate solutions.
Deshpande et al (2007)uses ILP and a randomized algorithm to find theoptimal solution.2.2 Sentence OrderingIt is known that the readability of a collection ofsentences, a summary, can be greatly improvedby appropriately ordering them (Barzilay et al,2002).
Features proposed to create the appropri-ate order include publication date of document(Barzilay et al, 2002), content words (Lapata,2003; Althaus et al, 2004), and syntactic role of911                       	              Figure 1: Graph representation of summarization.words (Barzilay and Lapata, 2005).
Some ap-proaches use machine learning to integrate thesefeatures (Soricut and Marcu, 2006; Elsner et al,2007).
Generally speaking, these methods scorethe discourse coherence of a fixed set of sentences.These methods are separated from the extractionstep so they may fail if the set includes sentencesthat are impossible to order naturally.As mentioned above, there is a preceding workthat attempted to perform sentence extraction andordering simultaneously (Nishikawa et al, 2010).Differences between this paper and that work areas follows:?
This work adopts ILP solver as a decoder.ILP solver allows the summarizer to searchfor the optimal solution much more rapidlythan beam search (Deshpande et al, 2007),which was adopted by the prior work.
Topermit ILP solver incorporation, we proposein this paper a totally new ILP formulation.The formulation can be widely used for textsummarization and generation.?
Moreover, to learn better discourse coher-ence, we adopt the Passive-Aggressive al-gorithm (Crammer et al, 2006) and useKendall?s tau (Lapata, 2006) as the loss func-tion.
In contrast, the above work adopts Av-eraged Perceptron (Collins, 2002) and has noexplicit loss function.These advances make this work very differentfrom that work.3 Our Method3.1 The ModelWe consider a summary as a sequence of sen-tences.
As an example, document set D ={d1, d2, d3} is given to a summarizer.
We de-fine d as a single document.
Document d1,which consists of four sentences, is describeby d1 = {s11, s12, s13, s14}.
Documents d2and d3 consist of five sentences and three sen-tences (i.e.
d2 = {s21, s22, s23, s24, s25}, d3 =e1 e2 e3 .
.
.
e6 e7 e8s11 1 0 0 1 0 0s12 0 1 0 0 0 0s13 0 0 0 0 0 1......s31 0 0 0 0 0 0s32 0 0 1 0 1 0s33 0 0 0 0 0 1Table 2: Sentence-Concept Matrix.
{s31, s32, s33}).
If the summary consists of foursentences s11, s23, s32, s33 and they are ordered ass11 ?
s23 ?
s32 ?
s33, we add symbols indicat-ing the beginning of the summary s0 and the endof the summary s4, and describe the summary asS = ?s0, s11, s23, s32, s33, s4?.
Summary S canbe represented as a directed path that starts at s0and ends at s4 as shown in Fig.
1.We describe a directed arc between si and sj asai,j ?
A.
The directed path shown in Fig.
1 is de-composed into nodes, s0, s11, s23, s32, s33, s4, andarcs, a0,11, a11,23, a23,32, a32,33, a33,4.To represent the discourse coherence of two ad-jacent sentences, we define weight ci,j ?
C asthe coherence score on the directed arc ai,j .
Weassume that better summaries have higher coher-ence scores, i.e.
if the sum of the scores of the arcs?ai,j?S ci,jai,j is high, the summary is coherent.We also assume that the source document setD includes set of concepts e ?
E. Each concepte is covered by one or more of the sentences inthe document set.
We show this schema in Ta-ble 2.
According to Table 2, document set D haseight concepts e1, e2, .
.
.
, e7, e8 and sentence s11includes concepts e1 and e6 while sentence s12 in-cludes e2.We consider each concept ei has a weight wi.We assume that concept ei will have high weightwi if it is important.
This paper improves sum-mary quality by maximizing the sum of theseweights.We define, based on the above assumption, thefollowing objective function:L(S) = ?ei?S wiei +?ai,j?S ci,jai,j (2)s.t.
length(S) ?
KSummarization is, in this paper, realized bymaximizing the sum of weights of concepts in-cluded in the summary and the coherence score ofall adjacent sentences in the summary under the912limit of maximum summary size.
Note that whileS and T represents the set of sentences in Eq.1,they represent the sequence of sentences in Eq.2.Maximizing Eq.2 is NP-hard.
If each sen-tence in the source document set has one concept(i.e.
Table 2 is a diagonal matrix), Eq.2 becomesthe Prize Collecting Traveling Salesman Problem(Balas, 1989).
Therefore, a highly efficient decod-ing method is essential.3.2 Parameter EstimationOur method requires two parameters: weightsw ?
W of concepts and coherence c ?
C of twoadjacent sentences.
We describe them here.Content ScoreIn this paper, as mentioned above, since we at-tempt to summarize reviews, we adopt opinionas a concept.
We define opinion e = ?t, a, p?as the tuple of target t, aspect a and its polarityp ?
{?1, 0, 1}.
We define target t as the tar-get of an opinion.
For example, the target t ofthe sentence ?This digital camera has good im-age quality.?
is digital camera.
We define aspecta as a word that represents a standpoint appro-priate for evaluating products and services.
Withregard to digital cameras, aspects include imagequality, design and battery life.
In the above ex-ample sentence, the aspect is image quality.
Po-larity p represents whether the opinion is positiveor negative.
In this paper, we define p = ?1 asnegative, p = 0 as neutral and p = 1 as posi-tive.
Thus the example sentence contains opinione = ?digital camera, image quality, 1?.Opinions are extracted using a sentiment ex-pression dictionary and pattern matching from de-pendency trees of sentences.
This opinion extrac-tor is the same as that used in Nishikawa et al(2010).As the weight wi of concept ei, we use onlythe frequency of each opinion in the input docu-ment set, i.e.
we assume that an opinion that ap-pears frequently in the input is important.
Whilethis weighting is relatively naive compared to Ler-man et al (2009)?s method, our ROUGE evalua-tion shows that this approach is effective.Coherence ScoreIn this section, we define coherence score c.Since it is not easy to model the global coherenceof a set of sentences, we approximate the globalcoherence by the sum of local coherence i.e.
thesum of coherence scores of sentence pairs.
Wedefine local coherence score ci,j of two sentencesx = {si, sj} and their order y = ?si, sj?
repre-senting si ?
sj as follows:ci,j = w ?
?
(x, y) (3)w??
(x, y) is the inner product ofw and ?
(x, y),w is a parameter vector and ?
(x, y) is a featurevector of the two sentences si and sj .Since coherence consists of many different el-ements and it is difficult to model all of them,we approximate the features of coherence as theCartesian product of the following features: con-tent words, POS tags of content words, named en-tity tags (e.g.
LOC, ORG) and conjunctions.
Lap-ata (2003) proposed most of these features.We also define feature vector ?
(x,y) of the bagof sentences x = {s0, s1, .
.
.
, sn, sn+1} and itsentire order y = ?s0, s1, .
.
.
, sn, sn+1?
as follows:?
(x,y) =?x,y?
(x, y) (4)Therefore, the score of order y is w ?
?
(x,y).Given a training set, if trained parameter vector wassigns score w ?
?
(x,yt) to correct order yt thatis higher than score w ??
(x, y?)
assigned to incor-rect order y?, it is expected that the trained parame-ter vector will give a higher score to coherently or-dered sentences than to incoherently ordered sen-tences.We use the Passive-Aggressive algorithm(Crammer et al, 2006) to find w. The Passive-Aggressive algorithm is an online learning algo-rithm that updates the parameter vector by takingup one example from the training examples andoutputting the solution that has the highest scoreunder the current parameter vector.
If the outputdiffers from the training example, the parametervector is updated as follows;min ||wi+1 ?wi|| (5)s.t.
s(x,yt;wi+1)?
s(x, y?
;wi+1) ?
`(y?
;yt)s(x,y;w) = w ?
?
(x,y)wi is the current parameter vector and wi+1 isthe updated parameter vector.
That is, Eq.5 meansthat the score of the correct order must exceed thescore of an incorrect order by more than loss func-tion `(y?
;yt) while minimizing the change in pa-rameters.When updating the parameter vector, this al-gorithm requires the solution that has the highestscore under the current parameter vector, so wehave to run an argmax operation.
Since we are913attempting to order a set of sentences, the opera-tion is regarded as solving the Traveling SalesmanProblem (Althaus et al, 2004); that is, we locatethe path that offers the maximum score throughall n sentences where s0 and sn+1 are starting andending points, respectively.
This operation is NP-hard and it is difficult to find the global optimalsolution.
To overcome this, we find an approxi-mate solution by beam search.1We define loss function `(y?
;yt) as follows:`(y?
;yt) = 1?
?
(6)?
= 1 ?
4 S(y?,yt)N(N ?
1) (7)?
indicates Kendall?s tau.
S(y?,yt) is the mini-mum number of operations that swap adjacent ele-ments (i.e.
sentences) needed to bring y?
to yt (La-pata, 2006).
N indicates the number of elements.Since Lapata (2006) reported that Kendall?s taureliably reproduces human ratings with regard tosentence ordering, using it to minimize the lossfunction is expected to yield more reliable param-eters.We omit detailed derivations due to space limi-tations.
Parameters are updated as per the follow-ing equation.wi+1 = wi + ?i(?(x,yt)?
?
(x, y?))
(8)?i = `(y?
;yt) ?
s(x,yt;wi) + s(x, y?;wi)||?(x,yt)?
?
(x, y?
)||2 + 12C(9)C in Eq.9 is the aggressiveness parameter thatcontrols the degree of parameter change.Note that our method learns w from documentsautomatically annotated by a POS tagger and anamed entity tagger.
That is, manual annotationisn?t required.3.3 Decoding with Integer LinearProgramming FormulationThis section describes an ILP formulation of theabove model.
We use the same notation con-vention as introduced in Section 3.1.
We uses ?
S, a ?
A, e ?
E as the decision variable.Variable si ?
S indicates the inclusion of the ith sentence.
If the i th sentence is part of thesummary, then si is 1.
If it is not part of the1Obviously, ILP can be used to search for the path thatmaximizes the score.
While beam search tends to fail to findout the optimal solution, it is tractable and the learning al-gorithm can estimate the parameter from approximate solu-tions.
For these reasons we use beam search.summary, then si is 0.
Variable ai,j ?
A indi-cates the adjacency of the i th and j th sentences.If these two sentences are ordered as si ?
sj ,then ai,j is 1.
Variable ei ?
E indicates the in-clusion of the i th concept ei.
Taking Fig.1 asan example, variables s0, s11, s23, s32, s33, s4 anda0,11, a11,23, a23,32, a32,33, a33,4 are 1. ei, whichcorrespond to the concepts in the above extractedsentences, are also 1.We represent the above objective function(Eq.2) as follows:max????
?ei?Ewiei + (1 ?
?)?ai,j?Aci,jai,j???
(10)Eq.10 attempts to cover as much of the conceptsincluded in input document set as possible accord-ing to their weights w ?
W and orders sentencesaccording to discourse coherence c ?
C. ?
is ascaling factor to balance w and c.We then impose some constraints on Eq.10 toacquire the optimum solution.First, we range the above three variables s ?S, a ?
A, e ?
E.si, ai,j , ei ?
{0, 1} ?i, jIn our model, a summary can?t include the samesentence, arc, or concept twice.
Taking Table 2for example, if s13 and s33 are included in a sum-mary, the summary has two e8, but e8 is 1.
Thisconstraint avoids summary redundancy.The summary must meet the condition of maxi-mum summary size.
The following inequality rep-resents the size constraint:?si?Slisi ?
Kli ?
L indicates the length of sentence si.
K isthe maximum size of the summary.The following inequality represents the rela-tionship between sentences and concepts in thesentences.
?imijsi ?
ej ?jThe above constraint represents Table 2. mi,j isan element of Table 2.
If si is not included in thesummary, the concepts in si are not included.Symbols indicating the beginning and end ofthe summary must be part of the summary.914s0 = 1sn+1 = 1n is the number of sentences in the input docu-ment set.Next, we describe the constraints placed onarcs.The beginning symbol must be followed by asentence or a symbol and must not have any pre-ceding sentences/symbols.
The end symbol mustbe preceded by a sentence or a symbol and mustnot have any following sentences/symbols.
Thefollowing equations represent these constraints:?ia0,i = 1?iai,0 = 0?ian+1,i = 0?iai,n+1 = 1Each sentence in the summary must be pre-ceded and followed by a sentence/symbol.
?iai,j +?iaj,i = 2sj ?j?iai,j =?iaj,i ?jThe above constraints fail to prevent cycles.
Torectify this, we set the following constraints.
?if0,i = n?ifi,0 ?
1?ifi,j ?
?ifj,i = sj ?jfi,j ?
nai,j ?i, jThe above constraints indicate that flows f aresent from s0 as a source to sn+1 as a sink.
n unitflows are sent from the source and each node ex-pends one unit of flows.
More than one flow hasto arrive at the sink.
By setting these constraints,the nodes consisting of a cycle have no flow.
Thussolutions that contain a cycle are prevented.
Theseconstraints have also been used to avoid cycles inheadline generation (Deshpande et al, 2007).4 ExperimentsThis section evaluates our method in terms ofROUGE score and readability.
We tested ourmethod and two baselines in two domains: re-views of commodities and restaurants.
We col-lected 4,475 reviews of 100 commodities and2,940 reviews of 100 restaurants from websites.The commodities included items such as digitalcameras, printers, video games, and wines.
Theaverage document size was 10,173 bytes in thecommodity domain and 5,343 bytes in the restau-rant domain.
We attempted to generate 300 bytesummaries, so the summarization rates were about3% and 6%, respectively.We prepared 4 references for each review, thusthere were 400 references in each domain.
The au-thors were not those who made up the references.These references were used for ROUGE and read-ability evaluation.Since our method requires the parameter vec-tor w for determining the coherence scores.
Wetrained the parameter vector for each domain.Each parameter vector was trained using 10-foldcross validation.
We used 8 samples to train, 1to develop, and 1 to test.
In the restaurant do-main, we added 4,390 reviews to each training setto alleviate data sparseness.
In the commodity do-main, we add 47,570 reviews.2As the solver, we used glpk.3 According to thedevelopment set, ?
in Eq.10 was set as 0.1.4.1 BaselinesWe compare our method to the references (whichalso provide the upper bound) and the opinionsummarizers proposed by Carenini et al (2006)and Lerman et al (2009) as the baselines.In the ROUGE evaluations, Human indicatesROUGE scores between references.
To compareour summarizer to human summarization, we cal-culated ROUGE scores between each referenceand the other three references, and averaged them.In the readability evaluations, we randomly se-lected one reference for each commodity and eachrestaurant and compared them to the results of thethree summarizers.Carenini et al (2006)Carenini et al (2006) proposed two opinion2The commodities domain suffers from stronger reviewvariation than the restaurant domain so more training datawas needed.3http://www.gnu.org/software/glpk/915summarizers.
One uses a natural language genera-tion module, and other is based on MEAD (Radevet al, 2004).
Since it is difficult to mimic the natu-ral language generation module, we implementedthe latter one.
The objective function Carenini etal.
(2006) proposed is as follows:L1(S) =?a?S?s?D|polaritys(a)| (11)polaritys(a) indicates the polarity of aspect ain sentence s present in source document set D.That is, this function gives a high score to a sum-mary that covers aspects frequently mentioned inthe input, and whose polarities tend to be eitherpositive or negative.The solution is identified using the greedymethod.
If there is more than one sentence thathas the same score, the sentence that has thehigher centroid score (Radev et al, 2004) is ex-tracted.Lerman et al (2009)Lerman et al (2009) proposed three objectivefunctions for opinion summarization, and we im-plemented one of them.
The function is as fol-lows:L2(S) = ?
(KL(pS(a), pD(a)) (12)+?a?AKL(N (x|?aS , ?2aS ),N (x|?aD , ?2aD)))KL(p, q) means the Kullback-Leibler diver-gence between probability distribution p and q.pS(a) and pD(a) are probability distributions in-dicating how often aspect a ?
A occurs in sum-mary S and source document set D respectively.N (x|?, ?2) is a Gaussian distribution indicatingdistribution of polarity of an aspect whose meanis ?
and variance is ?2.
?aS , ?aD and ?2aS , ?2aDare the means and the variances of aspect a insummary S and source document set D, respec-tively.
These parameters are determined usingmaximum-likelihood estimation.That is, the above objective function gives highscore to a summary whose distributions of aspectsand polarities mirror those of the source documentset.To identify the optimal solution, Lerman et al(2009) use a randomized algorithm.
First, thesummarizer randomly extracts sentences from thesource document set, then iteratively performs in-sert/delete/swap operations on the summary to in-crease Eq.12 until summary improvement satu-rates.
While this method is prone to lock ontoCommodity R-2 R-SU4 R-SU9(Carenini et al, 2006) 0.158 0.202 0.186(Lerman et al, 2009) 0.205 0.247 0.227Our Method 0.231 0.251 0.230Human 0.384 0.392 0.358Restaurant R-2 R-SU4 R-SU9(Carenini et al, 2006) 0.251 0.281 0.258(Lerman et al, 2009) 0.260 0.296 0.273Our Method 0.285 0.303 0.273Human 0.358 0.370 0.335Table 3: Automatic ROUGE evaluation.# of Sentences(Carenini et al, 2006) 3.79(Lerman et al, 2009) 6.28Our Method 7.88Human 5.83Table 4: Average number of sentences in the sum-mary.local solutions, the summarizer can reach the op-timal solution by changing the starting sentencesand repeating the process.
In this experiment, weused 100 randomly selected starting points.4.2 ROUGEWe used ROUGE (Lin, 2004) for evaluating thecontent of summaries.
We chose ROUGE-2,ROUGE-SU4 and ROUGE-SU9.
We preparedfour reference summaries for each document set.The results of these experiments are shown inTable 3.
ROUGE scores increase in the order of(Carenini et al, 2006), (Lerman et al, 2009) andour method, but no method could match the per-formance of Human.
Our method significantlyoutperformed Lerman et al (2009)?s method overROUGE-2 according to the Wilcoxon signed-ranktest, while it shows no advantage over ROUGE-SU4 and ROUGE-SU9.Although our weighting of the set of sentencesis relatively naive compared to the weighting pro-posed by Lerman et al (2009), our method out-performs their method.
There are two reasonsfor this; one is that we adopt ILP for decoding,so we can acquire preferable solutions efficiently.While the score of Lerman et al (2009)?s methodmay be improved by adopting ILP, it is difficultto do so because their objective function is ex-tremely complex.
The other reason is the coher-ence score.
Since our coherence score is based on916Commodity (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human(Carenini et al, 2006) - 27/45 18/29 8/46(Lerman et al, 2009) 18/45 - 29/48 11/47Our Method 11/29 19/48 - 5/46Human 38/46 36/47 41/46 -Restaurant (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human(Carenini et al, 2006) - 31/45 17/31 8/48(Lerman et al, 2009) 14/45 - 25/47 7/46Our Method 14/31 22/47 - 8/50Human 40/48 39/46 42/50 -Table 5: Readability evaluation.content words, it may impact the content of thesummary.4.3 ReadabilityReadability was evaluated by human judges.Since it is difficult to perform absolute evalua-tion to judge the readability of summaries, weperformed a paired comparison test.
The judgeswere shown two summaries of the same input anddecided which was more readable.
The judgesweren?t informed which method generated whichsummary.
We randomly chose 50 sets of reviewsfrom each domain, so there were 600 paired sum-maries.4 However, as shown in Table 4, the aver-age numbers of sentences in the summary differedwidely from the methods and this might affect thereadability evaluation.
It was not fair to includethe pairs that were too different in terms of thenumber of sentences.
Therefore, we removed thepairs that differed by more than five sentences.In the experiment, 523 pairs were used, and 21judges evaluated about 25 summaries each.
Wedrew on DUC 2007 quality questions5 for read-ability assessment.Table 5 shows the results of the experiment.Each element in the table indicates the numberof times the corresponding method won againstother method.
For example, in the commodity do-main, the summaries that Lerman et al (2009)?smethod generated were compared with the sum-maries that Carenini et al (2006)?s method gener-ated 45 times, and Lerman et al (2009)?s methodwon 18 times.
The judges significantly preferredthe references in both domains.
There were nosignificant differences between our method andthe other two methods.
In the restaurant do-44C2 ?
100 = 6005http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txtmain, there was a significant difference between(Carenini et al, 2006) and (Lerman et al, 2009).Since we adopt ILP, our method tends to packshorter sentences into the summary.
However,our coherence score prevents this from degradingsummary readability.5 ConclusionThis paper proposed a novel algorithm for opinionsummarization that takes account of content andcoherence, simultaneously.
Our method directlysearches for the optimum sentence sequence byextracting and ordering sentences present in theinput document set.
We proposed a novel ILPformulation against selection-and-ordering prob-lems; it is a powerful mixture of the MaximumCoverage Problem and the Traveling SalesmanProblem.
Experiments revealed that the algo-rithm creates summaries that have higher ROUGEscores than existing opinion summarizers.
Wealso performed readability experiments.
Whileour summarizer tends to extract shorter sentencesto optimize summary content, our proposed co-herence score prevented this from degrading thereadability of the summary.One future work includes enriching the featuresused to determine the coherence score.
We expectthat features such as entity grid (Barzilay and La-pata, 2005) will improve overall algorithm perfor-mance.
We also plan to apply our model to tasksother than opinion summarization.AcknowledgmentsWe would like to sincerely thank Tsutomu Hiraofor his comments and discussions.
We would alsolike to thank the anonymous reviewers for theircomments.917ReferencesAlthaus, Ernst, Nikiforos Karamanis and Alexander Koller.2004.
Computing Locally Coherent Discourses.
In Proc.of the 42nd Annual Meeting of the Association for Com-putational Linguistics.Balas, Egon.
1989.
The prize collecting traveling salesmanproblem.
Networks, 19(6):621?636.Banko, Michele, Vibhu O. Mittal and Michael J. Witbrock.2000.
Headline Generation Based on Statistical Transla-tion.
In Proc.
of the 38th Annual Meeting of the Associa-tion for Computational Linguistics.Barzilay, Regina, Noemie Elhadad and Kathleen McKeown.2002.
Inferring Strategies for Sentence Ordering in Mul-tidocument Summarization.
Journal of Artificial Intelli-gence Research, 17:35?55.Barzilay, Regina and Mirella Lapata.
2005.
Modeling Lo-cal Coherence: An Entity-based Approach.
In Proc.
ofthe 43rd Annual Meeting of the Association for Compu-tational Linguistics.Carbonell, Jaime and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering docu-ments and producing summaries.
In Proc.
of the 21st An-nual International ACM SIGIR Conference on Researchand Development in Information Retrieval.Carenini, Giuseppe, Raymond Ng and Adam Pauls.
2006.Multi-Document Summarization of Evaluative Text.
InProc.
of the 11th Conference of the European Chapter ofthe Association for Computational Linguistics.Clarke, James and Mirella Lapata.
2007.
Modelling Com-pression with Discourse Constraints.
In Proc.
of the 2007Joint Conference on Empirical Methods in Natural Lan-guage Processing and Computational Natural LanguageLearning.Collins, Michael.
2002.
Discriminative Training Methods forHidden Markov Models: Theory and Experiments withPerceptron Algorithms.
In Proc.
of the 2002 Conferenceon Empirical Methods in Natural Language Processing.Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine Learning Re-search, 7:551?585.Deshpande, Pawan, Regina Barzilay and David R. Karger.2007.
Randomized Decoding for Selection-and-OrderingProblems.
In Proc.
of Human Language Technologies2007: The Conference of the North American Chapter ofthe Association for Computational Linguistics.Elsner, Micha, Joseph Austerweil and Eugene Charniak.2007.
A unified local and global model for discourse co-herence.
In Proc.
of Human Language Technologies 2007:The Conference of the North American Chapter of the As-sociation for Computational Linguistics.Filatova, Elena and Vasileios Hatzivassiloglou.
2004.
A For-mal Model for Information Selection in Multi-SentenceText Extraction.
In Proc.
of the 20th International Con-ference on Computational Linguistics.Gillick, Dan and Benoit Favre.
2009.
A Scalable GlobalModel for Summarization.
In Proc.
of Human LanguageTechnologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics Workshop on Integer Linear Programming forNLP.Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and YujiMatsumoto.
2002.
Extracting important sentences withsupport vector machines.
In Proc.
of the 19th Interna-tional Conference on Computational Linguistics.Kupiec, Julian, Jan Pedersen and Francine Chen.
1995.
ATrainable Document Summarizer.
In Proc.
of the 18th An-nual International ACM SIGIR Conference on Researchand Development in Information Retrieval.Lapata, Mirella.
2003.
Probabilistic Text Structuring: Exper-iments with Sentence Ordering.
In Proc.
of the 41st An-nual Meeting of the Association for Computational Lin-guistics.Lapata, Mirella.
2006.
Automatic Evaluation of Informa-tion Ordering: Kendall?s Tau.
Computational Linguistics,32(4):471?484.Lerman, Kevin, Sasha Blair-Goldensohn and Ryan McDon-ald.
2009.
Sentiment Summarization: Evaluating andLearning User Preferences.
In Proc.
of the 12th Confer-ence of the European Chapter of the Association for Com-putational Linguistics.Lin, Chin-Yew.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In Proc.
of Text SummarizationBranches Out.Martins, Andre F. T., and Noah A. Smith.
2009.
Summariza-tion with a Joint Model for Sentence Extraction and Com-pression.
In Proc.
of Human Language Technologies: The2009 Annual Conference of the North American Chapterof the Association for Computational Linguistics Work-shop on Integer Linear Programming for NLP.McDonald, Ryan.
2007.
A Study of Global Inference Algo-rithms in Multi-document Summarization.
In Proc.
of the29th European Conference on Information Retrieval.Nishikawa, Hitoshi, Takaaki Hasegawa, Yoshihiro Matsuoand Genichiro Kikui.
2010.
Optimizing Informativenessand Readability for Sentiment Summarization.
In Proc.
ofthe 48th Annual Meeting of the Association for Computa-tional Linguistics.Radev, Dragomir R., Hongyan Jing, Magorzata Sty andDaniel Tam.
2004.
Centroid-based summarization of mul-tiple documents.
Information Processing and Manage-ment, 40(6):919?938.Soricut, Radu and Daniel Marcu.
2006.
Discourse Genera-tion Using Utility-Trained Coherence Models.
In Proc.
ofthe 21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Association forComputational Linguistics.Takamura, Hiroya and Manabu Okumura.
2009.
Text Sum-marization Model based on Maximum Coverage Problemand its Variant.
In Proc.
of the 12th Conference of the Eu-ropean Chapter of the Association for Computational Lin-guistics.Yih, Wen-tau, Joshua Goodman, Lucy Vanderwende andHisami Suzuki.
2007.
Multi-Document Summarization byMaximizing Informative Content-Words.
In Proc.
of the20th International Joint Conference on Artificial Intelli-gence.918
