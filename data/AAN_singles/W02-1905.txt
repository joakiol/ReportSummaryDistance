Extracting Exact Answers to Questions Based on Structural Links?Wei Li, Rohini K. Srihari, Xiaoge Li, M. Srikanth, Xiuhong Zhang, Cheng NiuCymfony Inc.600 Essjay Road, Williamsville, NY 14221.
USA.
{wei, rohini, xli, srikanth, xzhang, cniu}@cymfony.comKeywords:  Question Answering, Information Extraction, Semantic Parsing, Dependency Link?
This  work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate(AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090.AbstractThis paper presents a novel approach toextracting phrase-level answers in a questionanswering system.
This approach usesstructural support provided by an integratedNatural Language Processing (NLP) andInformation Extraction (IE) system.
Bothquestions and the sentence-level candidateanswer strings are parsed by this NLP/IEsystem into binary dependency structures.Phrase-level answer extraction is modelled bycomparing the structural similarity involvingthe question-phrase and the candidate answer-phrase.There are two types of structural support.
Thefirst type involves predefined, specific entityassocia tions such as Affiliation, Position, Agefor a person entity.
If a question asks aboutone of these associations, the answer-phrasecan be determined as long as the systemdecodes such pre-defined dependency linkscorrectly, despite the syntactic differenceused in expressions between the question andthe candidate answer string.
The second typeinvolves generic grammatical relationshipssuch as V-S (verb-subject), V-O (verb-object).Preliminary experimental results show animprovement in both precision and recall inextracting phrase-level answers, comparedwith a baseline system which only uses NamedEntity constraints.
The proposed methods areparticularly effective in cases where thequestion-phrase does not correspond to aknown named entity type and in cases wherethere are multiple candidate answer-phrasessatisfying the named entity constraints.IntroductionNatural language Question Answering (QA) isrecognized as a capability with great potential.The NIST-sponsored Text Retrieval Conference(TREC) has been the driving force for developingthis technology through its QA track since TREC-8(Voorhees 1999).
There has been significantprogress and interest in QA research in recentyears (Voorhees 2000, Pasca and Harabagiu 2001).QA is different than search engines in two aspects:(i) instead of a string of keyword search terms, thequery is a natural language question, necessitatingquestion parsing, (ii) instead of a list of documentsor URLs, a list of candidate answers at phrase levelor sentence level are expected to be returned inresponse to a query, hence the need for textprocessing beyond keyword indexing, typicallysupported by Natural Language Processing (NLP)and Information Extraction (IE) (Chinchor andMarsh 1998, Hovy, Hermjakob and Lin 2001, Liand Srihari 2000).
Examples of the use of NLP andIE in Question Answering include shallow parsing(Kupiec, 1993), semantic parsing (Litkowski1999), Named Entity tagging (Abney et al 2000,Srihari and Li 1999) and high-level IE (Srihariand Li, 2000).Identifying exact or phrase-level answers is amuch more challenging task than sentence-levelanswers.
Good performance on the latter can beachieved by using sophisticated passage retrievaltechniques and/or shallow level NLP/IEprocessing (Kwok et al 2001, Clarke et al 2001).The phrase-level answer identification involvessophisticated NLP/IE and it is difficult to applyonly IR techniques for this task (Prager et al1999).
These two tasks are closely related.
Manysystems (e.g.
Prager et al1999; Clark et al2001)take a two-stage approach.
The first stageinvolves retrieving sentences or paragraphs indocuments as candidate answer strings.
StageTwo focuses on extracting phrase-level exactanswers from the candidate answer strings.This paper focuses on methods involving StageTwo.
The input is a sentence pair consisting of aquestion and a sentence-level candidate answerstring.
The output is defined to be a phrase, calledanswer-point, extracted from the candidateanswer string.
In order to identify the answer-point, the pair of strings are parsed by the samesystem to generate binary dependency structuresfor both specific entity associations and genericgrammatical relationships.
An integrated NaturalLanguage Processing (NLP) and InformationExtraction (IE) engine is used to extract namedentities (NE) and their associations and to decodegrammatical relationships.
The system searchesfor an answer-point by comparing the structuralsimilarity involving the question-phrase and acandidate answer-phrase.
Generic grammaticalrelationships are used as a back-off for specificentity associations when the question goes beyondthe scope of the specific associations or when thesystem fails to identify the answer-point whichmeets the specific  entity association constraints.The proposed methods are particularly helpful incases where the question-phrase does notcorrespond to a known named entity type and incases where there are multiple candidate answer-points to select from.The rest of the paper is structured as follows:Section 1 presents the NLP/IE engine used,sections 2 discusses how to identify and formallyrepresent what is being asked, section 3 presentsthe algorithm on identifying exact answersleveraging structural support, section 4 presentscase studies and benchmarks, and section 5 is theconclusion.Kernel IE Modules Linguistic  ModulesEntityAssociationNamedEntityPart-Of-SpeechAsking-pointIdentificationOutput(Entity,PhraseandStructuralLinks)ShallowParsingSemanticParsingTokenizerInputFigure 1: InfoXtract?
NLP/IE System Architecture1 NLP/IE Engine DescriptionThe NLP/IE engine used in the QA systemdescribed here is named InfoXtract?.
It consistsof an NLP component and IE component, eachconsisting of a set of pipeline modules (Figure 1).The NLP component serves as underlying supportfor IE.
A brief description of these modules isgiven below.?
Part-of-Speech Tagging: tagging syntacticcategories such as noun, verb, adjective, etc.?
Shallow Parsing: grouping basic linguisticunits as building blocks for structural links,such as Basic Noun Phrase, Verb Group, etc.?
Asking-point Identification: analysis ofquestion sentences to determine what is beingasked?
Semantic Parsing: decoding grammaticaldependency relationships at the logical levelbetween linguistic units, such as Verb-Subject(V-S), Verb-Object (V-O), Head-Modifier(H-M) relationships; both active patterns andpassive patterns will be parsed into the sameunderlying logical S-V-O relationships?
Named Entity Tagger: classifying propernames and other phrases to differentcategories such as Person, Organization,Location, Money, etc.?
Entity Association Extractor: relating namedentities with predefined associations such asAffiliation, Position, Age, Spouse, Address,etc.The NE tagger in our system is benchmarked toachieve close to human performance, around orabove 90% precision and recall for mostcategories of NE.
This performance providesfundamental support to QA.
Many questionsrequire a named entity or information associatedwith a named entity as answers.
A subset of theNE hierarchy used in our system is illustratedbelow:Person: woman, manOrganization: company, government,association, school, army, mass-mediaLocation: city, province, country, continent,ocean, lake, etc.Time Expressions: hour, part-of-day, day-of-week, date, month, season, year, decade,century, durationNumerical Expressions: percentage, money,number, weight, length, area, etc.Contact expressions: email, address,telephone, etc.The Entity Association module correlates namedentities and extracts their associations with otherentities or phrases.
These are specific, predefinedrelationships for entities of person andorganization.
Currently, our system can extractthe following entity associations with highprecision (over 90%) and modest recall rangingfrom 50% to 80% depending on the size ofgrammars written for each specific association.Person: affiliation, position, age, spouse,birth-place, birth-time, etc.Organization: location, staff, head, products,found-time, founder, etc.Entity associations are semantic structures veryuseful in supporting QA.
For example, from thesentence Grover Cleveland , who in June 1886married 21-year-old Frances Folsom,?the IEengine can identify the following associations:Spouse: Grover Cleveland ?Frances FolsomSpouse: Frances?Grover ClevelandAge:  Frances Folsom?21-year-oldA question asking about such an association, say,Q11: Who was President Cleveland ?s wife, will beparsed into the following association link betweena question-phrase ?Who?
and the entity ?Cleveland?
(see Section 2): Spouse: Cleveland ?
Who.
Thesemantic similarity between this structure and thestructure Spouse: Grover Cleveland ?
FrancesFolsom can determine the answer point to be?Frances Folsom?.The Semantic Parsing module decodes thegrammatical dependency relationships: V-S, V-O,V-C (Verb-Complement), H-M of time, location,reason, manner, purpose, result, etc.
This moduleextends the shallow parsing module through theuse of a cascade of handcrafted pattern matchingrules.
Manual benchmarking shows results withthe following performance:H-M: Precision 77.5%V-O: Precision 82.5%V-S: Precision 74%V-C: Precision 81.4%In our semantic parsing, not only passive patternswill be decoded into the same underlyingstructures as active patterns, but structures forverbs such as acquire and for de-verbal nouns suchas acquisition lead to the same dependency links,as shown below.AOL acquired Netscape in 1998.
?V-S: acquired?
AOLV-O: acquired ?
NetscapeH-M: acquired ?
in 1998 (time-modifier)Netscape was acquired by AOL in 1998.
?V-S: was acquired ?
by AOLV-O: was acquired ?
NetscapeH-M: was acquired ?
in 1998 (time-modifier)the acquisition of Netscape by AOL in 1998?
?V-S: acquisition ?
by AOLV-O: acquisition ?
of NetscapeH-M: acquired ?
in 1998 (time-modifier)These links can be used as structural support toanswer questions like Who acquired Netscape orwhich company was acquired by AOL.Obviously, our semantic parser goes one stepfurther than parsers which only decode syntacticrelationships.
It consumes some surface structurevariations to provide the power of comparing thestructural similarity at logical level.
However,compared with the entity association structureswhich sits at deep semantic level, the logical SVO(Subject-Verb-Object) structures still cannotcapture semantic relations which are expressedusing different head verbs with differentstructures.
An example is the pair : X borrows Yfrom Z versus Z lends Y to X.2 Asking Point Link IdentificationAsking point link identification is a crucial step ina QA system.
It provides the necessaryinformation decoded from question processing fora system to locate the corresponding answer-points from candidate answer strings.The Asking-point (Link) Identification Module ischarged with the task of parsing wh-phrases intheir context into three categories: NE Asking-point, Asking-point Association  Link andAsking-point Grammar  Link.
Asking Point refersto the question phrases with its constraints  that acorresponding answer-point should satisfy inmatching.
Asking-point Link is the decoded binaryrelationship from the asking point to another unitin the question.The identification of the NE asking point isessentially mapping the wh-phrase to the NEtypes or subtypes.
For example, which year ismapped to [which year]/NeYear, how old mappedto [how old]/NeAge, and how long mapped to[how long]/NeLength or [how long]/NeDuration,etc.The identification of the Asking-point AssociationLink is to decide whether the incoming questionasks about a predefined association relationship.For Asking-point Association  Link, the moduleneeds to identify the involved entity and the askedassociation.
For example, the Asking-pointAssociation  Link for How old is John Smith is theAGE relationship of the NePerson John Smith,represented as AGE: John Smith ?
[howold]/NeAge.The wh-phrases which may or may not be mappedto NE asking points and whose dependency linksare beyond predefined associations lead to Asking-point Grammar Links, e.g.
How did Julian Hilldiscover nylon?
This asking-point link isrepresented as H-M: discover ?
[How]/manner-modifier.
As seen, an asking-point grammar linkonly involves generic grammatical constraints: inthis case, the constraints for a candidate answer-point to satisfy during matching are H-M link with?discover?
as head and a phrase which must be amodifier of manner.These three types of asking points and theirpossible links form a natural hierarchy that can beused to facilitate the backoff strategy for theanswer-point extraction module (see Section 3):Asking-point Association Link ?
Asking-pointGrammar Link ?
NE Asking Point.
Thishierarchy defines the sequence of matching stepswhich should be followed during the answer-pointextraction.The backoff from Asking-point Association  Linkto Asking-point Grammar  Link is necessary as thelatter represents more generic structural constraintsthan the former.
For example, in the sentencewhere is IBM located, the Asking-pointAssociation Link is LOCATION: IBM ?
[where]/NeLocation while the default GrammarLink is H-M: located ?
[where]/location-modifier.
When the specific association constraintscannot be satisfied, the system should attempt tolocate an answer-point by searching for a location-modifier of the key verb ?located?.The NE asking point constraints are also markedfor asking-point association links and those asking-point grammar links whose wh-phrases can bemapped to NE asking points.
Backing off to theNE asking point is required in cases where theasking-point association constraints andgrammatical structural constraints cannot besatisfied.
For How old is John Smith, the asking-point grammar  link is represented as H-M: JohnSmith ?
[how old]/NeAge.
If the system cannotfind a corresponding AGE association or amodifier of NeAge for the entity John Smith tosatisfy the structural constraints, it will at leastattempt to locate a candidate answer-point byenforcing the NE asking point constraints NeAge.When there is only one NeAge in the answerstring, the system can extract it as the onlypossible answer-point even if the structuralconstraints are not honored.3 Answer Point IdentificationThe answer-point identification is accomplishedthrough  matching the asking-point to candidateanswer-points using the following back-offalgorithm based on the processing results of thequestion and the sentence-level candidate answerstring.
(1) if there is Asking-point AssociationLink, call Match(asking-point associationlink, candidate answer-point associationlink) to search for the correspondingassociation to locate answer-point(2) if step (1) fails and there is an asking-point grammar link, call Match(asking-point grammar link, candidate answer-point grammar link) to search for thecorresponding grammar link to locate theanswer-point(3) if step (2) fails and there is an NE askingpoint, search for the corresponding NEs:if there is only one corresponding NE,then extract this as the answer-point elsemark all corresponding NEs as candidateanswer-pointsThe function Match(asking-point link, candidateanswer-point link) is defined as (i) exact match orsynonym match of the related units (synonymmatch currently confined to verb vs. de-verbalnoun); (ii) match the relation type directly (e.g.
V-S matches V-S, AGE matches AGE, etc.
); (iii)match the type of asking point and answer point(e.g.
NePerson asking point matches NePerson andits sub-types NeMan and NeWoman; ?how?matches manner-modifier; etc.
): either throughdirect link or indirect link based on conjunctivelink (ConjLink) or equivalence link (S-P, subject-predicative or appositive relations between twoNPs).Step (1) and Step (2) attempt to leverage thestructural support from parsing and high-levelinformation extraction beyond NE.
It is worthnoticing that in our experiment, the structuralsupport used for answer-point identification onlychecks the binary links involving the asking pointand the candidate answer points, instead of fulltemplate matching as proposed in (Srihari and Li,2000).Full template matching is best exemplified by thefollowing example.
If the incoming question isWho won the Nobel Prize in 1991, and thecandidate answer string is John Smith won theNobel Prize in 1991, the question template andanswer template are shown below:winV-S: NePerson [Who]V-O: NP [the Nobel Prize]H-M: NeYear [1991]winV-S: NePerson [John Smith]V-O: NP [the Nobel Prize]H-M: NeYear [1991]The template matching will match the asking pointWho with the answer point John Smith because forall the dependency links in the trees, theinformation is all compatible (in this case, exactmatch).
This is the ideal case of full templatematching and guarantees the high precision of theextracted answer point.However, in practice, full template matching isneither realistic for most of cases nor necessary forachieving the objective of extracting answer pointsin a two-stage approach.
It is not realistic becausenatural language semantic parsing is such achallenging problem that a perfect dependency tree(or full template) which pieces together everylinguistic unit is not always easy to decode.
ForInfoXtract,, in most cases, the majority, but notall, of the decoded binary dependency links areaccurate, as shown in the benchmarks above.
Insuch situations, insisting on checking everydependency link of a template tree is too strong acondition to meet.
On the other hand, it is actuallynot necessary to check all the links in thedependency trees for full template matching.
Withthe modular design and work division betweensentence level candidate answer string generationmodule (Stage One) and answer-point extractionfrom the candidate answer strings (Stage Two),all the candidate answer strings are alreadydetermined by previous modules as highlyrelevant.
In this situation, a simplified partialtemplate matching, namely, ?asking/answer pointbinary relation matching?, will be sufficient toselect the answer-point, if present, from thecandidate answer string.
In other words, thesystem only needs to check this one dependencylink in extracting the answer-point.
For theprevious example, only the asking/answer pointbinary dependency links need to be matched asillustrated below:V-S win?
[Who]/NePersonV-S win?
[John Smith]/NeManSome sample results are given in section 4 toillustrate how answer-points are identified basedon matching binary relations involvingasking/answer points.4 Experiments and ResultsIn order to conduct the feasibility study on theproposed method, we selected the first 100questions from the TREC-8 QA track pool andthe corresponding first candidate answersentences for this preliminary experiment.
TheStage One processing for generating candidateanswer sentences was conducted by the existingranking module of our QA system.
The StageTwo processing for answer-point identificationwas accomplished by using the algorithmdescribed in Section 3.As shown in Table 1, out of the 100 question-answer pairs we selected, 9 have detectedassociation links involving asking/answer points,44 are found to have grammar links involvingasking/answer points.Table 1: Experiment Resultsdetected correct fail precision recallAssociationLinks 9 8 1 89% 8%GrammarLinks 44 39 6 89% 39%NE Points(Baseline) 76 41 35 54% 41%Overallperformance 86 71 14 83% 71%As for NE asking points, 76 questions wereidentified to require some type of NE as answers.Assume that a baseline answer-point identificationsystem only uses NE asking points as constraints,out of the 76 questions requiring NEs as answers,41 answer-points were identified successfullybecause there was only one NE in the answerstring which matches the required NE type.
Thefailed cases in matching NE asking pointconstraints include two situations: (i) no NE existsin the answer string; (ii) multiple NEs satisfy thetype constraints of NE asking points (i.e.
morethan one candidate answer-points found from theanswer string) or there is type conflict during thematching of NE asking/answer points.
Therefore,the baseline system would achieve 54% precisionand 41% recall based on the standard precision andrecall formulas:Precision = Correct / DetectedRecall = Correct / Relevant.In comparison, in our answer-point identificationsystem which leverages structural support fromboth the entity association links and grammar linksas well as the NE asking points, both the precisionand recall are raised: from the baseline 54% to83% for precision and from 41% to 71% for recall.The significant improvement in precision andrecall is attributed to the performance of structuralmatching in identifying exact answers.
Thisdemonstrates the benefits of making use ofsophisticated NLP/IE technology, beyond NE andshallow parsing.Using grammar links alone, exact answers wereidentified for 39 out of the 44 candidate answer-points satisfying the types of grammar links in 100cases.
During matching, 6 cases failed either due tothe parsing error or due to the type conflictbetween the asking/answer points (e.g.
violatingthe type constraints such as manner-modifier onthe answer-point for ?how?
question).
The highprecision and modest recall in using the grammarconstraints is understandable as the grammar linksimpose very strong constraints on both the nodesand the structural type.
The high precisionperformance indicates that grammar links notonly have the distinguishing power to identifyexact answers in the presence of multiple NEoptions but also recognize answers in the absenceof asking point types.Even stronger structural support comes from thesemantic relations decoded by the entityassociation extraction module.
In this case, theperformance is naturally high-precision (89%)low-recall (8%) as predefined association linksare by nature more sparse than genericgrammatical relations.In the following, we illustrate with someexamples with questions from the TREC-8 QAtask on how the match function identified inSection 3 applies to different question types.Q4: How much did Mercury spend onadvertising in 1993?
?
asking-point grammarlink:V-O spend ?
[How much]/NeMoneyA: Last year the company spent Pounds 12mon advertising.
?
candidate answer-pointgrammar link:V-O spent?
[Pounds 12m]/NeMoneyAnswer-point Output: Pounds 12mThis case requires (i) exact match in its originalverb form between spend and spent; (ii) V-O typematch; and (iii) asking/answer point typeNeMoney match through direct link.Q63: What nuclear-powered Russiansubmarine sank in the Norwegian Sea on April7, 1989??
asking-point grammar link:H-M submarine?
[What]A: NEZAVISIMAYA GAZETA on theKomsomolets nuclear-powered submarinewhich sank in the Norwegian Sea five yearsago:?
candidate answer-point grammar link:H-M submarine?KomsomoletsAnswer-point Output: KomsomoletsThis case requires (i) exact match of submarine;(ii) H-M type match; and (iii) asking/answer pointmatch through direct link:  there are no askingpoint type constraints because the asking pointgoes beyond existing NE.
This case highlights thepower of semantic parsing in answer-pointextraction.
Since there are no type constraints onanswer point,1 candidate answer points cannot beextracted without bringing in structural context bychecking the NE type.
Most of what-related askingpoints such as those in the patterns?what/which?N?, ?what type/kind of ?N?
gobeyond NE and require this type of structuralrelation checking to locate the exact answer.
Thecase below is another example.Q79: What did Shostakovich write forRostropovich??
asking-point grammar link:V-O write?
[What]A: The Polonaise from Tchaikovsky?s operaEugene was a brief but cracking opener and itsbrilliant bluster was no sooner in our ears thanforcibly contradicted by the bleak depression ofShostakovich?s second cello concerto, Op.
126,a late work written for Rostropovich in 1966between the thirteenth and fourteenthsymphonies.
?
candidate answer-pointgrammar link:V-O written?
[a late work]/NPS-P [Op.
126]/NP ?
[a late work]/NPAnswer-point Output: Op.
126This case requires (i) exact match in its originalverb form between ?written?
and ?write?
;(ii) V-O type match; and (iii) asking/answer pointmatch through indirect link based on equivalencelink S-P.
When there are no NE constraints on theanswer point, a proper name or an initial-capitalized NP is preferred over an ordinary,lower-case NP as an answer point.
This heuristic isbuilt-in so that ?Op.
126?
is output as the answer-point in this case instead of ?a late work?.1 Strictly speaking, there are some type constraints onthe answer point.
The type constraints are something tothe effect of ?a name for a kind of ship?
which goesbeyond the existing NE types defined.ConclusionThis paper presented an approach to exact answeridentification to questions using only binarystructural links involving the question-phrases.Based on the experiments conducted, somepreliminary conclusions can be arrived at.?
The Entity Association extraction helps inpinpointing exact answers precisely?
Grammar dependency links enable thesystem to not only identify exact answersbut answer questions not covered by thepredefined set of availableNEs/Associations?
Binary dependency links instead of fullstructural templates provide sufficient andeffective structural leverage for extractingexact answersSome cases remain difficult however, beyond thecurrent level of NLP/IE.
For example,Q92: Who released the Internet worm in thelate 1980s??
asking point link:V-S (released, NePerson[Who])A: Morris, suspended from graduate studies atCornell University at Syracuse, N,Y,, isaccused of designing and disseminating inNovember, 1988, a rogue program or ?worm?that immobilized some 6,000 computers linkedto a research network, including some used byNASA and the Air Force.?
answer point link:V-S (disseminating, NePerson[Morris])In order for this case to be handled, the followingsteps are required: (i) the semantic parser shouldbe able to ignore the past participle postmodifierphrase headed by ?suspended?
; (ii) the V-Odependency should be decoded between ?isaccused?
and ?Morris?
; (iii) the V-S dependencyshould be decoded between ?designing anddisseminating?
and ?Morris?
based on the patternrule ?accuse NP of Ving??
V-S(Ving, NP); (iv)the conjunctive structure should map the V-S(?designing and disseminating?, ?Morris?)
into twoV-S links; (v)  ?disseminate?
and ?release?
shouldbe linked somehow for synonym expansion.
Itmay be unreasonable to expect an NLP/IE systemto accomplish all of these, but each of the abovechallenges indicates some directions for furtherresearch in this topic.We would like to extend the experiments on alarger set of questions to further investigate theeffectiveness of structural support in extractingexact answers.
The TREC-9 and TREC 2001 QApool and the candidate answer sentences generatedby both NLP-based or IR-based QA systems wouldbe ideal for further testing this method.5 AcknowledgementThe authors wish to thank Walter Gadz and CarriePine of AFRL for supporting this work.
Thanksalso go to anonymous reviewers for their valuablecomments.ReferencesAbney, S., Collins, M and Singhal, A.
(2000) AnswerExtraction.
In Proceedings of ANLP -2000, Seattle.Chinchor, N. and Marsh, E. (1998) MUC -7 InformationExtraction Task Definition (version 5.1), In?Proceedings of MUC-7?.
Also published athttp://www.muc.saic.com/Clarke, C. L. A., Cormack, G. V. and Lynam, T. R.(2001), Exploiting Redundancy in QuestionAnswering.
In Proceedings of SIGIR?01, NewOrleans, LA.Hovy, E.H., U. Hermjakob, and Chin-Yew Lin.
2001.The Use of External Knowledge of Factoid QA.
InProceedings of the 10th Text Retrieval Conference(TREC 2001), Gaithersburg, MD, U.S.A., November13-16, 2001Kupiec, J.
(1993) MURAX: A Robust LinguisticApproach For Question Answering Using An On-LineEncyclopaedia .
In Proceedings of SIGIR-93,Pittsburgh, PA.Kwok, K. L., Grunfeld, L., Dinstl, N. and Chan, M.(2001), TREC2001 Question-Answer, Web and CrossLanguage Experiments using PIRCS.
In Proceedingsof TREC-10, Gaithersburg, MD.Li, W. and Srihari, R. (2000) A Domain IndependentEvent Extraction Toolkit , Phase 2 Final TechnicalReport, Air Force Research Laboratory/Rome, NY.Litkowski, K. C. (1999) Question-Answering UsingSemantic Relation Triples.
In Proceedings of TREC-8, Gaithersburg, MD.Pasca, M. and Harabagiu, S. M. High PerformanceQuestion/Answering.
In Proceedings of SIGIR 2001:pages 366-374Prager, J., Radev, D., Brown, E., Coden, A. and Samn,V., The use of predictive annotation for questionanswering in TREC8.
In Proceedings of TREC-8,Gaithersburg, MD.Srihari, R. and Li, W. (1999) Information Extractionsupported Question Answering.
In Proceedings ofTREC-8, Gaithersberg, MD.Srihari, R and Li, W. (2000b).
A Question AnsweringSystem Supported by Information Extraction.
InProceedings of ANLP 2000, Seattle.Voorhees, E. (1999), The TREC-8 Question AnsweringTrack Report, In Proceedings of TREC-8,Gaithersburg, MD.Voorhees, E. (2000), Overview of the TREC-9Question Answering Track , In Proceedings ofTREC-9, Gaithersburg, MD.
