FORUM ON CONNECTIONISMLanguage Learn ing  in Mass ive ly -Para l le l  NetworksTerrence J. SejnowskiBiophysics DepartmentJohns Hopkins UniversityBaltimore, MD 21218PANEL IST  STATEMENTMassively-parallel connectionist networks have tradition-ally been applied to constraint-satisfaction in early visualprocessing (Ballard, Hinton & Sejnowski, 1983), but are nowbeing applied to problems ranging from the Traveling-Salesman Problem to language acquisition (Rumelhart &MeClelland, 1986).
In these networks, knowledge isrepresented by the distributed pattern of activity in a largenumber of relatively simple neuron-like processing units, andcomputation is performed in parallel by the use of connec-tions between the units.A network model can be "programmed" by specifying thestrengths of the connections, or weights, on all the links be-tween the processing units.
In vision, it is sometimes possibleto design networks from a task analysis of ~he problem, aidedby the homogeneity of the domain.
For example, Sejnowski& Hinton (1986) designed a network that can separate figurefrom ground for shapes with incomplete bounding contours.Constructing a network is much more difficult in an in-homogeneous domain like natural language.
This problemhas been partially overcome by the discovery of powerfullearning algorithms that allow the strengths of connection ina network to be shaped by experience; that is, a good set ofweights can be found to solve a problem given only examplesof typical inputs and the desired outputs (Sejnowski, Kienker& Hinton, 198{}; Rumelhart, Hinton & Williams, 198{}).Network learning will be demonstrated for the problem ofconverting unrestricted English text to phonemes.
NETtalkis a network of 309 processing units connected by 18,629weights (Sejnowski & Rosenberg, 1986).
It was trained onthe 1,000 most common words in English taken from theBrown corpus and achieved 98% accuracy.
The same net-work was then tested for generalization on a 20,000 worddictionary: without further training it was 80% accurate andreached 92% with additional training.
The network mas-tered different letter-to-sound correspondence rules in vary-ing lengthsJof time; for example, the "hard e rule", c -  /k / ,was learned much faster than the "soft c rule", c ->  /s/ .NETtalk demonstrably learns the regular patterns ofEnglish pronunciation and also copes with the problem of ir-regularity in the corpus.
Irregular words are learned not bycreating a look-up table of exceptions, as is common in com-mercial text-to-speech systems uch as DECtalk, but by pat-tern recognition.
As a consequence, xceptional words are in-corporated into the network as easily as words with a regularpronunciation.
NETtalk is being used as a research tool tostudy phonology; it can also be used as a model for studyingacquired dyslexia and recovery from brain damage; severalinteresting phenomena in human learning and memory suchas the power law for practice and the spacing effect are in-herent properties of the distributed form of knowledgerepresentation used by NETtalk (Rosenberg & Sejnowski,1986).NETtalk has no access to syntactic or semantic infor-mation and cannot, for example, disambiguate the twopronunciations of "read".
Grammatical  analysis requireslonger range interactions at the level of word representations.However, it may be possible to train larger and more sophis-ticated networks on problems in these domains and incor-porate them into a system of networks that form a highlymodularized and distributed language analyzer.
At presentthere is no way to assess the computational complexity ofthese tasks for network models; the experience with NETtalksuggests that conventional measures of complexity derivedfrom rule-based models of language are not accurate in-dicators.REFERENCESBallard, D. H., Hinton, G. E., & Sejnowski, T. J., 1983.Parallel visual computation, Nature  306: 21-26.Rosenberg, C. R. & Sejnowski, T. J.
1986.
The effects ofdistributed vs massed practice on NETtalk, a massively-parallel network that learns to read aloud, (submitted forpublication).Rumelhart,  D. E., Hinton, G. E. & Williams, R. J.
1986.In: Para l le l  D is t r ibuted  Processing: Explorations inthe Microstructure of Cognition.
Edited by Rumelhart,D.
E. & McClelland, J. L. (Cambridge: MIT Press.
)Rumelhart,  D. E. & McClelland, J. L.
(Eds.)
198{}.Para l le l  D is t r ibuted  Processing: Explorations in theMicrostructure of Cognition.
(Cambridge: MIT Press.
)Sejnowski, T. J., Kienker, P. K. & Hinton, G. E. (inpress) Learning symmetry groups with hidden units: Beyondthe perceptron, Phys ica  D.Sejnowski, T. J.
& Hinton, G. E. 1986.
Separating figurefrom ground with a Boltzmann Machine, In: Vision, Brain& Cooperative Computation, Edited by M. A. Arbib &A. R. Hanson (Cambridge: MIT Press).Sejnowski, T. J.
& Rosenberg, C. R. 1986.
NETtalk: Aparallel network that learns to read aloud, Johns HopkinsUniversity Department of Electrical Engineering and Com-puter Science Technical Report 86/01.184
