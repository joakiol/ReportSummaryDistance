Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1133?1143, Dublin, Ireland, August 23-29 2014.A Structured Language Model for Incremental Tree-to-String TranslationHeng Yu11Institute of Computing Technology.
CASUniversity of Chinese Academy of Sciencesyuheng@ict.ac.cnHaitao MiT.J.
Watson Research CenterIBMhmi@us.ibm.comLiang HuangQueens College & Grad.
CenterCity University of New Yorkhuang@cs.qc.cuny.eduQun Liu1,22Centre for Next Generation Localisation.Faculty of Engineering and ComputingDublin City Universityqliu@computing.dcu.ieAbstractTree-to-string systems have gained significant popularity thanks to their simplicity and efficien-cy by exploring the source syntax information, but they lack in the target syntax to guaranteethe grammaticality of the output.
Instead of using complex tree-to-tree models, we integratea structured language model, a left-to-right shift-reduce parser in specific, into an incrementaltree-to-string model, and introduce an efficient grouping and pruning mechanism for this integra-tion.
Large-scale experiments on various Chinese-English test sets show that with a reasonablespeed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than astate-of-the-art tree-to-string system.1 IntroductionTree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gainedsignificant popularity in recent years, as they run faster than string-to-tree counterparts (e.g.
(Galley etal., 2006)), and do not need binarized grammars.
Especially, Huang and Mi (2010) make it much fasterby proposing an incremental tree-to-string model, which generates the target translation exactly in a left-to-right manner.
Although, tree-to-string models have made those progresses, they can not utilize thetarget syntax information to guarantee the grammaticality of the output, as they only generate strings onthe target side.One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-treemodels (e.g.
(Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)).
However, tree-to-tree approachesstill significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al.,2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010).Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Char-niak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achievedbetter perplexity than the traditional n-gram word sequence model.
More importantly, the Slm is inde-pendent of any translation model.
Thus, integrating a Slm into a tree-to-string model will not face theproblems that tree-to-tree models have.
However, integration is not easy, as the following two questionsarise.
First, the search space grows significantly, as a partial translation has a lot of syntax structures.Second, hypotheses in the same bin may not be comparable, since their syntactic structures may not becomparable, and the future costs are hard to estimate.
Hassan et al.
(2009) skip those problems by onlykeeping the best parsing structure for each hypothesis.In this paper, we integrate a shift-reduce parser into an incremental tree-to-string model, and intro-duce an efficient grouping and pruning method to handle the growing search space and incomparablehypotheses problems.
Large-scale experiments on various Chinese-English test sets show that with a rea-This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1133sonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than astate-of-the-art tree-to-string system.2 Linear-time Shift-reduce Parsingparsingaction signature dependency structures1 s0 q0Bush S 0sh Bush held S 1: Bushsh Bush held a S 2: Bush heldrexheldBusha S 3: Bush heldsh heldBusha meeting S 4: Bush held ash a meeting with S 5: Bush held a meetingrexheldBushmeetingawith S 6: Bush held a meetingreyheldBush meetingwith S 7: Bush held a meetingsh heldBush meetingwith Sharon S 8: Bush held a meeting withsh with Sharon S 9: Bush held a meeting with SharonreyheldBush meetingwithSharonS 10: Bush held a meeting with SharonreyheldBush meeting withS 11: Bush held a meeting with SharonFigure 1: Linear-time left-to-right dependency parsing.A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step,chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re)the top two (or more) items at the end of the stack (Aho and Ullman, 1972).
In the dependency parsingscenario, the reduce action is further divided into two cases: left-reduce (rex) and right-reduce (rey),depending on which one of the two items becomes the head after reduction.
Each parsing derivation canbe represented by a sequence of parsing actions.11342.1 Shift-reduce Dependency ParsingWe will use the following sentence as the running example:Bush held a meeting with SharonGiven an input sentence e, where ei is the ith token, ei...e j is the substring of e from i to j, a shift-reduceparser searches for a dependency tree with a sequence of shift-reduce moves (see Figure 1).
Startingfrom an initial structure S 0, we first shift (sh) a word e1, ?Bush?, onto the parsing stack s0, and form astructure S 1 with a singleton tree.
Then e2, ?held?, is shifted, and there are two or more structures in theparsing stack, we can use rexor reystep to combine the top two trees on the stack, replace them withdependency structure e1 x e0 or e1 y e0 (shown as S 3), and add one more dependency edge betweene0 and e1.Note that the shade nodes are exposed heads on which rexor reyparsing actions can be performed.The middle columns in Figure 1 are the parsing signatures: q0 (parsing queue), s0 and s1 (parsing stack),where s0 and s1 only have one level dependency.
Take the line of S 11 for example, ?a?
is not in thesignature.
As each action results in an update of cost, we can pick the best one (or few, with beam) aftereach action.
Costs are accumulated in each step by extracting contextual features from the structure andthe action.
As the sentence gets longer, the number of partial structures generated at each steps growsexponentially, which makes it impossible to search all of the hypothesis.
In practice, we usually use beamsearch instead.
(a) atomic featuress0.w s0.ts1.w s1.ts0.lc.t s0.rc.tq0.w q0.t(b) feature templatesunigrams0.w s0.t s0.w ?
s0.ts1.w s1.t s1.w ?
s1.tq0.w q0.t q0.w ?
q0.tbigrams0.w ?
s1.w s0.t ?
s1.ts0.t ?
q0.t s0.w ?
s0.t ?
s1.ts0.w ?
s1.w ?
s1.t s0.t ?
s1.w ?
s1.ts0.w ?
s0.t ?
s1.wtrigram s0.t ?
s1.t ?
q0.t s1.t ?
s0.t ?
s0.lc.ts1.t ?
s0.t ?
q0.t s1.t ?
s0.t ?
s0.rc.t(c) ??
parsing stack parsing queue ??.
.
.
s1 s0s0.lc ?
?
?
s0.rcq0Table 1: (a) atomic features, used for parsing signatures.
(b): parsing feature templates, adapted fromHuang and Sagae (2010).
x.w and x.t denotes the root word and POS tag of the partial dependency tree,x.lc and x.rc denote x?s leftmost and rightmost child respectively.
(c) the feature window.2.2 FeaturesWe view features as ?abstractions?
or (partial) observations of the current structure.
Feature templates fare functions that draw information from the feature window, consisting of current partial tree and firstword to be processed.
All Feature functions are listed in Table 1(b), which is a conjunction of atomic1135IPNPBu`sh??VPPPPyu?NPSha?lo?ngVPVVju?x?
?ngASleNPhu?`ta?nFigure 2: A parse treefeatures in Table 1(a).
To decide which action is the best of the current structure, we perform a three-wayclassification based on f, and conjoin these feature instances with each action:[f ?
(action=sh/rex/rey)]We extract all the feature templates from training data, and use the average perceptron algorithm andearly-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model.3 Incremental Tree-to-string Translation with SlmThe incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps:parsing and decoding.
A parser first parses the source language input into a 1-best tree in Figure 2, andthe linear incremental decoder then searches for the best derivation that generates a target-language stringin strictly left-to-right manner.
Figure 3 works out the full running example, and we describe it in thefollowing section.3.1 Decoding with SlmSince the incremental tree-to-string model generates translation in strictly left-to-right fashion, and theshift-reduce dependency parser also processes an input sentence in left-to-right order, it is intuitive tocombine them together.
The last two columns in Figure 3 show the dependency structures for the corre-sponding hypotheses.
Start at the root translation stack with a dot  before the root node IP:[ IP ],we first predict (pr) with rule r1,(r1) IP (x1:NP x2:VP)?
x1 x2,and push its English-side to the translation stack, with variables replaced by matched tree nodes, herex1 for NP and x2 for VP.
Since this translation action does not generate any translation string, we don?tperform any dependency parsing actions.
So we have the following translation stack[ IP ][ NP VP],where the dot  indicates the next symbol to process in the English word-order.
Since node NP is the nextsymbol, we then predict with rule r2,(r2) NP(Bu`sh??)?
Bush,and add it to the translation stack:[ IP ] [ NP VP ] [ Bush]Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the currenttranslation, which results in the new translation stack[ IP ] [ NP VP ] [Bush  ]1136translation parsingstack string dependency structure Slm[  IP ] S 01 pr [  IP ] [  NP VP] S 02 pr [  IP ] [ NP VP ] [  Bush ] S 03 sc [  IP ] [ NP VP] [Bush  ] Bush S 1: Bush P(Bush | S 0)4 co [  IP ] [NP  VP] S 1:5 pr [  IP ] [NP  VP] [ held NP with NP] S 1:6 sc [  IP ] [NP  VP] [held  NP with NP] held S 3: Bush held P(held | S 1)7 pr [ IP] [NP VP] [held NP with NP] [ a meeting] S 38 sc [ IP] [NP VP] [held  NP with NP] [a meeting  ] a meeting S 7: Bush held a meeting P(a meeting | S 3)9 co [ IP ] [NP VP] [held NP  with NP] S 710 sc [ IP] [NP VP] [held NP with  NP] with S 8: Bush held a meeting with P(with | S 7)S ?8: Bush held a meeting with P?
(with | S 7)11 pr [ IP] [NP VP] [held NP with  NP] [ Sharon] S 8S 8?12 sc [ IP ] [NP  VP] [held NP with  NP] [Sharon ] Sharon S 11: Bush held a meeting with Sharon P(Sharon | S 8)S ?11?
: Bush held a meeting with Sharon P?
(Sharon | S ?8)13 co [  IP ] [NP  VP] [held NP with NP ] S 1114 co [  IP ] [NP VP ] S 1115 co [ IP  ] S 11Figure 3: Simulation of the integraton of an Slm into an incremental tree-to-string decoding.
The firstcolumn is the line number.
The second column shows the translation actions: predict (pr), scan (sc), andcomplete (co).
S i denotes a dependency parsing structure.
The shaded nodes are exposed roots of S i.Immediately after each sc translation action, our shift-reduce parser is triggered.
Here, our parser appliesthe parsing action sh, and shift ?Bush?
into a partial dependency structure S 1 as a root ?Bush?
(shadednode) in Figure 3.
Now the top rule on the translation stack has finished (dot is at the end), so we complete(co) it, pop the top rule and advance the dot in the second-to-top rule, denoting that NP is completed:[ IP ] [NP  VP].Following this procedure, we have a dependency structure S 3 after we scan (sc) the word ?held?
andtake a shift (sh) and a left reduce (rex) parsing actions.
The shaded node ?held?
means exposed roots,that the shift-reduce parser takes actions on.Following Huang and Mi (2010), the hypotheses with same translation step1 fall into the same bin.Thus, only the prediction (pr) actions actually make a jump from a bin to another.
Here line 2 to 4 fallinto one bin (translation step = 4, as there are 4 nodes, IP, NP, VP and Bu`sh?
?, in the source tree arecovered).
Similarly, lines from 7 to 10 fall into another bin (translation step = 15).1The step number is defined by the number of tree nodes covered in the source tree, and it is not equal to the number oftranslation actions taken so far.1137Noted that as we number the bins by the translation step, only pr actions make progress, the sc andco actions are treated as ?closure?
operators in practice.
Thus we always do as many sc/co actions aspossible immediately after a pr step until the symbol after the dot is another non-terminal.
The totalnumber of bins is equal to the size of the parse tree, and each hypothesis has a constant number ofoutgoing hyper-edges to predict, so the time complexity is linear in the sentence length.After adding our Slm to this translation, an interesting branch occurs after we scan the word ?with?,we have two different partial dependency structures S 8 and S?8 for the same translation.
If we denoteN(S i) as the number of re actions that S i takes, N(S 8) is 3, while N(S ?8) is 4.
Here N(S i) does not takeinto account the number of sh parsing actions, since all partial structures with same translations shouldshift the same number of translations.
Thus, N(S i) determines the score of dependency structures, andonly the hypotheses with same N(S i) are comparable to each other.
In this case, we should distinguishS 8 with S?8, and if we make a prediction over the hypothesis of S 8, we can reach the correct parsing stateS 11 (shown in the red dashed line in Figure 3).So the key problem of our integration is that, after each translation step, we will apply different se-quences of parsing actions, which result in different and incomparable dependency structures with thesame translation.
In the following two Sections, we introduce three ways for this integration.3.2 Na?
?ve: Adding Parsing Signatures into Translation SignaturesOne straightforward approach is to add the parsing signatures (in Figure 1) of each dependency structure(in Figure 1 and Figure 3) to translation signatures.
Here, we only take into account of the s0 and s1 inthe parsing stack, as the q0 is the future word that is not available in translation strings.
For example, thedependency structure S 8 has parsing signatures:heldBush meetingwithWe add those information to its translation signature, and only the hypothesis that have same translationand parsing signatures can be recombined.So, in each translation bin, different dependency structures with same translation strings are treated asdifferent hypothesis, and all the hypothesis are sorted and ranked in the same way.
For example, S 8 andS ?8 are compared in the bin, and we only keep top b (the beam size) hypothesis for each bin.Obviously, this simple approach suffers from the incomparable problem for those hypothesis that havedifferent number of parsing actions (e.g.
S 8 and S ?8).
Moreover, it may result in very low translationvariance in each beam.3.3 Best-parse: Keeping the Best Dependency Structure for Each TranslationFollowing Hassan et al.
(2009), we only keep the best parsing tree for each translation.
That means aftera consecutive translation sc actions, our shift-reduce parser applies all the possible parsing actions, andgenerates a set of new partial dependency structures.
Then we only choose the best one with the highestSlm score, and only use this dependency structure for future predictions.For example, for the translation in line 10 in Figure 3, we only keep S 8, if the parsing score of S 8 ishigher than S ?8, although they are not comparable.
Another complicate example is shown in Figure 4,within the translation step 15, there are many alternatives with different parsing structures for the sametranslation (?a meeting with?)
in the third column, but we can only choose the top one in the final.3.4 Grouping: Regrouping Hypothesis by N(S ) in Each BinIn order to do comparable sorting and pruning, our basic idea is to regroup those hypotheses in a samebin into small groups by N(S ).
For each translation, we first apply all the possible parsing actions,and generate all dependency structures.
Then we regroup all the hypothesis with different dependencystructures based on the size of N(S ).1138Bush held al Bush held a meetingl ishBush held alreBush held a meetingl iBush held a meetingl ireshBush held a meetingl ireBush held a meeting withl i ishBush held a meeting withl i ishshBush held a meeting withl i iBush held a meeting withl i ishrereBush held a meeting withl i iBush held a meeting with Sharonl i ishBush held a meeting with Sharonl i iBush held a meeting with Sharonl i ireshBush held a meeting with Sharonl i ish......Bush held a meeting with Sharonl i ishBush held a meeting with Sharonl i ireBush held a meeting with Sharonl i ishBush held a meeting with Sharonl i ish......Bush held a meeting with Sharonl i ireStep 15 Step 16G1: N(S)=1......Bush held a meeting withl i iG2: N(S)=2G3: N(S)=3G4: N(S)=4Figure 4: Multi-beam structures of two bins with different translation steps (15 and 16).
The first threecolumns show the parsing movements in bin 15.
Each dashed box is a group based on the number ofreduce actions over the new translation strings (?a meeting with?
for bin 15, and ?Sharon?
for bin 16).G2 means two reduce actions have been applied.
After this regrouping, we perform the pruning in twophases: 1) keep top b states in each group, and labeled each group with the state with the highest parsingscore in this group; 2) sort the different groups, and keep top g groups.For example, Figure 4 shows two bins with two different translation steps (15 and 16).
In bin 15, thegraph shows the parsing movements after we scan three new words (?a?, ?meeting?, and ?with?).
Theparsing sh action happens from a parsing state in one column to another state in the next column, whilere happens from a state to another state in the same column.
The third column in bin 15 lists some partialdependency structures that have all new words parsed.
Here each dashed box is a group of hypothesiswith a same N(S ), e.g.
the G2 contains all the dependency structures that have two reduce actions afterparsed all the new words.
Then, we sort and prune each group by the beam size b, and each group labeledas the highest hypothesis in this group.
Finally, we sort those groups and only keep top g groups for thefuture predictions.
Again, in Figure 4, we can keep the whole group G3 and partial group of G2 if b = 2.In our experiments, we set the group size g to 5.3.5 Log-linear ModelWe integrate our dependency parser into the log-linear model as an additional feature.
So the decodersearches for the best translation e?
with a latent tree structure (evaluated by our Slm) according to thefollowing equation:e?
= argmaxe?Eexp(Slm(e) ?
ws +?ifi ?
wi) (1)where Slm(e) is the dependency parsing score calculated by our parser, ws is the weight of Slm(e), fi arethe features in the baseline model and wi are the weights.11394 Experiments4.1 Data PreparationThe training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respec-tively.
We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08(newswire portion) as our test sets.
We word-aligned the training data using GIZA++ with refinementoption ?grow-diag-and?
(Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeleyparser (Petrov and Klein, 2007).
we applied the algorithm of Galley et al.
(2004) to extract tree-to-stringtranslation rules.
Our trigram word language model was trained on the target side of the training corpususing the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing.
At decoding time, weagain parse the input sentences using the Berkeley parser, and convert them into translation forests usingrule pattern-matching (Mi et al., 2008).Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010).
We use thesame feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-ratetraining (Och, 2003) to maximize the Bleu score on the development set.Our dependency parser is an implementation of the ?arc-standard?
shift-reduce parser (Nivre, 2004),and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the trainingset, Section 22 as the held-out set, and Section 23 as the test set.
Using the same features as Huang andSagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010).
Weadd the structured language model as an additional feature into the baseline system.We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the scrip-t mteval-v13a.pl.
We also report the Ter scores.4.2 Complete Comparisons on MT08To explore the soundness of our approach, we carry out some experiments in Table 2.
With a beam size100, the baseline decoder achieves a Bleu score of 21.06 with a speed of 1.7 seconds per sentence.Since our dependency parser is trained on the English PTB, which is not included in the MT trainingset, there is a chance that the gain of Bleu score is due to the increase of new n-grams in the PTB data.In order to rule out this possibility, we use the tool SRILM to train another tri-gram language model onEnglish PTB and use it as a secondary language model for the decoder.
The Bleu score is 21.10, whichis similar to the baseline result.
Thus we can conclude that any gain of the following +Slm experimentsis not because of the using of the additional English PTB.Our second experiment re-ranks the 100-best translations of the baseline with our structured languagemodel trained on PTB.
The improvement is less than 0.2 Bleu, which is not statistically significant, asthe search space for re-ranking is relatively small compared with the decoding space.As shown in Section 3, we have three different ways to integrate an Slm to the baseline system:?
na?
?ve: adding the parsing signature to the translation signature;?
best-parse: keeping the best dependency structure for each translation;?
grouping: regrouping the hypothesis by N(S ) in each bin.The na?
?ve approach achieves a Bleu score of 19.12, which is significantly lower than the baseline.
Themain reason is that adding parsing signatures leads to very restricted translation variance in each beam.We also tried to increase the beam size to 1000, but we do not see any improvement.The fourth line in Table 2 shows the result of the best-parse (Hassan et al., 2009).
This approach onlyslows the speed by a factor of two, but the improvement is not statistically significant.
We manuallylooked into some dependency trees this approach generates, and found this approach always introducelocal parsing errors.The last line shows our efficient beam grouping scheme with a grouping size 5, it achieves a significantimprovement with an acceptable speed, which is about 6 times slower than the baseline system.1140System Bleu Speedbaseline 21.06 1.7+Slmre-ranking 21.23 1.73na?
?ve 19.12 2.6best-parse 21.30 3.4grouping (g=5) 21.64 10.6Table 2: Results on MT08.
The bold score is significantly better than the baseline result at level p < 0.05.System MT03 MT04 MT05 MT08 Avg.Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2baseline 19.94 10.73 22.03 18.63 19.92 11.45 21.06 10.37 12.80+Slm 21.49 9.44 22.33 18.38 20.51 10.71 21.64 9.88 12.10Table 3: Results on all test sets.
Bold scores are significantly better than the baseline system (p < 0.5).4.3 Final Results on All Test SetsTable 3 shows our main results on all test sets.
Our method gains an average improvement of 0.7 pointsin terms of (T-B)/2.
Results on NIST MT 03, 05, and 08 are statistically significant with p < 0.05, usingbootstrap re-sampling with 1000 samples (Koehn, 2004).
The average decoding speed is about 10 timesslower than the baseline.5 Related WorkThe work of Schwartz et al.
(2011) is similar in spirit to ours.
We are different in the following ways.First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention toa syntax-based system.
Second, their approach slowdowns the speed at near 2000 times, thus, they canonly tune their system on short sentences less than 20 words.
Furthermore, their results are from a muchbigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, thelarger beam size or the Slm.
In contrast, our approach gains significant improvements over a state-of-the-art tree-to-string baseline at a reasonable speed, about 6 times slower.
And we answer some questionsbeyond their work.Hassan et al.
(2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significantimprovement.
Different from their work, we pay more attention to the dependency parser, and we alsotest this approach in our experiments.
As they only keep 1-best parsing states during the decoding, theyare suffering from the local parsing errors.Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al.
(2005)to an incremental dependency parsing, and incorporate it into a phrase-based system.
But this incrementalparser remains in quadratic time.Besides, there are also some other efforts that are less closely related to ours.
Shen et al.
(2008)and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency andtree-to-tree models.
But they need parse the target side first, and encode target syntactic structures intranslation rules.
Both papers integrate dependency structures into translation model, we instead modelthe dependency structures with a monolingual parsing model over translation strings.6 ConclusionIn this paper, we presented an efficient algorithm to integrate a structured language model (an incremen-tal shift-reduce parser in specific) into an incremental tree-to-string system.
We calculate the structuredlanguage model scores incrementally at the decoding step, rather than re-scoring a complete transla-tion.
Our experiments suggest that it is important to design efficient pruning strategies, which have been1141overlooked in previous work.
Experimental results on large-scale data set show that our approach signif-icantly improves the translation quality at a reasonable slower speed than a state-of-the-art tree-to-stringsystem.The structured language model introduced in our work only takes into account the target string, andignores the reordering information in the source side.
Thus, our future work seeks to incorporate moresource side syntax information to guide the parsing of the target side, and tune a structured languagemodel for both Bleu and paring accuracy.
Another potential work lies in the more efficient searching andpruning algorithms for integration.AcknowledgmentsWe thank the three anonymous reviewers for helpful suggestions, and Dan Gildea and Licheng Fang fordiscussions.
Yu and Liu were supported in part by CAS Action Plan for the Development of WesternChina (No.
KGZD-EW-501) and a grant from Huawei Noah?s Ark Lab, Hong Kong.
Liu was partiallysupported by the Science Foundation Ireland (Grant No.
07/CE/I1142) as part of the CNGL at Dublin C-ity University.
Huang was supported by DARPA FA8750-13-2-0041 (DEFT), a Google Faculty ResearchAward, and a PSC-CUNY Award, and Mi by DARPA HR0011-12-C-0015.
The views and findings inthis paper are those of the authors and are not endorsed by the US or Chinese governments.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
Parsing of series in automatic computation.
In The Theory of Parsing,Translation, and Compiling, page Volume I.Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003.
Syntax-based language models for statistical machinetranslation.
In Proceedings of MT Summit IX.
Intl.
Assoc.
for Machine Translation.Ciprian Chelba and Frederick Jelinek.
2000.
Structured language modeling.
volume 14, pages 283 ?
332.Michael Collins and Brian Roark.
2004.
Incremental parsing with the perceptron algorithm.
In Proceedings ofACL.Michel Galley and Christopher D. Manning.
2009.
Quadratic-time dependency parsing for machine translation.In Proceedings of the Joint Conference of ACL 2009 and AFNLP, pages 773?781, Suntec, Singapore, August.Association for Computational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu.
2004.
What?s in a translation rule?
In Proceed-ings of HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.2006.
Scalable inference and training of context-rich syntactic translation models.
In Proceedings of COLING-ACL, pages 961?968.Hany Hassan, Khalil Sima?an, and Andy Way.
2009.
A syntactified direct translation model with linear-time de-coding.
In Proceedings of EMNLP 2009, pages 1182?1191, Singapore, August.
Association for ComputationalLinguistics.Liang Huang and Haitao Mi.
2010.
Efficient incremental decoding for tree-to-string translation.
In Proceedingsof EMNLP, pages 273?283.Liang Huang and Kenji Sagae.
2010.
Dynamic programming for linear-time incremental parsing.
In Proceedingsof ACL 2010, pages 1077?1086, Uppsala, Sweden, July.
Association for Computational Linguistics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Statistical syntax-directed translation with extended domainof locality.
In Proceedings of AMTA, pages 66?73.Liang Huang, Suphan Fayong, and Yang Guo.
2012.
Structured perceptron with inexact search.
In Proceedingsof NAACL 2012, Montreal, Quebec.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedingsof NAACL, pages 127?133.1142Philipp Koehn.
2004.
Statistical significance tests for machine translation evaluation.
In Proceedings of EMNLP,pages 388?395.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine translation.In Proceedings of COLING-ACL, pages 609?616.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improving tree-to-tree translation with packed forests.
In Proceedingsof ACL/IJCNLP, pages 558?566, Suntec, Singapore, August.Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedings of HLT-EMNLP, pages 523?530, Vancouver, British Columbia,Canada, October.Haitao Mi and Qun Liu.
2010.
Constituency to dependency translation with forests.
In Proceedings of ACL, pages1433?1442, Uppsala, Sweden, July.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT, pages192?199.Joakim Nivre.
2004.
Incrementality in deterministic dependency parsing.
In Frank Keller, Stephen Clark, MatthewCrocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engi-neering and Cognition Together, pages 50?57, Barcelona, Spain, July.
Association for Computational Linguis-tics.Franz Joseph Och.
2003.
Minimum error rate training in statistical machine translation.
In Proceedings of ACL,pages 160?167.Slav Petrov and Dan Klein.
2007.
Improved inference for unlexicalized parsing.
In Proceedings of HLT-NAACL,pages 404?411.Matt Post and Daniel Gildea.
2008.
Language modeling with tree substitution grammars.
In Proceedings ofAMTA.Matt Post and Daniel Gildea.
2009.
Language modeling with tree substitution grammars.
In Proceedings of NIPSworkshop on Grammar Induction, Representation of Language, and Language Learning.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
Dependency treelet translation: Syntactically informedphrasal smt.
In Proceedings of the 43rd ACL, Ann Arbor, MI, June.Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu.
2011.
Incremental syntactic languagemodels for phrase-based translation.
In Proceedings of ACL 2011, pages 620?631, June.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
A new string-to-dependency machine translation algorithmwith a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,June.
Association for Computational Linguistics.Andreas Stolcke.
2002.
SRILM ?
an extensible language modeling toolkit.
In Proceedings of ICSLP, volume 30,pages 901?904.1143
