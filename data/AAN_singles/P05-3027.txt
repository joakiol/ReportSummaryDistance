Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 105?108, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSenseClusters: Unsupervised Clustering and Labeling of Similar ContextsAnagha Kulkarni and Ted PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812{kulka020,tpederse}@d.umn.eduhttp://senseclusters.sourceforge.netAbstractSenseClusters is a freely available systemthat identifies similar contexts in text.
Itrelies on lexical features to build first andsecond order representations of contexts,which are then clustered using unsuper-vised methods.
It was originally devel-oped to discriminate among contexts cen-tered around a given target word, but cannow be applied more generally.
It alsosupports methods that create descriptiveand discriminating labels for the discov-ered clusters.1 IntroductionSenseClusters seeks to group together units of text(referred to as contexts) that are similar to each otherusing lexical features and unsupervised clustering.Our initial work (Purandare and Pedersen, 2004)focused on word sense discrimination, which takesas input contexts that each contain a given targetword, and produces as output clusters that are pre-sumed to correspond to the different senses of theword.
This follows the hypothesis of (Miller andCharles, 1991) that words that occur in similar con-texts will have similar meanings.We have shown that these methods can be ex-tended to proper name discrimination (Pedersen etal., 2005).
People, places, or companies often sharethe same name, and this can cause a considerableamount of confusion when carrying out Web searchor other information retrieval applications.
Namediscrimination seeks to group together the contextsthat refer to a unique underlying individual, and al-low the user to recognize that the same name is beingused to refer to multiple entities.We have also extended SenseClusters to clus-ter contexts that are not centered around any tar-get word, which we refer to as headless clustering.Automatic email categorization is an example of aheadless clustering task, since each message can beconsidered a context.
SenseClusters will group to-gether messages if they are similar in content, with-out requiring that they share any particular targetword between them.We are also addressing a well known limitation tounsupervised clustering approaches.
After cluster-ing contexts, it is often difficult to determine whatunderlying concepts or entities each cluster repre-sents without manually inspecting their contents.Therefore, we are developing methods that automat-ically assign descriptive and discriminating labels toeach discovered cluster that provide a characteriza-tion of the contents of the clusters that a human caneasily understand.2 Clustering MethodologyWe begin with the collection of contexts to be clus-tered, referred to as the test data.
These may all in-clude a given target word, or they may be headlesscontexts.
We can select the lexical features from thetest data, or from a separate source of data.
In eithercase, the methodology proceeds in exactly the sameway.SenseClusters is based on lexical features, in par-ticular unigrams, bigrams, co?occurrences, and tar-105get co?occurrences.
Unigrams are single words thatoccur more than five times, bigrams are orderedpairs of words that may have intervening words be-tween them, while co-occurrences are simply un-ordered bigrams.
Target co-occurrences are thoseco?occurrences that include the given target word.We select bigrams and co?occurrences that occurmore than five times, and that have a log?likelihoodratio of more than 3.841, which signifies a 95% levelof certainty that the two words are not independent.We do not allow unigrams to be stop words, and weeliminate any bigram or co?occurrence feature thatincludes one or more stop words.Previous work in word sense discrimination hasshown that contexts of an ambiguous word can be ef-fectively represented using first order (Pedersen andBruce, 1997) or second order (Schu?tze, 1998) rep-resentations.
SenseClusters provides extensive sup-port for both, and allows for them to be applied in awider range of problems.In the first order case, we create a context (rows)by lexical features (columns) matrix, where the fea-tures may be any of the above mentioned types.
Thecell values in this matrix record the frequencies ofeach feature occurring in the context represented bya given row.
Since most lexical features only occur asmall number of times (if at all) in each context, theresulting matrix tends to be very sparse and nearlybinary.
Each row in this matrix forms a vector thatrepresents a context.
We can (optionally) use Sin-gular Value Decomposition (SVD) to reduce the di-mensionality of this matrix.
SVD has the effect ofcompressing a sparse matrix by combining redun-dant columns and eliminating noisy ones.
This al-lows the rows to be represented with a smaller num-ber of hopefully more informative columns.In the second order context representation we startwith creating a word by word co-occurrence ma-trix where each row represent the first word and thecolumns represent the second word of either bigramor co?occurrence features previously identified.
Ifthe features are bigrams then the word matrix isasymmetric whereas for co-occurrences it is sym-metric and the rows and columns do not suggest anyordering.
In either case, the cell values indicate howoften the two words occur together, or contains theirlog?likelihood score of associativity.
This matrix islarge and sparse, since most words do not co?occurwith each other.
We may optionally apply SVD tothis co-occurrence matrix to reduce its dimension-ality.
Each row of this matrix is a vector that repre-sents the given word at the row via its co?occurrencecharacteristics.
We create a second order represen-tation of a context by replacing each word in thatcontext with its associated vector, and then averag-ing together all these word vectors.
This results in asingle vector that represents the overall context.For contexts with target words we can restrict thenumber of words around the target word that are av-eraged for the creation of the context vector.
In ourname discrimination experiments we limit this scopeto five words on either side of the target word whichis based on the theory that words nearer to the tar-get word are more related to it than the ones that arefarther away.The goal of the second order context represen-tation is to capture indirect relationships betweenwords.
For example, if the word Dictionary occurswith Words but not with Meanings, and Words oc-curs with Meanings, then the words Dictionary andMeanings are second order co-occurrences via thefirst order co-occurrence of Words.In either the first or second order case, once wehave each context represented as a vector we pro-ceed with clustering.
We employ the hybrid clus-tering method known as Repeated Bisections, whichoffers nearly the quality of agglomerative clusteringat the speed of partitional clustering.3 Labeling MethodologyFor each discovered cluster, we create a descriptiveand a discriminating label, each of which is madeup of some number of bigram features.
These areidentified by treating the contexts in each cluster asa separate corpora, and applying our bigram featureselection methods as described previously on eachof them.Descriptive labels are the top N bigrams accord-ing to the log?likelihood ratio.
Our goal is that theselabels will provide clues as to the general nature ofthe contents of a cluster.
The discriminating labelsare any descriptive labels for a cluster that are notdescriptive labels of another cluster.
Thus, the dis-criminating label may capture the content that sep-arates one cluster from another and provide a more106Table 1: Name Discrimination (F-measure)MAJ. O1 O22-Way Name(M);+ (N) k=2 k=2AAIRLINES(1075); 50.0 66.6 58.8TCRUISE(1075) (2150)AAIRLINES(3966); 51.7 61.7 59.6HPACKARD(3690) (7656)BGATES(1981); 64.8 63.4 53.8TCRUISE(1075) (3056)BSPEARS(1380); 50.0 56.6 65.8GBUSH(1380) (2760)3-Way Name (M);+ k=3 k=3AAIRLINES(2500); 33.3 41.4 45.1HPACKARD(2500); (7500)BMW(2500);AAIRLINES(1300); 33.3 46.0 45.3HPACKARD(1300); (3900)BSPEARS(1300);BGATES(1075); 33.3 53.7 53.6TCRUISE(1075); (3225)GBUSH(1075)detailed level of information.4 Experimental DataWe evaluate these methods on proper name discrim-ination and email (newsgroup) categorization.For name discrimination we use the 700 millionword New York Times portion of the English Giga-Word corpus as the source of contexts.
While thereare many ambiguous names in this data, it is difficultto evaluate the results of our approach given the ab-sence of a disambiguated version of the text.
Thus,we automatically create ambiguous names by con-flating the occurrences associated with two or threerelatively unambiguous names into a single obfus-cated name.For example, we combine Britney Spears andGeorge Bush into an ambiguous name Britney Bush,and then see how well SenseClusters is able to cre-ate clusters that reflect the true underlying identityof the conflated name.Our email experiments are based on the 20-NewsGroup Corpus of USENET articles.
This isa collection of approximately 20,000 articles thatTable 2: Email Categorization (F-measure)MAJ. O1 O2Newsgroup(M);+ (N) k=2 k=2comp.graphics(389); 50.1 61.1 63.9misc.forsale(390) (779)comp.graphics(389); 50.8 73.6 54.8talk.pol.mideast(376) (756)rec.motorcycles(398); 50.13 83.1 60.5sci.crypt(396) (794)rec.sport.hockey(399); 50.1 77.6 58.5soc.relig.christian(398) (797)sci.electronics(393); 50.3 67.8 52.3soc.relig.christian(398) (791)have been taken from 20 different newsgroups.
Assuch they are already classified, but since our meth-ods are unsupervised we ignore this information un-til it is time to evaluate our approach.
We presentresults that make two way distinctions between se-lected pairs of newsgroups.5 Experimental Results and DiscussionTable 1 presents the experimental results for 2-wayand 3-way name discrimination experiments, andTable 2 presents results for a 2-way email cate-gorization experiment.
The results are reported interms of the F-measure, which is the harmonic meanof precision and recall.The first column in both tables indicates the possi-ble names or newgroups, and the number of contextsassociated with each.
The next column indicates thepercentage of the majority class (MAJ.) and count(N) of the total number of contexts for the namesor newsgroups.
The majority percentage provides asimple baseline for level of performance, as this isthe F?measure that would be achieved if every con-text were simply placed in a single cluster.
We referto this as the unsupervised majority classifier.The next two columns show the F?measure asso-ciated with the order 1 and order 2 representationsof context, with all other options being held con-stant.
These experiments used bigram features, SVDwas performed as appropriate for each representa-tion, and the method of Repeated Bisections wasused for clustering.107Table 3: Cluster Labels (for Table 1)True Name Created LabelsCLUSTER 0: Flight 11, Flight 587, Sept 11,AMERICAN Trade Center, World Trade,AIRLINES Los Angeles, New YorkCLUSTER 1: Jerry Maguire,TOM Mission Impossible,CRUISE Minority Report, Tom Cruise,Penelope Cruz, Nicole Kidman,United Airlines, Vanilla Sky,Los Angeles, New YorkCLUSTER 0: George Bush , George W,GEORGE Persian Gulf, President, U S,BUSH W Bush, former President,lifting feeling, White HouseCLUSTER 1: Chairman , Microsoft ,BILL Microsoft Chairman,GATES co founder, News Service,operating system,chief executive, White HouseCLUSTER 2: Jerry Maguire,TOM Mission Impossible,CRUISE Minority Report, Al Gore,New York , Nicole Kidman,Penelope Cruz, Vanilla Sky,Ronald Reagan, White HouseFinally, note that the number of clusters to be dis-covered must be provided by the user.
In these ex-periments we have taken the best case approach andasked for a number of clusters equal to that whichactually exists.
We are currently working to developmethods that will automatically stop at an optimalnumber of clusters, to avoid setting this value man-ually.In general all of our results significantly improveupon the majority classifier, which suggests that theclustering of contexts is successfully discriminatingamong ambiguous names and uncategorized email.Table 3 shows the descriptive and discriminatinglabels assigned to the 2?way experimental case ofAmerican Airlines and Tom Cruise, as well as the3?way case of George Bush, Bill Gates and TomCruise.
The bold face labels are those that serveas both descriptive and discriminating labels.
Thefact that most labels serve both roles suggests thatthe highest ranked bigrams in each cluster were alsounique to that cluster.
The normal font indicateslabels that are only descriptive, and are shared be-tween multiple clusters.
There are only a few suchcases, for example White House happens to be a sig-nificant bigram in all three of the clusters in the 3?way case.
There were no labels that were exclu-sively discriminating in these experiments, suggest-ing that the clusters are fairly clearly distinguished.Please note that some labels include unigrams(e.g., President for George Bush).
These are createdfrom bigrams where the other word is the conflatedform, which is not included in the labels since it isby definition ambiguous.6 AcknowledgementsThis research is partially supported by a NationalScience Foundation Faculty Early CAREER Devel-opment Award (#0092784).ReferencesG.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsenses in untagged text.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natural Lan-guage Processing, pages 197?207, Providence, RI,August.T.
Pedersen, A. Purandare, and A. Kulkarni.
2005.
Namediscrimination by clustering similar contexts.
In Pro-ceedings of the Sixth International Conference on In-telligent Text Processing and Computational Linguis-tics, pages 220?231, Mexico City, February.A.
Purandare and T. Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
In Proceedings of the Conferenceon Computational Natural Language Learning, pages41?48, Boston, MA.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.108
