A Case Study on Inter-Annotator Agreement forWord Sense DisambiguationHwee Tou  NgChung Yong L imShou King FooDSO Natmna l  Laborator ies20 Scmnce Park  Dr ive,  S ingapore  118230{nhweetou, Ichungyo, fshoukln}@dso org.
sgAbst ractThere is a general concern within the field of wordsense dusamb~guatmn about the rater-annotatoragreement between human annotators.
In thus pa-per, we examine th~s msue by comparing the agree-ment rate on a large corpus of more than 30,000sense-tagged instances Thin corpus us the mtersec-tmn of the WORDNET Semcor corpus and the DSOcorpus, which has been independently tagged by twoseparate groups of human annotators The contri-bution of this paper us two-fold First, ~t presents agreedy search algorithm that can automatically de-rive coarser sense classes based on the sense tagsassigned by two human annotators The resultingderived coarse sense classes achmve a h~gher agree-ment rate but we st f l !mamtam as many of the orig-inal sense classes as posmble Second, the coarsesense grouping derived by the algorithm, upon veri-fication by human, can potentially serve as a bettersense inventory for evaluating automated word sensed~samb~guatmn algorithms Moreover, we examinedthe derived coarse sense classes and found some in-teresting roupings of word senses that correspondto human mtmtlve judgment of sense granularity1 In t roduct ion  .It us widely acknowledged that word sense d~sam-blguatmn (WSD) us a central problem m naturallanguage processing In order for computers to beable to understand and process natural anguage be-yond simple keyword matching, the problem of d~s-amblguatmg word sense, or dlscermng the meamngof a word m context, must be effectively dealt withAdvances in WSD v, ill have slgmficant Impact onapphcatlons hke information retrieval and machinetranslationFor natural anguage subtasks hke part-of-speechtagging or s)ntactm parsing, there are relatlvely welldefined and agreed-upon cnterm of what it means tohave the "correct" part of speech or syntactic struc-ture assigned to a word or sentence For instance,the Penn Treebank corpus (Marcus et a l ,  1993) pro-~ide~ ,t large repo.~tory of texts annotated w~th part-of-speech and s}ntactm structure mformatlon Tv.oindependent human annotators can achieve a highrate of agreement on assigning part-of-speech tagsto words m a g~ven sentenceUnfortunately, th~s us not the case for word senseassignment F~rstly, it is rarely the case that any twodictionaries will have the same set of sense defim-tmns for a g~ven word Different d~ctlonanes tend tocarve up the "semantic space" m a different way, soto speak Secondly, the hst of senses for a word m atypical dmtmnar~ tend to be rather efined and com-prehensive This is especmlly so for the commonlyused words which have a large number of senses Thesense dustmctmn between the different senses for acommonly used word m a d~ctmnary hke WoRDNET(Miller, 1990) tend to be rather fine Hence, twohuman annotators may genuinely dusagree m theirsense assignment to a word m contextThe agreement rate between human annotators onword sense assignment us an Important concern forthe evaluatmn of WSD algorithms One would pre-fer to define a dusamblguatlon task for which thereus reasonably hlgh agreement between human an-notators The agreement rate between human an-notators will then form the upper ceiling againstwhmh to compare the performance of WSD algo-rithms For instance, the SENSEVAL exerclse hasperformed a detaded study to find out the rater-annotator agreement among ~ts lexicographers tag-grog the word senses (Kllgamff, 1998c, Kllgarnff,1998a, Kflgarrlff, 1998b)2 A Case  StudyIn this-paper,  we examine the ~ssue of rater-annotator agreement by comparing the agreementrate of human annotators on a large sense-taggedcorpus of more than 30,000 instances of the most fre-quently occurring nouns and verbs of Enghsh Thiscorpus is the intersection of the WORDNET Semcorcorpus (Miller et a l ,  1993) and the DSO corpus (Ngand Lee, 1996, Ng, 1997), which has been indepen-dently tagged wlth the refined senses of WORDNETby two separate groups of human annotatorsThe Semcor corpus us a subset of the Brown corpustagged with ~VoRDNET senses, and consists of morethan 670,000 words from 352 text files Sense tag-gmg was done on the content words (nouns, ~erbs,adjectives and adverbs) m this subsetThe DSO corpus consists of sentences drawn fromthe Brown corpus and the Wall Street Journal Foreach word w from a hst of 191 frequently occur-ring words of Enghsh (121 nouns and 70 verbs), sen-tences containing w (m singular or plural form, andm its various reflectional verb form) are selected andeach word occurrence w ~s tagged w~th a sense fromWoRDNET There ~s a total of about 192,800 sen-tences in the DSO corpus m which one word occur-rence has been sense-tagged m each sentenceThe intersection of the Semcor corpus and theDSO corpus thus consists of Brown corpus sentencesm which a word occurrence w is sense-tagged m eachsentence, where w Is one of.the 191 frequently oc-,currmg English nouns or verbs Since this commonpomon has been sense-tagged by two independentgroups of human annotators, ~tserves as our data setfor investigating inter-annotator agreement in thispaper3 Sentence MatchingTo determine the extent of inter-annotator agree-ment, the first step ~s to match each sentence mSemcor to its corresponding counterpart In the DSOcorpus This step ~s comphcated by the followingfactors1 Although the intersected portion of both cor-pora came from Brown corpus, they adopteddifferent okemzatmn convention, and segmen-tartan into sentences differed sometimes2 The latest versmn of Semcor makes use of thesenses from WORDNET 1 6, whereas the sensesused m the DSO corpus were from WoRDNET15 1To match the sentences, we first converted thesenses m the DSO corpus to those of WORDNET1 6 We ignored all sentences m the DSO corpus mwhich a word is tagged with sense 0 or -1 (A word istagged with sense 0 or -1 ff none of the given sensesm WoRDNFT applies )4, sentence from Semcor is considered to matchone from the DSO corpus ff both sentences are ex-actl) ldent~cal or ff the~ differ only m the pre~enceor absence of the characters " (permd) or -' (hy-phen)For each remaining Semcor sentence, taking intoaccount word ordering, ff 75% or more of the wordsm the sentence match those in a DSO corpus sen-tence, then a potential match ~s recorded Thesei -kctua\[ly, the WORD~q'ET senses used m the DSO corpuswere from a shght var iant  of the official WORDNE'I 1 5 releaseTh~s ssas brought  to our attent ion after the pubhc release ofthe DSO corpuspotential matches are then manually verffied to en-sure that they are true matches and to ~eed out anyfalse matchesUsing this method of matching, a total of13,188 sentence-palrs contasnmg nouns and 17,127sentence-pa~rs containing verbs are found to matchfrom both corpora, ymldmg 30,315 sentences whichform the intersected corpus used m our presentstudy4 The Kappa StatisticSuppose there are N sentences m our corpus whereeach sentence contains the word w Assume thatw has M senses Let 4 be the number of sentenceswhich are assigned identical sense b~ two human an-notators Then a simple measure to quantify theagreement rate between two human annotators IPc, where Pc, = A /NThe drawback of this simple measure is that itdoes not take into account chance agreement be-tween two annotators The Kappa statistic a (Co-hen, 1960) is a better measure of rater-annotatoragreement which takes into account he effect ofchance agreement It has been used recentlyw~thm computatmnal hngu~stlcs to measure rater-annotator agreement (Bruce and Wmbe, 1998, Car-letta, 1996, Veroms, 1998)Let Cj be the sum of the number of sentenceswhich have been assigned sense 3 by annotator 1 andthe number of sentences whmh have been assignedsense 3 by annotator 2 ThenP~-P~1-P~whereMj= land Pe measures the chance agreement between twoannotators A Kappa ~alue of 0 indicates thatthe agreement is purely due to chance agreement,whereas a Kappa ~alue of 1 indicates perfect agree-ment A Kappa ~alue of 0 8 and above is consideredas mdmatmg ood agreement (Carletta, 1996)Table 1 summarizes the inter-annotator agree-ment on the mtersected corpus The first (becond)row denotes agreement on the nouns (xerbs), wh~lethe lass row denotes agreement on all words com-bined The a~erage ~reported m the table is a s~m-pie average of the individual ~ value of each wordThe agreement rate on the 30,315 sentences asmeasured by P= is 57% This tallies with the fig-ure reported ~n our earlier paper (Ng and Lee, 1996)where we performed a quick test on a subset of 5,317sentences ,n the intersection ofboth the Semcor cor-pus and the DSO corpus10\[\]mmmmmmmmmmmmmmmmmmType Num of v, ords A N \[ P~ AvgNouns 121 7,676 13,188 I 0 582 0 300Verbs 70 9,520 17,127 I 0 555 0 347All I 191 I 17,196 30,315 I 056T 0317Table 1 Raw inter-annotator agreement5 A lgor i thmSince the rater-annotator agreement on the inter-sected corpus is not high, we would like to find outhow the agreement rate would be affected if differentsense classes were in useIn this section, we present a greedy search algo-rithm that can automatmalb derive coarser senseclasses based on the sense tags assigned by two hu-man annotators The resulting derived coarse senseclasses achmve a higher agreement rate but we stillmaintain as many of the original sense classes aspossible The algorithm is given m Figure 1The algorithm operates on a set of sentences whereeach sentence contains an occurrence of the word wwhmh has been sense-tagged by two human anno-tators - At each Iteration of the algorithm, tt findsthe pair of sense classes Ct and Cj such that merg-ing these two sense classes results in the highest t~value for the resulting merged group of sense classesIt then proceeds to merge Cz and C~ Thin processIs repeated until the ~ value reaches a satisfactoryvalue ~,~t,~, which we set as 0 8Note that this algorithm is also applicable to de-riving any coarser set of classes from a refined setfor any NLP tasks in which prior human agreementrate may not be high enough Such NLP tasks couldbe discourse tagging, speech-act categorization, etc6 Resu l tsFor each word w from the list of 121 nouns and70 verbs, ~e applied the greedy search algorithm toeach set of sentences in the intersected corpus con-taming w For a subset of 95 words (53 nouns and 42verbs), the algorithm was able to derive a coarser setof 2 or more senses for each of these 95 words suchthat the resulting Kappa ~alue reaches 0 8 or higherFor the other 96 words, m order for the Kappa valueto reach 0 8 or higher, the algorithm collapses allsenses of the ~ord to a single (trivial) class Table 2and 3 summarizes the results for the set of 53 nounsand 42 ~erbs, respectivelyTable 2 md~cates that before the collapse of senseclasses, these 53 nouns have an average of 7 6 sensesper noun There is a total of 5,339 sentences in theintersected corpus containing these nouns, of which3,387 sentences were assigned the same sense bythe two groups of human annotators The averageKappa statistic (computed as a simple average of theKappa statistic of ~he mdlwdual nouns) is 0 463After the collapse of sense classes by the greedysearch algorithm, the average number of senses pernoun for these 53 nouns drops to 40 Howe~er,the number of sentences which have been asmgnedthe same coarse sense by the annotators increases to5,033 That is, about 94 3% of the sentences havebeen assigned the same coarse sense, and that theaverage Kappa statistic has improved to 0 862, mgm-fymg high rater-annotator agreement on the derivedcoarse senses Table3 gl~es the analogous figures forthe 42 verbs, agmn mdmatmg that high agreementis achieved on the coarse sense classes den~ed forverbs7 D iscuss ionOur findings on rater-annotator agreement for wordsense tagging indicate that for average languageusers, it is quite dl~cult  to achieve high agreementwhen they are asked to assign refned sense tags(such as those found in WORDNET)  given only thescanty definition entries m the WORDNET dlctio-nary and a few or no example sentences for theusage of each word sense Thin observation agreeswlth that obtmned m a recent study done by (Vero-ms, 1998), where the agreement on sense-tagging bynaive users was also not hlgh Thus It appears thatan average language user is able to process languagewlthout needing to perform the task of dlsamblguat-mg word sense to a very fine-grained resolutmn asformulated m a tradltlonal dmtlonaryIn contrast, expert lexicographers tagged the ~ ordsense in the sentences used m the SENSEVAL  exer-clse, where high rater-annotator agreement was re-ported There are also fuller dlctlonary entries mthe HECTOR dlctlonary used and more e<amplesshowing the usage of each word sense m HECTORThese factors are likely to have contributed to thedifference in rater-annotator agreement observed mthe three studies conductedWe also examined the coarse sense classes derivedby the greedy search algorithm Vv'e found some in-teresting roupings of coarse senses for nouns which~e hst in Table 4From Table 4, it is apparent that the greedy searchalgorithm can derive interesting roupings of wordsenses that correspond to human mtmtwe judgmentof sense graz}.ulanty It Is clear that some of the dis-agreement between the two groups of human anno-tators can be attributed solely to the overly refinedsenses of WoRDNET As an example, there is a totalI iloop:  let Ct, , C M denote the current M sense classes~* +-- -oofor all z,3 such that 1 <,  < 3 < Mlet C\[, ,C~w_ 1 denote the resulting M - 1 sense classes by mergmg C, and C 3compute ~(C\[, , C~/_t)ff ~(C{, , C~4_x) > ~* then~" +- ~(C~, ,C~_t), z* +- ~, ~* +-end formerge the sense class C,.
and C~.M+--M-1If E* < ~rn,n gore loopTable 2F~gure 1 ~ greedy search algorithmType Avg Num of senses A N P~ ~'~g ~ IBefore 76 3,387 5,339 0 634 0463 \]After 40 5,033 5,339 0 943 0862 jInter-annotator agreement for 53 nouns before and after the collapse of sensesSense 1 change, alteratmn, modfficatlon - (an eventthat occurs when something passes from one stateor phase to another "the change was intended toincrease sales", thLs storm ~s certaanly a change forthe worse")Sense 2 change - (a relaUonal &fference betweenstate.% esp between states before and after someevent "he attr ibuted the change to their marnage")Sense 3 change - (the act of changing something,' the change of government had no ~mpact on theeconomy", "hLs change on abortmn cost h~m the elec-tion" )Sense 4 change - (the result of alterauon or modlfi-carton, there were marked changes m the hmng ofthe lungs", "there had been no change m the moun-tains" )Sense 5 change - (the balance of money recmved~hen the amount you tender is greater than theamount due, ' I  paid with a twenty and pocketedthe change")Sense 6 change - (a thing that Is different, 'he re-spected several changes before selecting one")Sense 8 change - (corns of small denommatlon re-garded collecuvel~, 'he had a pocketful of change")Figure 2 Seven senses of the noun "change" usedb~ the human annotatorsof III sentences m the intersected corpus containingthe noun with root word form 'change" They areassigned one of the seven senses hsted in F~gure 2 bythe two groups of human annotatorsBased on the imtml word senses assigned, Pa =0 38 and ~ = -0 09 (~ is negative when there Is sys-~ematlc dlsagreement ) Hovve~er, the greed?
searchalgorithm collapses sense I, 2, 3, 4 and 6 into onecoarse sense and sense 5 and 8 into another coarsesense As a result, Pa = ~ = 1, mdmaung perfectagreement when the senses are collapsed m the man-ner found This corresponds to our mtum~e judg-ment of the relauve closeness of the various senseshereSimilarly, some of the 96 words for whmh thegreedy search algorithm collapses into one singlesense are such that the various enses are too close tobe rehably dmtmgmshed In short, we believe thatthe coarse sense classes derived by the greedy searchalgorithm, upon verlficauon by human, can poten-ually serve as a better sense inventory for evaluatingautomated word sense d~samb~guatlon algorithms8 Related WorkRecently, both Bruce and Wmbe (1998) and Veroms(1998) have looked into algorithms to automancallvgenerate better sense classes m a corpus-based, ata-driven manner However, the algorithms they useddiffer from ours Bruce and Wlebe (1998) made useof an EM algorithm ~la a latent class model to de-nse better sense classes Veroms (1998) performeda Muluple Correspondence Analyms on the table ofannotatmns (a triple composed of a context, a judgeand a sense) to reduce dlmensmnaht?
follo~ed b?tree-clustering In contrast, our greedb search al-gorithm ~s a rumple but effecuve method that makesuse of the Kappa statlsUC to search the space of pos-sible sense groupings dlrecdy9 Conc lus ionIn th~s paper, we examined the tssue of rater-annotator agreement on word sense tagging and pre-sented a greedy search algorithm capable of gener-ating coarse sense classes based on the sense tags12Table 3Type Avg Num of senses A N P. Avg nBefore 12 8 5,115 8,602 0 595 0 441After 5 6 8,042 8,602 0 935 0.852Inter-annotator agreement for 42 verbs before and after the collapse of sensesNounalrboardbodychangeCoarse senseswind/gas vs aura/atmospherecommittee vs plankphysmal/natural object vs group/coIlectLonmodfficatmn vs cornscountry natron vs region/countrysidecourse class vs action vs dlrectmnfield land vs subjectfoot human body part ~s umt vs lower part/supportforce strength vs personnelhghtmatterpartyTable 4lumlnatlon vs perspectlveconcern/~ssue vs substancepohtlcal party vs socml gathering vs groupCoarse senses derived by the greedy search algorlthmassigned by two human annotators We found inter-esting groupings of word senses that correspond tohuman lntumve judgment of sense granularityReferencesRebecca Bruce and Janyce Wmbe 1998 Word-sense dlstmgmshabdlty and inter-coder agree-ment In Proceedings of the Thtrd Conference onEmpzmcal Methods m Natural Language Process-mgJean Carletta 1996 Assessing agreement on clas-sfficat~on tasks The kappa statlstm Computa-tzonal Lmgutstzcs, 22(2) 249-254J Cohen 1960 A coeffictent of agreement for nom-inal scales Educatwnal and Psychological Mea-surement, 20 37-46~.dam Kllgarnff 1998a Gold standard atasets forevaluating word sense d~sambiguatlon programsComputer Speech and LanguageAdam Kflgamff 1998b Inter-tagger agreement InAdvanced Papers of the SENSEVAL WorkshopAdam Kllgarrtff 1998c SENSEVAL An exercise mevaluating word sense d~samblguatlon programsIn Proceedings o/LRECMitchell P Marcus, Beatrice Santorml, andMary Ann Marcmkmwlcz 1993 Building a largeannotated corpus of enghsh The Penn TreebankComputatzonal Lmgutst~cs, 19(2) 313-330George A MlUer, Claudia Lea?ock, Randee Tengl,and Ross T Bunker 1993 A semantm concor-dance In Proceedings o/the ARPA Human Lan-guage Technology Workshop, ages 303-308George A Mllter 1990 Wordnet An on-hne lexlcaldatabase International Journal o/Lexicography,3(4) 235-312Hwee Tou Ng and Hlan Beng Lee 1996 Integratingmultiple knowledge sources to dlsamblguate wordsense An exemplar-based approach In Proceed-rags of the 34th Annual Meeting of the Assocza-twn for Computatzonal Lmguzstscs (ACL), pages40--47Hwee Tou Ng 1997 Exemplar-based word sensedlsambiguatlon Some recent ~mprovements InProceedings o/ the Second Conference on Em-pw~cal Methods m Natural Language Processing,pages 208-213Jean Veroms 1998 ~ study of polysemy judge-meats and rater-annotator agreement In Ad-vanced Papers o/the SENSEVAL Workshop13
