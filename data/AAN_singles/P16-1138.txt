Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1456?1465,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSimpler Context-Dependent Logical Forms via Model ProjectionsReginald LongStanford Universityreggylong@cs.stanford.eduPanupong PasupatStanford Universityppasupat@cs.stanford.eduPercy LiangStanford Universitypliang@cs.stanford.eduAbstractWe consider the task of learning a context-dependent mapping from utterances to de-notations.
With only denotations at train-ing time, we must search over a combina-torially large space of logical forms, whichis even larger with context-dependent ut-terances.
To cope with this challenge, weperform successive projections of the fullmodel onto simpler models that operateover equivalence classes of logical forms.Though less expressive, we find that thesesimpler models are much faster and canbe surprisingly effective.
Moreover, theycan be used to bootstrap the full model.Finally, we collected three new context-dependent semantic parsing datasets, anddevelop a new left-to-right parser.1 IntroductionSuppose we are only told that a piece of text (acommand) in some context (state of the world) hassome denotation (the effect of the command)?seeFigure 1 for an example.
How can we build a sys-tem to learn from examples like these with no ini-tial knowledge about what any of the words mean?We start with the classic paradigm of trainingsemantic parsers that map utterances to logicalforms, which are executed to produce the deno-tation (Zelle and Mooney, 1996; Zettlemoyer andCollins, 2005; Wong and Mooney, 2007; Zettle-moyer and Collins, 2009; Kwiatkowski et al,2010).
More recent work learns directly from de-notations (Clarke et al, 2010; Liang, 2013; Be-rant et al, 2013; Artzi and Zettlemoyer, 2013),but in this setting, a constant struggle is to con-tain the exponential explosion of possible logicalforms.
With no initial lexicon and longer context-dependent texts, our situation is exacerbated.Context:Text: Pour the last green beaker intobeaker 2.
Then into the first beaker.Mix it.Denotation:Figure 1: Our task is to learn to map a piece oftext in some context to a denotation.
An exam-ple from the ALCHEMY dataset is shown.
In thispaper, we ask: what intermediate logical form issuitable for modeling this mapping?In this paper, we propose projecting a full se-mantic parsing model onto simpler models overequivalence classes of logical form derivations.
Asillustrated in Figure 2, we consider the followingsequence of models:?
Model A: our full model that derives logi-cal forms (e.g., in Figure 1, the last utter-ance maps to mix(args[1][1])) compo-sitionally from the text so that spans of the ut-terance (e.g., ?it?)
align to parts of the logicalform (e.g., args[1][1], which retrieves anargument from a previous logical form).
Thisis based on standard semantic parsing (e.g.,Zettlemoyer and Collins (2005)).?
Model B: collapse all derivations with thesame logical form; we map utterances to fulllogical forms, but without an alignment be-tween the utterance and logical forms.
This?floating?
approach was used in Pasupat andLiang (2015) and Wang et al (2015).?
Model C: further collapse all logical formswhose top-level arguments have the same de-notation.
In other words, we map utterances1456Mix         itModel Amix args[1][1]mix(args[1][1])Model Bmix(args[1][1])Mix it Model CMix         itmixargs[1][1]mix(args[1][1])Mix   itmix pos(2)mix(pos(2))Mix     itmixpos(2)mix(pos(2))mix(pos(2))Mix itmix(beaker2)Mix itFigure 2: Derivations generated for the last ut-terance in Figure 1.
All derivations above ex-ecute to mix(beaker2).
Model A generatesanchored logical forms (derivations) where wordsare aligned to predicates, which leads to multiplederivations with the same logical form.
Model Bdiscards these alignments, and Model C collapsesthe arguments of the logical forms to denotations.to flat logical forms (e.g., mix(beaker2)),where the arguments of the top-level predi-cate are objects in the world.
This model isin the spirit of Yao et al (2014) and Bordeset al (2014), who directly predicted concretepaths in a knowledge graph for question an-swering.Model A excels at credit assignment: the latentderivation explains how parts of the logical formare triggered by parts of the utterance.
The priceis an unmanageably large search space, given thatwe do not have a seed lexicon.
At the other end,Model C only considers a small set of logicalforms, but the mapping from text to the correctlogical form is more complex and harder to model.We collected three new context-dependent se-mantic parsing datasets using Amazon MechanicalTurk: ALCHEMY (Figure 1), SCENE (Figure 3),and TANGRAMS (Figure 4).
Along the way, wedevelop a new parser which processes utterancesleft-to-right but can construct logical forms with-out an explicit alignment.Our empirical findings are as follows: First,Model C is surprisingly effective, mostly surpass-ing the other two given bounded computational re-sources (a fixed beam size).
Second, on a syntheticdataset, with infinite beam, Model A outperformsthe other two models.
Third, we can bootstrap upto Model A from the projected models with finitebeam.Context:Text: A man in a red shirt and orange hatleaves to the right, leaving behind aman in a blue shirt in the middle.He takes a step to the left.Denotation:Figure 3: SCENE dataset: Each person has a shirtof some color and a hat of some color.
They enter,leave, move around on a stage, and trade hats.Context:Text: Delete the second figure.
Bring itback as the first figure.Denotation:Figure 4: TANGRAMS dataset: One can add fig-ures, remove figures, and swap the position of fig-ures.
All the figures slide to the left.2 TaskIn this section, we formalize the task and describethe new datasets we created for the task.2.1 SetupFirst, we will define the context-dependent seman-tic parsing task.
Define w0as the initial worldstate, which consists of a set of entities (beakersin ALCHEMY) and properties (location, color(s),and amount filled).
The text x is a sequenceof utterances x1, .
.
.
, xL.
For each utterance xi(e.g., ?mix?
), we have a latent logical form zi(e.g., mix(args[1][2])).
Define the contextci= (w0, z1:i?1) to include the initial world statew0and the history of past logical forms z1:i?1.Each logical form ziis executed on the contextcito produce the next state: wi= Exec(ci, zi)for each i = 1, .
.
.
, L. Overloading notation, wewrite wL= Exec(w0, z), where z = (z1, .
.
.
, zL).The learning problem is: given a set of trainingexamples {(w0,x, wL)}, learn a mapping from thetext x to logical forms z = (z1, .
.
.
, zL) that pro-duces the correct final state (wL= Exec(w0, z)).1457Dataset # examples # train # test words/example utterancesSCENE 4402 3363 1039 56.2 ?then one more?, ?he moves back?ALCHEMY 4560 3661 899 39.9 ?mix?, ?throw the rest out?TANGRAMS 4989 4189 800 27.2 ?undo?, ?replace it?, ?take it away?Table 1: We collected three datasets.
The number of examples, train/test split, number of tokens perexample, along with interesting phenomena are shown for each dataset.2.2 DatasetsWe created three new context-dependent datasets,ALCHEMY, SCENE, and TANGRAMS (see Table 1for a summary), which aim to capture a diverse setof context-dependent linguistic phenomena suchas ellipsis (e.g., ?mix?
in ALCHEMY), anaphoraon entities (e.g., ?he?
in SCENE), and anaphoraon actions (e.g., ?repeat step 3?, ?bring it back?in TANGRAMS).For each dataset, we have a set of propertiesand actions.
In ALCHEMY, properties are color,and amount; actions are pour, drain, andmix.
In SCENE, properties are hat-colorand shirt-color; actions are enter, leave,move, and trade-hats.
In TANGRAMS, thereis one property (shape), and actions are add,remove, and swap.
In addition, we include theposition property (pos) in each dataset.
Each ex-ample has L = 5 utterances, each denoting sometransformation of the world state.Our datasets are unique in that they aregrounded to a world state and have rich linguis-tic context-dependence.
In the context-dependentATIS dataset (Dahl et al, 1994) used by Zettle-moyer and Collins (2009), logical forms of utter-ances depend on previous logical forms, thoughthere is no world state and the linguistic phenom-ena is limited to nominal references.
In the mapnavigation dataset (Chen and Mooney, 2011), usedby Artzi and Zettlemoyer (2013), utterances onlyreference the current world state.
Vlachos andClark (2014) released a corpus of annotated di-alogues, which has interesting linguistic context-dependence, but there is no world state.Data collection.
Our strategy was to automati-cally generate sequences of world states and askAmazon Mechanical Turk (AMT) workers to de-scribe the successive transformations.
Specifi-cally, we started with a random world state w0.For each i = 1, .
.
.
, L, we sample a validaction and argument (e.g., pour(beaker1,beaker2)).
To encourage context-dependentdescriptions, we upweight recently used ac-tions and arguments (e.g., the next action ismore like to be drain(beaker2) rather thandrain(beaker5)).
Next, we presented anAMT worker with statesw0, .
.
.
, wLand asked theworker to write a description in between each pairof successive states.In initial experiments, we found it rather non-trivial to obtain interesting linguistic context-dependence in these micro-domains: often acontext-independent utterance such as ?beaker 2?is just clearer and not much longer than a possi-bly ambiguous ?it?.
We modified the domains toencourage more context.
For example, in SCENE,we removed any visual indication of absolute posi-tion and allowed people to only move next to otherpeople.
This way, workers would say ?to the leftof the man in the red hat?
rather than ?to position2?.3 ModelWe now describe Model A, our full context-dependent semantic parsing model.
First, let Zdenote the set of candidate logical forms (e.g.,pour(color(green),color(red))).Each logical form consists of a top-level actionwith arguments, which are either primitive values(green, 3, etc.
), or composed via selection andsuperlative operations.
See Table 2 for a fulldescription.
One notable feature of the logicalforms is the context dependency: for example,given some context (w0, z1:4), the predicateactions[2] refers to the action of z2andargs[2][1] refers to first argument of z2.1We use the term anchored logical forms (a.k.a.derivations) to refer to logical forms augmentedwith alignments between sub-logical forms of ziand spans of the utterance xi.
In the exampleabove, color(green) might align with ?greenbeaker?
from Figure 1; see Figure 2 for anotherexample.1These special predicates play the role of references inZettlemoyer and Collins (2009).
They perform context-independent parsing and resolve references, whereas we re-solve them jointly while parsing.1458Property[p] Value[v] ?
Set[p(v)] all entities whose property p is vSet[s] Property[p] ?
Value[argmin/argmax(s, p)] element in s with smallest/largest pSet[s] Int[i] ?
Value[s[i]] i-th element of sAction[a] Value[v1] Value[v2] ?
Root[a(v1, v2)] top-level action applied to arguments v1, v2Table 2: Grammar that defines the space of candidate logical forms.
Values include numbers, colors,as well as special tokens args[i][j] (for all i ?
{1, .
.
.
, L} and j ?
{1, 2}) that refer to the j-th ar-gument used in the i-th logical form.
Actions include the fixed domain-specific set plus special tokensactions[i] (for all i ?
{1, .
.
.
, L}), which refers to the i-th action in the context.Derivation condition Example(F1) zicontains predicate r (zicontains predicate pour, ?pour?
)(F2) property p of zi.bjis y (color of arg 1 is green, ?green?
)(F3) action zi.a is a and property p of zi.yjis y (action is pour and pos of arg 2 is 2, ?pour, 2?
)(F4) properties p of zi.v1is y and p?of zi.v2is y?
(color of arg 1 is green and pos of arg 2 is 2, ?first green, 2?
)(F5) arg zi.vjis one of zi?1?s args (arg reused, ?it?
)(F6) action zi.a = zi?1.a (action reused, ?pour?
)(F7) properties p of zi.yjis y and p?of zi?1.ykis y?
(pos of arg 1 is 2 and pos of prev.
arg 2 is 2, ?then?
)(F8) t1< s2spans don?t overlapTable 3: Features ?
(xi, ci, zi) for Model A: The left hand side describes conditions under which thesystem fires indicator features, and right hand side shows sample features for each condition.
For eachderivation condition (F1)?
(F7), we conjoin the condition with the span of the utterance that the referencedactions and arguments align to.
For condition (F8), we just fire the indicator by itself.Log-linear model.
We place a conditional dis-tribution over anchored logical forms zi?Z given an utterance xiand context ci=(w0, z1:i?1), which consists of the initial worldstate w0and the history of past logical formsz1:i?1.
We use a standard log-linear model:p?
(zi| xi, ci) ?
exp(?
(xi, ci, zi) ?
?
), (1)where ?
is the feature mapping and ?
is the param-eter vector (to be learned).
Chaining these distri-butions together, we get a distribution over a se-quence of logical forms z = (z1, .
.
.
, zL) giventhe whole text x:p?
(z | x, w0) =L?i=1p?
(zi| xi, (w0, z1:i?1)).
(2)Features.
Our feature mapping ?
consists of twotypes of indicators:1.
For each derivation, we fire features based onthe structure of the logical form/spans.2.
For each span s (e.g., ?green beaker?
)aligned to a sub-logical form z (e.g.,color(green)), we fire features on uni-grams, bigrams, and trigrams inside s con-joined with various conditions of z.The exact features given in Table 3, references thefirst two utterances of Figure 1 and the associatedlogical forms below:x1= ?Pour the last green beaker into beaker 2.?z1= pour(argmin(color(green),pos),pos(2))x2= ?Then into the first beaker.
?z2= actions[1](args[1][2],pos(3)).We describe the notation we use for Table 3, re-stricting our discussion to actions that have twoor fewer arguments.
Our featurization scheme,however, generalizes to an arbitrary number ofarguments.
Given a logical form zi, let zi.a beits action and (zi.b1, zi.b2) be its arguments (e.g.,color(green)).
The first and second argu-ments are anchored over spans [s1, t1] and [s2, t2],respectively.
Each argument zi.bjhas a corre-sponding value zi.vj(e.g., beaker1), obtainedby executing zi.bjon the context ci.
Finally, letj, k ?
{1, 2} be indices of the arguments.
For ex-ample, we would label the constituent parts of z1(defined above) as follows:?
z1.a = pour?
z1.b1= argmin(color(green),pos)?
z1.v1= beaker3?
z1.b2= pos(2)?
z1.v2= beaker214594 Left-to-right parsingWe describe a new parser suitable for learningfrom denotations in the context-dependent setting.Like a shift-reduce parser, we proceed left to right,but each shift operation advances an entire utter-ance rather than one word.
We then sit on theutterance for a while, performing a sequence ofbuild operations, which either combine two logi-cal forms on the stack (like the reduce operation)or generate fresh logical forms, similar to what isdone in the floating parser of Pasupat and Liang(2015).Our parser has two desirable properties: First,proceeding left-to-right allows us to build andscore logical forms zithat depend on the worldstatewi?1, which is a function of the previous log-ical forms.
Note that wi?1is a random variable inour setting, whereas it is fixed in Zettlemoyer andCollins (2009).
Second, the build operation allowsus the flexibility to handle ellipsis (e.g., ?Mix.?
)and anaphora on full logical forms (e.g., ?Do itagain.?
), where there?s not a clear alignment be-tween the words and the predicates generated.The parser transitions through a sequence of hy-potheses.
Each hypothesis is h = (i, b, ?
), where iis the index of the current utterance, where b is thenumber of predicates constructed on utterance xi,and ?
is a stack (list) of logical forms.
The stackincludes both the previous logical forms z1:i?1andfragments of logical forms built on the current ut-terance.
When processing a particular hypothesis,the parser can choose to perform either the shift orbuild operation:Shift: The parser moves to the next utteranceby incrementing the utterance index i and resettingb, which transitions a hypothesis from (i, b, ?)
to(i+ 1, 0, ?
).Build: The parser creates a new logical formby combining zero or more logical forms on thestack.
There are four types of build operations:1.
Create a predicate out of thin air (e.g.,args[1][1] in Figure 5).
This is usefulwhen the utterance does not explicitly refer-ence the arguments or action.
For example,in Figure 5, we are able to generate the log-ical form args[1][1] in the presence ofellipsis.2.
Create a predicate anchored to some span ofthe utterance (e.g., actions[1] anchoredto ?Repeat?).
This allows us to do credit as-signment and capture which part of the utter-ance explains which part of the logical form.3.
Pop z from the stack ?
and push z?onto ?,where z?is created by applying a rule in Ta-ble 2 to z.4.
Pop z, z?from the stack ?
and push z?
?onto ?,where z?
?is created by applying a rule in Ta-ble 2 to z, z?
(e.g., actions[1](args[1][1]) by the top-level root rule).The build step stops once a maximum number ofpredicates B have been constructed or when thetop-level rule is applied.We have so far described the search space overlogical forms.
In practice, we keep a beam of theK hypotheses with the highest score under the cur-rent log-linear model.5 Model ProjectionsModel A is ambitious, as it tries to learn fromscratch how each word aligns to part of the log-ical form.
For example, when Model A parses?Mix it?, one derivation will correctly align ?Mix?to mix, but others will align ?Mix?
to args[1][1], ?Mix?
to pos(2), and so on (Figure 2).As we do not assume a seed lexicon that couldmap ?Mix?
to mix, the set of anchored logicalforms is exponentially large.
For example, parsingjust the first sentence of Figure 1 would generate1,216,140 intermediate anchored logical forms.How can we reduce the search space?
The keyis that the space of logical forms is much smallerthan the space of anchored logical forms.
Eventhough both grow exponentially, dealing directlywith logical forms allows us to generate pourwithout the combinatorial choice over alignments.We thus define Model B over the space of theselogical forms.
Figure 2 shows that the two an-chored logical forms, which are treated differentlyin Model A are collapsed in Model B.
This dra-matically reduces the search space; parsing thefirst sentence of Figure 1 generates 7,047 interme-diate logical forms.We can go further and notice that manycompositional logical forms reduce to the sameflat logical form if we evaluate all the argu-ments.
For example, in Figure 2, mix(args[1][1]) and mix(pos(2)) are equivalent tomix(beaker2).
We define Model C to be the1460Delete the second figure.
Repeat.delete 2pos(2)actions[1](args[1])delete(pos(2))delete(pos(2)) actions[1](args[1][1])actions[1] args[1][1]Delete the second figure.
Repeat.delete 2pos(2)args[1][1]actions[1]delete(pos(2))delete(pos(2))actions[1] args[1][1]Delete the second figure.
Repeat.delete 2pos(2)actions[1]delete(pos(2))delete(pos(2))actions[1]Delete the second figure.
Repeat.delete 2pos(2)delete(pos(2))delete(pos(2))(1) (2) (3) (4)Figure 5: Suppose we have already constructed delete(pos(2)) for ?Delete the second figure.
?Continuing, we shift the utterance ?Repeat?.
Then, we build action[1] aligned to the word ?Repeat.
?followed by args[1][1], which is unaligned.
Finally, we combine the two logical forms.space of these flat logical forms which consist ofa top-level action plus primitive arguments.
Us-ing Model C, parsing the first sentence of Figure 1generates only 349 intermediate logical forms.Note that in the context-dependent setting, thenumber of flat logical forms (Model C) still in-creases exponentially with the number of utter-ances, but it is an overwhelming improvementover Model A.
Furthermore, unlike other formsof relaxation, we are still generating logical formsthat can express any denotation as before.
Thegains from Model B to Model C hinge on the factthat in our world, the number of denotations ismuch smaller than the number of logical forms.Projecting the features.
While we have definedthe space over logical forms for Models B and C,we still need to define a distribution over thesespaces to to complete the picture.
To do this, wepropose projecting the features of the log-linearmodel (1).
Define ?A?Bto be a map from aanchored logical form zA(e.g., mix(pos(2)) aligned to ?mix?)
to an unanchored one zB(e.g., mix(pos(2))), and define ?B?Cto bea map from zBto the flat logical form zC(e.g.,mix(beaker2)).We construct a log-linear model for Model Bby constructing features ?
(zB) (omitting the de-pendence on xi, cifor convenience) based on theModel A features ?(zA).
Specifically, ?
(zB) isthe component-wise maximum of ?
(zA) over allzAthat project down to zB; ?
(zC) is defined simi-larly:?
(zB)def= max{?
(zA) : ?A?B(zA) = zB}, (3)?
(zC)def= max{?
(zB) : ?B?C(zB) = zC}.
(4)Concretely, Model B?s features include indica-tor features over LF conditions in Table 3 con-joined with every n-gram of the entire utterance,as there is no alignment.
This is similar to themodel of Pasupat and Liang (2015).
Note thatmost of the derivation conditions (F2)?
(F7) al-ready depend on properties of the denotations ofthe arguments, so in Model C, we can directly rea-son over the space of flat logical forms zC(e.g.,mix(beaker2)) rather than explicitly comput-ing the max over more complex logical forms zB(e.g., mix(color(red))).Expressivity.
In going from Model A to ModelC, we gain in computational efficiency, but we losein modeling expressivity.
For example, for ?sec-ond green beaker?
in Figure 1, instead of predict-ing color(green)[2], we would have to pre-dict beaker3, which is not easily explained bythe words ?second green beaker?
using the simplefeatures in Table 3.At the same time, we found that simple fea-tures can actually simulate some logical forms.For example, color(green) can be explainedby the feature that looks at the color property ofbeaker3.
Nailing color(green)[2], how-ever, is not easy.
Surprisingly, Model C can usea conjunction of features to express superlatives(e.g., argmax(color(red),pos)) by usingone feature that places more mass on selecting ob-jects that are red and another feature that placesmore mass on objects that have a greater positionvalue.6 ExperimentsOur experiments aim to explore the computation-expressivity tradeoff in going from Model A toModel B to Model C. We would expect that un-der the computational constraint of a finite beamsize, Model A will be hurt the most, but with an1461Dataset Model 3-acc 3-ora 5-acc 5-oraALCHEMYB 0.189 0.258 0.037 0.055C 0.568 0.925 0.523 0.809SCENEB 0.068 0.118 0.017 0.031C 0.232 0.431 0.147 0.253TANGRAMSB 0.649 0.910 0.276 0.513C 0.567 0.899 0.272 0.698Table 4: Test set accuracy and oracle accuracy forexamples containing L = 3 and L = 5 utterances.Model C surpasses Model B in both accuracy andoracle on ALCHEMY and SCENE, whereas ModelB does better in TANGRAMS.infinite beam, Model A should perform better.We evaluate all models on accuracy, the frac-tion of examples that a model predicts correctly.A predicted logical form z is deemed to be correctfor an example (w0,x, wL) if the predicted logi-cal form z executes to the correct final world statewL.
We also measure the oracle accuracy, whichis the fraction of examples where at least one zon the beam executes to wL.
All experiments trainfor 6 iterations using AdaGrad (Duchi et al, 2010)and L1regularization with a coefficient of 0.001.6.1 Real data experimentsSetup.
We use a beam size of 500 within eachutterance, and prune to the top 5 between utter-ances.
For the first two iterations, Models B andC train on only the first utterance of each example(L = 1).
In the remaining iterations, the modelstrain on two utterance examples.
We then evaluateon examples with L = 1, .
.
.
, 5, which tests ourmodels ability to extrapolate to longer texts.Accuracy with finite beam.
We compare mod-els B and C on the three real datasets for bothL = 3 and L = 5 utterances (Model A was too ex-pensive to use).
Table 4 shows that on 5 utteranceexamples, the flatter Model C achieves an averageaccuracy of 20% higher than the more composi-tional Model B.
Similarly, the average oracle accu-racy is 39% higher.
This suggests that (i) the cor-rect logical form often falls off the beam for ModelB due to a larger search space, and (ii) the expres-sivity of Model C is sufficient in many cases.On the other hand, Model B outperforms ModelC on the TANGRAMS dataset.
This happens fortwo reasons.
The TANGRAMS dataset has thesmallest search space, since all of the utterancesrefer to objects using position only.
Addition-ally, many utterances reference logical forms thatModel C is unable to express, such as ?repeat thefirst step?, or ?add it back?.Figure 6 shows how the models perform as thenumber of utterances per example varies.
Whenthe search space is small (fewer number of ut-terances), Model B outperforms or is competitivewith Model C. However, as the search space in-creases (tighter computational constraints), ModelC does increasingly better.Overall, both models perform worse as L in-creases, since to predict the final world state wLcorrectly, a model essentially needs to predict anentire sequence of logical forms z1, .
.
.
, zL, anderrors cascade.
Furthermore, for larger L, the ut-terances tend to have richer context-dependence.6.2 Artificial data experimentsSetup.
Due to the large search space, runningmodel A on real data is impractical.
In order feasi-bly evaluate Model A, we constructed an artificialdataset.
The worlds are created using the proce-dure described in Section 2.2.
We use a simpletemplate to generate utterances (e.g., ?drain 1 fromthe 2 green beaker?
).To reduce the search space for Model A, weonly allow actions (e.g., drain) to align to verbsand property values (e.g., green) to align to ad-jectives.
Using these linguistic constraints pro-vides a slightly optimistic assessment of ModelA?s performance.We train on a dataset of 500 training examplesand evaluate on 500 test examples.
We repeat thisprocedure for varying beam sizes, from 40 to 260.The model only uses features (F1) through (F3).Accuracy under infinite beam.
Since Model Ais more expressive, we would expect it to be morepowerful when we have no computational con-straints.
Figure 7 shows that this is indeed thecase: When the beam size is greater than 250, allmodels attain an oracle of 1, and Model A out-performs Model B, which performs similarly toModel C. This is because the alignments provide apowerful signal for constructing the logical forms.Without alignments, Models B and C learn noisierfeatures, and accuracy suffers accordingly.Bootstrapping.
Model A performs the best withunconstrained computation, and Model C per-forms the best with constrained computation.
Isthere some way to bridge the two?
Even though1462(a) ALCHEMY (b) SCENE (c) TANGRAMSFigure 6: Test results on our three datasets as we vary the number of utterances.
The solid lines arethe accuracy, and the dashed line are the oracles: With finite beam, Model C significantly outperformsModel B on ALCHEMY and SCENE, but is slightly worse on TANGRAMS.Figure 7: Test results on our artificial dataset withvarying beam sizes.
The solid lines are the accura-cies, and the dashed line are the oracle accuracies.Model A is unable to learn anything with beamsize < 240.
However, for beam sizes larger than240, Model A attains 100% accuracy.
Model Cdoes better than Models A and B when the beamsize is small< 40, but otherwise performs compa-rably to Model B. Bootstrapping Model A usingModel C parameters outperforms all of the othermodels and attains 100% even with smaller beams.Model C has limited expressivity, it can still learnto associate words like ?green?
with their corre-sponding predicate green.
These should be use-ful for Model A too.To operationalize this, we first train Model Cand use the parameters to initialize model A. Thenwe train Model A.
Figure 7 shows that althoughModel A and C predict different logical forms, theinitialization allows Model C to A to perform wellin constrained beam settings.
This bootstrappingdelete the last figure.
add it back.Text:Context:Model C:Model B:Denotation:z1=remove(pos(5))z2=add(cat,1)z1=remove(pos(5))z2=add(args[1][1],pos(args[1][1]))Figure 8: Predicted logical forms for this text: Thelogical form add takes a figure and position as in-put.
Model B predicts the correct logical form.Model C does not understand that ?back?
refers toposition 5, and adds the cat figure to position 1.model beam action argument context noiseB 0.47 0.03 0.17 0.23 0.04C 0.15 0.03 0.25 0.5 0.07Table 5: Percentage of errors for Model B and C:Model B suffers predominantly from computationconstraints, while Model C suffers predominantlyfrom a lack of expressivity.works here because Model C is a projection ofModel A, and thus they share the same features.6.3 Error AnalysisWe randomly sampled 20 incorrect predictions on3 utterance examples from each of the three realdatasets for Model B and Model C. We catego-rized each prediction error into one of the follow-ing categories: (i) logical forms falling off thebeam; (ii) choosing the wrong action (e.g., map-ping ?drain?
to pour); (iii) choosing the wrong1463argument due to misunderstanding the description(e.g., mapping ?third beaker?
to pos(1)); (iv)choosing the wrong action or argument due to mis-understanding of context (see Figure 8); (v) noisein the dataset.
Table 5 shows the fraction of eacherror category.7 Related Work and DiscussionContext-dependent semantic parsing.
Utter-ances can depend on either linguistic context orworld state context.
Zettlemoyer and Collins(2009) developed a model that handles referencesto previous logical forms; Artzi and Zettlemoyer(2013) developed a model that handles referencesto the current world state.
Our system considersboth types of context, handling linguistic phenom-ena such as ellipsis and anaphora that referenceboth previous world states and logical forms.Logical form generation.
Traditional semanticparsers generate logical forms by aligning eachpart of the logical form to the utterance (Zelleand Mooney, 1996; Wong and Mooney, 2007;Zettlemoyer and Collins, 2007; Kwiatkowski etal., 2011).
In general, such systems rely on alexicon, which can be hand-engineered, extracted(Cai and Yates, 2013; Berant et al, 2013), or au-tomatically learned from annotated logical forms(Kwiatkowski et al, 2010; Chen, 2012).Recent work on learning from denotations hasmoved away from anchored logical forms.
Pa-supat and Liang (2014) and Wang et al (2015)proposed generating logical forms without align-ments, similar to our Model B. Yao et al (2014)and Bordes et al (2014) have explored predictingpaths in a knowledge graph directly, which is sim-ilar to the flat logical forms of Model C.Relaxation and bootstrapping.
The idea offirst training a simpler model in order to work up toa more complex one has been explored other con-texts.
In the unsupervised learning of generativemodels, bootstrapping can help escape local op-tima and provide helpful regularization (Och andNey, 2003; Liang et al, 2009).
When it is difficultto even find one logical form that reaches the de-notation, one can use the relaxation technique ofSteinhardt and Liang (2015).Recall that projecting from Model A to C cre-ates a more computationally tractable model atthe cost of expressivity.
However, this is becauseModel C used a linear model.
One might imag-ine that a non-linear model would be able to re-cuperate some of the loss of expressivity.
In-deed, Neelakantan et al (2016) use recurrent neu-ral networks attempt to perform logical operations.One could go one step further and bypass log-ical forms altogether, performing all the logicalreasoning in a continuous space (Bowman et al,2014; Weston et al, 2015; Guu et al, 2015; Reedand de Freitas, 2016).
This certainly avoids thecombinatorial explosion of logical forms in ModelA, but could also present additional optimizationchallenges.
It would be worth exploring this av-enue to completely understand the computation-expressivity tradeoff.ReproducibilityOur code, data, and experiments areavailable on CodaLab at https://worksheets.codalab.org/worksheets/0xad3fc9f52f514e849b282a105b1e3f02/.AcknowledgmentsWe thank the anonymous reviewers for their con-structive feedback.
The third author is supportedby a Microsoft Research Faculty Fellowship.ReferencesY.
Artzi and L. Zettlemoyer.
2013.
Weakly supervisedlearning of semantic parsers for mapping instruc-tions to actions.
Transactions of the Association forComputational Linguistics (TACL), 1:49?62.J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).A.
Bordes, S. Chopra, and J. Weston.
2014.
Ques-tion answering with subgraph embeddings.
In Em-pirical Methods in Natural Language Processing(EMNLP).S.
R. Bowman, C. Potts, and C. D. Manning.
2014.Can recursive neural tensor networks learn logicalreasoning?
In International Conference on Learn-ing Representations (ICLR).Q.
Cai and A. Yates.
2013.
Large-scale semantic pars-ing via schema matching and lexicon extension.
InAssociation for Computational Linguistics (ACL).D.
L. Chen and R. J. Mooney.
2011.
Learning to in-terpret natural language navigation instructions fromobservations.
In Association for the Advancement ofArtificial Intelligence (AAAI), pages 859?865.1464D.
L. Chen.
2012.
Fast online lexicon learning forgrounded language acquisition.
In Association forComputational Linguistics (ACL).J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Computational Natural Language Learn-ing (CoNLL), pages 18?27.D.
A. Dahl, M. Bates, M. Brown, W. Fisher,K.
Hunicke-Smith, D. Pallett, C. Pao, A. Rudnicky,and E. Shriberg.
1994.
Expanding the scope of theATIS task: The ATIS-3 corpus.
In Workshop on Hu-man Language Technology, pages 43?48.J.
Duchi, E. Hazan, and Y.
Singer.
2010.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
In Conference on Learning Theory(COLT).K.
Guu, J. Miller, and P. Liang.
2015.
Travers-ing knowledge graphs in vector space.
In Em-pirical Methods in Natural Language Processing(EMNLP).T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic CCGgrammars from logical form with higher-order uni-fication.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1223?1233.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2011.
Lexical generalization inCCG grammar induction for semantic parsing.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 1512?1523.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InAssociation for Computational Linguistics and In-ternational Joint Conference on Natural LanguageProcessing (ACL-IJCNLP), pages 91?99.P.
Liang.
2013.
Lambda dependency-based composi-tional semantics.
arXiv.A.
Neelakantan, Q. V. Le, and I. Sutskever.
2016.Neural programmer: Inducing latent programs withgradient descent.
In International Conference onLearning Representations (ICLR).F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29:19?51.P.
Pasupat and P. Liang.
2014.
Zero-shot entity extrac-tion from web pages.
In Association for Computa-tional Linguistics (ACL).P.
Pasupat and P. Liang.
2015.
Compositional semanticparsing on semi-structured tables.
In Association forComputational Linguistics (ACL).S.
Reed and N. de Freitas.
2016.
Neural programmer-interpreters.
In International Conference on Learn-ing Representations (ICLR).J.
Steinhardt and P. Liang.
2015.
Learning with re-laxed supervision.
In Advances in Neural Informa-tion Processing Systems (NIPS).A.
Vlachos and S. Clark.
2014.
A new corpus andimitation learning framework for context-dependentsemantic parsing.
Transactions of the Associationfor Computational Linguistics (TACL), 2:547?559.Y.
Wang, J. Berant, and P. Liang.
2015.
Building asemantic parser overnight.
In Association for Com-putational Linguistics (ACL).J.
Weston, A. Bordes, S. Chopra, and T. Mikolov.2015.
Towards AI-complete question answering: Aset of prerequisite toy tasks.
arXiv.Y.
W. Wong and R. J. Mooney.
2007.
Learningsynchronous grammars for semantic parsing withlambda calculus.
In Association for ComputationalLinguistics (ACL), pages 960?967.X.
Yao, J. Berant, and B. Van-Durme.
2014.
FreebaseQA: Information extraction or semantic parsing.
InWorkshop on Semantic parsing.M.
Zelle and R. J. Mooney.
1996.
Learning toparse database queries using inductive logic pro-gramming.
In Association for the Advancement ofArtificial Intelligence (AAAI), pages 1050?1055.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Un-certainty in Artificial Intelligence (UAI), pages 658?666.L.
S. Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to log-ical form.
In Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP/CoNLL), pages 678?687.L.
S. Zettlemoyer and M. Collins.
2009.
Learningcontext-dependent mappings from sentences to logi-cal form.
In Association for Computational Linguis-tics and International Joint Conference on NaturalLanguage Processing (ACL-IJCNLP).1465
