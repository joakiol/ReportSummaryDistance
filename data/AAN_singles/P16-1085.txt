Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 897?907,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsEmbeddings for Word Sense Disambiguation: An Evaluation StudyIgnacio Iacobacci1, Mohammad Taher Pilehvar2and Roberto Navigli11Department of Computer Science, Sapienza University of Rome, Italy2Language Technology Lab, University of Cambridge, UK{iacobacci,navigli}@di.uniroma1.itmp792@cam.ac.ukAbstractRecent years have seen a dramatic growthin the popularity of word embeddingsmainly owing to their ability to capture se-mantic information from massive amountsof textual content.
As a result, manytasks in Natural Language Processing havetried to take advantage of the potential ofthese distributional models.
In this work,we study how word embeddings can beused in Word Sense Disambiguation, oneof the oldest tasks in Natural LanguageProcessing and Artificial Intelligence.
Wepropose different methods through whichword embeddings can be leveraged in astate-of-the-art supervised WSD systemarchitecture, and perform a deep analysisof how different parameters affect perfor-mance.
We show how a WSD system thatmakes use of word embeddings alone, ifdesigned properly, can provide significantperformance improvement over a state-of-the-art WSD system that incorporates sev-eral standard WSD features.1 IntroductionEmbeddings represent words, or concepts in alow-dimensional continuous space.
These vec-tors capture useful syntactic and semantic infor-mation, such as regularities in language, where re-lationships are characterized by a relation-specificvector offset.
The ability of embeddings to cap-ture knowledge has been exploited in several tasks,such as Machine Translation (Mikolov et al, 2013,MT), Sentiment Analysis (Socher et al, 2013),Word Sense Disambiguation (Chen et al, 2014,WSD) and Language Understanding (Mesnil etal., 2013).
Supervised WSD is based on the hy-pothesis that contextual information provides agood approximation to word meaning, as sug-gested by Miller and Charles (1991): semanticallysimilar words tend to have similar contextual dis-tributions.Recently, there have been efforts on leverag-ing embeddings for improving supervised WSDsystems.
Taghipour and Ng (2015) showed thatthe performance of conventional supervised WSDsystems can be increased by taking advantage ofembeddings as new features.
In the same direc-tion, Rothe and Sch?utze (2015) trained embed-dings by mixing words, lexemes and synsets, andintroducing a set of features based on calculationson the resulting representations.
However, noneof these techniques takes full advantage of the se-mantic information contained in embeddings.
Asa result, they generally fail in providing substantialimprovements in WSD performance.In this paper, we provide for the first time astudy of different techniques for taking advantageof the combination of embeddings with standardWSD features.
We also propose an effective ap-proach for leveraging embeddings in WSD, andshow that this can provide significant improve-ment on multiple standard benchmarks.2 Word EmbeddingsAn embedding is a representation of a topologi-cal object, such as a manifold, graph, or field, ina certain space in such a way that its connectiv-ity or algebraic properties are preserved (Insall etal., 2015).
Presented originally by Bengio et al(2003), word embeddings aim at representing, i.e.,embedding, the ideal semantic space of words in areal-valued continuous vector space.
In contrast totraditional distributional techniques, such as La-tent Semantic Analysis (Landauer and Dutnais,1997, LSA) and Latent Dirichlet Allocation (Bleiet al, 2003, LDA), Bengio et al (2003) designed a897feed-forward neural network capable of predictinga word given the words preceding (i.e., leading upto) that word.
Collobert and Weston (2008) pre-sented a much deeper model consisting of severallayers for feature extraction, with the objective ofbuilding a general architecture for NLP tasks.
Amajor breakthrough occurred when Mikolov et al(2013) put forward an efficient algorithm for train-ing embeddings, known as Word2vec.
A similarmodel to Word2vec was presented by Penningtonet al (2014, GloVe), but instead of using latentfeatures for representing words, it makes an ex-plicit representation produced from statistical cal-culation on word countings.Numerous efforts have been made to improvedifferent aspects of word embeddings.
One wayto enhance embeddings is to represent more fine-grained semantic items, such as word sensesor concepts, given that conventional embeddingsconflate different meanings of a word into a sin-gle representation.
Several research studies haveinvestigated the representation of word senses,instead of words (Reisinger and Mooney, 2010;Huang et al, 2012; Camacho-Collados et al,2015b; Iacobacci et al, 2015; Rothe and Sch?utze,2015).
Another path of research is aimed atrefining word embeddings on the basis of ad-ditional information from other knowledge re-sources (Faruqui et al, 2015; Yu and Dredze,2014).
A good example of this latter approach isthat proposed by Faruqui et al (2015), which im-proves pre-trained word embeddings by exploit-ing the semantic knowledge from resources suchas PPDB1(Ganitkevitch et al, 2013), WordNet(Miller, 1995) and FrameNet (Baker et al, 1998).In the following section we discuss how embed-dings can be integrated into an important lexicalsemantic task, i.e., Word Sense Disambiguation.3 Word Sense DisambiguationNatural language is inherently ambiguous.
Mostcommonly-used words have several meanings.
Inorder to identify the intended meaning of a wordone has to analyze the context in which it ap-pears by directly exploiting information from rawtexts.
The task of automatically assigning pre-defined meanings to words in contexts, knownas Word Sense Disambiguation, is a fundamentaltask in computational lexical semantics (Navigli,2009).
There are four conventional approaches to1www.paraphrase.org/#/downloadWSD which we briefly explain in the following.3.1 Supervised methodsThese methods make use of manually sense-annotated data, which are curated by human ex-perts.
They are based on the assumption that aword?s context can provide enough evidence forits disambiguation.
Since manual sense annotationis a difficult and time-consuming process, some-thing known as the ?knowledge acquisition bot-tleneck?
(Pilehvar and Navigli, 2014), supervisedmethods are not scalable and they require repe-tition of a comparable effort for each new lan-guage.
Currently, the best performing WSD sys-tems are those based on supervised learning.
ItMakes Sense (Zhong and Ng, 2010, IMS) and thesystem of Shen et al (2013) are good represen-tatives for this category of systems.
We providemore information on IMS in Section 4.1.3.2 Unsupervised methodsThese methods create their own annotated corpus.The underlying assumption is that similar sensesoccur in similar contexts, therefore it is possibleto group word usages according to their sharedmeaning and induce senses.
These methods leadto the difficulty of mapping their induced sensesinto a sense inventory and they still require man-ual intervention in order to perform such mapping.Examples of this approach were studied by Agirreet al (2006), Brody and Lapata (2009), Manand-har et al (2010), Van de Cruys and Apidianaki(2011) and Di Marco and Navigli (2013).3.3 Semi-supervised methodsOther methods, called semi-supervised, take amiddle-ground approach.
Here, a small manually-annotated corpus is usually used as a seed for boot-strapping a larger annotated corpus.
Examples ofthese approaches were presented by Mihalcea andFaruque (2004).
A second option is to use a word-aligned bilingual corpus approach, based on theassumption that an ambiguous word in one lan-guage could be unambiguous in the context of asecond language, hence helping to annotate thesense in the first language (Ng and Lee, 1996).3.4 Knowledge-based methodsThese methods are based on existing lexical re-sources, such as knowledge bases, semantic net-works, dictionaries and thesauri.
Their main fea-ture is their coverage, since they function indepen-898dently of annotated data and can exploit the graphstructure of semantic networks to identify the mostsuitable meanings.
These methods are able to ob-tain wide coverage and good performance usingstructured knowledge, rivaling supervised meth-ods (Patwardhan and Pedersen, 2006; Mohammadand Hirst, 2006; Agirre et al, 2010; Guo and Diab,2010; Ponzetto and Navigli, 2010; Miller et al,2012; Agirre et al, 2014; Moro et al, 2014; Chenet al, 2014; Camacho-Collados et al, 2015a).3.5 Standard WSD featuresAs was analyzed by Lee and Ng (2002), conven-tional WSD systems usually make use of a fixedset of features to model the context of a word.
Thefirst feature is based on the words in the surround-ings of the target word.
The feature usually rep-resents the local context as a binary array, whereeach position represents the occurrence of a partic-ular word.
Part-of-speech (POS) tags of the neigh-boring words have also been used extensively asa WSD feature.
Local collocations represent an-other standard feature that captures the ordered se-quences of words which tend to appear around thetarget word (Firth, 1957).
Though not very popu-lar, syntactic relations have also been studied as apossible feature (Stetina et al, 1998) in WSD.More sophisticated features have also beenstudied.
Examples are distributional semanticmodels, such as Latent Semantic Analysis (Van deCruys and Apidianaki, 2011) and Latent DirichletAllocation (Cai et al, 2007).
Inasmuch as they arethe dominant distributional semantic model, wordembeddings have also been applied as features toWSD systems.
In this paper we study differentmethods through which word embeddings can beused as WSD features.3.6 Word Embeddings as WSD featuresWord embeddings have become a prominent tech-nique in distributional semantics.
These methodsleverage neural networks in order to model thecontexts in which a word is expected to appear.Thanks to their ability in efficiently learning thesemantics of words, word embeddings have beenapplied to a wide range of NLP applications.
Sev-eral studies have also investigated their integra-tion into the Word Sense Disambiguation setting.These include the works of Zhong and Ng (2010),Taghipour and Ng (2015), Rothe and Sch?utze(2015), and Chen et al (2014), which leverageembeddings for supervised (the former three) andknowledge-based (the latter) WSD.
However, toour knowledge, no previous work has investigatedmethods for integrating word embeddings in WSDand the role that different training parameters canplay.
In this paper, we put forward a framework fora comprehensive evaluation of different methodsof leveraging word embeddings as WSD featuresin a supervised WSD system.
We provide an anal-ysis of the impact of different parameters in thetraining of embeddings on the WSD performance.We consider four different strategies for integrat-ing a pre-trained word embedding in a supervisedWSD system, discussed in what follows.3.6.1 ConcatenationConcatenation is our first strategy, which is in-spired by the model of Bengio et al (2003).
Thismethod consists of concatenating the vectors ofthe words surrounding a target word into a largervector that has a size equal to the aggregated di-mensions of all the individual embeddings.
Letwijbe the weight associated with the ithdimen-sion of the vector of the jthword in the sentence,let D be the dimensionality of this vector, and Wbe the window size which is defined as the num-ber of words on a single side.
We are interestedin representing the context of the Ithword in thesentence.
The ithdimension of the concatenationfeature vector, which has a size of 2WD, is com-puted as follows:ei={wi mod D, I?W+biDcif biDc < Wwi mod D, I?W+1+biDcotherwisewhere mod is the modulo operation, i.e., the re-mainder after division.3.6.2 AverageAs its name indicates, the average strategy com-putes the centroid of the embeddings of all the sur-rounding words.
The formula divides each dimen-sion by 2W since the number of context words istwice the window size:ei=I+W?j=I?Wj 6=Iwij2W3.6.3 Fractional decayOur third strategy for constructing a feature vectoron the basis of the context word embeddings is in-spired by the way Word2vec combines the wordsin the context.
Here, the importance of a word899for our representation is assumed to be inverselyproportional to its distance from the target word.Hence, surrounding words are weighted based ontheir distance from the target word:ei=I+W?j=I?Wj 6=IwijW ?
|I ?
j|W3.6.4 Exponential decayExponential decay functions similarly to the frac-tional decay, which gives more importance to theclose context, but in this case the weighting in theformer is performed exponentially:ei=I+W?j=I?Wj 6=Iwij(1?
?
)|I?j|?1where ?
= 1 ?
0.1(W?1)?1is the decay parame-ter.
We choose the parameter in such a way that theimmediate surrounding words contribute 10 timesmore than the last words on both sides of the win-dow.4 FrameworkOur goal was to experiment with a state-of-the-artconventional supervised WSD system and a variedset of word embedding techniques.
In this sectionwe discuss the WSD system as well as the wordembeddings used in our experiments.4.1 WSD SystemWe selected It Makes Sense (Zhong and Ng, 2010,IMS) as our underlying framework for supervisedWSD.
IMS provides an extensible and flexibleplatform for supervised WSD by allowing the ver-ification of different WSD features and classifica-tion techniques.
By default, IMS makes use ofthree sets of features: (1) POS tags of the sur-rounding words, with a window of three words oneach side, restricted by the sentence boundary, (2)the set of words that appear in the context of thetarget word after stopword removal, and (3) localcollocations which consist of 11 features aroundthe target word.
IMS uses a linear support vectormachine (SVM) as its classifier.4.2 Embedding FeaturesWe take the real-valued word embeddings as newfeatures of IMS and introduce them into the sys-tem without performing any further modifications.We carried out experiments with three differentembeddings:?
Word2vec (Mikolov et al, 2013): We usedthe Word2vec toolkit2to learn 400 dimen-sional vectors on the September-2014 dumpof the English Wikipedia which comprisesaround three billion tokens.
We chosethe Skip-gram architecture with the negativesampling set to 10.
The sub-sampling of fre-quent words was set to 10?3and the windowsize to 10 words.?
C&W (Collobert and Weston, 2008): These50 dimensional embeddings were learnt us-ing a neural network model, consisting ofseveral layers for feature extraction.
The vec-tors were trained on a subset of the EnglishWikipedia.3?
Retrofitting: Finally, we used the approachof Faruqui et al (2015) to retrofit ourWord2vec vectors.
We used the ParaphraseDatabase (Ganitkevitch et al, 2013, PPDB)as external knowledge base for retrofittingand set the number of iterations to 10.5 ExperimentsWe evaluated the performance of our embedding-based WSD system on two standard WSD tasks:lexical sample and all-words.
In all the experi-ments in this section we used the exponential de-cay strategy (cf.
Section 3.6) and a window size often words on each side of the target word.5.1 Lexical Sample WSD ExperimentThe lexical sample WSD tasks provide trainingdatasets in which different occurrences of a smallset of words are sense annotated.
The goal is fora WSD system to analyze the contexts of the indi-vidual senses of these words and to capture cluesthat can be used for distinguishing different sensesof a word from each other at the test phase.Datasets.
As our benchmark for the lexical sam-ple WSD, we chose the Senseval-2 (Edmonds andCotton, 2001), Senseval-3 (Mihalcea et al, 2004),and SemEval-2007 (Pradhan et al, 2007) EnglishLexical Sample WSD tasks.
The former twocover nouns, verbs and adjectives in their datasetswhereas the latter task focuses on nouns and verbs2code.google.com/archive/p/word2vec/3http://ronan.collobert.com/senna/900TaskTraining Testnoun verb adjective noun verb adjectiveSenseval-2 (SE2) 4851 3566 755 1740 1806 375Senseval-3 (SE3) 3593 3953 314 1807 1978 159SemEval-07 (SE7) 13287 8987 ?
2559 2292 ?Table 1: The number of sentences per part of speech in the datasets of the English lexical sample taskswe considered for our experiments.System SE2 SE3 SE7IMS (2010) 65.3 72.9 87.9Taghipour and Ng (2015) 66.2 73.4 ?AutoExtend (2015) 66.5 73.6 ?IMS + C&W 64.3 70.1 88.0IMS + Word2vec 69.9 75.2 89.4IMS + Retrofitting 65.9 72.8 88.3C&W feature only 55.0 61.6 83.4Word2vec feature only 65.6 69.4 87.0Retrofitting feature only 67.2 72.7 88.0Table 2: F1 performance on the three English lexical sam-ple datasets.
IMS + X denotes the improved IMS systemwhen the X set of word representations were used as addi-tional features.
We also show in the last three rows the resultsfor the IMS system when word representations were used asthe only features.only.
Table 1 shows the number of sentences perpart of speech for the training and test datasets ofeach of these tasks.Comparison systems.
In addition to the vanillaIMS system in its default setting we comparedour system against two recent approaches that alsomodify the IMS system so that it can benefit fromthe additional knowledge derived from word em-beddings for improved WSD performance: (1) thesystem of Taghipour and Ng (2015), which com-bines word embeddings of Collobert and Weston(2008) using the concatenation strategy (cf.
Sec-tion 3.6) and introduces the combined embeddingsas a new feature in addition to the standard WSDfeatures in IMS; and (2) AutoExtend (Rothe andSch?utze, 2015), which constructs a whole new setof features based on vectors made from words,senses and synsets of WordNet and incorporatesthem in IMS.5.1.1 Lexical sample WSD resultsTable 2 shows the F1 performance of the differentsystems on the three lexical sample datasets.
Ascan be seen, the IMS + Word2vec system improvesover all comparison systems including those thatcombine standard WSD and embedding features(i.e., the system of Taghipour and Ng (2015) andAutoExtend) across all the datasets.
This showsthat our proposed strategy for introducing wordembeddings into the IMS system on the basis ofexponential decay was beneficial.
In the last threerows of the table, we also report the performanceof the WSD systems that leverage only word em-beddings as their features and do not incorporateany standard WSD feature.
It can be seen thatword embeddings, in isolation, provide compet-itive performance, which proves their capabilityin obtaining the information captured by standardWSD features.
Among different embeddings, theretrofitted vectors provide the best performancewhen used in isolation.5.2 All-Words WSD ExperimentsThe goal in this task is to disambiguate all thecontent words in a given text.
In order to learnmodels for disambiguating a large set of contentwords, a high-coverage sense-annotated corpus isrequired.
Since all-words tasks do not usuallyprovide any training data, the challenge here isnot only to learn accurate disambiguation modelsfrom the training data, as is the case in the lexi-cal sample task, but also to gather high-coveragetraining data and to learn disambiguation modelsfor as many words as possible.Training corpus.
As our training corpus weopted for two available resources: SemCor andOMSTI.
SemCor (Miller et al, 1994) is a man-ually sense-tagged corpus created by the WordNetproject team at Princeton University.
The datasetis a subset of the English Brown Corpus and com-prises around 360,000 words, providing annota-tions for more than 200K content words.4OM-4We used automatic mappings to WordNet 3.0 provided inweb.eecs.umich.edu/?mihalcea/downloads.html.901STI5(One Million Sense-Tagged for Word SenseDisambiguation and Induction) was constructedbased on the DSO corpus (Ng and Lee, 1996)and provides annotations for around 42K differentnouns, verbs, adjectives, and adverbs.Datasets.
As benchmark for this experiment, weconsidered the Senseval-2 (Edmonds and Cotton,2001), Senseval-3 (Snyder and Palmer, 2004), andSemEval-2007 (Pradhan et al, 2007) English all-words tasks.
There are 2474, 2041, and 465 wordsfor which at least one of the occurrences has beensense annotated in the Senseval-2, Senseval-3 andSemEval-2007 datasets, respectively.Experimental setup.
Similarly to the lexicalsample experiment, in the all-words setting weused the exponential decay strategy (cf.
Section.4.2) in order to incorporate word embeddings asnew features in IMS.
For this experiment, we onlyreport the results for the best-performing word em-beddings in the lexical sample experiment, i.e.,Word2vec (see Table 2).Comparison systems.
We benchmarked theperformance of our system against five other sys-tems.
Similarly to our lexical sample experiment,we compared against the vanilla IMS system andthe work of Taghipour and Ng (2015).
In addition,we performed experiments on the nouns subsetsof the datasets in order to be able to provide com-parisons against two other WSD approaches: Ba-belfy (Moro et al, 2014) and Muffin (Camacho-Collados et al, 2015a).
Babelfy is a multilin-gual knowledge-based WSD and Entity Linkingalgorithm based on the semantic network of Ba-belNet.
Muffin is a multilingual sense repre-sentation technique that combines the structuralknowledge derived from semantic networks withthe distributional statistics obtained from text cor-pora.
The system uses sense-based representationsfor performing WSD.
Camacho-Collados et al(2015a) also proposed a hybrid system that aver-ages the disambiguation scores of IMS with theirs(shown as ?Muffin + IMS?
in our tables).
Wealso report the results for UKB w2w (Agirre andSoroa, 2009), another knowledge-based WSD ap-proach based on Personalized PageRank (Haveli-wala, 2002).
Finally, we also carried out experi-ments with the pre-trained models6that are pro-5www.comp.nus.edu.sg/?nlp/corpora.html6www.comp.nus.edu.sg/?nlp/sw/models.tar.gzSystem SE2 SE3 SE7MFS baseline 60.1 62.3 51.4IMS (Zhong and Ng, 2010) 68.2 67.6 58.3Taghipour and Ng (2015) ?
68.2 ?IMS (pre-trained models) 67.7 67.5 58.0IMS (SemCor) 62.5 65.0 56.5IMS (OMSTI) 67.0 66.4 57.6IMS + Word2vec (SemCor) 63.4 65.3 57.8IMS + Word2vec (OMSTI) 68.3 68.2 59.1Table 3: F1 performance on different English all-words WSD datasets.System SE2 SE3 SE7MFS baseline 71.6 70.3 65.8Babelfy ?
68.3 62.7Muffin ?
?
66.0Muffin + IMS ?
?
68.5UBK w2w ?
65.3 56.0IMS (pre-trained models) 77.5 74.0 66.5IMS (SemCor) 73.0 70.8 64.2IMS (OMSTI) 76.6 73.3 67.7IMS + Word2vec (SemCor) 74.2 70.1 68.6IMS + Word2vec (OMSTI) 77.7 74.1 71.5Table 4: F1 performance in the nouns subsets ofdifferent all-words WSD datasets.vided with the IMS toolkit, as well as IMS trainedon our two training corpora, i.e., SemCor and OM-STI.5.2.1 All-words WSD resultsTables 3 and 4 list the performance of differentsystems on, respectively, the whole and the noun-subset datasets of the three all-words WSD tasks.Similarly to our lexical sample experiment, theIMS + Word2vec system provided the best per-formance across datasets and benchmarks.
Thecoupling of Word2vec embeddings to the IMS sys-tem proved to be consistently helpful.
Among thetwo training corpora, as expected, OMSTI pro-vided a better performance owing to its consid-erably larger size and higher coverage.
Anotherpoint to be noted here is the difference betweenresults of the IMS with the pre-trained models andthose trained on the OMSTI corpus.
Since we usedthe same system configuration across the two runs,we conclude that the OMSTI corpus is either sub-stantially smaller or less representative than thecorpus used by Zhong and Ng (2010) for building902the pre-trained models of IMS.
Despite this fact,the IMS + Word2vec system can consistently im-prove the performance of IMS (pre-trained mod-els) across the three datasets.
This shows that aproper introduction of word embeddings into a su-pervised WSD system can compensate the nega-tive effect of using lower quality training data.6 AnalysisWe carried out a series of experiments in order tocheck the impact of different system parameterson the final WSD performance.
We were partic-ularly interested in observing the role that vari-ous training parameters of embeddings as well asWSD features have in the WSD performance.
Weused the Senseval-2 English Lexical Sample taskas our benchmark for this analysis.6.1 The effect of different parametersTable 5 shows F1 performance of different config-urations of our system on the task?s dataset.
Westudied five different parameters: the type (i.e.,w2v or Retrofitting) and dimensionality (200, 400,or 800) of the embeddings, combination strategy(concatenation, average, fractional or exponentialdecay), window size (5, 10, 20 and words), andWSD features (collocations, POS tags, surround-ing words, all of these or none).
All the embed-dings in this experiment were trained on the sametraining data and, unless specified, with the sameconfiguration as described in Section 4.2.
As base-line we show in the table the performance of thevanilla WSD system, i.e., IMS.
For better read-ability, we report the differences between the per-formances of our system and the baseline.We observe that the addition of Word2vec wordembeddings to IMS (+w2v in the table) wasbeneficial in all settings.
Among combinationstrategies, concatenation and average produced thesmallest gain and did not benefit from embeddingsof higher dimensionality.
However, the other twostrategies, i.e., fractional and exponential decay,showed improved performance with the increasein the size of the employed embeddings, irre-spective of the WSD features.
The window sizeshowed a peak of performance when 10 wordswere taken in the case of standard word embed-dings.
For retrofitting, a larger window seemsto have been beneficial, except when no standardWSD features were taken.
Another point to notehere is that, among the three WSD features, POSproved to be the most effective one while due tothe nature of the embeddings, the exclusion of theSurroundings features in addition to the inclusionof the embeddings was largely beneficial in all theconfigurations.
Furthermore, we found that thebest configurations for this task were the ones thatexcluded Surroundings, and included w2v embed-dings with a window of 10 and 800 dimensionswith exponential decay strategy (70.2% of F1 per-formance) as well as the configuration used in ourexperiments, with all the standard features, andw2v embeddings with 400 dimensions, a windowof 10 and exponential decay strategy (69.9% of F1performance).The retrofitted embeddings provided lower per-formance improvement when added on top ofstandard WSD features.
However, when they wereused in isolation (shown in the right-most col-umn), the retrofitted embeddings interestingly pro-vided the best performance, improving the vanillaWSD system with standard features by 2.8 per-centage points (window size 5, dimensionality800).
In fact, the standard features had a destruc-tive role in this setting as the overall performancewas reduced when they were combined with theretrofitted embeddings.
Finally, we point out themissing values in the configuration with 800 di-mensions and a window size of 20.
Due to the na-ture of the concatenation strategy, this configura-tion greatly increased the number of features fromembeddings only, reaching 32000 (800 x 2 x 20)features.
Not only was the concatenation strategyunable to take advantage of the increased dimen-sionality, but also it was not able to scale.These results show that a state-of-the-art su-pervised WSD system can be constructed withoutincorporating any of the conventional WSD fea-tures, which in turn demonstrates the potential ofretrofitted word embeddings for WSD.
This find-ing is interesting, because it provides the basis forfurther studies on how synonymy-based semanticknowledge introduced by retrofitting might play arole in effective WSD, and how retrofitting mightbe optimized for improved WSD.
Indeed, suchstudies may provide the basis for re-designing thestandard WSD features.6.2 Comparison of embedding typesWe were also interested in comparing differenttypes of embeddings in our WSD framework.We tested for seven sets of embeddings with dif-903Collocations X X XPOS X X XSurroundings X X XDimensionality 200 400 800 200 400 800 200 400 800 200 400 800 200 400 800SystemStrategyWindowIMS 62.4 63.7 62.0 65.2 ?+ w2v Con5 +0.1 +0.4 +0.1 -0.1 +0.3 +0.2 +0.1 +0.5 +0.1 -0.2 +0.1 +0.1 46.9 48.7 44.210 -0.1 +0.5 +0.3 -0.1 +0.5 0.0 +0.6 +1.0 +0.5 -0.1 +0.1 -0.1 48.6 51.1 49.720 -0.2 +0.4 ?
-0.3 +0.3 ?
+0.7 +1.5 ?
-0.5 +0.4 ?
52.5 54.1 ?+ w2v Avg5 +0.8 +1.0 +1.0 +1.3 +1.3 +1.4 +3.9 +4.2 +4.1 +1.7 +1.4 +1.6 58.3 59.9 61.310 +0.8 +0.9 +0.9 +0.6 +0.7 +0.8 +3.6 +3.7 +3.9 +0.6 +0.6 +0.7 63.7 64.1 64.720 +0.3 +0.3 +0.3 +0.5 +0.3 +0.4 +2.4 +2.3 +2.3 +0.2 +0.2 +0.2 62.7 63.1 63.5+ w2v Frac5 +3.9 +4.9 +5.2 +4.2 +4.6 +5.3 +6.3 +6.6 +6.8 +3.0 +3.6 +3.8 61.2 63.1 64.810 +4.9 +5.8 +5.7 +4.6 +5.2 +5.1 +5.9 +7.0 +7.4 +3.6 +4.3 +4.0 61.3 63.8 65.220 +4.4 +4.5 +4.7 +3.7 +4.0 +4.3 +4.8 +6.1 +5.4 +3.2 +3.3 +3.4 61.2 63.4 63.9+ w2v Exp5 +4.1 +5.0 +5.2 +4.1 +4.7 +5.0 +6.1 +6.1 +6.4 +2.9 +3.5 +3.7 62.3 64.7 64.910 +5.4 +6.6 +6.4 +4.9 +5.8 +6.0 +7.2 +7.7 +8.2 +4.1 +4.7 +4.6 63.2 65.6 66.920 +5.2 +5.6 +5.9 +4.4 +5.1 +4.9 +6.1 +7.0 +6.8 +3.9 +4.3 +4.2 61.9 64.4 65.2+ Ret Con5 -0.1 -0.1 -0.1 -0.1 -0.1 0.0 +0.1 +0.1 -0.1 -0.1 +0.1 +0.1 50.7 53.5 50.910 +0.1 0.0 0.0 -0.3 0.0 0.0 +0.1 +0.2 +0.1 0.0 0.0 0.0 52.1 54.2 53.420 0.0 0.0 ?
-0.2 0.0 ?
+0.7 +0.3 ?
0.0 -0.1 ?
53.7 54.8 ?+ Ret Avg5 +0.1 0.0 -0.1 +0.1 0.0 -0.1 +0.8 +0.8 +0.7 +0.1 0.0 +0.1 60.7 60.3 60.510 -0.2 -0.1 0.0 -0.2 -0.3 0.0 +0.7 +0.7 +0.5 0.0 +0.1 +0.1 58.9 58.4 58.220 -0.1 +0.1 +0.1 -0.2 -0.2 -0.2 +0.5 +0.4 +0.4 0.0 0.0 0.0 56.5 56.0 55.5+ Ret Frac5 +1.4 +1.3 +1.2 +1.2 +1.0 +0.9 +3.3 +3.1 +2.9 +0.5 +0.3 +0.3 66.5 67.3 67.710 +1.7 +1.4 +1.2 +1.5 +1.4 +1.2 +5.2 +4.7 +4.5 +0.7 +0.8 +0.6 64.4 66.2 66.120 +2.2 +2.2 +1.8 +2.2 +1.8 +2.0 +6.7 +6.4 +5.9 +1.3 +1.2 +1.0 64.0 64.2 64.7+ Ret Exp5 +1.1 +1.1 +1.1 +0.8 +0.8 +0.7 +2.7 +2.6 +2.2 +0.3 +0.3 +0.3 66.8 67.7 68.010 +1.5 +1.3 +1.0 +1.2 +1.1 +1.0 +4.4 +4.2 +3.8 +0.7 +0.7 +0.3 65.9 67.2 67.520 +1.8 +1.7 +1.5 +1.7 +1.5 +1.5 +6.3 +5.9 +5.4 +1.1 +0.8 +0.7 65.1 65.8 66.5Table 5: F1 performance of different models on the Senseval-2 English Lexical Sample task.
We showresults for varied dimensionality (200, 400, and 800), window size (5, 10 and 20 words) and combinationstrategy, i.e., Concatenation (Con), Averaging (Avg), Fractional decay (Frac), and Exponential decay(Exp).
To make the table easier to read, we highlight each cell according to the relative performance gainin comparison to the IMS baseline (top row in the table).ferent dimensionalities and learning techniques:Word2vec embeddings trained on Wikipedia, withthe Skip-gram model for dimensionalities 50, 300and 500 (for comparison reasons) and CBOW with300 dimensions, Word2vec trained on the GoogleNews corpus with 300 dimensions and the Skip-gram model, the 300 dimensional embeddings ofGloVe, and the 50 dimensional C&W embed-dings.
Additionally we include experiments on anon-embedding model, a PMI-SVD vector spacemodel trained by Baroni et al (2014).Table 6 lists the performance of our systemwith different word representations in vector spaceon the Senseval-2 English Lexical Sample task.The results corroborate the findings of Levy et al(2015) that Skip-gram is more efficient in captur-904Word representations Dim.Combination strategyConcatenation Average Fractional ExponentialSkip-gram - GoogleNews 300 65.5 65.5 69.4 69.6GloVe 300 61.7 66.3 66.7 68.3CBOW - Wiki 300 65.1 65.4 68.9 68.8Skip-gram - Wiki 300 65.2 65.6 68.9 69.7PMI - SVD - Wiki 500 65.5 65.3 67.3 66.8Skip-gram - Wiki 500 65.1 65.6 69.1 69.9Collobert & Weston 50 58.6 67.3 62.9 64.3Skip-gram - Wiki 50 65.0 65.7 68.3 68.6Table 6: F1 percentage performance on the Senseval-2 English Lexical Sample dataset with differentword representations models, vector dimensionalities (Dim.)
and combination strategies.ing the semantics than CBOW and GloVe.
Ad-ditionally, the use of embeddings with decay fareswell, independently of the type of embedding.
Theonly exception is the C&W embeddings, for whichthe average strategy works best.
We attribute thisbehavior to the nature of these embeddings, ratherthan to their dimensionality.
This is shown in ourcomparison against the 50-dimensional Skip-gramembeddings trained on the Wikipedia corpus (bot-tom of Table 6), which performs well with both de-cay strategies, outperforming C&W embeddings.7 ConclusionsIn this paper we studied different ways of inte-grating the semantic knowledge of word embed-dings in the framework of WSD.
We carried out adeep analysis of different parameters and strate-gies across several WSD tasks.
We draw threemain findings.
First, word embeddings can beused as new features to improve a state-of-the-artsupervised WSD that only uses standard features.Second, integrating embeddings on the basis ofan exponential decay strategy proves to be moreconsistent in producing high performance than theother conventional strategies, such as vector con-catenation and centroid.
Third, the retrofitted em-beddings that take advantage of the knowledge de-rived from semi-structured resources, when usedas the only feature for WSD can outperform state-of-the-art supervised models which use standardWSD features.
However, the best performanceis obtained when standard WSD features areaugmented with the additional knowledge fromWord2vec vectors on the basis of a decay func-tion strategy.
Our hope is that this work will serveas the first step for further studies on re-designingstandard WSD features.
We release at https://github.com/iiacobac/ims_wsd_emb allthe codes and resources used in our experimentsin order to provide a framework for research onthe evaluation of new VSM models in the WSDframework.
As future work, we plan to investigatethe possibility of designing word representationsthat best suit the WSD framework.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank For Word Sense Disambiguation.
In Pro-ceedings of the 12th Conference of the EACL, pages33?41, Athens, Greece.Eneko Agirre, David Mart?
?nez, Oier L?opez de Lacalle,and Aitor Soroa.
2006.
Two graph-based algorithmsfor state-of-the-art wsd.
In Proceedings of the 2006EMNLP, pages 585?593, Sydney, Australia.Eneko Agirre, Aitor Soroa, and Mark Stevenson.
2010.Graph-based word sense disambiguation of biomed-ical documents.
Bioinformatics, 26(22):2889?2896.Eneko Agirre, Oier L?opez de Lacalle, and Aitor Soroa.2014.
Random Walks for Knowledge-based WordSense Disambiguation.
Comp.
Ling., 40(1):57?84.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 36th ACL, pages 86?90, Montreal, Que-bec, Canada.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
A905systematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceed-ings of the 52nd ACL, volume 1, pages 238?247,Baltimore, Maryland.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A Neural Probabilistic Lan-guage Model.
The Journal of Machine Learning Re-search, 3:1137?1155.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of the 12thConference of the EACL, pages 103?111, Athens,Greece.Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.2007.
NUS-ML:Improving Word Sense Disam-biguation Using Topic Features.
In Proceedings ofthe SemEval-2007, pages 249?252, Prague, CzechRepublic.Jos?e Camacho-Collados, Mohammad Taher Pilehvar,and Roberto Navigli.
2015a.
A Unified Multilin-gual Semantic Representation of Concepts.
In Pro-ceedings of the 53rd ACL, volume 1, pages 741?751,Beijing, China.Jos?e Camacho-Collados, Mohammad Taher Pilehvar,and Roberto Navigli.
2015b.
NASARI: a novelapproach to a semantically-aware representation ofitems.
In Proceedings of the 2015 NAACL, pages567?577, Denver, Colorado.Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.2014.
A unified model for word sense representa-tion and disambiguation.
In Proceedings of the 2014EMNLP, pages 1025?1035, Doha, Qatar.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language processing:Deep neural networks with multitask learning.
InProceedings of the 25th ICML, pages 160?167,Helsinki, Finland.Antonio Di Marco and Roberto Navigli.
2013.
Clus-tering and diversifying web search results withgraph-based word sense induction.
Comp.
Ling.,39(3):709?754.Philip Edmonds and Scott Cotton.
2001.
Senseval-2:Overview.
In The Proceedings of the 2nd Interna-tional Workshop on Evaluating Word Sense Disam-biguation Systems, pages 1?5, Toulouse, France.Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,Chris Dyer, Eduard Hovy, and Noah A. Smith.2015.
Retrofitting Word Vectors to Semantic Lex-icons.
In Proceedings of the 2015 NAACL, pages1606?1615, Denver, Colorado.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-55.
Studies in Linguistic Analysis (special volume ofthe Philological Society), 1952-59:1?32.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings 2013 NAACL, pages 758?764, Atlanta, Georgia.Weiwei Guo and Mona Diab.
2010.
Combining Or-thogonal Monolingual and Multilingual Sources ofEvidence for All Words WSD.
In Proceedings ofthe 48th ACL, pages 1542?1551, Uppsala, Sweden.Taher H. Haveliwala.
2002.
Topic-sensitive PageR-ank.
In Proceedings of the 11th international con-ference on World Wide Web, pages 517?526, Hon-olulu, Hawai.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving WordRepresentations Via Global Context And MultipleWord Prototypes.
In Proceedings of 50th ACL, vol-ume 1, pages 873?882, Jeju Island, South Korea.Ignacio Iacobacci, Mohammad Taher Pilehvar, andRoberto Navigli.
2015.
SensEmbed: LearningSense Embeddings for Word and Relational Simi-larity.
In Proceedings of the 53rd ACL, volume 1,pages 95?105, Beijing, China.Matt Insall, Todd Rowland, and Eric W. Weis-stein.
2015.
?Embedding?.
From MathWorld?A Wolfram Web Resource (access Sep 11,2015) http://mathworld.wolfram.com/Embedding.html.Thomas K. Landauer and Susan T. Dutnais.
1997.
ASolution to Platos Problem: The Latent SemanticAnalysis Theory of Acquisition, Induction, and Rep-resentation of Knowledge.
Psychological Review,104(2):211?240.Yoong Keok Lee and Hwee Tou Ng.
2002.
An Empir-ical Evaluation of Knowledge Sources and Learn-ing Algorithms for Word Sense Disambiguation.
InProceedings of the 2002 EMNLP, volume 10, pages41?48, Philadelphia, Pennsylvania.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
TACL, 3:211?225.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
Semeval-2010task 14: Word sense induction & disambiguation.In Proceedings of SemEval-2010, pages 63?68, Up-psala, Sweden.Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of Recurrent-neural-network Architectures and Learning Methods forSpoken Language Understanding.
In INTER-SPEECH, pages 3771?3775, Lyon, France.Rada Mihalcea and Ehsanul Faruque.
2004.
Sense-learner: Minimally Supervised Word Sense Disam-biguation for All Words in Open Text.
In Proceed-ings of ACL/SIGLEX Senseval-3, volume 3, pages155?158, Barcelona, Spain.906Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English Lexical SampleTask.
In Proceedings of ACL/SIGLEX Senseval-3,pages 25?28, Barcelona, Spain.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013.Exploiting similarities among languages for ma-chine translation.
arXiv preprint arXiv:1309.4168.George A. Miller and Walter G Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.George A. Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert G Thomas.
1994.
Us-ing a Semantic Concordance for Sense Identifica-tion.
In Proceedings of the Workshop on HLT, pages240?243, Plainsboro, New Jersey.Tristan Miller, Chris Biemann, Torsten Zesch, andIryna Gurevych.
2012.
Using Distributional Sim-ilarity for Lexical Expansion in Knowledge-basedWord Sense Disambiguation.
In COLING, pages1781?1796, Mumbai, India.George A. Miller.
1995.
WordNet: A LexicalDatabase for English.
Comm.
ACM, 38(11):39?41.Saif Mohammad and Graeme Hirst.
2006.
Determin-ing Word Sense Dominance Using a Thesaurus.
InProceedings of the 11th Conference of EACL, pages121?128, Trento, Italy.Andrea Moro, Alessandro Raganato, and Roberto Nav-igli.
2014.
Entity Linking meets Word Sense Dis-ambiguation: a Unified Approach.
Transactions ofthe ACL, 2:231?244.Roberto Navigli.
2009.
Word sense disambiguation: asurvey.
ACM COMPUTING SURVEYS, 41(2):1?69.Hwee Tou Ng and Hian Beng Lee.
1996.
IntegratingMultiple Knowledge Sources to Disambiguate WordSense: An Exemplar-based Approach.
In Proceed-ings of the 34th Meeting on ACL, pages 40?47, SantaCruz, California.Siddharth Patwardhan and Ted Pedersen.
2006.
Us-ing WordNet-based Context Vectors to Estimate theSemantic Relatedness of Concepts.
In Proceedingsof the EACL 2006 Workshop Making Sense of Sense,volume 1501, pages 1?8, Trento, Italy.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
GloVe: Global Vectors forWord Representation.
In Proceedings of the 2014EMNLP, pages 1532?1543, Doha, Qatar.Mohammad Taher Pilehvar and Roberto Navigli.2014.
A Large-scale Pseudoword-based EvaluationFramework for State-of-the-Art Word Sense Disam-biguation.
Computational Linguistics, 40(4):837?881.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rival-ing supervised systems.
In Proceedings of the 48thACL, pages 1522?1531, Uppsala, Sweden.Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,and Martha Palmer.
2007.
SemEval-2007 Task17: English Lexical Sample, SRL and All Words.In Proceedings of the SemEval-2007, pages 87?92,Prague, Czech Republic.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-Prototype Vector-Space Models of WordMeaning.
In Proceedings of the 2010 Annual Con-ference of the NAACL, pages 109?117, Los Angeles,California.Sascha Rothe and Hinrich Sch?utze.
2015.
Autoex-tend: Extending word embeddings to embeddingsfor synsets and lexemes.
In Proceedings of the 53rdACL, volume 1, pages 1793?1803, Beijing, China.Hui Shen, Razvan Bunescu, and Rada Mihalcea.
2013.Coarse to Fine Grained Sense Disambiguation inWikipedia.
In *SEM 2013: The Secound Joint Con-ference on Lexical and Computational Semantics,pages 22?31, Atlanta, Georgia.Benjamin Snyder and Martha Palmer.
2004.
TheSenseval-3 English All-Words Task.
In Proceed-ings of ACL/SIGLEX Senseval-3, pages 41?43,Barcelona, Spain.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive Deep Mod-els for Semantic Compositionality Over a SentimentTreebank.
In Proceedings of the 2013 EMNLP,pages 1631?1642, Seattle, USA.Jiri Stetina, Sadao Kurohashi, and Makoto Nagao.1998.
General Word Sense Disambiguation MethodBased on a Full Sentential Context.
In Usage ofWordNet in Natural Language Processing, Proceed-ings of COLING-ACL Workshop, Montreal, Quebec,Canada.Kaveh Taghipour and Hwee Tou Ng.
2015.
Semi-Supervised Word Sense Disambiguation UsingWord Embeddings in General and Specific Domains.In Proceedings of the 2015 Annual Conference of theNAACL, pages 314?323, Denver, Colorado.Tim Van de Cruys and Marianna Apidianaki.
2011.Latent Semantic Word Sense Induction and Disam-biguation.
In Proceedings of the 49th ACL, vol-ume 1, pages 1476?1485, Portland, Oregon.Mo Yu and Mark Dredze.
2014.
Improving LexicalEmbeddings with Semantic Knowledge.
In Pro-ceedings of the 52nd ACL, volume 2, pages 545?550, Baltimore, Maryland.Zhi Zhong and Hwee Tou Ng.
2010.
It Makes Sense:A Wide-coverage Word Sense Disambiguation Sys-tem for Free Text.
In Proceedings of the 48th ACL,pages 78?83, Uppsala, Sweden.907
