Conditions on Consistency ofProbabilistic Tree Adjoining Grammars*Anoop SarkarDept.
of Computer  and Information ScienceUniversity of Pennsylvania200 South 33rd Street,Philadelphia, PA 19104-6389 USAanoop@linc, cis.
upenn, eduAbst ractMuch of the power of probabilistic methods inmodelling language comes from their ability tocompare several derivations for the same stringin the language.
An important starting pointfor the study of such cross-derivational proper-ties is the notion of consistency.
The probabil-ity model defined by a probabilistic grammar issaid to be consistent if the probabilities assignedto all the strings in the language sum to one.From the literature on probabilistic ontext-freegrammars (CFGs), we know precisely the con-ditions which ensure that consistency is true fora given CFG.
This paper derives the conditionsunder which a given probabilistic Tree Adjoin-ing Grammar (TAG) can be shown to be con-sistent.
It gives a simple algorithm for checkingconsistency and gives the formal justificationfor its correctness.
The conditions derived herecan be used to ensure that probability modelsthat use TAGs can be checked for deficiency(i.e.
whether any probability mass is assignedto strings that cannot be generated).1 In t roduct ionMuch of the power of probabilistic methodsin modelling language comes from their abil-ity to compare several derivations for the samestring in the language.
This cross-derivationalpower arises naturally from comparison of vari-ous derivational paths, each of which is a prod-uct of the probabilities associated with each stepin each derivation.
A common approach usedto assign structure to language is to use a prob-abilistic grammar where each elementary rule* This research was partially supported by NSF grantSBR8920230 and ARO grant DAAH0404-94-G-0426.The author would like to thank Aravind Joshi, Jeff Rey-nat, Giorgio Satta, B. Srinivas, Fei Xia and the twoanonymous reviewers for their valuable comments.or production is associated with a probability.Using such a grammar, a probability for eachstring in the language is computed.
Assum-ing that the probability of each derivation of asentence is well-defined, the probability of eachstring in the language is simply the sum of theprobabilities of all derivations of the string.
Ingeneral, for a probabilistic grammar G the lan-guage of G is denoted by L(G).
Then if a stringv is in the language L(G) the probabilistic gram-mar assigns v some non-zero probability.There are several cross-derivational proper-ties that can be studied for a given probabilis-tic grammar formalism.
An important startingpoint for such studies is the notion of consis-tency.
The probability model defined by a prob-abilistic grammar is said to be consistent if theprobabilities assigned to all the strings in thelanguage sum to 1.
That is, if Pr defined by aprobabilistic grammar, assigns a probability toeach string v 6 E*, where Pr(v) = 0 ifv ~ L(G),thenPr(v) = i (i)veL(G)From the literature on probabilistic ontext-free grammars (CFGs) we know precisely theconditions which ensure that (1) is true for agiven CFG.
This paper derives the conditionsunder which a given probabilistic TAG can beshown to be consistent.TAGs are important in the modelling of nat-ural language since they can be easily lexical-ized; moreover the trees associated with wordscan be used to encode argument and adjunct re-lations in various syntactic environments.
Thispaper assumes ome familiarity with the TAGformalism.
(Joshi, 1988) and (Joshi and Sch-abes, 1992) are good introductions to the for-malism and its linguistic relevance.
TAGs have1164been shown to have relations with both phrase-structure grammars and dependency grammars(Rambow and Joshi, 1995) and can handle(non-projective) long distance dependencies.Consistency of probabilistic TAGs has prac-tical significance for the following reasons:?
The conditions derived here can be usedto ensure that probability models that useTAGs can be checked for deficiency.?
Existing EM based estimation algorithmsfor probabilistic TAGs assume that theproperty of consistency holds (Schabes,1992).
EM based algorithms begin with aninitial (usually random) value for each pa-rameter.
If the initial assignment causesthe grammar to be inconsistent, then it-erative re-estimation might converge to aninconsistent grammar 1.?
Techniques used in this paper can be usedto determine consistency for other proba-bility models based on TAGs (Carroll andWeir, 1997).2 Notat ionIn this section we establish some notational con-ventions and definitions that we use in this pa-per.
Those familiar with the TAG formalismonly need to give a cursory glance through thissection.A probabilistic TAG is represented by(N, E, 2:, A, S, ?)
where N, E are, respectively,non-terminal and terminal symbols.
2: U ,4 is aset of trees termed as elementary trees.
We takeV to be the set of all nodes in all the elementarytrees.
For each leaf A E V, label(A) is an ele-ment from E U {e}, and for each other node A,label(A) is an element from N. S is an elementfrom N which is a distinguished start symbol.The root node A of every initial tree which canstart a derivation must have label(A) = S.2: axe termed initial trees and ,4 are auxil-iary trees which can rewrite a tree node A E V.This rewrite step is called ad junct ion .
?
is afunction which assigns each adjunction with aprobability and denotes the set of parameters1Note that for CFGs it has been shown in (Chaud-hari et al, 1983; S~nchez and Bened~, 1997) that inside-outside reestimation can be used to avoid inconsistency.We will show later in the paper that the method used toshow consistency in this paper precludes a straightfor-ward extension of that result for TAGs.in the model.
In practice, TAGs also allow aleaf nodes A such that label(A) is an elementfrom N. Such nodes A are rewritten with ini-tial trees from I using the rewrite step calledsubst i tu t ion .
Except in one special case, wewill not need to treat substitution as being dis-tinct from adjunction.For t E 2: U .4, `4(t) are the nodes in treet that can be modified by adjunction.
Forlabel(A) E N we denote Adj(label(A)) as theset of trees that can adjoin at node A E V.The adjunction of t into N E V is denoted byN ~-~ t. No adjunction at N E V is denotedby N ~ nil.
We assume the following proper-ties hold for every probabilistic TAG G that weconsider:1.
G is lexicalized.
There is at least oneleaf node a that lexicalizes each elementarytree, i.e.
a E E.2.
G is proper.
For each N E V,?
(g  ~-~ nil) + ~ ?
(g  ~-+ t) = 1t..Adjunction is prohibited on the foot nodeof every auxiliary tree.
This condition isimposed to avoid unnecessary ambiguityand can be easily relaxed.There is a distinguished non-lexicalized ini-tial tree T such that each initial tree rootedby a node A with label(A) = S substitutesinto T to complete the derivation.
This en-sures that probabilities assigned to the in-put string at the start of the derivation arewell-formed.We use symbols S, A, B , .
.
.
to range over V,symbols a ,b,c , .
.
,  to range over E. We uset l , t2 , .
.
,  to range over I U A and e to denotethe empty string.
We use Xi to range over all inodes in the grammar.3 App ly ing  probab i l i ty  measures  toT ree  Ad jo in ing  LanguagesTo gain some intuition about probability assign-ments to languages, let us take for example, alanguage well known to be a tree adjoining lan-guage:L(G) = {anbncndnln > 1}1165It seems that we should be able to use a func-tion ?
to assign any probability distribution tothe strings in L(G) and then expect hat we canassign appropriate probabilites to the adjunc-tions in G such that the language generated byG has the same distribution as that given by?.
However a function ?
that grows smallerby repeated multiplication as the inverse of anexponential function cannot be matched by anyTAG because of the constant growth property ofTAGs (see (Vijay-Shanker, 1987), p. 104).
Anexample of such a function ?
is a simple Pois-son distribution (2), which in fact was also usedas the counterexample in (Booth and Thomp-son, 1973) for CFGs, since CFGs also have theconstant growth property.1 ?
(anbncndn) = e. n!
(2)This shows that probabilistic TAGs, like CFGs,are constrained in the probabilistic languagesthat they can recognize or learn.
As shownabove, a probabilistic language can fail to havea generating probabilistic TAG.The reverse is also true: some probabilis-tic TAGs, like some CFGs, fail to have acorresponding probabilistic language, i.e.
theyare not consistent.
There are two reasonswhy a probabilistic TAG could be inconsistent:"dirty" grammars, and destructive or incorrectprobability assignments.
"D i r ty"  g rammars .
Usually, when appliedto language, TAGs are lexicalized and so prob-abilities assigned to trees are used only whenthe words anchoring the trees are used in aderivation.
However, if the TAG allows non-lexicalized trees, or more precisely, auxiliarytrees with no yield, then looping adjunctionswhich never generate a string are possible.
How-ever, this can be detected and corrected by asimple search over the grammar.
Even in lexi-calized grammars, there could be some auxiliarytrees that are assigned some probability massbut which can never adjoin into another tree.Such auxiliary trees are termed unreachable andtechniques imilar to the ones used in detectingunreachable productions in CFGs can be usedhere to detect and eliminate such trees.Destructive probability assignments.This problem is a more serious one, and is themain subject of this paper.
Consider the prob-abilistic TAG shown in (3) 2.tl ~1 t2 $2!
S312-o ?
(S1 t2) = 1.o?
($2 ~+ t2) = 0.99-+ ni l)  = 0.01?
($3 ~-+ t2) = 0.98?
($3 ~ nd) = 0.02 (3)Consider a derivation in this TAG as a genera-tive process.
It proceeds as follows: node $1 intl is rewritten as t2 with probability 1.0.
Node$2 in t2 is 99 times more likely than not to berewritten as t2 itself, and similarly node $3 is 49times more likely than not to be rewritten as t2.This however, creates two more instances of $2and $3 with same probabilities.
This continues,creating multiple instances of t2 at each level ofthe derivation process with each instance of t2creating two more instances of itself.
The gram-mar itself is not malicious; the probability as-signments are to blame.
It is important o notethat inconsistency is a problem even though forany given string there are only a finite numberof derivations, all halting.
Consider the prob-ability mass function (pmf) over the set of allderivations for this grammar.
An inconsistentgrammar would have a pmfwhich assigns a largeportion of probability mass to derivations thatare non-terminating.
This means there is a fi-nite probability the generative process can entera generation sequence which has a finite proba-bility of non-termination.4 Cond i t ions  for  Cons is tencyA probabilistic TAG G is consistent if and onlyif:Pr(v) = 1 (4)veLCG)where Pr(v) is the probability assigned to astring in the language.
If a grammar G doesnot satisfy this condition, G is said to be incon-sistent.To explain the conditions under which a prob-abilistic TAG is consistent we will use the TAG2The subscripts are used as a simple notation touniquely refer to the nodes in each elementary tree.
Theyare not part of the node label for purposes of adjunction.1166in (5) as an example.tl ~ t2?
(A1 ~-~ t2) = 0.8?
(A1 ~-+ nil) = 0.2B1 A*IIa2B* a3?
(A2 ~-~ t2) = 0.2 ?
(B2 ~-~ t3) = 0.1?
(A2~+ni l )=0.8  ?
(B2~ni l )=0.9?
(B1 ~+ t3) = 0.2?
(B1 ~-+ nil) = 0.8?
(A3 ~-~ t2) = 0.4?
(A3 ~-~ nil) = 0.6 (5)From this grammar, we compute a square ma-trix A4 which of size IVI, where V is the setof nodes in the grammar that can be rewrit-ten by adjunction.
Each AzIij contains the ex-pected value of obtaining node Xj when nodeXi is rewritten by adjunction at each level of aTAG derivation.
We call Ad the stochastic ex-pectation matrix associated with a probabilisticTAG.To get A4 for a grammar we first write a ma-trix P which has IVI rows and I I U A\[ columns.An element Pij corresponds to the probabilityof adjoining tree tj at node Xi, i.e.
?
(Xi ~'+ t j )  3.t l  t2A1 0 0.8A2 0 0.2P= BI 0 0A3 0 0.4B2 0 0t3000.200.1We then write a matrix N which has \[I U A\[rows and IV\[ columns.
An element Nij is 1.0 ifnode Xj is a node in tree ti.N =A1 A2 B1 A3 B2t 1 \[ 1.0 0 0 0 0 \]t2 \[ 0 1.0 1.0 1.0 0 \] t3 0 0 0 0 1.0Then the stochastic expectation matrix A4 issimply the product of these two matrices.3Note that P is not a row stochastic matrix.
Thisis an important difference in the construction of .h4 forTAGs when compared to CFGs.
We will return to thispoint in ?5..M=P.N=A1A2B1A3B2A1 A2 B1 A3 B20 0.8 0.8 0.8 00 0.2 0.2 0.2 00 0 0 0 0.20 0.4 0.4 0.4 00 0 0 0 0.1By inspecting the values of A4 in terms of thegrammar probabilities indicates that .h4ij con-tains the values we wanted, i.e.
expectation ofobtaining node Aj when node Ai is rewritten byadjunction at each level of the TAG derivationprocess.By construction we have ensured that thefollowing theorem from (Booth and Thomp-son, 1973) applies to probabilistic TAGs.
Aformal justification for this claim is given inthe next section by showing a reduction of theTAG derivation process to a multitype Galton-Watson branching process (Harris, 1963).Theorem 4.1 A probabilistic grammar is con-sistent if the spectral radius p(A4) < 1, where,h,4 is the stochastic expectation matrix com-puted from the grammar.
(Booth and Thomp-son, 1973; Soule, 1974)This theorem provides a way to determinewhether a grammar is consistent.
All we need todo is compute the spectral radius of the squarematrix A4 which is equal to the modulus of thelargest eigenvalue of .
If this value is less thanone then the grammar is consistent 4.
Comput-ing consistency can bypass the computation ofthe eigenvalues for A4 by using the followingtheorem by Ger~gorin (see (Horn and Johnson,1985; Wetherell, 1980)).Theorem 4.2 For any square matrix .h4,p(.M) < 1 if and only if there is an n > 1such that the sum of the absolute values ofthe elements of each row of .M n is less thanone.
Moreover, any n' > n also has this prop-erty.
(GerSgorin, see (Horn and Johnson, 1985;Wetherell, 1980))4The grammar may be consistent when the spectralradius is exactly one, but this case involves many specialconsiderations and is not considered in this paper.
Inpractice, these complicated tests are probably not worththe effort.
See (Harris, 1963) for details on how thisspecial case can be solved.1167This makes for a very simple algorithm tocheck consistency of a grammar.
We sum thevalues of the elements of each row of the stochas-tic expectation matrix A4 computed from thegrammar.
If any of the row sums are greaterthan one then we compute A42, repeat the testand compute :~422 if the test fails, and so on un-til the test succeeds 5.
The algorithm does nothalt ifp(A4) _> 1.
In practice, such an algorithmworks better in the average case since compu-tation of eigenvalues i more expensive for verylarge matrices.
An upper bound can be set onthe number of iterations in this algorithm.
Oncethe bound is passed, the exact eigenvalues canbe computed.For the grammar in (5) we computed the fol-lowing stochastic expectation matrix:0 0.8 0.80 0.2 0.2A4= 0 0 00 0.4 0.40 0 0The first row sum is 2.4.0.8 00.2 00 0.20.4 00 0.1Since the sum ofeach row must be less than one, we compute thepower matrix ,~v/2.
However, the sum of one ofthe rows is still greater than 1.
Continuing wecompute A422 .j~  220 0.1728 0.1728 0.1728 0.06880 0.0432 0.0432 0.0432 0.01720 0 0 0 0.00020 0.0864 0.0864 0.0864 0.03440 0 0 0 0.0001This time all the row sums are less than one,hence p(,~4) < 1.
So we can say that the gram-mar defined in (5) is consistent.
We can confirmthis by computing the eigenvalues for A4 whichare 0, 0, 0.6, 0 and 0.1, all less than 1.Now consider the grammar (3) we had con-sidered in Section 3.
The value of .
?4 for thatgrammar is computed to be:$1 s2 s3 slI0 10 10\].A~(3 ) : $2 0 0.99 0.99$3 0 0.98 0.98SWe compute A422 and subsequently only successivepowers of 2 because Theorem 4.2 holds for any n' > n.This permits us to use a single matrix at each step inthe algorithm.The eigenvalues for the expectation matrixM computed for the grammar (3) are 0, 1.97and 0.
The largest eigenvalue is greater than1 and this confirms (3) to be an inconsistentgrammar.5 TAG Der ivat ions  and  Branch ingProcessesTo show that Theorem 4.1 in Section 4 holdsfor any probabilistic TAG, it is sufficient o showthat the derivation process in TAGs is a Galton-Watson branching process.A Galton-Watson branching process (Harris,1963) is simply a model of processes that haveobjects that can produce additional objects ofthe same kind, i.e.
recursive processes, with cer-tain properties.
There is an initial set of ob-jects in the 0-th generation which produces withsome probability a first generation which in turnwith some probability generates a second, andso on.
We will denote by vectors Z0, Z1, Z2, .
.
.the 0-th, first, second, ... generations.
Thereare two assumptions made about Z0, Z1, Z2,.
.
.
:.
The size of the n-th generation does notinfluence the probability with which any ofthe objects in the (n + 1)-th generation isproduced.
In other words, Z0, Z1,Z2, .
.
.form a Markov chain..
The number of objects born to a parentobject does not depend on how many otherobjects are present at the same level.We can associate a generating function foreach level Zi.
The value for the vector Zn is thevalue assigned by the n-th iterate of this gen-erating function.
The expectation matrix A4 isdefined using this generating function.The theorem attributed to Galton and Wat-son specifies the conditions for the probabilityof extinction of a family starting from its 0-thgeneration, assuming the branching process rep-resents a family tree (i.e, respecting the condi-tions outlined above).
The theorem states thatp(.~4) < 1 when the probability of extinction is11681.0.t lt2 (0)t2 (0) t3 (1) t2 (1.1)I It2 (1.1)t3 (o)BI AA 2 B 2 AB 1 A B a3 alA3 a2 B a3I Ias ASBI AI I,~ asIlevel 0level 1level 2level 3level 4 (6).s (~)The assumptions made about the generatingprocess intuitively holds for probabilistic TAGs.
(6), for example, depicts a derivation of thestring a2a2a2a2a3a3al by a sequence of adjunc-tions in the grammar given in (5) 6.
The parsetree derived from such a sequence is shown inFig.
7.
In the derivation tree (6), nodes in thetrees at each level i axe rewritten by adjunctionto produce a level i + 1.
There is a final level 4in (6) since we also consider the probability thata node is not rewritten further, i.e.
Pr(A ~-~ nil)for each node A.We give a precise statement of a TAG deriva-tion process by defining a generating functionfor the levels in a derivation tree.
Each leveli in the TAG derivation tree then correspondsto Zi in the Maxkov chain of branching pro-6The numbers in parentheses next to the tree namesare node addresses where each tree has adjoined intoits parent.
Recall the definition of node addresses inSection 2.cesses.
This is sufficient o justify the use ofTheorem 4.1 in Section 4.
The conditions onthe probability of extinction then relates to theprobability that TAG derivations for a proba-bilistic TAG will not recurse infinitely.
Hencethe probability of extinction is the same as theprobability that a probabilistic TAG is consis-tent.For each Xj E V, where V is the set of nodesin the grammar where adjunction can occur,we define the k-argument adjunction generating\]unction over variables i , .
.
.
,  Sk correspondingto the k nodes in V.g j (s l , .
.
.
,  8k) =EteAdj(Xj)u{niQ?
(xj t).
k?
*)where, rj (t) = 1 iff node Xj is in tree t, rj (t) = 0otherwise.For example, for the grammar in (5) we getthe following adjunction generating functionstaking the variable sl, s2, 83, 84, 85 to representthe nodes A1, A2, B1, A3, B2 respectively.g1(81 , .
.
.
,85)  =?
(A1 ~"~t2)" 82"83" s4+?
(A1 ~--~nil)g2(81,.. .
,8~)=?
(A2~-~t2) ?
82"83" s4+?(A2~--~nil)g~(81,..
.
,85)=?
(B1 ~-~t3)" 85+?
(B1 ~ni l )g4(81 , .
.
.
,85)=?
(A3~-+t2)  "82"83"844.?
(A3~-+ni l )g5(81,...,s~) =?
(B2~-~t3)" ss+?
(B2~-~nil)The n-th level generating functionGn(sl,...,sk) is defined recursively as fol-lows.G0(81 , .
.
.
,Sk )  = 81Gl(sl, .
.
.
,sk) = gl(sl,...,Sk)G,(s l , .
.
.
,sk)  = G, - l \ [g l (s l , .
.
.
, sk) , .
.
.
,gk(sl,...,Sk)\]For the grammar in (5) we get the followinglevel generating functions.O0(s l , .
.
.
,  85) = 811169GI (S l , .
.
.
,  85) = gl(Sl,..., 85)= ?
(A1 ~-+ t2)" se.
83" 84 + ?
(A1 ~-+ nil)= 0 .8 .s2 .s3 .s4+0.2G2(sl , .
.
.
,85) =?
(A2 ~-+ t2)\[g2(sy,.
.
.
,  85)\]\[g3(81,..., 85)\]\ [g4(81, .
.
.
,  85)\] -\[- ?
(A2 ~ nil)222 222  = 0.0882838485 + 0.03828384 + 0.0482838485 +0.18828384 -t- 0.04s5 + 0.196Examining this example, we can expressGi (s1 , .
.
.
,Sk )  as a sum Di (s l , .
.
.
,Sk )  + Ci,where Ci is a constant and Di(.)
is a polyno-mial with no constant erms.
A probabilisticTAG will be consistent if these recursive qua-tions terminate, i.e.
iffl imi+ooDi(s l ,  .
.
.
, 8k) --+ 0We can rewrite the level generation functions interms of the stochastic expectation matrix Ad,where each element mi, j of .A4 is computed asfollows (cf.
(Booth and Thompson, 1973)).Ogi (81 , .
.
.
, 8k)mi,j = 08j sl,...,sk=l(8)The limit condition above translates to the con-dition that the spectral radius of 34 must beless than 1 for the grammar to be consistent.This shows that Theorem 4.1 used in Sec-tion 4 to give an algorithm to detect inconsis-tency in a probabilistic holds for any given TAG,hence demonstrating the correctness of the al-gorithm.Note that the formulation of the adjunctiongenerating function means that the values for?
(X ~4 nil) for all X E V do not appear inthe expectation matrix.
This is a crucial differ-ence between the test for consistency in TAGsas compared to CFGs.
For CFGs, the expecta-tion matrix for a grammar G can be interpretedas the contribution of each non-terminal to thederivations for a sample set of strings drawnfrom L(G).
Using this it was shown in (Chaud-hari et al, 1983) and (S?nchez and Bened~,1997) that a single step of the inside-outsidealgorithm implies consistency for a probabilis-tic CFG.
However, in the TAG case, the inclu-sion of values for ?
(X ~-+ nil) (which is essen-tim if we are to interpret he expectation ma-trix in terms of derivations over a sample set ofstrings) means that we cannot use the methodused in (8) to compute the expectation matrixand furthermore the limit condition will not beconvergent.6 Conc lus ionWe have shown in this paper the conditionsunder which a given probabilistic TAG can beshown to be consistent.
We gave a simple al-gorithm for checking consistency and gave theformal justification for its correctness.
The re-sult is practically significant for its applicationsin checking for deficiency in probabilistic TAGs.ReferencesT.
L. Booth and R. A. Thompson.
1973.
Applying prob-ability measures to abstract languages.
IEEE Trans-actions on Computers, C-22(5):442-450, May.J.
Carroll and D. Weir.
1997.
Encoding frequency in-formation in lexicalized grammars.
In Proc.
5th Int'lWorkshop on Parsing Technologies IWPT-97, Cam-bridge, Mass.R.
Chaudhari, S. Pham, and O. N. Garcia.
1983.
Solu-tion of an open problem on probabilistic grammars.IEEE Transactions on Computers, C-32(8):748-750,August.T.
E. Harris.
1963.
The Theory of Branching Processes.Springer-Verlag, Berlin.R.
A. Horn and C. R. Johnson.
1985.
Matrix Analysis.Cambridge University Press, Cambridge.A.
K. Joshi and Y. Schabes.
1992.
Tree-adjoining ram-mar and lexicalized grammars.
In M. Nivat andA.
Podelski, editors, Tree automata nd languages,pages 409-431.
Elsevier Science.A.
K. Joshi.
1988.
An introduction to tree adjoininggrammars.
In A. Manaster-Ramer, editor, Mathemat-ics of Language.
John Benjamins, Amsterdam.O.
Rainbow and A. Joshi.
1995.
A formal look at de-pendency grammars and phrase-structure grammars,with special consideration of word-order phenomena.In Leo Wanner, editor, Current Issues in Meaning-Text Theory.
Pinter, London.J.-A.
S?nchez and J.-M. Bened\[.
1997.
Consistency ofstochastic ontext-free grammars from probabilisticestimation based on growth transformations.
IEEETransactions on Pattern Analysis and Machine Intel-ligence, 19(9):1052-1055, September.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoininggrammars.
In Proc.
of COLING '92, volume 2, pages426-432, Nantes, France.S.
Soule.
1974.
Entropies of probabilistic grammars.
Inf.Control, 25:55-74.K.
Vijay-Shanker.
1987.
A Study of Tree AdjoiningGrammars.
Ph.D. thesis, Department of Computerand Information Science, University of Pennsylvania.C.
S. Wetherell.
1980.
Probabilistic languages: A re-view and some open questions.
Computing Surveys,12(4):361-379.1170
