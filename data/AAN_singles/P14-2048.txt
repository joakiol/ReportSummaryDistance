Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289?295,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAutomatic Detection of Machine Translated Text and Translation QualityEstimationRoee AharoniDept.
of Computer ScienceBar Ilan UniversityRamat-Gan, Israel 52900roee.aharoni@gmail.comMoshe KoppelDept.
of Computer ScienceBar Ilan UniversityRamat-Gan, Israel 52900moishk@gmail.comYoav GoldbergDept.
of Computer ScienceBar Ilan UniversityRamat-Gan, Israel 52900yoav.goldberg@gmail.comAbstractWe show that it is possible to automati-cally detect machine translated text at sen-tence level from monolingual corpora, us-ing text classification methods.
We showfurther that the accuracy with which alearned classifier can detect text as ma-chine translated is strongly correlated withthe translation quality of the machinetranslation system that generated it.
Fi-nally, we offer a generic machine transla-tion quality estimation technique based onthis approach, which does not require ref-erence sentences.1 IntroductionThe recent success and proliferation of statisticalmachine translation (MT) systems raise a numberof important questions.
Prominent among theseare how to evaluate the quality of such a systemefficiently and how to detect the output of suchsystems (for example, to avoid using it circularlyas input for refining MT systems).In this paper, we will answer both these ques-tions.
First, we will show that using style-relatedlinguistic features, such as frequencies of parts-of-speech n-grams and function words, it is pos-sible to learn classifiers that distinguish machine-translated text from human-translated or nativeEnglish text.
While this is a straightforward andnot entirely novel result, our main contribution isto relativize the result.
We will see that the suc-cess of such classifiers are strongly correlated withthe quality of the underlying machine translationsystem.
Specifically, given a corpus consisting ofboth machine-translated English text (English be-ing the target language) and native English text(not necessarily the reference translation of themachine-translated text), we measure the accuracyof the system in classifying the sentences in thecorpus as machine-translated or not.
This accu-racy will be shown to decrease as the quality ofthe underlying MT system increases.
In fact, thecorrelation is strong enough that we propose thatthis accuracy measure itself can be used as a mea-sure of MT system quality, obviating the need fora reference corpus, as for example is necessary forBLEU (Papineni et al, 2001).The paper is structured as follows: In the nextsection, we review previous related work.
In thethird section, we describe experiments regardingthe detection of machine translation and in thefourth section we discuss the use of detection tech-niques as a machine translation quality estimationmethod.
In the final section we offer conclusionsand suggestions for future work.2 Previous Work2.1 TranslationeseThe special features of translated texts have beenstudied widely for many years.
Attempts to de-fine their characteristics, often called ?TranslationUniversals?, include (Toury, 1980; Blum-Kulkaand Levenston, 1983; Baker, 1993; Gellerstam,1986).
The differences between native and trans-lated texts found there go well beyond systematictranslation errors and point to a distinct ?Transla-tionese?
dialect.Using automatic text classification methods inthe field of translation studies had many use casesin recent years, mainly as an empirical methodof measuring, proving or contradicting translationuniversals.
Several works (Baroni and Bernar-dini, 2006; Kurokawa et al, 2009; Ilisei et al,2010) used text classification techniques in orderto distinguish human translated text from nativelanguage text at document or paragraph level, us-ing features like word and POS n-grams, propor-tion of grammatical words in the text, nouns, fi-nite verbs, auxiliary verbs, adjectives, adverbs, nu-289merals, pronouns, prepositions, determiners, con-junctions etc.
Koppel and Ordan (2011) classi-fied texts to original or translated, using a listof 300 function words taken from LIWC (Pen-nebaker et al, 2001) as features.
Volanski etal.
(2013) also tested various hypotheses regarding?Translationese?, using 32 different linguistically-informed features, to assess the degree to whichdifferent sets of features can distinguish betweentranslated and original texts.2.2 Machine Translation DetectionRegarding the detection of machine translatedtext, Carter and Inkpen (2012) translated theHansards of the 36th Parliament of Canada us-ing the Microsoft Bing MT web service, andconducted three detection experiments at docu-ment level, using unigrams, average token length,and type-token ratio as features.
Arase andZhou (2013) trained a sentence-level classifier todistinguish machine translated text from humangenerated text on English and Japanese web-pagecorpora, translated by Google Translate, Bing andan in-house SMT system.
They achieved very highdetection accuracy using application-specific fea-ture sets for this purpose, including indicators ofthe ?Phrase Salad?
(Lopez, 2008) phenomenon or?Gappy-Phrases?
(Bansal et al, 2011).While Arase and Zhou (2013) considered MTdetection at sentence level, as we do in this pa-per, they did not study the correlation between thetranslation quality of the machine translated textand the ability to detect it.
We show below thatsuch detection is possible with very high accuracyonly on low-quality translations.
We examine thisdetection accuracy vs. quality correlation, withvarious MT systems, such as rule-based and sta-tistical MT, both commercial and in-house, usingvarious feature sets.3 Detection Experiments3.1 FeaturesWe wish to distinguish machine translated En-glish sentences from either human-translated sen-tences or native English sentences.
Due to thesparseness of the data at the sentence level, weuse common content-independent linguistic fea-tures for the classification task.
Our features arebinary, denoting the presence or absence of eachof a set of part-of-speech n-grams acquired usingthe Stanford POS tagger (Toutanova et al, 2003),as well as the presence or absence of each of 467function words taken from LIWC (Pennebaker etal., 2001).
We consider only those entries that ap-pear at least ten times in the entire corpus, in orderto reduce sparsity in the data.
As our learning al-gorithm we use SVM with sequential minimal op-timization (SMO), taken from the WEKA machinelearning toolkit (Hall et al, 2009).3.2 Detecting Different MT SystemsIn the first experiment set, we explore the abilityto detect outputs of machine translated text fromdifferent MT systems, in an environment contain-ing both human generated and machine translatedtext.
For this task, we use a portion of the Cana-dian Hansard corpus (Germann, 2001), containing48,914 parallel sentences from French to English.We translate the French portion of the corpus usingseveral MT systems, respectively: Google Trans-late, Systran, and five other commercial MT sys-tems available at the http://itranslate4.eu website,which enables to query example MT systems builtby several european MT companies.
After trans-lating the sentences, we take 20,000 sentencesfrom each engine output and conduct the detectionexperiment by labeling those sentences as MT sen-tences, and another 20,000 sentences, which arethe human reference translations, labeled as ref-erence sentences.
We conduct a 10-fold cross-validation experiment on the entire 40,000 sen-tence corpus.
We also conduct the same exper-iment using 20,000 random, non-reference sen-tences from the same corpus, instead of the ref-erence sentences.
Using simple linear regression,we also obtain an R2value (coefficient of deter-mination) over the measurements of detection ac-curacy and BLEU score, for each of three featureset combinations (function words, POS tags andmixed) and the two data combinations (MT vs.reference and MT vs. non reference sentences).The detection and R2results are shown in Table 1.As can be seen, best detection results are ob-tained using the full combined feature set.
It canalso be seen that, as might be expected, it is easierto distinguish machine-translated sentences froma non-reference set than from the reference set.
InFigure 1, we show the relationship of the observeddetection accuracy for each system with the BLEUscore of that system.
As is evident, regardlessof the feature set or non-MT sentences used, thecorrelation between detection accuracy and BLEU29010 20 3060708090BLEUdetectionaccuracy(%)mix-nrmix-rfw-nrfw-rpos-nrpos-rFigure 1: Correlation between detection accu-racy and BLEU score on commercial MT systems,using POS, function words and mixed featuresagainst reference and non-reference sentences.score is very high, as we can also see from the R2values in Table 1.3.3 In-House SMT SystemsParallel Monolingual BLEUSMT-1 2000k 2000k 28.54SMT-2 1000k 1000k 27.76SMT-3 500k 500k 29.18SMT-4 100k 100k 23.83SMT-5 50k 50k 24.34SMT-6 25k 25k 22.46SMT-7 10k 10k 20.72Table 3: Details for Moses based SMT systemsIn the second experiment set, we test our de-tection method on SMT systems we created, inwhich we have control over the training data andthe expected overall relative translation quality.
Inorder to do so, we use the Moses statistical ma-chine translation toolkit (Koehn et al, 2007).
Totrain the systems, we take a portion of the Europarlcorpus (Koehn, 2005), creating 7 different SMTsystems, each using a different amount of train-ing data, for both the translation model and lan-guage model.
We do this in order to create dif-ferent quality translation systems, details of whichare described in Table 3.
For purposes of classifi-cation, we use the same content independent fea-tures as in the previous experiment, based on func-20 22 24 26 28 307273747576BLEUdetectionaccuracy(%)R2= 0.789Figure 2: Correlation between detection accu-racy and BLEU score on in-house Moses-basedSMT systems against non-reference sentences us-ing content independent features.tion words and POS tags, again with SMO-basedSVM as the classifier.
For data, we use 20,000 ran-dom, non reference sentences from the Hansardcorpus, against 20,000 sentences from one MTsystem per experiment, again resulting in 40,000sentence instances per experiment.
The relation-ship between the detection results for each MTsystem and the BLEU score for that system, re-sulting in R2= 0.774, is shown in Figure 2.4 Machine Translation Evaluation4.1 Human Evaluation ExperimentsAs can be seen in the above experiments, there isa strong correlation between the BLEU score andthe MT detection accuracy of our method.
In fact,results are linearly and negatively correlated withBLEU, as can be seen both on commercial systemsand our in-house SMT systems.
We also wish toconsider the relationship between detection accu-racy and a human quality estimation score.
Todo this, we use the French-English data from the8th Workshop on Statistical Machine Translation- WMT13?
(Bojar et al, 2013), containing out-puts from 13 different MT systems and their hu-man evaluations.
We conduct the same classifi-cation experiment as above, with features basedon function words and POS tags, and SMO-basedSVM as the classifier.
We first use 3000 refer-291Features Data Google Moses Systran ProMT Linguatec Skycode Trident R2mixed MT/non-ref 63.34 72.02 72.36 78.2 79.57 80.9 89.36 0.946mixed MT/ref 59.51 69.47 69.77 75.86 78.11 79.24 88.85 0.944func.
w. MT/non-ref 60.43 69.17 69.87 69.78 71.38 75.46 84.97 0.798func.
w. MT/ref 57.27 66.05 67.48 67.06 68.58 73.37 84.79 0.779POS MT/non-ref 60.32 64.39 66.61 73 73.9 74.33 79.6 0.978POS MT/ref 57.21 65.55 64.12 70.29 73.06 73.04 78.84 0.948Table 1: Classifier performance, including the R2coefficient describing the correlation with BLEU.MT Engine ExampleGoogle Translate ?These days, all but one were subject to a vote,and all had a direct link to the post September 11th.
?Moses ?these days , except one were the subject of a vote ,and all had a direct link with the after 11 September .
?Systran ?From these days, all except one were the object of a vote,and all were connected a direct link with after September 11th.
?Linguatec ?Of these days, all except one were making the object of a voteand all had a straightforward tie with after September 11.?ProMT ?These days, very safe one all made object a vote,and had a direct link with after September 11th.
?Trident ?From these all days, except one operated object voting,and all had a direct rope with after 11 septembre.
?Skycode ?In these days, all safe one made the object in a voteand all had a direct connection with him after 11 of September.
?Table 2: Outputs from several MT systems for the same source sentence (function words marked in bold)0.3 0.4 0.5 0.658606264human evaluation scoredetectionaccuracy(%)R2= 0.774Figure 3: Correlation between detection accuracyand human evaluation scores on systems fromWMT13?
against reference sentences.0.3 0.4 0.5 0.67374757677human evaluation scoredetectionaccuracy(%)R2= 0.556Figure 4: Correlation between detection accu-racy and human evaluation scores on systems fromWMT 13?
against non-reference sentences.2920.3 0.4 0.5 0.6626466human evaluation scoredetectionaccuracy(%)R2= 0.829Figure 5: Correlation between detection accu-racy and human evaluation scores on systems fromWMT 13?
against non-reference sentences, usingthe syntactic CFG features described in section 4.2ence sentences from the WMT13?
English refer-ence translations, against the matching 3000 out-put sentences from one MT system at a time, re-sulting in 6000 sentence instances per experiment.As can be seen in Figure 3, the detection accuracyis strongly correlated with the evaluations scores,yielding R2= 0.774.
To provide another mea-sure of correlation, we compared every pair ofdata points in the experiment to get the proportionof pairs ordered identically by the human evalu-ators and our method, with a result of 0.846 (66of 78).
In the second experiment, we use 3000random, non reference sentences from the new-stest 2011-2012 corpora published in WMT12?
(Callison-Burch et al, 2012) against 3000 outputsentences from one MT system at a time, again re-sulting in 6000 sentence instances per experiment.While applying the same classification method aswith the reference sentences, the detection accu-racy rises, while the correlation with the transla-tion quality yields R2= 0.556, as can be seen inFigure 4.
Here, the proportion of identically or-dered pairs is 0.782 (61 of 78).4.2 Syntactic FeaturesWe note that the second leftmost point in Figures3, 4 is an outlier: that is, our method has a hardtime detecting sentences produced by this systemalthough it is not highly rated by human evalu-ators.
This point represents the Joshua (Post etal., 2013) SMT system.
This system is syntax-based, which apparently confound our POS andFW-based classifier, despite it?s low human evalu-ation score.
We hypothesize that the use of syntax-based features might improve results.
To ver-ify this intuition, we create parse trees using theBerkeley parser (Petrov and Klein, 2007) and ex-tract the one-level CFG rules as features.
Again,we represent each sentence as a boolean vector,in which each entry represents the presence or ab-sence of the CFG rule in the parse-tree of the sen-tence.
Using these features alone, without the FWand POS tag based features presented above, weobtain an R2= 0.829 with a proportion of iden-tically ordered pairs at 0.923 (72 of 78), as shownin Figure 5.5 Discussion and Future WorkWe have shown that it is possible to detect ma-chine translation from monolingual corpora con-taining both machine translated text and humangenerated text, at sentence level.
There is a strongcorrelation between the detection accuracy thatcan be obtained and the BLEU score or the humanevaluation score of the machine translation itself.This correlation holds whether or not a referenceset is used.
This suggests that our method might beused as an unsupervised quality estimation methodwhen no reference sentences are available, suchas for resource-poor source languages.
Furtherwork might include applying our methods to otherlanguage pairs and domains, acquiring word-levelquality estimation or integrating our method ina machine translation system.
Furthermore, ad-ditional features and feature selection techniquescan be applied, both for improving detection ac-curacy and for strengthening the correlation withhuman quality estimation.AcknowledgmentsWe would like to thank Noam Ordan and ShulyWintner for their help and feedback on the earlystages of this work.
This research was funded inpart by the Intel Collaborative Research Institutefor Computational Intelligence.293ReferencesYuki Arase and Ming Zhou.
2013.
Machine transla-tion detection from monolingual web-text.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1597?1607, Sofia, Bulgaria, August.Association for Computational Linguistics.Mona Baker.
1993.
Corpus linguistics and transla-tion studies: Implications and applications.
Text andtechnology: in honour of John Sinclair, 233:250.Mohit Bansal, Chris Quirk, and Robert C. Moore.2011.
Gappy phrasal alignment by agreement.
InDekang Lin, Yuji Matsumoto, and Rada Mihalcea,editors, ACL, pages 1308?1317.
The Association forComputer Linguistics.Marco Baroni and Silvia Bernardini.
2006.
A newapproach to the study of translationese: Machine-learning the difference between original and trans-lated text.
LLC, 21(3):259?274.Shoshana Blum-Kulka and Eddie A. Levenston.
1983.Universals of lexical simplification.
Strategies in In-terlanguage Communication, pages 119?139.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Dave Carter and Diana Inkpen.
2012.
Searchingfor poor quality machine translated text: Learningthe difference between human writing and machinetranslations.
In Leila Kosseim and Diana Inkpen,editors, Canadian Conference on AI, volume 7310of Lecture Notes in Computer Science, pages 49?60.Springer.Martin Gellerstam.
1986.
Translationese in swedishnovels translated from english.
In Lars Wollinand Hans Lindquist, editors, Translation Studies inScandinavia, pages 88?95.Ulrich Germann.
2001.
Aligned hansards of the 36thparliament of canada release 2001-1a.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, andRuslan Mitkov.
2010.
Identification of transla-tionese: A machine learning approach.
In Alexan-der F. Gelbukh, editor, CICLing, volume 6008 ofLecture Notes in Computer Science, pages 503?511.Springer.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL.
The Association for Computer Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit, pages 79?86, Phuket, Thailand.
AAMT, AAMT.Moshe Koppel and Noam Ordan.
2011.
Translationeseand its dialects.
In Dekang Lin, Yuji Matsumoto,and Rada Mihalcea, editors, ACL, pages 1318?1326.The Association for Computer Linguistics.David Kurokawa, Cyril Goutte, and Pierre Isabelle.2009.
Automatic Detection of Translated Text andits Impact on Machine Translation.
In ConferenceProceedings: the twelvth Machine Translation Sum-mit.Adam Lopez.
2008.
Statistical machine translation.ACM Computing Surveys (CSUR), 40(3):8.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Technical report,IBM Research Report.J.W.
Pennebaker, M.E.
Francis, and R.J. Booth.
2001.Linguistic inquiry and word count: Liwc 2001.Mahway: Lawrence Erlbaum Associates.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Proceedings of the Main Confer-ence, pages 404?411.Marius Popescu.
2011.
Studying translationese atthe character level.
In Galia Angelova, KalinaBontcheva, Ruslan Mitkov, and Nicolas Nicolov, ed-itors, RANLP, pages 634?639.
RANLP 2011 Organ-ising Committee.Matt Post, Juri Ganitkevitch, Luke Orland, JonathanWeese, Yuan Cao, and Chris Callison-Burch.
2013.Joshua 5.0: Sparser, better, faster, server.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, August 8-9, 2013., pages 206?212.
Association for Computational Linguistics.Gideon Toury.
1980.
In Search of a Theory of Transla-tion.294Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In IN PROCEEDINGS OF HLT-NAACL, pages 252?259.Hans van Halteren.
2008.
Source language markersin europarl translations.
In Donia Scott and HansUszkoreit, editors, COLING, pages 937?944.Vered Volansky, Noam Ordan, and Shuly Wintner.2013.
On the features of translationese.
Literaryand Linguistic Computing.295
