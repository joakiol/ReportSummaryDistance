Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 567?572,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDependency-based Gated Recursive Neural Networkfor Chinese Word SegmentationJingjing Xu and Xu SunMOE Key Laboratory of Computational Linguistics, Peking UniversitySchool of Electronics Engineering and Computer Science, Peking University{xujingjing, xusun}@pku.edu.cnAbstractRecently, many neural network modelshave been applied to Chinese word seg-mentation.
However, such models focusmore on collecting local information whilelong distance dependencies are not welllearned.
To integrate local features withlong distance dependencies, we propose adependency-based gated recursive neuralnetwork.
Local features are first collect-ed by bi-directional long short term mem-ory network, then combined and refined tolong distance dependencies via gated re-cursive neural network.
Experimental re-sults show that our model is a competitivemodel for Chinese word segmentation.1 IntroductionWord segmentation is an important pre-processstep in Chinese language processing.
Most wide-ly used approaches treat Chinese word segmenta-tion (CWS) task as a sequence labeling problem inwhich each character in the input sequence is as-signed with a tag.
Many previous approaches havebeen effectively applied to CWS problem (Laf-ferty et al, 2001; Xue and Shen, 2003; Sun etal., 2012; Sun, 2014; Sun et al, 2013; Cheng etal., 2015).
However, these approaches incorpo-rated many handcrafted features, thus restrictingthe generalization ability of these models.
Neuralnetwork models have the advantage of minimiz-ing the effort in feature engineering.
Collobert etal.
(2011) developed a general neural network ar-chitecture for sequence labeling tasks.
Followingthis work, neural network approaches have beenwell studied and widely applied to CWS task withgood results (Zheng et al, 2013; Pei et al, 2014;Ma and Hinrichs, 2015; Chen et al, 2015).??
?
?
??
?
??
?
?
??
?
?
??
?The ground is covered with thick snow ?
?This area is really not small.
?Figure 1: An illustration for the segmentation am-biguity.
The character ???
is labeled as ?E?
(endof word) in the top sentence while labeled as ?B?
(begin of word) in the bottom one even though???
has the same adjacent characters, ???
and??
?.However, these models focus more on collect-ing local features while long distance dependen-cies are not well learned.
In fact, relying on theinformation of adjacent words is not enough forCWS task.
An example is shown in Figure 1.
Thecharacter ???
has different tags in two sentences,even with the same adjacent characters, ???
and?
??.
Only long distance dependencies can helpthe model recognize tag correctly in this example.Thus, long distance information is an importan-t factor for CWS task.The main limitation of chain structure for se-quence labeling is that long distance dependenciesdecay inevitably.
Though forget gate mechanis-m is added, it is difficult for bi-directional longshort term memory network (Bi-LSTM), a kind ofchain structure, to avoid this problem.
In general,tree structure works better than chain structure tomodel long term information.
Therefore, we usegated recursive neural network (GRNN) (Chen etal., 2015) which is a kind of tree structure to cap-ture long distance dependencies.Motivated by the fact, we propose thedependency-based gated recursive neural network(DGRNN) to integrate local features with long dis-567tance dependencies.
Figure 2 shows the structureof DGRNN.
First of all, local features are col-lected by Bi-LSTM.
Secondly, GRNN recursive-ly combines and refines local features to capturelong distance dependencies.
Finally, with the helpof local features and long distance dependencies,our model generates the probability of the tag ofword.The main contributions of the paper are as fol-lows:?
We present the dependency-based gated re-cursive neural network to combine local fea-tures with long distance dependencies.?
To verify the effectiveness of the proposedapproach, we conduct experiments on threewidely used datasets.
Our proposed modelachieves the best performance compared withother state-of-the-art approaches.2 Dependency-based Gated RecursiveNeural NetworkIn order to capture local features and long distancedependencies, we propose dependency-based gat-ed recursive neural network.
Figure 2 illustratesthe structure of the model.?????
?Window Context ?This area is really not small.?
?
(Ci-2) ?
(Ci-1) ?
(Ci) ?
(Ci+1) ?
(Ci+2)Layer 1Layer 2 CellOutput LayerCell Cell CellLayer 3 Cell Cell CellFigure 2: Architecture of DGRNN for ChineseWord Segmentation.
Cell is the basic unit of GRN-N.2.1 Collect Local FeaturesWe use bi-directional long short term memory(Bi-LSTM) with single layer to collect local fea-tures.
Bi-LSTM is composed of two directionaltanh sig sigtanhf(t)h(t)s(t)i(t)s(t-1)sigo(t)x(t) , h(t-1)Figure 3: Structure of LSTM unit.
The behaviorof the LSTM cell is controlled by three ?gates?,namely input gate i(t), forget gate f(t)and outputgate o(t).long short term memory networks with single lay-er, which can model word representation with con-text information.
Figure 3 shows the calculationprocess of LSTM.
The behavior of LSTM cell iscontrolled by three ?gates?, namely input gate i(t),forget gate f(t)and output gate o(t).
The inputof LSTM cell are x(t), s(t?1)and h(t?1).
x(t)isthe character embeddings of input sentence.
s(t?1)and h(t?1)stand for the state and output of the for-mer LSTM cell, respectively.
The core of the L-STM model is s(t), which is computed using theformer state of cell and two gates, i(t)and f(t).
Inthe end, the output of LSTM cell h(t)is calculatedmaking use of s(t)and o(t).2.2 Refine Long Distance DependenciesGRNN recursively combines and refines local fea-tures to capture long distance dependencies.
Thestructure of GRNN is like a binary tree, where ev-ery two continuous vectors in a sentence is com-bined to form a new vector.
For a sequence s withlength n, there are n layers in total.
Figure 4 showsthe calculation process of GRNN cell.
The core ofGRNN cell are two kinds of gates, reset gates, rL,rR, and update gates z. Reset gates control howto adjust the proportion of the input hi?1and hi,which results to the current new activation h?.
Bythe update gates, the activation of an output neu-ron can be regarded as a choice among the currentnew activation h?, the left child hi?1and the rightchild hi.2.3 Loss FunctionFollowing the work of Pei et al (2014), we adop-t the max-margin criterion as loss function.
Foran input sentence c[1:n]with a tag sequence t[1:n],a sentence-level score is given by the sum of net-568hi hi-1h?YrL rRZFigure 4: The structure of GRNN cell.work scores:s(c[1:n], t[1:n], ?)
=n?i=1f?
(ti|c[i?2:i+2]) (1)where s(c[1:n], t[1:n], ?)
is the sentence-level score.n is the length of c[1:n].
f?
(ti|c[i?2:i+2]) is the s-core output for tag tiat the ithcharacter by thenetwork with parameters ?.We define a structured margin loss ?
(yi, y?)
forpredicting a tag sequence y?
and a given correcttag sequence yi:?
(yi, y?)
=n?j=1?1{yi,j?= yi} (2)where ?
is a discount parameter.
This leads to theregularized objective function for m training ex-amples:J(?)
=1mm?i=1li(?)
+?2???2(3)li(?)
= maxy?
?Y (xi)((s(xi, y?, ?
)+ ?
(yi, y?))
?
s(xi, yi, ?))
(4)where J(?)
is a loss function with parameters ?.?
is regularization factor.
By minimizing this ob-ject, the score of the correct tag sequence yiis in-creased and score of the highest scoring incorrecttag sequence y?
is decreased.2.4 Amplification Gate and TrainingA direct adaptive method for faster backpropaga-tion learning method (RPROP) (Riedmiller andBraun, 1993) was a practical adaptive learningmethod to train large neural networks.
We usemini-batch version RPROP (RMSPROP) (Hinton,2012) to minimize the loss function.Intuitively, extra hidden layers are able to im-prove accuracy performance.
However, it is com-mon that extra hidden layers decrease classifica-tion accuracy.
This is mainly because extra hiddenlayers lead to the inadequate training of later lay-ers due to the vanishing gradient problem.
Thisproblem will decline the utilization of local andlong distance information in our model.
To over-come this problem, we propose a simple ampli-fication gate mechanism which appropriately ex-pands the value of gradient while not changing thedirection.Higher amplification may not always perfor-m better while lower value may bring about theunsatisfied result.
Therefore, the amplificationgate must be carefully selected.
Large magnifi-cation will cause expanding gradient problem.
Onthe contrary, small amplification gate will hardlyreach the desired effect.
Thus, we introduce thethreshold mechanism to guarantee the robustnessof the algorithm, where gradient which is greaterthan threshold will not be expanded.
Amplifica-tion gate of difference layer is distinct.
For everysample, the training procedure is as follows.First, recursively calculate mtand vtwhich de-pend on the gradient of time t?
1 or the square ofgradient respectively.
?1and ?2aim to control theimpact of last state.mt= ?1?
mt?1+ (1 ?
?1) ?
gt(5)vt= ?2?
vt?1+ (1 ?
?2) ?
g2t(6)Second, calculate ?W (t) based on vtandsquare of mt.
?
and ?
are smooth parameters.M(w, t) = vt?
m2t(7)?W (t) =?gt,i?M(w, t) + ?
(8)Third, update weight based on the amplificationgate and ?W (t).
The parameter update for the ithparameter for the ?t,iat time step t with amplifi-cation gate ?
is as follows:?t,i= ?t,i?
?
?W (t) (9)5690 5 10 150.920.930.940.95EpochF?scoreDGRNNDGRNN+AG(a) PKU0 5 10 150.930.940.950.96EpochF?scoreDGRNNDGRNN+AG(b) MSRA0 5 10 150.910.920.930.940.95EpochF?scoreDGRNNDGRNN+AG(c) CTB6Figure 5: Results for DGRNN with amplification gate (AG) on three development datasets.3 Experiments3.1 Data and SettingsWe evaluate our proposed approach on threedatasets, PKU, MSRA and CTB6.
The PKU andMSRA data both are provided by the second In-ternational Chinese Word Segmentation Bakeof-f (Emerson, 2005) and CTB6 is from ChineseTreeBank 6.01(Xue et al, 2005).
We randomlydivide the whole training data into the 90% sen-tences as training set and the rest 10% sentencesas development set.
All datasets are preprocessedby replacing the Chinese idioms and the continu-ous English characters.
The character embeddingsare pre-trained on unlabeled data, Chinese Giga-word corpus2.
We use MSRA dataset to prepro-cess model weights before training on CTB6 andPKU datasets.Following previous work and our experimen-tal results, hyper parameters configurations are setas follows: minibatch size n = 16, window sizew = 5, character embedding size d1= 100, am-plification gate range ?
= [0, 4] and margin lossdiscount ?
= 0.2.
All weight matrixes are diag-onal matrixes and randomly initialized by normaldistribution.3.2 Experimental Results and DiscussionsWe first compare our model with baseline meth-ods, Bi-LSTM and GRNN on three datasets.
Theresults evaluated by F-score (F1score) are report-ed in Table 1.?
Bi-LSTM.
First, the output of Bi-LSTM isconcatenated to a vector.
Second, softmaxlayer takes the vector as input and generateseach tag probability.1https://catalog.ldc.upenn.edu/LDC2007T362https://catalog.ldc.upenn.edu/LDC2003T09Model (Unigram) PKU MSRA CTB6Bi-LSTM 95.0 95.8 95.2GRNN 95.8 96.2 95.5Pei et al (2014) 94.0 94.9 *Chen et al (2015) 96.1 96.2 95.6DGRNN 96.1 96.3 95.8Table 1: Comparisons for DGRNN and other neu-ral approaches based on traditional unigram em-beddings.Model PKU MSRA CTB6Zhang et al (2006) 95.1 97.1 *Zhang et al (2007) 94.5 97.2 *Sun et al (2009) 95.2 97.3 *Sun et al (2012) 95.4 97.4 *Zhang et al (2013) 96.1 97.4 *DGRNN 96.1 96.3 95.8Table 2: Comparisons for DGRNN and state-of-the-art non-neural network approaches on F-score.?
GRNN.
The structure of GRNN is recursive.GRNN combines adjacent word vectors tothe more abstract representation in bottom-upway.Furthermore, we conduct experiments with am-plification gate on three development datasets.Figure 5 shows that amplification gate significant-ly increases F-score on three datasets.
Amplifi-cation even achieves 0.9% improvement on CTB6dataset.
It is demonstrated that amplification gateis an effective mechanism.We compare our proposed model with previ-ous neural approaches on PKU, MSRA and CT-B6 test datasets.
Experimental results are report-ed in Table 1.
It can be clearly seen that ourapproach achieves the best results compared with570Dataset Model ResultMSRABi-LSTM t = 5.94, p < 1 ?
10?4GRNN t = 1.22, p = 0.22PKUBi-LSTM t = 15.54, p < 1 ?
10?4GRNN t = 4.43, p < 1 ?
10?4CTB6Bi-LSTM t = 5.01, p < 1 ?
10?4GRNN t = 2.55, p = 2.48 ?
10?2Table 3: The t-test results for DGRNN and base-lines.other neural networks on traditional unigram em-beddings.
It is possible that bigram embeddingsmay achieve better results.
With the help of bi-gram embeddings, Pei et al (2014) can achieve95.2% and 97.2% F-scores on PKU and MSRAdatasets and Chen et al (2015) can achieve 96.4%,97.6% and 95.8% F-scores on PKU, MSRA andCTB6 datasets.
However, performance varies a-mong these bigram models since they have dif-ferent ways of involving bigram embeddings.
Be-sides, the training speed would be very slow afteradding bigram embeddings.
Therefore, we onlycompare our model on traditional unigram embed-dings.We also compare DGRNN with other state-of-the-art non-neural networks, as shown in Table 2.Chen et al (2015) implements the work of Sunand Xu (2011) on CTB6 dataset and achieves95.7% F-score.
We achieve the best result on P-KU dataset only with unigram embeddings.
Theexperimental results show that our model is a com-petitive model for Chinese word segmentation.3.3 Statistical Significance TestsWe use the t-test to intuitively show the improve-ment of DGRNN over baselines.
According to theresults shown in Table 3, we can draw a conclu-sion that, by conventional criteria, this improve-ment is considered to be statistically significantbetween DGRNNwith baselines, except for GRN-N approach on MSRA dataset.4 ConclusionsIn this work, we propose dependency-based recur-sive neural network to combine local features withlong distance dependencies, which achieves sub-stantial improvement over the state-of-the-art ap-proaches.
Our work indicates that long distancedependencies can improve the performance of lo-cal segmenter.
In the future, we will study alterna-tive ways of modeling long distance dependencies.5 AcknowledgmentsWe thank Xiaoyan Cai for her valuable sug-gestions.
This work was supported in part byNational Natural Science Foundation of China(No.
61300063), National High Technology Re-search and Development Program of China (863Program, No.
2015AA015404), and DoctoralFund of Ministry of Education of China (No.20130001120004).
Xu Sun is the correspondingauthor.ReferencesXinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuan-jing Huang.
2015.
Gated recursive neural networkfor chinese word segmentation.
In ACL (1), pages1744?1753.
The Association for Computer Linguis-tics.Fei Cheng, Kevin Duh, and Yuji Matsumoto.
2015.Synthetic word parsing improves chinese word seg-mentation.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (Volume 2: Short Papers),pages 262?267, Beijing, China, July.
Association forComputational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHANWorkshop on Chinese LanguageProcessing, pages 123?133.G.
Hinton.
2012.
Lecture 6.5: rmsprop: divide the gra-dient by a running average of its recent magnitude.coursera: Neural networks for machine learning.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, num-ber 8 in ICML ?01, pages 282?289, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Jianqiang Ma and Erhard Hinrichs.
2015.
Accuratelinear-time chinese word segmentation via embed-ding matching.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 1733?1743, Beijing, China, July.
As-sociation for Computational Linguistics.571Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 293?303, Bal-timore, Maryland, June.
Association for Computa-tional Linguistics.Martin Riedmiller and Heinrich Braun.
1993.
Adirect adaptive method for faster backpropagationlearning: The rprop algorithm.
In IEEE INTERNA-TIONAL CONFERENCE ON NEURAL NETWORK-S, pages 586?591.Weiwei Sun and Jia Xu.
2011.
Enhancing chi-nese word segmentation using unlabeled data.
InConference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2011, 27-31 July 2011,John Mcintyre Conference Centre, Edinburgh, Uk,A Meeting of Sigdat, A Special Interest Group of theACL, pages 970?979.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable chinese segmenter withhybrid word/character information.
In Proceedingsof Human Language Technologies: The 2009 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 56?64, Boulder, Colorado, June.
Associationfor Computational Linguistics.Xu Sun, HoufengWang, andWenjie Li.
2012.
Fast on-line training with frequency-adaptive learning ratesfor chinese word segmentation and new word detec-tion.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 253?262, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Xu Sun, Yao zhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2013.
Prob-abilistic chinese word segmentation with non-localinformation and stochastic training.
Inf.
Process.Manage., 49(3):626?636.Xu Sun.
2014.
Structure regularization for structuredprediction.
In Advances in Neural Information Pro-cessing Systems 27, pages 2402?2410.N.
Xue and L. Shen.
2003.
Chinese Word Segmen-tation as LMR Tagging.
In Proceedings of the 2ndSIGHAN Workshop on Chinese Language Process-ing.Naiwen Xue, Fei Xia, Fu-dong Chiou, and MartaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238, June.Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-based perceptron algorithm.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 840?847, Prague, Czech Republic, June.
Association forComputational Linguistics.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-ta.
2006.
Subword-based tagging by condition-al random fields for chinese word segmentation.In Proceedings of the Human Language Technolo-gy Conference of the NAACL, Companion Volume:Short Papers, NAACL-Short ?06, pages 193?196,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013.
Exploring representations from un-labeled data with co-training for chinese word seg-mentation.
In EMNLP, pages 311?321.
ACL.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for chinese word segmentationand pos tagging.
In EMNLP, pages 647?657.
ACL.572
