The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 174?179,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsAn Interactive Analytic Tool for Peer-Review ExplorationWenting Xiong1,2, Diane Litman1,2, Jingtao Wang1,2 and Christian Schunn21 Department of Computer Science & 2 Learning Research and Development CenterUniversity of Pittsburgh, Pittsburgh, PA, 15260wex12@cs.pitt.eduAbstractThis paper presents an interactive analytic toolfor educational peer-review analysis.
It em-ploys data visualization at multiple levels ofgranularity, and provides automated analyticsupport using clustering and natural languageprocessing.
This tool helps instructors dis-cover interesting patterns in writing perfor-mance that are reflected through peer reviews.1 IntroductionPeer review is a widely used educational approachfor coaching writing in many domains (Topping,1998; Topping, 2009).
Because of the large numberof review comments to examine, instructors givingpeer review assignments find it difficult to examinepeer comments.
While there are web-based peer-review systems that help instructors set up peer-review assignments, no prior work has been doneto support instructors?
comprehension of the textualreview comments.To address this issue, we have designed and devel-oped an interactive analytic interface (RevExplore)on top of SWoRD1 (Cho and Schunn, 2007), a web-based peer-review reciprocal system that has beenused by over 12,000 students over the last 8 years.In this paper, we show how RevExplore visualizespeer-review information in multiple dimensions andvarious granularity levels to support investigativeexploration, and applies natural language process-ing (NLP) techniques to facilitate review compre-hension and comparison.1https://sites.google.com/site/swordlrdc/2 Design GoalsInstructors face challenges when they try to makesense of the peer-review data collected by SWoRDfor their assignments.
Instructors we have inter-viewed have complained that peer reviews are time-consuming to read and almost ?impossible?
to in-terpret: 1) to understand the pros and cons of onestudent?s paper, they need to synthesize all the peerreviews received by that student by reading them oneby one; 2) furthermore, if instructors would like todiscover general patterns regarding students?
writ-ing performance, they have to additionally comparepeer reviews across multiple students which requirestheir simultaneously remembering various opinionsfor many students; 3) in the initial stage of peer re-view analysis, instructors have no clear idea of whatpotential patterns they should be looking for (?coldstart?
).These challenges motivate our design of RevEx-plore, a peer-review analytic tool that is a pluginto SWoRD.
We set our design goals to address thechallenges mentioned above, respectively: 1) cre-ate a simple and informative representation of peer-review data which automatically aggregates peer-reviews at the level of student; 2) provide intelligentsupport of text mining and semantic abstraction forthe purpose of comparison; 3) enable an overview ofkey characteristics of peer reviews for initial explo-ration.To fulfill our design goals, we design an inter-active visualization system to ease the explorationprocess, following the pattern of overview plus de-tail (Card et al, 1999).
In the overview, RevExplore174provides a high level of visualization of overall peer-review information at the student level for initial ex-ploration.
In the detail-view, RevExplore automati-cally abstracts the semantic information of peer re-views at the topic-word level, with the original textsvisible on demand.
In addition, we introduce clus-tering and NLP techniques to support automated an-alytics.3 Related WorkOne major goal of peer review studies in educationalresearch is to understand how to better improve stu-dent learning, directly or indirectly.
Empirical stud-ies of textual review comments based on manualcoding have discovered that certain review features(e.g., whether the solution to a problem is explicitlystated in a comment) can predict both whether theproblem will be understand and the feedback imple-mented (Nelson and Schunn, 2009).
Our previousstudies used machine learning and NLP techniquesto automatically identify the presence of such usefulfeatures in review comments (Xiong et al, 2010);similar techniques have also been used to determinereview comment helpfulness (Xiong and Litman,2011; Cho, 2008).
With respect to paper analysis,Sa?ndor and Vorndran (2009) used NLP to highlightkey sentences, in order to focus reviewer attentionon important paper aspects.
Finally, Giannoukos etal.
(2010) focused on peer matching based on stu-dents?
profile information to maximize learning out-comes, while Crespo Garcia and Pardo (2010) ex-plored the use of document clustering to adaptivelyguide the assignment of papers to peers.
In contrastto the prior work above, the research presented hereis primarily motivated by the needs of instructors,instead of the needs of students.
In particular, thegoal of RevExplore is to utilize the information inpeer reviews and papers, to help instructors betterunderstand student performance in the peer-reviewassignments for their courses.Many computer tools have already been de-veloped to support peer review activities in var-ious types of classrooms, from programmingcourses (Hyyrynen et al, 2010) to courses involvingwriting in the disciplines (Nelson and Schunn, 2009;Yang, 2011).
Within the writing domain, systemssuch as SWoRD (Cho and Schunn, 2007) mainly as-sist instructors by providing administrative manage-ment support and/or (optional) automatic gradingservices.
While peer review systems especially de-signed for instructors do exist, their goal is typicallyto create a collaborative environment for instructorsto improve their professional skills (Fu and Hawkes,2010).
In terms of artificial intelligence support, toour knowledge no current peer review system has thepower to provide instructors with insights about thesemantic content of peer reviews, due to the diver-sity and complexity of the textual review comments.For example, SWoRD currently provides teachers anumerical summary view that includes the numberof reviews received for each paper, and the meanand standard deviation of numerical reviewing rat-ings for each paper.
SWoRD also allows instruc-tors to automatically compute a grade based on astudent?s writing and reviewing quality; the gradingalgorithm uses the numerical ratings but not the as-sociated text comments.
In this work, we attemptedto address the lack of semantic insight both by hav-ing humans in the loop to identify points of interestfor interactive data exploration, and by adapting ex-isting natural language processing techniques to thepeer review domain to support automated analytics.4 RevExploreAs an example for illustration, we will use data col-lected in a college level history class (Nelson andSchunn, 2009): the instructor created the writingassignment through SWoRD and provided a peer-review rubric which required students to assess ahistory paper?s quality on three dimensions (logic,flow and insight) separately, by giving a numericrating on a scale of 1-7 in addition to textual com-ments.
While reviewing dimensions and associatedguidelines (see below) are typically created by an in-structor for a particular assignment, instructors canalso set up their rubric using a library provided bySWoRD.For instance, the instructor created the followingguidance for commenting on the ?logic?
dimension:?Provide specific comments about the logic of theauthor?s argument.
If points were just made withoutsupport, describe which ones they were.
If the sup-port provided doesn?t make logical sense, explainwhat that is.
If some obvious counter-argument was175not considered, explain what that counter-argumentis.
Then give potential fixes to these problems if youcan think of any.
This might involve suggesting thatthe author change their argument.
?Instructor guidance for numerically rating the log-ical arguments of the paper based on the commentswas also given.
For this history assignment, thehighest rating of 7 (?Excellent?)
was described as?All arguments strongly supported and no logicalflaws in the arguments.?
The lowest rating of 1(?Disastrous?)
was described as ?No support pre-sented for any arguments, or obvious flaws in allarguments.
?24 students submitted their papers online throughSWoRD and then reviewed 6 peers?
papers assignedto them in a ?double blind?
manner (review exam-ples are available in Figure 2).
When peer reviewis finished, RevExplore loads all papers and peerreviews, both textual comments and numeric rat-ings, and then goes through several text processingsteps to prepare for interactive analytics.
This pre-processing includes computing the domain words,sentence simplification, domain-word masking, syn-tactic analysis, and key noun-phrase extraction.4.1 Overview ?
Student ClusteringRevExplore starts with a student-centric visualiza-tion overview.
It uses a visual node of a bar chartto represent each student, visualizing the average ofthe student?s peer ratings in gray, as well as the rat-ing histogram with gradient colors (from red to blue)that are mapped to the rating scale from 1 to 7 (de-noted by the legend in Figure 1).To investigate students?
writing performance, in-structors can manually group similar nodes togetherinto one stacked bar chart, or use automatic group-ing options that RevExplore supports to inform ini-tial hypotheses about peer review patterns.
In theauto-mode, RevExplore can group students regard-ing a certain property (e.g.
rating average); it canalso cluster students using standard clustering algo-rithms2 based on either rating statistics or Bag-Of-Words extracted from the relevant peer reviews.If a instructor is curious about the review contentfor certain students during exploration, the instruc-2RevExplore implements both K-Means and a hierarchicalclustering algorithm.Figure 1: RevExplore overview.
Stacked bar charts rep-resent student groups.
The tooltip shows the ID of thecurrent student, writing performance (average peer rat-ings), review helpfulness (average helpfulness ratings), aswell as the main issues in the descending order of theirfrequency, which are extracted from the peer reviews re-ceived by a highlighted student using NLP techniques.tor can read the main issues, in the form of nounphrases (NPs) of a student?s peer reviews in a tooltipby mouse hovering on the bar squares which the stu-dent corresponds to.
For example, Figure 1 showsthat the peer reviews received by this student aremainly focused on the argumentation and the intro-duction part of the paper.To extract peer-review main issues, RevExploresyntactically simplifies each review sentence (Heil-man and Smith, 2010), parses each simplified sen-tence using the Stanford dependency parser (deMarneffe et al, 2006), and then traverses each de-pendency tree to find the key NP in a rule-basedmanner.3 Due to reviewers?
frequent references tothe relevant paper, most of the learned NPs are do-main related facts used in the paper, rather than eval-uative texts that suggest problems or suggestions.
Toavoid the interference of the domain content, we ap-ply domain-word masking (explained in Section 4.2)to the simplified sentences before parsing, and elim-inate any key NP that contains the mask.4.2 Detail-View ?
Topic ComparisonWhen two groups of students are selected in theoverview, their textual peer reviews can be further3Rules are constructed purely based on our intuition.176Figure 2: Peer-review exploration using RevExplore, for mining differences between strong and weak students.compared with respect to specific reviewing dimen-sions using a list of topic words that are automati-cally computed in real-time.Extracting topic words of peer reviews for com-parison purposes is different from most traditionaltopic-word extraction tasks that are commonly in-volved in text summarization.
In traditional textsummarization, the informativeness measurementis designed to extract the common themes, whilein our case of comparison, instructors are moreconcerned with the uniqueness of each target setof peer reviews compared to the others.
Thus atopic-signature acquisition algorithm (Lin and Hovy,2000), which extracts topic words through compar-ing the vocabulary distribution of a target corpusagainst that of a generic background corpus usinga statistic metric, suits our application better thanother approaches, such as probabilistic graphicalmodels (e.g.
LDA) and frequency based methods.Therefore, RevExplore considers topic signatures asthe topic words for a group of reviews, using all peerreviews as the background corpus.4 Again, to min-imize the impact of the domain content of the rele-vant papers, we apply topic-masking which replacesall domain words5 with ?ddd?
before computing thetopic signatures.As the software outputs topic signatures togetherwith their associated weights which reflect signatureimportance, RevExplore uses this weight informa-tion to order the topic words as a list, and visualizesthe weight as the font size and foreground color ofthe relevant topic word.
These lists are placed intwo rows regarding their group membership dimen-sion by dimension.
For each dimension, the cor-responding lists of both rows are aligned verticallywith the same background color to indicate that di-mension (e.g.
Topic-list detail view of Figure 2).To further facilitate the comparison within a dimen-sion, RevExplore highlights the topic words that areunique to one group with a darker background color.4We use TopicS (Nenkova and Louis, 2008) provided by An-nie Louis.5learned from all student papers against 5000 documentsfrom the English Gigaword Corpus using TopicS.177If the user cannot interpret the topic that an ex-tracted word might imply, the user can click on theword to read the relevant original reviews, with thatword highlighted in red (e.g.
Original reviews paneof Figure 2).5 Analysis ExampleFigure 2 shows how RevExplore is used to discoverthe difference between strong and weak studentswith respect to their writing performance on ?logic?in the history peer-review assignment introduced inSection 4.First we group students into strong versus weakregarding their writing performance on logic by se-lecting the K-Means algorithm to cluster studentsinto two groups based on their rating histogram onlogic.
As shown in the Overview pane of Figure 2,we then label them as A and B for further topic com-parison.Next, in the topic-list detail view, we check?praise?
and ?problem?6, and fire the ?enter?
but-ton to start extracting topic words for group A and Bon every selected dimension.
Note that ?logic?
willbe automatically selected since the focus has alreadybeen narrowed down to logic in the overview.To first compare the difference in general logic is-sues between these two groups, we refer to the twolists on ?logic?
(in the middle of the topic-list de-tail view, Figure 2).
As we can see, the weak stu-dents?
reviews (Group A) are more about the logicof statements and the usage of facts (indicated by theunique words ?examples?
and ?details?
); the strongstudents?
peer reviews (group B) focus more on ar-gumentation (noted by ?counter?
and ?supporting?
).To further compare the two groups regarding dif-ferent review sentiment, we look at the lists corre-sponding to ?problem?
and ?praise?
(left and rightcolumns).
For instance, we can see that strong stu-dents?
suffer more from context specific problems,which is indicated by the bigger font size of thedomain-word mask.
Meanwhile, to understand whata topic word implies, say, ?logic?
in group A?s topiclist on ?problem?, we can click the word to bring outthe relevant peer reviews, in which all occurrences6Although ?praise?
and ?problem?
are manually annotatedin this corpus (Nelson and Schunn, 2009), Xiong et al (2010)have shown that they can be automatically learned in a data-driven fashion.of ?logic?
are colored in red (original reviews panein Figure 2).6 Ongoing EvaluationWe are currently evaluating our work along two di-mensions.
First, we are interested in examiningthe utility of RevExplore for instructors.
After re-ceiving positive feedback from several instructorsat the University of Pittsburgh, as an informal pilotstudy, we deployed RevExplore for some of theseinstructors during the Spring 2012 semester and letthem explore the peer reviews of their own ongo-ing classes.
Instructors did observe interesting pat-terns using this tool after a short time of exploration(within two or three passes from the overview to thetopic-word detail view).
In addition, we are con-ducting a formal user study of 40 subjects to validatethe topic-word extraction component for comparingreviews in groups.
Our preliminary result shows thatour use of topic signatures is significantly better thana frequency-based baseline.7 Summary and Future workRevExplore demonstrates the usage of data visual-ization in combination with NLP techniques to helpinstructors interactively make sense of peer reviewdata, which was almost impracticable before.
In thefuture we plan to further analyze the data collectedin our formal user study, to validate the helpful-ness of our proposed topic-word approach for mak-ing sense of large quantities of peer reviews.
Wealso plan to incorporate NLP information beyond theword and NP level, to support additional types of re-view comparisons.
In addition, we plan to summa-rize the interview data that we informally collectedfrom several instructors, and will mine the log filesof their interactions with RevExplore to understandhow the tool would (and should) be used by instruc-tors in general.
Last but not least, we will continuerevising our design of RevExplore based on instruc-tor feedback, and plan to conduct a more formalevaluation with instructors.AcknowledgmentsThanks to Melissa Patchan for providing the historypeer-review corpus.
We are also grateful to LRDCfor financial support.178ReferencesStuart K. Card, Jock D. Mackinlay, and Ben Shneider-man.
1999.
Readings in information visualization:using vision to think.
San Francisco, CA, USA.Kwangsu Cho and Christian D. Schunn.
2007.
Scaf-folded writing and rewriting in the discipline: A web-based reciprocal peer review system.
Computers andEducation, 48(3):409?426.Kwangsu Cho.
2008.
Machine classification of peercomments in physics.
In Proceedings First Interna-tional Conference on Educational Data Mining, pages192?196.Raquel M Crespo Garcia and Abelardo Pardo.
2010.
Asupporting system for adaptive peer review based onlearners?
profiles.
In Proceedings of Computer Sup-ported Peer Review in Education Workshop, pages 22?31.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC 2006.Hongxia Fu and Mark Hawkes.
2010.
Technology-supported peer assessment as a means for teacherlearning.
In Proceedings of the 2010 Workshop onComputer Supported Peer Review in Education.Ioannis Giannoukos, Ioanna Lykourentzou, GiorgosMpardis, Vassilis Nikolopoulos, Vassilis Loumos, andEleftherios Kayafas.
2010.
An adaptive mechanismfor author-reviewer matching in online peer assess-ment.
In Semantics in Adaptive and Personalized Ser-vices, pages 109?126.Michael Heilman and Noah A. Smith.
2010.
Extractingsimplified statements for factual question generation.In Proceedings of the 3rd Workshop on Question Gen-eration.Ville Hyyrynen, Harri Ha?ma?la?inen, Jouni Ikonen, andJari Porras.
2010.
Mypeerreview: an online peer-reviewing system for programming courses.
In Pro-ceedings of the 10th Koli Calling International Con-ference on Computing Education Research, pages 94?99.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summarization.In Proceedings COLING.Melissa M. Nelson and Christian D. Schunn.
2009.
Thenature of feedback: how different types of peer feed-back affect writing performance.
Instructional Sci-ence, 37:375?401.Ani Nenkova and Annie Louis.
2008.
Can you summa-rize this?
Identifying correlates of input difficulty forgeneric multi-document summarization.
In Proceed-ings of Association for Computational Linguistics.
?Agnes Sa?ndor and Angela Vorndran.
2009.
Detectingkey sentences for automatic assistance in peer review-ing research articles in educational sciences.
In Pro-ceedings of the 2009 Workshop on Text and CitationAnalysis for Scholarly Digital Libraries, pages 36?44,Suntec City, Singapore, August.
Association for Com-putational Linguistics.Keith Topping.
1998.
Peer assessment between studentsin colleges and universities.
Review of EducationalResearch, 68(3):249?276.Keith J.
Topping.
2009.
Peer assessment.
Theory IntoPractice, 48(1):20?27.Wenting Xiong and Diane Litman.
2011.
Automaticallypredicting peer-review helpfulness.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies: short papers - Volume 2, HLT ?11, pages 502?507.Wenting Xiong, Diane J. Litman, and Christian D.Schunn.
2010.
Assessing reviewers performancebased on mining problem localization in peer-reviewdata.
In Proceedings Third International Conferenceon Educational Data Mining.Yu-Fen Yang.
2011.
A reciprocal peer review system tosupport college students?
writing.
British Journal ofEducational Technology, 42(4):687?700.179
