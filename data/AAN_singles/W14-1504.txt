Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 31?39,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsExtractive Summarization using Continuous Vector Space ModelsMikael K?ageb?ack, Olof Mogren, Nina Tahmasebi, Devdatt DubhashiComputer Science & EngineeringChalmers University of TechnologySE-412 96, G?oteborg{kageback, mogren, ninat, dubhashi}@chalmers.seAbstractAutomatic summarization can help usersextract the most important pieces of infor-mation from the vast amount of text digi-tized into electronic form everyday.
Cen-tral to automatic summarization is the no-tion of similarity between sentences intext.
In this paper we propose the use ofcontinuous vector representations for se-mantically aware representations of sen-tences as a basis for measuring similar-ity.
We evaluate different compositionsfor sentence representation on a standarddataset using the ROUGE evaluation mea-sures.
Our experiments show that the eval-uated methods improve the performanceof a state-of-the-art summarization frame-work and strongly indicate the benefitsof continuous word vector representationsfor automatic summarization.1 IntroductionThe goal of summarization is to capture the im-portant information contained in large volumes oftext, and present it in a brief, representative, andconsistent summary.
A well written summary cansignificantly reduce the amount of work needed todigest large amounts of text on a given topic.
Thecreation of summaries is currently a task best han-dled by humans.
However, with the explosion ofavailable textual data, it is no longer financiallypossible, or feasible, to produce all types of sum-maries by hand.
This is especially true if the sub-ject matter has a narrow base of interest, either dueto the number of potential readers or the durationduring which it is of general interest.
A summarydescribing the events of World War II might forinstance be justified to create manually, while asummary of all reviews and comments regardinga certain version of Windows might not.
In suchcases, automatic summarization is a way forward.In this paper we introduce a novel applicationof continuous vector representations to the prob-lem of multi-document summarization.
We evalu-ate different compositions for producing sentencerepresentations based on two different word em-beddings on a standard dataset using the ROUGEevaluation measures.
Our experiments show thatthe evaluated methods improve the performance ofa state-of-the-art summarization framework whichstrongly indicate the benefits of continuous wordvector representations for this tasks.2 SummarizationThere are two major types of automatic summa-rization techniques, extractive and abstractive.
Ex-tractive summarization systems create summariesusing representative sentences chosen from the in-put while abstractive summarization creates newsentences and is generally considered a more dif-ficult problem.Figure 1: Illustration of Extractive Multi-Document Summarization.For this paper we consider extractive multi-document summarization, that is, sentences arechosen for inclusion in a summary from a set ofdocuments D. Typically, extractive summariza-tion techniques can be divided into two compo-nents, the summarization framework and the sim-ilarity measures used to compare sentences.
Next31we present the algorithm used for the frameworkand in Sec.
2.2 we discuss a typical sentence sim-ilarity measure, later to be used as a baseline.2.1 Submodular OptimizationLin and Bilmes (2011) formulated the problem ofextractive summarization as an optimization prob-lem using monotone nondecreasing submodularset functions.
A submodular function F on theset of sentences V satisfies the following property:for any A ?
B ?
V \{v}, F (A+ {v})?F (A) ?F (B + {v})?
F (B) where v ?
V .
This is calledthe diminishing returns property and captures theintuition that adding a sentence to a small set ofsentences (i.e., summary) makes a greater contri-bution than adding a sentence to a larger set.
Theaim is then to find a summary that maximizes di-versity of the sentences and the coverage of the in-put text.
This objective function can be formulatedas follows:F(S) = L(S) + ?R(S)where S is the summary, L(S) is the coverage ofthe input text,R(S) is a diversity reward function.The ?
is a trade-off coefficient that allows us todefine the importance of coverage versus diversityof the summary.
In general, this kind of optimiza-tion problem is NP-hard, however, if the objectivefunction is submodular there is a fast scalable al-gorithm that returns an approximation with a guar-antee.
In the work of Lin and Bilmes (2011) a sim-ple submodular function is chosen:L(S) =?i?Vmin{?j?SSim(i, j), ?
?j?VSim(i, j)}(1)The first argument measures similarity betweensentence i and the summary S, while the sec-ond argument measures similarity between sen-tence i and the rest of the input V .
Sim(i, j) isthe similarity between sentence i and sentence jand 0 ?
?
?
1 is a threshold coefficient.
The di-versity reward functionR(S) can be found in (Linand Bilmes, 2011).2.2 Traditional Similarity MeasureCentral to most extractive summarization sys-tems is the use of sentence similarity measures(Sim(i, j) in Eq.
1).
Lin and Bilmes measuresimilarity between sentences by representing eachsentence using tf-idf (Salton and McGill, 1986)vectors and measuring the cosine angle betweenvectors.
Each sentence is represented by a wordvector w = (w1, .
.
.
, wN) where N is the size ofthe vocabulary.
Weights wkicorrespond to the tf-idf value of word k in the sentence i.
The weightsSim(i, j) used in the L function in Eq.
1 are foundusing the following similarity measure.Sim(i, j) =?w?itfw,i?
tfw,j?
idf2w??w?itf2w,i?
idf2w??w?jtf2w,j?
idf2w(2)where tfw,iand tfw,jare the number of occur-rences of w in sentence i and j, and idfwis theinverse document frequency (idf ) of w.In order to have a high similarity between sen-tences using the above measure, two sentencesmust have an overlap of highly scored tf-idf words.The overlap must be exact to count towards thesimilarity, e.g, the terms The US President andBarack Obama in different sentences will not addtowards the similarity of the sentences.
To cap-ture deeper similarity, in this paper we will inves-tigate the use of continuous vector representationsfor measuring similarity between sentences.
In thenext sections we will describe the basics neededfor creating continuous vector representations andmethods used to create sentence representationsthat can be used to measure sentence similarity.3 Background on Deep LearningDeep learning (Hinton et al., 2006; Bengio, 2009)is a modern interpretation of artificial neural net-works (ANN), with an emphasis on deep networkarchitectures.
Deep learning can be used for chal-lenging problems like image and speech recogni-tion (Krizhevsky et al., 2012; Graves et al., 2013),as well as language modeling (Mikolov et al.,2010), and in all cases, able to achieve state-of-the-art results.Inspired by the brain, ANNs use a neuron-likeconstruction as their primary computational unit.The behavior of a neuron is entirely controlled byits input weights.
Hence, the weights are wherethe information learned by the neuron is stored.More precisely the output of a neuron is computedas the weighted sum of its inputs, and squeezedinto the interval [0, 1] using a sigmoid function:yi= g(?Tix) (3)g(z) =11 + e?z(4)32x1x2x3x4y3HiddenlayerInputlayerOutputlayerFigure 2: FFNN with four input neurons, one hid-den layer, and 1 output neuron.
This type of ar-chitecture is appropriate for binary classificationof some data x ?
R4, however depending on thecomplexity of the input, the number and size of thehidden layers should be scaled accordingly.where ?iare the weights associated with neuron iand x is the input.
Here the sigmoid function (g) ischosen to be the logistic function, but it may alsobe modeled using other sigmoid shaped functions,e.g.
the hyperbolic tangent function.The neurons can be organized in many differ-ent ways.
In some architectures, loops are permit-ted.
These are referred to as recurrent neural net-works.
However, all networks considered here arenon-cyclic topologies.
In the rest of this sectionwe discuss a few general architectures in more de-tail, which will later be employed in the evaluatedmodels.3.1 Feed Forward Neural NetworkA feed forward neural network (FFNN) (Haykin,2009) is a type of ANN where the neurons arestructured in layers, and only connections to sub-sequent layers are allowed, see Fig 2.
The algo-rithm is similar to logistic regression using non-linear terms.
However, it does not rely on theuser to choose the non-linear terms needed to fitthe data, making it more adaptable to changingdatasets.
The first layer in a FFNN is called theinput layer, the last layer is called the output layer,and the interim layers are called hidden layers.The hidden layers are optional but necessary to fitcomplex patterns.Training is achieved by minimizing the networkerror (E).
How E is defined differs between dif-ferent network architectures, but is in general adifferentiable function of the produced output andx1x2x3x4x?1x?2x?3x?4CodinglayerInputlayerReconstructionlayerFigure 3: The figure shows an auto-encoder thatcompresses four dimensional data into a two di-mensional code.
This is achieved by using a bot-tleneck layer, referred to as a coding layer.the expected output.
In order to minimize thisfunction the gradient?E?
?first needs to be calcu-lated, where ?
is a matrix of all parameters, orweights, in the network.
This is achieved usingbackpropagation (Rumelhart et al., 1986).
Sec-ondly, these gradients are used to minimize E us-ing e.g.
gradient descent.
The result of this pro-cesses is a set of weights that enables the networkto do the desired input-output mapping, as definedby the training data.3.2 Auto-EncoderAn auto-encoder (AE) (Hinton and Salakhutdinov,2006), see Fig.
3, is a type of FFNN with a topol-ogy designed for dimensionality reduction.
Theinput and the output layers in an AE are identical,and there is at least one hidden bottleneck layerthat is referred to as the coding layer.
The net-work is trained to reconstruct the input data, andif it succeeds this implies that all information inthe data is necessarily contained in the compressedrepresentation of the coding layer.A shallow AE, i.e.
an AE with no extra hid-den layers, will produce a similar code as princi-pal component analysis.
However, if more layersare added, before and after the coding layer, non-linear manifolds can be found.
This enables thenetwork to compress complex data, with minimalloss of information.3.3 Recursive Neural NetworkA recursive neural network (RvNN), see Fig.
4,first presented by Socher et al.
(2010), is a type offeed forward neural network that can process datathrough an arbitrary binary tree structure, e.g.
a33x1x2x3yRootlayerInputlayerFigure 4: The recursive neural network architec-ture makes it possible to handle variable length in-put data.
By using the same dimensionality for alllayers, arbitrary binary tree structures can be re-cursively processed.binary parse tree produced by linguistic parsing ofa sentence.
This is achieved by enforcing weightconstraints across all nodes and restricting the out-put of each node to have the same dimensionalityas its children.The input data is placed in the leaf nodes ofthe tree, and the structure of this tree is used toguide the recursion up to the root node.
A com-pressed representation is calculated recursively ateach non-terminal node in the tree, using the sameweight matrix at each node.
More precisely, thefollowing formulas can be used:zp= ?Tp[xl;xr] (5a)yp= g(zp) (5b)where ypis the computed parent state of neuronp, and zpthe induced field for the same neuron.
[xl;xr] is the concatenation of the state belongingto the right and left sibling nodes.
This process re-sults in a fixed length representation for hierarchi-cal data of arbitrary length.
Training of the modelis done using backpropagation through structure,introduced by Goller and Kuchler (1996).4 Word EmbeddingsContinuous distributed vector representation ofwords, also referred to as word embeddings, wasfirst introduced by Bengio et al.
(2003).
A wordembedding is a continuous vector representationthat captures semantic and syntactic informationabout a word.
These representations can be usedto unveil dimensions of similarity between words,e.g.
singular or plural.4.1 Collobert & WestonCollobert andWeston (2008) introduce an efficientmethod for computing word embeddings, in thiswork referred to as CW vectors.
This is achievedfirstly, by scoring a valid n-gram (x) and a cor-rupted n-gram (x?)
(where the center word has beenrandomly chosen), and secondly, by training thenetwork to distinguish between these two n-grams.This is done by minimizing the hinge lossmax(0, 1?
s(x) + s(x?))
(6)where s is the scoring function, i.e.
the output ofa FFNN that maps between the word embeddingsof an n-gram to a real valued score.
Both the pa-rameters of the scoring function and the word em-beddings are learned in parallel using backpropa-gation.4.2 Continuous Skip-gramA second method for computing word embeddingsis the Continuous Skip-gram model, see Fig.
5, in-troduced by Mikolov et al.
(2013a).
This model isused in the implementation of their word embed-dings tool Word2Vec.
The model is trained to pre-dict the context surrounding a given word.
This isaccomplished by maximizing the objective func-tion1TT?t=1?
?c?j?c,j 6=0log p(wt+j|wt) (7)where T is the number of words in the trainingset, and c is the length of the training context.The probability p(wt+j|wt) is approximated usingthe hierarchical softmax introduced by Bengio etal.
(2002) and evaluated in a paper by Morin andBengio (2005).5 Phrase EmbeddingsWord embeddings have proven useful in many nat-ural language processing (NLP) tasks.
For sum-marization, however, sentences need to be com-pared.
In this section we present two differentmethods for deriving phrase embeddings, whichin Section 5.3 will be used to compute sentence tosentence similarities.5.1 Vector additionThe simplest way to represent a sentence is toconsider it as the sum of all words without re-garding word orders.
This was considered by34wtwt?1wt?2wt+1wt+2projectionlayerInputlayerOutputlayerFigure 5: The continuous Skip-gram model.
Us-ing the input word (wt) the model tries to predictwhich words will be in its context (wt?c).Mikolov et al.
(2013b) for representing shortphrases.
The model is expressed by the followingequation:xp=?xw?
{sentence}xw(8)where xpis a phrase embedding, and xwis a wordembedding.
We use this method for computingphrase embeddings as a baseline in our experi-ments.5.2 Unfolding Recursive Auto-encoderThe second model is more sophisticated, tak-ing into account also the order of the wordsand the grammar used.
An unfolding recursiveauto-encoder (RAE) is used to derive the phraseembedding on the basis of a binary parse tree.The unfolding RAE was introduced by Socher etal.
(2011) and uses two RvNNs, one for encodingthe compressed representations, and one for de-coding them to recover the original sentence, seeFigure 6.
The network is subsequently trained byminimizing the reconstruction error.Forward propagation in the network is done byrecursively applying Eq.
5a and 5b for each tripletin the tree in two phases.
First, starting at the cen-ter node (root of the tree) and recursively pullingthe data from the input.
Second, again startingat the center node, recursively pushing the datatowards the output.
Backpropagation is done ina similar manner using backpropagation throughstructure (Goller and Kuchler, 1996).x1x2x3x?1x?2x?3RootlayerInputlayerOutputlayer?e?dFigure 6: The structure of an unfolding RAE, ona three word phrase ([x1, x2, x3]).
The weight ma-trix ?eis used to encode the compressed represen-tations, while ?dis used to decode the representa-tions and reconstruct the sentence.5.3 Measuring SimilarityPhrase embeddings provide semantically awarerepresentations for sentences.
For summarization,we need to measure the similarity between tworepresentations and will make use of the followingtwo vector similarity measures.
The first similar-ity measure is the cosine similarity, transformed tothe interval of [0, 1]Sim(i, j) =(xTixj?xj?
?xj?+ 1)/2 (9)where x denotes a phrase embedding The secondsimilarity is based on the complement of the Eu-clidean distance and computed as:Sim(i, j) = 1?1maxk,n??
xk?
xn?2??
xj?
xi?2(10)6 ExperimentsIn order to evaluate phrase embeddings for sum-marization we conduct several experiments andcompare different phrase embeddings with tf-idfbased vectors.6.1 Experimental SettingsSeven different configuration were evaluated.
Thefirst configuration provides us with a baseline andis denoted Original for the Lin-Bilmes methoddescribed in Sec.
2.1.
The remaining configura-tions comprise selected combinations of word em-beddings, phrase embeddings, and similarity mea-sures.35The first group of configurations are based onvector addition using bothWord2Vec and CW vec-tors.
These vectors are subsequently compared us-ing both cosine similarity and Euclidean distance.The second group of configurations are built uponrecursive auto-encoders using CW vectors and arealso compared using cosine similarity as well asEuclidean distance.The methods are named according to:VectorType EmbeddingMethodSimilarityMethod,e.g.
W2V_AddCosfor Word2Vec vectors com-bined using vector addition and compared usingcosine similarity.To get an upper bound for each ROUGE scorean exhaustive search were performed, where eachpossible pair of sentences were evaluated, andmaximized w.r.t the ROUGE score.6.2 Dataset and EvaluationThe Opinosis dataset (Ganesan et al., 2010) con-sists of short user reviews in 51 different top-ics.
Each of these topics contains between 50 and575 sentences and are a collection of user reviewsmade by different authors about a certain charac-teristic of a hotel, car or a product (e.g.
?Loca-tion of Holiday Inn, London?
and ?Fonts, Ama-zon Kindle?).
The dataset is well suited for multi-document summarization (each sentence is con-sidered its own document), and includes between4 and 5 gold-standard summaries (not sentenceschosen from the documents) created by human au-thors for each topic.Each summary is evaluated with ROUGE, thatworks by counting word overlaps between gener-ated summaries and gold standard summaries.
Ourresults include R-1, R-2, and R-SU4, which countsmatches in unigrams, bigrams, and skip-bigramsrespectively.
The skip-bigrams allow four wordsin between (Lin, 2004).The measures reported are recall (R), precision(P), and F-score (F), computed for each topic indi-vidually and averaged.
Recall measures what frac-tion of a human created gold standard summarythat is captured, and precision measures what frac-tion of the generated summary that is in the goldstandard.
F-score is a standard way to combinerecall and precision, computed as F = 2P?RP+R.6.3 ImplementationAll results were obtained by running an imple-mentation of Lin-Bilmes submodular optimizationsummarizer, as described in Sec.
2.1.
Also, wehave chosen to fix the length of the summariesto two sentences because the length of the gold-standard summaries are typically around two sen-tences.
The CW vectors used were trained byTurian et al.
(2010)1, and the Word2Vec vectorsby Mikolov et al.
(2013b)2.
The unfolding RAEused is based on the implementation by Socheret al.
(2011)3, and the parse trees for guidingthe recursion was generated using the StanfordParser (Klein and Manning, 2003)4.6.4 ResultsThe results from the ROUGE evaluation are com-piled in Table 1.
We find for all measures (recall,precision, and F-score), that the phrase embed-dings outperform the original Lin-Bilmes.
For re-call, we find that CW_AddCosachieves the high-est result, while for precision and F-score theCW_AddEucperform best.
These results are con-sistent for all versions of ROUGE scores reported(1, 2 and SU4), providing a strong indication forphrase embeddings in the context of automaticsummarization.Unfolding RAE on CW vectors and vector ad-dition on W2V vectors gave comparable resultsw.r.t.
each other, generally performing better thanoriginal Linn-Bilmes but not performing as well asvector addition of CW vectors.The results denoted OPT in Table 1 describethe upper bound score, where each row repre-sents optimal recall and F-score respectively.
Thebest results are achieved for R-1 with a maxi-mum recall of 57.86%.
This is a consequence ofhand created gold standard summaries used in theevaluation, that is, we cannot achieve full recallor F-score when the sentences in the gold stan-dard summaries are not taken from the underly-ing documents and thus, they can never be fullymatched using extractive summarization.
R-2 andSU4 have lower maximum recall and F-score, with22.9% and 29.5% respectively.6.5 DiscussionThe results of this paper show great potential foremploying word and phrase embeddings in sum-marization.
We believe that by using embeddingswe move towards more semantically aware sum-marization systems.
In the future, we anticipate1http://metaoptimize.com/projects/wordreprs/2https://code.google.com/p/word2vec/3http://nlp.stanford.edu/ socherr/codeRAEVectorsNIPS2011.zip4http://nlp.stanford.edu/software/lex-parser.shtml36Table 1: ROUGE scores for summaries using dif-ferent similarity measures.
OPT constitutes theoptimal ROUGE scores on this dataset.ROUGE-1R P FOPTR57.86 21.96 30.28OPTF45.93 48.84 46.57CW_RAECos27.37 19.89 22.00CW_RAEEuc29.25 19.77 22.62CW_AddCos34.72 11.75 17.16CW_AddEuc29.12 22.75 24.88W2V_AddCos30.86 16.81 20.93W2V_AddEuc28.71 16.67 20.75Original 25.82 19.58 20.57ROUGE-2R P FOPTR22.96 12.31 15.33OPTF20.42 19.94 19.49CW_RAECos4.68 3.18 3.58CW_RAEEuc4.82 3.24 3.67CW_AddCos5.89 1.81 2.71CW_AddEuc5.12 3.60 4.10W2V_AddCos5.71 3.08 3.82W2V_AddEuc3.86 1.95 2.54Original 3.92 2.50 2.87ROUGE-SU4R P FOPTR29.50 13.53 17.70OPTF23.17 26.50 23.70CW_RAECos9.61 6.23 6.95CW_RAEEuc9.95 6.17 7.04CW_AddCos12.38 3.27 5.03CW_AddEuc10.54 7.59 8.35W2V_AddCos11.94 5.52 7.12W2V_AddEuc9.78 4.69 6.15Original 9.15 6.74 6.73improvements for the field of automatic summa-rization as the quality of the word vectors im-prove and we find enhanced ways of composingand comparing the vectors.It is interesting to compare the results of dif-ferent composition techniques on the CW vec-tors, where vector addition surprisingly outper-forms the considerably more sophisticated unfold-ing RAE.
However, since the unfolding RAE usessyntactic information, this may be a result of usinga dataset consisting of low quality text.In the interest of comparing word embeddings,results using vector addition and cosine similaritywere computed based on both CW and Word2Vecvectors.
Supported by the achieved results CWvectors seems better suited for sentence similari-ties in this setting.An issue we encountered with using precom-puted word embeddings was their limited vocab-ulary, in particular missing uncommon (or com-mon incorrect) spellings.
This problem is par-ticularly pronounced on the evaluated Opinosisdataset, since the text is of low quality.
Futurework is to train word embeddings on a dataset usedfor summarization to better capture the specific se-mantics and vocabulary.The optimal R-1 scores are higher than R-2 andSU4 (see Table 1) most likely because the score ig-nores word order and considers each sentence as aset of words.
We come closest to the optimal scorefor R-1, where we achieve 60% of maximal recalland 49% of F-score.
Future work is to investigatewhy we achieve a much lower recall and F-scorefor the other ROUGE scores.Our results suggest that the phrase embeddingscapture the kind of information that is needed forthe summarization task.
The embeddings are theunderpinnings of the decisions on which sentencesthat are representative of the whole input text, andwhich sentences that would be redundant whencombined in a summary.
However, the fact thatwe at most achieve 60% of maximal recall sug-gests that the phrase embeddings are not completew.r.t summarization and might benefit from beingcombined with other similarity measures that cancapture complementary information, for exampleusing multiple kernel learning.7 Related WorkTo the best of our knowledge, continuous vectorspace models have not previously been used insummarization tasks.
Therefore, we split this sec-tion in two, handling summarization and continu-ous vector space models separately.7.1 Continuous Vector Space ModelsContinuous distributed vector representation ofwords was first introduced by Bengio et al.
(2003).37They employ a FFNN, using a window of wordsas input, and train the model to predict the nextword.
This is computed using a big softmax layerthat calculate the probabilities for each word in thevocabulary.
This type of exhaustive estimation isnecessary in some NLP applications, but makesthe model heavy to train.If the sole purpose of the model is to deriveword embeddings this can be exploited by usinga much lighter output layer.
This was suggestedby Collobert and Weston (2008), which swappedthe heavy softmax against a hinge loss function.The model works by scoring a set of consecutivewords, distorting one of the words, scoring the dis-torted set, and finally training the network to givethe correct set a higher score.Taking the lighter concept even further,Mikolov et al.
(2013a) introduced a model calledContinuous Skip-gram.
This model is trainedto predict the context surrounding a given wordusing a shallow neural network.
The model is lessaware of the order of words, than the previouslymentioned models, but can be trained efficientlyon considerably larger datasets.An early attempt at merging word represen-tations into representations for phrases and sen-tences is introduced by Socher et al.
(2010).
Theauthors present a recursive neural network archi-tecture (RvNN) that is able to jointly learn parsingand phrase/sentence representation.
Though notable to achieve state-of-the-art results, the methodprovides an interesting path forward.
The modeluses one neural network to derive all merged rep-resentations, applied recursively in a binary parsetree.
This makes the model fast and easy to trainbut requires labeled data for training.7.2 Summarization TechniquesRadev et al.
(2004) pioneered the use of clustercentroids in their work with the idea to group, inthe same cluster, those sentences which are highlysimilar to each other, thus generating a numberof clusters.
To measure the similarity between apair of sentences, the authors use the cosine simi-larity measure where sentences are represented asweighted vectors of tf-idf terms.
Once sentencesare clustered, sentence selection is performed byselecting a subset of sentences from each cluster.In TextRank (2004), a document is representedas a graph where each sentence is denoted by avertex and pairwise similarities between sentencesare represented by edges with a weight corre-sponding to the similarity between the sentences.The Google PageRank ranking algorithm is usedto estimate the importance of different sentencesand the most important sentences are chosen forinclusion in the summary.Bonzanini, Martinez, Roelleke (2013) pre-sented an algorithm that starts with the set ofall sentences in the summary and then iterativelychooses sentences that are unimportant and re-moves them.
The sentence removal algorithm ob-tained good results on the Opinosis dataset, in par-ticular w.r.t F-scores.We have chosen to compare our work with thatof Lin and Bilmes (2011), described in Sec.
2.1.Future work is to make an exhaustive comparisonusing a larger set similarity measures and summa-rization frameworks.8 ConclusionsWe investigated the effects of using phrase embed-dings for summarization, and showed that thesecan significantly improve the performance of thestate-of-the-art summarization method introducedby Lin and Bilmes in (2011).
Two implementa-tions of word vectors and two different approachesfor composition where evaluated.
All investi-gated combinations improved the original Lin-Bilmes approach (using tf-idf representations ofsentences) for at least two ROUGE scores, and topresults where found using vector addition on CWvectors.In order to further investigate the applicabilityof continuous vector representations for summa-rization, in future work we plan to try other sum-marization methods.
In particular we will use amethod based on multiple kernel learning werephrase embeddings can be combined with othersimilarity measures.
Furthermore, we aim to usea novel method for sentence representation similarto the RAE using multiplicative connections con-trolled by the local context in the sentence.AcknowledgmentsThe authors would like to acknowledge the projectTowards a knowledge-based culturomics sup-ported by a framework grant from the SwedishResearch Council (2012?2016; dnr 2012-5738),and the project Data-driven secure business intel-ligence grant IIS11-0089 from the Swedish Foun-dation for Strategic Research (SSF).38ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Yoshua Bengio.
2002.
New distributed prob-abilistic language models.
Technical Report1215, D?epartement d?informatique et rechercheop?erationnelle, Universit?e de Montr?eal.Yoshua Bengio.
2009.
Learning deep architectures forai.
Foundations and trendsR?
in Machine Learning,2(1):1?127.Marco Bonzanini, Miguel Martinez-Alvarez, andThomas Roelleke.
2013.
Extractive summarisa-tion via sentence removal: Condensing relevant sen-tences into a short summary.
In Proceedings of the36th International ACM SIGIR Conference on Re-search and Development in Information Retrieval,SIGIR ?13, pages 893?896.
ACM.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 340?348.
ACL.Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In IEEE Inter-national Conference on Neural Networks, volume 1,pages 347?352.
IEEE.Alex Graves, Abdel-rahman Mohamed, and Geof-frey Hinton.
2013.
Speech recognition withdeep recurrent neural networks.
arXiv preprintarXiv:1303.5778.S.S.
Haykin.
2009.
Neural Networks and LearningMachines.
Number v. 10 in Neural networks andlearning machines.
Prentice Hall.Geoffrey E Hinton and Ruslan R Salakhutdinov.
2006.Reducing the dimensionality of data with neural net-works.
Science, 313(5786):504?507.Geoffrey E Hinton, Simon Osindero, and Yee-WhyeTeh.
2006.
A fast learning algorithm for deep be-lief nets.
Neural computation, 18(7):1527?1554.Dan Klein and Christopher D Manning.
2003.
Fast ex-act inference with a factored model for natural lan-guage parsing.
Advances in neural information pro-cessing systems, pages 3?10.Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.2012.
Imagenet classification with deep convolu-tional neural networks.
In Advances in Neural Infor-mation Processing Systems 25, pages 1106?1114.Hui Lin and Jeff Bilmes.
2011.
A class of submodu-lar functions for document summarization.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 510?520.
ACL.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings ofEMNLP, volume 4.
Barcelona, Spain.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
ArXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InAISTATS?05, pages 246?252.Dragomir R Radev, Hongyan Jing, Ma?gorzata Sty?s,and Daniel Tam.
2004.
Centroid-based summariza-tion of multiple documents.
Information Processing& Management, 40(6):919?938.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1986.
Learning representations by back-propagating errors.
Nature, 323(6088):533?536.Gerard Salton and Michael J. McGill.
1986.
Intro-duction to Modern Information Retrieval.
McGraw-Hill, Inc., New York, NY, USA.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic Pooling and Unfolding Recursive Autoen-coders for Paraphrase Detection.
In Advances inNeural Information Processing Systems 24.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
ACL.39
