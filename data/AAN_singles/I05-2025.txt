Investigating the features that affect cue usage of non-native speakers ofEnglishXinyu DengKyoto UniversityYoshida Honmachi, SakyokuKyoto, 606-8501, Japandeng@pine.kuee.kyoto-u.ac.jpJun-ichi NakamuraKyoto UniversityYoshida Honmachi, SakyokuKyoto, 606-8501, JapanAbstractAt present, the population of non-nativespeakers is twice that of native speak-ers.
It is necessary to explore thetext generation strategies for non-nativeusers.
However, little has been donein this field.
This study investigatesthe features that affect the placement(where to place a cue) of because fornon-native speakers.
A machine learn-ing program ?
C4.5 was applied to in-duce the classification models of theplacement.1 IntroductionAs an international language, English has becomemore and more important for non-native speak-ers.
However, almost all English documents arewritten for the native speakers.
To some degree,some documents can not be understood quite wellby non-native speakers.
This paper concentrateson exploring the differences in cue usage at dis-course level between native and non-native speak-ers.
The aim is to find the decision-making mech-anisms of text generation for users at differentreading levels.While investigating texts written for non-nativespeakers, we found that cue phrase because some-times occurs in the first span of a discourse rela-tion.
This is different from the conclusion men-tioned in (Quirk and Greenbaum and Leech andSvartvik, 1972), that is, (for native speakers) be-cause typically occurs in the second span.
Thisproblem could be considered from the viewpointof text generation as well.
The following threetexts may have the same abstract text structure,though the differences among them are apparent.E.g., cue placement is different.
In text (1), cuephrase because occurs at first span of discourserelation ?explanation?, while in (2) and (3), be-cause occurs in the second span.Example 1.1:1.
Global warming will be a major threat to thewhole world over the next century.
But be-cause it will take many years for our actionsto produce a significant effect, the problemneeds attention now.2.
Global warming will be a major threat to thewhole world over the next century, but theproblem needs attention now, because it willtake many years for our actions to produce asignificant effect.3.
Global warming will be a major threat to thewhole world over the next century.
But theproblem needs attention now, because it willtake many years for our actions to produce asignificant effect.This paper reports the results of the research onthe different placement (where to place a cue) ofbecause between native and non-native speakersthrough analyzing two annotated corpora.
At thesame time, we study the features that affect place-ment of because for non-native speakers.
The restof the paper is arranged as follows.
Section 2 de-scribes related work.
Section 3 demonstrates howto create two corpora (SUB-BNC and CNNSE).144Section 4 shows the method of annotating cor-pora.
Section 5 demonstrates the difference in us-age of because between two corpora.
In section 6,a machine learning program ?
C4.5 is introduced.Section 7 shows the experimental results.
Section8 draws a conclusion.2 Related workAlmost all researches on cue phrases have beendone for native speakers.
(Elhadad and McKe-own, 1990) explored the problem of cue selec-tion.
They presented a model that distinguishesa small set of similar cue phrases.
(Moser andMoore, 1995a) put forward a method to identifythe features that predict cue selection and place-ment.
(Eugenio and Moore and Paolucci, 1997)used C4.5 to predict cue occurrence and place-ment.
Until now, the research similar to ours isthe GIRL system (Williams, 2004) which gener-ates texts for poor readers and good readers ofnative speakers.
The author measured the differ-ences of reading speed (especially cue phrases)between good readers and bad readers, by whichthey inferred how discourse level choice (e.g., cueselection) makes the difference for the two kindsof readers.3 Creating two corporaWe used two corpora (SUB-BNC and CNNSE) toinvestigate difference in cue usage between nativeand non-native speakers.
The two corpora havethe same size (200,000 words each).
Accordingto the Flesch Reading Ease scale, the readabilityof SUB-BNC and CNNSE is 47.5 (difficult) and68.7 (easy) respectively.The two corpora are comparable.
SUB-BNC isa sub-corpus of BNC (British National Corpus).While creating SUB-BNC, we selected the writ-ten texts according to the three features: domain(?natural and pure science?
), medium (?book?
),target audience (?adult?).
CNNSE (Corpus ofNon-Native Speaker of English) was created bythe first author.
Non-native speakers have threelevels: primary (middle school student level), in-termediate (high school student level) and ad-vanced (university student level).
The users ofthis study are assumed to be at intermediate level.We extracted English texts (written or rewrittenby native speakers) from the books published inChina and in Japan.
The target audiences of thesebooks were high school students in the two coun-tries.
The domain of the selected texts is naturaland pure science as well.4 Annotating two corporaWe followed (Carlson and Marcu and Okurowski,2001) to classify the discourse relations.
In themanual, some relations share some type of rhetor-ical meaning, so we defined several relations asfollows:1. background: background, circumstance2.
cause: cause, result, consequence3.
comparison: comparison, preference, anal-ogy, proportion4.
condition: condition, hypothetical, contin-gency, otherwise5.
contrast: contrast, concession, antithesis6.
elaboration: elaboration-additional,elaboration-general-specific, elaboration-part-whole, elaboration-process-step,elaboration-object-attribute, elaboration-set-member7.
enablement: purpose, enablement8.
evaluation: evaluation, interpretation, con-clusion, comment9.
explanation: evidence, explanation-argumentative, reason10.
summary: summary, restatementAnnotation includes two stages: first, we al-lowed two coders to choose ?explanation?
rela-tions signaled by because using (Hirschberg andLitman, 1993)?s 3-way classification.
The wordbecause could signal not only ?explanation?
rela-tion, but other relations.
On the other hand, we donot consider some structures, e.g., ?not because... but because?.
Thus, because could be judgedas ?explanation?, ?other?, or ?not considered?.
Ifboth coders classified because as ?explanation?,this discourse was selected.
Lastly, 228 becausewere selected from two corpora.145At the second stage, two coders annotated theboundary of nucleus and satellite of each dis-course selected.
Moreover, a selected discoursecould be a span (nucleus or satellite) of anotherone (we call it embedding structure).
The coderslabeled the discourse relation of the embeddingstructure and determined the boundary of its nu-cleus and satellite.
Example 4.1 shows an exam-ple.Example 4.1[Global warming will be a major threat to thewhole world over the next century.]?S?
contrast?N?
[But [because it will take many years forour actions to produce a significant effect,]?S?explanation ?N?
[the problem needs attentionnow.]]
(From CNNSE)In order to assess reliability of annotation, we fol-lowed (Moser and Moore, 1995b)?s approach tocompare the disagreements of results annotatedby two independent coders from three aspects.First, the boundary of nucleus and satellite ofthe relation signaled by because.
The disagree-ments occurred 7 times (96.9% agreement).
Sec-ond, the discourse relation of embedding struc-ture.
The disagreements occurred 16 times (93%agreement).
Third, the boundary of nucleus andsatellite of the embedding structure.
The dis-agreements occurred 9 times (96.1% agreement).That is, the agreement of the two coders is 86%.This is better than that mentioned in (Moser andMoore, 1995b).5 Analyzing the usage of because withintwo corporaThrough investigating annotated SUB-BNC, wefound that there are 104 ?explanation?
relationssignaled by because, in which 96/104 (92.3%)(Table 1) occurs in the second span.
This con-clusion is the same as (Quirk and Greenbaumand Leech and Svartvik, 1972) and (Moser andMoore, 1995b)?
opinion, i.e., because typicallyoccurs in the second span.
However, withinCNNSE, we found that only 88/124 (71%) occursin the second span.
This result is quite differentfrom that of SUB-BNC.
Moreover, Chi Squarecritical values (?2 = 16.54, p < 0.001) also sup-port this conclusion.Corpus First span Second spanSUB-BNC 8 96CNNSE 36 88Table 1: Placement of because within twocorpora (?2 = 16.54, p < 0.001)6 Machine learning program ?
C4.56.1 Evaluation methodThe results of C4.5 are learned classification mod-els from the training sets.
The error rates of thelearned models are estimated by cross-validation(Weiss and Kulikowski, 1991), which is widelyapplied to evaluating decision trees, especiallywhose dataset is relatively small.
Data for learn-ing is randomly divided into N test sets.
The pro-gram is run for N times, each run uses (N-1) testsets as the training set and the remaining one asthe test set.
The error rate of a tree obtained by us-ing the whole dataset for training is then assumedto be the average error rate on the test set over theN runs (Eugenio and Moore and Paolucci, 1997).The advantage of this method is that all data areeventually used for testing, and almost all exam-ples are used in any given training run (Litman,1996).
This study follows (Eugenio and Mooreand Paolucci, 1997) (Litman, 1996)?
s approachto identify the best learned models by comparingtheir error rates to the error rates of the other mod-els.
The method of determining whether two errorrates are significantly different is by computingand comparing the 95% confidence intervals forthe two error rates.
If the upper bound of the 95%confidence interval for error rate ?1 is lower thanthe lower bound of the 95% confidence intervalfor ?2, then the difference between ?1 and ?2 isconsidered to be significant.6.2 FeaturesWe classified features into two groups: sentencefeatures and embedding structure features.
Sen-tence features are concerned with the informationof relations signaled by because.
Nt and St rep-resent tense of nucleus and satellite respectively.Nv and Sv represent voice of nucleus and satel-lite respectively.
We also used the features Ng(nucleus length) and Sg (satellite length).
Mean-146while, nucleus structure (Ns) and satellite struc-ture (Ss) were considered.Another group of features reflect informationof the embedding structures that contain relationssignaled by because.
R represents discourse re-lation of the embedding structure.
C representswhether the embedding structure is cued or not.N-S indicates that in the embedding structure, therelation signaled by because could be either nu-cleus or satellite.
P indicates that the relation sig-naled by because could occur either in the firstspan or in the second span.
Bs represents thestructure of the span containing the relation sig-naled by because.
Os represents the structure ofthe span not containing the relation signaled bybecause.
Features used in the experiments are asfollows:?
Sentence features?
Nt.
Tense of nucleus: past, present, fu-ture.?
St. Tense of satellite: past, present, fu-ture.?
Nv.
Voice of nucleus: active, passive.?
Sv.
Voice of satellite: active, passive.?
Ng.
Length of nucleus (in words): inte-ger.?
Sg.
Length of satellite (in words): inte-ger.?
Ns.
Structure of nucleus: simple, other.?
Ss.
Structure of satellite: simple, other.?
Embedding structure features?
R. Discourse relation of embeddingstructure: attribution, background,cause, comparison, condition, con-trast, elaboration, example, enable-ment, evaluation, explanation, list,summary, temporal.?
C. Signaled by cue or not: yes, no.?
N-S. Role of the relation signaled bybecause: nucleus, satellite.?
P. Position of relation signaled by be-cause: first span, second span.?
Bs.
Structure of the span containing therelation signaled by because: complexsentence, other.?
Os.
Structure of the span not contain-ing the relation signaled by because:simple sentence, other.7 ExperimentsWe divided the experiments into four sets.
Exper-iment Set 1 were run for examining the best indi-vidual feature whose predictive power was betterthan the baseline.
Experiment Set 2, 3 and 4 wererun for classifying the placement of because.
InExperiment Set 2, we only used sentence features.In Experiment Set 3, we used both sentence fea-tures and embedding structure features.
Experi-ment Set 4 were run using only embedding struc-ture features.7.1 Experiment Set 1First we introduce a concept ?
baseline, whichcan be obtained by choosing the majority class.E.g., 71.0% (88/124) because occurs in the sec-ond span.
That is, if because is placed directlyin the second span, one would be wrong 29% ofthe times.
So 29% is the error rate of the baselinemodel that is used in the experiment.We ran the experiment 14 times using eachfeature mentioned above.
By analyzing the re-sults, we found that only feature R has predictivepower.
Because the 95% confidence interval of itserror rate was 16.2 ?
0.7, whose upper bound forerror rate (16.9%) was much lower than the base-line (29%).
Table 2 shows the results by usingfeature R. When discourse relation of the embed-ding structure is ?cause?, ?contrast?, ?example?,or ?explanation?, because occurs in the first span.......R = cause: first span (14.0/5.0)......R = contrast: first span (14.0/5.0).....R = example: first span (5.0/1.0)......R = explanation: first span (4.0)............Table 2: Experiment results using feature Rin Experiment Set 1147Nt St Nv Sv Ng Sg Ns Ss R C N-S P Bs Os Result1 x x x x x x x x 29.2 ?
4.92 x x x x x x 27.6 ?
5.23 x x x x x x 30.8 ?
4.24 x x x x 27.3 ?
3.0Table 3: Feature sets and 95%-confidence intervals for the error rates (%) ofclassification models in Experiment Set 2Nt St Nv Sv Ng Sg Ns Ss R C N-S P Bs Os Result1 x x x x x x x x x x x x x x 23.5 ?
2.52 x x x x x 31.7 ?
2.63 x x x x x x x x x 33.3 ?
3.34 x x x x x x x x x x 26.9 ?
3.0Table 4: Feature sets and 95%-confidence intervals for the error rates (%) ofclassification models in Experiment Set 37.2 Experiment Set 2Experiment Set 2 had four subsets.
Each exper-iment was run only using sentence features (Ta-ble 3).
In the first experiment, all eight sen-tence features were used.
However, the upperbound of the 95% confidence interval for errorrate (34.1%) was higher than the baseline (29%).So the learned model was not a good one.
Thenwe ran three other experiments using a combina-tion of different sentence features.
In subset 2, thefeatures representing span structure (Ns and Ss)were deleted.
In subset 3, compared with the firstone, span length (Ng and Sg) were deleted.
Insubset 4, only the features relating to span length(Ng and Sg) and span structure (Ns and Ss) wereused.
However, no good classification model wasobtained.7.3 Experiment Set 3Experiment Set 3 had four subsets as well.
In thefirst subset, experiment was run using all sentencefeatures and embedding structure features.
Ex-perimental results show that the upper bound ofthe 95% confidence interval for error rate (26%)was lower than the baseline (29%).
It means thatembedding structure feature(s) could improve theaccuracy of the learned classification models.
Inthe next three experiments, we tried three otherfeature combinations.
One feature set concernedwith the placement of because (P) and span struc-ture (Ns and Ss, Bs and Os).
Experimentalresults show that the average error rate is higherthan the baseline.
In subset 3, two sentence fea-tures (Ng and Sg) and two embedding structurefeatures (C and N-S) were added.
However, theaverage error rate of the learned model was stillhigher than the baseline.
It means that these fourfeatures can not help to improve the accuracy ofclassification models.
In subset 4, feature R wasadded.
Though the average error rate was lowerthan subset 2 and 3, its upper bound of the 95%confidence interval for error rate was higher thanthe baseline.
The fourth learned model can not beregarded as a good one.7.4 Experiment Set 4Experiment Set 4 had five subsets.
In subset 1, theexperiment was run using all the six embeddingstructure features.
The upper bound of the 95%confidence interval for error rate of the learnedmodel was lower than the baseline.
In subset 2,we ran the experiment by deleting one feature Rfrom subset 1.
Its average error rate was higherthan that of subset 1, and its upper bound of the95% confidence interval for error rate was higherthan the baseline.
It again proves that R is the fea-ture that affects the accuracy of learned models.In the subset 3 and 4, experiments were run bydeleting feature C and P respectively.
The aver-age error rates of the results were nearly the sameas that of subset 1.
It demonstrates that features C148Nt St Nv Sv Ng Sg Ns Ss R C N-S P Bs Os Result1 x x x x x x 22.8 ?
3.22 x x x x x 30.1 ?
4.83 x x x x x 22.6 ?
2.84 x x x x x 21.2 ?
3.75 x x x x 21.9 ?
3.6Table 5: Feature sets and 95%-confidence intervals for the error rates (%) ofclassification models in Experiment Set 4and P do not affect the accuracy of learned mod-els.
In the subset 5, features Bs and Os weredeleted from the subset 1.
The experimental re-sult did not change so much as well.
So we caninfer that span structure do not affect the accuracyof the learned model.7.5 DiscussionThe experimental results show that machinelearning program C4.5 is useful to induce a clas-sification model of placement of because for non-native speakers.
The results of Experiment Set1 demonstrate that feature R is the best individ-ual feature whose predictive power is better thanthe baseline.
Experiment Set 2 and 3 show thatgood learned model can not be obtained usingsentence features, or the combination of sentencefeatures and embedding structure features.
Theresults of Experiment Set 4 demonstrate that highperforming classification models can be obtainedby combining feature R with several other embed-ding structure features.
However, the best learnedmodel can?t be obtained.8 ConclusionThis study proves that the placement of becauseis connected with reading ease.
We used a ma-chine learning program to induce the best classi-fication model of placement of because for non-native speakers.
The experiment results show thatdiscourse relation of embedding structure is themost powerful feature to predict the placement ofbecause.
E.g., when relation is ?cause?, ?con-trast?, ?example?
or ?explanation?, because oc-curs in the first span.
The heuristics obtained frommachine learning experiments can be applied toNLG systems.ReferencesBarbara Eugenio and Johanna Moore and MassimoPaolucci.
1997.
Learning Features that PredictCue Usage.
Proceedings of the 35th Conference ofthe Association for Computational Linguistics.Diane Litman.
1996.
Cue Phrase Classification Us-ing Machine Learning.
Journal of Artificial Intelli-gence Research, Vol.5, 53-94.Julia Hirschberg and Diane Litman.
1993.
Empiri-cal studies on the disambiguation of cue phrases.Computational Linguistics, 19(3) 501?530.Lynn Carlson and Daniel Marcu and Mary Okurowski.2001.
Discourse tagging reference manual.
USCInformation Science Institute (ISI) technical report.Megan Moser and Johanna Moore.
1995a.
Using dis-course analysis and automatic text generation tostudy discourse cue usage.
AAAI Spring Sympo-sium Series: Empirical Methods in Discourse Inter-pretation and Generation, 92-98.Megan Moser and Johanna Moore.
1995b.
Investi-gating cue selection and placement in tutorial dis-course.
Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics.Michael Elhadad and Kathleen McKeown.
1990.Generating connectives.
Proceedings of the 12thInternational Conference on Computational Lin-guistics.Randolph Quirk and Sidney Greenbaum and GeoffreyLeech and Jan Svartvik.
1972.
A Grammar of con-temporary English.
Longman, London.Sandra Williams.
2004.
Natural language generation(NLG) of discourse relations for different readinglevels.
Ph.D. Thesis, University of Aberdeen.Sholom Weiss and Casimir Kulikowski.
1991.
Com-puter Systems That Learn: Classification and Pre-diction Methods from Statistics, Neural Nets, Ma-chine Learning, and Expert Systems.
San Mateo,CA: Morgan Kaufmann.149
