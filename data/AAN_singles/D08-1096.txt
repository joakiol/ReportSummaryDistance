Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 917?926,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA graph-theoretic model of lexical syntactic acquisitionHinrich Schu?tze and Michael WalshInstitute for Natural Language ProcessingUniversity of Stuttgart, Germany{hs999,walsh}@ifnlp.orgAbstractThis paper presents a graph-theoretic model ofthe acquisition of lexical syntactic representa-tions.
The representations the model learnsare non-categorical or graded.
We propose anew evaluation methodology of syntactic ac-quisition in the framework of exemplar theory.When applied to the CHILDES corpus, theevaluation shows that the model?s graded syn-tactic representations perform better than pre-viously proposed categorical representations.1 IntroductionIn recent years, exemplar theory has had great ex-planatory success in phonetics.
Exemplar theoryposits that linguistic production and perception arenot mediated via abstract categories, but that insteadeach production and perception of a linguistic unitis stored and retained.
Linguistic inference then di-rectly operates on these stored exemplars.
In this pa-per, we propose a new approach to lexical syntacticacquisition in the framework of exemplar theory.Our approach uses an evaluation measure thatis different from previous work.
Lexical syntac-tic acquisition is most often evaluated with respectto standard syntactic categories like verb and noun.Our first contribution in this paper is that we insteadevaluate learned representations in the context of asyntactic task.
This task is the determination of anaspect of grammaticality that we call local syntacticcoherence.Our second contribution is a graph-theoreticmodel of the acquisition of lexical syntactic rep-resentations that is more rigorous than previousheuristic proposals.
The graph-theoretic modelcan learn both categorical and non-categorical (orgraded) representations.
The model is also a unifiedframework for syntagmatic and paradigmatic rela-tions (as will be discussed below), and for lower-order syntactic relations (those that can be directlyobserved from the input) and higher-order syntac-tic relations (those that require some generalizationfrom what is directly observable).Redington et al (1998) give an influential accountof the acquisition of lexical syntactic representationsin which a standard syntactic category like verb ornoun is assigned to each word.
Our third contribu-tion is to show that, in the context of acquisition,graded representations are superior to standard cat-egorical representations in supporting judgments oflocal syntactic coherence.
A graded representationformalism is one that, for any two words, can rep-resent a third word whose syntactic properties areintermediate between the two words (Manning andSchu?tze, 1999).Clearly exemplar theory is not the only frame-work in which lexical acquisition has been explored.Gleitman (1990) for example argues for syntacticbootstrapping to infer lexical semantics, work not atodds with our own (see discussion on the role of se-mantics below).
Our argument for the importanceof distributional evidence does not call into questionthe large body of work in child language acquisitionthat demonstrates that ?part of the capacity to learnlanguages must be ?innate?
?
(Gleitman and New-port, 1995).
Tabula rasa learning is not possible.
Ourgoal is not to show that language acquisition pro-ceeds with a minimum of inductive bias.
Rather, weattempt to formalize one aspect of language acquisi-tion, the use of distributional information.The paper is organized as follows.
Section 2 moti-vates the exemplar-theoretic approach by reviewingits success in phonetics.
Section 3 defines local syn-tactic coherence, which is the basis for a new evalu-ation methodology for the acquisition of lexical rep-resentations.
Section 4 develops the graph-theoreticmodel.
Section 5 compares graded and categoricalrepresentations for the task of inferring local syn-917tactic coherence.
Section 6 presents our evaluation.Sections 7 and 8 discuss related and future work, andpresent our conclusions.2 Exemplar theoryThe general idea of research into exemplars inspeech production and perception is that encoun-tered items (segments, words, sentences etc.)
arestored in great detail in memory along with richlinguistic and extra-linguistic context information.These exemplars are organized into clouds of mem-ory traces with similar traces lying close to eachother while dissimilar traces are more distant.
Anumber of such models have had great success inaccounting for production and perception phenom-ena in phonetics.
E.g., Johnson (1997) offers anexemplar model which challenges the notion thatspeech is perceived through a process of normal-ization whereby a speaker-specific representation ismapped or normalized into a speaker-neutral cate-gorical abstraction.
Johnson?s model successfullytreats aspects of vowel perception, sex identifica-tion, and speaker variability.
Crucially, no normal-ization of percepts into categorical representationstakes place.
The correct identification of phonemesand words in his model is a function of direct com-parison to richly detailed exemplars stored in mem-ory.
Other examples of exemplar-theoretic phoneticaccounts include (Goldinger, 1997), (Pierrehumbert,2001), and our own work (Schu?tze et al, 2007).
Ex-emplar theory?s success in phonetics motivates us toinvestigate its use as a model for local syntactic phe-nomena.3 Local syntactic coherenceIn the context sequence model for exemplar-theoretic phonetics (Wade et al, 2008), we representspeech using amplitude envelopes derived from theacoustic signal and then compute similarity as theintegral over the correlation of the two acoustic sig-nals.For the syntactic level, we need a representa-tion that has two key properties of the represen-tation we use in phonetics in order to support anexemplar-theoretic account.
First, the representa-tion must be directly derivable from the perceivedinput.
In particular, it cannot rely on the results ofany disambiguation that would occur either as partof exemplar-theoretic perception or in further down-stream processing.
Second, it must support similar-ity computations.
Accordingly, we first motivate therepresentation we use and then introduce a similaritymeasure on these representations.Representation.
There are two main sources1 ofdirectly observable information about the syntacticproperties of words: semantic cues (e.g., things areoften referred to with nouns) and the neighbors ofa word in sentences that it is used in.
In this pa-per, we only consider the second source of informa-tion for acquisition, lexical neighbors.2 We furtherlimit ourselves to the immediate left and right lexicalneighbors (see discussion in Section 7).When using lexical neighbors as the basis of rep-resentation, we have to make a basic choice as towhether we look at left and right neighbors sepa-rately or whether we only look at the ?correlated?neighborhood information of left and right neigh-bors jointly.
Our approach is based on the first alter-native: we separate the processing of left and rightneighbors.
We do this for two reasons.
First, gener-alization improves and model complexity decreasesif left-neighbor information and right-neighbor in-formation are looked at separately.
E.g., the rightneighbors of to, might and not are similar becauseall three words can be followed by base verbs likedance: to dance, might dance, (might) not dance.But their left neighbors are very different.Second, exemplar-theoretic similarity is best de-fined at the smallest possible scale in order to allowoptimal matching between parts of the stimulus andparts of memory.
In phonetics, we use a time scaleof 10s of milliseconds or even less.
Conceivably,one could also use segments (e.g., consonants andvowels) as the smallest unit; however, this wouldpresume a segmented signal.
And segmentation ispart of the perception task we want to explain in thefirst place.Separating left and right neighbors ?
whichamounts to looking at left and right local contextsof each word separately ?
is the smallest scale wecan operate at when doing syntactic matching.
We1A comprehensive account of acquisition must also includemorphology.
See Christiansen et al (2004).2Psycholinguistic evidence for the importance of neighborinformation for learning categories includes (Mintz, 2002).918choose this small scale for the same reasons as wechoose a small scale in phonetics: to ensure maxi-mum flexibility when matching parts of the stimuluswith exemplars in memory.
Using words, bigrams orlarger units would reduce the flexibility in matchingand require a larger amount of experience (or train-ing data) to learn a particular generalization.We refer to the representations of left and rightcontexts of a given word as half-words.
In otherwords, we split a word into two entities, a left half-word that characterizes its behavior to the left anda right half-word that characterizes its behavior tothe right.
Thus left-context and right-context com-ponents of the representation of a given focus wordare defined, where a left (right) half-word consistsof a probability distribution over all words that oc-cur to the left (right) of the focus word and thedimensionality of the vector for each word is de-pendent on the number of distinct neighbors (leftand right).
For example, having experienced takedoll twice and drop doll once, then the left con-text distribution, or left half-word of doll, dolll, isP (take) = 2/3, P (drop) = 1/3.
By extension, thephrase take the doll is represented as the followingsix half-words: takel, taker , thel, ther , dolll , anddollr .Distance measure.
The basic intuition behind lo-cal syntactic coherence is that an important compo-nent of syntactic wellformedness ?
and a compo-nent that is of particular importance in acquisition?
is whether a similar sequence has already beenstored as grammatical in memory.
The same waythat a phonetic signal that is well-formed in a partic-ular language has many similar exemplars in mem-ory, a syntactic sequence should also be licensed bysimilar, previously perceived sequences in memory.To operationalize this notion, we need to be able tocompute the similarity or distance between an in-put stimulus and exemplars in memory.
We do thisby first defining a distance measure for sequences offixed length.The distance ?
between two sequences of half-words < g1, .
.
.
, gn > and < h1, .
.
.
, hn > is de-fined to be the sum of the distances of their half-words:?
(<g1, .
.
.
, gn>,<h1, .
.
.
, hn>) =?ni=1 ?
(gi, hi)This definition presupposes a definition of the dis-tance of two half-words which will be given below.We then call a sequence of n half-wordsg1, .
.
.
, gn locally coherent if there is a sequenceh1, .
.
.
, hn in memory with ?
(< g1, .
.
.
, gn >,<h1, .
.
.
, hn >) < ?
where ?
is a parameter.Finally, we define a sentence to be locally n-coherent if all of its subsequences of length n arelocally coherent.The graph-theoretic model that is introduced inthe next section will be evaluated with respect tohow well it captures local syntactic coherence.
Thisenables us to evaluate the model with respect to atask as opposed to its ability to reproduce a particu-lar linguistic representation of syntactic categories.3Obviously, the notion of local syntactic coherenceonly captures some aspects of syntax ?
e.g., it doesnot capture long-distance dependencies.
However,it is a plausible component of syntactic competenceand a plausible intermediate step in the acquisitionof syntax.4 Graph-theoretic modelWe briefly review the structuralist notions of syntag-matic and paradigmatic relationships that have beenfrequently used in prior work in NLP (e.g., (Churchet al, 1994)).
De Saussure defined a syntagmaticrelationship between two words as their contigu-ous occurrence in a sentence and a paradigmatic re-lationship as mutual substitutability (de Saussure,1962) (although he used the term rapport associ-atif instead of paradigmatic).
E.g., brown and dogstand in a syntagmatic relationship with each otherin the phrase brown dog; brown and black stand in aparadigmatic relationship with each other with re-spect to the position between the and dog in thephrase the X dog.
De Saussure?s conceptualizationof syntactic relationships captures the fact that bothadmissible neighbors and admissible substitutes inlanguage are an important part of the characteriza-tion of the syntactic properties of a word.We formalize the two relations as distribu-tions over words, where we assume a vocabulary{w1, .
.
.
, wV } and V is the number of words in thevocabulary.We denote the left syntagmatic distribution of wi3Freudenthal et al (2004) have much the same motivationin introducing an evaluation measure of syntactic acquisitionbased on chunking.919by pi,s,l,m where i is the vocabulary index of wi, sstands for syntagmatic, l for left and m is the orderof the distribution as discussed below.
Intuitively,pi,s,l,m(wj) is the probability that word wj occurs tothe left of wi.
Similarly, for the left paradigmaticdistribution of wi, pi,p,l,m(wj) is the probability thatwj can be substituted for wi without changing localsyntactic coherence as far as the context to the leftis concerned.
Note that we distinguish between leftand right paradigmatic distributions.
A word wj canbe a perfect substitute for wi as far as the context tothe left is concerned, but a very unlikely substitute asfar as the context to the right is concerned.
E.g., inthe phrase She loves her job, the word him is a goodleft-context substitute for her, but a terrible right-context substitute for her.We will now show how the syntag-matic/paradigmatic (henceforth: syn/para) dis-tributions are defined iteratively, based on thebigram distribution pww, and grounded by definingpi,p,l,1 and pi,p,r,1.pww(wiwj) is the probability that the bigramwiwj occurs, that is, that wi and wj occur next toeach other (and in that order).
We define the V ?
Vjoint probability matrix J by Jij = pww(wiwj).Denote by N the diagonal V ?V matrix that con-tains in Nii the reciprocal of pw(wi) where pw is themarginal distribution of pww:V?j=1pww(wiwj) =V?j=1pww(wjwi) = pw(wi) =1NiiThe conditional probability pleft of the fol-lowing word and the conditional probabilitypright of the preceding word can be computedby multiplying (the transpose of) J and N :pleft(wi|wj) = pww(wiwj)/pw(wj) = (JN)ij ; andpright(wi|wj) = (JTN)ij .The ?grounding?
paradigmatic distributions of or-der 1 are defined as follows.pi,p,l,1(wj) = pi,p,r,1(wj) ={0 if wi 6= wj1 if wi = wjIn other words, each word has only one perfect left/ right substitute and that perfect substitute is itself.We define the syn/para distributions of higher orderrecursively:pi,s,l,m = JNpi,p,l,m (1)pi,p,r,m pi,s,r,mwomangirlboymanransanglaughedcriedFigure 1: The distribution of typical right neighbors (theright syntagmatic distribution pi,s,r,m) is computed fromthe distribution of typical ?right substitutes?
(the rightparadigmatic distribution pi,p,r,m).pi,p,l,m = JTNpi,s,l,m?1 (2)pi,s,r,m = JTNpi,p,r,m (3)pi,p,r,m = JNpi,s,r,m?1 (4)Basic matrix arithmetic shows that pi,s,l,1 is sim-ply pleft(.|wi) and pi,s,r,1 is pright(.|wi).For higher orders, the principle underlying Eq.s1?4 is that when moving from left to right, we usepright (that is, JTN ), the conditional distribution thatcharacterizes right neighbors; when moving fromright to left, we use pleft (that is, JN ), the condi-tional distribution that characterizes left neighbors.This is graphically shown in Fig.
1.As illustrated by Fig.
1, the underlying graph forpi,s,r,m and pi,p,r,m is a weighted bipartite directedgraph that connects the vocabulary on the left withthe vocabulary on the right.
A directed edge fromwi on the left to wj on the right is weighted withpww(wiwj)/pw(wi).
A directed edge from wj onthe right to wi on the left (not shown) is weightedwith pww(wiwj)/pw(wj).Eq.s 1?4 define four Markov chains:pi,s,l,m = (JNJTN)pi,s,l,m?1 (5)pi,p,l,m = (JTNJN)pi,p,l,m?1 (6)pi,s,r,m = (JTNJN)pi,s,r,m?1 (7)pi,p,r,m = (JNJTN)pi,p,r,m?1 (8)It is easy to see that pw is a stationary distributionfor Eq.
1?4.
Writing ~x for pw, we have:(JN~x)i =V?j=1pww(wiwj)pw(wj)pw(wj) = pw(wi) = xi(JTN~x)i =V?j=1pww(wjwi)pw(wj)pw(wj) = pw(wi) = xi920Hence, pw is a solution for Eq.s (5)?
(8).The series converge if JNJTN and JTNJNare ergodic, i.e., if the chain is aperiodic and irre-ducible (Kemeny and Snell, 1976).
Observe thatfor many simple probabilistic context-free gram-mars (PCFGs) the series in Eq.
1?4 will not con-verge.
For simple PCFGs, the alternation betweensyntagmatic and paradigmatic distributions is peri-odic.
E.g., if inflected verb forms only occur afternouns and nouns only before inflected verb forms,then the right syntagmatic distributions of nouns willhave non-zero activation only for verbs and the rightparadigmatic distributions of nouns will have non-zero activation only for nouns, thus preventing con-vergence.4The key difference between a simple PCFG andnatural language is ambiguity and noise.
Becauseof ambiguity and noise, JNJTN and JTNJN arelikely to be ergodic ?
there is always a small non-zero probability that two words can occur next toeach other.
Ambiguity and noise have the same ef-fect as teleportation for PageRank (Brin and Page,1998) in the sense that we can jump from each wordto each other word with non-zero probability.Assuming that the Markov chains are ergodic, allfour converge to pw: pi,p,r,?
= pi,p,l,?
= pi,s,r,?
=pi,s,l,?
= pw, for 1 ?
i ?
V .Thus, in this formalization, given enough itera-tions, syntagmatic and paradigmatic distributions ofwords eventually all become identical with the priordistribution pw.
This is surprising because linguisti-cally and computationally syntagmatic and paradig-matic relations are fundamentally different.However, on closer inspection, we observe thatlimiting the number of iterations is often beneficialwhen computing solutions to a problem iteratively.E.g., the expectation-maximization algorithm is of-ten stopped early because results close to conver-gence are worse than results obtained after a smallnumber of iterations.
From the point of view ofmodeling human language acquisition, early stop-ping is perhaps also more realistic since humans areunlikely to perform a large number of iterations.4However, non-ergodicity of JN does not imply non-ergodicity of JNJT N and JT NJN , so Eq.
(5)?
(8) can con-verge even for non-ergodic JN .ggggggg g g g g g g g g2 4 6 8 10 12 140.00.10.20.30.40.50.60.7iteration mJSdivergenceof rightsynt.distributionst t t t t t t t t t t t t t tgtelephant?giraffeelephant?theFigure 2: The distance between elephant and giraffe(measured by the Jensen-Shannon divergence) is accu-rately represented after a number of iterations.
The wordselephant and the retain their large distance.Example 1.
For the following matrix J???
?w1 w2 w3w1 82/1002 77/1002 112/1002w2 90/1002 18/1002 107/1002w3 99/1002 120/1002 297/1002???
?we get p1,s,r,1 = (0.31, 0.28, 0.41) by comput-ing the product JTNp1,p,r,1.
E.g., p1,s,r,1(w2) =pww(w1w2)/pw(w1) ?
1.0 = 77/(82 + 77 + 112) ?0.28.By iteration m = 4, the series pi,s,r,m (Eq.
(7))and pi,p,r,m (Eq.
(8)) have converged to:pi,s,r,m = pi,p,r,m = (0.2704, 0.2145, 0.5149)for all three words wi.
One can easily verify thatthis is pw.
E.g., pw(w1) = (82 + 90 + 99)/1002 =(82 + 77 + 112)/1002 ?
0.27045.Example 2.
We computed 15 iterations ofsyn/para distributions for the corpus: The girafferan.
An elephant fell.
The man ran.
An aunt fell.
Theman slept.
The aunt slept.
Fig.
2 shows that the dis-tance between the right syntagmatic distributions ofelephant and giraffe is large for m = 1.
The reasonis that the two words have no right neighbors in com-mon.
The right neighbors of the two words are ranand fell.
Although ran and fell have no left neighborsin common, their left neighbors have a right neigh-bor in common: the word slept.
This indirect simi-larity information is exploited to deduce by iteration92115 that the two words are very similar with respect totheir right syntactic context.
In contrast, no such in-ference, even a very indirect one, is possible for theright contexts of elephant and the.
Consequently, thedistance between the two distributions remains highand unchanged with higher iterations.In this case, the Markov chain is not ergodic andthe syntagmatic and paradigmatic series (Eq.s (5)?
(8)) do not converge to pw.5 Experimental evaluationRecall from Section 3 that our evaluation task is todiscriminate sentences that exhibit local coherencefrom those that do not; that sentences are repre-sented as sequences of half-words; that syntactic co-herence of a sentence is defined as all subsequencesof a given length n exhibiting local coherence; andthat a subsequence is locally coherent if its distancefrom a sequence in memory is less than ?.These definitions can be applied to the graphmodel as follows.
A left half-word is a left syntag-matic (or paradigmatic) distribution and a right half-word is a right syntagmatic (or paradigmatic) distri-bution.
We compute the distance of two half-wordseither as the Jensen-Shannon (JS) divergence (Lin,1991) or as (1?
cos(?)).
JS divergence is more ap-propriate for the comparison of probability distribu-tions.
But the cosine is more efficient when a sparsevector is compared to a dense vector.5 We thereforeemploy the cosine for the compute-intensive experi-ments in Section 6.The baseline representation is the categorical rep-resentation proposed by Redington et al (1998).
Adifficulty in replicating their experiments is that theyuse hierarchical agglomerative clustering (HAC),which eventually agglomerates all words in a sin-gle category.
To circumvent the need for a stop-ping criterion, we represent each word as the tem-poral sequence of clusters it occurred in during ag-glomeration and define the distance of two words asthe agglomeration step in which the two words arejoined in a cluster.
E.g., given the agglomeration se-quences {1}, {1, 2}, {1, 2, 4}, {1, 2, 3, 4} for w1 and{4}, {4}, {1, 2, 4}, {1, 2, 3, 4} for w4, the distance5This is so because, when computing the cosine, we can ig-nore all dimensions where one of the two vectors has a zerovalue.between w1 and w4 is 3 since they are joined in step3 when cluster {1, 2, 4} is created.For both graded (graph-theoretic) and categorical(cluster-based) representations, we need to set theparameter ?
that is the boundary between locally co-herent and locally incoherent sentences.
This pa-rameter gives rise to a precision-recall tradeoff.
Asmall ?
will impose strict requirements on which se-quences in memory match, resulting in false nega-tive decisions for local grammaticality.
A large ?will incorrectly judge many locally incoherent se-quences to be grammatical.We will pick the optimal ?
in both cases.
Forcategorical representations, this amounts to select-ing the HAC dendrogram with optimal performance.The experiment below evaluates whether grammati-cal and ungrammatical sentences are well separatedby the proposed measure.6Experiment on CHILDES.
We used the well-known CHILDES database (MacWhinney, 2000), acorpus of conversations between young children andtheir playmates, siblings, and caretakers.
In order toavoid mixing varieties of English (e.g., British En-glish vs. American English), we selected the largesthomogeneous subcorpus of CHILDES, the Manch-ester corpus.
It contains roughly 350,000 sentencesand 1.5 million words.
This is a conservative esti-mate of the amount of child-directed speech a childwould receive annually (Redington et al, 1998).
Allnames in the corpus (i.e., all capitalized words) werereplaced with a special word ?
n ?.
A boundarysymbol ?
b ?
was introduced to separate sentences.The representation of the corpus is then a concate-nation of all its sentences.
The vocabulary consistsof V = 8601 words.Construction of the evaluation set.
We testedthe ability of the two models to distinguish locallycoherent vs. incoherent sentences by selecting 100unattested sentences from the corpus, which werenot used to train the model.
We only selected unat-tested sentences that were not a substring of a sen-tence in the training corpus since, presumably, anysubstring of a sentence in the training corpus is lo-cally coherent.
A further constraint was that the6This evaluation of ?separation?
is not directly an evaluationof classification performance, but more similar to an evaluationof ranking using AUC or an evaluation of clustering using ameasure like purity.922unattested sentence was not allowed to contain aword that did not occur in the training corpus, therationale being that we want to address the prob-lem of local coherence for known words only sinceunknown words present special challenges.
Finally,we ensured that each unattested sentence containeda word that occurred in only one sentence type inthe training corpus.
In early experiments, we foundthat local grammatical inference for frequent wordsis easy as there is redundant evidence available thatcharacterizes legal syntactic environments for fre-quent words.
Since rare words are a key challenge insyntactic acquisition, we only selected sentences asunattested sentences that contained at least one rareword (where a rare word is defined as a word thatoccurs once in the training set).100 ungrammatical sentences were generated byrandomly selecting and concatenating words fromthe vocabulary.
Ungrammatical sentences werematched in length to unattested sentences, so thatboth sets contained the same number of sentencesof a given length.
As with unattested sentences, un-grammatical sentences that were substrings of sen-tences in the training corpus were eliminated.
Asthere are many more infrequent words than frequentwords in the vocabulary, the construction ensuredthat, as with unattested sentences, infrequent wordswere overrepresented in ungrammatical sentences.To summarize, our setup consists of 348,463training sentences, 100 unattested grammatical sen-tences and 100 ungrammatical sentences.The task of discriminating the 100 unattestedfrom the 100 ungrammatical sentences cannot besolved perfectly as CHILDES contains ungrammat-ical sentences, a few of which were randomly se-lected as unattested sentences (e.g., yes pleas, whichis missing the final letter).
Similarly, one or twoof the automatically generated ungrammatical sen-tences were actually grammatical.Since the test set does not consist of a randomsample of sentences, performance on the test set isnot a direct indicator of the percentage of sentencesthat the model can correctly discriminate in a child?stypical input.
A large proportion of sentences inchild input are simple 1-word, 2-word, and 3-wordsentences that even simplistic models can evaluatewith high accuracy.
However, the test set is appro-priate for a comparative evaluation of graded andxxxxxxx xx x2 4 6 8 100.50.60.70.80.91.0number of half wordsaccuracyof discriminationcc c cccccc cxcgradedcategoricalFigure 3: Accuracy of discrimination between grammati-cal and ungrammatical sentences for graded and categor-ical representations.categorical syntactic representations in language ac-quisition, which is one of the goals of the paper.
Dif-ficult sentences (those with rare words and greaterlength) are overrepresented in the test set as the dis-crimination of short sentences containing only fre-quent words can easily be done by simplistic mod-els.
Thus, a test set of ?easy?
sentences would notdistinguish good models from bad models.Discrimination experiment.
In order to train thegraph model, the entries of matrix J were estimatedusing maximum likelihood based on the trainingcorpus.
pi,s,l,1 and pi,s,r,1 were then computed forall 8601 words.
Replicating (Redington et al, 1998),the most frequent 1000 words were clustered (usingsingle-link HAC, Manning and Schu?tze (1999)).
Foreach remaining word w, the closest neighbor w?
inthe 1000 most frequent words was determined andw was then assigned to the cluster of w?.Fig.
3 shows the performance of graded and cat-egorical representations for different subsequencesizes n. To compute the accuracy for each n, the ?with optimal discrimination performance was cho-sen (for both graded and categorical).For a subsequence of size n = 1, the performanceis 0.5 in both cases since the 200-sentence test setdoes not contain unknown words.
So for every half-word, there is a sequence of one half-word in thetraining corpus with distance 0.
Thus, all sentences923get the same local coherence scores, both for gradedand categorical representations.This argument does not apply to n = 2 since weearlier defined a sentence to be locally coherent ifall of its subsequences are coherent.
While subse-quences of 2 half-words that are part of the sameword have local coherence score 0, this is not true ofsubsequences of 2 half-words that are part of differ-ent words, e.g., the subsequence <blackr,dogl> inblack dog.
If black dog does not occur in the train-ing set, then its local coherence score is > 0.The main result of the experiment is that exceptfor n=1 (p = 1) and n=2 (p = 0.39) the differencesbetween categorical and graded representations aresignificant (?2 test, p < 0.05 for 3 ?
n ?
10).
Thisis evidence that graded representations are more ac-curate when determining local syntactic coherenceand grammaticality than categorical representations.The experimental results demonstrate that, forsyntagmatic distributions of order 1, graded repre-sentations discriminate locally coherent vs. incoher-ent sentences better than categorical representations.We attribute this to the ability of exemplar theory toincorporate rich context information into discrimi-nation decisions.
This is of particular importancefor ambiguous words.
Categorical representations ofambiguous words are problematic because they areeither too similar or not similar enough to the twoalternatives.
E.g., if a word with a verb/noun ambi-guity is represented as one of the alternatives, say,as a verb, then subsequences containing its noun usewill no longer be similar to other subsequences withnouns.
If a special conflation category noun/verb isintroduced, then we are faced with the same prob-lem: subsequences containing the noun/verb cate-gory are not similar to subsequences containing ei-ther non-ambiguous verbs or non-ambiguous nouns.6 Higher-order distributionsThe main motivation for higher-order distributionsis that syntagmatic vectors of order 1 do not per-form well for some infrequent words.
In the ele-phant/giraffe example above, the distance betweenthe two words is close to maximum for order 1 repre-sentations because each occurs only once, in entirelydifferent contexts.
As we showed in Fig.
2, higher-order representations address this problem becausesssssssss2 4 6 8 100.700.750.800.850.900.95number of half wordsaccuracyof discriminationp pppppp pptttttt tttqq qqqq qqqsptqsynt?1para?2synt?2para?3Figure 4: Accuracy of discrimination between grammat-ical and ungrammatical sentences of the exemplar-basedmethod for different orders.
Key: synt = syntagmatic,para = paradigmatic; s is of order 1; p and t are of order2; q is of order 3.they exploit indirect evidence about the syntacticproperties of words.To evaluate higher-order representations onCHILDES, we used the same setup as before, butcomputed several additional iterations.
We also lim-ited the experiments to a subset consisting of 60,000words of the Manchester corpus.
It contains onlyV=1666 different words, which reduces the storagerequirements for the syn/para distributions (which is2 ?V 2 for each order) and the cost of the matrix mul-tiplications.
We also used (1?
cos(?))
instead of JSdivergence as distance measure.The results of the experiment are shown in Fig.
4.Higher-order representations are clearly superior forshort subsequences, especially for n = 2 and n = 3(and up to 5 half-words when comparing synt-1 andpara-2).
However, for long subsequences, there is noconsistent difference between the syntagmatic distri-bution of order 1 (synt-1) and higher order distribu-tions.
Apparently, the generalized information avail-able in higher orders is not helpful in local grammat-ical inference if long contexts are considered.We were surprised that the best-performing dis-tribution for short sequences is para-2 (paradigmaticdistribution of order 2), not a higher order distri-bution.
E.g., para-3 performs worse than para-2.924We would expect the performance to decrease withhigher order eventually since the distributions con-verge towards pw.
The fact that this happens so earlyin this experiment merits further investigation.7 Related workData-oriented parsing (Bod et al, 2003) sharesbasic assumptions about linguistic inference withexemplar-based theory, but it does not model or usethe similarity between input and stored exemplars.Previous work on exemplar theory in syntax (Abbot-Smith and Tomasello, 2006; Bybee, 2006; Hay andBresnan, 2006) has not been computational or for-mal.
Previous work on non-categorical representa-tions of words has viewed these representations asan intermediate step for arriving at categorical partsof speech (Redington et al, 1998; Schu?tze, 1995;Clark, 2003).
Consequently, all of these papers eval-uate their results by comparing induced categories togold-standard parts of speech.Redington et al (1998) did not find a difference incategorization accuracy between simple syntagmaticrepresentation and those using non-adjacent words.The BEAGLE model (Jones and Mewhort, 2007),and related work (Sahlgren et al, 2008), merges co-occurrence information and word order informationinto a single composite vector through a process ofvector convolution.
Our model differs in that it ex-plicitly captures the recursive relationship betweenthe orders in a unified framework.Previous graph-theoretic work (Biemann, 2006)uses order 1 representations.
Several papers havelooked at higher-order representations, but have notexamined the equivalence of syn/para distributionswhen formalized as Markov chains (Schu?tze andPedersen, 1993; Lund and Burgess, 1996; Edmonds,1997; Rapp, 2002; Biemann et al, 2004; Lemaireand Denhie`re, 2006).
Toutanova et al (2004) foundthat their graph model of predicate argument struc-ture deteriorated after a small number of iterationsof the random walk, similar to our findings.8 Conclusions and Future WorkIn this paper, we have presented a graph-theoreticmodel of the acquisition of lexical syntactic rep-resentations and a new exemplar-based evaluationof lexical syntactic acquisition.
When applied tothe CHILDES corpus, the evaluation shows thatthe graded syntactic representations learned by themodel perform significantly better than previouslyproposed categorical representations.
An initialevaluation of high-order representations showed lit-tle improvement over low-order representations.In future work, we intend to investigate the in-fluence of noise and ambiguity on the quality ofthe representations in order to characterize whenhigher order representations improve generalizationand exemplar-theoretic inference.
We also wantto address that the model as it currently stands istrained under the false assumption that the train-ing input is grammatical.
Ungrammatical test inputwhich matches a learned ungrammatical sequencewill be deemed grammatical.
Future work will ex-amine how to best treat this challenge, e.g., by usingan estimation of density instead of the simplistic ?1nearest neighbor?
distance used here.The most important future work concerns class-based language models.
The cognitive-linguistictradition we have mainly addressed in this paperhas focused on the task of learning traditional partsof speech and has usually not discussed the rele-vance of language models to acquisition.
If, as wehave argued, instead of learning traditional parts ofspeech the focus should be on performance in par-ticular language processing tasks (like grammatical-ity judgments), then language models are the nat-ural competing account that we must compare ourwork to.
Of particular relevance are class-based lan-guage models (e.g., (Saul and Pereira, 1997; Brownet al, 1992)).
In ongoing work, we are attemptingto show that the exemplar-theoretic model performsbetter on grammaticality judgments than class-basedlanguage models.Acknowledgements.
This research was funded bythe German Research Council (DFG, Grant SFB732).
We thank K. Rothenha?usler, H. Schmid andthe reviewers for their valuable comments.ReferencesAbbot-Smith, Kirsten and Michael Tomasello.
2006.Exemplar-learning and schematization in a usage-based account of syntactic acquisition.
The LinguisticReview, 23:275?290.925Biemann, Chris, Stefan Bordag, and Uwe Quasthoff.2004.
Automatic acquisition of paradigmatic relationsusing iterated co-occurrences.
In LREC.Biemann, Chris.
2006.
Unsupervised part-of-speech tag-ging employing efficient graph clustering.
In ACL.Bod, Rens, Remko Scha, and Khalil Sima?an.
2003.Data-Oriented Parsing.
CSLI Publications.Brin, Sergey and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.
InWWW, pages 107?117.Brown, Peter F., Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18(4):467?479.Bybee, Joan L. 2006.
From usage to grammar: Themind?s response to repetition.
Language, 82:711?733.Christiansen, Morten, Luca Onnis, Padraic Monaghan,and Nick Chater.
2004.
Happy endings in languageacquisition.
In AMLaP.Church, Kenneth, Patrick Hanks, Donald Hindle,William Gale, and Rosamund Moon.
1994.
Lexicalsubstitutability.
In Atkins, B.T.S.
and A. Zampolli, ed-itors, Computational Approaches to the Lexicon.
OUP.Clark, Alexander.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In EACL, pages 59?66.de Saussure, Ferdinand.
1962.
Cours de linguistiquege?ne?rale.
Payot, Paris.
Originally published in 1916.Edmonds, Philip.
1997.
Choosing the word most typicalin context using a lexical co-occurrence network.
InACL, pages 507?509.Freudenthal, Daniel, Julian Pine, and Fernand Gobet.2004.
Resolving ambiguities in the extraction of syn-tactic categories through chunking.
In ICCM.Gleitman, Lila and Elissa Newport.
1995.
The inventionof language by children: Environmental and biologi-cal influences on the acquisition of language.
In Gleit-man, Lila and Mark Liberman, editors, Language: Aninvitation to cognitive science.
MIT Press, 2nd edition.Gleitman, Lila.
1990.
The structural sources of verbmeanings.
Language Acquisition, 1:3?55.Goldinger, Stephen D. 1997.
Words and voices?perception and production in an episodic lexicon.
In(Johnson and Mullennix, 1997).Hay, Jennifer and Joan Bresnan.
2006.
Spoken syntax:The phonetics of giving a hand in New Zealand En-glish.
The Linguistic Review, 23.Johnson, Keith and John W. Mullennix, editors.
1997.Talker Variability in Speech Processing.
AcademicPress.Johnson, Keith.
1997.
Speech perception withoutspeaker normalization.
In (Johnson and Mullennix,1997).Jones, Michael N. and Douglas J.K. Mewhort.
2007.Representing word meaning and order information ina composite holographic lexicon.
Psychological Re-view, 114:1?37.Kemeny, John G. and J. Laurie Snell.
1976.
FiniteMarkov Chains.
Springer, New York.Lemaire, Benoit and Guy Denhie`re.
2006.
Effects ofhigh-order co-occurrences on word semantic similar-ity.
Behaviour, Brain & Cognition, 18(1).Lin, Jianhua.
1991.
Divergence measures based on theShannon entropy.
IEEE Trans.
Inf.
Theory, 37(1):145?151.Lund, Kevin and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instrumen-tation, and Computers, 28:203?208.MacWhinney, Brian.
2000.
The CHILDES project:Tools for analyzing talk.
Lawrence Erlbaum.Manning, Christopher D. and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press, Boston, MA.Mintz, Toben H. 2002.
Category induction from dis-tributional cues in an artificial language.
Memory &Cognition, 30:678?686.Pierrehumbert, Janet.
2001.
Exemplar dynamics: Wordfrequency, lenition and contrast.
In Bybee, Joan andPaul Hopper, editors, Frequency and the Emergence ofLinguistic Structure, pages 137?157.
Benjamins.Rapp, Reinhard.
2002.
The computation of word as-sociations: comparing syntagmatic and paradigmaticapproaches.
In Coling.Redington, Martin, Nick Chater, and Steven Finch.1998.
Distributional information: A powerful cuefor acquiring syntactic categories.
Cognitive Science,22(4):425?469.Sahlgren, Magnus, Anders Holst, and Jussi Karlgren.2008.
Permutations as a means to encode order inword space.
In CogSci.Saul, Lawrence and Fernando Pereira.
1997.
Aggre-gate and mixed-order markov models for statisticallanguage processing.
In EMNLP, pages 81?89.Schu?tze, Hinrich and Jan Pedersen.
1993.
A vectormodel for syntagmatic and paradigmatic relatedness.In UW Centre for the New OED and Text Research.Schu?tze, Hinrich, Michael Walsh, Travis Wade, andBernd Mo?bius.
2007.
Towards a unified exemplar-theoretic model of phonetic and syntactic phenomena.In CogSci, Poster Session.Schu?tze, Hinrich.
1995.
Distributional part-of-speechtagging.
In EACL, pages 141?148.Toutanova, Kristina, Christopher D. Manning, and An-drew Y. Ng.
2004.
Learning random walk models forinducing word dependency distributions.
In ICML.Wade, Travis, Grzegorz Dogil, Hinrich Schu?tze, MichaelWalsh, and Bernd Mo?bius.
2008.
Syllable fre-quency effects in a context-sensitive segment produc-tion model.
Submitted.926
