Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 489?500, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsJoint Entity and Event Coreference Resolution across DocumentsHeeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, Dan JurafskyStanford University, Stanford, CA 94305{heeyoung,recasens,angelx,mihais,jurafsky}@stanford.eduAbstractWe introduce a novel coreference resolutionsystem that models entities and events jointly.Our iterative method cautiously constructsclusters of entity and event mentions using lin-ear regression to model cluster merge opera-tions.
As clusters are built, information flowsbetween entity and event clusters through fea-tures that model semantic role dependencies.Our system handles nominal and verbal eventsas well as entities, and our joint formulationallows information from event coreference tohelp entity coreference, and vice versa.
In across-document domain with comparable doc-uments, joint coreference resolution performssignificantly better (over 3 CoNLL F1 points)than two strong baselines that resolve entitiesand events separately.1 IntroductionMost coreference resolution systems focus on enti-ties and tacitly assume a correspondence betweenentities and noun phrases (NPs).
Focusing on NPsis a way to restrict the challenging problem of coref-erence resolution, but misses coreference relationslike the one between hanged and his suicide in (1),and between placed and put in (2).1.
(a) One of the key suspected Mafia bosses ar-rested yesterday has hanged himself.
(b) Police said Lo Presti had hanged himself.
(c) His suicide appeared to be related to clan feuds.2.
(a) The New Orleans Saints placed Reggie Bushon the injured list on Wednesday.
(b) Saints put Bush on I.R.As (1c) shows, NPs can also refer to events, andso corefer with phrases other than NPs (Webber,1988).
By being anchored in spatio-temporal dimen-sions, events represent the most frequent referent ofverbal elements.
In addition to time and location,events are characterized by their participants or ar-guments, which often correspond with discourse en-tities.
This two-way feedback between events andtheir arguments (or entities) is the core of our ap-proach.
Since arguments play a key role in describ-ing an event, knowing that two arguments coreferis useful for finding coreference relations betweenevents, and knowing that two events corefer is use-ful for finding coreference relations between enti-ties.
In (1), the coreference relation between Oneof the key suspected Mafia bosses arrested yesterdayand Lo Presti can be found by knowing that theirpredicates (i.e., has hanged and had hanged) core-fer.
On the other hand, the coreference relations be-tween the arguments Saints and Bush in (2) helpsto determine the coreference relation between theirpredicates placed and put.In this paper, we take a holistic approach to coref-erence.
We annotate a corpus with cross-documentcoreference relations for nominal and verbal men-tions.
We focus on both intra and inter-documentcoreference because this scenario is at the same timemore challenging and more relevant to real-worldapplications such as news aggregation.
We use thiscorpus to train a model that jointly addresses refer-ences to both entities and events across documents.The contributions of this work are the following:?
We introduce a novel approach for entity andevent coreference resolution.
At the core of489our approach is an iterative algorithm that cau-tiously constructs clusters of entity and eventmentions using linear regression to model clus-ter merge operations.
Importantly, our modelallows information to flow between clusters ofboth types through features that model contextusing semantic role dependencies.?
We annotate and release a new corpus withcoreference relations between both entities andevents across documents.
The relations anno-tated are both intra and inter-document, whichmore accurately models real-world scenarios.?
We evaluate our cross-document coreferenceresolution system on this corpus and show thatour joint approach significantly outperformstwo strong baselines that resolve entities andevents separately.2 Related WorkEntity coreference resolution is a well studied prob-lem with many successful techniques for identify-ing mention clusters (Ponzetto and Strube, 2006;Haghighi and Klein, 2009; Stoyanov et al2009;Haghighi and Klein, 2010; Raghunathan et al2010;Rahman and Ng, 2011, inter alia).
Most of thesetechniques focus on matching compatible noun pairsusing various syntactic and semantic features, withefforts targeted toward improving features and clus-tering models.Prior work showed that models that jointly resolvementions across multiple entities result in better per-formance than simply resolving mentions in a pair-wise fashion (Denis and Baldridge, 2007; Poon andDomingos, 2008; Wick et al2008; Lee et al2011,inter alia).
A natural extension is to perform coref-erence jointly across both entities and events.
Yetthere has been little attempt in this direction.We know of only limited work that incorporatesevent-related information in entity coreference, typ-ically by incorporating the verbs in context as fea-tures.
For instance, Haghighi and Klein (2010) in-clude the governor of the head of nominal mentionsas features in their model.
Rahman and Ng (2011)also used event-related information by looking atwhich semantic role the entity mentions can haveand the verb pairs of their predicates.
We confirmthat such features are useful but also show that thecomplementary features for verbal mentions lead toeven better performance, especially when event andentity clusters are jointly modeled.Compared to the extensive work on entity coref-erence, the related problem of event coreference re-mains relatively under-explored, with minimal workon how entity and event coreference can be con-sidered jointly on an open domain.
Early work onevent coreference for MUC (Humphreys et al1997;Bagga and Baldwin, 1999) focused on scenario-specific events.
More recently, there have beenapproaches that looked at event coreference forwider domains.
Chen and Ji (2009) proposed us-ing spectral graph clustering to cluster events.
Be-jan and Harabagiu (2010) proposed a nonparamet-ric Bayesian model for open-domain event resolu-tion.
However, most of this prior work focused onlyon event coreference, whereas we address both en-tities and events with a single model.
Humphreyset al1997) considered entities as well as events,but due to the lack of a corpus annotated with eventcoreference, their approach was only evaluated im-plicitly in the MUC-6 template filling task.
To ourknowledge, the only previous work that consideredentity and event coreference resolution jointly isHe (2007), but limited to the medical domain andfocused on just five semantic categories.3 ArchitectureFollowing the intuition introduced in Section 1, ourapproach iteratively builds clusters of event and en-tity mentions jointly.
As more information becomesavailable (e.g., finding out that two verbal mentionshave arguments that belong to the same entity clus-ter), the features of both entity and event mentionsare re-generated, which prompts future clusteringoperations.
Our model follows a cautious (or ?babysteps?)
approach, which we previously showed to besuccessful for entity coreference resolution (Raghu-nathan et al2010; Lee et al2011).
However,unlike our previous work, which used deterministicrules, in this paper we learn a coreference resolutionmodel using linear regression.
Algorithm 1 summa-rizes the flow of the proposed algorithm.
We detailits steps next.
We describe the training procedure inSection 4 and the features used in Section 5.490Algorithm 1: Joint Coreference Resolutioninput : set of documents Dinput : coreference model ?// clusters of mentions:E= {}1// clusters of documents:C = clusterDocuments(D)2foreach document cluster c in C do3// all mentions in one doc cluster:M = extractMentions(c)4// singleton mention clusters:E ?
= buildSingletonClusters(M)5// high-precision deterministic sieves:E ?
= applyHighPrecisionSieves(E ?
)6// iterative event/entity coreference:while ?
e1, e2 ?
E ?s.t.
score(e1, e2,?)
> 0.5 do7(e1, e2) = arg max e1,e2?E?
score(e1, e2,?
)8E ?
= merge(e1, e2, E ?
)9// pronoun sieve:E ?
= applyPronounSieve(E ?
)10// append to global output:E = E + E ?11output : E3.1 Document ClusteringOur approach starts with several steps that reducethe search space for the actual coreference resolutiontask.
The first is document clustering, which clustersthe set of input documents (D) into a set of docu-ment clusters (C).
In the subsequent steps we onlycluster mentions that appear in the same documentcluster.
We found this to be very useful in practicebecause, in addition to reducing the search space, itprovides a word sense disambiguation mechanismbased on corpus-wide topics.
For example, with-out document clustering, our algorithm may decideto cluster two mentions of the verb hit, but know-ing that one belongs to a cluster containing earth-quake reports and the other to a cluster with reportson criminal activities, this decision can be avoided.1Any non-parametric clustering algorithm can beused in this step.
In this paper, we used the algo-rithm proposed by Surdeanu et al2005).
This algo-rithm is an Expectation Maximization (EM) variantwhere the initial points (and the number of clusters)are selected from the clusters generated by a hierar-chical agglomerative clustering algorithm using ge-1Since different mentions of the verb say in the same topicmight refer to different events, they are only merged if they havecoreferent arguments.ometric heuristics.
This algorithm performs well onour data.
For example, in the training dataset, onlytwo topics (handling different earthquake events) areincorrectly merged into the same cluster.3.2 Mention ExtractionIn this step (4 in Algorithm 1) we extract nominal,pronominal, and verbal mentions.
We extract nom-inal and pronominal mentions using the mentionidentification component in the publicly download-able Stanford coreference resolution system (Raghu-nathan et al2010; Lee et al2011).
We consideras verbal mentions all words whose part of speechstarts with VB, with the exception of some auxil-iary/copulative verbs (have, be and seem).
For eachof the identified mentions we build a singleton clus-ter (step 5 in Algorithm 1).Crucially, we do not make a formal distinction be-tween entity and event mentions.
This distinction isnot trivial to implement (e.g., is the noun earthquakean entity or an event mention?)
and an imperfectclassification would negatively affect the followingcoreference resolution.
Instead, we simply classifymentions into verbal or nominal, and use this dis-tinction later during feature generation (Section 5).To compare event nouns (e.g., development) withverbal mentions, the ?derivationally related form?relation in WordNet is used.3.3 High-precision Entity Resolution SievesTo further reduce the problem?s search space, instep 6 of Algorithm 1 we apply a set of high-precision filters from the Stanford coreference res-olution system.
This system is a collection of deter-ministic models (or ?sieves?)
for entity coreferenceresolution that incorporate lexical, syntactic, seman-tic, and discourse information.
These sieves are ap-plied from higher to lower precision.
As clusters arebuilt, information such as mention gender and num-ber is propagated across mentions in the same clus-ter, which helps subsequent decisions.
The Stanfordsystem obtained the highest score at the CoNLL-2011 shared task on English coreference resolution.For this step, we selected all the sieves from theStanford system with the exception of the pronounresolution sieve.
All the remaining sieves (listedin Table 1) have high precision because they em-ploy linguistic heuristics with little ambiguity, e.g.,491High-precision sievesDiscourse processing sieveExact string match sieveRelaxed string match sievePrecise constructs sieve (e.g., appositives)Strict head match sievesProper head noun match sieveRelaxed head matching sieveTable 1: Deterministic sieves in step 6 of Algorithm 1.one sieve clusters together two entity mentions onlywhen they have the same head word.
Note that allthese heuristics were designed for within-documentcoreference.
They work well in our context be-cause we apply them in individual document clus-ters, where the one-sense-per-discourse principlestill holds (Yarowsky, 1995).Importantly, these sieves do not address verbalmentions.
That is, all verbal mentions are still in sin-gleton clusters after this step.
Furthermore, none ofthese sieves use features that facilitate the joint reso-lution of nominal and verbal mentions (e.g., featuresfrom semantic role frames).
All these limitations areaddressed next.3.4 Iterative Entity/Event ResolutionIn this stage (steps 7 ?
9 in Algorithm 1), we con-struct entity and event clusters using a cautious or?baby steps?
approach.
We use a single linear re-gressor (?)
to model cluster merge operations be-tween both verbal and nominal clusters.
Intuitively,the linear regressor models the quality of the mergeoperation, i.e., a score larger than 0.5 indicates thatmore than half of the mention pairs introduced bythis merge are correct.
We discuss the training pro-cedure that yields this scoring function in Section 4.In each iteration, we perform the merge operationthat has the highest score.
Once two clusters aremerged (step 9) we regenerate all the mention fea-tures to reflect the current clusters.
We stop when nomerging operation with an overall benefit is found.This iterative procedure is the core of our jointcoreference resolution approach.
This algorithmtransparently merges both entity and event men-tions and, importantly, allows information to flowbetween clusters of both types as merge operationstake place.
For example, assume that during iter-ation i we merge the two hanged verbs in the firstexample in Section 1 (because they have the samelemma).
Because of this merge, in iteration i+ 1 thenominal mentions Lo Presti and One of the key sus-pected Mafia bosses have the same semantic role forverbs assigned to the same cluster.
This is a stronghint that these two nominal mentions belong to thesame cluster.
Indeed, the feature that models thisstructure received one of the highest weights in ourlinear regression model (see Section 7).3.5 Pronoun SieveOur approach concludes with the pronominal coref-erence resolution sieve from the Stanford system.This sieve is necessary because our current reso-lution algorithm ignores mention ordering and dis-tance (i.e., in step 7 we compare all clusters regard-less of where their mentions appear in the text).
Asprevious work has proved, the structure of the text iscrucial for pronominal coreference (Hobbs, 1978).For this reason, we handle pronouns outside of themain algorithm block.4 Training the Cluster Merging ModelTwo observations drove our choice of model andtraining algorithm.
First, modeling the merge op-eration as a classification task is not ideal, becauseonly a few of the resulting clusters are entirely cor-rect or incorrect.
In practice, most of the clusterswill contain some mention pairs that are correct andsome that are not.
Second, generating training datafor the merging model is not trivial: a brute forceapproach that looks at all the possible combinationsis exponential in the number of mentions.
This isboth impractical and unnecessary, as some of thesecombinations are unlikely to be seen in practice.We address these observations with Algorithm 2.The algorithm uses gold coreference labels to train alinear regressor that models the quality of the clus-ters produced by merge operations.
We define thequality score q of a new cluster as the percentage ofnew mention pairs (i.e., not present in either one ofthe clusters to be merged) that are correct:q =linkscorrectlinkscorrect + linksincorrect(1)where links(in)correct is the number of newly intro-duced (in)correct pairwise mention links when twoclusters are merged.492Algorithm 2: Training Procedureinput : set of documents Dinput : correct mention clusters GC = clusterDocuments(D)1// linear regression coreference model:?
= assignInitialWeights(C,G)2// repeat for T epochs:for t = 1 to T do3// training data for linear regressor:?
= {}4foreach document cluster c in C do5M = extractMentions(c)6E = buildSingletonClusters(M)7E = applyHighPrecisionSieves(E)8// gather training examples// as clusters are built:while ?
e1, e2 ?
Es.t.
sco(e1, e2,?)
> 0.5 do9forall e?1, e?2 ?
E do10q = qualityOfMerge(e?1, e?2,G)11?
= append(e?1, e?2, q,?
)12(e1, e2) = arg max e1,e2?E sco(e1, e2,?
)13E = merge(e1, e2, E)14// train using data from last epoch:??
= trainLinearRegressor(?
)15// interpolate with older model:?
= ??
+ (1?
?)?
?16output : ?We address the potential explosion in training datasize by considering only merge operations that arelikely to be inspected by the algorithm as it runs.To achieve this, Algorithm 2 repeatedly runs the ac-tual clustering algorithm (as given by the currentmodel ?)
over the training dataset (steps 5 ?
14).2When the algorithm iteratively constructs its clus-ters (steps 9 ?
14), we generate training data fromall possible cluster pairs available during a particulariteration (steps 10 ?
12).
For each pair, we computeits score using Equation 1 (step 11) and add it to thetraining corpus ?
(step 12).
Note that this avoids in-specting many of the possible cluster combinations:once a cluster is built (e.g., during the previous iter-ations or by the deterministic sieves in step 8), wedo not generate training data from its members, butrather treat it as an atomic unit.
On the other hand,our approach generates more training data than on-line learning, which trains using only the actual de-cisions taken during inference in each iteration (i.e.,2We skip the pronoun sieve here because it does not affectthe decisions taken during the iterative resolution steps.the pair (e1, e2) in step 13).After each epoch we have a new training cor-pus ?, which we use to train the new linear regres-sion model ??
(step 15), which is then interpolatedwith the old one (step 16).Our training procedure is similar in spirit to trans-formation based learning (TBL) (Brill, 1995).
Sim-ilarly to TBL, our approach repeatedly applies themodel over the training data and attempts to mini-mize the error rate of the current model.
However,while TBL learns rules that directly minimize thecurrent error rate, our approach achieves this indi-rectly, by incorporating the reduction in error rate inthe score of the generated datums.
This allows usto fit a linear regression to this task, which, as dis-cussed before, is a better model for this task.Just like any hill-climbing algorithm, our ap-proach has the risk of converging to a local max-imum.
To mitigate this risk, we do not initializeour model ?
with random weights, but rather usehints from the deterministic sieves.
This procedure(listed in step 2) runs the high-precision sieves in-troduced in Section 3.3 and, just like the data gen-eration loop in Algorithm 2, creates training exam-ples from the clusters available after every mergeoperation.
Since these deterministic models addressonly nominal clusters, at the end we generate train-ing data for events by inspecting all the pairs of sin-gleton verbal clusters.
Using this data, we train theinitial linear regression model.We trained our model using L2 regularized linearregression with a regularization coefficient of 1.0.We did not tune the regularization coefficient.
Weran the training algorithm for 10 epochs, althoughwe observed minimal changes after three epochs.We tuned the interpolation weight (?)
to a valueof 0.7 using our development corpus.5 FeaturesWe list in Table 2 the features used by the lin-ear regression model.
As the table indicates, ourfeature set relies heavily on semantic roles, whichwere extracted using the SwiRL semantic role la-beling (SRL) system (Surdeanu et al2007).3 Be-cause SwiRL addresses only verbal predicates, weextended it to handle nominal predicates.
In this3http://www.surdeanu.name/mihai/swirl/493Feature NameApplies toEntities (E)or Events (V)Description and ExampleEntity Heads ECosine similarity of the head-word vectors of two clusters.
The head-word vectorstores the head words of all mentions in a cluster and their frequencies.
For example,the vector for the three-mention cluster {Barack Obama, President Obama, USpresident}, is {Obama:2, president:1}.Event Lemmas VCosine similarity of the lemma vectors of two clusters.
For example, the lemmavector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}.Links betweenSynonymsE, VThe percentage of newly-introduced mention links after the merge that are WordNetsynonyms (Fellbaum, 1998).
For example, when merging the following two clus-ters, {hit, strike} and {strike, join, say}, two out of the six new links are betweenwords that belong to the same WordNet synset: (hit ?
strike) and (strike ?
strike).Number of CoreferentArguments orPredicatesE, VThe total number of shared arguments and predicates between mentions in thetwo clusters.
We use the cluster IDs of the corresponding arguments/predicatesto check for identity.
For example, when comparing the event clusters {bought}and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and[AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because the two men-tions share one Arg0 and one Arg1 argument (assuming that the clusters {AMD,AMD} and {ATI, ATI} were previously created).
For entity clusters, this featurecounts the number of coreferent predicates.
In addition to PropBank-style roles, forevent mentions we also include the closest left and right entity mentions in order tocapture any arguments missed by the SRL system.Coreferent Argumentsin a Specific Role?E, VIndicator feature set to 1 if the two clusters have at least one coreferent argument ina given role.
We generate one variant of this feature for each argument label, e.g.,Arg0, Arg1, etc.
For example, the value of this feature for Arg0 for the clusters{bought} and {acquired} in the above example is 1.Coreferent Predicate ina Specific Role?EIndicator feature set to 1 if the two clusters have at least one coreferent predicate fora given role.
For example, for the clusters {the man} and {the person}, extractedfrom the sentences helped [the man]Arg1 and helped [the person]Arg1, the value ofthis feature is 1 if the two helped verbs were previously clustered together.2nd Order Similarity ofMention WordsECosine similarity of vectors containing words that are distributionally similar towords in the cluster mentions.
We built these vectors by extracting the top-tenmost-similar words in Dekang Lin?s similarity thesaurus (Lin, 1998) for all thenouns/adjectives/verbs in a cluster.
For example, for the singleton cluster {a newhome}, we construct this vector by expanding new and home to: {new:1, original:1,old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1,small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1,mansion:1, school:1, restaurant:1, hospital:1 }.Number; Animacy;Gender; NE LabelECosine similarity of number, gender, animacy, and NE label vectors.
For example,the number and gender vectors for the two-mention cluster {systems, a pen} areNumber = {singular:1, plural:1}, Gender = {neutral:2}.Table 2: List of features used when comparing two clusters.
If any of the two clusters contains a verbal mention weconsider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters.
Weappend to all entity features the suffix Proper or Common based on the type of the head word of the first mention ineach of the two clusters.
We use the suffix Proper only if both head words are proper nouns.paper we used a single heuristic: the possessor ofa nominal event?s predicate is marked as its Arg0,e.g., Logan is the Arg0 to run in Logan?s run.44A principled solution to this problem is to use an SRL sys-tem for nominal predicates trained using NomBank (Meyers etal., 2004).
We will address this in future work.494We extracted named entity labels using the namedentity recognizer from the Stanford CoreNLP suite.6 Evaluation6.1 CorpusThe training and test data sets were derived fromthe EventCorefBank (ECB) corpus5 created by Be-jan and Harabagiu (2010) to study event coreferencesince standard corpora such as OntoNotes (Pradhanet al2007) contain a small number of annotatedevent clusters.
The ECB corpus consists of 482 doc-uments from Google News clustered into 43 topics,where a topic is described as a seminal event.
Thereason for including comparable documents was toincrease the number of cross-document coreferencerelations.
Bejan and Harabagiu (2010) only anno-tated a selection of events.For the purpose of our study, we extended theoriginal corpus in two directions: (i) fully anno-tated sentences, and (ii) entity coreference relations.In addition, we removed relations other than coref-erence (e.g., subevent, purpose, related, etc.)
thathad been originally annotated.
We revised and com-pleted the original annotation by annotating everyentity and event in the sentences that were (partially)annotated.
The annotation was performed by fourexperts, using the Callisto annotation tool.6 Theannotation guidelines and the generated corpus areavailable here.7Our annotation of the ECB corpus followed theOntoNotes (Pradhan et al2007) standard for coref-erence annotation, with a few extensions to handleevents.
For nouns, we annotated full NPs (with allmodifiers), excluding appositive phrases and nomi-nal predicates.
Only premodifiers that were propernouns or possessive phrases were annotated.
Forevents, we annotated the semantic head of the verbphrase.
We extended the OntoNotes guidelines byalso annotating singletons (but we do not scorethem; see below), and by including all events men-tions (not only those mentioned at least once with anNP).
This required us to be specific with respect to:5http://faculty.washington.edu/bejan/data/ECB1.0.tar.gz6http://callisto.mitre.org7http://nlp.stanford.edu/pubs/jcoref-corpus.zipTraining Dev Test Total# Topics 12 3 28 43# Documents 112 39 331 482# Entities 459 46 563 1068# Entity Mentions 1723 259 3465 5447# Events 300 30 444 774# Event Mentions 751 140 1642 2533Table 3: Corpus statistics.
?ENTITY COREFID=?26??
A publicist ?/ENTITY?
?EVENTCOREFID=?4??
says ?/EVENT?
?ENTITY COREFID=?23?
?Tara Reid ?/ENTITY?
has ?EVENT COREFID=?3??
checked?/EVENT?
?ENTITY COREFID=?23??
herself ?/ENTITY?
?EVENTCOREFID=?3*??
into ?/EVENT?
?ENTITY COREFID=?28??
rehab?/ENTITY?.Figure 1: Annotation example.Light verbs Verbs such as give and make followedby a noun (e.g., make an offer) were not anno-tated, but the noun was.Phrasal verbs We annotated the verb together withthe preposition or adverb (e.g., check in).Idioms They were annotated with all their elements(e.g., booze it up).The first topic was annotated by all four anno-tators as burn-in.
Afterwards, annotation disagree-ments were resolved between all annotators and thenext three topics were annotated again by all four an-notators to measure agreement.
Following Passon-neau (2004), we computed an inter-annotator agree-ment of ?
= 0.55 (Krippendorff, 2004) on thesethree topics, indicating moderate agreement amongthe annotators.
Given the complexity of the task, weconsider this to be a good score.
For example, theaverage of the CoNLL F1 between any two annota-tors is 73.58, which is much higher than the systemscores reported in the literature.After annotating the four topics, disagreementswere resolved again and all the documents in thefour topics were corrected to match the consensus.The rest of the corpus was split between the four an-notators, and each document was annotated by a sin-gle annotator.
Figure 1 shows an example.
Table 3shows the corpus statistics, including the training,development (dev) and test set splits.
The dev topicswere used for tuning the interpolation parameter ?from Section 4.495MUC B3 CEAF-?4 BLANCSystem R P F1 R P F1 R P F1 R P F1 CoNLL F1Baseline 1Wo/ SRLEntity 47.4 72.3 57.2 44.1 82.7 57.5 42.5 21.9 28.9 60.1 78.3 64.8 47.9Event 56.0 56.8 56.4 59.8 71.9 65.3 32.2 31.6 31.9 63.5 68.8 65.7 51.2Both 49.9 75.4 60.0 44.9 83.9 58.5 46.2 23.3 31.0 60.9 81.2 66.1 49.8Baseline 2With SRLEntity 52.7 73.0 61.2 48.6 80.8 60.7 41.8 24.1 30.6 63.4 78.4 68.2 50.8Event 59.2 57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6This paperEntity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9Table 4: Performance of the two baselines and our model.
We report scores for entity clusters, event clusters and thecomplete task using five metrics.6.2 EvaluationWe use five coreference evaluation metrics widelyused in the literature:MUC (Vilain et al1995) Link-based metric whichmeasures how many predicted and gold clus-ters need to be merged to cover the gold andpredicted clusters, respectively.B3 (Bagga and Baldwin, 1998) Mention-basedmetric which measures the proportion of over-lap between predicted and gold clusters for agiven mention.CEAF (Luo, 2005) Entity-based metric that, unlikeB3, enforces a one-to-one alignment betweengold and predicted clusters.
We employ theentity-based version of CEAF.BLANC (Recasens and Hovy, 2011) Metric basedon the Rand index (Rand, 1971) that consid-ers both coreference and non-coreference linksto address the imbalance between singleton andcoreferent mentions.CoNLL F1 Average of MUC, B3, and CEAF-?4.This was the official metric in the CoNLL-2011shared task (Pradhan et al2011).We followed the CoNLL-2011 evaluation methodol-ogy, that is, we removed all singleton clusters, andapposition/copular relations before scoring.We evaluated the systems on three different set-tings: only on entity clusters, only on event clus-ters, and on the complete task, i.e., both entities andevents.
Note that the gold corpus separates clustersinto entity and event clusters (see Table 3), but oursystem does not make this distinction at runtime.In order to compute the entity-only and event-onlyscores in Table 4, we implemented the followingprocedure: (a) when scoring entity clusters, we re-moved all mentions that were found to be coreferentwith at least one gold event mention and not coref-erent with any gold entity mentions; and (b) we per-formed the opposite action when scoring event clus-ters.
This procedure is necessary because our men-tion identification component is not perfect, i.e., itgenerates mentions that do not exist in the gold an-notation.
Furthermore, this procedure is conserva-tive with respect to the clustering errors of our sys-tem, e.g., all spurious mentions that our system in-cludes in a cluster with a gold entity mention areconsidered for the entity score, regardless of theirgold type (event or entity).6.3 ResultsTable 4 compares the performance of our systemagainst two strong baselines that resolve entities andevents separately.
Baseline 1 uses a modified Stan-ford coreference resolution system after our doc-ument clustering and mention identification steps.Because the original Stanford system implementsonly entity coreference, we extended it with an extrasieve that implements lemma matching for events.This additional sieve merges two verbal clusters(i.e., clusters that contain at least one verbal men-tion) or a verbal and a nominal cluster when at leasttwo lemmas of mention head words are the same be-tween clusters, e.g., helped and the help.The second baseline adds two more sieves toBaseline 1.
Both these sieves model entity and event496contextual information using semantic roles.
Thefirst sieve merges two nominal clusters when twomentions in the respective clusters have the samehead words and two mentions (possibly with dif-ferent heads) modify with the same role label twopredicates that have the same lemma.
For exam-ple, this sieve merges the clusters {Obama, the pres-ident} (seen in the text [Obama]Arg0 attended and[the president]Arg1 was elected) and {Obama} (seenin the text [Obama]Arg1 was elected), because theyshare a mention with the same head word (Obama)and two mentions modify with the same role (Arg1)predicates with the same lemma (elect).
The sec-ond sieve implements the complementary action forevent clusters.
That is, it merges two verbal clusterswhen at least two mentions have the same lemmaand at least two mentions have semantic argumentswith the same role label and the same lemma.7 DiscussionThe first block in Table 4 indicates that lemmamatching is a strong baseline for event resolution.Most of the event scores for Baseline 1 are actuallyhigher than the corresponding entity scores, whichwere obtained using the highest ranked system at theCoNLL-2011 shared task (Lee et al2011).
Addingcontextual information using semantic roles (Base-line 2) helps both entities and events.
The CoNLLF1 for Baseline 2 increases almost 3 points for enti-ties and 1 point for events.
This demonstrates thatlocal syntactico-semantic context is important forcoreference resolution even in a cross-document set-ting and that the current state-of-the-art in SRL canmodel this context accurately.The best scores (almost unanimously) are ob-tained by the model proposed in this paper, whichscores 3.4 CoNLL F1 points higher than Baseline 2for entities, and 2.6 points higher for events.
For thecomplete task, our approach scores 3.3 CoNLL F1points higher than Baseline 2, and 6.1 points higherthan Baseline 1.
This demonstrates that a holisticapproach to coreference resolution improves the res-olution of both entities and events more than modelsthat address aspects of the task separately.
To fur-ther understand our experiments, we listed the topfive entity/event features with the highest weights inour model in Table 5.
The table indicates that six outof the ten features serve the purpose of passing infor-Entity Feature WeightEntity Heads ?
Proper 1.10Coreferent Predicate for ArgM-LOC ?
Common 0.45Entity Heads ?
Common 0.36Coreferent Predicate for Arg0 ?
Proper 0.29Coreferent Predicate for Arg2 ?
Common 0.28Event Feature WeightEvent Lemmas 0.45Coreferent Argument for Arg1 0.19Links between Synonym 0.16Coreferent Argument for Arg2 0.13Number of Coreferent Arguments 0.07Table 5: Top five features with the highest weights.mation between entity and event clusters.
For exam-ple, the ?Coreferent Argument for Arg1?
feature istriggered when two event clusters have Arg1 argu-ments that already belong to the same entity cluster.This allows information from previous entity coref-erence operations to impact future merges of eventclusters.
This is the crux of our iterative approach tojoint coreference resolution.Finally, we performed an error analysis by man-ually evaluating 100 errors.
We distinguished ninemajor types of errors.
Their ratios together with adescription and an example are given in Table 6.This work demonstrates that an approach thatjointly models entities and events is better for cross-document coreference resolution.
However, ourmodel can be improved.
For example, documentclustering and coreference resolution can be solvedjointly, which we expect would improve both tasks.Furthermore, our iterative coreference resolutionprocedure (Algorithm 1) could be modified to ac-count for mention ordering and distance, whichwould allow us to include pronominal resolution inour joint model, rather than addressing it with a sep-arate deterministic sieve.8 ConclusionWe have presented a holistic model for cross-document coreference resolution that jointly solvesreferences to events and entities by handling bothnominal and verbal mentions.
Our joint resolutionalgorithm allows event coreference to help improveentity coreference, and vice versa.
In addition, ouriterative procedure, based on a linear regressor thatmodels the quality of cluster merges, allows each497Error Type (Ratio)DescriptionExamplePronoun resolution(36%)The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entitysystem.
These errors include (only a small number of) event pronouns.He said Timmons aimed and missed his target.Semantics beyondrole frames(20%)The semantics of the coreference relation cannot be captured by role frames or WordNet.Israeli forces on Tuesday killed at least 40 people .
.
.
The Israeli army said the UN school in theJabaliya refugee camp was hit .
.
.
and that the dead included a number of Hamas militants.Arguments ofnominal events(17%)The arguments of two nominal events are not detected and thus not coreferred.The attack on the school has caused widespread shock across Israel .
.
.
while Israeli forces onTuesday killed at least 40 people during an attack on a United Nations-run school in Gaza.Cascaded errors(7%)Entities or events are not coreferred due to errors in a previous merge iteration in the samesemantic frame.
In the example below, we failed to link the two die verbs, which leads to thelisted entity error.An Australian climber who survived two nights stuck on Mount Cook after seeing his brotherdie .
.
.
Dr Mark Vinar, 43, is presumed dead .
.
.Initial high-precisionsieves(6%)An error made by the initial high-precision entity resolution sieves is propagated to our model.Timmons told police he fired when he thought he saw someone in the other group reach fora gun .
.
.
15-year-old Timmons was at the scene of the shooting and had a gun.Phrasal verbs(6%)The meaning of a phrasal verb is not captured.A relative unknown will take over the title role of Doctor Who .
.
.
But the casting of Smith isa stroke of genius.Linear regression(4%)Recall error made by the regression model when the features are otherwise correct.The Interior Department on Thursday issued ?revised?
regulations .
.
.
Interior Secretary DirkKempthorne announced major changes .
.
.Mention detection(3%)The mention detection module detects a spurious mention.Police have arrested a man .
.
.
in the parking lot crosswalk at Sam?s Club in Bloomington.SRL(1%)The SRL system fails to label the semantic role.
In this example, jail is detected as the ArgM-MNR of hanged instead of ArgM-LOC.A Mafia boss in Palermo hanged himself in jail.Table 6: Error analysis.
Mentions to be resolved are in bold face, correct antecedents are in italics, and our system?spredictions are underlined.merging state to benefit from the previous mergedentity and event mentions.
This approach allows usto start with a set of high-precision coreference rela-tions and gradually add new ones to increase recall.The experimental evaluation shows that our coref-erence algorithm gives markedly better F1 for bothentities and events, outperforming two strong base-lines that handle entities and events separately, mea-sured by all the standard measures: MUC, B3,CEAF-?4, BLANC and the official CoNLL-2011metric.
This is noteworthy since each measure hasbeen shown to place primary emphasis in evaluatinga different aspect of the coreference resolution task.Our system is tailored for cross-document coref-erence resolution on a corpus that contains news ar-ticles that repeatedly report on a smaller number oftopics.
This makes it particularly suitable for real-world applications such as multi-document summa-rization and cross-document information extraction.We also release our labeled corpus to facilitate ex-tensions and comparisons to our work.AcknowledgementsWe acknowledge the support of Defense Advanced Re-search Projects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL)prime contract no.
FA8750-09-C-0181.
Any opinions,findings, and conclusion or recommendations expressedin this material are those of the author(s) and do not nec-essarily reflect the view of the DARPA, AFRL, or the USgovernment.
MR is supported by a Beatriu de Pino?s post-doctoral scholarship (2010 BP-A 00149) from Generali-tat de Catalunya.
AC is supported by a SAP StanfordGraduate Fellowship.
We also gratefully thank CosminBejan for sharing his code and the useful discussions.498ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings ofthe LREC 1998 Workshop on Linguistic Coreference,pages 563?566.Amit Bagga and Breck Baldwin.
1999.
Cross-documentevent coreference: Annotations, experiments, and ob-servations.
In Proceedings of the ACL 1999 Workshopon Coreference and Its Applications, pages 1?8.Cosmin Bejan and Sanda Harabagiu.
2010.
Unsuper-vised Event Coreference Resolution with Rich Lin-guistic Features.
In Proceedings of ACL 2010, pages1412?1422.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a case studyin part of speech tagging.
Computational Linguistics,21(4):543?565.Zheng Chen and Heng Ji.
2009.
Graph-based eventcoreference resolution.
In Proceedings of the ACL-IJCNLP 2009 Workshop on Graph-based Methods forNatural Language Processing, pages 54?57.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Proceedings of NAACL-HLT 2007.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.
InProceedings of EMNLP 2009, pages 1152?1161.Aria Haghighi and Dan Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In Proceed-ings of HLT-NAACL 2010, pages 385?393.Tian He.
2007.
Coreference Resolution on Entities andEvents for Hospital Discharge Summaries.
Thesis,Massachusetts Institute of Technology.Jerry R. Hobbs.
1978.
Resolving pronoun references.Lingua, 44(4):311?338.Kevin Humphreys, Robert Gaizauskas, and Saliha Az-zam.
1997.
Event coreference for information extrac-tion.
In Proceedings of the Workshop On OperationalFactors In Practical Robust Anaphora Resolution ForUnrestricted Texts, pages 75?81.Klaus Krippendorff.
2004.
Content Analysis: An In-troduction to its Methodology.
Sage, Thousand Oaks,CA, second edition.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 shared task.
In Proceedingsof CoNLL 2011: Shared Task, pages 28?34.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL 1998,pages 768?774.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of HLT-EMNLP 2005,pages 25?32.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The Nom-Bank project: an interim report.
In Proceedings of theHLT-NAACL 2004 Workshop on Frontiers in CorpusAnnotation, pages 24?31.Rebecca Passonneau.
2004.
Computing reliability forcoreference annotation.
In Proceedings of LREC2004, pages 1503?1506.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedingsof HLT-NAACL 2006, pages 192?199.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov logic.
InProceedings of EMNLP 2008, pages 650?659.Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Un-restricted coreference: Identifying entities and eventsin OntoNotes.
In Proceedings of ICSC 2007, pages446?453.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 shared task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofCoNLL 2011: Shared Task, pages 1?27.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Chris Manning.
2010.
A multi-passsieve for coreference resolution.
In Proceedings ofEMNLP 2010, pages 492?501.Altaf Rahman and Vincent Ng.
2011.
Coreferenceresolution with world knowledge.
In Proceedings ofACL 2011, pages 814?824.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the AmericanStatistical Association, 66(336):846?850.Marta Recasens and Eduard Hovy.
2011.
BLANC: Im-plementing the Rand index for coreference evaluation.Natural Language Engineering, 17(4):485?510.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.In Proceedings of ACL-IJCNLP 2009, pages 656?664.Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2005.A hybrid unsupervised approach for document cluster-ing.
In Proceedings of KDD 2005, pages 685?690.499Mihai Surdeanu, Llu?
?s Ma`rquez, Xavier Carreras, andPere R. Comas.
2007.
Combination strategies for se-mantic role labeling.
Journal of Artificial IntelligenceResearch, 29:105?151.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC-6, pages 45?52.Bonnie Lynn Webber.
1988.
Discourse deixis: referenceto discourse segments.
In Proceedings of ACL 1988,pages 113?122.Michael L. Wick, Khashayar Rohanimanesh, KarlSchultz, and Andrew McCallum.
2008.
A unified ap-proach for schema matching, coreference and canoni-calization.
In Proceedings of KDD 2008, pages 722?730.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of ACL 1995, pages 189?196.500
