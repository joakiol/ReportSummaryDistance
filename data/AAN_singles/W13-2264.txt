Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512?520,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsAn MT Error-driven Discriminative Word Lexiconusing Sentence Structure FeaturesJan Niehues and Alex WaibelInstitute for AnthropomaticsKarlsruhe Institute of Technology, Germanyfirstname.secondname@kit.eduAbstractThe Discriminative Word Lexicon (DWL)is a maximum-entropy model that pre-dicts the target word probability given thesource sentence words.
We present twoways to extend a DWL to improve its abil-ity to model the word translation probabil-ity in a phrase-based machine translation(PBMT) system.
While DWLs are able tomodel the global source information, theyignore the structure of the source and tar-get sentence.
We propose to include thisstructure by modeling the source sentenceas a bag-of-n-grams and features depend-ing on the surrounding target words.
Fur-thermore, as the standard DWL does notget any feedback from the MT system, wechange the DWL training process to ex-plicitly focus on addressing MT errors.By using these methods we are able to im-prove the translation performance by upto 0.8 BLEU points compared to a systemthat uses a standard DWL.1 IntroductionIn many state-of-the-art SMT systems, the phrase-based (Koehn et al 2003) approach is used.
Inthis approach, instead of building the translationby translating word by word, sequences of sourceand target words, so-called phrase pairs, are usedas the basic translation unit.
A table of correspon-dences between source and target phrases formsthe translation model.
Target language fluency ismodeled by a language model storing monolingualn-gram occurrences.
A log-linear combination ofthese main models as well as additional features isused to score the different translation hypotheses.Then the decoder searches for the translation withthe highest score.One problem of this approach is that bilingualcontext is only modeled within the phrase pairs.Therefore, different approaches to increase thecontext available during decoding have been pre-sented (Haque et al 2011; Niehues et al 2011;Mauser et al 2009).
One promising approach isthe Discriminative Word Lexicon (DWL).
In thisapproach, a discriminative model is used to predictthe probability of a target word given the words inthe source sentence.In contrast to other models in the phrase-basedsystem, this approach is capable of modeling thetranslation probability using information from thewhole sentence.
Thus it is possible to modellong-distance dependencies.
But the model is notable to use the structure of the sentence, sincethe source sentence is modeled only as a bag-of-words.
Furthermore, the DWL is trained todiscriminate between all translation options with-out knowledge about the other models used in aphrase-based machine translation system such asthe translation model, language model etc.
Incontrast, we try to feedback information aboutpossible errors of the MT system into the DWL.Thereby, the DWLs are able to focus on improvingthe errors of the other models of an MT system.We will introduce features that encode infor-mation about the source sentence structure.
Fur-thermore, the surrounding target words will alsobe used in the model to encode information aboutthe target sentence structure.
Finally, we incor-porate information from the other models into thecreation of the training examples.
We create thenegative training examples using possible errors ofthe other models.2 Related WorkBangalore et al(2007) presented an approach tomachine translation using discriminative lexicalselection.
Motivated by their results, Mauser etal.
(2009) integrated the DWL into the PBMT ap-512proach.
Thereby, they are able to use global sourceinformation.This was extended by Huck et al(2010) by afeature selection strategy in order to reduce thenumber of weights.
In Mediani et al(2011) a firstapproach to use information about MT errors inthe training of DWLs was presented.
They selectthe training examples by using phrase table infor-mation also.The DWLs are related to work that was donein the area of word sense disambiguation (WSD).Carpuat and Wu (2007) presented an approach todisambiguate between different phrases instead ofperforming the disambiguation at word level.A different lexical model that uses target sideinformation was presented in Jeong et al(2010).The focus of this work was to model complex mor-phology on the target language.3 Discriminative Word LexiconThe DWL is a maximum entropy model used todetermine the probability of using a target wordin the translation.
Therefore, we train individ-ual models for every target word.
Each model istrained to return the probability of this word giventhe input sentence.The input of the model is the source sentence.Therefore, we need to represent the input sentenceby features.
In this approach this is done by usingbinary features.
We use an indicator feature forevery input word.
Therefore, the sentence is mod-eled as a bag-of-words and the order of the wordsis ignored.
More formally, a given source sen-tence F = f1 .
.
.
fI is represented by the featuresI(F ) = {if (F ) : f ?
SourceV ocabulary}:if (F ) ={1 : f ?
F0 : f /?
F (1)The models are trained on examples generatedby the parallel training data.
The labels for train-ing the classifier of target word e are defined asfollows:labele(F,E) ={1 : e ?
E0 : e /?
E (2)We used the MegaM Toolkit1 to train the maxi-mum entropy models.
This model approximatesthe probability p(ej |F ) of a target word ej giventhe source sentence F .1http://www.umiacs.umd.edu/ hal/megam/index.htmlWhen we have the probability for every word ejgiven the source sentence F , we need to combinethese probabilities into a probability of the wholetarget sentence E = e1 .
.
.
eJ given F .
Making anassumption of independence on the target side aswell, the models can be combined to the probabil-ity of E given F :p(E|F ) =?ej?ep(ej |F ) (3)In this equation we multiply the probability ofone word only once even if the word occurs sev-eral times in the sentence.
Since we build the tar-get sentence from left to right during decoding,we would need to change the score for this fea-ture only if a new word is added to the hypothesis.If a word is added second time we do not wantto change the feature value.
In order to keep trackof this, additional bookkeeping would be required.But the other models in our translation system willprevent us from using a word too often in any case.Therefore, we approximate the probability of thesentence differently as defined in Equation 4.p(E|F ) =J?j=1p(ej |F ) (4)In this case we multiply the probabilities of allword occurrences in the target sentence.
There-fore, we can calculate the score for every phrasepair before starting with the translation.4 Modeling Sentence StructureAs mentioned before one main drawback of DWLsis that they do not encode any structural informa-tion about the source or target sentence.
We in-corporated this information with two types of fea-tures.
First, we tried to encode the informationfrom the source sentence better by using a bag-of-n-grams approach.
Secondly, we introduced newfeatures to be able to encode information about theneighboring target words also.4.1 Source Sentence StructureIn the default approach the sentence is representedas a bag-of-words.
This has the advantage thatthe model can use a quite large context of thewhole sentence.
In contrast to the IBM models,where the translation probability only depends onthe aligned source word, here the translation prob-ability can be influenced by all words in the sen-tence.513On the other hand, the local context is ignoredby the bag-of-words approach.
Information aboutthe word order get lost.
No information about theprevious and next word is available.
The problemis illustrated in the example in Figure 1.Figure 1: Example for source structural informa-tionSource: Die Lehrer wussten nicht, ...Reference: The teachers didn?t know ...The German word Lehrer (engl.
teacher) is thesame word for singular or plural.
It is only pos-sible to distinguish whether singular or plural ismeant through the context.
This can be determinedby the plural article die.
If only one teacher wouldbe meant, the corresponding article would be der.In order be able to use the DWL to distinguishbetween these two translations, we need to im-prove the representation of the input sentence.
Asshown in the example, it would be helpful to knowthe order of the words.
If we know that the worddie precedes Lehrer, it would be more probablethat the word is translated into teachers rather thanteacher.Therefore, we propose to use a bag-of-n-gramsinstead of a bag-of-words to represent the inputsentence.
In this case we will use an indicator fea-ture for every n-gram occurring in the input sen-tence and not only for every word.
This way weare also able to encode the sequence of the words.For the example, we would have the input featuredie Lehrer, which would increase the probabilityof using teachers in the translation compared toteacher.By increasing the order of the n-grams, we willalso increase the number of features and run intodata sparseness problems.
Therefore, we usedcount filtering on the features for higher order n-grams.
Furthermore, we combine n-grams of dif-ferent orders to better handle the data sparsenessproblem.4.2 Target Sentence StructureIn the standard DWL approach, the probability ofthe target word depends only on the source wordsin the input sentence.
But this is a quite rough ap-proximation.
In reality, the probability of a targetword occurring in the sentence also depends on theother target words in the sentence.If we look at the word langsam (engl.
slow orslowly) in the example sentence in Figure 2, wecan only determine the correct translation by usingthe target context.
The word can be translated asslow or slowly depending on how it is used in theEnglish sentence.In order to model the translation probability bet-ter we need structural information of the targetside.
For example, if the preceding word on thetarget side is be, the translation will be more prob-ably slow than slowly.We encoded the target context of the word byfeatures indicating the preceding or next word.Furthermore, we extend the context to up to threewords before and after the word.
Therefore thefollowing target features are added to the set offeatures for the classifier of word e:iTC e?
k(E) ={1 : ?j : ej = e ?
ej+k = e?0 : else(5)where k ?
{?1, 1} for a context of one wordbefore and after.5 TrainingApart from the missing sentence structure theDWL is not able to make use of feedback fromthe other models in the MT system.
We try to in-corporate information about possible errors intro-duced by the other models into the training of theDWL.The DWL is trained on the paral-lel data that is available for the taskT = (F1, E1), .
.
.
, (FM , EM ).
In order totrain it, we need to create positive and negativeexamples from this data.
We will present differentapproaches to generate the training examples,which differ in the information used for creatingthe negative examples.In the original approach, one training exampleis created for every sentence of the parallel dataand for every DWL classifier.
If the target wordoccurs in the sentence, we create a positive ex-ample and if not the source sentence is used as anegative example as described in Equation 2.
Formost words, this results in a very unbalanced set oftraining examples.
Most words will only occur inquite few sentences and therefore, we have mostlynegative examples.Mediani et al(2011) presented an approachto create the training examples that is driven bylooking at possible errors due to the different514Figure 2: Example for target structural informationSource: Die Anerkennung wird langsam sein in den Vereinigten Staaten ...Reference: The recognition is going to be slow in the United States, ...translations in the phrase table (Phrase pair ap-proach).
Since a translation is generated alwaysusing phrase pairs (f?
, e?)
with matching sourceside, wrong words can only be generated in thetranslation if the word occurs in the target sidewords of those matching phrase pairs.
There-fore, we can define the possible target vocabularyTV (F ) of a source sentence:TV (F ) = {e|?(f?
, e?)
: f?
?
F ?
e ?
e?}
(6)As a consequence, we generate a negative train-ing example for one target word only from thosetraining sentences where the word is in the targetvocabulary but not in the reference.labele(F,E) ={1 : e ?
E0 : e /?
E ?
e ?
TV (F )(7)All training sentences for which the label is notdefined are not used in the training of the modelfor word e. Thereby, not only can we focus theclassifiers on improving possible errors made bythe phrase table, but also reduce the amount oftraining examples and therefore the time neededfor training dramatically.In the phrase pair approach we only use in-formation about possible errors of the translationmodel for generating the negative training exam-ples.
But it would be preferable to consider possi-ble errors of the whole MT system instead of onlyusing the phrase table.
Some of the errors of thephrase table might already be corrected by the lan-guage model.
The possible errors of the wholesystem can be approximated by using the N -Bestlist.We first need to translate the whole cor-pus and save the N -Best list for all sentencesNBEST (F ) = {E?1 .
.
.
E?N}.
Then we canapproximate the possible errors of the MT sys-tem with the errors that occur in the N -Best list.Therefore, we create a negative example for a tar-get word only if it occurs in the N -Best list andnot in the reference.
Compared to the phrase pairapproach, the only difference is the definition ofthe target vocabulary:TV (F ) = {e|e ?
NBEST (F )} (8)The disadvantage of the N-Best approach is, ofcourse, that we need to translate the whole cor-pus.
This is quite time consuming, but it can beparallelized.5.1 Training Examples for Target FeaturesIf we use target features, the creation of the train-ing examples gets more difficult.
When using onlysource features, we can create one example fromevery training sentence.
Even if the word occursin several phrase pairs or in several entries of theN -Best list, all of them will create the same train-ing example, since the features only depend on thesource sentence.When we use target features, the features of thetraining example depend also on the target wordsthat occur around the word.
Therefore, we canonly use the N -Best list approach to create the tar-get features since previous approaches mentionedin the last part do not have the target context in-formation.
Furthermore, we can create differentexamples from the same sentence.
If we have, forexample, the N -Best list entries I think ... and I be-lieve .., we can use the context think or the contextbelieve for the model of I.In the approach using all target features (AllTF), we created one training example for everysentence where the word occurs.
If we see theword in different target contexts, we create all thefeatures for these contexts and use them in thetraining example.I(F,E) = max( I(F ); I(E); (9)I(E?)|E?
?
NBEST (F ))The maximum is defined component-wise.
Soall features, which have in I(F ),I(E) or I(E?)
thevalue one, also have the value one in I(F,E).
Ifwe use the context that was given by the reference,this might not exist in the phrase-based MT sys-tem.
Therefore, in the next approach (N-Best TF),we only used target features from the N -Best list.I(F,E) = max(I(F ); I(E?)|E?
?
NBEST (F ))(10)In both examples, we still have the problem thatwe can use different contexts in one training ex-515ample.
This condition can not happen when ap-plying the DWL model.
Therefore, we changedthe set of training examples in the separate targetfeatures approach (Separate TF).
We no longercreate one training example for every training sen-tence (F,E), but one for every training sentenceN -Best list translation (F,E,E?).
We only con-sidered the examples for the classifier of targetword e, where e occurs in the N -Best list entry E?.If the word does not occur in any N -Best list en-try of a training sentence, but in the reference, wecreated an additional example (F,E, ??).
The fea-tures of this examples can then be created straightforward as:I((F,E,E?))
= max(I(F ); I(E?))
(11)If we have seen the word only in the reference,we create an training example without target fea-tures.
Therefore, we have again a training exam-ple which can not happen when using the DWLmodel.
Therefore, we removed these examples inthe last method (Restricted TF).6 ExperimentsAfter presenting the different approaches to per-form feature and example selection, we will nowevaluate them.
First, we will give a short overviewof the MT system.
Then we will give a detailedevaluation on the task of translating German lec-tures into English and analyze the influence of thepresented approaches.
Afterwards, we will presentoverview experiments on the German-to-Englishand English-to-German translation task of WMT13 Shared Translation Task.6.1 System DescriptionThe translation system was trained on the EPPScorpus, NC corpus, the BTEC corpus and TEDtalks.2 The data was preprocessed and compoundsplitting (Koehn and Knight, 2003) was appliedfor German.
Afterwards the discriminative wordalignment approach as described in Niehues andVogel (2008) was applied to generate the align-ments between source and target words.
Thephrase table was built using the scripts from theMoses package (Koehn et al 2007).
A 4-gramlanguage model was trained on the target side ofthe parallel data using the SRILM toolkit (Stolcke,2002).
In addition we used a bilingual languagemodel as described in Niehues et al(2011).2http://www.ted.comReordering was performed as a preprocessingstep using part-of-speech information generatedby the TreeTagger (Schmid, 1994).
We usedthe reordering approach described in Rottmannand Vogel (2007) and the extensions presented inNiehues and Kolss (2009) to cover long-range re-orderings, which are typical when translating be-tween German and English.An in-house phrase-based decoder was used togenerate the translation hypotheses and the opti-mization was performed using MERT (Venugopalet al 2005).We optimized the weights of the log-linearmodel on a separate set of TED talks and alsoused TED talks for testing.
The development setconsists of 1.7k segments containing 16k words.As test set we used 3.5k segments containing 31kwords.
We will refer to this system as System 1.In order to show the influence of the approachesbetter, we evaluated them also in a second system.In addition to the models used in the first systemwe performed a log-linear language model andphrase table adaptation as described in Niehuesand Waibel (2012).
To this system we refer as Sys-tem 2 in the following experiments.6.2 German - English TED Experiments6.2.1 Source FeaturesIn a first set of experiments, we analyzed the dif-ferent types of source structure features describedin Section 4.1.
In all the experiments, we generatethe negative training examples using the candidatetranslations generated by the phrase pairs.
The re-sults can be found in Table 1.First, we added the unigram DWL to the base-line system.
The higher improvements for the Sys-tem 1 is due to the fact that the DWL is onlytrained on the TED corpus and therefore also per-forms some level of domain adaptation.
This ismore important for the System 1, since System 2is already adapted to the TED domain.If we use features based on bigrams instead ofunigrams, the number of features increases by afactor of eight.
Furthermore, in both cases thetranslation quality drops.
Especially for System1, we have a significant drop in the BLEU scoreof the test set by 0.6 BLEU points.
One prob-lem might be that most of the bigrams occur quiterarely and therefore, we have a problem of datasparseness and generalization.If we combine the features of unigram and bi-516Table 1: Experiments using different source featuresSystem FeatureSize System 1 System 2Dev Test Dev TestBaseline 0 26.32 24.24 28.40 25.89Unigram 40k 27.46 25.56 28.58 26.15Bigram 319k 27.34 24.92 28.53 25.82Uni+bigram 359k 27.69 25.55 28.66 26.51+ Count filter 2 122k 27.75 25.71 28.75 26.74+ Count filter 5 63k 27.81 25.67 28.72 26.81+ Trigram 77k 27.76 25.76 28.82 26.94gram features, for System 1, we get an improve-ment of 0.2 BLEU points on the development dataand the same translation quality on the test dataas the baseline DWL system using only unigrams.For System 2, we can improve by 0.1 on the devel-opment data and 0.4 on the test data.
So we can geta first improvement using these additional sourcefeatures, but the number of features increased by afactor of nine.In order to decrease the number of featuresagain, we applied count filtering to the bigramfeatures.
In a first experiment we only used thebigram features that occur at least twice.
Thisreduced the number of features dramatically bya factor of three.
Furthermore, this even im-proved the translation quality.
In both systems wecould improve the translation quality by 0.2 BLEUpoints.
So it seems to be quite important to addonly the relevant bigram features.If we use a minimum occurrence of five for thebigram features, we can even decrease the num-ber of features further by a factor of two withoutlosing any translation performance.Finally, we added the trigram features.
Forthese features we applied count filtering of five.For System 1, the translation quality stays thesame, but for System 2 we can improve the trans-lation quality by additional 0.2 BLEU points.In summary, we could improve the translationquality by 0.2 for the System 1 and 0.8 BLEUpoints for the System 2 on the test set.
Due to thecount filtering, this is achieved by only using lessthan twice as many features.6.3 Training ExamplesIn a next step we analyzed the different exam-ple selection approaches.
The results are summa-rized in Table 2.
In these experiments we used thesource features using unigrams, bigrams and tri-grams with count filtering in all experiments.In the first experiment, we used the original ap-proach to create the training examples.
In thiscase, all sentences where the word does not occurin the reference generate negative examples.
Inour setup, we needed 8,461 DWL models to trans-late the development and test data.
These are alltarget words that occur in phrase pairs that can beused to translate the development or test set.In each of approaches we have 0.75M posi-tive examples for these models.
In the origi-nal approach, we have 428M negative examples.So in this case the number of positive and nega-tive examples is very unbalanced.
This trainingdata leads to models with a total of 659M featureweights.If we use the target side of the phrase pairs togenerate our training examples, we dramaticallyreduce the number of negative training examples.In this case only 5M negative training examplesare generated.
The size of the models is reduceddramatically to 38M weights.
Furthermore, wecould improve the translation quality by 0.3 BLEUpoints on both System 1 and System 2.If we use the 300-Best lists produced by Sys-tem 1 to generate the training examples, we canreduce the model size further.
This approach leadsto models only half the size of the phrase pairs ap-proach using only 1.59M negative examples.
Fur-thermore, for System 1 the translation quality canbe improved further to 25.87 BLEU points.
ForSystem 2 the BLEU score on the development dataincreases, but the score on the test sets drops by 0.4BLEU points.In the next experiment we used the N -Best listsgenerated by System 2.
The results are shown inthe line N -Best list 2.
In this case, the model sizeis slightly reduced further.
And on the adaptedsystem a similar performance is achieved.
But for517Table 2: Experiments using different methods to create training examplesSystem #weight #neg.
Examples System 1 System 2Dev Test Dev TestOriginal Approach 659 M 428 M 27.39 25.44 28.64 26.63Phrase pairs 38 M 5.26 M 27.76 25.76 28.82 26.94N -Best list 1 16 M 1.59 M 27.93 25.87 29.07 26.57N -Best list 2 11 M 1.22 M 27.46 25.37 28.79 26.59N -Best list 1 nonUnique 16 M 1.41M 27.99 25.97 29.07 26.65System 1 the performance of this approach drops.Consequently, it seems to be fine to use an N -Best list of a more general system to generate thenegative examples.
But the N -Best list should notstem from an adapted system.Finally, the phrase table was trained on the samecorpus as the one that was used to generate the N -Best lists for DWL training.
Since we have seenthe data before, longer phrases can be used thanin a real test scenario.
To compensate partly forthat, we removed all phrase pairs that occur onlyonce in the phrase table.
The results are shown inthe last line.
This approach could slightly improvethe translation quality leading to a BLEU score of25.97 for System 1 and 26.65 for the System 2.6.4 Target FeaturesAfter evaluating the different approaches to gen-erate the negative examples, we also evaluated thedifferent approaches for the target features.
Theresults are summarized in Table 3.
In all these ex-periments we use the training examples generatedby the N -Best list of System 1 using the phrasetable without unique phrase pairs.First, we tested the four different methods usinga context of one word before and one word afterthe word.In the experiments the first two methods, AllTF and N-Best TF , perform worse than the lasttwo approaches, Separate TF and Restricted TF.So it seems to be important to have realistic exam-ples and not to mix different target contexts in oneexample.
The Separate and Restricted approachperform similarly well.
In both cases the perfor-mance can be improved slightly by using a contextof three words before and after instead of usingonly one word.If we look at the model size, the number ofweights increases from 16M to 17M, when usinga context of one word and to 21M using a contextof three words.If we compare the results to the systems usingno target features in the first row, no or only slightimprovements can be achieved.
One reason mightbe that the morphology of English is not very com-plex and therefore, the target context is not as im-portant to determine the correct translation.6.4.1 OverviewIn Table 4, we give an overview of the results us-ing the different extensions to DWLs given in thispaper.
The baseline system does not use any DWLat all.
If we use a DWL using only bag-of-wordsfeatures and the training examples from the phrasepairs, we can improve by 1.3 BLEU points on Sys-tem 1 and 0.3 BLEU points on System 2.By adding the source-context features, the firstsystem can be improved by 0.2 BLEU points andthe second one by 0.8 BLEU points.
If we use thetraining examples from the N -Best list instead ofusing the ones from the phrase table, we improveby 0.2 on System 1, but perform 0.3 worse on Sys-tem 2.
Adding the target context features does notimprove System 1, but System 2 can be improvedby 0.3 BLEU points.
This system results in thebest average performance.
Compared to the base-line system with DWLs, we can improve by 0.4and 0.8 BLEU points, respectively.Table 4: Overview of results for TED lecturesSystem System 1 System 2Dev Test Dev TestBaseline 26.32 24.24 28.40 25.89DWL 27.46 25.56 28.58 26.15sourceContext 27.76 25.76 28.82 26.94N -Best 27.99 25.97 29.07 26.65TargetContext 28.15 25.91 29.12 26.906.5 German - English WMT 13 ExperimentsIn addition to the experiments on the TED data,we also tested the models in the systems for the518Table 3: Experiments using different target featuresSystem Context System 1 System 2Dev Test Dev TestNo Target Features 0-0 27.99 25.97 29.07 26.65All TF 1-1 27.80 25.48 28.80 26.38N-Best TF 1-1 27.99 25.74 28.86 26.37Separate TF 1-1 28.06 25.81 28.98 26.80Restricted TF 1-1 28.13 25.84 28.94 26.68Separate TF 3-3 27.87 25.90 28.99 26.75Restricted TF 3-3 28.15 25.91 29.12 26.90WMT 2013.
The systems are similar to the oneused before, but were trained on all available train-ing data and use additional models.
The systemswere tested on newstest2012.
The results for Ger-man to English are summarized in Table 5.
In thiscase the DWLs were trained on the EPPS and theNC corpus.
Since the corpora are bigger, we per-form an additional weight filtering on the models.The baseline system uses already a DWLtrained with the bag-of-words features and thetraining examples were created using the phrasetable.
If we add the bag-of-n-grams features upto a n-gram length of 3, we cannot improve thetranslation quality on this task.
But by addition-ally generating the negative training examples us-ing the 300-Best list, we can improve this systemby 0.2 BLEU points.Table 5: Experiments on German to English WMT2013System Dev TestUnigram DWL 25.79 24.36+ Bag-of-n-gram 25.85 24.33+ N -Best 25.84 24.526.6 English - German WMT 13 ExperimentsWe also tested the approach also on the reversedirection.
Since the German morphology is muchmore complex than the English one, we hope thatin this case the target features can help more.
Theresults for this task are shown in Table 6.
Here, thebaseline system again already uses DWLs.
If weadd the bag-of-n-grams features and generate thetraining examples from the 300-Best list, we canagain slightly improve the translation quality.
Inthis case we can improve the translation quality byadditional 0.1 BLEU points by adding the targetfeatures.
This leads to an overall improvement bynearly 0.2 BLEU points.Table 6: Experiments on English to German WMT2013System Dev Testunigram DWL 16.97 17.41+ Bag-of-n-gram 16.89 17.45+ N -Best 17.10 17.47+ Target Features 17.08 17.587 ConclusionDiscriminative Word Lexica have been recentlyused in several translation systems and have shownto improve the translation quality.
In this work, weextended the approach to improve its modeling ofthe translation process.First, we added features which represent thestructure of the sentence better.
By using bag-of-n-grams features instead of bag-of-words features,we are able to encode the order of the source sen-tence.
Furthermore, we use features for the sur-rounding target words to also model the target con-text of the word.
In addition, we tried to train theDWLs in a way that they help to address possi-ble errors of the MT system by feeding informa-tion from the MT system back into the generationof the negative training examples.
Thereby, wecould reduce the size of the models and improvethe translation quality.
Overall, we were able toimprove the translation quality on three differenttasks in two different translation directions.
Im-provements of up to 0.8 BLEU points could beachieved.5198 AcknowledgementsThis work was partly achieved as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
The research lead-ing to these results has received funding fromthe European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementn?
287658.ReferencesS.
Bangalore, P. Haffner, and S. Kanthak.
2007.
Sta-tistical Machine Translation through Global LexicalSelection and Sentence Reconstruction.
In AnnualMeeting-Association for Computational Linguistics,volume 45, page 152.M.
Carpuat and D. Wu.
2007.
Improving Statis-tical Machine Translation using Word Sense Dis-ambiguation.
In In The 2007 Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning.R.
Haque, S.K.
Naskar, A. Bosch, and A. Way.2011.
Integrating source-language context intophrase-based statistical machine translation.
Ma-chine Translation, 25(3):239?285.M.
Huck, M. Ratajczak, P. Lehnen, and H. Ney.
2010.A Comparison of Various Types of Extended Lexi-con Models for Statistical Machine Translation.
InProc.
of the Conf.
of the Assoc.
for Machine Trans-lation in the Americas (AMTA).M.
Jeong, K. Toutanova, H. Suzuki, and C. Quirk.2010.
A Discriminative Lexicon Model for Com-plex Morphology.
In Proceedings of the Ninth Con-ference of the Association for Machine Translationin the Americas (AMTA 2010).P.
Koehn and K. Knight.
2003.
Empirical Methods forCompound Splitting.
In EACL, Budapest, Hungary.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statis-tical Phrase-Based Translation.
In Proceedings ofthe 2003 Conference of the North American Chap-ter of the Association for Computational Linguisticson Human Language Technology, pages 48?54, Ed-monton, Canada.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In ACL 2007,Demonstration Session, Prague, Czech Republic.A.
Mauser, S. Hasan, and H. Ney.
2009.
ExtendingStatistical Machine Translation with Discriminativeand Trigger-based Lexicon Models.
In Proceedingsof the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 1 ?
Volume1, Emnlp?09, Singapore.M.
Mediani, E. Cho, J. Niehues, T. Herrmann, andA.
Waibel.
2011.
The KIT English-French Trans-lation Systems for IWSLT 2011.
Proceedings of theeight International Workshop on Spoken LanguageTranslation (IWSLT).Jan Niehues and Mutsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
FourthWorkshop on Statistical Machine Translation (WMT2009), Athens, Greece.J.
Niehues and S. Vogel.
2008.
Discriminative WordAlignment via Alignment Matrix Modeling.
Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, pages 18?25.J.
Niehues and A. Waibel.
2012.
Detailed Analysis ofdifferent Strategies for Phrase Table Adaptation inSMT.
In Proceedings of the Tenth Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA).J.
Niehues, T. Herrmann, S. Vogel, and A. Waibel.2011.
Wider Context by Using Bilingual LanguageModels in Machine Translation.
Sixth Workshop onStatistical Machine Translation (WMT 2011), Edin-burgh, UK.K.
Rottmann and S. Vogel.
2007.
Word Reordering inStatistical Machine Translation with a POS-BasedDistortion Model.
In TMI, Sko?vde, Sweden.H.
Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Con-ference on New Methods in Language Processing,Manchester, UK.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
In Icslp, Denver, Colorado, USA.A.
Venugopal, A. Zollman, and A. Waibel.
2005.Training and Evaluation Error Minimization Rulesfor Statistical Machine Translation.
In Workshop onData-drive Machine Translation and Beyond (WPT-05), Ann Arbor, MI.520
