First Joint Conference on Lexical and Computational Semantics (*SEM), pages 189?198,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsAn Evaluation of Graded Sense Disambiguation using Word Sense InductionDavid Jurgens1,21HRL Laboratories, LLCMalibu, California, USA2Department of Computer ScienceUniversity of California, Los Angelesjurgens@cs.ucla.eduAbstractWord Sense Disambiguation aims to label thesense of a word that best applies in a givencontext.
Graded word sense disambiguationrelaxes the single label assumption, allow-ing for multiple sense labels with varying de-grees of applicability.
Training multi-labelclassifiers for such a task requires substan-tial amounts of annotated data, which is cur-rently not available.
We consider an alter-nate method of annotating graded senses usingWord Sense Induction, which automaticallylearns the senses and their features from cor-pus properties.
Our work proposes three ob-jective to evaluate performance on the gradedsense annotation task, and two new methodsfor mapping between sense inventories usingparallel graded sense annotations.
We demon-strate that sense induction offers significantpromise for accurate graded sense annotation.1 IntroductionWord Sense Disambiguation (WSD) aims to identifythe sense of a word in a given context, using a pre-defined sense inventory containing the word?s differ-ent meanings (Navigli, 2009).
Traditionally, WSDapproaches have assumed that each occurrence ofa word is best labeled with a single sense.
How-ever, human annotators often disagree about whichsense is present (Passonneau et al, 2010), espe-cially in cases where some of the possible sensesare closely related (Chugur et al, 2002; McCarthy,2006; Palmer et al, 2007).Recently, Erk et al (2009) have shown that incases of sense ambiguity, a graded notion of senselabeling may be most appropriate and help reducethe ambiguity.
Specifically, within a given context,multiple senses of a word may be salient to thereader, with different levels of applicability.
For ex-ample, in the sentence?
The athlete won the gold metal due to her hardwork and dedication.multiple senses could be considered applicable for?won?
according to theWordNet 3.0 sense inventory(Fellbaum, 1998):1. win (be the winner in a contest or competition; be victo-rious)2. acquire, win, gain (win something through one?s efforts)3. gain, advance, win, pull ahead, make headway, get ahead,gain ground (obtain advantages, such as points, etc.)4.
succeed, win, come through, bring home the bacon, de-liver the goods (attain success or reach a desired goal)In this context, many annotators would agree that theathlete has both won an object (the gold metal itself)and won a competition (signified by the gold medal).Although contexts can be constructed to elicit onlyone of these senses, in the example above, a gradedannotation best matches human perception.Graded word sense (GWS) annotation offers sig-nificant advantages for sense annotation with a fine-grained sense inventory.
However, creating a suf-ficiently large annotated corpus for training super-vised GWS disambiguation models presents a sig-nificant challenge, i.e., the laborious task of gath-ering annotations for all combinations of a word?ssenses, along with variation in those senses appli-cabilities.
To our knowledge, Erk et al (2009) haveprovided the only data set with GWS annotations for11 terms.189Therefore, we consider the use of Word Sense In-duction (WSI) for GWS annotation.
WSI removesthe need for substantial training data by automati-cally deriving a word?s senses and associated sensefeatures through examining its contextual uses.
Fur-thermore, the data-driven sense discovery definessenses as they are present in the corpus, which mayidentify usages not present in traditional sense in-ventories (Lau et al, 2012).
Last, many WSI modelsrepresent senses loosely as abstractions over usages,which potentially may transfer well to expressingGWS annotations as a blend of their sense usages.In this paper, we consider the performance ofWSImodels on a GWS task.
The contributions of thispaper are as follows.
First, in Sec.
2, we motivatethree GWS annotation objectives and propose corre-sponding measures that provide fine-grained analy-sis of the capabilities of different WSI models.
Sec-ond, in Sec.
4, we propose two new sense mappingprocedures for converting an induced sense inven-tory to a reference sense inventory when GWS an-notations are present, and demonstrate significantperformance improvement using these procedureson GWS annotation.
Last, in Sec.
5, we demon-strate a complete evaluation framework using threegraph-based WSI models as examples, generatingseveral insights for how to better evaluate GWS dis-ambiguation systems.2 Evaluating GWS AnnotationsGraded word sense annotation conveys multiple lev-els of information, both in which senses are presentand their relative levels of applicability; and so, nosingle evaluation measure alone is appropriate forassessing GWS annotation capability.
Therefore, wepropose three objectives for the evaluating the senselabeling: (1) Detection of which senses are present,(2) Ranking senses according to applicability, and(3) Perception of the graded presence of each sense.We separate the three objectives as a way to evaluatehow well different techniques perform on each as-pect individually, which may encourage future workin ensemble WSD methods that use combinations ofthe techniques.
Figure 1 illustrates each evaluationon example annotations.
We note that Erk and Mc-Carthy (2009) have also proposed an alternate set ofevaluation measures for GWS annotations.
Whereapplicable, we describe and compare their measuresto ours for the three objectives.In the following definitions, let SiG refer to the setof senses {s1, .
.
.
, sn} present in context i accordingto the gold standard, and similarly, let SiL refer tothe set of senses for context i as labeled by a WSDsystem using the same sense inventory.
Let peri(sj)refer to the perceived numeric applicability rating ofsense sj in context i.Detection measures the ability to accurately iden-tify which senses are applicable in a given context,independent of their applicability.
While the mostbasic of the evaluations, systems that are highly ac-curate at multi-sense detection could be used for rec-ognizing ambiguous contexts where multiple sensesare applicable or for evaluating the granularity ofsense ontologies by testing for correlations betweensenses in a multi-sense labeling.
Detection is mea-sured using the Jaccard Index between SiG and SiLfor a given context i: SiG?SiLSiG?SiLRanking measures the ability to order the sensespresent in context i according to their applicabil-ity but independent of their quantitative applicabil-ity scores.
Even though multiple senses are present,a context may have a clear primary senses.
By pro-viding a ranking in agreement with human judge-ments, systems create a primary sense label for eachcontext.
When the induced senses are mapped to asense inventory, selecting the primary sense is analo-gous to non-graded WSD where a context is labeledwith its most applicable sense.To compare sense rankings, we use Goodman andKruskal?s ?, which is related to Kendall?s ?
rank cor-relation.
When the data has many tied ranks, ?
ispreferable to both Kendall?s ?
as well as Spearman?s?
rank correlation (Siegel and Castellan Jr., 1988),the latter of which is used by Erk and McCarthy(2009) for evaluating sense rankings.
The use of ?was motivated by our observation that in the GWSdataset (described later in Section 5.1), roughly 65%of the instances contained at least one tied rankingbetween senses.To compute ?, we examine all pair-wise combi-nations of senses (si, sj) of the target word.
LetrG(si) and rL(si) denote the ranks of sense si inthe gold standard and provided annotations.
In theevent that a ranking does not include senses, allof the inapplicable senses are assigned a tied rank190Instance Gold Standard AnnotationThe athlete won the gold metal due to herhard work and dedication.win.v.1: 0.6, win.v.2: 0.4(not applicable: win.v.3, win.v.4)Test Annotation Detection Ranking Perceptionwin.v.1: 0.7, win.v.2: 0.3 1.0 1.0 0.983win.v.1: 1.0 0.5 1.0 0.832win.v.2: 1.0 0.5 0.333 0.554win.v.3: 0.5, win.v.1: 0.3, win.v.4: 0.2 0.25 -0.2 0.405Figure 1: Example annotations of the same context compared with the gold standard according to Detection,Ranking, and Perception.lower than the least applicable sense; i.e., for mapplicable senses, all inapplicable senses have rankm+1.
A pair of senses, (si, sj) is said to be con-cordant if rG(si) < rG(sj) and rL(si) < rL(sj) orrG(si) > rG(sj) and rL(si) > rL(sj), and discor-dant otherwise.
?
is defined as c?dc+d where c is thenumber of concordant pairs and d is the number ofdiscordant.Perception measures the ability to equal humanjudgements on the levels of applicability for eachsense in a context.
Unlike ranking, this evaluationquantifies the difference in sense applicability.
As apotential application, these differences can be usedto quantify the contextual ambiguity.
For example,the relative applicability differences can be used todistinguish between ambiguous contexts where mul-tiple highly-applicable senses exist and unambigu-ous contexts where a single main sense exists butother senses are still minimally applicable.To quantify Perception, we compare sense label-ings using the cosine similarity.
Each labeling is rep-resented as a vector with a separate component foreach sense, whose value is the applicability of thatsense.
The Perception for two annotations of con-text j is then calculated as?i perj(sGi )?
perj(sLi )?
?i perj(sGi )2 ??
?i perj(sLi )2.Note that because all sense perceptibilities are non-negative, the cosine similarity is bounded to [0, 1].Erk and McCarthy (2009) propose an alternatemeasure for comparing the applicability values us-ing the Jensen-Shannon divergence.
The sense an-notations are normalized to probability distributions,denotedG and L, and the divergence is computed as:JSD(G||L) = 12DKL(G||M) +12DKL(L||M)where M is the average of the distributions G and Land DKL denotes the Kullback-Leibler divergence.While both approaches are similar in intent, we findthat the cosine similarity better matches the expecteddifference in Perception for cases where two anno-tations use different numbers of senses.
For exam-ple, the fourth test annotation in Fig.
1 has a JSS1of 0.593, despite its significant differences in order-ing and the omission of a sense.
Indeed, in caseswhere the set of senses in a test annotation is com-pletely disjoint from the set of gold standard senses,the JSS will be positive due to comparing the twodistributions against their average; In contrast, thecosine similarity in such cases will be zero, whichwe argue better matches the expectation that such anannotation does not meet the Perception objective.3 WSI ModelsFor evaluation we adapt three recent graph-basedWSI methods for the task of graded-sense annota-tion: Navigli and Crisafulli (2010), referred to asSquares, Jurgens (2011), referred to as Link, andUoY (Korkontzelos and Manandhar, 2010).
At anabstract level, these methods operate in two stages.First, a graph is built, using either words or wordpairs as vertices, and edges are added denoting someform of association between the vertices.
Second,senses are derived by clustering or partitioning thegraph.
We selected these methods based on their su-perior performance on recent benchmarks and also1The JSD is a distance measure in [0, 1], which we convertto a similarity JSS = 1?
JSD for easier comparison.191for their significant differences in approach.
Follow-ing, we briefly summarize each method to highlightits key parameters and then describe its adaptationto GWS annotation.Squares Navigli and Crisafulli (2010) propose amethod that builds a separate graph for each termfor sense induction.
First, a large corpus is used toidentify associated terms using the Dice coefficient:For two terms w1, w2, Dice(w1, w2) = 2c(w1,w2)c(w1)+c(w2)where c(w) is the frequency of occurrence.
Next,for a given term w the initial graph, G, is con-structed by adding edges to every term w2 whereDice(w,w2) ?
?, and then the step is repeated forthe neighbors of each term w2 that was added.Once the initial graph is constructed, edges arepruned to separate the graph into components.
Nav-igli and Crisafulli (2010) found improved perfor-mance on their target application using a pruningmethod based on the number of squares (closedpaths of length 4) in which an edge participates.
Lets denote the number of squares that an edge e par-ticipates in and p denote the number of squares thatwould be possible from the set of neighbors of e.Edges with sp < ?
are removed.
The remaining con-nected components in G denote the senses of w.Sense disambiguation on a context of w is per-formed by computing the intersection of the con-text?s terms with the terms in each of the connectedcomponents.
As originally specified, the componentwith the largest overlap is labeled as the sense of w.We adapt this to graded senses by returning all inter-secting components with applicability proportionalto their overlap.
Furthermore, for efficiency, we useonly noun, verb, and adjective lemmas in the graphs.Link Jurgens (2011) use an all-words methodwhere a single graph is built in order to derive thesenses of all words in it.
Here, the graph?s clus-ters do not correspond to a specific word?s sensesbut rather to contextual features that can be used todisambiguate any of the words in the cluster.In its original specification, the graph is built withedges between co-occurring words and edge weightscorresponding to co-occurrence frequency.
Edgesbelow a specified threshold ?
are removed, and thenlink community detection (Ahn et al, 2010) is ap-plied to discover sense-disambiguating word com-munities, which are overlapping cluster of verticesin the graph, rather than hard partitions.
Once the setof communities is produced, communities with threeor fewer vertices are removed, under the assumptionthat these communities contain too few features toreliably disambiguate.Senses are disambiguated by finding the commu-nity with the largest overlap score, computed as theweighted Jaccard Index.
For a context with the setof features Fi and a community with features Fj , theoverlap is measured as |Fj | ?
|Fi?Fj ||Fi?Fj | .We adapt this algorithm in three ways.
First,rather than use co-occurrence frequency to weightedges between terms, we weight edges accord totheir statistical association with the G-test (Dunning,1993).
The G-test weighting helps remove edgeswhose large edge weights are due to high corpus fre-quency but provide no disambiguating information,and the weighting also allows the ?
parameter tobe more consistently set across corpora of differentsizes.
Second, while Jurgens (2011) used only nounsas vertices in the graph, we include both verbs andadjectives due to needing to identify senses for both.Third, for graded senses, we disambiguate a contextby reporting all overlapping communities, weightedby their overlap score.UoY Korkontzelos and Manandhar (2010) pro-pose a WSI model that builds a graph for each termfor disambiguation.
The graph is built in four stages,with four main tuning parameters, summarized next.First, using a reference corpus, all contexts of thetarget word w are selected to build a list of co-occurring noun lemmas, retaining all those with fre-quency above P1.
Second, the Log-Likelihood ratio(Dunning, 1993) is computed between all selectednouns and w, retaining only those with an associa-tion above P2.
Third, all remaining nouns are usedto create all(n2)noun pairs.
Next, each term andpair is mapped to the set of contexts in the referencecorpus in which it is present.
A pair (wi, wj) is re-tained only if its set of contexts is dissimilar to thesets of contexts of both its member terms, using theDice coefficient to measure the similarity of the sets.Pairs with a Dice coefficient above P4 with either ofits constituent terms are removed.
Last, edges areadded between nouns and noun pairs according totheir conditional probabilities of occurring with eachother.
Edges with a conditional probability less than192P3 are not included.Once the graph has been constructed, the Chi-nese Whispers graph partitioning algorithm (Bie-mann, 2006) is used to identify word senses.
Eachgraph partition is assigned a separate sense of w.Next, each partition is mapped to the set of contextsin the reference corpus in which at least one of itsvertices occurs.
Partitions whose context sets are astrict subset of another are merged with the subsum-ing partition.Word sense disambiguation occurs by countingthe number of overlapping vertices for each parti-tion and selecting the partition with the highest over-lap as the sense of w. We extend this to graded an-notation by selecting all partitions with at least onevertex present and set the applicability equal to thedegree of overlap.4 Evaluation Across Sense InventoriesDirectly comparing GWS annotations from the in-duced and gold standard sense inventories requiresfirst creating a mapping from the induced senses tothe gold standard inventory.
Agirre et al (2006) pro-pose a sense-mapping procedure, which was used inthe previous two SemEval WSI Tasks (Agirre andSoroa, 2007; Manandhar et al, 2010).
We considerthis procedure and two extensions of it to supportlearning a mapping from graded sense annotations.The procedure of Agirre et al (2006) uses threecorpora: (1) a base corpus from which the sensesare derived, (2) a mapping corpus annotated withboth gold standard senses, denoted gs, and inducedsenses, denoted is, and (3) a test corpus annotatedwith is senses that will be converted to gs senses.Once the senses are induced from the base cor-pus, the mapping corpus is annotated with is sensesand a matrix M is built where cell i, j initially con-tains the counts of each time gsj and isi were usedto label the same instance.
The rows of this matrixare then normalized such that each cell now repre-sents p(gsj|isi).
The final mapping selects the mostprobable gs sense for each is sense.To label the test corpus, each instance that islabeled with isi is relabeled with the gs sensewith the highest conditional probability given isi.When a context c is annotated by a set of labelsL = {isi, .
.
.
, isj}, the final sense labeling con-tains the set of all gs to which the is senses weremapped, weighted by their mapping frequencies:perc(gsj) = 1|L|?isi?L ?
(isi, gsj) where ?
returns1 if isi is mapped to gsj and 0 otherwise.The original algorithm of Agirre et al (2006) doesnot consider the role of applicability in evaluatingwhether an is sense should be mapped to a gs sense;is senses with different levels of applicability in thesame context are treated equivalently in updatingM .
Therefore, as a first extension, referred to asGraded, we revise the update rule for constructingM where for the set of contexts C labeled by bothisi and gsj ,Mi,j =?c?C perc(isi)?perc(gsj).
Asin (Agirre et al, 2006), M is normalized and each issense is mapped to its most probable gs sense.To label the test corpus using theGraded method,the applicability of the is sense is also included.For a context c is annotated with senses L ={isi, .
.
.
, isj}, the final sense labeling contains theset of all gs senses to which the is senses weremapped, weighted by their mapping frequencies:perc(gsj) =?isi?L [?
(isi, gsj)?
perc(isi)] .
Theapplicabilities are then normalized to sum to 1.The prior two methods restrict an is sense to map-ping to only a single gs sense.
However, an is sensemay potentially correspond to multiple gs senses,each with different levels of applicability.
There-fore, we consider a second extension, referred to asDistribution, that uses the same matrix construc-tion as the Graded procedure, but rather than map-ping each is to a single sense, maps it to a distribu-tion over all gs senses for which it was co-annotated,which is the normalized row vector in M for an issense.
Labeling in the test corpus is then done bysumming the distributions of the is senses annotatedin the context and normalizing to create a probabilitydistribution over the union of their gs senses.5 ExperimentsWe adapt the supervised WSD setting used in priorSemEval WSI Tasks (Agirre and Soroa, 2007; Man-andhar et al, 2010) to evaluation the models accord-ing to the three proposed objectives.
In the super-vised setting, WSI systems provide GWS annotationof their induced senses for the test corpus, whichis already labeled with the gold-standard GWS an-notations.
Then, a portion of the test corpus withgold standard annotations is used to build a mappingfrom induced senses to the reference sense inven-193Term PoS # senses Avg.
# Sensesper Instanceadd verb 6 4.18ask verb 7 5.98win verb 4 3.98argument noun 7 5.18interest noun 7 5.12paper noun 7 5.54different adj.
5 4.98important adj.
5 4.82Table 1: The terms from the GWS dataset (Erk etal., 2009) used in this evaluationtory using one of the three algorithms described inSection 4.
The remaining, held-out test corpus in-stances have their induced senses converted to thegold standard sense inventory and the sense label-ings are evaluated for the three objectives from Sec-tion 2.
In our experiments we divide the referencecorpus into five evenly-sized segments and then usefour segments (80% of the test corpus) for construct-ing the mapping and then evaluate the convertedGWS annotations of the remaining segment.5.1 Graded Annotation DataThe gold standard GWS annotations are derivedfrom a subset of the GWS data provided by Erk etal.
(2009).
Here, three annotators rated the applica-bility of all WordNet 3.0 senses of a word in a singlesentence context.
Ratings were done using a 5-pointordinal ranking according to the judgements from 1?
this sense is not applicable to 5 ?
this usage exactlyreflects this sense.
Annotators used a wide-range ofresponses, leading to many applicable senses per in-stance.
We selected the subset of the GWS datasetwhere each term has 50 annotated contexts, whichwere distributed evenly between SemCor (Miller etal., 1993) and the SENSEVAL-3 lexical substitutioncorpus (Mihalcea et al, 2004).
Table 1 summarizesthe target terms in this context.To prepare the data for evaluation, we constructedthe gold standard GWS annotations using the meanapplicability ratings of all three annotators for eachcontext.
Senses that received a mean rating of 1 (notapplicable) were not listed in gold standard labelingfor that instance.
All remaining responses were nor-malized to sum to 1.5.2 Model ConfigurationFor consistency, all three WSI models were trainedusing the same reference corpus.
We used a 2009snapshot of Wikipedia,2 which was PoS tagged andlemmatized using the TreeTagger (Schmid, 1994).All of target terms occurred over 12,000 times.
TheG-test between terms was computed using a three-sentence sliding window within each article in thecorpus.
The Dice coefficient was calculated using asingle sentence as context.For all three models, we performed a limited gridsearch to find the best performing system param-eters, within reasonable computational limits.
Wesummarize the parameters and models, selecting theconfiguration with the highest average Perceptionscore.
For all models, the applicability ratings foreach instance are normalized to sum to 1.Model Parameter Range SelectedSquares ?={0.008, 0.009, .
.
.
, 0.092} 0.037?={0.25, 0.30, .
.
.
, 0.50, 0.55} 0.55Link ?={400, 500, .
.
.
, 900, 1000} 500UoYP1={10, 20} 20P2={10, 20, 30} 20P3={0.2, 0.3, 0.4} 0.3P4={0.4, 0.6, 0.8} 0.45.3 BaselinesPrior WSI evaluations have used the Most FrequentSense (MFS) labeling a strong baseline in the super-vised WSD task.
For the GWS setting, we considerfive other baselines that select one, some, or all ofthe sense of the target word, with different orderingstrategies.
In the six baselines, each instance is la-beled as follows:MFS: the most frequent sense of the wordRS: a single, randomly-selected senseASF: all senses, ranked in order of frequency startingwith the most frequentASR: all senses, randomly rankedASE: all senses, ranked equallyRSM: a random number of senses, ranked arbitrarilyTo establish applicability values from a ranking of nsenses, we set applicability to the ith ranked sense of(n?i)+1?nk=1 k, where rank 1 is the highest ranked sense.2http://wacky.sslmit.unibo.it/194Agirre et al (2006) Mapping Graded Mapping Distribution MappingModel D R P D R P D R P RecallSquares 0.192 -0.024 0.382 0.198 0.555 0.504 0.879 0.562 0.925 0.560Link 0.282 0.081 0.454 0.335 0.436 0.528 0.854 0.503 0.907 0.800UoY 0.238 0.116 0.445 0.244 0.486 0.528 0.848 0.528 0.907 0.940Table 2: Average performance of the three WSI models according to Detection, Ranking, and PercetionBaseline Detection Ranking PerceptionMFS 0.204 0.334 0.469RS 0.167 -0.036 0.363ASF 0.846 0.218 0.830ASR 0.846 0.006 0.776ASE 0.846 0.000 0.862RSM 0.546 0.005 0.632Table 3: Average performance of the six baselines5.4 Results and DiscussionEach WSI model was trained and then used to la-bel the sense of each target term in the GWS corpus.The three sense-mapping procedures were then ap-plied to the induced sense labels on the held-out in-stances to perform a comparison in the graded senseannotations.
Table 2 reports the performance for thethree evaluation measures for each model and map-ping configuration on all instances where the sensemapping is defined.
The sense mapping is unde-fined when (1) a WSI model cannot match an in-stance?s features to any of its senses therefore leavesthe instance unannotated or (2) when an instance islabeled with an is sense not seen in the training data.Therefore, we report the additional statistic, Recall,that indicates the percentage of instances that wereboth labeled by the WSI model and mapped to gssenses.
Table 3 summarizes the baselines?
perfor-mance.The results show three main trends.
First, intro-ducing applicability into the sense mapping processnoticeably improves performance.
For almost allmodels and scores, using the Graded Mapping im-proves performance a small amount.
However, thelargest increase comes from using the Distributionmapping where induced senses are represented asdistributions over the gold standard senses.Second, performance was well ahead of the base-lines across the three evaluations, when consider-ing the models?
best performances.
The Squaresand Link models were able to outperform the base-lines that list all senses on the Detection objec-tive, which the UoY model only improves slightlyfrom this baseline.
For the Ranking objective, allmodels substantially outperform the best baseline,MFS; and similarly, for the Perception objective,all models outperform the best performing baseline,ASE.
Overall, these performance suggest that in-duce senses can be successfully used to producequality GWS annotations.Third, the WSI models themselves show signif-icant differences in their recall and multi-labelingfrequencies.
The Squares model is only able to la-bel approximately 56% of the GWS instances due tosparseness in its sense representation.
Indeed, only12 of its 237 annotated instances received more thanone sense label, revealing that the model?s perfor-mance is mostly based on correctly identifying theprimary sense in a context and not on identifyingthe less applicable senses.
The UoY model shows asimilar trend, with most instances being assigned amedian of 2 senses.
However, its sense representa-tion is sufficiently dense to have the highest recall ofany of the models.
In contrast to the other two mod-els, the Link model varies significantly in the num-ber of induced senses assigned: ?argument,?
?ask,??different,?
and ?win?
were assigned over 60 senseson average to each of their instances, with ?differ-ent?
having an average of 238, while the remainingterms were assigned under two senses on average.Furthermore, the results also revealed two unex-pected findings.
First, the ASE baseline performedunexpectedly high in Perception, despite its assign-ment of uniform applicability to all senses.
We hy-pothesize this is due to the majority of instances inthe GWS dataset being labeled with most of a word?ssenses, as indicated by Table 1, which results in their195perceptibilities becoming normalized to small val-ues.
Because the ASE solution has applicability rat-ings for all senses, normalization brings the ratingsclose to those of the gold standard solution, and fur-thermore, the difference in score between applicableand inapplicable senses become too small to signifi-cantly affect the resulting cosine similarity.
As an al-ternate model, we reevaluated the baselines againstthe gold standard using the Jensen-Shannon diver-gence as proposed by Erk and McCarthy (2009).Again, ASE is still the highest performing baselineon Perception.
The high performance for both eval-uation measures suggests that an alternate measuremay be better suited for quantifying the differencein solutions?
GWS applicabilities.Second, performance was higher on the Percep-tion task than on Ranking, the former of which wasanticipated being more difficult.
We attribute thelower Ranking performance to two factors.
First,the GWS data contains main tied rank senses; how-ever, ties in sense ranks after the mapping processare relatively rare, which reduces ?.
Second, in-stances in the GWS often have senses within closeapplicability ranges.
When scoring an induced an-notation that swaps the applicability, the Perceptionis less affected by the small change in applicabilitymagnitude, whereas Ranking is more affected due tothe change in ordering.6 Conclusion and Future WorkGWS annotations offer great potential for reli-ably annotating using fine-grained sense invento-ries, where word instance may elicit several concur-rent meanings.
Given the expense of creating an-notated training corpora with sufficient examples ofthe graded senses, WSI offers significant promise forlearning senses automatically while needing only asmall amount GWS annotated data to learn the sensemapping for a WSD task.In this paper, we have carried out an initial studyon the performance of WSI systems on a GWS an-notation task.
Our primary contribution is an end-to-end framework for mapping and evaluating in-duced GWS data.
We first proposed three objectivesfor graded sense annotation along with correspond-ing evaluation measures that reliably convey the ef-fectiveness given the nature of GWS annotations.Second, we proposed two new mapping proceduresthat use graded sense applicability for converting in-duced senses into a reference sense inventory.
Usingthree graph-based WSI models, we demonstratedthat incorporating graded sense applicability into thesense mapping significantly improves GWS perfor-mance over the commonly used method of Agirre etal.
(2006).
Furthermore, our study demonstrated thepotential of WSI systems, showing that all the mod-els were able to outperform all six of the proposedbaseline on the Ranking and Perception objectives.Our findings raise several avenues for futurework.
First, our study only considered three graph-based WSI models; future work is needed to as-sess the capabilities other WSI approaches, such asvector-based or Bayesian.
We are also interested incomparing the performance of the Link model withother recently developed all-words WSI approachessuch as Van de Cruys and Apidianaki (2011).Second, the proposed evaluation relies on a su-pervised mapping to the gold standard sense inven-tory, which has potential to lose information and in-correctly map new senses not in the gold standard.While unsupervised clustering evaluations such asthe V-measure (Rosenberg and Hirschberg, 2007)and paired Fscore (Artiles et al, 2009) are capableof evaluating without such a mapping, future workis needed to test extrinsic soft clustering evaluationssuch as BCubed (Amigo?
et al, 2009) or developanalogous techniques that take into account gradedclass membership used in GWS annotations.Last, we note that our setup normalized the GWSratings into probability distribution, which is stan-dard in the SemEval evaluation setup.
However, thisnormalization incorrectly transforms GWS annota-tions where no predominant sense was rated at thehighest value, e.g., an annotation of only two sensesrated as 3 on a scale of 1 to 5.
While these percepti-bilities may be left unnormalized, it is not clear howto compare the induced GWS annotations with suchmid-interval values, or when the rating scale of theWSI system is potentially unbounded.
Future workis needed both in GWS evaluation and in quantify-ing applicability along a range in GWS-based WSIsystems to address this issue.All models and data will be released as a part ofthe S-Space Package (Jurgens and Stevens, 2010).33https://github.com/fozziethebeat/S-Space196ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task02: Evaluating word sense induction and discrimina-tion systems.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations, pages 7?12.ACL, June.Eneko Agirre, David Mart?
?nez, Oier o?
de Lacalle, andAitor Soroa.
2006.
Evaluating and optimizing the pa-rameters of an unsupervised graph-based WSD algo-rithm.
In Proceedings of TextGraphs: the First Work-shop on Graph Based Methods for Natural LanguageProcessing, pages 89?96.
Association for Computa-tional Linguistics.Yong-Yeol Ahn, James P. Bagrow, and Sune Lehmann.2010.
Link communities reveal multiscale complexityin networks.
Nature, (466):761?764, August.Enrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Infor-mation Retrieval, 12(4):461?486.Javier Artiles, Enrique Amigo?, and Julio Gonzalo.
2009.The role of named entities in web people search.
InProceedings of EMNLP, pages 534?542.
Associationfor Computational Linguistics.Chris Biemann.
2006.
Chinese whispers: an efficientgraph clustering algorithm and its application to natu-ral language processing problems.
In Proceedings ofthe First Workshop on Graph Based Methods for Nat-ural Language Processing, pages 73?80.
Associationfor Computational Linguistics.Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and sense proximity in the senseval-2 testsuite.
In Proceedings of the ACL-02 workshop onWord sense disambiguation: recent successes and fu-ture directions - Volume 8, WSD ?02, pages 32?39,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational linguis-tics, 19(1):61?74.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1, pages 440?449.Association forComputational Linguistics.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word us-ages.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 1, pages 10?18.Associationfor Computational Linguistics.Christine Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.David Jurgens and Keith Stevens.
2010.
The S-SpacePackage: An Open Source Package for Word SpaceModels.
In Proceedings of the ACL 2010 SystemDemonstrations.
Association for Computational Lin-guistics.David Jurgens.
2011.
Word sense induction by com-munity detection.
In Proceedings of Sixth ACL Work-shop on Graph-based Methods for Natural LanguageProcessing (TextGraphs-6).
Association for Computa-tional Linguistics.Ioannis Korkontzelos and Suresh Manandhar.
2010.Uoy: Graphs of unambiguous vertices for word senseinduction and disambiguation.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 355?358.
Association for Computational Lin-guistics.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense induc-tion for novel sense detection.
In Proceedings of the13th Conference of the European Chapter of the Asso-ciation for computational Linguistics (EACL 2012).Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
SemEval-2010 task 14: Word sense induction & disambigua-tion.
In Proceedings of the 5th International Workshopon Semantic Evaluation, pages 63?68.
Association forComputational Linguistics.Diana McCarthy.
2006.
Relating WordNet senses forword sense disambiguation.
Making Sense of Sense:Bringing Psycholinguistics and Computational Lin-guistics Together, page 17.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The senseval-3 english lexical sample task.In Senseval-3: Third International Workshop on theEvaluation of Systems for the Semantic Analysis ofText, pages 25?28.
Barcelona, Spain, Association forComputational Linguistics.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.In Proceedings of the workshop on Human LanguageTechnology, pages 303?308.
Association for Compu-tational Linguistics.Roberto Navigli and Giuseppe Crisafulli.
2010.
Inducingword senses to improve web search result clustering.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 116?126.
Association for Computational Linguistics.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys (CSUR), 41(2):10.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
2007.
Making fine-grained and coarse-grained197sense distinctions, both manually and automatically.Natural Language Engineering, 13(02):137?163.Rebecca J. Passonneau, Ansaf Salleb-Aoussi, VikasBhardwaj, and Nancy Ide.
2010.
Word sense anno-tation of polysemous words by multiple annotators.
InProceedings of Seventh International Conference onLanguage Resources and Evaluation (LREC-7).Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).
ACL, June.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing.Sidney Siegel and N. John Castellan Jr. 1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, second edition.Tim Van de Cruys and Marianna Apidianaki.
2011.
La-tent Semantic Word Sense Induction and Disambigua-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies (ACL/HLT), pages 1476?1485.198
