Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 201?209,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsFine-Grained Classification of Named EntitiesExploiting Latent Semantic KernelsClaudio GiulianoFBK-irstI-38100, Trento, Italygiuliano@fbk.euAbstractWe present a kernel-based approach for fine-grained classification of named entities.
Theonly training data for our algorithm is a fewmanually annotated entities for each class.
Wedefined kernel functions that implicitly mapentities, represented by aggregating all con-texts in which they occur, into a latent seman-tic space derived from Wikipedia.
Our methodachieves a significant improvement over thestate of the art for the task of populating anontology of people, although requiring con-siderably less training instances than previousapproaches.1 IntroductionPopulating an ontology with relevant entities ex-tracted from unstructured textual documents is acrucial step in Semantic Web and knowledge man-agement systems.
As the concepts in an ontologyare generally arranged in deep class/subclass hierar-chies, the problem of populating ontologies is typi-cally solved top-down, firstly identifying and classi-fying entities in the most general concepts, and thenrefining the classification process.Recent advances have made supervised ap-proaches very successful in entity identification andclassification.
However, to achieve satisfactory per-formance, supervised systems must be supplied witha sufficiently large amount of training data, usuallyconsisting of hand tagged texts.
As domain specificontologies generally contains hundreds of subcate-gories, such approaches are not directly applicablefor a more fine-grained categorization because thenumber of documents required to find sufficient pos-itive examples for all subclasses becomes too large,making the manual annotation very expensive.Consequently, in the literature, supervised ap-proaches are confined to classify entities into broadcategories, such as persons, locations, and or-ganizations, while the fine-grained classificationhas been approached with minimally supervised(e.g., Tanev and Magnini (2006) and Giuliano andGliozzo (2008)) and unsupervised learning algo-rithms (e.g., Cimiano and Vo?lker (2005) and Giu-liano and Gliozzo (2007)).Following this trend, we present a minimally su-pervised approach to fine-grained categorization ofnamed entities previously recognized into coarse-grained categories, e.g., by a named-entity recog-nizer.
The only training data for our algorithm is afew manually annotated entities for each class.
Forexample, Niels Bohr, Albert Einstein, and EnricoFermi might be used as examples for the class physi-cists.
In some cases, training entities can be acquired(semi-) automatically from existing ontologies al-lowing us to automatically derive training entitiesfor use with our machine learning algorithm.
Forinstance, we may easily obtain tens of training en-tities for very specific classes, such as astronomers,materials scientists, nuclear physicists, by queryingthe Yago ontology (Suchanek et al, 2008).We represent the entities using features extractedfrom the textual contexts in which they occur.Specifically, we use a search engine to collect suchcontexts from the Web.
Throughout this paper, wewill refer to such a representation as multi-contextrepresentation, in contrast to the single-context rep-201resentation in which an entity is categorized usingsolely features extracted from the local context sur-rounding it, usually a window of a few words aroundthe entity occurrence.
Single-context features arecommonly used in named-entity recognition, how-ever to assign very specific categories the local con-text might not provide sufficient information.
Forexample, in the sentence ?Prof.
Enrico Fermi dis-covered a way to induce artificial radiation in heavyelements by shooting neutrons into their atomic nu-clei,?
single-context features such as, the prefix Prof.and the capital letters, provides enough evidence thatEnrico Fermi is a person and a professor.
However,to discover that he is a physicist we need to analyzea wider context, or alternatively multiple ones.
Re-cently, Ganti et al (2008) has shown that exploitingmulti-context information can greatly improve thefine-grained classification of named entities, whencompared to methods using single context only.In order to effectively represent entities?
multi-contexts, we extend the traditional vector spacemodel (VSM), offering a way to integrate externalsemantic information in the classification process bymeans of latent semantic kernels (Shawe-Taylor andCristianini, 2004).
As a result, we obtain a general-ized similarity function between multi-contexts thatincorporates semantic relations between terms, auto-matically learned from unlabeled data.
In particular,we use Wikipedia to build the latent semantic space.The underlying idea is that similar named entitiestend to have a similar description in Wikipedia.
AsWikipedia provides reliable information and it ex-ceeds all other encyclopedias in coverage, it shouldbe a valuable resource for the task of populating anontology.
To validate this hypothesis, we comparethis model with one built from a news corpus.Our approach achieves a significant improvementover the state of the art for the task of populating thePeople Ontology (Giuliano and Gliozzo, 2008), al-though requiring considerably less training instancesthan previous approaches.
The task consists in clas-sifying person names into a multi-level taxonomycomposed of 21 categories derived from WordNet,making very fine-grained distinctions (e.g., physi-cists vs. mathematicians).
It provides a more real-istic and challenging benchmark than the ones pre-viously available (e.g., Tanev and Magnini (2006)and Fleischman and Hovy (2002)), that consider asmaller number of categories arranged in a one-leveltaxonomy.2 Entity RepresentationThe goal of our research is to determine the fine-grained categories of named entities requiring a min-imal amount of human supervision.Our method is based on the common assump-tion that named entities co-occurring with the same(domain-specific) terms are highly probable to referto the same categories.
For example, quantum me-chanics, atomic physics, and Nobel Prize in physicsare all terms that bound Niels Bohr and Enrico Fermito the concept of physics.To automatically derive features for the trainingand testing entities we proceed as follows.
We paireach entity i with a multi-context mi obtained byquerying a search engine with the entity ?i?
andmerging the first M snippets si,j returned (1 6 j 6M ).
A multi-context is therefore a fictitious doc-ument obtained by aggregating snippets, i.e., sum-mary texts of the search engine result.
Formally,mi = ?Mj=1si,j , where the operator ?
denotes theconcatenation of strings.
For example, Figure 1 (a)and (b) show some snippets retrieved for ?EnricoFermi?
and ?Albert Einstein,?
while s1?
s2?
s3 ands4 ?
s5 ?
s6 represent their multi-contexts, respec-tively.The following section describes how entities?multi-contexts are embedded into the feature spacein order to train a kernel-based classifier.3 Kernels for Fine-Grained Classificationof EntitiesThe strategy adopted by kernel methods (Shawe-Taylor and Cristianini, 2004; Scho?lkopf and Smola,2002) consists of splitting the learning problem intwo parts.
They first embed the input data in a suit-able feature space, and then use a linear algorithm(e.g., the perceptron) to discover nonlinear pattern inthe input space.
Typically, the mapping is performedimplicitly by a so-called kernel function.
The ker-nel function is a similarity measure between the in-put data that depends exclusively on the specific datatype and domain.
A typical similarity function is theinner product between feature vectors.
Characteriz-ing the similarity of the inputs plays a crucial role in202s1: [Enrico Fermi]PER discovered that many nuclear transformations could be conducted by using neutrons.s2: [Enrico Fermi]PER led the manhattan project?s effort to create the first man-made and self-sustaining nuclear chain.s3: [Enrico Fermi]PER was most noted for his work on the development of the first nuclear reactor.
(a)s4: [Albert Einstein]PER did not directly participate in the invention of the atomic bomb.s5: [Albert Einstein]PER is one of the most recognized and well-known scientists of the century.s6: [Albert Einstein]PER was born at Ulm, in Wu?rttemberg, Germany, on March 14, 1879.
(b)Figure 1: Examples of snippets retrieved for Enrico Fermi (a) and Albert Einstein (b).determining the success or failure of the learning al-gorithm, and it is one of the central questions in thefield of machine learning.Formally, the kernel is a function k : X?X ?
Rthat takes as input two data objects (e.g., vectors,texts, parse trees) and outputs a real number charac-terizing their similarity, with the property that thefunction is symmetric and positive semi-definite.That is, for all xi, xj ?
X , it satisfiesk(xi, xj) = ??
(xi), ?(xj)?
(1)where ?
is an explicit mapping from X to an (innerproduct) feature space F .In the next sections, we define and combine differ-ent kernel functions that calculate the pairwise sim-ilarly between multi-contexts.
They are the only do-main specific element of our classification system,while the learning algorithm is a general purposecomponent.
Many classifiers can be used with ker-nels.
The most popular ones are perceptron, sup-port vector machines (SVM), and k-nearest neighbor(KNN).3.1 Bag-of-Words KernelThe simplest method to estimate the similarity be-tween two multi-contexts is to compute the innerproduct of their vector representations in the VSM.Formally, we define a space of dimensionality N inwhich each dimension is associated with one wordfrom the dictionary, and the multi-context m is rep-resented by a row vector?
(m) = (f(t1,m), f(t2,m), .
.
.
, f(tN ,m)), (2)where the function f(ti,m) records whether a par-ticular token ti is used in m. Using this representa-tion we define bag-of-words kernel between multi-contexts asKBOW (m1,m2) = ??
(m1), ?(m2)?
(3)However, the bag-of-words representation doesnot deal well with lexical variability.
To significantlyreduce the training set size, we need to map contextscontaining semantically equivalent terms into simi-lar feature vectors.
To this aim, in the next section,we introduce the class of semantic kernels and showhow to define an effective semantic VSM using (un-labeled) external knowledge.3.2 Semantic KernelsIt has been shown that semantic information is fun-damental for improving the accuracy and reducingthe amount of training data in many natural languagetasks, including fine-grained classification of namedentities (Fleischman and Hovy, 2002), question clas-sification (Li and Roth, 2005), text categorization(Giozzo and Strapparava, 2005), word sense disam-biguation (Gliozzo et al, 2005).In the context of kernel methods, semantic infor-mation can be integrated considering linear trans-formations of the type ??
(cj) = ?
(cj)S, where Sis a N ?
k matrix (Shawe-Taylor and Cristianini,2004).
The matrix S can be rewritten as S = WP,where W is a diagonal matrix determining the wordweights, while P is the word proximity matrix cap-turing the semantic relations between words.
Theproximity matrix P can be defined by setting non-zero entries between those words whose semanticrelation is inferred from an external source of do-main knowledge.
The semantic kernel takes the gen-eral formk?
(mi,mj) = ?(mi)SS??(mj)?
= ??(mi)??(mj)?.
(4)It follows directly from the explicit construction thatEquation 4 defines a valid kernel.WordNet and manually constructed lists of se-mantically related words typically provide a sim-ple way to introduce semantic information into the203kernel.
To define a semantic kernel from such re-sources, we could explicitly construct the proximitymatrix P by setting its entries to reflect the semanticproximity between the words i and j in the specificlexical resource.
However, we prefer an approachthat exploits unlabeled data to automatically buildthe proximity matrix, defining a language and do-main independent approach.3.2.1 Latent Semantic KernelTo define a proximity matrix, we look at co-occurrence information in a (large) corpus.
Twowords are considered semantically related if theyfrequently co-occur in the same texts.
We use sin-gular valued decomposition (SVD) to automaticallyderive the proximity matrix ?
from a corpus, rep-resented by its term-by-document matrix D, wherethe Di,j entry gives the frequency of term ti in doc-ument dj .1 SVD decomposes the term-by-documentmatrix D into three matrixes D = U?V?, where Uand V are orthogonal matrices (i.e., U?U = I andV?V = I) whose columns are the eigenvectors ofDD?
and D?D respectively, and ?
is the diagonalmatrix containing the singular values of D.Under this setting, we define the proximity matrix?
as follows:?
= Uk?k, (5)where Uk is the matrix containing the first kcolumns of U and k is the dimensionality of the la-tent semantic space and can be fixed in advance.
Byusing a small number of dimensions, we can define avery compact representation of the proximity matrixand, consequently, reduce the memory requirementswhile preserving most of the information.The matrix ?
is used to define a linear transfor-mation pi : RN ?
Rk, that maps the vector ?
(mj),represented in the standard VSM, into the vector??
(mj) in the latent semantic space.
Formally, pi isdefined as followspi(?
(mj)) = ?(mj)(W?)
= ??
(mj), (6)where ?
(mj) is a row vector, W is a N ?
N diag-onal matrix determining the word weights such thatWi,i = log(idf(wi)), where idf(wi) is the inversedocument frequency of wi.1SVD has been first applied to perform latent semantic anal-ysis of terms and latent semantic indexing of documents in largecorpora by Deerwester et al (1990).Finally, the latent semantic kernel is explicitly de-fined as followsKLS(mi,mj) = ?pi(?
(mi)), pi(?
(mj))?, (7)where ?
is the mapping defined in Equation 2 andpi is the linear transformation defined in Equation 6.Note that we have used a series of successive map-pings each of which adds some further improvementto the multi-context representation.3.3 Composite KernelFinally, to combine the two representations of multi-contexts, we define the composite kernel as followsKBOW (m1,m2) +KLS(m1,m2).
(8)It follows directly from the explicit construction ofthe feature space and from closure properties of ker-nels that it is a valid kernel.4 ExperimentsIn this section, we compare performance of differentkernel setups and previous approaches on an ontol-ogy population task.4.1 BenchmarkExperiments were carried out on the People Ontol-ogy (Giuliano and Gliozzo, 2008).
An ontologyextracted from WordNet, containing 1,657 distinctperson instances arranged in a multi-level taxonomyhaving 21 fine-grained categories (Figure 2).
To pro-vide a formal distinction between classes and in-stances, required to assign instances to classes, theauthors followed the directives defined by Gangemiet al (2003) for OntoWordNet, in which the infor-mal WordNet semantics is re-engineered in terms ofa description logic.In order to have a fair comparison, we reproducedthe same experimental settings used in Giuliano andGliozzo (2008).
The population task is cast as a cate-gorization problem, trying to assign person instancesto the most specific category.
For each class, the in-stances were randomly split into two equally sizedsubsets.
One is used for training and the other fortest, and vice versa.
The reported results are the av-erage performance over these two subsets.
When aninstance is assigned to a sub-class it is also implic-itly assigned to all its super-classes.
For instance,204Figure 2: The People Ontology defined by Giuliano and Gliozzo (2008).
Numbers in brackets are the total numbersof person instances per category.
Concepts with less than 40 instances were removed.classifying Salvador Dali as painter we implicitlyclassify him as artist and creator.
The evaluationis performed as proposed by Melamed and Resnik(2000) for a similar hierarchical categorization task.For instance, classifying John Lennon as painter, weobtain a false positive for the spurious classificationpainter, a false negative for missing class musician,and two true positives for the correct assignment tothe super-classes artist and creator.4.2 Experimental SettingsWe built two proximity matrices ?W and ?NY T .The former is derived from the 200,000 most visitedWikipedia articles, while the latter from 200,000 ar-ticles published by the New York Times betweenJune 1, 1998 and January 01, 2000.
After remov-ing terms that occur less than 5 times, the result-ing dictionaries contain about 300,000 and 150,000terms respectively.
We used the SVDLIBC pack-age2 to compute the SVD, truncated to 400 dimen-sions.
To derive the multi-context representation, wecollected 100 english snippets for each person in-stance by querying GoogleTM.
To classify each per-son instance into one of the fine-grained categories,we used a KNN classifier (K = 1).
No parameteroptimization was performed.2http://tedlab.mit.edu/?dr/svdlibc/4.3 ResultsTable 1 shows micro- and macro-averaged resultsfor KBOW , KW , KBOW +KW , KNY T , KBOW +KNY T , the IBOP method (Giuliano and Gliozzo,2008), the random baseline, and most frequent base-line.3 Where KW and KNY T are instances of thelatent semantic kernel, KLS , using the proximitymatrices ?W and ?NY T , derived from Wikipediaand the New York Times corpus, respectively.
Ta-ble 2 shows detailed results for each sub- and super-category for KBOW +KW .
Table 3 shows the con-fusion matrix of KBOW + KW , in which the rowsare ground truth classes and the columns are predic-tions.
The matrix has been calculated for the finer-grained categories and, then, grouped according totheir super-class.
To be compared with the IBOPmethod, all experiments were conducted using only20 training examples per category.
Finally, figure3 shows the learning curves for KBOW + KW ob-tained varying the number of snippets (12, 25, 50,and 100) used to derive the multi-contexts.4.4 DiscussionOn the one hand, the results (Table 2) show thatlearning the semantic model from Wikipedia givesno significant improvement.
Therefore, we reject thehypothesis that encyclopedic knowledge can provide3The most frequent category has been estimated on the train-ing data.2050.50.550.60.650.70.750.82  4  6  8  10  12  14  16  18  20Micro F1Number of instances12 snippetsKBOWKWKBOW+KW  0.50.550.60.650.70.750.82  4  6  8  10  12  14  16  18  20Micro F1Number of instances25 snippetsKBOWKWKBOW+KW0.50.550.60.650.70.750.82  4  6  8  10  12  14  16  18  20Micro F1Number of instances50 snippetsKBOWKWKBOW+KW  0.50.550.60.650.70.750.82  4  6  8  10  12  14  16  18  20Micro F1Number of instances100 snippetsKBOWKWKBOW+KWFigure 3: Learning curves for KBOW +KW obtained varying the number of snippets used to derive the training andtest sets.
From top-left to bottom right: 12, 25, 50, and 100.Method Micro-F1 Macro-F1KBOW 75.6 70.6KW 78.1 73.1KBOW +KW 80.0 75.4KNY T 77.6 72.9KBOW + KNY T 79.7 75.1IBOP 70.1 62.3Random 15.4 15.5Most Frequent 20.7 3.3Table 1: Comparison among the kernel-based ap-proaches, the IBOP method (Giuliano and Gliozzo,2008), the random baseline, and most frequent baseline.more accurate semantic models than general pur-pose corpora.
Moreover, further experiments haveshown that even a larger number of Wikipedia ar-ticles (600,000) does not help.
On the other hand,the latent semantic kernels outperform all the othermethods, and their composite (KBOW + KW andKBOW + KNY T ) perform the best on every con-figuration, demonstrating the effectiveness of la-tent semantic kernels in fine-grained classificationof named entities.
As in text categorization andword sense disambiguation, they have proven effec-tive tools to overcome the limitation of the VSM byintroducing semantic similarity among words.An important characteristic of the approach is thesmall number of training examples required per cat-egory.
This affects both the prediction accuracy andthe computation time (this is generally a commonproperty of instance-based algorithms).
The learn-ing curves (Figure 3) show that the composite ker-nel (KBOW +KLS) obtained the same performanceof the bag-of-word kernel (KBOW ) using less thanhalf of the training examples per category.
Thedifference is much more pronounced when usingless snippets.
The composite kernel KBOW + KWreaches a plateau around 10 examples, and after 20examples adding more examples does not signifi-cantly improve the classification performance.As most of entities in the People Ontol-ogy are celebrities, all the snippets retrieved byGoogleTMgenerally refer to them, alleviating theproblem of ambiguity of proper names.
However,person names are highly ambiguous.
In a more real-istic scenario, the result of a search engine for a per-son name is usually a mix of contexts about differententities sharing the same name.
In this case, our ap-proach have to be combined with a system that clus-ters the search engine result, where each cluster isassumed to contain all (and only those) contexts thatrefer to the same entity.
The WePS evaluation cam-paign on disambiguation of person names (Artiles etal., 2007; Artiles et al, 2009) has shown that the bestclustering systems achieve a precision of about 90%206Scientist Performer Creator Communicator Business HealthPhy Mat Che Bio Soc Act Mus Fil Pai Mus Poe Dra Rep man profPhy 118 24 10 4 2 0 0 0 0 0 0 0 0 7 2Mat 2 33 0 0 1 0 0 0 0 0 1 0 0 3 0Che 13 2 68 9 2 0 0 0 0 0 0 0 0 5 2Bio 3 0 7 52 0 0 0 0 1 0 0 0 1 6 6Soc 0 4 1 1 55 0 0 0 0 0 3 1 1 4 2Act 0 0 0 0 0 98 5 27 0 0 2 14 0 3 0Mus 0 0 0 0 0 17 67 0 0 32 1 0 1 2 1Fil 0 0 0 0 0 13 0 45 0 0 1 4 0 2 0Pai 0 0 0 1 1 2 0 1 100 0 1 0 0 1 0Mus 0 0 0 0 0 4 29 0 0 139 0 1 0 0 0Poe 0 2 0 0 0 0 0 0 7 3 98 26 1 2 3Dra 0 0 0 1 1 9 0 1 0 1 12 61 1 4 1Rep 0 0 0 0 0 1 1 0 2 0 0 0 197 22 0Bus 1 0 1 0 1 0 0 1 0 1 0 0 1 36 0Hea 0 0 0 8 4 0 1 0 0 0 0 1 1 2 31Table 3: Confusion matrix of KBOW +KW for the more fine-grained categories grouped according to their top-levelconcepts of the People Ontology.Category Prec.
Recall F1Scientist 95.1 90.1 92.6Physicist 86.1 70.7 77.6Mathematician 50.8 82.5 62.9Chemist 78.2 67.3 72.3Biologist 68.4 68.4 68.4Social scientist 82.1 76.4 79.1Performer 75.7 69.3 72.3Actor 68.1 65.8 66.9Musician 65.0 55.4 59.8Creator 78.9 82.6 80.7Film Maker 60.0 69.2 64.3Artist 83.6 85.4 84.5Painter 90.9 93.5 92.2Musician 79.0 80.3 79.7Communicator 91.9 86.7 89.2Representative 96.6 88.3 92.3Writer 86.8 84.2 85.5Poet 82.4 69.0 75.1Dramatist 56.5 66.3 61.0Business man 36.4 85.7 51.1Health professional 64.6 64.6 64.6micro 80.9 79.6 80.2macro 75.1 76.3 75.7Table 2: Results for each category using KBOW +KW .and a recall of about 70% and that, in the major-ity of the cases, the number of contexts per entity isless than 20.
This shows that latent semantic kernelsare an effective tool for fine-grained classification ofperson names.Finally, table 3 shows that misclassification er-rors are largely distributed among categories belong-ing to the same super-class (i.e., the blocks on themain diagonal are more densely populated than oth-ers).
As expected, the algorithm is much more accu-rate for the top-level concepts (i.e., Scientist, Com-municator, etc.
), where the category distinctions areclearer, while a further fine-grained classification, insome cases, is even difficult for human annotators.5 Related WorkFleischman and Hovy (2002) approach the fine-grained classification of person instances using su-pervised learning, where the training set is gener-ated semi-automatically, bootstrapping from a smalltraining set.
They compare different machine learn-ing algorithms, providing local features as well asglobal semantic information derived from topic sig-nature and WordNet.
Person instances were classi-fied into one of eight categories.Cimiano and Vo?lker (2005) present an approachfor the fine-grained classification of entities relyingon the Harris?
distributional hypothesis and the vec-tor space model.
They assign a particular instanceto the most similar concept representing both withlexical-syntactic features extracted from the contextof the instance and the lexicalization of the concept,respectively.
Experiments were performed using alarge ontology with 682 concepts (unfortunately notyet available).Tanev and Magnini (2006) proposed a weakly-supervised method that requires as training data alist of named entities, without context, for each cat-egory under consideration.
Given a generic syntacti-cally parsed corpus containing at least each trainingentity twice, the algorithm learns, for each category,207a feature vector describing the contexts where thoseentities occur.
Then, it compares the new (unknown)entity with the so obtained feature vectors, assigningit to the most similar category.
Experiments are per-formed on a benchmark of 5 sub-classes of locationand 5 sub-classes of person.Giuliano and Gliozzo (2007) propose an unsuper-vised approach based on lexical entailment, consist-ing in assigning an entity to the category whose lex-icalization can be replaced with its occurrences ina corpus preserving the meaning.
Using unsuper-vised learning, they obtained slightly worst resultsthan Tanev and Magnini (2006) on the same bench-mark.Picca et al (2007) present an approach for on-tology learning from open domain text collections,based on the combination of Super Sense Taggingand Domain Modeling techniques.
The system rec-ognizes terms pertinent to the domain and assignsthem the correct ontological type.Giuliano and Gliozzo (2008) present an instance-based learning algorithm for fine-grained named en-tity classification based on syntactic features (word-order, case-marking, agreement, verb tenses, etc.
).Their method can handle much finer distinctionsthan previous methods, and it is evaluated on a hi-erarchical taxonomy of 21 ancestors of people thatwas induced from WordNet.
One contribution is tocreate this richer People Ontology.
Another is tomake effective use of the Web 1T 5-gram corpus(Brants and Franz, 2006) to represent syntactic in-formation.
The main difference between the two ap-proaches lies primarily in the use of syntactic andsemantic information.
Our experiments show thatsemantic features do provide richer information thansyntactic ones for a more fine-grained classificationof named entities.
In fact, the accuracy improve-ment achieved by our approach is more evident forthe more specific classes.
For example, the improve-ment in accuracy is about 14% for the class scientist,while it ranges from 25% to 46% for its sub-classes(physicist, mathematician, etc.
).Kozareva et al (2008) propose an approach forperson name categorization based on the domaindistribution.
They use the information provided byWordNet Domains to generated lists of words rele-vant for a given domain, by mapping and ranking thewords from the WordNet glosses to their WordNetDomains.
A named entity is then classified accord-ing the similarity between the word-domain lists andthe global context in which the entity appears.
How-ever, the evaluation was performed only on 6 personnames using two categories.Ganti et al (2008) present a method that considersan entity?s context across multiple documents con-taining it, and exploiting word n-grams and existinglarge list of related entities as features.
They gener-ated training and test data using Wikipedia articlesthat contain list of instances.
They compare theirsystem with a single-context classifier, showing thattheir approach based on aggregate context performbetter.Finally, Talukdar et al (2008) propose a graph-based semi-supervised label propagation algorithmfor acquiring open-domain labeled classes and theirinstances from a combination of unstructured andstructured text.6 ConclusionsWe presented an approach to automatic fine-grainedcategorization of named entities based on kernelmethods.
Entities are represented by aggregating allcontexts in which they occur.
We employed latentsemantic kernels to extend the bag-of-words repre-sentation.
The latent semantic models were derivedfrom Wikipedia and a news corpus We evaluated ourapproach on the People Ontology, a multi-level on-tology of people derived from WordNet.
Althoughthis benchmark is still far from being ?large?, it al-lows drawing more valid conclusions than past ones.We significantly outperformed the previous resultson both coarse- and fine-grained classification, al-though requiring much less training instances.
Fromthis preliminary analysis, it appears that semantic in-formation is much more effective that syntactic onefor this task, and deriving the semantic model fromWikipedia gives no significant improvement, as wellas, using a larger number of Wikipedia articles.AcknowledgmentsClaudio Giuliano is supported by the X-Media project (http://www.x-media-project.org), sponsored by the Euro-pean Commission as part of the Information Society Technolo-gies (IST) programme under EC grant number IST-FP6-026978and the ITCH project (http://itch.fbk.eu), sponsoredby the Italian Ministry of University and Research and by theAutonomous Province of Trento.208ReferencesJavier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007.The semeval-2007 weps evaluation: Establishing abenchmark for the web people search task.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 64?69,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009.Weps 2 evaluation campaign: overview of the webpeople search clustering task.
In 2nd Web Peo-ple Search Evaluation Workshop (WePS 2009), 18thWWW Conference, Madrid, Spain, April.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram corpus version 1, Linguistic Data Consortium,Philadelphia.Philipp Cimiano and Johanna Vo?lker.
2005.
Towardslarge-scale, open-domain and ontology-based namedentity classification.
In Proceedings of RANLP?05,pages 66?
166?172, Borovets, Bulgaria.Scott C. Deerwester, Susan T. Dumais, Thoms K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proceed-ings of the 19th International Conference on Compu-tational Linguistics, Taipei, Taiwan.Aldo Gangemi, Roberto Navigli, and Paola Velardi.2003.
Axiomatizing WordNet glosses in the On-toWordNet project.
In Proocedings of the Workshopon Human Language Technology for the SemanticWeb and Web Services at ISWC 2003, Sanibel Island,Florida.Venkatesh Ganti, Arnd C. Ko?nig, and Rares Vernica.2008.
Entity categorization over large document col-lections.
In KDD ?08: Proceeding of the 14th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 274?282, New York,NY, USA.
ACM.Alfio Giozzo and Carlo Strapparava.
2005.
Domain ker-nels for text categorization.
In Ninth Conference onComputational Natural Language Learning (CoNLL-2005), pages 56?63, Ann Arbor, Michigan, June.Claudio Giuliano and Alfio Gliozzo.
2007.
Instancebased lexical entailment for ontology population.
InProceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 248?256.Claudio Giuliano and Alfio Gliozzo.
2008.
Instance-based ontology population exploiting named-entitysubstitution.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 265?272, Manchester, UK, August.Alfio Massimiliano Gliozzo, Claudio Giuliano, and CarloStrapparava.
2005.
Domain kernels for word sensedisambiguation.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 403?410, Ann Arbor, Michigan,June.Zornitsa Kozareva, Sonia Vazquez, and Andres Montoyo.2008.
Domain information for fine-grained personname categorization.
In 9th International Conferenceon Intelligent Text Processing and Computational Lin-guistics (CICLing 2008), pages 311?321, Haifa, Israel,17-23 February.Xin Li and Dan Roth.
2005.
Learning question classi-fiers: the role of semantic information.
Natural Lan-guage Engineering, 12(3):229?249.I.
Dan Melamed and Philip Resnik.
2000.
Tagger eval-uation given hierarchical tag sets.
Computers and theHumanities, pages 79?84.Davide Picca, Alfio Gliozzo, and Massimiliano Cia-ramita.
2007.
Semantic domains and supersense tag-ging for domain-specific ontology learning.
In DavidEvans, Sadaoki Furui, and Chantal Soule?-Dupuy, edi-tors, Recherche d?Information Assiste?e par Ordinateur(RIAO), Pittsburgh, PA, USA, May.B.
Scho?lkopf and A. Smola.
2002.
Learning with Ker-nels.
MIT Press, Cambridge, Massachusetts.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
YAGO: A large ontology fromwikipedia and wordnet.
Elsevier Journal of Web Se-mantics.Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,Deepak Ravichandran, Rahul Bhagat, and FernandoPereira.
2008.
Weakly-supervised acquisition of la-beled class instances using graph random walks.
InProceedings of the conference on Empirical Methodsin Natural Language Processing (EMNLP), Waikiki,Honolulu, Hawaii, October 25-27.Hristo Tanev and Bernardo Magnini.
2006.
Weakly su-pervised approaches for ontology population.
In Pro-ceedings of the Eleventh Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-2006), Trento, Italy.209
