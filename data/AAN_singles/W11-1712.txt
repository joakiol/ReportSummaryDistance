Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 96?103,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsFeature Selection for Sentiment AnalysisBased on Content and Syntax ModelsAdnan Duric and Fei SongSchool of Computer Science, University of Guelph, 50 Stone Road East,Guelph, Ontario, N1G 2W1, Canada{aduric,fsong}@uoguelph.caAbstractRecent solutions for sentiment analysis haverelied on feature selection methods rangingfrom lexicon-based approaches where the setof features are generated by humans, to ap-proaches that use general statistical measureswhere features are selected solely on empiri-cal evidence.
The advantage of statistical ap-proaches is that they are fully automatic, how-ever, they often fail to separate features thatcarry sentiment from those that do not.
In thispaper we propose a set of new feature selec-tion schemes that use a Content and Syntaxmodel to automatically learn a set of featuresin a review document by separating the enti-ties that are being reviewed from the subjec-tive expressions that describe those entities interms of polarities.
By focusing only on thesubjective expressions and ignoring the enti-ties, we can choose more salient features fordocument-level sentiment analysis.
The re-sults obtained from using these features in amaximum entropy classifier are competitivewith the state-of-the-art machine learning ap-proaches.1 IntroductionAs user generated data become more commonplace,we seek to find better approaches to extract and clas-sify relevant content automatically.
This gives usersa richer, more informative, and more appropriate setof information in an efficient and organized manner.One way for organizing such data is text classifica-tion, which involves mapping documents into topi-cal categories based on the occurrences of particularfeatures.
Sentiment Analysis (SA) can be framed asa text classification task where the categories are po-larities such as positive and negative.
However, thesimilarities end here.
Whereas general text classi-fication is concerned with features that distinguishdifferent topics, sentiment analysis deals with fea-tures about subjectivity, affect, emotion, and points-of-view that describe or modify the related entities.Since user-generated review documents contain bothkinds of features, SA solutions ultimately face thechallenge of separating the factual content from thesubjective content describing it.For example, taking a segment from a randomlychosen document in Pang et al?s movie review cor-pus1, we see how entities and modifiers are relatedto each other:... Of course, it helps that Kaye has anactor as talented as Norton to play thispart.
It?s astonishing how frighteningNorton looks with a shaved head and aswastika on his chest.
... Visually, the filmis very powerful.
Kaye indulges in a lot ofinteresting artistic choices, and most ofthem work nicely.Indeed, most of the information about an entitythat relates it to a particular polarity comes from themodifying words.
In the example above, these wordsare adjectives such as talented, frightening, interest-ing, and powerful.
They can also be verbs such aswork and adverbs such as nicely.
The entities are1http://www.cs.cornell.edu/people/pabo/movie-review-data/96represented by various nouns and pronouns such as:Kaye, Norton, actor and them.Therefore, the task of classifying a review doc-ument can be explored by taking into account amixture of entities and their modifiers.
An impor-tant characteristic of review documents is that thereviewers tend to discuss the whole set of entitiesthroughout the entire document, whereas the modi-fiers for those entities tend to be more localized atthe sentence or phrase level.
In other words, eachentity can be polymorphous within the document,with a long-range semantic relationship between itsforms while the modifiers in each case are boundto the entity in a short-range, syntactic relationship.Generalizing a single entity to all the entities that arefound in a document, and taking all their respectivemodifiers into account, we can start to infer the po-larity of the entire document based on the set of allthe modifiers.
This reduces to finding all the syn-tactic words in the document and disregarding theentities.Taking another look at the example modifiers, wemight assume that all of the relevant indicators forSA come from specific parts of speech categoriessuch as adjectives and adverbs, while other partsof speech classes such as nouns are more relevantfor general text classification, and can be discarded.However, as demonstrated by Pang et al (2002),Pang and Lee (2004), Hu and Liu (2004), and Riloffet al (2003), there are some nouns and verbs thatare useful sentiment indicators as well.
Therefore,a clear distinction cannot be made along parts ofspeech categories.To address this issue, we propose a feature selec-tion scheme in which we can obtain important senti-ment indicators that:1.
Do not rely on specific parts of speech classeswhile maintaining the focus on syntax words.2.
Separate semantic words that do not indicatesentiment while keeping nouns that do.3.
Reflect the domain for the set of documents.By using feature selection schemes that focus onthe outlined sentiment indicators as a basis for ourmachine learning approach, we should achieve com-petitive accuracy results when classifying documentpolarities.The rest of this paper is organized as follows.
Sec-tion 2 discusses some important work and resultsfor SA and outlines the modelling and classificationtechniques used by our approach.
Section 3 providesdetails about our feature selection methods.
Our ex-periments and analyses are given in section 4, andconclusions and future directions are presented insection 5.2 Related Work2.1 Feature Selection in Sentiment AnalysisThe majority of the approaches for SA involve atwo-step process:1.
Identify the parts of the document that willlikely contribute to positive or negative senti-ments.2.
Combine these parts of the document in waysthat increase the odds of the document fallinginto one of these two polar categories.The simplest approach for (1) by Pang et al(2002) is to use the most frequently-occurring wordsin the corpus as polarity indicators.
This approachis commonly used with general text classification,and the results achieved indicate that simple docu-ment frequency cutoffs can be an effective featureselection scheme.
However, this scheme picks upon many entity words that do not contain any sub-jectivity.The most common approach, used by researcherssuch as Das and Chen (2007), starts with a manu-ally created lexicon specific to their particular do-main whereas others (Hurst and Nigam, 2004; Yi etal., 2003) attempt to craft a general-purpose opin-ion lexicon that can be used across domains.
Morerecent lexicon-based approaches (Ding et al, 2008;Hu and Liu, 2004; Kim and Hovy, 2004; Riloff etal., 2003) begin with a small set of ?seed?
wordsand bootstrap this set through synonym detectionor various on-line resources to obtain a larger lex-icon.
However, lexicon-based approaches have sev-eral key difficulties.
First, they take time to com-pile.
Whitelaw et al (2005) report that their featureselection process took 20 person-hours, since it in-volves work done by human annotators.
In separatequalitative experiments done by Pang et al (2002),97Wilson et al (2005) and Kim and Hovy (2004), theagreement between human judges when given a listof sentiment-bearing words is as low as 58% and nohigher than 76%.
In addition, some words may notbe frequent enough for a classification algorithm.2.2 Topic Modelling and HMM-LDATopic models such as Latent Dirichlet Allocation(LDA) are generative models that allow documentsto be explained by a set of unobserved (latent) top-ics.
Hidden Markov Model LDA (HMM-LDA)(Griffiths et al, 2005) is a topic model that simul-taneously models topics and syntactic structure in acollection of documents.
The idea behind the modelis that a typical word can play different roles.
It caneither be part of the content and serve in a seman-tic (topical) purpose or it can be used as part of thegrammatical (syntactic) structure.
It can also be usedin both contexts.
HMM-LDA models this behaviorby inducing syntactic classes for each word basedon how they appear together in a sentence using aHidden Markov Model.
Each word gets assigned toa syntactic class, but one class is reserved for the se-mantic words.
Words in this class behave as theywould in a regular LDA topic model, participatingin different topics and having certain probabilities ofappearing in a document.
More formally, the modelis defined in terms of three sets of variables and agenerative process.
Let w = {w1, ..., wn} be a se-quence of words where each word wi is one of Vwords; z = {z1, ..., zn}, a sequence of topic as-signments where each zi is one of K topics; andc = {c1, ..., cn}, a sequence of class assignmentswhere each ci is one of C classes.
One class, ci = 1is designated as the ?semantic class?, and the rest,the ?syntactic?
classes.Since we are dealing with a Hidden MarkovModel, we require a variable representing the tran-sition probabilities between the classes, given by aC ?
C transition matrix ?
that models transitionsbetween classes ci?1 and ci.
The generative processis described as follows:1.
Sample ?
(d) from a Dirichlet prior Dir(?)2.
For each word wi in document d:(a) Draw zi ?
?
(d)(b) Draw ci ?
?
(ci?1)(c) If ci = 1, then draw wi ?
?
(zi), else drawwi ?
?
(ci)where ?
(zi) ?
Dir(?)
and ?
(ci) ?
Dir(?
), bothfrom Dirichlet distributions.2.3 Text Classification Based on MaximumEntropy ModellingMaximum Entropy Modelling (Manning andSchu?tze, 1999) is a framework whereby the featuresrepresent constraints on the overall model and theidea is to incorporate the knowledge that we havewhile preserving as much uncertainty as possibleabout the knowledge we do not have.
The featuresfi are binary functions where there is a vector xrepresenting input elements (unigram features in ourcase) and c, the class label for one of the possiblecategories.
More specifically, a feature function isdefined as follows:fi,c?
(x, c) ={1 if x contains wi and c = c?0 otherwise(2.1)where word wi and category c?
correspond to a spe-cific feature.Employing the feature functions described above,a Maximum Entropy model takes the followingform:P (x, c) = 1ZK?i=1?fi(x,c)i (2.2)where K is the number of features, ?i is the weightfor feature fi, and Z is a normalizing constant.
Bytaking the logarithm on both sides, we get the log-linear model:logP (x, c) = ?
logZ +K?i=1fi(x, c) log?i (2.3)To classify a document, we compute P (c|x) sothat the c with the highest probability will be the cat-egory for the given document.983 Feature Selection (FS) Based onHMM-LDA3.1 Characteristics of Salient FeaturesTo motivate our approach, we first describe criteriathat are useful in selecting salient features for SA:1.
Features should be expressive enough to adduseful information to the classification process.As discussed in section 1, the most expressivefeatures in terms of polarity are the modifyingwords that describe an entity in a certain way.These are usually, but not restricted to, adjec-tives, adverbs, subjective verbs and nouns.2.
All features together should form a broad andcomprehensive viewpoint of the entire corpus.In a corpus of many documents, some featurescan represent a subset of the corpus very accu-rately, while other features may represent an-other subset of the corpus.
The problem ariseswhen representing the whole corpus with a spe-cific feature set (Sebastiani, 2002).3.
Features should be as domain-dependent aspossible.
Examples from Hurst and Nigam(2004) and Das and Chen (2007) as well asmany other approaches indicate that SA is adomain-dependant task, and the final featuresshould reflect the domain of the corpus thatthey are representing.4.
Features must be frequent enough.
Rare fea-tures do not occur in many documents andmake it difficult to train a machine learning al-gorithm.
Experiments by Pang et al (2002) in-dicate that having more features does not helplearning, and the best accuracy was achievedby selecting features based on document fre-quency.5.
Features should be discriminative enough.
Alearning system needs to be able to pick up ontheir presence in certain documents for one out-come and absence in other documents for an-other outcome in classification.3.2 FS Based on Syntactic ClassesOur proposed FS scheme is to utilize HMM-LDAto obtain words that, for the most part, follow thecriteria we set out in subsection 3.1.
We train anHMM-LDA model to give us the syntactic classesthat we further combine to form our final features.Let word wi ?
V where V is the vocabulary.
Alsolet cj ?
C be a class.
We define Pcj (wi) as the prob-ability of word wi in class cj , and one class, cj = 1indicates the semantic class.
Since each class (syn-tactic and semantic) has a probability distributionover all words, we need to select words that offera good representation of the class.
The representa-tive words in each class have a much higher proba-bility than the other words.
Therefore, we can selectthe representative words by the cumulative probabil-ity.
Specifically, we select the top percentage of thewords in a class whereby the sum of their probabil-ities will be within some pre-defined range.
This isnecessary since there are many words in each classwith low probabilities in which we are not interested(Steyvers and Griffiths, 2006).
The cumulative dis-tribution function is defined as:Fj(wi) =?Pcj (w)?Pcj (wi)Pcj (w) (3.1)Then, we can define the set of words in class cj as:Wcj = {wi|Fj(wi) ?
?}
(3.2)where ?
is a pre-defined threshold such that 0 ?
?
?1.
Next, we define the set of words in all the syntac-tic classes Wsyn as:Wsyn = {wi|wi ?
Wcj and cj 6= 1} (3.3)and the set of words in the semantic class Wsem as:Wsem = {wi|wi ?
Wcj and cj = 1} (3.4)Since modifying words for sentiment typicallyfall into syntactic classes, we could use words inWsyn as features for SA.
However, as observed byPang et al (2002), the best classification perfor-mance is achieved by a subset of features (typicallyaround 2500).
As a general step, we can apply adocument frequency (DF) cutoff to select the mostfrequent features.
Let df(wi) denote the documentfrequency of word wi, indicating the number of doc-uments in which wi occurs in the corpus.
Then the99resulting features selected based on df can be de-fined as:cut(Wsyn, ?)
= {wi|wi ?
Wsyn and df(wi) ?
?
}(3.5)where ?
is the minimum document frequency re-quired for feature selection.3.3 FS Based on Set Difference betweenSyntactic and Semantic ClassesThe main characteristic of using HMM-LDA classesfor feature selection is that the set of words in thesyntactic classes and the set of words in the semanticclass are not disjoint.
In fact, there is quite a largeoverlap.
In this and the next subsections, we dis-cuss ways to remedy and even exploit this situationto get a higher level of accuracy.
In the Pang et almovie review data, there is about 35% overlap be-tween words in the syntactic and semantic classesfor ?
= 0.9.
Our first systematic approach attemptsto gain better accuracy by lowering the ratio of se-mantic words in the final feature set.More formally, given the set of syntactic wordsWsyn, we can reduce the overlap with Wsem by do-ing a set difference operation:Wsyn ?Wsem (3.6)This will give us all the words that are morefavoured in the syntactic classes.
However, as weshall see shortly, and also as we earlier speculated,by subtracting all the words in the semantic class, weare actually getting rid of some useful features.
Thisis because (a) it is possible for the semantic classto contain words that are syntactic, and as a resultare useful, and (b) there exist some semantic wordsthat are good indicators of polarity.
Therefore, weseek to ?lessen?
the influence of the semantic classby cutting only a certain portion of it out, but not allof them.For the above scheme, we outline Algorithm 1that enables us to select features from Wsyn by ap-plying a percentage cutoff for Wsem and then doinga set difference operation.
We define top(Wsem, ?
)to be the ?% of the words with top probabilities inWsem.Note that when ?
= 1.0, we get the same result asWsyn ?
Wsem.
In our experiments, we try a rangeof ?
values for SA.Algorithm 1 Syntactic-Semantic Set DifferenceRequire: Wsyn and Wsem as input1: W ?sem = top(Wsem, ?
)2: Wdiff = Wsyn ?W ?sem3: W ?syn = cut(Wdiff , ?
)3.4 FS Based on Max Scores of SyntacticFeaturesThe running theme through the HMM-LDA featureselection schemes is that if a word is highly ranked(has a high probability of occurring) in a syntacticclass, we should use that word in our feature set.Moreover, if a word is highly ranked in the seman-tic class, we usually do not want to use that wordin our feature set because the word usually indicatesa frequent noun.
Therefore, the desirable words arethose that occur with high probability in the syntac-tic classes, but do not occur with high probability inthe semantic class, or do not occur there at all.To this end, we have formulated a scheme thatadds such words to our feature set.
For each word,we obtain its highest probability in the set of syn-tactic classes.
Comparing this probability with theprobability of the same word in the semantic class,we disregard the word if the probability in the se-mantic class is greater.We define the max scores for word wi for both thesyntactic and semantic classes and describe how weselect features based on the max scores in Algorithm2.Algorithm 2 Max Scores of Syntactic FeaturesRequire: cj ?
C where 1 ?
j ?
|C|1: for all wi ?
V do2: Ssyn(wi) = maxcj 6=1Pcj (wi)3: Ssem(wi) = Pc1(wi)4: Wmax = {wi|Ssyn(wi) > Ssem(wi)}5: end for6: W ?syn = cut(Wmax, ?
)4 ExperimentsThis section describes the steps taken to gener-ate some experimental results for each scheme de-scribed in the previous section.
Before we can an-alyze these sets of results, we take a look at some100baselines.4.1 EvaluationWe use the corpus of 2000 movie reviews (Pang andLee, 2004) that consists of 1000 positive and 1000negative documents selected from on-line forums.In our experiments, we randomize the documentsand split the data into 1800 for training / testing pur-poses and 200 as the validation set.
For the 1800documents, we run a 3-fold cross validation proce-dure where we train on 1200 documents and test on600.
We compare the resultant feature sets after eachFS scheme using the OpenNLP2 Maximum Entropyclassifier.Throughout these experiments, we are interestedin the classification accuracy.
This is evaluatedsimply by comparing the resultant class from theclassifier and the actual class annotated by Pangand Lee (2004).
The number of matches is di-vided by the number of documents in the testset.
Thus, given an annotated test set dtestA ={(d1, o1), (d2, o2), .
.
.
(dS , oS)} and the classifiedset, dtestB = {(d1, q1), (d2, q2), .
.
.
(dS , qS)}, wecalculate the accuracy as follows:?Si=1 I(oi = qi)S (4.1)where I(?)
is the indicator function.4.2 Baseline ResultsAfter replicating the results from Pang et al (2002),we varied the number of iterations per fold by usinga held-out validation set ?eval?.
The higher accu-racy achieved suggests that the model was not fullytrained after 10 iterations.In order to compare with our HMM-LDA basedschemes, we ran experiments to explore a basicPOS-based feature selection scheme.
In this ap-proach, we first tagged the words in each documentwith POS tags and selected the most frequently-occurring unigrams that were not tagged as ?NN?,?NNP?, ?NNS?
or ?NNPS?
(the ?noun?
categories).This corresponds to POS (-NN*) in Table 1.
Next,we tagged all the words and only selected the wordsthat were tagged as ?JJ*?, ?RB*?, and ?VB*?
cate-gories (the ?syntactic?
categories).
The idea is to2http://incubator.apache.org/opennlp/include as part of the feature set al the words thatare not ?semantically oriented?.
This corresponds toPOS (JJ* + RB* + VB*) in Table 1.Iterations DFcutoffPOS(-NN*)POS(JJ*+RB*+VB*)10 0.821 0.827 0.81125 0.836 0.831 0.824eval 0.845 0.848 0.826Table 1: Baseline results with a different number of iter-ations.
Each column represents a different feature selec-tion method.4.3 HMM-LDA TrainingOur feature selection methods involve training anHMM-LDA model on the Pang et al corpus ofmovie reviews, taking the class assignments, andcombining the resultant unigrams to create featuresfor the MaxEnt classifier.
Since HMM-LDA is anunsupervised topic model, we can train it on the en-tire corpus.
We trained the model using the TopicModelling Toolbox3 MATLAB package on the 2000movie reviews.
Since the HMM-LDA model re-quires sentences to be outlined, we used the usualend-of-sentence markers (?.
?, ?!
?, ??
?, ?:?).
The train-ing parameters are T = 50 topics, S = 20 classes, AL-PHA = 1.0, BETA = 0.01, and GAMMA = 0.1.
Wefound that 1000 iterations is sufficient as we trackedthe log-likelihood of every 10 iterations.After training, we have both the topic assignmentsz and the class assignments c for each word in eachof the samples.4.4 Selecting Features Based on SyntacticClassesIn this experiment we fix ?
= 0.9 to get the topwords in each class having a cumulative probabil-ity under 0.9.
These are the representative wordsin each class which we merge into Wsyn.
Finally,we select 2500 words by the df cutoff method.
Thislist of words is then used as features for the Max-Ent classifier.
We run the classifier for 10, 25 and?eval?
number of iterations in order to compare withthe baseline results.3http://psiexp.ss.uci.edu/research/programs data/toolbox.htm101Iterations FS Based onSyntactic Features10 0.82325 0.839eval 0.863Table 2: Results for FS Based on Syntactic Classes at 10,25 and ?eval?
iterations.At ?
= 0.9, there are 6,189 words in Wsyn beforewe select the top 2500 using the df cutoff.
FromTable 2, we see that the accuracy has increased from0.845 to 0.863 at the ?eval?
number iterations.In all of our experiments, we use df cutoff toget a manageable number of features for the clas-sifier.
This is partly based on Pang et al (2002)and partly based on calculating the Pearson correla-tion for each class between the document frequencyand word probability at ?
= 0.9.
Since every classhas a positive correlation in the range of [0.313938,0.888160] where the average is 0.576, we can saythat there is a correlation between the two values.4.5 Selecting Features Based on Set DifferenceThe result for Set Difference is derived by varyingthe percentage of top semantic words that should beexcluded in the final feature set.
For example, somewords in Wsyn?Wsem that have a higher probabilityin Wsem are: ?hollywod?, ?war?, and ?fiction?
whilesome words that have a higher probability in Wsyninclude: ?good?, ?love?
and ?funny?.
The ?
value isdefined by the percentage of the words in Wsem thatwe exclude from Wsyn.
The results for 0.0 ?
?
?1.0 for increments of ?
?|Wsem|, are summarized inTable 3.?
FS Based onSet Difference?
FS Based onSet Difference0.0 0.861 0.5 0.8520.1 0.862 0.6 0.8460.2 0.865 0.7 0.8490.3 0.858 0.8 0.8470.4 0.857 0.9 0.8401.0 0.831Table 3: Results for FS Based on Syntactic-Semanticset difference method.
Each row represents the accuracyachieved at a particular ?
value.From the results, we can see that as we removemore and more words from Wsem, the accuracy leveldecreases.
This suggests that Wsem?Wsyn containssome important features and if we subtract Wsem en-tirely, we essentially eliminate them.
At each cutofflevel, we are eliminating 10% until we have elimi-nated the whole set.
Clearly, a more fine-grained ap-proach is needed, and that leads us to the Max-Scoreresults.4.6 Selecting Features Based on Max ScoresFor the method based on Max Scores, we may selectfeatures that are in both Wsem and Wsyn sets as longas their max scores in Wsyn are higher than those inWsem.Iterations FS Based onMax Scoreseval 0.875Table 4: Result for FS Based on Max Scores.Comparing the accuracy in Table 4 with thosein the previous subsections, we can say that usingthe fine-grained Max-Score algorithm improves theclassification accuracy.
This means that iterativelyremoving words that have a relatively higher prob-ability in Wsem compared to Wsyn does not elim-inate important words occurring in both sets, butlessens the influence of some high probability wordsin Wsem.4.7 Discussion of the ResultsFor our experiments, the best accuracy is achievedby utilizing the Max-Score algorithm (outlined insubsection 3.4) after a further selection of 2500 withthe df cutoff.
As discussed in subsection 3.4, theMax-Score algorithm enables us to select words thathave a higher score in Wsyn than in Wsem.
This ap-proach has the dual advantage of keeping the wordsthat are present in both Wsyn and Wsem but havehigher scores in Wsyn and ignoring the words thatare also present in both sets but have higher scoresin Wsem.
Ultimately, this decreases the influence ofthe frequent and overlapped words that have a highprobability in Wsem.Finally, to quantify the significance level of ourbest approach against the baseline methods in sub-102section 4.2, we calculated the p-values for the one-tailed t-tests comparing our best approach based onMax Scores with the DF and POS (-NN*) baselines,respectively.
The resulting p-values of 0.011 and0.014 suggest that our best approach is significantlybetter than the baseline approaches.5 Conclusions and Future DirectionsIn this paper, we have described a method for fea-ture selection based on long-range and short-rangedependencies given by the HMM-LDA topic model.By modelling review documents based on the com-binations of syntactic and semantic classes, we havedevised a method of separating the topical con-tent that describes the entities under review fromthe opinion context (given by sentiment modifiers)about that entity in each case.
By grouping all thesentiment modifiers for each entity in a document,we are selecting the features that are intuitively inline with the outlined characteristics of salient fea-tures for SA (see subsection 3.1).
This is backed upby our experiments where we achieve competitiveresults for document polarity classification.One avenue for future development of this frame-work could include identifying and extracting as-pects from a review document.
So far, we have notidentified aspects from the entities, choosing insteadto classify a document as a whole.
However, thisframework can be readily applied to extract relevant(most probable) aspects using the LDA topic modeland then restrict the syntactic modifiers to the rangeof sentences where an aspect occurs.
This wouldgive us an unsupervised aspect extraction schemethat we can combine with a classifier to predict po-larities for each aspect.ReferencesSanjiv R. Das and Mike Y. Chen.
2007.
Yahoo!
forAmazon: Sentiment extraction from small talk on theWeb.
Management Science, 53(9):1375?1388.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.
InProceedings of the Conference on Web Search and WebData Mining (WSDM).Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In In Advances in Neural Information Pro-cessing Systems 17, pages 537?544.
MIT Press.Minqing Hu and Bing Liu.
2004.
Mining opinion fea-tures in customer reviews.
In Proceedings of AAAI,pages 755?760.Matthew Hurst and Kamal Nigam.
2004.
Retrieving top-ical sentiments from online document collections.
InDocument Recognition and Retrieval XI, pages 27?34.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of the In-ternational Conference on Computational Linguistics(COLING).Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the As-sociation for Computational Linguistics (ACL), pages271?278.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 79?86.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of the Conference onNatural Language Learning (CoNLL), pages 25?32.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Computing Surveys,34(1):1?47.Mark Steyvers and Tom Griffiths.
2006.
Probabilistictopic models.
In T. Landauer, D. Mcnamara, S. Den-nis, and W. Kintsch, editors, Latent Semantic Analysis:A Road to Meaning.
Laurence Erlbaum.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analy-sis.
In Proceedings of the ACM SIGIR Conferenceon Information and Knowledge Management (CIKM),pages 625?631.
ACM.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the Human Lan-guage Technology Conference and the Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP), pages 347?354.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Ex-tracting sentiments about a given topic using naturallanguage processing techniques.
In Proceedings ofthe IEEE International Conference on Data Mining(ICDM).103
