Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 66?74,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsEvaluating Neighbor Rank and Distance Measuresas Predictors of Semantic PrimingGabriella LapesaUniversita?t Osnabru?ckInstitut fu?r KognitionswissenschaftAlbrechtstr.
28, 49069 Osnabru?ckglapesa@uos.deStefan EvertFAU Erlangen-Nu?rnbergProfessur fu?r KorpuslinguistikBismarckstr.
6, 91054 Erlangensevert@fau.deAbstractThis paper summarizes the results of alarge-scale evaluation study of bag-of-words distributional models on behavioraldata from three semantic priming experi-ments.
The tasks at issue are (i) identifi-cation of consistent primes based on theirsemantic relatedness to the target and (ii)correlation of semantic relatedness withlatency times.
We also provide an evalu-ation of the impact of specific model pa-rameters on the prediction of priming.
Tothe best of our knowledge, this is the firstsystematic evaluation of a wide range ofDSM parameters in all possible combina-tions.
An important result of the studyis that neighbor rank performs better thandistance measures in predicting semanticpriming.1 IntroductionLanguage production and understanding make ex-tensive and immediate use of world knowledgeinformation that concerns prototypical events.Plenty of experimental evidence has been gatheredto support this claim (see McRae and Matzuki,2009, for an overview).
Specifically, a number ofpriming studies have been conducted to demon-strate that event knowledge is responsible for fa-cilitation of processing of words that denote eventsand their participants (Ferretti et al 2001; McRaeet al 2005; Hare et al 2009).
The aim of our re-search is to investigate to which extent such eventknowledge surfaces in linguistic distribution andcan thus be captured by Distributional SemanticModels (henceforth, DSMs).
In particular, we testthe capabilities of bag-of-words DSMs in simu-lating priming data from the three aforementionedstudies.DSMs have already proven successful in sim-ulating priming effects (Pado?
and Lapata, 2007;Herdag?delen et al 2009; McDonald and Brew,2004).
Therefore, in this work, we aim at a morespecific contribution to the study of distributionalmodeling of priming: to identify the indexes ofdistributional relatedness that produce the bestperformance in simulating priming data and to as-sess the impact of specific model parameters onsuch performance.
In addition to distance in thesemantic space, traditionally used as an index ofdistributional relatedness in DSMs, we also intro-duce neighbor rank as a predictor of priming ef-fects.
Distance and a number of rank-based mea-sures are compared with respect to their perfor-mance in two tasks: the identification of congruentprimes on the basis of distributional relatednessto the targets (we measure accuracy in picking upthe congruent prime) and the prediction of latencytimes (we measure correlation between distribu-tional relatedness and reaction times).
The resultsof our experiments show that neighbor rank is abetter predictor than distance for priming data.Our approach to DSM evaluation constitutesa methodological contribution of this study: weuse linear models with performance (accuracy orcorrelation) as a dependent variable and variousmodel parameters as independent variables, in-stead of looking for optimal parameter combina-tions.
This approach is robust to overfitting andallows to analyze the influence of individual pa-rameters as well as their interactions.The paper is structured as follows.
Section2 provides an overview of the modeled datasets.Section 3 introduces model parameters and in-dexes of distributional relatedness evaluated in thispaper, describes the experimental tasks and out-lines our statistical approach to DSM evaluation.Section 4 presents results for the accuracy and cor-relation tasks and evaluates the impact of modelparameters on performance.
We conclude in sec-tion 5 by sketching ongoing work and future de-velopments of our research.66Dataset Relation N Primec Primei Target FacV-NAGENT 28 Pay Govern Customer 27*PATIENT 18 Invite Arrest Guest 32*PATIENT FEATURE 20 Comfort Hire Upset 33*INSTRUMENT 26 Cut Dust Rag 32*LOCATION 24 Confess Dance Court - 5N-VAGENT 30 Reporter Carpenter Interview 18*PATIENT 30 Bottle Ball Recycle 22*INSTRUMENT 32 Chainsaw Detergent Cut 16*LOCATION 24 Beach Pub Tan 18*N-NEVENT-PEOPLE 18 Trial War Judge 32*EVENT-THING 26 War Gun Banquet 33*LOCATION-LIVING 24 Church Gym Athlete 37*LOCATION-THING 30 Pool Garage Car 29*PEOPLE-INSTRUMENT 24 Hiker Barber Compass 45*INSTRUMENT-PEOPLE 24 Razor Compass Barber -10INSTRUMENT-THING 24 Hair Scissors Oven 58*Table 1: Overview of datasets: thematic relations, number of triples, example stimuli, facilitation effects2 DataThis section introduces the priming datasets whichare the object of the present study.
All the experi-ments we aim to model were conducted to provideevidence for the immediate effect of event knowl-edge in language processing.The first dataset comes from Ferretti et al(2001), who found that verbs facilitate the process-ing of nouns denoting prototypical participants inthe depicted event and of adjectives denoting fea-tures of prototypical participants.
In what follows,the dataset from this study will be referred to asV-N dataset.The second dataset comes from McRae et al(2005).
In this experiment, nouns were found tofacilitate the processing of verbs denoting eventsin which they are prototypical participants.
In thispaper, this dataset is referred to as N-V dataset.The third dataset comes from Hare et al(2009),who found a facilitation effect from nouns tonouns denoting events or their participants.
Wewill refer to this dataset as N-N dataset.Experimental items and behavioral data fromthese three experiments have been pooled togetherin a global dataset that contains 404 word triples(Target, Congruent Prime, Incongruent Prime).For every triple, the dataset contains mean reac-tion times for the congruent and incongruent con-ditions, and a label for the thematic relation in-volved.
Table 1 provides a summary of the exper-imental data.
It specifies the number of triples forevery relation in the datasets (N) and gives an ex-ample triple (Primecongruent , Primeincongruent , Tar-get).
Facilitation effects and stars marking signif-icance by participants and items reported in theoriginal studies are also specified for every rela-tion (Fac).
Relations for which the experimentsshowed no priming effect are highlighted in bold.3 Method3.1 ModelsBuilding on the Distributional Hypothesis (Har-ris, 1954), DSMs are employed to produce seman-tic representations of words from patterns of co-occurrence in texts or documents (Sahlgren, 2006;Turney and Pantel, 2010).
Semantic representa-tions in the form of distributional vectors are com-pared to quantify the amount of shared contexts asan empirical correlate of semantic similarity.
Forthe purposes of this study, similarity is understoodin terms of topical relatedness (words connectedto a particular situation) rather than attributionalsimilarity (synonyms and near-synonyms).DSMs evaluated in this study belong to the classof bag-of-words models: the distributional vectorof a target word consists of co-occurrence countswith other words, resulting in a word-word co-occurrence matrix.
The models cover a large vo-cabulary of target words (27668 words in the un-tagged version; 31713 words in the part-of-speechtagged version).
It contains the stimuli from thedatasets described in section 2 and further targetwords from state-of-the-art evaluation studies (Ba-roni and Lenci, 2010; Baroni and Lenci, 2011;Mitchell and Lapata, 2008).
Contexts are fil-tered by part-of-speech (nouns, verbs, adjectives,and adverbs) and by frequency thresholds.
Nei-ther syntax nor word order were taken into ac-count when gathering co-occurrence information.Distributional models were built using the UCS67toolkit1 and the wordspace package for R2.
Theevaluated parameters are:?
Corpus: British National Corpus3; ukWaC4;WaCkypedia EN5; WP5006; and a concate-nation of BNC, ukWaC, and WaCkype-dia EN (called the joint corpus);?
Window size: 2, 5, or 15 words to the leftand to the right of the target;?
Part of speech: no part of speech tags; partof speech tags for targets; part of speech tagsfor targets and contexts;?
Scoring measure: frequency; Dice coeffi-cient; simple log-likelihood; Mutual Infor-mation; t-score; z-score;7?
Vector transformation: no transformation;square root, sigmoid or logarithmic transfor-mation;?
Dimensionality reduction: no dimension-ality reduction; Singular Value Decompo-sition to 300 dimensions using randomizedSVD (Halko et al 2009); Random Indexing(Sahlgren, 2005) to 1000 dimensions;?
Distance measure: cosine, euclidean ormanhattan distance.3.2 Indexes of Distributional Relatedess3.2.1 Distance and RankThe indexes of distributional relatedness describedin this section represent alternative perspectiveson the semantic representation inferred by DSMsfrom co-occurrence data.Given a target, a prime, and a matrix of dis-tances produced by a distributional model, we testthe following indexes of relatedness between tar-get and prime:?
Distance: distance between the vectors oftarget and prime in the semantic space;?
Backward association: rank of primeamong the neighbors of target, as in Hare etal.
(2009);8?
Forward association: rank of target in theneighbors of prime;1http://www.collocations.de/software.html2http://r-forge.r-project.org/projects/wordspace/3http://www.natcorp.ox.ac.uk/4http://wacky.sslmit.unibo.it/doku.php?id=corpora5http://wacky.sslmit.unibo.it/doku.php?id=corpora6A subset of WaCkypedia EN containing the initial 500words of each article, which amounts to 230 million tokens.7See Evert (2004) for a description of these measures anddetails on the calculation of association scores.8This type of association is labeled as ?backward?
be-cause it goes from targets to primes, while in the experimentalsetting targets are shown after primes.?
Average rank: average of backward and for-ward association.Indexes of distributional relatedness were consid-ered as an additional parameter in the evaluation,labeled relatedness index below.
Every combi-nation of the parameters described in section 3.1with each value of the relatedness index param-eter defines a DSM.
The total number of modelsevaluated in our study amounts to 38880.3.2.2 Motivation for RankThis section provides some motivation for the useof neighbor rank as a predictor of priming effectsin DSMs, on the basis of general cognitive princi-ples and of previous modeling experiments.In distributional semantic modeling, similar-ity between words is calculated according to Eu-clidean geometry: the more similar two words are,the closer they are in the semantic space.
One ofthe axioms of spatial models is symmetry (Tver-sky, 1977): the distance between point a and pointb is equal to the distance between point b and pointa.
Cognitive processes, however, often violate thesymmetry axiom.
For example, asymmetric asso-ciations are often found in word association norms(Griffiths et al 2007).Our study also contains a case of asymmetry.In particular, the results from Hare et al(2009),which constitute our N-N dataset, show primingfrom PEOPLE to INSTRUMENTs, but not from IN-STRUMENTs to PEOPLE.
This asymmetry can-not be captured by distance measures for reasonsstated above.
However, the use of rank-based in-dexes allows to overcome the limitation of sym-metrical distance measures by introducing direc-tionality (in our case, target?
prime vs. prime?target), and this without discarding the establishedand proven measures.Rank has already proven successful in model-ing priming effects with DSMs.
Hare et al(2009)conducted a simulation on the N-N dataset usingLSA (Landauer and Dumais, 1997) and BEAGLE(Jones and Mewhort, 2007) trained on the TASAcorpus.
Asymmetric priming was correctly pre-dicted by the context-only version of BEAGLE us-ing rank (namely, rank of prime among neighborsof target, cf.
backward rank in section 3.2.1).Our study extends the approach of Hare et al(2009) in a number of directions.
First, we in-troduce and evaluate several different rank-basedmeasures (section 3.2.1).
Second, we evaluaterank in connection with specific parameters and on68larger corpora.
Third, we extend the use of rank-based measures to the distributional simulation oftwo other experiments on event knowledge (Fer-retti et al 2001; McRae et al 2005).
Note thatour simulation differs from the one by Hare et al(2009) with respect to tasks (they test for a sig-nificant difference of mean distances between tar-get and related vs. unrelated prime) and the classof DSMs (we use term-term models, rather thanLSA; our models are not sensitive to word order,unlike BEAGLE).3.3 Tasks and Analysis of ResultsThe aim of this section is to introduce the exper-imental tasks whose results will be discussed insection 4 and to describe the main features of theanalysis we applied to interpret these results.Two experiments have been carried out:?
A classification experiment: given a targetand two primes, distributional information isused to identify the congruent prime.
Perfor-mance in this task is measured by classifica-tion accuracy (section 4.1).?
A prediction experiment: the informa-tion concerning distributional relatedness be-tween targets and congruent primes is testedas a predictor for latency times.
Performancein this task is quantified by Pearson correla-tion (section 4.2).Concerning the interpretation of the evaluation re-sults, it would hardly be meaningful to look at thebest parameter combination or the average acrossall models.
The best model is likely to be over-fitted tremendously (after testing 38880 param-eter settings over a dataset of 404 data points).Mean performance is largely determined by theproportions of ?good?
and ?bad?
parameter set-tings among the evaluation runs, which includemany non-optimal parameter values that were onlyincluded for completeness.Instead, we analyze the influence of individ-ual DSM parameters and their interactions usinglinear models with performance (accuracy or cor-relation) as a dependent variable and the variousmodel parameters as independent variables.
Thisapproach allows us to identify parameters thathave a significant effect on model performanceand to test for interactions between the parameters.Based on the partial effects of each parameter (andsignificant interactions) we can select a best modelin a robust way.This statistical analysis contains some elementsof novelty with respect to the state-of-the-art DSMevaluation.
Broadly speaking, approaches to DSMevaluation described in the literature fall into twoclasses.
The first one can be labeled as best modelfirst, as it implies the identification of the opti-mal configuration of parameters on an initial task,considered more basic; the best performing modelon the general task is therefore evaluated on othertasks of interest.
This is the approach adopted, forexample, by Pado?
and Lapata (2007).
In the sec-ond approach, described in Bullinaria and Levy(2007; 2012), evaluation is conducted via incre-mental tuning of parameters: parameters are eval-uated sequentially to identify the best performingvalue on a number of tasks.
Such approaches toDSM evaluation have specific limitations.
Theformer approach does not assess which parame-ters are crucial in determining model performance,since its goal is the evaluation of performance ofthe same model on different tasks.
The latter ap-proach does not allow for parameter interactions,considering parameters individually.
Both limita-tions are avoided in the analysis used here.4 Results4.1 Identification of Congruent PrimeThis section presents the results from the first taskevaluated in our study.
We used the DSMs to iden-tify which of the two primes is the congruent onebased on their distributional relatedness to the tar-get.
For every triple in the dataset, the different in-dexes of distributional relatedness (parameter re-latedness index) were used to compare the associ-ation between the target and the congruent primewith the association between the target and the in-congruent prime.
Accuracy of DSMs in picking upthe congruent prime was calculated on the globaldataset and separately for each subset.9Figure 1 displays the distribution of the accu-racy scores of all tested models in the task, on theglobal dataset.
All accuracy values are specifiedas percentages.
Minimum, maximum, mean andstandard deviation of the accuracy values for theglobal dataset and for the three subsets are dis-played in table 2.The mean performance on N-N is lower than on9The small number of triples for which no predictioncould be made because of missing words in the DSMs wereconsidered mistakes.
The coverage of the models ranges from97.8% to 100% of the triples, with a mean of 99%.69050010001500200050 60 70 80 90 100Figure 1: Identification of congruent prime: distri-bution of accuracy (%) for global datasetDataset Min Max Mean ?Global 50.2 96.5 80.2 9.2V-N 45.8 95.8 80.0 8.4N-V 49.1 99.1 82.7 9.7N-N 47.6 97.6 78.7 10.0Table 2: Identification of congruent prime: meanand range for global dataset and subsetsN-V and slightly lower than on V-N.
This effectmay be interpreted as being due to mediated prim-ing, as no verb is explicitly involved in the N-Nrelationship.
Yet, the relatively high accuracy onN-N and its relatively small difference from N-Vand V-N does not speak in favor of a different un-derlying mechanism responsible for this effect.
In-deed, McKoon and Ratcliff (1992) suggested thateffects traditionally considered as instances of me-diated priming are not due to activation spreadingthrough a mediating node, but result from a directbut weaker relatedness between prime and targetwords.
This hypothesis found computational sup-port in McDonald and Lowe (2000).104.1.1 Model Parameters and AccuracyThe aim of this section is to assess which param-eters have the most significant impact on the per-formance of DSMs in the task of identification ofthe congruent prime.We trained a linear model with the eight DSMparameters as independent variables (R2 = 0.70)and a second model that also includes all two-wayinteractions (R2 = 0.89).
Given the improvementin R2 as a consequence of the inclusion of two-wayinteractions in the linear model, we will focus onthe results from the model with interactions.
Table3 shows results from the analysis of variance for10The interpretation of the N-N results in terms of spread-ing activation is also rejected by Hare et al(2009, 163).the model with interactions.
For every parameter(and interaction of parameters) we report degreesof freedom (df ), percentage of explained variance(R2), and a significance code (signif ).
We onlylist significant interactions that explain at least 1%of the variance.
Even though all parameters andmany interactions are highly significant due to thelarge number of DSMs that were tested, an analy-sis of their predictive power in terms of explainedvariance allows us to make distinctions betweenparameters.Parameter df R2 signifcorpus 4 7.44 ***window 2 4.39 ***pos 2 0.92 ***score 5 7.39 ***transformation 3 3.79 ***distance 2 22.20 ***dimensionality reduction 2 10.52 ***relatedness index 3 13.67 ***score:transformation 15 4.53 ***distance:relatedness index 12 2.24 ***distance:dim.reduction 4 2.16 ***window:dim.reduction 4 1.73 ***Table 3: Accuracy: Parameters and interactionsResults in table 3 indicate that distance, dimen-sionality reduction and relatedness index are theparameters with the strongest explanatory power,followed by corpus and score.
Window and trans-formation have a weaker explanatory power, whilepos falls below the 1% threshold.
There is astrong interaction between score and transforma-tion, which has more influence than one of the in-dividual parameters, namely transformation.Figures 2 to 7 display the partial effects of dif-ferent model parameters (pos was excluded be-cause of its low explanatory power).
One of themain research questions behind this work waswhether neighbor rank performs better than dis-tance in predicting priming data.
The partial ef-fect of relatedness index in Figure 6 confirms ourhypothesis: forward rank achieves the best perfor-mance, distance the worst.11Accuracy improves for models trained on big-ger corpora (parameter corpus, figure 2; corporaare ordered by size) and larger context windows(parameter window, figure 3).
Cosine is the bestperforming distance measure (figure 4).
Interest-ingly, dimensionality reduction is found to neg-atively affect model performance: as shown infigure 7, both random indexing (ri) and singular11Backward rank is equivalent to distance in this task.7074767880828486bnc wp500 wacky ukwac jointlllllFigure 2: Corpus747678808284862 5 15lllFigure 3: Window74767880828486cos eucl manlllFigure 4: Distancel lllllnonefreq Dice MI s?ll t?sc z?sc74767880828486l nonelogrootsigmoidFigure 5: Score + Transformation74767880828486dist back_rank forw_rank avg_rankl lllFigure 6: Rel.
Index74767880828486none ri rsvdlllFigure 7: Dim.
Reductionvalue decomposition (rsvd) cause a decrease inpredicted accuracy.Because of the strong interaction between scoreand transformation, only their combined effectis shown (figure 5).
Among the scoring mea-sures, stochastic association measures performbetter than frequency: in particular log-likelihood(simple-ll), z-score and t-score are the best mea-sures.
We can identify a general tendency of trans-formation to lower accuracy.
This is true for allscores except log-likelihood: square root and (to alesser extent) logarithmic transformation result inan improvement for this measure.Figure 8 displays the interaction between theparameters distance and dimensionality reduction.Despite a general tendency for dimensionality re-duction to lower accuracy, we found an interac-tion between cosine distance and singular valuedecomposition: in this combination, accuracy re-mains stable and is even minimally higher com-pared to no dimensionality reduction.l llcos eucl man6870727476788082848688l nonerirsvdFigure 8: Distance + Dimensionality Reduction4.2 Correlation to Reaction TimesThe results reported in section 4.1 demonstratethat forward rank is the best index for identifyingwhich of the two primes is the congruent one.
Theaim of this section is to find out whether rank isalso a good predictor of latency times.
We checkcorrelation between distributional relatedness andreaction times and evaluate the impact of modelparameters on this task.Figure 9 displays the distribution of Pearsoncorrelation coefficient achieved by the differentDSMs on the global dataset.0500100015000.0 0.1 0.2 0.3 0.4 0.5Figure 9: Distribution of Pearson correlation be-tween relatedness and RT in the global datasetFigure 9 shows that the majority of the modelsperform rather poorly, and that only few modelsachieve moderate correlation with RT.
DSM per-71formance in the correlation task appears to be lessrobust to non-optimal parameter settings than inthe accuracy task (cf.
figure 1).Minimum, maximum, mean and standard devi-ation correlation for the global dataset and for thethree evaluation subsets are shown in table 4.
In allthe cases, absolute correlation values are used soas not to distinguish between positive and negativecorrelation.Dataset Min Max Mean ?Global -0.26 0.47 0.19 0.10V-N -0.34 0.57 0.2 0.12N-V -0.35 0.41 0.11 0.06N-N -0.29 0.42 0.16 0.09Table 4: Mean and range of Pearson correlationcoefficients on global dataset and subsets4.2.1 Model Parameters and CorrelationIn this section we discuss the impact of differ-ent model parameters on correlation with reactiontimes.We trained a linear model with absolute Pearsoncorrelation on the global dataset as dependent vari-able and the eight DSM parameters as independentvariables (R2 = 0.53), and a second model that in-cludes two-way interactions (R2 = 0.77).
Table5 is based on the model with interactions; it re-ports the degrees of freedom (df ), proportion ofexplained variance (R2) and a significance code(signif ) for every parameter and every interactionof parameters (above 1% of explained variance).Parameter df R2 signifcorpus 4 7.45 ***window 2 0.47 ***pos 2 0.20 ***score 5 3.03 ***transformation 3 3.52 ***distance 2 4.27 ***dimensionality reduction 2 10.57 ***relatedness index 3 23.40 ***dim.reduction:relatedness index 6 5.21 ***distance:dim.reduction 4 4.11 ***distance:relatedness index 6 3.77 ***score:transformation 15 3.22 ***score:relatedness index 15 1.37 ***Table 5: Correlation: Parameters and interactionsRelatedness index is the most important param-eter, followed by dimensionality reduction andcorpus.
The explanatory power of the other pa-rameters (score, transformation, distance) is lowerthan for the accuracy task, and two parameters(window and pos) explain less than 1% of the vari-ance each.
By contrast, the explanatory power ofinteractions is higher in this task.
Table 5 showsthe five relevant interactions with an overall higherR2 compared to the accuracy task (cf.
table 3).The partial effect plot for relatedness index (fig-ure 14) confirms the findings of the accuracy task:forward rank is the best value for this parameter.The best values for the other parameters, however,show opposite tendencies with respect to the accu-racy task.
Models trained on smaller corpora (fig-ure 10) perform better than those trained on big-ger ones.
Cosine is still the best distance measure,but manhattan distance performs equally well inthis task (parameter distance, figure 12).
Singu-lar value decomposition (parameter dimensional-ity reduction, figure 15) weakens the correlationvalues achieved by the models, but no significantdifference is found between random indexing andthe unreduced data.Co-occurrence frequency performs better thanstatistical association measures and transforma-tion improves correlation: figure 13 displays theinteraction between these two parameters.
Trans-formation has a positive effect for every score, butthe optimal transformation differs.
Its impact isparticularly strong for the Dice coefficient, whichreaches the same performance as frequency whencombined with a square root transformation.Let us conclude by discussing the interactionbetween distance and dimensionality reduction(figure 16).
Based on the partial effects of the indi-vidual parameters, any combination of manhattanor cosine distance with random indexing or no di-mensionality reduction should be close to optimal.However, the interaction plot reveals that manhat-tan distance with random indexing is the best com-bination, outperforming the second best (cosinewithout dimensionality reduction) by a consider-able margin.
The positive effect of random index-ing is quite surprising and will require further in-vestigation.l l lcos eucl man0.080.10.120.140.160.180.20.220.240.26l nonerirsvdFigure 16: Distance + Dimensionality Reduction720.120.140.160.180.20.220.240.26bnc wp500 wacky ukwac jointll l llFigure 10: Corpus0.120.140.160.180.20.220.240.262 5 15l llFigure 11: Window0.120.140.160.180.20.220.240.26cos eucl manlllFigure 12: Distancel lllllfreq Dice MI s?ll t?sc z?sc0.120.140.160.180.20.220.240.26l nonelogrootsigmoidFigure 13: Score + Transformation0.120.140.160.180.20.220.240.26dist back_rank forw_rank avg_rankllllFigure 14: Rel.
Index0.120.140.160.180.20.220.240.26none ri rsvdl llFigure 15: Dim.
Reduction5 ConclusionIn this paper, we presented the results of a large-scale evaluation of distributional models and theirparameters on behavioral data from priming ex-periments.
Our study is, to the best of our knowl-edge, the first systematic evaluation of such a widerange of DSM parameters in all possible combina-tions.
Our study also provides a methodologicalcontribution to the problem of DSM evaluation.We propose to apply linear modeling to determinethe impact of different model parameters and theirinteractions on the performance of the models.
Webelieve that this type of analysis is robust againstoverfitting.
Moreover, effects can be tested forsignificance and various forms of interactions be-tween model parameters can be captured.The main findings of our evaluation can be sum-marized as follows.
Forward association (rank oftarget among the nearest neighbors of the prime)performs better than distance in both tasks at is-sue: identification of congruent prime and correla-tion with latency times.
This finding confirms andextends the results of previous studies (Hare et al2009).
The relevance of rank-based measures forcognitive modeling is discussed in section 3.2.2.Identification of congruent primes on the ba-sis of distributional relatedness between prime andtarget is improved by employing bigger corporaand by using statistical association measures asscoring functions, while correlation to reactiontimes is strengthened by smaller corpora and co-occurrence frequency or Dice coefficient.
A sig-nificant interaction between transformation andscoring function is found in both tasks: consider-ing the interaction between these two parametersturned out to be vital for the identification of opti-mal parameter values.Some preliminary analyses of individual the-matic relations showed substantial improvementsof correlations.
Therefore, future work will focuson finer-grained linear models for single relationsand on further modeling of reaction times, extend-ing the study by Hutchinson et al(2008).Further research steps also include an evalua-tion of syntax-based models (Baroni and Lenci,2010; Pado?
and Lapata, 2007) and term-documentmodels on the tasks tackled in this paper, as wellas an evaluation of all models on standard tasks.AcknowledgmentsWe are grateful to Ken MacRae for providing usthe priming data modeled here and to AlessandroLenci for his contribution to the development ofthis study.
We would also like to thank the Com-putational Linguistics group at the University ofOsnabru?ck and the Corpus Linguistics group at theUniversity Erlangen for feedback.
Thanks also goto three anonymous reviewers, whose commentshelped improve our analysis, and to Sascha Alex-eyenko for helpful advice.
The first author?s PhDproject is funded by a Lichtenberg grant from theMinistry of Science and Culture of Lower Saxony.73ReferencesMarco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):1?49.Marco Baroni and Alessandro Lenci.
2011.
Howwe blessed distributional semantic evaluation.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,GEMS ?11, pages 1?10.
Association for Computa-tional Linguistics.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39:510?526.John A. Bullinaria and Joseph P. Levy.
2012.
Ex-tracting semantic representations from word co-occurrence statistics: stop-lists, stemming and svd.Behavior Research Methods, 44:890?907.Stefan Evert.
2004.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis,IMS, University of Stuttgart.Todd Ferretti, Ken McRae, and Ann Hatherell.
2001.Integrating verbs, situation schemas, and thematicrole concepts.
Journal of Memory and Language,44(4):516?547.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological Review, 114:211?244.Nathan Halko, Per-Gunnar Martinsson, and Joel A.Tropp.
2009.
Finding structure with randomness:Stochastic algorithms for constructing approximatematrix decompositions.
Technical Report 2009-05,ACM, California Institute of Technology.Mary Hare, Michael Jones, Caroline Thomson, SarahKelly, and Ken McRae.
2009.
Activating eventknowledge.
Cognition, 111(2):151?167.Zelig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Amac Herdag?delen, Marco Baroni, and Katrin Erk.2009.
Measuring semantic relatedness with vectorspace models and random walks.
In Proceedingsof the 2009 Workshop on Graph-based Methods forNatural Language Processing, pages 50?53.Keith A. Hutchinson, David A. Balota, Michael J.Cortese, and Jason M. Watson.
2008.
Predictingsemantic priming at the item level.
The QuarterlyJournal of Experimental Psychology, 61(7):1036?1066.Michael Jones and Douglas Mewhort.
2007.
Repre-senting word meaning and order information in acomposite holographic lexicon.
Psychological Re-view, 114:1?37.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent seman-tic analysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Review,104:211?240.Will Lowe and Scott McDonald.
2000.
The directroute: mediated priming in semantic space.
Tech-nical report, Division of Informatics, University ofEdinburgh.Scott McDonald and Chris Brew.
2004.
A distribu-tional model of semantic context effects in lexicalprocessing.
In Proceedings of ACL-04, pages 17?24.Gain McKoon and Roger Ratcliff.
1992.
Spreading ac-tivation versus compound cue accounts of priming:Mediated priming revisited.
Journal of Experimen-tal Psychology: Learning, Memory and Cognition,18:1155?1172.Ken McRae and Kazunaga Matzuki.
2009.
People usetheir knowledge of common events to understandlanguage, and do so as quickly as possible.
Lan-guage and Linguistics Compass, 3(6):1417?1429.Ken McRae, Mary Hare, Jeffrey L. Elman, and ToddFerretti.
2005.
A basis for generating expectan-cies for verbs from nouns.
Memory & Cognition,33(7):1174?1184.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, Ohio.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Proceedings of the Methods and Appli-cations of Semantic Indexing Workshop at the 7th In-ternational Conference on Terminology and Knowl-edge Engineering, TKE 2005.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Universityof Stockolm.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Amos Tversky.
1977.
Features of similarity.
Psycho-logical Review, 84:327?352.74
