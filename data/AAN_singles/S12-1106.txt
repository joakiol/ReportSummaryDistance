First Joint Conference on Lexical and Computational Semantics (*SEM), pages 706?709,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsBUAP: Lexical and Semantic Similarity for Cross-lingual TextualEntailmentDarnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban CastilloBeneme?rita Universidad Auto?noma de Puebla,Faculty of Computer Science14 Sur & Av.
San Claudio, CUPuebla, Puebla, Me?xico{darnes, dpinto, mtovar}@cs.buap.mxsaul.ls@live.com, ecjbuap@gmail.comAbstractIn this paper we present a report of the two di-fferent runs submitted to the task 8 of Semeval2012 for the evaluation of Cross-lingual Tex-tual Entailment in the framework of ContentSynchronization.
Both approaches are basedon textual similarity, and the entailment judg-ment (bidirectional, forward, backward or noentailment) is given based on a set of decisionrules.
The first approach uses textual simi-larity on the translated and original versionsof the texts, whereas the second approach ex-pands the terms by means of synonyms.
Theevaluation of both approaches show a similarbehavior which is still close to the average andmedian.1 IntroductionCross-lingual Textual Entailment (CLTE) has beenrecently proposed by (Mehdad et al, 2010; Mehdadet al, 2011) as an extension of the Textual Entail-ment task (Dagan and Glickman, 2004).
Given a text(T ) and an hypothesis (H) in different languages,the CLTE task consists of determining if the mea-ning of H can be inferred from the meaning of T .In this paper we present a report of the obtainedresults after submitting two different runs for theTask 8 of Semeval 2012, named ?Cross-lingual Tex-tual Entailment for Content Synchronization?
(Negriet al, 2012).
In this task, the Cross-Lingual Tex-tual Entailment addresses textual entailment recog-nition under a new dimension (cross-linguality), andwithin a new challenging application scenario (con-tent synchronization).
The task 8 of Semeval 2012may be formally defined as follows:Given a pair of topically related text fragments(T1 and T2) in different languages, the task consistsof automatically annotating it with one of the follo-wing entailment judgments:?
Bidirectional (T1 ?
T2 & T1 ?
T2): the twofragments entail each other (semantic equiva-lence)?
Forward (T1 ?
T2 & T1 !
?
T2): unidirec-tional entailment from T1 to T2?
Backward (T1 !
?
T2 & T1 ?
T2): unidirec-tional entailment from T2 to T1?
No Entailment (T1 !
?
T2 & T1 !
?
T2): thereis no entailment between T1 and T2In this task, both T1 and T2 are assumed to beTRUE statements; hence in the dataset there are nocontradictory pairs.
Cross-lingual datasets are avai-lable for the following language combinations:?
Spanish/English (SPA-ENG)?
German/English (DEU-ENG)?
Italian/English (ITA-ENG)?
French/English (FRA-ENG)The remaining of this paper is structured as fo-llows: Section 2 describes the two different approa-ches presented in the competition.
The obtained re-sults are shown and dicussed in Section 3.
Finally,the findings of this work are given in Section 4.7062 Experimental setupFor this experiment we have considered to tackle theCLTE task by means of textual similarity and textuallength.
In particular, the textual similarity is used todetermine whether some kind of entailment exists ornot.
We have established the threshold of 0.5 for thesimilarity function as evidence of textual entailment.Since the two sentences to be evaluated are writtenin two different languages, we have translated eachsentence to the other language, so that, we have twosentences in English, and two sentences in the origi-nal language (Spanish, German, Italian and French).We have used the Google translate for this purpose1.The corpora used in the experiments comes froma cross-lingual Textual Entailment dataset presentedin (Negri et al, 2011), and provided by the task orga-nizers.
We have employed the training dataset onlyfor adjust some parameters of the system, but theapproach is knowledge-based and, therefore, it doesnot need a training corpus.
Both, the training andtest corpus contain 500 sentences for each language.The textual length is used to determine the entail-ment judgment (bidirectional, forward, backward,no entailment).
We have basically, assumed that thelength of a text may give some evidence of the typeof entailment.
The decision rules used for determi-ning the entailment judgment are described in Sec-tion 2.3.In this competition we have submitted two diffe-rent runs which differ with respect to the type of tex-tual similarity used (lexical vs semantic).
The firstone, calculates the similarity using only the trans-lated version of the original sentences, whereas thesecond approach uses text expansion by means ofsynonyms and, thereafter, it calculates the similaritybetween the pair of sentences.Let T1 be the sentence in the original language,T2 the T1 topically related text fragment (written inEnglish).
Let T3 be the English translation of T1,and T4 the translation of T2 to the original language(Spanish, German, Italian and French).
The formaldescription of these two approaches are given as fo-llows.1http://translate.google.com.mx/2.1 Approach 1: Lexical similarityThe evidence of textual entailment between T1 andT2 is calculated using two formulae of lexical si-milarity.
Firstly, we determine the similarity bet-ween the two texts written in the source language(SimS).
Additionally, we calculate the lexical simi-larity between the two sentences written in the targetlanguage (SimT ), in this case English.Given the limited text length of the text fragments,we have used the Jaccard coefficient as similaritymeasure.
Eq.
(1) shows the lexical similarity for thetwo texts written in the original language, whereas,Eq.
(2) presents the Jaccard coefficient for the textswritten in English.simS = simJaccard(T1, T4) =|T1 ?
T4||T1 ?
T4|(1)simT = simJaccard(T2, T3) =|T2 ?
T3||T2 ?
T3|(2)2.2 Approach 2: Semantic similarityIn this case we calculate the semantic similarity bet-ween the two texts written in the original language(simS), and the semantic similarity between the twotext fragments written in English (simT ).
The se-mantic level of similarity is given by consideringthe synonyms of each term for each sentence (inthe original and target language).
For this purpose,we have employed five dictionaries containing syno-nyms for the five different languages considered inthe competition (English, Spanish, German, Italian,and French)2.
In Table 1 we show the number ofterms, so as the number of synonyms in average byterm considered for each language.Let T1 = w1,1w1,2...w1,|T1|, T2 =w2,1w2,2...w2,|T2| be the source and targetsentences, and let T3 = w3,1w3,2...w3,|T3|,T4 = w4,1w4,2...w4,|T4| be translated version of theoriginal source and target sentences, respectively.The synonyms of a given word wi,k, expressed assynset(wi,k), are obtained from the aforementioneddictionaries by extracting the synonyms of wi,k.
Inorder to obtain a better matching between the termscontained in the text fragments and the terms in the2http://extensions.services.openoffice.org/en/dictionaries707Table 1: Dictionaries of synonyms used for term expan-sionLanguage Terms synonyms per term(average)English 2,764 60Spanish 9,887 45German 21,958 115Italian 25,724 56French 36,207 93dictionary, we have stemmed all the terms using thePorter stemmer.In order to determine the semantic similarity bet-ween two terms of sentences written in the sourcelanguage (w1,i and w4,j) we use Eq.
(3).
The se-mantic similariy between two terms of the Englishsentences are calculated as shown in Eq.
(4).sim(w1,i, w4,j) =????????
?1 if (w1,i == w4,j) ||w1,i ?
synset(w4,j) ||w4,j ?
synset(w1,i)0 otherwise(3)sim(w2,i, w3,j) =????????
?1 if (w2,i == w3,j) ||w2,i ?
synset(w3,j) ||w3,j ?
synset(w2,i)0 otherwise(4)Both equations consider the existence of semanticsimilarity when the two words are identical, or whenthe some of the two words appear in the synonym setof the other word.The semantic similarity of the complete text frag-ments T1 and T4 (simS) is calculated as shown inEq.
(5).
Whereas, the semantic similarity of thecomplete text fragments T2 and T3 (simT ) is cal-culated as shown in Eq.
(6).simS(T1, T4) =?|T1|i=1?|T4|j=1 sim(w1,i,w4,j)|T1?T4|(5)simT (T2, T3) =?|T2|i=1?|T3|j=1 sim(w2,i,w3,j)|T2?T3|(6)2.3 Decision rulesBoth approches used the same decision rules in or-der to determine the entailment judgment for a givenpair of text fragments (T1 and T2).
The following al-gorithm shows the decision rules used.Algorithm 1.If |T2| < |T3| thenIf (simT > 0.5 and simS > 0.5)then forwardElseIf |T2| > |T3| thenIf (simT > 0.5 and simS > 0.5)then backwardElseIf (|T1| == |T4| and |T2| == |T3|) thenIf (simT > 0.5 and simS > 0.5)then bidirectionalElse no entailmentAs mentioned above, the rules employed the le-xical or semantic textual similarity, and the textuallength for determining the textual entailment.3 ResultsIn Table 2 we show the overall results obtained bythe two approaches submitted to the competition.We also show the highest, lowest, average and me-dian overall results obtained in the competition.SPA-ENGITA-ENGFRA-ENGDEU-ENGHighest 0.632 0.566 0.57 0.558Average 0.407 0.362 0.366 0.357Median 0.346 0.336 0.336 0.336Lowest 0.266 0.278 0.278 0.262BUAP run1 0.35 0.336 0.334 0.33BUAP run2 0.366 0.344 0.342 0.268Table 2: Overall statistics obtained in the Task 8 of Se-meval 2012The runs submitted perform similar, but the se-mantic approach obtained a slightly better perfor-mance.
The two results are above the median butbelow the average.
We consider that better resultsmay be obtained if the two features used (textual si-milarity and textual length) were introduced into asupervised classifier, so that, the decision rules wereapproximated on the basis of a training dataset, ins-tead of the empirical setting done in this work.
Fu-ture experiments will be carried out in this direction.7084 Discussion and conclusionTwo different approaches for the Cross-lingual Tex-tual Entailment for Content Synchronization task ofSemeval 2012 are reported in this paper.
We usedtwo features for determining the textual entailmentjudgment between two texts T1 and T2 (written intwo different languages).
The first approach pro-posed used lexical similarity, meanwhile the secondused semantic similarity by means of term expan-sion with synonyms.Even if the performance of both approaches isabove the median and slighly below the average,we consider that we may easily improve this perfor-mance by using syntactic features of the text frag-ments.
Additionally, we are planning to integratesome supervised techniques based on decision ruleswhich may be trained in a supervised dataset.
Futureexperiments will be executed in this direction.AcknowledgmentsThis project has been partially supported by projectsCONACYT #106625, #VIAD-ING11-II and VIEP#PIAD-ING11-II.ReferencesIdo Dagan and Oren Glickman.
2004.
Probabilistic Tex-tual Entailment: Generic Applied Modeling of Lan-guage Variability.
In Learning Methods for Text Un-derstanding and Mining, January.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards Cross-Lingual Textual Entailment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 321?324, Los Angeles, California, June.
Association forComputational Linguistics.Yashar Mehdad, Matteo Negri, and Marcello Federico.2011.
Using bilingual parallel corpora for cross-lingual textual entailment.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.Association for Computational Linguistics.Matteo Negri, Luisa Bentivogli, Yashar Mehdad, DaniloGiampiccolo, and Alessandro Marchetti.
2011.
Di-vide and conquer: crowdsourcing the creation of cross-lingual textual entailment corpora.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 670?679,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.M.
Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, andD.
Giampiccolo.
2012.
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchroniza-tion.
In Proceedings of the 6th International Workshopon Semantic Evaluation (SemEval 2012).709
