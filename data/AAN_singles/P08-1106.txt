Proceedings of ACL-08: HLT, pages 932?940,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLinguistically Motivated Features for Enhanced Back-of-the-Book IndexingAndras Csomai and Rada MihalceaDepartment of Computer ScienceUniversity of North Texascsomaia@unt.edu,rada@cs.unt.eduAbstractIn this paper we present a supervised methodfor back-of-the-book index construction.
Weintroduce a novel set of features that goes be-yond the typical frequency-based analysis, in-cluding features based on discourse compre-hension, syntactic patterns, and informationdrawn from an online encyclopedia.
In exper-iments carried out on a book collection, themethod was found to lead to an improvementof roughly 140% as compared to an existingstate-of-the-art supervised method.1 IntroductionBooks represent one of the oldest forms of writ-ten communication and have been used since thou-sands of years ago as a means to store and trans-mit information.
Despite this fact, given that alarge fraction of the electronic documents avail-able online and elsewhere consist of short textssuch as Web pages, news articles, scientific reports,and others, the focus of natural language process-ing techniques to date has been on the automa-tion of methods targeting short documents.
Weare witnessing however a change: more and morebooks are becoming available in electronic for-mat, in projects such as the Million Books project(http://www.archive.org/details/millionbooks), theGutenberg project (http://www.gutenberg.org), orGoogle Book Search (http://books.google.com).Similarly, a large number of the books publishedin recent years are often available ?
for purchaseor through libraries ?
in electronic format.
Thismeans that the need for language processing tech-niques able to handle very large documents such asbooks is becoming increasingly important.This paper addresses the problem of automaticback-of-the-book index construction.
A back-of-the-book index typically consists of the most impor-tant keywords addressed in a book, with pointers tothe relevant pages inside the book.
The construc-tion of such indexes is one of the few tasks relatedto publishing that still requires extensive human la-bor.
Although there is a certain degree of computerassistance, consisting of tools that help the profes-sional indexer to organize and edit the index, thereare no methods that would allow for a complete ornearly-complete automation.In addition to helping professional indexers intheir task, an automatically generated back-of-the-book index can also be useful for the automatic stor-age and retrieval of a document; as a quick referenceto the content of a book for potential readers, re-searchers, or students (Schutze, 1998); or as a start-ing point for generating ontologies tailored to thecontent of the book (Feng et al, 2006).In this paper, we introduce a supervised methodfor back-of-the-book index construction, using anovel set of linguistically motivated features.
Thealgorithm learns to automatically identify importantkeywords in a book based on an ensemble of syntac-tic, discourse-based and information-theoretic prop-erties of the candidate concepts.
In experiments per-formed on a collection of books and their indexes,the method was found to exceed by a large marginthe performance of a previously proposed state-of-the-art supervised system for keyword extraction.2 Supervised Back-of-the-Book IndexingWe formulate the problem of back-of-the-book in-dexing as a supervised keyword extraction task, bymaking a binary yes/no classification decision at the932level of each candidate index entry.
Starting with aset of candidate entries, the algorithm automaticallydecides which entries should be added to the back-of-the-book index, based on a set of linguistic andinformation theoretic features.
We begin by iden-tifying the set of candidate index entries, followedby the construction of a feature vector for each suchcandidate entry.
In the training data set, these fea-ture vectors are also assigned with a correct label,based on the presence/absence of the entry in thegold standard back-of-the-book index provided withthe data.
Finally, a machine learning algorithm isapplied, which automatically classifies the candidateentries in the test data for their likelihood to belongto the back-of-the-book index.The application of a supervised algorithm re-quires three components: a data set, which is de-scribed next; a set of features, which are described inSection 3; and a machine learning algorithm, whichis presented in Section 4.2.1 DataWe use a collection of books and monographs fromthe eScholarship Editions collection of the Univer-sity of California Press (UC Press),1 consisting of289 books, each with a manually constructed back-of-the-book index.
The average length of the booksin this collection is 86053 words, and the averagelength of the indexes is 820 entries.
A collectionof 56 books was previously introduced in (Csomaiand Mihalcea, 2006); however, that collection is toosmall to be split in training and test data to supportsupervised keyword extraction experiments.The UC Press collection was provided in a stan-dardized XML format, following the Text EncodingInitiative (TEI) recommendations, and thus it wasrelatively easy to process the collection and separatethe index from the body of the text.In order to use this corpus as a gold standardcollection for automatic index construction, we hadto eliminate the inversions, which are typical inhuman-built indexes.
Inversion is a method used byprofessional indexers by which they break the order-ing of the words in each index entry, and list the headfirst, thereby making it easier to find entries in analphabetically ordered index.
As an example, con-sider the entry indexing of illustrations, which, fol-lowing inversion, becomes illustrations, indexing of.To eliminate inversion, we use an approach that gen-1http://content.cdlib.org/escholarship/erates each permutation of the composing words foreach index entry, looks up the frequency of that per-mutation in the book, and then chooses the one withthe highest frequency as the correct reconstructionof the entry.
In this way, we identify the form of theindex entries as appearing in the book, which is theform required for the evaluation of extraction meth-ods.
Entries that cannot be found in the book, whichwere most likely generated by the human indexers,are preserved in their original ordering.For training and evaluation purposes, we used arandom split of the collection into 90% training and10% test.
This yields a training corpus of 259 docu-ments and a test data set of 30 documents.2.2 Candidate Index EntriesEvery sequence of words in a book represents a po-tential candidate for an entry in the back-of-the-bookindex.
Thus, we extract from the training and the testdata sets all the n-grams (up to the length of four),not crossing sentence boundaries.
These representthe candidate index entries that will be used in theclassification algorithm.
The training candidate en-tries are then labeled as positive or negative, depend-ing on whether the given n-gram was found in theback-of-the-book index associated with the book.Using a n-gram-based method to extract candidateentries has the advantage of providing high cover-age, but the unwanted effect of producing an ex-tremely large number of entries.
In fact, the result-ing set is unmanageably large for any machine learn-ing algorithm.
Moreover, the set is extremely unbal-anced, with a ratio of positive and negative exam-ples of 1:675, which makes it unsuitable for mostmachine learning algorithms.
In order to addressthis problem, we had to find ways to reduce the sizeof the data set, possibly eliminating the training in-stances that will have the least negative effect on theusability of the data set.The first step to reduce the size of the data set wasto use the candidate filtering techniques for unsuper-vised back-of-the-book index construction that weproposed in (Csomai and Mihalcea, 2007).
Namely,we use the commonword and comma filters, whichare applied to both the training and the test collec-tions.
These filters work by eliminating all the n-grams that begin or end with a common word (weuse a list of 300 most frequent English words), aswell as those n-grams that cross a comma.
This re-sults in a significant reduction in the number of neg-933positive negative total positive:negative ratioTraining dataAll (original) 71,853 48,499,870 48,571,723 1:674.98Commonword/comma filters 66,349 11,496,661 11,563,010 1:173.2710% undersampling 66,349 1,148,532 1,214,881 1:17.31Test dataAll (original) 7,764 6,157,034 6,164,798 1:793.02Commonword/comma filters 7,225 1,472,820 1,480,045 1:203.85Table 1: Number of training and test instances generated from the UC Press data setative examples, from 48 to 11 million instances, witha loss in terms of positive examples of only 7.6%.The second step is to use a technique for balanc-ing the distribution of the positive and the negativeexamples in the data sets.
There are several meth-ods proposed in the existing literature, focusing ontwo main solutions: undersampling and oversam-pling (Weiss and Provost, 2001).
Undersampling(Kubat and Matwin, 1997) means the elimination ofinstances from the majority class (in our case nega-tive examples), while oversampling focuses on in-creasing the number of instances of the minorityclass.
Aside from the fact that oversampling hashard to predict effects on classifier performance, italso has the additional drawback of increasing thesize of the data set, which in our case is undesirable.We thus adopted an undersampling solution, wherewe randomly select 10% of the negative examples.Evidently, the undersampling is applied only to thetraining set.Table 1 shows the number of positive and neg-ative entries in the data set, for the different pre-processing and balancing phases.3 FeaturesAn important step in the development of a super-vised system is the choice of features used in thelearning process.
Ideally, any property of a word ora phrase indicating that it could be a good keywordshould be represented as a feature and included inthe training and test examples.
We use a numberof features, including information-theoretic featurespreviously used in unsupervised keyword extraction,as well as a novel set of features based on syntacticand discourse properties of the text, or on informa-tion extracted from external knowledge repositories.3.1 Phraseness and InformativenessWe use the phraseness and informativeness featuresthat we previously proposed in (Csomai and Mihal-cea, 2007).
Phraseness refers to the degree to whicha sequence of words can be considered a phrase.
Weuse it as a measure of lexical cohesion of the com-ponent terms and treat it as a collocation discoveryproblem.
Informativeness represents the degree towhich the keyphrase is representative for the docu-ment at hand, and it correlates to the amount of in-formation conveyed to the user.To measure the informativeness of a keyphrase,various methods can be used, some of which werepreviously proposed in the keyword extraction liter-ature:?
tf.idf, which is the traditional information re-trieval metric (Salton and Buckley, 1997), em-ployed in most existing keyword extraction ap-plications.
We measure inverse document fre-quency using the article collection of the onlineencyclopedia Wikipedia.?
?2 independence test, which measures the de-gree to which two events happen together moreoften than by chance.
In our work, we use the?2 in a novel way.
We measure the informa-tiveness of a keyphrase by finding if a phraseoccurs in the document more frequently thanit would by chance.
The information requiredfor the ?2 independence test can be typicallysummed up in a contingency table (Manningand Schutze, 1999):count(phrase in count(all other phrasesdocument) in document)count(phrase in other count(all other phrasesdocuments) in all other documents)The independence score is calculated based onthe observed (O) and expected (E) counts:?2 =?i,j(Oi,j ?
Ei,j)2Ei,jwhere i, j are the row and column indices of the934contingency table.
The O counts are the cells ofthe table.
The E counts are calculated from themarginal probabilities (the sum of the values ofa column or a row) converted into proportionsby dividing them with the total number of ob-served events (N ):N = O1,1 + O1,2 + O2,1 + O2,2Then the expected count for seeing the phrasein the document is:E1,1 =O1,1 + O1,2N ?O1,1 + O2,1N ?NTo measure the phraseness of a candidate phrasewe use a technique based on the ?2 independencetest.
We measure the independence of the eventsof seeing the components of the phrase in the text.This method was found to be one of the best per-forming models in collocation discovery (Pecina andSchlesinger, 2006).
For n-grams where N > 2we apply the ?2 independence test by splitting thephrase in two (e.g.
for a 4-gram, we measure theindependence of the composing bigrams).3.2 Discourse Comprehension FeaturesVery few existing keyword extraction methods lookbeyond word frequency.
Except for (Turney andLittman, 2003), who uses pointwise mutual infor-mation to improve the coherence of the keyword set,we are not aware of any other work that attemptsto use the semantics of the text to extract keywords.The fact that most systems rely heavily on term fre-quency properties poses serious difficulties, sincemany index entries appear only once in the docu-ment, and thus cannot be identified by features basedsolely on word counts.
For instance, as many as 52%of the index entries in our training data set appearedonly once in the books they belong to.
Moreover,another aspect not typically covered by current key-word extraction methods is the coherence of the key-word set, which can also be addressed by discourse-based properties.In this section, we propose a novel feature forkeyword extraction inspired by work on discoursecomprehension.
We use a construction integrationframework, which is the backbone used by manydiscourse comprehension theories.3.2.1 Discourse ComprehensionDiscourse comprehension is a field in cognitivescience focusing on the modeling of mental pro-cesses associated with reading and understandingtext.
The most widely accepted theory for discoursecomprehension is the construction integration the-ory (Kintsch, 1998).
According to this theory,the elementary units of comprehension are proposi-tions, which are defined as instances of a predicate-argument schema.
As an example, consider the sen-tence The hemoglobin carries oxygen, which gener-ates the predicate CARRY[HEMOGLOBIN,OXIGEN].The processing cycle of the construction integra-tion model processes one proposition at a time, andbuilds a local representation of the text in the work-ing memory, called the propositional network.During the construction phase, propositions areextracted from a segment of the input text (typ-ically a single sentence) using linguistic features.The propositional network is represented as a graph,with nodes consisting of propositions, and weightededges representing the semantic relations betweenthem.
All the propositions generated from the in-put text are inserted into the graph, as well as all thepropositions stored in the short term memory.
Theshort term memory contains the propositions thatcompose the representation of the previous few sen-tences.
The second phase of the construction stepis the addition of past experiences (or backgroundknowledge), which is stored in the long term mem-ory.
This is accomplished by adding new elementsto the graph, usually consisting of the set of closelyrelated propositions from the long term memory.After processing a sentence, the integration stepestablishes the role of each proposition in the mean-ing representation of the current sentence, through aspreading activation applied on the propositions de-rived from the original sentence.
Once the weightsare stabilized, the set of propositions with the high-est activation values give the mental representationof the processed sentence.
The propositions withthe highest activation values are added to the shortterm memory, the working memory is cleared andthe process moves to the next sentence.
Figure 3.2.1shows the memory types used in the construction in-tegration process and the main stages of the process.3.2.2 Keyword Extraction using DiscourseComprehensionThe main purpose of the short term memory is toensure the coherence of the meaning representationacross sentences.
By keeping the most importantpropositions in the short term memory, the spreadingactivation process transfers additional weight to se-935SemanticMemoryShort-termMemoryAddAssociatesAddPreviousPropositionsDecayIntegrationWorkingMemoryNextPropositionFigure 1: The construction integration processmantically related propositions in the sentences thatfollow.
This also represents a way of alleviating oneof the main problems of statistical keyword extrac-tion, namely the sole dependence on term frequency.Even if a phrase appears only once, the construc-tion integration process ensures the presence of thephrase in the short term memory as long as it is rele-vant to the current topic, thus being a good indicatorof the phrase importance.The construction integration model is not directlyapplicable to keyword extraction due to a number ofpractical difficulties.
The first implementation prob-lem was the lack of a propositional parser.
We solvethis problem by using a shallow parser to extractnoun phrase chunks from the original text (Munozet al, 1999).
Second, since spreading activation isa process difficult to control, with several parame-ters that require fine tuning, we use instead a dif-ferent graph centrality measure, namely PageRank(Brin and Page, 1998).Finally, to represent the relations inside the longterm semantic memory, we use a variant of latentsemantic analysis (LSA) (Landauer et al, 1998) asimplemented in the InfoMap package,2 trained on acorpus consisting of the British National Corpus, theEnglish Wikipedia, and the books in our collection.To alleviate the data sparsity problem, we also usethe pointwise mutual information (PMI) to calculatethe relatedness of the phrases based on the book be-ing processed.The final system works by iterating the followingsteps: (1) Read the text sentence by sentence.
Foreach new sentence, a graph is constructed, consist-ing of the noun phrase chunks extracted from theoriginal text.
This set of nodes is augmented withall the phrases from the short term memory.
(2) A2http://infomap.stanford.edu/weighted edge is added between all the nodes, basedon the semantic relatedness measured between thephrases by using LSA and PMI.
We use a weightedcombination of these two measures, with a weight of0.9 assigned to LSA and 0.1 to PMI.
For the nodesfrom the short term memory, we adjust the connec-tion weights to account for memory decay, which isa function of the distance from the last occurrence.We implement decay by decreasing the weight ofboth the outgoing and the incoming edges by n ?
?,where n is the number of sentences since we last sawthe phrase and ?
= 0.1.
(3) Apply PageRank onthe resulting graph.
(4) Select the 10 highest rankedphrases and place them in the short term memory.
(5) Read the next sentence and go back to step (1).Three different features are derived based on theconstruction integration model:?
CI short term memory frequency (CI short-term), which measures the number of iterationsthat the phrase remains in the short term mem-ory, which is seen as an indication of the phraseimportance.?
CI normalized short term memory fre-quency (CI normalized), which is the same asCI shortterm, except that it is normalized by thefrequency of the phrase.
Through this normal-ization, we hope to enhance the effect of the se-mantic relatedness of the phrase to subsequentsentences.?
CI maximum score (CI maxscore), whichmeasures the maximum centrality score thephrase achieves across the entire book.
Thiscan be thought of as a measure of the impor-tance of the phrase in a smaller coherent seg-ment of the document.3.3 Syntactic FeaturesPrevious work has pointed out the importance ofsyntactic features for supervised keyword extraction(Hulth, 2003).
The construction integration modeldescribed before is already making use of syntacticpatterns to some extent, through the use of a shal-low parser to identify noun phrases.
However, thatapproach does not cover patterns other than nounphrases.
To address this limitation, we introduce anew feature that captures the part-of-speech of thewords composing a candidate phrase.936There are multiple ways to represent such a fea-ture.
The simplest is to create a string feature con-sisting of the concatenation of the part-of-speechtags.
However, this representation imposes limita-tions on the machine learning algorithms that canbe used, since many learning systems cannot handlestring features.
The second solution is to introducea binary feature for each part-of-speech tag patternfound in the training and the test data sets.
In ourcase this is again unacceptable, given the size of thedocuments we work with and the large number ofsyntactic patterns that can be extracted.
Instead, wedecided on a novel solution which, rather than us-ing the part-of-speech pattern directly, determinesthe probability of a phrase with a certain tag patternto be selected as a keyphrase.
Formally:P (pattern) = C(pattern, positive)C(pattern)where C(pattern, positive) is the number of dis-tinct phrases having the tag pattern pattern and be-ing selected as keyword, and C(pattern) representsthe number of distinct phrases having the tag patternpattern.
This probability is estimated based on thetraining collection, and is used as a numeric feature.We refer to this feature as part-of-speech pattern.3.4 Encyclopedic FeaturesRecent work has suggested the use of domainknowledge to improve the accuracy of keyword ex-traction.
This is typically done by consulting a vo-cabulary of plausible keyphrases, usually in the formof a list of subject headings or a domain specificthesaurus.
The use of a vocabulary has the addi-tional benefit of eliminating the extraction of incom-plete phrases (e.g.
?States of America?).
In fact,(Medelyan and Witten, 2006) reported an 110% F-measure improvement in keyword extraction whenusing a domain-specific thesaurus.In our case, since the books can cover several do-mains, the construction and use of domain-specificthesauruses is not plausible, as the advantage of suchresources is offset by the time it usually takes tobuild them.
Instead, we decided to use encyclope-dic information, as a way to ensure high coverage interms of domains and concepts.We use Wikipedia, which is the largest and thefastest growing encyclopedia available today, andwhose structure has the additional benefit of beingparticularly useful for the task of keyword extrac-tion.
Wikipedia includes a rich set of links that con-nect important phrases in an article to their corre-sponding articles.
These links are added manuallyby the Wikipedia contributors, and follow the gen-eral guidelines of annotation provided by Wikipedia.The guidelines coincide with the goals of keywordextraction, and thus the Wikipedia articles and theirlink annotations can be treated as a vast keyword an-notated corpus.We make use of the Wikipedia annotations in twoways.
First, if a phrase is used as the title of aWikipedia article, or as the anchor text in a link,this is a good indicator that the given phrase is wellformed.
Second, we can also estimate the proba-bility of a term W to be selected as a keyword ina new document by counting the number of docu-ments where the term was already selected as a key-word (count(Dkey)) divided by the total number ofdocuments where the term appeared (count(DW )).These counts are collected from the entire set ofWikipedia articles.P (keyword|W ) ?
count(Dkey)count(DW )(1)This probability can be interpreted as ?the moreoften a term was selected as a keyword among itstotal number of occurrences, the more likely it is thatit will be selected again.?
In the following, we willrefer to this feature as Wikipedia keyphraseness.3.5 Other FeaturesIn addition to the features described before, we addseveral other features frequently used in keywordextraction: the frequency of the phrase inside thebook (term frequency (tf)); the number of documentsthat include the phrase (document frequency (df)); acombination of the two (tf.idf); the within-documentfrequency, which divides a book into ten equally-sized segments, and counts the number of segmentsthat include the phrase (within document frequency);the length of the phrase (length of phrase); and fi-nally a binary feature indicating whether the givenphrase is a named entity, according to a simpleheuristic based on word capitalization.4 Experiments and EvaluationWe integrate the features described in the previoussection in a machine learning framework.
The sys-tem is evaluated on the data set described in Sec-tion 2.1, consisting of 289 books, randomly split into93790% training (259 books) and 10% test (30 books).We experiment with three learning algorithms, se-lected for the diversity of their learning strategy:multilayer perceptron, SVM, and decision trees.
Forall three algorithms, we use their implementation asavailable in the Weka package.For evaluation, we use the standard informationretrieval metrics: precision, recall, and F-measure.We use two different mechanisms for selecting thenumber of entries in the index.
In the first evaluation(ratio-based), we use a fixed ratio of 0.45% from thenumber of words in the text; for instance, if a bookhas 100,000 words, the index will consist of 450 en-tries.
This number was estimated based on previousobservations regarding the typical size of a back-of-the-book index (Csomai and Mihalcea, 2006).
Inorder to match the required number of entries, wesort all the candidates in reversed order of the confi-dence score assigned by the machine learning algo-rithm, and consequently select the top entries in thisranking.
In the second evaluation (decision-based),we allow the machine learning algorithm to decideon the number of keywords to extract.
Thus, in thisevaluation, all the candidates labeled as keywordsby the learning algorithm will be added to the index.Note that all the evaluations are run using a train-ing data set with 10% undersampling of the negativeexamples, as described before.Table 2 shows the results of the evaluation.
Asseen in the table, the multilayer perceptron and thedecision tree provide the best results, for an over-all average F-measure of 27%.
Interestingly, the re-sults obtained when the number of keywords is auto-matically selected by the learning method (decision-based) are comparable to those when the number ofkeywords is selected a-priori (ratio-based), indicat-ing the ability of the machine learning algorithm tocorrectly identify the correct keywords.Additionally, we also ran an experiment to de-termine the amount of training data required by thesystem.
While the learning curve continues to growwith additional amounts of data, the steepest part ofthe curve is observed for up to 10% of the trainingdata, which indicates that a relatively small amountof data (about 25 books) is enough to sustain the sys-tem.It is worth noting that the task of creating back-of-the-book indexes is highly subjective.
In orderto put the performance figures in perspective, oneshould also look at the inter-annotator agreement be-tween human indexers as an upper bound of per-formance.
Although we are not aware of any com-prehensive studies for inter-annotator agreement onback-of-the-book indexing, we can look at the con-sistency studies that have been carried out on theMEDLINE corpus (Funk and Reid, 1983), where aninter-annotator agreement of 48% was found on anindexing task using a domain-specific controlled vo-cabulary of subject headings.4.1 Comparison with Other SystemsWe compare the performance of our system with twoother methods for keyword extraction.
One is thetf.idf method, traditionally used in information re-trieval as a mechanism to assign words in a text witha weight reflecting their importance.
This tf.idf base-line system uses the same candidate extraction andfiltering techniques as our supervised systems.
Theother baseline is the KEA keyword extraction system(Frank et al, 1999), a state-of-the-art algorithm forsupervised keyword extraction.
Very briefly, KEA isa supervised system that uses a Na?
?ve Bayes learn-ing algorithm and several features, including infor-mation theoretic features such as tf.idf and positionalfeatures reflecting the position of the words with re-spect to the beginning of the text.
The KEA systemwas trained on the same training data set as used inour experiments.Table 3 shows the performance obtained by thesemethods on the test data set.
Since none of thesemethods have the ability to automatically determinethe number of keywords to be extracted, the evalua-tion of these methods is done under the ratio-basedsetting, and thus for each method the top 0.45%ranked keywords are extracted.Algorithm P R Ftf.idf 8.09 8.63 8.35KEA 11.18 11.48 11.32Table 3: Baseline systems4.2 Performance of Individual FeaturesWe also carried out experiments to determine therole played by each feature, by using the informa-tion gain weight as assigned by the learning algo-rithm.
Note that ablation studies are not appropri-ate in our case, since the features are not orthogonal(e.g., there is high redundancy between the construc-tion integration and the informativeness features),and thus we cannot entirely eliminate a feature fromthe system.938ratio-based decision-basedAlgorithm P R F P R FMultilayer perceptron 27.98 27.77 27.87 23.93 31.98 27.38Decision tree 27.06 27.13 27.09 22.75 34.12 27.30SVM 20.94 20.35 20.64 21.76 30.27 25.32Table 2: Evaluation resultsFeature Weightpart-of-speech pattern 0.1935CI shortterm 0.1744Wikipedia keyphraseness 0.1731CI maxscore 0.1689CI shortterm normalized 0.1379ChiInformativeness 0.1122document frequency (df) 0.1031tf.idf 0.0870ChiPhraseness 0.0660length of phrase 0.0416named entity heuristic 0.0279within document frequency 0.0227term frequency (tf) 0.0209Table 4: Information gain feature weightTable 4 shows the weight associated with eachfeature.
Perhaps not surprisingly, the featureswith the highest weight are the linguistically moti-vated features, including syntactic patterns and theconstruction integration features.
The Wikipediakeyphraseness also has a high score.
The smallestweights belong to the information theoretic features,including term and document frequency.5 Related WorkWith a few exceptions (Schutze, 1998; Csomai andMihalcea, 2007), very little work has been carriedout to date on methods for automatic back-of-the-book index construction.The task that is closest to ours is perhaps keywordextraction, which targets the identification of themost important words or phrases inside a document.The state-of-the-art in keyword extraction is cur-rently represented by supervised learning methods,where a system is trained to recognize keywords in atext, based on lexical and syntactic features.
This ap-proach was first suggested in (Turney, 1999), whereparameterized heuristic rules are combined with agenetic algorithm into a system for keyphrase ex-traction (GenEx) that automatically identifies key-words in a document.
A different learning algo-rithm was used in (Frank et al, 1999), where a NaiveBayes learning scheme is applied on the documentcollection, with improved results observed on thesame data set as used in (Turney, 1999).
Neither Tur-ney nor Frank report on the recall of their systems,but only on precision: a 29.0% precision is achievedwith GenEx (Turney, 1999) for five keyphrases ex-tracted per document, and 18.3% precision achievedwith Kea (Frank et al, 1999) for fifteen keyphrasesper document.
Finally, in recent work, (Hulth, 2003)proposes a system for keyword extraction from ab-stracts that uses supervised learning with lexical andsyntactic features, which proved to improve signifi-cantly over previously published results.6 Conclusions and Future WorkIn this paper, we introduced a supervised method forback-of-the-book indexing which relies on a novelset of features, including features based on discoursecomprehension, syntactic patterns, and informationdrawn from an online encyclopedia.
According toan information gain measure of feature importance,the new features performed significantly better thanthe traditional frequency-based techniques, leadingto a system with an F-measure of 27%.
This rep-resents an improvement of 140% with respect to astate-of-the-art supervised method for keyword ex-traction.
Our system proved to be successful bothin ranking the phrases in terms of their suitability asindex entries, as well as in determining the optimalnumber of entries to be included in the index.
Fu-ture work will focus on developing methodologiesfor computer-assisted back-of-the-book indexing, aswell as on the use of the automatically extracted in-dexes in improving the browsing of digital libraries.AcknowledgmentsWe are grateful to Kirk Hastings from the Califor-nia Digital Library for his help in obtaining the UCPress corpus.
This research has been partially sup-ported by a grant from Google Inc. and a grant fromthe Texas Advanced Research Program (#003594).939ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networksand ISDN Systems, 30(1?7).A.
Csomai and R. Mihalcea.
2006.
Creating a testbedfor the evaluation of automatically generated back-of-the-book indexes.
In Proceedings of the InternationalConference on Computational Linguistics and Intelli-gent Text Processing, pages 19?25, Mexico City.A.
Csomai and R. Mihalcea.
2007.
Investigations inunsupervised back-of-the-book indexing.
In Proceed-ings of the Florida Artificial Intelligence Research So-ciety, Key West.D.
Feng, J. Kim, E. Shaw, and E. Hovy.
2006.
Towardsmodeling threaded discussions through ontology-based analysis.
In Proceedings of National Confer-ence on Artificial Intelligence.E.
Frank, G. W. Paynter, I. H. Witten, C. Gutwin,and C. G. Nevill-Manning.
1999.
Domain-specifickeyphrase extraction.
In Proceedings of the 16th In-ternational Joint Conference on Artificial Intelligence.M.
E. Funk and C.A.
Reid.
1983.
Indexing consistencyin medline.
Bulletin of the Medical Library Associa-tion, 71(2).A.
Hulth.
2003.
Improved automatic keyword extractiongiven more linguistic knowledge.
In Proceedings ofthe 2003 Conference on Empirical Methods in NaturalLanguage Processing, Japan, August.W.
Kintsch.
1998.
Comprehension: A paradigm for cog-nition.
Cambridge Uniersity Press.M.
Kubat and S. Matwin.
1997.
Addressing the curseof imbalanced training sets: one-sided selection.
InProceedings of the 14th International Conference onMachine Learning.T.
K. Landauer, P. Foltz, and D. Laham.
1998.
Introduc-tion to latent semantic analysis.
Discourse Processes,25.C.
Manning and H. Schutze.
1999.
Foundations of Natu-ral Language Processing.
MIT Press.O.
Medelyan and I. H. Witten.
2006.
Thesaurus basedautomatic keyphrase indexing.
In Proceedings of theJoint Conference on Digital Libraries.M.
Munoz, V. Punyakanok, D. Roth, and D. Zimak.1999.
A learning approach to shallow parsing.
In Pro-ceedings of the Conference on Empirical Methods forNatural Language Processing.P.
Pecina and P. Schlesinger.
2006.
Combining asso-ciation measures for collocation extraction.
In Pro-ceedings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 651?658, Sydney, Australia.G.
Salton and C. Buckley.
1997.
Term weighting ap-proaches in automatic text retrieval.
In Readings inInformation Retrieval.
Morgan Kaufmann Publishers,San Francisco, CA.H.
Schutze.
1998.
The hypertext concordance: a betterback-of-the-book index.
In Proceedings of Comput-erm, pages 101?104.P.
Turney and M. Littman.
2003.
Measuring praise andcriticism: Inference of semantic orientation from as-sociation.
ACM Transactions on Information Systems,4(21):315?346.P.
Turney.
1999.
Learning to extract keyphrases fromtext.
Technical report, National Research Council, In-stitute for Information Technology.G.
Weiss and F. Provost.
2001.
The effect of class distri-bution on classifier learning.
Technical Report ML-TR43, Rutgers University.940
