Mode preference in a s imple data-retr ieval  taskAlexander L RudnickySchool  of  Computer  Science,  Carneg ie  Mel lon  Un ivers i tyP i t t sburgh ,  PA 15213 USAABSTRACTThis paper describes ome recent experiments thatassess user behavior in a multi-modal environmentin which actions can be performed with equivalenteffect in speech, keyboard or scroller modes.
Resultsindicate that users freely choose speech over othermodalities, even when it is less efficient in objectiveterms, such as time-to-completion r input error.INTRODUCTIONMulti-modal systems allow users to both tailor theirinput style to the task at hand and to use input strate-gies that combine several modes in a single trans-action.
As yet no consistent body of knowledge isavailable for predicting user behavior in multi-modalenvironments or to guide the design of multi-modalsystems.
This is particularly true when interfaces in-corporate new technologies such as speech recogni-tion.For activities in a workstation environment, formalcomparisons of speech with other input modes havefailed to demonstrate a clear advantage for speechon conventional aggregate measures of performancesuch as time-to-completion \[1, 8, 4\], despite a con-sistent advantage displayed by speech at the levelof single input operations.
The difference can actu-ally be attributed to the additional incurred costs ofnon-real-time r cognition and error correction.
Whilereal-time performance can he achieved, it is unlikelythat error-free recognition will be available in the nearfuture.
Given these shortcomings, we might ask ifspeech can provide advantages to the user along di-mensions other than task speed, for example by re-ducing the effort needed to generate an input.There is reason to believe that users are quite goodat estimating the response characteristics of an inter-face and can choose an input strategy that optimizessalient aspects of performance, for example decreas-ing time-to-completion or minimizing task error \[5, 9\].By observing the behavior of users in a situation inwhich they can freely choose between different strate-gies, we can gain insight into the factors that governtheir preference for different input styles.A simple data retrieval task was chosen for this study,as the task was one amenable to execution in eachof the three modalities that were examined: speech,keyboard and scroller.
The database contained in-formation about individuals, such as address, tele-phone, etc selected from a list of conference atten-dees.
The task consisted of retrieving the record foran individual and recording the last group of digitsin their work telephone number (typically of lengthfour).
The database contained 225 names for the firstexperiment and was expanded to 240 names for thesecond experiment.SYSTEM IMPLEMENTATIONThe Personal Information Database (PID) compo-nent of the OM system \[3, 7\] served as the databasesystem in this study.
Given a search request specifiedin some combination of first name, last name and affil-iation, PID displays a window with the requested in-formation (in this study, the information consisted ofname, affiliation and all known telephone numbers).If an unknown name was entered, an error panel cameup.
If a query was underspecified, a choice panel con-taining all entries atisfying the query was shown; forexample asking for "Smith" produced a panel show-ing all Smiths in the database.
The existing PID wasaltered to incorporate a scroll window in addition tothe already available keyboard and speech interfaces.The remainder of this section provides detailed de-scriptions for each input mode.Speech InputThe OM system uses a hidden Markov model (I-IMM)recognizer based on Sphinx \[2\] and is capable ofspeaker-independent continuous peech recognition.The subject interacted with the system through a364NeXT computer which provided attention manage-ment \[3\] as well as application-specific displays.
Tooffload computation, the recognition engine ran on aseparate NeXT computer and communicated throughan ethernet connection.
For the 731-word vocab-ulary and perplexity 33 grammar used in the firstexperiment, he system responded in 2.1 times real-time (xRT).
Database retrieval was by a commandphrase such as SHOW ME ALEX RUDNICKY.
While sub-jects were instructed to use this specific phrase, thesystem also understood several variants, such as SHON,GIVE (ME), LIST, etc.
The input protocol was "Pushand Hold", meaning that the user had to depress themouse button before beginning to speak and releaseit after the utterance was complete.
Subjects were in-structed to keep repeating a spoken command in caseof recognition error, until it was processed correctlyand the desired information appeared in the resultwindow.KeyboardSubjects were required to click a field in a windowthen type a name into it, followed by a carriage return(which would drop them to the next field or would ini-tial the retrieval).
Three fields were provided: Firstname, Last Name and Organization.
Subjects wereprovided with some shortcuts: last names were oftenunique and might be sufficient for a retrieval.
Theywere also informed about the use of a wildcard char-acter which would allow then to minimize the num-ber of keystrokes need for a retrieval.
Ambiguoussearch patterns produced a panel of choices; the sub-ject could click on the desired one.Scro l lerThe scroller window displayed the names in thedatabase sorted alphabetically by last name.
Elevennames were visible in the window at any one time,providing approximately 4-5% exposure of the 225name list.
The NeXT scroller provides a handleand two arrow buttons for navigation.
Clicks on thescrollbar move the window to the corresponding po-sition in the text and the arrow buttons can be am-plified to jump by page when a control key is simul-taneously depressed.
Each navigation technique wasdemonstrated to the subject.Session cont ro l le rThe experiment was controlled by a separate processvisible to the subject as a window displaying a nameto look up, a field in which to enter the retrievedinformation and a field containing special instruc-tions such as Please use KEYBOARD only or Useany mode.
The subject progressed through the ex-periment by clicking a button in this window labeledFigure 1:the control program.TO T1 T2.........................!
Reedy Acquke task i Initiateselect mode responsetravelTrial time line, showing events logged byT8 T9 iT10I I!
i i end!
stad i responsei i responsei end appi processingNext; this would display the next name to retrieve.Equidistant from the the Next button were three win-dows corresponding to the three input modes used inthe experiment: voice, keyboard and scroller.
Allmodes required a mouse action to initiate input, ei-ther a click on the speech input button, a click on atext input field or button in the keyboard window orthe (direct) initiation of activity in the scroller.Ins t rumentat ionAll applications were instrumented to generate astream of time-stamped events corresponding to userand system actions.
Figure 1 shows the time linefor a single trial.
In addition to the overall time-line, each mode was also instrumented to generatelogging events corresponding to significant internalevents.
All logged events were time-stamped usingabsolute system time, then merged in analysis to pro-duce a composite timeline corresponding to the entireexperimental session.The merged event stream was processed using a hi-erarchical set of finite-state machines (FSMs).
Fig-ure 2 shows the FSM for a single transaction withthe database retrieval program.
Figures 3 show theFSM for the voice mode.
During the analysis pro-cess, the latter FSM (as well as FSMs for keyboardand scroller) would be invoked within state 1 of thetransaction FSM (Figure 2).
An intermediate l velof analysis (corresponding to conditions) is also usedto simplify analysis.
Arcs in the FSMs correspond toobservable vents, either system outputs or user in-puts.
The products of the analysis include transitionfrequencies for all arcs in an FSM as well as transi-tion times.
The analysis can be treated in terms ofMarkov chains \[6\] to compactly describe recognitionerror, user-mode preferences and other system char-acteristics.USER MODE PREFERENCE IN DATA RE-TR IEVALThe purpose of the first experiment was to establishwhat mode-preference patterns users would displaywhen using the PID system.
To ensure that subjects365Figure 2: FSM for a single transaction.
From theinitial state (0) the subject can click the Next buttonto move to state 1 at which point the subject has aname to look up and can initiate a query.
Queries aredescribed by mode~specific FSMs which are invokedwithin this state.
Figure 3 shows one such FSM.
I fproperly formed, a query will produce a database re-trieval and move the transaction to state 4.
The sub-ject can opt to enter a response, moving the trans-action to state 2 or to repeat queries (by re-enteringstate 1).
At this point, the subject is ready to begin anew trial by transitioning to state O.t_tFigure 3: FSM used for the analysis of voice input.
?0t~fllcnwere equally familiar with each of the input modes,the experiment was divided into two parts (althoughit was run as a single session, without breaks).
Inthe first part, subjects were asked to perform 20 re-trievals using each mode.
Initial testing determinedthat this was sufficient o acquaint he subjects withthe operation of each mode.
In the second part, theywere instructed to use "any mode", with the expec-tation that they would choose on the basis of theirassessment of the suitability of each mode.
A total of55 entries were presented in the second part.The same sequence of 60 entries was used for thefamiliarization stage for all subjects.
However, theorder in which the subject was exposed to the differ-ent modes was counter-balanced according to a Latinsquare.
Three different blocks of test items (each con-taining 55 entries) were used, for a total of nine dif-ferent combinations.Details about the operation of the different modesas well as the experiment controller were explainedto the subject during a practice session prior to theexperiment proper (a total of four practice retrievalswere performed by the subject in this phase).Sub jectsNine subjects participated in this study, 7 male and 2female.
All had had some previous exposure to speechsystems, primarily through their participation in on-going speech data collection efforts conducted by ourresearch group.
This prior exposure nsured that thesubjects were familiar with the mechanics of usinga microphone and of interacting with a computer byvoice.
No attempt was made to select on demographiccharacteristics or on computer skills.
The group con-sisted primarily of students, none of whom howeverwere members of our research group.Results and AnalysisA finite state machine (FSM) description of user be-havior was used to analyze session data.
SeparateFSMs were defined for condition, transaction, se-quence and intra-modal levels and were used to tab-ulate metrics of interest.Table 1 shows the durations of transactions for each ofthe modes during the familiarization phase.
A trans-action is timed from the click on the Next button tothe carriage return terminating the entry of the re-trieved telephone number.
Speech input leads to thelongest transaction times.
Input time measures theduration between the initiation of input and systemresponse (note that these times include recognitiontime, as well as the consequences of mis-recognition,366Table 1: Times (in sec) for the familiarization blocksin the first experiment.ModeScrollerKeyboardVoiceI UtteranceTransaction Input duration13.623 4.917 - -14.526 5.371 - -15.041 5.593 2.464Table 2: User mode choices in the Free block (trials81-"5).Transaction I FirstMode Choice (%) Choice (%)ScrollerKeyboardVoicemixed14.321.848.315.514.722.462.8Table 3: User mode preference in the Free block ofthe second experiment.Transaction Input FilteredMode Choice (%) Choice (%)Scroller 5.8 4.4Keyboard 14.2 11.3Voice 74.9 79.9mixed 5.1 4.4Table 4: Times (in see) for the second experiment(using unfiltered data).
The input time for voice isthe utterance duration.Mode I TransactionStroller 10.863Keyboard 9.560Voice 9.463Input4.3943.0352.078i.e., having to repeat an input).
Here speech is alsoat a disadvantage (though note that the durationof a single utterance is only 2.464 see).
Transac-tion durations for modes are statistically different(F(2, 14) = 5.54, MS~rr = 0.836, p < 0.05), thoughin individual comparisons only voice and scroller dif-fer (p < 0.05, the Neuman-Keuls procedure was usedfor this and all subsequent comparisons).
Order ofpresentation was a significant factor (F(2, 14) = 8.3,p < 0.01), with the first mode encountered requiringthe greatest amount of time.Table 2 shows choice of mode in the Free block.
Themixed mode line refers to cases where subjects wouldfirst attempt a lookup in one mode then switch to an-other (for example because of misrecognition i thespeech mode).
The right-hand column in the tableshows the first mode chosen in a mixed-mode transac-tion.
In this case, voice is preferred 62.8% of the timeas a first choice.
The pattern of choices is statisticallysignificant (F(2, 14) = 6.31,MSerr = 288,p < 0.01),with speech preferred significantly more than eitherkeyboard or scroller(p < 0.05).This experiment suggests that speech is the preferredmode of interaction for the task we examined.
This isparticularly notable since speech is the least efficientof the three modes offered to the user, as measuredin traditional terms uch as time-to-completion.
Mostprevious investigations ( see, e.g.
the review in \[4\])have concentrated on this dimension, treating it asthe single most important criterion for the suitabil-ity of speech input.
The present result suggests thatother aspects of performance may be equally impor-tant to the user.EXTENDED EXPERIENCEOne possible explanation of the above result is thatit's due to a novelty effect.
That is, users displayed apreference for speech input in this task not because ofany inherent preference or benefit but simply becauseit was something new and interesting.
Over time wemight expect he novelty to wear off and users to refo-cus their attention on system response characteristicsand perhaps hift their preference.To test this possibility, we performed a second exper-iment, scaling up the amount of time spent on a taskby different amounts.
Since it was not possible topredict the length of a novelty effect a priori, threeseparate xperience l vels were examined.
A total of9 subjects participated (4 male and 5 female): 3 did720 trials, 3 did 1440 trials and 3 did 2160.
This isin contrast o the 115 trials per subject in the firstexperiment.MethodBased on observations made during the first experi-ment, several changes were made to the system, pri-marily to make the speech and keyboard inputs moreefficient.
Recognition response was improved from 2.1xRT to 1.5 xRT by the use of an IBM 6000/530 com-puter as the recognition engine.
Keyboard entry wasmade more efficient by eliminating the need for theuser to clear entry fields prior to entry.
These changes367resulted in improved transaction times for these twomodes relative to the scroller, which was unchangedexcept for a slight reduction in exposure (this due toan increase of the number of entries to 240, done tofacilitate details of the design).Figure 4: User preference over blocks (filtered data).Note that the spikes at blocks 19 and 3~ are due toeqnipment failure.OO1?
77 0.80.6ScroUer -?--Keyboard -o---o.5?"
' ?o I0 5 10 15 20 25 30 35Block40Resu l ts  and  Ana lys i sThe mean preference for different modes in this ex-periment is shown in Table 3.
Subjects display astrong bias in favor of voice input (74.9%).
Prefer-ence for voice across individual subjects ranged from28% to 91% with all but one subject ($3) showingpreference levels above 70% (the median preferenceis 82.5%).
Differences in mode preference are signifi-cant (F(2, 16) = 34.6, MSerr = 0.037,p < 0.01) andthe preference is greater (p < 0.01) for voice than foreither of the other input modes.Since some of the names in the database were difficultto pronounce, we also tabulated choice data exclud-ing such names.
Nineteen ames (about 8% of thedatabase) were excluded on the basis of ratings pro-vided by subjects.
1 The data thus filtered are shownin Table 3; in this case (for names that subjects werereasonably comfortable about pronouncing) prefer-ence for speech rises to 79.9% (median of 86.1%).1 Part ic ipants  in this exper iment  rated  each name in thedatabase prior to the  exper iment  i self.
A name was presentedto the subject ,  who was asked to  rate on a 4-point scale theirlack of confidence in their  abil ity to pronounce it.
They  thenheard a recording of the name pronounced as expected by therecognizer and  finally ra ted  the degree to which the canonicalpronunciat ion disagreed with their  own expectat ion.
A conser-vative criterion was used to place names  on the exclusion l ist:any name for which both  rat ings averaged over 1.0 (on a 0-3scale) was excluded.Table 4 shows the mean transaction and input timesfor the second experiment, computed over subjects.Compared to the first experiment, hese times arefaster, probably reflecting the greater amount of ex-perience with the task for the second group of sub-jects.
Transaction times are significantly different(F(2,16) = 16.8,MS~rr = 0.327,p < 0.01), withscroller times longer than keyboard or speech times(p < 0.01) which in turn are not different.
If sub-jects were attending to the time necessary to carryout the task, keyboard and voice should have beenchosen with about equal frequency.
The subjects inthis experiment nevertheless chose speech over key-board (and scroller) input.Figure 4 shows preference for voice input over thecourse of the experiment.
Preference for speech in-creases over time, and begins to asymptote at about10-15 blocks (representing about 250 utterances).This phenomenon suggests that speech input, whilehighly appealing to the user requires acertain amountof confidence building, certainly a period of extendedfamiliarization with what is after all a novel inputmode.
Additional investigation would be needed,however, to establish the accuracy of this observation.In any case, this last result underlines the importanceof providing sufficient training.As can be seen in Figure 4 that preference for speechshows no sign of decreasing over time for the durationexamined in this experiment.
Preference for voiceinput appears to be robust.
The 36 block versionof the experiment took on the average 8-9 hours tocomplete, with subjects working up to 2 hours perday.A possible explanation for this finding may be that,rather than basing their choice on overall transactiontime, users focus on simple input time (in both exper-iments voice input is the fastest).
This would implythat users are willing to disregard the cost of recogni-tion error, at least for the error levels associated withthe system under investigation.
Data from followupexperiments not reported here suggest that this maybe the case: increasing the duration of the query ut-terance decreases the preference for speech.CONCLUSIONThe study reported in this paper indicates that usersshow a preference for speech input despite its inade-quacies in terms of classic measures of performance,such as time-to-completion.
Subjects in this studybased their choice of mode on attributes other thantransaction time (quite possibly input time) and werewilling to use speech input even if this meant Spend-368ing a longer time on the task.
This preference ap-pears to persist and even increase with continuinguse, suggesting that preference for speech cannot beattributed to short-term novelty effects.This paper also sketches an analysis technique basedon FSM representations of human-computer interac-tion that permits rapid automatic processing of longevent streams.
The statistical properties of theseevent streams (as characterized by Markov chains)may provide insight into the types of information thatusers themselves compute in the course of developingsatisfactory interaction strategies.\[9\] TEAL, S. L., AND RUDNICKY, A. I.
A perfor-mance model of system delay and user strategyselection.
In Proceedings of CHI (Monterey, CA,May 1992), ACM,  New York, 1992, pp.
295-206.References\[I\] BIERMANN, A. W., FINEMAN, L., AND HEI-DLAGE, J, F. A voice- and touch-driven at-ural language ditor and its performance.
In-ternational Journal of Man-Machine Studies 37(1992), 1-21.\[2\] LEE, K.-F. Automatic Speech Recognition: TheDevelopment of the SPHINX System.
KluwerAcademic Publishers, Boston, 1989.\[3\] LUNATI, J.-M., AND RUDNICKY, A. I.
The de-sign of a spoken language interface.
In Proceedingsof the Third Darpa Speech and Natural LanguageWorkshop (Hidden Valley, June 1990), MorganKaufmann, San Mateo, CA, 1990, pp.
225-229.\[4\] MARTIN, G. The utility of speech input in user-computer interfaces.
International Journal ofMan-Machine Studies P9 (1989), 355-376.\[5\] RUDNICKY, A.
System response delay and userstrategy selection in a spreadsheet task.
CHI'90,invited poster, April 1990.\[6\] RUDNICKY, A. I., AND HAUPTMANN, A. G.Models for evaluating interaction protocols inspeech recognition.
In Proceedings of CHI (NewOrleans, Louisiana, April 1991), ACM, New York,1991, pp.
285-291.\[7\] RUDNICKY, A. I., LUNATI, J.-M., AND FRANZ,A .M.
Spoken language recognition in an of-fice management domain.
Proceedings oflCASSP(May 1991), 829-832.\[8\] RUDNICKY, A. I., SAKAMOTO, M. H., AND PO-LIFRONI, J. H. Spoken language interaction ina spreadsheet task.
In Human-Computer Inter-action - INTERACT'90, D. Diaper et al, Eds.Elsevier, 1990, pp.
767-772.369
