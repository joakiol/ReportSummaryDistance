2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 211?220,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTopical Segmentation: a Study of Human Performanceand a New Measure of QualityAnna KazantsevaSchool of Electrical Engineeringand Computer Science,University of Ottawaankazant@eecs.uottawa.caStan SzpakowiczSchool of Electrical Engineeringand Computer Science,University of Ottawa &Institute of Computer Science,Polish Academy of Sciencesszpak@eecs.uottawa.caAbstractIn a large-scale study of how people find top-ical shifts in written text, 27 annotators wereasked to mark topically continuous segmentsin 20 chapters of a novel.
We analyze the re-sulting corpus for inter-annotator agreementand examine disagreement patterns.
The re-sults suggest that, while the overall agree-ment is relatively low, the annotators showhigh agreement on a subset of topical breaks?
places where most prominent topic shiftsoccur.
We recommend taking into accountthe prominence of topical shifts when evalu-ating topical segmentation, effectively penal-izing more severely the errors on more impor-tant breaks.
We propose to account for this in asimple modification of the windowDiff metric.We discuss the experimental results of evaluat-ing several topical segmenters with and with-out considering the importance of the individ-ual breaks, and emphasize the more insightfulnature of the latter analysis.1 IntroductionTopical segmentation is a useful intermediate stepin many high-level NLP applications such as in-formation retrieval, automatic summarization andquestion answering.
It is often necessary to split along document into topically continuous segments.Segmentation may be particularly beneficial whenworking with documents without overt structure:speech transcripts (Malioutov and Barzilay, 2006),newswire (Misra et al, 2011) or novels (Kazantsevaand Szpakowicz, 2011).
The customary approachis to cast text segmentation as a binary problem: isthere a shift of topic between any two adjacent tex-tual units (e.g., sentences or paragraphs)?
Whilenecessary, this simplification is quite crude.
Topic indiscourse usually changes continually; some shiftsare subtle, others ?
more prominent.The evaluation of text segmentation remains anopen research problem.
It is a tradition to compile agold-standard segmentation reference using one ormore annotations created by humans.
If an auto-matic segmenter agrees with the reference, it is re-warded, otherwise it is penalized (see Section 4 fordetails).
The nature of the task, however, is such thatcreating and applying a reference segmentation is farfrom trivial.
The identification of topical shifts re-quires discretization of a continuous concept ?
howmuch the topic changes between two adjacent units.That is why annotators often operate at different lev-els of granularity.
Some people mark only the mostprominent topic fluctuations, while others also in-clude finer changes.
The task is also necessarilyunder-defined.
In addition to topic changes per se,annotators effectively must classify some rhetoricaland pragmatic phenomena ?
exactly how much it isdepends on the document genre.
For simplicity wedo not directly address the latter problem here; weconcentrate on the former.To study how people identify topical shifts inwritten text, we asked 27 annotators to segment intoepisodes 20 chapters of the novel The Moonstoneby Wilkie Collins.
Each chapter was annotated by4-6 people.
An episode roughly corresponds to atopically continuous segment ?
the term is defined211in Section 3.
The analysis of the resulting corpusreveals that while the overall inter-annotator agree-ment is quite low and is not uniform throughout eachchapter.
Some topical shifts are marked by most orall annotators, others ?
by one or by a minority.
Infact, only about 50% of all annotated topical shiftsare supported by at least 50% of annotators (includ-ing near-hits), while the other half is only marked bya minority.
In this work we take the agreement abouta certain topical shift as a measure of its prominence,and show how this measure can be simply utilizedfor the purpose of evaluation.The main claim of this paper is perhaps the fol-lowing: when evaluating the performance of auto-matic segmenters, it is important to consider notonly the overall similarity between human and ma-chine segmentations, but also to examine the regionsof disagreement.
When a program misses or mis-places a prominent topic shift ?
a segment bound-ary marked by all annotators ?
it should be penal-ized more than if it was mistaken about a boundarymarked by one person.
Similarly, a false positivein the region where none of the annotators found achange in topic is worse than a boundary inserted ina place where at least one person perceived a topicchange.
We suggest that it is important to use allavailable reference segmentations instead of com-piling them into a single gold standard.
We showhow a small modification to the popular windowD-iff (Pevzner and Hearst, 2002) metric can allow con-sidering multiple annotations at once.To demonstrate the increased interpretive powerof such evaluation we run and evaluate several state-of-the art segmenters on the corpus described in thiswork.
We evaluate their performance first in a con-ventional manner ?
by combining all available ref-erences into one ?
and then by using the proposedmodification.
Comparing the results suggests thatthe information provided by this method differs fromwhat existing methods provide.Section 2 gives a brief background on text seg-mentation.
Section 3 describes the corpus and howit was collected.
Section 4 contain quantitative andqualitative analysis of the corpus and its interpreta-tions.
Section 5 proposes a modified version of win-dowDiff and motivates it.
Section 6 compares eval-uation of three segmenters in several different ways.Section 7 contains the conclusions and outlines di-rections for future work.2 Background and Related WorkThe goal of topical text segmentation is to identifysegments within which the topic under discussionremains relatively constant.
A flip-side of this def-inition is identifying topic shifts ?
places where thetopic shifts significantly or abruptly.
In the contextof this paper we allow ourselves to use these two def-initions interchangeably, sometimes talking aboutidentifying topic shifts, at other times ?
about identi-fying topically continuous segments.
While the the-oretical correctness of such usage remains question-able, it is sufficient for the purpose of our discussion,and it is in line with the literature on the topic.There is a number of corpora annotated for thepresence of topical shifts by one or more annotators.Passonneau and Litman (1997) describe an experi-ment where seven untrained annotators were askedto find discourse segments in a corpus of transcribednarratives about a movie.
While the authors showthat the agreement is significant, they also note thatpeople include segment boundaries at different rates.Gruenstein, Niekrasz, and Purver (2005) describethe process of annotating parts of two corpora ofmeeting transcripts: ICSI (Janin et al, 2003) andISL (Burger, MacLaren, and Yu, 2002).
Two peo-ple annotated the texts at two levels: major and mi-nor, corresponding to the more and less importanttopic shifts.
Topical shifts were to be annotated soas to allow an outsider to glance at the transcriptand get the gist of what she missed.
Not unlikeour work, the authors report rather low overall inter-annotator agreement.
Galley et al (2003) also com-piled a layer of annotation for topical shifts for partof the ICSI corpus, using a somewhat different pro-cedure with three annotators.
Malioutov and Barzi-lay (2006) created a corpus of course lectures seg-mented by four annotators, noting that the annota-tors operated at different levels of granularity.
Inthese three projects, manual annotations were com-piled into a single gold standard reference for use inevaluating and fine-tuning automatic segmenters.The work described in this paper is different inseveral ways.
To the best of our knowledge, this is212the first attempt to annotate literary texts for topicalshifts.
Because we collected relatively many anno-tations for each chapter (four to six), we can makesome generalizations as to the nature of the process.In addition to compiling and describing the corpus,we analyze disagreement patterns between annota-tors.
We claim that even though the annotators maynot agree on granularity, they do agree at some level,at least with respect to most prominent breaks.
Wepropose that instead of compiling a single referencefrom multiple annotations it may be more useful toevaluate automatic segmenters against several anno-tations at once.
We will show how to do that.3 The Overview of the CorpusOur current work on text segmentation is part of alarger project on automatic summarization of fic-tion, which is why we chose a XIX century novel,The Moonstone by Wilkie Collins, as the text tobe annotated.
We used two chapters for a pilotstudy and then another 20 for the large-scale experi-ment.
The annotators worked with individual chap-ters and were required to align segment boundarieswith paragraph breaks.Objectives.
The main question behind this studywas this: ?How do people identify topical shifts inliterature??
This vague question can be mapped toseveral more specific objectives.
First, we soughtto verify that topical segmentation of literature wasa sensible task from the viewpoint of an untrainedannotator.
Next, it was important to examine inter-annotator agreement to make sure that the annota-tors in fact worked on the same phenomena and thatthe resulting corpus is a reasonable approximation ofhow people segment literature in general.
Third, inaddition to analyzing the overall agreement we alsotook a close look at the type of common disagree-ments, in search of patterns and insights to evaluateautomatic segmenters.Subjects.
The participants were undergraduatestudents of an English department at the Universityof Ottawa, recruited by email.
They received $50each for their participation.
Everyone had to anno-tate four chapters from The Moonstone, not neces-sarily consecutive ones.
The chapters were dividedso as to ensure an approximately equal workload.We had planned six independent annotations foreach chapter of the novel.1 The annotators were di-vided into five groups, each group asked to read andannotate four distinct chapters.
In the end we hadthree groups with six people, one group with fiveand one group with four.Procedure.
The experiment was conducted re-motely.
The students received email packages withdetailed instructions and an example of a segmentedchapter from a different novel.
They had two weeksto annotate the first two chapters and then two moreweeks to annotate another two chapters.The annotators were instructed to read each chap-ter and split it into episodes ?
topically continuousspans of text demarcated by the most perceptibleshifts of topic in the chapter.
We asked the anno-tators to provide a brief one-sentence description ofeach episode, effectively creating a chapter outline.The students were also asked to record places theyfound challenging and to note the time it takes tocomplete the task.Because even short chapters of most traditionalnovels are rather lengthy, we chose to use paragraphsas the basic unit of annotation (sentences are morecommon in text segmentation literature).4 Corpus AnalysisTime.
On average, an annotator required 137.9 min-utes to complete both tasks.
The standard devia-tion was ?
= 98.32 minutes appropriately reflectingthe fact that some students are very fast readers andbesides have already read the novel in one of theirclasses, while others are quite slow.The average chapter has 53.85 paragraphs (?
=29.31), the average segment length across all anno-tators is 9.25 paragraphs (?
= 9.77).
On average theannotators identified 5.80 episodes (?
= 2.45) perchapter.
Figure 1 shows the distribution of the num-ber of segments identified in each chapter.
An indi-vidual box plot is compiled using all available anno-tations for that chapter ?
six for most, four or fivefor several.
The data are plotted for individual chap-ters, so the only source of variance is the disagree-ment between annotators as to what is the appropri-ate level of detail for the task.
Figure 1 confirms1We hired 30 students.
Three did not complete the task.213Figure 1: Distribution of segment counts across chapters.other researchers?
findings: people find topical shiftsat different levels of granularity (Malioutov andBarzilay, 2006; Gruenstein, Niekrasz, and Purver,2005).
We take this investigation further and explorewhether there are patterns to this disagreement andhow they can be interpreted and leveraged.4.1 Inter-annotator AgreementIn order to make sure that our guidelines are suffi-ciently clear and the annotators in fact annotate thesame phenomenon, it is important to measure inter-annotator agreement (Artstein and Poesio, 2008).This is particularly important given the fact that theresulting corpus is intended as a benchmark datasetfor evaluation of automatic segmenters.When looking at inter-annotator agreement inde-pendently of the domain, the most commonly usedmetrics are coefficients of agreement ?
?
(Krippen-dorff, 2004), ?
(Cohen, 1960; Shrout and Fleiss,1979), pi (Scott, 1955) and several others.
In thiswork we use a multi-annotator version of pi, alsoknown in the CL community as Fleiss?s ?
(Shroutand Fleiss, 1979; Siegel and Castellan, 1988) .Fleiss?s ?
is computed as follows:?
=Agreementobserved ?
Agreementexpected1?
Agreementexpected(1)Agreementobserved =1ic(c?
1)Xi?IXk?Knik(nik ?
1) (2)Agreementexpected =1(ic)2Xk?Kn2k (3)where i is the number of items to be classified in setI, k is the number of available categories in set K, c isthe number of annotators, nik is the number of anno-tators who assign item i to category k, nk is the totalnumber of items assigned to category k by all anno-tators (Artstein and Poesio, 2008, pp.
562-563).
Ef-fectively ?measures how much the annotators agreeabove what can be expected by chance.
The valueof ?
is 0 where there is no agreement above chanceand 1 where the annotators agree completely.While we report ?
values for our dataset, it isimportant to note that ?
is ill-suited to measuringagreement in segmentation.
The main problem is itsinsensitivity to near-hits.
When asked to segmenta document, the annotators often disagree about theexact placement of the boundary but agree that thereis a boundary somewhere in the region (e.g., con-sider paragraphs 9-11 in segmentations in Figure 2).It is desirable to give partial credit to such near-hitsinstead of dismissing them as utter disagreement.This cannot be achieved with ?.
The second prob-lem is the independence assumption: the label foreach item must be independent from the labels of allother items.
In our case, this would amount to claim-ing, highly unrealistically, that the probability of atopical shift between two sentences is independentof the topical landscape of the rest of the document.Two other commonly used agreement metrics arePk (Beeferman, Berger, and Lafferty, 1999) and win-dowDiff (Pevzner and Hearst, 2002), both designedto compare a hypothetical segmentation to a refer-ence, not to measure agreement per se.
A com-mon feature of both metrics is that they award partialcredit to near-hits by sliding a fixed-length windowthrough the sequence and comparing the referencesegmentation and hypothetical segmentation at eachwindow position.
The window size is generally setat half the average segment length.Pk (Equation 4) measures the probability that twounits randomly drawn from a document are correctlyclassified as belonging to the same topical segment.Pk has been criticized for penalizing false negativesless than false positives and for being altogether in-sensitive to certain types of error; see (Pevzner and214Hearst, 2002, pp.
22-26) for details.
Despite itsshortcomings, Pk is widely used.
We report it forcomparison with other corpora.Pk(ref, hyp) =X1?i?j?nD(i, j)(?ref (i, j) XNOR ?hyp(i, j))(4)Functions ?hyp and ?ref indicate whether the twosegment endpoints i and j belong to the same seg-ment in the hypothetical segmentation and referencesegmentation respectively.windowDiff was designed to remedy some of Pk?sshortcomings.
It counts erroneous windows in thehypothetical sequence normalized by the total num-ber of windows.
A window is judged erroneous ifthe boundary counts in the reference segmentationand hypothetical segmentation differ; that is (|ref -hyp| 6= 0) in Equation 5).winDiff =1N ?
kN?kXi=1(|ref ?
hyp| 6= 0) (5)Both Pk and windowDiff produce penalty scores be-tween 0 and 1, with 1 corresponding to all windowsbeing in error, and 0 ?
to a perfect segmentation.Table 1 reports Pk, windowDiff and ?
values forour corpus.
Pk and windowDiff are computed pair-wise for all annotators within one group and thenaveraged.
We set the window size to half the aver-age segment length as measured across all annota-tors who worked on a given chapter.
The values arecomputed for each group separately; Table 1 showsthe averages across five groups.Even by most relaxed standards, e.g., (Landis andKoch, 1977), the ?
value of 0.38 corresponds to lowagreement.
This is not surprising, since it only in-cludes the cases when the annotators agree exactlywhere the boundary should be.
For the purpose ofour task, such a definition is too strict.The values of windowDiff and Pk are more rea-sonable; windowDiff = 0.34 means that on aver-age a pair of annotators disagrees on 34% of win-dows.
windowDiff was originally designed to com-pare only two segmentations.
Our strategy of com-puting its values pairwise is perhaps not optimal butin the absence of another metric allowing to accountfor near-hits we are practically forced to use it as aprimary means of inter-annotator agreement.Table 1: Overview of inter-annotator agreement.Mean Std.
dev.?
0.29 0.15Pk 0.33 0.17windowDiff 0.38 0.09Figure 2: Example segmentation for Chapter 1.4.2 Patterns of DisagreementFigure 2 shows the segmentation of the shortestchapter in the dataset.
The overall agreement isquite low (windowDiff=0.38, ?
= 0.28).
This is notsurprising, since annotators 1 and 3 found two seg-ments, annotator 3 ?
five segments, and annotator 4?
four.
Yet al annotators agree on certain things: ev-eryone found that there was a significant change oftopic between paragraphs 9 and 11 (though they dis-agree on its exact placement).
It is therefore likelythat the topical shift between paragraphs 9 and 11 isquite prominent.
Annotators 2 and 4 chose to placea segment boundary after paragraph 2, while anno-tators 1 and 3 did not place one there.
It is likely thatthe topical shift occurring there is less prominent, al-though perceptible.
According to these annotations,the least perceptible topic shifts in the chapter oc-cur after paragraph 4 (marked only by annotator 2)and possibly after paragraph 11 (marked only by an-notator 1).
Overall, glancing at these segmentationssuggests that there is a prominent topical shift be-tween paragraphs 9-11, three significant ones (after2, 10 and 12) and several minor fluctuations (after 3and possibly after 10 and 11).Looking at the segmentations in Figure 2 it seemslikely that the disagreements between annotators 2and 4 are due to granularity, while the annotators1 and three disagree more fundamentally on wherethe topic changes.
When measuring agreement, wewould like to be able to distinguish between dis-215Figure 3: Quality of segment boundaries.agreements due to granularity and disagreementsdue to true lack of agreement (annotator 1 and 3).We would also like to leverage this information forthe evaluation of automatic segmenters.Distinguishing between true disagreement anddifferent granularity while taking into account near-hits is not trivial, especially since we are workingwith multiple annotations simultaneously and thereis no one correct segmentation.In order to estimate the quality of individualboundaries and look inside the segmented sequence,we approximate the quality of each suggested seg-ment boundary by the percentage of annotators whomarked it.
Since the annotators may disagree on theexact placement of the boundaries, our measurementmust be relaxed to allow for near-hits.Figure 3 shows the distribution of segment bound-aries using three different standards of quality.
Weconsider all segment boundaries introduced by atleast one annotator.
Then, for each suggested bound-ary we compute how much support there is frompeer annotators: what percentage of annotators in-cluded this boundary in their segmentation.
The left-most box plot in Figure 3 corresponds to the moststrict standard.
When computing support we onlyconsider perfect matches: segment boundaries spec-ified in exactly the same location (window size =0).
The middle box plot is more relaxed: we con-sider boundaries found within half of a windowD-iff window size of the boundary under inspection.The rightmost box plot corresponds to the inclusionof boundaries found within a full windowDiff win-dow size of the boundary under inspection.Looking at exact matches (the leftmost box plot),we observe that at least a half of segment bound-aries were specified by less than 25% of annotators(which corresponds to one person).
It explains why?
values in Table 1 are so low: this is the only sortof agreement ?
captures.
Also one can notice thatat most 25% of the boundaries have the support ofmore than 50% of the annotators.The picture changes if we consider all boundarieswithin a tight window around the candidate bound-ary (the middle box plot).
This standard is twiceas strict as the regular windowDiff evaluation.
Here50% of all boundaries are marked by at least 35% atand most 80% of annotators.
Only 25% of bound-aries are marked by less than 30% of the annotators.The rightmost plot looks even better.
If we con-sider the support found within a window size of anycandidate boundary, then 50% of all boundaries aresupported by over 70% of annotators.
However, wefind this way of measuring support too optimistic.The reason is, again, the difference in the granu-larity of segmentations.
The window size used forthese measurements is based on the average segmentlength across all annotations.
For example, the aver-age segment length for segmentation shown in Fig-ure 2 is 4, making the window size 2.
This size istoo relaxed for annotators 2 and 3, who were verydetailed.
Due to the excessively large window therewill almost always be a boundary where fine-grainedannotations are concerned, but those boundaries willnot correspond to the same phenomena.
That is whywe think that a stricter standard is generally moreappropriate.
This is especially the case since wework with paragraphs, not sentences.
A distance of2-3 sentences is quite tolerable, but a distance of 2-3paragraphs is considerable, and it is far more likelythat a stricter notion of near-hits must be considered.5 Proposed Modification to windowDiffWindowDiff compares two segmentations by takinginto account near-hits ?
penalizing them proportion-ally to how far a hypothetical segment boundary is216from a reference boundary.
Section 4.2 argued thatsome boundaries are more prominent.
We aim tomodify windowDiff so the prominence of the bound-aries matters in evaluating automatic segmenters.Recall that to compute windowDiff we slide awindow through the reference and the hypotheti-cal segmentation and check whether the number ofboundaries is equal at each window position.
Thenumber of erroneous windows is then normalized:winDiff =1N ?
kN?kXi=1(|refi ?
hypi| 6= 0) (6)refi and hypoi are the counts of boundaries in agiven window in the reference and the hypotheticalsequence, N is the length of the complete sequence,k is the window size (so there are N - k windows).The prominence of a boundary can be approxi-mated by how many annotators specified it in theirsegmentations.
One simple way to take prominenceinto account is to slide a window through all avail-able segmentations, not just one.
A straighforwardmodification to equation (6) achieves that:winDiff ?
=1h(N ?m)hXa=1N?mXi=1(|refai ?
hypi| 6= 0) (7)A is the set of all available annotations and h istheir total number.
Effectively, for each position ofthe window the hypothetical output is penalized asmany times as there are reference annotations withwhich it disagrees.
Note that the window size m isdifferent from that used for pair-wise comparisons.Following the convention, we recommend setting itto half of the size of an average segment length (av-eraged over all available references).
The size ofthe window effectively specifies a tolerance thresh-old for what is an acceptable near-hit (as opposed toa plain miss), and can be modified accordingly.windowDiff and Pk range from 0 to 1, with 0corresponding to an ideal segmentation.
The upperand lower bounds for Equation 7 are different anddepend on how much the reference segmentationsagree between themselves.22We find that the upper bound corresponds to the worst-case,and the lower bound to the best-case scenario.
To avoid confu-sion, we talk of the best-case bound and the worst-case bound.Let us refer to the most popular opinion for agiven position of the window as the majority opin-ion.
Then, for each window, the smallest possiblepenalty is assigned if the hypothetical segmentationcorrectly ?guesses?
the majority opinion (the win-dow then receives a penalty equal to the number ofannotators disagreeing with the majority opinion):best case =1N ?mN?mXi=1(h?majority support) (8)Here majority support is the number of annota-tors who support the most frequent opinion.Conversely, to merit the highest penalty, a hypo-thetical segmentation must ?guess?
the least popu-lar opinion (possibly an opinion not supported byany annotators) at each window position.
In Equa-tion 9, unpopular support is the number of anno-tators who agree with the least popular opinion.worst case =1N ?mN?mXi=1(h?
unpopular support) (9)In order to have a multi-annotator version of win-dowDiff interpretable within the familiar [0, 1] in-terval, we normalize Equation 7:multWinDiff =(Pha=1PN?mi=1 (|refa ?
hyp| 6= 0))?
best caseh(N ?m)(worst case?
best case)(10)The best and the worst-case bounds serve as indi-cators of how much agreement there can be betweenreference segmentations and so as indicators of howdifficult to segment a given document is.The multWinDiff metric in Equation 10 has thesame desirable properties as the original metric,namely it takes into account near hits and penal-izes according to how far the reference and hypo-thetical boundaries are.
Additionally, for each win-dow position it takes into account how much a hy-pothetical segmentation is similar to all available an-notations, thus penalizing mistakes according to theprominence of boundaries (or to the certainty thatthere are no boundaries).33Java code to compute multWinDiff is available as a part ofthe APS segmenter.
The corpus and the software can be down-loaded at ?www.eecs.uottawa.ca/?ankazant?.2176 ExperimentsIn order to illustrate why using a single gold-standard reference segmentation can be problem-atic, we evaluate three publicly available seg-menters, MinCutSeg (Malioutov and Barzilay,2006), BayesSeg (Eisenstein and Barzilay, 2008)and APS (Kazantseva and Szpakowicz, 2011), us-ing several different gold standards and then usingall available annotations.
The corpus used for eval-uation is The Moonstone corpus described in Sec-tions 3-4.
We withheld the first four chapters for de-velopment and used the remaining 16 for testing.
Wealso compared the segmenters to a random baselinewhich consisted of randomly selecting a number ofboundaries equal to the average number of segmentsacross all available annotations.None of the segmenters requires training in theconventional sense, but APS and MinCutSeg seg-meters come with scripts allowing to fine-tune sev-eral parameters.
We selected the best parameters forthese two segmenters using the first four chapters ofthe corpus.
BayesSeg segmeter, a probabilistic seg-menter, does not require setting any parameters.Table 2 sums up the results.
Each row corre-sponds to one reference segmentation and metric ?regular windowDiff in the first six rows.
We com-piled several flavours of consensus reference seg-mentations: 1) all boundaries marked by ?
50% ofthe annotators (windowDiff ?
50%), 2) all boundariesmarked by ?
30% of the annotators (windowDiff ?30%), 3) all boundaries marked by at least one an-notator (windowDiff union).
To illustrate why com-paring against a single annotation is unreliable, wereport comparisons against three single-person an-notations (windowDiff annotator 1, 4, 2).
multWinDiffis the proposed multi-annotator version from Equa-tion 10.
The best-case bound for multWinDiff is 0.21and the worst-case bound is 1.0.Each segmenter produced just one segmentation,so the numbers in the Table 2 differ only dependingon the mode of evaluation.
The cells are coloured.The lightest shade correspond to the best perfor-mance, darker shades ?
to poorer performance.
Theactual values for the first six rows are rather low, butwhat is more bothersome is the lack of consistencyin the ranking of segmenters.
Only the random base-APS Bayes MinCut Rand.windowDiff?50%0.60.
0.66 0.73 0.73windowDiff?30%0.61 0.52 0.69 0.61windowDiffunion0.6 0.53 0.63 0.65windowDiffannotator 10.66 0.57 0.74 0.76windowDiffannotator 40.62 0.7 0.69 0.74windowDiffannotator 20.61 0.6 0.66 0.69multWinDiff 0.23 0.28 0.31 0.41Table 2: The three segmenters and a random baselinecompared using different references for computing win-dowDiff.
windowDiff ?50%: the gold standard consistsof all boundaries specified by at least 50% of the anno-tators; windowDiff ?30%: all boundaries specified byat least 30% of the annotators; windowDiff union: allboundaries specified by at least one person; windowD-iff annotator a: comparisons against individual annota-tors.
multWinDiff is multi-annotator windowDiff fromequation (10).line remains the worst in most cases.
The APS andBayesSeg segmenters tend to appear better than theMinCutSeg but it is not always the case and the rank-ings among the three are not consistent.The last row reports multi-annotator windowD-iff which takes into account all available referencesand also the best-case and the worst-case bounds.
Inprinciple, there is no way to prove that the metric isbetter than using windowDiff and a single referenceannotation.
It does, however, take into account allavailable information and provides a different, if notunambiguously more true, picture of the compara-tive performance of automatic segmenters.7 Conclusions and Future WorkWe have described a new corpus which can be usedin research on topical segmentation.
The corpus iscompiled for fiction, a genre for which no such cor-pus exists.
It contains a reasonable number of anno-tations per chapter to allow an in-depth analysis oftopical segmentation as performed by humans.218Our analysis of the corpus confirms the hypothe-sis that when asked to find topical segments, peopleoperate at different levels of granularity.
We showthat only a small percentage of segment boundariesis agreed upon by all or almost all annotators.
If,however, near-hits are considered, suggested seg-ment boundaries can be ranked by their prominenceusing the information about how many people in-clude each boundary in their annotation.We propose a simple modification to windowD-iff which allows for taking into account more thanone reference segmentation, and thus rewards or pe-nalizes the output of automatic segmenters by con-sidering the severity of their mistakes.
The proposedmetric is not trouble-free.
It is a window-based met-ric so its value depends on the choice of the windowsize.
While it has become a convention to set thewindow size to half of the average segment length inthe reference segmentation, it is not obvious that thesame logic applies in case of multi-annotator win-dowDiff.
The metric also hides whether false posi-tives or false negatives are the main source of error.All these shortcomings notwithstanding, the met-ric offers an advantage of allowing the evaluation ofhypothetical segmentations with more subtlety thanthose using a single gold standard reference.
Whenusing regular windowDiff and a single reference seg-mentation, one is restricted to an evaluation basedon binary comparisons: whether a given hypothet-ical boundary is similar to the gold standard seg-mentation (e.g., the majority opinion).
Divergentsegmentations are penalized even if they are simi-lar to minority opinions (and thus feasible, thoughmaybe less likely) or if they are completely differentfrom anything created by humans (and thus proba-bly genuinely erroneous).
Our version of windowD-iff, however, takes into account multiple annotationsand gives partial reward to segmentations based onhow similar there are to any human segmentation,not just the majority opinion (while giving prefer-ence to high agreement with the majority opinion).To evaluate the output of topical segmenters ishard.
There is disagreement between the annota-tors about the appropriate level of granularity andabout the exact placement of segment boundaries.The task itself is also a little vague.
Just as it isthe case in automatic text summarization, generationand other advanced NLP tasks, there is no single cor-rect answer and the goal of a good evaluation met-ric is to reward plausible hypotheses and to penalizeimprobable ones.
It is quite possible that a bettermetric than the one proposed here can be devised;see, for example, (Fournier and Inkpen, 2012)(Sca-iano and Inkpen, 2012).
We feel, however, that anyreliable metric for evaluating segmentations must ?in one manner or another ?
take into account morethan one annotation and the prominence of segmentbreaks.AcknowledgmentsWe thank William Klement and Chris Fournier forcommenting on an early draft of this paper.
The firstauthor also thanks Chris Fournier and Martin Sca-iano for insightful discussions about the evaluationof topical segmenters.
This work is partially fundedby the National Sciences and Engineering ResearchCouncil of Canada and by the Ontario GraduateScholarship program.ReferencesArtstein, Ron and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Computa-tional Linguistics, 34(4):555?596.Beeferman, Doug, Adam Berger, and John Lafferty.1999.
Statistical Models for Text Segmentation.
Ma-chine Learning, 34:177?210, February.Burger, Susanne, Victoria MacLaren, and Hua Yu.
2002.The ISL meeting corpus: the impact of meeting typeon speech style.
In INTERSPEECH?02.Cohen, Jacob.
1960.
A coefficient of agreement fornominal scales .
Educational and Psychological Mea-surement, 20:37?46.Eisenstein, Jacob and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 334?343, Honolulu,Hawaii, October.Fournier, Chris and Diana Inkpen.
2012.
Segmentationsimilarity and agreement.
In Proceedings of NAACL-HLT 2012 (this volume), Montre?al, Canada, June.Galley, Michel, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentationof multi-party conversation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 562?219569, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Gruenstein, Alexander, John Niekrasz, and MatthewPurver.
2005.
Meeting Structure Annotation: Dataand Tools.
In In Proceedings of the SIGdial Workshopon Discourse and Dialogue, pages 117?127.Janin, Adam, Don Baron, Jane Edwards, D. Ellis,David Gelbart, Nelson Morgan, Barbara Peskin, ThiloPfau, Elizabeth Shriberg, A.ndreas Stolcke, and ChuckWooters.
2003.
The ICSI Meeting Corpus.
InProceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP-03), volume 1, pages 364?367, April.Kazantseva, Anna and Stan Szpakowicz.
2011.
Lin-ear Text Segmentation Using Affinity Propagation.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 284?293, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.Krippendorff, Klaus.
2004.
Content Analysis.
An Intro-duction to Its Methodology.
Sage Publications.Landis, J. Richards and Garry G. Koch.
1977.
The Mea-surement of Observer Agreement for Categorical Data.Biometrics, 33(1):159?174.Malioutov, Igor and Regina Barzilay.
2006.
MinimumCut Model for Spoken Lecture Segmentation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages 25?32, Sydney, Australia, July.Misra, Hemant, Franc?ois Yvon, Olivier Cappe?, and Joe-mon M. Jose.
2011.
Text segmentation: A topic mod-eling perspective.
Information Processing and Man-agement, 47(4):528?544.Passonneau, Rebecca J. and Diane J. Litman.
1997.
Dis-course segmentation by human and automated means.Computational Linguistics, 23(1):103?139, March.Pevzner, Lev and Marti A. Hearst.
2002.
A Critique andImprovement of an Evaluation Metric for Text Seg-mentation.
Computational Linguistics, 28(1):19?36.Scaiano, Martin and Diana Inkpen.
2012.
Gettingmore from segmentation evaluation.
In Proceedingsof NAACL-HLT 2012 (this volume), Montre?al, Canada,June.Scott, William.
1955.
Reliability of content analysis:The case of nominal scale coding.
Public OpinionQuaterly, 19(3):321?325.Shrout, Patrick E. and Joseph L. Fleiss.
1979.
Intraclasscorrelations: uses in assessing rater reliability.
Psy-chological Bulletin, 86(2):420?428.Siegel, Sidney and John.
N. Jr. Castellan.
1988.
Non-parametric statistics for the behavioral sciences.
Mc-Graw Hill, Boston, MA.220
