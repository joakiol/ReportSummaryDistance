Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?189,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsActive Learning for Post-Editing Based Incrementally Retrained MTAswarth Dara Josef van Genabith Qun Liu John Judge Antonio ToralSchool of ComputingDublin City UniversityDublin, Ireland{adara,josef,qliu,jjudge,atoral}@computing.dcu.ieAbstractMachine translation, in particular statis-tical machine translation (SMT), is mak-ing big inroads into the localisation andtranslation industry.
In typical work-flows (S)MT output is checked and (whererequired) manually post-edited by hu-man translators.
Recently, a significantamount of research has concentrated oncapturing human post-editing outputs asearly as possible to incrementally up-date/modify SMT models to avoid repeatmistakes.
Typically in these approaches,MT and post-edits happen sequentiallyand chronologically, following the wayunseen data (the translation job) is pre-sented.
In this paper, we add to the ex-isting literature addressing the questionwhether and if so, to what extent, thisprocess can be improved upon by ActiveLearning, where input is not presentedchronologically but dynamically selectedaccording to criteria that maximise perfor-mance with respect to (whatever is) the re-maining data.
We explore novel (sourceside-only) selection criteria and show per-formance increases of 0.67-2.65 pointsTER absolute on average on typical indus-try data sets compared to sequential PE-based incrementally retrained SMT.1 Introduction and Related ResearchMachine Translation (MT) has evolved dramati-cally over the last two decades, especially sincethe appearance of statistical approaches (Brown etal., 1993).
In fact, MT is nowadays succesfullyused in the localisation and translation industry,as for many relevant domains such as technicaldocumentation, post-editing (PE) of MT output byhuman translators (compared to human translationfrom scratch) results in notable productivity gains,as a number of industry studies have shown con-vincingly, e.g.
(Plitt and Masselot, 2010).
Fur-thermore, incremental retraining and update tech-niques (Bertoldi et al., 2013; Levenberg et al.,2010; Mathur et al., 2013; Simard and Foster,2013) allow these PEs to be fed back into the MTmodel, resulting in an MT system that is contin-uously updated to perform better on forthcomingsentences, which should lead to a further increasein productivity.Typically, post-editors are presented with MToutput units (sentences) in the order in which inputsentences appear one after the other in the trans-lation job.
Because of this, incremental MT re-training and update models based on PE outputsalso proceed in the same chronological order de-termined by the input data.
This may be sub-optimal.
In this paper we study the application ofActive Learning (AL) to the scenario of PE MTand subsequent PE-based incremental retraining.AL selects data (here translation inputs and theirMT outputs for PE) according to criteria that max-imise performance with respect to the remainingdata and may diverge from processing data itemsin chronological order.
This may allow incremen-tally PE-based retrained MT to (i) improve morerapidly than chronologically PE-based retrainedMT and (ii) result in overall productivity gains.The main contributions of this paper include:?
Previous work (Haffari et al., 2009; Blood-good and Callison-Burch, 2010) shows that,given a (static) training set, AL can im-prove the quality of MT.
By contrast, herewe show that AL-based data selection for hu-man PE improves incrementally and dynami-cally retrained MT, reducing overall PE timeof translation jobs in the localisation industryapplication scenarios.?
We propose novel selection criteria for AL-based PE: we adapt cross-entropy difference(Moore and Lewis, 2010; Axelrod et al.,2011), originally used for domain adaptation,and propose an extension to cross entropydifference with a vocabulary saturation filter(Lewis and Eetemadi, 2013).?
While much of previous work concentrateson research datasets (e.g.
Europarl, NewsCommentary), we use industry data (techni-185cal manuals).
(Bertoldi et al., 2013) showsthat the repetition rate of news is consider-ably lower than that of technical documenta-tion, which impacts on the results obtainedwith incremental retraining.?
Unlike in previous research, our AL-basedselection criteria take into account only thesource side of the data.
This supports se-lection before translation, keeping costs to aminimum, a priority in commercial PE MTapplications.?
Our experiments show that AL-based selec-tion works for PE-based incrementally re-trained MT with overall performance gainsaround 0.67 to 2.65 TER absolute on average.AL has been successfully applied to many tasksin natural language processing, including pars-ing (Tang et al., 2002), named entity recogni-tion (Miller et al., 2004), to mention just a few.
See(Olsson, 2009) for a comprehensie overview ofthe application of AL to natural language process-ing.
(Haffari et al., 2009; Bloodgood and Callison-Burch, 2010) apply AL to MT where the aim is tobuild an optimal MT model from a given, staticdataset.
To the best of our knowledge, the mostrelevant previous research is (Gonz?alez-Rubio etal., 2012), which applies AL to interactive MT.
Inaddition to differences in the AL selection criteriaand data sets, our goals are fundamentally differ-ent: while the previous work aimed at reducinghuman effort in interactive MT, we aim at reduc-ing the overall PE time in PE-based incrementalMT update applications in the localisation indus-try.In our experiments reported in Section 3 belowwe want to explore a space consisting of a con-siderable number of selection strategies and incre-mental retraining batch sizes.
In order to be able todo this, we use the target side of our industry trans-lation memory data to approximate human PE out-put and automatic TER (Snover et al., 2006) scoresas a proxy for human PE times (O?Brien, 2011).2 MethodologyGiven a translation job, our goal is to reduce theoverall PE time.
At each stage, we select sen-tences that are given to the post editor in such away that uncertain sentences (with respect to theMT system at hand)1are post-edited first.
We thentranslate the n top-ranked sentences using the MTsystem and use the human PEs of the MT outputsto retrain the system.
Algorithm 1 describes our1The uncertainty of a sentence with respect to the modelcan be measured according to different criteria, e.g.
percent-age of unknown n-grams, perplexity etc.method, where s and t stand for source and target,respectively.Algorithm 1 Sentence Selection AlgorithmInput:L??
Initial training dataM??
Initial MT modelfor C ?
(Random,Sequential,Ngram,CED,CEDN) doU??
Translation jobwhile size(U) > 0 doU1.s??
SelectTopSentences(C, U.s)U11.t??
Translate(M, U1.s)U1.t??
PostEdit(U11.t)U??
U - U1L??
L ?
U1M??
TrainModel (L)end whileend forWe use two baselines, i.e.
random and sequen-tial.
In the random baseline, the batch of sentencesat each iteration are selected randomly.
In the se-quential baseline, the batches of sentences followthe same order as the data.Aside from the Random and Sequential base-lines we use the following selection criteria:?
N-gram Overlap.
An SMT system will en-counter problems translating sentences con-taining n-grams not seen in the training data.Thus, PEs of sentences with high number ofunseen n-grams are considered to be more in-formative for updating the current MT sys-tem.
However, for the MT system to trans-late unseen n-grams accurately, they need tobe seen a minimum number V times.2Weuse an n-gram overlap function similar tothe one described in (Gonz?alez-Rubio et al.,2012) given in Equation 1 where N(T(i)) andN(S(i)) return i-grams in training data andthe sentence S, respectively.unseen(S) =n?i=1{|N(T(i)) ?N(S(i))|>V }n?i=1N(S(i))(1)?
Cross Entropy Difference (CED).
This met-ric is originally used in data selection (Mooreand Lewis, 2010; Axelrod et al., 2011).Given an in-domain corpus I and a generalcorpus O, language models are built fromboth,3and each sentence in O is scored ac-cording to the entropy H difference (Equation2Following (Gonz?alez-Rubio et al., 2012) we use V =10.3In order to make the LMs comparable they have the samesize.
As commonly the size of O is considerable bigger thanI, this means that the LM for O is built from a subset of thesame size as I.1862).
The lower the score given to a sentence,the more useful it is to train a system for thespecific domain I .score(s) = HI(s)?HO(s) (2)In our AL scenario, we have the current train-ing corpus L and an untranslated corpus U.CED is applied to select sentences from Uthat are (i) different from L (as we would liketo add sentences that add new information tothe model) and (ii) similar to the overall cor-pus U (as we would like to add sentences thatare common in the untranslated data).
Hencewe apply CED and select sentences from Uthat have high entropy with respect to L andlow entropy with respect to U (Equation 3).score(s) = HU(s)?HL(s) (3)?
CED + n-gram (CEDN).
This is an exten-sion of the CED criterion inspired by the con-cept of the vocabulary saturation filter (Lewisand Eetemadi, 2013).
CED may select manyvery similar sentences, and thus it may be thecase that some of them are redundant.
Bypost-processing the selection made by CEDwith vocabulary saturation we aim to spotand remove redudant sentences.
This worksin two steps.
In the first step, all the sentencesfrom U are scored using the CED metric.
Inthe second step, we down-rank sentences thatare considered redundant.
The top sentence isselected, and its n-grams are stored in local-ngrams.
For the remaining sentences, one byone, their n-grams are matched against local-ngrams.
If the intersection between them islower than a predefined threshold, the currentsentence is added and localngrams is updatedwith the n-grams from the current sentence.Otherwise the sentence is down-ranked to thebottom.
In our experiments, the value n = 1produces best results.3 Experiments and ResultsWe use technical documentation data taken fromSymantec translation memories for the English?French (EN?FR) and English?German (EN?DE)language pairs (both directions) for our experi-ments.
The statistics of the data (training and in-cremental splits) are shown in Table 1.All the systems are trained using theMoses (Koehn et al., 2007) phrase-based sta-tistical MT system, with IRSTLM (Federico etal., 2008) for language modelling (n-grams upto order five) and with the alignment heuristicgrow-diag-final-and.For the experiments, we considered two settingsfor each language pair in each direction.
In thefirst setting, the initial MT system is trained usingthe training set (39,679 and 54,907 sentence pairsfor EN?FR and EN?DE, respectively).
Then, abatch of 500 source sentences is selected from theincremental dataset according to each of the se-lection criteria, and translations are obtained withthe initial MT system.
These translations are post-edited and the corrected translations are added tothe training data.4We then train a new MT sys-tem using the updated training data (initial trainingdata plus PEs of the first batch of sentences).
Theupdated model will be used to translate the nextbatch.
The same process is repeated until the in-cremental dataset is finished (16 and 20 iterationsfor English?French and English?German, respec-tively).
For each batch we compute the TER scorebetween the MT output and the refererence trans-lations for the sentences of that batch.
We thencompute the average TER score for all the batches.These average scores, for each selection criterion,are reported in Table 2.In the second setting, instead of using the wholetraining data, we used a subset of (randomly se-lected) 5,000 sentence pairs for training the initialMT system and a subset of 20,000 sentences fromthe remaining data as the incremental dataset.Here we take batches of 1,000 sentences (thus 20batches).
The results are shown in Table 3.The first setting aims to reflect the situationwhere a translation job is to be completed for a do-main for which we have a considerable amount ofdata available.
Conversely, the second setting re-flects the situation where a translation job is to becarried out for a domain with little (if any) avail-able data.Dir Random Seq.
Ngram CED CEDNEN?FR 29.64 29.81 28.97 29.25 29.05FR?EN 27.08 27.04 26.15 26.63 26.39EN?DE 24.00 24.08 22.34 22.60 22.32DE?EN 19.36 19.34 17.70 17.97 17.48Table 2: TER average scores for Setting 1Dir Random Seq.
Ngram CED CEDNEN?FR 36.23 36.26 35.20 35.48 35.17FR?EN 33.26 33.34 32.26 32.69 32.17EN?DE 32.23 32.19 30.58 31.96 29.98DE?EN 27.24 27.29 26.10 26.73 24.94Table 3: TER average scores for Setting 2For Setting 1 (Table 2), the best result is ob-tained by the CEDN criterion for two out of thefour directions.
For EN?FR, n-gram overlap4As this study simulates the post-editing, we use the ref-erences of the translated segments instead of the PEs.187TypeEN?FR EN?DESentences Avg.
EN SL Avg.
FR SL Sentences Avg.
EN SL Avg.
DE SLTraining 39,679 13.55 15.28 54,907 12.66 12.90Incremental 8,000 13.74 15.50 10,000 12.38 12.61Table 1: Data Statistics for English?French and English?German Symantec Translation Memory Data.SL stands for sentence length, EN stands for English, FR stands for French and DE stands for Germanperforms slightly better than CEDN (0.08 pointslower) with a decrease of 0.67 and 0.84 pointswhen compared to the baselines (random and se-quential, respectively).
For FR?EN, n-gramoverlap results in a decrease of 0.93 and 0.89points compared to the baselines.
The decrease inaverage TER score is higher for the EN?DE andfor DE?EN directions, i.e.
1.68 and 1.88 pointsrespectively for CEDN compared to the randombaseline.In the scenario with limited data available be-forehand (Table 3), CEDN is the best performingcriterion for all the language directions.
For theEN?FR and FR?EN language pairs, CEDN resultsin a decrease of 1.06 and 1.09 points compared tothe random baseline.
Again, the decrease is higherfor the EN?DE and DE?EN language pairs, i.e.2.25 and 2.30 absolute points on average.Figure 1 shows the TER scores per iteration foreach of the criteria, for the scenario DE?EN Set-ting 2 (the trends are similar for the other scenar-ios).
The two baselines exhibit slight improve-ment over the iterations, both starting at around.35 TER points and finishing at around .25 points.Conversely, all the three criteria start at very highscores (in the range [.5,.6]) and then improve con-siderably to arrive at scores below .1 for the lastiterations.
Compared to Ngram and CED, CEDNreaches better scores earlier on, being the criterionwith the lowest score up to iteration 13.1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.00.10.20.30.40.50.60.7 RandomSeqNgramCEDCEDNIterationTERscoreFigure 1: Results per iteration, DE?EN Setting 2Figure 1 together with Tables 2 and 3 showthat AL for PE-based incremental MT retrain-ing really works: all AL based methods (Ngram,CED, CEDN) show strong improvements overboth baselines after the initial 8-9 iterations (Fig-ure 1) and best performance on the complete incre-mental data sets, resulting in a noticeable decreaseof the overall TER score (Tables 2 and 3).
In sixout of eight scenarios, our novel metric CEDN ob-tains the best result.4 Conclusions and Future WorkThis paper has presented an application of AL toMT for dynamically selecting automatic transla-tions of sentences for human PE, with the aim ofreducing overall PE time in a PE-based incremen-tal MT retraining scenario in a typical industriallocalisation workflow that aims to capitalise onhuman PE as early as possible to avoid repeat mis-takes.Our approach makes use of source side informa-tion only, uses two novel selection criteria basedon cross entropy difference and is tested on indus-trial data for two language pairs.
Our best per-forming criteria allow the incrementally retrainedMT systems to improve their performance earlierand reduce the overall TER score by around oneand two absolute points for English?French andEnglish?German, respectively.In order to be able to explore a space of selec-tion criteria and batch sizes, our experiments sim-ulate PE, in the sense that we use the target ref-erence (instead of PEs) and approximate PE timewith TER.
Given that TER correlates well with PEtime (O?Brien, 2011), we expect AL-based selec-tion of sentences for human PE to lead to overallreduction of PE time.
In the future work, we planto do the experiments using PEs to retrain the sys-tem and measuring PE time.In this work, we have taken batches of sentences(size 500 to 1,000) and do full retraining.
As fu-ture work, we plan to use fully incremental retrain-ing and perform the selection on a sentence-by-sentence basis (instead of taking batches).Finally and importantly, a potential drawback ofour approach is that by dynamically selecting in-dividual sentences for PE, the human post-editorlooses context, which they may use if processingsentences sequentially.
We will explore the tradeoff between the context lost and the productivitygain achieved, and ways of supplying context (e.g.previous and following sentence) for real PE.188AcknowledgementsThis work is supported by Science FoundationIreland (Grants 12/TIDA/I2438, 07/CE/I1142 and12/CE/I2267) as part of the Centre for Next Gen-eration Localisation (www.cngl.ie) at Dublin CityUniversity.
We would like to thank Symantec forthe provision of data sets used in our experiments.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,USA.
Association for Computational Linguistics.Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-erico.
2013.
Cache-based online adaptation for ma-chine translation enhanced computer assisted trans-lation.
In Proceedings of the XIV Machine Transla-tion Summit, pages 35?42, Nice, France.Michael Bloodgood and Chris Callison-Burch.
2010.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In JanHajic, Sandra Carberry, and Stephen Clark, editors,ACL, pages 854?864.
The Association for ComputerLinguistics.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Comput.
Linguist., 19(2):263?311, June.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In INTER-SPEECH, pages 1618?1621.
ISCA.Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart?
?nez, andFrancisco Casacuberta.
2012.
Active learning forinteractive machine translation.
In Proceedings ofthe 13th Conference of the European Chapter of theAssociation for Computational Linguistics, EACL?12, pages 245?254, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-basedmachine translation.
In HLT-NAACL, pages 415?423.
The Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL 2007, pages 177?180, Prague, Czech Repub-lic.
Association for Computational Linguistics.Abby Levenberg, Chris Callison-Burch, and Miles Os-borne.
2010.
Stream-based translation modelsfor statistical machine translation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 394?402, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.William Lewis and Sauleh Eetemadi.
2013.
Dramati-cally reducing training data size through vocabularysaturation.
In Proceedings of the Eighth Workshopon Statistical Machine Translation, pages 281?291,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Prashant Mathur, Mauro Cettolo, and Marcello Fed-erico.
2013.
Online learning approaches in com-puter assisted translation.
In Proceedings of theEighth Workshop on Statistical Machine Transla-tion, ACL, pages 301?308, Sofia, Bulgaria.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and dis-criminative training.
In Proceedings of HLT, pages337?342.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Sharon O?Brien.
2011.
Towards predictingpost-editing productivity.
Machine Translation,25(3):197?215, September.Fredrik Olsson.
2009.
A literature survey of activemachine learning in the context of natural languageprocessing.
Technical Report T2009:06.Mirko Plitt and Franc?ois Masselot.
2010.
A productiv-ity test of statistical machine translation post-editingin a typical localisation context.
Prague Bull.
Math.Linguistics, 93:7?16.Michel Simard and George Foster.
2013.
Pepr: Post-edit propagation using phrase-based statistical ma-chine translation.
In Proceedings of the XIV Ma-chine Translation Summit, pages 191?198, Nice,France.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Trans-lation in the Americas, pages 223?231, Cambridge,MA.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.Active learning for statistical natural language pars-ing.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 120?127, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.189
