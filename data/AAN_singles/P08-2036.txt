Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 141?144,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSmoothing a Tera-word Language ModelDeniz YuretKoc?
Universitydyuret@ku.edu.trAbstractFrequency counts from very large corpora,such as the Web 1T dataset, have recently be-come available for language modeling.
Omis-sion of low frequency n-gram counts is a prac-tical necessity for datasets of this size.
Naiveimplementations of standard smoothing meth-ods do not realize the full potential of suchlarge datasets with missing counts.
In this pa-per I present a new smoothing algorithm thatcombines the Dirichlet prior form of (Mackayand Peto, 1995) with the modified back-off es-timates of (Kneser and Ney, 1995) that leads toa 31% perplexity reduction on the Brown cor-pus compared to a baseline implementation ofKneser-Ney discounting.1 IntroductionLanguage models, i.e.
models that assign probabili-ties to sequences of words, have been proven usefulin a variety of applications including speech recog-nition and machine translation (Bahl et al, 1983;Brown et al, 1990).
More recently, good resultson lexical substitution and word sense disambigua-tion using language models have also been reported(Yuret, 2007).The recently introduced Web 1T 5-gram dataset(Brants and Franz, 2006) contains the counts ofword sequences up to length five in a 1012 word cor-pus derived from publicly accessible Web pages.
Asthis corpus is several orders of magnitude larger thanthe ones used in previous language modeling stud-ies, it holds the promise to provide more accuratedomain independent probability estimates.
How-ever, naive application of the well-known smooth-ing methods do not realize the full potential of thisdataset.In this paper I present experiments with modifica-tions and combinations of various smoothing meth-ods using the Web 1T dataset for model building andthe Brown corpus for evaluation.
I describe a newsmoothing method, Dirichlet-Kneser-Ney (DKN),that combines the Bayesian intuition of MacKay andPeto (1995) and the improved back-off estimation ofKneser and Ney (1995) and gives significantly betterresults than the baseline Kneser-Ney discounting.The next section describes the general structureof n-gram models and smoothing.
Section 3 de-scribes the data sets and the experimental methodol-ogy used.
Section 4 presents experiments with adap-tations of various smoothing methods.
Section 5 de-scribes the new algorithm.2 N-gram Models and SmoothingN-gram models are the most commonly used lan-guage modeling tools.
They estimate the probabilityof each word using the context made up of the previ-ous n?1 words.
Let abc represent an n-gram wherea is the first word, c is the last word, and b repre-sents zero or more words in between.
One way toestimate Pr(c|ab) is to look at the number of timesword c has followed the previous n?
1 words ab,Pr(c|ab) =C(abc)C(ab)(1)where C(x) denotes the number of times x has beenobserved in the training corpus.
This is the max-imum likelihood (ML) estimate.
Unfortunately it141does not work very well because it assigns zeroprobability to n-grams that have not been observedin the training corpus.
To avoid the zero probabil-ities, we take some probability mass from the ob-served n-grams and distribute it to unobserved n-grams.
Such redistribution is known as smoothingor discounting.Most existing smoothing methods can be ex-pressed in one of the following two forms:Pr(c|ab) = ?
(c|ab) + ?
(ab) Pr(c|b) (2)Pr(c|ab) ={?
(c|ab) if C(abc) > 0?
(ab) Pr(c|b) otherwise (3)Equation 2 describes the so-called interpolatedmodels and Equation 3 describes the back-off mod-els.
The highest order distributions ?
(c|ab) and?
(c|ab) are typically discounted to be less than theML estimate so we have some leftover probabilityfor the c words unseen in the context ab.
Differentmethods mainly differ on how they discount the MLestimate.
The back-off weights ?
(ab) are computedto make sure the probabilities are normalized.
Theinterpolated models always incorporate the lower or-der distribution Pr(c|b) whereas the back-off modelsconsider it only when the n-gram abc has not beenobserved in the training data.3 Data and MethodAll the models in this paper are interpolated mod-els built using the counts obtained from the Web 1Tdataset and evaluated on the million word Browncorpus using cross entropy (bits per token).
The low-est order model is taken to be the word frequenciesin the Web 1T corpus.
The Brown corpus was re-tokenized to match the tokenization style of the Web1T dataset resulting in 1,186,262 tokens in 52,108sentences.
The Web 1T dataset has a 13 millionword vocabulary consisting of words that appear 100times or more in its corpus.
769 sentences in Brownthat contained words outside this vocabulary wereeliminated leaving 1,162,052 tokens in 51,339 sen-tences.
Capitalization and punctuation were left in-tact.
The n-gram patterns of the Brown corpus wereextracted and the necessary counts were collectedfrom the Web 1T dataset in one pass.
The end-of-sentence tags were not included in the entropy cal-culation.
For parameter optimization, numerical op-timization was performed on a 1,000 sentence ran-dom sample of Brown.4 ExperimentsIn this section, I describe several smoothing meth-ods and give their performance on the Brown corpus.Each subsection describes a single idea and its im-pact on the performance.
All methods use interpo-lated models expressed by ?
(c|ab) and ?
(ab) basedon Equation 2.
The Web 1T dataset does not includen-grams with counts less than 40, and I note the spe-cific implementation decisions due to the missingcounts where appropriate.4.1 Absolute DiscountingAbsolute discounting subtracts a fixed constant Dfrom each nonzero count to allocate probability forunseen words.
A different D constant is chosen foreach n-gram order.
Note that in the original study, Dis taken to be between 0 and 1, but because the Web1T dataset does not include n-grams with counts lessthan 40, the optimized D constants in our case rangefrom 0 to 40.
The interpolated form is:?
(c|ab) =max(0, C(abc)?D)C(ab?)(4)?
(ab) =N(ab?)DC(ab?
)The ?
represents a wildcard matching any word andC(ab?)
is the total count of n-grams that start withthe n ?
1 words ab.
If we had complete counts,we would have C(ab?)
=?c C(abc) = C(ab).However because of the missing counts in generalC(ab?)
?
C(ab) and we need to use the former forproper normalization.
N(ab?)
denotes the numberof distinct words following ab in the training data.Absolute discounting achieves its best performancewith a 3-gram model and gives 8.53 bits of cross en-tropy on the Brown corpus.4.2 Kneser-NeyKneser-Ney discounting (Kneser and Ney, 1995)has been reported as the best performing smooth-ing method in several comparative studies (Chen andGoodman, 1999; Goodman, 2001).
The ?
(c|ab)and ?
(ab) expressions are identical to absolute dis-counting (Equation 4) for the highest order n-grams.142However, a modified estimate is used for lower ordern-grams used for back-off.
The interpolated form is:Pr(c|ab) = ?
(c|ab) + ?(ab)Pr?
(c|b) (5)Pr?
(c|ab) = ??
(c|ab) + ??(ab)Pr?
(c|b)Specifically, the modified estimate Pr?
(c|b) for alower order n-gram is taken to be proportional to thenumber of unique words that precede the n-gram inthe training data.
The ??
and ??
expressions for themodified lower order distributions are:??
(c|b) =max(0, N(?bc)?D)N(?b?)(6)??
(b) =R(?b?)DN(?b?
)where R(?b?)
= |c : N(?bc) > 0| denotes the num-ber of distinct words observed on the right hand sideof the ?b?
pattern.
A different D constant is chosenfor each n-gram order.
The lowest order model istaken to be Pr(c) = N(?c)/N(??).
The best resultsfor Kneser-Ney are achieved with a 4-gram modeland its performance on Brown is 8.40 bits.4.3 Correcting for Missing CountsKneser-Ney takes the back-off probability of a lowerorder n-gram to be proportional to the number ofunique words that precede the n-gram in the trainingdata.
Unfortunately this number is not exactly equalto the N(?bc) value given in the Web 1T dataset be-cause the dataset does not include low count abc n-grams.
To correct for the missing counts I used thefollowing modified estimates:N ?
(?bc) = N(?bc) + ?(C(bc)?
C(?bc))N ?(?b?)
= N(?b?)
+ ?(C(b?)?
C(?b?
))The difference between C(bc) and C(?bc) is dueto the words preceding bc less than 40 times.
Wecan estimate their number to be a fraction of thisdifference.
?
is an estimate of the type token ra-tio of these low count words.
Its valid range is be-tween 1/40 and 1, and it can be optimized along withthe other parameters.
The reader can confirm that?c N?
(?bc) = N ?(?b?)
and |c : N ?
(?bc) > 0| =N(b?).
The expression for the Kneser-Ney back-offestimate becomes??
(c|b) =max(0, N ?
(?bc)?D)N ?(?b?)(7)??
(b) =N(b?
)DN ?(?b?
)Using the corrected N ?
counts instead of the plain Ncounts achieves its best performance with a 4-grammodel and gives 8.23 bits on Brown.4.4 Dirichlet FormMacKay and Peto (1995) show that based on Dirich-let priors a reasonable form for a smoothed distribu-tion can be expressed as?
(c|ab) =C(abc)C(ab?)
+A(8)?
(ab) =AC(ab?)
+AThe parameter A can be interpreted as the extracounts added to the given distribution and these ex-tra counts are distributed as the lower order model.Chen and Goodman (1996) suggest that these ex-tra counts should be proportional to the number ofwords with exactly one count in the given contextbased on the Good-Turing estimate.
The Web 1Tdataset does not include one-count n-grams.
A rea-sonable alternative is to take A to be proportionalto the missing count due to low-count n-grams:C(ab)?
C(ab?
).A(ab) = max(1,K(C(ab)?
C(ab?
)))A different K constant is chosen for each n-gramorder.
Using this formulation as an interpolated 5-gram language model gives a cross entropy of 8.05bits on Brown.4.5 Dirichlet with KN Back-OffUsing a modified back-off distribution for lower or-der n-grams gave us a big boost in the baseline re-sults from 8.53 bits for absolute discounting to 8.23bits for Kneser-Ney.
The same idea can be appliedto the missing-count estimate.
We can use Equa-tion 8 for the highest order n-grams and Equation 7for lower order n-grams used for back-off.
Such a5-gram model gives a cross entropy of 7.96 bits onthe Brown corpus.5 A New Smoothing Method: DKNIn this section, I describe a new smoothing methodthat combines the Dirichlet form of MacKay and143Peto (1995) and the modified back-off distributionof Kneser and Ney (1995).
We will call this newmethod Dirichlet-Kneser-Ney, or DKN for short.The important idea in Kneser-Ney is to let the prob-ability of a back-off n-gram be proportional to thenumber of unique words that precede it.
Howeverwe do not need to use the absolute discount form forthe estimates.
We can use the Dirichlet prior formfor the lower order back-off distributions as well asthe highest order distribution.
The extra counts Ain the Dirichlet form are taken to be proportionalto the missing counts, and the coefficient of pro-portionality K is optimized for each n-gram order.Where complete counts are available, A should betaken to be proportional to the number of one-countn-grams instead.
This smoothing method with a 5-gram model gives a cross entropy of 7.86 bits onthe Brown corpus achieving a perplexity reductionof 31% compared to the naive implementation ofKneser-Ney.The relevant equations are repeated below for thereader?s convenience.Pr(c|ab) = ?
(c|ab) + ?(ab)Pr?(c|b)Pr?
(c|ab) = ??
(c|ab) + ??(ab)Pr?(c|b)?
(c|b) =C(bc)C(b?)
+A(b)?
(b) =A(b)C(b?)
+A(b)??
(c|b) =N ?
(?bc)N ?(?b?)
+A(b)??
(b) =A(b)N ?(?b?)
+A(b)A(b) = max(1,K(C(b)?
C(b?
)))or max(1,K|c : C(bc) = 1|)6 Summary and DiscussionFrequency counts based on very large corpora canprovide accurate domain independent probability es-timates for language modeling.
I presented adapta-tions of several smoothing methods that can prop-erly handle the missing counts that may exist insuch datasets.
I described a new smoothing method,DKN, combining the Bayesian intuition of MacKayand Peto (1995) and the modified back-off distri-bution of Kneser and Ney (1995) which achieves asignificant perplexity reduction compared to a naiveimplementation of Kneser-Ney smoothing.
Thisis a surprizing result because Chen and Goodman(1999) partly attribute the performance of Kneser-Ney to the use of absolute discounting.
The re-lationship between Kneser-Ney smoothing to theBayesian approach have been explored in (Goldwa-ter et al, 2006; Teh, 2006) using Pitman-Yor pro-cesses.
These models still suggest discount-basedinterpolation with type frequencies whereas DKNuses Dirichlet smoothing throughout.
The condi-tions under which the Dirichlet form is superior isa topic for future research.ReferencesLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.1983.
A maximum likelihood approach to continu-ous speech recognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 5(2):179?190.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.
Linguistic Data Consortium, Philadelphia.LDC2006T13.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16(2):79?85.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th Annual Meeting ofthe ACL.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech and Language.S.
Goldwater, T.L.
Griffiths, and M. Johnson.
2006.
In-terpolating between types and tokens by estimatingpower-law generators.
In Advances in Neural Infor-mation Processing Systems, volume 18.
MIT Press.Joshua Goodman.
2001.
A bit of progress in languagemodeling.
Computer Speech and Language.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In International Confer-ence on Acoustics, Speech, and Signal Processing.David J. C. Mackay and Linda C. Bauman Peto.
1995.
Ahierarchical Dirichlet language model.
Natural Lan-guage Engineering, 1(3):1?19.Y.W.
Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the ACL, pages 985?992.Deniz Yuret.
2007.
KU: Word sense disambiguationby substitution.
In SemEval-2007: 4th InternationalWorkshop on Semantic Evaluations.144
