Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?14,Baltimore, Maryland, 26-27 July 2014.c?2014 Association for Computational LinguisticsThe CoNLL-2014 Shared Task on Grammatical Error CorrectionHwee Tou Ng1Siew Mei Wu2Ted Briscoe3Christian Hadiwinoto1Raymond Hendy Susanto1Christopher Bryant11Department of Computer Science, National University of Singapore{nght,chrhad,raymondhs,bryant}@comp.nus.edu.sg2Centre for English Language Communication, National University of Singaporeelcwusm@nus.edu.sg3Computer Laboratory, University of CambridgeTed.Briscoe@cl.cam.ac.ukAbstractThe CoNLL-2014 shared task was devotedto grammatical error correction of all errortypes.
In this paper, we give the task defi-nition, present the data sets, and describethe evaluation metric and scorer used inthe shared task.
We also give an overviewof the various approaches adopted by theparticipating teams, and present the eval-uation results.
Compared to the CoNLL-2013 shared task, we have introduced thefollowing changes in CoNLL-2014: (1)A participating system is expected to de-tect and correct grammatical errors of alltypes, instead of just the five error typesin CoNLL-2013; (2) The evaluation metricwas changed from F1to F0.5, to empha-size precision over recall; and (3) We havetwo human annotators who independentlyannotated the test essays, compared to justone human annotator in CoNLL-2013.1 IntroductionGrammatical error correction is the shared task ofthe Eighteenth Conference on Computational Nat-ural Language Learning in 2014 (CoNLL-2014).In this task, given an English essay written by alearner of English as a second language, the goalis to detect and correct the grammatical errors ofall error types present in the essay, and return thecorrected essay.This task has attracted much recent research in-terest, with two shared tasks Helping Our Own(HOO) organized in 2011 and 2012 (Dale and Kil-garriff, 2011; Dale et al., 2012), and a CoNLLshared task on grammatical error correction orga-nized in 2013 (Ng et al., 2013).
In contrast toprevious CoNLL shared tasks which focused onparticular subtasks of natural language process-ing, such as named entity recognition, semanticrole labeling, dependency parsing, or coreferenceresolution, grammatical error correction aims atbuilding a complete end-to-end application.
Thistask is challenging since for many error types,current grammatical error correction systems donot achieve high performance and much researchis still needed.
Also, tackling this task has far-reaching impact, since it is estimated that hun-dreds of millions of people worldwide are learn-ing English and they benefit directly from an auto-mated grammar checker.The CoNLL-2014 shared task provides a forumfor participating teams to work on the same gram-matical error correction task, with evaluation onthe same blind test set using the same evaluationmetric and scorer.
This overview paper contains adetailed description of the shared task, and is orga-nized as follows.
Section 2 provides the task def-inition.
Section 3 describes the annotated trainingdata provided and the blind test data.
Section 4 de-scribes the evaluation metric and the scorer.
Sec-tion 5 lists the participating teams and outlines theapproaches to grammatical error correction usedby the teams.
Section 6 presents the results of theshared task, including a discussion on cross anno-tator comparison.
Section 7 concludes the paper.2 Task DefinitionThe goal of the CoNLL-2014 shared task is toevaluate algorithms and systems for automati-cally detecting and correcting grammatical errors1present in English essays written by second lan-guage learners of English.
Each participatingteam is given training data manually annotatedwith corrections of grammatical errors.
The testdata consists of new, blind test essays.
Prepro-cessed test essays, which have been sentence-segmented and tokenized, are also made availableto the participating teams.
Each team is to submitits system output consisting of the automaticallycorrected essays, in sentence-segmented and tok-enized form.Grammatical errors consist of many differenttypes, including articles or determiners, preposi-tions, noun form, verb form, subject-verb agree-ment, pronouns, word choice, sentence structure,punctuation, capitalization, etc.
However, mostprior published research on grammatical error cor-rection only focuses on a small number of fre-quently occurring error types, such as article andpreposition errors (Han et al., 2006; Gamon, 2010;Rozovskaya and Roth, 2010; Tetreault et al., 2010;Dahlmeier and Ng, 2011b).
Article and preposi-tion errors were also the only error types featuredin the HOO 2012 shared task.
Likewise, althoughall error types were included in the HOO 2011shared task, almost all participating teams dealtwith article and preposition errors only (besidesspelling and punctuation errors).
In the CoNLL-2013 shared task, the error types were extendedto include five error types, comprising article ordeterminer, preposition, noun number, verb form,and subject-verb agreement.
Other error typessuch as word choice errors (Dahlmeier and Ng,2011a) were not dealt with.In the CoNLL-2014 shared task, it was felt thatthe community is now ready to deal with all er-ror types.
Table 1 shows examples of the 28 errortypes in the CoNLL-2014 shared task.Since there are 28 error types in our shared taskcompared to two in HOO 2012 and five in CoNLL-2013, there is a greater chance of encounteringmultiple, interacting errors in a sentence in ourshared task.
This increases the complexity of ourshared task.
To illustrate, consider the followingsentence:Social network plays a role in providingand also filtering information.The noun number error networks needs to be cor-rected (network ?
networks).
This necessitatesthe correction of a subject-verb agreement error(plays ?
play).
A pipeline system in which cor-rections for subject-verb agreement errors occurstrictly before corrections for noun number errorswould not be able to arrive at a fully correctedsentence for this example.
The ability to correctmultiple, interacting errors is thus necessary in ourshared task.
The recent work of Dahlmeier and Ng(2012a) and Wu and Ng (2013), for example, isdesigned to deal with multiple, interacting errors.3 DataThis section describes the training and test datareleased to each participating team in our sharedtask.3.1 Training DataThe training data provided in our shared task isthe NUCLE corpus, the NUS Corpus of LearnerEnglish (Dahlmeier et al., 2013).
As noted by(Leacock et al., 2010), the lack of a manually an-notated and corrected corpus of English learnertexts has been an impediment to progress in gram-matical error correction, since it prevents com-parative evaluations on a common benchmark testdata set.
NUCLE was created precisely to fill thisvoid.
It is a collection of 1,414 essays writtenby students at the National University of Singa-pore (NUS) who are non-native speakers of En-glish.
The essays were written in response to someprompts, and they cover a wide range of topics,such as environmental pollution, health care, etc.The grammatical errors in these essays have beenhand-corrected by professional English instructorsat NUS.
For each grammatical error instance, thestart and end character offsets of the erroneous textspan are marked, and the error type and the cor-rection string are provided.
Manual annotation iscarried out using a graphical user interface specif-ically built for this purpose.
The error annotationsare saved as stand-off annotations, in SGML for-mat.To illustrate, consider the following sentence atthe start of the sixth paragraph of an essay:Nothing is absolute right or wrong.There is a word form error (absolute?
absolutely)in this sentence.
The error annotation, also calledcorrection or edit, in SGML format is shown inFigure 1. start par (end par) denotes theparagraph ID of the start (end) of the erroneous2Type Description ExampleVt Verb tense Medical technology during that time [is ?
was] not advanced enough tocure him.Vm Verb modal Although the problem [would ?
may] not be serious, people [would ?might] still be afraid.V0 Missing verb However, there are also a great number of people [who?
who are] againstthis technology.Vform Verb form A study in 2010 [shown?
showed] that patients recover faster when sur-rounded by family members.SVA Subject-verb agreement The benefits of disclosing genetic risk information [outweighs ?
out-weigh] the costs.ArtOrDet Article or determiner It is obvious to see that [internet?
the internet] saves people time and alsoconnects people globally.Nn Noun number A carrier may consider not having any [child ?
children] after gettingmarried.Npos Noun possessive Someone should tell the [carriers?
carrier?s] relatives about the geneticproblem.Pform Pronoun form A couple should run a few tests to see if [their?
they] have any geneticdiseases beforehand.Pref Pronoun reference It is everyone?s duty to ensure that [he or she ?
they] undergo regularhealth checks.Prep Preposition This essay will [discuss about?
discuss] whether a carrier should tell hisrelatives or not.Wci Wrong collocation/idiom Early examination is [healthy?
advisable] and will cast away unwanteddoubts.Wa Acronyms After [WOWII ?
World War II], the population of China decreasedrapidly.Wform Word form The sense of [guilty?
guilt] can be more than expected.Wtone Tone (formal/informal) [It?s?
It is] our family and relatives that bring us up.Srun Run-on sentences,comma splicesThe issue is highly [debatable, a?
debatable.
A] genetic risk could comefrom either side of the family.Smod Dangling modifiers [Undeniable,?
It is undeniable that] it becomes addictive when we spendmore time socializing virtually.Spar Parallelism We must pay attention to this information and [assisting ?
assist] thosewho are at risk.Sfrag Sentence fragment However, from the ethical point of view.Ssub Subordinate clause This is an issue [needs?
that needs] to be addressed.WOinc Incorrect word order [Someone having what kind of disease?What kind of disease someonehas] is a matter of their own privacy.WOadv Incorrect adjective/adverb orderIn conclusion, [personally I?
I personally] feel that it is important to tellone?s family members.Trans Linking words/phrases It is sometimes hard to find [out?
out if] one has this disease.Mec Spelling, punctuation,capitalization, etc.This knowledge [maybe relavant?
may be relevant] to them.Rloc?
Redundancy It is up to the [patient?s own choice?
patient] to disclose information.Cit Citation Poor citation practice.Others Other errors An error that does not fit into any other category but can still be corrected.Um Unclear meaning Genetic disease has a close relationship with the born gene.
(i.e., no cor-rection possible without further clarification.
)Table 1: The 28 error types in the shared task.3text span (paragraph ID starts from 0 by conven-tion).
start off (end off) denotes the char-acter offset of the start (end) of the erroneous textspan (again, character offset starts from 0 by con-vention).
The error tag is Wform, and the correc-tion string is absolutely.The NUCLE corpus was first used in(Dahlmeier and Ng, 2011b), and has beenpublicly available for research purposes sinceJune 20111.
All instances of grammatical errorsare annotated in NUCLE.To help participating teams in their prepara-tion for the shared task, we also performed au-tomatic preprocessing of the NUCLE corpus andreleased the preprocessed form of NUCLE.
Thepreprocessing operations performed on the NU-CLE essays include sentence segmentation andword tokenization using the NLTK toolkit (Birdet al., 2009), and part-of-speech (POS) tagging,constituency and dependency tree parsing usingthe Stanford parser (Klein and Manning, 2003;de Marneffe et al., 2006).
The error annotations,which are originally at the character level, arethen mapped to error annotations at the word to-ken level.
Error annotations at the word tokenlevel also facilitate scoring, as we will see in Sec-tion 4, since our scorer operates by matching to-kens.
Note that although we released our ownpreprocessed version of NUCLE, the participatingteams were however free to perform their own pre-processing if they so preferred.NUCLE release version 3.2 was used in theCoNLL-2014 shared task.
In this version, 17 es-says were removed from the first release of NU-CLE since these essays were duplicates with mul-tiple annotations.
In addition, in order to facilitatethe detection and correction of article/determinererrors and preposition errors, we performed someautomatic mapping of error types in the originalNUCLE corpus to arrive at release version 3.2.
Nget al.
(2013) gives more details of how the map-ping was carried out.The statistics of the NUCLE corpus (release 3.2version) are shown in Table 2.
The distribution oferrors among all error types is shown in Table 3.While the NUCLE corpus is provided in ourshared task, participating teams are free to not useNUCLE, or to use additional resources and toolsin building their grammatical error correction sys-tems, as long as these resources and tools are pub-1http://www.comp.nus.edu.sg/?nlp/corpora.htmlTraining data Test data(NUCLE)# essays 1,397 50# sentences 57,151 1,312# word tokens 1,161,567 30,144Table 2: Statistics of training and test data.licly available and not proprietary.
For example,participating teams are free to use the CambridgeFCE corpus (Yannakoudakis et al., 2011; Nicholls,2003) (the training data provided in HOO 2012(Dale et al., 2012)) as additional training data.3.2 Test DataSimilar to CoNLL-2013, 25 NUS students, whoare non-native speakers of English, were recruitedto write new essays to be used as blind test datain the shared task.
Each student wrote two essaysin response to the two prompts shown in Table 4,one essay per prompt.
The first prompt was alsoused in the NUCLE training data, but the secondprompt is entirely new and not used previously.
Asa result, 50 new test essays were collected.
Thestatistics of the test essays are also shown in Ta-ble 2.Error annotation on the test essays was carriedout independently by two native speakers of En-glish.
One of them is a lecturer at the NUS Cen-tre for English Language Communication, and theother is a freelance English linguist with exten-sive prior experience in error annotation of Englishlearners?
essays.
The distribution of errors in thetest essays among the error types is shown in Ta-ble 3.
The test essays were then preprocessed inthe same manner as the NUCLE corpus.
The pre-processed test essays were released to the partic-ipating teams.
Similar to CoNLL-2013, the testessays and their error annotations in the CoNLL-2014 shared task will be made freely available af-ter the shared task.4 Evaluation Metric and ScorerA grammatical error correction system is evalu-ated by how well its proposed corrections or editsmatch the gold-standard edits.
An essay is firstsentence-segmented and tokenized before evalua-tion is carried out on the essay.
To illustrate, con-sider the following tokenized sentence S writtenby an English learner:4<MISTAKE start par="5" start off="11" end par="5" end off="19"><TYPE>Wform</TYPE><CORRECTION>absolutely</CORRECTION></MISTAKE>Figure 1: An example error annotation.Error type Training % Test % Test %data data data(NUCLE) (Annotator 1) (Annotator 2)Vt 3,204 7.1% 133 5.5% 150 4.5%Vm 431 1.0% 49 2.0% 37 1.1%V0 414 0.9% 31 1.3% 37 1.1%Vform 1,443 3.2% 132 5.5% 91 2.7%SVA 1,524 3.4% 105 4.4% 154 4.6%ArtOrDet 6,640 14.8% 332 13.9% 444 13.3%Nn 3,768 8.4% 215 9.0% 228 6.8%Npos 239 0.5% 19 0.8% 15 0.5%Pform 186 0.4% 47 2.0% 18 0.5%Pref 927 2.1% 96 4.0% 153 4.6%Prep 2,413 5.4% 211 8.8% 390 11.7%Wci 5,305 11.8% 340 14.2% 479 14.4%Wa 50 0.1% 0 0.0% 1 0.0%Wform 2,161 4.8% 77 3.2% 103 3.1%Wtone 593 1.3% 9 0.4% 15 0.5%Srun 873 1.9% 7 0.3% 26 0.8%Smod 51 0.1% 0 0.0% 5 0.2%Spar 519 1.2% 3 0.1% 24 0.7%Sfrag 250 0.6% 13 0.5% 5 0.2%Ssub 362 0.8% 68 2.8% 10 0.3%WOinc 698 1.6% 22 0.9% 54 1.6%WOadv 347 0.8% 12 0.5% 27 0.8%Trans 1,377 3.1% 94 3.9% 79 2.4%Mec 3,145 7.0% 231 9.6% 496 14.9%Rloc?
4,703 10.5% 95 4.0% 199 6.0%Cit 658 1.5% 0 0.0% 0 0.0%Others 1,467 3.3% 44 1.8% 49 1.5%Um 1,164 2.6% 12 0.5% 42 1.3%All types 44,912 100.0% 2,397 100.0% 3,331 100.0%Table 3: Error type distribution of the training and test data.
The test data were annotated independentlyby two annotators.5ID Prompt1 ?The decision to undergo genetic testing can only be made by the individual at risk for a disor-der.
Once a test has been conducted and the results are known, however, a new, family-relatedethical dilemma is born: Should a carrier of a known genetic risk be obligated to tell his or herrelatives??
Respond to the question above, supporting your argument with concrete examples.2 While social media sites such as Twitter and Facebook can connect us closely to people inmany parts of the world, some argue that the reduction in face-to-face human contact affectsinterpersonal skills.
Explain the advantages and disadvantages of using social media in yourdaily life/society.Table 4: The two prompts used for the test essays.There is no a doubt , tracking systemhas brought many benefits in this infor-mation age .The set of gold-standard edits of a human annota-tor is g = {a doubt ?
doubt, system ?
systems,has ?
have}.
Suppose the tokenized output sen-tence H of a grammatical error correction systemgiven the above sentence is:There is no doubt , tracking system hasbrought many benefits in this informa-tion age .That is, the set of system edits is e = {a doubt?
doubt}.
The performance of the grammaticalerror correction system is measured by how wellthe two sets g and e match, in the form of recallR, precision P , and F0.5measure: R = 1/3, P =1/1, F0.5= (1 + 0.52)?RP/(R + 0.52?
P ) =5/7.More generally, given a set of n sentences,where giis the set of gold-standard edits for sen-tence i, and eiis the set of system edits for sen-tence i, recall, precision, and F0.5are defined asfollows:R =?ni=1|gi?
ei|?ni=1|gi|(1)P =?ni=1|gi?
ei|?ni=1|ei|(2)F0.5=(1 + 0.52)?R?
PR + 0.52?
P(3)where the intersection between giand eifor sen-tence i is defined asgi?
ei= {e ?
ei|?g ?
gi,match(g, e)} (4)Note that we have adopted F0.5as the evaluationmetric in the CoNLL-2014 shared task instead ofthe standard F1used in CoNLL-2013.
F0.5em-phasizes precision twice as much as recall, whileF1weighs precision and recall equally.
When agrammar checker is put into actual use, it is im-portant that its proposed corrections are highly ac-curate in order to gain user acceptance.
Neglectingto propose a correction is not as bad as proposingan erroneous correction.Similar to CoNLL-2013, we use the MaxMatch(M2) scorer2(Dahlmeier and Ng, 2012b) as the of-ficial scorer in CoNLL-2014.
The M2scorer3effi-ciently searches for a set of system edits that max-imally matches the set of gold-standard edits spec-ified by an annotator.
It overcomes a limitation ofthe scorer used in HOO shared tasks, which canreturn an erroneous score since the system editsare computed deterministically by the HOO scorerwithout regard to the gold-standard edits.5 Approaches45 teams registered to participate in the sharedtask, out of which 13 teams submitted the out-put of their grammatical error correction systems.These teams are listed in Table 5.
Each team is as-signed a 3 to 4-letter team ID.
In the remainder ofthis paper, we will use the assigned team ID to re-fer to a participating team.
Every team submitteda system description paper (the only exception isthe NARA team).
Four of the 13 teams submittedtheir system output only after the deadline (theywere given up to one week of extension).
Thesefour teams (IITB, IPN, PKU, and UFC) have anasterisk affixed after their team names in Table 5.Each participating team in the CoNLL-2014shared task tackled the error correction problemin a different way.
A full list summarizing each2http://www.comp.nus.edu.sg/?nlp/software.html3A few minor bugs were fixed in the M2scorer before itwas used in the CoNLL-2014 shared task.6Team ID AffiliationAMU Adam Mickiewicz UniversityCAMB University of CambridgeCUUI Columbia University and the University of Illinois at Urbana-ChampaignIITB?Indian Institute of Technology, BombayIPN?Instituto Polit?ecnico NacionalNARA Nara Institute of Science and TechnologyNTHU National Tsing Hua UniversityPKU?Peking UniversityPOST Pohang University of Science and TechnologyRAC Research Institute for Artificial Intelligence, Romanian AcademySJTU Shanghai Jiao Tong UniversityUFC?University of Franche-Comt?eUMC University of MacauTable 5: The list of 13 participating teams.
The teams that submitted their system output after thedeadline have an asterisk affixed after their team names.
NARA did not submit any system descriptionpaper.team?s approach can be found in Table 6.
Whilemachine-learnt classifiers for specific error typesproved popular in last year?s CoNLL-2013 sharedtask, since this year?s task required the correctionof all 28 error types, teams tended to prefer meth-ods that could deal with all error types simultane-ously.
In fact, most teams built hybrid systems thatmade use of a combination of different approachesto identify and correct errors.One of the most popular approaches to non-specific error type correction, incorporated to var-ious extents in many teams?
systems, was the Lan-guage Model (LM) based approach.
Specifically,the probability of a learner n-gram is comparedwith the probability of a candidate corrected n-gram, and if the difference is greater than somethreshold, an error was perceived to have been de-tected and a higher scoring replacement n-gramcould be suggested.
Some teams used this ap-proach only to detect errors, e.g., IPN (Hernandezand Calvo, 2014), which could then be correctedby other methods, whilst other teams used othermethods to detect errors first, and then made cor-rections based on the alternative highest n-gramprobability score, e.g., RAC (Boros?
et al., 2014).No single team used a uniquely LM-based solutionand the LM approach was always a component ina hybrid system.An alternative solution to correcting all er-rors was to use a phrase-based statistical machinetranslation (MT) system to ?translate?
learner En-glish into correct English.
Teams that followed theMT approach mainly differed in terms of their at-titude toward tuning; CAMB (Felice et al., 2014)performed no tuning at all, IITB (Kunchukut-tan et al., 2014) and UMC (Wang et al., 2014b)tuned F0.5using MERT, while AMU (Junczys-Dowmunt and Grundkiewicz, 2014) explored a va-riety of tuning options, ultimately tuning F0.5us-ing a combination of kb-MIRA and MERT.
Noteam used a syntax-based translation model, al-though UMC did include POS tags and morphol-ogy in a factored translation model.With regard to correcting single error types,rule-based (RB) approaches were also common inmost teams?
systems.
A possible reason for thisis that some error types are more regular than oth-ers, and so in order to boost accuracy, simple rulescan be written to make sure that, for example, thenumber of a subject agrees with the number ofa verb.
In contrast, it is a lot harder to write arule to consistently correct Wci (wrong colloca-tion/idiom) errors.
As such, RB methods were of-ten, but not always, used as a preliminary or sup-plementary stage in a larger hybrid system.Finally, although there were fewer machine-learnt classifier (ML) approaches than last year,some teams still used various classifiers to correctspecific error types.
In fact, CUUI (Rozovskayaet al., 2014) only built classifiers for specific er-ror types and did not attempt to tackle the wholerange of errors.
SJTU (Wang et al., 2014a) alsopreprocessed the training data into more preciseerror categories using rules (e.g., verb tense (Vt)7errors might be subcategorized into present, past,or future tense etc.)
and then built a single max-imum entropy classifier to correct all error types.See Table 6 to find out which teams tackled whicherror types.While every effort has been made to make clearwhich team used which approach to correct whichset of error types, as there were more error typesthan last year, it was sometimes impractical to fitall this information into Table 6.
For more infor-mation on the specific methods used to correct aspecific error type, we must refer the reader to thatteam?s CoNLL-2014 system description paper.Table 6 also shows the linguistic features usedby the participating teams, which include lexicalfeatures (i.e., words, collocations, n-grams), parts-of-speech (POS), constituency parses, and depen-dency parses.While all teams in the shared task used the NU-CLE corpus, they were also allowed to use addi-tional external resources (both corpora and tools)so long as they were publicly available and notproprietary.
Three teams also used last year?sCoNLL-2013 test set as a development set in thisyear?s CoNLL-2014 shared task.
The external re-sources used by the teams are also listed in Ta-ble 6.6 ResultsAll submitted system output was evaluated usingthe M2scorer, based on the error annotations pro-vided by our annotators.
The recall (R), pre-cision (P ), and F0.5measure of all teams areshown in Table 7.
The performance of the teamsvaries greatly, from little more than five per cent to37.33% for the top team.The nature of grammatical error correction issuch that multiple, different corrections are of-ten acceptable.
In order to allow the participatingteams to raise their disagreement with the origi-nal gold-standard annotations provided by the an-notators, and not understate the performance ofthe teams, we allow the teams to submit theirproposed alternative answers.
This was also thepractice adopted in HOO 2011, HOO 2012, andCoNLL-2013.
Specifically, after the teams sub-mitted their system output and the error annota-tions on the test essays were released, we allowedthe teams to propose alternative answers (gold-standard edits), to be submitted within four daysafter the initial error annotations were released.Team ID Precision Recall F0.5CAMB 39.71 30.10 37.33CUUI 41.78 24.88 36.79AMU 41.62 21.40 35.01POST 34.51 21.73 30.88NTHU 35.08 18.85 29.92RAC 33.14 14.99 26.68UMC 31.27 14.46 25.37PKU?32.21 13.65 25.32NARA 21.57 29.38 22.78SJTU 30.11 5.10 15.19UFC?70.00 1.72 7.84IPN?11.28 2.85 7.09IITB?30.77 1.39 5.90Table 7: Scores (in %) without alternative an-swers.
The teams that submitted their system out-put after the deadline have an asterisk affixed aftertheir team names.The same annotators who provided the error an-notations on the test essays also judged the alter-native answers proposed by the teams, to ensureconsistency.
In all, three teams (CAMB, CUUI,UMC) submitted alternative answers.The same submitted system output was thenevaluated using the M2scorer, with the originalannotations augmented with the alternative an-swers.
Table 8 shows the recall (R), precision (P ),and F0.5measure of all teams under this new eval-uation setting.The F0.5measure of every team improves whenevaluated with alternative answers.
Not surpris-ingly, the teams which submitted alternative an-swers tend to show the greatest improvements intheir F0.5measure.
Overall, the CUUI team (Ro-zovskaya et al., 2014) achieves the best F0.5mea-sure when evaluated with alternative answers, andthe CAMB team (Felice et al., 2014) achieves thebest F0.5measure when evaluated without alterna-tive answers.For future research which uses the test data ofthe CoNLL-2014 shared task, we recommend thatevaluation be carried out in the setting that doesnot use alternative answers, to ensure a fairer eval-uation.
This is because the scores of the teamswhich submitted alternative answers tend to behigher in a biased way when evaluated with alter-native answers.We are also interested in the analysis of thesystem performance for each of the error types.8TeamErrorApproachDescriptionofApproachLinguisticFeaturesExternalResourcesAMUAllMTPhrase-basedtranslationoptimizedforF-scoreusingacombinationofkb-MIRAandMERTwithaugmentedlanguagemodelsandtask-specificfeatures.LexicalWikipedia,CommonCrawl,Lang-8CAMBAllRB/LM/MTPipeline:Rule-based?LMranking?UntunedSMT?LMranking?TypefilteringLexical,POSCambridge?WriteandImprove?SATsystem,CambridgeLearnerCorpus,CoNLL-2013TestSet,FirstCertificateinEnglishcorpus,EnglishVocabularyProfilecorpus,MicrosoftWebLMCUUIArtOrDet,Mec,Nn,Prep,SVA,Vform,Vt,Wform,WtoneMLDifferentcombinationsofaveragedperceptron,na??veBayes,andpattern-basedlearningtrainedondifferentdatasetsfordifferenterrortypes.Lexical,POS,lemma,shallowparse,depen-dencyparseCoNLL-2013TestSet,GoogleWeb1TIITBAllMT/MLPhrase-basedtranslationoptimizedforF-scoreusingMERTandsupplementedwithadditionalRBmodulesforSVAerrorsandMLmodulesforNnandArtOrDet.Lexical,shallowparseNoneIPNAllexceptPrepLM/RBLowLMscoretrigramsareidentifiedaserrorswhicharesubsequentlycorrectedbyrules.Lexical,lemma,depen-dencyparseWikipediaNTHUArtOrDet,Nn,Prep,?Prep+Verb?,SpellingandCommas,SVA,WformRB/LM/MTExternalresourcescorrectspellingerrorswhileacondi-tionalrandomfieldmodelcorrectscommaerrors.SVAerrorscorrectedusingaRBapproach.Allothererrorscorrectedbymeansofalanguagemodel.InteractingerrorscorrectedusinganMTsystem.Lexical,POS,depen-dencyparseAspell,GingerIt,AcademicWordList,BritishNationalCorpus,GoogleWeb1T,GoogleBooksSyntacticN-Grams,EnglishGigawordPKUAllLM/MLALMisusedtofindthehighestscoringvariantofawordwithacommonstemwhilemaximumentropyclassifiersdealwitharticlesandprepositions.Lexical,POS,stemGigaword,ApacheLuceneSpellcheckerPOSTAllLM/RBN-gram-basedapproachfindsunlikelyn-gram?frames?whicharethencorrectedviahighscoringLMalterna-tives.Rule-basedmethodsthenimprovetheresultsforcertainerrortypes.Lexical,POS,de-pendencyparse,constituencyparseGoogleWeb1T,CoNLL-2013TestSet,PyEnchantSpellcheckingLibraryRACSeeFootnoteaRB/LMRule-basedmethodsareusedtodetecterrorswhichcanthenbecorrectedbasedonLMscores.Lexical,POS,lemma,shallowparseGoogleWeb1T,NewsCRAWL(2007?2012),Europarl,UNFrench-EnglishCorpus,NewsCommentary,Wikipedia,LanguageTool.orgSJTUAllRB/MLRule-basedsystemgeneratesmoredetailederrorcate-gorieswhicharethenusedtotrainasinglemaximumentropymodel.Lexical,POS,lemma,dependencyparseNoneUFCSVA,Vform,WformRBMismatchedPOStagsgeneratedbytwodifferenttag-gersaretreatedaserrorswhicharethencorrectedbyrules.POSNodeboxEnglishLinguisticsLibraryUMCAllMTFactoredtranslationmodelusingmodifiedPOStagsandmorphologyasfeatures.Lexical,POS,prefix,suffix,stemWMT2014MonolingualDataTable6:Profileoftheparticipatingteams.TheErrorcolumnliststheerrortypestackledbyateamifnotallwerecorrected.TheApproachcolumnliststhetypeofapproachused,whereLMdenotesaLanguageModelingbasedapproach,MLaMachineLearningclassifierbasedapproach,MTastatisticalMachineTranslationapproach,andRBaRule-Basedapproach.aTheRACteamusesrulestocorrecterrortypesthatdifferfromthe28officialerrortypes.Theyinclude:?thecorrectionoftheverbtenseespeciallyintimeclauses,theuseoftheshortinfinitiveaftermodals,thepositionoffrequencyadverbsinasentence,subject-verbagreement,wordorderininterrogativesentences,punctuationaccompanyingcertainlexicalelements,theuseofarticles,ofcorrelatives,etc.
?9TypeAMUCAMBCUUIIITBIPNNARANTHUPKUPOSTRACSJTUUFCUMCVt10.6619.123.791.740.8814.1810.6112.303.7626.194.170.0014.84Vm10.8122.580.000.000.0029.030.003.230.0035.900.000.006.45V017.8625.000.000.000.0036.670.000.000.000.000.000.0025.93Vform22.7624.3721.431.854.6327.6224.3025.641.8927.353.670.9514.68SVA24.3031.3670.341.0614.1427.5062.6717.3120.5630.3614.8528.7014.41ArtOrDet15.5249.4858.850.680.3350.8933.638.2054.450.6312.540.0024.05Nn58.7454.1156.104.4910.3657.3246.7641.7855.6036.4510.110.0017.03Npos14.297.694.760.000.0020.000.000.000.004.764.550.005.26Pform22.2222.587.140.000.0014.8116.1312.000.003.700.000.0017.24Pref9.3319.351.320.000.0010.001.201.351.320.000.000.0012.05Prep18.4138.2615.452.120.0029.7219.420.002.280.007.920.0014.55Wci12.009.170.940.360.357.550.631.651.270.340.000.003.23Wa0.000.000.000.000.000.000.000.000.000.000.000.000.00Wform45.5645.0517.244.052.6039.0814.8125.886.4911.251.391.3016.46Wtone81.8236.3636.360.000.0014.290.000.0028.570.0016.670.0044.44Srun0.000.000.000.000.000.000.000.000.000.000.000.000.00Smod0.000.000.000.000.000.000.000.000.000.000.000.000.00Spar0.000.000.000.000.000.000.000.000.0050.000.000.000.00Sfrag0.000.000.000.000.0020.000.000.000.000.0025.000.0025.00Ssub7.8914.630.000.002.2715.380.000.009.522.382.270.006.98WOinc0.003.030.003.570.003.030.000.000.000.000.000.006.67WOadv0.0047.620.0012.500.0043.750.000.000.000.000.000.0044.44Trans13.4321.432.861.430.0011.251.411.522.670.000.000.0012.16Mec29.3528.7515.791.024.3336.696.6730.2836.6143.510.510.0016.80Rloc?5.4120.167.760.005.5618.649.6810.489.269.092.500.0015.84Cit0.000.000.000.000.000.000.000.000.000.000.000.000.00Others0.000.000.000.000.000.000.000.000.003.120.000.000.00Um7.699.090.000.000.004.000.0015.798.708.330.000.000.00Table9:Recall(in%)foreacherrortypewithoutalternativeanswers,indicatinghowwelleachteamperformsagainstaparticularerrortype.10TypeAMUCAMBCUUIIITBIPNNARANTHUPKUPOSTRACSJTUUFCUMCVt11.6120.005.791.900.9816.1812.9014.163.3129.174.590.0017.60Vm11.1123.330.000.000.0029.030.003.330.0039.470.000.007.69V019.2329.630.000.000.0038.710.000.000.000.000.000.0030.77Vform23.9327.4221.051.924.8529.0924.0726.792.8326.963.770.9815.32SVA25.0033.9072.411.1114.7428.5763.7617.8222.8632.4315.4630.0914.95ArtOrDet18.7554.7467.381.810.3654.4237.969.6559.410.6614.630.0033.42Nn62.1462.0365.534.9112.2962.6952.8951.0164.1442.6711.930.0022.22Npos23.3340.004.350.000.0029.170.000.000.009.524.550.0013.64Pform22.2223.337.690.000.0014.8117.8612.500.004.000.000.0022.22Pref9.5918.561.320.000.009.801.251.331.370.000.000.0011.11Prep18.4138.6318.222.210.0030.2820.420.002.250.008.950.0016.98Wci15.2615.180.960.790.388.051.333.171.940.360.000.009.57Wa0.000.000.000.000.000.000.000.000.000.000.000.000.00Wform45.4546.5921.112.902.6740.9115.5827.386.4912.501.471.3717.11Wtone88.2438.4652.630.000.0012.500.000.0050.000.0033.330.0055.56Srun0.000.000.000.000.000.000.000.000.000.000.000.000.00Smod0.000.000.000.000.000.000.000.000.000.000.000.000.00Spar0.000.000.000.000.000.000.000.000.0050.000.000.000.00Sfrag0.000.000.000.000.0016.670.000.000.000.0025.000.0020.00Ssub7.8914.290.000.002.3315.380.000.009.762.442.330.006.98WOinc0.003.450.004.000.003.330.000.000.000.000.000.007.14WOadv0.0050.000.0016.670.0044.440.000.000.000.000.000.0050.00Trans14.5222.393.081.670.0011.841.561.642.820.000.000.0020.78Mec31.5630.6717.471.134.7937.287.1731.6937.8845.821.100.0022.31Rloc?5.4526.477.380.005.6221.4311.3412.3811.8210.003.660.0029.66Cit0.000.000.000.000.000.000.000.000.000.000.000.000.00Others0.003.030.000.000.000.000.000.000.003.330.000.000.00Um7.699.090.000.000.004.350.0015.004.558.700.000.000.00Table10:Recall(in%)foreacherrortypewithalternativeanswers,indicatinghowwelleachteamperformsagainstaparticularerrortype.11Team ID Precision Recall F0.5CUUI 52.44 29.89 45.57CAMB 46.70 34.30 43.55AMU 45.68 23.78 38.58POST 41.28 25.59 36.77UMC 43.17 19.72 34.88NTHU 38.34 21.12 32.97PKU?36.64 15.96 29.10RAC 35.63 16.73 29.06NARA 23.83 31.95 25.11SJTU 32.95 5.95 17.28UFC?72.00 1.90 8.60IPN?11.66 3.17 7.59IITB?34.07 1.66 6.94Table 8: Scores (in %) with alternative answers.The teams that submitted their system output af-ter the deadline have an asterisk affixed after theirteam names.Computing the recall of an error type is straight-forward as the error type of each gold-standardedit is provided.
Conversely, computing the pre-cision of each of the 28 error types is difficult asthe error type of each system edit is not availablesince the submitted system output only containscorrected sentences with no indication of the er-ror type of the system edits.
Predicting the errortype out of the 28 types for a particular systemedit not found in gold-standard annotation can betricky and error-prone.
Therefore, we decided tocompute the per-type performance based on recall.The recall scores when distinguished by error typeare shown in Tables 9 and 10.6.1 Cross Annotator ComparisonTo measure the agreement between our two an-notators, we computed Cohen?s Kappa coefficient(Cohen, 1960) for identification, which measuresthe extent to which annotators agreed which wordsneeded correction and which did not, regardlessof the error type or correction.
We obtained aKappa coefficient value of 0.43, indicating mod-erate agreement (since it falls between 0.40 and0.60).
While this may seem low, it is worth point-ing out that the Kappa coefficient does not takeinto account the fact that there is often more thanone valid way to correct a sentence.In addition to computing the performance ofeach team against the gold standard annotations ofboth annotators with and without alternative anno-tations, we also had an opportunity to compare theperformance of each team?s system against eachannotator individually.A recent concern is that there can be a highdegree of variability between individual annota-tors which can dramatically affect a system?s out-put score.
For example, in a much simplified er-ror correction task concerning only the correctionof prepositions, Tetreault and Chodorow (2008)showed an actual difference of 10% precision and5% recall between two annotators.
Table 11 henceshows the precision (P ), recall (R), and F0.5scores for all error types against the gold standardannotations of each CoNLL-2014 annotator indi-vidually.The results show that there can indeed be a highamount of disagreement between two annotators,the most noticeable being precision in the UFCsystem: precision was 70% for Annotator 2 butonly 28% for Annotator 1.
This 42% difference is,however, likely to be an extreme case, and mostteams show little more than 10% variation in pre-cision and 5% variation in F0.5.
Recall remainedfairly constant between annotators.
10% is stilla large margin however, and these results rein-force the idea that error correction systems shouldbe judged against the gold-standard annotations ofmultiple annotators.Table 12 additionally shows how each annotatorcompares against each other; i.e., what score An-notator 1 gets if Annotator 2 was the gold standard(part (a) of Table 12) and vice versa (part (b)).The low F0.5scores of 45.36% and 38.54% rep-resent an upper bound for system performance onthis data set and again emphasize the difficulty ofthe task.
The low human F0.5scores imply thatthere are many ways to correct a sentence.7 ConclusionsThe CoNLL-2014 shared task saw the participa-tion of 13 teams worldwide to evaluate their gram-matical error correction systems on a common testset, using a common evaluation metric and scorer.The best systems in the shared task achieve anF0.5score of 37.33% when it is scored withoutalternative answers, and 45.57% with alternativeanswers.
There is still much room for improve-ment in the accuracy of grammatical error correc-tion systems.
The evaluation data sets and scorerused in our shared task serve as a benchmark for12Team ID Annotator 1 Annotator 2P R F0.5P R F0.5AMU 27.30 13.55 22.69 35.49 12.90 26.29CAMB 24.96 19.62 23.67 35.22 20.29 30.70CUUI 26.05 15.60 22.97 36.91 16.37 29.51IITB 23.33 0.88 3.82 24.18 0.66 2.99IPN 5.80 1.25 3.36 9.62 1.51 4.63NARA 13.54 19.20 14.38 18.74 19.69 18.92NTHU 22.19 11.38 18.64 31.48 11.79 23.60PKU 21.53 8.36 16.37 27.47 7.72 18.17POST 22.39 13.89 19.94 29.53 13.42 23.81RAC 19.68 8.28 15.43 28.52 8.80 19.70SJTU 21.08 3.09 9.75 24.64 2.59 9.12UFC 28.00 0.59 2.70 70.00 1.06 4.98UMC 20.41 8.78 16.14 26.63 8.38 18.55Table 11: Performance (in %) for each team?s output scored against the annotations of a single annotator.P R F0.550.47 32.29 45.36(a)P R F0.537.14 45.38 38.54(b)Table 12: Performance (in %) for output of one gold standard annotation scored against the other goldstandard annotation: (a) The score of Annotator 1 if Annotator 2 was the gold standard, (b) The score ofAnnotator 2 if Annotator 1 was the gold standard.future research on grammatical error correction4.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.We thank our two annotators Mark Brooke and Di-ane Nicholls who provided the gold-standard an-notations.ReferencesSteven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Tiberiu Boros?, Stefan Daniel Dumitrescu, AdrianZafiu, Dan Tufis?, Verginica Mititelu Barbu, andPaul Ionut?
V?aduva.
2014.
RACAI GEC ?
a hybridapproach to grammatical error correction.
In Pro-ceedings of the Eighteenth Conference on Computa-tional Natural Language Learning: Shared Task.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.4http://www.comp.nus.edu.sg/?nlp/conll14st.htmlDaniel Dahlmeier and Hwee Tou Ng.
2011a.
Cor-recting semantic collocation errors with L1-inducedparaphrases.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 107?117.Daniel Dahlmeier and Hwee Tou Ng.
2011b.
Gram-matical error correction with alternating structureoptimization.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics, pages 915?923.Daniel Dahlmeier and Hwee Tou Ng.
2012a.
A beam-search decoder for grammatical error correction.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages568?578.Daniel Dahlmeier and Hwee Tou Ng.
2012b.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages568?572.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS Corpus of Learner English.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31.13Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th EuropeanWorkshop on Natural Lan-guage Generation, pages 242?249.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the 7th Workshop on the Innovative Use ofNLP for Building Educational Applications, pages54?62.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth Conference on LanguageResources and Evaluation, pages 449?454.Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-len Yannakoudakis, and Ekaterina Kochmar.
2014.Grammatical error correction using hybrid systemsand type filtering.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning: Shared Task.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing: A meta-classifierapproach.
In Proceedings of the Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics, pages 163?171.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(2):115?129.S.
David Hernandez and Hiram Calvo.
2014.
CoNLL2014 shared task: Grammatical error correction witha syntactic n-gram language model from a big cor-pora.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2014.
The AMU system in the CoNLL-2014shared task: Grammatical error correction by data-intensive and feature-rich statistical machine trans-lation.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.Anoop Kunchukuttan, Sriram Chaudhury, and PushpakBhattacharyya.
2014.
Tuning a grammar correctionsystem for increased precision.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morgan &Claypool Publishers.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.In Proceedings of the Seventeenth Conference onComputational Natural Language Learning: SharedTask, pages 1?12.Diane Nicholls.
2003.
The Cambridge Learner Cor-pus: Error coding and analysis for lexicography andELT.
In Proceedings of the Corpus Linguistics 2003Conference, pages 572?581.Alla Rozovskaya and Dan Roth.
2010.
Generatingconfusion sets for context-sensitive error correction.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages961?970.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,Dan Roth, and Nizar Habash.
2014.
The Illinois-Columbia system in the CoNLL-2014 shared task.In Proceedings of the Eighteenth Conference onComputational Natural Language Learning: SharedTask.Joel R. Tetreault and Martin Chodorow.
2008.
Na-tive judgments of non-native usage: Experiments inpreposition error detection.
In COLING Workshopon Human Judgments in Computational Linguistics,Manchester, UK.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In Proceedings of the ACL 2010Conference Short Papers, pages 353?358.Peilu Wang, Zhongye Jia, and Hai Zhao.
2014a.Grammatical error detection and correction using asingle maximum entropy model.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task.Yiming Wang, Longyue Wang, Derek F. Wong,Lidia S. Chao, Xiaodong Zeng, and Yi Lu.
2014b.Factored statistical machine translation for gram-matical error correction.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning: Shared Task.Yuanbin Wu and Hwee Tou Ng.
2013.
Grammat-ical error correction using integer linear program-ming.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics,pages 1456?1465.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 180?189.14
