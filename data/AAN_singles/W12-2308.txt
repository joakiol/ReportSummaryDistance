Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 62?71,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsLearning probabilities over underlying representationsJoe Pater*pater@linguist.umass.eduKaren Jesney?jesney@usc.edu*Department of LinguisticsUniversity of Massachusetts AmherstAmherst, MA 01003 USARobert Staubs*rstaubs@linguist.umass.eduBrian Smith*bwsmith@linguist.umass.edu?Department of LinguisticsUniversity of Southern CaliforniaLos Angeles, CA 90089 USAAbstractWe show that a class of cases that has beenpreviously studied in terms of learning ofabstract phonological underlying representa-tions (URs) can be handled by a learner thatchooses URs from a contextually conditioneddistribution over observed surface representa-tions.
We implement such a learner in a Max-imum Entropy version of Optimality Theory,in which UR learning is an instance of semi-supervised learning.
Our objective functionincorporates a term aimed to ensure general-ization, independently required for phonotac-tic learning in Optimality Theory, and doesnot have a bias for single URs for morphemes.This learner is successful on a test languageprovided by Tesar (2006) as a challenge forUR learning.
We also provide successful re-sults on learning of a toy case modeled onFrench vowel alternations, which have alsobeen previously analyzed in terms of abstractURs.
This case includes lexically conditionedvariation, an aspect of the data that cannot behandled by abstract URs, showing that in thisrespect our approach is more general.1 IntroductionPhonological underlying representations (URs) in-troduce structural ambiguity.
For example, a mor-pheme that alternates in voicing, like the one mean-ing ?cat?
in Table 1, could have as its underlyingrepresentation /bet/ or /bed/, amongst other possibil-ities.
Underlying /bed/ for surface [bet] requires fi-nal devoicing, while intervocalic voicing is requiredfor underlying /bet+a/ for [beda] (/-a/ marks the plu-ral).
The ambiguity can often be resolved on theUR SR Meaninga.
/bed/ [bet] catb.
/bed+a/ [beda] catsc.
/mot/ [mot] dogd.
/mot+a/ [mota] dogsTable 1: Standard URs for final devoicingbasis of further data.
For example, if the languageincludes both voiced and voiceless consonants inter-vocalically, as in our toy language which also con-tains [mota], then intervocalic voicing cannot applyacross-the-board.
The standard phonological anal-ysis, proposed by Jakobson (1948) for similar datafrom Russian, would thus posit /bed/ as the underly-ing form for ?cat?, as in Table 1, along with a phono-logical grammar that generates final devoicing.An alternating morpheme can also be given a URthat encodes only the fixed aspects of its structure.For example, ?cat?
could have as its UR /beT/, where/T/ represents an alveolar plosive unspecified forvoicing.
The grammar would then fill in its voicingspecification appropriately in both contexts, adding[?voice] finally, and [+voice] intervocalically.
Oneuse of this underspecification is to capture instancesof three-way contrast.
For example, the language inTable 2 has consonants that alternate in voicing, asin the singular and plural of ?cat?, as well as conso-nants that are both fixed voiceless (?dog?/?dogs?)
andvoiced (?pig?/?pigs?).
Given the URs shown in Table2, the surface forms are generated if a grammar fillsin voicing on underspecified consonants, and doesnot change specified ones, as in the analysis of Turk-ish in Inkelas et al (1997).62UR SR Meaninga.
/beT/ [bet] catb.
/beT+a/ [beda] catsc.
/mot/ [mot] dogd.
/mot+a/ [mota] dogse.
/wid/ [wid] pigf.
/wid+a/ [wida] pigsTable 2: Underspecified URs and ternary contrastThere are alternatives to this sort of underspec-ification.
For example, the analysis of Turkish inBecker et al (2011) posits lexically specific intervo-calic voicing, applying to some words but not others.Here we pursue the learning consequences of a pro-posal in Kager (2008), which involves a grammarthat chooses different URs across surface contexts.In this example, /bet/ would be chosen when themorpheme occurs word-finally as in [bet], and /bed/when it occurs prevocalically, as in [beda] (see Table3 rows a. and b.).
This is a kind of over-specificationin that the meaning ?cat?
has two phonological URs.The non-alternating morphemes /mot/ and /wid/ dif-fer in having only a single UR, with voiceless andvoiced final consonants respectively, thus yieldingthe three-way contrast.Grammars must be able to choose between URsacross surface contexts in order to handle phonolog-ically conditioned suppletive allomorphy - i.e.
al-ternation between forms of a morpheme that are notrelatable by a phonological derivation even thoughthe contexts in which each occurs is phonologicallydefined.
The alternation between the forms of the in-definite determiner ?a?
and ?an?
in English is some-times analyzed as UR choice, since there is no gen-eral process in English of [n] insertion or deletion,but the conditioning context is phonological (vowel-vs. consonant-initial following word).
That gram-mars have the power to choose URs in this wayis uncontroversial; the only controversies concernthe proper formalization of UR choice, and whetherparticular cases involve UR choice or derivation(Nevins, 2011).Kager?s proposal for ternary contrast is unusualin that it uses UR choice for cases that do seem rel-atively amenable to analysis in terms of derivationsfrom single URs.
Phonologists tend to regard a URUR SR Meaninga.
/bet/ [bet] catb.
/bed+a/ [beda] catsc.
/mot/ [mot] dogd.
/mot+a/ [mota] dogse.
/wid/ [wid] pigf.
/wid+a/ [wida] pigsTable 3: UR choice and ternary contrastchoice analysis as more of a last resort, but as far aswe know, there exists no explicit proposal for whenan analyst, or a learner, should adopt an analysiswith multiple URs for a single morpheme, and whena single UR analysis is required.One worry about a multiple UR analysis is that itcould fail to generalize appropriately.
If a learnersimply memorized which phonological forms ofeach morpheme appeared in which contexts, it couldfail to extract generalizations, such as the restrictionagainst voicing of word-final consonants in our lan-guage in Table 1.
This is of course a familiar generalissue in learning, and it is the focus of our attentionhere.
We consider a learner to have successfully ac-quired a language if it finds a grammar that general-izes appropriately, irrespective of the extent to whichthe learner uses a single phonological UR for eachmeaning.Presumably, the assumption that multiple URanalyses of alternations are incompatible with gen-eralization is the basis for their traditional last resortstatus in phonological theory.
However, in at leastthe grammatical framework that we adopt, and prob-ably in many others, it is possible to construct analy-ses in which alternations are handled by UR choice,and in which generalizations are still captured.
Aconcrete example is provided by the analysis of thefinal devoicing language illustrated in Tables 4 and5, and also by each of the results of the learning sim-ulations presented in sections 3 and 4.Table 4 shows the distribution over URs that ourlearner, described with references to precedents inthe next section, posits for the final devoicing lan-guage.
The learner?s final grammar is using URchoice to get context-appropriate surface forms of?cat?, as can be seen in rows a. and b.
The grammarusually picks /bet/ as the UR for ?cat?
when it oc-63UR SR Meaninga.
/bet/ (0.92) /bed/ (0.08) [bet] catb.
/bed+a/ [beda] catsc.
/mot/ [mot] dogd.
/mot+a/ [mota] dogsTable 4: Learned URs for final devoicingcurs finally as in [bet], and almost always picks /bed/when it occurs prevocalically as in [beda].
This anal-ysis diverges even further from standard phonolog-ical practice than Kager?s ternary contrast analyses,since we have multiple URs where a single UR anal-ysis would not require underspecification or a lexi-cally specific grammar.
Furthermore, in this anal-ysis UR choice is probabilistic, as shown visuallyin Table 4 row a: /bed/ chosen as the UR in word-final position with probability 0.08.
ProbabilisticUR choice, which also diverges from the analyticnorm in phonology, does not have any observableeffect here since the URs neutralize to [bet], but weput it to use in the analysis of French in section 4.These choices of URs and SRs are being made bya probabilistic weighted constraint version of Opti-mality Theory (OT) (Prince and Smolensky, 2004),described in the next section.
The Input is a string ofmorphemes (?meanings?
), and a candidate is a (UR,SR) pair.
Throughout this paper, the candidate URsfor a morpheme are all and only its forms observedas SRs (given morphologically segmented words).For the current languages, we include as candidateSRs the identity maps from the URs, and the SRsformed by devoicing any final consonant, or voicingany intervocalic one.There are three types of constraint.
UR con-straints (Zuraw, 2000; Boersma, 2001) demand aparticular UR for a given morpheme, and are vi-olated when a UR differs from the specified one(Boersma and Zuraw?s own formalizations differsomewhat).
In Table 5, there are two such con-straints, CAT?/bed/ and CAT?/bet/.
We omit URconstraints for non-alternating morphemes, sincetheir candidate (UR, SR) pairs always have the sameUR, and they always satisfy the single UR con-straint.
Faithfulness constraints demand (UR, SR)fidelity; here we employ only IDENT-VOICE, whichrequires a match in voicing specification (McCarthyConstraint Devoicing ContrastCAT?/bed/ 3.65 0CAT?/bet/ 0 0IDENT-VOICE 6.05 43.62NO-CODA-VOICE 401.41 39.83INTER-V-VOICE 1.94 39.83Table 5: Learned weightsand Prince, 1999).
Finally, Output constraints (AKAMarkedness constraints) place demands on the SRs.Here we use NO-CODA-VOICE, which penalizes fi-nal voicing, and INTER-V-VOICE, which penalizesan intervocalic voiceless consonant.Table 5 shows the weights for the constraints thatwere found for the final devoicing language (De-voicing), and for the language with ternary con-trast (Contrast); these yield with high probabilitythe (UR, SR) choices for Tables 4 and 3 respec-tively.
The competition between (/bet/, [bet]) and(/bed/, [bet]) as (UR, SR) pairs for ?cat?
illustratesthe effects of the first three constraints.
The two URconstraints obviously differ in their assessments ofthe two candidates, as does IDENT-VOICE, whichprefers the faithful mapping (/bet/, [bet]) over a voic-ing change in (/bed/, [bet]).
For the final devoic-ing language, the summed weight of IDENT-VOICEand CAT?/bet/ (6.05) is greater than the weightof CAT?/bed/ (3.65), and so the grammar assignshigher probability to (/bet/, [bet]), as shown in Ta-ble 4.
For the ternary contrast language on the otherhand, the UR constraints have zero weight, and sothe decision is fully determined by the relativelyhigh weighted IDENT-VOICE, favoring (/bet/, [bet]).Even though the learner of the final devoicing lan-guage has not acquired the single UR of the tradi-tional phonological analysis, it has acquired a con-textually conditioned distribution over UR choicesthat is appropriate for the learning data.
There areweights on the UR constraints that would fail toyield this result.
For example, if CAT?/bet/ hada sufficiently high weight relative to the other con-straints, then the UR would be fixed as /bet/, andthere would be no weighting of the remaining con-straints that would pick both [beda] as the highestprobability candidate for ?cats?, and [mota] as thehighest probability candidate for ?dogs?.64Anticipating the discussion of learning in the nextsection, the weight configuration just described canform a local minimum for our learner.
In our simu-lations, it does not fall into this minimum, nor otherslike it, when weights are initialized at zero.The effects of the Output constraints are seen inthe choice of URs for ?cat?
across phonological con-texts in both the final devoicing and ternary con-trast languages.
NO-CODA-VOICE prefers word-final (/bet/, [bet]) over (/bed/, [bed]), and INTER-V-VOICE prefers intervocalic (/bed+a/, [beda]) over(/bet+a/, [beta]).
The high weight on IDENT-VOICEin the ternary contrast language results in very lowprobability for the unfaithful (UR, SR) mappings(/bed/, [bet]) and (/bet+a/, [beda]).
The weights forthe coda devoicing language are such that a non-negligible proportion of the probability is reservedfor unfaithful (/bed/, [bet]).Since we have in the case of final devoicing an ex-ample of a multiple UR analysis for a language witha phonological regularity, we need to ask whetherthe grammar generalizes appropriately.
The an-swer is yes.
Because of the high weight of NO-CODA-VOICE (401.41) and relatively low weight ofIDENT-VOICE (6.05), an underlying voiced obstru-ent will with extremely high probability map to asurface voiceless one in word-final position.
In gen-erating final devoicing this grammar produces pre-dictable relationships between morphologically re-lated words.
For example, if a learner with thisgrammar were to see a plural like [maga] and no sin-gular form, it would posit only /mag/ as the UR forthe root.
Nonetheless, it would predict with proba-bility near 1 that the singular is pronounced [mak].Given the observed data from the language inTable 4, it would not have been necessary for thelearner to construct a grammar that generalizes inthis way.
For example, the grammar learned for theternary contrast language also generates the alter-nation between [bet] and [bed+a], without produc-ing generalized final devoicing.
We thus require alearner with a bias for generalization.
Our learner,described in the next section, meets this requirementby incorporating an independently motivated prefer-ence for high weighted Output constraints, and lowweighted Faithfulness.
After describing the learner,we go on to provide simulations for somewhat morecomplex learning problems.2 The grammar and learning modelsIn Maximum Entropy or MaxEnt grammar (Gold-water and Johnson, 2003), the probability of an in-put/output pair (xi, yij) is determined by its har-mony.
The harmony Hij of such a pair is thesum of constraint violations fc(xi, yij) scaled by theweights of the constraints wc.Hij =?cwcfc(xi, yij)This definition of harmony is a common prop-erty of grammars that use weighted constraints, asin Harmonic Grammar (Smolensky and Legendre,2006).
A MaxEnt grammar maps harmonies to prob-abilities, where the probability of a particular outputfor a particular input p(yij | xi) is proportional tothe exponential of its harmony.
These exponentialsare normalized within an input, yielding probabilitydistributions.p(yij | xi) =1ZieHijZi =?j?eHij?As discussed above, our output candidates aremore elaborate than simple surface forms.
Instead,inputs are strings of morphemes and candidates are(UR, SR) pairs.
A string of input morphemes xican map to an SR yij in potentially many ways?through many possible URs.
Each of these (Input,UR, SR) triples potentially incurs distinct constraintviolations.
The Input/UR pairing is controlled bythe UR constraints, while the UR/SR pairing is con-trolled by Faithfulness.
We thus expand our defini-tion of the probability of a mapping from Input toSR to include all options for the URs zijk.p(yij | xi) =?kp(yij , zijk | xi)The probabilities p(yij , zijk | xi) are defined justas for simple input/output probabilities?they sim-ply include a contribution from candidates on URs.This definition encodes an idea that all URs arepotentially valid ways of reaching a particular SR,determined only by the relevant violations of con-straints, and does not require a single UR to exist forevery Input/SR pairing.65The URs zijk considered for an input xi are deter-mined by the UR constraints.
A UR zijk is includedin the probability calculation for input xi only ifthere exists some constraint xi ?
zijk.
These URconstraints, in turn, rely on observed mappings.
Forevery SR yij corresponding to an input xi, we in-clude a UR constraint xi ?
yij .
Thus the candidateURs are simply observed surface forms.
In the caseof a non-alternating form, only one UR constraintwill be included and thus only one UR is entertained.In such cases these constraints are always satisfied;we therefore omit them from our analyses withoutloss of correctness.This grammatical framework allows a way ofviewing the problem of learning as somewhat ag-nostic with respect to URs.
The learner observessome particular distribution over SRs for a partic-ular input morpheme string and can make any con-sistent choice about the distribution over URs.
It isin this respect that our approach diverges most im-portantly from prior work on learning URs in Opti-mality Theory-like frameworks.
Our model incorpo-rates ideas from Apoussidou (2007), who uses URconstraints for on-line learning of URs in a prob-abilistic OT framework, and Eisenstat (2009), whouses a log-linear model very similar to ours.
Our ap-proach differs, however, in that learning of uniqueURs is not taken as a goal.With the above explicit statement of probabilities,the learner?s problem is then to minimize the distinc-tion between its predicted Input/SR distribution andthe observed probabilities.
For the results presentedhere, we minimize the Kullback-Leibler (KL) diver-gence (Kullback and Leibler, 1951) between the pre-dicted distribution pw and observed distribution p?.D(p?
|| pw) =?i?jp?
(yij | xi) logp?
(yij | xi)pw(yij | xi)We use an L2 (Gaussian) prior (Tychonoff andArsenin, 1977) on the weights.
Such a prior in-troduces a pressure for lower weights, which is es-pecially important for categorical learning cases (inwhich KL minimization reduces to likelihood max-imization).
These problems contain probabilities atunity, causing weights to scale arbitrarily high with-out additional restriction.
We used a regularizationwith ?2 = 10, 000 for all solutions presented in thispaper.w?
= argminwD(p?
|| pw) +12?2?cw2cWe also include in our prior a term that maximizesthe sum of the weights of Output constraints, andminimizes the sum of the weights of Faithfulnessconstraints.
The objective function remains boundedfrom above by the L2 prior, and is also boundedfrom below by a restriction to non-negative weights.This term is adapted from research on phonotacticlearning in OT starting with Smolensky (1996); seefurther references in Jesney and Tessier (2011).
Itresembles somewhat the R-measure of Prince andTesar (2004), but unlike the R-measure this addedprior is continuous, improving performance in opti-mization.???
?f?Fwf ??o?Owo?
?In experimentation, we found that this term wasnecessary to ensure generalization; the L2 prioralone, even with a smaller variance for Faithfulnessthan Output constraints, was insufficient.
It mightbe possible to create a more refined version of thisterm that is sensitive to dependencies between con-straints, but this version has sufficed for our pur-poses.
The scaling factor ?
controls the relativeimportance of generalization compared to KL mini-mization.
For the solutions presented here, the valueof ?
was chosen on the basis of repeated optimiza-tions.
?
was decreased gradually until a criterionlevel of performance was reached.
For categoricalcases, this criterion level was a likelihood of greaterthan 0.95.
For non-categorical cases, criterion wasa sum squared error of less than 0.05.
The mini-mization problem presented here was solved usingthe L-BFGS-B method (Byrd et al, 1995) as im-plemented in R (R Development Core Team, 2010),and all optimizations were constrained to use non-negative weights, with weights initialized at zero.11Scripts and input files are available athttp://blogs.umass.edu/hgr/examples-and-other-resources-for-perceptron-and-solver-r/.66/re-/ /ra:-/ /ro?-/ /ru?
:-//-se/ [re?se] [ra?
:se] [ro?se] [ru?
:se]/-sa?/ [resa?]
[rasa?]
[ro?sa] [ru?:sa]/-so?
:/ [reso?
:] [raso?
:] [ro?so] [ru?
:so]Table 6: Abstract UR analysis of Tesar?s language3 Stress-length interactionTo illustrate some of the challenges of UR learning,Tesar (2006) provides the toy language in Table 6.The table shows the phonological results of combin-ing four initial, perhaps root, morphemes with threefinal, perhaps suffix, morphemes.
The phonologi-cally relevant differences between the vowels are inlength, marked with a colon, and stress, marked withan acute accent.
The rows and columns are labeledwith the URs that Tesar posits; we will discuss theirjustification shortly.Stressed vowels can either be short or long, butthere is an absolute surface restriction against stress-less long vowels.
The stress-alternating morphemesthat have long allomorphs, ?ra?
and ?so?, show a pre-dictable alternation in length: long when stressed,short when stressless.
There is also a preference forstress on roots.
Although the suffixes ?sa?
and ?so?attract stress over roots ?re?
and ?ra?, they lose theirstress to fixed stress roots ?ru?
and ?ro?, and there areno fixed stress suffixes.Tesar?s URs represent the contrastive propertiesof the morphemes.
The contrast between vowelsthat are long when stressed and those that are al-ways short is encoded as an underlying difference inlength.
The contrast between the suffixes that attractstress and those that don?t is similarly encoded asan underlying difference in stress, as is the contrastbetween roots that alternate in stress and those thatdon?t.
The abstract UR is /ra:/, which never surfacesin that shape due to the restriction against unstressedlong vowels.
The vowel must be long to contrastwith /re/, and stressless to contrast with /ru?
:/.We adopt Tesar?s Output and Faithfulness con-straints.
STRESS-ROOT demands stress on the root,and STRESS-SUFFIX demands stress on the suffix.Output words are limited to a single stress, so oneof these constraints is always violated.
NO-LONG-UNSTRESS is violated by a surface long stresslessvowel.
NO-LONG penalizes all long vowels.
TheUR SR p UR SR p/re?+se/ [re?se] 0.98 /re+sa?/ [resa?]
1/re+se/ 0.02/re+so?
:/ [reso?
:] 1 /ra?
:+se/ [ra?
:se] 0.99/ra+se/ [ra?se] 0.01/ra+sa?/ [rasa?]
1 /ra+so?
:/ [raso?
:] 0.99/ra?
:+so/ [ra?
:so] 0.01/ro?+se/ [ro?se] 1 /ro?+sa/ [ro?sa] 0.93/ro?+sa?/ 0.07/ro?+sa?/ [rosa?]
0.01/ro?+so/ [ro?so] 0.99 /ru?
:+se/ [ru?
:se] 1/ro?+so?
:/ [roso?
:] 0.01/ru?
:+sa/ [ru?
:sa] 0.93 /ru?
:+so/ [ru?
:so] 1/ru?
:+sa?/ 0.07Table 7: Learned analysis of Tesar?s languageFaithfulness constraint IDENT-STRESS demands a(UR, SR) match in stress, and IDENT-LONG de-mands (UR, SR) fidelity in length.
We include inaddition a set of UR constraints that demand formscorresponding to each of the observed SRs, exceptfor those that have only a single SR, whose UR isfixed.
Candidate SRs for each UR were all combina-tions of stress on either the root or suffix (not both),and faithful and shortened long vowels.The resulting analysis is shown in Table 7, withprobabilities rounded to two decimal points.
Can-didates whose probabilities round to zero are omit-ted.
In all cases a candidate (UR, SR) pair with thecorrect SR is given highest probability, and is listedin the first row of each cell.
Subsequent rows thatcontain only a UR have the same SR; identical SRsare omitted to aid readability.
Given a probabilis-tic model like a MaxEnt grammar, one cannot de-fine success on a categorical language like this onein terms of granting p = 1 to the correct forms, sincethis will by definition never happen (unless there isonly one candidate in a candidate set).
Our objec-tive function is stated in terms of maximizing thesummed probability of all (UR, SR) pairs that havethe correct SR, and an appropriate criterion is there-fore to require that the summed probability over fullstructures be greater for the correct SR than for anyother SR. We thus term this simulation successful.We further note that given a MaxEnt grammar thatmeets this criterion, one can make the probabilities67Constraint WeightNO-LONG-UNSTRESS 26.43STRESS-ROOT 26.05STRESS-SUFFIX 23.50IDENT-STRESS 7.66IDENT-LONG 6.50?SA?
?/sa?/ 5.04?SO??/so?
:/ 4.96?RE?
?/re/ 3.85?RA?
?/ra/ 3.15?RA??/ra?
:/ 0.25?SO?
?/so/ 0.02?SA?
?/sa/ 0?RE?
?/re?/ 0NO-LONG 0Table 8: Learned weights for Tesar?s languageof the correct forms arbitrarily close to 1 by scalingthe weights (multiplying them by some constant).The constraint weights for the analysis are shownin Table 8.
Both of the faithfulness constraintsIDENT-STRESS and IDENT-LONG have reasonablyhigh weights, which is expected given the observedcontrasts in stress and vowel length across mor-phemes.
The highest probability (UR, SR) map-pings are in fact always faithful, with alternationsarising from different URs being chosen acrossphonological contexts.The crucial case for comparison with the abstractUR analysis is the choice between long stressed/ra?
:/ and short stressless /ra/, shown with underlin-ing in Table 7.
When the morpheme ?ra?
combineswith ?se?, (/ra?
:+se/, [ra?
:se]) is preferred to (/ra+se/,[rase?
]), partly because it avoids an IDENT-STRESSviolation on the suffix, and also partly because ofthe greater weight of STRESS-ROOT than STRESS-SUFFIX.
On the other hand, when the input is ?ra?and ?sa?, IDENT-STRESS is no longer at issue since?sa?, unlike ?se?, provides the option of a stressedUR.
In this case, the sum of the weights of the con-straints preferring short stressless /ra/ in (/ra+sa?/,[rasa?])
is greater than for those preferring /ra?
:/ in(/ra?
:+sa/, [ra?:sa]).
The fixed stress roots differ from?ra?
in not providing the option of a stressless UR, sothat a violation of IDENT-STRESS would be incurredif the suffix were stressed.
While the constraint in-teractions are more complex here, UR choice suc-ceeds in replacing underspecification in a parallelfashion to the simpler case of the ternary voicingcontrast discussed in the introduction.The Output constraints sensitive to vowel lengthare in the expected configuration given the restric-tion of long vowels to stressed syllables: unviolatedNO-LONG-UNSTRESS has a relatively high weight(the highest), while the often-violated NO-LONG,which penalizes all long vowels, has a relatively lowweight (the lowest).
IDENT-LONG is sandwichedin between, with the result that an underlying longvowel that surfaces in a stressed syllable will retainits length, while one that surfaces in a stressless syl-lable will be realized as short, with probabilities ap-proaching 1.Because of the availability of UR choice, the map-ping from an underlying long vowel to a surfaceshort stressless one that high-weighted NO-LONG-UNSTRESS generates is never observed in Table 7.However, it is the high probability of this mappinggiven underlying length and surface stresslessnessthat ensures that the grammar generalizes appropri-ately.
One paradigmatic regularity in this languageis that stressless vowels are short, even when theyoccur in morphemes whose stressed variants havelong vowels.
To see how this is captured, imaginethat a learner with the grammar in Table 8 were pre-sented with a new morpheme ?su?
in combinationwith ?re?, which resulted in SR [resu?:].
Given thesegmentation [re+su?
:], it would then form the UR/su?
:/, containing the long stressed vowel of the onlyalternant that it had seen.
The morpheme ?ru?
alsohas a single UR, /ru?
:/, since it is only observed in thelearning data as [ru?:].
When these are combined as/ru?:+su?
:/ the resulting SR will be [ru?
:su], with prob-ability near 1.
That is, the grammar generalizes thelength alternations, as well as the stress alternationsthat occur because of the preference for root oversuffix stress.4 Lexically conditioned variationHere we apply our model to a case of variation,French vowel deletion, which is formalized in termsof candidate SRs having probabilities intermediatebetween 1 and 0.
This case is of particular inter-est because the probability of deletion varies across68Word UR SR pa. femelle /f?mEl/ [f?mEl] 1b.
semestre /sVmEstK/ [s?mEstK] 0.8[smEstK] 0.2c.
semelle /sVmEl/ [s?mEl] 0.5[smEl] 0.5d.
Fnac /fnak/ [fnak] 1e.
breton /bK?tO?/ [bK?tO?]
1Table 9: Underspecified URs for French and datawords, which can be captured in terms of differencesin weights of UR constraints.In French, the mid-vowel [?]
is variably deleted(this vowel is sometimes called ?schwa?, though itis not an IPA schwa in most varieties).
Like one ofthe toy voicing languages in section 1, French hasa ternary contrast, this time in vowel specification.Words either have a non-alternating [?]
(?femelle?
),an alternating [?]
(?semestre?, ?semelle?
), or no [?](?Fnac?).
The ternary contrast has been analyzedby Anderson (1982) as the result of underspecifica-tion.2 As shown in Table 9, a UR with an under-specified vowel (/V/) is able to be deleted, while aUR with a fully specified vowel (/?/) is not.The proportions in Table 9 are partially arbitrary,but accurately reflect the relative probabilities in de-scriptions such as Dell (1973) and in speaker judg-ments (Racine, 2007).
These show that alternatingvowels exhibit a range of deletability.
Dictionariesalso find the two-way distinction between deletingand non-deleting vowels descriptively inadequate,and a number of experimental and corpus studiesfind a range of deletion rates across words.
Near-minimal pairs in which deletion can occur in bothwords but at different rates, such as ?semaine?
and?semestre?, show that differences in deletion ratescannot be attributed solely to phonological differ-ences, and must be encoded in the the lexicon.Although [?
]s can be optionally deleted when pre-ceded by a single consonant as in Table 9, [?]
cannever be deleted when its deletion would create a2Anderson (1982) argues that underspecification explainsthe fact that the alternating vowels can both participate in dele-tion and alternate with [E], while the non-alternating /?/ can doneither.
However, Morin (1988) presents a number of examplesof words that participate in [E]-alternation without participatingin deletion.UR V SR p UR V SR pY s?mestre 0.08 Y s?melle 0.04N s?mestre 0.15 N s?melle 0.45Y semestre 0.77 Y semelle 0.47N semestre 0.01 N semelle 0.03Y f?melle 0.09 N F[?
]nac 0.07Y femelle 0.91 N Fnac 0.93Y breton 1Table 10: Learned analysis of FrenchConstraint Weight*CCC 467.26MAX 4.93?SEMESTRE?
?/s?mEstK/ 4.23?SEMELLE?
?/s?mEl/ 2.71*[?]
2.58?SEMELLE?
?/smEl/ 0.10?SEMESTRE?
?/smEstK/ 0.03DEP 0.00Table 11: Learned weights for Frenchthree-consonant sequence within a word, as in ?bre-ton?
[bK?tO?].
There are also no words with this sortof three-consonant sequence.
In addition to learningthe differences in the deletion rates of optional [?
]s,the learner must learn the generalization that an [?
]must be present in the ?breton?
environment.
Givena /CCC/ input, we want the grammar to avoid thethree-consonant cluster by inserting a vowel.The phonological conditioning of deletion in realFrench is far more complex than our simple sketch,but this simplified version is sufficient for presentpurposes.
We use the following constraints.
TheOutput constraints *[?]
and *CCC militate against[?]
and three-consonant sequences in the SR, respec-tively.
The faithfulness constraint MAX requiressegments in the UR to be present in the SR (?no dele-tion?
), while DEP requires SR segments to be in theUR (?no insertion?).
As in the previous sections, URconstraints are only included for morphemes withmore than one SR.
The learning data consisted ofthe SRs and probabilities from Table 9.The resulting analysis is shown in Table 10, us-ing the orthographic convention of marking the lackof a vowel with an apostrophe.
The presence of69an underlying vowel is indicated with a ?Y?
in theUR column, and its lack with an ?N?.
The analysiscaptures the difference between the rates of [?]
in?semelle?
and ?semestre?
as a difference in UR selec-tion.
The UR with [?]
is more likely for ?semestre?than ?semelle?.
The source of this difference can beseen in the constraint weights in Table 11.
The dif-ference between the weights of the UR constraintfor ?semestre?
requiring the vowel and the one thatomits it is greater than that for ?semelle?.
Thephonological generalization that three-consonant se-quences are forbidden is captured by the high weightof *CCC relative to DEP, which means that thegrammar will add a vowel to a /CCC/ input.The contrast between the rates of deletion in?semelle?
and ?semestre?
illustrates a widespreadphenomenon that is unaddressed by most OT ap-proaches to variation and learning, termed lexicallyconditioned variation (Coetzee and Pater, 2011).That it is handled in at least this toy version ofFrench is a great benefit of this approach.
Under-specification, on the other hand, offers no leverageon this problem, since it provides only a distinctionbetween deleting and non-deleting vowels, and notthe finer grained distinctions that the data require.5 ConclusionsIt is a generally unresolved issue how a learner de-cides whether to use one, or more, URs in an analy-sis of an alternation.
Presumably, learners begin byencoding the various phonological realizations of amorpheme.
How, and when, do they decide to col-lapse these into a single UR?
The problem is mademore difficult because as noted in the introduction,learners need to consider contextually conditionedUR choice, which is required for at least phonolog-ically conditioned suppletive allomorphy.
Previouswork on UR learning, including Tesar (2006), ab-stracts from this issue by allowing only single URs.As a reviewer suggests, a Minimum DescriptionLength criterion might create a bias for fewer URs,but this seems not yet to have been implemented.In the present approach, phonological general-izations can be acquired even when multiple URsare used, as shown in all of our simulations.
Thismeans that the issue raised in the last paragraph canbe completely sidestepped by never requiring learn-ers to adopt single URs for alternating morphemes.This approach also sidesteps the difficult issues ofchoosing which parts of each alternant make up thesingle UR, and when to leave some structure un-derspecified.
With the French simulation, we havefurther shown that UR choice handles data that es-cape underspecification.
These advantages suggestthat the single UR doctrine, in place since Jakobson(1948), is worth reconsidering, especially in frame-works like OT that can formalize contextual choiceof URs without loss of generalization.One direction for further research is in model-ing not only choice between allomorphs, but alsotheir discovery in morpheme segmentation, whichinvolves increasing the size of the hypothesizedUR constraint set.
Our initial explorations showpromise, and this could lead to useful applicationsin natural language processing, in which MaxEntmodels are of course already common.
Another ex-tension is to other cases of semi-supervised learn-ing.
Here we sum over all of the (UR, SR) pairscorresponding to an observed form.
Similar sum-mations can be made over other full structures whenthe learning data are incomplete: over representa-tions such as syllable structures and syntactic trees,and even over derivations.
One such extension thatwe have explored is to learning ?opacity?
(Kiparsky,1973); see Staubs and Pater (2012) for initial re-sults, which do rely on a type of abstract UR.
Finally,one might attempt to model learning of paradigmaticgeneralizations that are probabilistic across the lexi-con, as in Turkish voicing (Becker et al, 2011) - seethe related MaxEnt results in Hayes et al (2009) andMoore-Cantwell (2012).AcknowledgmentsWe especially thank Diana Apoussidou and DavidSmith for their collaboration on earlier presentationsof this work, and Mark Johnson for extended discus-sion.
Thanks also to Adam Albright, Paul Boersma,Naomi Feldman, Jeff Heinz, John McCarthy, PaulSmolensky, Colin Wilson and three anonymous re-viewers for helpful comments.
This research wassupported by NSF Grant 0813829 to the Universityof Massachusetts Amherst, by an NSF Graduate Re-search Fellowship to Robert Staubs, and a SSHRCCdoctoral fellowship to Karen Jesney.70ReferencesStephen Anderson.
1982.
The analysis of french shwa:or how to get something for nothing.
Language,58:534?573.Diana Apoussidou.
2007.
The learnability of metricalphonology.
Ph.D. thesis, University of Amsterdam.Michael Becker, Nihan Ketrez, and Andrew Nevins.2011.
The surfeit of the stimulus: Analytic biases fil-ter lexical statistics in turkish laryngeal alternations.Language, 87:84?125.Paul Boersma.
2001.
Phonology-semantics interactionin OT, and its acquisition.
In Robert Kirchner, WolfWikeley, and Joe Pater, editors, Papers in Experimen-tal and Theoretical Linguistics, volume 6, pages 24?35.
University of Alberta, Edmonton.Richard Byrd, Peihuang Lu, Jorge Nocedal, and CiyouZhu.
1995.
A limited memory algorithm for boundconstrained optimization.
SIAM J.
Scientific Comput-ing, 16:1190?1208.Andries Coetzee and Joe Pater.
2011.
The place of vari-ation in phonological theory.
In John Goldsmith, Ja-son Riggle, and Alan Yu, editors, The Handbook ofPhonological Theory, pages 401?431.
Blackwell, 2ndedition.Franc?ois Dell.
1973.
Les re`gles et les sons.
Introduc-tion a` la phonologie ge?ne?rative.
Hermann, Paris, 2ndedition.Sarah Eisenstat.
2009.
Learning underlying forms withMaxEnt.
Master?s thesis, Brown University.Sharon Goldwater and Mark Johnson.
2003.
Learn-ing OT constraint rankings using a maximum entropymodel.
In Proceedings of the Stockholm Workshop onVariation within Optimality Theory, pages 111?120.Bruce Hayes, Kie Zuraw, Peter Siptar, and Zsuzsa Londe.2009.
Natural and unnatural constraints in Hungarianvowel harmony.
Language, 85:822?863.Sharon Inkelas, Cemil Orhan Orgun, and Cheryl Zoll.1997.
The implications of lexical exceptions for thenature of grammar.
In Derivations and Constraints inPhonology, pages 393?418.
Oxford, Clarendon.Roman Jakobson.
1948.
Russian conjugation.
Word,4:155?167.Karen Jesney and Anne-Michelle Tessier.
2011.
Biasesin Harmonic Grammar: the road to restrictive learning.Natural Language and Linguistic Theory, 29:251?290.Rene?
Kager.
2008.
Lexical irregularity and the typologyof contrast.
In Kristin Hanson and Sharon Inkelas, edi-tors, The Nature of the Word: Studies in Honor of PaulKiparsky, pages 397?432.
MIT Press.Paul Kiparsky.
1973.
Abstractness, opacity, and globalrules.
In Osamu Fujimura, editor, Three Dimensionsof Linguistic Theory, pages 57?86.
TEC, Tokyo.Solomon Kullback and Richard Leibler.
1951.
On in-formation and sufficiency.
Annals of Mathematics andStatistics, pages 22?79.John McCarthy and Alan Prince.
1999.
Faithfulnessand identity in prosodic morphology.
In Rene?
Kager,Harry van der Hulst, and Wim Zonneveld, editors, TheProsody-Morphology Interface, pages 218?309.
Cam-bridge University Press.Claire Moore-Cantwell.
2012.
Over- and under-generalization in derivational morphology.
In NELSProceedings.Yves-Charles Morin.
1988.
De l?ajustement du schwaen syllabe ferme?e dans la phonologie du franc?ais.
InHans Basb?ll, Yves-Charles Morin, Roland Noske,and Bernard Tranel, editors, La phonologie du schwafranc?ais, pages 133?189.
John Benjamins, Amster-dam.Andrew Nevins.
2011.
Phonologically conditionedallomorph selection.
In Colin Ewen, Beth Hume,Marc van Oostendorp, and Keren Rice, editors, TheCompanion to Phonology, pages 2357?2382.
Wiley-Blackwell.Alan Prince and Paul Smolensky.
2004.
Optimality The-ory: Constraint interaction in generative grammar.Blackwell.Alan Prince and Bruce Tesar.
2004.
Learning phonotac-tic distributions.
In Rene?
Kager, Joe Pater, and WimZonneveld, editors, Fixing Priorities: Constraints inPhonological Acquisition, pages 245?291.
CambridgeUniversity Press.R Development Core Team.
2010.
R: A language andenvironment for statistical computing.
Technical re-port, R Foundation for Statistical Computing, Vienna,Austria.Isabelle Racine.
2007.
Effacement du schwa dans desmots lexicaux: constitution d?une base de donne?es etanalyse comparative.
In Proceedings of JEL?2007,pages 125?130.
Universite?
de Nantes.Paul Smolensky and Ge?raldine Legendre.
2006.The Harmonic Mind: From Neural Computation toOptimality-Theoretic Grammar.
MIT Press.Paul Smolensky.
1996.
The initial state and ?Richnessof the Base?
in Optimality Theory.
Technical ReportJHU-CogSci-96-4, Johns Hopkins University.Robert Staubs and Joe Pater.
2012.
Learning serialconstraint-based grammars.
In John J. McCarthy andJoe Pater, editors, Harmonic Grammar and HarmonicSerialism.
Equinox Press.Bruce Tesar.
2006.
Faithful contrastive features in learn-ing.
Cognitive Science, 30:863?903.Andrey Nikolayevich Tychonoff and V. Y. Arsenin.
1977.Solutions of ill-posed problems.
Winston, New York.Kie Zuraw.
2000.
Exceptions and regularities in phonol-ogy.
Ph.D. thesis, UCLA.71
