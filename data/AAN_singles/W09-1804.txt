Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28?35,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA New Objective Function for Word AlignmentTugba Bodrumlu Kevin Knight Sujith RaviInformation Sciences Institute & Computer Science DepartmentUniversity of Southern Californiabodrumlu@usc.edu, knight@isi.edu, sravi@isi.eduAbstractWe develop a new objective function for wordalignment that measures the size of the bilin-gual dictionary induced by an alignment.
Aword alignment that results in a small dictio-nary is preferred over one that results in a largedictionary.
In order to search for the align-ment that minimizes this objective, we cast theproblem as an integer linear program.
We thenextend our objective function to align corporaat the sub-word level, which we demonstrateon a small Turkish-English corpus.1 IntroductionWord alignment is the problem of annotating a bilin-gual text with links connecting words that have thesame meanings.
Figure 1 shows sample input fora word aligner (Knight, 1997).
After analyzing thetext, we may conclude, for example, that sprok cor-responds to dat in the first sentence pair.Word alignment has several downstream con-sumers.
One is machine translation, where pro-grams extract translation rules from word-alignedcorpora (Och and Ney, 2004; Galley et al, 2004;Chiang, 2007; Quirk et al, 2005).
Other down-stream processes exploit dictionaries derived byalignment, in order to translate queries in cross-lingual IR (Scho?nhofen et al, 2008) or re-score can-didate translation outputs (Och et al, 2004).Many methods of automatic alignment have beenproposed.
Probabilistic generative models like IBM1-5 (Brown et al, 1993), HMM (Vogel et al, 1996),ITG (Wu, 1997), and LEAF (Fraser and Marcu,2007) define formulas for P(f | e) or P(e, f), withok-voon ororok sprokat-voon bichat daterok sprok izok hihok ghiroktotat dat arrat vat hilatok-drubel ok-voon anok plok sprokat-drubel at-voon pippat rrat datok-voon anok drok brok jokat-voon krat pippat sat latwiwok farok izok stoktotat jjat quat catlalok sprok izok jok stokwat dat krat quat catlalok farok ororok lalok sprok izok enemokwat jjat bichat wat dat vat eneatlalok brok anok plok nokiat lat pippat rrat nnatwiwok nok izok kantok ok-yurptotat nnat quat oloat at-yurplalok mok nok yorok ghirok clokwat nnat gat mat bat hilatlalok nok crrrok hihok yorok zanzanokwat nnat arrat mat zanzanatlalok rarok nok izok hihok mokwat nnat forat arrat vat gatFigure 1: Word alignment exercise (Knight, 1997).28hidden alignment variables.
EM algorithms estimatedictionary and other probabilities in order to maxi-mize those quantities.
One can then ask for Viterbialignments that maximize P(alignment | e, f).
Dis-criminative models, e.g.
(Taskar et al, 2005), in-stead set parameters to maximize alignment accu-racy against a hand-aligned development set.
EMDtraining (Fraser and Marcu, 2006) combines genera-tive and discriminative elements.Low accuracy is a weakness for all systems.
Mostpractitioners still use 1990s algorithms to align theirdata.
It stands to reason that we have not yet seenthe last word in alignment models.In this paper, we develop a new objective functionfor alignment, inspired by watching people manu-ally solve the alignment exercise of Figure 1.
Whenpeople attack this problem, we find that once theycreate a bilingual dictionary entry, they like to re-use that entry as much as possible.
Previous ma-chine aligners emulate this to some degree, but theyare not explicitly programmed to do so.We also address another weakness of currentaligners: they only align full words.
With few ex-ceptions, e.g.
(Zhang et al, 2003; Snyder and Barzi-lay, 2008), aligners do not operate at the sub-wordlevel, making them much less useful for agglutina-tive languages such as Turkish.Our present contributions are as follows:?
We offer a simple new objective function thatscores a corpus alignment based on how manydistinct bilingual word pairs it contains.?
We use an integer programming solver tocarry out optimization and corpus alignment.?
We extend the system to perform sub-word alignment, which we demonstrate on aTurkish-English corpus.The results in this paper constitute a proof of con-cept of these ideas, executed on small corpora.
Weconclude by listing future directions.2 New Objective Function for AlignmentWe search for the legal alignment that minimizes thesize of the induced bilingual dictionary.
By dictio-nary size, we mean the number of distinct word-pairs linked in the corpus alignment.
We can im-mediately investigate how different alignments stackup, according to this objective function.
Figure 2garcia and associatesgarcia y asociadoshis associates are not strongsus asociados no son fuertescarlos garcia has three associatescarlos garcia tiene tres asociadosgarcia has a company alsogarcia tambien tiene una empresaits clients are angrysus clientes estan enfadadosthe associates are also angrylos asociados tambien estan enfadadosthe clients and the associates are enemieslos clientes y los asociados son enemigosthe company has three groupsla empresa tiene tres gruposits groups are in europesus grupos estan en europathe modern groups sell strong pharmaceuticalslos grupos modernos venden medicinas fuertesthe groups do not sell zenzaninelos grupos no venden zanzaninathe small groups are not modernlos grupos pequenos no son modernosFigure 2: Gold alignment.
The induced bilingual dic-tionary has 28 distinct entries, including garcia/garcia,are/son, are/estan, not/no, has/tiene, etc.29garcia and associatesgarcia y asociadoshis associates are not strongsus asociados no son fuertescarlos garcia has three associatescarlos garcia tiene tres asociadosgarcia has a company alsogarcia tambien tiene una empresaits clients are angrysus clientes estan enfadadosthe associates are also angrylos asociados tambien estan enfadadosthe clients and the associates are enemieslos clientes y los asociados son enemigosthe company has three groupsla empresa tiene tres gruposits groups are in europesus grupos estan en europathe modern groups sell strong pharmaceuticalslos grupos modernos venden medicinas fuertesthe groups do not sell zenzaninelos grupos no venden zanzaninathe small groups are not modernlos grupos pequenos no son modernosFigure 3: IP alignment.
The induced bilingual dictionaryhas 28 distinct entries.shows the gold alignment for the corpus in Figure 1(displayed here as English-Spanish), which resultsin 28 distinct bilingual dictionary entries.
By con-trast, a monotone alignment induces 39 distinct en-tries, due to less re-use.Next we look at how to automatically rifle throughall legal alignments to find the one with the bestscore.
What is a legal alignment?
For now, we con-sider it to be one where:?
Every foreign word is aligned exactly once(Brown et al, 1993).?
Every English word has either 0 or 1 align-ments (Melamed, 1997).We formulate our integer program (IP) as follows.We set up two types of binary variables:?
Alignment link variables.
If link-i-j-k = 1, thatmeans in sentence pair i, the foreign word atposition j aligns to the English words at posi-tion k.?
Bilingual dictionary variables.
If dict-f-e = 1,that means word pair (f, e) is ?in?
the dictio-nary.We constrain the values of link variables to sat-isfy the two alignment conditions listed earlier.
Wealso require that if link-i-j-k = 1 (i.e., we?ve decidedon an alignment link), then dict-fij -eik should alsoequal 1 (the linked words are recorded as a dictio-nary entry).1 We do not require the converse?justbecause a word pair is available in the dictionary, thealigner does not have to link every instance of thatword pair.
For example, if an English sentence hastwo the tokens, and its Spanish translation has twola tokens, we should not require that all four linksbe active?in fact, this would conflict with the 1-1link constraints and render the integer program un-solvable.
The IP reads as follows:minimize:?f,e dict-f-esubject to:?i,j?k link-i-j-k = 1?i,k?j link-i-j-k ?
1?i,j,k link-i-j-k ?
dict-fij -eikOn our Spanish-English corpus, the cplex2 solverobtains a minimal objective function value of 28.
To1fij is the jth foreign word in the ith sentence pair.2www.ilog.com/products/cplex30get the second-best alignment, we add a constraintto our IP requiring the sum of the n variables activein the previous solution to be less than n, and were-run cplex.
This forces cplex to choose differentvariable settings on the second go-round.
We repeatthis procedure to get an ordered list of alignments.3We find that there are 8 distinct solutions thatyield the same objective function value of 28.
Fig-ure 3 shows one of these.
This alignment is not bad,considering that word-order information is not en-coded in the IP.
We can now compare several align-ments in terms of both dictionary size and alignmentaccuracy.
For accuracy, we represent each alignmentas a set of tuples < i, j, k >, where i is the sentencepair, j is a foreign index, and k is an English index.We use these tuples to calculate a balanced f-scoreagainst the gold alignment tuples.4Method Dict size f-scoreGold 28 100.0Monotone 39 68.9IBM-1 (Brown et al, 1993) 30 80.3IBM-4 (Brown et al, 1993) 29 86.9IP 28 95.9The last line shows an average f-score over the 8 tiedIP solutions.Figure 4 further investigates the connection be-tween our objective function and alignment accu-racy.
We sample up to 10 alignments at each ofseveral objective function values v, by first addinga constraint that dict variables add to exactly v, theniterating the n-best list procedure above.
We stopwhen we have 10 solutions, or when cplex fails tofind another solution with value v. In this figure, wesee a clear relationship between the objective func-tion and alignment accuracy?minimizing the for-mer is a good way to maximize the latter.3This method not only suppresses the IP solutions generatedso far, but it suppresses additional solutions as well.
In partic-ular, it suppresses solutions in which all link and dict variableshave the same values as in some previous solution, but someadditional dict variables are flipped to 1.
We consider this a fea-ture rather than a bug, as it ensures that all alignments in then-best list are unique.
For what we report in this paper, we onlycreate n-best lists whose elements possess the same objectivefunction value, so the issue does not arise.4P = proportion of proposed links that are in gold,R = proportion of gold links that are proposed, and f-score = 2PR/(P+R).0.50.550.60.650.70.750.80.850.90.951Averagealignment accuracy(f-score)Number of bilingual dictionary entries in solution(objective function value)28 30 32 34 36 38 40 42 44 46Figure 4: Relationship between IP objective (x-axis =size of induced bilingual dictionary) and alignment ac-curacy (y-axis = f-score).Turkish Englishyururum i walkyururler they walkFigure 5: Two Turkish-English sentence pairs.3 Sub-Word AlignmentWe now turn to alignment at the sub-word level.Agglutinative languages like Turkish present chal-lenges for many standard NLP techniques.
An ag-glutinative language can express in a single word(e.g., yurumuyoruz) what might require many wordsin another language (e.g., we are not walking).Naively breaking on whitespace results in a verylarge vocabulary for Turkish, and it ignores themulti-morpheme structure inside Turkish words.Consider the tiny Turkish-English corpus in Fig-ure 5.
Even a non-Turkish speaker might plausi-bly align yurur to walk, um to I, and ler to they.However, none of the popular machine alignersis able to do this, since they align at the whole-word level.
Designers of translation systems some-times employ language-specific word breakers be-fore alignment, though these are hard to build andmaintain, and they are usually not only language-specific, but also language-pair-specific.
Good un-31supervised monolingual morpheme segmenters arealso available (Goldsmith, 2001; Creutz and Lagus,2005), though again, these do not do joint inferenceof alignment and word segmentation.We extend our objective function straightfor-wardly to sub-word alignment.
To test our exten-sion, we construct a Turkish-English corpus of 1616sentence pairs.
We first manually construct a regu-lar tree grammar (RTG) (Gecseg and Steinby, 1984)for a fragment of English.
This grammar producesEnglish trees; it has 86 rules, 26 states, and 53 ter-minals (English words).
We then construct a tree-to-string transducer (Rounds, 1970) that converts En-glish trees into Turkish character strings, includingspace.
Because it does not explicitly enumerate theTurkish vocabulary, this transducer can output a verylarge number of distinct Turkish words (i.e., charac-ter sequences preceded and followed by space).
Thistransducer has 177 rules, 18 states, and 23 termi-nals (Turkish characters).
RTG generation producesEnglish trees that the transducer converts to Turk-ish, both via the tree automata toolkit Tiburon (Mayand Knight, 2006).
From this, we obtain a parallelTurkish-English corpus.
A fragment of the corpus isshown in Figure 6.
Because we will concentrate onfinding Turkish sub-words, we manually break offthe English sub-word -ing, by rule, as seen in thelast line of the figure.This is a small corpus, but good for demonstrat-ing our concept.
By automatically tracing the inter-nal operation of the tree transducer, we also producea gold alignment for the corpus.
We use the goldalignment to tabulate the number of morphemes perTurkish word:n % Turkish types % Turkish tokenswith n morphemes with n morphemes1 23.1% 35.5%2 63.5% 61.6%3 13.4% 2.9%Naturally, these statistics imply that standard whole-word aligners will fail.
By inspecting the corpus, wefind that 26.8 is the maximium f-score available towhole-word alignment methods.Now we adjust our IP formulation.
We broadenthe definition of legal alignment to include breakingany foreign word (token) into one or more sub-word(tokens).
Each resulting sub-word token is alignedto exactly one English word token, and every En-glish word aligns to 0 or 1 foreign sub-words.
Ourdict-f-e variables now relate Turkish sub-words toEnglish words.
The first sentence pair in Figure 5would have previously contributed two dict vari-ables; now it contributes 44, including things likedict-uru-walk.
We consider an alignment to be a setof tuples < i, j1, j2, k >, where j1 and j2 are startand end indices into the foreign character string.
Wecreate align-i-j1-j2-k variables that connect Turkishcharacter spans with English word indices.
Align-ment variables constrain dictionary variables as be-fore, i.e., an alignment link can only ?turn on?
whenlicensed by the dictionary.We previously constrained every Turkish word toalign to something.
However, we do not want ev-ery Turkish character span to align?only the spansexplicitly chosen in our word segmentation.
So weintroduce span-i-j1-j2 variables to indicate segmen-tation decisions.
Only when span-i-j1-j2 = 1 do werequire?k align-i-j1-j2-k = 1.For a coherent segmentation, the set of active spanvariables must cover all Turkish letter tokens in thecorpus, and no pair of spans may overlap each other.To implement these constraints, we create a latticewhere each node represents a Turkish index, andeach transition corresponds to a span variable.
In acoherent segmentation, the sum of all span variablesentering an lattice-internal node equals the sum ofall span variables leaving that node.
If the sum ofall variables leaving the start node equals 1, then weare guaranteed a left-to-right path through the lat-tice, i.e., a coherent choice of 0 and 1 values for spanvariables.The IP reads as follows:minimize:?f,e dict-f-esubject to:?i,j1,j2?k align-i-j1-j2-k = span-i-j1-j2?i,k?j1,j2 align-i-j1-j2-k ?
1?i,j1,j2,k align-i-j1-j2-k ?
dict-fi,j1,j2-ei,k?i,j?j3 span-i-j3-j =?j3 span-i-j-j3?i,w?j>w span-i-w-j = 1(w ranges over Turkish word start indices)With our simple objective function, we obtain anf-score of 61.4 against the gold standard.
Samplegold and IP alignments are shown in Figure 7.32Turkish Englishonlari gordum i saw themgidecekler they will goonu stadyumda gordum i saw him in the stadiumogretmenlerim tiyatroya yurudu my teachers walked to the theatrecocuklar yurudu the girls walkedbabam restorana gidiyor my father is walk ing to the restaurant.
.
.
.
.
.Figure 6: A Turkish-English corpus produced by an English grammar pipelined with an English-to-Turkish tree-to-string transducer.you   go   to   his   officeonun ofisi- -ne   gider- -sinGold alignment IP sub-word alignmentyou   go   to   his   officeonun ofisi- -ne   gider- -sinmy  teachers  ran  to  their  houseogretmenler- -im onlarin evi- -ne  kostumy  teachers  ran  to  their  houseogretmenler- -im onlarin evi- -ne  kostui  saw  himonu gordu- -mi  saw  himonu gordu- -mwe  go  to  the  theatretiyatro- -ya gider- -izwe  go  to  the  theatretiyatro- -ya gider- -izthey  walked  to  the  storemagaza- -ya yurudu- -lerthey  walked  to  the  storemagaza- -ya yurudu- -lermy aunt goes to their househala- -m  onlarin evi- -ne  gidermy aunt goes to their househal- -am  onlarin evi- -ne  gider1.2.3.5.6.4.Figure 7: Sample gold and (initial) IP sub-word alignments on our Turkish-English corpus.
Dashes indicate where theIP search has decided to break Turkish words in the process of aligning.
For examples, the word magazaya has beenbroken into magaza- and -ya.33The last two incorrect alignments in the figureare instructive.
The system has decided to alignEnglish the to the Turkish noun morphemes tiyatroand magaza, and to leave English nouns theatre andstore unaligned.
This is a tie-break decision.
It isequally good for the objective function to leave theunaligned instead?either way, there are two rele-vant dictionary entries.We fix this problem by introducing a specialNULL Turkish token, and by modifying the IP to re-quire every English token to align (either to NULLor something else).
This introduces a cost for fail-ing to align an English token x to Turkish, becausea new x/NULL dictionary entry will have to be cre-ated.
(The NULL token itself is unconstrained inhow many tokens it may align to.
)Under this scheme, the last two incorrect align-ments in Figure 7 induce four relevant dictio-nary entries (the/tiyatro, the/magaza, theatre/NULL,store/NULL) while the gold alignment induces onlythree (the/NULL, theatre/tiyatro, store/magaza), be-cause the/NULL is re-used.
The gold alignment istherefore now preferred by the IP optimizer.
Thereis a rippling effect, causing the system to correctmany other decisions as well.
This revision raisesthe alignment f-score from 61.4 to 83.4.The following table summarizes our alignment re-sults.
In the table, ?Dict?
refers to the size of theinduced dictionary, and ?Sub-words?
refers to thenumber of induced Turkish sub-word tokens.Method Dict Sub-words f-scoreGold (sub-word) 67 8102 100.0Monotone (word) 512 4851 5.5IBM-1 (word) 220 4851 21.6IBM-4 (word) 230 4851 20.3IP (word) 107 4851 20.1IP (sub-word, 60 7418 61.4initial)IP (sub-word, 65 8105 83.4revised)Our search for an optimal IP solution is not fast.It takes 1-5 hours to perform sub-word alignment onthe Turkish-English corpus.
Of course, if we wantedto obtain optimal alignments under IBM Model 4,that would also be expensive, in fact NP-complete(Raghavendra and Maji, 2006).
Practical Model 4systems therefore make substantial search approxi-mations (Brown et al, 1993).4 Related Work(Zhang et al, 2003) and (Wu, 1997) tackle the prob-lem of segmenting Chinese while aligning it to En-glish.
(Snyder and Barzilay, 2008) use multilingualdata to compute segmentations of Arabic, Hebrew,Aramaic, and English.
Their method uses IBM mod-els to bootstrap alignments, and they measure the re-sulting segmentation accuracy.
(Taskar et al, 2005) cast their alignment model asa minimum cost quadratic flow problem, for whichoptimal alignments can be computed with off-the-shelf optimizers.
Alignment in the modified modelof (Lacoste-Julien et al, 2006) can be mapped to aquadratic assignment problem and solved with linearprogramming tools.
In that work, linear program-ming is not only used for alignment, but also fortraining weights for the discriminative model.
Theseweights are trained on a manually-aligned subset ofthe parallel data.
One important ?mega?
feature forthe discriminative model is the score assigned by anIBM model, which must be separately trained on thefull parallel data.
Our work differs in two ways: (1)our training is unsupervised, requiring no manuallyaligned data, and (2) we do not bootstrap off IBMmodels.
(DeNero and Klein, 2008) gives an integerlinear programming formulation of another align-ment model based on phrases.
There, integer pro-gramming is used only for alignment, not for learn-ing parameter values.5 Conclusions and Future WorkWe have presented a novel objective function foralignment, and we have applied it to whole-word andsub-word alignment problems.
Preliminary resultslook good, especially given that new objective func-tion is simpler than those previously proposed.
Theinteger programming framework makes the modeleasy to implement, and its optimal behavior frees usfrom worrying about search errors.We believe there are good future possibilities forthis work:?
Extend legal alignments to cover n-to-mand discontinuous cases.
While morpheme-to-morpheme alignment is more frequently a341-to-1 affair than word-to-word alignment is,the 1-to-1 assumption is not justified in eithercase.?
Develop new components for the IP objec-tive.
Our current objective function makes noreference to word order, so if the same wordappears twice in a sentence, a tie-break en-sues.?
Establish complexity bounds for optimiz-ing dictionary size.
We conjecture that opti-mal alignment according to our model is NP-complete in the size of the corpus.?
Develop a fast, approximate alignment al-gorithm for our model.?
Test on large-scale bilingual corpora.AcknowledgmentsThis work was partially supported under DARPAGALE, Contract No.
HR0011-06-C-0022.ReferencesP.
Brown, V. Della Pietra, S. Della Pietra, and R. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational linguis-tics, 19(2).D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).M.
Creutz and K. Lagus.
2005.
Inducing the morpho-logical lexicon of a natural language from unannotatedtext.
In Proc.
AKRR.J.
DeNero and D. Klein.
2008.
The complexity of phrasealignment problems.
In Proc.
ACL.A.
Fraser and D. Marcu.
2006.
Semi-supervised trainingfor statistical word alignment.
In Proc.
ACL-COLING.A.
Fraser and D. Marcu.
2007.
Getting the structure rightfor word alignment: LEAF.
In Proc.
EMNLP-CoNLL.M.
Galley, M. Hopkins, K. Knight, and D Marcu.
2004.What?s in a translation rule.
In Proc.
NAACL-HLT.F.
Gecseg and M. Steinby.
1984.
Tree automata.Akademiai Kiado.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27(2).K.
Knight.
1997.
Automating knowledge acquisition formachine translation.
AI Magazine, 18(4).S.
Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.2006.
Word alignment via quadratic assignment.
InProc.
HLT-NAACL.J.
May and K. Knight.
2006.
Tiburon: A weighted treeautomata toolkit.
In Proc.
CIAA.I.
D. Melamed.
1997.
A word-to-word model of transla-tional equivalence.
In Proc.
ACL.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Computa-tional Linguistics, 30(4).F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbordof features for statistical machine translation.
In Proc.HLT-NAACL.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proc.
ACL.U.
Raghavendra and H. K. Maji.
2006.
Computationalcomplexity of statistical machine translation.
In Proc.EACL.W.
Rounds.
1970.
Mappings and grammars on trees.Theory of Computing Systems, 4(3).P.
Scho?nhofen, A. Benczu?r, I.
B?
?ro?, and K. Csaloga?ny,2008.
Cross-language retrieval with wikipedia.Springer.B.
Snyder and R. Barzilay.
2008.
Unsupervised multi-lingual learning for morphological segmentation.
InProc.
ACL.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A dis-criminative matching approach to word alignment.
InProc.
EMNLP.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proc.
ACL.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3).Y.
Zhang, S. Vogel, and A. Waibel.
2003.
Integratedphrase segmentation and alignment algorithm for sta-tistical machine translation.
In Proc.
Intl.
Conf.
onNLP and KE.35
