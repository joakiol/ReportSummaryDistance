A Corpus Study of Evaluative and Speculative LanguageJanyce Wiebe, Rebecca Brucey, Matthew Bell, Melanie Martinz, Theresa WilsonUniversity of Pittsburgh, University of North Carolina at Ashevilley, New Mexico State Universityzwiebe,mbell,twilson@cs.pitt.edu, bruce@cs.unca.edu, mmartin@cs.nmsu.eduAbstractThis paper presents a corpus studyof evaluative and speculative language.Knowledge of such language would beuseful in many applications, such astext categorization and summarization.Analyses of annotator agreement and ofcharacteristics of subjective language areperformed.
This study yields knowl-edge needed to design eective machinelearning systems for identifying subjec-tive language.1 IntroductionSubjectivity in natural language refers to aspectsof language used to express opinions and evalua-tions (Baneld, 1982; Wiebe, 1994).
Subjectivitytagging is distinguishing sentences used to presentopinions and other forms of subjectivity (subjec-tive sentences) from sentences used to objectivelypresent factual information (objective sentences).This task is especially relevant for news report-ing and Internet forums, in which opinions of var-ious agents are expressed.
There are numerousapplications for which subjectivity tagging is rele-vant.
Two are information retrieval and informa-tion extraction.
Current extraction and retrievaltechnology focuses almost exclusively on the sub-ject matter of documents.
However, additionalaspects of a document inuence its relevance, in-cluding, e.g., the evidential status of the materialpresented, and the attitudes expressed about thetopic (Kessler et al, 1997).
Knowledge of subjec-tive language would also be useful iname recog-nition (Spertus, 1997; Kaufer, 2000), email clas-sication (Aone et al, 2000), intellectual attribu-tion in text (Teufel and Moens, 2000), recogniz-ing speaker role in radio broadcasts (Barzilay etal., 2000), review mining (Terveen et al, 1997),generation and style (Hovy, 1987), clustering doc-uments by ideological point of view (Sack, 1995),and any other application that would benet fromknowledge of how opinionated the language is, andwhether or not the writer purports to objectivelypresent factual material.To use subjectivity tagging in applications,good linguistic clues must be found.
As with manypragmatic and discourse distinctions, existing lex-ical resources are not comprehensively coded forsubjectivity.
The goal of our current work is learn-ing subjectivity clues from corpora.
This papercontributes to this goal by empirically examin-ing subjectivity.
We explore annotating subjectiv-ity at dierent levels (expression, sentence, docu-ment) and produce corpora annotated at dierentlevels.
Annotator agreement is analyzed to un-derstand and assess the viability of such annota-tions.
In addition, because expression-level anno-tations are ne-grained and thus very informative,these annotations are examined to gain knowledgeabout subjectivity.We also use our annotations and existing ed-itorial annotations to generate and test featuresof subjectivity.
Altogether, the observations andresults of these studies provide valuable informa-tion that will facilitate designing eective machinelearning systems for recognizing subjectivity.The remainder of this paper rst provides back-ground about subjectivity, then presents resultsfor document-level annotations, followed by ananalysis of expression-level annotations.
Resultsfor features generated using document-level anno-tations are next, ending with conclusions.2 SubjectivitySentence (1) is an example of a simple subjectivesentence, and (2) is an example of a simple objec-tive sentence:1(1) At several dierent layers, it's a fascinatingtale.1The term subjectivity is due to Ann Baneld(1982).
For references to work on subjectivity, pleasesee (Baneld, 1982; Fludernik, 1993; Wiebe, 1994;Stein and Wright, 1995).
(2) Bell Industries Inc. increased its quarterly to10 cents from 7 cents a share.The main types of subjectivity are:1.
Evaluation.
This category includes emotionssuch as hope and hatred as well as evalua-tions, judgements, and opinions.
Examplesof expressions involving positive evaluationare enthused, wonderful, and great product!.Examples involving negative evaluation arecomplained, you idiot!, and terrible product.2.
Speculation.
This category includes anythingthat removes the presupposition of events oc-curring or states holding, such as speculationand uncertainty.
Examples of speculative ex-pressions are speculated, and maybe.Following are examples of strong negativeevaluative language from a corpus of Usenetnewsgroup messages:(3a) I had in mind your facts, buddy, not hers.
(3b) Nice touch.
\Alleges" whenever facts postedare not in your persona of what is \real".Following is an example of opinionated, edito-rial language, taken from an editorial in the WallStreet Journal:(4) We stand in awe of the Woodstock genera-tion's ability to be unceasingly fascinated by thesubject of itself.Sentences (5) and (6) illustrate the fact thatsentences about speech events may be subjectiveor objective:(5) Northwest Airlines settled the remaininglawsuits led on behalf of 156 people killed ina 1987 crash, but claims against the jetliner'smaker are being pursued, a federal judge said.
(6) \The cost of health care is eroding our stan-dard of living and sapping industrial strength,"complains Walter Maher, a Chrysler health-and-benets specialist.In (5), the material about lawsuits and claims ispresented as factual information, and a federaljudge is given as the source of information.
In(6), in contrast, a complaint is presented.
An NLPsystem performing information extraction on (6)should not treat the material in the quoted stringas factual information, with the complainer as asource of information, whereas a correspondingtreatment of sentence (5) would be appropriate.Subjective sentences often contain individualexpressions of subjectivity.
Examples are fasci-nating in (1), and eroding, sapping, and complainsin (6).
The following paragraphs mention aspectsof subjectivity expressions that are relevant forNLP applications.First, although some expressions, such as !, aresubjective in all contexts, many, such as sappingand eroding, may or may not be subjective, de-pending on the context in which they appear.
Apotential subjective element (PSE) is a linguisticelement that may be used to express subjectivity.A subjective element is an instance of a potentialsubjective element, in a particular context, that isindeed subjective in that context (Wiebe, 1994).Second, a subjective element expresses the sub-jectivity of a source, who may be the writer orsomeone mentioned in the text.
For example, thesource of fascinating in (1) is the writer, whilethe source of the subjective elements in (6) is Ma-her.
In addition, a subjective element has a tar-get, i.e., what the subjectivity is about or directedtoward.
In (1), the target is a tale; in (6), the tar-get of Maher's subjectivity is the cost of healthcare.
These are examples of object-centric sub-jectivity, which is about an object mentioned inthe text (other examples: \I love this project";\The software is horrible").
Subjectivity may alsobe addressee-oriented, i.e., directed toward the lis-tener or reader (e.g., \You are an idiot").Third, there may be multiple subjective ele-ments in a sentence, possibly of dierent typesand attributed to dierent sources and targets.For example, in (4), subjectivity of the Woodstockgeneration is described (specically, its fascina-tion with itself).
In addition, subjectivity of thewriter is expressed (e.g., `we stand in awe').
As de-scribed below, individual subjective elements wereannotated as part of this work, rening previouswork on sentence-level annotations.
Finally, PSEsmay be complex expressions such as `village id-iot', `powers that be', `You' NP, and `What a'NP.
There is a great variety of such expressions,including many studied under the rubric of idioms(see, for example, (Nunberg et al, 1994)).
We ad-dress learning such expressions in another project.3 Previous Work on SubjectivityTaggingIn previous work (Wiebe et al, 1999; Bruce andWiebe, 1999), a corpus of sentences from the WallStreet Journal Treebank Corpus (Marcus et al,1993) was manually annotated with subjectivityclassications by multiple judges.
The judges wereinstructed to consider a sentence to be subjectiveif they perceived any signicant expression of sub-jectivity (of any source) in the sentence, and toconsider the sentence to be objective, otherwise.Agreement was summarized in terms of Cohen's (Cohen, 1960), which compares the total proba-bility of agreement to that expected if the taggers'classications were statistically independent (i.e.,\chance agreement").
After two rounds of tag-ging by three judges, an average pairwise  valueof .69 was achieved on a test set.
The EM learn-ing algorithm was used to produce corrected tagsrepresenting the consensus opinions of the taggers(Goodman, 1974; Dawid and Skene, 1979).
Anautomatic system to perform subjectivity taggingwas developed using the new tags as training andtesting data.
In 10-fold cross validation experi-ments, a probabilistic classier obtained an aver-age accuracy on subjectivity tagging of 72.17%,more than 20 percentage points higher than abaseline accuracy obtained by always choosing themore frequent class.
Five part-of-speech features,two lexical features, and a paragraph feature wereused.To identify richer features, (Wiebe, 2000) usedLin's (1998) method for clustering words accord-ing to distributional similarity, seeded by a smallamount of detailed manual annotation, to auto-matically identify adjective PSEs.
There are twoparameters of this process, neither of which wasvaried in (Wiebe, 2000): C, the cluster size con-sidered, and FT , a ltering threshold, such that, ifthe seed word and the words in its cluster have, asa set, lower precision than the ltering thresholdon the training data, the entire cluster, includ-ing the seed word, is ltered out.
This process isadapted for use in the current paper, as describedin section 7.4 Choices in AnnotationIn expression-level annotation, the judges rstidentify the sentences they believe are subjective.They next identify the subjective elements inthe sentence, i.e., the expressions they feel areresponsible for the subjective classication.
Forexample (subjective elements are in parentheses):They promised (yet) more for (really good stu).
(Perhaps you'll forgive me) for reposting hisresponse.Subjective-element (expression-level) annota-tions are probably the most natural.
Ultimately,we would like to recognize the subjective elementsin a text, and their types, targets, and sources.However, both manual and automatic tagging atthis level are dicult because the tags are veryne-grained, and there is no predetermined clas-sication unit; a subjective element may be a sin-gle word or a large expression.
Thus, in the shortterm, it is probably best to use subjective-elementannotations for knowledge acquisition (analysis,training, feature generation) alone, and not targetautomatic classication of subjective elements.In this work, document-level subjectivity anno-tations are text categories of which subjectivityis a key aspect.
We use three text categories:editorials (Kessler et al, 1997), reviews, and\ames", i.e., hostile messages (Spertus, 1997;Kaufer, 2000).
For ease of discussion, we groupeditorials and reviews together under the termopinion pieces.There are benets to using such document-levelannotations.
First, they are more directly re-lated to applications (e.g., ltering hostile mes-sages and mining reviews from Internet forums).Second, there are existing annotations to be ex-ploited, such as editorials and arts reviews markedas such by newspapers, as well as on-line productreviews accompanied by formal numerical ratings(e.g., 4 on a scale from 1 to 5).However, a challenging aspect of such data isthat opinion pieces andames contain objectivesentences, while documents in other text cate-gories contain subjective sentences.
News reportspresent reactions to and attitudes toward reportedevents (van Dijk 1988); they often contain seg-ments starting with expressions such as criticsclaim and supporters argue.
In addition, quoted-speech sentences in which individuals express theirsubjectivity are often included (Barzilay et al,2000).
On the other hand, editorials contain ob-jective sentences presenting facts supporting thewriter's argument, and reviews contain sentencesobjectively presenting facts about the product.This \impure" aspect of opinionated text cate-gories must be considered when such data is usedfor training and testing.
Some specic results aregiven below in section 7.We believe that sentence-level classicationswill continue to provide an important level ofanalysis.
The sentence provides a prespeci-ed classication unit2and, while sentence-leveljudgements are not as ne-grained as subjective-2While sentence boundaries are not always unam-biguous in unedited text or spoken language, the datacan always be segmented into sentence-like units be-fore subjectivity tagging is performed.element judgements, they do not involve the largeamount of noise we face with document-level an-notations.5 Document-Level AnnotationResults5.1 Flame AnnotationsIn this study, newsgroup messages were assignedthe tagsame or not-ame.
The corpus con-sists of 1140 Usenet newsgroup messages, bal-anced among the categories alt, sci, comp, and recin the Usenet hierarchy.
The corpus was divided,preserving the category balance, into a training setof 778 messages and a test set of 362 messages.The annotators were instructed to mark a mes-sage as aame if the \main intention of the mes-sage is a personal attack, containing insulting orabusive language."
A number of policy decisionswere made in the instructions, dealing, primarily,with included messages (part or all of a previousmessage, included in the current message as partof a reply).
Some additional issues addressed inthe instructions were who the attack was directedat, nonsense, sarcasm, humor, rants, and raves.During the training phase, two annotators, MMand R, participated in multiple rounds of tagging,revising the annotation instructions as they pro-ceeded.
During the testing phase, MM and R in-dependently annotated the test set, achieving a value on these messages of 0.69.
A third annota-tor, L, trained on 492 messages from the trainingset, and then annotated 88 of the messages in thetest set.
The pairwise  values on this set of 88are: MM & R: 0.80; MM & L: 0.75; R & MM:0.80; for an average pairwise  of .78.This study provides evidence for the viabilityof document-levelame annotation.
We plan tobuild aame-recognition system in the future.
Aswill be seen below, MM and R also tagged thisdata at the subjective-element level.5.2 Opinion-Piece ClassicationsOur opinion-piece classications are built on exist-ing annotations in the Wall Street Journal.
Specif-ically, there are articles explicitly identied to beEditorials, Letters to the Editor, Arts & Leisure,and Viewpoints; together, we call these opinionpieces.
This data is a good resource for subjectiv-ity recognition.
However, an inspection of somedata revealed that some editorials and reviews arenot marked as such.
For example, there are arti-cles written in the rst person, and the purpose ofthe article is to present an argument rather thancover a news story, but there is no explicit indi-cation that they are editorials.
To create highquality test data, two judges manually annotatedWSJ data for opinion pieces.
The instructionswere to nd any additional opinion pieces thatare not marked as such.
The annotators also hadthe option of disagreeing with the existing anno-tations, but did not opt to do so in any instances.One judge annotated all articles in four datasetsof the Wall Street Journal Treebank corpus (Mar-cus et al, 1993) (W9-4, W9-10, W9-22, and W9-33, each approximately 160K words) as well asthe corpus of Wall Street Journal articles used in(Wiebe et al, 1999) (called WSJ-SE below).
An-other judge annotated all articles in two of thedatasets (W9-22 and W9-33).This annotation task appears to be relativelyeasy.
With no training at all, the  values are veryhigh: .94 for dataset W9-33 and .95 for datasetW9-22.The agreement data for W9-22 is given in Table1 in the form of a contingency table.
In section7, this data is used to generate and test candidatepotential subjective elements (PSEs).6 Subjective-Element AnnotationResults and Analyses6.1 Annotations and DataThese subsections analyze subjective element an-notations performed on three datasets, WSJ-SE,NG-FE, and NG-SE.WSJ-SE is the corpus of 1001 sentences of theWall Street Journal Treebank Corpus referred toabove in section 3.
Recall that the sentences ofthis corpus were manually annotated with subjec-tivity classications as described in (Wiebe et al,1999; Bruce and Wiebe, 1999).For this paper, two annotators (D and M ) wereasked to identify the subjective elements in WSJ-SE.
Specically, the taggers were given the sub-jective sentences identied in the previous study,and asked to put brackets around the words theybelieve cause the sentence to be classied as sub-jective.Note that inammatory language is a kind ofsubjective language.
NG-FE is a subset of theUsenet newsgroup corpus used in the document-levelame-annotation study described in section5.1.
Specically, NG-FE consists of the 362-message test set for taggers R and MM.
For thisstudy, R and MM were asked to identify theameelements in NG-FE.
Flame elements are the sub-set of subjective elements that are perceived tobe inammatory.
R and MM were asked to dothis in all 362 messages, because some messagesthat were not judged to beames at the messagelevel do contain individual inammatory phrasesTagger 2Op Not OpTagger 1 Op n11= 23 n12= 0 n1+= 23Not Op n21= 2 n22= 268 n2+= 270n+1= 25 n+2= 268 n++= 293Table 1: Contingency Table for Opinion Piece Agreement in W9-22(in these cases, the tagger does not believe thatthese phrases express the main intent of the mes-sage).In addition to the above annotations, tagger Mperformed subjective-element tagging on a dier-ent set of Usenet newsgroup messages, corpus NG-SE.
The size of this corpus is 15413 words.In datasets WSJ-SE and NG-SE, the taggerswere also asked to specify one of ve subjectiveelement types: e+ (positive evaluative), e  (neg-ative evaluative), e?
(some other type of evalua-tion), u (uncertainty), and o (none of the above),with the option to assign multiple types to an in-stance.
All corpora were stemmed (Karp et al,1992) and part-of-speech tagged (Brill, 1992).6.2 Agreement Among TaggersThere are techniques for analyzing agreementwhen annotations involve segment boundaries(Litman and Passonneau, 1995; Marcu et al,1999), but our focus in this paper is on words.Thus, our analyses are at the word level: eachword is classied as either appearing in a subjec-tive element or not.
Punctuation is excluded fromour analyses.
The WSJ data is divided into twosubsets in this section, Exp1 and Exp2.As mentioned above, in WSJ-SE Exp1 andExp2, the taggers also classied subjective ele-ments with respect to the type of subjectivitybeing expressed.
Subjectivity type agreement isagain analyzed at the word level, but, in this anal-ysis, only the words classied as belonging to sub-jective elements by both taggers are considered.Table 2 provides  values for word agreementin NG-FE (theame data) as well as for WSJ-SEExp1 and Exp2.
The task of identifying subjec-tive elements in a body of text is dicult, and theagreement results reect this fact; agreement ismuch stronger than that expected by chance, butless than what we would like to see when verify-ing a new classication.
Further renement of thecoding manual is required.
Additionally, it may bepossible to rene the classications automaticallyusing methods such as those described in (Wiebeet al, 1999).
In this analysis, we explore the pat-terns of agreement exhibited by the taggers in aneort to better understand the classication.We begin by looking at word agreement.
Wordagreement is higher in theame experiment(NG-FE) than it is in either WSJ experiment(WSJ-SE Exp1 and Exp2).
Looking at the WSJdata provides one plausible explanation for thelower word agreement in the WSJ experiments.As exhibited in the subjective elements identiedfor the single clause below,D: (e+ played the role well) (e?
obligatoryragged jeans a thicket of long hair and rejectionof all things conventional)M : (e+ well) (e?
obligatory) (e- ragged) (e?thicket) (e- rejection) (e- all things conventional)tagger D consistently identies entire phrasesas subjective, while Tagger M prefers to selectdiscrete lexical items.
This dierence in inter-pretation of the tagging instructions does notoccur in theame experiment.
Nonetheless, evenwithin theame data, there are many instanceswhere both taggers identify the same segment ofa sentence as forming a subjective element butdisagree on the boundaries of that segment, as inthe example below.R: (classic case of you deliberately misinterpret-ing my comments)MM : (you deliberately misinterpreting mycomments)These patterns of partial agreement are also evi-dent in the  values for words from specic syn-tactic categories (see Table 2 again).
In the WSJdata, agreement on determiners is particularly lowbecause they are often included as part of a phraseby tagger D but typically not included in the spe-cic lexical items chosen by tagger M. Interest-ingly, in the WSJ experiments, the taggers mostfrequently agreed on the selection of modals andadjectives, while in theame experiment, agree-ment was highest on nouns and adjectives.
Thehigh agreement on adjectives in both genres is con-All Words Nouns Verbs Modals Adj's Adverbs Det'sNG-FE 0:4657 0:5213 0.4571 0:4008 0:5011 0:3576 0:4286WSJ-SE, Exp1 0:4228 0:3999 0.4235 0:6992 0:6000 0:4328 0:2661WSJ-SE, Exp2 0:3703 0:3705 0.4261 0:4298 0:4294 0:2256 0:1234Table 2:  Values for Word Agreementsistent with results from other work (Bruce andWiebe, 1999; Wiebe et al, 1999), but high agree-ment on nouns in theame data verses high agree-ment on modals in the WSJ data suggests a genrespecic usage of these categories.
This would bethe case if, for example, modals were most fre-quently used to express uncertainty, a type of sub-jectivity that would be relatively rare inames.Turning to subjective-element type, in bothWSJ experiments, the  values for type agreementare comparable to those for word agreement.
Re-call that multiple types may be assigned to a singlesubjective instance.
All such instances in the WSJdata are u in combination with an evaluative tag(i.e., e+, e- and e?
), and they are not common:each tagger assigned multiple tags to fewer than7% of the subjective instances.
However, if partialmatches between type tags are recognized, i.e., ifthey share a common tag, then the  values im-prove signicantly.
Table 3 shows both types ofresults.It is interesting to note the variation in type agree-ment for words of dierent syntactic categories.Agreement on adjectives is consistently high whilethe agreement on the type of subjectivity ex-pressed by modals and adverbs is consistently low.This contrasts with the fact that word agreementfor modals, in particular, and, to a lesser extent,adverbs was high.
This lack of agreement sug-gests that the type of subjectivity expressed byadjectives is more easily distinguished than thatof modals or adverbs.
This is particularly impor-tant because the number of adjectives included insubjective elements is high.
In contrast, the num-bers of modals and adverbs are relatively low.Additional insight can be gained by combiningthe 3 evaluative classications (i.e., e+, e- ande?)
to form a single tag, e, representing anyform of evaluative expression.
Table 4 presentstype agreement results for the tag set e, u, o.In contrasting Tables 3 and 4, it is surprisingto note that most of the  values decrease whenthe distinction among the evaluative types is re-moved.
This suggests that the three evaluativetypes are natural classications.
Only for adverbsdoes type agreement improve with the smallertag set; this indicates that it is dicult to dis-tinguish the evaluative nature of adverbs.
Notealso that agreement for modals is not impactedby the change in tag sets.
This fact supports thehypothesis that modals are used primary to ex-press uncertainty.
As a nal point, we look atpatterns of agreement in type classication usingthe models of symmetry, marginal homogeneity,quasi-independence, and quasi-symmetry.
Eachmodel tests for a specic pattern of agreement:symmetry tests the interchangeability of taggers,marginal homogeneity veries the absence of biasamong taggers, quasi-independence veries thatthe taggers act independently when they disagree,and quasi-symmetry tests for the presence of anypattern in their disagreements.
For a more com-plete description of these models and their usein analyzing intercoder reliability see (Bruce andWiebe, 1999).
In short, the results presented inTable 5 indicate that the taggers are not inter-changeable: they exhibit biases in their type clas-sications, and there is a pattern of correlated dis-agreement in the assignment of the original typetags.
Surprisingly, the taggers appear to act in-dependently when they disagree in assigning thecompressed type tags (i.e., tags e, u and o).
Thisshift in the pattern of disagreement between tag-gers again suggests that the compression of theevaluative tags was inappropriate.
Additionally,these ndings suggest that it may be possible toautomatically correct the type biases expressedby the taggers using the technique described in(Bruce and Wiebe, 1999), a topic that will be in-vestigated in future work.6.3 UniquenessBased on previous work (Wiebe et al, 1998), wehypothesized that low-frequency words are associ-ated with subjectivity.
Table 6 provides evidencethat the number of unique words (words that ap-pear just once) in subjective elements is higherthan expected.
The rst row gives informationfor all words and the second gives information forwords that appear just once.
The gures in theNum columns are total counts, and the gures inthe P columns give the proportion that appear insubjective elements.
The Agree columns give in-All Words Nouns Verbs Modals Adj's Adverbs Det'sExp1 Full Match 0:4216 0:4228 0.2933 0:1422 0:5919 0:1207 0:5000Partial Match 0:5156 0:4570 0.4447 0:3011 0:6607 0:3305 0:5000Exp2 Full Match 0:3041 0:2353 0.2765 0:1429 0:5794 0:1207 0:0000Partial Match 0:4209 0:2353 0.3994 0:3494 0:6719 0:4439 0:1429Table 3:  Values for Type Agreement Using All Types in the WSJ DataAll Words Nouns Verbs Modals Adj's Adverbs Det'sExp1 Full Match 0:3377 0:0440 0.1648 0:1968 0:5443 0:3810 0:0000Partial Match 0:5287 0:1637 0.3765 0:4903 0:8125 0:3810 0:0000Exp2 Full Match 0:2569 0:0000 0.1923 0:1509 0:4783 0:1707 0:1429Partial Match 0:4789 0:0000 0.4167 0:4000 0:8056 0:7671 0:4000Table 4:  Values for Type Agreement Using E,O,U in the WSJ DataSym.
M.H.
Q.S.
Q.I.Exp1 All Types G2112:351 92:447 19:904 66:771Sig.
0:000 0:000 0:527 0:007e,o,u G285:478 84:142 1:336 12:576Sig.
0:000 0:000 0:248 0:027Exp2 All Types G294:669 76:247 18:422 58:892Sig.
0:000 0:000 0:241 0:001e,o,u G266:822 66:819 0:003 0:0003Sig.
0:000 0:000 0:986 0:987Table 5: Tests for Patterns of Agreement in WSJ Type-Tagged DataWSJ-SE NG-FED M Agree Agree R MMNum P Num P Num P Num P Num P Num PAll words 18341 .07 18341 .08 16857 .04 15413 .15 86279 .01 88210 .02unique 2615 .14 2615 .20 2522 .15 2348 .17 5060 .07 4836 .03Table 6: Proportions of Unique Words in Subjective Elementsformation for the subset of the corresponding dataset upon which the two annotators agree.Comparison of rows 1 and 2 across columnsshows that the proportion of unique words thatare subjective is higher than the proportion of allwords that are subjective.
In all cases, this dier-ence in proportions is highly statistically signi-cant.6.4 Types and ContextAn interesting question is, when a word appearsin multiple subjective elements, are those subjec-tive elements all the same type?
Table 7 showsthat a signicant portion are used in more thanone type.
Each item considered in the table is aword-POS pair that appears more than once in thecorpus.
The gures shown are the total number ofword-POS items that appear more than once (thecolumns labeled MultInst) and the proportion ofthose items that appear in more than one typeof subjective element (the columns labeled Mult-Type).
These results highlight the need for contex-tual disambiguation.
For example, one thinks ofgreat as a positive evaluative term, but its polaritydepends on the context; it can be used negativelyevaluatively in a context such as \Just great."
Agoal of performing subjective-element annotationsis to support learning such local contextual inu-ences.7 Generating and Testing PSEsusing Document-LevelAnnotationsThis section uses the opinion-piece annotations toexpand our set of PSEs beyond those that can bederived from the subjective-element annotations.Precision is used to assess feature quality.
Theprecision of feature F for class C is the numberof Fs that occur in units of class C over the totalnumber of Fs that occur anywhere in the data.An important motivation for using the opinion-piece data is that there is a large amount of it,and manually rening existing annotations as de-scribed in section 5.2 is much easier and more re-liable than other types of subjectivity annotation.However, we cannot expect absolutely high pre-cisions for two reasons.
First, the distribution ofopinions and non-opinions is highly skewed in fa-vor of non-opinions.
For example, in Table 1, tag-ger 1 classies only 23 of 293 articles as opinionpieces.
Second, as discussed in section 4, opin-ion pieces contain objective sentences and nonopinion-pieces contain subjective sentences.
Forexample, in WSJ-SE, which has been annotatedat the sentence and document levels, 70% of thesentences in opinion pieces are subjective and 30%are objective.
In non-opinion pieces, 44% of thesentences are subjective and only 56% are objec-tive.To give an idea of expected precisions, let usconsider the precision of subjective sentences withrespect to opinion pieces.
Suppose that 15% ofthe sentences in the dataset are in opinions, 85%in non-opinions.
Let us assume the proportions ofsubjective and objective sentences in opinion andnon-opinion pieces given just above.
Let N be thetotal number of sentences.
The desired precisionis the number of subjective sentences in opinionsover the total number of subjective sentences.
Itis .22:p=.15 * N * .70 / (.15 * N * .70 + .85 * N * .44).In addition, we are assessing PSEs, which areonly potentially subjective; many have objectiveas well as subjective uses.Thus, even if precisions are much lower than 1,we use increases in precision over a baseline as ev-idence of promising PSEs.
The baseline for com-parison is the number of word instances in opin-ion pieces, divided by the total number of wordinstances.
Table 8 shows the precisions for threetypes of PSEs.
The freq columns give total fre-quencies, and the +prec columns show the im-provements in precision from the baseline.
Thebaseline precisions are given at the bottom of thetable.As mentioned above, (Wiebe, 2000) showed suc-cess automatically identifying adjective PSEs us-ing Lin's method, seeded by a small amount of de-tailed manual annotations.
Desiring to move awayfrom manually annotated data, for this paper thesame process is used, but the seed words are allthe adjectives (verbs) in the training data.
In ad-dition, in the current setting, there are no a priorivalues to use for parameters C (cluster size) andFT (ltering threshold), as there were in (Wiebe,2000), and results vary with dierent parametersettings.
Thus, a train-validate-test process is ap-propriate.
In Table 8, the numbers given under,e.g., W9-10, are the results obtained when W9-10is used as the test set.
One of the other datasets,say W9-22, was used as the training set, meaningthat all the adjectives (verbs) in that dataset arethe seed words, and all ltering was performed us-ing only that data.
The seed-ltering process wasrepeated with dierent settings of C and FT , pro-ducing a dierent set of adjectives (verbs) for eachsetting.
A third dataset, say W9-33, was used as avalidation set, i.e., among all the sets of adjectivesgenerated from the training set, those with goodperformance on the validation set were selected asWSJ-SE-M WSJ-SE-D NG-SE-MMultInst MultType MultInst MultType MultInst MultType413 .17 378 .16 571 .29Table 7: Word-POS-Types Used in Multiple Types of Subjective ElementsW9-10 W9-22 W9-33 W9-04freq +prec freq +prec freq +prec freq +precadjectives 373 .21 1340 .11 2137 .09 2537 .14verbs 721 .16 1436 .08 3139 .07 3720 .11unique words 6065 .10 5441 .07 6045 .06 6171 .09baseline precision .17 .13 .14 .18freq: Total frequency +prec: Increase in precision over baselineTable 8: Frequencies and Increases in Precisionthe PSEs to test on the test set.
A set was consid-ered to have good performance on the validationset if its precision is at least .25 and its frequencyis at least 100.
Since this process is meant tobe a method for mining existing document-levelannotations for PSEs, the existing opinion-pieceannotations were used for training and validation.Our manual opinion-piece annotations were usedfor testing.The row labeled unique words shows the preci-sion on the test set of the individual words thatare unique in the test set.
The increase over base-line precision shows that low-frequency words canbe informative for recognizing subjectivity.Note that the features all do better and worseon the same data sets.
This shows that the subjec-tivity is somehow harder to identify in, say, W9-33than in W9-10; it also shows an important consis-tency among the features, even though they areidentied in dierent ways.8 ConclusionsThis paper presents the results of an empirical ex-amination of subjectivity at the dierent levels ofa text: the expression level, the sentence level,and the document level.
While analysis of subjec-tivity is perhaps most natural and precise at theexpression level, document-level annotations arefreely available from a number of sources and areappropriate for many applications.
The sentence-level annotation is a workable intermediate level:sentence-level judgments are not as ne-grained asexpression-level judgments, and they don't involvethe large amount of noise found at the documentlevel.As part of this examination, we present a studyof annotator agreement characterizing the di-culty of identifying subjectivity at the dierentlevels of a text.
The results demonstrate that notonly can subjectivity be identied at the docu-ment level with high reliability, but that it is alsopossible to identify expression-level subjectivity,albeit with lower reliability.Using manual annotations, we are able to char-acterize subjective language.
At the expressionlevel, we found that it is natural to distinguishamong positively evaluative, negatively evalua-tive, and speculative uses of a word.
We alsofound that subjective text contains a high pro-portion of unique word occurrences, much more sothan ordinary text.
Rather than ignoring or dis-carding unique words, we demonstrate that theoccurrence of a unique word is a PSE.
We alsofound that agreement is higher for some syntac-tic word classes, e.g., for adjectives in comparisonwith determiners.Finally, we are able to mine PSEs from texttagged at the document level.
Given the dicultyof evaluating PSEs in document-level subjectiv-ity classication due to the mix of subjective andobjective sentences, the PSEs identied in thisstudy exhibit relatively high precision.
In futurework, we will investigate document-level classi-cation using these PSEs, as well as other methodsfor extracting PSEs from text tagged at the doc-ument level; methods to be investigated includemutual-bootstrapping and/or co-training.ReferencesC.
Aone, M. Ramos-Santacruz, and W. Niehaus.2000.
Assentor: An nlp-based solution to e-mailmonitoring.
In Proc.
IAAI-2000, pages 945{950.A.
Baneld.
1982.
Unspeakable Sentences.
Rout-ledge and Kegan Paul, Boston.R.
Barzilay, M. Collins, J. Hirschberg, andS.
Whittaker.
2000.
The rules behind roles:Identifying speaker role in radio broadcasts.
InProc.
AAAI.E.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
of the 3rd Conference on Ap-plied Natural Language Processing (ANLP-92),pages 152{155.R.
Bruce and J. Wiebe.
1999.
Recognizing subjec-tivity: A case study of manual tagging.
NaturalLanguage Engineering, 5(2).J.
Cohen.
1960.
A coecient of agreement fornominal scales.
Educational and PsychologicalMeas., 20:37{46.A.
P. Dawid and A. M. Skene.
1979.
Max-imum likelihood estimation of observer error-rates using the EM algorithm.
Applied Statis-tics, 28:20{28.M.
Fludernik.
1993.
The Fictions of Languageand the Languages of Fiction.
Routledge, Lon-don.L.
Goodman.
1974.
Exploratory latent structureanalysis using both identiable and unidenti-able models.
Biometrika, 61:2:215{231.E.
Hovy.
1987.
Generating Natural Language un-der Pragmatic Constraints.
Ph.D. thesis, YaleUniversity.D.
Karp, Y. Schabes, M. Zaidel, and D. Egedi.1992.
A freely available wide coverage mor-phological analyzer for English.
In Proc.
ofthe 14th International Conference on Compu-tational Linguistics (COLING-92).D.
Kaufer.
2000.
Flaming: A White Paper.www.eudora.com.B.
Kessler, G. Nunberg, and H. Schutze.
1997.Automatic detection of text genre.
In Proc.ACL-EACL-97.D.
Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
COLING-ACL '98,pages 768{773.Diane J. Litman and R. J. Passonneau.
1995.Combining multiple knowledge sources for dis-course segmentation.
In Proc.
33rd AnnualMeeting of the Association for ComputationalLinguistics (ACL-95), pages 108{115.
Associa-tion for Computational Linguistics, june.D.
Marcu, M. Romera, and E. Amorrortu.
1999.Experiments in constructing a corpus of dis-course trees: Problems, annotation choices, is-sues.
In The Workshop on Levels of Represen-tation in Discourse, pages 71{78.M.
Marcus, Santorini, B., and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The penn treebank.
Computational Lin-guistics, 19(2):313{330.G.
Nunberg, I.
Sag, and T. Wasow.
1994.
Idioms.Language, 70:491{538.W.
Sack.
1995.
Representing and recognizingpoint of view.
In Proc.
AAAI Fall Symposiumon AI Applications in Knowledge Navigationand Retrieval.E.
Spertus.
1997.
Smokey: Automatic recogni-tion of hostile messages.
In Proc.
IAAI.D.
Stein and S. Wright, editors.
1995.
Subjectiv-ity and Subjectivisation.
Cambridge UniversityPress, Cambridge.L.
Terveen, W. Hill, B. Amento, D. McDonald,and J. Creter.
1997.
Building task-specic in-terfaces to high volume conversational data.
InProc.
CHI 97, pages 226{233.S.
Teufel and M. Moens.
2000.
What's yours andwhat's mine: Determining intellectual attribu-tion in scientic texts.
In Proc.
Joint SIGDATConverence on EMNLP and VLC.J.
Wiebe, K. McKeever, and R. Bruce.
1998.Mapping collocational properties into machinelearning features.
In Proc.
6th Workshop onVery Large Corpora (WVLC-98), pages 225{233, Montreal, Canada, August.
ACL SIGDAT.J.
Wiebe, R. Bruce, and T. O'Hara.
1999.
Devel-opment and use of a gold standard data set forsubjectivity classications.
In Proc.
37th An-nual Meeting of the Assoc.
for ComputationalLinguistics (ACL-99), pages 246{253, Univer-sity of Maryland, June.
ACL.J.
Wiebe.
1994.
Tracking point of view in narra-tive.
Computational Linguistics, 20(2):233{287.J.
Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In 17th National Conference onArticial Intelligence (AAAI-2000).
