Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 85?93,Sydney, July 2006. c?2006 Association for Computational LinguisticsExploiting Discourse Structure forSpoken Dialogue Performance AnalysisMihai RotaruUniversity of PittsburghPittsburgh, USAmrotaru@cs.pitt.eduDiane J. LitmanUniversity of PittsburghPittsburgh, USAlitman@cs.pitt.eduAbstractIn this paper we study the utility of dis-course structure for spoken dialogue per-formance modeling.
We experiment withvarious ways of exploiting the discoursestructure: in isolation, as context infor-mation for other factors (correctness andcertainty) and through trajectories in thediscourse structure hierarchy.
Our corre-lation and PARADISE results show that,while the discourse structure is not usefulin isolation, using the discourse structureas context information for other factorsor via trajectories produces highly predic-tive parameters for performance analysis.1 IntroductionPredictive models of spoken dialogue system(SDS) performance are an important tool for re-searchers and practitioners in the SDS domain.These models offer insights on what factors areimportant for the success of a SDS and allowresearchers to assess the performance of futuresystem improvements without running additionalcostly user experiments.One of the most popular models of perform-ance is the PARADISE framework proposed by(Walker et al, 2000).
In PARADISE, a set ofinteraction parameters are measured in a SDScorpus, and then used in a multivariate linearregression to predict the target performance met-ric.
A critical ingredient in this approach is therelevance of the interaction parameters for theSDS success.
A number of parameters thatmeasure the dialogue efficiency (e.g.
number ofsystem/user turns, task duration) and the dia-logue quality (e.g.
recognition accuracy, rejec-tions, helps) have been shown to be successful in(Walker et al, 2000).
An extensive set of pa-rameters can be found in (M?ller, 2005a).In this paper we study the utility of discoursestructure as an information source for SDS per-formance analysis.
The discourse structure hier-archy has been shown to be useful for othertasks: understanding specific lexical and pro-sodic phenomena (Hirschberg and Nakatani,1996; Levow, 2004), natural language generation(Hovy, 1993), predictive/generative models ofpostural shifts (Cassell et al, 2001), and essayscoring (Higgins et al, 2004).We perform our analysis on a corpus ofspeech-based tutoring dialogues.
A tutoring SDS(Litman and Silliman, 2004; Pon-Barry et al,2004) has to discuss concepts, laws and relation-ships and to engage in complex subdialogues tocorrect student misconceptions.
As a result, dia-logues with such systems have a rich discoursestructure.We perform three experiments to measurethree ways of exploiting the discourse structure.In our first experiment, we test the predictiveutility of the discourse structure in itself.
For ex-ample, we look at whether the number of pop-uptransitions in the discourse structure hierarchypredicts performance in our system.The second experiment measures the utility ofthe discourse structure as contextual informationfor two types of student states: correctness andcertainty.
The intuition behind this experiment isthat interaction events should be treated differ-ently based on their position in the discoursestructure hierarchy.
For example, we test if thenumber of incorrect answers after a pop-up tran-sition has a higher predictive utility than the totalnumber of incorrect student answers.
In contrast,the majority of the previous work either ignoresthis contextual information (M?ller, 2005a;Walker et al, 2000) or makes limited use of the85discourse structure hierarchy by flattening it(Walker et al, 2001) (Section 5).As another way to exploit the discourse struc-ture, in our third experiment we look at whetherspecific trajectories in the discourse structure areindicative of performance.
For example, we testif two consecutive pushes in the discourse struc-ture are correlated with higher learning.To measure the predictive utility of our inter-action parameters, we focus primarily on corre-lations with our performance metric (Section 4).There are two reasons for this.
First, a significantcorrelation between an interaction parameter andthe performance metric is a good indicator of theparameter?s relevance for PARADISE modeling.Second, correlations between factors and the per-formance metric are commonly used in tutoringresearch to analyze the tutoring/learning process(Chi et al, 2001).Our correlation and PARADISE results showthat, while the discourse structure is not useful inisolation, using the discourse structure as contextinformation for other factors or via trajectoriesproduces highly predictive parameters for per-formance analysis.2 AnnotationOur annotation for discourse structure and stu-dent state has been performed on a corpus of 95experimentally obtained spoken tutoring dia-logues between 20 students and our systemITSPOKE (Litman and Silliman, 2004).ITSPOKE is a speech-enabled version of thetext-based Why2-Atlas conceptual physics tutor-ing system (VanLehn et al, 2002).
When inter-acting with ITSPOKE, students first type an es-say answering a qualitative physics problem us-ing a graphical user interface.
ITSPOKE thenengages the student in spoken dialogue (usinghead-mounted microphone input and speech out-put) to correct misconceptions and elicit morecomplete explanations, after which the studentrevises the essay, thereby ending the tutoring orcausing another round of tutoring/essay revision.Each student went through the same procedure:1) read a short introductory material, 2) took apretest to measure the initial physics knowledge,3) work through a set of 5 problems withITSPOKE, and 4) took a posttest similar to thepretest.
The resulting corpus had 2334 studentturns and a comparable number of system turns.2.1 Discourse structureWe base our annotation of discourse structure onthe Grosz & Sidner theory of discourse structure(Grosz and Sidner, 1986).
A critical ingredient ofthis theory is the intentional structure.
Accordingto the theory, each discourse has a discourse pur-pose/intention.
Satisfying the main discoursepurpose is achieved by satisfying several smallerpurposes/intentions organized in a hierarchicalstructure.
As a result, the discourse is segmentedin discourse segments each with an associateddiscourse segment purpose/intention.
This theoryhas inspired several generic dialogue managersfor spoken dialogue systems (Bohus and Rud-nicky, 2003).Figure 1.
The discourse structure and transition anno-tationWe automate our annotation of the discoursestructure by taking advantage of the structure ofthe tutored information.
A dialogue withITSPOKE follows a question-answer format (i.e.system initiative): ITSPOKE asks a question, thestudent provides the answer and then the processis repeated.
Deciding what question to ask, inwhat order and when to stop is hand-authoredbeforehand in a hierarchical structure that resem-bles the discourse segment structure (see Figure1).
Tutor questions are grouped in segmentswhich correspond roughly to the discourse seg-ments.
Similarly to the discourse segment pur-pose, each question segment has an associatedtutoring goal or purpose.
For example, in86ITSPOKE there are question segments discuss-ing about forces acting on the objects, others dis-cussing about objects?
acceleration, etc.In Figure 1 we illustrate ITSPOKE?s behaviorand our discourse structure annotation.
First,based on the analysis of the student essay,ITSPOKE selects a question segment to correctmisconceptions or to elicit more complete expla-nations.
This question segment will correspondto the top level discourse segment (e.g.
DS1).Next, ITSPOKE asks the student each questionin DS1.
If the student answer is correct, the sys-tem moves on to the next question (e.g.
Tu-tor1?Tutor2).
If the student answer is incorrect,there are two alternatives.
For simple questions,the system will simply give out the correct an-swer and move on to the next question (e.g.
Tu-tor3?Tutor4).
For complex questions (e.g.
apply-ing physics laws), ITSPOKE will engage into aremediation subdialogue that attempts to reme-diate the student?s lack of knowledge or skills.The remediation subdialogue is specified in an-other question segment and corresponds to a newdiscourse segment (e.g DS2).
The new discoursesegment is dominated by the current discoursesegment (e.g.
DS2 dominated by DS1).
Tutor2system turn is a typical example; if the studentanswers it incorrectly, ITSPOKE will enter dis-course segment DS2 and go through its questions(Tutor3 and Tutor4).
Once all the questions inDS2 have been answered, a heuristic determineswhether ITSPOKE should ask the original ques-tion again (Tutor2) or simply move on to the nextquestion (Tutor5).To compute interaction parameters from thediscourse structure, we focus on the transitions inthe discourse structure hierarchy.
For each sys-tem turn we define a transition feature.
This fea-ture captures the position in the discourse struc-ture of the current system turn relative to theprevious system turn.
We define six labels (seeTable 1).
NewTopLevel label is used for the firstquestion after an essay submission (e.g.
Tutor1).If the previous question is at the same level withthe current question we label the current questionas Advance (e.g.
Tutor2,4).
The first question in aremediation subdialogue is labeled as Push (e.g.Tutor3).
After a remediation subdialogue is com-pleted, ITSPOKE will pop up and it will eitherask the original question again or move on to thenext question.
In the first case, we label the sys-tem turn as PopUp.
Please note that Tutor2 willnot be labeled with PopUp because, in suchcases, an extra system turn will be created be-tween Tutor4 and Tutor5 with the same content asTutor2.
In addition, variations of ?Ok, back to theoriginal question?
are also included in the newsystem turn to mark the discourse segmentboundary transition.
If the system moves on tothe next question after finishing the remediationsubdialogue, we label the system turn asPopUpAdv (e.g.
Tutor5).
Note that while thesum of PopUp and PopUpAdv should be equalwith Push, it is smaller in our corpus because insome cases ITSPOKE popped up more than onelevel in the discourse structure hierarchy.
In caseof rejections, the system question is repeated us-ing variations of ?Could you please repeat that?
?.We label such cases as SameGoal (e.g.
Tutor6).Discourse structure transitionsAdvanceNewTopLevelPopUpPopUpAdvPushSameGoal53.4%13.5%9.2%3.5%14.5%5.9%CertaintyCertainUncertainMixedNeutral41.3%19.1%2.4%37.3%CorrectnessCorrectIncorrectPartially CorrectUnable to Answer63.3%23.3%6.2%7.1%Table 1: Transition and student state distribution.Please note that each student dialogue has aspecific discourse structure based on the dialoguethat dynamically emerges based on the correct-ness of her answers.
For this reason, the samesystem question in terms of content may get adifferent transition label for different students.For example, in Figure 1, if the student wouldhave answered Tutor2 correctly, the next tutorturn would have had the same content as Tutor5but the Advance label.
Also, while a human an-notation of the discourse structure will be morecomplex but more time consuming (Hirschbergand Nakatani, 1996; Levow, 2004), its advan-tages are outweighed by the automatic nature ofour discourse structure annotation.We would like to highlight that our transitionannotation is domain independent and automatic.Our transition labels capture behavior like start-ing a new dialogue (NewTopLevel), crossingdiscourse segment boundaries (Push, PopUp,PopUpAdv) and local phenomena inside a dis-course segment (Advance, SameGoal).
If the dis-course structure information is available, the87transition information can be automatically com-puted using the procedure described above.2.2 Student stateBecause for our tutoring system student learningis the relevant performance metric, we hypothe-size that information about student state in eachstudent turn, in terms of correctness and cer-tainty, will be an important indicator.
For exam-ple, a student being more correct and certain dur-ing her interaction with ITSPOKE might beindicative of a higher learning gain.
Also,previous studies have shown that tutoringspecific parameters can improve the quality ofSDS performance models that model the learninggain (Forbes-Riley and Litman, 2006).In our corpus, each student turn was manuallylabeled for correctness and certainty (Table 1).While our system assigns a correctness label toeach student turn to plan its next move, wechoose to use a manual annotation of correctnessto eliminate the noise introduced by the auto-matic speech recognition component and thenatural language understanding component.
Ahuman annotator used the human transcripts andhis physics knowledge to label each student turnfor various degrees of correctness: correct, par-tially correct, incorrect and unable to answer.
?Unable to Answer?
label was used for turnswhere the student did not answer the systemquestion or used variants of ?I don?t know?.Previous work has shown that certainty playsan important role in the learning and tutoringprocess (Pon-Barry et al, 2006; VanLehn et al,2003).
A human annotator listened to the dia-logues between students and ITSPOKE and la-beled each student turn for its perceived degreeof certainness.
Four labels were used: certain,uncertain, neutral and mixed (both certain anduncertain).
To date, one annotator has labeled allstudent turns in our corpus1.3 Interaction parametersFor each user, interaction parameters measurespecific aspects of the dialogue with the system.We use our transition and student state annota-tion to create two types of interaction parame-1 The agreement between the manual correctness an-notation and the correctness assigned by ITSPOKE is90% (kappa of 0.79).
In a preliminary agreementstudy, a second annotator labeled our corpus for abinary version of certainty (uncertainty versus other),resulting in a 90% inter-annotator agreement and akappa of 0.68.ters: unigrams and bigrams.
The difference be-tween the two types of parameters is whether thediscourse structure context is used or not.
Foreach of our 12 labels (4 for correctness, 4 forcertainty and 6 for discourse structure), we de-rive two unigram parameters per student over the5 dialogues for that student: a total parameterand a percentage parameter.
For example, for the?Incorrect?
unigram we compute, for each stu-dent, the total number of student turns labeledwith ?Incorrect?
(parameter Incorrect) and thepercentage of such student turns out of all stu-dent turns (parameter Incorrect%).
For example,if we consider only the dialogue in Figure 1, In-correct = 3 (Student2,3,5) and Incorrect% = 60%(3 out of 5).Bigram parameters exploit the discourse struc-ture context.
We create two classes of bigramparameters by looking at transition?student statebigrams and transition?transition bigrams.
Thetransition?student state bigrams combine the in-formation about the student state with the transi-tion information of the previous system turn.
Go-ing back to Figure 1, the three incorrect answerswill be distributed to three bigrams: Advance?Incorrect (Tutor2?Student2), Push?Incorrect (Tu-tor3?Student3) and PopUpAdv?Incorrect (Tutor5?Student5).
The transition?transition bigram looksat the transition labels of two consecutive systemturns.
For example, the Tutor4?Tutor5 pair willbe counted as an Advance?PopUpAdv bigram.Similar to the unigrams, we compute a totalparameter and a percentage parameter for eachbigram.
The percentage denominator is numberof student turns for the transition?student statebigrams and the number of system turns minusone for the transition?transition bigram.
In addi-tion, for each bigram we compute a relative per-centage parameter (bigram followed by %rel) bycomputing the percentage relative to the totalnumber of times the transition unigram appearsfor that student.
For example, we will computethe Advance?Incorrect %rel parameter by divid-ing the number of Advance?Incorrect bigramswith the number of Advance unigrams (1 dividedby 2 in Figure 1); this value will capture the per-centage of times an Advance transition is fol-lowed by an incorrect student answer.4 ResultsWe use student learning as our evaluation metricbecause it is the primary metric for evaluatingthe performance of tutoring systems.
Previouswork (Forbes-Riley and Litman, 2006) has suc-88cessfully used student learning as the perform-ance metric in the PARADISE framework.
Twoquantities are used to measure student learning:the pretest score and the posttest score.
Both testsconsist of 40 multiple-choice questions; the test?sscore is computed as the percentage of correctlyanswered questions.
The average score and stan-dard deviation for each test are: pretest 0.47(0.17) and posttest 0.68 (0.17).We focus primarily on correlations betweenour interaction parameters and student learning.Because in our data the pretest score is signifi-cantly correlated with the posttest score, westudy partial Pearson?s correlations between ourparameters and the posttest score that account forthe pretest score.
This correlation methodology iscommonly used in the tutoring research (Chi etal., 2001).
For each trend or significant correla-tion we report the unigram/bigram, its averageand standard deviation over all students, thePearson?s Correlation Coefficient (R) and thestatistical significance of R (p).First we report significant correlations for uni-grams to test our first hypothesis.
Next, for oursecond and third experiment, we report correla-tions for transition?student state and transition?transition parameters.
Finally, we report our pre-liminary results on PARADISE modeling.4.1 Unigram correlationsIn our first proposed experiment, we want to testthe predictive utility of discourse structure inisolation.
We compute correlations between ourtransition unigram parameters and learning.
Wefind no trends or significant correlations.
Thisresult suggests that discourse structure in isola-tion has no predictive utility.Here we also report all trends and significantcorrelations for student state unigrams as thebaseline for contextual correlations to be pre-sented in Section 4.2.
We find only one signifi-cant correlation (Table 2): students with a higherpercentage of neutral turns (in terms of certainty)are negatively correlated with learning.
We hy-pothesize that this correlation captures the stu-dent involvement in the tutoring process: moreinvolved students will try harder thus expressingmore certainty or uncertainty.
In contrast, lessinvolved students will have fewer certain/uncer-tain/mixed turns and, in consequence, more neu-tral turns.
Surprisingly, student correctness doesnot significantly correlate with learning.Parameter Mean (SD) R. pNeutral % 37% (8%) -.47 .04Table 2: Trend and significant unigram correlations4.2 Transition?student state correlationsFor our second experiment, we need to determinethe predictive utility of transition?student statebigram parameters.
We find a large number ofcorrelations for both transition?correctness bi-grams and transition?certainty bigrams.Transition?correctness bigramsThis type of bigram informs us whether ac-counting for the discourse structure transitionwhen looking at student correctness has any pre-dictive value.
We find several interesting trendsand significant correlations (Table 3).The student behavior, in terms of correctness,after a PopUp or a PopUpAdv transition is veryinformative about the student learning process.In both situations, the student has just finished aremediation subdialogue and the system is pop-ping up either by reasking the original questionagain (PopUp) or by moving on to the next ques-tion (PopUpAdv).
We find that after PopUp, thenumber of correct student answers is positivelycorrelated with learning.
In contrast, the number,the percentage and the relative percentage of in-correct student answers are negatively correlatedwith learning.
We hypothesize that this correla-tion indicates whether the student took advantageof the additional learning opportunities offeredby the remediation subdialogue.
By answeringcorrectly the original system question (PopUp?Correct), the student demonstrates that she hasabsorbed the information from the remediationdialogue.
This bigram is an indication of a suc-cessful learning event.
In contrast, answering theoriginal system question incorrectly (PopUp?Incorrect) is an indication of a missed learningopportunity; the more events like this happen theless the student learns.Parameter Mean (SD) R. pPopUp?Correct 7 (3.3) .45 .05PopUp?Incorrect 2 (1.8) -.42 .07PopUp?Incorrect % 1.6% (1.2%) -.46 .05PopUp?Incorrect %rel 17% (13%) -.39 .10PopUpAdv?Correct 2.5 (2) .43 .06PopUpAdv?Correct % 2% (1.3%) .52 .02NewTopLevel?Incorrect 2.3 (1.8) .56 .01NewTopLevel?Incorrect % 1.9% (1.4%) .49 .03NewTopLevel?Incorrect %rel 15% (12%) .51 .02Advance?Correct 40.5 (9.8) .45 .05Table 3: Trend and significant transition?correctnessbigram correlationsSimilarly, being able to correctly answer thetutor question after popping up from a remedia-tion subdialogue (PopUpAdv?Correct) is posi-tively correlated with learning.
Since in manycases, these system questions will make use of89the knowledge taught in the remediation subdia-logues, we hypothesize that this correlation alsocaptures successful learning opportunities.Another set of interesting correlations is pro-duced by the NewTopLevel?Incorrect bigram.We find that the number, the percentage and therelative percentage of times ITSPOKE starts anew essay revision dialogue that results in anincorrect student answer is positively correlatedwith learning.
The content of the essay revisiondialogue is determined based on ITSPOKE?sanalysis of the student essay.
We hypothesizethat an incorrect answer to the first tutor questionis indicative of the system?s picking of a topicthat is problematic for the student.
Thus, we seemore learning in students for which more knowl-edge gaps are discovered and addressed byITSPOKE.Finally, we find the number of times the stu-dent answers correctly after an advance transitionis positively correlated with learning (the Ad-vance?Correct bigram).
We hypothesize that thiscorrelation captures the relationship betweenstudents that advance without having major prob-lems and a higher learning gains.Transition?certainty bigramsNext we look at the combination between thetransition in the dialogue structure and the stu-dent certainty (Table 4).
These correlations offermore insight on the negative correlation betweenthe Neutral % unigram parameter and studentlearning.
We find that out of all neutral studentanswers, those that follow an Advance transi-tions are negatively correlated with learning.Similar to the Neutral % correlation, we hy-pothesize that Advance?Neutral correlations cap-ture the lack of involvement of the student in thetutoring process.
This might be also due toITSPOKE engaging in teaching concepts that thestudent is already familiar with.Parameter Mean (SD) R. pAdvance?Neutral 27 (8.3) -.40 .08Advance?Neutral % 21% (6%) -.62 .00Advance?Neutral %rel 38% (10%) -.73 .00SameGoal?Neutral %rel 35% (31%) .46 .05Table 4: Trend and significant transition?certaintybigram correlationsIn contrast, staying neutral in terms of cer-tainty after a system rejection is positively corre-lated with learning.
These correlations show thatbased on their position in the discourse structure,neutral student answers will be correlated eithernegatively or positively with learning.Unlike student state unigram parameterswhich produce only one significant correlation,transition?student state bigram parameters pro-duce a large number of trend and significant cor-relations (14).
This result suggests that exploitingthe discourse structure as a contextual informa-tion source can be beneficial for performancemodeling.4.3 Transition?transition bigramsFor our third experiment, we are looking at thetransition?transition bigram correlations (Table5).
These bigrams help us find trajectories oflength two in the discourse structure that are as-sociated with better student learning.
Becauseour student state is domain dependent, translatingthe transition?student state bigrams to a newdomain will require finding a new set of relevantfactors to replace the student state.
In contrast,because our transition information is domain in-dependent, transition?transition bigrams can beeasily implemented in a new domain.The Advance?Advance bigram covers situa-tions where the student is covering tutoring ma-terial without major knowledge gaps.
This is be-cause an Advance transition happens when thestudent either answers correctly or his incorrectanswer can be corrected without going into aremediation subdialogue.
Just like with the Ad-vance?Correct correlation (recall Table 3), wehypothesize that these correlations links higherlearning gains to students that cover a lot of ma-terial without many knowledge gap.Parameter Mean (SD) R. pAdvance?Advance 35 (9.1) .47 .04Push?Push 2.2 (1.7) .50 .03Push?Push % 1.8% (1.3%) .52 .02Push?Push %rel 11% (7%) .52 .02SameGoal?Push %rel 18% (23%) .49 .03Table 5: Trend and significant transition?transitionbigram correlationsThe Push?Push bigrams capture another inter-esting behavior.
In these cases, the student incor-rectly answers a question, entering a remediationsubdialogue; she also incorrectly answers thefirst question in the remediation dialogue enter-ing an even deeper remediation subdialogue.
Wehypothesize that these situations are indicative ofbig student knowledge gaps.
In our corpus, wefind that the more such big knowledge gaps arediscovered and addressed by the system thehigher the learning gain.The SameGoal?Push bigram captures anothertype of behavior after system rejections that ispositively correlated with learning (recall theSameGoal?Neutral bigram, Table 4).
In our pre-vious work (Rotaru and Litman, 2006), we per-90formed an analysis of the rejected student turnsand studied how rejections affect the studentstate.
The results of our analysis suggested a newstrategy for handling rejections in the tutoringdomain: instead of rejecting student answers, atutoring SDS should make use of the availableinformation.
Since the recognition hypothesis fora rejected student turn would be interpreted mostlikely as an incorrect answer thus activating aremediation subdialogue, the positive correlationbetween SameGoal?Push and learning suggeststhat the new strategy will not impact learning.Similar to the second experiment, the resultsof our third experiment are also positive: in con-trast to transition unigrams, our domain inde-pendent trajectories can produce parameters witha high predictive utility.4.4 PARADISE modelingHere we present our preliminary results on ap-plying the PARADISE framework to modelITSPOKE performance.
A stepwise multivariatelinear regression procedure (Walker et al, 2000)is used to automatically select the parameters tobe included in the model.
Similar to (Forbes-Riley and Litman, 2006), in order to model thelearning gain, we use posttest as the dependentvariable and force the inclusion of the pretestscore as the first variable in the model.For the first experiment, we feed the model alltransition unigrams.
As expected due to lack ofcorrelations, the stepwise procedure does notselect any transition unigram parameter.
Theonly variable in the model is pretest resulting in amodel with a R2 of .22.For the second and third experiment, we firstbuild a baseline model using only unigram pa-rameters.
The resulting model achieves an R2 of.39 by including the only significantly correlatedunigram parameter: Neutral %.
Next, we build amodel using all unigram parameters and all sig-nificantly correlated bigram parameters.
The newmodel almost doubles the R2 to 0.75.
Besides thepretest, the parameters included in the resultingmodel are (ordered by the degree of contributionfrom highest to lowest): Advance?Neutral %rel,and PopUp?Incorrect %.
These results strengthenour correlation conclusions: discourse structureused as context information or as trajectories in-formation is useful for performance modeling.Also, note that the inclusion of student certaintyin the final PARADISE model provides addi-tional support to a hypothesis that has gained alot of attention lately: detecting and respondingto student emotions has the potential to improvelearning (Craig et al, 2004; Forbes-Riley andLitman, 2005; Pon-Barry et al, 2006).The performance of our best model is compa-rable or higher than training performances re-ported in previous work (Forbes-Riley and Lit-man, 2006; M?ller, 2005b; Walker et al, 2001).Since our training data is relatively small (20data points) and overfitting might be involvedhere, in the future we plan to do a more in-depthevaluation by testing if our model generalizes ona larger ITSPOKE corpus we are currently anno-tating.5 Related workPrevious work has proposed a large number ofinteraction parameters for SDS performancemodeling (M?ller, 2005a; Walker et al, 2000;Walker et al, 2001).
Several information sourcesare being tapped to devise parameters classifiedby (M?ller, 2005a) in several categories: dia-logue and communication parameters (e.g.
dia-logue duration, number of system/user turns),speech input parameters (e.g.
word error rate,recognition/concept accuracy) and meta-communication parameters (e.g.
number of helprequest, cancel requests, corrections).But most of these parameters do not take intoaccount the discourse structure information.
Anotable exception is the DATE dialogue act an-notation from (Walker et al, 2001).
The DATEannotation captures information on three dimen-sions: speech acts (e.g.
acknowledge, confirm),conversation domain (e.g.
conversation- versustask-related) and the task model (e.g.
subtaskslike getting the date, time, origin, and destina-tion).
All these parameters can be linked to thediscourse structure but flatten the discoursestructure.
Moreover, the most informative ofthese parameters (the task model parameters) aredomain dependent.
Similar approximations of thediscourse structure are also common for otherSDS tasks like predictive models of speech rec-ognition problems (Gabsdil and Lemon, 2004).We extend over previous work in several ar-eas.
First, we exploit in more detail the hierarchi-cal information in the discourse structure.
Wequantify this information by recording the dis-course structure transitions.
Second, in contrastto previous work, our usage of discourse struc-ture is domain independent (the transitions).Third, we exploit the discourse structure as acontextual information source.
To our knowl-edge, previous work has not employed parame-ters similar with our transition?student state bi-91gram parameters.
Forth, via the transition?transition bigram parameters, we exploit trajecto-ries in the discourse structure as another domainindependent source of information for perform-ance modeling.
Finally, similar to (Forbes-Rileyand Litman, 2006), we are tackling a more prob-lematic performance metric: the student learninggain.
While the requirements for a successfulinformation access SDS are easier to spell out,the same can not be said about tutoring SDS dueto the current limited understanding of the hu-man learning process.6 ConclusionIn this paper we highlight the role of discoursestructure for SDS performance modeling.
Weexperiment with various ways of using the dis-course structure: in isolation, as context informa-tion for other factors (correctness and certainty)and through trajectories in the discourse structurehierarchy.
Our correlation and PARADISE re-sults show that, while the discourse structure isnot useful in isolation, using the discourse struc-ture as context information for other factors orvia trajectories produces highly predictive pa-rameters for performance analysis.
Moreover, thePARADISE framework selects in the final modelonly discourse-based parameters ignoring pa-rameters that do not use the discourse structure(certainty and correctness unigrams are ignored).Our significant correlations also suggest wayswe should modify our system.
For example, thePopUp?Incorrect negative correlations suggestthat after a failed learning opportunity the systemshould not give out the correct answer but en-gage in a secondary remediation subdialoguespecially tailored for these situations.In the future, we plan to test the generality ofour PARADISE model on other corpora and tocompare models built using our interaction pa-rameters against models based on parameterscommonly used in previous work (M?ller,2005a).
Testing if our results generalize to a hu-man annotation of the discourse structure andautomated models of certainty and correctness isalso of importance.
We also want to see if ourresults hold for performance metrics based onuser satisfaction questionnaires; in the newITSPOKE corpus we are currently annotating,each student also completed a user satisfactionsurvey (Forbes-Riley and Litman, 2006) similarto the one used in the DARPA Communicatormulti-site evaluation (Walker et al, 2002).Our work contributes to both the computa-tional linguistics domain and the tutoring do-main.
For the computational linguistics researchcommunity, we show that discourse structure isan important information source for SDS per-formance modeling.
Our analysis can be ex-tended easily to other SDS.
First, a similar auto-matic annotation of the discourse structure canbe performed in SDS that rely on dialogue man-agers inspired by the Grosz & Sidner theory ofdiscourse (Bohus and Rudnicky, 2003).
Second,the transition?transition bigram parameters aredomain independent.
Finally, for the other suc-cessful usage of discourse structure (transition?student state bigrams) researchers have only toidentify relevant factors and then combine themwith the discourse structure information.
In ourcase, we show that instead of looking at the userstate in isolation (Forbes-Riley and Litman,2006), combining it with the discourse structuretransition can generate informative interactionparameters.For the tutoring research community, we showthat discourse structure, an important concept incomputational linguistics theory, can provideuseful insights regarding the learning process.The correlations we observe in our corpus haveintuitive interpretations (successful/failed learn-ing opportunities, discovery of deep studentknowledge gaps, providing relevant tutoring).AcknowledgementsThis work is supported by NSF Grant No.0328431.
We would like to thank Kate Forbes-Riley, Joel Tetreault and our anonymous review-ers for their helpful comments.ReferencesD.
Bohus and A. Rudnicky.
2003.
RavenClaw:Dialog Management Using Hierarchical TaskDecomposition and an Expectation Agenda.
In Proc.of Eurospeech.J.
Cassell, Y. I. Nakano, T. W. Bickmore, C. L.Sidner and C. Rich.
2001.
Non-Verbal Cues forDiscourse Structure.
In Proc.
of ACL.M.
T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchiand R. G. Hausmann.
2001.
Learning from humantutoring.
Cognitive Science, 25.S.
D. Craig, A. C. Graesser, J. Sullins and B.Gholson.
2004.
Affect and learning: an exploratorylook into the role affect in learning with AutoTutor.Journal of Educational Media, 29.92K.
Forbes-Riley and D. Litman.
2005.
UsingBigrams to Identify Relationships Between StudentCertainness States and Tutor Responses in a SpokenDialogue Corpus.
In Proc.
of SIGdial.K.
Forbes-Riley and D. Litman.
2006.
ModellingUser Satisfaction and Student Learning in a SpokenDialogue Tutoring System with Generic, Tutoring,and User Affect Parameters.
In Proc.
ofHLT/NAACL.M.
Gabsdil and O.
Lemon.
2004.
CombiningAcoustic and Pragmatic Features to PredictRecognition Performance in Spoken DialogueSystems.
In Proc.
of ACL.B.
Grosz and C. L. Sidner.
1986.
Attentions,intentions and the structure of discourse.Computational Lingustics, 12(3).D.
Higgins, J. Burstein, D. Marcu and C. Gentile.2004.
Evaluating Multiple Aspects of Coherence inStudent Essays.
In Proc.
of HLT-NAACL.J.
Hirschberg and C. Nakatani.
1996.
A prosodicanalysis of discourse segments in direction-givingmonologues.
In Proc.
of ACL.E.
Hovy.
1993.
Automated discourse generationusing discourse structure relations.
ArticialIntelligence, 63(Special Issue on NLP).G.-A.
Levow.
2004.
Prosodic Cues to DiscourseSegment Boundaries in Human-Computer Dialogue.In Proc.
of SIGdial.D.
Litman and S. Silliman.
2004.
ITSPOKE: Anintelligent tutoring spoken dialogue system.
In Proc.of HLT/NAACL.S.
M?ller.
2005a.
Parameters for Quantifying theInteraction with Spoken Dialogue Telephone Services.In Proc.
of SIGDial.S.
M?ller.
2005b.
Towards Generic QualityPrediction Models for Spoken Dialogue Systems - ACase Study.
In Proc.
of Interspeech.H.
Pon-Barry, B. Clark, E. O. Bratt, K. Schultz andS.
Peters.
2004.
Evaluating the effectiveness of Scot:aspoken conversational tutor.
In Proc.
of ITSWorkshop on Dialogue-based Intelligent TutoringSystems.H.
Pon-Barry, K. Schultz, E. O. Bratt, B. Clark andS.
Peters.
2006.
Responding to Student Uncertainty inSpoken Tutorial Dialogue Systems.
InternationalJournal of Artificial Intelligence in Education, 16.M.
Rotaru and D. Litman.
2006.
Dependenciesbetween Student State and Speech RecognitionProblems in Spoken Tutoring Dialogues.
In Proc.
ofACL.K.
VanLehn, P. W. Jordan, C. P.
Ros?, D. Bhembe,M.
B?ttner, A. Gaydos, M. Makatchev, U.Pappuswamy, M. Ringenberg, A. Roque, S. Siler andR.
Srivastava.
2002.
The Architecture of Why2-Atlas:A Coach for Qualitative Physics Essay Writing.
InProc.
of Intelligent Tutoring Systems (ITS).K.
VanLehn, S. Siler, C. Murray, T. Yamauchi andW.
B. Baggett.
2003.
Why do only some events causelearning during human tutoring?
Cognition andInstruction, 21(3).M.
Walker, D. Litman, C. Kamm and A. Abella.2000.
Towards Developing General Models ofUsability with PARADISE.
Natural LanguageEngineering.M.
Walker, R. Passonneau and J. Boland.
2001.Quantitative and Qualitative Evaluation of DarpaCommunicator Spoken Dialogue Systems.
In Proc.
ofACL.M.
Walker, A. Rudnicky, R. Prasad, J. Aberdeen,E.
Bratt, J. Garofolo, H. Hastie, A.
Le, B. Pellom, A.Potamianos, R. Passonneau, S. Roukos, G. Sanders, S.Seneff and D. Stallard.
2002.
DARPA Communicator:Cross-System Results for the 2001 Evaluation.
InProc.
of ICSLP.93
