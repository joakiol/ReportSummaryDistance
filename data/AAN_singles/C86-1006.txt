USER MODELS: THE PROBLEM OF DISPARITYSandra CarberryI)epartment of Computer and Information SciencesUniversity of DelawareNewark, Delaware 19716, U.S.A.ABSTRACTA signific~mt component of a user raodel in auinfornration-seeking dialogue is tim task-related plazlmotivating the information-seeker's queries.
A numberof researchers have Irmdeled the plan inference processand used these models to design more robust naturallanguage interfaces.
However in each case, it has beenassumed that the system's context model and the planunder construction by the information-seeker are neverat variance.
This paper addre~es the problem ofdisparate plans.
It presents a four phase approach andargues that hmldling disparate plans requires anenriched context model.
This model nmst permit tileaddition of companents suggested by the information-,'~eeker but not fully supported by the system's domainknowledge, and must differentiate mnong its com-ponents according to the kind of support accorded eachcomponent as a correct part of the information-seeker'soverall plan.
It is shown how a component's supportshould affect the system's hypothesis about the sourceof error once plan disparity is suggested.I .
INTRODUCTIONCorranunication as we know it involves more thml ~inlplyanswering isolated queries.
When two individuals participate inan iuformation-seeking dialogue, tile information-provider uses thecontext within which each query occurs to interpret the query,determine tile desired information, and formulate an appropriateresponse.
This context consists of more than mere knowledge ofthe previous questions and answers.
A cooperative participantuses the information exchanged during the dialogue andknowledge of the domain to hypothesize a model of the speaker;this model is adjusted and expanded as the dialogue progressesand is called a user model.Perhaps the most significant component of a user model isthe listener's belief about the underlying task motivating theinformation-seeker's queries and his partially developed plan foraccomplishing this task.
A number of researchers have modeledthe plan inference process \[Allen 1980\], \[Cacberry 1983\], \[Grosz1977\], \[Litman 1984\], \[Perrault 19801, \[Robinson 1981\], \[Sidner1983\], and these models have been used to understand indirectspeech acts \[Perranlt 1980\], provide helpful responses \[Allen 1980\],interpret pragmatically ill-formed queries \[Carberry 1986\], under-stand intersentential ellipsis \[Allen 1980, Carberry 1985\], and iden-tify the kind of response intended by a speaker \[Sidner 1983\].However in each case, four critical assmnptions have beenmagic:\[1\] Tile inforroation-seeker's knowledge about the task domainmay be lacking but is not erroneous.\[2\] The infornmtion-seeker's queries never address aspects of thetask outside tile system's knowledge.
Such systems maintainthe closed world assumption \[Reiter 1978\].\[3\] The information provided by the information-seeker iscorrect a~ld not misleading.\[4\] The underlying plan inferred by the system prior to analysisof a new utterance is a partially instantiated version of theplan under consideration by the information-seeker.These assumptions eliminate the possibility that tile information-seeker might ask queries irrelevant o the task at hand, that theinformation..seeker might ask about details outside tile system'slimited knowledge, that the information-seeker might accidentallyprovide misleading information, and that the system might havemade erroneous inferences from previous queries.
The end resultis that tbe system believes that the underlying task-related planinferred by the system and the task-related plan under construc-tion by the information-seeker a e never at variance with oneanother.If we want systems capable of understanding and appropri-ately responding to naturally occurring dialogue, natural anguageinterfaces must be able to deal with situations where thoseassumptions are not true.
Our analysis of transcripts of naturallyoccurring information-seeking dialogues indicates that human par-ticipants attempt to detect inconsistencies in the models andrepair them whenever possible.
We claim that natural languagesystenm must do likewise; othe~'ise they will be unable to respondappropriately and cooperatively to dialogue that humans regard asnatural.This paper presents a taxonomy of disparate plan models,according to how the model inferrod by the information-providerreflects the information-seeker's model of his task.
We claim thatplan inference must be extended to include a four phase approachto handling disparate plans ~md that this approach requires aricher model than maintained by current systems.
We show howthe support that an information-provider accords a component asa correct past of the model affects her hypothesis about the sourceof error once plan disparity is suggested.2.
TYPES OF MODELSAn information-seeking dialogue contains two participauts,one seeking hfformation and the other attempting to provide thatinformation.
Underlying such a dialogue is a task which theinformation-seeker wants to perform, generally at some time inthe future.
The information-seeker poses queries in order toobtain the information ecessary to construct a plan for accom-plishing this task.
Examples of such tasks include pursuing a pro-gram of study in a university domain, treating a patient in a med-ical domain, and taking a vacation in a travel domain.A cooperative natural hmguage system must attempt toinfer the underlying task-related plan motivating theinformation-seeker's queries mad use this plan to provide coopera-tive, helpful responses \[Carberry 1983, 1985\].
We call the system'smodel of this plan a context model.
A context model is one com-ponent of a user model.29We are concerned here with cases in which the system's con-text model fails to mirror the plan under construction by theinformation-seeker.
Disparate plan models may be classifiedaccording to how the model inferred by the system differs fromthe information-seeker's model of his task:\[1\] erroneous models, representing eases in which the modelinferred by the system is inconsistent with the information-seeker's model.
If the information-seeker were to examinethe system's model in such cases, he would regard it as con-taining errors.\[2\] overly-speclalized models, representing cases in which themodel inferred by the system is more restricted than thatintended by the information-seeker.\[3\] overly-generalized models, representing cases in which themodel inferred by the system is less specific than thatintended by the information-seeker.\[4\] knowledge-liraited models~ representing cases in which themodel inferred by the system fails to mirror the plan underconstruction by the information-seeker, due to the system'slimited domain knowledge.The use of default inferencing rules may produce erroneous oroverly-specialized models.
Erroneous models may also result if theinformatlon-seeker's statements are inaccurate or misleading or ifthe system uses focusing heuristics to relate new utterances to theexisting plan context.
Overly-generalized models may result if theinformation-seeker fails to adequately communicate his intentions(or the system fails to recognize these intentions).
Knowledge-limited models may result if the information-seeker's domainknowledge xceeds that of the system.A fifth category, partial models, represents cases in whichthe system has inferred only part of the information-seeker's plan;subsequent dialogue will enable the system to further expand andrefine this context model as more of the information-seeker'sintentions are communicated.
We do not regard partial models asdisparate structures: were the informatlon-seeker to examine thesystem's inferred partial plan, he would regard it as correctlymodeling his intentions as communicated in the dialogue thus far.3.
RELATED WORKSeveral research efforts have addressed problems related toplan disparity.
Kaplan\[1982\] and McCoy\[1986\] investigatedmisconceptions about domain knowledge and proposed responsesintended to remove the misconception.
However such misconcep-tions may not be exhibited when they first influence theinformation-seeker's plan construction; in such cases, disparateplans may result and correction will entail both a responsecorrecting the misconception and further processing to bring thesystem's context model and the plan under construction by theinformation-seeker back into alignment.Pollack\[1986\] is studying removal of what she terms the"appropriate query assumption" of previous planning systems; sheproposes a richer model of planning that explicitly reasons aboutthe information-seeker's possible beliefs and intentions.
Heroverall goal is to develop a better model of plan inference.
Sheaddresses the problem of queries that indicate the information-seeker's plan is inappropriate to his overall goal., and attempts toisolate the erroneous beliefs that led to the inappropriate query.This is a subclass of "erroneous plans", since upon hearing thequery, the system should detect that its context model no longeragrees with that of the information-seeker.
However, queriesdeemed inappropriate by the system may signal phenomena otherthan inappropriate user plans.
For example, the information-seeker may have shifted focus to another aspect of the overalltask without successfully conveying this to the system, the30information-seeker may be addressing aspects of the task outsidethe system's limited knowledge, or the system's context modelmay have been in error prior to the query.Pollack is concerned with issues that arise when theinformation-s.~eker's plan is incorrect due to a misconception.
Sheassumes Chat, immediately prior to the user making the "prob~lematic" q~ery, the system's partial model of the user's plan iscorrect.
We argue that since the system's inference mechanismsare not infallible and communication itself is imperfect, the sys-tem must contend with the possibility that its inferred model doesnot accurately reflect the user's plan.
Previous research as failedto address this problem.4.
PROBLEM POSED BY  D ISPARATE MODELSGrosz\[1981\] claimed that communication can proceedsmoothly only if both dialogue participants are focused on thesame subset of knowledge.
Extending this to inferred plans, weclaim that communication is most successful when theinformatlon-provider's and information-seeker's models mirror oneanother.
But clearly i t  is unrealistic to expect that these modelswill never diverge, given the different knowledge bases of the twoparticipants and the imperfections of communication via dialogue.Thus the information-provider (IP) and the information-seeker(IS) must be able to detect inconsistencies in the models wheneverpossible and repair them.
Clearly a natural anguage system mustdo the same.This view is supported by the work of Pollack, Hirsehberg,and Webber\[1982\].
They conducted a study of naturally occurringexpert-novice dialogues and suggested that such interaction couldbe viewed as a negotiation process, during which not only anacceptable solution is negotiated but also understanding of theterminology and the beliefs of the participants.
The contextmodel is one component of IP's beliefs, as is her belief that itaccurately reflects the plan under construction by IS.5.
AN APPROACH TO D ISPARATE MODELSA study of transcripts of naturally occurring information-seeking dialogues indicates that humans often employ a fourphase approach in detecting and recovering from disparate planstructures.
Therefore a natural language interface that pursuesthe same strategy will be viewed as acting naturally by humanusers.
The next sections discuss each of these phases.5.1.
DETECT ION AND HYPOTHESIS  FORMATIONAs claimed earlier, since IP is presumed to be a cooperativedialogue participant, IP must be on the lookout for plan disparity.We have identified three sources of clues to the existence of suchdisparity:\[1\] the discourse goals of IS, such as expressing surprise or con-fusion\[2\] relevance of ISis current utterence to IP's inferred model\[31 focus of attention in the modelIS can express surprise or confusion about IP's response,thereby cuing the possibility of plan disparity.
Consider forexample the dialogue presented in Figure 1.
This dialogue wastranscribed from a radio talk show on investments~and will bereferred to as the "IRA example"; utterances are numbered forlater reference.
Plan disparity is suggested when IS, in utterance\[5\], expresses confusion at IP's previous response.On the other hand, IS's query may contradict or appearirrelevant o what IP believes is IS's overall task, leading IP tosuspect hat her context model may not reflect IS's plan.
Or IS's~-~:r~;;~\[,tfo-f-ih~;:-d\]~\[o-gu-es were provided by the Depart-ment of Computer Science of the University of Pennsylvania\[1\] IS:\[2\] IP:\[3\] IS:\[41 IP:\[5\] IS:\[6\] IP:\[7\] IS:\[81 IP:"I 'm ~ retired government employee but I 'm stillworking.
I'd like to start out an IRA for myselfmid my wife --- she doesn't work.
""Did you work outside of the government last year?
""Yes I did.
""There's no reason why you shouldn't have an IRAfor last year.
""I thought hey just started this year.
""Oh no.
IRA's were available as long as you arenot a participant in an existing pension.
""Well, I do work for a company that has a pension.""Ahh.
Then you're not eligible for 81.
"Figure 1.
Individual Retirement Account Dialogue ~tquery may require so sharp an unsignaled shift in focus as tocause IP to be suspicious; the strongest expectations are forspeakers to address aspects of the task closely related to thecurrent focus of attention \[Sidner 1981, McKeown 1985, Carberry1983\].
The dialogue presented in Figure 2, and henceforthreferred to as the "Kennit example", illustrates a toque in whichplan disparity is suggested by an abrupt shift in focus of atten-tion.
Upon completion of utterance \[4\], IP's model of IS's planmight be represented asGoal: Tnmsfer-Files(IS,KERMIT,VAX,PC)Precox~dltion: Have(IS,KERMIT)oo\[o:::7::;:: 5:=:;Precondition: Have(IS,<x>)Both humans mid machines have limited knowledge.
Sup-pose that IP does not know how to purchase floppy disks.
Thenfrom IP's limited knowledge, IS's next query,"How late is the University Bookstore open?
"will not appear to address an aspect of the plan inferred for IS, orany expansion of it.
IP could just respond by\[1\] answering the direct question~ if possible, ignoring itsramifications\[2\] responding "I don~t know", if the direct answer is not avail-ableHowever cooperative human information-providers a e expected totry to understand the import of a query and provide as coopera-tive a response ~ they can.Griee's maxim of relation \[Grice 1975\] suggests that ISbelieves the query to be relevant o the overall dialogue.
Severalpossibilities exist.
IS may be shifting focus to some aspect of ahigher-level task that incindes transferring files as a subaction.One such higher-level task might be to compose a document usingthe SCRIBE text formatting system~ and the aspect queried bythe new uttere~me might be the purchase of a SCRIBE manualfrom the univemity bookstore; in this ease, the subtask of theoverall task represented by the existing context model might be.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Minor alterations have been made to the dialogue to removerestarts and extraneous phrasing.\[!\] IS: "I wish I could transfer files between theVax and my PC.
"I2\] IP: "Kermit lets you do that.
"\[3\] IS: "How do I get Kemfit?
"\[4\] IP: "The computing center will give you a copyif you bring them a floppy disk.
"\[5\] IS: "How late is the University Bookstore open?
"Figure 2.
File Transfer Via Kermlt Dialoguethe transfer of files containing the document so that they can bemodified using a PC editor.On the other hand~ focusing heuristics and the absence ofdiscourse rrmrkers \[Sidner 1985\] suggest hat the new query ismost likely to be relevant o the current focus of attention.
So IPshould begin trying to determine how IS's utterance mght relateto the currently focused subtask in tim context model, and con-sider the possibility that IS's domain knowledge might exceed IP'sor irfight be erroneous.5.2.
RESPONSE PHASEWebber\[1986\] distinguishes between answers and responses.She defines an answer as the production of the information or exe-cution of the action requested by the speaker but a response ~s"tile rcspondent's complete informative and performs-tire reaction to the question which can include ... addi-tional information provided or actions performed thatare salient o this substitute for an answer.
"Our analysis of naturally oecurring dialogue indicates thathumans respond, rather than answer, once disparate models aredetected.
Ttmse responses often entail additional actions, includ-ing a negotiation dialogue to ascertain the cause of thediscrepancy and enable the models to be modified so that they areonce again in alignment.
A robust natural language interfacemust do the same, since the system must have an accurate modelof the information-seeker's plan in order for cooperative behaviorto resume.The appropriate response depends on the cause of thediscrepancies.
In the case of a knowledge-limited model, IP shouldattempt to understand IS's uttermme in terms of IP'8 limitedknowledge ~ld provide any pertinent helpful information, butinform IS of these limitations in order to avoid misleading IS byappearing to implicitly support his task-related plan.Consider again our exmnple of file transfer via Kermit,presented in Figure 2.
We assume that, in addition to a domain-dependent set of plans, IP's knowledge base contains a generaliza-tion hierarchy of actions and entities.Suppose that IP's knowledge base contains the plansTo: Have(<agent>:PERSON, <x>:BOOK)Action: Purchase(<agent>, <x>)To: Purchase(<agent>:PERSON, <x>:TEXTBOOK)Action: GoTo(<agant>, <p>:BOOKSTORE, <t>:TIME)where Sells(<p>, <x>)Between(<t>~ <tl>:TIME, <t2>:TIME)Opens(<p>~ <t l>)Closes(<p>, <t2>)IP can reason that IS's last query is relevant o a plan for pur-chasing a textbook at the bookstore.
This is simple plan inference31bookscoxmc light craft/hobbybooks books booksfiction non-fictioneducational-useitemseducational computerbooks suppliestextbooks technical non-technlcal disksbooks booksFigure 3.
Object Taxonomy for Kermit Exampleas embodied in our TRACK system \[Carberry 1983\].
However IPcannot connect purchasing a book with her model of IS.
So IPmay begin trying to expand on her knowledge.
Suppose that IP'staxonomy of objects is as shown in Figure 3 and that IP's domainknowledge includes the existence of many instances of <u>,  <v>,<w>, and <x> such thatSeils(UDEL-BOOKSTORE, <u>:NOVEL)Selis(UDEL-BOOKSTORE~ <v>:TECHBOOK)Sells(UDEL-BOOKSTORE, <w>:NONTECHBOOK)Sells(.UDEL- BOOKSTORE, <x>:TEXTBOOK)Novels are a subclass of light-books and~ technical-books, non-technical-books, and textbooks are subclasses of educational-books.
But educational-books are a subclass of educatlonal-use-items, as are floppy disks.
Thus IP can generalize textbooks toeducational-use-ltems, note that this class also contains disks, andthen hypothesize that perhaps IS thinks that the bookstore sellsfloppy disks~ since it sells other educational-use it ms.
This rea-soning might be represented by the ruleIf Clo.ss-I is a subclass of Class-2, and for many of theother subclasses of Class-2 there exist many members<y> such thatV(...,<y>) ,then one can hypothesize that perhaps there exists<z> such thatP(...,<z>:Class-1)This rule can be applied in the absence of contradictory domainknowledge.
Having thus hypothesized that perhapsSells(UDEL-BOOKSTORE, <z>:DISK)fromSells(UDEL-BOOKSTORE, <v>:TECHBOOK)Sells(UDEL-BOOKSTORE, <w>:NONTECHBOOK)Sells(UDEL-BOOKSTORE, <x>:TEXTBOOK)IP can hypothesize the higher-level goaLsPurchase(IS, <z>:DISK)Have(IS, <z>:DISK)the last of which is a component of IP's model of IS.Since IP has constructed a plan that may reasonably beascribed to IS, is relevant o the current focus of attention~ andabout which IP's knowledge is neutral, IP can hypothesize thatthe cause of the plan disparity may be that IS has more extensivcdomain knowledge.
IP can now respond to IS.
This reply shouldof course contain a direct answer to IS's posited question.
Butthis alone is insufficient.
In a cooperative information-seeklngdialogue, IS expects IP to assimilate the dialogue and relate utter-ances to IS's inferred underlying task in order to provide the mosthelpful information.
If IP limits herself to a direct response, ISmay infer that IP has related IS's current utterance to this taskand  that IP~, knowledge supports it --- that is, that IP alsobelieves IS can purchase a floppy disk at the bookstore.
Joshi'srevised maxim of quality \[Joehl 1983\] asserts that IP's responsemust block false inferences.
In addition, as a helpful participant,IP should include whatever evidence IP has for or against he pla~xcomponent proposed by IS.
An appropriate response wouhl be:"The University Bookstore is open until 4:30 PM.
ButI don't know whether it sells floppy disks.
However itdoes sell many other items of an educational nature, soit is perhaps a good place to try.
"The above example concerned a knowledge-limited modelcaused by IP's limited domain knowledge.
Other kinds of modelssuggest different reasoning and response strategies.
If IP hasfailed to nm~e the inferences IS assumed would be made, thensubsequent utter*races by IS may appear appropriate to a morespecific model than IP's current modeh Earlier, we referred tothis class as overly-generalized models.
In these cases, IP amyenter a clarification dialogue to ~certaln what IS intends.In other cases, such as when overly-specialized or erroneousmodels are detected, a negotiation dialogue must be initiated to"square away" \[Joshi 1983\] the modeis; otherwise, IS will lackconfidence in the responds provided by IP (and therefore shouldnot continue the dialogue), and IP will lack confidence in her abil-ity to provide useful replies (and therefore cannot continue as acooperative participant).
As with any negotiation, this is a two-way process:\[1\] IP may select portions of the context model that she feelsare suspect and justify them~ in an attempt o convince ISthat IS's plan needs adjustment, not IP's inferred model ofthat plan.\[2\] IP may formulate queries to IS in order to ascertain why thetask models diverge and where IP's model might be in error.The IRA example illustrates a negotiation dialogue.
In utterance\[6\], IP selects a suspect component of her context model and pro-vides justification for it.
IS's next utterance informs IP that theassumption on which this component was based is incorrect; IPthen notifies IS that IP recognizes the error and that her contextmodel has been repaired.
The information-seeking dialogue thenresumes .5.3.
MODEL l tECONSTRUCTIONOnce the cause of model disparity is identified, IP and ISmust a~ljust heir models to remove the disparities.
Once again,32this depends o~ the cause of the disagreement.
In the case of aknowledge-limited model, IP should hmorporate the componentsshe believes to be part of IS's plan structure into her contextmodel, noting however that her own knowledge oilers only liafitedsupport for thr.m.
In this way, IP's model reflects IS's, enables IPto understand (within her limited knowh!dge) how IS plazm toaccomplish is objectives, and permits IP to use this knowledge tounderstand subsequent utterances and provide helpful informa-tion.If IP's m(~lel is in error~ she must alter her context model, asdetermined through the negotiation dialogue.
She may also com-municate to IS the changes that she is making, so that IS canassure himself that the models now agree.
On the other hand, ifIS's model is in error, IP may inform IS of any information eee~sary for 1S to construct an appropriate plan and achieve his goals.g.4.
SUMMAI t?The argunmnts in the preceding sections are based on ananalysis of transcripts of hunm~l information-seeking dialoguesand indicate that au appropriate approach for hazldling the plandisparity problem entails four phases:\[1\] detection of disparate mc)dels\[2\] hypothesis for:marion as to the cause of the disparities\[3\] extended response, often including a negotiation dialogue toidentify the cause of the disparities\[4\] model modification, to "square away" the plm~ structures.Since this appre~mh is representative of that employed by humandialogue partlcipants, a natural language interface that pursuesthe s~nne strugegy will be viewed as acting naturally by its humanusers .O.
ENRICHED CONTEXT MODELThe knowledge acquired from the dialogue and how it wasused to constrt~ct he context model are important factors indetecting, responding to, and recovering from disparate models.l\[tumazl dialogue participants typically employ various teclmiquessuch as focusing strategies and default rules for understandinga~xd relating dialogue, but they appear to have greater confidencein some parts of the resultant model than others.
Naturallanguage systems mnst employ similar mechanisms in order to dothe kind of inferencing expected by humans and provide the mosthelpful responses.
We claim that the representation of theinferred plan must differentiate among its components accordingto the support which the system accords each component as acorrect and intended part of the inferred plan.
This view parallelsDoyle's Truth Maintenance System \[Doyle 1979\], in which atti-tudes are associated with reasons justifying them.We see font kinds of support for plan components:\[1\] whether the system has inferred the component directlyfrom what IS said.\[2\] whether the system has inferred the component on the basisof its own domain knowledge, which the system eamlot becerLain IS i~s aware of.\[3\] the kinds of k~mehanismu used to add each component to themodel, (for example, default rules that select one componentfrom among several possibilities, or heuristics that suggest ashift in f(~:us of attention), and the evidence for applyingthe mechar~ism.\[41 whether the system's domain knowledge supports, contrad-icts, or is :neutral regarding inclusion of the component aspart of a correct overall plan.The first three are importmlt factors in formulating ahypothesis regarding the source of disparity between the system'smodel and IS's plmL If the system believes that IS intends thesystem to recognize from IS's utterance that G is a component ofIS's plan, then the system can add G to its context model andhave the greatest faith that it really is a component of IS's plan.Therefore such components are unlikely sources of disparitybetween the system's context model and IS's plan.Components that the system adds to the context model onthe basis of its donmin knowledge will be strongly believed by thesystem to be part of IS's plan, bnt not as much as if IS haddirectly coatmunicated them.
Ttmse components resemble"keyhole recognition" rather thml "intended recognition" \[Sidner1985, 1983\].
Since IS amy not have intended to eonnnunieatethem, they are more likely r~ources of error tha~l componentswhich IS intended IP to recognize.Consider for example a student advisement system.
If onlyBA degrees have a foreign lar~guage r quirement, the query"What course must I take to satisfy the foreignlanguage requirement in French?
"may lead the system to infer that IS is pursuing a Bachelor ofArts degree.
If only BS degrees require a senior project, then asubsequent query such as"Ilow many credits of senior project are required?
"suggests plan disparity.
Either the second query is inappropriateto IS's overall goal \[Pollack 1986\] or the system's context model isalready in error.
Since the componentObtain-Degree(IS, BACHELOR-OF-ARTS)was inferred on the basis of the system's domain knowledge rathertitan directly from IS's utterance, it is suspect as the source ofer ror .The mechanisms u2~ed to add a component to the contextmodel affect IP's faith in that component as part of ISis overallplan.
Consider again the IRA example in Figure 1. in utteranceI4\], IP has applied the default assumption that IS was not coveredby a pension progrmn during the year in question (at that tim%rules on IRAs were different).
IS's next utterance xpresses con-fusion at IP's response, thereby cuing the possibility of plandisparity.
In utterance \[61, IP selects the component added to thecontext model via.
the default assumption as a possible source ofthe disparity, tells IS that it is part of IP's context model, andattempts to justify its inclusion.Analysis of naturally occurring dialogues uch as that in Fig-ure 1 indicate that humans use mechanisms such as defanlt infer-cnee rules and focusing heuristics to expand the context modeland provide a more detailed and tractable arena in which tounderstand and respond to subsequent utterances.
Naturallanguage systems must use similar mechanisms in order tocooperatively and naturally engage in dialogue with humans.IIowever these rules select from among multiple possibilities andtherefore produce components that are more likely sources oferror than components added as a result of IS's direct statementsor inferences made on the basis of the system's domainknowledge.The fourth kind of differentiation among components ---whether the system's domain knowledge supports, contradicts, oris neutral regarding inclusion of the component as part of acorrect overall plan - -  is important in recovering from disparateplans.
Even an expert system has limited domain knowledge.Furthermore, in a rapidly eh~mging world, knowledgeable usersmay have more accurate information about some aspects of thedomain than does the system.
For example, a student advisementsystem may not be altered intmediately upon changing the teacherof a course.
Thus we believe that the context model must allowfor inclusion of components suggested by the informatiomseeker,including whether the system's domain knowledge contradicts,supports, or is neutral regarding the component.33For example, upon determining that IS's domain knowledgemay exceed the system's in the Kermit dialogue, the systemshould expand its existing model to incorporate the acquiredknowledge about how IS believes floppy disks can be obtained.The plan components creatively constructed can be added to thesystem's model, but as components proposed by IS and not fullysupported by the system's knowledge.
In this manner, the systemcan assimilate new utterances that exceed or contradict i s limiteddomain knowledge and develop an expanded context model whichserves as "knowledge" that can be referred back to in the ensuingdialogue.7.
SUMMARYThis paper has addressed the problem of disparity betweenthe context model inferred by a natural anguage system and theplan under construction by an information-seeker.
We havepresented a four phase approach and have argued that handlingdisparate plans requires an enriched context model.
This modelmust permit the addition of components uggested by theinformation-seeker but not fully supported by the system'sdomain knowledge and must differentiate among its componentsaccording to the kind of support accorded each component as acorrect part of the information-seeker's overall plan.
We havefurther argued that support for a component should affect thesystem's hypothesis about the source of error once plan disparityis suggested.8.
ACKNOWLEDGEMENTSI want to thank Joe Brady, Kathy Cebulka, Dan Chester,Kathy McCoy, Martha Pollack, and Ralph Weiscbedel for theirmany helpful discussions and coxxmaents on this work, and DanChester and Kathy McCoy for their comments and suggestions onthis paper.9.
REFERENCESAllen, James F., "Analyzing Intention in Utterances", ArtificialIntelligence 15(3), 1980Carberry, Sandra, "Pragmatic Modeling in Information SystemInterfaces", Ph.D. Dissertation, Department of Computer Science,University of Delaware, 1985Carberry, Sandra, "Tracking User Goals in an Information-Seeking Environment", Proceedings of the National Conferenceon Artificial Intelligence, 1983Carberry, Sandra, "Using Inferred Knowledge to UnderstandPragmatically Ill-Formed Queries", to appear in CommunicationFailure in Dialogue, Ronan Reilly editor, North Holland, 1986Doyle, Jon, "A Truth Maintenance System", Artificial Intelligence12(3), 1979Grice, H. P., "Logic and Conversation", In Syntax and Semantics,Cole and Morgan, editors, Academic Press, 1975Grice, H. P., "Utterer's Meaning and Intentions", PhilosophicalReview 68, 1969Grice, H. P., "Meaning", Philosophical Review 56, 1957Grosz, Barbara, "Focusing and Description in Natural LanguageDialogues", in Elements of Discourse Understanding, Joshi, A.,Webber, B., and Sag, I., editors, Cambridge University Press,1981Grosz, Barbara, "The Representation a d Use of Focus in a Sys=tem for Understanding Dialogs", Proceedings of the InternationalJoint Conference on Artificial Intelligence, 1977Joshi, Aravind K., "Mutual Beliefs in Question-Answer Systems",Mutual Knowledge, Academic Press, 1983Kaplan, S. Jerroid, "Cooperative Responses from a PortableNatural Language Query System", Artificial Intelligence 19j 1982Litmus, Diane J. and Alien, James F., "A Plan Recognition Modelfor Clarification Subdialogues", Proceedings of the InternationalConference on Computational Linguistics, 1984McCoy, Kathleen F., "Generating Responses to Property Miscon-ceptions Using Perspective"~ to appear in Communication Failurein Dialogue, Ronan Reilly, Editor, 1986McKeown, Kathleen R., Te~t Generation, Cambridge UniversityPress, 1985Perrault, C. 1L and Allen, J. F., "A Plan-Based Analysis ofIndirect Speech Acts", American Journal of ComputationalLinguistics, 1980Pollack, Martha, "Inferring Domain Plans in Question-Answering", forthcoming Ph.D. Dissertation, University ofPennsylvania, 1986Pollack, Martha, "Some Requirements for a Model of the Plan-Inference Process in Conversation", to appear in CommunicationFailure in Dialogue, Ronan Reilly, editor, North Holland, 1986Pollack, Martha, Hirsehberg, Julia, and Webber, Bonnie, "UserParticipation in the Reasoning Processes of Expert Systems",Proceedings of the National Conference on Artificial Intelligence,1982Reiter, P~y, "On Closed World Data Bases", Logic and DataBases, Gallaire, It.
and Minker, J.~ editors, Plenum Pre~s, 1978Robinson, Ann E., "Determining Verb Phrase Referents in Dia-logs", American Journal of Computational Linguis$ics, 1981Sidner, Candace L., "Plan Parsing for Intended Response Recogonitlon in Discourse", Computational Intelligence 1(1), 1985Sidner, Candace L., "What the Speaker Means: The Recognitionof Speakers' Plans in Discourse", Computers and MathematicsWith Applications 9(1), 1983Sidner, Candace L., "Focusing for Interpretation of Pronouns",American Journal of Computational Linguistics, 1981Webber, Bonnie L., "Questions, Answers, and Responses:Interact\[ag with Knowledge Base Systems", to appear in OnKnowledge Base Management Systems, M. Brodie and J. Mylo~poulos, editors, Springer-Verlag, 198634
