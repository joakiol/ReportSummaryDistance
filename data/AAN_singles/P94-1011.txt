PRECISE N-GRAM PROBABILITIES FROMSTOCHASTIC CONTEXT-FREE GRAMMARSAndreas Stolcke and Jonathan SegalUniversity of California, BerkeleyandInternational Computer Science Institute1947 Center StreetBerkeley, CA 94704{stolcke, j segal}@icsi, berkeley, eduAbstractWe present an algorithm for computing n-gram probabil-ities from stochastic ontext-free grammars, a procedurethat can alleviate some of the standard problems associatedwith n-grams (estimation from sparse data, lack of linguis-tic structure, among others).
The method operates via thecomputation of substring expectations, which in turn is ac-complished by solving systems of linear equations derivedfrom the grammar.
The procedure is fully implemented andhas proved viable and useful in practice.INTRODUCTIONProbabilistic language modeling with n-gram grammars(particularly bigram and trigram) has proven extremely use-ful for such tasks as automated speech recognition, part-of-speech tagging, and word-sense disambiguation, and lead tosimple, efficient algorithms.
Unfortunately, working withthese grammars can be problematic for several reasons: theyhave large numbers of parameters, o reliable estimationrequires a very large training corpus and/or sophisticatedsmoothing techniques (Church and Gale, 1991); it is veryhard to directly model inguistic knowledge (and thus thesegrammars are practically incomprehensible to human inspec-tion); and the models are not easily extensible, i.e., if a newword is added to the vocabulary, none of the informationcontained in an existing n-gram will tell anything about hen-grams containing the new item.
Stochastic ontext-freegrammars (SCFGs), on the other hand, are not as suscep-tible to these problems: they have many fewer parameters(so can be reasonably trained with smaller corpora); theycapture linguistic generalizations, and are easily understoodand written, by linguists; and they can be extended straight-forwardly based on the underlying linguistic knowledge.In this paper, we present a technique for computing ann-gram grammar f om an existing SCFG--an attempt to getthe best of both worlds.
Besides developing the mathematicsinvolved in the computation, we also discuss efficiency andimplementation issues, and briefly report on our experienceconfirming its practical feasibility and utility.The technique of compiling higher-level grammat-ical models into lower-level ones has precedents:Zue et al (1991) report building a word-pair grammar frommore elaborate language models to achieve good coverage,by random generation of sentences.
In our own group,the current approach was predated by an alternative onethat essentially relied on approximating bigram probabili-ties through Monte-Carlo sampling from SCFGs.PRELIMINARIESAn n-gram grammar is a set of probabil-ities P(w,~lWlW2...wn_a), giving the probability that wnfollows a word string Wl w2.. .
wn-1, for each possible com-bination of the w's in the vocabulary of the language.
Sofor a 5000 word vocabulary, a bigram grammar would haveapproximately 5000 x 5000 = 25,000,000 free parameters,and a trigram grammar would have ~ 125,000,000,000.This is what we mean when we say n-gram grammars havemany parameters.A SCFG is a set of phrase-structure rules, annotated withprobabilities of choosing acertain production given the left-hand side nonterminal.
For example, if we have a simpleCFG, we can augment i with the probabilities specified:S --+ NPVP \[1.0\]NP --+ N \[0.4\]N P -+ Det N \[0.6\]VP --+ V \[0.8\]V P --+ V UP  \[0.2\]Det --~ the \[0.4\]Det --+ a \[0.6\]N ~ book \[1.0\]V --+ close \[0.3\]V ~ open \[0.7\]The language this grammar generates contains 5 words.Including markers for sentence beginning and end, a bigramgrammar would contain 6 x 6 probabilities, or 6 x 5 = 3074free parameters ( ince probabilities have to sum to one).
Atrigram grammar would come with (5 x 6 + 1) x 5 = 155parameters.
Yet, the above SCFG has only 10 probabilities,only 4 of which are free parameters.
The divergence betweenthese two types of models generally grows as the vocabularysize increases, although this depends on the productions inthe SCFG.The reason for this discrepancy, ofcourse, is that the struc-ture of the SCFG itself is a discrete (hyper-)parameter with alot of potential variation, but one that has been fixed before-hand.
The point is that such a structure is comprehensible byhumans, and can in many cases be constrained using priorknowledge, thereby reducing the estimation problem for theremaining probabilities.
The problem of estimating SCFGparameters from data is solved with standard techniques,usually by way of likelihood maximization and a variant ofthe Baum-Welch (EM) algorithm (Baker, 1979).
A tutorialintroduction to SCFGs and standard algorithms can be foundin Jelinek et al (1992).MOTIVAT IONThere are good arguments hat SCFGs are in principle not ad-equate probabilistic models for natural languages, due to theconditional independence assumptions they embody (Mager-man and Marcus, 1991; Jones and Eisner, 1992; Briscoe andCarroll, 1993).
Such shortcomings can be partly remediedby using SCFGs with very specific, semantically orientedcategories and rules (Jurafsky et al, 1994).
If the goal is touse n-grams nevertheless, then their their computation froma more constrained SCFG is still useful since the results canbe interpolated with raw n-gram estimates for smoothing.An experiment illustrating this approach is reported later inthe paper.On the other hand, even if vastly more sophisticated lan-guage models give better esults, r~-grams will most likelystill be important in applications uch as speech recogni-tion.
The standard speech decoding technique of frame-synchronous dynamic programming (Ney, 1984) is basedon a first-order Markov assumption, which is satisfied by bi-grams models (as well as by Hidden Markov Models), but notby more complex models incorporating non-local or higher-order constraints (including SCFGs).
A standard approach istherefore to use simple language models to generate a prelim-inary set of candidate hypotheses.
These hypotheses, e.g.,represented as word lattices or N-best lists (Schwartz andChow, 1990), are re-evaluated later using additional criteriathat can afford to be more costly due to the more constrainedoutcomes.
In this type of setting, the techniques developedin this paper can be used to compile probabilistic knowledgepresent in the more elaborate language models into n-gramestimates that improve the quality of the hypotheses gener-ated by the decoder.Finally, comparing directly estimated, reliable n-gramswith those compiled from other language models is a poten-tially useful method for evaluating the models in question.For the purpose of this paper, then, we assume that comput-ing n-grams from SCFGs is of either practical or theoreticalinterest and concentrate on the computational aspects of theproblem.It should be noted that there are alternative, unrelatedmethods for addressing the problem of the large parameterspace in n-gram models.
For example, Brown et al (1992)describe an approach based on grouping words into classes,thereby reducing the number of conditional probabilities inthe model.THE ALGORITHMNormal form for SCFGsA grammar is in Chomsky Normal Form (CNF) if everyproduction is of the form A ~ B C or A ~ terminal.Any CFG or SCFG can be converted into one in CNF whichgenerates exactly the same language, each of the sentenceswith exactly the same probability, and for which any parse inthe original grammar would be reconstructible from a parsein the CNF grammar.
In short, we can, without loss ofgenerality, assume that the SCFGs we are dealing with arein CNF.
In fact, our algorithm generalizes straightforwardlyto the more general Canonical Two-Form (Graham et al,1980) format, and in the case of bigrams (n =- 2) it can evenbe modified to work directly for arbitrary SCFGs.
Still, theCNF form is convenient, and to keep the exposition simplewe assume all SCFGs to be in CNF.Probabilities from expectationsThe first key insight towards a solution is that the n-gramprobabilities can be obtained from the associated expectedfrequencies for n-grams and (n - 1)-grams:c (wl .
.
.wn lL)P(w,dwlw2...w,~-a) = c(wx .
.
.wn- l lL)  (1)where c(wlL ) stands for the expected count of occurrencesof the substring w in a sentence of L.1Proof Write the expectation for n-grams recursively interms of those of order n - 1 and the conditional n-gramprobabilities:C(Wl...Wr~\[L) ~---C(Wl...W~_llL)P(w~lw lw2 .
.
.wr~_ l ) .So if we can compute c(wlG) for all substrings w oflengths n and n - 1 for a SCFG G, we immediately have ann-gram grammar for the language generated by G.Computing expectationsOur goal now is to compute the substring expectations fora given grammar.
Formalisms uch as SCFGs that have arecursive rule structure suggest a divide-and-conquer algo-rithrn that follows the recursive structure of the grammar, zWe generalize the problem by considering c(wIX), theexpected number of (possibly overlapping) occurrences of1The only counts appearing in this paper are expectations, sobe will not be using special notation to make a distinction betweenobserved and expected values.2A similar, even simpler approach applies to probabilistic finitestate (i.e., Hidden Markov) models.75XY ZW(a)X XY Z Y Z(b) (c)Figure 1: Three ways of generating a substring w from a nonterminal X.113 .~- 2131 .
.
.
W n in strings generated by an arbitrary nonter-minal X.
The special case c(wIS) = c(wlL) is the solutionsought, where S is the start symbol for the grammar.Now consider all possible ways that nonterminal X cangenerate string w = wl ... wn as a substring, denoted byX ::~ ... wl ?
.. wn .... and the associated probabilities.
Foreach production of X we have to distinguish two main cases,assuming the grammar is in CNF.
If the string in question isof length I, w = wl, and if X happens to have a productionX --~ Wl, then that production adds exactly P(X  --~ wt) tothe expectation c(w IX).If X has non-terminal productions, ay, X ~ YZ thenw might also be generated by recursive xpansion of theright-hand side.
Here, for each production, there are threesubcases.
(a) First, Y can by itself generate the complete w (seeFigure l(a)).
(b) Likewise, Z itself can generate w (Figure l(b)).
(c) Finally, Y could generate wl .. .
wj as a suffix (Y ~Rwl .
.
.w j )  and Z, Wj+l.
.
.wn as a prefix (Z ~Lwj+l ... w,O, thereby resulting in a single occurrenceof w (Figure l(c)).
3Each of these cases will have an expectation for generatingwl ... wn as a substring, and the total expectation c(w}X)will be the sum of these partial expectations.
The totalexpectations for the first two cases (that of the substringbeing completely generated by Y or Z) are given recursively:c(wlY) and c(wlY ) respectively.
The expectation for thethird case isn--1E P (Y  :~zR w l .
.
.
w j )P(Z  :~'L wj+, .
.
.
W,), (2)j=lwhere one has to sum over all possible split points j of thestring w.3We use the notation X =~R c~ to denote that non-terminal Xgenerates the string c~ as a suffix, and X :~z c~ to denote that Xgenerates c~ as a prefix.
Thus P(X :~t.
~) and P(X ::~n o~) arethe probabilities a sociated with those vents.To compute the total expectation c(wlX), then, we haveto sum over all these choices: the production used (weightedby the rule probabilities), and for each nonterminal rule thethree cases above.
This givesc(wlx)  = P(x  -~ w)+ E P (X~YZ)X-+ Y Z ( c(w\[Y) + ~(~lz)n--1+ P(Y :;Rj=l\P(Z  ::~L wj+, .
.
,  wn))J(3)In the important special case of bigrams, this summationsimplifies quite a bit, since the terminal productions are ruledout and splitting into prefix and suffix allows but one possi-bility:c(wlw21X) = E P (X  ~ YZ)X--~ Y ZC(WlW2IY) q- C(WlW2IZ)\+P(Y  ---~t~ w, )P(Z  :~L w2))(4)For unigrams equation (3) simplifies even more:C(WllX) = P(X --+ wl)+ ~_, P (X -+YZ) (c (w ' IY )+c(w1IZ) )X--+YZ(5)We now have a recursive specification fthe quantitiesc(wlX ) we need to compute.
Alas, the recursion does notnecessarily bottom out, since the c(wlY) and c(wlZ) quanti-ties on the right side of equation (3) may depend themselveson c(wlX).
Fortunately, the recurrence is linear, so for eachstring w, we can find the solution by solving the linear systemformed by all equations of type (3).
Notice there are exactly76as many equations as variables, equal to the number of non-terminals in the grammar.
The solution of these systems isfurther discussed below.Computing prefix and suffix probabilitiesThe only substantial problem left at this point is the com-putation of the constants in equation (3).
These are derivedfrom the rule probabilities P(X ~ w) and P(X --+ YZ),as well as the prefix/suffix generation probabilities P(Y =~Rwl ... wj) and P(Z =~z wj+l ... w,~).The computation ofprefix probabilities for SCFGs is gen-erally useful for applications, and has been solved withthe LRI algorithm (Jelinek and Lafferty, 1991).
Recently,Stolcke (1993) has shown how to perform this computationefficiently for sparsely parameterized SCFGs using a proba-bilistic version of Earley's parser (Earley, 1970).
Computingsuffix probabilities i obviously a symmetrical task; for ex-ample, one could create a 'mirrored' SCFG (reversing theorder of right-hand side symbols in all productions) and thenrun any prefix probability computation on that mirror gram-mar.Note that in the case of bigrams, only a particularly simpleform of prefix/suffix probabilities are required, namely, the'left-corner' and 'right-corner' probabilities, P(X ~z  wl)and P(Y ~ R w2), which can each be obtained from a singlematrix inversion (Jelinek and Lafferty, 1991).It should be mentioned that there are some technical con-ditions that have to be met for a SCFG to be well-definedand consistent (Booth and Thompson, 1973).
These condi-tion are also sufficient to guarantee that the linear equationsgiven by (3) have positive probabilities as solutions.
Thedetails of this are discussed in the Appendix.Finally, it is interesting to compare the relative ase withwhich one can solve the substring expectation problem to theseemingly similar problem of finding substringprobabilities:the probability that X generates (one or more instances of)w. The latter problem is studied by Corazza et al (1991),and shown to lead to a non-linear system of equations.
Thecrucial difference here is that expectations are additive withrespect to the cases in Figure 1, whereas the correspondingprobabilities are not, since the three cases can occur simul-taneously.EFF IC IENCY AND COMPLEXITY  ISSUESSummarizing from the previous ection, we can computeany n-gram probability by solving two linear systems ofequations of the form (3), one with w being the n-gram itselfand one for the (n - 1)-gram prefix wl ... wn-1.
The lattercomputation can be shared among all n-grams with the sameprefix, so that essentially one system needs to be solved foreach n-gram we are interested in.
The good news here is thatthe work required is linear in the number of n-grams, andcorrespondingly limited if one needs probabilities for onlya subset of the possible n-grams.
For example, one couldcompute these probabilities on demand and cache the results.Let us examine these systems of equations one more time.Each can be written in matrix notation in the form(I - A )c  = b (6)where I is the identity matrix, A = (axu) is a coefficientmatrix, b = (bx) is the right-hand side vector, and c rep-resents the vector of unknowns, c(wlX ).
All of these areindexed by nonterminals X, U.We getaxu = Z P(X-+ YZ)(6(Y,U)+6(Z,U))(7)X--+ YZbx = P(X ~ w)+ Z P(X--4 YZ)X--+ YZn-1~ P(Y :~R wl ...wj)j=lP(Z ~L Wj+l , .
.
'OJn) (8)where 6(X, Y) = 1 i fX  = Y, and 0 otherwise.
Theexpression I - A arises from bringing the variables c(wlY )and c(wlZ ) to the other side in equation (3) in order to collectthe coefficients.We can see that all dependencies onthe particular bigram,w, are in the right-hand side vector b, while the coefficientmatrix I - A depends only on the grammar.
This, togetherwith the standard method of LU decomposition (see, e.g.,Press et al (1988)) enables us to solve for each bigram intime O(N2), rather than the standard O(N 3) for a full sys-tem (N being the number of nonterminals/variables).
TheLU decomposition itself is cubic, but is incurred only once.The full computation is therefore dominated by the quadraticeffort of solving the system for each n-gram.
Furthermore,the quadratic ost is a worst-case figure that would be in-curred only if the grammar contained every possible rule;empirically we have found this computation tobe linear in thenumber of nonterminals, for grammars that are sparse, i.e.,where each nonterminal makes reference only to a boundednumber of other nonterminals.SUMMARYListed below are the steps of the complete computation.
Forconcreteness we give the version specific to bigrams (n = 2).1.
Compute the prefix (left-corner) and suffix (right-corner) probabilities for each (nonterminal,word) pair.2.
Compute the coefficient matrix and right-hand sides forthe systems of linear equations, as per equations (4)and (5).3.
LU decompose the coefficient matrix.4.
Compute the unigram expectations for each word in thegrammar, by solving the LU system for the unigramright-hand sides computed in step 2.5.
Compute the bigram expectations for each word pair bysolving the LU system for the bigram right-hand sidescomputed in step 2.77.
Compute ach bigram probability P (w2 \]wl ), by divid-ing the bigram expectation c(wlw2\[S) by the unigramexpectation C(Wl IS).EXPERIMENTSThe algorithm described here has been implemented, andis being used to generate bigrams for a speech recognizerthat is part of the BeRP spoken-language system (Jurafskyet al, 1994).
An early prototype of BeRP was used in anexperiment to assess the benefit of using bigram probabili-ties obtained through SCFGs versus estimating them directlyfrom the available training corpus.
4 The system's domain areinquiries about restaurants in the city of Berkeley.
The train-ing corpus used had only 2500 sentences, with an averagelength of about 4.8 words/sentence.Our experiments made use of a context-free grammarhand-written for the BeRP domain.
With 1200 rules anda vocabulary of 1 I00 words, this grammar was able to parse60% of the training corpus.
Computing the bigram proba-bilities from this SCFG takes about 24 hours on a SPARC-station 2-class machine.
5In experiment 1, the recognizer used bigrams that wereestimated irectly from the training corpus, without anysmoothing, resulting in a word error rate of 35.1%.
In ex-periment 2, a different set of bigram probabilities was used,computed from the context-free grammar, whose probabil-ities had previously been estimated from the same trainingcorpus, using standard EM techniques.
This resulted in aword error rate of 35.3%.
This may seem surprisingly goodgiven the low coverage of the underlying CFGs, but noticethat the conversion i to bigrams is bound to result in a lessconstraining language model, effectively increasing cover-age.Finally, in experiment 3, the bigrams generated from theSCFG were augmented by those from the raw training data,in a proportion of 200,000 : 2500.
We have not attempted tooptimize this mixture proportion, e.g., by deleted interpola-tion (Jelinek and Mercer, 1980).
6 With the bigram estimatesthus obtained, the word error rate dropped to 33.5%.
(Allerror rates were measured on a separate t st corpus.
)The experiment therefore supports our earlier argumentthat more sophisticated language models, even if far fromperfect, can improve n-gram estimates obtained irectlyfrom sample data.4Corpus and grammar sizes, as well as the recognition per-formance figures reported here are not up-to-date with respect tothe latest version of BeRP.
For ACL-94 we expect to have revisedresults available that reflect he current performance ofthe system.5Unlike the rest of BeRP, this computation is implemented inLisp/CLOS and could be speeded up considerably if necessary.6This proportion comes about because in the original system,predating the method escribed in this paper, bigrams had to beestimated from the SCFG by random sampling.
Generating 200,000sentence samples was found to give good converging estimates forthe bigrams.
The bigrams from the raw training sentences were thensimply added to the randomly generated ones.
We later verified thatthe bigrams estimated from the SCFG were indeed identical to theones computed directly using the method escribed here.CONCLUSIONSWe.
have described an algorithm to compute in closed formthe distribution of n-grams for a probabilistic languagegiven by a stochastic ontext-free grammar.
Our methodis based on computing substring expectations, which can beexpressed as systems of linear equations derived from thegrammar.
The algorithm was used successfully and foundto be practical in dealing with context-free grammars andbigram models for a medium-scale speech recognition task,where it helped to improve bigram estimates obtained fromrelatively small amounts of data.Deriving n-gram probabilities from more sophisticatedlanguage models appears to be a generally useful techniquewhich can both improve upon direct estimation of n-grams,and allow available higher-level linguistic knowledge to beeffectively integrated into the speech decoding task.ACKNOWLEDGMENTSDan Jurafsky wrote the BeRP grammar, carried out the recog-nition experiments, and was generally indispensable.
SteveOmohundro planted the seed for our n-gram algorithm dur-ing lunch at the California Dream Caf6 by suggesting sub-string expectations a an interesting computational linguis-tics problem.
Thanks also to Jerry Feldman and LokendraShastri for improving the presentation with their comments.This research as been supported by ICSI and ARPA con-tract #N0000 1493 C0249.CThis leads toAPPENDIX: CONSISTENCY OF SCFGSBlindly applying the n-gram algorithm (and many others)to a SCFG with arbitrary probabilities can lead to surprisingresults.
Consider the following simple grammarS-~ z Iv\]S ---r SS  \ [q= l -p \ ]  (9)What is the expected frequency of unigram x?
Using theabbreviation c = c(X\]S) and equation 5, we see thatP(S  --4 z) + P(S  ~ SS)(c + c)p + 2qeP - P (10)c- -  1 -2q  2p-  1Now, for p = 0.5 this becomes infinity, and for probabilitiesp < 0.5, the solution is negative!
This is a rather strikingmanifestation of the failure of this grammar, for p < 0.5,to be consistent.
A grammar is said to be inconsistent ifthe underlying stochastic derivation process has non-zeroprobability of not terminating (Booth and Thompson, 1973).The expected length of the generated strings hould thereforebe infinite in this case.Fortunately, Booth and Thompson derive a criterion forchecking the consistency of a SCFG: Find the first-order ex-pectancy matrix E = (exy),  where exy  is the expectednumber of occurrences of nonterminal Y in a one-step ex-pansion of nonterminal X, and make sure its powers E k78converge to 0 as k ~ oe.
If so, the grammar is consistent,otherwise it is not\]For the grammar in (9), E is the 1 x 1 matrix (2q).
Thuswe can confirm our earlier observation by noting that (2q) kconverges to 0 iff q < 0.5, or p > 0.5.Now, it so happens that E is identical to the matrix A thatoccurs in the linear equations (6) for the n-gram computation.The actual coefficient matrix is I - A, and its inverse, if itexists, can be written as the geometric sum( I -A )  -~ = I+A+A2+A 3 +.
.
.This series converges precisely if A k converges to 0.
Wehave thus shown that the existence of a solution for the n-gram problem is equivalent tothe consistency ofthe grammarin question.
Furthermore, the solution vector c = (I -A ) - lb  will always consist of non-negative numbers: it isthe sum and product of the non-negative values given byequations (7) and (8).REFERENCESJames K. Baker.
1979.
Trainable grammars for speechrecognition.
In Jared J. Wolf and Dennis H. Klatt, editors,Speech Communication Papers for the 97th Meeting ofthe Acoustical Society of America, pages 547-550, MIT,Cambridge, Mass.Taylor L. Booth and Richard A. Thompson.
1973.
Ap-plying probability measures to abstract languages.
IEEETransactions on Computers, C-22(5):442--450.Ted Briscoe and John Carroll.
1993.
Generalized prob-abilistic LR parsing of natural anguage (corpora) withunification-based grammars.
Computational Linguistics,19(1):25-59.Peter E Brown, Vincent J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-basedn-gram models of natural language.
Computational Lin-guistics, 18(4):467--479.Kenneth W. Church and William A. Gale.
1991.
A compar-ison of the enhanced Good-Turing and deleted estimationmethods for estimating probabilities of English bigrams.Computer Speech and Language, 5:19-54.Anna Corazza, Renato De Mori, Roberto Gretter, and Gior-gio Satta.
1991.
Computation of probabilities for anisland-driven parser.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 13(9):936-950.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 6(8):451-455.Susan L. Graham, Michael A. Harrison, and Walter L.Ruzzo.
1980.
An improved context-freerecognizer.
ACMTransactions on Programming Languages and Systems,2(3):415-462.7A further version of this criterion is to check the magnitude ofthe largest of E's eigenvalues (its spectral radius).
If that value is> 1, the grammar is inconsistent; if < 1, it is consistent.Frederick Jelinek and John D. Lafferty.
1991.
Computa-tion of the probability of initial substring eneration bystochastic ontext-free grammars.
Computational Lin-guistics, 17(3):315-323.Frederick Jelinek and Robert L. Mercer.
1980.
Interpo-lated estimation of Markov source parameters from sparsedata.
In Proceedings Workshop on Pattern Recognition inPractice, pages 381-397, Amsterdam.Frederick Jelinek, John D. Lafferty, and Robert L. Mer-cer.
1992.
Basic methods of probabilistic ontext freegrammars.
In Pietro Laface and Renato De Mori, editors,Speech Recognition and Understanding.
Recent Advances,Trends, and Applications, volume F75 of NATO AdvancedSciences Institutes Series, pages 345-360.
Springer Ver-lag, Berlin.
Proceedings of the NATO Advanced StudyInstitute, Cetraro, Italy, July 1990.Mark A. Jones and Jason M. Eisner.
1992.
A probabilisticparser applied to software testing documents.
InProceed-ings of the 8th National Conference on Artificial Intelli-gence, pages 332-328, San Jose, CA.
AAAI Press.Daniel Jurafsky, Chuck Wooters, Gary Tajchman, JonathanSegal, Andreas Stolcke, and Nelson Morgan.
1994.
In-tegrating rammatical, phonological, and dialect/accentinformation with a speech recognizer in the BerkeleyRestaurant Project.
In Paul McKevitt, editor, AAAI Work-shop on the Integration of Natural Language and SpeechProcessing, Seattle, WA.
To appear.David M. Magerman and Mitchell P. Marcus.
1991.
Pearl:A probabilistic hart parser.
In Proceedings of the 6thConference of the European Chapter of the Associationfor Computational Linguistics, Berlin, Germany.Hermann Ney.
1984.
The use of a one-stage dynamicprogramming algorithm for connected word recognition.IEEE Transactions on Acoustics, Speech, and Signal Pro-cessing, 32(2):263-271.William H. Press, Brian P. Flannery, Saul A. Teukolsky, andWilliam T. Vetterling.
1988.
Numerical Recipes in C: TheArt of Scientific Computing.
Cambridge University Press,Cambridge.Richard Schwartz and Yen-Lu Chow.
1990.
The N-bestalgorithm: An efficient and exact procedure for finding then most likely sentence hypotheses.
In Proceedings IEEEConference on Acoustics, Speech and Signal Processing,volume 1, pages 81-84, Albuquerque, NM.Andreas Stolcke.
1993.
An efficient probabilistic ontext-free parsing algorithm that computes prefix probabilities.Technical Report TR-93-065, International Computer Sci-ence Institute, Berkeley, CA.
To appear in ComputationalLinguistics.Victor Zue, James Glass, David Goodine, Hong Leung,Michael Phillips, Joseph Polifroni, and Stephanie Sen-eft.
1991.
Integration of speech recognition and natu-ral language processing in the MIT Voyager system.
InProceedings IEEE Conference on Acoustics, Speech andSignal Processing, volume 1, pages 713-716, Toronto.79
