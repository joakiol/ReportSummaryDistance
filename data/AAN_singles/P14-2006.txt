Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30?35,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsScoring Coreference Partitions of Predicted Mentions:A Reference ImplementationSameer Pradhan1, Xiaoqiang Luo2, Marta Recasens3,Eduard Hovy4, Vincent Ng5and Michael Strube61Harvard Medical School, Boston, MA,2Google Inc., New York, NY3Google Inc., Mountain View, CA,4Carnegie Mellon University, Pittsburgh, PA5HLTRI, University of Texas at Dallas, Richardson, TX,6HITS, Heidelberg, Germanysameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.orgAbstractThe definitions of two coreference scoringmetrics?B3and CEAF?are underspeci-fied with respect to predicted, as opposedto key (or gold) mentions.
Several varia-tions have been proposed that manipulateeither, or both, the key and predicted men-tions in order to get a one-to-one mapping.On the other hand, the metric BLANC was,until recently, limited to scoring partitionsof key mentions.
In this paper, we (i) ar-gue that mention manipulation for scoringpredicted mentions is unnecessary, and po-tentially harmful as it could produce unin-tuitive results; (ii) illustrate the applicationof all these measures to scoring predictedmentions; (iii) make available an open-source, thoroughly-tested reference imple-mentation of the main coreference eval-uation measures; and (iv) rescore the re-sults of the CoNLL-2011/2012 shared tasksystems with this implementation.
Thiswill help the community accurately mea-sure and compare new end-to-end corefer-ence resolution algorithms.1 IntroductionCoreference resolution is a key task in naturallanguage processing (Jurafsky and Martin, 2008)aiming to detect the referential expressions (men-tions) in a text that point to the same entity.Roughly over the past two decades, research incoreference (for the English language) had beenplagued by individually crafted evaluations basedon two central corpora?MUC (Hirschman andChinchor, 1997; Chinchor and Sundheim, 2003;Chinchor, 2001) and ACE (Doddington et al,2004).
Experimental parameters ranged from us-ing perfect (gold, or key) mentions as input forpurely testing the quality of the entity linking al-gorithm, to an end-to-end evaluation where pre-dicted mentions are used.
Given the range ofevaluation parameters and disparity between theannotation standards for the two corpora, it wasvery hard to grasp the state of the art for thetask of coreference.
This has been expounded inStoyanov et al (2009).
The activity in this sub-field of NLP can be gauged by: (i) the contin-ual addition of corpora manually annotated forcoreference?The OntoNotes corpus (Pradhan etal., 2007; Weischedel et al, 2011) in the generaldomain, as well as the i2b2 (Uzuner et al, 2012)and THYME (Styler et al, 2014) corpora in theclinical domain would be a few examples of suchemerging corpora; and (ii) ongoing proposals forrefining the existing metrics to make them moreinformative (Holen, 2013; Chen and Ng, 2013).The CoNLL-2011/2012 shared tasks on corefer-ence resolution using the OntoNotes corpus (Prad-han et al, 2011; Pradhan et al, 2012) were anattempt to standardize the evaluation settings byproviding a benchmark annotated corpus, scorer,and state-of-the-art system results that would al-low future systems to compare against them.
Fol-lowing the timely emphasis on end-to-end evalu-ation, the official track used predicted mentionsand measured performance using five coreferencemeasures: MUC (Vilain et al, 1995), B3(Baggaand Baldwin, 1998), CEAFe(Luo, 2005), CEAFm(Luo, 2005), and BLANC (Recasens and Hovy,2011).
The arithmetic mean of the first three wasthe task?s final score.An unfortunate setback to these evaluations hadits root in three issues: (i) the multiple variationsof two of the scoring metrics?B3and CEAF?used by the community to handle predicted men-tions; (ii) a buggy implementation of the Cai andStrube (2010) proposal that tried to reconcile thesevariations; and (iii) the erroneous computation of30the BLANC metric for partitions of predicted men-tions.
Different interpretations as to how to com-pute B3and CEAF scores for coreference systemswhen predicted mentions do not perfectly alignwith key mentions?which is usually the case?led to variations of these metrics that manipulatethe gold standard and system output in order toget a one-to-one mention mapping (Stoyanov etal., 2009; Cai and Strube, 2010).
Some of thesevariations arguably produce rather unintuitive re-sults, while others are not faithful to the originalmeasures.In this paper, we address the issues in scor-ing coreference partitions of predicted mentions.Specifically, we justify our decision to go backto the original scoring algorithms by arguing thatmanipulation of key or predicted mentions is un-necessary and could in fact produce unintuitive re-sults.
We demonstrate the use of our recent ex-tension of BLANC that can seamlessly handle pre-dicted mentions (Luo et al, 2014).
We make avail-able an open-source, thoroughly-tested referenceimplementation of the main coreference evalua-tion measures that do not involve mention manip-ulation and is faithful to the original intentions ofthe proposers of these metrics.
We republish theCoNLL-2011/2012 results based on this scorer, sothat future systems can use it for evaluation andhave the CoNLL results available for comparison.The rest of the paper is organized as follows.Section 2 provides an overview of the variationsof the existing measures.
We present our newlyupdated coreference scoring package in Section 3together with the rescored CoNLL-2011/2012 out-puts.
Section 4 walks through a scoring examplefor all the measures, and we conclude in Section 5.2 Variations of Scoring MeasuresTwo commonly used coreference scoring metrics?B3and CEAF?are underspecified in their ap-plication for scoring predicted, as opposed to keymentions.
The examples in the papers describingthese metrics assume perfect mentions where pre-dicted mentions are the same set of mentions askey mentions.
The lack of accompanying refer-ence implementation for these metrics by its pro-posers made it harder to fill the gaps in the speci-fication.
Subsequently, different interpretations ofhow one can evaluate coreference systems whenpredicted mentions do not perfectly align with keymentions led to variations of these metrics that ma-nipulate the gold and/or predicted mentions (Stoy-anov et al, 2009; Cai and Strube, 2010).
All thesevariations attempted to generate a one-to-one map-ping between the key and predicted mentions, as-suming that the original measures cannot be ap-plied to predicted mentions.
Below we first pro-vide an overview of these variations and then dis-cuss the unnecessity of this assumption.Coining the term twinless mentions for thosementions that are either spurious or missing fromthe predicted mention set, Stoyanov et al (2009)proposed two variations to B3?
B3alland B30?tohandle them.
In the first variation, all predictedtwinless mentions are retained, whereas the lat-ter discards them and penalizes recall for twin-less predicted mentions.
Rahman and Ng (2009)proposed another variation by removing ?all andonly those twinless system mentions that are sin-gletons before applying B3and CEAF.?
Follow-ing upon this line of research, Cai and Strube(2010) proposed a unified solution for both B3andCEAFm, leaving the question of handling CEAFeas future work because ?it produces unintuitiveresults.?
The essence of their solution involvesmanipulating twinless key and predicted mentionsby adding them either from the predicted parti-tion to the key partition or vice versa, depend-ing on whether one is computing precision or re-call.
The Cai and Strube (2010) variation was usedby the CoNLL-2011/2012 shared tasks on corefer-ence resolution using the OntoNotes corpus, andby the i2b2 2011 shared task on coreference res-olution using an assortment of clinical notes cor-pora (Uzuner et al, 2012).1It was later identifiedby Recasens et al (2013) that there was a bug inthe implementation of this variation in the scorerused for the CoNLL-2011/2012 tasks.
We havenot tested the correctness of this variation in thescoring package used for the i2b2 shared task.However, it turns out that the CEAF metric (Luo,2005) was always intended to work seamlessly onpredicted mentions, and so has been the case withthe B3metric.2In a latter paper, Rahman and Ng(2011) correctly state that ?CEAF can compare par-titions with twinless mentions without any modifi-cation.?
We will look at this further in Section 4.3.We argue that manipulations of key and re-sponse mentions/entities, as is done in the exist-ing B3variations, not only confound the evalu-ation process, but are also subject to abuse andcan seriously jeopardize the fidelity of the evalu-1Personal communication with Andreea Bodnari, andcontents of the i2b2 scorer code.2Personal communication with Breck Baldwin.31ation.
Given space constraints we use an exam-ple worked out in Cai and Strube (2010).
Letthe key contain an entity with mentions {a, b, c}and the prediction contain an entity with mentions{a, b, d}.
As detailed in Cai and Strube (2010,p.
29-30, Tables 1?3), B30assigns a perfect pre-cision of 1.00 which is unintuitive as the systemhas wrongly predicted a mention d as belonging tothe entity.
For the same prediction, B3allassigns aprecision of 0.556.
But, if the prediction containstwo entities {a, b, d} and {c} (i.e., the mention cis added as a spurious singleton), then B3allpreci-sion increases to 0.667 which is counter-intuitiveas it does not penalize the fact that c is erroneouslyplaced in its own entity.
The version illustrated inSection 4.2, which is devoid of any mention ma-nipulations, gives a precision of 0.444 in the firstscenario and the precision drops to 0.333 in thesecond scenario with the addition of a spurioussingleton entity {c}.
This is a more intuitive be-havior.Contrary to both B3and CEAF, the BLANC mea-sure (Recasens and Hovy, 2011) was never de-signed to handle predicted mentions.
However, theimplementation used for the SemEval-2010 sharedtask as well as the one for the CoNLL-2011/2012shared tasks accepted predicted mentions as input,producing undefined results.
In Luo et al (2014)we have extended the BLANC metric to deal withpredicted mentions3 Reference ImplementationGiven the potential unintuitive outcomes of men-tion manipulation and the misunderstanding thatthe original measures could not handle twinlesspredicted mentions (Section 2), we redesigned theCoNLL scorer.
The new implementation:?
is faithful to the original measures;?
removes any prior mention manipulation,which might depend on specific annotationguidelines among other problems;?
has been thoroughly tested to ensure that itgives the expected results according to theoriginal papers, and all test cases are includedas part of the release;?
is free of the reported bugs that the CoNLLscorer (v4) suffered (Recasens et al, 2013);?
includes the extension of BLANC to handlepredicted mentions (Luo et al, 2014).This is the open source scoring package3thatwe present as a reference implementation for the3http://code.google.com/p/reference-coreference-scorers/SYSTEM MD MUC B3CEAF BLANC CONLLm e AVERAGEF1F11F21F1F31CoNLL-2011; Englishlee 70.7 59.6 48.9 53.0 46.1 48.8 51.5sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0song 67.3 60.0 41.4 41.0 33.1 30.9 44.8sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5CoNLL-2012; Englishfernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6li 59.9 50.8 32.3 36.3 25.2 31.9 36.1CoNLL-2012; Chinesechen 71.6 62.2 55.7 60.0 55.0 54.1 57.6yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5li 51.5 44.7 31.5 36.7 25.3 30.4 33.8chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0CoNLL-2012; Arabicfernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2li 29.7 18.1 13.1 21.0 17.3 8.4 16.2Table 1: Performance on the official, closed trackin percentages using all predicted information forthe CoNLL-2011 and 2012 shared tasks.community to use.
It is written in perl and stemsfrom the scorer that was initially used for theSemEval-2010 shared task (Recasens et al, 2010)and later modified for the CoNLL-2011/2012shared tasks.4Partitioning detected mentions into entities (orequivalence classes) typically comprises two dis-tinct tasks: (i) mention detection; and (ii) coref-erence resolution.
A typical two-step coreferencealgorithm uses mentions generated by the best4We would like to thank Emili Sapena for writing the firstversion of the scoring package.32a     bcde fgha     bcdehi if    g f    gh icda     bSolid: KeyDashed: Response Solid: KeyDashed: partition wrt Response Solid: Partition wrt KeyDashed: ResponseFigure 1: Example key and response entities alongwith the partitions for computing the MUC score.possible mention detection algorithm as input tothe coreference algorithm.
Therefore, ideally onewould want to score the two steps independentlyof each other.
A peculiarity of the OntoNotescorpus is that singleton referential mentions arenot annotated, thereby preventing the computationof a mention detection score independently of thecoreference resolution score.
In corpora where allreferential mentions (including singletons) are an-notated, the mention detection score generated bythis implementation is independent of the corefer-ence resolution score.We used this reference implementation torescore the CoNLL-2011/2012 system outputs forthe official task to enable future comparisons withthese benchmarks.
The new CoNLL-2011/2012results are in Table 1.
We found that the over-all system ranking remained largely unchanged forboth shared tasks, except for some of the lowerranking systems that changed one or two places.However, there was a considerable drop in themagnitude of all B3scores owing to the combi-nation of two things: (i) mention manipulation, asproposed by Cai and Strube (2010), adds single-tons to account for twinless mentions; and (ii) theB3metric allows an entity to be used more thanonce as pointed out by Luo (2005).
This resultedin a drop in the CoNLL averages (B3is one of thethree measures that make the average).4 An Illustrative ExampleThis section walks through the process of com-puting each of the commonly used metrics foran example where the set of predicted mentionshas some missing key mentions and some spu-rious mentions.
While the mathematical formu-lae for these metrics can be found in the originalpapers (Vilain et al, 1995; Bagga and Baldwin,1998; Luo, 2005), many misunderstandings dis-cussed in Section 2 are due to the fact that thesepapers lack an example showing how a metric iscomputed on predicted mentions.
A concrete ex-ample goes a long way to prevent similar misun-derstandings in the future.
The example is adaptedfrom Vilain et al (1995) with some slight modifi-cations so that the total number of mentions in thekey is different from the number of mentions inthe prediction.
The key (K) contains two entitieswith mentions {a, b, c} and {d, e, f, g} and the re-sponse (R) contains three entities with mentions{a, b}; {c, d} and {f, g, h, i}:K =K1?
??
?
{a, b, c}K2?
??
?
{d, e, f, g} (1)R =R1?
??
?
{a, b}R2?
??
?
{c, d}R3?
??
?
{f, g, h, i}.
(2)Mention e is missing from the response, and men-tions h and i are spurious in the response.
The fol-lowing sections use R to denote recall and P forprecision.4.1 MUCThe main step in the MUC scoring is creating thepartitions with respect to the key and response re-spectively, as shown in Figure 1.
Once we havethe partitions, then we compute the MUC score by:R =?Nki=1(|Ki| ?
|p(Ki)|)?Nki=1(|Ki| ?
1)=(3?
2) + (4?
3)(3?
1) + (4?
1)= 0.40P =?Nri=1(|Ri| ?
|p?
(Ri)|)?Nri=1(|Ri| ?
1)=(2?
1) + (2?
2) + (4?
3)(2?
1) + (2?
1) + (4?
1)= 0.40,where Kiis the ithkey entity and p(Ki) is theset of partitions created by intersecting Kiwithresponse entities (cf.
the middle sub-figure in Fig-ure 1); Riis the ithresponse entity and p?
(Ri) isthe set of partitions created by intersectingRiwithkey entities (cf.
the right-most sub-figure in Fig-ure 1); and Nkand Nrare the number of key andresponse entities, respectively.The MUC F1score in this case is 0.40.4.2 B3For computing B3recall, each key mention is as-signed a credit equal to the ratio of the number ofcorrect mentions in the predicted entity contain-ing the key mention to the size of the key entity towhich the mention belongs, and the recall is just33the sum of credits over all key mentions normal-ized over the number of key mentions.
B3preci-sion is computed similarly, except switching therole of key and response.
Applied to the example:R =?Nki=1?Nrj=1|Ki?Rj|2|Ki|?Nki=1|Ki|=17?
(223+123+124+224) =17?3512?
0.42P =?Nki=1?Nrj=1|Ki?Rj|2|Rj|?Nri=1|Rj|=18?
(222+122+122+224) =18?41= 0.50Note that terms with 0 value are omitted.
The B3F1score is 0.46.4.3 CEAFThe first step in the CEAF computation is gettingthe best scoring alignment between the key andresponse entities.
In this case the alignment isstraightforward.
Entity R1aligns with K1and R3aligns with K2.
R2remains unaligned.CEAFmCEAFmrecall is the number of aligned mentionsdivided by the number of key mentions, and preci-sion is the number of aligned mentions divided bythe number of response mentions:R =|K1?
R1|+ |K2?
R3||K1|+ |K2|=(2 + 2)(3 + 4)?
0.57P =|K1?
R1|+ |K2?
R3||R1|+ |R2|+ |R3|=(2 + 2)(2 + 2 + 4)= 0.50The CEAFmF1score is 0.53.CEAFeWe use the same notation as in Luo (2005):?4(Ki, Rj) to denote the similarity between a keyentity Kiand a response entity Rj.
?4(Ki, Rj) isdefined as:?4(Ki, Rj) =2?
|Ki?
Rj||Ki|+ |Rj|.CEAFerecall and precision, when applied to thisexample, are:R =?4(K1, R1) + ?4(K2, R3)Nk=(2?2)(3+2)+(2?2)(4+4)2= 0.65P =?4(K1, R1) + ?4(K2, R3)Nr=(2?2)(3+2)+(2?2)(4+4)3?
0.43The CEAFeF1score is 0.52.4.4 BLANCThe BLANC metric illustrated here is the one inour implementation which extends the originalBLANC (Recasens and Hovy, 2011) to predictedmentions (Luo et al, 2014).Let Ckand Crbe the set of coreference linksin the key and response respectively, and NkandNrbe the set of non-coreference links in the keyand response respectively.
A link between a men-tion pair m and n is denoted by mn; then for theexample in Figure 1, we haveCk= {ab, ac, bc, de, df, dg, ef, eg, fg}Nk= {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}Cr= {ab, cd, fg, fh, fi, gh, gi, hi}Nr= {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,cf, cg, ch, ci, df, dg, dh, di}.Recall and precision for coreference links are:Rc=|Ck?
Cr||Ck|=29?
0.22Pc=|Ck?
Cr||Cr|=28= 0.25and the coreference F-measure, Fc?
0.23.
Sim-ilarly, recall and precision for non-coreferencelinks are:Rn=|Nk?Nr||Nk|=812?
0.67Pn=|Nk?Nr||Nr|=820= 0.40,and the non-coreference F-measure, Fn= 0.50.So the BLANC score isFc+Fn2?
0.36.5 ConclusionWe have cleared several misunderstandings aboutcoreference evaluation metrics, especially when aresponse contains imperfect predicted mentions,and have argued against mention manipulationsduring coreference evaluation.
These misunder-standings are caused partially by the lack of il-lustrative examples to show how a metric is com-puted on predicted mentions not aligned perfectlywith key mentions.
Therefore, we provide detailedsteps for computing all four metrics on a represen-tative example.
Furthermore, we have a referenceimplementation of these metrics that has been rig-orously tested and has been made available to thepublic as open source software.
We reported newscores on the CoNLL 2011 and 2012 data sets,which can serve as the benchmarks for future re-search work.AcknowledgmentsThis work was partially supported by grantsR01LM10090 from the National Library ofMedicine and IIS-1219142 from the National Sci-ence Foundation.34ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings ofLREC, pages 563?566.Jie Cai and Michael Strube.
2010.
Evaluation metricsfor end-to-end coreference resolution systems.
InProceedings of SIGDIAL, pages 28?36.Chen Chen and Vincent Ng.
2013.
Linguisticallyaware coreference evaluation metrics.
In Pro-ceedings of the Sixth IJCNLP, pages 1366?1374,Nagoya, Japan, October.Nancy Chinchor and Beth Sundheim.
2003.
Mes-sage understanding conference (MUC) 6.
InLDC2003T13.Nancy Chinchor.
2001.
Message understanding con-ference (MUC) 7.
In LDC2001T02.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extrac-tion (ACE) program-tasks, data, and evaluation.
InProceedings of LREC.Lynette Hirschman and Nancy Chinchor.
1997.
Coref-erence task definition (v3.0, 13 jul 97).
In Proceed-ings of the 7th Message Understanding Conference.Gordana Ilic Holen.
2013.
Critical reflections onevaluation practices in coreference resolution.
InProceedings of the NAACL-HLT Student ResearchWorkshop, pages 1?7, Atlanta, Georgia, June.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics, and Speech Recognition.
Prentice Hall.
SecondEdition.Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, andEduard Hovy.
2014.
An extension of BLANC tosystem mentions.
In Proceedings of ACL, Balti-more, Maryland, June.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of HLT-EMNLP,pages 25?32.Sameer Pradhan, Eduard Hovy, Mitchell Marcus,Martha Palmer, Lance Ramshaw, and RalphWeischedel.
2007.
OntoNotes: A Unified Rela-tional Semantic Representation.
International Jour-nal of Semantic Computing, 1(4):405?419.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
CoNLL-2011 shared task: Modelingunrestricted coreference in OntoNotes.
In Proceed-ings of CoNLL: Shared Task, pages 1?27.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedingsof CoNLL: Shared Task, pages 1?40.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings ofEMNLP, pages 968?977.Altaf Rahman and Vincent Ng.
2011.
Coreference res-olution with world knowledge.
In Proceedings ofACL, pages 814?824.Marta Recasens and Eduard Hovy.
2011.
BLANC:Implementing the Rand Index for coreference eval-uation.
Natural Language Engineering, 17(4):485?510.Marta Recasens, Llu?
?s M`arquez, Emili Sapena,M.
Ant`onia Mart?
?, Mariona Taul?e, V?eronique Hoste,Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution inmultiple languages.
In Proceedings of SemEval,pages 1?8.Marta Recasens, Marie-Catherine de Marneffe, andChris Potts.
2013.
The life and death of discourseentities: Identifying singleton mentions.
In Pro-ceedings of NAACL-HLT, pages 627?633.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP, pages656?664.William F. Styler, Steven Bethard an Sean Finan,Martha Palmer, Sameer Pradhan, Piet C de Groen,Brad Erickson, Timothy Miller, Chen Lin, GuerganaSavova, and James Pustejovsky.
2014.
Temporalannotation in the clinical domain.
Transactions ofComputational Linguistics, 2(April):143?154.Ozlem Uzuner, Andreea Bodnari, Shuying Shen, TylerForbush, John Pestian, and Brett R South.
2012.Evaluating the state of the art in coreference res-olution for electronic medical records.
Journal ofAmerican Medical Informatics Association, 19(5),September.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model theo-retic coreference scoring scheme.
In Proceedings ofthe 6th Message Understanding Conference, pages45?52.Ralph Weischedel, Eduard Hovy, Mitchell Marcus,Martha Palmer, Robert Belvin, Sameer Pradhan,Lance Ramshaw, and Nianwen Xue.
2011.OntoNotes: A large training corpus for enhancedprocessing.
In Joseph Olive, Caitlin Christian-son, and John McCary, editors, Handbook of Natu-ral Language Processing and Machine Translation:DARPA Global Autonomous Language Exploitation.Springer.35
