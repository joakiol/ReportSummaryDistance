Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 209?216, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMinimum Sample Risk Methods for Language Modeling1Jianfeng GaoMicrosoft Research Asiajfgao@microsoft.comHao Yu,  Wei YuanShanghai Jiaotong Univ., ChinaPeng XuJohn Hopkins Univ., U.S.A.xp@clsp.jhu.edu1The work was done while the second, third and fourth authors were visiting Microsoft Research Asia.
Thanks to Hisami Suzuki forher valuable comments.AbstractThis paper proposes a new discriminativetraining method, called minimum sample risk(MSR), of estimating parameters of languagemodels for text input.
While most existingdiscriminative training methods use a lossfunction that can be optimized easily butapproaches only approximately to the objec-tive of minimum error rate, MSR minimizesthe training error directly using a heuristictraining procedure.
Evaluations on the taskof Japanese text input show that MSR canhandle a large number of features and train-ing samples; it significantly outperforms aregular trigram model trained using maxi-mum likelihood estimation, and it also out-performs the two widely applied discrimi-native methods, the boosting and the per-ceptron algorithms, by a small but statisti-cally significant margin.1 IntroductionLanguage modeling (LM) is fundamental to a widerange of applications, such as speech recognitionand Asian language text input (Jelinek 1997; Gao etal.
2002).
The traditional approach uses a paramet-ric model with maximum likelihood estimation (MLE),usually with smoothing methods to deal with datasparseness problems.
This approach is optimalunder the assumption that the true distribution ofdata on which the parametric model is based isknown.
Unfortunately, such an assumption rarelyholds in realistic applications.An alternative approach to LM is based on theframework of discriminative training, which uses amuch weaker assumption that training and testdata are generated from the same distribution butthe form of the distribution is unknown.
Unlike thetraditional approach that maximizes the function(i.e.
likelihood of training data) that is loosely as-sociated with error rate, discriminative trainingmethods aim to directly minimize the error rate ontraining data even if they reduce the likelihood.
So,they potentially lead to better solutions.
However,the error rate of a finite set of training samples isusually a step function of model parameters, andcannot be easily minimized.
To address this prob-lem, previous research has concentrated on thedevelopment of a loss function that approximatesthe exact error rate and can be easily optimized.Though these methods (e.g.
the boosting method)have theoretically appealing properties, such asconvergence and bounded generalization error, weargue that the approximated loss function mayprevent them from attaining the original objectiveof minimizing the error rate.In this paper we present a new estimation pro-cedure for LM, called minimum sample risk (MSR).
Itdiffers from most existing discriminative trainingmethods in that instead of searching on an ap-proximated loss function, MSR employs a simpleheuristic training algorithm that minimizes theerror rate on training samples directly.
MSR oper-ates like a multidimensional function optimizationalgorithm: first, it selects a subset of features thatare the most effective among all candidate features.The parameters of the model are then optimizediteratively: in each iteration, only the parameter ofone feature is adjusted.
Both feature selection andparameter optimization are based on the criterionof minimizing the error on training samples.
Ourevaluation on the task of Japanese text input showsthat MSR achieves more than 20% error rate reduc-tion over MLE on two newswire data sets, and italso outperforms the other two widely applieddiscriminative methods, the boosting method andthe perceptron algorithm, by a small but statisti-cally significant margin.Although it has not been proved in theory thatMSR is always robust, our experiments of cross-domain LM adaptation show that it is.
MSR caneffectively adapt a model trained on one domain to209different domains.
It outperforms the traditionalLM adaptation method significantly, and achievesat least comparable or slightly better results to theboosting method and the perceptron algorithm.2 IME Task and LMThis paper studies LM on the task of Asian lan-guage (e.g.
Chinese or Japanese) text input.
This isthe standard method of inputting Chinese orJapanese text by converting the input phoneticsymbols into the appropriate word string.
In thispaper we call the task IME, which stands for inputmethod editor, based on the name of the commonlyused Windows-based application.Performance on IME is measured in terms of thecharacter error rate (CER), which is the number ofcharacters wrongly converted from the phoneticstring divided by the number of characters in thecorrect transcript.
Current IME systems makeabout 5-15% CER in conversion of real data in awide variety of domains (e.g.
Gao et al 2002).Similar to speech recognition, IME is viewed asa Bayes decision problem.
Let A be the input pho-netic string.
An IME system?s task is to choose themost likely word string W* among those candidatesthat could be converted from A:)|()(maxarg)|(maxarg(A))(* WAPWPAWPWWAW GENGEN ?
?==  (1)where GEN(A) denotes the candidate set given A.Unlike speech recognition, however, there is noacoustic ambiguity since the phonetic string isinputted by users.
Moreover, if we do not take intoaccount typing errors, it is reasonable to assume aunique mapping from W and A in IME, i.e.
P(A|W)= 1.
So the decision of Equation (1) depends solelyupon P(W), making IME a more direct evaluationtest bed for LM than speech recognition.
Anotheradvantage is that it is easy to convert W to A (forChinese and Japanese), which enables us to obtaina large number of training data for discriminativelearning, as described later.The values of P(W) in Equation (1) are tradi-tionally calculated by MLE: the optimal modelparameters ?
* are chosen in such a way thatP(W|?
*) is maximized on training data.
The argu-ments in favor of MLE are based on the assumptionthat the form of the underlying distributions isknown, and that only the values of the parameterscharacterizing those distributions are unknown.
Inusing MLE for LM, one always assumes a multi-nomial distribution of language.
For example, atrigram model makes the assumption that the nextword is predicted depending only on two preced-ing words.
However, there are many cases innatural language where words over an arbitrarydistance can be related.
MLE is therefore not opti-mal because the assumed model form is incorrect.What are the best estimators when the model isknown to be false then?
In IME, we can tackle thisquestion empirically.
Best IME systems achieve theleast CER.
Therefore, the best estimators are thosewhich minimize the expected error rate on unseentest data.
Since the distribution of test data is un-known, we can approximately minimize the errorrate on some given training data (Vapnik 1999).Toward this end, we have developed a very simpleheuristic training procedure called minimum samplerisk, as presented in the next section.3 Minimum Sample Risk3.1 Problem DefinitionWe follow the general framework of linear dis-criminant models described in (Duda et al 2001).
Inthe rest of the paper we use the following notation,adapted from Collins (2002).?
Training data is a set of example input/outputpairs.
In LM for IME, training samples are repre-sented as {Ai, WiR}, for i = 1?M, where each Ai is aninput phonetic string and WiR is the reference tran-script of Ai.?
We assume some way of generating a set ofcandidate word strings given A, denoted byGEN(A).
In our experiments, GEN(A) consists oftop N word strings converted from A using a base-line IME system that uses only a word trigrammodel.?
We assume a set of D+1 features fd(W), for d =0?D.
The features could be arbitrary functions thatmap W to real values.
Using vector notation, wehave f(W)?
?D+1, where f(W) = [f0(W), f1(W), ?,fD(W)]T. Without loss of generality, f0(W) is calledthe base feature, and is defined in our case as thelog probability that the word trigram model as-signs to W. Other features (fd(W), for d = 1?D) aredefined as the counts of word n-grams (n = 1 and 2in our experiments) in W.?
Finally, the parameters of the model form avector of D+1 dimensions, each for one featurefunction, ?
= [?0, ?1, ?, ?D].
The score of a wordstring W can be written as210)(),( WWScore ?f?
= ?==Dddd Wf?0)( .
(2)The decision rule of Equation (1) is rewritten as),(maxarg),((A)* ?
?GENWScoreAWW?= .
(3)Equation (3) views IME as a ranking problem,where the model gives the ranking score, notprobabilities.
We therefore do not evaluate themodel via perplexity.Now, assume that we can measure the numberof conversion errors in W by comparing it with areference transcript WR using an error functionEr(WR,W) (i.e.
the string edit distance function inour case).
We call the sum of error counts over thetraining samples sample risk.
Our goal is to mini-mize the sample risk while searching for the pa-rameters as defined in Equation (4), hence the nameminimum sample risk (MSR).
Wi* in Equation (4) isdetermined by Equation (3),?==MiiiRidefMSR AWW...1* )),(,Er(minarg ???.
(4)We first present the basic MSR training algorithm,and then the two improvements we made.3.2 Training AlgorithmThe MSR training algorithm is cast as a multidi-mensional function optimization approach (Presset al 1992): taking the feature vector as a set ofdirections; the first direction (i.e.
feature) is selectedand the objective function (i.e.
sample risk) isminimized along that direction using a line search;then from there along the second direction to itsminimum, and so on, cycling through the whole setof directions as many times as necessary, until theobjective function stops decreasing.This simple method can work properly undertwo assumptions.
First, there exists an implemen-tation of line search that optimizes the functionalong one direction efficiently.
Second, the numberof candidate features is not too large, and thesefeatures are not highly correlated.
However, nei-ther of the assumptions holds in our case.
First ofall, Er(.)
in Equation (4) is a step function of ?, thuscannot be optimized directly by regular gradient-based procedures ?
a grid search has to be usedinstead.
However, there are problems with simplegrid search: using a large grid could miss the op-timal solution whereas using a fine-grained gridwould lead to a very slow algorithm.
Secondly, inthe case of LM, there are millions of candidatefeatures, some of which are highly correlated.
Weaddress these issues respectively in the next twosubsections.3.3 Grid Line SearchOur implementation of a grid search is a modifiedversion of that proposed in (Och 2003).
The modi-fications are made to deal with the efficiency issuedue to the fact that there is a very large number offeatures and training samples in our task, comparedto only 8 features used in (Och 2003).
Unlike asimple grid search where the intervals between anytwo adjacent grids are equal and fixed, we deter-mine for each feature a sequence of grids withdifferently sized intervals, each corresponding to adifferent value of sample risk.As shown in Equation (4), the loss function (i.e.sample risk) over all training samples is the sum ofthe loss function (i.e.
Er(.))
of each training sample.Therefore, in what follows, we begin with a discus-sion on minimizing Er(.)
of a training sample usingthe line search.Let ?
be the current model parameter vector,and fd be the selected feature.
The line search aims tofind the optimal parameter ?d* so as to minimizeEr(.).
For a training sample (A, WR), the score of eachcandidate word string W?GEN(A), as in Equation(2), can be decomposed into two terms:)()()(),('0''' WfWfWWScore ddDddddd ??
+== ???=?f?
,where the first term on the right hand side does notchange with ?d.
Note that if several candidate wordstrings have the same feature value fd(W), theirrelative rank will remain the same for any ?d.
Sincefd(W) takes integer values in our case (fd(W) is thecount of a particular n-gram in W), we can group thecandidates using fd(W) so that candidates in eachgroup have the same value of fd(W).
In each group,we define the candidate with the highest value of?
?
?=D ddd dd Wf'0' '' )(?as the active candidate of the group because nomatter what value ?d takes, only this candidatecould be selected according to Equation (3).Now, we reduce GEN(A) to a much smaller listof active candidates.
We can find a set of intervalsfor ?d, within each of which a particular activecandidate will be selected as W*.
We can computethe Er(.)
value of that candidate as the Er(.)
value forthe corresponding interval.
As a result, for each211training sample, we obtain a sequence of intervalsand their corresponding Er(.)
values.
The optimalvalue ?d* can then be found by traversing the se-quence and taking the midpoint of the interval withthe lowest Er(.)
value.3053063073083093103113120.
85 0.
9 0.
95 1 1.05 1.1 1.
15 1.
2lambdaSR(.
)sample risksmoothed sample riskFigure 1.
Examples of line search.This process can be extended to the wholetraining set as follows.
By merging the sequence ofintervals of each training sample in the training set,we obtain a global sequence of intervals as well astheir corresponding sample risk.
We can then findthe optimal value ?d* as well as the minimal samplerisk by traversing the global interval sequence.
Anexample is shown in Figure 1.The line search can be unstable, however.
Insome cases when some of the intervals are verynarrow (e.g.
the interval A in Figure 1), moving theoptimal value ?d* slightly can lead to much largersample risk.
Intuitively, we prefer a stable solutionwhich is also known as a robust solution (with evenslightly higher sample risk, e.g.
the interval B inFigure 1).
Following Quirk et al (2004), we evaluateeach interval in the sequence by its correspondingsmoothed sample risk.
Let ?
be the midpoint of aninterval and SR(?)
be the corresponding sample riskof the interval.
The smoothed sample risk of theinterval is defined as????
dbb)SR(?
+?where b is a smoothing factor whose value is de-termined empirically  (0.06 in our experiments).
Asshown in Figure 1, a more stable interval B is se-lected according to the smoothed sample risk.In addition to reducing GEN(A) to an activecandidate list described above, the efficiency of theline search can be further improved.
We find thatthe line search only needs to traverse a small subsetof training samples because the distribution offeatures among training samples are very sparse.Therefore, we built an inverted index that lists foreach feature all training samples that contain it.
Aswill be shown in Section 4.2, the line search is veryefficient even for a large training set with millions ofcandidate features.3.4 Feature Subset SelectionThis section describes our method of selectingamong millions of features a small subset of highlyeffective features for MSR learning.
Reducing thenumber of features is essential for two reasons: toreduce computational complexity and to ensure thegeneralization property of the linear model.
A largenumber of features lead to a large number of pa-rameters of the resulting linear model, as describedin Section 3.1.
For a limited number of trainingsamples, keeping the number of features suffi-ciently small should lead to a simpler model that isless likely to overfit to the training data.The first step of our feature selection algorithmtreats the features independently.
The effectivenessof a feature is measured in terms of the reduction ofthe sample risk on top of the base feature f0.
For-mally, let SR(f0) be the sample risk of using the basefeature only, and SR(f0 + ?dfd) be the sample risk ofusing both f0 and fd and the parameter ?d that hasbeen optimized using the line search.
Then theeffectiveness of fd, denoted by E(fd), is given by))SR()(SR(max)SR()SR()(00...1,00iiDifddd fffffffEi?
?+?+?==, (5)where the denominator is a normalization term toensure that E(f) ?
[0, 1].The feature selection procedure can be stated asfollows: The value of E(.)
is computed according toEquation (5) for each of the candidate features.Features are then ranked in the order of descendingvalues of E(.).
The top l features are selected to formthe feature vector in the linear model.Treating features independently has the ad-vantage of computational simplicity, but may notbe effective for features with high correlation.
Forinstance, although two features may carry richdiscriminative information when treated sepa-rately, there may be very little gain if they are com-bined in a feature vector, because of the high cor-relation between them.
Therefore, in what follows,we describe a technique of incorporating correla-tion information in the feature selection criterion.Let xmd, m = 1?M and d = 1?D, be a Booleanvalue: xmd = 1 if the sample risk reduction of usingthe d-th feature on the m-th training sample, com-BA212puted by Equation (5), is larger than zero, and 0otherwise.
The cross correlation coefficient be-tween two features fi and fj is estimated as??
?====Mm mjMm miMm mjmixxxxjiC12121),( .
(6)It can be shown that C(i, j) ?
[0, 1].
Now, similar to(Theodoridis and Koutroumbas 2003), the featureselection procedure consists of the following steps,where fi denotes any selected feature and fj denotesany candidate feature to be selected.Step 1.
For each of the candidate features (fd, for d =1?D), compute the value of E(f) according toEquation (5).
Rank them in a descending order andchoose the one with the highest E(.)
value.
Let usdenote this feature as f1.Step 2.
To select the second feature, compute thecross correlation coefficient between the selectedfeature f1 and each of the remaining M-1 features,according to Equation (6).Step 3.
Select the second feature f according to { } ),1()1()(maxarg*...2jCfEj jDj??
?
?==where ?
is the weight that determines the relativeimportance we give to the two terms.
The value of?
is optimized on held-out data (0.8 in our experi-ments).
This means that for the selection of thesecond feature, we take into account not only itsimpact of reducing the sample risk but also thecorrelation with the previously selected feature.
Itis expected that choosing features with less corre-lation gives better sample risk minimization.Step 4.
Select k-th features, k = 3?K, according to????????
?= ??=11),(11)(maxarg*kijjjiCkfEj??
(7)That is, we select the next feature by taking intoaccount its average correlation with all previouslyselected features.
The optimal number of features, l,is determined on held-out data.Similarly to the case of line search, we need todeal with the efficiency issue in the feature selec-tion method.
As shown in Equation (7), the esti-mates of E(.)
and C(.)
need to be computed.
Let Dand K (K << D) be the number of all candidatefeatures and the number of features in the resultingmodel, respectively.
According to the feature se-lection method described above, we need to esti-mate E(.)
for each of the D candidate features onlyonce in Step 1.
This is not very costly due to theefficiency of our line search algorithm.
Unlike thecase of E(.
), O(K?D) estimates of C(.)
are required inStep 4.
This is computationally expensive even for amedium-sized K. Therefore, every time a new fea-ture is selected (in Step 4), we only estimate thevalue of C(.)
between each of the selected featuresand each of the top N remaining features with thehighest value of E(.).
This reduces the number ofestimates of C(.)
to O(K?N).
In our experiments weset N = 1000, much smaller than D. This reduces thecomputational cost significantly without producingany noticeable quality loss in the resulting model.The MSR algorithm used in our experiments issummarized in Figure 2.
It consists of feature se-lection (line 2) and optimization (lines 3 - 5) steps.1 Set ?0 = 1 and ?d = 0 for d=1?D2 Rank all features and select the top K features, usingthe feature selection method described in Section 3.43 For t = 1?T (T= total number of iterations)4 For each k = 1?K5    Update the parameter of fk using line search.Figure 2: The MSR algorithm4 Evaluation4.1 SettingsWe evaluated MSR on the task of Japanese IME.Two newspaper corpora are used as training andtest data: Nikkei and Yomiuri Newspapers.
Bothcorpora have been pre-word-segmented using alexicon containing 167,107 entries.
A 5,000-sentencesubset of the Yomiuri Newspaper corpus  was usedas held-out data (e.g.
to determine learning rate,number of iterations and features etc.).
We testedour models on another  5,000-sentence subset of theYomiuri Newspaper corpus.We used an 80,000-sentence subset of the NikkeiNewspaper corpus as the training set.
For each A,we produced a word lattice using the baselinesystem described in (Gao et al 2002), which uses aword trigram model trained via MLE on anther400,000-sentence subset of the Nikkei Newspapercorpus.
The two subsets do not overlap so as tosimulate the case where unseen phonetic symbolstrings are converted by the baseline system.
Forefficiency, we kept for each training sample thebest 20 hypotheses in its candidate conversion setGEN(A) for discriminative training.
The oracle besthypothesis, which gives the minimum number oferrors, was used as the reference transcript of A.2134.2 ResultsWe used unigrams and bigrams that occurred morethan once in the training set as features.
We did notuse trigram features because they did not result in asignificant improvement in our pilot study.
Thetotal number of candidate features we used wasaround 860,000.Our main experimental results are shown inTable 1.
Row 1 is our baseline result using the wordtrigram model.
Notice that the result is much betterthan the state-of-the-art performance currentlyavailable in the marketplace (e.g.
Gao et al 2002),presumably due to the large amount of trainingdata we used, and to the similarity between thetraining and the test data.
Row 2 is the result of themodel trained using the MSR algorithm describedin Section 3.
We also compared the MSR algorithmto two of the state-of-the-art discriminative trainingmethods: Boosting in Row 3 is an implementationof the improved algorithm for the boosting lossfunction proposed in (Collins 2000), and Percep-tron in Row 4 is an implementation of the averagedperceptron algorithm described in (Collins 2002).We see that all discriminative training methodsoutperform MLE significantly (p-value < 0.01).
Inparticular, MSR outperforms MLE by more than20% CER reduction.
Notice that we used only uni-gram and bigram features that have been includedin the baseline trigram model, so the improvementis solely attributed to the high performance of MSR.We also find that MSR outperforms the perceptronand boosting methods by a small but statisticallysignificant margin.The MSR algorithm is also very efficient: using asubset of 20,000 features, it takes less than 20 min-utes to converge on an XEON(TM) MP 1.90GHzmachine.
It is as efficient as the perceptron algo-rithm and slightly faster than the boosting method.4.3 Robustness IssuesMost theorems that justify the robustness of dis-criminative training algorithms concern two ques-tions.
First, is there a guarantee that a given algo-rithm converges even if the training samples arenot linearly separable?
This is called the convergenceproblem.
Second, how well is the training errorreduction preserved when the algorithm is appliedto unseen test samples?
This is called the generali-zation problem.
Though we currently cannot give atheoretical justification, we present empirical evi-dence here for the robustness of the MSR approach.As Vapnik (1999) pointed out, the most robustlinear models are the ones that achieve the leasttraining errors with the least number of features.Therefore, the robustness of the MSR algorithm aremainly affected by the feature selection method.
Toverify this, we created four different subsets offeatures using different settings of the feature se-lection method described in Section 3.4.
We se-lected different numbers of features (i.e.
500 and2000) with and without taking into account thecorrelation between features (i.e.
?
in Equation (7)is set to 0.8 and 1, respectively).
For each of the fourfeature subsets, we used the MSR algorithm togenerate a set of models.
The CER curves of thesemodels on training and test data sets are shown inFigures 3 and 4, respectively.2.082.102.122.142.162.182.202.222.242.262.281 250 500 750 1000 1250 1500 1750 2000# of roundsCER(%)MSR(?=1)-2000MSR(?=1)-500MSR(?=0.8)-2000MSR(?=0.8)-500Figure 3.
Training error curves of the MSR algorithm2.942.993.043.093.143.193.241 250 500 750 1000 1250 1500 1750 2000# of roundsCER(%)MSR(?=1)-2000MSR(?=1)-500MSR(?=0.8)-2000MSR(?=0.8)-500Figure 4.
Test error curves of the MSR algorithmThe results reveal several facts.
First, the con-vergence properties of MSR are shown in Figure 3where in all cases, training errors drop consistentlywith more iterations.
Secondly, as expected, usingmore features leads to overfitting, For example,MSR(?
=1)-2000 makes fewer errors than MSR(?=1)-500 on training data but more errors on testdata.
Finally, taking into account the correlationbetween features (e.g.
?
= 0.8 in Equation (7)) re-Model CER (%) % over MLE1.
MLE  3.70 --2.
MSR (K=2000) 2.95 20.93.
Boosting  3.06 18.04.
Perceptron 3.07 17.8Table 1.
Comparison of CER results.214sults in a better subset of features that lead to notonly fewer training errors, as shown in Figure 3,but also better generalization properties (fewer testerrors), as shown in Figure 4.4.4 Domain Adaptation ResultsThough MSR achieves impressive performance inCER reduction over the comparison methods, asdescribed in Section 4.2, the experiments are allperformed using newspaper text for both trainingand testing, which is not a realistic scenario if weare to deploy the model in an application.
Thissection reports the results of additional experi-ments in which we adapt a model trained on onedomain to a different domain, i.e., in a so-calledcross-domain LM adaptation paradigm.
See (Su-zuki and Gao 2005) for a detailed report.The data sets we used stem from five distinctsources of text.
The Nikkei newspaper corpus de-scribed in Section 4.1 was used as the backgrounddomain, on which the word trigram model wastrained.
We used four adaptation domains: Yomi-uri (newspaper corpus), TuneUp (balanced corpuscontaining newspapers and other sources of text),Encarta (encyclopedia) and Shincho (collection ofnovels).
For each of the four domains, we used an72,000-sentence subset as adaptation training data,a 5,000-sentence subset as held-out data and an-other 5,000-sentence subset as test data.
Similarly,all corpora have been word-segmented, and wekept for each training sample, in the four adapta-tion domains, the best 20 hypotheses in its candi-date conversion set for discriminative training.We compared MSR with three other LM adap-tation methods:Baseline is the background word trigram model,as described in Section 4.1.MAP (maximum a posteriori) is a traditional LMadaptation method where the parameters of thebackground model are adjusted in such a way thatmaximizes the likelihood of the adaptation data.Our implementation takes the form of linear in-terpolation as P(wi|h) = ?Pb(wi|h) + (1-?
)Pa(wi|h),where Pb is the probability of the backgroundmodel, Pa is the probability trained on adaptationdata using MLE and the history h corresponds totwo preceding words (i.e.
Pb and Pa are trigramprobabilities).
?
is the interpolation weight opti-mized on held-out data.Perceptron, Boosting and MSR are the threediscriminative methods described in the previoussections.
For each of them, the base feature wasModel Yomiuri TuneUp Encarta ShinchoBaseline 3.70 5.81 10.24 12.18MAP  3.69 5.47 7.98 10.76MSR  2.73 5.15 7.40 10.16Boosting  2.78 5.33 7.53 10.25Perceptron 2.78 5.20 7.44 10.18Table 2.
CER(%) results on four adaptation test sets .derived from the word trigram model trained onthe background data, and other n-gram features (i.e.fd, d = 1?D in Equation (2)) were trained on adap-tation data.
That is, the parameters of the back-ground model are adjusted in such a way thatminimizes the errors on adaptation data made bybackground model.Results are summarized in Table 2.
First of all,in all four adaptation domains, discriminativemethods outperform MAP significantly.
Secondly,the improvement margins of discriminativemethods over MAP correspond to the similaritiesbetween background domain and adaptation do-mains.
When the two domains are very similar tothe background domain (such as Yomiuri), dis-criminative methods outperform MAP by a largemargin.
However, the margin is smaller when thetwo domains are substantially different (such asEncarta and Shincho).
The phenomenon is attrib-uted to the underlying difference between the twoadaptation methods: MAP aims to improve thelikelihood of a distribution, so if the adaptationdomain is very similar to the background domain,the difference between the two underlying distri-butions is so small that MAP cannot adjust themodel effectively.
However, discriminative meth-ods do not have this limitation for they aim toreduce errors directly.
Finally, we find that in mostadaptation test sets, MSR achieves slightly betterCER results than the two competing discriminativemethods.
Specifically, the improvements of MSRare statistically significant over the boostingmethod in three out of four domains, and over theperceptron algorithm in the Yomiuri domain.
Theresults demonstrate again that MSR is robust.5 Related WorkDiscriminative models have recently been provedto be more effective than generative models insome NLP tasks, e.g., parsing (Collins 2000), POStagging (Collins 2002) and LM for speech recogni-tion (Roark et al 2004).
In particular, the linearmodels, though simple and non-probabilistic innature, are preferred to their probabilistic coun-215terpart such as logistic regression.
One of the rea-sons, as pointed out by Ng and Jordan (2002), isthat the parameters of a discriminative model canbe fit either to maximize the conditional likelihoodon training data, or to minimize the training errors.Since the latter optimizes the objective function thatthe system is graded on, it is viewed as being moretruly in the spirit of discriminative learning.The MSR method shares the same motivation: tominimize the errors directly as much as possible.Because the error function on a finite data set is astep function, and cannot be optimized easily,previous research approximates the error functionby loss functions that are suitable for optimization(e.g.
Collins 2000; Freund et al 1998; Juang et al1997; Duda et al 2001).
MSR uses an alternativeapproach.
It is a simple heuristic training proce-dure to minimize training errors directly withoutapplying any approximated loss function.MSR shares many similarities with previousmethods.
The basic training algorithm described inSection 3.2 follows the general framework of multi-dimensional optimization (e.g., Press et al 1992).The line search is an extension of that described in(Och 2003; Quirk et al 2005.
The extension lies inthe way of handling large number of features andtraining samples.
Previous algorithms were used tooptimize linear models with less than 10 features.The feature selection method described in Section3.4 is a particular implementation of the featureselection methods described in (e.g., Theodoridisand Koutroumbas 2003).
The major differencebetween the MSR and other methods is that it es-timates the effectiveness of each feature in terms ofits expected training error reduction while previ-ous methods used metrics that are loosely coupledwith reducing training errors.
The way of dealingwith feature correlations in feature selection inEquation (7), was suggested by Finette et al (1983).6 Conclusion and Future WorkWe show that MSR is a very successful discrimina-tive training algorithm for LM.
Our experimentssuggest that it leads to significantly better conver-sion performance on the IME task than either theMLE method or the two widely applied discrimi-native methods, the boosting and perceptronmethods.
However, due to the lack of theoreticalunderpinnings, we are unable to prove that MSRwill always succeed.
This forms one area of ourfuture work.One of the most interesting properties of MSR isthat it can optimize any objective function (whetherits gradient is computable or not), such as error ratein IME or speech, BLEU score in MT, precision andrecall in IR (Gao et al 2005).
In particular, MSR canbe performed on large-scale training set with mil-lions of candidate features.
Thus, another area ofour future work is to test MSR on wider varieties ofNLP tasks such as parsing and tagging.ReferencesCollins, Michael.
2002.
Discriminative training methodsfor Hidden Markov Models: theory and experimentswith the perceptron algorithm.
In EMNLP 2002.Collins, Michael.
2000.
Discriminative reranking fornatural language parsing.
In ICML 2000.Duda, Richard O, Hart, Peter E. and Stork, David G. 2001.Pattern classification.
John Wiley & Sons, Inc.Finette S., Blerer A., Swindel W. 1983.
Breast tissue clas-sification using diagnostic ultrasound and pattern rec-ognition techniques: I.
Methods of pattern recognition.Ultrasonic Imaging, Vol.
5, pp.
55-70.Freund, Y, R. Iyer, R. E. Schapire, and Y.
Singer.
1998.
Anefficient boosting algorithm for combining preferences.In ICML?98.Gao, Jianfeng, Hisami Suzuki and Yang Wen.
2002.Exploiting headword dependency and predictive clus-tering for language modeling.
In EMNLP 2002.Gao, J, H. Qin, X. Xiao and J.-Y.
Nie.
2005.
Linear dis-criminative model for information retrieval.
In SIGIR.Jelinek, Fred.
1997.
Statistical methods for speech recognition.MIT Press, Cambridge, Mass.Juang, B.-H., W.Chou and C.-H. Lee.
1997.
Minimumclassification error rate methods for speech recognition.IEEE Tran.
Speech and Audio Processing 5-3: 257-265.Ng, A. N. and M. I. Jordan.
2002.
On discriminative vs.generative classifiers: a comparison of logistic regres-sion and na?ve Bayes.
In NIPS 2002: 841-848.Och, Franz Josef.
2003.
Minimum error rate training instatistical machine translation.
In ACL 2003Press, W. H., S. A. Teukolsky, W. T. Vetterling and B. P.Flannery.
1992.
Numerical Recipes In C: The Art of Scien-tific Computing.
New York: Cambridge Univ.
Press.Quirk, Chris, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: syntactically informedphrasal SMT.
In ACL 2005: 271-279.Roark, Brian, Murat Saraclar and Michael Collins.
2004.Corrective language modeling for large vocabulary ASRwith the perceptron algorithm.
In ICASSP 2004.Suzuki, Hisami and Jianfeng Gao.
2005.
A comparativestudy on language model adaptation using newevaluation metrics.
In HLT/EMNLP 2005.Theodoridis, Sergios and Konstantinos Koutroumbas.2003.
Pattern Recognition.
Elsevier.Vapnik, V. N. 1999.
The nature of statistical learning theory.Springer-Verlag, New York.216
