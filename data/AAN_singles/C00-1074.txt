Hybrid Neuro and Rule-Based Part of Speech TaggersQing Ma, Masaki  Murata,  Kiyotaka Uch imoto ,  H i tosh i  IsaharaCommunic~t ion s l{esea.rch Labora.toryMinistry of Posts a.nd Telecommm~ications588-2, lwa,oka,, Nishi-ku, Kobe 6511-2/192, 3a, pa,n{qma, murata., uchimoto,  isa.hara}{))crl.go.jpAbstractA hybrid system R)r tagging part of speech isdescril)ed that consists of a neuro tagger anda rule-based correcter.
The neuro tagger isan initia.1--state a.nnotator tha.t uses difl'ertnth_,,ngths of contexts based on longe, st context l)ri-ority.
Its inputs a.re weighted 1)y informationgains tha.t are obtained by information ma.xi-mization.
The rule-1)ased correcter is construct-ed by a. sol; of trm~sfc)rma.tion rules to xna.ke Ul)for the shortcomings o\[' the nou17o tagger.
Corn-puter experiments show that ahnost 20% of theerrors ma.de by the neuro tagger a.re correct-ed by the, st trans\[orma.tion rules, so tha.t thehybrid system ca.n reach a.n a,tcura.cy of 95.5%counting only the ambiguous words and 99.1%counting all words when a. small Thai corpuswith 22,311 a mbig;uous words is used t))v tra.in-ing.
This a(;cu racy is far higher than that usingan IIMM and is also higher tha.n that using a.rule-1)ased model.1 Introduct ionMany pa.rt of speech (POS) tatters  proposedso far (e.g., Brill, 1994; Meria.ldo, 1994; l)aele-marls, el.
al., 1996; and Schmid, 1994) ha.reachieved a. high accura.ey partly because a. verylarge amount of dal,~ was used to 1;rain them(e.g., on the order of 1,000,000 words for \]'hl-glish).
For ma.ny other la.nguages (e.g., Thai,which we treat in this paper)~ however, it is notas easy to cremate \]a.rge corpora from which lm:geamounts of tr~fining data can be extra.cted.
It istherefore desirable to construct a practic;d tag-ger tha.t needs as little training d a.t;a~ as possible.A multi-neuro tagger (Ma a.nd ls~hara, 11998)and its slimmed-down version called the ela.s-tic neuro tagger (Ma, el; al., 1999), which havehigh genera.lizing ability and therefore are goodat dealing with the problems of data sp~u:se-hess, were proposed to satist~y this requh:ement.These taggers perform POS tagging using difl'er-ent lengths of corltexts I)~.~sed on longest contextprk)rity, and each element of tile input is weight-ed with information gains (Quinla.n, 1993) forretlecting that tile elements of the input h~vedifferent rtlevances in t~Gging.
They ha.d a tag-ging accuracy of 94.4% (counting only the am-biguous words in part of speech) in computer ex-periments when a. small 'l'ha.i corpus with 22,311am biguous words was u se(l for tr~fi n ing.
This ~(:-curacy is bu" higher thml t\]lat; USillg tile hiddenMarker model (IIMM), the main approach to\])art o\[ speech tagging, ~nd is ~dso higher t,\]lantha.t using a. rule-based mode\].Neuro taggers, however, htwe several crucialshortcomings.
First, even in the case where thePOS of a word is uniquely determined by theword on its left, for example, a neural net willalso try to perlbrnl tagging based on tile com-plete context.
As a result, even for" when theword on tile left; is the same, the tagging result~s will be difl'erent if the complete contexts aredifferent, rl'ha, t is~ the neuro tagger carl hard-ly acquire the rules with single inputs.
Fur-thermore, although lexica.l in\[brma.tion is veryilnport~ult in t~gging, it is difficult for: neuralnets to use it becmme doing so would make thenetwork enorlnous.
That is, the neuro taggerca.nnot acquire (;lit rules with lexical informs>tion.
Additionally, Imca.use of convergence and509over-training l)roblems, it is impossible and alsonot advisM)le to train neural nets to an a.ccura,-cy of 100%.
The training should be stopped atan appropriate level of a.ceuracy.
Consequently,neural nets may not acquire some usefnl rules.To make up for these shortcomings of theneuro tagger, we introduce in this pa.per a rule-based corrector as tile post-processor and con-struct a hyl)rid system.
The rule-based cot-rector is constructed by a set of transforma-tion rules, which is acqnired by transforma?ion-based error-driven learning (Brill, 1.994:) fromtraining corpus using a set of templates.
Thetemplates are designed to SUl)l)ly the rules thatthe neuro tagger can hardly acquire.
Actual-ly, by examining the transformation rules ac-quired in the computer experiments, the 99.9%of them are exactly those that the neuro taggercan hardly acquire~ even when using a templateset including those for generating the'rules thatthe neuro tagger can easily acquire.
This rein-forces onr expectation that the rule-based ap-proach is a well-suited method to cope with theshortcomings of the neuro t~gger.
Computer ex-periments hows thai; about  200/0 of errors madeby the neuro tagger can be corrected by usingthese rules and that the hybrid system ca.n reachan accuracy of 95.5% counting only the aml)ign-ous words and 99.1% counting all words in l, hetesting corpus, when tile same corpus describedabove is used for training.2 POS Tagg ing  Prob lemsIn this paper, suppose there is a lexicon V,where the POSs that can be served by each wordare list.ed, and tiler(; is a set of POSs, l?.
That is,unknown words that do not exist in the lexiconare not dealt with.
The POS tagging problemis thus to find a string of POSs T = T172..-%(ri C F, i = 1 , - .
.
, s )  by following procedure~o when sentence W = wlw2...w.~ (wi C V,i = 1 , .
- .
, s )  is given.#:W ~ -+ rt, (1)where t is the index of the target word (the wordto be tagged), and W t is a word sequence withlength l + 1 + r (:entered on the target word:l i e  t : wt_  1 .
.
.
.
i o  t ?
.
.
Wt+r~ (2)where t - 1 > 1, t + r _< s. 'l'agging ca.n thus beregarded as a classification problem 1) 3, replacingthe POS with (;lass and can therefore be handledby using neural nets.3 Hybr id  SystemOur hybrid system (Fig.
J) consists of a neurotagger, which is used as an initial-state an nota-i;or, ~nd a rule-based corrector, which correctsthe outputs of the neuro tagger.
When a wordseque,~ce W t \[see l~q.
(2)\] is given, the neurotagger outl)ut a tagging result rN(Wt) for tiletarget word wt at first.
The rule-based correc-tor then corrects the output of the neuro taggeras a fine tuner and gives tile final tagging resultNcuro Tagger Rule-BasedCorrcctorFigure 1: Hybrid neuro and rule-based tagger.3.1 Neuro  taggerAs shown in Fig.
2, the neuro tagger consistsof a three-layer I)erceptron with elastic input.This section mainly descril)es the constructionof inl)ut and output of the nenro tagger, andthe elasticity by which it; becomes possible touse variable length of context for tagging.
Fordetails of the architecture of l)erceptron see e.g.,Haykin, 1994 and for details of the features ofthe neuro tagger see Ma and isahara, 1998 andMa, et aJ., 1999.lnl)ut I PT  is constructed fi'om word se-quence W t \[Eq.
(2)\], which is centered on targetword wt and has length l + 1 + r:I PT  = (iptt_ l , .
. '
,  i ph , .
.
.
,  iph+.,.
), (3)provided that input length l+ J+r  has elasticity,as described a,t tile end of this section.
WhenwoM wis given in position x (x = t - l , .
.
.
, t+r ) ,510OPTip t t_  I ...... i l ) \[t_ I ipl t11"1"il)lt+l ...... i\]Ht+rFigure 2: Neuro tagger.element ipt ,  of input I PT  is a. weighted pattern,defined asipt.,: = :/.,,.
(e,,,,('-,, 'e,."
,q,,~), (4)where g;,; is the inIbrmation gain which can beobtained using information theory (for detailssee Ma and lsahara., 11998) and 7 is the numberof tyl)es of POSs.
l\[' w is a word that apl)earsin the tra.ining data, then ea.ch bit e,,,i can beobtained:= P,,ot,(/ I , , ,) ,  (s)where l>'rob(ril'w) is a prior l>robal)ility of r itha.t (;he woM 'w can take,.
It is estitnated fromthe t raining (la,ta:C _ (<,  "')c' ( ,4  'where C'(r ( ,w)  is the number of lfimes both r:at++d w al)pea, r , a,nd C(w)  is the number oi' timesw appears in the training data.
1\[ w is a wordthat does not at)pear ill (,he training data~ theneach t)it c,,,,i is obtained:~ i\['r i is a. candidate(7) e.,,,," = (} otherwise,where 7,, is the number of P()Ss that the word'w Call ta.ke.
Output OPT is defined asOFT= ,o+), (s)provi<led that the output OI)T is decoded asS r/ i fO i= l  &O/=0for j?
iYN(Wt) Unknown otherwise,(.9)where rN(W,) is the ?a.gging result obtained bythe neuro tagger.There is more inforlnation available for con-structing the input for words on the left, be-cause they have already been tagged.
In thetagging phase, instead of using (4:)-(6), the in-put can be constructed simply asi / , ,_4 = .
oPT( - i ) ,  (1())where i = 1, .
.
.
,1,  and O I )T ( - i )  means the out-put of the tagger for the ith word before thetarget word.
ltowever, in the training process,the out;put of the tagger is not alway.a correcta.nd cannot be ted back to the inputs directly.Instead, a weighted awerage of the actual outputa.nd tlm desired output is used:iptt_i = 9t- i  ' (wol ,T " 0 PT  ( -  i) + WlOJ,:s " I) l iS) ,(1.1)where 1)l':,q' is the desired output,o: , : ,5 '  : (& , / )2 , .
.
.
,whose bits are defined as\] iI' r i is a desired answerI)i = 0 otherwise, (la)and WOl,'r and w/)l,\],q' are respecLh:ely de\[(ned as1'\]013J~,:o1 '~ .
.
.
.
(.14)1JACT 'a,nd'w>l,; ,s,  = :1 - wopT ,  (15)where \]@,uo and \]'JAC'T are  the objective andactual errors.
Thus, at the beginning of train-ing, the weighting of the desired output is largo.It decreases to zero during training.Elastic inputs are used in the neuro taggerso that the length of COlltext is variable in tag-ging based on longest context priority.
In (te~tail, (l, r) is initially set as large as possible fortagging.
If rN('Wi) = Unknown,  then (1, r) isreduced by some constant interval.
This l)ro-cess is repeated until rN(W~) 7 k Unknown or(1, r) = (0,0).
On the other hand, to nmke thesame set of connection weights of the neu ro tag-ger with the largest (1,'r) ava.ilable a.s lnuch as511possible when using short inputs for tagging, intraining phase the neuro tagger is regarded as aneural network that has gradually grown fi'omsmall one.
The training is therefore performedstep by step from small networks to large ones(for details see Ma, et al 1999).3.2 Rule-based eorreetorEven when the POS of a word can be deter-mined with certainty by only the word on theleft, for example, tile neuro tagger still tries totag based on the complete context.
That is,in general, what tile neuro tagger can easilyacquire by learning is the rules whose condi-tional parts are constructed by all inpttts ip tx(x = t - l , .
.
.
, t  + r) that are .joined with allAND logical operator, i.e., ( ip t t - t  & "'" iptt  &?
.. iptt+~, -+ OPT) .
In other words, it is (lit:ticult for tile neuro tagger to learn rules whoseconditional parts are constructed by only a sin-gle input like ( ipt,.
--+ OPT)  ~).
Also, althoughlexical information is very important in tagging,it is difficult for the neuro tagger to use it, be-cause doing so would make the network euofmous.
Tha.t is, the neuro tagger cannot acquirerules whose conditional parts consist of lexicalinformation like (w -4 OPT) ,  (w&r  -4  OPT) ,and (w~w2 --+ OPT) ,  where w, Wl, and w2 arewords and 7- is tile POS.
Furthermore, becauseof convergence and over-training 1)rol)lems, it isiml)ossible and also not advisable to train net>ral nets to all accuracy of 100%.
The trainingshould be stopped at an apl)rol)riate level of a.c-curacy.
Thus, neural net may not acquire someuseful rules.The transfbrmation rule-based correctormakes up for these crucial shortcomings.The rules are acquired Dora a training co lpus using a set of transformation templatesby transformation-based rror-driven learning(Brill, 1994).
Tile templates are constructedusing only those that supply the rules that tilenenro tagger can hardly acquire, i.e., are those1)The neuro tagger can also learn this kind of rulesbecause it can tag tile word using only ipt, (the inputof tile target word), ill the case of reducing tile (I, r) to(0,0), as described in Sec.
a.l.
The rules with singleinput described here, however, are a more general case,ill which the input call be ipt,~ (~: = t - 1, .
.
.
,  t + r).for acquiring the rules with single input, withlexical information, and with AND logica.1 in-put of POSs and lexical information.
The set oftempla,tes i  shown in Table 112).According to the learning procedure shownin Table 2, an ordered list of transformationrules are acquired by applying the template setto a training corpus, which had ah'eady beentagged by the neuro tagger.
After tile trans-formation rules are acquired, a corl)us is taggedas tbllows.
It is first tagged by the neuro tag-ger.
The tagged corpus is then corrected byusing the ordered list of transformation rules.The correction is a repetitive process applyingthe rules ill order to the corptlS, which is thenupdated, until all rules have been applied.4 Exper imenta l  ResultsData :  For our computer experiments, we usedtile same Thai corpus used by Ma et al (1999).Its 10,d52 sentences were randomly divided in-to two sets: one with 8,322 sentences for trail>ing and the other with 2,1.30 sentences for test-int.
The training set contained 12d,331 word-s, of which 22,311 were ambiguous; the testingset contained a4,5~14 words, of which 6,717 wereambiguous.
For training tile n euro tagger, onlythe ambiguous wor(ls in the.
training set wereused.
For training the HMM, all tilt words inthe training set were used.
In both cases, all thewords in tile training set were used to estimateProb( r i lw) ,  tim probability of "c i that wor(I wcan be (for details on the HMM, see Ma, et al,1999).
In the corpus, 4:7 types of POSs are de-fined (Charoenporn et al, 1997); i.e., 7 = 47.Neuro tagger: The neuro tagger was con-structed by a three-layer perceptron whoseinput-middle-outI)ut layers had p z, 2 7 units,respectively, where p = 7 ?
(1 + I + r).
The(l + 1 + r) had tile following elasticity.
In train-ing, tile (I, r) was increased step by step as (71,1)-+ (u,2) (a,2) (a,a) a,d gra,dualtraining fl'om a small to a large network waspertbrmed.
Ill tagging, on the other hand, the2)To see whether this set is suitable, a immloer of ad-ditional experiments were conducted using various setsof templates.
The details are described in Sec.
4.512r ~ I a,1)lc 1: Set o\[' templa.tes for tra.ns\[orln;~tion rulesChange t;ag v a to t;ag v ?
when:(single inlm|;)( input ('onsists of a POS)1. left (right) word is tagged v.2.
second left (right) word is tagged r.3.
third left (right) word is ta.gged r.(inI)n|; consist;s of a word)4. ta.rget word is ~.5.
left (right) word is w.6.
second left, (right) word is w.(AND logical inpu?
ot" words)7. l, arget word is 'UO 1 ~tlld left (right) word is wu.8.
left; (right) word is u,1 and second lcfl, (,'ight) word is w2.9.
left, word is w~ a.nd right, word is 'wu.
(AND logical in.lint; of POS and words)10. ta.rget word is uq and left (right) word is llaggod r.:11. left (righl;) word is .w~ and left.
(right) word is tagged r.12.
ta.rget word is w~, left (right) word is ,w.,, and left (right) word is tagged r.Ta,1)le 2: l)roetdure for learning transi'orma,tion rules1.
Apply neuro taggtr to training corpus, which is then updated.2.
Compare tagged results with desired ones and find errors.3.
Ma.teh templates l'or all errors and obtain set of tra.nsformation rules.d.
Select rule in corpus with the maximum value of' (cn l , _qood-h .
cnl,_bad), wherecnZ_qood: number that transforms incorrect ags to correct elliS:c'nl._bad: number that transforms correct tags to incorrect ones,h: weight to control the strict, hess of generating 1;he rule.5.
Apply selecttd rule to training corpus, which is then updated.6.
Append selected rule to ordered list o1" trausl'orma.tion rules.7.
Ih'4)eal; steps :2 through (j until no such rule can I)e selected, i.e., c 'n , t _good-h,.
cnl,_bad < O.
(l, 'r) was inversely reduce(l ste l) by step as (3,3)-+ (3,2) (2,2) (2,:1) O,:l) (:l,o)(0,0) a.s needed, provided tJlat the number ofunits in the middle layer was kept a.t the ma.xi-I l l  l l l l l  vahle.Ru le -based  cor re t to r :  The parameter h inthe tw~Juat;ion function (cnl ,_9ood - h, .
c'M._bad)used in 1;he learning procedure (Table 2) is aweight to control the strictness of generating a.rule.
IF It is large, the weight of cnt_bad is la.rgeand the possibility of generating incorrect rulesis reduced.
By regarding the neuro tagger as ~d-ready having high accuracy and using tile rule--based correcter as a fine tuner, weight h. was setto a. large vahm, 100.
Applying |;lit templatesCo the training corptm, which had already beentagged 1) 5, the neuro ta.gger, we obta.ined a.n or-dered list; of 520 transfbrmation rules.
'.l'~d)le 3shows the first 15 transfbrmation rules.Results :  Table 4 shows the resull;s of I)()S tag-ging for the testing data..
In addition to theaccuracy o\[" the neuro tagger and hybrid sys-tem, the ta.ble also shows tile accuracy of a, bast-line model, the IIMM, and a rule-based model\['or comparison.
The baseline model is one thatperforms tagging without using the contextualinlorma.tion; instead, it performs ta.gging usingonly f'requency informa.tion: the proba.bility ofP()S that; tach word can be.
The rule-basedmodel, to be exact, is also a hybrid system con-513'l'a.1)le 3: First 15 transfbrmation rulesNo.
F rom To  Cond i t i on1 PREL2 PREL3 Unknown4 XVHI45 VATT6 Unknown7 NCI4N8 VATTO PRELi0 VST~ii VfiTT12 NCMN13 NCHN14 Unknown15 NCNNRPRE le f t  word is punctuation and r ight  tuord is 5~guRPRE le f t  yard is ~RDVN le f t  ~ord Ls tagged XVfiEXVBH le f t  word is II~DflDVN le f t  word is  ~VRTT le f t  word is  tagged PRELRPRE le f t  word is uaVSTfi l e f t  word is ~q~RPRE r ight  word is ~gu and second r ight  word is a~q;JRDVN target word is ~t~4ADVN target  word is  ~4~RPRE target word is n14 and le f t  word is eentluuRPRE le f t  word is ~tt and le f t  ward is tagged NCHNfiDVN th i rd  le f t  word is tagged WCTCNIT taPget ~ord is nn~where PREL: Relat ive Pronoun, RPRE: Preposit ion,  fiDVN: fidverb with normal form .
.
.
.Table d: Results of POS ta,gging for testing data*model baseline IIMM rule-based lleuro hybridaccuracy 0.836 0.891 0.935 0.944 0.955*Accurac9 was determiued only for am lfiguous words.sisting of an initial-state annotator and a set oftransformation rules.
As the initial-state anno-b~tor, however, the baseline model is used in-stea.d of' the neuro tagger.
And, its rule set.
has1,177 transformation rules acquired h'om a moregeneral teml)late set, which is described at theend of this section.
The reason for using a gener-al template set is that the sol; of tra.nsibrma.tionrules in the rule-based model should be the mainannotator, not a fine post-processing tuner.
Forthe same reason, the parameter to control thestrictness of generating a rule, h, was set to  asmall value, \], so that a larger number of ruleswere generated.As shown in the table, the accuracy of thenenro tagger was far higher than that of theHMM and higher than that of the rule-basedmodel.
The accuracy of the rule-based mod-el, on the other hand, was also far higher thanthat of the IIMM, ~lthough it was inferior tothat of the neuro tagger.
The accuracy of thehybrid system was 1.1% higher than that of theneuro tagger.
Actually, the rule-based correctorcorrected 88.4% and 19.7% of the errors madeby the neuro tagger for the training and testingdata, respectively.Because the template set shown in Table 1was designed only to make up for the short-comings of the neuro tagger, tile set is smal-l compared to that used by Brill (1994).
Tosee whether this set is la.rge enough for our sys-tem, we perlbrmed two additional experimentsin which (\]) a sol; constructed 193' adding thetemplates with OR logical input of words to theoriginal set and (2) a, set constructed 1)5' fnrtheradding the templates with AND and OR logi-cal inputs of POSs to the set of case (1) wereused.
The set used in case (2) inclnded the setused by Brill (\]994) and all the nets nsed in ourexperiments.
It was also used for acquiring thetransformation rules in the rule-based model.The experimental results show that comparedto the original case, the accuracy in case (1)was improved very little and the accuracy incase (2) was also improved only 0.03%.
Theseresults show that the original set is nearly la.rgeenough for our system.To see whether tile set is snitable tbr oursystem, we performed ~tn additional experimen-t using the original set in which the templa.teswith OR logical inputs were used instead of thetemplates with AND logical inputs.
The accu-racy dropped by 0.1%.
Therefore, tile templateswith AND logical inputs are more suitable than514those with O11 logical inputs.We also performed an experiment using atemplate set without lexical intbrmation.
In thiscase, l;he accuracy dropl)ed by 0.9%, indicatingthat lexical informatioll is important in tagging.To determine the effect o1' using a. large h,for generating rules, we per\['ormed an experi-ment with h = 1.
In this case, the accuracydropped by only 0.045%, an insignifica.nt differ-ence compared to the case of h, = 100.By examining the acquired rules that wereobtained by al)plying the most COml)lete tem-plate set, i.e., the set used in case (2) describedabove, we found that 99.9% of them were thosethat can be obtained by a.pl)lying the originalset of templates, rl'ha.t is, the acquired ruleswere almost those that are dif\[icult \['or the neu-re tagger to acquire.
'.l'his rein forced our expec-tat;ion that the rule-based al)l)roach is a well-suited method to cope with the shortcoming ofthe neuro tagger.Finally, il, should 1)e noted that ill the liter-atures, tile tagging a.ccuracy is usua.lly delinedby counting a.ll tile words regardless of whetherthey are a.nlbiguous or not.
If we used this dell-nil:ion, t\]le accura.cy of our hybrid system wouldbe 99.1%.5 Conc lus ionTo collstruct a 1)tactical tagger that needs aslittle training data.
a.s possible, neuro taggers,which have high generalizing al)ility and there-fore a.re good at dealing with the problems ofda~ta.
sl)a,rseness, have been proposed so fa.r.
Neu-re tatters,  however, have crucial shortcomings:they ca.nnot utilize lexical information; theyhave trouble learning rules with single inputs;and they cannot learn training data to an ac~curacy of 100%.
To make up for these short-comings, we introduced a rule-based correcter,which is constructed by a. set of trans\[brma.tionrules obtained by error-driven learning, for post1)recessing and constructed a hybrid taggingsystem, l{y examining the transtbrma.tion rulesacquired in the computer experiments, we foundthat 1;he 99.9% of them were those that; the neu-re tagger can hardly acquire, even when using a.template set including t;hose for generating therules that the neuro tagger can easily acquire.This reinlbrced our expecta.tion that the rule-based approach is a well-suited method to copewith the shortcoming of the neuro tagger.
Com-puter experiments showed that 19.7% of the er-rors made by the neuro tagger were correctedby the tra.nslbrmation rules, so the hybrid sys-tem rea.ched an accuracy of 95.5% counting onlythe ambiguous words and 99.\]% counting all thewords in the testing data, when a small corpuswith only 22,311 ambiguous words was used tbrtrain int.
~l'h is ind icates thai; ou r tagging ,qystemcan nearly reach a pra.ctica.l level in terms of tag-ging accuracy even when a small Thai corpus isused tbr tra.ining.
This kind of tagging systemcan be used to constructs multilingua.1 corporathat include languages in which large corporahave not yet been constructed.Referencesl~rill, E.: Transfornmtion-based rror-driven lca.rn-ing and natural language processing: ~ case s-tudy ill 1)art-of-sl)eech tagging, ComputationalLi~g'uistics, Vol.
21, No.
4, pp.
543-565, 199~1.Cha.roenporll, T., Sornlertlanlva.nich, V., ~md Isa-hara, 11.: Building a la.rge Thai text corpusparl; of speech tagged corpus: OI{CIlll), Pro<Natural Language Processi~fl Pacific lNm ,5'gn~-po.du'm \[997, Phuket, Thailand, pp.
509-5\]2,1997.I)aelemans, W., Z~wrel, a., Berck, P., and C,i/lis, S.:MI3'I': A m<mlory-based pm-t of speech tagger-genera.tot, P'roc.
/tl.h Workshop on Very LargeCo,'po~zl, Copenhagen, l)em na.rk, pp.
1-1+1, 99(5.l\]aykin, S.: Neural Nchvorlcs, Macmillan CollegePublishing Coral)any, Inc., 199/t.Ma, Q. and lsahm'a., H.: A multi-neuro tagger us-ing variable lengths of contexts, Prec.
COLING-ACL'g8, Montreal, pp.
802-806, 1998.Ma, Q., Uchimoto, K., Mura.ta, M., and 1sahara H.:F, lastic neural networks tbr part of speech tag:ging, Prec.
IJCNN'99, Washington, \])C., pp.2991-2996, 1999.Meriaklo, B.: Tagging English text with a proba-bilistic model, Computational Linguistics, Vo\].20, No.
2, pp.
1.55-171, 19(.
)4.Quinla.n, 3.: G'~.5: Programs Jot Machine Learning,San Mateo, CA: Morgan Kaufinann, 1993.Schmid, 1t.
: l'art-of-speech tagging with neural net-works, Prec.
COLING'94, Kyoto, Japan, pp.172-176, 1994.515
