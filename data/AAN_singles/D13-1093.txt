Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 908?913,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOnline Learning for Inexact Hypergraph SearchHao ZhangGooglehaozhang@google.comLiang Huang Kai ZhaoCity University of New York{lhuang@cs.qc,kzhao@gc}.cuny.eduRyan McDonaldGoogleryanmcd@google.comAbstractOnline learning algorithms like the percep-tron are widely used for structured predic-tion tasks.
For sequential search problems,like left-to-right tagging and parsing, beamsearch has been successfully combined withperceptron variants that accommodate searcherrors (Collins and Roark, 2004; Huang etal., 2012).
However, perceptron training withinexact search is less studied for bottom-upparsing and, more generally, inference overhypergraphs.
In this paper, we generalizethe violation-fixing perceptron of Huang etal.
(2012) to hypergraphs and apply it to thecube-pruning parser of Zhang and McDonald(2012).
This results in the highest reportedscores on WSJ evaluation set (UAS 93.50%and LAS 92.41% respectively) without the aidof additional resources.1 IntroductionStructured prediction problems generally deal withexponentially many outputs, often making exactsearch infeasible.
For sequential search problems,such as tagging and incremental parsing, beamsearch coupled with perceptron algorithms that ac-count for potential search errors have been shownto be a powerful combination (Collins and Roark,2004; Daume?
and Marcu, 2005; Zhang and Clark,2008; Huang et al 2012).
However, sequen-tial search algorithms, and in particular left-to-rightbeam search (Collins and Roark, 2004; Zhang andClark, 2008), squeeze inference into a very narrowspace.
To address this, Huang (2008) formulatedconstituency parsing as approximate bottom-up in-ference in order to compactly represent an exponen-tial number of outputs while scoring features of ar-bitrary scope.
This idea was adapted to graph-baseddependency parsers by Zhang and McDonald (2012)and shown to outperform left-to-right beam search.Both these examples, bottom-up approximate de-pendency and constituency parsing, can be viewedas specific instances of inexact hypergraph search.Typically, the approximation is accomplished bycube-pruning throughout the hypergraph (Chiang,2007).
Unfortunately, as the scope of features ateach node increases, the inexactness of search andits negative impact on learning can potentially be ex-acerbated.
Unlike sequential search, the impact onlearning of approximate hypergraph search ?
as wellas methods to mitigate any ill effects ?
has not beenstudied.
Motivated by this, we develop online learn-ing algorithms for inexact hypergraph search by gen-eralizing the violation-fixing percepron of Huang etal.
(2012).
We empirically validate the benefit ofthis approach within the cube-pruning dependencyparser of Zhang and McDonald (2012).2 Structured Perceptron for InexactHypergraph SearchThe structured perceptron algorithm (Collins, 2002)is a general learning algorithm.
Given training in-stances (x, y?
), the algorithm first solves the decod-ing problem y?
= argmaxy?Y(x)w ?
f(x, y) giventhe weight vector w for the high-dimensional fea-ture representation f of the mapping (x, y), wherey?
is the prediction under the current model, y?
is thegold output and Y(x) is the space of all valid outputsfor input x.
The perceptron update rule is simply:w?
= w + f(x, y?)
?
f(x, y?
).The convergence of original perceptron algorithmrelies on the argmax function being exact so thatthe conditionw ?f(x, y?)
> w ?f(x, y?)
(modulo ties)always holds.
This condition is called a violationbecause the prediction y?
scores higher than the cor-rect label y?.
Each perceptron update moves weights908A B C D E FG H I JK LMNFigure 1: A hypergraph showing the union of the goldand Viterbi subtrees.
The hyperedges in bold and dashedare from the gold and Viterbi trees, respectively.away from y?
and towards y?
to fix such violations.But when search is inexact, y?
could be suboptimalso that sometimes w ?
f(x, y?)
< w ?
f(x, y?).
Huanget al(2012) named such instances non-violationsand showed that perceptron model updates for non-violations nullify guarantees of convergence.
To ac-count for this, they generalized the original updaterule to select an output y?
within the pruned searchspace that scores higher than y?, but is not necessar-ily the highest among all possibilities, which repre-sents a true violation of the model on that traininginstance.
This violation fixing perceptron thus re-laxes the argmax function to accommodate inexactsearch and becomes provably convergent as a result.In the sequential cases where y?
has a linear struc-ture such as tagging and incremental parsing, theviolation fixing perceptron boils down to findingand updating along a certain prefix of y?.
Collinsand Roark (2004) locate the earliest position in achain structure where y?pref is worse than y?pref bya margin large enough to cause y?
to be droppedfrom the beam.
Huang et al(2012) locate the po-sition where the violation is largest among all pre-fixes of y?, where size of a violation is defined asw ?
f(x, y?pref) ?
w ?
f(x, y?pref).For hypergraphs, the notion of prefix must be gen-eralized to subtrees.
Figure 1 shows the packed-forest representation of the union of gold subtreesand highest-scoring (Viterbi) subtrees at every goldnode for an input.
At each gold node, there aretwo incoming hyperedges: one for the gold subtreeand the other for the Viterbi subtree.
After bottom-up parsing, we can compute the scores for the goldsubtrees as well as extract the corresponding Viterbisubtrees by following backpointers.
These Viterbisubtrees need not necessarily to belong to the fullViterbi path (i.e., the Viterbi tree rooted at node N ).An update strategy must choose a subtree or a set ofsubtrees at gold nodes.
This is to ensure that themodel is updating its weights relative to the inter-section of the search space and the gold path.Our first update strategy is called single-nodemax-violation (s-max).
Given a gold tree y?, it tra-verses the gold tree and finds the node n on whichthe violation between the Viterbi subtree and thegold subtree is the largest over all gold nodes.
Theviolation is guaranteed to be greater than or equal tozero because the lower bound for the max-violationon any hypergraph is 0 which happens at the leafnodes.
Then we choose the subtree pair (y?n, y?n) anddo the update similar to the prefix update for the se-quential case.
For example, in Figure 1, suppose themax-violation happens at node K , which covers theleft half of the input x, then the perceptron updatewould move parameters to the subtree representedby nodes B , C , H and K and away from A ,B , G and K .Our second update strategy is called parallel max-violation (p-max).
It is based on the observation thatviolations on non-overlapping nodes can be fixedin parallel.
We define a set of frontiers as a setof nodes that are non-overlapping and the union ofwhich covers the entire input string x.
The frontierset can include up to |x| nodes, in the case where thefrontier is equivalent to the set of leaves.
We traversey?
bottom-up to compute the set of frontiers suchthat each has the max-violation in the span it cov-ers.
Concretely, for each node n, the max-violationfrontier set can be defined recursively,ft(n) ={n, if n = maxv(n)?ni?children(n) ft(ni), otherwisewhere maxv(n) is the function that returns the nodewith the absolute maximum violation in the subtreerooted at n and can easily be computed recursivelyover the hypergraph.
To make a perceptron update,we generate the max-violation frontier set for the en-tire hypergraph and use it to choose subtree pairs?n?ft(root(x))(y?n, y?n), where root(x) is the root ofthe hypergraph for input x.
For example, in Figure 1,if the union of K and L satisfies the definition offt, then the perceptron update would move feature909weights away from the union of the two Viterbi sub-trees and towards their gold counterparts.In our experiments, we compare the performanceof the two violation-fixing update strategies againsttwo baselines.
The first baseline is the standard up-date, where updates always happen at the root nodeof a gold tree, even if the Viterbi tree at the root nodeleads to a non-violation update.
The second baselineis the skip update, which also always updates at theroot nodes but skips any non-violations.
This is thestrategy used by Zhang and McDonald (2012).3 ExperimentsWe ran a number of experiments on the cube-pruning dependency parser of Zhang and McDonald(2012), whose search space can be represented as ahypergraph in which the nodes are the complete andincomplete states and the hyperedges are the instan-tiations of the two parsing rules in the Eisner algo-rithm (Eisner, 1996).The feature templates we used are a superset ofZhang and McDonald (2012).
These features in-clude first-, second-, and third-order features andtheir labeled counterparts, as well as valency fea-tures.
In addition, we also included a feature tem-plate from Bohnet and Kuhn (2012).
This tem-plate examines the leftmost child and the rightmostchild of a modifier simultaneously.
All other high-order features of Zhang and McDonald (2012) onlylook at arcs on the same side of their head.
Wetrained the parser with hamming-loss-augmentedMIRA (Crammer et al 2006), following Martins etal.
(2010).
Based on results on the English valida-tion data, in all the experiments, we trained MIRAwith 8 epochs and used a beam of size 6 per node.To speed up the parser, we used an unlabeledfirst-order model to prune unlikely dependency arcsat both training and testing time (Koo and Collins,2010; Martins et al 2013).
We followed Rush andPetrov (2012) to train the first-order model to min-imize filter loss with respect to max-marginal filter-ing.
On the English validation corpus, the filteringmodel pruned 80% of arcs while keeping the oracleunlabeled attachment score above 99.50%.
Duringtraining only, we insert the gold tree into the hy-pergraph if it was mistakenly pruned.
This ensuresthat the gold nodes are always available, which isrequired for model updates.3.1 English and Chinese ResultsWe report dependency parsing results on the PennWSJ Treebank and the Chinese CTB-5 Treebank.Both treebanks are constituency treebanks.
We gen-erated two versions of dependency treebanks by ap-plying commonly-used conversion procedures.
Forthe first English version (PTB-YM), we used thePenn2Malt1 software to apply the head rules of Ya-mada and Matsumoto and the Malt label set.
Forthe second English version (PTB-S), we used theStanford dependency framework (De Marneffe etal., 2006) by applying version 2.0.5 of the Stan-ford parser.
We split the data in the standard way:sections 2-21 for training; section 22 for validation;and section 23 for evaluation.
We utilized a linearchain CRF tagger which has an accuracy of 96.9%on the validation data and 97.3% on the evaluationdata2.
For Chinese, we use the Chinese Penn Tree-bank converted to dependencies and split into train/-validation/evaluation according to Zhang and Nivre(2011).
We report both unlabeled attachment scores(UAS) and labeled attachment scores (LAS), ignor-ing punctuations (Buchholz and Marsi, 2006).Table 1 displays the results.
Our improvedcube-pruned parser represents a significant improve-ment over the feature-rich transition-based parser ofZhang and Nivre (2011) with a large beam size.
Italso improves over the baseline cube-pruning parserwithout max-violation update strategies (Zhang andMcDonald, 2012), showing the importance of up-date strategies in inexact hypergraph search.
TheUAS score on Penn-YM is slightly higher than thebest result known in the literature which was re-ported by the fourth-order unlabeled dependencyparser of Ma and Zhao (2012), although we didnot utilize fourth-order features.
The LAS score onPenn-YM is on par with the best reported by Bohnetand Kuhn (2012).
On Penn-S, there are not manyexisting results to compare with, due to the traditionof reporting results on Penn-YM in the past.
Never-theless, our result is higher than the second best bya large margin.
Our Chinese parsing scores are thehighest reported results.1http://stp.lingfil.uu.se//?nivre/research/Penn2Malt.html2The data was prepared by Andre?
F. T. Martins as was donein Martins et al(2013).910Penn-YM Penn-S CTB-5Parser UAS LAS Toks/Sec UAS LAS Toks/Sec UAS LAS Toks/SecZhang and Nivre (2011) 92.9- 91.8- ?680 - - - 86.0- 84.4- -Zhang and Nivre (reimpl.)
(beam=64) 93.00 91.98 800 92.96 90.74 500 85.93 84.42 700Zhang and Nivre (reimpl.)
(beam=128) 92.94 91.91 400 93.11 90.84 250 86.05 84.50 360Koo and Collins (2010) 93.04 - - - - - - - -Zhang and McDonald (2012) 93.06 91.86 220 - - - 86.87 85.19 -Rush and Petrov (2012) - - - 92.7- - 4460 - - -Martins et al(2013) 93.07 - 740 92.82 - 600 - - -Qian and Liu (2013) 93.17 - 180 - - - 87.25 - 100Bohnet and Kuhn (2012) 93.39 92.38 ?120 - - - 87.5- 85.9- -Ma and Zhao (2012) 93.4- - - - - - 87.4- - -cube-pruning w/ skip 93.21 92.07 300 92.92 90.35 200 86.95 85.23 200w/ s-max 93.50 92.41 300 93.59 91.17 200 87.78 86.13 200w/ p-max 93.44 92.33 300 93.64 91.28 200 87.87 86.24 200Table 1: Parsing results on test sets of the Penn Treebank and CTB-5.
UAS and LAS are measured on all tokens exceptpunctuations.
We also include the tokens per second numbers for different parsers whenever available, although thenumbers from other papers were obtained on different machines.
Speed numbers marked with ?
were converted fromsentences per second.The speed of our parser is around 200-300 tokensper second for English.
This is faster than the parserof Bohnet and Kuhn (2012) which has roughly thesame level of accuracy, but is slower than the parserof Martins et al(2013) and Rush and Petrov (2012),both of which only do unlabeled dependency pars-ing and are less accurate.
Given that predicting la-bels on arcs can slow down a parser by a constantfactor proportional to the size of the label set, thespeed of our parser is competitive.
We also tried toprune away arc labels based on observed labels foreach POS tag pair in the training data.
By doing so,we could speed up our parser to 500-600 tokens persecond with less than a 0.2% drop in both UAS andLAS.3.2 Importance of Update StrategiesThe lower portion of Table 1 compares cube-pruningparsing with different online update strategies in or-der to show the importance of choosing an updatestrategy that accommodates search errors.
The max-violation update strategies (s-max and p-max) im-proved results on both versions of the Penn Treebankas well as the CTB-5 Chinese treebank.
It madea larger difference on Penn-S relative to Penn-YM,improving as much as 0.93% in LAS against the skipupdate strategy.
Additionally, we measured the per-centage of non-violation updates at root nodes.
Inthe last epoch of training, on Penn-YM, there was24% non-violations if we used the skip update strat-egy; on Penn-S, there was 36% non-violations.
Theportion of non-violations indicates the inexactness9292.292.492.692.89393.293.493.693.8941  2  3  4  5  6  7  8UASepochsUAS on Penn-YM devs-maxp-maxskipstandardFigure 2: Constrast of different update strategies on thevalidation data set of Penn-YM.
The x-axis is the numberof training epochs.
The y-axis is the UAS score.
s-maxstands for single-node max-violation.
p-max stands forparallel max-violation.of the underlying search.
Search is harder on Penn-Sdue to the larger label set.
Thus, as expected, max-violation update strategies improve most where thesearch is the hardest and least exact.Figure 2 shows accuracy per training epoch on thevalidation data.
It can be seen that bad update strate-gies are not simply slow learners.
More iterationsof training cannot close the gap between strategies.Forcing invalid updates on non-violations (standardupdate) or simply ignoring them (skip update) pro-duces less accurate models overall.911ZN 2011 (reimpl.)
skip s-max p-max Best Published?Language UAS LAS UAS LAS UAS LAS UAS LAS UAS LASSPANISH 86.76 83.81 87.34 84.15 87.96 84.95 87.68 84.75 87.48 84.05CATALAN 94.00 88.65 94.54 89.14 94.58 89.05 94.98 89.56 94.07 89.09JAPANESE 93.10 91.57 93.40 91.65 93.26 91.67 93.20 91.49 93.72 91.7-BULGARIAN 93.08 89.23 93.52 89.25 94.02 89.87 93.80 89.65 93.50 88.23ITALIAN 87.31 82.88 87.75 83.41 87.57 83.22 87.79 83.59 87.47 83.50SWEDISH 90.98 85.66 90.64 83.89 91.62 85.08 91.62 85.00 91.44 85.42ARABIC 78.26 67.09 80.42 69.46 80.48 69.68 80.60 70.12 81.12 66.9-TURKISH 76.62 66.00 76.18 65.90 76.94 66.80 76.86 66.56 77.55 65.7-DANISH 90.84 86.65 91.40 86.59 91.88 86.95 92.00 87.07 91.86 84.8-PORTUGUESE 91.18 87.66 91.69 88.04 92.07 88.30 92.19 88.40 93.03 87.70GREEK 85.63 78.41 86.37 78.29 86.14 78.20 86.46 78.55 86.05 77.87SLOVENE 84.63 76.06 85.01 75.92 86.01 77.14 85.77 76.62 86.95 73.4-CZECH 87.78 82.38 86.92 80.36 88.36 82.16 88.48 82.38 90.32 80.2-BASQUE 79.65 71.03 79.57 71.43 79.59 71.52 79.61 71.65 80.23 73.18HUNGARIAN 84.71 80.16 85.67 80.84 85.85 81.02 86.49 81.67 86.81 81.86GERMAN 91.57 89.48 91.23 88.34 92.03 89.44 91.79 89.28 92.41 88.42DUTCH 82.49 79.71 83.01 79.79 83.57 80.29 83.35 80.09 86.19 79.2-AVG 86.98 81.55 87.33 81.56 87.76 82.08 87.80 82.14Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks.
When a language is in both years,we use the 2006 data set.
The best results with ?
are the maximum in the following papers: Buchholz and Marsi(2006), Nivre et al(2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al(2013), Forconsistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script.
ZN 2011 (reimpl.)
isour reimplementation of Zhang and Nivre (2011), with a beam of 64.
Results in bold are the best among ZN 2011reimplementation and different update strategies from this paper.3.3 CoNLL ResultsWe also report parsing results for 17 languages fromthe CoNLL 2006/2007 shared-task (Buchholz andMarsi, 2006; Nivre et al 2007).
The parser inour experiments can only produce projective depen-dency trees as it uses an Eisner algorithm backboneto generate the hypergraph (Eisner, 1996).
So, attraining time, we convert non-projective trees ?
ofwhich there are many in the CoNLL data ?
to projec-tive ones through flattening, i.e., attaching words tothe lowest ancestor that results in projective trees.
Attesting time, our parser can only predict projectivetrees, though we evaluate on the true non-projectivetrees.Table 2 shows the full results.
We sort thelanguages according to the percentage of non-projective trees in increasing order.
The Spanishtreebank is 98% projective, while the Dutch tree-bank is only 64% projective.
With respect to theZhang and Nivre (2011) baseline, we improved UASin 16 languages and LAS in 15 languages.
The im-provements are stronger for the projective languagesin the top rows.
We achieved the best publishedUAS results for 7 languages: Spanish, Catalan, Bul-garain, Italian, Swedish, Danish, and Greek.
Asthese languages are typically from the more projec-tive data sets, we speculate that extending the parserused in this study to handle non-projectivity willlead to state-of-the-art models for the majority oflanguages.4 ConclusionsWe proposed perceptron update strategies for in-exact hypergraph search and experimented witha cube-pruning dependency parser.
Both single-node max-violation and parallel max-violation up-date strategies signficantly improved parsing resultsover the strategy that ignores any invalid udpatescaused by inexactness of search.
The update strate-gies are applicable to any bottom-up parsing prob-lems such as constituent parsing (Huang, 2008) andsyntax-based machine translation with online learn-ing (Chiang et al 2008).Acknowledgments: We thank Andre?
F. T. Martinsfor the dependency converted Penn Treebank withautomatic POS tags from his experiments; the re-viewers for their useful suggestions; the NLP teamat Google for numerous discussions and comments;Liang Huang and Kai Zhao are supported in part byDARPA FA8750-13-2-0041 (DEFT), PSC-CUNY,and a Google Faculty Research Award.912ReferencesB.
Bohnet and J. Kuhn.
2012.
The best of bothworlds- a graph-based completion model for transition-basedparsers.
In Proc.
of EACL.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proc.
of EMNLP.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proc.
of ACL.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proc.
of ACL.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research.H.
Daume?
and D. Marcu.
2005.
Learning as searchoptimization: Approximate large margin methods forstructured prediction.
In Proc.
of ICML.M.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of LREC.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: an exploration.
In Proc.
of COL-ING.L.
Huang, S. Fayong, and G. Yang.
2012.
Structuredperceptron with inexact search.
In Proc.
of NAACL.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In Proc.
of ACL.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of ACL.X.
Ma and H. Zhao.
2012.
Fourth-order dependencyparsing.
In Proc.
of COLING.A.
F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,and M. A. T. Figueiredo.
2010.
Turbo parsers: Depen-dency parsing by approximate variational inference.In Proc.
of EMNLP.A.
F. T. Martins, M. B. Almeida, and N. A. Smith.
2013.Turning on the turbo: Fast third-order non-projectiveturbo parsers.
In Proc.
of ACL.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.
ofEMNLP-CoNLL.X.
Qian and Y. Liu.
2013.
Branch and bound algo-rithm for dependency parsing with non-local features.TACL, Vol 1.A.
Rush and S. Petrov.
2012.
Efficient multi-pass depen-dency pruning with vine parsing.
In Proc.
of NAACL.Y.
Zhang and S. Clark.
2008.
A Tale of TwoParsers: Investigating and Combining Graph-basedand Transition-based Dependency Parsing.
In Proc.of EMNLP.H.
Zhang and R. McDonald.
2012.
Generalized higher-order dependency parsing with cube pruning.
In Proc.of EMNLP.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proc.
ofACL-HLT, volume 2.913
