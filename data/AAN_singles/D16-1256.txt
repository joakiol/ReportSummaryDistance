Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2325?2330,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMorphological Segmentation Inside-OutRyan CotterellDepartment of Computer ScienceJohns Hopkins Universityryan.cotterell@jhu.eduArun KumarFaculty of Arts and HumanitiesUniversitat Oberta de CatalunyaHinrich Schu?tzeCISLMU MunichAbstractMorphological segmentation has traditionallybeen modeled with non-hierarchical models,which yield flat segmentations as output.
Inmany cases, however, proper morphologi-cal analysis requires hierarchical structure?especially in the case of derivational morphol-ogy.
In this work, we introduce a discrimina-tive joint model of morphological segmenta-tion along with the orthographic changes thatoccur during word formation.
To the bestof our knowledge, this is the first attempt toapproach discriminative segmentation with acontext-free model.
Additionally, we releasean annotated treebank of 7454 English wordswith constituency parses, encouraging futureresearch in this area.1 IntroductionIn NLP, supervised morphological segmentation hastypically been viewed as either a sequence labelingor a segmentation task (Ruokolainen et al, 2016).
Incontrast, we consider a hierarchical approach, em-ploying a context-free grammar (CFG).
CFGs pro-vide a richer model of morphology: they capture(i) the intuition that words themselves have internalconstituents, which belong to different categories,as well as (ii) the order in which affixes are at-tached.
Moreover, many morphological processes,e.g., compounding and reduplication, are best mod-eled as hierarchical; thus, context-free models areexpressively more appropriate.The purpose of morphological segmentation is todecompose words into smaller units, known as mor-phemes, which are typically taken to be the smallestmeaning bearing units in language.
This work con-cerns itself with modeling hierarchical structure overthese morphemes.
Note a simple flat morphologicalsegmentation can also be straightforwardly derivedfrom the CFG parse tree.
Segmentations have founduse in a diverse set of NLP applications, e.g., auto-matic speech recognition (Afify et al, 2006), key-word spotting (Narasimhan et al, 2014), machinetranslation (Clifton and Sarkar, 2011) and parsing(Seeker and C?etinog?lu, 2015).
In contrast to priorwork, we focus on canonical segmentation, i.e., weseek to jointly model orthographic changes and seg-mentation.
For instance, the canonical segmentationof untestably is un+test+able+ly, where we mapably to able+ly, restoring the letters le.We make two contributions: (i) We introduce ajoint model for canonical segmentation with a CFGbackbone.
We experimentally show that this modeloutperforms a semi-Markov model on flat segmenta-tion.
(ii) We release the first morphology treebank,consisting of 7454 English word types, each anno-tated with a full constituency parse.2 The Case For Hierarchical StructureWhy should we analyze morphology hierarchically?It is true that we can model much of morphol-ogy with finite-state machinery (Beesley and Kart-tunen, 2003), but there are, nevertheless, manycases where hierarchical structure appears requi-site.
For instance, the flat segmentation of the worduntestably7?un+test+able+ly is missing importantinformation about how the word was derived.
Thecorrect parse [[un[[test]able]]ly], on the other hand,does tell us that this is the order in which the com-2325WORDPREFIXunWORDWORDtestSUFFIXableSUFFIXly(a)WORDWORDWORDPREFIXunWORDtestSUFFIXableSUFFIXly(b)WORDWORDPREFIXunWORDlockSUFFIXable(c)WORDPREFIXunWORDWORDlockSUFFIXable(d)Figure 1: Canonical segmentation parse trees for untestably and unlockable.
For both words, the scope ofun is ambiguous.
Arguably, (a) is the only correct parse tree for untestably; the reading associated with (b)is hard to get.
On the other hand, unlockable is truly ambiguous between ?able to be unlocked?
(c) and?unable to be locked?
(d).plex form was derived:test able7??
?testable un7?
?untestable ly7?
?untestably.This gives us clear insight into the structure of thelexicon?we should expect that the segment testableexists as an independent word, but ably does not.Moreover, a flat segmentation is often semanti-cally ambiguous.
There are two potentially validreadings of untestably depending on how the nega-tive prefix un scopes.
The correct tree (see Figure 1)yields the reading ?in the manner of not able tobe tested?.
A second?likely infelicitous reading?where the segment untest forms a constituent yieldsthe reading ?in a manner of being able to untest?.Recovering the hierarchical structure allows us to se-lect the correct reading; note there are even cases oftrue ambiguity; e.g., unlockable has two readings:?unable to be locked?
and ?able to be unlocked?.Theoretical linguists often implicitly assume acontext-free treatment of word formation, e.g., byemploying brackets to indicate different levels ofaffixation.
Others have explicitly modeled word-internal structure with grammars (Selkirk, 1982;Marvin, 2002).3 Parsing the LexiconA novel component of this work is the developmentof a discriminative parser (Finkel et al, 2008; Hallet al, 2014) for morphology.
The goal is to define aprobability distribution over all trees that could arisefrom the input word, after reversal of orthographicand phonological processes.
We employ the simplegrammar shown in Table 1.
Despite its simplicity, itmodels the order in which morphemes are attached.More formally, our goal is to map a surface formw (e.g., w=untestably) into its underlying canonicalform u (e.g., u=untestablely) and then into a parsetree t over its morphemes.
We assume u,w ?
?
?,for some discrete alphabet ?.1 Note that a parsetree over the string implicitly defines a flat segmen-tation given our grammar?one can simply extractthe characters spanned by all preterminals in the re-sulting tree.
Before describing the joint model indetail, we first consider its pieces individually.3.1 Restoring Orthographic ChangesTo extract a canonical segmentation (Naradowskyand Goldwater, 2009; Cotterell et al, 2016), we re-store orthographic changes that occur during wordformation.
To this end, we define the score function,score?
(u,w) =?a?A(u,w)exp(g(u, a, w)>?
), (1)whereA(u,w) is the set of all monotonic alignmentsbetween the strings u and w. The goal is for score?to assign higher values to better matched pairs, e.g.,(w=untestably, u=untestablely).We have left these out of the equation for simplic-ity, but we refer to the reader Dreyer et al (2008) fora more thorough exposition.
To ensure that the par-tition function Z?
(w) is finite, we cap the maximumstring length u, which yieldsZ?
(w) =?u??
?|w|+kexp(g(u?, w)>?
), (2)1For efficiency, we assume u ?
?|w|+k, k = 5.2326ROOT ?
WORDWORD ?
PREFIX WORDWORD ?
WORD SUFFIXWORD ?
?+PREFIX ?
?+SUFFIX ?
?+Table 1: The context-free grammar used in this workto model word formation.
The productions closelyresemble those of Johnson et al (2006)?s AdaptorGrammar.where |w|+ k is the maximum length for u.For ease of computation, we can encode thisfunction as a weighted finite-state machine (WFST)(Mohri et al, 2002).
This requires, however, that thefeature function g factors over the topology of thefinite-state encoding.
Since our model conditions onthe word w, the feature function g can extract fea-tures from any part of this string.
Features on theoutput string, u, however, are more restricted.
In thiswork, we employ a bigram model over output char-acters.
This implies that each state remembers ex-actly one character, the previous one.
See Cotterellet al (2014) for details.
We can compute the scorefor two strings u and w using a weighted generaliza-tion of the Levenshtein algorithm.
Computing thepartition function requires a different dynamic pro-gram, which runs in O(|w|2 ?
|?|2) time.
Note thatsince |?| ?
26 (lower case English letters), it takesroughly 262 = 676 times longer to compute the par-tition function than to score a pair of strings.Our model includes several simple feature tem-plates, including features that fire on individual editactions as well as conjunctions of edit actions andcharacters in the surrounding context.
See Cotterellet al (2016) for details.3.2 Morphological Analysis as ParsingNext, we need to score an underlying canonicalform (e.g., u=untestablely) together with a parsetree (e.g., t=[[un[[test]able]]ly]).
Thus, we definethe parser scorescore?
(t, u) = exp??
?pi??
(t)f(pi, u)>???
, (3)where ?
(t) is the set of anchored productions inthe tree t. An anchored production pi is a grammarrule in Chomsky normal form attached to a span,e.g., Ai,k ?
Bi,jCj+1,k.
Each pi is then assigneda weight by the linear function f(pi, u)>?, wherethe function f extracts relevant features from the an-chored production as well as the corresponding spanof the underlying form u.
This model is typicallyreferred to as a weighted CFG (WCFG) (Smith andJohnson, 2007) or a CRF parser.Luckily, we can exploit the structure of the prob-lem to efficiently compute the partition function,Z?
(u) =?t?T (u)exp??
?pi??
(t)f(pi, u)>???
, (4)where T (u) is the set of all trees under the gram-mar that have yield u.
Specifically, we make use ofthe inside algorithm, which is just CKY (Aho andUllman, 1979) in the (+,?)
semiring (Goodman,1998), which runs inO(|G| ?
|u|3) time, where |G| isthe size of the grammar.For f , we define three span features: (i) indicatorfeatures on the span?s segment, (ii) an indicator fea-ture that fires if the segment appears in an externalcorpus2 and (iii) the conjunction of the segment withthe label (e.g., PREFIX) of the subtree root.
Follow-ing Hall et al (2014), we employ an indicator featurefor each production as well as production backofffeatures.4 A Joint ModelOur complete model is a joint CRF (Koller andFriedman, 2009) where each of the above scores arefactors.
We define the likelihood asp?
(t, u | w) =1Z?score?
(t, u) ?
score?
(u,w), (5)where ?
= {?,?}
is the parameter vector andZ?
=?u?????t??Tu?score?
(t?, u?)
?
score?
(u?, w) (6)is the partition function and Tu?
is set of all parsetrees for the string u?.
We see now that both WFSTand WCFG are just structured factors in the model.2We use the Wikipedia dump from 2016-05-01.2327The joint approach has the advantage that it allowsboth factors to work together to influence the choiceof the underlying form u.
This is useful as the parsernow has access to which words are attested in thelanguage; this helps guide the relatively weak trans-duction model.
On the downside, the partition func-tion Z?
now involves a sum over both all strings in?|w|+k and all possible parses of each string!
Infer-ence in this joint model is intractable, so we resortto approximate methods.4.1 Learning and InferenceWe use stochastic gradient descent to opti-mize the log-probability of the training data?Ni=1 log p?
(t(i), u(i) | w(i)); this requires the com-putation of the gradient of the partition function??
logZ?, which is intractable.
As in all CRFs, thisgradient is in fact an expectation:??
logZ?
= (7)E(u,t)?p?
[log (score?
(t, u) ?
score?
(u,w))] .To approximate this expectation, we use an impor-tance sampling routine.
Roughly speaking, we ap-proximate the hard-to-sample-from distribution p?by taking samples from an easy-to-sample-from pro-posal distribution q.
We then reweight the samplesusing the unnormalized score from p?.
Due to a lackof space, we omit the derivation of the approximategradient.4.2 DecodingWe also decode by importance sampling.
Given w,we sample canonical forms u and then run the CKYalgorithm to get the highest scoring tree.5 Related WorkWe believe our attempt to train discriminative gram-mars for morphology is novel.
Nevertheless, otherresearchers have described parsers for morphology.Most of this work is unsupervised: Johnson et al(2007) applied a Bayesian PCFG to unsupervisedmorphological segmentation.
Similarly, AdaptorGrammars (Johnson et al, 2006), a non-parametricBayesian generalization of PCFGs, have been ap-plied to the unsupervised version of the task (Bothaand Blunsom, 2013; Sirts and Goldwater, 2013).Relatedly, Schmid (2005) performed unsuperviseddisambiguation of a German morphological ana-lyzer (Schmid et al, 2004) using a PCFG, using theinside-outside algorithm (Baker, 1979).
Also, dis-criminative parsing approaches have been applied tothe related problem of Chinese word segmentation(Zhang et al, 2014).6 Morphological TreebankSupervised morphological segmentation has histor-ically been treated as a segmentation problem, de-void of hierarchical structure.
A core reason behindthis is that?to the best of our knowledge?there areno hierarchically annotated corpora for the task.
Toremedy this, we provide tree annotations for a sub-set of the English portion of CELEX (Baayen et al,1993).
We reannotated 7454 English types with afull constituency parse.3 The resource will be freelyavailable for future research.6.1 Annotation GuidelinesThe annotation of the morphology treebank wasguided by three core principles.
The first princi-ple concerns productivity: we exclusively anno-tate productive morphology.
In the context of mor-phology, productivity refers to the degree that na-tive speakers actively employ the affix to create newwords (Aronoff, 1976).
We believe that for NLP ap-plications, we should focus on productive affixation.Indeed, this sets our corpus apart from many existingmorphologically annotated corpora such as CELEX.For example, CELEX contains warmth7?warm+th,but th is not a productive suffix and cannot beused to create new words.
Thus, we do not wantto analyze hearth7?hear+th or, in general, allowwug7?wug+th.
Second, we annotate for semanticcoherence.
When there are several candidate parses,we choose the one that is best compatible with thecompositional semantics of the derived form.Interestingly, multiple trees can be consideredvalid depending on the linguistic tier of interest.Consider the word unhappier.
From a semantic per-spective, we have the parse [[un [happy]] er] whichgives us the correct meaning ?not happy to a greaterdegree?.
However, since the suffix er only attachesto mono- and bisyllabic words, we get [un[[happy]er]] from a phonological perspective.
In the linguis-3In many cases, we corrected the flat segmentation as well.2328Segmentation TreeMorph.
F1 Edit Acc.
Const.
F1Flat 78.89 (0.9) 0.72 (0.04) 72.88 (1.21) N/AHier 85.55 (0.6) 0.55 (0.03) 73.19 (1.09) 79.01 (0.5)Table 2: Results for the 10 splits of the treebank.
Segmentation quality is measured by morpheme F1, editdistance and accuracy; tree quality by constituent F1.tics literature, this problem is known as the brack-eting paradox (Pesetsky, 1985; Embick, 2015).
Weannotate exclusively at the syntactic-semantic tier.Thirdly, in the context of derivational morphol-ogy, we force spans to be words themselves.Since derivational morphology?by definition?forms new words from existing words (Lieber andS?tekauer, 2014), it follows that each span rootedwith WORD or ROOT in the correct parse corre-sponds to a word in the lexicon.
For example,consider unlickable.
The correct parse, under ourscheme, is [un [[lick] able]].
Each of the spans (lick,lickable and unlickable) exists as a word.
By con-trast, the parse [[un [lick]] able] contains the spanunlick, which is not a word in the lexicon.
Thespan in the segmented form may involve changes,e.g., [un [[achieve] able]], where achieveable is nota word, but achievable (after deleting e) is.7 ExperimentsWe run a simple experiment to show the empiri-cal utility of parsing words?we compare a WCFG-based canonical segmenter with the semi-Markovsegmenter introduced in Cotterell et al (2016).
Wedivide the corpus into 10 distinct train/dev/test splitswith 5454 words for train and 1000 for each of devand test.
We report three evaluation metrics: fullform accuracy, morpheme F1 (Van den Bosch andDaelemans, 1999) and average edit distance to thegold segmentation with boundaries marked by a dis-tinguished symbol.
For the WCFG model, we alsoreport constituent F1?typical for sentential con-stituency parsing?
as a baseline for future systems.This F1 measures how well we predict the wholetree (not just a segmentation).
For all models, we useL2 regularization and run 100 epochs of ADAGRAD(Duchi et al, 2011) with early stopping.
We tune theregularization coefficient by grid search considering?
?
{0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.7.1 Results and DiscussionTable 2 shows the results.
The hierarchical WCFGmodel outperforms the flat semi-Markov model onall metrics on the segmentation task.
This shows thatmodeling structure among the morphemes, indeed,does help segmentation.
The largest improvementsare found under the morpheme F1 metric (?
6.5points).
In contrast, accuracy improves by < 1%.Edit distance is in between with an improvement of0.2 characters.
Accuracy, in general, is an all ornothing metric since it requires getting every canon-ical segment correct.
Morpheme F1, on the otherhand, gives us partial credit.
Thus, what this showsus is that the WCFG gets a lot more of the mor-phemes in the held-out set correct, even if it onlygets a few complete forms correct.
We provide ad-ditional results evaluating the entire tree with con-stituency F1 as a future baseline.8 ConclusionWe presented a discriminative CFG-based model forcanonical morphological segmentation and showedempirical improvements on its ability to seg-ment words under three metrics.
We arguethat our hierarchical approach to modeling mor-phemes is more often appropriate than the tradi-tional flat segmentation.
Additionally, we haveannotated 7454 words with a morphological con-stituency parse.
The corpus is available onlineat http://ryancotterell.github.io/data/morphological-treebank to allow for exactcomparison and to spark future research.AcknowledgementsThe first author was supported by a DAAD Long-Term Research Grant and an NDSEG fellowship.The third author was supported by DFG (SCHU2246/10-1).2329ReferencesMohamed Afify, Ruhi Sarikaya, Hong-Kwang Jeff Kuo,Laurent Besacier, and Yuqing Gao.
2006.
On the useof morphological analysis for dialectal Arabic speechrecognition.
In INTERSPEECH.Alfred W Aho and Jeffrey Ullman.
1979.
Introduc-tion to Automata theory, Languages and Computation.Addison-Wesley.Mark Aronoff.
1976.
Word Formation in GenerativeGrammar.
MIT Press.R Harald Baayen, Richard Piepenbrock, and Rijn van H.1993.
The CELEX lexical data base on CD-ROM.James K Baker.
1979.
Trainable grammars for speechrecognition.
The Journal of the Acoustical Society ofAmerica, 65(S1):S132?S132.Kenneth R Beesley and Lauri Karttunen.
2003.
Finite-state Morphology: Xerox Tools and Techniques.
CSLI,Stanford.Jan A Botha and Phil Blunsom.
2013.
Adaptor gram-mars for learning non-concatenative morphology.
InEMNLP, pages 345?356.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In ACL.Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2014.Stochastic contextual edit distance and probabilisticFSTs.
In ACL.Ryan Cotterell, Tim Vieira, and Hinrich Schu?tze.
2016.A joint model of orthography and morphological seg-mentation.
In NAACL.Markus Dreyer, Jason R Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In EMNLP.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
JMLR, 12:2121?2159.David Embick.
2015.
The Morpheme: A TheoreticalIntroduction, volume 31.
Walter de Gruyter GmbH &Co KG.Jenny Rose Finkel, Alex Kleeman, and Christopher DManning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In ACL, volume 46, pages 959?967.Joshua Goodman.
1998.
Parsing Inside-out.
Ph.D. the-sis, Harvard University.David Leo Wright Hall, Greg Durrett, and Dan Klein.2014.
Less grammar, more features.
In ACL, pages228?237.Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-ter.
2006.
Adaptor grammars: A framework for spec-ifying compositional nonparametric Bayesian models.In NIPS, pages 641?648.Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via MarkovChain Monte Carlo.
In HLT-NAACL, pages 139?146.Daphne Koller and Nir Friedman.
2009.
ProbabilisticGraphical Models: Principles and Techniques.
MITpress.Rochelle Lieber and Pavol S?tekauer.
2014.
The OxfordHandbook of Derivational Morphology.
Oxford Uni-versity Press, USA.Tatjana Marvin.
2002.
Topics in the Stress and Syntax ofWords.
Ph.D. thesis, Massachusetts Institute of Tech-nology.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech & Language,16(1):69?88.Jason Naradowsky and Sharon Goldwater.
2009.
Im-proving morphology induction by learning spellingrules.
In IJCAI.Karthik Narasimhan, Damianos Karakos, RichardSchwartz, Stavros Tsakalidis, and Regina Barzilay.2014.
Morphological segmentation for keyword spot-ting.
In EMNLP.David Pesetsky.
1985.
Morphology and logical form.Linguistic Inquiry, 16(2):193?246.Teemu Ruokolainen, Oskar Kohonen, Kairit Sirts, Stig-Arne Gro?nroos, Mikko Kurimo, and Sami Virpioja.2016.
Comparative study of minimally supervisedmorphological segmentation.
Computational Linguis-tics.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.SMOR: A German computational morphology cover-ing derivation, composition and inflection.
In LREC.Helmut Schmid.
2005.
Disambiguation of morpholog-ical structure using a PCFG.
In EMNLP, pages 515?522.
Association for Computational Linguistics.Wolfgang Seeker and O?zlem C?etinog?lu.
2015.
A graph-based lattice dependency parser for joint morphologi-cal segmentation and syntactic analysis.
TACL.Elisabeth Selkirk.
1982.
The Syntax of Words.
Number 7in Linguistic Inquiry Monograph Series.
MIT Press.Kairit Sirts and Sharon Goldwater.
2013.
Minimally-supervised morphological segmentation using adaptorgrammars.
TACL, 1:255?266.Noah A Smith and Mark Johnson.
2007.
Weighted andprobabilistic context-free grammars are equally ex-pressive.
Computational Linguistics, 33(4):477?491.Antal Van den Bosch and Walter Daelemans.
1999.Memory-based morphological analysis.
In ACL.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2014.
Character-level Chinese dependency pars-ing.
In ACL, pages 1326?1336.2330
