Proceedings of the ACL 2010 Conference Short Papers, pages 220?224,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsIntelligent Selection of Language Model Training DataRobert C. Moore William LewisMicrosoft ResearchRedmond, WA 98052, USA{bobmoore,wilewis}@microsoft.comAbstractWe address the problem of selecting non-domain-specific language model trainingdata to build auxiliary language modelsfor use in tasks such as machine transla-tion.
Our approach is based on comparingthe cross-entropy, according to domain-specific and non-domain-specifc languagemodels, for each sentence of the textsource used to produce the latter languagemodel.
We show that this produces betterlanguage models, trained on less data, thanboth random data selection and two otherpreviously proposed methods.1 IntroductionStatistical N-gram language models are widelyused in applications that produce natural-languagetext as output, particularly speech recognition andmachine translation.
It seems to be a univer-sal truth that output quality can always be im-proved by using more language model trainingdata, but only if the training data is reasonablywell-matched to the desired output.
This presentsa problem, because in virtually any particular ap-plication the amount of in-domain data is limited.Thus it has become standard practice to com-bine in-domain data with other data, either bycombining N-gram counts from in-domain andother data (usually weighting the counts in someway), or building separate language models fromdifferent data sources, interpolating the languagemodel probabilities either linearly or log-linearly.Log-linear interpolation is particularly popularin statistical machine translation (e.g., Brants etal., 2007), because the interpolation weights caneasily be discriminatively trained to optimize anend-to-end translation objective function (such asBLEU) by making the log probability according toeach language model a separate feature function inthe overall translation model.The normal practice when using multiple lan-guages models in machine translation seems to beto train models on as much data as feasible fromeach source, and to depend on feature weight opti-mization to down-weight the impact of data that isless well-matched to the translation application.
Inthis paper, however, we show that for a data sourcethat is not entirely in-domain, we can improve thematch between the language model from that datasource and the desired application output by intel-ligently selecting a subset of the available data aslanguage model training data.
This not only pro-duces a language model better matched to the do-main of interest (as measured in terms of perplex-ity on held-out in-domain data), but it reduces thecomputational resources needed to exploit a largeamount of non-domain-specific data, since the re-sources needed to filter a large amount of data aremuch less (especially in terms of memory) thanthose required to build a language model from allthe data.2 Approaches to the ProblemOur approach to the problem assumes that we haveenough in-domain data to train a reasonable in-domain language model, which we then use tohelp score text segments from other data sources,and we select segments based on a score cutoff op-timized on held-out in-domain data.We are aware of two comparable previous ap-proaches.
Lin et al (1997) and Gao et al (2002)both used a method similar to ours, in which themetric used to score text segments is their perplex-ity according to the in-domain language model.The candidate text segments with perplexity lessthan some threshold are selected.The second previous approach does not explic-itly make use of an in-domain language model, butis still applicable to our scenario.
Klakow (2000)estimates a unigram language model from theentire non-domain-specific corpus to be selected220from, and scores each candidate text segment fromthat corpus by the change in the log likelihoodof the in-domain data according to the unigrammodel, if that segment were removed from the cor-pus used to estimate the unigram model.
Thosesegments whose removal would decrease the loglikelihood of the in-domain data more than somethreshold are selected.Our method is a fairly simple variant of scoringby perplexity according to an in-domain languagemodel.
First, note that selecting segments basedon a perplexity threshold is equivalent to selectingbased on a cross-entropy threshold.
Perplexity andcross-entropy are monotonically related, since theperplexity of a string s according to a model M issimply bHM (s), where HM (s) is the cross-entropyof s according to M and b is the base with re-spect to which the cross-entropy is measured (e.g.,bits or nats).
However, instead of scoring text seg-ments by perplexity or cross-entropy according tothe in-domain language model, we score them bythe difference of the cross-entropy of a text seg-ment according to the in-domain language modeland the cross-entropy of the text segment accord-ing to a language model trained on a random sam-ple of the data source from which the text segmentis drawn.To state this formally, let I be an in-domain dataset and N be a non-domain-specific (or otherwisenot entirely in-domain) data set.
Let HI(s) be theper-word cross-entropy, according to a languagemodel trained on I , of a text segment s drawn fromN .
Let HN (s) be the per-word cross-entropy of saccording to a language model trained on a ran-dom sample of N .
We partition N into text seg-ments (e.g., sentences), and score the segments ac-cording to HI(s) ?
HN (s), selecting all text seg-ments whose score is less than a threshold T .This method can be justified by reasoning sim-liar to that used to derive methods for trainingbinary text classifiers without labeled negativeexamples (Denis et al, 2002; Elkin and Noto,2008).
Let us imagine that our non-domain-specific corpus N contains an in-domain subcor-pus NI , drawn from the same distribution as ourin-domain corpus I .
Since NI is statistically justlike our in-domain data I , it would seem to be agood candidate for the data that we want to extractfrom N .
By a simple variant of Bayes rule, theprobability P (NI |s,N) of a text segment s, drawnrandomly from N , being in NI is given byP (NI |s,N) =P (s|NI , N)P (NI |N)P (s|N)Since NI is a subset of N , P (s|NI , N) =P (s|NI), and by our assumption about the rela-tionship of I and NI , P (s|NI) = P (s|I).
Hence,P (NI |s,N) =P (s|I)P (NI |N)P (s|N)If we could estimate all the probabilities in theright-hand side of this equation, we could use itto select text segments that have a high probabilityof being in NI .We can estimate P (s|I) and P (s|N) by train-ing language models on I and a sample of N , re-spectively.
That leaves us only P (NI |N), to es-timate, but we really don?t care what P (NI |N)is, because knowing that would still leave us won-dering what threshold to set on P (NI |s,N).
Wedon?t care about classification accuracy; we careonly about the quality of the resulting languagemodel, so we might as well just attempt to finda threshold on P (s|I)/P (s|N) that optimizes thefit of the resulting language model to held-out in-domain data.Equivalently, we can work in the log domainwith the quantity log(P (s|I)) ?
log(P (s|N)).This gets us very close to working with the differ-ence in cross-entropies, because HI(s)?HN (s) isjust a length-normalized version of log(P (s|I))?log(P (s|N)), with the sign reversed.
The rea-son that we need to normalize for length is thatthe value of log(P (s|I)) ?
log(P (s|N)) tends tocorrelate very strongly with text segment length.If the candidate text segments vary greatly inlength?e.g., if we partition N into sentences?this correlation can be a serious problem.We estimated this effect on a 1000-sentencesample of our experimental data described be-low, and found the correlation between sentencelog probability difference and sentence length tobe r = ?0.92, while the cross-entropy differ-ence was almost uncorrelated with sentence length(r = 0.04).
Hence, using sentence probability ra-tios or log probability differences as our scoringfunction would result in selecting disproportion-ately very short sentences.
We tested this in anexperiment not described here in detail, and foundit not to be significantly better as a selection crite-rion than random selection.221Corpus Sentence count Token countGigaword 133,310,562 3,445,946,266Europarl train 1,651,392 48,230,859Europarl test 2,000 55,566Table 1: Corpus size statistics3 ExperimentsWe have empirically evaluated our proposedmethod for selecting data from a non-domain-specific source to model text in a specific domain.For the in-domain corpus, we chose the Englishside of the English-French parallel text from re-lease v5 of the Europarl corpus (Koehn, 2005).This consists of proceedings of the European Par-liament from 1999 through 2009.
We used thetext from 1999 through 2008 as in-domain train-ing data, and we used the first 2000 sentencesfrom January 2009 as test data.
For the non-domain-specific corpus, we used the LDC Eng-lish Gigaword Third Edition (LDC Catalog No.
:LDC2007T07).We used a simple tokenization scheme on alldata, splitting on white space and on boundariesbetween alphanumeric and nonalphanumeric (e.g.,punctuation) characters.
With this tokenization,the sizes of our data sets in terms of sentences andtokens are shown in Table 1.
The token counts in-clude added end-of-sentence tokens.To implement our data selection method we re-quired one language model trained on the Europarltraining data and one trained on the Gigaworddata.
To make these language models comparable,and to show the feasibility of optimizing the fit tothe in-domain data without training a model on theentire Gigaword corpus, we trained the Gigawordlanguage model for data selection on a randomsample of the Gigaword corpus of a similar size tothat of the Europarl training data: 1,874,051 sen-tences, 48,459,945 tokens.To further increase the comparability of theseEuroparl and Gigaword language models, we re-stricted the vocabulary of both models to the to-kens appearing at least twice in the Europarl train-ing data, treating all other tokens as instances of<UNK>.
With this vocabulary, 4-gram languagemodels were trained on both the Europarl trainingdata and the Gigaword random sample using back-off absolute discounting (Ney et al 1994), with adiscount of 0.7 used for all N-gram lengths.
Thediscounted probability mass at the unigram levelwas added to the probability of <UNK>.
A countcutoff of 2 occurrences was applied to the trigramsand 4-grams in estimating these models.We computed the cross-entropy of each sen-tence in the Gigaword corpus according to bothmodels, and scored each sentence by the differ-ence in cross-entropy, HEp(s)?HGw(s).
We thenselected subsets of the Gigaword data correspond-ing to 8 cutoff points in the cross-entropy differ-ence scores, and trained 4-gram models (again us-ing absolute discounting with a discount of 0.7) oneach of these subsets and on the full Gigaword cor-pus.
These language models were estimated with-out restricting the vocabulary or applying countcutoffs, but the only parameters computed werethose needed to determine the perplexity of theheld-out Europarl test set, which saves a substan-tial amount of computation in determining the op-timal selection threshold.We compared our selection method to threeother methods.
As a baseline, we trained lan-guage models on random subsets of the Gigawordcorpus of approximately equal size to the datasets produced by the cutoffs we selected for thecross-entropy difference scores.
Next, we scoredall the Gigaword sentences by the cross-entropyaccording to the Europarl-trained model alone.As we noted above, this is equivalent to the in-domain perplexity scoring method used by Lin etal.
(1997) and Gao et al (2002).
Finally, we im-plemented Klakow?s (2000) method, scoring eachGigaword sentence by removing it from the Giga-word corpus and computing the difference in thelog likelihood of the Europarl corpus according tounigram models trained on the Gigaword corpuswith and without that sentence.
With the latter twomethods, we chose cutoff points in the resultingscores to produce data sets approximately equal insize to those obtained using our selection method.4 ResultsFor all four selection methods, plots of test set per-plexity vs. the number of training data tokens se-lected are displayed in Figure 1.
(Note that thetraining data token counts are displayed on a log-arithmic scale.)
The test set perplexity for the lan-guage model trained on the full Gigaword corpusis 135.
As we might expect, reducing trainingdata by random sampling always increases per-plexity.
Selecting Gigaword sentences by their2221001201401601802002202400.01 0.1 1 10Test-setperplexityBillions of words of training datarandom selectionin-domain cross-entropy scoringKlakow's methodcross-entropy difference scoringFigure 1: Test set perplexity vs. training set sizeSelection Method Original LM PPL Modified LM PPLin-domain cross-entropy scoring 124.4 124.8Klakow?s method 110.5 110.8cross-entropy difference scoring 100.7 101.9Table 2: Results adjusted for vocabulary coveragecross-entropy according to the Europarl-trainedmodel is effective in reducing both test set perplex-ity and training corpus size, with an optimum per-plexity of 124, obtained with a model built from36% of the Gigaword corpus.
Klakow?s methodis even more effective, with an optimum perplex-ity of 111, obtained with a model built from 21%of the Gigaword corpus.
The cross-entropy differ-ence selection method, however, is yet more effec-tive, with an optimum perplexity of 101, obtainedwith a model built from less than 7% of the Giga-word corpus.The comparisons implied by Figure 1, how-ever, are only approximate, because each perplex-ity (even along the same curve) is computed withrespect to a different vocabulary, resulting in a dif-ferent out-of-vocabulary (OOV) rate.
OOV tokensin the test data are excluded from the perplexitycomputation, so the perplexity measurements arenot strictly comparable.Out of the 55566 test set tokens, the numberof OOV tokens ranges from 418 (0.75%), for thesmallest training set based on in-domain cross-entropy scoring, to 20 (0.03%), for training onthe full Gigaword corpus.
If we consider onlythe training sets that appear to produce the lowestperplexity for each selection method, however, thespread of OOV counts is much narrower, ranging53 (0.10%) for best training set based on cross-entropy difference scoring, to 20 (0.03%), for ran-dom selection.To control for the difference in vocabulary, weestimated a modified 4-gram language model foreach selection method (other than random se-lection) using the training set that appeared toproduce the lowest perplexity for that selectionmethod in our initial experiments.
In the modifiedlanguage models, the unigram model based on theselected training set is smoothed by absolute dis-counting, and backed-off to an unsmoothed uni-gram model based on the full Gigaword corpus.This produces language models that are normal-ized over the same vocabulary as a model trainedon the full Gigaword corpus; thus the test set hasthe same OOVs for each model.Test set perplexity for each of these modifedlanguage models is compared to that of the orig-inal version of the model in Table 2.
It can beseen that adjusting the vocabulary in this way, sothat all models are based on the same vocabulary,223yields only very small changes in the measuredtest-set perplexity, and these differences are muchsmaller than the differences between the differentselection methods, whichever way the vocabularyof the language models is determined.5 ConclusionsThe cross-entropy difference selection method in-troduced here seems to produce language mod-els that are both a better match to texts in a re-stricted domain, and require less data for train-ing, than any of the other data selection methodstested.
This study is preliminary, however, in thatwe have not yet shown improved end-to-end taskperformance applying this approach, such as im-proved BLEU scores in a machine translation task.However, we believe there is reason to be opti-mistic about this.
When a language model trainedon non-domain-specific data is used in a statisti-cal translation model as a separate feature func-tion (as is often the case), lower perplexity on in-domain target language test data derived from ref-erence translations corresponds directly to assign-ing higher language model feature scores to thosereference translations, which should in turn lead totranslation system output that matches referencetranslations better.ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, FranzJ.
Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedingsof the Joint Conference on Empirical Methodsin Natural Language Processing and Computa-tional Natural Language Learning, June 28?30,Prague, Czech Republic, 858?867.Franc?ois Denis, Remi Gilleron, and Marc Tom-masi.
2002.
Text classification from positiveand unlabeled examples.
In The 9th Interna-tional Conference on Information Processingand Management of Uncertainty in Knowledge-Based Systems (IPMU 2002), 1927?1934.Charles Elkin and Keith Noto.
2008.
Learn-ing classifiers from only positive and unlabeleddata.
In KDD 2008, August 24?27, Las Vegas,Nevada, USA, 213?220.Jianfeng Gao, Joshua Goodman, Mingjing Li, andKai-Fu Lee.
2002.
Toward a unified approachto statistical language modeling for Chinese.ACM Transactions on Asian Language Informa-tion Processing, 1(1):3?33.Dietrich Klakow.
2000.
Selecting articles fromthe language model training corpus.
In ICASSP2000, June 5?9, Istanbul, Turkey, vol.
3, 1695?1698.Philipp Koehn.
2005.
Europarl: a parallel cor-pus for statistical machine translation.
In MTSummit X, September 12?16, Phuket, Thailand,79?86.Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,Ker-Jiann Chen, and Lin-Shan Lee.
1997.Chinese language model adaptation based ondocument classification and multiple domain-specific language models.
In EUROSPEECH-1997, 1463?1466.Hermann Ney, Ute Essen, and Reinhard Kneser.1994.
On structuring dependencies in stochas-tic language modelling.
Computer Speech andLanguage, 8:1?38.224
