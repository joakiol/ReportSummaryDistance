Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUniversal Dependency Annotation for Multilingual ParsingRyan McDonald?
Joakim Nivre??
Yvonne Quirmbach-Brundage?
Yoav Goldberg?
?Dipanjan Das?
Kuzman Ganchev?
Keith Hall?
Slav Petrov?
Hao Zhang?Oscar Ta?ckstro?m??
Claudia Bedini?
Nu?ria Bertomeu Castello??
Jungmee Lee?Google, Inc.?
Uppsala University?
Appen-Butler-Hill?
Bar-Ilan University?Contact: ryanmcd@google.comAbstractWe present a new collection of treebankswith homogeneous syntactic dependencyannotation for six languages: German,English, Swedish, Spanish, French andKorean.
To show the usefulness of such aresource, we present a case study of cross-lingual transfer parsing with more reliableevaluation than has been possible before.This ?universal?
treebank is made freelyavailable in order to facilitate research onmultilingual dependency parsing.11 IntroductionIn recent years, syntactic representations basedon head-modifier dependency relations betweenwords have attracted a lot of interest (Ku?bler etal., 2009).
Research in dependency parsing ?
com-putational methods to predict such representations?
has increased dramatically, due in large part tothe availability of dependency treebanks in a num-ber of languages.
In particular, the CoNLL sharedtasks on dependency parsing have provided overtwenty data sets in a standardized format (Buch-holz and Marsi, 2006; Nivre et al, 2007).While these data sets are standardized in termsof their formal representation, they are still hetero-geneous treebanks.
That is to say, despite themall being dependency treebanks, which annotateeach sentence with a dependency tree, they sub-scribe to different annotation schemes.
This caninclude superficial differences, such as the renam-ing of common relations, as well as true diver-gences concerning the analysis of linguistic con-structions.
Common divergences are found in the1Downloadable at https://code.google.com/p/uni-dep-tb/.analysis of coordination, verb groups, subordinateclauses, and multi-word expressions (Nilsson etal., 2007; Ku?bler et al, 2009; Zeman et al, 2012).These data sets can be sufficient if one?s goalis to build monolingual parsers and evaluate theirquality without reference to other languages, asin the original CoNLL shared tasks, but there aremany cases where heterogenous treebanks are lessthan adequate.
First, a homogeneous represen-tation is critical for multilingual language tech-nologies that require consistent cross-lingual anal-ysis for downstream components.
Second, consis-tent syntactic representations are desirable in theevaluation of unsupervised (Klein and Manning,2004) or cross-lingual syntactic parsers (Hwa etal., 2005).
In the cross-lingual study of McDonaldet al (2011), where delexicalized parsing modelsfrom a number of source languages were evalu-ated on a set of target languages, it was observedthat the best target language was frequently not theclosest typologically to the source.
In one stun-ning example, Danish was the worst source lan-guage when parsing Swedish, solely due to greatlydivergent annotation schemes.In order to overcome these difficulties, somecross-lingual studies have resorted to heuristics tohomogenize treebanks (Hwa et al, 2005; Smithand Eisner, 2009; Ganchev et al, 2009), but weare only aware of a few systematic attempts tocreate homogenous syntactic dependency anno-tation in multiple languages.
In terms of auto-matic construction, Zeman et al (2012) attemptto harmonize a large number of dependency tree-banks by mapping their annotation to a version ofthe Prague Dependency Treebank scheme (Hajic?et al, 2001; Bo?hmova?
et al, 2003).
Addition-ally, there have been efforts to manually or semi-manually construct resources with common syn-92tactic analyses across multiple languages using al-ternate syntactic theories as the basis for the repre-sentation (Butt et al, 2002; Helmreich et al, 2004;Hovy et al, 2006; Erjavec, 2012).In order to facilitate research on multilingualsyntactic analysis, we present a collection of datasets with uniformly analyzed sentences for six lan-guages: German, English, French, Korean, Span-ish and Swedish.
This resource is freely avail-able and we plan to extend it to include more dataand languages.
In the context of part-of-speechtagging, universal representations, such as that ofPetrov et al (2012), have already spurred numer-ous examples of improved empirical cross-lingualsystems (Zhang et al, 2012; Gelling et al, 2012;Ta?ckstro?m et al, 2013).
We aim to do the same forsyntactic dependencies and present cross-lingualparsing experiments to highlight some of the bene-fits of cross-lingually consistent annotation.
First,results largely conform to our expectations ofwhich target languages should be useful for whichsource languages, unlike in the study of McDon-ald et al (2011).
Second, the evaluation scoresin general are significantly higher than previouscross-lingual studies, suggesting that most of thesestudies underestimate true accuracy.
Finally, un-like all previous cross-lingual studies, we can re-port full labeled accuracies and not just unlabeledstructural accuracies.2 Towards A Universal TreebankThe Stanford typed dependencies for English(De Marneffe et al, 2006; de Marneffe and Man-ning, 2008) serve as the point of departure for our?universal?
dependency representation, togetherwith the tag set of Petrov et al (2012) as the under-lying part-of-speech representation.
The Stanfordscheme, partly inspired by the LFG framework,has emerged as a de facto standard for depen-dency annotation in English and has recently beenadapted to several languages representing different(and typologically diverse) language groups, suchas Chinese (Sino-Tibetan) (Chang et al, 2009),Finnish (Finno-Ugric) (Haverinen et al, 2010),Persian (Indo-Iranian) (Seraji et al, 2012), andModern Hebrew (Semitic) (Tsarfaty, 2013).
Itswidespread use and proven adaptability makes it anatural choice for our endeavor, even though ad-ditional modifications will be needed to capturethe full variety of grammatical structures in theworld?s languages.Alexandre re?side avec sa famille a` Tinqueux .NOUN VERB ADP DET NOUN ADP NOUN PNSUBJ ADPMODADPOBJPOSSADPMODADPOBJPFigure 1: A sample French sentence.We use the so-called basic dependencies (withpunctuation included), where every dependencystructure is a tree spanning all the input tokens,because this is the kind of representation that mostavailable dependency parsers require.
A sampledependency tree from the French data set is shownin Figure 1.
We take two approaches to generat-ing data.
The first is traditional manual annotation,as previously used by Helmreich et al (2004) formultilingual syntactic treebank construction.
Thesecond, used only for English and Swedish, is toautomatically convert existing treebanks, as in Ze-man et al (2012).2.1 Automatic ConversionSince the Stanford dependencies for English aretaken as the starting point for our universal annota-tion scheme, we begin by describing the data setsproduced by automatic conversion.
For English,we used the Stanford parser (v1.6.8) (Klein andManning, 2003) to convert the Wall Street Jour-nal section of the Penn Treebank (Marcus et al,1993) to basic dependency trees, including punc-tuation and with the copula verb as head in cop-ula constructions.
For Swedish, we developed aset of deterministic rules for converting the Tal-banken part of the Swedish Treebank (Nivre andMegyesi, 2007) to a representation as close as pos-sible to the Stanford dependencies for English.This mainly consisted in relabeling dependencyrelations and, due to the fine-grained label set usedin the Swedish Treebank (Teleman, 1974), thiscould be done with high precision.
In addition,a small number of constructions required struc-tural conversion, notably coordination, which inthe Swedish Treebank is given a Prague style anal-ysis (Nilsson et al, 2007).
For both English andSwedish, we mapped the language-specific part-of-speech tags to universal tags using the map-pings of Petrov et al (2012).2.2 Manual AnnotationFor the remaining four languages, annotators weregiven three resources: 1) the English Stanford93guidelines; 2) a set of English sentences with Stan-ford dependencies and universal tags (as above);and 3) a large collection of unlabeled sentencesrandomly drawn from newswire, weblogs and/orconsumer reviews, automatically tokenized with arule-based system.
For German, French and Span-ish, contractions were split, except in the case ofclitics.
For Korean, tokenization was more coarseand included particles within token units.
Annota-tors could correct this automatic tokenization.The annotators were then tasked with producinglanguage-specific annotation guidelines with theexpressed goal of keeping the label and construc-tion set as close as possible to the original Englishset, only adding labels for phenomena that do notexist in English.
Making fine-grained label dis-tinctions was discouraged.
Once these guidelineswere fixed, annotators selected roughly an equalamount of sentences to be annotated from each do-main in the unlabeled data.
As the sentences werealready randomly selected from a larger corpus,annotators were told to view the sentences in or-der and to discard a sentence only if it was 1) frag-mented because of a sentence splitting error; 2) notfrom the language of interest; 3) incomprehensibleto a native speaker; or 4) shorter than three words.The selected sentences were pre-processed usingcross-lingual taggers (Das and Petrov, 2011) andparsers (McDonald et al, 2011).The annotators modified the pre-parsed trees us-ing the TrEd2 tool.
At the beginning of the annota-tion process, double-blind annotation, followed bymanual arbitration and consensus, was used itera-tively for small batches of data until the guidelineswere finalized.
Most of the data was annotatedusing single-annotation and full review: one an-notator annotating the data and another reviewingit, making changes in close collaboration with theoriginal annotator.
As a final step, all annotateddata was semi-automatically checked for annota-tion consistency.2.3 HarmonizationAfter producing the two converted and four an-notated data sets, we performed a harmonizationstep, where the goal was to maximize consistencyof annotation across languages.
In particular, wewanted to eliminate cases where the same labelwas used for different linguistic relations in dif-ferent languages and, conversely, where one and2Available at http://ufal.mff.cuni.cz/tred/.the same relation was annotated with different la-bels, both of which could happen accidentally be-cause annotators were allowed to add new labelsfor the language they were working on.
Moreover,we wanted to avoid, as far as possible, labels thatwere only used in one or two languages.In order to satisfy these requirements, a numberof language-specific labels were merged into moregeneral labels.
For example, in analogy with thenn label for (element of a) noun-noun compound,the annotators of German added aa for compoundadjectives, and the annotators of Korean added vvfor compound verbs.
In the harmonization step,these three labels were merged into a single labelcompmod for modifier in compound.In addition to harmonizing language-specific la-bels, we also renamed a small number of relations,where the name would be misleading in the uni-versal context (although quite appropriate for En-glish).
For example, the label prep (for a mod-ifier headed by a preposition) was renamed adp-mod, to make clear the relation to other modifierlabels and to allow postpositions as well as prepo-sitions.3 We also eliminated a few distinctions inthe original Stanford scheme that were not anno-tated consistently across languages (e.g., mergingcomplm with mark, number with num, and purpclwith advcl).The final set of labels is listed with explanationsin Table 1.
Note that relative to the universal part-of-speech tagset of Petrov et al (2012) our finallabel set is quite rich (40 versus 12).
This is duemainly to the fact that the the former is based ondeterministic mappings from a large set of annota-tion schemes and therefore reduced to the granu-larity of the greatest common denominator.
Such areduction may ultimately be necessary also in thecase of dependency relations, but since most of ourdata sets were created through manual annotation,we could afford to retain a fine-grained analysis,knowing that it is always possible to map fromfiner to coarser distinctions, but not vice versa.42.4 Final Data SetsTable 2 presents the final data statistics.
The num-ber of sentences, tokens and tokens/sentence vary3Consequently, pobj and pcomp were changed to adpobjand adpcomp.4The only two data sets that were created through con-version in our case were English, for which the Stanford de-pendencies were originally defined, and Swedish, where thenative annotation happens to have a fine-grained label set.94Label Descriptionacomp adjectival complementadp adpositionadpcomp complement of adpositionadpmod adpositional modifieradpobj object of adpositionadvcl adverbial clause modifieradvmod adverbial modifieramod adjectival modifierappos appositiveattr attributeaux auxiliaryauxpass passive auxiliarycc conjunctionccomp clausal complementLabel Descriptioncompmod compound modifierconj conjunctcop copulacsubj clausal subjectcsubjpass passive clausal subjectdep genericdet determinerdobj direct objectexpl expletiveinfmod infinitival modifieriobj indirect objectmark markermwe multi-word expressionneg negationLabel Descriptionnmod noun modifiernsubj nominal subjectnsubjpass passive nominal subjectnum numeric modifierp punctuationparataxis parataxispartmod participial modifierposs possessiveprt verb particlercmod relative clause modifierrel relativexcomp open clausal complementTable 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).source(s) # sentences # tokensDE N, R 4,000 59,014EN PTB?
43,948 1,046,829SV STB?
6,159 96,319ES N, B, R 4,015 112,718FR N, B, R 3,978 90,000KO N, B 6,194 71,840Table 2: Data set statistics.
?Automatically con-verted WSJ section of the PTB.
The data releaseincludes scripts to generate this data, not the dataitself.
?Automatically converted Talbanken sec-tion of the Swedish Treebank.
N=News, B=Blogs,R=Consumer Reviews.due to the source and tokenization.
For example,Korean has 50% more sentences than Spanish, but?40k less tokens due to a more coarse-grained to-kenization.
In addition to the data itself, anno-tation guidelines and harmonization rules are in-cluded so that the data can be regenerated.3 ExperimentsOne of the motivating factors in creating such adata set was improved cross-lingual transfer eval-uation.
To test this, we use a cross-lingual transferparser similar to that of McDonald et al (2011).In particular, it is a perceptron-trained shift-reduceparser with a beam of size 8.
We use the featuresof Zhang and Nivre (2011), except that all lexicalidentities are dropped from the templates duringtraining and testing, hence inducing a ?delexical-ized?
model that employs only ?universal?
proper-ties from source-side treebanks, such as part-of-speech tags, labels, head-modifier distance, etc.We ran a number of experiments, which can beseen in Table 3.
For these experiments we ran-domly split each data set into training, develop-ment and testing sets.5 The one exception is En-glish, where we used the standard splits.
Eachrow in Table 3 represents a source training lan-guage and each column a target evaluation lan-guage.
We report both unlabeled attachment score(UAS) and labeled attachment score (LAS) (Buch-holz and Marsi, 2006).
This is likely the first re-liable cross-lingual parsing evaluation.
In partic-ular, previous studies could not even report LASdue to differences in treebank annotations.We can make several interesting observations.Most notably, for the Germanic and Romance tar-get languages, the best source language is fromthe same language group.
This is in stark contrastto the results of McDonald et al (2011), who ob-serve that this is rarely the case with the heteroge-nous CoNLL treebanks.
Among the Germaniclanguages, it is interesting to note that Swedishis the best source language for both German andEnglish, which makes sense from a typologicalpoint of view, because Swedish is intermediate be-tween German and English in terms of word or-der properties.
For Romance languages, the cross-lingual parser is approaching the accuracy of thesupervised setting, confirming that for these lan-guages much of the divergence is lexical and notstructural, which is not true for the Germanic lan-guages.
Finally, Korean emerges as a very clearoutlier (both as a source and as a target language),which again is supported by typological consider-ations as well as by the difference in tokenization.With respect to evaluation, it is interesting tocompare the absolute numbers to those reportedin McDonald et al (2011) for the languages com-5These splits are included in the release of the data.95SourceTrainingLanguageTarget Test LanguageUnlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)Germanic Romance Germanic RomanceDE EN SV ES FR KO DE EN SV ES FR KODE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85Table 3: Cross-lingual transfer parsing results.
Bolded are the best per target cross-lingual result.mon to both studies (DE, EN, SV and ES).
In thatstudy, UAS was in the 38?68% range, as comparedto 55?75% here.
For Swedish, we can even mea-sure the difference exactly, because the test setsare the same, and we see an increase from 58.3%to 70.6%.
This suggests that most cross-lingualparsing studies have underestimated accuracies.4 ConclusionWe have released data sets for six languages withconsistent dependency annotation.
After the ini-tial release, we will continue to annotate data inmore languages as well as investigate further au-tomatic treebank conversions.
This may also leadto modifications of the annotation scheme, whichshould be regarded as preliminary at this point.Specifically, with more typologically and morpho-logically diverse languages being added to the col-lection, it may be advisable to consistently en-force the principle that content words take func-tion words as dependents, which is currently vi-olated in the analysis of adpositional and copulaconstructions.
This will ensure a consistent analy-sis of functional elements that in some languagesare not realized as free words or are not obliga-tory, such as adpositions which are often absentdue to case inflections in languages like Finnish.
Itwill also allow the inclusion of language-specificfunctional or morphological markers (case mark-ers, topic markers, classifiers, etc.)
at the leaves ofthe tree, where they can easily be ignored in appli-cations that require a uniform cross-lingual repre-sentation.
Finally, this data is available on an opensource repository in the hope that the communitywill commit new data and make corrections to ex-isting annotations.AcknowledgmentsMany people played critical roles in the pro-cess of creating the resource.
At Google, Fer-nando Pereira, Alfred Spector, Kannan Pashu-pathy, Michael Riley and Corinna Cortes sup-ported the project and made sure it had the re-quired resources.
Jennifer Bahk and Dave Orrhelped coordinate the necessary contracts.
AndreaHeld, Supreet Chinnan, Elizabeth Hewitt, Tu Tsaoand Leigha Weinberg made the release processsmooth.
Michael Ringgaard, Andy Golding, TerryKoo, Alexander Rush and many others providedtechnical advice.
Hans Uszkoreit gave us per-mission to use a subsample of sentences from theTiger Treebank (Brants et al, 2002), the source ofthe news domain for our German data set.
Anno-tations were additionally provided by Sulki Kim,Patrick McCrae, Laurent Alamarguy and He?ctorFerna?ndez Alcalde.ReferencesAlena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and BarboraHladka?.
2003.
The Prague Dependency Treebank:A three-level annotation scenario.
In Anne Abeille?,editor, Treebanks: Building and Using Parsed Cor-pora, pages 103?127.
Kluwer.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERTreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL.Miriam Butt, Helge Dyvik, Tracy Holloway King,Hiroshi Masuichi, and Christian Rohrer.
2002.The parallel grammar project.
In Proceedings ofthe 2002 workshop on Grammar engineering andevaluation-Volume 15.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminativereordering with Chinese grammatical relations fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation (SSST-3)at NAACL HLT 2009.96Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of ACL-HLT.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation.Marie-Catherine De Marneffe, Bill MacCartney, andChris D. Manning.
2006.
Generating typed depen-dency parses from phrase structure parses.
In Pro-ceedings of LREC.Tomaz Erjavec.
2012.
MULTEXT-East: Morphosyn-tactic resources for Central and Eastern Europeanlanguages.
Language Resources and Evaluation,46:131?142.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar inductionvia bitext projection constraints.
In Proceedings ofACL-IJCNLP.Douwe Gelling, Trevor Cohn, Phil Blunsom, and JoaoGrac?a.
2012.
The pascal challenge on grammar in-duction.
In Proceedings of the NAACL-HLT Work-shop on the Induction of Linguistic Structure.Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,Eva Hajic?ova?, Petr Sgall, and Petr Pajas.
2001.Prague Dependency Treebank 1.0.
LDC, 2001T10.Katri Haverinen, Timo Viljanen, Veronika Laippala,Samuel Kohonen, Filip Ginter, and Tapio Salakoski.2010.
Treebanking finnish.
In Proceedings ofThe Ninth International Workshop on Treebanks andLinguistic Theories (TLT9).Stephen Helmreich, David Farwell, Bonnie Dorr, NizarHabash, Lori Levin, Teruko Mitamura, FlorenceReeder, Keith Miller, Eduard Hovy, Owen Rambow,and Advaith Siddharthan.
2004.
Interlingual anno-tation of multilingual text corpora.
In Proceedingsof the HLT-EACL Workshop on Frontiers in CorpusAnnotation.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of NAACL.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(03):311?325.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL.Dan Klein and Chris D. Manning.
2004.
Corpus-basedinduction of syntactic structure: models of depen-dency and constituency.
In Proceedings of ACL.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan and Claypool.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn treebank.
Compu-tational Linguistics, 19(2):313?330.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of EMNLP.Jens Nilsson, Joakim Nivre, and Johan Hall.
2007.Generalizing tree transformations for inductive de-pendency parsing.
In Proceedings of ACL.Joakim Nivre and Bea?ta Megyesi.
2007.
Bootstrap-ping a Swedish treebank using cross-corpus harmo-nization and annotation projection.
In Proceedingsof the 6th International Workshop on Treebanks andLinguistic Theories.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task ondependency parsing.
In Proceedings of EMNLP-CoNLL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC.Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.2012.
Bootstrapping a Persian dependency tree-bank.
Linguistic Issues in Language Technology,7(18):1?10.David A. Smith and Jason Eisner.
2009.
Parser adap-tation and projection with quasi-synchronous gram-mar features.
In Proceedings of EMNLP.Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the ACL.Ulf Teleman.
1974.
Manual fo?r grammatisk beskrivn-ing av talad och skriven svenska.
Studentlitteratur.Reut Tsarfaty.
2013.
A unified morpho-syntacticscheme of stanford dependencies.
Proceedings ofACL.Daniel Zeman, David Marecek, Martin Popel,Loganathan Ramasamy, Jan S?tepa?nek, Zdene?kZ?abokrtsky`, and Jan Hajic.
2012.
Hamledt: Toparse or not to parse.
In Proceedings of LREC.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL-HLT.Yuan Zhang, Roi Reichart, Regina Barzilay, and AmirGloberson.
2012.
Learning to map into a universalpos tagset.
In Proceedings of EMNLP.97
