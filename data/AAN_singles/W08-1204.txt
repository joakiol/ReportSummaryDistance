Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 17?23Manchester, August 2008Human judgment as a parameter in evaluation campaignsJean-Baptiste Berthelin and Cyril Grouin and Martine Hurault-Plantet and Patrick ParoubekLIMSI-CNRSBP 133F-91403 Orsay Cedexfirstname.lastname@limsi.frAbstractThe relevance of human judgment in anevaluation campaign is illustrated herethrough the DEFT text mining campaigns.In a first step, testing a topic for a cam-paign among a limited number of humanevaluators informs us about the feasibilityof a task.
This information comes from theresults obtained by the judges, as well asfrom their personal impressions after pass-ing the test.In a second step, results from individualjudges, as well as their pairwise matching,are used in order to adjust the task (choiceof a marking scale for DEFT?07 and selec-tion of topical categories for DEFT?08).Finally, the mutual comparison of com-petitors?
results, at the end of the evalu-ation campaign, confirms the choices wemade at its starting point, and providesmeans to redefine the task when we shalllaunch a future campaign based on thesame topic.1 IntroductionFor the past four years, the DEFT1 (De?fi Fouillede Texte) campaigns have been aiming to evalu-ate methods and software developed by several re-search teams in French text mining, on a variety oftopics.The different editions concerned, in this or-der, the identification of speakers in politicalc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1See http://deft.limsi.fr/ for a presentation inFrench.speeches (2005), the topical segmentation of po-litical, scientific and juridical corpora (2006), theautomatic affectation of opinion values to texts de-veloping an argumented judgment (2007), and theidentification of the genre and topic of a docu-ment (2008).Human judgment was used during the prepara-tion of the last two campaigns, to assess the dif-ficulty of the task, and to see which parameterscould be modified.
To do this, before the partic-ipants start competing via their software, we puthuman judges in front of versions of the task withvarious sets of parameters.
This allows us to adjustthe definition of the task according to which diffi-culties were encountered, and how judges agree to-gether.
These human judges are in small number,and belong to our team.
However, results of thecampaign are automatically evaluated with refer-ence to results attached to the corpus from the start.This is because the evaluation of a campaign?sresults by human judges is expensive.
For in-stance, TREC2 international evaluation campaignsare supported by the NIST institute and funded bystate agencies.
In Europe, on the same domains,the CLEF3 campaigns are funded by the Euro-pean Commission, and in France, evaluation cam-paigns are also funded by projects, such as Tech-nolangue4.
DEFT campaigns, however, are con-ducted with small budgets.
That means for us tohave selected corpora that contain the desired re-sults.
For instance, in a campaign for topical cat-egorization, we must start with a topically taggedcorpus.
By so doing, we also can, at the end ofa campaign, compare results from human judgeswith results from competitors, using an identical2http://trec.nist.gov3http://www.clef-campaign.org4http://www.technolangue.net17common reference.In this paper, we describe experiments we per-formed with human judgments when preparingDEFT campaigns.
We survey the various stepsin the preparation of the last two campaigns, andwe go through the detail of how human evalua-tion, performed during these steps, led us to theparametrization of these two campaigns.
We alsopresent a comparative analysis of results found byhuman judges and results submitted by competi-tors in the challenge.
We conclude about the rel-evance of the human evaluation of a task, prior toevaluating software dedicated to this task.2 Parametrization of the campaignWe were competitors in the 2005 and 2006 edi-tions, and became organisators for the 2007 and2008 campaigns.
For both challenges that we or-ganized, we went through the classical steps of theevaluation paradigm (Adda et al, 1999), to whichwe systematically added a step of human test of thetask, in order to adjust those parameters that couldbe modified.
The steps, therefore, are following:1. thinking about potential topics;2. choice of a task and collection of corpora;3. choice of measurements;4. test of the task by human judges on an extractof the corpus in order to precisely define itsparameters;5. launching the task, recruiting participants;6. testing period;7. adjudication: possibility of complaints aboutthe results;8. workshop that closes the campaign.Whenever human judges have to evaluate theresults of participants in a campaign, the mainproblems are about correctly defining the judgingcriteria to be applied by judges, and that judgesbe in sufficient number to vote on judging eachdocument.
Hovy et al (2002) describe work to-ward formalization of software evaluation method-ology in NLP, developed in the EAGLES5 and5http://www.ilc.cnr.it/EAGLES96/home.htmlISLE6 projects.
For cost-efficiency reasons, au-tomatic evaluation is relevant, and its results havesometimes been compared to results from humanjudges.
For instance, Eck and Hori (2005) com-pare results of evaluation measurements used inautomatic translation with human judgments onthe same corpus.
In (Burstein and Wolska, 2003),the authors describe an experiment in the evalua-tion of writing style and find a better agreementbetween the automatic evaluation system and onehuman judge, than between two human judges.Returning to the DEFT campaign, once the taskis chosen, the corpora are collected, and evaluationmeasurements are defined, there can remain somenecessity of adjusting parameters, according to theexpected difficulty of the task.
This could be, forinstance, the level of granularity in a task of top-ical segmentation, or which categories should berelevant in a task of categorization.
To get this ad-justing done, we submit the task to human judges.In 2007, the challenge was about the automaticaffectation of opinion values to texts developing anargumented judgment (Grouin et al, 2007).
Wecollected opinion texts already tagged by an opin-ion value, such as film reviews that, in additionto a text giving the judgment of the critic on thefilm, also feature a mark in the shape of a variablenumber of stars.
The adjustable parameter of thetask, therefore, is the scale of opinion values.
Thetask will be more or less difficult, according to therange of this scale.The 2008 campaign was about classifying a setof documents by genre and topic (Hurault-Plantetet al, 2008).
The choice of genres and topics isa crucial one.
Some pairs of topics or genres aremore difficult to separate than other ones.
We alsohad to find different genres sharing a set of topicalcategories, while corpora in French are not so veryabundant.
So we selected two genres, encyclo-pedia and daily newspaper, and about ten generaltopical categories.
The parameter we had to ad-just was the set of categories to be matched againsteach other.3 Assessing the difficulty of a task3.1 Calibration of an opinion value scaleIn 2007, the challenge was about the automatic af-fectation of opinion values to texts developing anargumented judgment.
In view of that, we col-lected four corpora that covered various domains:6http://www.ilc.cnr.it/EAGLES96/isle/18reviews of films and books, of video games and ofscientific papers, as well as parliamentary debatesabout a draft law.Each corpus had the interesting feature of com-bining a mark or opinion with a descriptive text, asthe mark was used to sum up the judment in theargumentative part of this text.
Due to the diver-sity of sources, we found as many marking scalesas involved copora:?
2 values for parliamentary debates7 (the rep-resentative who took part in the debate waseither in favour or in disfavour of the draftlaw) ;?
4 values for scientific paper reviews (acceptedas it stands ?
accepted with minor changes?
accepted with major changes and secondoverall reviewing ?rejected), based on a set ofcriteria including interestingness, relevanceand originality of the paper?s content ;?
5 values for film and book reviews8 (a markbetween 0 and 4, from bad to excellent) ;?
20 values for video game reviews9 (a globalmark calculated from a set of advices aboutvarious aspects of the game: graphics, playa-bility, life span, sound track and scenario).In order to, first, assess the feasibility of the task,and to, secondly, define the scale of values to beused in the evaluation campaign, we submitted hu-man judges to several tests (Paek, 2001): they wereinstructed to assign a mark on two kinds of scale, awide one with the original values, and a restrictedone with 2 or 3 values, depending on the corpus itwas applying to.
The results from various judgeswere evaluated in terms of precision and recall, andmatched to each other by way of the Kappa coeffi-cient (Carletta, 1996) (Cohen, 1960).We present hereunder the values of the ?
coef-ficient between pairs of human judges, and withthe reference, on the video game corpus.
The widescale (Table 1) uses the original values (marks be-tween 0 and 20), while the restricted scale (Ta-ble 2) relies upon 3 values with following defini-tions: class 0 for original marks between 0 and 10,class 1 for marks between 11 and 14, and class 2for marks between 15 and 20.7http://www.assemblee-nationale.fr/12/debats/8http://www.avoir-alire.com9http://www.jeuxvideo.com/etajvbis.htmJudge Ref.
1 2 3Ref.
0.17 0.12 0.071 0.17 0.03 0.052 0.12 0.03 0.073 0.07 0.05 0.07Table 1: Video game corpus: wide scale, marksfrom 0 to 20.Judge Ref.
1 2 3Ref.
0.74 0.79 0.691 0.74 0.74 0.542 0.79 0.74 0.693 0.69 0.54 0.69Table 2: Video game corpus: restricted scale,marks from 0 to 2.Table 1 and 2 show that agreement betweenjudges varies widely when marking scales aremodified.
Table 1 shows that there is an insuffi-cient agreement among judges on the wide scale,with ?
coefficients lower than 0.20, while theagreement between these same judges can be con-sidered as good on the restricted scale, with ?
co-efficients between 0.54 and 0.79 (Table 2), the me-dian being at 0.74.In order to confirm the validity of the changein scales, we used the ?
to test how each judgeagreed with himself, between his two sets of re-sults (Table 3).
Therefore, we compared judg-ments made by each judge using the initial valuescale and converted towards the restricted scale,with judgments made by the same judge directlyusing the restricted value scale.
This measurementshows the degree of correspondence between bothscales for each judge.
Among the three judges whotook part in the test, the first and third one agreewell with themselves, while for the second one, theagreement is only moderate.Judge 1 2 31 0.742 0.463 0.70Table 3: Video game corpus: agreement of eachjudge with himself when scales change.We did the same for a second corpus, of film re-views.
The test involved five judges, and the scale19change was smaller, since it was from five valuesto three, and not from twenty to three.
For thisscale change, we merged the two lowest values (0and 1) into one (0), and the two highest ones (3and 4) into one (2), and the middle value in thewide scale (2) remained the intermediate one inthe restricted scale (1).
This scale change was themost relevant one, since, with 29.7% of the docu-ments, the class of the middle mark (2) accountedfor almost one third of the corpus.
However, thetwo other groups of documents are less well bal-anced.
Indeed, the lowest mark concerns less doc-uments than the highest one: 4.6% and 10.3% re-spectively for the initial marks 0 and 1, while onefinds 39.8% and 15.6% of documents for the marks3 and 4.
Grouping the documents in only twoclasses, by joining the middle class with the twolowest ones, would have yielded a better balancebetween classes, with 44.6% of documents for thelower mark and 55.4% for the higher one, but thatwould have been less meaningful.Results from human judges are shown in the Ta-bles 4 and 5 for both scales.Judge Ref.
1 2 3 4 5Ref.
0.10 0.29 0.39 0.46 0.471 0.10 0.37 0.49 0.48 0.352 0.29 0.37 0.36 0.30 0.433 0.39 0.49 0.36 0.49 0.544 0.46 0.48 0.30 0.49 0.605 0.47 0.35 0.43 0.54 0.60Table 4: Film review corpus: wide scale, marksfrom 0 to 4Judge Ref.
1 2 3 4 5Ref.
0.27 0.62 0.53 0.56 0.671 0.27 0.45 0.43 0.57 0.372 0.62 0.45 0.73 0.48 0.543 0.53 0.43 0.73 0.62 0.624 0.56 0.57 0.48 0.62 0.765 0.67 0.37 0.54 0.62 0.76Table 5: Film review corpus: restricted scale,marks from 0 to 2.Agreements between human judges ranked frombad to moderate for the wide scale (the five origi-nal values in this corpus), while these agreementsrank from insufficient to good in the case of therestricted scale with three values.
We can see thatdifferences induced by the scale change are muchless important than with the video game corpus.This agrees well with the scales being much closerto each other.By first performing a hand-made evaluation, andsecondly, matching between themselves the resultsfrom the judges, we found a way to assess withgreater precision the difficulty of the evaluationtask we were about to launch.
Concerning thefirst two review corpora (films and books, videogames), we attached values good, average and badto the three selected classes.
The scale for sci-entific paper reviews was also restricted to threeclasses for which following values were selected:paper accepted as it stands or with minor edits, pa-per accepted after major edits, paper rejected.
Fi-nally, since its original scale had only two values,the corpus of parliamentary debates underwent nochange of scale.3.2 Choice of a topical category setIn order to determine which topical categoriesshould be recognized in the 2008 task of classify-ing documents by genre and topic, we performed amanual evaluation of a sample of the corpus with 4human judges.
The sample included 30 Le Mondepapers for the journalistic genre, and 30 Wikipediaentries for the encyclopedic genre.
Only the titleand body of each article was kept in the sample,and the tables were deleted.
All marks of inclu-sion in either corpus were also deleted (referencesto Le Monde and Wikipedia tags).The test ran this way: each article was put in aseparate file, and the evaluators had to identify thegenre and the topical category under which it waspublished.
All articles were included in one set,which means evaluators had to choose, between allcategories and genres, which ones to match witheach document.
This test was made with a firstselection of 8 categories, shared by both genres,listed in Table 6.Table 7 shows that results from human judgesin terms of precision and recall were excellent onthe identification of genre (F-scores between 0.94and 1.00) and quite good on the identification ofcategories (F-scores between 0.66 and 0.82).We also proceeded to the pairwise matching ofresults from human judges via the ?
coefficient.Results show an excellent agreement of judgesamong themselves and with the reference for genreidentification (Table 8).
The agreement is mod-20Le Monde WikipediaNotebook PeopleEconomy EconomyFrance French PoliticsInternational International Politics,minus categoryFrench PoliticScience ScienceSociety Society,minus subcategoriesPolitics, People,Sport, MediaSport SportTelevision TelevisionTable 6: Correspondence between categories fromLe Monde and Wikipedia for the 8 categories inthe test.Judge 1 2 3 4Genres 1.00 0.98 0.97 0.94Categories 0.79 0.77 0.82 0.66Table 7: F-scores obtained by human judges on theidentification of genre and categories.erate to good for categoy identification (Table 9).These good results led us to keep the corpora asthey stood, since they appeared to constitute agood reference for the defined task.
However, wemade an exception for category Notebook (biogra-phies of celebrities) which we discarded for tworeasons.
First, it is more of a genre, namely, ?bi-ography?, rather than a topical category.
Secondly,we found it rather difficult to assign a single cate-gory to articles which could belong in two differentones, as would be the case for the biography of asportsman, which would fall under both categoriesNotebook et Sport.Judge Re?f.
1 2 3 4Re?f.
1.00 0.97 0.93 0.871 1.00 0.97 0.93 0.872 0.97 0.97 0.90 0.833 0.93 0.93 0.90 0.874 0.87 0.87 0.83 0.87Table 8: ?
coefficient between human judges andthe reference: Identification of genre.Our task of genre and topic classification in-Judge Re?f.
1 2 3 4Re?f.
0.56 0.52 0.60 0.391 0.56 0.69 0.75 0.552 0.52 0.69 0.71 0.613 0.60 0.75 0.71 0.524 0.39 0.55 0.61 0.52Table 9: ?
coefficient between human judges andthe reference: Identification of categories.cluded two subtasks, one being genre and topicrecognition for a first set of categories, the otherone being only topic recognition for a second setof categories.
Therefore, the corpus had to be di-vided in two parts.
In order to find which cate-gories had to go into which subcorpus, we decidedto estimate, for each category, the difficulty of rec-ognizing it.
To do so, we calculated the precisionand recall of each evaluator for each category.
Thismeasurement was obtained via a second evaluationof human judges, with a wider set of categories (byadding categories Art and Literature).The ordering of categories by decreasing pre-cision is following: Sport (1.00), International(0.80), France (0.76), Literature (0.76), Art (0.74),Television (0.71), Economy (0.58), Science (0.33),Society (0.26).
This means no document in theSport category was misclassified, and, contrari-wise, categories Science and Society were the mostproblematic ones.The ordering by decreasing recall is slightlydifferent: International (0.87), Economy (0.80),Sport (0.75), France (0.70), Art (0.62), Literature(0.49), Television (0.46), Society (0.42), Science(0.33).
Hence, articles in the International cate-gory were best identified.
This ordering also con-firms the difficulty felt by human judges concern-ing the categories Society and Science.We decided to distribute the categories for eachsubtask according to a balance between easy anddiffucult ones in terms of human evaluation:?
Art, Economy, Sport, Television for the sub-task with both genre and category recogni-tion;?
France, International, Literature, Science,Society for the subtask with only categoryrecognition.
For this second subset, we puttogether three categories which are topicallyclose (France, International and Society).214 Human judgments and software4.1 Confirming the difficulty of a taskThe 2007 edition of DEFT highlighted two mainphenomena concerning the corpora involved in thetask.First, each corpus yielded a different level of dif-ficulty, and this gradation of difficulty among cor-pora appeared both for human evaluators and com-petitors in the challenge (Paroubek et al, 2007).Judges CompetitorsDebates 0.77/1.00 0.54/0.72Game reviews 0.73/0.90 0.46/0.78Film reviews 0.52/0.79 0.38/0.60Paper reviews 0.41/0.58 0.40/0.57Table 10: Minimal and maximal strict F-scoresbetween human evaluators and competitors in thechallenge, 2007 edition.During human tests, judges mentioned the greatfacility of finding about opinions expressed in thecorpus of parliamentary debate.
Next came cor-pora of video game reviews, and then of film andbook reviews, whose difficulty was considered av-erage, and last, the corpus of scientific paper re-views, which the judges perceived as particularlydifficult.
This gradation of difficulty among cor-pora was also found among competitors, followingthe same ordering of three levels of difficulty.Secondly, the difficulties met by human eval-uators are also found in the case of competitors.Upon finishing human tests, judges felt difficultiesin evaluating the corpus of scientific paper reviews,yielding poor results.
Now, the results of competi-tors on the same corpus are quite as poor, occupy-ing exactly the same value interval as for humanjudges.
Most competitors, by the way, obtainedtheir worst results on this corpus.The alikeness of results between judges andcompetitors reflects the complexity of the corpus:when preparing the campaign, we observed thatreviews were quite short.
Therefore, assigning avalue had to rely upon a small amount of data.From that, we can derive a minimal size for docu-ments to be used in this kind of evaluation.
More-over, a paper review can be seen as an aid for theauthor, to be expressed as positively as possible,even if it is also addressed to the Program Commit-tee which has to accept or reject the paper.
There-fore, the mark could prove more negative than thetext of the review.The case of comments about videogames is adifferent one.
Indeed, giving a global mark on ascale of 20 is a difficult task.
Therefore, this markcomes most often from a sum of smaller markswhich rate either the whole document accordingto various criteria, or parts of this document.
Inour corpus, each reviewer rates the game accord-ing to several criteria, namely, graphics, playa-bility, life span, sound track and scenario, fromwhich a rather long text is produced, making thejudgment an easier task to perform.
However, theglobal mark differs from the sum of the smallerones from various criteria, hence the difficulty forhuman judges to reckon this global mark on a scaleof 20.4.2 Confirmation of the expected success ofcompetitorsContrary to the 2007 edition, in which competi-tors obtained results that confirmed those of humanjudges, the 2008 edition gave them the opportunityto reach a higher level than human evaluators.While genre identification yielded no specialproblem, either for human evaluators or for com-petitors, and the results obtained by both groupsare similar, competitors reached better results thanhuman judges in topical categorization.Concerning genre identification, strict F-scoresare situated between 0.94 and 1.00 for humanjudges, and between 0.95 and 0.98 for the bestruns of competitors (each competitor was allowedto submit up to three collections of results, onlythe best one being used for the final ranking).
Asfor topical categorization, strict F-scores go from0.66 to 0.82 for human evaluators, and from 0.84to 0.89 for best runs from competitors.The equivalence of results on genre identifica-tion between judges and competitors can be ex-plained by the fact that it was a simple, binarychoice (the newspaper Le Monde vs. Wikipedia).Contrariwise, competitors obtained better re-sults in topical categorization, since machines havea stronger abstraction capacity than humans inpresence of the 9 topical categories we defined(Art, Economy, France, International, Literature,Science, Society, Sport and Television).
However,conditions were not quite similar, since humanjudges had to pick a category among eight, andnot, like the automatic systems, a category withintwo subsets of four and five categories.
Indeed,22we dispatched the categories into two sets, by bal-ancing categories that are easy or difficult for hu-man evaluators.
For the second set of categories,we carefully put together three semantically closeones, (France, International and Society, all threeof them being about political and societal con-tents), to make the task more difficult.
Althoughthe second set of categories seems more compli-cated for human judges, half of the competitors ob-tained better results in topical categorization of thesecond set than of the first one.5 ConclusionThe relevance of human judgment in an evaluationcampaign is present from the beginning to the endof a campaign.In a first step, testing a topic for a campaignamong a limited number of human evaluators al-lows us to check the feasibility of a task.
Thischecking relies both on the results obtained byjudges (recall, precision, F-scores) and on theirpersonal impressions after passing the test.In a second step, the study of both the results ob-tained by the judges, and their pairwise matchinginvolving such a comparator as the ?
coefficientallows us to adjust the task (choice of a markingscale for DEFT?07 and selection of topical cate-gories for DEFT?08).Finally, the mutual comparison of competitors?results, at the end of the evaluation campaign, al-lows us to validate the choices we made at its start-ing point, and even to reposition the task when weshall launch a future campaign based on the sametopic.ReferencesAdda, Gilles, Joseph Mariani, Patrick Paroubek, Mar-tin Rajman, and Josette Lecomte.
1999.
L?actionGRACE d?e?valuation de l?assignation des parties dudiscours pour le franc?ais.
Langues, 2(2):119?129,juin.Burstein, Jill and Magdalena Wolska.
2003.
Towardevaluation of writing style: Finding overly repetitiveword use in student essays.
In 10th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, EACL?03, pages 35?42, Budapest,Hungary, april.Carletta, J.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistics.
Computational Lin-guistics, 2(22):249?254.Cohen, Jacob.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, (20):37?46.Eck, Matthias and Chiori Hori.
2005.
Overview ofthe iwslt 2005 evaluation campaign.
In InternationalWorkshop on Spoken Language Translation, pages5?14, Pittsburg, PA.Grouin, Cyril, Jean-Baptiste Berthelin, Sarra El Ayari,Thomas Heitz, Martine Hurault-Plantet, Miche`leJardino, Zohra Khalis, and Michel Lastes.
2007.Pre?sentation de DEFT?07 (D ?Efi Fouille de Textes).In Actes de l?atelier de clo?ture du 3e`me D ?EfiFouille de Textes, pages 1?8, Grenoble.
AssociationFranc?aise d?Intelligence Artificielle.Hovy, Eduard, Margaret King, and Andrei Popescu-Belis.
2002.
Principles of context-based machinetranslation evaluation.
Machine Translation.Hurault-Plantet, Martine, Jean-Baptiste Berthelin,Sarra El Ayari, Cyril Grouin, Patrick Paroubek, andSylvain Loiseau.
2008.
Re?sultats de l?e?dition 2008du D ?Efi Fouille de Textes.
In Actes TALN?08, Avi-gnon.
Association pour le Traitement Automatiquedes Langues.Paek, Tim.
2001.
Empirical Methods for Evaluat-ing Dialog Systems.
In Proceedings of the ACL2001 Workshop on Evaluation Methodologies forLanguage and Dialogue Systems, pages 3?10.Paroubek, Patrick, Jean-Baptiste Berthelin, Sarra ElAyari, Cyril Grouin, Thomas Heitz, Martine Hurault-Plantet, Miche`le Jardino, Zohra Khalis, and MichelLastes.
2007.
Re?sultats de l?e?dition 2007 du D ?EfiFouille de Textes.
In Actes de l?atelier de clo?ture du3e`me D ?Efi Fouille de Textes, pages 9?17, Grenoble.Association Franc?aise d?Intelligence Artificielle.23
