Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 17?24,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Topic Modelling for Multi-Party Spoken DiscourseMatthew PurverCSLIStanford UniversityStanford, CA 94305, USAmpurver@stanford.eduKonrad P. Ko?rdingDept.
of Brain & Cognitive SciencesMassachusetts Institute of TechnologyCambridge, MA 02139, USAkording@mit.eduThomas L. GriffithsDept.
of Cognitive & Linguistic SciencesBrown UniversityProvidence, RI 02912, USAtom griffiths@brown.eduJoshua B. TenenbaumDept.
of Brain & Cognitive SciencesMassachusetts Institute of TechnologyCambridge, MA 02139, USAjbt@mit.eduAbstractWe present a method for unsupervisedtopic modelling which adapts methodsused in document classification (Blei etal., 2003; Griffiths and Steyvers, 2004) tounsegmented multi-party discourse tran-scripts.
We show how Bayesian infer-ence in this generative model can beused to simultaneously address the prob-lems of topic segmentation and topicidentification: automatically segmentingmulti-party meetings into topically co-herent segments with performance whichcompares well with previous unsuper-vised segmentation-only methods (Galleyet al, 2003) while simultaneously extract-ing topics which rate highly when assessedfor coherence by human judges.
We alsoshow that this method appears robust inthe face of off-topic dialogue and speechrecognition errors.1 IntroductionTopic segmentation ?
division of a text or dis-course into topically coherent segments ?
andtopic identification ?
classification of those seg-ments by subject matter ?
are joint problems.
Bothare necessary steps in automatic indexing, retrievaland summarization from large datasets, whetherspoken or written.
Both have received significantattention in the past (see Section 2), but most ap-proaches have been targeted at either text or mono-logue, and most address only one of the two issues(usually for the very good reason that the datasetitself provides the other, for example by the ex-plicit separation of individual documents or newsstories in a collection).
Spoken multi-party meet-ings pose a difficult problem: firstly, neither thesegmentation nor the discussed topics can be takenas given; secondly, the discourse is by nature lesstidily structured and less restricted in domain; andthirdly, speech recognition results have unavoid-ably high levels of error due to the noisy multi-speaker environment.In this paper we present a method for unsuper-vised topic modelling which allows us to approachboth problems simultaneously, inferring a set oftopics while providing a segmentation into topi-cally coherent segments.
We show that this modelcan address these problems over multi-party dis-course transcripts, providing good segmentationperformance on a corpus of meetings (compara-ble to the best previous unsupervised method thatwe are aware of (Galley et al, 2003)), while alsoinferring a set of topics rated as semantically co-herent by human judges.
We then show that itssegmentation performance appears relatively ro-bust to speech recognition errors, giving us con-fidence that it can be successfully applied in a realspeech-processing system.The plan of the paper is as follows.
Section 2below briefly discusses previous approaches to theidentification and segmentation problems.
Sec-tion 3 then describes the model we use here.
Sec-tion 4 then details our experiments and results, andconclusions are drawn in Section 5.2 Background and Related WorkIn this paper we are interested in spoken discourse,and in particular multi-party human-human meet-ings.
Our overall aim is to produce informationwhich can be used to summarize, browse and/orretrieve the information contained in meetings.User studies (Lisowska et al, 2004; Banerjee etal., 2005) have shown that topic information is im-portant here: people are likely to want to know17which topics were discussed in a particular meet-ing, as well as have access to the discussion onparticular topics in which they are interested.
Ofcourse, this requires both identification of the top-ics discussed, and segmentation into the periods oftopically related discussion.Work on automatic topic segmentation of textand monologue has been prolific, with a variety ofapproaches used.
(Hearst, 1994) uses a measure oflexical cohesion between adjoining paragraphs intext; (Reynar, 1999) and (Beeferman et al, 1999)combine a variety of features such as statisticallanguage modelling, cue phrases, discourse infor-mation and the presence of pronouns or namedentities to segment broadcast news; (Maskey andHirschberg, 2003) use entirely non-lexical fea-tures.
Recent advances have used generative mod-els, allowing lexical models of the topics them-selves to be built while segmenting (Imai et al,1997; Barzilay and Lee, 2004), and we take a sim-ilar approach here, although with some importantdifferences detailed below.Turning to multi-party discourse and meetings,however, most previous work on automatic seg-mentation (Reiter and Rigoll, 2004; Dielmannand Renals, 2004; Banerjee and Rudnicky, 2004),treats segments as representing meeting phases orevents which characterize the type or style of dis-course taking place (presentation, briefing, discus-sion etc.
), rather than the topic or subject matter.While we expect some correlation between thesetwo types of segmentation, they are clearly differ-ent problems.
However, one comparable study isdescribed in (Galley et al, 2003).
Here, a lex-ical cohesion approach was used to develop anessentially unsupervised segmentation tool (LC-Seg) which was applied to both text and meet-ing transcripts, giving performance better than thatachieved by applying text/monologue-based tech-niques (see Section 4 below), and we take thisas our benchmark for the segmentation problem.Note that they improved their accuracy by com-bining the unsupervised output with discourse fea-tures in a supervised classifier ?
while we do notattempt a similar comparison here, we expect asimilar technique would yield similar segmenta-tion improvements.In contrast, we take a generative approach,modelling the text as being generated by a se-quence of mixtures of underlying topics.
The ap-proach is unsupervised, allowing both segmenta-tion and topic extraction from unlabelled data.3 Learning topics and segmentsWe specify our model to address the problem oftopic segmentation: attempting to break the dis-course into discrete segments in which a particu-lar set of topics are discussed.
Assume we have acorpus of U utterances, ordered in sequence.
Theuth utterance consists of Nu words, chosen froma vocabulary of size W .
The set of words asso-ciated with the uth utterance are denoted wu, andindexed as wu,i.
The entire corpus is representedby w.Following previous work on probabilistic topicmodels (Hofmann, 1999; Blei et al, 2003; Grif-fiths and Steyvers, 2004), we model each utteranceas being generated from a particular distributionover topics, where each topic is a probability dis-tribution over words.
The utterances are orderedsequentially, and we assume aMarkov structure onthe distribution over topics: with high probability,the distribution for utterance u is the same as forutterance u?1; otherwise, we sample a new distri-bution over topics.
This pattern of dependency isproduced by associating a binary switching vari-able with each utterance, indicating whether itstopic is the same as that of the previous utterance.The joint states of all the switching variables de-fine segments that should be semantically coher-ent, because their words are generated by the sametopic vector.
We will first describe this generativemodel in more detail, and then discuss inferencein this model.3.1 A hierarchical Bayesian modelWe are interested in where changes occur in theset of topics discussed in these utterances.
To thisend, let cu indicate whether a change in the distri-bution over topics occurs at the uth utterance andlet P (cu = 1) = pi (where pi thus defines the ex-pected number of segments).
The distribution overtopics associated with the uth utterance will be de-noted ?
(u), and is a multinomial distribution overT topics, with the probability of topic t being ?
(u)t .If cu = 0, then ?
(u) = ?(u?1).
Otherwise, ?
(u)is drawn from a symmetric Dirichlet distributionwith parameter ?.
The distribution is thus:P (?
(u)|cu, ?
(u?1)) =(?(?
(u), ?
(u?1)) cu = 0?(T?)?(?)TQTt=1(?
(u)t )?
?1 cu = 118Figure 1: Graphical models indicating the dependencies among variables in (a) the topic segmentationmodel and (b) the hidden Markov model used as a comparison.where ?
(?, ?)
is the Dirac delta function, and ?(?
)is the generalized factorial function.
This dis-tribution is not well-defined when u = 1, sowe set c1 = 1 and draw ?
(1) from a symmetricDirichlet(?)
distribution accordingly.As in (Hofmann, 1999; Blei et al, 2003; Grif-fiths and Steyvers, 2004), each topic Tj is a multi-nomial distribution ?
(j) over words, and the prob-ability of the word w under that topic is ?
(j)w .
Theuth utterance is generated by sampling a topic as-signment zu,i for each word i in that utterance withP (zu,i = t|?
(u)) = ?
(u)t , and then sampling aword wu,i from ?
(j), with P (wu,i = w|zu,i =j, ?
(j)) = ?
(j)w .
If we assume that pi is generatedfrom a symmetric Beta(?)
distribution, and each?
(j) is generated from a symmetric Dirichlet(?
)distribution, we obtain a joint distribution over allof these variables with the dependency structureshown in Figure 1A.3.2 InferenceAssessing the posterior probability distributionover topic changes c given a corpus w can be sim-plified by integrating out the parameters ?, ?, andpi.
According to Bayes rule we have:P (z, c|w) =P (w|z)P (z|c)P (c)Pz,c P (w|z)P (z|c)P (c)(1)Evaluating P (c) requires integrating over pi.Specifically, we have:P (c) =R 10 P (c|pi)P (pi) dpi= ?(2?)?(?)2?(n1+?)?(n0+?)?(N+2?
)(2)where n1 is the number of utterances for whichcu = 1, and n0 is the number of utterances forwhich cu = 0.
Computing P (w|z) proceeds alongsimilar lines:P (w|z) =R?TWP (w|z, ?
)P (?)
d?=??(W?)?(?
)W?T QTt=1QWw=1 ?
(n(t)w +?)?(n(t)?
+W?
)(3)where ?TW is the T -dimensional cross-product ofthe multinomial simplex on W points, n(t)w is thenumber of times word w is assigned to topic t inz, and n(t)?
is the total number of words assignedto topic t in z.
To evaluate P (z|c) we have:P (z|c) =Z?UTP (z|?
)P (?|c) d?
(4)The fact that the cu variables effectively dividethe sequence of utterances into segments that usethe same distribution over topics simplifies solvingthe integral and we obtain:P (z|c) =??(T?)?(?
)T?n1 Yu?U1QTt=1 ?
(n(Su)t + ?)?(n(Su)?
+ T?).
(5)19P (cu|c?u, z,w) ?8>><>>:QTt=1 ?
(n(S0u)t +?)?(n(S0u)?
+T?)n0+?N+2?
cu = 0?(T?)?(?
)TQTt=1 ?
(n(S1u?1)t +?)?(n(S1u?1)?
+T?
)QTt=1 ?
(n(S1u)t +?)?(n(S1u)?
+T?)n1+?N+2?
cu = 1(7)where U1 = {u|cu = 1}, U0 = {u|cu = 0}, Sudenotes the set of utterances that share the sametopic distribution (i.e.
belong to the same segment)as u, and n(Su)t is the number of times topic t ap-pears in the segment Su (i.e.
in the values of zu?corresponding for u?
?
Su).Equations 2, 3, and 5 allow us to evaluate thenumerator of the expression in Equation 1.
How-ever, computing the denominator is intractable.Consequently, we sample from the posterior dis-tribution P (z, c|w) using Markov chain MonteCarlo (MCMC) (Gilks et al, 1996).
We use Gibbssampling, drawing the topic assignment for eachword, zu,i, conditioned on all other topic assign-ments, z?
(u,i), all topic change indicators, c, andall words, w; and then drawing the topic changeindicator for each utterance, cu, conditioned on allother topic change indicators, c?u, all topic as-signments z, and all words w.The conditional probabilities we need can bederived directly from Equations 2, 3, and 5.
Theconditional probability of zu,i indicates the prob-ability that wu,i should be assigned to a particu-lar topic, given other assignments, the current seg-mentation, and the words in the utterances.
Can-celling constant terms, we obtain:P (zu,i|z?
(u,i), c,w) =n(t)wu,i + ?n(t)?
+ W?n(Su)zu,i + ?n(Su)?
+ T?.
(6)where all counts (i.e.
the n terms) exclude zu,i.The conditional probability of cu indicates theprobability that a new segment should start at u.In sampling cu from this distribution, we are split-ting or merging segments.
Similarly we obtain theexpression in (7), where S1u is Su for the segmen-tation when cu = 1, S0u is Su for the segmentationwhen cu = 0, and all counts (e.g.
n1) exclude cu.For this paper, we fixed ?, ?
and ?
at 0.01.Our algorithm is related to (Barzilay and Lee,2004)?s approach to text segmentation, which usesa hiddenMarkov model (HMM) to model segmen-tation and topic inference for text using a bigramrepresentation in restricted domains.
Due to theadaptive combination of different topics our algo-rithm can be expected to generalize well to largerdomains.
It also relates to earlier work by (Bleiand Moreno, 2001) that uses a topic representationbut also does not allow adaptively combining dif-ferent topics.
However, while HMM approachesallow a segmentation of the data by topic, theydo not allow adaptively combining different topicsinto segments: while a new segment can be mod-elled as being identical to a topic that has alreadybeen observed, it can not be modelled as a com-bination of the previously observed topics.1 Notethat while (Imai et al, 1997)?s HMM approach al-lows topic mixtures, it requires supervision withhand-labelled topics.In our experiments we therefore compared ourresults with those obtained by a similar but simpler10 state HMM, using a similar Gibbs sampling al-gorithm.
The key difference between the twomod-els is shown in Figure 1.
In the HMM, all variationin the content of utterances is modelled at a singlelevel, with each segment having a distribution overwords corresponding to a single state.
The hierar-chical structure of our topic segmentation modelallows variation in content to be expressed at twolevels, with each segment being produced from alinear combination of the distributions associatedwith each topic.
Consequently, our model can of-ten capture the content of a sequence of words bypostulating a single segment with a novel distribu-tion over topics, while the HMM has to frequentlyswitch between states.4 Experiments4.1 Experiment 0: Simulated dataTo analyze the properties of this algorithm we firstapplied it to a simulated dataset: a sequence of10,000 words chosen from a vocabulary of 25.Each segment of 100 successive words had a con-1Say that a particular corpus leads us to infer topics corre-sponding to ?speech recognition?
and ?discourse understand-ing?.
A single discussion concerning speech recognition fordiscourse understanding could be modelled by our algorithmas a single segment with a suitable weighted mixture of thetwo topics; a HMM approach would tend to split it into mul-tiple segments (or require a specific topic for this segment).20Figure 2: Simulated data: A) inferred topics; B)segmentation probabilities; C) HMM version.stant topic distribution (with distributions for dif-ferent segments drawn from a Dirichlet distribu-tion with ?
= 0.1), and each subsequence of 10words was taken to be one utterance.
The topic-word assignments were chosen such that when thevocabulary is aligned in a 5?5 grid the topics werebinary bars.
The inference algorithm was then runfor 200,000 iterations, with samples collected afterevery 1,000 iterations to minimize autocorrelation.Figure 2 shows the inferred topic-word distribu-tions and segment boundaries, which correspondwell with those used to generate the data.4.2 Experiment 1: The ICSI corpusWe applied the algorithm to the ICSI meetingcorpus transcripts (Janin et al, 2003), consist-ing of manual transcriptions of 75 meetings.
Forevaluation, we use (Galley et al, 2003)?s set ofhuman-annotated segmentations, which covers asub-portion of 25 meetings and takes a relativelycoarse-grained approach to topic with an averageof 5-6 topic segments per meeting.
Note thatthese segmentations were not used in training themodel: topic inference and segmentation was un-supervised, with the human annotations used onlyto provide some knowledge of the overall segmen-tation density and to evaluate performance.The transcripts from all 75 meetings were lin-earized by utterance start time and merged into asingle dataset that contained 607,263 word tokens.We sampled for 200,000 iterations of MCMC, tak-ing samples every 1,000 iterations, and then aver-aged the sampled cu variables over the last 100samples to derive an estimate for the posteriorprobability of a segmentation boundary at each ut-terance start.
This probability was then thresh-olded to derive a final segmentation which wascompared to the manual annotations.
More pre-cisely, we apply a small amount of smoothing(Gaussian kernel convolution) and take the mid-points of any areas above a set threshold to be thesegment boundaries.
Varying this threshold allowsus to segment the discourse in a more or less fine-grained way (and we anticipate that this could beuser-settable in a meeting browsing application).If the correct number of segments is known fora meeting, this can be used directly to determinethe optimum threshold, increasing performance; ifnot, we must set it at a level which corresponds tothe desired general level of granularity.
For eachset of annotations, we therefore performed twosets of segmentations: one in which the thresholdwas set for each meeting to give the known gold-standard number of segments, and one in whichthe threshold was set on a separate developmentset to give the overall corpus-wide average numberof segments, and held constant for all test meet-ings.2 This also allows us to compare our resultswith those of (Galley et al, 2003), who apply asimilar threshold to their lexical cohesion func-tion and give corresponding results produced withknown/unknown numbers of segments.Segmentation We assessed segmentation per-formance using the Pk and WindowDiff (WD) er-ror measures proposed by (Beeferman et al, 1999)and (Pevzner and Hearst, 2002) respectively; bothintuitively provide a measure of the probabilitythat two points drawn from the meeting will beincorrectly separated by a hypothesized segmentboundary ?
thus, lower Pk and WD figures indi-cate better agreement with the human-annotatedresults.3 For the numbers of segments we are deal-ing with, a baseline of segmenting the discourseinto equal-length segments gives both Pk and WDabout 50%.
In order to investigate the effect of thenumber of underlying topics T , we tested mod-els using 2, 5, 10 and 20 topics.
We then com-pared performance with (Galley et al, 2003)?s LC-Seg tool, and with a 10-state HMM model as de-scribed above.
Results are shown in Table 1, aver-aged over the 25 test meetings.Results show that our model significantly out-performs the HMM equivalent ?
because theHMM cannot combine different topics, it placesa lot of segmentation boundaries, resulting in in-ferior performance.
Using stemming and a bigram2The development set was formed from the other meet-ings in the same ICSI subject areas as the annotated test meet-ings.3WD takes into account the likely number of incorrectlyseparating hypothesized boundaries; Pk only a binary cor-rect/incorrect classification.21Figure 3: Results from the ICSI corpus: A) the words most indicative for each topic; B) Probability of asegment boundary, compared with human segmentation, for an arbitrary subset of the data; C) Receiver-operator characteristic (ROC) curves for predicting human segmentation, and conditional probabilitiesof placing a boundary at an offset from a human boundary; D) subjective topic coherence ratings.Number of topics TModel 2 5 10 20 HMM LCSegPk .284 .297 .329 .290 .375 .319known unknownModel Pk WD Pk WDT = 10 .289 .329 .329 .353LCSeg .264 .294 .319 .359Table 1: Results on the ICSI meeting corpus.representation, however, might improve its perfor-mance (Barzilay and Lee, 2004), although simi-lar benefits might equally apply to our model.
Italso performs comparably to (Galley et al, 2003)?sunsupervised performance (exceeding it for somesettings of T ).
It does not perform as well as theirhybrid supervised system, which combined LC-Seg with supervised learning over discourse fea-tures (Pk = .23); but we expect that a similar ap-proach would be possible here, combining our seg-mentation probabilities with other discourse-basedfeatures in a supervised way for improved per-formance.
Interestingly, segmentation quality, atleast at this relatively coarse-grained level, seemshardly affected by the overall number of topics T .Figure 3B shows an example for one meeting ofhow the inferred topic segmentation probabilitiesat each utterance compare with the gold-standardsegment boundaries.
Figure 3C illustrates the per-formance difference between our model and theHMM equivalent at an example segment bound-ary: for this example, the HMM model gives al-most no discrimination.Identification Figure 3A shows the most indica-tive words for a subset of the topics inferred at thelast iteration.
Encouragingly, most topics seemintuitively to reflect the subjects we know werediscussed in the ICSI meetings ?
the majority ofthem (67 meetings) are taken from the weeklymeetings of 3 distinct research groups, where dis-cussions centered around speech recognition tech-niques (topics 2, 5), meeting recording, annotationand hardware setup (topics 6, 3, 1, 8), robust lan-guage processing (topic 7).
Others reflect generalclasses of words which are independent of subjectmatter (topic 4).To compare the quality of these inferred topicswe performed an experiment in which 7 humanobservers rated (on a scale of 1 to 9) the seman-tic coherence of 50 lists of 10 words each.
Ofthese lists, 40 contained the most indicative wordsfor each of the 10 topics from different models:the topic segmentation model; a topic model thathad the same number of segments but with fixedevenly spread segmentation boundaries; an equiv-22alent with randomly placed segmentation bound-aries; and the HMM.
The other 10 lists containedrandom samples of 10 words from the other 40lists.
Results are shown in Figure 3D, with thetopic segmentation model producing the most co-herent topics and the HMM model and randomwords scoring less well.
Interestingly, using aneven distribution of boundaries but allowing thetopic model to infer topics performs similarly wellwith even segmentation, but badly with randomsegmentation ?
topic quality is thus not very sus-ceptible to the precise segmentation of the text,but does require some reasonable approximation(on ICSI data, an even segmentation gives a Pk ofabout 50%, while random segmentations can domuch worse).
However, note that the full topicsegmentation model is able to identify meaningfulsegmentation boundaries at the same time as infer-ring topics.4.3 Experiment 2: Dialogue robustnessMeetings often include off-topic dialogue, in par-ticular at the beginning and end, where infor-mal chat and meta-dialogue are common.
Gal-ley et al (2003) annotated these sections explic-itly, together with the ICSI ?digit-task?
sections(participants read sequences of digits to providedata for speech recognition experiments), and re-moved them from their data, as did we in Ex-periment 1 above.
While this seems reasonablefor the purposes of investigating ideal algorithmperformance, in real situations we will be facedwith such off-topic dialogue, and would obviouslyprefer segmentation performance not to be badlyaffected (and ideally, enabling segmentation ofthe off-topic sections from the meeting proper).One might suspect that an unsupervised genera-tive model such as ours might not be robust in thepresence of numerous off-topic words, as spuri-ous topics might be inferred and used in the mix-ture model throughout.
In order to investigate this,we therefore also tested on the full dataset with-out removing these sections (806,026 word tokensin total), and added the section boundaries as fur-ther desired gold-standard segmentation bound-aries.
Table 2 shows the results: performance isnot significantly affected, and again is very simi-lar for both our model and LCSeg.4.4 Experiment 3: Speech recognitionThe experiments so far have all used manual wordtranscriptions.
Of course, in real meeting pro-known unknownExperiment Model Pk WD Pk WD2 T = 10 .296 .342 .325 .366(off-topic data) LCSeg .307 .338 .322 .3863 T = 10 .266 .306 .291 .331(ASR data) LCSeg .289 .339 .378 .472Table 2: Results for Experiments 2 & 3: robust-ness to off-topic and ASR data.cessing systems, we will have to deal with speechrecognition (ASR) errors.
We therefore also testedon 1-best ASR output provided by ICSI, and re-sults are shown in Table 2.
The ?off-topic?
and?digits?
sections were removed in this test, so re-sults are comparable with Experiment 1.
Segmen-tation accuracy seems extremely robust; interest-ingly, LCSeg?s results are less robust (the drop inperformance is higher), especially when the num-ber of segments in a meeting is unknown.It is surprising to notice that the segmentationaccuracy in this experiment was actually slightlyhigher than achieved in Experiment 1 (especiallygiven that ASR word error rates were generallyabove 20%).
This may simply be a smoothing ef-fect: differences in vocabulary and its distributioncan effectively change the prior towards sparsityinstantiated in the Dirichlet distributions.5 Summary and Future WorkWe have presented an unsupervised generativemodel which allows topic segmentation and iden-tification from unlabelled data.
Performance onthe ICSI corpus of multi-party meetings is compa-rable with the previous unsupervised segmentationresults, and the extracted topics are rated well byhuman judges.
Segmentation accuracy is robustin the face of noise, both in the form of off-topicdiscussion and speech recognition hypotheses.Future Work Spoken discourse exhibits severalfeatures not derived from the words themselvesbut which seem intuitively useful for segmenta-tion, e.g.
speaker changes, speaker identities androles, silences, overlaps, prosody and so on.
Asshown by (Galley et al, 2003), some of these fea-tures can be combined with lexical information toimprove segmentation performance (although in asupervised manner), and (Maskey and Hirschberg,2003) show some success in broadcast news seg-mentation using only these kinds of non-lexicalfeatures.
We are currently investigating the addi-tion of non-lexical features as observed outputs in23our unsupervised generative model.We are also investigating improvements into thelexical model as presented here, firstly via simpletechniques such as word stemming and replace-ment of named entities by generic class tokens(Barzilay and Lee, 2004); but also via the use ofmultiple ASR hypotheses by incorporating wordconfusion networks into our model.
We expectthat this will allow improved segmentation andidentification performance with ASR data.AcknowledgementsThis work was supported by the CALO project(DARPA grant NBCH-D-03-0010).
We thankElizabeth Shriberg and Andreas Stolcke for pro-viding automatic speech recognition data for theICSI corpus and for their helpful advice; JohnNiekrasz and Alex Gruenstein for help with theNOMOS corpus annotation tool; and Michel Gal-ley for discussion of his approach and results.ReferencesSatanjeev Banerjee and Alex Rudnicky.
2004.
Usingsimple speech-based features to detect the state of ameeting and the roles of the meeting participants.
InProceedings of the 8th International Conference onSpoken Language Processing.Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.2005.
The necessity of a meeting recording andplayback system, and the benefit of topic-level anno-tations to meeting browsing.
In Proceedings of the10th International Conference on Human-ComputerInteraction.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Proceedings of the Main Conference, pages113?120.Doug Beeferman, Adam Berger, and John D. Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34(1-3):177?210.David Blei and Pedro Moreno.
2001.
Topic segmenta-tion with an aspect hidden Markov model.
In Pro-ceedings of the 24th Annual International Confer-ence on Research and Development in InformationRetrieval, pages 343?348.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Alfred Dielmann and Steve Renals.
2004.
DynamicBayesian Networks for meeting structuring.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP).Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 562?569.W.R.
Gilks, S. Richardson, and D.J.
Spiegelhalter, edi-tors.
1996.
Markov Chain Monte Carlo in Practice.Chapman and Hall, Suffolk.Thomas Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Science, 101:5228?5235.Marti A. Hearst.
1994.
Multi-paragraph segmenta-tion of expository text.
In Proc.
32nd Meeting ofthe Association for Computational Linguistics, LosCruces, NM, June.Thomas Hofmann.
1999.
Probablistic latent semanticindexing.
In Proceedings of the 22nd Annual SIGIRConference on Research and Development in Infor-mation Retrieval, pages 50?57.Toru Imai, Richard Schwartz, Francis Kubala, andLong Nguyen.
1997.
Improved topic discriminationof broadcast news using a model of multiple simul-taneous topics.
In Proceedings of the IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing (ICASSP), pages 727?730.Adam Janin, Don Baron, Jane Edwards, Dan Ellis,David Gelbart, Nelson Morgan, Barbara Peskin,Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,and Chuck Wooters.
2003.
The ICSI Meeting Cor-pus.
In Proceedings of the IEEE International Con-ference on Acoustics, Speech, and Signal Processing(ICASSP), pages 364?367.Agnes Lisowska, Andrei Popescu-Belis, and SusanArmstrong.
2004.
User query analysis for the spec-ification and evaluation of a dialogue processing andretrieval system.
In Proceedings of the 4th Interna-tional Conference on Language Resources and Eval-uation.Sameer R. Maskey and Julia Hirschberg.
2003.
Au-tomatic summarization of broadcast news usingstructural features.
In Eurospeech 2003, Geneva,Switzerland.Lev Pevzner and Marti Hearst.
2002.
A critique andimprovement of an evaluation metric for text seg-mentation.
Computational Linguistics, 28(1):19?36.Stehpan Reiter and Gerhard Rigoll.
2004.
Segmenta-tion and classification of meeting events using mul-tiple classifier fusion and dynamic programming.
InProceedings of the International Conference on Pat-tern Recognition.Jeffrey Reynar.
1999.
Statistical models for topic seg-mentation.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 357?364.24
