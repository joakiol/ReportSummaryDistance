Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291?1300,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsExploiting Syntactic and Distributional Informationfor Spelling Correction with Web-Scale N-gram ModelsWei Xuc,?Joel Tetreaulta Martin Chodorowb Ralph Grishmanc Le ZhaodaEducational Testing Service, Princeton, NJ, USAjtetreault@ets.orgbHunter College of CUNY, New York, NY, USAmartin.chodorow@hunter.cuny.educNew York University, NY, USA{xuwei,grishman}@cs.nyu.edudCarnegie Mellon University, Pittsburgh, PA, USAlezhao@cs.cmu.eduAbstractWe propose a novel way of incorporating de-pendency parse and word co-occurrence in-formation into a state-of-the-art web-scale n-gram model for spelling correction.
The syn-tactic and distributional information providesextra evidence in addition to that provided by aweb-scale n-gram corpus and especially helpswith data sparsity problems.
Experimentalresults show that introducing syntactic fea-tures into n-gram based models significantlyreduces errors by up to 12.4% over the currentstate-of-the-art.
The word co-occurrence in-formation shows potential but only improvesoverall accuracy slightly.1 IntroductionThe function of context-sensitive text correction isto identify word-choice errors in text (Bergsma etal., 2009).
It can be viewed as a lexical disambigua-tion task (Lapata and Keller, 2005), where a systemselects from a predefined confusion word set, suchas {affect, effect} or {complement, compliment},and provides the most appropriate word choice giventhe context.
Typically, one determines if a word hasbeen used correctly based on lexical, syntactic andsemantic information from the context of the word.One of the top performing models of spelling cor-rection (Bergsma et al, 2010) is based on web-scalen-gram counts, which reflect both syntax and mean-ing.
However, even with a large-scale n-gram cor-pus, data sparsity can hurt performance in two ways.
?This work was done when the first author was an internfor Educational Testing Service.First, n-gram based methods require exact word andorder matches.
If there is a low frequency word inthe context, such as a person?s name, there will belittle, if any, evidence in the n-gram data to sup-port the usage.
Second, if the target confusable wordis rare, there will not be enough n-gram support ortraining data to render a confident decision.
Becauseof the data sparsity problem, language modeling isnot always sufficient to capture the meaning of thesentence and the correct usage of the word.Take a sentence from The New York Times(NYT) for example: ?
?This fellow?s won a war,?
thedean of the capital?s press corps, David Broder, an-nounced on ?Meet the Press?
after complimentingthe president on the ?great sense of authority andcommand?
he exhibited in a flight suit.?
Unfortu-nately, neither the phrase ?complementing the pres-ident?
nor ?complimenting the president?
exists inthe web-scale Google N-gram corpus (Brants andFranz, 2006).
The n-gram models decide solelybased on the frequency of the bi-grams ?after com-ple(i)menting?
and ?comple(i)menting the?, whichare common usages for both words.
The real ques-tion is whether we are more likely to ?compliment?or ?complement?
a person, the ?president?.
Severalclues could help us answer that question.
A de-pendency parser can identify the word ?president?as the subject of ?compliment?
or ?complement?which also may be the case in some of the train-ing data.
Lexical co-occurrence (Edmonds, 1997)and semantic word relatedness measurements, suchas Random Indexing (Sahlgren, 2006), could pro-vide evidence that ?compliment?
is more likely toco-occur with ?president?
than ?complement?.
Fur-1291thermore, some important clues can be quite distantfrom the target word, e.g.
outside the 9-word contextwindow Bergsma et al (2010) and Carlson (2007)used.
Consider another sentence in the NYT corpus,?GM says the addition of OnStar, which includes asystem that automatically notifies an OnStar opera-tor if the vehicle is involved in a collision, comple-ments the Vue?s top five-star safety rating for thedriver and front passenger in both front- and side-impact crash tests.?
The dependency parser finds theobject of ?complement?
is ?rating?, which is outsidethe 9-word window.We propose enhancing state-of-the-art web-scalen-gram models for spelling correction with syntac-tic structures and distributional information.
For ourwork, we build on a baseline system that combinesn-gram and lexical features (Bergsma et al, 2010).Specifically, this paper makes the following contri-butions:1.
We show that the baseline system can beimproved by augmenting it with dependencyparse features.2.
We show that the impact of parse features canbe further augmented when combined with dis-tributional information, specifically word co-occurrence information.In the following section, we describe relatedwork and how our approach differs from these ap-proaches.
In Sections 3 and 4, we discuss our meth-ods for using parse features and word co-occurrenceinformation.
In Section 5, we present experimentalresults and analysis.2 Related WorkA variety of approaches have been proposed forcontext-sensitive spelling correction ranging fromsemantic methods to machine learning classifiers tolarge-scale n-gram models.Some semantics-based systems have been devel-oped based on an intuitive assumption that the in-tended word is more likely to be semantically coher-ent with the context than is a spelling error.
Jonesand Martin (1997) made use of the semantic simi-larity produced by Latent Semantic Analysis.
Bu-danitsky and Hirst (2001) investigated the effective-ness of predicting words based on different semanticsimilarity/distance measures in WordNet.
Both sys-tems report performance that is lower than systemsdeveloped more recently.A variety of machine-learning methods have beenproposed in spelling correction and preposition andarticle error correction fields, such as Bayesian clas-sifiers (Golding, 1995; Golding and Roth, 1996),Winnow-based learning (Golding and Roth, 1999),decision lists (Golding, 1995), transformation-basedlearning (Mangu and Brill, 1997), augmented mix-ture models (Cucerzan and Yarowsky, 2002) andmaximum entropy classifiers (Izumi et al, 2003;Han et al, 2006; Chodorow et al, 2007; Tetreaultand Chodorow, 2008; Felice and Pulman, 2008).Despite their differences, these approaches mainlyuse contextual features to capture the lexical, seman-tic and/or syntactic environment of the target word.The use of distributional similarity measures forspelling correction has been previously explored in(Mohammad and Hist, 2006).
In our work, distribu-tional similarity is not the primary contribution butwe show the impact it can have when used in con-junction with a large scale n-gram model and withparse features, which allows the system to selectwords outside the local window for distributionalsimilarity.
In the prior work, the words for distri-butional similarity are constrained to the local win-dow, and positional information of the words is notencoded.Recent work (Carlson and Fette, 2007; Gamonet al, 2008; Bergsma et al, 2009) has demon-strated that large-scale language modeling is ex-tremely helpful for contextual spelling correctionand other lexical disambiguation tasks.
These sys-tems make the word choice depending on how fre-quently each candidate word has been seen in thegiven context in web-scale data.
As n-gram data hasbecome more readily available, such as the GoogleN-gram Corpus, the likelihood of a word being usedin a certain context can be better estimated.Bergsma et al (2009; 2010) presented a seriesof simple but powerful models which relied heavilyon web-scale n-gram counts.
From the Google WebN-gram Corpus, they retrieve counts of n-grams ofdifferent sizes (2-5) and positions that span the tar-get word w0 within a window of 9 words.
Forexample, for the following sentence: ?The systemtried to decide {among, between} the two confus-1292able words.
?, the method would extract the five 5-gram patterns, shown below in Figure 2, where w0can be either word in the confusion set {among, be-tween} in this particular example.
Similarly, thereare four 4-grams, three 3-grams, and two 2-grams,in total, 14 n-grams for each of the words in the con-fusion set.system tried to decide w0tried to decide w0 theto decide w0 the twodecide w0 the two confusablew0 the two confusable wordsWe briefly describe three of Bergsma et al?s(2009; 2010) best systems below, which are reportedto achieve state-of-the-art accuracy (NG = n-gram;LEX = lexical).1. sumLM: For each candidate word, (Bergsmaet al, 2009) sum the log-counts of all 14 pat-terns filled with the candidate, and choose thecandidate with the highest total.2.
NG: Bergsma et al (2009) exploit each can-didate?s 14 log-counts of n-gram patterns asfeatures in a Support Vector Machine (SVM)model.3.
NG+LEX: Bergsma et al (2010) augment theNG model with lexical features (described indetail in Section 3.1).Bergsma et al (2009; 2010) restricted their exper-iments to only five confusion sets where the reportedperformance in (Golding and Roth, 1999) was below90%: {among, between}, {amount, number}, {cite,sight, site}, {peace, piece} and {raise, rise}.
Theyreported that the SVM model with NG features out-performed its unsupervised version, sumLM.
How-ever, the limited confusion word sets they evaluatedmay not comprehensively represent the word usageerrors that writers typically make.
In this paper, wetest nine additional commonly confused word pairsto expand the scope of the evaluation.
These wordswere selected based on their lower frequencies com-pared to the five pairs in the above work (as shownlater in Table 2).3 Enhanced N-gram Models with ParseFeaturesTo our knowledge, only (Elmi and Evans, 1998)have used parsing for spell correction.
They focuson using a parser as a filter to discriminate betweenpossible real-world corrections where the part-of-speech differs.
In our work, we show that parse fea-tures are effective when used directly in the classifi-cation mode (as opposed to as a final filter) to selectthe best correction regardless of whether or not thepart-of-speech of the choices differ.Statistical parsers have also seen limited use inthe sister tasks of preposition and article error detec-tion (Hermet et al, 2008; Lee and Knutsson, 2008;Felice and Pulman, 2009; Tetreault et al, 2010)and verb sense disambiguation (Dligach and Palmer,2008).
In those instances where parsers have beenused, they have mainly provided shallow analysesor relations involving specific target words, such asa preposition or verb.
Unlike preposition errors,spelling errors can occur in any word.In this paper, we propose a novel way to incor-porate the parse into spelling correction, applyingthe parser to sentences filled by each candidate wordequivalently and extracting salient features.
Thisovercomes two problem in the existing methods: 1)the parse trees of the same sentence filled by differ-ent confusion words can be different.
However, inthe test phase, we do not know which word shouldbe put in the sentences to create parse features fortest examples.
Previous studies (Tetreault et al,2010) failed to discuss this issue.
2) Some existingwork (Whitelaw et al, 2009; Rozovskaya and Roth,2010) in the text correction field introduced artificialerrors into training data to adapt the system to bet-ter handle ill-formed text.
But this method will en-counter serious data sparsity problems when facingrare words.3.1 Baseline SystemWe chose one of the leading spelling correction sys-tems, (Bergsma et al, 2010), as our primary base-line.
As noted earlier, it is an SVM-based systemcombining web-scale n-gram counts (NG) and con-textual words (LEX) as features.
To simplify the ex-planation, throughout the paper, we will only con-sider the situation with two confusion words.
The1293problem with more than two words in pre-definedconfusion sets can be solved similarly by using aone-vs.-all strategy.
As we mentioned in Section 2,NG features include log-counts of 3-to-5-gram pat-terns for each candidate word with the given context.LEX features can be broken down into three sub-categories: 1) bag-of-words (words at all positionsin a 9-word window around the target word), 2) in-dicators for the words preceding or following the tar-get word, and 3) indicators for all n-grams and theirpositions.
For the sentence ?The system tried to de-cide {among, between} the two confusable words.
?,examples of bag-of-word features would be ?tried?,?two?, etc., the two positional bigrams would be?decide?
and ?the?, and examples of the n-gram fea-tures would be right-trigram = ?among the two?
andleft-4-gram = ?tried to decide between?.3.2 Parse FeaturesThe benefit of introducing dependency parse fea-tures is that 1) parse features capture contextual in-formation in a larger context window; 2) parse fea-tures specify which words in the context are salientto the usage of the target word while purely lexi-cally based approaches treat all words in the contextequally.
We use the Stanford dependency parser (deMarneffe et al, 2006) to extract six relevant featureclasses.Parse Features (PAR):1. relation names (target word as head)2. complement of the target word3.
combination of 1 and 24. relation names (target word as complement)5. head of the target word6.
combination of 4 and 5Each of these six classes of PAR features cancontain zero to many values, since the target wordcan be involved in none to multiple grammaticalrelations and features of different filler words aremerged together.
The PAR features, like the LEXfeatures, are binary.
In Table 1, we present the parsefeatures for an example sentence.
The parse fea-tures here are listed as string values, but are laterconverted into binary numbers in the vectors for theSVM model.4 Distributional Word Co-occurrenceThough lexical and parse features are complemen-tary to n-gram models, they are learned from a nor-mal training corpus and may not have enough cov-erage due to data sparsity.
Take a sentence from theNYT for example: ?An economist, he began his ca-reer as a professor ?
he is still called ?the professor,?by friends as a compliment and by foes as an insult ?and taught at Harvard and Stanford .?
If the most in-dicative word ?friends?
does not appear or does notappear enough times in the local context or depen-dencies with ?compliment?
as compared to ?com-plement?
in the training corpus, then the classifiermay be unable to make the correct selection.It is impractical and computationally costly to en-large the training corpus without limit to includeall possible language phenomena.
A good compro-mise is to use word co-occurrence information fromweb-scale data.
The other option is to make use ofhigh-order word co-occurrence, which is included inmany semantic word relatedness measures, such asLatent Semantic Analysis (LSA) (Landauer et al,1998; Deerwester et al, 1990) or Random Indexing,both of which can be estimated from a moderate-sizecorpus.Our intuition is to choose the confusion wordwhich is most relevant to a given context.
We definethe salient words in context as a set M=m1, m2, m3,..., and the relevance between two words as a func-tion Relevance(w1, w2), which can either be calcu-lated fromword co-occurrence or Random Indexing.The score of each candidate word c in the confusionset given a context with meaningful words M is cal-culated by the following formula:Score(c) =?m?MRelevance(c,m)In this paper, we experiment with first-order wordco-occurrence and Random Indexing as relevancemeasures.
And we define salient contextual wordsas heads or complements in the dependency rela-tions with the target word.
In this way, we use theparse information to constrain the two distributionmodels.
Thus the word co-occurrence information1294Feature Name PAR Features (compliment) PAR Features (complement)1.
Head Relation Name ccomp appos2.
Head of Relation says collisions3.
Head Combination ccomp says appos collisions4.
Comp Relation Name nsubj dep5.
Comp of Relation addition rating6.
Comp Combination nsub addition dep ratingTable 1: Parse Feature Example for the sentence: ?GM says the addition of OnStar, which includes a system thatautomatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue?s top five-starsafety rating for the driver and front passenger in both front- and side-impact crash tests.
?considerably overlaps with some values of the PARfeatures, but provides extra evidence from web-scaledata rather than a limited amount of training data.4.1 First-order Word Co-occurrenceThe relevance based on first-order word co-occurrence is calculated from the Google Web 5-gram Corpus in a fashion similar to how we dealtwith n-gram counts in the previous section.
Giventwo words, w1 and w2, we consider all 8 possiblepatterns that appear in a local context (5-word win-dow), where we use wildcard (*) to indicate any to-ken:w1 w2w1 * w2w1 * * w2w1 * * * w2w2 w1w2 * w1w2 * * w1w2 * * * w1The relevance is then calculated by summing thelogarithm of each of the 8 different counts.
Finally,we compare the score of each candidate word andoutput the one with higher score.4.2 Random IndexingThe relevance scores based on Random Indexingare provided by a tool FRanI (Higgins, 2004) anda model trained on the Touchstone Applied ScienceAssociates (TASA) corpus which contains 750k sen-tences and covers diverse topics (from a diversity oftextbooks up to the college level).
Take the sentenceat the beginning of this section for example, whereonly the words ?a?
and ?friends?
are related to thetarget word (either ?complement?
or ?compliment?
)by either relevance measure.
The relevance basedon Random Indexing for (complement, friends) is0.08, (compliment, friends) is 0.19 and both (com-pliment, a) and (complement, a) are 0 because ?a?is in the stop word list.
Meanwhile, the relevancebased on first order word co-occurrence for (com-pliment, friends) is 7.39, (complement, friends) is5.38, (compliment, a) is 13.25, and (complement, a)is 13.42.
The system with either kind of relevanceoutputs ?compliment?.4.3 System CombinationSince the numeric measurement of word co-occurrence is not as specific as the PAR features andless trustworthy, adding word co-occurrence infor-mation as features into the classifier along with n-gram counts, lexical and parse features will hurt theoverall performance.
It is more practical to combinethe two approaches in the following fashion:1.
When the SVM classifier (using NG, LEX andPAR features) has high confidence (over a cer-tain threshold) in the output label, output thatlabel;2.
Otherwise, output the results of the wordrelatedness/co-occurrence-based system.5 EvaluationWe evaluate the effectiveness of syntactic and dis-tributional information on spelling correction.
Theperformance of the system is measured by accu-racy: the percentage of sentences in the test datafor which the system chooses the correct word.
Wecompare our results against two baselines: 1) MA-JOR chooses the most frequent candidate from the1295confusion set in the training corpus, and 2) Bergsmaet al?s (2010) best systems, NG+LEX.
We includeinflectional variants (?-ing?, ?-ed?, ?-s?, ?-ly?)
ofconfusion words in the evaluation, such as comple-menting, complimenting in addition to complement,compliment, because this better corresponds to therange of errors that may be encountered in actualuse and thus increases the scope of the system as areal world application.
Also following Bergsma etal.
(2010), we use a linear SVM, more exactly, theL2-regularized L2-loss dual SVM in LIBLINEAR(Fan et al, 2008).
Unlike Bergsma et al, who useddevelopment data to optimize parameters, we alwaysuse default parameters, since training data is limitedfor many of the words we are dealing with.5.1 DataFollowing Bergsma et al (2009; 2010), the testexamples are extracted from The New York Times(NYT) portion of Gigaword1, but constrained to a9-month publication time frame from October 2005to July 2006.
Unlike Bergsma et al who use thesame source as training data for the lexical features,our training data (for both lexical and parse features)comes from larger and more diverse news sources.We use the very large database from Sekine?s n-gramsearch engine (Sekine, 2008) as training data, whichconsists of 1.9B words of newspaper text spanning89 years from NYT, BBC, WSJ, Xinhua, etc.We evaluate our systems on 5 confusion sets fromBergsma et al (2009; 2010) and 9 commonly con-fused word pairs with moderate frequency in dailyusage (randomly selected from those listed in En-glish educational resources2).
Shown in Table 2,these 9 sets of words appear much less frequentlythan the words selected by Bergsma et al, evengiven the fact that we are using a considerably largetraining corpus.For each confusable word pair, sentences thatcontain either of the words are extracted to formtraining and test data.
The word that appears in theoriginal sentences of the news article is treated asthe gold standard.
For frequently occurring confu-sion word sets used by Bergsma et al, we extractup to 10k examples for testing, and up to 100k ex-1Available from the LDC as LDC2003T052Such as an English learning blog post athttp://elisaenglish.pixnet.net/blog/post/1335194Word Confusion Set # in Training Corpusadverse / averse 13.5k / 1.8kadvice / advise 62.k / 12.9kallusion / illusion 1.0k / 5.4kcomplement / compliment 6.8k / 3.1kconfidant / confident 2.4k / 63.6kdesert / dessert 24.7k / 3.7kdiscreet / discrete 0.7k / 2.4kelicit / illicit 1.9k / 10.0kstationary / stationery 2.5k/2.3kwander / wonder 3.3k / 39.5kTable 2: Training Data Sizes for Common ESL ConfusedWordsamples for training.
For the 9 less frequent confu-sion word sets, we extract all the unique examplesfor training and testing from the above sources.
Thespelling correction system is evaluated by measur-ing its accuracy in comparison to the gold standardin test data.
The error rate is the complement of ac-curacy.Following Carlson et al (2007) and Bergsmaet al (2009; 2010), we obtain the n-gram countsfrom the GoogleWeb 1T 5-gram Corpus (Brants andFranz, 2006).5.2 Experimental ResultsWe present the results for each set separately be-cause each set may behave very differently, depend-ing upon its frequency, part-of-speech, number ofsenses and other differences between the words ineach confusion set.
The overall accuracy across con-fusion sets is also presented to show the effective-ness of different approaches.
The results are testedfor statistical significance using McNemar?s test ofcorrelated proportions.
The performance differencesare marked as significant when p < 0.05.5.2.1 Effectiveness of Parse FeaturesWe exploit the n-gram counts (NG), lexical fea-tures (LEX) of Bergsma et al (2010) and our ownparse features (PAR) in linear SVM models.The first comparison is between the supervisedlearning systems with LEX and LEX+PAR.
Asshown in Table 3, by exploiting our unique parsefeatures, for the total 14 confusion sets, the accuracyincreases on 12 sets and decreases on 2 sets.
Over-all, the spelling correction accuracy improves an ab-1296solute 1.35% for our 9 confusion sets and 0.60% forBergsma et al?s 5 confusion sets.The second comparison is to see how parse fea-tures interact with n-gram count features in a su-pervised classifier.
The best system from (Bergsmaet al, 2010) is listed in the table as ?NG+LEX?.As shown in Table 3, the parse features proved tobe beneficial when augmenting this baseline, exceptfor the decrease in accuracy on adverse, averse byonly 2 cases out of 368, and among, between by2 cases out of 10227.
For all other confusion sets,parse features decrease the error rate by as much as2.74% (absolute) and as much as 38.5% (relative).Improvements are statistically significant on all con-fusion sets together, although for each separate set,improvements are significant on only 5 sets, in partdue to an insufficient number of test cases.The reason that parse features are occasionally nothelpful is because they sometimes include an un-common word in dependencies, which happens toappear once with the wrong word but not with thecorrect word in the training data; or they sometimesinclude too common words, which bias the classifierin favor of the more frequent word in the confusionset.
We also noticed that lexical features are not al-ways helpful when added to n-gram count features,even for in-domain applications (i.e., with trainingdata and test data coming from the same domain orcorpus), as marked by underlines.
However, lexicaland parse features together show more significantand constant improvement over n-gram count-basedmodels, as marked by ?.Of the six systems, every system that uses parsefeatures gets the example correct in Section 1, ?com-plementing the president?
; LEX by itself also getsthe example correct, but NG and NG+LEX fail.In summary, our system NG+LEX+PAR outper-forms the state-of-the-art system NG+LEX.
It re-duces the error rate by 12.4% across our 9 confusionsets and by 8.4% across Bergsma et al?s 5 confusionsets.
Both improvements are significant (p < 0.05)by the McNemar test.
In addition, while NG+LEXis not always better than NG, NG+LEX+PAR is con-sistently better than NG.5.2.2 Impact of Word Co-occurrenceThe LIBLINEAR tool does not provide probabil-ity estimates for SVM models but Logistic Regres-sion can.
In this set of experiments, we train a Logis-tic Regression model with NG+LEX+PAR featuresand empirically set the confidence threshold at 0.6,as described in Section 4, based on the performanceon two word pairs.
In the combined system, whenthe Logistic Regression model estimates a probabil-ity higher than the threshold we output its results,otherwise we output the result of the system basedon word co-occurrence.Surprisingly, although Random Indexing takesinto account more information than first-order wordco-occurrence, it lowered overall performance sub-stantially.
Thus in Table 4, we only present resultsof using first-order word co-occurrence rather thanRandom Indexing.
For all 12 confusion sets, distri-butional word co-occurrence information improves9 sets and hurts 5 sets.
Overall, it reduces the er-ror rate slightly by 0.2% for our 9 sets and 1.5% forBergsma et al?s sets.We believe there are two reasons why Ran-dom Indexing fared worse than first-order wordco-occurrence: 1) Random Indexing considers co-occurrence on a document level, while our first-order word co-occurrence is limited to a 5-word win-dow context.
The latter is more suitable to context-sensitive spelling correction.
2) The model for Ran-dom Indexing is trained on a relatively small sizecorpus compared to the web-scale data we used toget n-gram count features for the classifier and thusis not able to introduce much new evidence besidesthe information carried by NG+LEX+PAR features.Reason 2) also suggests why first-order co-occurrence helps on some occasions while not onother occasions.
Its impact is limited because theword co-occurrence information overlaps with someof the PAR feature values as mentioned earlier.
Itimproves some cases because it provides some newevidence from web-scale data to the system based onNG+LEX+PAR features.
It introduces new errorsbecause it simply favors the word that co-occurredmore often regardless of other factors.
Its impact isalso limited because it is only considered when clas-sifiers with NG+LEX+PAR features are not confi-dent.1297CONFUSION SET # TEST MAJOR LEX LEX+PAR NG NG+LEX NG+LEX+PAR (&)9 commonly cited ESL confusion pairsadverse / averse 368 85.87 97.01 96.74 91.03 97.55 97.01 (+22.2%) ?allusion / illusion 535 76.64 91.22 91.40 91.40 92.52 93.08 (-7.5%) ?complement / compliment 860 51.51 83.84 85.12 88.49 88.37 89.53 (-10.0%)confidant / confident 2416 94.41 97.97 98.30 98.51 99.05 99.09 (-4.3%) ?desert / dessert 2357 70.81 90.71 91.56 87.31 93.68 94.57 (-14.1%) ?
*discreet / discrete 219 79.45 84.48 85.84 85.84 90.41 91.32 (-9.5%) ?elicit / illicit 563 53.46 82.77 95.56 97.51 97.51 98.22 (-28.6%)stationary / stationery 182 62.64 87.36 92.31* 93.96 92.86 95.60 (-38.5%)wander / wonder 6506 86.37 96.42 97.42* 97.56 98.23 98.48 (-13.9%) ?
*Total 13972 81.08 93.94 95.29* 94.82 96.56 96.99 (-12.4%) ?
*5 Original Bergsma pairs# among / between 10227 57.46 91.89 91.86 88.34 93.60 93.58 (+3.1%) ?# amount / number 7398 76.44 92.34 93.16* 93.03 93.42 94.08 (-10.1%) ?
*# cite / site 10185 95.71 99.42 99.53 99.16 99.52 99.63 (-22.4%)?# peace / piece 7330 56.81 95.01 97.01* 95.55 96.74 97.46 (-22.2%)?
*# raise / rise 9464 55.98 96.12 96.64* 94.45 96.68 97.05 (-11.5%) ?Total 44604 68.92 95.09 95.69* 94.07 96.09 96.42 (-8.4%) ?Table 3: Spelling correction precision (%), impact of adding parse featuresSVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.?
: Improvement of NG+LEX+PAR vs. NG is statistically significant.&: Relative increase or decrease of error rate compared to ?NG+LEX?#: As in Bergsma et al (2009; 2010) no morphological variants of the words are used in evaluationCONFUSION SET # TEST MAJOR CLASSIFIER COMBINED SYSTEM (&)9 commonly cited ESL confusion pairsadverse / averse 368 85.87 97.55 96.74 (+33.3%)allusion / illusion 535 76.64 92.34 92.34 (- 0.0%)complement / compliment 860 51.51 89.88 90.81 (-9.2%)confidant / confident 2416 94.41 99.13 99.05 (+9.5%)desert / dessert 2357 70.81 93.98 94.23 (-3.7%)discreet / discrete 219 79.45 90.41 91.78 (-14.3%)elicit / illicit 563 53.46 98.40 98.76 (-22.2%)stationary / stationery 182 62.64 93.41 93.96 (-9.1%)wander / wonder 6506 86.37 98.49 98.36 (+9.2%)5 Original Bergsma pairs# among / between 10227 57.46 92.73 92.73 (-0.1%)# amount / number 7398 76.44 93.44 93.76 (-4.74%)# cite / site 10185 95.71 99.49 99.47 (+3.8%)# peace / piece 7330 56.81 96.19 96.38 (-5.0%)# raise / rise 9464 55.98 96.66 96.59 (+2.2%)Table 4: Spelling correction accuracy (%), impact of combining word co-occurrenceCLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.&: Relative increase or decrease in error rate compared to CLASSIFIER#: As in Bergsma et al (2009; 2010), no morphological variants of the words are used in evaluation12986 ConclusionsWe propose a novel approach that uses parsefeatures and lexical features together to improvethe performance of web-scale n-gram models forspelling correction.
This method is especially adap-tive when less training data are available, which isthe case for confusable words that are not very fre-quently used.
We also investigate the effectivenessof incorporating web-scale word co-occurrence andcorpus-based semantic word relatedness (RandomIndexing).For future work, we will investigate using seman-tic information (e.g.
WordNet) to extend n-grammodels.
It will be interesting to see if the usage ofthe word ?compliment?
in ?complimenting the pres-ident?
can be estimated by considering similar us-ages in the corpus, such as ?complimenting the stu-dent?
or by creating an n-gram database of synsetpatterns.
We will investigate extending, to other ap-plications, this general methodology combining dis-tributional, semantic and syntactic information withlanguage models.AcknowledgmentsWe wish to thank Michael Flor of EducationalTesting Service for his TrendStream tool, whichprovides fast access and easy manipulation of theGoogle N-gram Corpus.
We also thank Derrick Hig-gins of Educational Testing Service for his RandomIndexing support.
We also thank Satoshi Sekine ofNew York University, Matthew Snover of City Uni-versity of New York, and Jing Jiang of SingaporeManagement University for their advice.ReferencesShane Bergsma, Dekang Lin, and Randy Goebel.
2009.Web-scale n-gram models for lexical disambiguation.In IJCAI.Shane Bergsma, Emily Pitler, and Dekang Lin.
2010.Creating robust supervised classifiers via web-scale n-gram data.
In ACL.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.
Available at http://www.ldc.upenn.edu.Alexander Budanitsky and Graeme Hirst.
2001.
Seman-tic distance in wordnet: An experimental, application-oriented evaluation of five measures.
In ACL Work-shop on WordNet and Other Lexical Resources.Andrew Carlson and Ian Fette.
2007.
Memory-basedcontext sensitive spelling correction at web scale.
InICMLA.Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007.Detection of grammatical errors involving preposi-tions.
In Proceedings of the Fourth ACL-SIGSEMWorkshop on Prepositions, pages 25?30.Silviu Cucerzan and David Yarowsky.
2002.
Aug-mented mixture models for lexical disambigua-tion.
InEMNLP.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, Genoa, Italy.Scott Deerwester, Susan Dumais, George Furmas,Thomas Landauer, and Richar Harshman.
1990.
In-dexing by latent semantic analysis.
The American So-ciety for Information Science.Dmitriy Dligach and Martha Palmer.
2008.
Novel se-mantic features for verb sense disambiguation.
InACL.Philip Edmonds.
1997.
Choosing the word most typicalin context using a lexical co-occurrence network.
InEACL.Mohammed Ali Elmi and Martha Evans.
1998.
Spellingcorrection using context.
In COLING.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
Machine Learning Re-search, 9(1871-1874).Rachele De Felice and Stephen G. Pulman.
2008.A classifier-based approach to preposition and deter-miner error correction in L2 English.
In Proceedingsof COLING, Manchester, UK.Rachele De Felice and Stephen G. Pulman.
2009.
Auto-matic detection of preposition errors in learner writing.CALICO Journal, 26(3).Michael Gamon, Jianfeng Gao, Chris Brockett, AlexKlementiev, William B. Dolan, Dmitriy Belenko, andLucy Vanderwende.
2008.
Using contextual spellertechniques and language modeling for ESL error cor-rection.
In Proceedings of the International Joint Con-ference on Natural Language Processing (IJCNLP),pages 449?456, Hyderabad, India.Andrew Golding and Dan Roth.
1996.
Applying Win-now to context-sensitive spelling correction.
In Pro-ceedings of the International Conference on MachineLearning (ICML), pages 182?190.Andrew Golding and Dan Roth.
1999.
A winnow-basedapproach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107?130.Andrew Golding.
1995.
A Bayesian hybrid method forcontext sensitive spelling correction.
In Proceedings1299of the Third Workshop on Very Large Corpora (WVLC-3), pages 39?53.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2):115?129.Matthieu Hermet, Alain De?silets, and Stan Szpakowicz.2008.
Using the web as a linguistic resource to au-tomatically correct lexico-syntactic errors.
In Pro-ceedings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC), pages 390?396, Marrekech, Morocco.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, ThepchaiSupnithi, and Hitoshi Isahara.
2003.
Automatic er-ror detection in the Japanese learners?
English spokendata.
In Companion Volume to the Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 145?148.Michael Jones and James Martin.
1997.
Contextualspelling correction using latent semantic analysis.
InANLC.Thomas Landauer, Darrell Laham, and Peter Foltz.
1998.Learning human-like knowledge by singular value de-composition: A progress report.
Advances in NeuralInformation Processing Systems, 10:45?51.Mirella Lapata and Frank Keller.
2005.
Web-based mod-els for natural language processing.
ACM Transac-tions on Speech and Language Processing, 21:1?31.John Lee and Ola Knutsson.
2008.
The role of pp attach-ment in preposition generation.
In CICLING.Lidia Mangu and Eric Brill.
1997.
Automatic rule acqui-sition for spelling correction.
In ICML.Saif Mohammad and Graeme Hist.
2006.
Distributionalmeasures of concept distance: A task-oriented evalua-tion.
In EMNLP.Alla Rozovskaya and Dan Roth.
2010.
Trainingparadigms for correcting errors in grammar and usage.In ACL.Magnus Sahlgren.
2006.
The Word-Space Model: us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis.Joel Tetreault and Martin Chodorow.
2008.
The ups anddowns of prepostion error detection in esl writing.
InCOLING.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In ACL.Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, andGerard Ellis.
2009.
Using the web for language inde-pendent spellchecking and autocorrection.
In ACL.1300
