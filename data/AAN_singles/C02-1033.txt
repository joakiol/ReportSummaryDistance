Using collocations for topic segmentation and link detectionOlivier FERRETCEA ?
LIST/LIC2MRoute du Panorama ?
BP 692265 Fontenay-aux-Roses Cedexolivier.ferret@cea.frAbstractWe present in this paper a method for achiev-ing in an integrated way two tasks of topicanalysis: segmentation and link detection.
Thismethod combines word repetition and the lexi-cal cohesion stated by a collocation network tocompensate for the respective weaknesses ofthe two approaches.
We report an evaluationof our method for segmentation on two cor-pora, one in French and one in English, andwe propose an evaluation measure that spe-cifically suits that kind of systems.1 IntroductionTopic analysis, which aims at identifying the top-ics of a text, delimiting their extend and findingthe relations between the resulting segments, hasrecently raised an important interest.
The largestpart of it was dedicated to topic segmentation, alsocalled linear text segmentation, and to the TDT(Topic Detection and Tracking) initiative (Fiscuset al, 1999), which addresses all the tasks we havementioned but from a domain-dependent view-point and not necessarily in an integrated way.Systems that implement this work can be catego-rized according to what kind of knowledge theyuse.
Most of those that achieve text segmentationonly rely on the intrinsic characteristics of texts:word distribution, as in (Hearst, 1997), (Choi,2000) and (Utiyama and Isahara, 2001), or lin-guistic cues as in (Passonneau and Litman, 1997).They can be applied without restriction about do-mains but have low results when a text doesn?tcharacterize its topical structure by surface clues.Some systems exploit domain-independent knowl-edge about lexical cohesion: a network of wordsbuilt from a dictionary in (Kozima, 1993); a largeset of collocations collected from a corpus in (Fer-ret, 1998), (Kaufmann, 1999) and (Choi, 2001).
Tosome extend, this knowledge permits these sys-tems to discard some false topical shifts withoutlosing their independence with regard to domains.The last main type of systems relies on knowledgeabout the topics they may encounter in the textsthey process.
This is typically the kind of approachdeveloped in TDT where this knowledge is auto-matically built from a set of reference texts.
Thework of Bigi (Bigi et al, 1998) stands in the sameperspective but focuses on much larger topics thanTDT.
These systems have a limited scope due totheir topic representations but they are also moreprecise for the same reason.Hybrid systems that combine the approaches wehave presented were also developed and illustratedthe interest of such a combination: (Jobbins andEvett, 1998) combined word recurrence, colloca-tions and a thesaurus; (Beeferman et al, 1999)relied on both collocations and linguistic cues.The topic analysis we propose implements such ahybrid approach: it relies on a general languageresource, a collocation network, but exploits ittogether with word recurrence in texts.
Moreover,it simultaneously achieves topic segmentation andlink detection, i.e.
determining whether two seg-ments discuss the same topic.We detail in this paper the implementation of thisanalysis by the TOPICOLL system, we reportevaluations of its capabilities concerning segmen-tation for two languages, French and English, andfinally, we propose an evaluation measure thatintegrates both segmentation and link detection.2 Overview of TOPICOLLIn accordance with much work about discourseanalysis, TOPICOLL processes texts linearly: it de-tects topic shifts and finds links between segmentswithout delaying its decision, i.e., by only takinginto account the part of text that has been alreadyanalyzed.
A window that delimits the current focusof the analysis is moved over each text to be proc-essed.
This window contains the lemmatized con-tent words of the text, resulting from itspre-processing.
A topic context is associated tothis focus window.
It is made up of both the wordsof the window and the words that are selectedfrom a collocation network1 as strongly linked tothe words of the window.
The current segment isalso given a topic context.
This context resultsfrom the fusion of the contexts associated to thefocus window when this window was in the seg-ment space.
A topic shift is then detected when thecontext of the focus window and the context of thecurrent segment are not similar any more for seve-ral successive positions of the focus window.
Thisprocess also performs link detection by comparingthe topic context of each new segment to the con-text of the already delimited segments.The use of a collocation network permitsTOPICOLL to find relations beyond word recur-rence and to associate a richer topical representa-tion to segments, which facilitates tasks such aslink detection or topic identification.
But worksuch as (Kozima, 1993), (Ferret, 1998) or (Kauf-mann, 1999) showed that using a domain-independent source of knowledge for text seg-mentation doesn?t necessarily lead to get betterresults than work that is only based on word distri-bution in texts.
One of the reasons of this fact isthat these methods don?t precisely control the re-lations they select or don?t take into account thesparseness of their knowledge.
Hence, while theydiscard some incorrect topic shifts found by meth-ods based on word recurrence, they also find in-correct shifts when the relevant relations are notpresent in their knowledge or don?t find some cor-rect shifts because of the selection of non relevantrelations from a topical viewpoint.
By combiningword recurrence and relations selected from acollocation network, TOPICOLL aims at exploitinga domain-independent source of knowledge fortext segmentation in a more accurate way.3 Collocation networksTOPICOLL depends on a resource, a collocationnetwork, that is language-dependent.
Two collo-cation networks were built for it: one for French,1 A collocation network is a set of collocations betweenwords.
This set can be viewed as a network whosenodes are words and edges are collocations.from the Le Monde newspaper (24 months be-tween 1990 and 1994), and one for English, fromthe L.A. Times newspaper (2 years, part of theTREC corpus).
The size of each corpus was around40 million words.The building process was the same for the twonetworks.
First, the initial corpus was pre-processed in order to characterize texts by theirtopically significant words.
Thus, we retained onlythe lemmatized form of plain words, that is, nouns,verbs and adjectives.
Collocations were extractedaccording to the method described in (Church andHanks, 1990) by moving a window on texts.
Pa-rameters were chosen in order to catch topicalrelations: the window was rather large, 20-wordwide, and took into account the boundaries oftexts; moreover, collocations were indifferent toword order.
We also adopted an evaluation ofmutual information as a cohesion measure of eachcollocation.
This measure was normalized ac-cording to the maximal mutual information rela-tive to the considered corpus.After filtering the less significant collocations(collocations with less than 10 occurrences andcohesion lower than 0.1), we got a network withapproximately 23,000 words and 5.2 million col-locations for French, 30,000 words and 4.8 millioncollocations for English.4 Description of TOPICOLLTOPICOLL is based on the creation, the update andthe use of a topical representation of both the seg-ments it delimits and the content of its focus win-dow at each position of a text.
These topical repre-sentations are called topic contexts.
Topic shiftsare found by detecting that the topic context of thefocus window is not similar anymore to the topiccontext of the current segment.
Link detection isperformed by comparing the context of a newsegment to the context of the previous segments.4.1 Topic contextsA topic context characterizes the topical dimen-sion of the entity it is associated to by two vectorsof weighted words.
One of these vectors, calledtext vector, is made up of words coming from thetext that is analyzed.
The other one, called collo-cation vector, contains words selected from a col-location network and strongly linked to the wordsof the processed text.
For both vectors, the weightof a word expresses its importance with regard tothe other words of the vector.4.1.1 Topic context of the focus windowThe text vector of the context associated to thefocus window is made up of the content words ofthe window.
Their weight is given by:)()()( wsignifwoccNbwwght txt ?= (1)where occNb(w) is the number of occurrences ofthe word w in the window and signif(w) is the sig-nificance of w. The weight given by (1) combinesthe importance of w in the part of text delimited bythe window and its general significance.
This sig-nificance is defined as in (Kozima, 1993) as itsnormalized information in a reference corpus2:)1(log)(log)(22ccwSSfwsignif--= (2)where fw is the number of occurrences of the wordw in the corpus and Sc, the size of the corpus.0.140.21 0.100.18 0.130.17w5w4w3w2w10.48 = pw3x0.18+pw4x0.13+pw5x0.17selected word from the collocation network (with its weight)1.0word from text (with p its weight in the window, equal to0.21 link in the collocation network (with its cohesion value)1.0 1.0 1.0 1.0 1.0wi,n1 n21.0 for all words of the window in this example)0.48Figure 1 ?
Selection and weighting of words fromthe collocation networkThe building of the collocation vector for the win-dow?s context comes from the procedure presentedin (Ferret, 1998) for evaluating the lexical cohe-sion of a text.
It consists in selecting words of thecollocation network that are topically close tothose in the window.
We assume that this close-ness is related to the number of links that existbetween a word of the network and the words ofthe window.
Thus, a word of the network is se-2 In our case, this is the corpus used for building thecollocation network.lected if it is linked to at least wst (3 in our ex-periments) words of the window.
A collocationvector may also contain some words of the win-dow as they are generally part of the collocationnetwork and may be selected as its other words.Each selected word from the network is then as-signed a weight.
This weight is equal to the sum ofthe contributions of the window words to which itis linked to.
The contribution of a word of thewindow to the weight of a selected word is equalto its weight in the window, given by (1), modu-lated by the cohesion measure between these twowords in the network (see Figure 1).
More pre-cisely, the combination of these two factors isachieved by a geometric mean:?
?=iiitxtcoll wwcohwwgthwwght ),()()( (3)where coh(w,wi) is the measure of the cohesionbetween w and wi in the collocation network.4.1.2 Topic context of a segmentThe topic context of a segment results from thefusion of the contexts associated to the focus win-dow when it was inside the segment.
The fusion isachieved as the segment is extended: the contextassociated to each new position of the segment iscombined with the current context of the segment.This combination, which is done separately fortext vectors and collocation vectors, consists inmerging two lists of weighted words.
First, thewords of the window context that are not in thesegment context are added to it.
Then, the weightof each word of the resulting list is computed ac-cording to its weight in the window context and itsprevious weight in the segment context:)),,()(()1,,(),,(tCwwwghtwsigniftCswwghttCswwghtxxx?+-=(4)with Cw, the context of the window, Cs, the con-text of the segment and wghtx(w,C{s,w},t), theweight of the word w in the vector x (txt or coll) ofthe context C{s,w} for the position t. For the wordsfrom the window context that are not part of thesegment context, wghtx(w,Cs,t-1) is equal to 0.The revaluation of the weight of a word in a seg-ment context given by (4) is a solution halfwaybetween a fast and a slow evolution of the contentof segment contexts.
The context of a segment hasto be stable because if it follows too narrowly thetopical evolution of the window context, topicshifts could not be detected.
However, it must alsoadapt itself to small variations in the way a topic isexpressed when progressing in the text in order notto detect false topic shifts.4.1.3 Similarity between contextsIn order to determine if the content of the focuswindow is topically coherent or not with the cur-rent segment, the topic context of the window iscompared to the topic context of the segment.
Thiscomparison is performed in two stages: first, asimilarity measure is computed between the vec-tors of the two contexts; then, the resulting valuesare exploited by a decision procedure that states ifthe two contexts are similar.As (Choi, 2000) or (Kaufmann, 1999), we use thecosine measure for evaluating the similarity be-tween a vector of the context window (Vw) and theequivalent vector in the segment context (Vs):????
?=iixiixixiixxxCwwwgCswwgCwwgwCswwgVwVssim22 ),(),(),(),(),((5)where wgx(wi,C{s,w}) is the weight of the word wi inthe vector x (txt or coll) of the context C{s,w}.As we assume that the most significant words of asegment context are the most recurrent ones, thesimilarity measure takes into account only thewords of a segment context whose the recurrence3is above a fixed threshold.
This one is higher fortext vectors than for collocation vectors.
This fil-tering is applied only when the context of a seg-ment is considered as stable (see 4.2).The decision stage takes root in work about com-bining results of several systems that achieve thesame task.
In our case, the evaluation of the simi-larity between Cs and Cw at each position is basedon a vote that synthesizes the viewpoint of the textvector and the viewpoint of the collocation vector.First, the value of the similarity measure for eachvector is compared to a fixed threshold and a posi-3 The recurrence of a word in a segment context is gi-ven by the ratio between the number of windowcontexts in which the word was present and the numberof window contexts gathered by the segment context.tive vote in favor of the similarity of the two con-texts is decided if the value exceeds this threshold.Then, the global similarity of the two contexts isrejected only if the votes for the two vectors arenegative.4.2 Topic segmentationThe algorithm for detecting topic shifts is takenfrom (Ferret and Grau, 2000) and basically relieson the following principle: at each text position, ifthe similarity between the topic context of thefocus window and the topic context of the currentsegment is rejected (see 4.1.3), a topic shift is as-sumed and a new segment is opened.
Otherwise,the active segment is extended up to the currentposition.This algorithm assumes that the transition betweentwo segments is punctual.
As TOPICOLL only oper-ates at word level, its precision is limited.
Thisimprecision makes necessary to set a short delaybefore deciding that the active segment really endsand similarly, before deciding that a new segmentwith a stable topic begins.
Hence, the algorithmfor detecting topic shifts distinguishes four states:?
the NewTopicDetection state takes place when anew segment is going to be opened.
This openingis then confirmed provided that the content of thefocus window context doesn?t change for severalpositions.
Moreover, the core of the segment con-text is defined when TOPICOLL is in this state;?
the InTopic state is active when the focus win-dow is inside a segment with a stable topic;?
the EndTopicDetection state occurs when thefocus window is inside a segment but a differencebetween the context of the window and the contextof the current segment suggests that this segmentcould end soon.
As for the NewTopicDetectionstate, this difference has to be confirmed for sev-eral positions before a change of state is decided;?
the OutOfTopic state is active between twosegments.
Generally, TOPICOLL stays in this stateno longer than 1 or 2 positions but when neitherthe words from text nor the words selected fromthe collocation network are recurrent, i.e.
no stabletopic can be detected according to these features,this number of positions may be equal to the sizeof a segment.The transition from one state to another followsthe automaton of Figure 2 according to three pa-rameters:?
its current state;?
the similarity between the context of the focuswindow and the context of the current segment:Sim or no Sim;?
the number of successive positions of the focuswindow for which the current state doesn?tchange: confirmNb.
It must exceed the Tconfirmthreshold (equal to 3 in our experiments) for leav-ing the NewTopicDetection or the EndTopicDe-tection state.NewTopicDetectionEndTopicDetectionOutOfTopicInTopicSimSimSimno Simno Simno Simno Sim&confirmNb =Tconfirmno Sim&confirmNb <TconfirmSim&confirmNb =TconfirmSim&confirmNb <TconfirmFigure 2 ?
Automaton for topic shift detectionThe processing of a segment starts with the Ou-tOfTopic state, after the end of the previous seg-ment or at the beginning of the text.
As soon as thecontext of the focus window is stable enough be-tween two successive positions, TOPICOLL entersinto the NewTopicDetection state.
The InTopicstate can then be reached only if the window con-text is found stable for the next confirmNb-1 posi-tions.
Otherwise, TOPICOLL assumes that it is afalse alarm and returns to the OutOfTopic state.The detection of the end of a segment is symmetri-cal to the detection of its beginning.
TOPICOLLgoes into the EndTopicDetection state as soon asthe content of the window context begins tochange significantly between two successive posi-tions but the transition towards the OutOfTopicstate is done only if this change is confirmed forthe next confirmNb-1 next positions.This algorithm is completed by a specific mecha-nism related to the OutOfTopic state.
WhenTOPICOLL stays in this state for a too long time(this time is defined as 10 positions of the focuswindow in our experiments), it assumes that thetopic of the current part of text is difficult to char-acterize by using word recurrence or selectionfrom a collocation network and it creates a newsegment that covers all the concerned positions.4.3 Link detectionThe algorithm of TOPICOLL for detecting identitylinks between segments is closely associated to itsalgorithm for delimiting segments.
When TO-PICOLL goes from the NewTopicDetection state tothe InTopic state, it first checks whether the cur-rent context of the new segment is similar to oneof the contexts of the previous segments.
In thiscase, the similarity between contexts only relies onthe similarity measure (see (5)) between their col-location vectors.
A specific threshold is used forthe decision.
If the similarity value exceeds thisthreshold, the new segment is linked to the corre-sponding segment and takes the context of this oneas its own context.
In this way, TOPICOLL assumesthat the new segment continues to develop a pre-vious topic.
When several segments fulfills thecondition for link detection, TOPICOLL selects theone with the highest similarity value.5 Experiments5.1 Topic segmentationFor evaluating TOPICOLL about segmentation, weapplied it to the ?classical?
task of discoveringboundaries between concatenated texts.
TOPICOLLwas adapted for aligning boundaries with ends ofsentences.
We used the probabilistic error metricPk proposed in (Beeferman et al, 1999) for meas-uring segmentation accuracy4.
Recall and precisionwas computed for the Le Monde corpus to com-pare TOPICOLL with older systems5.
In this case,the match between a boundary from TOPICOLL anda document break was accepted if the boundarywas not farther than 9 plain words.5.1.1 Le Monde corpusThe evaluation corpus for French was made up of49 texts, 133 words long on average, from the Le4 Pk evaluates the probability that a randomly chosenpair of words, separated by k words, is wrongly classi-fied, i.e.
they are found in the same segment byTOPICOLL while they are actually in different ones (missof a document break) or they are found in differentsegments by TOPICOLL while they are actually in thesame one (false alarm).5 Precision is given by Nt / Nb and recall by Nt / D, withD the number of document breaks, Nb the number ofboundaries found by TOPICOLL and Nt the number ofboundaries that are document breaks.Monde newspaper.
Results in Tables 1 and 2 areaverage values computed from 10 different se-quences of them.
The baseline procedure consistedin randomly choosing a fixed number of sentenceends as boundaries.
Its results in Tables 1 and 2are average values from 1,000 draws.Systems Recall Precision F1-measurebaseline 0.51 0.28 0.36SEGCOHLEX 0.68 0.37 0.48SEGAPSITH 0.92 0.52 0.67TextTiling 0.72 0.81 0.76TOPICOLL1 0.86 0.74 0.80TOPICOLL2 0.86 0.78 0.81TOPICOLL3 0.66 0.60 0.63Table 1 ?
Precision/recall for Le Monde corpusTOPICOLL1 is the system described in section 4.TOPICOLL2 is the same system but without its linkdetection part.
The results of these two variantsshow that the search for links between segmentsdoesn?t significantly debase TOPICOLL?s capabili-ties for segmentation.
TOPICOLL3 is a version ofTOPICOLL that only relies on word recurrence.SEGCOHLEX and SEGAPSITH are the systems de-scribed in (Ferret, 1998) and (Ferret and Grau,2000).
TextTiling is our implementation ofHearst?s algorithm with its standard parameters.Systems Miss False alarm Errorbaseline 0.46 0.55 0.50TOPICOLL1 0.17 0.24 0.21TOPICOLL2 0.17 0.22 0.20Table 2 ?
Pk for Le Monde corpusFirst, Table 1 shows that TOPICOLL is more accu-rate when its uses both word recurrence and collo-cations.
Furthermore, it shows that TOPICOLL getsbetter results than a system that only relies on acollocation network such as SEGCOHLEX.
It alsogets better results than a system such as TextTilingthat is based on word recurrence and as TOPICOLL,works with a local context.
Thus, Table 1 confirmsthe fact reported in (Jobbins and Evett, 1998) thatusing collocations together with word recurrenceis an interesting approach for text segmentation.Moreover, TOPICOLL is more accurate than a sys-tem such as SEGAPSITH that depends on topic rep-resentations.
Its accuracy is also slightly higherthan the one reported in (Bigi et al, 1998) for asystem that uses topic representations in a prob-abilistic way: 0.75 as precision, 0.80 as recall and0.77 as f1-measure got on a corpus made of LeMonde?s articles too.5.1.2 C99 corpusFor English, we used the artificial corpus built byChoi (Choi, 2000) for comparing several segmen-tation systems.
This corpus is made up of 700samples defined as follows: ?A sample is a con-catenation of ten text segments.
A segment is thefirst n sentences of a randomly selected documentfor the Brown corpus?.
Each column of Table 3states for an interval of values for n.Systems 3-11 3-5 6-8 9-11baseline 0.45 0.38 0.39 0.36CWM 0.09 0.10 0.07 0.05U00 0.10 0.09 0.07 0.05C99 0.12 0.11 0.09 0.09DotPlot 0.18 0.20 0.15 0.12Segmenter 0.36 0.23 0.33 0.43TextTiling 0.46 0.44 0.43 0.48TOPICOLL1 0.30 0.28 0.27 0.34TOPICOLL2 0.31 0.28 0.28 0.34Table 3 ?
Pk for C99 corpusThe first seven lines of Table 3 results from Choi?sexperiments (Choi, 2001).
The baseline is a proce-dure that partitions a document into 10 segmentsof equal length.
CWM is described in (Choi, 2001),U00 in (Utiyama and Isahara, 2001), C99 in (Choi,2000), DotPlot in (Reynar, 1998) and Segmenterin (Kan et al, 1998).Table 3 confirms first that the link detection partof TOPICOLL doesn?t debase its segmentation ca-pabilities.
It also shows that TOPICOLL?s results onthis corpus are significantly lower than its resultson the Le Monde corpus.
This is partially due toour collocation network for English: its density,i.e.
the ratio between the size of its vocabulary andits number of collocations, is 30% lower than thedensity of the network for French, which has cer-tainly a significant effect.
Table 3 also shows thatTOPICOLL has worse results than systems such asCWM, U00, C99 or DotPlot.
This can be explainedby the fact that TOPICOLL only works with a localcontext whereas these systems rely on the wholetext they process.
As a consequence, they have aglobal view on texts but are more costly thanTOPICOLL from an algorithmic viewpoint.
Moreo-ver, link detection makes TOPICOLL functionallyricher than they are.5.2 Global evaluationThe global evaluation of a system such asTOPICOLL faces a problem: a reference for linkdetection is relative to a reference for segmenta-tion.
Hence, mapping it onto the segments delim-ited by a system to evaluate is not straightforward.To bypass this problem, we chose an approachclose the one adopted in TDT for the link detectiontask: we evaluated the probability of an error inclassifying each couple of positions in a text asbeing part of the same topic (Cpsame) or belongingto different topics (Cpdiff).
A miss is detected if acouple is found about different topics while theyare about the same topic and a false alarm corre-sponds to the complementary case.Systems Miss False alarm Errorbaseline 0.85 0.06 0.45TOPICOLL 0.73 0.01 0.37Table 4 ?
Error rates for Le Monde corpusAs the number of Cpdiff couples is generally muchlarger than the number of Cpsame couples, we ran-domly selected a number of Cpdiff couples equal tothe number of Cpsame couples in order to have alarge range of possible values.
Table 4 shows theresults of TOPICOLL for the considered measureand compares them to a baseline procedure thatrandomly set a fixed number of boundaries and afixed number of links between the delimited seg-ments.
This measure is a first proposition thatshould certainly be improved, especially for bal-ancing more soundly misses and false alarms.6 ConclusionWe have proposed a method for achieving bothtopic segmentation and link detection by usingcollocations together with word recurrence intexts.
Its evaluation showed the soundness of thisapproach for working with a local context.
Weplan to extend it to methods that rely on the wholetext they process.
We also aim at extending theevaluation part of this work by improving theglobal measure we have proposed and by com-paring our results to human judgments.ReferencesBeeferman D., Berger A. and Lafferty J.
(1999) Statis-tical Models for Text Segmentation, Machine Learn-ing, 34/1, pp.
177?210.Bigi B., de Mori R., El-B?ze M. and Spriet T. (1998)Detecting topic shifts using a cache memory, 5th In-ternational Conference on Spoken Language Proc-essing, pp.
2331?2334.Church K. W. and Hanks P. (1990) Word AssociationNorms, Mutual Information, And Lexicography.Computational Linguistics, 16/1, pp.
22?29.Choi F., Wiemer-Hastings P. and Moore J.
(2001) La-tent Semantic Analysis for Text Segmentation,NAACL?01, pp.
109?117.Choi F. (2000) Advances in domain independent lineartext segmentation, NAACL?00, pp.
26?33.Ferret O. and Grau B.
(2000) A Topic Segmentation ofTexts based on Semantic Domains, ECAI 2000,pp.
426?430.Ferret O.
(1998) How to thematically segment texts byusing lexical cohesion?, ACL-COLING?98,pp.
1481?1483.Fiscus J., Doddington G., Garofolo J. and Martin A.
(1999) NIST?s 1998 Topic Detection and TrackingEvaluation, DARPA Broadcast News Workshop.Hearst M. (1997) TextTiling: Segmenting Text intoMulti-paragraph Subtopic Passages, ComputationalLinguistics, 23/1, pp.
33?64.Jobbins A. and Evett L. (1998) Text Segmentation Us-ing Reiteration and Collocation, ACL-COLING?98,pp.
614?618.Kan M-Y., Klavans J. and McKeown K. (1998) Linearsegmentation and segment significance, 6th Workshopon Very Large Corpora, pp.
197?205.Kaufmann S. (1999) Cohesion and Collocation: UsingContext Vectors in Text Segmentation, ACL?99,pp.
591?595.Kozima H. (1993) Text Segmentation Based on Simi-larity between Words, ACL?93, pp.
286?288.Passonneau R. and Litman D. (1997) Discourse Seg-mentation by Human and Automated Means, Com-putational Linguistics, 23/1, pp.
103?139.Reynar R. (1998) Topic segmentation: Algorithms andapplications, Ph.D. thesis, Computer and InformationScience, University of Pennsylvania.Utiyama M. and Isahara H. (2001) A Statistical Modelfor Domain-Independent Text Segmentation,ACL?2001, pp.
491?498.
