Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111?120,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPReverse Engineering of Tree Kernel Feature SpacesDaniele PighinFBK-Irst, HLTVia di Sommarive, 18 I-38100 Povo (TN) Italypighin@fbk.euAlessandro MoschittiUniversity of Trento, DISIVia di Sommarive, 14 I-38100 Povo (TN) Italymoschitti@disi.unitn.itAbstractWe present a framework to extract themost important features (tree fragments)from a Tree Kernel (TK) space accordingto their importance in the target kernel-based machine, e.g.
Support Vector Ma-chines (SVMs).
In particular, our min-ing algorithm selects the most relevant fea-tures based on SVM estimated weightsand uses this information to automaticallyinfer an explicit representation of the in-put data.
The explicit features (a) improveour knowledge on the target problem do-main and (b) make large-scale learningpractical, improving training and test time,while yielding accuracy in line with tradi-tional TK classifiers.
Experiments on se-mantic role labeling and question classifi-cation illustrate the above claims.1 IntroductionThe last decade has seen a massive use of SupportVector Machines (SVMs) for carrying out NLPtasks.
Indeed, their appealing properties such as1) solid theoretical foundations, 2) robustness toirrelevant features and 3) outperforming accuracyhave been exploited to design state-of-the-art lan-guage applications.More recently, kernel functions, which im-plicitly represent data in some high dimensionalspace, have been employed to study and fur-ther improve many natural language systems, e.g.
(Collins and Duffy, 2002), (Kudo and Matsumoto,2003), (Cumby and Roth, 2003), (Cancedda et al,2003), (Culotta and Sorensen, 2004), (Toutanovaet al, 2004), (Kazama and Torisawa, 2005), (Shenet al, 2003), (Gliozzo et al, 2005), (Kudo et al,2005), (Moschitti et al, 2008), (Diab et al, 2008).Unfortunately, the benefit to easily and effectivelymodel the target linguistic phenomena is reducedby the the implicit nature of the kernel space,which prevents to directly observe the most rele-vant features.
As a consequence, even very accu-rate models generally fail in providing useful feed-back for improving our understanding of the prob-lems at study.
Moreover, the computational bur-den induced by high dimensional kernels makesthe application of SVMs to large corpora still moreproblematic.In (Pighin and Moschitti, 2009), we proposed afeature extraction algorithm for Tree Kernel (TK)spaces, which selects the most relevant features(tree fragments) according to the gradient compo-nents (weight vector) of the hyperplane learnt byan SVM, in line with current research, e.g.
(Rako-tomamonjy, 2003; Weston et al, 2003; Kudo andMatsumoto, 2003).
In particular, we provided al-gorithmic solutions to deal with the huge dimen-sionality and, consequently, high computationalcomplexity of the fragment space.
Our experimen-tal results showed that our approach reduces learn-ing and classification processing time leaving theaccuracy unchanged.In this paper, we present a new version of suchalgorithm which, under the same parameteriza-tion, is almost three times as fast while produc-ing the same results.
Most importantly, we ex-plored tree fragment spaces for two interestingnatural language tasks: Semantic Role Labeling(SRL) and Question Classification (QC).
The re-sults show that: (a) on large data sets, our ap-proach can improve training and test time whileyielding almost unaffected classification accuracy,and (b) our framework can effectively exploit theability of TKs and SVMs to, respectively, gener-ate and recognize relevant structured features.
Inparticular, we (i) study in more detail the relevantfragments identfied for the boundary classificationtask of SRL, (ii) closely observe the most relevantfragments for each QC class and (iii) look at the di-verse syntactic patterns characterizing each ques-111tion category.The rest of the paper is structured as follows:Section 2 will briefly review SVMs and TK func-tions; Section 3 will detail our proposal for the lin-earization of a TK feature space; Section 4 willreview previous work on related subjects; Section5 will detail the outcome of our experiments, andSection 6 will discuss some relevant aspects of theevaluation; finally, in Section 7 we will draw ourconclusions.2 Tree Kernel FunctionsThe decision function of an SVM is:f(~x) = ~w ?
~x + b =n?i=1?iyi~xi?
~x + b (1)where ~x is a classifying example and ~w and b arethe separating hyperplane?s gradient and its bias,respectively.
The gradient is a linear combinationof the training points ~xi, their labels yiand theirweights ?i.
Applying the so-called kernel trick itis possible to replace the scalar product with a ker-nel function defined over pairs of objects:f(o) =n?i=1?iyik(oi, o) + bwith the advantage that we do not need to providean explicit mapping ?(?)
of our examples in a vec-tor space.A Tree Kernel function is a convolution ker-nel (Haussler, 1999) defined over pairs of trees.Practically speaking, the kernel between two treesevaluates the number of substructures (or frag-ments) they have in common, i.e.
it is a measureof their overlap.
The function can be computed re-cursively in closed form, and quite efficient imple-mentations are available (Moschitti, 2006).
Dif-ferent TK functions are characterized by alterna-tive fragment definitions, e.g.
(Collins and Duffy,2002) and (Kashima and Koyanagi, 2002).
In thecontext of this paper we will be focusing on theSubSet Tree (SST) kernel described in (Collinsand Duffy, 2002), which relies on a fragment defi-nition that does not allow to break production rules(i.e.
if any child of a node is included in a frag-ment, then also all the other children have to).
Assuch, it is especially indicated for tasks involvingconstituency parsed texts.Implicitly, a TK function establishes a corre-spondence between distinct fragments and dimen-sions in some fragment space, i.e.
the space of allFragment spaceAB AAB AB AAB ACAB AB ACACDB ADB AC1 2 3 4 5 6 7T1AB AB ACT2DB AC?
(T1) = [2, 1, 1, 1, 1, 0, 0]?
(T2) = [0, 0, 0, 0, 1, 1, 1]K(T1, T2) = ??
(T1), ?(T2)?
= 1Figure 1: Esemplification of a fragment space andthe kernel product between two trees.the possible fragments.
To simplify, a tree t canbe represented as a vector whose attributes countthe occurrences of each fragment within the tree.The kernel between two trees is then equivalent tothe scalar product between pairs of such vectors,as exemplified in Figure 1.3 Linearization of a TK functionOur objective is to efficiently mine the most rele-vant fragments from the huge fragment space, sothat we can explicitly represent our input trees interms of these fragments and learn fast and accu-rate linear classifiers.The framework defines five distinct activities,detailed in the following paragraphs.3.1 Kernel Space Learning (KSL)The first step involves the generation of an approx-imation of the whole fragment space, i.e.
we canconsider only the trees that encode the most rele-vant fragments.
To this end, we can partition ourtraining data into S smaller sets, and use the SVMand the SST kernel to learn S models.
We willonly consider the fragments encoded by the sup-port vectors of the S models.
In the next stage, wewill use the SVM estimated weights to drive ourfeature selection process.Since time complexity of SVM training is ap-proximately quadratic in the number of examples,by breaking training data into smaller sets wecan considerably accelerate the process of filteringtrees and estimating support vector weights.
Ac-cording to statistical learning theory, being trainedon smaller subsets of the available data these mod-els will be less robust with respect to the min-imization of the empirical risk (Vapnik, 1998).112Algorithm 3.1: MINE MODEL(M,L, ?
)global maxexpprev?
?
; CLEAR INDEX()for each ?
?y, t?
?Mdo???????Ti?
?
?
y/?t?for each n ?
Ntdo{f ?
FRAG(n) ; rel = ?
?
Tiprev?
prev ?
{f, rel}PUT(f, rel)best pr?
BEST(L) ;while truedo???????????????????????????????next?
?for each ?f, rel?
?
prev if f ?
best prdo?????????
?X = EXPAND(f,maxexp)rel exp?
?
?
relfor each frag ?
Xdo{temp = {frag, rel exp}next?
next ?
tempPUT(frag, rel exp)best?
BEST(L)if not CHANGED()then breakbest pr ?
bestprev?
nextFL?
best prreturn (FL)Nonetheless, since we do not need to employ themfor classification (but just to direct our feature se-lection process, as we will describe shortly), wecan accept to rely on sub-optimal weights.
Fur-thermore, research results in the field of SVM par-allelization using cascades of SVMs (Graf et al,2004) suggest that support vectors collected fromlocally learnt models can encode many of the rel-evant features retained by models learnt globally.Henceforth, let Msbe the model associated withthe s-th split, and Fsthe fragment space that candescribe all the trees in Ms.3.2 Fragment Mining and Indexing (FMI)In Equation 1 it is possible to isolate the gradient~w =?ni=1?iyi~xi, with ~xi= [x(1)i, .
.
.
, x(N)i], Nbeing the dimensionality of the feature space.
Fora tree kernel function, we can rewrite x(j)ias:x(j)i=ti,j??(fj)?ti?=ti,j??(fj)??Nk=1(ti,k??
(fk))2(2)where: ti,jis the number of occurrences of thefragment fj, associated with the j-th dimension ofthe feature space, in the tree ti; ?
is the kernel de-cay factor; and ?
(fj) is the depth of the fragment.The relevance |w(j)| of the fragment fjcan bemeasured as:|w(j)| =?????n?i=1?iyix(j)i?????=????ni=1?iyiti,j??(fj)????ti?.
(3)We fix a threshold L and from each model Ms(learnt during KSL) we select the L most relevantfragments, i.e.
we build the set Fs,L= ?k{fk} sothat:|Fs,L| = L and |w(k)| ?
|w(i)|?fi?
F \ Fs,L.To generate all the fragments encoded in amodel, we adopt the greedy strategy described inAlgorithm 3.1.
Its arguments are: an SVM modelM represented as ?
?y, t?
pairs, where t is a treestructure; the threshold value L; and the kernel de-cay factor ?.The function FRAG(n) generates the smallestfragment rooted in node n (i.e.
for an SST kernel,the fragment consisting of n and its direct chil-dren).
We call such fragment a base fragment.
Thefunction EXPAND(f,maxexp) generates all thefragments that can be derived from the fragmentf by expanding, i.e.
including in the fragment thedirect children of some of its nodes.
These frag-ments are derived from f .
The parameter maxexplimits fragment proliferation by setting the maxi-mum number of nodes which can be expanded ina fragment expansion operation.
For example, ifthere are 10 nodes which can be expanded in frag-ment f , then only the fragments where at most 3of the 10 nodes are expanded will be generated bya call to EXPAND(f, 3).Every time we generate a fragment f , the func-tion PUT(f, rel) saves the fragment along with itsrelevance rel in an index.
The index keeps trackof the cumulative relevance of a fragment, and itsimplementation has been optimized for fast inser-tions and spatial compactness.A whole cycle of expansions is considered asan iteration of the mining process: we take intoaccount all the fragments that have undergone kexpansions and produce all the fragments that re-sult from a further expansion, i.e.
all the fragmentsexpanded k + 1 times.We keep iterating until we reach a stop crite-rion, which we base on the threshold value L, i.e.the limit on the number of fragments that we areinterested in mining from a model.
During each it-eration k+1, we only expand the best L fragmentsidentified during the previous iteration k. When113the iteration is complete we re-evaluate the set ofL best fragments in the index, and we stop only ifthe worst of them, i.e.
the L-th ranked fragmentat the step k + 1, and its score are the same as atthe end of the previous iteration.
That is, we as-sume that if none of the fragments mined duringthe (k + 1)-th iteration managed to affect the bot-tom of the pool of the L most relevant fragments,then none of their expansions is likely to succeed.In the algorithm, Ntis the set of nodes of the treet; BEST(L) returns the L highest ranked fragmentsin the index; CHANGED() verifies whether the bot-tom of the L-best set has been affected by the lastiteration or not.We call MINE MODEL(?)
on each of the mod-els Msthat we learnt from the S initial splits.
Foreach model, the function returns the set of L-bestfragments in the model.
The union of all the frag-ments harvested from each model is then savedinto a dictionary DLwhich will be used by the nextstage.3.2.1 Discussion on FMI algorithmWith respect to the algorithm presented in (Pighinand Moschitti, 2009), the one presented here hasthe following advantages:?
the process of building fragments is strictlysmall-to-large: fragments that span n+1 lev-els of the tree may be generated only after allthose spanning n levels;?
the threshold value L is a parameter of themining process, and it is used to prevent thealgorithm from generating more fragmentsthan necessary, thus making it more efficient;?
it has one less parameter (maxdepth) whichwas used to force fragments to span at-mosta given number of levels.
The new algorithmdoes not need it since the maximum numberof iterations is implicitly set via L.These differences result in improved efficiency forthe FMI stage.
For example, on the data for theboundary classification task (see Section 5), usingcomparable parameters the old algorithm required85 minutes to mine the most relevant fragments,whereas the new one only takes 31, i.e.
it is 2.74times as fast.3.3 Tree Fragment Extraction (TFX)During this phase we actually linearize our data:a file encoding label-tree pairs ?yi, ti?
is trans-formed to encode label-vector pairs ?yi, ~vi?.
Todo so, we generate the fragment space of ti, us-ing a variant of the mining algorithm described inAlgorithm 3.1, and encode in ~viall and only thefragments ti,jso that ti,j?
DL.
The algorithmexploits labels and production rules found in thefragments listed in the dictionary to generate onlythe fragments that may be in the dictionary.
Forexample, if the dictionary does not contain a frag-ment whose root is labeled N , then if a node N isencountered during TFX neither its base fragmentnor its expansions are generated.
The process isapplied to the whole training (TFX-train) and test(TFX-test) sets.
The fragment space is now ex-plicit, as there is a mapping between the input vec-tors and the fragments they encode.3.4 Explicit Space Learning (ESL)Linearized training data is used to learn a very fastmodel by using all the available data and a linearkernel.3.5 Explicit Space Classification (ESC)The linear model is used to classify linearized testdata and evaluate the accuracy of the resultingclassifier.4 Previous workA rather comprehensive overview of feature se-lection techniques is carried out in (Guyon andElisseeff, 2003).
Non-filter approaches for SVMsand kernel machines are often concerned withpolynomial and Gaussian kernels, e.g.
(Weston etal., 2001) and (Neumann et al, 2005).
Westonet al (2003) use the ?0norm in the SVM opti-mizer to stress the feature selection capabilitiesof the learning algorithm.
In (Kudo and Mat-sumoto, 2003), an extension of the PrefixSpan al-gorithm (Pei et al, 2001) is used to efficientlymine the features in a low degree polynomial ker-nel space.
The authors discuss an approximationof their method that allows them to handle highdegree polynomial kernels.Suzuki and Isozaki (2005) present an embed-ded approach to feature selection for convolutionkernels based on ?2-driven relevance assessment.To our knowledge, this is the only published workclearly focusing on feature selection for tree ker-nel functions, and indeed has been one of themajor sources of inspiration for our methodol-ogy.
With respect to their work, the difference114in our approach is that we want to exploit theSVM optimizer to select the most relevant fea-tures instead of a relevance assessment measurethat moves from different statistical assumptionsthan the learning algorithm.In (Graf et al, 2004), an approach to SVMparallelization is presented which is based on adivide-et-impera strategy to reduce optimizationtime.
The idea of using a compact graph rep-resentation to represent the support vectors of aTK function is explored in (Aiolli et al, 2006),where a Direct Acyclic Graph (DAG) is employed.In (Moschitti, 2006; Bloehdorn and Moschitti,2007a; Bloehdorn and Moschitti, 2007b; Mos-chitti et al, 2007), the SST kernel along with othertree and combined kernels are employed for ques-tion classification and semantic role labeling withinteresting results.5 ExperimentsWe evaluated the capability of our model to ex-tract relevant features on two data sets: theCoNLL 2005 shared task on Semantic Role Label-ing (SRL) (Carreras and Ma`rquez, 2005), and theQuestion Classification (QC) task based on datafrom the TREC 10 QA competition (Voorhees,2001).
The next sections will detail the setup andoutcome of the two sets of experiments.All the experiments were run on a machineequipped with 4 Intel R?
Xeon R?
CPUs clocked at1.6 GHz and 4 GB of RAM.
As a supervised learn-ing framework we used SVM-Light-TK1, whichextends the SVM-Light optimizer (Joachims,2000) with tree kernel support.
For each classi-fication task, we compare the accuracy of a vanillaSST classifier against the corresponding linearizedSST classifier (SST?).
For KSL and SST trainingwe used the default decay factor ?
= 0.4.
ForESL, we use a non-normalized, linear kernel.
Nofurther parametrization of the learning algorithmsis carried out.
Indeed, our focus is on showingthat, under the same conditions, our linearized treekernel can be as accurate as the original kernel,and choosing of parameters may just bias suchtest.5.1 Semantic Role LabelingFor our experiments on semantic role labeling weused PropBank annotations (Palmer et al, 2005)1http://disi.unitn.it/?moschitt/Tree-Kernel.htmSNPNNPMaryVPVBboughtNPDaNNcat(A1)(A0)?VPVB-PboughtNPD-BaVPVB-PboughtNP-BDaNNcat-1: BC +1: BC,A1-1: A0,A2,A3,A4,A5Figure 2: Examples of ASTmstructured features.and automatic Charniak parse trees (Charniak,2000) as provided for the CoNLL 2005 evaluationcampaign (Carreras and Ma`rquez, 2005).
SRL canbe decomposed into two tasks: boundary detec-tion, where the word sequences that are argumentsof a predicate word w are identified, and role clas-sification, where each argument is assigned theproper role.
The former task requires a binaryBoundary Classifier (BC), whereas the second in-volves a Role Multi-class Classifier (RM).5.1.1 SetupIf the constituency parse tree t of a sentence sis available, we can look at all the pairs ?p, ni?,where niis any node in the tree and p is the nodedominating w, and decide whether niis an argu-ment node or not, i.e.
whether it exactly dominatesall and only the words encoding any of w?s argu-ments.
The objects that we classify are subsetsof the input parse tree that encompass both p andni.
Namely, we use the ASTmstructure definedin (Moschitti et al, 2008), which is the minimaltree that covers all and only the words of p and ni.In the ASTm, p and niare marked so that they canbe distinguished from the other nodes.
An ASTmis regarded as a positive example for BC if niis anargument node, otherwise it is considered a nega-tive example.
Positive BC examples can be used totrain an efficient RM: for each role r we can traina classifier whose positive examples are argumentnodes whose label is exactly r, whereas negativeexamples are argument nodes labeled r?
6= r. TwoASTms extracted from an example parse tree areshown in Figure 2: the first structure is a negativeexample for BC and is not part of the data set ofRM, whereas the second is a positive instance forBC and A1.To train BC we used PropBank sections 1through 6, extracting ASTmstructures out of thefirst 1 million ?p, ni?
pairs from the correspondingparse trees.
As a test set we used the 149,140 in-stance collected from the annotations in Section24.
There are 61,062 positive examples in thetraining set (i.e.
6.1%) and 8,515 in the test set115(i.e.
5.7%).For RM we considered all the argument nodesof any of the six PropBank core roles (i.e.
A0,.
.
.
, A5) from all the available training sections,i.e.
2 through 21, for a total of 179,091 train-ing instances.
Similarly, we collected 5,928 testinstances from the annotations of Section 24.Columns Tr+ and Te+ of Table 1 show the num-ber of positive training and test examples, respec-tively, for BC and the role classifiers.For all the linearized classifiers, we used 50splits for the FMI stage and we set the thresholdvalue L = 50k and maxexp = 1 during FMI andTFX.
We did not validate these parameters, whichwe know to be sub-optimal.
These values wereselected during the development of the softwarebecause, on a very small test bed, they resulted ina responsive and accurate system.We should point out that other experiments haveshown that linearization is very robust with re-spect to parametrization: due to the huge num-ber and variety of fragments in the TK space, dif-ferent choices of the parameters result in differ-ent explicit spaces and more or less efficient solu-tions, but in most cases the final accuracy of thelinearized classifiers is affected only marginally.For example, it could be expected that reducingthe number of splits during KSL would improvethe final accuracy of a linearized classifier, as theweights used for FMI would then converge to theglobal optimum.
Instead, we have observed thatincreasing the number of splits does not necessar-ily decrease the accuracy of the linearized classi-fier.The evaluation on the whole SRL task usingthe official CoNLL?05 evaluator was not carriedout because producing complete annotations re-quires several steps (e.g.
overlap resolution, OvAor Pairwise combination of individual role classi-fiers) that would shade off the actual impact of themethodology on classification.5.1.2 ResultsThe left side of Table 1 shows the distribution ofpositive data points in the training and test sets ofeach classifier.
Columns SST and SST?compareside by side the F1measure of the non-linearizedand linearized classifier for each class.
The accu-racy of the RM classifier is the percentage of cor-rect class assignments.We can see that the accuracy of linearized clas-sifiers is always in line with vanilla SST, evenData set AccuracyClass Tr+ Te+ SST SST?BC 61,062 8,515 81.8 81.3A0 60,900 2,014 91.6 91.1A1 90,636 3,041 89.0 89.4A2 21,291 697 73.1 73.0A3 3,481 105 56.8 53.0A4 2,713 69 69.1 67.9A5 69 2 66.7 0.0RM 87.8 87.8Table 1: Number of positive training (Tr+) and test(Te+) examples in the SRL dataset.
Accuracy ofthe non-linearized (SST) and linearized (SST?)
bi-nary classifiers (i.e.
BC, A0, .
.
.
A5) is F1measure.Accuracy of RM is the percentage of correct classassignments.if the selected linearization parameters generatea very rough approximation of the original frag-ment space, generally consisting of billions offragments.
BC?(i.e.
the linearized BC) has anF1of 81.3, just 0.5% less than BC, i.e.
81.8.
Con-cerning RM?, its accuracy is the same as the nonlinearized classifier, i.e.
87.8.We should consider that the linearization frame-work can drastically improve the efficiency oflearning and classification when dealing with largeamounts of data.
For a linearized classifier, weconsider training time to be the overall time re-quired to carry out the following activities: KSL,FMI, TFX on training data and ESL.
Similarly,we consider test time the time necessary to per-form TFX on test data and ESC.
Training BC tookmore than two days of CPU time and testing about4 hours, while training and testing the linearizedboundary classifier required only 381 and 25 min-utes, respectively.
That is, on the same amountof data we can train a linearized classifier about8 times as fast, and test it in about 1 tenth of thetime.
Concerning RM, sequential training of the6 models took 2,596 minutes, while testing took27 minutes.
The linearized role multi classifier re-quired 448 and 24 minutes for training and test-ing, respectively, i.e.
training is about 5 times asfast while testing time is about the same.
If com-pared with the boundary classifier, the improve-ment in efficiency is less evident: indeed, the rel-atively small size of the role classifiers data setslimits the positive effect of splitting training datainto smaller chunks.SRL fragment space.
Table 3 lists the best frag-ments identified for the Boundary Classifier.
Weshould remember that we are using ASTmstruc-116tures as input to our classifiers: nodes whose la-bel end with ?-P?
are predicate nodes, while nodeswhose label ends with ?-B?
are candidate argu-ment nodes.All the most relevant fragments encode the min-imum sub-tree encompassing the predicate and theargument node.
This kind of structured featuresubsumes several features traditionally employedfor explicit SRL models: the Path (i.e.
the se-quence of nodes connecting the predicate and thecandidate argument node), Phrase Type (i.e.
thelabel of the candidate argument node), PredicatePOS (i.e.
the POS of the predicate word), Posi-tion (i.e.
whether the argument is to the left or tothe right of the predicate) and Governing Category(i.e.
the label of the common ancestor) definedin (Gildea and Jurafsky, 2002).The linearized model for BC contains about 160thousand fragments.
Of these, about 70 and 33thousand encompass the candidate argument or thepredicate node, respectively.
About 16 thousandfragments contain both.5.2 Question ClassificationFor question classification we used the data setfrom the TREC 10 QA evaluation campaign2, con-sisting of 5,500 training and 500 test questions.5.2.1 SetupGiven a question, the QC task consists in selectingthe most appropriate expected answer type from agiven set of possibilities.
We adopted the questiontaxonomy known as coarse grained, which hasbeen described in (Zhang and Lee, 2003) and (Liand Roth, 2006), consisting of six non overlap-ping classes: Abbreviations (ABBR), Descrip-tions (DESC, e.g.
definitions or explanations), En-tity (ENTY, e.g.
animal, body or color), Human(HUM, e.g.
group or individual), Location (LOC,e.g.
cities or countries) and Numeric (NUM, e.g.amounts or dates).For each question, we generate the full parseof the sentence and use it to train SST and (lin-earized) SST?models.
The automatic parses areobtained with the Stanford parser3 (Klein andManning, 2003).
We actually have only 5,483 sen-tences in our training set, due to parsing issueswith a few of them.2http://l2r.cs.uiuc.edu/cogcomp/Data/QA/QC/3http://nlp.stanford.edu/software/lex-parser.shtmlData set AccuracyClass Tr+ Te+ SST SST?ABBR 89 9 80.0 87.5DESC 1,164 138 96.0 94.5ENTY 1,269 94 63.9 63.5HUM 1,231 65 88.1 87.2LOC 834 81 77.6 77.9NUM 896 113 80.4 80.8Overall 86.2 86.6Table 2: Number of positive training (Tr+) and test(Te+) examples in the QA dataset.
Accuracy ofthe non-linearized (SST) and linearized (SST?)
bi-nary classifiers is F1measure.
Overall accuracy isthe percentage of correct class assignments.The classifiers are arranged in a one-vs.-all(OvA) configuration, where each sentence is apositive example for one of the six classes, andnegative for the other five.
Given the very smallsize of the data set, we used S = 1 during KSLfor the linearized classifier (i.e.
we didn?t parti-tion training data).
We carried out no validation ofthe parameters, and we used maxexp = 4 andL = 50k in order to generate a rich fragmentspace.5.2.2 ResultsTable 2 shows the number of positive examplesin the training and test set of each individual bi-nary classifiers.
Columns SST and SST?comparethe F1measure of the vanilla and linearized classi-fiers on the individual classes, and the accuracy ofthe complete QC task (Row Overall) in terms ofpercentage of correct class assignments.
Also inthis case, we can notice that the accuracy of thelinearized classifiers is always in line with non-linearized ones, e.g.
86.6 vs. 86.2 for the multi-classifiers.
These results are lower than those de-rived in (Moschitti, 2006; Moschitti et al, 2007),i.e.
88.2 and 90.4, respectively, where the param-eters for each classifier were carefully optimized.QC Fragment space.
Tables from 4 to 9 list thetop fragments identified for each class 4.As expected, for all the categories the domainlexical information is very relevant.
For example,film, color, book, novel and sport for ENTY orcity, country, state and capital for LOC.
Of the sixclasses, ENTY (Table 6) is mostly characterizedby lexical features.
Interestingly, function words,which would have been eliminated by a pure In-formation Retrieval approach (i.e.
by means of4Some categories show meaningful syntactic fragmentsafter the first 10, so for them we report more subtrees.117standard stop-list), are in the top positions, e.g.
:why and how for DESC, what for ENTY, who forHUM, where for LOC and when for NUM.
For thelatter, also how seems to be important suggestingthat features may strongly characterize more thanone given class.Characteristic syntactic features appear in thetop positions for each class, for example: (VP (VB(stand)) (PP)), which suggests that stand shouldbe followed by a prepositional phrase to character-ize ABBR; or (NP (NP (DT) (NN (abbreviation)))(PP)), which suggests that, to be in a relevant pat-tern, abbreviation should be preceded by an articleand followed by a PP.
Also, the syntactic struc-ture is useful to differentiate the use of the sameimportant words, e.g.
(SBARQ (WHADVP (WRB(How))) (SQ) (.))
for DESC better characterizesthe use of how with respect to NUM, in which arelevant use is (WHADJP (WRB (How)) (JJ)).In (Moschitti et al, 2007) it was shown that theuse of TK improves QC of 1.2 percent points, i.e.from 90.6 to 91.8: further analysis of these frag-ments may help us to device compact, less sparsesyntactic features and design more accurate mod-els for the task.6 DiscussionThe fact that our model doesn?t always improvethe accuracy of a standard SST model might berelated to the process of splitting training data andemploying locally estimated weights during FMI.Concerning the experiments presented in thispaper, this objection might apply to the results onSRL, where we used 50 splits to identify the mostrelevant fragments, but not to those on QC, wheregiven the limited size of the data set we decidednot to split training data at all as explained in Sec-tion 5.2.
Furthermore, as we already discussed,we have evidence that there is no direct correlationbetween the number of splits used for KSL andthe accuracy of the resulting classifier.
After all,the optimization carried out during ESL is global,and we can assume that, if we mined enough frag-ments during FMI, than those actually retained bythe global linear model would be by and large thesame, regardless of the split configuration.More in general, feature selection may give animprovement to some learning algorithm but if itcan help SVMs is debatable, since its related the-ory show that they are robust to irrelevant fea-tures.
In our specific case, we remove features(ADJP(RB-B)(VBN-P))(NP(VBN-P)(NNS-B))(S(NP-B)(VP))(VP(VBD-P(said))(SBAR))(VP(VB-P)(NP-B))(NP(VBG-P)(NNS-B))(VP(VBD-P)(NP-B))(VP(VBG-P)(NP-B))(VP(VBZ-P)(NP-B))(VP(VBN-P)(NP-B))(VP(VBP-P)(NP-B))(NP(NP-B)(VP))(NP(VBG-P)(NN-B))(S(S(VP(VBG-P)))(NP-B))Table 3: Best fragments for SRL BC.(NN(abbreviation))(NP(DT)(NN(abbreviation)))(NP(DT(the))(NN(abbreviation)))(IN(for))(VB(stand))(VBZ(does))(PP(IN))(VP(VB(stand))(PP))(NP(NP(DT)(NN(abbreviation)))(PP))(SQ(VBZ)(NP)(VP(VB(stand))(PP)))(SBARQ(WHNP)(SQ(VBZ)(NP)(VP(VB(stand))(PP)))(.
))(SQ(VBZ(does))(NP)(VP(VB(stand))(PP)))(VP(VBZ)(NP(NP(DT)(NN(abbreviation)))(PP)))Table 4: Best fragments for the ABBR class.(WRB(Why))(WHADVP(WRB(Why)))(WHADVP(WRB(How)))(WHADVP(WRB))(VB(mean))(VBZ(causes))(VB(do))(ROOT(SBARQ(WHADVP(WRB(How)))(SQ)(.)))(ROOT(SBARQ(WHADVP(WRB(How)))(SQ)(.(?))))(SBARQ(WHADVP(WRB(How)))(SQ))(WRB(How))(SBARQ(WHADVP(WRB(How)))(SQ)(.))(SBARQ(WHADVP(WRB(How)))(SQ)(.(?
)))(SBARQ(WHADVP(WRB(Why)))(SQ))(ROOT(SBARQ(WHADVP(WRB(Why)))(SQ)))(SBARQ(WHADVP(WRB))(SQ))Table 5: Best fragments for the DESC class.
(NN(film))(NN(color))(NN(book))(NN(novel))(NN(sport))(WP(What))(NN(fear))(NN(movie))(NN(word))(VP(VBN(called)))(NN(game))(NP(DT)(NN(fear)))(NP(NP(DT)(NN(fear)))(PP))Table 6: Best fragments for the ENTY class.118(NN(company))(WP(Who))(WHNP(WP(Who)))(NN(name))(NN(team))(NN(baseball))(WHNP(WP))(NN(character))(NNP(President))(NN(leader))(NN(actor))(NN(president))(JJ(Whose))(VP(VBD)(NP))(NP(NP)(JJ)(NN(name)))(VP(VBD)(VP))(NN(organization))(VP(VBD)(NP)(PP(IN)(NP)))(SBARQ(WHNP(WP(Who)))(SQ)(.))(ROOT(SBARQ(WHNP(WP(Who)))(SQ)(.)))(ROOT(SBARQ(WHNP(WP(Who)))(SQ)(.(?))))(SBARQ(WHNP(WP(Who)))(SQ)(.(?
)))Table 7: Best fragments for the HUM class.(NN(city))(NN(country))(WRB(Where))(NN(state))(WHADVP(WRB(Where)))(NN(capital))(NP(NN(city)))(NNS(countries))(NP(NN(state)))(PP(IN(in)))(SBARQ(WHADVP(WRB(Where)))(SQ)(.(?)))(SBARQ(WHADVP(WRB(Where)))(SQ)(.))(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)(.)))(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)(.(?
))))(NN(island))(NN(address))(NN(river))(NN(mountain))(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)))(SBARQ(WHADVP(WRB(Where)))(SQ))Table 8: Best fragments for the LOC class.(WRB(How))(WHADVP(WRB(When)))(WRB(When))(JJ(many))(NN(year))(WHADJP(WRB)(JJ))(NP(NN(year)))(WHADJP(WRB(How))(JJ))(NN(date))(SBARQ(WHADVP(WRB(When)))(SQ)(.(?)))(SBARQ(WHADVP(WRB(When)))(SQ)(.))(NN(day))(NN(population))(ROOT(SBARQ(WHADVP(WRB(When)))(SQ)(.)))(ROOT(SBARQ(WHADVP(WRB(When)))(SQ)(.(?
))))(JJ(average))(NN(number))Table 9: Best fragments for the NUM class.whose SVM weights are the lowest, i.e.
thosethat are (almost) irrelevant for the SVM.
There-fore, the chance of this resulting in an improve-ment is rather low.With respect to cases where our model is lessaccurate than a standard SST, we should considerthat our choice of parameters is sub-optimal andwe adopt a very aggressive feature selection strat-egy, that only retains a few thousand features froma space where there are hundreds of millions ofdifferent features.7 ConclusionsWe introduced a novel framework for support vec-tor classification that combines advantages of con-volution kernels, i.e.
the generation of a very highdimensional structure space, with the efficiencyand clarity of explicit representations in a linearspace.For this paper, we focused on the SubSet Treekernel and verified the potential of the proposedsolution on two NLP tasks, i.e.
semantic rolelabeling and question classification.
The exper-iments show that our framework drastically re-duces processing time, e.g.
boundary classifica-tion for SRL, while preserving the accuracy.We presented a selection of the most relevantfragments identified for the SRL boundary classi-fier as well as for each class of the coarse grainedQC task.
Our analysis shows that our frame-work can discover state-of-the-art features, e.g.the Path feature for SRL.
We believe that shar-ing these fragments with the NLP community andstudying them in more depth will be useful toidentify new, relevant features for the character-ization of several learning problems.
For thispurpose, we made available the fragment spacesat http://danielepighin.net and we will keepthem updated with new set of experiments on newtasks, e.g.
SRL based on FrameNet and VerbNet,e.g.
(Giuglea and Moschitti, 2004).In our future work, we plan to widen the listof covered tasks and to extend our algorithm tocope with different kernel families, such as thepartial tree kernel and kernels defined over pairsof trees, e.g.
the ones used for textual entailmentin (Moschitti and Zanzotto, 2007).
We also plan tomove from mining fragments to mining classes offragments, i.e.
to identify prototypical fragmentsin the fragment space that generalize topologicalsub-classes of the most relevant fragments.119ReferencesFabio Aiolli, Giovanni Da San Martino, Alessandro Sper-duti, and Alessandro Moschitti.
2006.
Fast on-line kernellearning for trees.
In Proceedings of ICDM?06.Stephan Bloehdorn and Alessandro Moschitti.
2007a.
Com-bined syntactic and semantic kernels for text classification.In Proceedings of ECIR 2007, Rome, Italy.Stephan Bloehdorn and Alessandro Moschitti.
2007b.
Struc-ture and semantics for expressive text kernels.
In In Pro-ceedings of CIKM ?07.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, 3:1059?1082.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction tothe CoNLL-2005 Shared Task: Semantic Role Labeling.In Proceedings of CoNLL?05.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL?00.Michael Collins and Nigel Duffy.
2002.
New Ranking Al-gorithms for Parsing and Tagging: Kernels over DiscreteStructures, and the Voted Perceptron.
In Proceedings ofACL?02.Aron Culotta and Jeffrey Sorensen.
2004.
DependencyTree Kernels for Relation Extraction.
In Proceedings ofACL?04.Chad Cumby and Dan Roth.
2003.
Kernel Methods for Re-lational Learning.
In Proceedings of ICML 2003.Mona Diab, Alessandro Moschitti, and Daniele Pighin.
2008.Semantic role labeling systems for Arabic using kernelmethods.
In Proceedings of ACL-08: HLT, pages 798?806.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics, 28:245?288.Ana-Maria Giuglea and Alessandro Moschitti.
2004.Knowledge discovery using framenet, verbnet and prop-bank.
In A. Meyers, editor, Workshop on Ontology andKnowledge Discovering at ECML 2004, Pisa, Italy.Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.2005.
Domain kernels for word sense disambiguation.
InProceedings of ACL?05, pages 403?410.Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Durdanovic,and Vladimir Vapnik.
2004.
Parallel support vector ma-chines: The cascade svm.
In Neural Information Process-ing Systems.Isabelle Guyon and Andre?
Elisseeff.
2003.
An introduc-tion to variable and feature selection.
Journal of MachineLearning Research, 3:1157?1182.David Haussler.
1999.
Convolution kernels on discrete struc-tures.
Technical report, Dept.
of Computer Science, Uni-versity of California at Santa Cruz.T.
Joachims.
2000.
Estimating the generalization perfor-mance of a SVM efficiently.
In Proceedings of ICML?00.Hisashi Kashima and Teruo Koyanagi.
2002.
Kernels forsemi-structured data.
In Proceedings of ICML?02.Jun?ichi Kazama and Kentaro Torisawa.
2005.
Speeding uptraining with tree kernels for node relation labeling.
InProceedings of HLT-EMNLP?05.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL?03, pages423?430.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.
Boosting-based parse reranking with subtree features.
In Proceed-ings of ACL?05.Xin Li and Dan Roth.
2006.
Learning question classifiers:the role of semantic information.
Natural Language En-gineering, 12(3):229?249.Alessandro Moschitti and Fabio Massimo Zanzotto.
2007.Fast and effective kernels for relational learning fromtexts.
In ICML?07.Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, andSuresh Manandhar.
2007.
Exploiting syntactic and shal-low semantic kernels for question/answer classification.In Proceedings of ACL?07.Alessandro Moschitti, Daniele Pighin, and Roberto Basili.2008.
Tree kernels for semantic role labeling.
Compu-tational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006.
Efficient convolution kernelsfor dependency and constituent syntactic trees.
In Pro-ceedings of ECML?06, pages 318?329.Julia Neumann, Christoph Schnorr, and Gabriele Steidl.2005.
Combined SVM-Based Feature Selection and Clas-sification.
Machine Learning, 61(1-3):129?150.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of semanticroles.
Comput.
Linguist., 31(1):71?106.J.
Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,and M. C. Hsu.
2001.
PrefixSpan Mining Sequential Pat-terns Efficiently by Prefix Projected Pattern Growth.
InProceedings of ICDE?01.Daniele Pighin and Alessandro Moschitti.
2009.
Efficientlinearization of tree kernel functions.
In Proceedings ofCoNLL?09.Alain Rakotomamonjy.
2003.
Variable selection using SVMbased criteria.
Journal of Machine Learning Research,3:1357?1370.Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
2003.
Us-ing LTAG Based Features in Parse Reranking.
In Proceed-ings of EMNLP?06.Jun Suzuki and Hideki Isozaki.
2005.
Sequence and TreeKernels with Statistical Feature Mining.
In Proceedingsof NIPS?05.Kristina Toutanova, Penka Markova, and Christopher Man-ning.
2004.
The Leaf Path Projection View of ParseTrees: Exploring String Kernels for HPSG Parse Selec-tion.
In Proceedings of EMNLP 2004.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.Wiley-Interscience.Ellen M. Voorhees.
2001.
Overview of the trec 2001 ques-tion answering track.
In In Proceedings of the Tenth TextREtrieval Conference (TREC, pages 42?51.Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-iano Pontil, Tomaso Poggio, and Vladimir Vapnik.
2001.Feature Selection for SVMs.
In Proceedings of NIPS?01.Jason Weston, Andre?
Elisseeff, Bernhard Scho?lkopf, andMike Tipping.
2003.
Use of the zero norm with lin-ear models and kernel methods.
J. Mach.
Learn.
Res.,3:1439?1461.Dell Zhang and Wee Sun Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedings of SI-GIR?03, pages 26?32.120
