Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 152?159,Sydney, July 2006. c?2006 Association for Computational LinguisticsTopic-Focused Multi-document SummarizationUsing an Approximate Oracle ScoreJohn M. Conroy, Judith D. SchlesingerIDA Center for Computing SciencesBowie, Maryland, USAconroy@super.org, judith@super.orgDianne P. O?LearyUniversity of MarylandCollege Park, Maryland, USAoleary@cs.umd.eduAbstractWe consider the problem of producing amulti-document summary given a collec-tion of documents.
Since most success-ful methods of multi-document summa-rization are still largely extractive, in thispaper, we explore just how well an ex-tractive method can perform.
We intro-duce an ?oracle?
score, based on the prob-ability distribution of unigrams in humansummaries.
We then demonstrate that withthe oracle score, we can generate extractswhich score, on average, better than thehuman summaries, when evaluated withROUGE.
In addition, we introduce an ap-proximation to the oracle score which pro-duces a system with the best known per-formance for the 2005 Document Under-standing Conference (DUC) evaluation.1 IntroductionWe consider the problem of producing a multi-document summary given a collection of doc-uments.
Most automatic methods of multi-document summarization are largely extractive.This mimics the behavior of humans for sin-gle document summarization; (Kupiec, Pendersen,and Chen 1995) reported that 79% of the sentencesin a human-generated abstract were a ?directmatch?
to a sentence in a document.
In contrast,for multi-document summarization, (Copeck andSzpakowicz 2004) report that no more than 55% ofthe vocabulary contained in human-generated ab-stracts can be found in the given documents.
Fur-thermore, multiple human summaries on the samecollection of documents often have little agree-ment.
For example, (Hovy and Lin 2002) reportthat unigram overlap is around 40%.
(Teufel andvan Halteren 2004) used a ?factoid?
agreementanalysis of human summaries for a single doc-ument and concluded that a resulting consensussummary is stable only if 30?40 summaries arecollected.In light of the strong evidence that nearly halfof the terms in human-generated multi-documentabstracts are not from the original documents, andthat agreement of vocabulary among human ab-stracts is only about 40%, we pose two coupledquestions about the quality of summaries that canbe attained by document extraction:1.
Given the sets of unigrams used by four hu-man summarizers, can we produce an extractsummary that is statistically indistinguish-able from the human abstracts when mea-sured by current automatic evaluation meth-ods such as ROUGE?2.
If such unigram information can producegood summaries, can we replace this infor-mation by a statistical model and still producegood summaries?We will show that the answer to the first questionis, indeed, yes and, in fact, the unigram set infor-mation gives rise to extract summaries that usuallyscore better than the 4 human abstractors!
Sec-ondly, we give a method to statistically approxi-mate the set of unigrams and find it produces ex-tracts of the DUC 05 data which outperform allknown evaluated machine entries.
We concludewith experiments on the extent that redundancyremoval improves extracts, as well as a methodof moving beyond simple extracting by employ-ing shallow parsing techniques to shorten the sen-tences prior to selection.1522 The DataThe 2005 Document Understanding Conference(DUC 2005) data used in our experiments is par-titioned into 50 topic sets, each containing 25?50documents.
A topic for each set was intendedto mimic a real-world complex questioning-answering task for which the answer could notbe given in a short ?nugget.?
For each topic,four human summarizers were asked to providea 250-word summary of the topic.
Topics werelabeled as either ?general?
or ?specific?.
Wepresent an example of one of each category.Set d408cGranularity: SpecificTitle: Human Toll of Tropical StormsNarrative: What has been the human toll in death or injuryof tropical storms in recent years?
Where and when haveeach of the storms caused human casualties?
What are theapproximate total number of casualties attributed to each ofthe storms?Set d436jGranularity: GeneralTitle: Reasons for Train WrecksNarrative: What causes train wrecks and what can be doneto prevent them?
Train wrecks are those events that resultin actual damage to the trains themselves not just accidentswhere people are killed or injured.For each topic, the goal is to produce a 250-word summary.
The basic unit we extract froma document is a sentence.To prepare the data for processing, wesegment each document into sentences usinga POS (part-of-speech) tagger, NLProcessor(http://www.infogistics.com/posdemo.htm).
Thenewswire documents in the DUC 05 data havemarkers indicating the regions of the document,including titles, bylines, and text portions.
All ofthe extracted sentences in this study are taken fromthe text portions of the documents only.We define a ?term?
to be any ?non-stop word.
?Our stop list contains the 400 most frequently oc-curring English words.3 The Oracle ScoreRecently, a crisp analysis of the frequency ofcontent words used by humans relative to thehigh frequency content words that occur in therelevant documents has yielded a simple andpowerful summarization method called SumBa-sic (Nenkova and Vanderwende, 2005).
SumBa-sic produced extract summaries which performednearly as well as the best machine systems forgeneric 100 word summaries, as evaluated in DUC2003 and 2004, as well as the Multi-lingual Sum-marization Evaluation (MSE 2005).Instead of using term frequencies of the corpusto infer highly likely terms in human summaries,we propose to directly model the set of terms (vo-cabulary) that is likely to occur in a sample of hu-man summaries.
We seek to estimate the proba-bility that a term will be used by a human sum-marizer to first get an estimate of the best possibleextract and later to produce a statistical model foran extractive summary system.
While the primaryfocus of this work is ?task oriented?
summaries,we will also address a comparison with SumBa-sic and other systems on generic multi-documentsummaries for the DUC 2004 dataset in Section 8.Our extractive summarization system is given atopic, ?
, specified by a text description.
It thenevaluates each sentence in each document in theset to determine its appropriateness to be includedin the summary for the topic ?.We seek a statistic which can score an individ-ual sentence to determine if it should be includedas a candidate.
We desire that this statistic takeinto account the great variability that occurs inthe space of human summaries on a given topic?.
One possibility is to simply judge a sentencebased upon the expected fraction of the ?humansummary?-terms that it contains.
We posit an or-acle, which answers the question ?Does humansummary i contain the term t?
?By invoking this oracle over the set of termsand a sample of human summaries, we canreadily compute the expected fraction of humansummary-terms the sentence contains.
To modelthe variation in human summaries, we use the or-acle to build a probabilistic model of the spaceof human abstracts.
Our ?oracle score?
will thencompute the expected number of summary terms asentence contains, where the expectation is takenfrom the space of all human summaries on thetopic ?.We model human variation in summary gener-ation with a unigram bag-of-words model on theterms.
In particular, consider P (t|?)
to be theprobability that a human will select term t in asummary given a topic ?.
The oracle score for asentence x, ?
(x), can then be defined in terms of153P :?
(x) =1|x|?t?Tx(t)P (t|?
)where |x| is the number of distinct terms sentencex contains, T is the universal set of all terms usedin the topic ?
and x(t) = 1 if the sentence x con-tains the term t and 0 otherwise.
(We affectionallyrefer to this score as the ?Average Jo?
score, as it isderived the average uni-gram distribution of termsin human summaries.
)While we will consider several approximationsto P (t|?)
(and, correspondingly, ?
), we first ex-plore the maximum-likelihood estimate of P (t|?
)given by a sample of human summaries.
Supposewe are given h sample summaries generated in-dependently.
Let cit(?)
= 1 if the i-th summarycontains the term t and 0 otherwise.
Then themaximum-likelihood estimate of P (t?)
is givenbyP?
(t|?)
=1hh?i=1cit(?
).We define ??
by replacing P with P?
in the defi-nition of ?.
Thus, ??
is the maximum-likelihoodestimate for ?, given a set of h human summaries.Given the score ?
?, we can compute an extractsummary of a desired length by choosing the topscoring sentences from the collection of docu-ments until the desired length (250 words) is ob-tained.
We limit our selection to sentences whichhave 8 or more distinct terms to avoid selecting in-complete sentences which may have been taggedby the sentence splitter.Before turning to how well our idealized score,?
?, performs on extract summaries, we first definethe scoring mechanism used to evaluate these sum-maries.4 ROUGEThe state-of-the-art automatic summarizationevaluation method is ROUGE (Recall OrientedUnderstudy for Gisting Evaluation, (Hovy and Lin2002)), an n-gram based comparison that was mo-tivated by the machine translation evaluation met-ric, Bleu (Papineni et.
al.
2001).
This system usesa variety of n-gram matching approaches, some ofwhich allow gaps within the matches as well asmore sophistcated analyses.
Surprisingly, simpleunigram and bigram matching works extremelywell.
For example, at DUC 05, ROUGE-2 (bi-gram match) had a Spearman correlation of 0.95and a Pearson correlation of 0.97 when comparedwith human evaluation of the summaries for re-sponsiveness (Dang 2005).
ROUGE-n for match-ing n?grams of a summary X against h modelhuman summaries is given by:Rn(X) =?hj=1?i?Nn min(Xn(i),Mn(i, j))?hj=1?i?Nn Mn(i, j),where Xn(i) is the count of the number oftimes the n-gram i occurred in the summary andMn(i, j) is the number of times the n-gram ioccurred in the j-th model (human) summary.
(Note that for brevity of notation, we assume thatlemmatization (stemming) is done apriori on theterms.
)When computing ROUGE scores, a jackknifeprocedure is done to make comparison of machinesystems and humans more amenable.
In particu-lar, if there are k human summaries available fora topic, then the ROUGE score is computed for ahuman summary by comparing it to the remainingk ?
1 summaries, while the ROUGE score for amachine summary is computed against all k sub-sets of size k ?
1 of the human summaries andtaking the average of these k scores.5 The Oracle or Average Jo SummaryWe now present results on the performance ofthe oracle method as compared with human sum-maries.
We give the ROUGE-2 (R2) scores aswell as the 95% confidence error bars.
In Fig-ure 1, the human summarizers are represented bythe letters A?H, and systems 15, 17, 8, and 4are the top performing machine summaries fromDUC 05.
The letter ?O?
represents the ROUGE-2scores for extract summaries produced by the ora-cle score, ??.
Perhaps surprisingly, the oracle pro-duced extracts which performed better than the hu-man summaries!
Since each human only summa-rized 10 document clusters, the human error barsare larger.
However, even with the large error bars,we observe that the mean ROUGE-2 scores for theoracle extracts exceeds the 95% confidence errorbars for several humans.While the oracle was, of course, given the un-igram term probabilities, its performance is no-table on two counts.
First, the evaluation met-ric scored on 2-grams, while the oracle was onlygiven unigram information.
In a sense, optimizingfor ROUGE-1 is a ?sufficient statistic?
scoring at154the human level for ROUGE-2.
Second, the hu-mans wrote abstracts while the oracle simply didextracting.
Consequently, the documents containsufficient text to produce human-quality extractsummaries as measured by ROUGE.
The humanperformance ROUGE scores indicate that this ap-proach is capable of producing automatic extrac-tive summaries that produce vocabulary compara-ble to that chosen by humans.
Human evaluation(which we have not yet performed) is required todetermine to what extent this high ROUGE-2 per-formance is indicative of high quality summariesfor human use.The encouraging results of the oracle score nat-urally lead to approximations, which, perhaps,will give rise to strong machine system perfor-mance.
Our goal is to approximate P (t|?
), theprobability that a term will be used in a humanabstract.
In the next section, we present two ap-proaches which will be used in tandem to makethis approximation.Figure 1: The Oracle (Average Jo score) Score ?
?6 Approximating P (t|?
)We seek to approximate P (t|?)
in an analo-gous fashion to the maximum-likelihood estimateP?
(t|?).
To this end, we devise methods to isolatea subset of terms which would likely be includedin the human summary.
These terms are gleanedfrom two sources, the topic description and thecollection of documents which were judged rele-vant to the topic.
The former will give rise to queryterms and the latter to signature terms.6.1 Query Term IdentificationA set of query terms is automatically ex-tracted from the given topic description.
Weidentified individual words and phrases fromboth the <topic> (Title) tagged paragraph aswell as whichever of the <narr> (Narrative)Set d408c: approximate, casualties,death, human, injury, number, recent,storms, toll, total, tropical, yearsSet d436j: accidents, actual, causes,damage, events, injured, killed, prevent,result, train, train wrecks, trains, wrecksTable 1: Query Terms for ?Tropical Storms?
and?Train Wrecks?
Topicstagged paragraphs occurred in the topic descrip-tion.
We made no use of the <granularity>paragraph marking.
We tagged the topic de-scription using the POS-tagger, NLProcessor(http://www.infogistics.com/posdemo.htm), andany words that were tagged with any NN (noun),VB (verb), JJ (adjective), or RB (adverb) tag wereincluded in a list of words to use as query terms.Table 1 shows a list of query terms for our twoillustrative topics.The number of query terms extracted in this wayranged from a low of 3 terms for document setd360f to 20 terms for document set d324e.6.2 Signature TermsThe second collection of terms we use to estimateP (t|?)
are signature terms.
Signature terms arethe terms that are more likely to occur in the doc-ument set than in the background corpus.
Theyare generally indicative of the content containedin the collection of documents.
To identify theseterms, we use the log-likelihood statistic suggestedby Dunning (Dunning 1993) and first used in sum-marization by Lin and Hovy (Hovy and Lin 2000).The statistic is equivalent to a mutual informationstatistic and is based on a 2-by-2 contingency ta-ble of counts for each term.
Table 2 shows a list ofsignature terms for our two illustrative topics.6.3 An estimate of P (t|?
)To estimate P (t|?
), we view both the query termsand the signature terms as ?samples?
from ideal-ized human summaries.
They represent the termsthat we would most likely see in a human sum-mary.
As such, we expect that these sample termsmay approximate the underlying set of humansummary terms.
Given a collection of query termsand signature terms, we can readily estimate ourtarget objective, P (t|?)
by the following:Pqs(t|?)
=12qt(?)
+12st(?
)155Set d408c: ahmed, allison, andrew,bahamas, bangladesh, bn, caribbean,carolina, caused, cent, coast, coastal,croix, cyclone, damage, destroyed, dev-astated, disaster, dollars, drowned, flood,flooded, flooding, floods, florida, gulf,ham, hit, homeless, homes, hugo, hurri-cane, insurance, insurers, island, islands,lloyd, losses, louisiana, manila, miles,nicaragua, north, port, pounds, rain,rains, rebuild, rebuilding, relief, rem-nants, residents, roared, salt, st, storm,storms, supplies, tourists, trees, tropi-cal, typhoon, virgin, volunteers, weather,west, winds, yesterday.Set d436j: accident, accidents, am-munition, beach, bernardino, board,boulevard, brake, brakes, braking, cab,car, cargo, cars, caused, collided, col-lision, conductor, coroner, crash, crew,crossing, curve, derail, derailed, driver,emergency, engineer, engineers, equip-ment, fe, fire, freight, grade, hit, holland,injured, injuries, investigators, killed,line, locomotives, maintenance, mechan-ical, miles, morning, nearby, ntsb, oc-curred, officials, pacific, passenger, pas-sengers, path, rail, railroad, railroads,railway, routes, runaway, safety, san,santa, shells, sheriff, signals, southern,speed, station, train, trains, transporta-tion, truck, weight, wreckTable 2: Signature Terms for ?Tropical Storms?and ?Train Wrecks?
TopicsFigure 2: Scatter Plot of ??
versus ?qswhere st(?
)=1 if t is a signature term for topic ?and 0 otherwise and qt(?)
= 1 if t is a query termfor topic ?
and 0 otherwise.More sophisticated weightings of the query andsignature have been considered; however, for thispaper we limit our attention to the above ele-mentary scheme.
(Note, in particular, a psuedo-relevance feedback method was employed by(Conroy et.
al.
2005), which gives improved per-formance.
)Similarly, we estimate the oracle score of a sen-tence?s expected number of human abstract termsas?qs(x) =1|x|?t?Tx(t)Pqs(t|?
)where |x| is the number of distinct terms that sen-tence x contains, T is the universal set of all termsand x(t) = 1 if the sentence x contains the term tand 0 otherwise.For both the oracle score and the approximation,we form the summary by taking the top scoringsentences among those sentences with at least 8distinct terms, until the desired length (250 wordsfor the DUC05 data) is achieved or exceeded.
(Thethreshold of 8 was based upon previous analysisof the sentence splitter, which indicated that sen-tences shorter than 8 terms tended not be be wellformed sentences or had minimal, if any, content.
)If the length is too long, the last sentence chosenis truncated to reach the target length.Figure 2 gives a scatter plot of the oracle score?
and its approximation ?qs for all sentences withat least 8 unique terms.
The overall Pearson corre-lation coefficient is approximately 0.70.
The cor-relation varies substantially over the topics.
Fig-ure 3 gives a histogram of the Pearson correlationcoefficients for the 50 topic sets.156Figure 3: Histogram of Document Set Pearson Co-efficients of ??
versus ?qs7 EnhancementsIn the this section we explore two approaches toimprove the quality of the summary, linguistic pre-processing (sentence trimming) and a redundancyremoval method.7.1 Linguistic PreprocessingWe developed patterns using ?shallow parsing?techniques, keying off of lexical cues in the sen-tences after processing them with the POS-tagger.We initially used some full sentence eliminationsalong with the phrase eliminations itemized be-low; analysis of DUC 03 results, however, demon-strated that the full sentence eliminations were notuseful.The following phrase eliminations were made,when appropriate:?
gerund clauses;?
restricted relative-clause appositives;?
intra-sentential attribution;?
lead adverbs.See (Dunlavy et.
al) for the specific rules usedfor these eliminations.
Comparison of two runsin DUC 04 convinced us of the benefit of applyingthese phrase eliminations on the full documents,prior to summarization, rather than on the selectedsentences after scoring and sentence selection hadbeen performed.
See (Conroy et.
al.
2004) fordetails on this comparison.After the trimmed text has been generated, wethen compute the signature terms of the documentsets and recompute the approximate oracle scores.Note that since the sentences have usually hadsome extraneous information removed, we expectsome improvement in the quality of the signatureterms and the resulting scores.
Indeed, the medianROUGE-2 score increases from 0.078 to 0.080.7.2 Redundancy RemovalThe greedy sentence selection process we de-scribed in Section 6 gives no penalty for sentenceswhich are redundant to information already con-tained in the partially formed summary.
A methodfor reducing redundancy can be employed.
Onepopular method for reducing redundancy is max-imum marginal relevance (MMR) (2).
Based onprevious studies, we have found that a pivotedQR, a method from numerical linear algebra, hassome advantages over MMR and performs some-what better.Pivoted QR works on a term-sentence matrixformed from a set of candidate sentences for in-clusion in the summary.
We start with enoughsentences so the total number of terms is approx-imately twice the desired summary length.
Let Bbe the term-sentence matrix with Bij = 1 if sen-tence j contains term i.The columns of B are then normalized so their2-norm (Euclidean norm) is the corresponding ap-proximate oracle score, i.e.
?qs(bj), where bj isthe j-th column ofB.We call this normalized termsentence matrix A.Given a normalized term-sentence matrix A,QR factorization attempts to select columns of Ain the order of their importance in spanning thesubspace spanned by all of the columns.
The stan-dard implementation of pivoted QR decomposi-tion is a ?Gram-Schmidt?
process.
The first r sen-tences (columns) selected by the pivoted QR areused to form the summary.
The number r is cho-sen so that the summary length is close to the tar-get length.
A more complete description can befound in (Conroy and O?Leary 2001).Note, that the selection process of using the piv-oted QR on the weighted term sentence matrixwill first choose the sentence with the highest ?pqscore as was the case with the greedy selectionprocess.
Its subsequent choices are affected byprevious choices as the weights of the columns aredecreased for any sentence which can be approxi-mated by a linear combination of the current set ofselected sentences.
This is more general than sim-ply demanding that the sentence have small over-lap with the set of previous chosen sentences as157Figure 4: ROUGE-2 Performance of Oracle ScoreApproximations ??
vs.
Humans and Peerswould be done using MMR.8 ResultsFigure 4 gives the ROUGE-2 scores with errorbars for the approximations of the oracle score aswell as the ROUGE-2 scores for the human sum-marizers and the top performing systems at DUC2005.
In the graph, qs is the approximate oracle,qs(p) is the approximation using linguistic prepro-cessing, and qs(pr) is the approximation with bothlinguistic preprocessing and redundancy removal.Note that while there is some improvement usingthe linguistic preprocessing, the improvement us-ing our redundancy removal technique is quite mi-nor.
Regardless, our system using signature termsand query terms as estimates for the oracle scoreperforms comparably to the top scoring system atDUC 05.Table 3 gives the ROUGE-2 scores for the re-cent DUC 06 evaluation which was essentiallythe same task as for DUC 2005.
The manner inwhich the linguistic preprocessing is performedhas changed from DUC 2005, although the typesof removals have remained the same.
In addition,pseudo-relevance feedback was employed for re-dundancy removal as mentioned earlier.
See (Con-roy et.
al.
2005) for details.While the main focus of this study is task-oriented multidocument summarization, it is in-structive to see how well such an approach wouldperform for a generic summarization task as withthe 2004 DUC Task 2 dataset.
Note, the ?
scorefor generic summaries uses only the signatureterm portion of the score, as no topic descrip-tion is given.
We present ROUGE-1 (rather thanSubmission Mean 95% CI Lower 95% CI UpperO (?)
0.13710 0.13124 0.14299C 0.13260 0.11596 0.15197D 0.12380 0.10751 0.14003B 0.11788 0.10501 0.13351G 0.11324 0.10195 0.12366F 0.10893 0.09310 0.12780H 0.10777 0.09833 0.11746J 0.10717 0.09293 0.12460I 0.10634 0.09632 0.11628E 0.10365 0.08935 0.11926A 0.10361 0.09260 0.1161724 0.09558 0.09144 0.09977?
(pr)qs 0.09160 0.08729 0.0957015 0.09097 0.08671 0.0947812 0.08987 0.08583 0.093858 0.08954 0.08540 0.0933823 0.08792 0.08371 0.09204?
(p)qs 0.08738 0.08335 0.09145?qs 0.08713 0.08317 0.0911028 0.08700 0.08332 0.09096Table 3: Average ROUGE 2 Scores for DUC06:Humans A-IROUGE-2) scores with stop words removed forcomparison with the published results given in(Nenkova and Vanderwende, 2005).Table 4 gives these scores for the top perform-ing systems at DUC04 as well as SumBasic and?
(pr)qs , the approximate oracle based on signatureterms alone with linguistic preprocess trimmingand pivot QR for redundancy removal.
As dis-played, ?
(pr)qs scored second highest and within the95% confidence intervals of the top system, peer65, as well as SumBasic, and peer 34.Submission Mean 95% CI Lower 95% CI UpperF 0.36787 0.34442 0.39467B 0.36126 0.33387 0.38754O (?)
0.35810 0.34263 0.37330H 0.33871 0.31540 0.36423A 0.33289 0.30591 0.35759D 0.33212 0.30805 0.35628E 0.33277 0.30959 0.35687C 0.30237 0.27863 0.32496G 0.30909 0.28847 0.32987?
(pr)qs 0.308 0.294 0.322peer 65 0.308 0.293 0.323SumBasic 0.302 0.285 0.319peer 34 0.290 0.273 0.307peer 124 0.286 0.268 0.303peer 102 0.285 0.267 0.302Table 4: Average ROUGE 1 Scores with stopwords removed for DUC04, Task 21589 ConclusionsWe introduced an oracle score based upon thesimple model of the probability that a humanwill choose to include a term in a summary.The oracle score demonstrated that for task-basedsummarization, extract summaries score as wellas human-generated abstracts using ROUGE.
Wethen demonstrated that an approximation of the or-acle score based upon query terms and signatureterms gives rise to an automatic method of summa-rization, which outperforms the systems enteredin DUC05.
The approximation also performedvery well in DUC 06.
Further enhancements basedupon linguistic trimming and redundancy removalvia a pivoted QR algorithm give significantly bet-ter results.ReferencesJamie Carbonnell and Jade Goldstein ?The of MMR,diversity-based reranking for reordering documentsand producing summaries.?
In Proc.
ACM SIGIR,pages 335?336.JohnM.
Conroy and Dianne P. O?Leary.
?Text Summa-rization via HiddenMarkovModels and Pivoted QRMatrix Decomposition?.
Technical report, Univer-sity of Maryland, College Park, Maryland, March,2001.John M. Conroy and Judith D. Schlesinger andJade Goldstein and Dianne P. O?Leary, Left-Brain Right-Brain Multi-Document Summariza-tion, Document Understanding Conference 2004http://duc.nist.gov/ 2004John M. Conroy and Judith D. Schlesinger and JadeGoldstein, CLASSY Tasked Based Summarization:Back to Basics, Document Understanding Confer-ence 2005 http://duc.nist.gov/ 2005John M. Conroy and Judith D. Schlesinger DianneP.
O?Leary, and Jade Goldstein, Back to Basciss:CLASSY 2006, Document Understanding Confer-ence 2006 http://duc.nist.gov/ 2006Terry Copeck and Stan Szpakowicz 2004 VocabularyAgreement Among Model Summaries and SourceDocuments In ACL Text Summarization Workshop,ACL 2004.Hoa Trang Dang Overview of DUC 2005 DocumentUnderstanding Conference 2005 http://duc.nist.govDaniel M. Dunlavy and John M. Conroy and JudithD.
Schlesinger and Sarah A. Goodman and MaryEllen Okurowski and Dianne P. O?Leary and Hansvan Halteren, ?Performance of a Three-Stage Sys-tem for Multi-Document Summarization?, DUC 03Conference Proceedings, http://duc.nist.gov/, 2003Ted Dunning, ?Accurate Methods for Statistics of Sur-prise and Coincidence?, Computational Linguistics,19:61-74, 1993.Julian Kupiec,, Jan Pedersen, and Francine Chen.
?ATrainable Document Summarizer?.
Proceedings ofthe 18th Annual International SIGIR Conferenceon Research and Development in Information Re-trieval, pages 68?73, 1995.Chin-Yew Lin and Eduard Hovy.
The automated ac-quisition of topic signatures for text summarization.In Proceedings of the 18th conference on Computa-tional linguistics, pages 495?501, Morristown, NJ,USA, 2000.
Association for Computational Linguis-tics.Chin-Yew Lin and Eduard Hovy.
Manual and Auto-matic Evaluation of Summaries In Document Un-derstanding Conference 2002 http:/duc.nist.govMulti-Lingual Summarization Evaluationhttp://www.isi.edu/ cyl/MTSE2005/MLSummEval.htmlNLProcessor http://www.infogistics.com/posdemo.htmAni Nenkova and Lucy Vanderwende.
2005.
TheImpact of Frequency on Summarization,MSR-TR-2005-101.
Microsoft Research Technical Report.Kishore Papineni and Salim Roukos and Todd Wardand Wei-Jing Zhu, Bleu: a method for automaticevaluation of machine translation, Technical Re-port RC22176 (W0109-022), IBM Research Divi-sion, Thomas J. Watson Research Center (2001)Simone Teufel and Hans van Halteren.
2004.
4:Evaluating Information Content by Factoid Analy-sis: Human Annotation and Stability, EMNLP-04,Barcelona159
