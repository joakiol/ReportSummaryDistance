Proceedings of the Third Workshop on Statistical Machine Translation, pages 187?190,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsThe Role of Pseudo References in MT EvaluationJoshua S. Albrecht and Rebecca HwaDepartment of Computer ScienceUniversity of Pittsburgh{jsa8,hwa}@cs.pitt.eduAbstractPrevious studies have shown automatic evalu-ation metrics to be more reliable when com-pared against many human translations.
How-ever, multiple human references may not al-ways be available.
It is more common to haveonly a single human reference (extracted fromparallel texts) or no reference at all.
Our ear-lier work suggested that one way to addressthis problem is to train a metric to evaluate asentence by comparing it against pseudo refer-ences, or imperfect ?references?
produced byoff-the-shelf MT systems.
In this paper, wefurther examine the approach both in terms ofthe training methodology and in terms of therole of the human and pseudo references.
Ourexpanded experiments show that the approachgeneralizes well across multiple years and dif-ferent source languages.1 IntroductionStandard automatic metrics are reference-based;that is, they compare system-produced translationsagainst human-translated references produced forthe same source.
Since there is usually no singlebest way to translate a sentence, each MT outputshould be compared against many references.
Onthe other hand, creating multiple human referencesis itself a costly process.
For many naturally occur-ring datasets (e.g., parallel corpora) only a single ref-erence is readily available.The focus of this work is on developing auto-matic metrics for sentence-level evaluation with atmost one human reference.
One way to supple-ment the single human reference is to use pseudoreferences, or sentences produced by off-the-shelfMT systems, as stand-ins for human references.However, since pseudo references may be imperfecttranslations themselves, the comparisons cannot befully trusted.
Previously, we have taken a learning-based approach to develop a composite metric thatcombines measurements taken from multiple pseudoreferences (Albrecht and Hwa, 2007).
Experimentalresults suggested the approach to be promising; butthose studies did not consider how well the metricmight generalize across multiple years and differentlanguages.
In this paper, we investigate the appli-cability of the pseudo-reference metrics under thesemore general conditions.Using the WMT06 Workshop shared-task re-sults (Koehn and Monz, 2006) as training exam-ples, we train a metric that evaluates new sentencesby comparing them against pseudo references pro-duced by three off-the-shelf MT systems.
We ap-ply the learned metric to sentences from the WMT07shared-task (Callison-Burch et al, 2007b) and com-pare the metric?s predictions against human judg-ments.
We find that additional pseudo referencesimprove correlations for automatic metrics.2 BackgroundThe ideal evaluation metric reports an accurate dis-tance between an input instance and its gold stan-dard, but even when comparing against imperfectstandards, the measured distances may still conveysome useful information ?
they may help to trian-gulate the input?s position relative to the true goldstandard.In the context of sentence-level MT evaluations,187the challenges are two-fold.
First, the ideal quantita-tive distance function between a translation hypoth-esis and the proper translations is not known; cur-rent automatic evaluation metrics produce approxi-mations to the true translational distance.
Second,although we may know the qualitative goodness ofthe MT systems that generate the pseudo references,we do not know how imperfect the pseudo refer-ences are.
These uncertainties make it harder to es-tablish the true distance between the input hypoth-esis and the (unobserved) acceptable gold standardtranslations.In order to combine evidence from these uncertainobservations, we take a learning-based approach.Each hypothesis sentence is compared with multi-ple pseudo references using multiple metrics.
Rep-resenting the measurements as a set of input featuresand using human-assessed MT sentences as trainingexamples, we train a function that is optimized tocorrelate the features with the human assessments inthe training examples.
Specifically, for each inputsentence, we compute a set of 18 kinds of reference-based measurements for each pseudo reference aswell as 26 monolingual fluency measurements.
Thefull set of measurements then serves as the input fea-ture vector into the function, which is trained viasupport vector regression.
The learned function canthen be used as an evaluation metric itself: it takesthe measurements of a new sentence as input and re-turns a composite score for that sentence.The approach is considered successful if the met-ric?s predictions on new test sentences correlate wellwith quantitative human assessments.
Like otherlearned models, the metric is expected to performbetter on data that are more similar to the traininginstances.
Therefore, a natural question that ariseswith a metric developed in this manner is: how welldoes it generalize?3 Research QuestionsTo better understand the capability of metrics thatcompare against pseudo-references, we consider thefollowing aspects:The role of learning Standard reference-basedmetrics can also use pseudo references; however,they would treat the imperfect references as goldstandard.
In contrast, the learning process aimsto determine how much each comparison with apseudo reference might be trusted.
To observe therole of learning, we compare trained metrics againststandard reference-based metrics, all using pseudoreferences.The amount vs. types of training data The suc-cess of any learned model depends on its training ex-periences.
We study the trade-off between the sizeof the training set and the specificity of the train-ing data.
We perform experiments comparing a met-ric trained from a large pool of heterogeneous train-ing examples that include translated sentences frommultiple languages and individual metrics trainedfrom particular source languages.The role of a single human reference Previousstudies have shown the importance of comparingagainst multiple references.
The approach in thispaper attempts to approximate multiple human ref-erences with machine-produced sentences.
Is a sin-gle trust-worthy translation more useful than multi-ple imperfect translations?
To answer this question,we compare three different reference settings: usingjust a single human reference, using just the threepseudo references, and using all four references.4 Experimental SetupFor the experiments reported in this paper, we usedhuman-evaluated MT sentences from past shared-tasks of the WMT 2006 and WMT 2007.
The dataconsists of outputs from German-English, Spanish-English, and French-English MT systems.
The out-puts are translations from two corpora: Europarl andnews commentary.
System outputs have been evalu-ated by human judges on a 5-point scale (Callison-Burch et al, 2007a).
We have normalized scoresto reduce biases from different judges (Blatz et al,2003).We experimented with using four different sub-sets of the WMT2006 data as training examples:only German-English, only Spanish-English, onlyFrench-English, all 06 data.
The metrics are trainedusing support vector regression with a Gaussiankernel as implemented in the SVM-Light package(Joachims, 1999).
The SVM parameters are tunedvia grid-search on development data, 20% of the fulltraining set that has been reserved for this purpose.188We used three MT systems to generate pseudo ref-erences: Systran1, GoogleMT 2, and Moses (Koehnet al, 2007).
We chose these three systems becausethey are widely accessible and because they takerelatively different approaches.
Moreover, althoughthey have not all been human-evaluated in the pastWMT shared tasks, they are well-known for produc-ing good translations.A metric is evaluated based on its Spearman rankcorrelation coefficient between the scores it gave tothe evaluative dataset and human assessments forthe same data.
The correlation coefficient is a realnumber between -1, indicating perfect negative cor-relations, and +1, indicating perfect positive correla-tions.Two standard reference-based metrics, BLEU(Papineni et al, 2002) and METEOR (Banerjee andLavie, 2005), are used for comparisons.
BLEU issmoothed (Lin and Och, 2004), and it considers onlymatching up to bigrams because this has higher cor-relations with human judgments than when higher-ordered n-grams are included.5 ResultsThe full experimental comparisons are summarizedin Table 1.
Each cell shows the correlation coef-ficient between the human judgments and a metric(column) that uses a particular kind of references(row) for some evaluation data set (block row).The role of learning With the exception of theGerman-English data, the learned metrics had highercorrelations with human judges than the baselines,which used standard metrics with a single humanreference.
On the other hand, results suggest thatpseudo references often also improve correlationsfor standard metrics.
This may seem counter-intuitive because we can easily think of cases inwhich pseudo references hurt standard metrics (e.g.,use poor outputs as pseudo references).
We hypoth-1Available from http://www.systransoft.com/.We note that Systran is also a participating system under eval-uation.
Although Sys-Test will be deemed to be identical toSys-Ref, it will not automatically receive a high score becausethe measurement is weighted by whether Sys-Ref was reliableduring training.
Furthermore, measurements between Sys-Testand other pseudo-references will provide alternative evidencesfor the metric to consider.2http://www.google.com/language tools/esize that because the pseudo references came fromhigh-quality MT systems and because standard met-rics are based on simple word matches, the chancesfor bad judgments (input words matched againstpseudo reference, but both are wrong) are relativelysmall compared to chances for good judgments.
Wefurther hypothesize that the learned metrics wouldbe robust against the qualities of the pseudo refer-ence MT systems.The amount vs. types of training data Com-paring the three metrics trained from single lan-guage datasets against the metric trained from allof WMT06 dataset, we see that the learning processbenefitted from the larger quantity of training exam-ples.
It may be the case that the MT systems for thethree language pairs are at a similar stage of maturitysuch that the training instances are mutually helpful.The role of a single human reference Our resultsreinforce previous findings that metrics are more re-liable when they have access to more than a sin-gle human reference.
Our experimental data sug-gests that a single human reference often may not beas reliable as using three pseudo references alone.Finally, the best correlations are achieved by usingboth human and pseudo references.6 ConclusionWe have presented an empirical study on automaticmetrics for sentence-level MT evaluation with atmost one human reference.
We show that pseudoreferences from off-the-shelf MT systems can beused to augment the single human reference.
Be-cause they are imperfect, it is important to weigh thetrustworthiness of these references through a train-ing phase.
The metric seems robust even when theapplied to sentences from different systems of a lateryear.
These results suggest that multiple imperfecttranslations make informative comparison points insupplement to human references.AcknowledgmentsThis work has been supported by NSF Grants IIS-0612791.189Eval.
Data Ref Type METEOR BLEU SVM(de06) SVM(es06) SVM(fr06) SVM(wmt06)de 1HR 0.458 0.471europarl 3PR 0.521* 0.527* 0.422 0.403 0.480* 0.46707 1HR+3PR 0.535* 0.547* 0.471 0.480* 0.477* 0.523*de 1HR 0.290 0.333news 3PR 0.400* 0.400* 0.262 0.279 0.261 0.26107 1HR+3PR 0.432* 0.417* 0.298 0.321 0.269 0.330es 1HR 0.377 0.412europarl 3PR 0.453* 0.483* 0.336 0.453* 0.432* 0.456*07 1HR+3PR 0.491* 0.503* 0.405 0.513* 0.483* 0.510*es 1HR 0.317 0.332news 3PR 0.320 0.317 0.393* 0.381* 0.426* 0.426*07 1HR+3PR 0.353* 0.325 0.429* 0.427* 0.380* 0.486*fr 1HR 0.265 0.246europarl 3PR 0.196 0.285* 0.270* 0.284* 0.355* 0.366*07 1HR+3PR 0.221 0.290* 0.277* 0.324* 0.304* 0.381*fr 1HR 0.226 0.280news 3PR 0.356* 0.383* 0.237 0.252 0.355* 0.373*07 1HR+3PR 0.374* 0.394* 0.272 0.339* 0.319* 0.388*Table 1: Correlation comparisons of metrics (columns) using different references (row): a single human reference(1HR), 3 pseudo references (3PR), or all (1HR+3PR).
The type of training used for the regression-trained metricsare specified in parentheses.
For each evaluated corpus, correlations higher than standard metric using one humanreference are marked by an asterisk(*).ReferencesJoshua S. Albrecht and Rebecca Hwa.
2007.
Regressionfor sentence-level MT evaluation with pseudo refer-ences.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL-2007).Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In ACL 2005Workshop on Intrinsic and Extrinsic Evaluation Mea-sures for Machine Translation and/or Summarization,June.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence estimationfor machine translation.
Technical Report NaturalLanguage Engineering Workshop Final Report, JohnsHopkins University.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007a.
(meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, pages 136?158, Prague, Czech Republic, June.Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Cameron ShawFordyce, and Christof Monz, editors.
2007b.
Proceed-ings of the Second Workshop on Statistical MachineTranslation.
Association for Computational Linguis-tics, Prague, Czech Republic, June.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scho?elkopf, Christo-pher Burges, and Alexander Smola, editors, Advancesin Kernel Methods - Support Vector Learning.
MITPress.Philipp Koehn and Christof Monz, editors.
2006.
Pro-ceedings on the Workshop on Statistical MachineTranslation.
Association for Computational Linguis-tics, New York City, June.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
Proceedingsof ACL, Demonstration Session.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metrics formachine translation.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics(COLING 2004), August.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics, Philadelphia, PA.190
