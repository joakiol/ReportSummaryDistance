Proceedings of NAACL HLT 2009: Short Papers, pages 101?104,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMulti-scale Personalization for Voice Search ApplicationsDaniel Bolan?osCenter for Spoken Language ResearchUniversity of Colorado at Boulder, USAbolanos@cslr.colorado.eduGeoffrey ZweigMicrosoft ResearchOne Microsoft Way, Redmond, WA 98052gzweig@microsoft.comPatrick NguyenMicrosoft ResearchOne Microsoft Way, Redmond, WA 98052panguyen@microsoft.comAbstractVoice Search applications provide a very con-venient and direct access to a broad varietyof services and information.
However, due tothe vast amount of information available andthe open nature of the spoken queries, theseapplications still suffer from recognition er-rors.
This paper explores the utilization of per-sonalization features for the post-processingof recognition results in the form of n-bestlists.
Personalization is carried out from threedifferent angles: short-term, long-term andWeb-based, and a large variety of features areproposed for use in a log-linear classificationframework.Experimental results on data obtained from acommercially deployed Voice Search systemshow that the combination of the proposedfeatures leads to a substantial sentence errorrate reduction.
In addition, it is shown thatpersonalization features which are very dif-ferent in nature can successfully complementeach other.1 IntroductionSearch engines are a powerful mechanism to findspecific content through the use of queries.
In recentyears, due to the vast amount of information avail-able, there has been significant research on the use ofrecommender algorithms to select what informationwill be presented to the user.
These systems try topredict what content a user may want based not onlyon the user?s query but on the user?s past queries,history of clicked results, and preferences.
In (Tee-van et al, 1996) it was observed that a significantpercent of the queries made by a user in a searchengine are associated to a repeated search.
Recom-mender systems like (Das et al, 2007) and (Dou etal., 2007) take advantage of this fact to refine thesearch results and improve the search experience.In this paper, we explore the use of personaliza-tion in the context of voice searches rather than webqueries.
Specifically, we focus on data from a multi-modal cellphone-based business search application(Acero et al, 2008).
In such an application, repeatedqueries can be a powerful tool for personalization.These can be classified into short and long-term rep-etitions.
Short-term repetitions are typically causedby a speech recognition error, which produces an in-correct search result and makes the user repeat orreformulate the query.
On the other hand, long-termrepetitions, as in text-based search applications, oc-cur when the user needs to access some informationthat was accessed previously, for example, the exactlocation of a pet clinic.This paper proposes several different user per-sonalization methods for increasing the recognitionaccuracy in Voice Search applications.
The pro-posed personalization methods are based on extract-ing short-term, long-term and Web-based featuresfrom the user?s history.
In recent years, other userpersonalization methods like deriving personalizedpronunciations have proven successful in the contextof mobile applications (Deligne et al, 2002).The rest of this paper is organized as follows: Sec-tion 2 describes the classification method used forrescoring the recognition hypotheses.
Section 3 de-scribes the proposed personalization methods.
Sec-tion 4 describes the experiments carried out.
Finally,101conclusions from this work are drawn in section 5.2 Rescoring procedure2.1 Log linear classificationOur work will proceed by using a log-linear clas-sifier similar to the maximum entropy approach of(Berger and Della Pietra, 1996) to predict whichword sequence W appearing on an n-best list N ismost likely to be correct.
This is estimated asP (W |N) = exp(?i ?ifi(W,N))?W ?
?N exp(?i ?ifi(W ?, N)) .
(1)The feature functions fi(W,N) can represent ar-bitrary attributes of W and N .
This can be seento be the same as a maximum entropy formulationwhere the class is defined as the word sequence (thusallowing potentially infinite values) but with sumsrestricted as a computational convenience to onlythose class values (word strings) appearing on the n-best list.
The models were estimated with a widelyavailable toolkit (Mahajan, 2007).2.2 Feature extractionGiven the use of a log-linear classifier, the crux ofour work lies in the specific features used.
As a base-line, we take the hypothesis rank, which results inthe 1-best accuracy of the decoder.
Additional fea-tures were obtained from the personalization meth-ods described in the following section.3 Personalization methods3.1 Short-term personalizationShort-term personalization aims at modeling the re-pair/repetition behavior of the user.
Short-term fea-tures are a mechanism suitable for representing neg-ative evidence: if the user repeats a utterance it nor-mally means that the hypotheses in the previous n-best lists are not correct.
For this reason, if a hy-pothesis is contained in a preceding n-best list, thathypothesis should be weighted negatively during therescoring.A straightforward method for identifying likelyrepetitions consists of using a fixed size time win-dow and considering all the user queries within thatwindow as part of the same repetition round.
Oncean appropriate window size has been determined,the proposed short-term features can be extracted foreach hypothesis using a binary tree like the one de-picted in figure 1, where feature values are in theleaves of the tree.Does a recent (60s) n-bestlist contain the hypothesiswe are scoring?seen = 1seen & clicked = 0seen & clicked = 0NoDid the user clickon that hypothesis?Yesseen = 0seen & clicked = 1seen & clicked = 0Noseen = 0seen & clicked = 0seen & clicked = 1YesFigure 1: Short-term feature extraction (note that over-lines mean ?do not?
).Given these features, we expect ?seen and notclicked?
to have a negative weight while ?seen andclicked?
should have a positive weight.3.2 Long-term personalizationLong-term personalization consists of using the userhistory (i.e.
recognition hypotheses that were con-firmed by the user in the past) to predict whichrecognition results are more likely.
The assumptionhere is that recognition hypotheses in the n-best listthat match or ?resemble?
those in the user history aremore likely to be correct.
The following list enumer-ates the long-term features proposed in this work:?
User history (occurrences): number of timesthe hypothesis appears in the user history.?
User history (alone): 1 if the hypothesis ap-pears in the user history and no other compet-ing hypothesis does, otherwise 0.?
User history (most clicked): 1 if the hypothe-sis appears in the user history and was clickedmore times than any other competing hypothe-sis.?
User history (most recent): 1 if the hypothe-sis appears in the user history and was clicked102more recently than any other competing hy-pothesis.?
User history (edit distance): minimum edit dis-tance between the hypothesis and the closestquery in the user history, normalized by thenumber of words.?
User history (words in common): maximumnumber of words in common between the hy-pothesis and each of the queries in the user his-tory, normalized by the number of words in thehypothesis.?
User history (plural/singular): 1 if either theplural or singular version of the hypothesis ap-pears in the user history, otherwise 0.?
Global history: 1 if the hypothesis has everbeen clicked by any user, otherwise 0.?
Global history (alone): 1 if the hypothesis is theonly one in the n-best that has ever been clickedby any user, otherwise 0.Note that the last two features proposed makeuse of the ?global history?
which comprises all thequeries made by any user.3.3 LiveSearch-based featuresTypically, users ask for businesses that exist, and ifa business exists it probably appears in a Web docu-ment indexed by Live Search (Live Search, 2006).
Itis reasonable to assume that the relevance of a givenbusiness is connected to the number of times it ap-pears in the indexed Web documents, and in this sec-tion we derive such features.For the scoring process, an application has beenbuilt that makes automated queries to Live Search,and for each hypothesis in the n-best list obtains thenumber of Web documents in which it appears.
De-noting by x the number of Web documents in whichthe hypothesis (the exact sequence of words, e.g.
?tandoor indian restaurant?)
appears, the followingfeatures are proposed:?
Logarithm of the absolute count: log(x).?
Search results rank: sort the hypotheses in then-best list by their relative value of x and usethe rank as a feature.?
Relative relevance (I): 1 if the hypothesis wasnot found and there is another hypothesis in then-best list that was found more than 100 times,otherwise 0.?
Relative relevance (II): 1 if the the hypothesisappears fewer than 10 times and there is an-other hypothesis in the n-best list that appearsmore than 100 times, otherwise 0.4 Experiments4.1 DataThe data used for the experiments comprises 22473orthographically transcribed business utterances ex-tracted from a commercially deployed large vocabu-lary directory assistance system.For each of the transcribed utterances two n-bestlists were produced, one from the commercially de-ployed system and other from an enhanced decoderwith a lower sentence error rate (SER).
In the exper-iments, due to their lower oracle error rate, n-bestsfrom the enhanced decoder were used for doing therescoring.
However, these n-bests do not correspondto the listings shown in the user?s device screen (i.e.do not match the user interaction) so are not suit-able for identifying repetitions.
For this reason, theshort term features were computed by comparing ahypothesis from the enhanced decoder with the orig-inal n-best list from the immediate past.
Note that allother features were computed solely with referenceto the n-bests from the enhanced decoder.A rescoring subset was made from the originaldataset using only those utterances in which the n-best lists contain the correct hypothesis (in any po-sition) and have more than one hypothesis.
For allother utterances, rescoring cannot have any effect.The size of the rescoring subset is 43.86% the sizeof the original dataset for a total of 9858 utterances.These utterances were chronologically partitionedinto a training set containing two thirds and a testset with the rest.4.2 ResultsThe baseline system for the evaluation of the pro-posed features consist of a ME classifier trained ononly one feature, the hypothesis rank.
The resultingsentence error rate (SER) of this classifier is that ofthe best single path, and it is 24.73%.
To evaluate103the contribution of each of the features proposed insection 3, a different ME classifier was trained us-ing that feature in addition to the baseline feature.Finally, another ME classifier was trained on all thefeatures together.Table 1 summarizes the Sentence Error Rate(SER) for each of the proposed features in isolationand all together respect to the baseline.
?UH?
standsfor user history.Features SERHypothesis rank (baseline) 24.73%base + repet.
(seen) 24.48%base + repet.
(seen & clicked) 24.32%base + repet.
(seen & clicked) 24.73%base + UH (occurrences) 23.76%base + UH (alone) 23.79%base + UH (most clicked) 23.73%base + UH (most recent) 23.88%base + UH (edit distance) 23.76%base + UH (words in common) 24.60%base + UH (plural/singular) 24.76%base + GH 24.63%base + GH (alone) 24.66%base + Live Search (absolute count) 24.35%base + Live Search (rank) 24.85%base + Live Search (relative I) 23.51%base + Live Search (relative II) 23.69%base + all 21.54%Table 1: Sentence Error Rate (SER) for each of the fea-tures in isolation and for the combination of all of them.5 ConclusionsThe proposed features reduce the SER of the base-line system by 3.19% absolute on the rescoring set,and by 1.40% absolute on the whole set of tran-scribed utterances.Repetition based features are moderately useful;by incorporating them into the rescoring it is possi-ble to reduce the SER from 24.73% to 24.32%.
Al-though repetitions cover a large percentage of thedata, it is believed that inconsistencies in the userinteraction (the right listing is displayed but not con-firmed by the user) prevented further improvement.As expected, long-term personalization based fea-tures contribute to improve the classification accu-racy.
The UH (occurrences) feature by itself is ableto reduce the SER in about a 1%.Live Search has shown a very good potential forfeature extraction.
In this respect it is interesting tonote that a right design of the features seems criticalto take full advantage of it.
The relative number ofcounts of one hypothesis respect to other hypothesesin the n-best list is more informative than an absoluteor ranked count.
A simple feature using this kind ofinformation, like Live Search (relative I), can reducethe SER in more than 1% respect to the baseline.Finally, it has been shown that personalizationbased features can complement each other very well.ReferencesAlex Acero, Neal Bernstein, Rob Chambers, Yun-ChengJu, Xiao Li, Julian Odell, Patrick Nguyen, OliverScholtz and Geoffrey Zweig.
2008.
Live Searchfor Mobile: Web Services by Voice on the Cellphone.ICASSP 2008, March 31 2008-April 4 2008.
Las Ve-gas, NV, USA.Adam L. Berger; Vincent J. Della Pietra; Stephen A.Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
Computational Lin-guistics, 1996.
22(1): p. 39-72.Abhinandan Das, Mayur Datar and Ashutosh Garg.2007.
Google News Personalization: Scalable OnlineCollaborative Filtering.
WWW 2007 / Track: Indus-trial Practice and Experience May 8-12, 2007.
Banff,Alberta, Canada.Sabine Deligne, Satya Dharanipragada, RameshGopinath, Benoit Maison, Peder Olsen and HarryPrintz.
2002.
A robust high accuracy speech recog-nition system for mobile applications.
Speech andAudio Processing, IEEE Transactions on, Nov 2002,Volume: 10, Issue: 8, On page(s): 551- 561.Zhicheng Dou, Ruihua Song, and Ji-Rong Wen.
2007.A large-scale evaluation and analysis of personalizedsearch strategies.
In WWW ?07: Proceedings of the16th international conference on World Wide Web,pages 581 - 590, New York, NY, USA, 2007.
ACMPress.Live Search.
?http://www.live.com,?.Milind Mahajan.
2007.
Conditional Maximum-EntropyTraining Tool http://research.microsoft.com/en-us/downloads/9f199826-49d5-48b6-ba1b-f623ecf36432/.Jaime Teevan, Eytan Adar, Rosie Jones and Michael A.S. Potts.
2007.
Information Re-Retrieval: RepeatQueries in Yahoos Logs.
SIGIR, 2007.104
