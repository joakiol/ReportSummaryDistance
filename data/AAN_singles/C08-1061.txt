Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 481?488Manchester, August 2008Classifying What-type Questions by Head Noun TaggingFangtao Li, Xian Zhang, Jinhui Yuan, Xiaoyan ZhuState Key Laboratory on Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Sci.
and Tech., Tsinghua University, Beijing 100084, Chinazxy-dcs@tsinghua.edu.cnAbstractClassifying what-type questions intoproper semantic categories is found morechallenging than classifying other typesin question answering systems.
In thispaper, we propose to classify what-typequestions by head noun tagging.
The ap-proach highlights the role of head nounsas the category discriminator of what-type questions.
To reduce the semanticambiguities of head noun, we integratelocal syntactic feature, semantic featureand category dependency among adjacentnouns with Conditional Random Fields(CRFs).
Experiments on standard ques-tion classification data set show that theapproach achieves state-of-the-art per-formances.1 IntroductionQuestion classification is a crucial component ofmodern question answering system.
It classifiesquestions into several semantic categories whichindicate the expected semantic type of answers tothe questions.
The semantic category helps tofilter out irrelevant answer candidates, and de-termine the answer selection strategies.
1The widely used question category criteria is atwo-layered taxonomy developed by Li and Roth(2002) from UIUC.
The hierarchy contains 6coarse classes and 50 fine classes as shown inTable 1.
In this paper, we focus on fine-categoryclassification.
Each fine category will be denotedas ?Coarse:fine?, such as ?HUM:individual?.A what-type question is defined as the onewhose question word is ?what?, ?which?,?name?
or ?list?.
It is a dominant type in ques-tion answering system.
Li and Roth (2006) find?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.Coarse FineABBR abbreviation, expressionDESC definition, description, manner, reasonENTY animal, body, color, creation, currency, dis-ease/medicine, event, food, instrument, language,letter, other, plant, product, religion, sport, sub-stance, symbol, technique, term, vehicle, wordHUM description, group, individual, titleLOC city, country, mountain, other, stateNUM code, count, date, distance, money, order, other,percent, period, speed, temperature, size, weightTable 1.
Question Ontologythat the distribution of what-type questions overthe semantic classes is quite diverse, and they aremore difficult to be classified than other types.Table 2 shows the classification accuracies ofeach question word in UIUC data set using Sup-port Vector Machine (SVM) with unigram fea-tures.
What-type questions account for more than70 percent in the data set, but the classificationaccuracy of this type only achieves 75.50%.
Inthis experiment, 90.53% (86 over 95) of the er-rors are generated by what-type questions.
Due toits challenge, this paper focuses on what-typequestion classification.Total Wrong AccuracyWhat-type 351 86 75.50%Where 26 2 92.31%When 26 0 100.0%Who 47 3 93.62%How 46 4 91.30%Why 4 0 100.0%Total 500 95 81.00%Table 2.
Classification performance for eachquestion words with unigramHead noun has been presented to  play an im-portant role in classifying what-type questions(Metzler and Croft, 2005).
It refers to the nounreflecting the focus of a question, such as?flower?
in the question ?What is Hawaii's state481flower??.
These nouns can effectively reduce thenoise generated by other words.
If the head noun?length?
is identified from the question ?What isthe length of the coastline of the state ofAlaska?
?, this question can be easily classifiedinto ?NUM:distance?.
However, the above SVMmisclassified this question into ?LOC:-state?, asthe words ?state?
and ?Alaska?
confused theclassifier.
Considering another two questionsexpressed in (Zhang and Lee, 2002), ?Whichuniversity did the president graduate from??
and?Which president is a graduate of the HarvardUniversity?, although they contain similar words,it is not difficult to distinguish them with thehead nouns ?university?
and ?president?
respec-tively.Nevertheless, a head noun may correspond toseveral semantic categories.
In this situation, weneed to incorporate the head noun context fordisambiguation.
The potentially useful contextfeatures include local syntactic features, semanticfeatures and neighbor?s semantic category.
Takethe noun ?money?
as an example, it possibly cor-responds to two categories: ?NUM:money?
and?ENTY:currency?.
If there is an adjacent wordfalling into ?Loc:country?
category, the ?money?tends to belong to ?ENTY:currency?.
Otherwise,if the ?ENTY:product?
or ?HUM:individual?surrounds it, the word ?money?
may refer to?NUM:money?.Based on the above notions, we propose a newstrategy to classify what-type questions by wordtagging, and the selected head noun determinesquestion category.
The question classificationtask is formulated into word sequence taggingproblem.
All the question words are divided intosemantic words and non-semantic words.
Thesemantic word expresses certain semantic cate-gory, such as ?dog?
corresponding to category?ENTITY:animal?, while ?have?
correspondingto no category.
The label for semantic words isone of the question categories, and ?O?
is fornon-semantic word.
Here, we just consider thenouns as semantic words, others as non-semanticwords.
Each word in a question will be tagged asa label using Conditional Random Fields model,and the head noun?s label is chosen as the ques-tion category.In conclusion, the CRFs based approach hastwo main steps: the first step is to tag all thewords in questions using CRFs, and the secondstep is choosing the head noun?s label as thequestion category.
It can use the head noun toeliminate the noisy words, and take advantagesof CRFs model to integrate not only the syntacticand semantic features, but also the adjacent cate-gories to tag head noun.The rest of this paper is organized as follows:Section 2 discusses related work.
Section 3 in-troduces the Condition Random Fields(CRFs)and the defined Long-Dependency CRFs(LDCRFs).
Section 4 describes the features usedin the LDCRFs.
The head noun extraction me-thod is presented in Section 5.
We evaluate theproposed approach in Section 6.
Section 7 con-cludes this paper and discusses future work.2 Related worksQuestion Answering Track was first introducedin the Text REtrieval Conference (TREC) in1999.
Since then, question classification has beena popular topic in the research community of textmining.
Simple question classification approach-es usually employ hand-crafted rules (such asPrager et.
al, 1999), which are effective for spe-cific question taxonomy.
However, laborioushuman effort is required to create these rules.Some other systems employed machine learn-ing approaches to classify questions.
Li andRoth (2002) presented a hierarchical classifierbased on the Sparse Network of Winnows (Snow)architecture.
Tow classifiers were involved inthis work: the first one classified questions intothe coarse categories; and the other classifiedquestions into fine categories.
Several syntacticand semantic features, including semi-automatically constructed class-specific relation-al features, were extracted and compared in theirexperiments.
The results showed that the hierar-chical classifier was effective for question classi-fication task.Metzler and Croft (2005) used prior know-ledge about correlations between question wordsand types to train word-specific question classifi-ers.
They identified the question words firstly,and trained separate classifier for each questionword.
WordNet was used as semantic features toboost the classification performance.
In this pa-per, according to question word, all the questionsare classifie into two categories: what-type onesand non-what-type one.Recent question classification methods havepaid more attention on the syntactic structure ofsentence.
They used a parser to get the syntactictree, and then took advantage of the structureinformation.
Zhang and Lee (2002) proposed atree kernel Support Vector Machine classifierand experiment results showed that syntactic in-formation and tree kernel could solve this prob-482lem.
Nguyen et al (2007) proposed a subtreemining method for question classification.
Theyformulated question classification as tree catego-ry determination, and maximum entropy andboosting model with subtree features were used.The experiment results showed that the subtreemining method can achieve a higher accuracy inquestion classification task.In this paper, we formulate the what-typequestion classification as word sequence taggingproblem.
The tagged label is either one of thequestion categories for nouns s or ?O?
for otherwords.
Since head noun can be the discriminatorfor a question, its tag is extracted as the questioncategory in our work.
A long-dependency Condi-tional Random Fields Classifier is defined to tagquestion words with the features which not onlyinclude the syntactic and semantic features, butalso the semantic categories?
transition features.3 Conditional Random FieldsConditional Random Fields (CRFs) are a type ofdiscriminative probabilistic model proposed forlabeling sequential data (Lafferty et al 2001).
Itsdefinition is as follows:Definition: Let ( )G V E= ,  be a graph such that( )v v VY ?=Y , so that Y  is indexed by the vertices of G .Then ( ),X Y  is a conditional random field in case, whenconditioned on X , the random variables vY  obey the Mar-kov property with respect to thegraph: ( )v wp Y Y w v| , , ?
=X ( )v wp Y Y w v| , ,X ?
, where w v?means that w  and v  are neighbors in G .The joint distribution over the label sequence Ygiven X  has the form1( ) exp ( ) ( )( ) i i i ie E i v V ip t e e s v vZ?
??
, ?
,| = , | , + , | , ,?
??
??
??
?Y X Y X Y XXwhere ( )Z X  is a normalization factor, is  is astate feature function and it  is a transition fea-ture function, i?
and i?
are the correspondingweights.Here we assume the features are given, thenthe parameter estimation problem is to determinethe parameters 1 2 1 2( , )?
?
?
?
?= , , , ,?
?
fromtraining data.
The inference problem is to findthe most probable label sequence y?
for inputsequence x .In the training set, we label all the nounwords with semantic question categories, andother words will be labeled by ?O?.
We supposethat only adjacent noun words connect with eachother, and there is no edge between noun andnon-noun words, i.e., noun word and non-nounwords may share neighbor?s state features, butthey are not connected by an edge.
A labeled ex-ample is shown as ?What/O was/OQueen/HUM:individual Victria/HUM:individual?s/O title/HUM:title regarding/O India/LOC:-country?.
In this labeled sentence, only threeedges connect four noun words: Queen, Victria,title and India.Figure 1.
Long-Dependency CRFs, the dottedlines summarize many outgoing edgesWith this assumption, we define a Long-Dependency Conditional Random Fields(LDCRFs) model (see Figure 1).
The long de-pendency means that the target words may haveno edge with its neighbors, but connect with oth-er words at a long distance.
It can be consideredas a type of linear- chain CRFs.
Its parameterestimation problem and inference problem can besolved by the algorithm for chain-structure CRFs(Sutton and McCallum, 2007).4 Feature SetsOne of the most attractive advantages  of CRFs isthat they can integrate rich features, includingnot only state features, but also transition fea-tures.
In this section, we will introduce the syn-tactic, semantic and transition features used inour sequence tagging approach.4.1 Syntactic FeaturesThe questions, which have similar syntactic style,intend to belong to the same category.
Besideswords, part-of-speech, chunker, parser informa-tion and question length are used as syntacticfeatures.All the words are lemmatized to root forms,and a window size (here is 4) is set to utilize thesurrounding words.The part-of-speech (POS) tagging is com-pleted by SS Tagger (Tsuruoka and Tsujii, 2005),with our own improvement.The noun phrase chunking (NP chunking)module uses the basic NP chunker software from483(Ramshaw and Marcus, 1995) to recognize thenoun phrases in the question.The importance of question syntactic structureis reported in (Zhang and Lee, 2002; Nguyen etal.
2007).
They used complex machine learningmethod to capture the tree architecture.
TheLDCRFs based approach just selects parent node,relation with parent and governor for each targetword generated from Minipar(Lin, 1999).The length of question is another importantsyntactic feature.
In our experiment, a thresholdis set to denote the length as ?high?
or ?low?.4.2 Semantic FeaturesSemantic features concern what words mean andhow these meanings combine in sentence to formsentence meanings.
Named Entity is a predefinedsemantic category for noun word.
WordNet(Fellbaum, 1998) is a public semantic lexicon forEnglish language, and it is used to get hypernymfor noun word and synset for head verb which isthe first notional verb in the sentence.Named Entity: Named entity recognizer as-signs a semantic category to the noun phrase.
Itis widely used to provide semantic information intext mining.
In this paper, Stanford Named EntityRecognizer (Finkel et al 2005) is used to classifynoun phrases into four semantic categories:PERSON, LOCATION, ORGANIZARION andMISC.Noun Hypernym: Hypernyms can be consi-dered as semantic abstractions.
It helps to narrowthe gap between training set and testing set.
Forexample, ?What is Maryland's state bird?
?, if werecursively find the bird?s hypernym ?animal?,which appeared in training set, this question canbe easily classified.In training set, we try to select appropriatehypernyms for each category.
An correct Word-Net sense is first assigned for each polysemousnoun, and then all its hypernyms are recursivelyextracted.
The sense determination step isprocessed with the algorithm in (Pedersen et al2005).
They disambiguate word sense by assign-ing a target word the sense, which is most relatedto the senses of its neighboring words.Since the word sense disambiguation methodhas low performance, with F1-measure below50% reported in (Pedersen et al 2005), a featureselection method is used to extract the most dis-criminative hypernyms.
The hypernyms selectionmethod is processed as follows: we first removethe low frequency hypernyms, and select thehypernyms using a chi-square method.
The chi-square value measures the lack of independencebetween a hypernym h and category jc .
It is de-fined as:22 ( ) ( )( , )( ) ( ) ( ) ( )jA B C D AD CBh cA C B D A B C D?
+ + + ?
?= + ?
+ ?
+ ?
+where A is the number of hypernym h, whichbelongs to category jc ; B is the number of  h outof jc ; C is the number of other hypernyms in jc ;D is the number of other hypernyms out of jc .We set a threshold to select the most discri-minative hypernym set.
Extracted examples areshown in Figure 2.Figure 2.
Examples of extracted hypernymIt can be seen that these hypernyms are appro-priate to describe the semantic meaning of thecategory.
They are expected to work as theclass-specific relational features which are semi-constructed by (Li and Roth, 2002).
In our ap-proach, we just use the noun?s minimum upperhypernym, existing in training set, as the feature.Head Verb Synset:  To avoid losing questionverb information, we extract head verb, which isthe first notional verb in a question, and expandit using WordNet synset as feature.
The headverb extraction is based on the following simplerules:If the first word is ?name?
or ?list?, the headverb will be denoted as this word.
If the first verbfollowing question word is ?do?
or other aux-iliary verb, the next verb is extracted as headverb.
Otherwise the head verb is extracted as thefirst verb after question word.4.3 Transition FeaturesState transition feature captures the contextualconstraints among labels.
We define it as( 1 ) ( )y yt e i i e e y y?
?, ?=< , + >, | , = | =< , > .Y X YENTITY:animal:animal, carnivore, chordate, equine,horse, living_thing, vertebrate, mammal,odd-toed_ungulate, organism, placentalENTITY:food:alcohol, beer, beverage, brew, cereal,condiment, crop, drink, drug_of_abuse,flavorer, food, foodstuff, helping, indefi-nite_quantity, ingredient, liquid, output,produce, small_indefinite_quantity, pro-duction, solid, substance, vegetable484Where e represents the edge between adjacentnouns.
It captures adjacent categories as featuresto tag the target noun.
Note that, for simplicity,the value of above feature is independent of theobservations X.5 Head Noun ExtractionAfter tagging all the words in a question, we willextract head noun and assign its tagged label tothe question as the final question classificationresult.The head noun extraction is a simple heuristicmethod inspired by  (Metzler and Croft, 2005).We first run a POS tagger on each question, andpost-process them to make sure that each sen-tence has at least one noun word.
Next, the firstNP chunk after the question word is extracted byshallow parsing.
The head noun is determined bythe following heuristic rules:1.
If the NP chunker is before the first verb,or the NP chunk is after the first verb butthere is no possessive case after the NPchunker, we mark the rightmost word inthe chunker as head noun.2.
Otherwise, extract the next NP chunkerand recursively process the above rules.Although this method may depend on the per-formance of POS tagger and shallow parser, itachieves the accuracy of over 95% on the UIUCdata set in our implementation.6 Experiments6.1 Experiment SettingsData Set:We evaluate the proposed approach on theUIUC data set (Li and Roth, 2002).
5500 ques-tions are selected for training, and 500 questionsare selected for testing.
The classification catego-ries have been introduced as question ontologyin section 1.
This paper only focuses on 50 fineclasses.To train the LDCRFs, we manually labeled allthe noun words with one of 50 fine categories.Other words are labeled with ?O?.
One of thelabeled examples is ?What/O was/OQueen/HUM:individual Victria/HUM:individual?s/O title/HUM:title regarding/O In-dia/LOC:country?.
Ten people labeled 3407what-type questions as training set.
Each ques-tion was independently annotated by two peopleand reviewed by the third.
For words which havemore than one category, the annotators selectedthe most salient one according to the context.
Fortesting set, 351 what-type questions were se-lected for experiments evaluation.Evaluation metric:Accuracy performance is widely used to evaluatequestion classification methods [Li and Roth,2002; Zhang and Lee, 2003, Melter and Croft,2004; Nguyen et al 2007].6.2 Approach Performance Evaluation# Wrong AccuracySVM 86 75.50%LDCRFs-based80 77.20%Table 3.
LDCRFS-based Approach V.S.
SVMTable 3 shows the compared results betweenthe proposed LDCRFs based approach and SVMwith unigram feature.
The LDCRFs based ap-proach achieves accuracy of 77.20%, comparedwith 75.50% of SVM.
Observing the detailedclassification results, we conclude two advantag-es of LDCRFs over SVMs.
First LDCRFs basedapproach focuses on head noun to reduce thenoise generated by other words.
The question?What is the length of the coastline of the state ofAlaska??
is misclassified as ?LOC:state?
bySVM, whereas it is correctly classified by ourapproach.
Second, LDCRFs based approach canutilize rich features, including not only state fea-tures, but also transition features.
With the newfeatures involved, LDCRFs is expected to im-prove classification performance.
This unigramresult is used as our baseline.
The following ex-periments are conducted to test the new featurecontribution.Syntactic Features:In addition to words, four types of features, in-cluding part-of-speech (POS), chunker, parserinformation (Parser), and question length(Length), are extracted as syntactic features.AccuracyUnigram (U) 77.20%U+POS 78.35%U+Chunker 77.20%U+Parser 79.20%U+Length 77.49%Total Syn 80.06%Table 4.
Syntactic Feature PerformanceFrom the syntactic feature results in Table 4,we can draw the following conclusions?(a).
Among four types of syntactic features, pars-485er information contributes mostly.
(Metzler andCroft, 2005) once claimed that it didn?t makeimprovement by just incorporating these infor-mation as explicit feature, and they should beused implicitly via a tree structure.
Without usingthe complex tree mining and representing tech-nique, our LDCRFs-based approach just incorpo-rates the word parent, relation with parent andword governor from Minipar as features.
Theexperiments show that the parser informationfeature is able to capture the syntactic structureinformation, and it makes much improvement inthis sequence tagging approach.
(b) Question length makes small improvement.However, the chunker features make no im-provement, consistent with the observation re-ported by (Li and Roth, 2006).?
The best accuracy (80.06%) is achieved byintegrating all the syntactic features.Semantic Features:AccuracyUnigram(U) 77.20%U+NE 77.20%U+HVSyn 78.63%U+NHype 78.35%Total Sem 80.06%Table 5.
Semantic Feature PerformanceThe semantic features include Named Entity(NE), Noun Hypernym (NHype) and Head VerbSynset (HVSyn).From Table 5 we can draw the following conclu-sions:(a) NE makes no improvement in classificationtask.
The reason is that the named entity recog-nizer contains only four semantic categories.
It istoo coarse to distinguish 50 fined-categories.
(b) The LDCRFs-based approach just considersthe noun words as semantic words.
The headverb synsets (HVSyn) are imported as one ofsemantic features.
The experiment results showthat it is effective to incorporate the head verb asfeatures, which achieves the best individual accu-racy among semantic features.
(c) Noun hypernyms (NHype) are the most im-portant semantic features.
They narrow the se-mantic gap between training set and testing set.From Section 4.2, we can see that the selectednoun hypernyms are appropriate for each catego-ry.
While, the experiment with NHype featuresdoesn?t make considerable improvement as wepreviously thought.
The reason may come fromthe fact that the word sense disambiguation me-thod has low performance.
A hypernym selectionmethod is used in training set, but we didn?ttackle the error in testing set.
Once the wordsense disambiguation is wrong, it will not makeimprovement, but generate noise (see the discus-sion examples in next section).
(d) It is an interesting result that using all the se-mantic features can achieve the same accuracy asthe syntactic features (80.06%).Feature Combination:In this section, we carry out experiments to ex-amine whether the performance can be boostedby integrating syntactic features and semanticfeatures.
Several results are shown in Table 6.The experiments show that:(a)  Parser Information and Head Verb Synset areboth the most contributive features for syntacticset and semantic feature set.
While the perfor-mance with these two features can?t beat the per-formance by combining Parser Information andNoun Hypernyms.AccuracyU+POS+NE+HVSyn 80.91%U+Parser+NHype 81.77%U+Parser+HVSyn 80.91%U+POS+Length+NHype 80.63%Total 82.05%Table 6.
Combined Feature Performance(b) The best result for classifying what-typequestions with our approach is achieved by inte-grating all the features.
The accuracy is 82.05%,which is 18.7 percent error reduction (from22.08% to 17.95%) over unigram feature set.
Itshows that the features we extract are effectivelyused in our CRFs based approach.Transition Feature:Transition feature can capture the informationbetween adjacent categories.
It offers anothersemantic feature for LDCRFs-based approach.No transitionfeaturesWith transi-tion featuresSyn 79.20% 80.06%Sem 79.49% 80.06%Total 81.48% 82.05%Table 7.
Transition Feature PerformanceThe performances of all these three experi-ment decline without the transition features.
Itshows that the dependency between adjacent se-486mantic categories contributes to the classifierperformance.6.3 System Performance Comparison andDiscussionIn this section, the what-type questions and non-what-type questions are combined to show thefinal result.
Non-what-type questions are classi-fied using SVM with unigrams as reported inSection 1, and what-type questions are classifiedby the LDCRFs based approach.
The combinedresults are used to compare with the currentquestion classification methods.Classifier AccuracyLi?s Hierarchical method 84.20%Nguyen?s tree method 83.60%Metzler?s U+ WordNet method 82.20%LDCRFs-based with U+Parser 83.60%LDCRFs-based with U+NHype 83.00%LDCRFs-based with total features 85.60%Table 8.
Comparison with related workTable 8 shows the accuracies of the LDCRFsbased question classification approach with dif-ferent feature sets, in comparison with the treemethod (Nguyen et al 2007), the WordNet Me-thod (Metzler and Croft, 2005) and the hierarchicalmethod (Li and Roth, 2002).
We can see theLDCRFs-based approach is effective:(a) Without formulating the syntactic structure asa tree, the LDCRFs-based approach still achievesaccuracy 83.60% with unigram and parser infor-mation, which is the same as Nguyen?s tree clas-sifier.
(b) Although the LDCRFs-based approach withunigrams and Noun Hypernyms generatesnoise as described in Section 6.2, it still out-performs the Metzler?s method using WordNetand unigram features (83.00% v.s.
82.20%).
(c) The experiment with total features achievesthe accuracy of 85.60%.
It outperforms Li?s Hie-rarchical classifier, even they use semi-automaticconstructed features.6.3.1 Analysis and DiscussionEven the sequence tagging model achieves highaccuracy performance, there still exists manyproblems.
We use the matrix defined in Li andRoth (2002) to show the performance errors.
Themetric is defined as follows:*2 /( )ij i j i jD Err N N= +Where i jErr  is the number of questions in classi that are misclassified as belong to class j, Niand Nj are the numbers of questions in class i andj respectively.From the matrix in Figure 3, we can see twomajor mistake pairs are ?ENTY:substance?
and?ENTY:other?, ?ENTY:currency?
and?NUM:money?.
They really have similar mean-ings, which confuses even human beings.Figure 3.
The gray-scale map of Matrix D[n,n].
Thegray scale of the small box in position [i,j] denotesD[i,j].
The larger Dij is, the darker the color is.Several factors influence the performance:(a) Head noun extraction error: This error ismainly caused by errors of POS tagger and shal-low parser.
For the wrong POS example?what/WP hemisphere/EX is/VBZ the/DT Phil-ippines/NNPS in/IN ?/.
?, ?Philippines?
is ex-tracted as head word.
The result is misclassifiedinto ?LOC:country?.
For the shallow parser errorexample ?what/WP/B-NP is/VBZ/B-VP the/DT/BNP speed/NN/I-NP humminbirds/NNS /I-NPfly/V- BP/B-VP ?/./O?, ?hummingbirds?
is ex-tract as head word, rather than ?speed?.
Thequestion is misclassified into ?ENTY:animal?.
(b) WordNet sense disambiguation errors: Inquestion ?What is the highest dam in the U.S.
?
?The real sense for dam is dam#1: a barrier con-structed to contain the flow of water or to keepout the sea; while the disambiguation methoddetermine the second sense as dam#2: a metricunit of length equal to ten meters.
(c) Lack of head nouns: the CRFs based ap-proach is sensitive to the Head Noun.
If the ques-tion doesn?t contain the head noun, it is difficultto produce the correct result, such as the question?What is done with worn or outdated flags??
Inthe future work, we will focus on the head nounabsence problem.4877 ConclusionIn this paper, we propose a novel approach withConditional Random Fields to classify what-typequestions.
We first use the CRFs model to labelall the words in a question, and then choose thelabel of head noun as the question category.
Asfar as we know, this is the first trial to formulatequestion classification into word sequence tag-ging problem.
We believe that the model has twodistinguished advantages:1.
Extracting head noun can eliminate the noisegenerated by the non-head words2.
The Conditional Random Fields model canintegrate rich features, including not only thesyntactic and semantic features, but also thetransition features between labels.Experiments show that the LDCRFs-based ap-proach can achieve comparable performance tothose of the state-of-the-art question answeringsystems.
With the addition of more features, theperformance of the LDCRFs based approach canbe expected to be further improved.AcknowledgementThis work is supported by National NaturalScience Foundation of China (60572084,60621062), Hi-tech Research and DevelopmentProgram of China (2006AA02Z321), NationalBasic Research Program of China(2007CB311003).
Thank Shuang Lin and JiaoLi for revising this paper.
Thanks for the review-ers?
comments.ReferencesChristiane Fellbaum.
1998.
WordNet: an ElectronicLexical Database.
MIT Press.Prager, J., D. Radev, E. Brown, A. Coden, and V.Samn.
1999.
`The use of predictive annotation forquestion answering in TREC'.
In: Proceedings ofthe 8th Text Retrieval Conference (TREC-8).John Lafferty, Andrew McCallum, Fernando Pereira.2001.
Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling Sequence Da-ta.
In Proceedings of ICML-2001.Li, X. and D. Roth.
2002.
Learning question classifi-ers.
In Proceedings of the 19th International Confe-rence on Compuatational Linguistics (COLING),pages 556?562.Zhang, D. and W. Lee.
2003.
Question classificationusing support vector machines.
In Proceedings ofthe 26th Annual International ACM SIGIR confe-rence, pages 26?32Donald Metzler and W. Bruce Croft.
2004.
Analysisof Statistical Question Classfication for Fact-basedQuestions.
In Journal of Information Retrieval.Ted Pedersen, Satanjeev Banerjee, and SiddharthPatwardhan .
2005.
Maximizing Semantic Related-ness to Perform Word Sense Disambiguation.
Uni-versity of Minnesota Supercomputing InstituteResearch Report UMSI 2005/25, March.Xin Li, Dan Roth.
2006.
Learning Question Classifi-ers: The Role of Semantic Information.
In NaturalLanguage Engineering, 12(3):229-249Minh Le Nguyen, Thanh Tri Nguyen and Akira Shi-mazu.
2007.
Subtree Mining for Question Classifi-cation Problem.
In Proceedings of the 20th Interna-tional Conference on Artificial Intelligence.
Pages1695-1700.C.
Sutton and A. McCallum.
2007.
An introduction toconditional random fields for relational learning.In L. Getoor and B. Taskar (Eds.).
Introduction tostatistical relational learning.
MIT Press.Y.
Tsuruoka and J. Tsujii,.
2005.
Bidirectional infe-rence with the easiest-first strategy for tagging se-quence data.
In Proc.
HLT/EMNLP?05, Vancouver,October, pp.
467-474.L.
Ramshaw and M. Marcus.
1995.
Text chunkingusing transformation-based learning, Proc.
3rdWorkshop on Very Large Corpora, pp.
82?94.J.R.
Finkel, T. Grenager and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by Gibbs sampling.
Proc.
43rdAnnual Meeting of ACL, pp.
363?370.D.
Lin.
1999.
MINIPAR: a minimalist parser.
In Mar-yland Linguistics Colloquium, University of Mary-land, College Park.488
