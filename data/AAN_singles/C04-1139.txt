Linguistic profiling of texts for the purpose of language verificationHans VAN HALTERENDept.
of Language and Speech, Univ.ofNijmegen, The NetherlandsP.O.
Box 9103, 6500 HD Nijmegenhvh@let.kun.nlNelleke OOSTDIJKDept.
of Language and Speech, Univ.
ofNijmegen, The NetherlandsP.O.
Box 9103, 6500 HD Nijmegenn.oostdijk@let.kun.nlAbstractIn order to control the quality of internet-basedlanguage corpora, we developed a method toverify automatically that texts are of (near-)native quality.
For the LOCNESS and ICLEcorpora, the method is rather successful inseparating native and non-native learner texts.The Equal Error Rate is about 10%.
However,for other domains, such as internet texts,separate classifiers have to be trained on thebasis of suitable seed corpora.1 IntroductionResearch in linguistics and language engineeringthrives on the availability of data.
Traditionally,corpora would be compiled with a specific purposein mind.
Such corpora characteristically were well-balanced collections of data.
In the form ofmetadata, record was kept of the design criteria,sampling procedures, etc.
Thus the researcherwould have a fair idea of where his data originatedfrom.
Over past decades, data collection has beenboosted by technological developments.
More andmore and increasingly large collections of datahave been and are being compiled.
It is tempting tothink that the problem of data sparseness has beensolved ?
at least for raw data or data without anyannotation other than can be provided fullyautomatically ?
especially now that large amountsof data can be accessed through the internet.However, with data coming to us from all over theworld, originating from all sorts of sources, wenow possibly have a new problem on our hands:often the origins of the found data remain obscure.It is not always clear what exactly theimplications for our research are of employing datawhose origin we do not know.
Is it legal to usethese data, ethical, appropriate, ??
In this paperwe will focus on the last point: the appropriatenessof the data in the light of a specific application orresearch goal.
More in particular, we willinvestigate to what extent we can devise aprocedure that will enable us to identify textsproduced by native speakers of the language (andthus by default those produced by non-nativespeakers).
The present study is motivated by thefact that for many uses the (near-)nativeness of thedata is a critical factor in the development ofadequate resources and applications.
Thus, forexample, a style checker or some other writingassistant tool which has been based on erroneousmaterials or at least materials deviant from thelanguage targeted, will not always respondappropriately.1.1 Assessing (near-)nativenessIn the general absence of metadata which attestthat texts have been produced by native speakers,there is one obvious approach that one mayconsider in order to assess the (near-)nativeness oftexts of unknown origin and that is to exploit theirspecific linguistic characteristics.Previous studies investigating language variation(eg Biber, 1995, 1998; Biber et al, 1998; Conradand Biber, 2001; Granger, 1998, Granger et al,2002) have shown that language use in differentgenres and by different (groups of) speakersdisplays characteristic use of specific linguisticfeatures (lexical, morphological, syntactic,semantic, discoursal).
These studies are all basedon data of known origin.
In the present study, wetake a somewhat different approach as we aim toprofile texts of unknown origin and identify nativevs non-native language use, a task for which wecoined the term language verification.1.2 Non-native language useTexts produced by non-native speakers willgenerally pass superficial inspection, i.e.
they aredeemed to be texts in the target language and willbe treated as such.
However, on closer inspectionthere is a wide range of features in the languageuse of non-natives which may have a disruptiveeffect on for instance derived language models.
Itis important to realize that non-native use is thecomplex result of different processes andconditions.
First of all, there is the level ofachievement.
A non-native user graduallydevelopes language skills in the target language.As he/she masters certain lexical items or morpho-syntactic structures and feels confident in usingthem, certain items and structures are bound to beoverused.
At the same time, other items andstructures remain underused as the user avoidsthem since he is not familiar with them or does not(yet) feel confident enough to employ them.Moreover, even for speakers who have attained arelatively high degree of proficiency, the influenceof the native language remains.
This may lead totransfer effects and interference (the effects ofwhich are found, for example, in the use of falsefriends and word order deviations).In the present paper, we report the resultsobtained in some experiments that were carried outand which aimed to assess whether texts are of(British English) native or non-native origin usingthe method of linguistic profiling.
The structure ofthe paper is as follows: In section 2, we describethe method of linguistic profiling.
Next, in section3, its application in establishing the nativeness oftexts is described, while in section 4 it isinvestigated whether the approach holds up whenwe shift from one domain to another.
Finally,section 5 presents the conclusions.2 Linguistic profilingIn linguistic profiling, the occurrences in a textare counted of a large number of linguisticfeatures, either individual items or combinations ofitems.
These counts are then normalized for textlength and it is determined to what extent(calculated on the basis of the number of standarddeviations) they differ from the mean observed in aprofile reference corpus.
For each text, thedeviation scores are combined into a profile vector,on which a variety of distance measures can beused to position the text relative to any group ofother texts.2.1 Language verificationLinguistic profiling makes it possible to identify(groups of) texts which are similar, at least similarin terms of the profiled features (cf.
van Halteren,2004).
We have found that the recognition processcan be vastly improved by not only providingpositive examples (in the present case native texts)but also negative examples (here the non-nativetexts).
So we expect that, given a seed corpuscontaining both native and non-native texts,linguistic profiling should be able to distinguishbetween these two types of texts.2.2 FeaturesAs previous research has shown (see e.g Biber1995), there are a great many linguistic featuresthat contribute to marked structural differencesbetween texts.
These features mark ?basicgrammatical, discourse, and communicativefunctions?
(Biber, 1995: 104).
They comprisefeatures referring to vocabulary, lexical patterning,syntax, semantics, pragmatics, information contentor item distribution through a text.
Here we restrictourselves to lexical features.Sufficiently frequent tokens, i.e.
those that wereobserved to occur with a certain frequency in somelanguage reference corpus, are used as features bythemselves.
In the present case these are ?tems thatoccur at least five times in the written texts fromthe BNC Sampler (BNC, 2002).
For less frequenttokens, we determine a token pattern consisting ofthe sequence of character types.
For example, thetoken Uefa-cup is represented by the pattern?#L#6+/CL-L?, where the first ?L?
indicates lowfrequency, 6+ the size bracket, and the sequence?CL-L?
a capital letter followed by one or morelower case letters followed by a hyphen and againone or more lower case letters.
For lower casewords, the final three letters of the word are alsoincluded in the pattern.
For example, the tokenaltercation is represented by the pattern?#L#6+/L/ion?.
These patterns were originallydesigned for English and Dutch and will probablyhave to be extended for use with other languages.Furthermore, for this specific task, we wanted toavoid recognizing text topics rather thannativeness, and decided to mask content words.Any high frequency word classified primarily asnoun, verb or adjective (see below), which had ahigh document bias (cf.
van Halteren, 2003) wasreplaced by the marker #HC# followed by thesame type of pattern we use for low frequencywords, but always without the final three letters.This occludes topical words like brain or injury,while leaving more functional words like case ortimes intact.In addition to the form of the token, we also usethe syntactic potential of the token as a feature.
Weapply the first few modules of a morphosyntactictagger (in this case the tagger described by vanHalteren, 2000) to the text, which determine whichword class tags could apply to each token.
Forknown words, the tags are taken from a lexicon;for unknown words, they are estimated on the basisof the word patterns described above.
The mostlikely tags (with a maximum of three) arecombined into a single feature.
Thus still isassociated with the feature ?RR-JJ-NN1?
andforms with the feature ?NN2-VVZ?.
Note that themost likely tags are determined exclusively on thebasis of the current token; the context in which thetoken occurs is not taken into account.
Themodules of the tagger which are normally used toobtain a context dependent disambiguation are notapplied.On top of the individual token and tag featureswe use all possible bi- and trigrams.
For example,the token combination an attractive option isassociated with the complex feature ?wcw=#HF#an#HC#JJ#HC#6+/L?.
Since the number offeatures quickly grows too big to allow forefficient processing, we filter the set of features.This done by requiring that a feature occur in a setminimum number of texts in the profile referencecorpus (in the present case a feature must occur inat least two texts).
A feature which is filtered outcontributes to a rest category feature.
Thus, thecomplex feature above would contribute to?wcw=<OTHER>?.The lexical features currently also includefeatures that relate to utterance length.
For eachutterance two such features are determined, viz.
theexact length (e.g.
?len=15?)
and the length bracket(e.g.
?len=10-19?
).2.3 ClassificationWhen offered a list of positive and negative textsfor training, and a list of test texts, the system firstconstructs a featurewise average of the profilevectors of all positive texts.
It then determines araw score for all text samples in the list.
Ratherthan using the normal distance measure, we optedfor a non-symmetric measure which is a weightedcombination of two factors: a) the differencebetween text score and average profile score foreach feature and b) the text score by itself.
Thismakes it possible to assign more importance tofeatures whose count deviates significantly fromthe norm.
The following distance formula is used:?T = (?
|Ti?Ai| D  |Ti| S) 1/(D+S)In this formula, Ti and Ai are the values for the ithfeature for the text sample profile and the positiveaverage profile respectively, and D and S are theweighting factors that can be used to assign moreor less importance to the two factors described.The distance measure is then transformed into ascore by the formulaScoreT = (?
|Ti|(D+S)) 1/(D+S)   ?
?TThe score will grow with the similarity betweentext sample profile and positive average profile.The first component serves as a correction factorfor the length of the text sample profile vector.The order of magnitude of the score valuesvaries with the setting of D and S, and with the textcollection.
In order to bring the values into a rangewhich is suitable for subsequent calculations, weexpress them as the number of standard deviationsthey differ from the mean of the scores of thenegative example texts.3 Language verificationIn order to test the feasibility of languageverification by way of linguistic profiling, we needdata which is guaranteed to be written by nativeand non-native speakers respectively.
Moreover,the texts (native and non-native) should be assimilar as possible with respect to the genre theyrepresent.
For the present study, therefore, weopted for the student essays in the Louvain Corpusof Native English Essays (LOCNESS) and theInternational Corpus of Learner English (ICLE;Granger et al, 2002).3.1 LOCNESS and ICLEICLE is a collection of mostly argumentativeessays written by advanced EFL students fromvarious mother-tongue backgrounds.
The essayseach are some 500-1000 words long (unabridged)and although they ?cover a variety of topics, thecontent is similar in so far as the topics are all non-technical and argumentative (rather than narrative,for instance)?
(cf.
Granger, 1998:10).
The size ofthe national sub-corpora is approx.
200,000 wordsper corpus.
With the data metadata are available asthey have been collected via a learner profilequestionnaire.The LOCNESS in various respects iscomparable to ICLE.
It is a 300,000-word corpusmainly of essays written by English and Americanuniversity students.
A small part of the corpus(60,000 odd words) is constituted by BritishEnglish A-level essays.
Topics include transport,the parliamentary system, fox hunting, boxing, theNational Lottery, and genetic engineering.3.2 Training and test textsIn order to be able to control for languagevariation between British and American English,we opted for only the British part of LOCNESS.Because this totalled only some 155,000 words, wedecided to hold out about one third as test materialand use the other two thirds for training.
In order tohave as little overlap as possible in essay type andtopic between training and test material, we usedsub-corpora 2, 3 and 8 of the A-level essays andsub-corpus 3 of the university student essays fortesting.For the ICLE texts, we chose to use each tenthtext for training purposes.
The remaining textswere used for testing.3.3 General resultsIn the first step of training, we selected thefeatures to be profiled.
We used all features whichoccurred in more than one training text, i.e.
about470K features.
In the second step, we selected thesystem parameters D and S for two classificationmodels: similarity to the native texts (D=1.0,S=0.0) and similarity to the non-native texts(D=1.2, S=0.2).
The selection was based on thequality of classifying half of the training texts withthe system having been trained on the other half.The verification results for the test set of A-leveltexts are shown in Figure 1.
The further the textsare plotted to the right, the more similar theirprofile is to the mean profile for the A-leveltraining texts.
The further the texts are plottedtowards the top, the more similar their profile is tothe mean profile for the ICLE training texts.Most of the texts form a central cluster in thebottom right quadrant.
A small gap separates themfrom a group of five near outliers, while there aretwo far outliers.
We decided to use the limits of thecentral cluster as our classification separator,accepting that 10% of the LOCNESS texts wouldbe rejected.
We added the separation line to theplot.
In order to create a reference frame linkingthis figure to the following ones, we add a secondline, along the core of the cluster of the LOCNESStexts.
Even though the core of the clusters in thesuccessive figures may shift, this line remainsconstant, as does the plotting area.Figure 1.
Text classification of the LOCNESStest texts in terms of similarity to native texts(horizontal axis) and similarity to non-nativetext (vertical axis).
The separation line (topright to bottom left) divides the plot area in anative part (bottom right) and a non-nativepart (top left).
The second line (top left tobottom right) is a reference line whichallows comparison between this Figure andFigures 2-4.Figure 2.
Text classification of the ICLE testtextsFigure 2 shows the results for the ICLE testtexts.
89% of the texts are rejected.
Theverification results differ per nationality.
A moredetailed examination of such variation, however, isbeyond the scope of the present paper.The two dimensions, the degree of similarity tonative texts and the degree of similarity to non-native texts, are strongly (negatively) correlated.Still, there are also clear differences, so that bothdimensions contribute substantially to the qualityof the separation.3.4 Distinguishing featuresWhen examining some of the features thatemerge from studies reported in the literature assalient in describing different language varieties,we find that none of these dominates theclassification.
Table 1 shows the influence of eachfeature in terms of its contribution (expressed asmillionths of the total influence, so e.g.
3173corresponds to 0.3% of the total influence) to thedecision to classify a text as native or non-native.The second and third column show the influence ofthe words (or word combinations) by themselves,which is extremely low.
However, whenexamining all patterns containing these words, thefourth and fifth columns, their usefulness becomesvisible.Previous studies into the use of intensifyingadverbs have shown an overuse of the token very.Thus it is a likely candidate to be considered as amarker of non-native language use.
The secondcolumn in the Table confirms this, but thecontribution is a mere 0.001%.
The picturechanges when we consider all patterns in whichvery occurs, it appears that there is indeed adifference in use of the token by natives and non-natives.
However, there are as many patterns thatpoint to nativeness as there are that point to non-nativeness.
Furthermore, the patterns provide asizeable contribution in the classification eitherway.Word(s) Sep?ICLESep?LOCPatterns?ICLEPatterns?LOCNESSifIfif1343931 4529because 4 - 3230 2925very 10 - 2860 3173however - 1 686 644therefore - 10 953 734for instance 4 - 30 32thus 2 - 411 287yet 4 - 606 349Table 1.
Relative contribution to the overallclassification of allegedly salient featuresAlthough the expected features (or ratherfeatures related to expected word or wordcombinations) have a visible contribution, theirinfluence is still only a small part of the totalinfluence.
In fact, all features have only very littleinfluence.
The most influential single feature isccc=#HF#AT--#HF#NN1--#HF#CC?RRx13, oneof the representations of the, followed by a singlecommon noun, followed by and, a pattern unlikelyto be spotted by humans.
It contributes 0.06% ofthe influence classifying texts as non-native.
Only137 features in total contribute more than 0.01%either way.
Classification by linguistic profiling isa matter of myriads of small hints rather than a fewpieces of strong evidence.
This is probably alsowhat makes it robust against high text variabilityand sometimes small text sizes.4 Domain ShiftsNow that we have seen that languageverification is viable within the restricted domainof student essays, we may examine whether itsurvives the shift to a new domain.
We tested thison two corpora: the FLOB corpus and (small)internet corpus that was especially collected forthis purpose.4.1 FLOBThe Freiburg LOB Corpus, informally known asFLOB (Hundt et al, 1998) is a modern counterpartto the much used Lancaster-Oslo/Bergen Corpus(LOB; Johansson, 1978) It is a one-million wordcorpus of written (educated) Modern BritishEnglish.
The composition of FLOB is essentiallythe same as that of LOB: it comprises 500 samplesof 2,000 words each.
In all, 15 text categories (A-R) are distinguished.
These fall into four mainclasses: newspaper text (A-C), miscellaneousinformative prose (D-H), learned and scientificEnglish (J), and fiction (K-R).Figure 3.
Text classification of the FLOBlearned and scientific texts (category J)Figure 4.
Text classification of the FLOB non-fiction texts (categories A-J)Of these texts, the learned and scientific class (J)is closest to the ICLE and LOCNESS texts, and weshould expect that the FLOB texts of this categoryare all accepted.
This is indeed the case, as can beseen in Figure 3, which shows the classification ofthese texts.
Only 1 text is rejected (1.25%).
Thisseems to confirm that we are indeed recognizingsomething like ?
(near-)native English?.As soon as we shift the domain of the texts,however, the native texts are no longerdistinguished as clearly.
The larger the domainshift, the more texts are rejected.
Within the non-fiction portion of FLOB, the system rejects 2.3%of the newspaper texts (categories A-C) and 8.7%of the miscellaneous and informative prose texts(D-H).
This leads to an overall reject rate of 5.6%for the non-fiction texts (Figure 4), which is stillreasonably acceptable.
When shifting to fictiontexts (K-R), the reject rate jumps to 39.2% (Figure5), indicating that a new classifier would have tobe trained for a proper handling of fiction texts.Figure 5.
Text classification of the FLOBfiction texts (categories K-R)4.2 Capital-BornSince our original goal was the filtering ofinternet texts, we compiled a small corpus of suchtexts.
We chose texts which were present asHTML.
These, we expected, were likely to berather abundant, while they would have beensubjected to a relatively low degree of editing.Thus they would constitute likely candidates forfiltering.
In order to be able to decide whether thetexts were native-written or not, we searchedautobiographical material, as indicated by thephrase I was born in CITY, with CITY replaced bya name of a capital city.
The initial set ofdocuments appeared to be of a reasonable size.However, after filtering out webpages by multipleauthors (e.g.
guest books), fictionalautobiographies (e.g.
a joke page about Al Gore),texts judged likely to be edited possibly with thehelp of a native speaker (e.g.
a page advertisingRussian brides), misclassified city names (e.g.authors from Paris, Texas should not be assumedto be French) and texts outside the desired lengthof 500-1500 words, we ended up with a mere 20native British English texts and 18 non-nativetexts.
We nicknamed the corpus ?Capital-Borncorpus?.When classifying these texts with the A-levelversus ICLE classifier, we see that they clustertightly, outside the area plotted so far, and showingno useful separation of native and non-native texts.This implies that if we want a filter for such texts,we have to train a new classifier.Figure 6.
Text classification of internet texts(for a description see section 4.2)We did train such a new classifier, using only theodd-numbered Capital-Born texts and classifiedthe even-numbered ones, using the sameparameters D and S as above.
We repeated theprocess with the two sets switching roles.
Figure 6shows a superposition of the classifications in thetwo experiments.
The native texts appear as plussigns (+), the non-native texts as minus signs (?
).Note that we adjusted the separation and supportlines in order to bring them in line with the data.Only a rough separation is visible, with 2 out of 20native texts misclassified and 6 out of 18 non-native texts.
Still, given the extremely small size ofthe training sets and the variety of non-nativenationalities, these results are rather promising.
Itappears that even internet texts can be filtered fornativeness, as long as a restricted, and moresizeable, seed corpus can be constructed.5 ConclusionThe results show that language verification isindeed possible, as long as we accept that near-native texts produced by non-natives will not befiltered out.Furthermore, whenever a verification filter isneeded, it will be necessary to create a new filter,based on a seed corpus which contains both nativeand non-native texts as similar as possible in typeto the texts which are to be filtered.There are now two avenues open for futureresearch.
First of all, we would like to explore theclassification procedure linguistically: a) examinethe distinguishing features in more detail andcompare our findings with those in the literature,and b) examine the correlation of the nativenessscore of the various texts to extra-linguistic textvariables such as mother tongue and learner level.Secondly, once more insight is gained into thelinguistic workings of the procedure, theclassification process can be refined.
At this point,we would also like to examine the effects ofdomain shift in more detail, and attempt toestimate a minimum size for seed corpora for usein filtering internet material.6 AcknowledgementsThanks are due to Sylviane Granger and SylvieDe Cock (Centre for English Corpus Linguistics,Universit?
Catholique de Louvain, Belgium) formaking the LOCNESS and ICLE data available tous.ReferencesDouglas Biber.
1995.
Dimensions of registervariation.
A cross-linguistic comparison.Cambridge: Cambridge University Press.Douglas Biber 1998.
Variation across Speech andWriting.
Cambridge: Cambridge UniversityPress.Douglas Biber, Susan Conrad and Randi Reppen.1998.
Corpus Linguistics: Investigatinglanguage structure and use.
Cambridge:Cambridge University Press.BNC.
2002.
The BNC sampler.
Web page:www.natcorp.ox.ac.uk/getting/sampler.htmlSusan Conrad and Douglas Biber (eds.)
2001.Variation in English: Multi-dimensionalstudies.
Harlow, England: Longman.Alan Davies.
2003.
The Native Speaker: Myth andReality.
Clevedon: Multimingual Matters Ltd.Sylviane Granger (ed.)
1998.
Learner English onComputer.
London and New York: Longman.Sylviane Granger.
1998.
The computer learnercorpus.
In Sylviane Granger (ed.
): 3-18.Sylviane Granger, Joseph Hung, and StephaniePetch-Tyson (eds.)
2002.
Computer LearnerCorpora, Second Language Acquisition andForeign Language Teaching.
Amsterdam:Benjamins.Sylviane Granger, E. Dagneaux, and FannyMeunier (eds.)
2002. International Corpus ofLearner English.
Louvain: UCL PressesUniversitaires de Louvain.Hans van Halteren.
2000.
The detection ofinconsistencies in manually tagged text.
Proc.Workshop on Linguistically InterpretedCorpora (LINC2000).
48-55.Hans van Halteren.
2003.
New feature sets forsummarization by sentence extraction.
IEEEIntelligent Systems, July/August 2003: 34-42.Hans van Halteren.
2004.
Linguistic profiling forauthor recognition and verification.
Proc.
ACL2004.Marianne Hundt, Andrea Sandt and RainerSiemund.
1998.
Flobman.
Manual ofInformation to accompany the Freiburg-LOBCorpus of British English (?FLOB?).
Freiburg:Englisches Seminar.Stig Johansson with Geoffrey Leech and HelenGoodluck.
1978.
Manual of Information toAccompany the Lancaster-Oslo/Bergen Corpusof British English, for Use with DigitalComputers.
Oslo: Dept.
of English, University ofOslo.M.
van der Laaken, R. Lankamp, and MichaelSharwood Smith.
1997.
Writing Better English.Bussum: Coutinho.LOCNESS.http://juppiter.fltr.ucl.ac.be/FLTR/GERM/ETAN/CECL/Cecl-Projects/Icle/LOCNESS.htm
