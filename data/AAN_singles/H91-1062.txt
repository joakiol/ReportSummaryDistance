A Proposal for Incremental DialogueMadeleine Bates and Damaris AyusoBBN Systems and  Techno log ies10 Mou l ton  Street.Cambr idge ,  MA 02138EvaluationABSTRACTThe SIS community has made progress recently toward evaluating SLSsystems that deal with dialogue, but there is still considerable work thatneeds to be done in this area.
Our goal is to develop incremental ways toevaluate dialogue processing, not just going from Class D1 (dialoguepairs) to Class D2 (dialogue triples), but measuring aspects of dialogueprocessing other than length.
We present two suggestions; one forextending the common evaluation procedures for dialogues, and one formodifying the scoring metric.INTRODUCTIONThere is no single dialogue problem.
By its nature, dialogueprocessing is composed of many different capabilities matched tomany different aspects of the problem.
It is reasonable to expectthat dialogue valuation methodologies should be multffaceted toreflect his richness of structure.Ideally, each new addition to the set of evaluationmethodologies should test a different aspect of dialogue processing,and should be harder than the methodologies that came before iLWe present wo suggestions: one which extends the commonevaluation procedure in order to test one new aspect of dialogues,and one which modifies the scoring metric.Difference:1.
Conversation is cooperative, but a game is competitive.2.
In chess, the goal is clear (checkmate), but in a conversationaldialogue, the goal is less dear.3.
In a chess game, any state can be completely and conciselyrepresented by a single board position; in a dialogue it is notknown what comprises a state, nor how to represent it.Like the game tree for chess, the human/computer dialogue treeis enormous, as indicated in figure 1.
There are usually hundredsor thousands of alternatives the human may produce.
The numberof responses the system can make is much smaller; someresponses may be clearly wrong, but seldom is there a single"right" or "best" response (just as there is seldom a single suchmove in chess).
Even when striving for the same goal, twodifferent people are very likely to choose very different paths.An Analogy with ChessWe as a community have been thinking about dialogueevaluation i  terms of whether the systems we are building givethe "right" answer (the one the wizard gave, or the one agreed uponby the Principles of Interpretation) at every step.
We have beentrying to come up with a methodology to measure whether oursystems can reproduce the wizard's answers at each step of alengthy dialogue.
But is this a reasonable approach?Participating in a dialogue, whether between two humans orbetween a human and a machine, bears a striking resemblance toplaying a complex game such as chess.Similarities:1.
Each involves precise turn-taking.2.
There is an extremely large tree of possible "next moves" (thetree for human dialogue, even in a limited domain, is muchlarger than that for chess).3.
Multiple paths through the tree can lead to the same results.
"4Figure I: A Human/Computer Dialogue Tree319As a community, we have been thinking about dialogueevaluation i terms of whether the system gives the "fight" answerat every step (the one the wizard gave at the same point in thesame dialogue).
The major problem with this type of thinking isthat it encourages u  to characterize a move that does not mimicthe expert's (or an answer that does not exactly match the wizard's)as wrong, when it may not be wrong at all, but just different.WHY EVALUATING DIALOGUE SYSTEMSBY SEEING WHETHER THEY REPRODUCEWIZARD SCENARIOS IS A BAD IDEAWe have been trying to think about dialogue valuation interms of measuring whether our systems can reproduce virtuallythe same answers that the wizard produced for an entire dialoguesequence.
This is like asking one chess expert to exactly reproduceevery move that some other expert made in a past game!There are several reasons why this cannot work in the human-computer environment either.
We will briefly discuss three ofthese: ambiguous queries, failure to answer by the system, andwrong interpretations by the system.Ambiguous QueriesAmbiguous queries abound in the training sentences, and willcertainly appear in test sets as well.
As soon as a system can givea valid answer that is different from the wizard's, there is thepotential for a significant divergence in the paths taken by thewizard's ession and the system's execution of the dialogue.This means that the query following the ambiguous query inthe test set (i.e., from the wizard's ession) may not make sense asa follow-up query when the system processes it!
Consider thefollowing examples:QI: Show me flights from San Francisco to Atlanta.Q2: Show me flights that arrive after noon.Q3: Show me flights that arrive before 5pm.A3' (List of flights between oon and 5pm)A3" (List of flights before 5pm, including morningarrivals)Q3: Show me the fare of AA123.The answer A3" is a superset of the answer A3'.
Therefore ifthe wizard answers as in A3", but a system being evaluatedproduces A3', and the flight referred to in Q3 is in A3" but notA3', then the answer produced by the system is likely to beofficially "wrong", although it is perfectly correct, given the actualcontext available to the system at the time Q4 was processed.Of course, some ambiguities in the class A sentences in thisdomain may in practice not affect he course of the dialogue thisway.
However, in extended dialogues with context-dependencies,substantial mbiguities can quickly develop.No AnswerAnother source of dialogue path divergence occurs when thesystem is unable to answer a query which the wizard answered.For example:QI: Show me flights from San Francisco to Atlanta.Q2: Which is the cheapest one if I want o travel Monday.Q3: What is the fare on that flight?Q4: Show me its restrictions.If the system were unable to understand Q2, producing noanswer, then Q3 and Q4 would not be understandable at all.But what would happen in a real human-computer dialogue(instead of the highly artificial task of trying to copy a pre-specified ialogue)?
If the system clearly indicated that it didn'tunderstand Q2 at all, a real live user with even moderateintelligence would never ask Q3 at all, because s/he would realizethat the system did not have the proper context o answer it.Instead, s/he might continue the dialogue quite differently, forexample:Q3: Give me the restrictions and fare of the cheapestMonday flight.Notice that by following this alternate path, the user mayactually be able to get the data s/be ultimately wants sooner thanthe wizard scenario provided it (in 3 queries instead of 4, eventhough one of those 3 was not understood).
How can one say thatsuch a system is less good than one that mimics the wizardperfectly in this case?Wrong InterpretationAs in the previous two cases, an incorrect interpretation bythesystem causes adivergence in the dialogue tree.
In addition, as inthe No Answer case, in practice awrong interpretation is likely toresult in a glaring error that elicits follow-up clarification queriesfrom the user.
Even if the user does not follow up in exactly thatway, s/he will take into aocount that the system did somethingunexpected, and will use that knowledge in the formulation ofsubsequent queries.
Just as in the No Answer case, the dialoguebranch followed after a wrong interpretation could easily befundamentally different than the branch which the wizard wouldfollow, but may lead to the same point (all the information thet~r  wanted).What does it mean?Our goal should not be to produce systems that behave xactlylike the wizard, but rather systems that respond reasonably toeveryinput and that allow the user to reach his or her goal.Perhaps ff we give up the notion of "the right answer" in favorof "a reasonable answer" then we can develop more effective andmeaningful evaluation methodologies.
The following twoproposals provide some concrete suggestions as to how to goabout his process.
The first deals with the aspect of breadth in thedialogue tree, the second deals with reasonable responses topartially understood input.320PROPOSAL: DIALOGUE BREADTH TESTOne of the central problems of dialogue valuation is that hereis no single path of questions and answers to which a system mustconform,but rather a plethora of possible questions at each point inthe dialogue.
A very important capability of a good system is tobe able to handle many different queries at each point in a dialogue.We suggest here a methodology that deals with one importantaspect of dialogue; specifically, it attempts to compare systems ontheir ability to handle diverse utterances incontext.
This methodbuilds on our existing methodology without imposing ur~ealisticconstraints on the system design.
It aL~ meets the requirement forobjective valuation that it must be possible to agree on whatconstitutes a reference answer.Consider a dialogue that begins with two queries.
At thispoint, if you ask ten different people what question they would asknext, you will likely get ten different queries, as illustrated infigure 2.Context:Qh I want to go from Boston to Dallas on Saturday.Ah (A list of flights and associated information)Q2: Which of those flights are nonstops?A2: (A smaller list of flights and other information)Some possible next queries:Q3a: How much do they cost?Q3b: Which one has the lowest fare, no restrictions?Q3c: What's the cost of a coach ticket on AA1237Q3d: Which flight is cheapest?Q3e: Now show me flights Dallas to Boston.Q3f: What is the cost of taxis in Dallas?Q3g: Is flight AA123 a DC-10?Q3h: Just the ones that leave before noon, please.Figure 2: An Example of Dialogue BreadthIt is reasonable to ask how many of these natural, possibledialogue continuation utterances a particular NL system canunderstand, and it is also reasonable tocompare one system withanother based on which one can handle more of the possiblecontinuation utterances.In this type of evaluation, the object is to control the initialpart of a dialogue to that which a number of different systems canhandle, and then to see how many possible alternative utterances(from a given set) each system could handle at that point in thedialogue.
Of course, the continuation utterances should be asmeaningful and natural as possible and should not necessarily becontext dependent, as in Q3e and Q3f.Dialogue Breadth Test MethodologyWe present here an example of how an evaluation toassess asystem's capabilities in handling dialogue breadth would takeplace.
We call this test the Dialogue Breadth (DB) test.
Thenumbers and other details are for illustrative purposes only.Each site would be given a set of 15 dialogue starters (initially,let's assume that is 15 Q1 utterances).
With each Q1, there wouldbe a set of 10 alternative Q2 utterances; this would form a test setof 150 Q1-Q2 dialogue pairs.
Sites would run the completedialogues through their'systems, and would return 100 test itemsand answers for scoring.
(The reduction from 150 would enablesites to remove many cases in which Q1 was not processedcorrectly, thus focussing the test on the issue of dialogue analysis,not Class A processing.)
These 100 answers would beautomatically compared to reference answers and scores computedin the usual way: as the number (and percentage) of thecontinuation uaerances that were answered correctly, incorrectly, ornot answered at all.How would the test sets of "next utterances" for the test set beobtained?
It could easily be done just as described above -- byshowing at least 10 people the original problem scenario that theoriginal wizard subject was trying to solve, showing them theinitial dialogue context, and then asking them to add a singleutl~rance.The first time the evaluation is tried, it might make sense touse a single Class A utterance as the context, as desoribed above,and consider the breadth that is possible at just the second step of adialogue.
Later evaluations could be carried out on slightly longerdialogue fragments, where each context is a pair of Q1 and Q2utterances, followed by a set of 10 alternate Q3 utterances.Advantages of the Dialogue Breadth TestThere are a number of advantages to be gained by adding thedialogue breadth test to the SLS community's growing set ofevaluation tools.1.
This methodology builds on the Class D methodology whichthe community has developed thus far.2.
It examines an extremely important aspect of dialoguesystems: the ability to handle avariety of utterances atapoint in mid-dialogue.3.
As long as short dialogue contexts are used, it does not dependon each site building systems that try to duplicate the outputof the wizard, since intermediate answers to the initialdialogue utterances are not scored.4.
It requires no more training data than our current dialogueevaluation mechanism.
Although it would be useful for sitesto have a small amount of dialogue breadth training data,most system development can proceed using the data that hasalready been collected, or more data of that ype.5.
It requires no changes to the classification scheme, or to otherinformation associated with the training data, such as thereference answers.321SUGGESTION: MODIFY METRICS TOENCOURAGE PARTIAL UNDERSTANDINGIn human-human dialogue (let us call the participants thequestioner and the information agent), it is often the case that oneparty only partially understands the other, and realizes that this isthe case.
There are several things that the information agent cando at this point:1. ask for clarification2.
provide art answer based on partial understanding3.
provide an answer based on partial understanding, whileindicating that it may not be precisely what was desired4.
decline to answer, and make the other party ask again.Clearly, 2 and 4 are the least preferred responses, ince theymay mislead or frustrate the questioner, espectively.
But both 1and 3 are often reasonable r sponses.For example, ff an information agent hears,"What are the flights from BOS to DFW that ..... lunch"(where some language in the middle was not heard or understoodfor some reason), it is reasonable torespond with either a requestfor clarification, such as,"Do you want BOS to DFW flights that serve lunch?
"or with a qualified answer, such as,"I didn't entirely understand you, but I think you were asking forBOS to DFW flights with lunch, so here they are:TWA 112 ...".Either response is acceptable, ven to a questioner who asked forflights "that do not serve lunch", or flights "that serve breakfast orlunch" because the system made clear that its understanding wasunceaain.The idea of allowing our SLS systems to respond, in effect, withan answer qualified by "I'm not sure I caught everything you said,but here's my best guess" is a powerful one that is clearly orientedtoward making systems useful in application.A Suggestion for ChangeThe need for permitting systems ome leeway in responding topartially understood input is clear, but the mechanism for doing sois less clear, and would require some thought by all of thoseinvolved in developing the SLS evaluation methodology.For example, a new class of system response could be allowed,called "Qualified Answer", and that two new categories calledQualified Answer Reasonable, Qualified Answer Not Reasonablebe added to the current set of Right, Wrong, and No Answer.Judging a qualified answer as reasonable or unreasonable wouldalmost certainly have to be done by a human judge or judges, sinceit is unlikely to be possible to anticipate all the possiblereasonable answers to a query.
It would also be necessary todevelop an explicit scoring metric for the new categories whichwould not penalize qualified answers too harshly.The evaluation committee and the SLS steering committeeshould consider these suggestions for possible inclusion in futurecommon SLS evaluations.ACKNOWLEDGEMENTSThe work reported here was supported by the AdvancedResearch Projects Agency and was monitored by the Office ofNaval Research under Contract No.
N00014-89-C-0008.
Theviews and conclusions contained in this document are those of theauthors and should not be interpreted as necessarily representingthe official policies, either expressed or implied, of the DefenseAdvanced Research projects Agency or the United StatesGovernment.REFERENCES1.
Bates, M., Boisen, S., and Makhoul, J.
"Developing andEvaluation Methodology for Spoken Language Systems",Proceedings ofthe Speech and Natural Language Workshop (June,1990), Morgan Kaufmarm Publishers, Inc., 1990.2.
Hemphill, C.T., Godfrey, J.J., and Doddington, G.R.
"TheATIS Spoken Language Systems Pilot Corpus", Proceedings ofthe Speech and Natural Language Workshop (June, 1990), MorganKaufmann Publishers, Inc., 1990.3.
Hirschman, L., Dalai, et al"Beyond Class A: A Proposal forAutomatic Evaluation of Discourse", Proceedings ofthe Speechand Natural Language Workshop (June, 1990), Morgan KaufmarmPublishers, Inc., 1990.4.
Pallett, D.S., Fisher, W.M., Fiscus, J.G., and Garofolo, J.S.
"DARPA ATIS Test Results", Proceedings ofthe Speech andNatural Language Workshop (June, 1990), Morgan KaufmarmPublishers, Inc., 1990.5.
Price, P. "Evaluation of Spoken Language Systems: the ATISDomain", Proceedings of the Speech and Natural LanguageWorkshop (June, 1990), Morgan Kaufmarm Publishers, Inc.,1990.322
