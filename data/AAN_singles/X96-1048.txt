OVERVIEW OF RESULTS OF THE MUC-6 EVALUATIONBeth M. SundheimNaval Command, Control, and Ocean Surveillance CenterRDT&E Division (NRaD)Information Access Technology Project Team, Code 44208San Diego, CA 92152-7420sundheim@nosc.milINTRODUCTIONThe latest in a series of natural languageprocessing system evaluations was concluded inOctober 1995 and was the topic of the SixthMessage Understanding Conference (MUC-6) inNovember.
Participants were invited to enter theirsystems in as many as four different ask-orientedevaluations.
The Named Entity and Coreferencetasks entailed Standard Generalized MarkupLanguage (SGML) annotation of texts and werebeing conducted for the first time.
The other twotasks, Template Element and Scenario Template,were information extraction tasks that followed onfrom the MUC evaluations conducted in previousyears.
The evolution and design of the MUC-6evaluation are discussed in the paper by Grishmanand Sundheim in this volume.
All except theScenario Template task are defined independently ofany particular domain.This paper surveys the results of theevaluation on each task and, to a more limitedextent, across tasks.
Discussion of the results foreach task is organized generally under the followingtopics:?
Results on task as whole;?
Results on some aspects of task;?
Performance on "walkthrougharticle.
"The walkthrough article is an article selected fromthe test set.
Participants were asked to analyze theirsystem's performance on that article and commenton it in their presentations and papers.EVALUATION TASKSA basic characterization of the challengepresented by each evaluation task is as follows:?
Named Entity (NE) -- Insert SGMLtags into the text to mark each string thatrepresents a person, organization, orlocation name, or a date or time stamp, ora currency or percentage figure.?
Coreference (CO) -- Insert SGML tagsinto the text to link strings that representcoreferring noun phrases.?
Template Element (TE) -- Extractbasic information related to organizationand person entities, drawing evidence fromanywhere in the text.?
Scenario Template (ST) -- Drawingevidence from anywhere in the text,extract prespecified event information, andrelate the event information to theparticular organization and person entitiesinvolved in the event.The two SGML-based tasks requiredinnovations to tie system-internal data structures tothe original text so that the annotations could beinserted by the system without altering the originaltext in any other way.
This capability has otheruseful applications as well, e.g., it enables texthighlighting in a browser.
It also facilitatesinformation extraction, since some of theinformation in the extraction templates is in theform of literal text strings, which some systemshave in the past had difficulty reproducing in theiroutput.The inclusion of four different tasks in theevaluation implicitly encouraged sites to designgeneral-purpose architectures that allow theproduction of a variety of types of output from asingle internal representation i order to allow useof the full range of analysis techniques for all tasks.Even the simplest of the tasks, Named Entity,occasionally requires in-depth processing, e.g., todetermine whether "60 pounds" is an expression ofweight or of monetary value.
Nearly half the siteschose to participate in all four tasks, and all but onesite participated in at least one SGML task and oneextraction task.The variety of tasks designed for MUC-6reflects the interests of both participants andsponsors in assessing and furthering research thatcan satisfy some urgent ext processing needs in thevery near term and can lead to solutions to more423challenging text understanding problems in thelonger term.
Identification of certain common typesof names, which constitutes a large portion of theNamed Entity task and a critical portion of theTemplate Element ask, has proven to be largely asolved problem.
Recognition of alternative ways ofidentifying an entity constitutes a large portion ofthe Coreference task and another critical portion ofthe Template Element ask and has been shown torepresent only a modest challenge when the referentsare names or pronouns.
The mix of challenges thatthe Scenario Template task represents has beenshown to yield levels of performance that are smilarto those achieved in previous MUCs, but this timewith a much shorter time required for porting.Documentation of each of the tasks andsummary scores for all systems evaluated can befound in the MUC-6 proceedings \[1\].CORPUSTesting was conducted using Wall StreetJournal texts provided by the Linguistic DataConsortium.
The articles used in the evaluationwere drawn from a corpus of approximately 58,000articles panning the period of January 1993 throughJune 1994.
This period comprised the "evaluationepoch."
As a condition for participation in theevaluation, the sites agreed not to seek out andexploit Wall Street Journal articles from that epochonce the training phase of the evaluation had begun,i.e., once the scenario for the Scenario Templatetask had been disclosed to the participants.The training set and test set each consisted of100 articles and were drawn from the corpus using atext retrieval system called Managing Gigabytes,whose retrieval engine is based on a context-vectormodel, producing a ranked list of hits according todegree of match with a keyword search query.
It canalso be used to do unranked, Boolean retrievals.
TheBoolean retrieval method was used in the initialprobing of the corpus to identify candidates for theScenario Template task, because the Booleanretrieval is relatively fast, and the unranked resultsare easy to scan to get a feel for the variety ofnonrelevant as well as relevant documents thatmatch all or some of the query terms.
Once thescenario had been identified, the ranked retrievalmethod was used, and the ranked list was sampled atdifferent points to collect approximately 200relevant and 200 nonrelevant articles, representing avariety of article types (feature articles, brief notices,editorials, etc.).
From those candidate articles, thetraining and test sets were selected blindly, withlater checks and corrections for imbalances in therelevant/nonrelevant categories and in article types.From the 100 test articles, a subset of 30articles (some relevant o the Scenario Templatetask, others not) was selected for use as the test setfor the Named Entity and Coreference tasks.
Theselection was again done blindly, with later checksto ensure that the set was fairly representative interms of article length and type.
Note that althoughNamed Entity, Coreference and Template Elementare defined as domain-independent tasks, the articlesthat were used for MUC-6 testing were selectedusing domain-dependent criteria pertinent to theScenario Template task.
The manually filledtemplates were created with the aid of Tabula Rasa,a software tool developed for the Tipster TextProgram by New Mexico State UniversityComputing Research Laboratory.NAMED ENTITYThe Named Entity (NE) task requires insertionof SGML tags into the text stream.
The tagelements are ENAMEX (for entity names,comprising organizations, persons, and locations),TIMEX (for temporal expressions, namely directmentions of dates and times), and NUMEX (fornumber expressions, consisting only of directmentions of currency values and percentages).
ATYPE attribute accompanies each tag element andidentifies the subtype of each tagged string: forENAMEX, the TYPE value can beORGANIZATION, PERSON, or LOCATION; forTIMEX, the TYPE value can be DATE or TIME;and for NUMEX, the TYPE value can be MONEYor PERCENT.Text strings that are to be annotated are termedmarkables.
As indicated above, markables includenames of organizations, persons, and locations, anddirect mentions of dates, times, currency values andpercentages.
Non-markables include names ofproducts and other miscellaneous names("Macintosh," "Wall Street Journal" (in reference tothe periodical as a physical object), "Dow JonesIndustrial Average"); names of groups of people andmiscellaneous usages of person names("Republicans," "Gramm-Rudman,""Alzheimer\['s\]"); addresses and adjectival forms oflocation names ("53140 Gatchell Rd.," "American");indirect and vague mentions of dates and times ("afew minutes after the hour," "thirty days before theend of the year"); and miscellaneous uses ofnumbers, including some that are similar tocurrency or percentage expressions ("\[Fees\] 1 3/4,""12 points," "1.5 times").424The evaluation metrics used for NE areessentially the same as those used for the twotemplate-filling tasks, Template Element andScenario Template.
The following breakdowns ofoverall scores on NE are computed:?
by slot, i.e., for performance acrosstag elements, across TYPEattributes, and across tag strings;?
by subcategorization, i.e., forperformance on each TYPE attributeseparately;?
by document section, i.e., forperformance on distinct subparts ofthe article, as identified by theSGML tags contained in the originaltext: <HL> ("headline"), <DD>("document date"), <DATELINE>,and <TXT> (the body of the article).NE Results OverallFifteen sites participated in the NE evaluation,including two that submitted two systemconfigurations for testing and one that submittedfour, for a total of 20 systems.
As shown in table 1,performance on the NE task overall was over 90%on the F-measure for half of the systems tested,which includes ystems from seven different sites.On the basis of the results of the dry run, in whichtwo of the nine systems cored over 90%, we werenot surprised to find official scores that weresimilarly high, but it was not expected that so manysystems would enter the formal evaluation andperform so well.It was also unexpected that one of the systemswould match human performance on the task.Human performance was measured by comparing the30 draft answer keys produced by the annotator atNRaD with those produced by the annotator atSAIC.
This test measures the amount of variabilitybetween the annotators.
When the outputs arescored in "key-to-response" mode, as though oneannotator's output represented the "key" and theother the "response," the humans achieved an overallF-measure of 96.68 and a corresponding error perresponse fill (ERR) score of 6%.
The top-scoringsystem, the baseline configuration of the SRAsystem, achieved an F-measure of 96.42 and acorresponding error score of 5%.In considering the significance of these resultsfrom a general standpoint, the following facts aboutthe test set need to be remembered:96.42 595.66 794.92 894.00 1093.65 1093.33 1192.88 1092.74 1292.61 1291.20 1390.84 1489.06 1888.19 1985.82 2085.73 2384.95 2296 9795 9693 9692 9694 9392 9594 9292 9389 9691 9191 9184 9486 9085 8780 9282 89Table 1.
Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order ofdecreasing F-Measure (P&R) 11 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA "fast" configuration 95.66, SRA"fastest" configuration 92.61, SRA "nonames" configuration 94.92, SRI 94.0, Sterling Software 92.74.425?
It represents just one style of writing(journalistic) and has a basic basictoward financial news and a specificbias toward the topic of the ScenarioTemplate task.?
It was very small (only 30 articles).There were no markable timeexpressions in the test set, and therewere only a few markable percentageexpressions.The results should also be qualified by sayingthat they reflect performance on data that makesaccurate usage of upper and lower case distinctions.What would performance be on data where caseprovided no (reliable) clues and for languages wherecase doesn't distinguish names?
SRA ran anexperiment on an upper-case version of the test setthat showed 85% recall and 89% precision overall,with identification of organization ames presentingthe greatest problem.
That result represents nearly a10-point decrease on the F-measure from theirofficial baseline.
The case-insensitive r sults wouldbe slightly better if the task guidelines themselvesdidn't depend on case distinctions in certainsituations, as when identifying the right boundaryfor the organization ame span in a string such as"the Chrysler division" (currently, only "Chrysler"would be tagged).NE  Resu l t s  on Some Aspects ofTaskFigures 1 and 2 show the sample size for thevarious tag elements and TYPE values.
Note thatnearly 80% of the tags were ENAMEX and thatalmost half of those were subcategofized asorganization ames.
As indicated in table 2, allsystems performed better on identifying personnames than on identifying organization or locationnames, and all but a few systems performed betteron location names than on organization names.Organization names are varied in their form,consisting of proper nouns, general vocabulary, or amixture of the two.
They can also be quite long andcomplex and can even have internal punctuationsuch as a commas or an ampersand.
Sometimes it isdifficult to distinguish them from names of othertypes, especially from person names.
Commonorganization names, first names of people, andlocation names can be handled by recourse to listlookup, although there are drawbacks: some namesmay be on more than one list, the lists will not betimex 10%(n=lll)zx 8%:93)location12%organization48%can(n=925)perso40%Figure 1.
Distribution of NE tag elements intest setFigure 2.
Subcategories of ENAMEX in test set42696.4295.6694.9294.0093.6593.3392.8892.7492.6191.2090.8489.0688.1985.8285.7384.9510111616131615161418162229292645i?
!i~ iiiiii~~iiiiiii~iiili246444910177914461512131929182016293179916431210171318100042661169432323636324032Table 2.
NE subcategory scores (ERR metric), in order of decreasing overall F-Measure (P&R)complete and may not match the name as it isrealized in the text (e.g., may not cover the neededabbreviated form of an organization ame, may notcover the complete person ame), etc.The difference that recourse to lists can makein performance is seen by comparing two runs madeby SRA.
The experimental configuration resultedin a three point decrease in recall and one pointdecrease in precision, compared to the performanceof the baseline system configuration.
The changesoccurred only in performance on identifyingorganizations.
BBN conducted a comparative t st inwhich the experimental configuration used a largerlexicon than the baseline configuration, but theexact nature of the difference is not known and theperformance differences are very small.
As with theSRA experiment, the only differences inperformance between the two BBN configurationsare with the organization type.
The University ofDurham reported that they had intended to usegazetteer and company name lists, but didn't,because they found that the lists did not have mucheffect on their system's performance.The error scores for persons, dates, andmonetary expressions was less than or equal to 10%for the large majority of systems.
Several systemsposted scores under 10% error for locations, butnone was able to do so for oganizations.
Forpercentages, about half the systems had 0% error,which reflects the simplicity of that particularsubtask.
Note that the number of instances ofpercentages in the test set is so small that a singlemistake could result in an error of 6%.Slot-level performance on ENAMEX follows adifferent pattern for most systems from slot-levelperformance on NUMEX and TIMEX.
The generalpattern is for systems to have done better on theTEXT slot than on the TYPE slot for ENAMEXtags and for systems to have done better on theTYPE slot than on the TEXT slot for NUMEX andTIMEX tags.
Errors on the TEXT slot are errors infinding the right span for the tagged string, and thiscan be a problem for all three subcategories of tag.The TYPE slot, however, is a more difficult slot forENAMEX than for the other subcategories.
Itinvolves a three-way distinction for ENAMEX andonly a two-way distinction for NUMEX andTIMEX, and it offers the possibility of confusingnames of one type with names of another, especiallythe possibility of confusing organization ameswith person names.Looking at the document section scores intable 3, we see that the error score on the body ofthe text was much lower than on the headline for allbut a few systems.
There was just one system thatposted ahigher error score on the body than on theheadline, the baseline NMSU CRL configuration,and the difference in scores is largely due to the factthat the system overgenerated to a greater extent onthe body than on the headline.
Its basic strategy for42796.42 095.66 0 0 7 794.92 0 0 8 894.00 0 0 20 993.65 0 2 16 1093.33 0 4 38 992.88 0 0 18 1092.74 0 0 22 1192.61 100 0 18 991.20 0 0 30 1390.84 3 11 19 1489.06 3 4 28 1888.19 0 0 22 2085.82 0 6 18 2185.73 0 44 53 2184.95 0 0 50 21Table 3.
NE document subsection scores (ERR metric), in order of decreasing overall F-measure (P&R)headlines was a conservative one: tag a string in theheadline as a name only if the system had found itin the body of the text or if the system had predictedthe name based on truncation of names found in thebody of the text.
Most, if not all, the systems thatwere evaluated on the NE task adopted the basicstrategy of processing the headline after processingthe body of the text.The interannotator variability test providesreference points indicating human performance onthe different aspects of the NE task.
The documentsection results show 0% error on Document Dateand Dateline, 7% error on Headline, and 6% error onText.
The subcategory error scores were 6% onOrganization, 1% on Person, and 4% on Location,8% on Date, and 0% on Money and Percent.
Theseresults show that human variability on this taskpatterns in a way that is similar to the performanceof most of the systems in all respects exceptperhaps one: the greatest source of difficulty for thehumans was on identifying dates.
Analysis of theresults shows that some Date errors were a result ofsimple oversight (e.g., "fiscal 1994") and otherswere a consequence of forgetting or misinterpretingthe task guidelines with respect o determining themaximal span of the date expression (e.g., tagging"fiscal 1993's second quarter" and "Aug. 1"separately, rather than tagging "fiscal 1993's secondquarter, ended Aug. 1" as a single expression inaccordance with the task guidelines).NE Results on "WalkthroughArticle"In the answer key for the walkthrough articlethere are 69 ENAMEX tags (including a fewoptional ones), six TIMEX tags and six NUMEXtags.
Interannotator scoring showed that oneannotator missed tagging one instance of "Coke" asan (optional) organization, and the other annotatormissed one date expression ("September").Common mistakes made by the systems includedmissing the date expression, "the 21st century," andspuriously identifying "60 pounds" (which appearedin the context, "Mr. Dooner, who recently lost 60pounds over three-and-a-half months .
.
.
.  ")
as amonetary value rather than ignoring it as a weight.In addition, a number of errors identifying entitynames were made; some of those errors also showedup as errors on the Template Element task and aredescribed in a later section of this paper.COREFERENCEThe task as defined for MUC-6 was restrictedto noun phrases (NPs) and was intended to belimited to phenomena that were relativelynoncontroversial nd easy to describe.
The varietyof high-frequency phenomena covered by the task ispartially represented in the following hypotheticalexample, where all bracketed text segments areconsidered coreferential:428\[Motor Vehicles International Corp.\]announced a major management shake-up .... \[MVI\] said the chief executiveofficer has resigned .... \[The Big 10 automaker\] is attempting toregain marketshare .... \[It\] will announce significantlosses for the fourth quarter .... A\[company\] spokesman said \[they\] aremoving \[their\] operations to Mexico in acost-saving effort .... \[MVI, \[the firstcompany to announce such a move sincethe passage of the new international tradeagreement\],\] is facing increasing demandsfrom unionized workers .
.
.
.
\[MotorVehicles International\] is \[the biggestAmerican auto exporter to LatinAmerica\].The example passage covers a broad spectrumof the phenomena included in the task.
At one endof the spectrum are the proper names and aliases,which are inherently definite and whose referent mayappear anywhere in the text.
In the middle of thespectrum are definite descriptions and pronounswhose choice of referent is constrained by suchfactors as structural relations and discourse focus.On the periphery of the central phenomena remarkables whose status as coreferring expressions idetermined by syntax, such as predicate nominals("Motor Vehicles International is the biggestAmerican auto exporter to Latin America") andappositives ("MVI, the first company to announcesuch a move since the passage of the newinternational trade agreement").
At the far end of thespectrum are bare common nouns, such as theprenominal "company" in the example, whosestatus as a referring expression may be questionable.An algorithm developed by the MITRECorporation for MUC-6 was implemented by SAICand used for scoring the task.
The algorithmcompares the equivalence classes defined by thecoreference links in the manually-generated answerkey and the system-generated response.
Theequivalence classes are the models of the identityequivalence coreference r lation.
Using a simplecounting scheme, the algorithm obtains recall andprecision scores by determining the minimalperturbations required to align the equivalenceclasses in the key and response.
No metrics otherthan recall and precision were defined for this task,and no statistical significance testing was performedon the scores.CO Results OverallIn all, seven sites participated in the MUC-6coreference valuation.
Most systems achievedapproximately the same levels of performance: fiveof the seven systems were in the 51%-63% recall1009080706050403020100O0 ?0 10 20 30 40 50 60 70 80 90 100RecallFigure 3.
Overall recall and precision on the CO task 22 Key to recall and precision scores: UDurham 36R/44P, UManitoba 63R/63P, UMass 44R/51P, NYU 53R/62P, UPenn55R/63P, USheffield 51R/71P, SRI 59R/72P.429range and 62%-72% precision range.
About half thesystems focused only on individual coreference,which has direct relevance to the other MUC-6evaluation tasks.A few of the evaluation sites reported thatgood name/alias recognition alone would buy asystem a lot of recall and precision points on thistask, perhaps about 30% recall (since proper namesconstituted a large minority of the annotations) and90% precision.
The precision figure is supported byevidence from the NE evaluation.
In that evaluation,a number of systems cored over 90% on the namedentity recall and precision metrics, providing asound basis for good performance on the coreferencetask for individual entities.In the middle of the effort of preparing the testdata for the formal evaluation, an interannotatorvariability test was conducted.
The two versions ofthe independently prepared, manual annotations of17 articles were scored against each other using thescoring program in the normal "key to response"scoring mode.
The amount of agreement betweenthe two annotators was found to be 80% recall and82% precision.
There was a large number of factorsthat contributed to the 20% disagreement, includingoverlooking coreferential NPs, using differentinterpretations of vague portions of the guidelines,and making different subjective decisions when thetext of an article was ambiguous, sloppy, etc.
Mosthuman errors pertained to definite descriptions andbare nominals, not to names and pronouns.CO Results on Some Aspects o fTask and on "Walkthrough Article"To keep the annotation of the evaluation datafairly simple, the MUC-6 planning committeedecided not to design the notation to subcategorizelinkages and markables in any way.
Two usefulattributes for the equivalence class as a whole wouldbe one to distinguish individual coreference fromtype coreference and one to identify the generalsemantic type of the class (organization, person,location, time, currency, etc.).
For each NP in theequivalence class, it would be useful to identify itsgrammatical type (proper noun phrase, definitecommon noun phrase, bare singular common nounphrase, personal pronoun, etc.).
The decision tominimize the annotation effort makes it difficult todo detailed quantitative analysis of the results.An analysis by the participating sites of theirsystem's performance on the walkthrough articleprovides ome insight into performance on aspectsof the coreference task that were dominant in thatarticle.
The article contains about 1000 words andapproximately 130 coreference links, of which allbut about a dozen are references to individualpersons or individual organizations.
Approximately50 of the anaphors are personal pronouns, includingreflexives and possessives, and 58 of the markables(anaphors and antecedents) are proper names,including aliases.
The percentage of personalpronouns is relatively high (38%), compared to thetest set overall (24%), as is the percentage of propernames (40% on this text versus an estimate of 30%overall).Performance on this particular article for somesystems was higher than performance on the test setoverall, reaching as high as 77% recall and 79%precision.
These scores indicate that pronounresolution techniques as well as proper nounmatching techniques are good, compared to thetechniques required to determine r ferences involvingcommon oun phrases.
For common noun phrases,the systems were not required to include the entireNP in the response; the response could minimallycontain only the head noun.
Despite this flexibilityin the expected contents of the response, thesystems nonetheless had to implicitly recognize thefull NP, since to be considered coreferential, thehead and its modifiers all had to be consistent withanother markable.TEMPLATE ELEMENTThe Template Element (TE) task requiresextraction of certain general types of informationabout entities and merging of the information aboutany given entity before presentation i  the form of atemplate (or "object").
For MUC-6 the entities thatwere to be extracted were limited to organizationsand persons) The ORGANIZATION objectcontains attributes ("slots") for the stringrepresenting the organization ame (ORG NAME),for strings representing any abbreviated versions ofthe name (ORG_ALIAS), for a string that describesthe particular organization (ORG_DESCRIPTOR),for a subcategory of the type of organization(ORG_TYPE, whose permissible values areGOVERNMENT, COMPANY, and OTHER), andfor canonical forms of the specific and generallocation of the organization (ORG LOCALE andORG_COUNTRY).
The PERSON object contains3The task documentation includes definition of an"artifact" entity, but that entity type was not used inMUC-6 for either the dry run or the formal run.
Theentity types that were involved in the evaluation arethe same as those required for the Scenario Templatetask.430slots only for the string representing the personname (PER_NAME), for strings representing anyabbreviated versions of the name (PERALIAS) ,and for strings representing a very limited range oftitles (PER_TITLE).The task places heavy emphasis onrecognizing proper noun phrases, as in the NE task,since all slots except ORG_DESCRIPTOR andPERT ITLE  expect proper names as slot fillers (instring or canonical form, depending on the slot.However, the organization portion of the TE task isnot limited to recognizing the referential identitybetween full and shortened names; it requires the useof text analysis techniques at all levels of textstructure to associate the descriptive and locativeinformation with the appropriate ntity.
Analysisof complex NP structures, such as appositionalstructures and postposed modifier adjuncts, is neededin order to relate the locale and descriptor to thename in "Creative Artists Agency, the bigHollywood talent agency" and in "Creative ArtistsAgency, a big talent agency based in Hollywood.
"Analysis of sentence structures to identifygrammatical relations such as predicate nominals isneeded in order to relate those same pieces ofinformation in "Creative Artists Agency is a bigtalent agency based in Hollywood."
Analysis ofdiscourse structure is needed in order to identifylong-distance r lationships.The answer key for the TE task contains oneobject for each specific organization and personmentioned in the text.
For generation of aPERSON object, the text must provide the name ofthe person (full name or part of a name).
Forgeneration of an ORGANIZATION object, the textmust provide either the name (full or part) or adescriptor of the organization.
Since the generationof these objects is independent of the relevancecriteria imposed by the Scenario Template (ST)task, there are many more ORGANIZATION andPERSON objects in the TE key than in the ST key.For the formal evaluation, there were 606ORGANIZATION and 496 PERSON objects in theTE key, versus 120 ORGANIZATION and 137PERSON objects in the ST key.The same set of articles was used for TE as forST; therefore, the content of the articles is orientedtoward the terms and subject matter covered by theST task, which concerns changes in corporatemanagement.
4 One effect of this bias is simply thenumber of entities mentioned in the articles: for the4 The method used for selecting the articles for the testset is described at the beginning of this article.test set used for the MUC-6 dry run, which wasbased on a scenario concerning labor union contractnegotiations, there were only about half as manyorganizations and persons mentioned as there werein the test set used for the formal run.TE Results OverallTwelve systems -- from eleven sites, includingone that submitted two system configurations fortesting-- were tested on the TE task.
All but twoof the systems posted F-measure scores in the 70-80% range, and four of the systems were able toachieve recall in the 70-80% range whilemaintaining precision in the 80-90% range, asshown in the figure 4.
Human performance wasmeasured in terms of variability between the outputsproduced by the two NRaD and SAIC evaluators for30 of the articles in the test set (the same 30 articlesthat were used for NE and CO testing).
Using thescoring method in which one annotator's draft keyserves as the "key" and the other annotator's draftkey serves as the "response," the overall consistencyscore was 93.14 on the F-measure, with 93% recalland 93% precision.TE Results on Some Aspects of TaskGiven the more varied extraction requirementsfor the ORGANIZATION object, it is notsurprising that performance on that portion of theTE task was not as good as on the PERSONobject 5, as is clear in figure 5.Figure 6 indicates the relative amount of errorcontributed by each of the slots in theORGANIZATION object.
It is evident that themore linguistic processing necessary to fill a slot,the harder the slot is to fill correctly.
TheORG_COUNTRY slot is a special case in a way,since it is required to be filled when theORG_LOCALE slot is filled.
(The reverse is notthe case, i.e., ORG_COUNTRY may be filled evenif ORG_LOCALE is not, but this situation isrelatively rare.)
Since a missing or spuriousORG_LOCALE is likely to incur the same error inORG_COUNTRY, the error scores for the two slotsare understandably similar.5 The highest score for the PERSON object, 95% recalland 95% precision, is close to the highest score on theNE subcategorization f rperson, which was 98% recalland 99% precision.431((XI9080~)5O4O20100 ..0 10 220Figure 4.?
, , v?
?
0430 41) 50 60 7(1 80 91)RecallOverall recall and precision on the TE task 6I(X)100 -90807060504030201000Figure 5.?
"7"o qb l ?10 20 30 40 50 60 70 80 90 100Reca l lOrganization and Person object recall and precision on the TE task6Key to recall and precision scores: BBN 66R/79P, UDurham 49R/60P, Lockheed-Martin 76R/77P, UManitoba71R/78P, UMass 53R/72P, MITRE 71R/85P, NYU 62R/83P, USheffield 66R/74P, SRA baseline configuration75R/86P, SRA "noref" configuration 74R/87P, SRI 74R/76P, Sterling Software 72R/83P.4328o I,o lI I  ii403o2o ,~10-type name alias country locale descriptorORGANIZATION Slot=Figure 6.
Best and average rror per response fill Organization object slot scores for TE taskWith respect to performance onORG_DESCRIPTOR, note that there may bemultiple descriptors (or none) in the text.
However,the task does not require the system to extract alldescriptors of an entity that are contained in the text;it requires only that the system extract one (ornone).
Frequently, at least one can be found inclose proximity to an organization's name, e.g., asan appositive ("Creative Artists Agency, the bigHollywood talent agency").
Nonetheless,performance is much lower on this slot than onothers.Leaving aside the fact that descriptors arecommon noun phrases, which makes them lessobvious candidates for extraction than proper nounphrases would be, what reasons can we find toaccount for the relatively low performance on theORG_DESCRIPTOR slot?
One reason for lowperformance is that an organization may beidentified in a text solely by a descriptor, i.e.,without a fill for the ORG_NAME slot andtherefore without the usual local clues that the NPis in fact a relevant descriptor.
It is, of course, alsopossible that a text may identify an organizationsolely by name.
Both possibilities present increasedopportunities for systems to undergenerate orovergenerate.
Also, the descriptor is not alwaysclose to the name, and some discourse processingmay be requ~ed in order to identify it -- this islikely to increase the opportunity for systems tomiss the information.
A third significant reason isthat the response fill had to match the key fillexactly in order to be counted correct; there was noallowance made in the scoring software forassigning full or partial credit if the response fillonly partially matched the key fill.
It should benoted that human performance on this task was alsorelatively low, but it is unclear whether the degreeof disagreement can be accounted for primarily bythe reasons given above or whether the disagreementis attributable to the fact that the guidelines for thatslot had not been finalized at the time when theannotators created their version of the keys.TE Results on "WalkthroughArticle"TE performance of all systems on thewalkthrough article was not as good as performanceon the test set as a whole, but the difference is smallfor about half the systems.
Viewed from theperspective of the TE task, the walkthrough articlepresents a number of interesting examples of entitytype confusions that can result from insufficientprocessing.
There are cases of organization amesmisidentified as person names, there is a case of alocation name misidentified as an organizationname, and there are cases of nonrelevant entity types(publications, products, indefinite references, etc.
)misidentified as organizations.
Errors of these kindsresult in a penalty at the object level, since theextracted information is contained in the wrong typeof object.
Examples of each of these types of errorappear below, along with the number of systemsthat committed the error.
(An experimentalconfiguration of the SRA system produced the sameoutput as the baseline configuration and has beendisregarded in the tallies; thus, the total number ofsystems tallied is eleven.)1.
Miscategorizations of entities as person(PER_NAME or PER_ALIAS) instead oforganization (ORG_NAME or ORG_ALIAS)4332..?
Six systems: McCann-Erickson (alsoextracted with the name of "McCann,""One McCann," "While McCann";organization category is indicated clearlyby context in which full name appears,"John Dooner Will Succeed James AtHelm of McCann-Erickson" in headlineand "Robert L. James, chairman and chiefexecutive officer of McCann-Erickson, andJohn J. Dooner Jr., the agency's presidentand chief operating officer" in the body ofthe article)eSix systems: J. Walter Thompson (alsoextracted with the name of "WalterThompson"; organization category isindicated by context, "Peter Kim was hiredfrom WPP Group's J. Walter Thompsonlast September...")eFour systems: Fallon McElligott(organization category is indicated bycontext, "...other ad agencies, such asFallon McElligott")eOne system: Ammirati & Puris (thepresence of the ampersand is a clue, as isthe context, "...president and chiefexecutive officer of Ammirati & Puris";but note that the article also mentions thename of one of the company's founders,Martin Puris)Miscategorization of entity asorganization (ORG NAME) instead oflocation (ORG_LOCALE)eTwo systems: Hollywood (locationcategory is indicated by context, "CreativeArtists Agency, the big Hollywood talentagency")Miscategorization of nonrelevant entitiesas organization ame, alias or descriptor(ORG NAME, ORG_ALIAS,ORG_DESCRIPTOR)oSix systems: New York Times(publication name in phrase, "a framedpage from the New York Times"; withoutsufficient context, the name can beambiguous in its reference to a physicalobject versus an organization)eThree systems: Coca-Cola Classic(product name deriving from "Coca-Cola,"which appears eparately in several placesin the article and is occasionallyambiguous even in context betweenproduct name and organization name)eOne system: Not Butter (part of productname, "I Can't Believe It's Not Butter")eOne system: Taster (part of product name,"Taster's Choice")?
One system: Choice (part of productname, "Taster's Choice")eFive systems: a hot agency (nonspecificuse of indefinite in phrase "...is interestedin acquiring ahot agency")Given the variety of contextual clues that mustbe taken into account in order to analyze the aboveentities correctly, it is understandable that just aboutany given system would commit at least one ofthem.
But the problems are certainly tractable; noneof the fifteen TE entities in the key (tenORGANIZATION entities and five PERSONentities) was miscategofized byall of the systems.In addition to miscategorization errors, thewalkthrough text provides other interestingexamples of system errors at the object level and theslot level, plus a number of examples of systemsuccesses.
One success for the systems as a groupis that each of the six smaller ORGANIZATIONobjects and four smaller PERSON objects (thosewith just one or two filled slots in the key) wasmatched perfectly by at least one system; inaddition, one larger ORGANIZATION object andtwo larger PERSON objects were perfectly matchedby at least one system.
Thus, each of the fivePERSON objects in the key and seven of the tenORGANIZATION objects in the key were matchedperfectly by at least one system.
The three largerORGANIZATION objects that none of the systemsgot perfectly correct are for the McCann-Erickson,Creative Artists Agency, and Coca-Cola companies.Common errors in these three ORGANIZATIONobjects included missing the descriptor orlocale/country or failing to identify theorganization's alias with its name.SCENARIO TEMPLATEA Scenario Template (ST) task capturesdomain- and task-specific information.
Threescenarios were defined in the course of MUC-6: (1)a scenario concerning the event of organizationsplacing orders to buy aircraft with aircraftmanufacturers (the "aircraft order" scenario); (2) ascenario concerning the event of contractnegotiations between labor unions and companies(the "labor negotiations" scenario); (3) a scenarioconcerning changes in corporate managersoccupying executive posts (the "managementsuccession" scenario).
The first scenario was used asan example of the general design of the ST task, thesecond was used for the MUC-6 dry run evaluation,and the third was used for the formal evauation.One of the innovations of MUC-6 was to formalizethe general structure of event emplates, and all three434scenarios defined in the course of MUC-6 conformedto that general structure.
In this article, themanagement succession scenario will be used as thebasis for discussion.The management succession template consistsof four object types, which are linked together viaone-way pointers to form a hierarchical structure.At the top level is the TEMPLATE object, ofwhich there is one instantiated for every document.This object points down to one or moreSUCCESSION_EVENT objects if the documentmeets the event relevance criteria given in the taskdocumentation.
Each event object captures thechanges occurring within a company with respect toone management post.
TheSUCCESSION_EVENT object points down to theIb~AND_OUT object, which in turn points downto PERSON Template Element objects thatrepresent the persons involved in the successionevent.
The IN_AND_OUT object contains ST-specific information that relates the event with thepersons.
The ORGANIZATION Template Elementobjects are present at the lowest level along with thePERSON objects, and they are pointed to not onlyby the IN_AND_OUT object but also by theSUCCESSION_EVENT object.
The organizationpointed to by the event object is the organizationwhere the relevant management post exists; theorganization pointed to by the relational object isthe organization that the person who is moving inor out of the post is coming from or going to.The scenario is designed around themanagement post rather than around the successionact itself.
Although the management post andinformation associated with it are represented in theSUCCESSION_EVENT object, that object does notactually represent an event, but rather a state, i.e.,the vacancy of some management post.
Therelational-level Iih~AND_OUT objects represent thepersonnel changes pertaining to that state.ST Results OverallNine sites submitted a total of eleven systemsfor evaluation on the ST task.
All the participatingsites also submitted systems for evaluation on theTE and NE tasks.
All but one of the developmentteams (UDurham) had members who were veteransof MUC-5.Of the 100 texts in the test set, 54 wererelevant o the management succession scenario,including six that were only marginally relevant.Marginally relevant event objects are marked in theanswer key as being optional, which means that asystem is not penalized if it does not produce suchan event object.
The approximate 50-50 splitbetween relevant and nonrelevant texts wasTemplate Level(Doc_Nr)JCCESSION_EVE/~(Post, Vacancy_Reason)IOTemplate Element LevelIn_and_Outr IN_AND_OUT "(New_Status, On_the_Job,Rel Other_Org) jSuccession OrgPERSON1ame, Per_Alias,Per_Title)ORGANIZAT ION(Org_Name, Org_Alias, Org_Descriptor,~Q0rg_Type, Org_Locale, Org_Country)Figure 7.
Management Succession Template Structure435intentional and is comparable to the richness of theMUC-3 "TST2" test set and the MUC-4 "TST4"test set.
(The test sets used for MUC-5 had a muchhigher proportion of relevant exts.)
Systems aremeasured for their performance on distinguishingrelevant from nonrelevant texts via the text filteringmetric, which uses the classic information retrievaldefinitions of recall and precision.For MUC-6, text filtering scores were as highas 98% recall (with precision in the 80th percentile)or 96% precision (with recall in the 80th percentile).Similar tradeoffs and upper bounds on performancecan be seen in the TST2 and TST4 results (see scorereports in sections 2 and 4 of appendix G in \[2\]).However, performance of the systems as a group isbetter on the MUC-6 test set.
The text filteringresults for MUC-6, MUC-4 (TST4) and MUC-3(TST2) are shown in figure 8.Whereas the Text Filter row in the score reportshows the system's ability to do text filtering(document detection), the All Objects row and theindividual Slot rows show the system's ability to doinformation extraction.
The measures used forinformation extraction include two overall ones, theF-measure and error per response fill, and severalother, more diagnostic ones (recall, precision,undergeneration, overgeneration, and substitution).The text filtering definition of precision is differentfrom the information extraction definition ofprecision; the latter definition includes an element inthe formula that accounts for the number ofspurious template fills generated.The All Objects recall and precision scores areshown in figure 9.
The highest ST F-measure scorewas 56.40 (47% recall, 70% precision).Statistically, large differences of up to 15 pointsmay not be reflected as a difference in the ranking ofthe systems.
Most of the systems fall into thesame rank at the high end, and the evaluation doesnot clearly distinguish more than two ranks (see thepaper on statistical significance testing by Chinchorin \[1\]).
Human performance was measured in termsof interannotator variability on only 30 texts in thetest set and showed agreement to be approximately83%, when one annotator's templates were treated asthe "key" and the other annotator's templates weretreated as the "response.
"100 ......9O807060?
~ 50g.403020100&& A &m ?I0 20 30 40 50 60 70 80 90Reca l l~/vlUC3 (TST2) IIMUC4 (TST4)tkMUC6100Figure 8.
Text filtering recall and precision for scenario test sets with approximately 50% richness436100-908070"~ 504O3Ob2010-0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0 10 20 30 40 50 60 70 80 rio I00RecallFigure 9.
Overall information extraction recall and precision on the ST task 7No analysis has been done of the relativedifficulty of the MUC-6 ST task compared toprevious extraction evaluation tasks.
The one-month limitation on development in preparation forMUC-6 would be difficult to factor into thecomputation, and even without that additionalfactor, the problem of coming up with a reasonable,objective way of measuring relative task difficultyhas not been adequately addressed.
Nonetheless, asone rough measure of progress in the area ofinformation extraction as a whole, we can considerthe F-measures of the top-scoring systems from theMUC-5 and MUC-6 evaluations.MUC-6 56.40MUC-5 EJV 52.75MUC-5 JJV 60.07MUC-5 EME 49.18MUC-5 JME 56.31Table 4.
Highest P&R F-Measure scores postedfor MUC-6 and MUC-5 ST tasksNote that table 4 shows four top scores for MUC-5,one for each language-domain pair: English JointVentures (EJV), Japanese Joint Ventures (JJV),English Microelectronics (EME), and JapaneseMicroelectronics (JME).
From this table, it may bereasonable toconclude that progress has been made,since the MUC-6 performance l vel is at least ashigh as for three of the four MUC-5 tasks and sincethat performance l vel was reached after a muchshorter time.ST Results on Some Aspects of Taskand on "Walkthrough Article"Three succession events are reported in thewalkthrough article.
Successful interpretation ofthree sentences from the walkthrough article isnecessary for high performance on these events.The tipoff on the first two events comes at the endof the second paragraph:Yesterday, McCann made official whathad been widely anticipated: Mr. James,57 years old, is stepping down as chiefexecutive officer on July 1 and will retireas chairman at the end of the year.
Hewill be succeeded by Mr. Dooner, 45.The basis of the third event comes halfway throughthe two-page article:In addition, Peter Kim was hired fromWPP Group's J. Walter Thompson lastSeptember as vice chairman, chiefstrategy officer, world-wide.7Key to recall and precision scores: BBN 50R/59P, UDurham 33R/34P, Lockheed-Martin 43R/64P, UManitoba39R/62P, UMass 36R/46P, NYU 47R/70P, USheffield 37R/73P, SRA baseline configuration 47R/62P, SRA "precision"configuration 32R/66P, SRA "recall" configuration 58R/46P, SRI 44R/61P.437Event#1Answer KeyJames out, Dooner in as CEO of McCann-Erickson as a result of James departing theworkforce; James is still on the job as CEO;Dooner is not on the job as CEO yet, and his oldjob was with the same org as his new job.Event James out, Dooner in as chairman of#2 McCann-Erickson as a result of James departingthe workforce; James is still on the job aschairman; Dooner is not on the job as chairmanyet, and his old job was with the same org as hisnew job.Event Kim in as "vice chairman, chief strategy#3 officer, world-wide" of McCann-Erickson, wherethe vacancy existed for other/unknown reasons;he is already on the job in the post, and his oldjob was with J. Walter ThompsonI I ITable 5.
Paraphrased summary of STThe article was relatively straightforward forthe annotators who prepared the answer key, andthere were no substantive differences in the outputproduced by each of the two annotators.Table 5 contains a paraphrased summary of theoutput that was to be generated for each of theseevents, along with a summary of the output thatwas actually generated by systems evaluated forMUC-6.
The system-generated outputs are fromthree different systems, since no one system didbetter than all other systems on all three events.The substantive differences between the system-generated output and the answer key are indicated byunderlining in the system output.Recurring problems in the system outputsinclude the information about whether the person iscurrently on the job or not and the information onwhere the outgoing person's next job would be andwhere the incoming person's previous job was.Note also that even the best system on the thirdevent was unable to determine that the successionevent was occurring at McCann-Efickson; inaddition, it only partially captured the full title ofthe post.
To its credit, however, it did recognizethat the event was relevant; only two systemsproduced output hat is recognizable as pertaining tothis event.
One common problem was the simplefailure to recognize "hire" as an indicator of asuccession.Two systems never filled the OTHER_ORGslot or its dependent slot, REL OTHER_ORG,System OutputJames out, Dooner in as CEO of McCann-Erickson as a result of a reassignment of James;James is no__!
on the job as CEO any more,his new job is at the same as his old job; Doonermay or may not be on the job as CEO yet, andhis old job was with the same org as his newjob.
(SRA satie_base system)James out, Dooner in as chairman ofMcCann-Erickson as a result of James departingthe workforce; James is no_4 on the job aschairman any more; Dooner is already on the jobas chairman, and his old job was with Ammirati& Puris.
(NYU system)Kim in as vice chairman of WPP Group,where the vacancy existed for other/unknownreasons; he may or may not be on the job in thatpost yet, and the article doesn't say where his oldjob was.
(BBN system)outputs for walkthrough articledespite the fact that data to fill those slots was oftenpresent; over half the IN_AND_OUT objects in theanswer key contain data for those two slots.Almost without exception, systems did more poorlyon those two slots than on any others in theSUCCESSION_EVENT and IN_AND_OUTobjects; the best scores posted were 70% error onOTHER_ORG (median score of 79%) and 72% erroron REL_OTHER ORG (median of 86%).Performance on the VACANCY_REASONand ON_THE JOB slots was better for nearly allsystems.
The lowest error scores were 56% onVACANCY_REASON (median of 70%) and 62%on ONZI'HE_JOB (median of 71%).The slot that most systems performed best onis NEWSTATUS;  the lowest error score posted onthat slot is 47% (median of 55%).
This slot has alimited number of fill options, and the right answeris almost always either IN or OUT, depending onwhether the person involved is assuming a post (IN)or vacating a post (OUT).
Performance on thePOST slot was not quite as good; the lowest errorwas 52% (median of 65%).
The POST slot requiresa text string as fill, and there is no finite list ofpossible fills for the slot.
As seen in the third eventof the walkthrough article, the fill can be anextended title such as "vice chairman, chief strategyofficer, world-wide."
For most events, however, thefill is one of a large handful of possibilities,including "chairman," president," chief executive\[officer\]," "CEO," "chief operating officer," "chieffinancial officer," etc.438DISCUSSION: CRITIQUE OFTASKSNamed EntityThe primary subject for review in the NEevaluation is its limited scope.
A variety of propername types were excluded, e.g.
product names.
Therange of numerical and temporal expressions coveredby the task was also limited; one notable xample isthe restriction of temporal expressions to exclude"relative" time expressions uch as "last week".Restriction of the corpus to Wall Street Journalarticles resulted in a limited variety of markables andin reliance on capitalization toidentify candidates forannotation.Some work on expanding the scope of the NEtask has been carried out in the context of a foreign-language NE evaluation conducted in the spring of1996.
This evaluation is called the MET(Multilingual Named Entity) and, like MUC-6, wascarried out under the auspices of the Tipster Textprogram.
The experience gained from thatevaluation will serve as critical input to revising theEngish version of the task.CoreferenceMany aspects of the CO task are in definiteneed of review for reasons of either theory orpractice.
One set of issues concerns the range ofsyntactically governed correference phenomena thatare considered markable.
For example, apposition asa markable phenomenon was restrictively defined toexclude constructs that could rather be analyzed asleft modification, such as "chief executive ScottMcNealy," which lacks the comma punctuation thatwould clearly identify "executive" as the head of anappositive construction.
Another set of issues issemantic in nature and includes fimdamentalquestions such as the validity of including typecoreferrence in the task and the legitimacy of theimplied definition of coteference versus reference.
Ifan antecedent expression is nonreferential, can itnonetheless be considered coreferential withsubsequent anaphoric expressions?
Or can onlyreferring expressions corefer?
Finally, the currentnotation presents a set of issues, such as itsinability to represent multiple antecedents, as inconjoined NPs, or alternate antecedents, as in thecase of referential mbiguity.In short, the preliminary nature of the taskdesign is reflected in the somewhat unmotivatedboundaries between markables and nonmarkables andin weaknesses in the notation.
One indication ofimmaturity of the task definition (as well as anindication of the amount of genuine textualambiguity) is the fact that over ten percent of thelinkages in the answer key were marked as"optional."
(Systems were not penalized if theyfailed to include such linkages in their output.)
Thetask definition is now under review by a discourseworking group formed in 1996 with representativesfrom both inside and outside the MUC commuity,including representatives from the spoken-languagecommunity.Template ElementThere are miscellaneous outstanding problemswith the TE task.
With respect to theORGANIZATION and PERSON objects, there areissues such as rather fuzzy distinctions among thethree organization subtypes and between theorganization ame and alias, the extremely limitedscope of the person title slot, and the lack of aperson descriptor slot.
The ARTIFACT object,which was not used for either the dry run or theformal evaluation, eeds to be reviewed with respectto its general utility, since its definition reflectsprimarily the requirements of the MUC-5microelectronics task domain.
There is a task-neutral DATE slot that is defined as a templateelement; it was used in the MUC-6 dry run as partof the labor negotiation scenario, but as currentlydefined, it fails to capture meaningfully some of therecurring kinds of date information.
In particular,problems remain with normalizing various types ofdate expressions, including ones that are vagueand/or equire xtensive use of calendar information.Scenario TemplateThe issues with respect o the ST task relateprimarily to the ambitiousness of the scenariotemplates defined for MUC-6.
Although themanagement scenario contained only five domain-specific slots (disregarding slots containing pointersto other objects), it nonetheless reflected an interestin capturing as complete a representation of thebasic event as possible.
As a result, a few"peripheral" facts about he event were included thatwere difficult to define in the task documentationand/or were not reported clearly in many of thearticles.Two of the slots, VACANCY_REASON andON_THE_JOB, had to be filled on the basis ofinference from subtle linguistic ues in many cases.An entire appendix to the scenario definition isdevoted to heuristics for filling the ON_THE JOBslot.
These two slots caused problems for the439annotators as well as for the systems.
Theannotators' problems with VACANCY_REASONmay have had more to do with understanding whatthe scenario definition was saying than withunderstanding what the news articles were saying.The annotators' problems with ONZI'HE_JOB wereprobably more substantive, since the heuristicsdocumented in the appendix were complex andsometimes hard to map onto the expressions foundin the news articles.
A third slot,REL_OTHER_ORG, required special inferencing onthe basis of both linguistics and world knowledge inorder to determine the corporate relationship betweenthe organization a manager is leaving and the onethe manager is going to.
There may, in fact, be justone organization involved -- the person could beleaving a post at a company in order to take adifferent (or an additional) post at the samecompany.Defining a generalized template structure andusing Template Element objects as one layer in thestructure reduced the amount of effort required forparticipants o move their system from one scenarioto another.
Further simplification may be advisablein order to focus on core information elements andexclude somewhat idiosyncratic ones such as thethree slots described above.
In the case of themanagement succession scenario, a proposal wasmade to eliminate the three slots discussed aboveand more, including the relational object itself, andto put the personnel information i  the event object.Much less information about the event would becaptured, but there would be a much stronger focuson the most essential information elements.
Thiswould possibly lead to significant improvements inperformance on the basic event-related elements andto development of good end-user tools forincorporating some of the domain-specific patternsinto a generic extraction system.CONCLUSIONSThe results of the evaluation give clearevidence of the challenges that have been overcomeand the ones that remain along dimensions of bothbreadth and depth in automated text analysis.
TheNE evaluation results serve mainly to document inthe MUC context what was already stronglysuspected:1.
Automated identification is extremely accuratewhen identification of lexical pattern typesdepends only on "shallow" information, such asthe form of the string that satisfies the patternand/or immediate context;2.
Automated identification is significantly lessaccurate when identification is clouded byuncertainty or ambiguity (as when casedistinctions are not made, when organizationsare named after persons, etc.)
and must dependon one or more "deep" pieces of information(such as world knowledge, pragmatics, orinferences drawn from structural analysis at thesentential and suprasentential levels).The vast majority of cases are simple ones; thus,some systems core extremely well -- well enough,in fact, to compete overall with human performance.Commercial systems are available already thatinclude identification of those defined for this MUC-6 task, and since a number of systems performedvery well for MUC-6, it is evident that highperformance is probably within reach of anydevelopment site that devotes enough effort to thetask.
Any participant in a future MUC evaluationfaces the challenge of providing a named entityidentification capability that would score in the 90thpercentile on the F-measure on a task such as theMUC-6 one.The TE evaluation task makes explicit oneaspect of extraction that is fundamental to a verybroad range of higher-level extraction tasks.
Theidentification of a name as that of an organization(hence, instantiation of an ORGANIZATIONobject) or as a person (PERSON object) is a namedentity identification task.
The association ofshortened forms of the name with the full namedepends on techniques that could be used for NE andCO as well as for TE.
The real challenge of TEcomes from associating other bits of informationwith the entity.
For PERSON objects, thischallenge is small, since the only additional bit ofinformation required is the person's title ("Mr.,""Ms.," "Dr.," etc.
), which appears immediatelybefore the name/alias in the text.
ForORGANIZATION objects, the challenge is greater,requiring extraction of location, description, andidentification of the type of organization.Performance on TE overall is as high as 80%on the F-measure, with performance onORGANIZATION objects significantly lower (70thpercentile) than on PERSON objects (90thpercentile).
Top performance on PERSON objectscame close to human performance, whileperformance on ORGANIZATION objects fellsignificantly short of human performance, with thecaveat hat human performance was measured ononly a portion of the test set.
Some of the shortfallin performance on the ORGANIZATION object isdue to inadequate discourse processing, which isneeded in order to get some of the non-localinstances of the ORG_DESCRIPTOR,ORG LOCALE and ORG_COUNTRY slot fills.440In the case of ORG_DESCRIPTOR, the results ofthe CO evaluation seem to provide further evidencefor the relative inadequacy of current echniques forrelating entity descriptions with entity names.Systems scored approximately 15-25 pointslower (F-measure) on ST than on TE.
As defined forMUC-6, the ST task presents a significant challengein terms of system portability, in that the testprocedure requ~ed that all domain-specificdevelopment be done in a period of one month.
Forpast MUC evaluations, the formal run had beenconducted using the same scenario as the dry run,and the task definition was released well before thedry run.
Since the development time for the MUC-6 task was extremely short, it could be expected thatthe test would result in only modest performancelevels.
However, there were at least three factorsthat might lead one to expect higher levels ofperformance than seen in previous MUCevaluations:1.
The standardized template structureminimizes the amount of idiosyncraticprogramming required to produce theexpected types of objects, links, and slotfills.2.
The fact that the domain-neutral TemplateElement evaluation was being conductedled to increased focus on getting the low-level information correct, which wouldcarry over to the ST task, sinceapproximately 25% of the expectedinformation in the ST test set wascontained in the low-level objects.3.
Many of the veteran participating sites hadgotten to the point in their ongoingdevelopment where they had fast andefficient methods for updating theirsystems and monitoring their progress.It appears that there is a wide variety of sources oferror that impose limits on system effectiveness,whatever the techniques employed by the system.In addition, the short time frame allocated fordomain-specific development aturally makes it verydifficult for developers to do sufficient developmentto fill complex slots that either are not alwaysexpected to be filled or are not crucial elements inthe template structure.Sites have developed architectures that are atleast as general-purpose techniques as ever, perhapsas a result of having to produce outputs for as manyas four different tasks.
Many of the sites haveemphasized their pattern-matching techniques indiscussing the strengths of their MUC-6 systems.However, we still have full-sentence parsing (e.g.USheffield, UDurham, UManitoba); we sometimeshave expectations of "deep understanding" (cf.UDurham's use of a world model) and sometimesnot (cf.
UManitoba's production of ST outputdirectly from dependency trees, with no semanticrepresentation per se).
Some systems completed allstages of analysis before producing outputs for anyof the tasks, including NE.
Six of the seven sitesthat participated in the coreference evaluation alsoparticipated in the MUC-6 information extractionevaluation, and five of the six made use of theresults of the processing that produced theircoreference output in the processing that producedtheir information extraction output.The introduction of two new tasks into theMUC evaluations and the restructuring ofinformation extraction into two separate tasks haveinfused new life into the evaluations.
Other sourcesof excitement are the spinoff efforts that the NE andCO tasks have inspired that bring these tasks andtheir potential applications to the attention of newresearch groups and new customer groups.
Inaddition, there are plans to put evaluations on line,with public access, starting with the NE evaluation;this is intended to make the NE task familiar to newsites and to give them a convenient and low-pressureway to try their hand at following a standardized testprocedure.
Finally, a change in administration ofthe MUC evaluations is occurring that will bringfresh ideas.
The author is turning over governmentleadership of the MUC work to Elaine Marsh at theNaval Research Laboratory in Washington, D.C.Ms.
Marsh has many years of experience incomputational linguistics to offer, along withextensive familiarity with the MUC evaluations,and will undoubtedly lead the work exceptionallywell.ACKNOWLEDGEMENTSThe definition and implementation of theevaluations reported on at the MessageUnderstanding Conference was once again a"community" effort, requiring active involvementon the part of the evaluation participants as well asthe organizers and sponsors.
Individual thanks go toRalph Grishman of NYU for serving as program co-chair, to Nancy Chinchor for her critical efforts onvirtually all aspects of MUC-6, and to the othermembers of the program committee, which includedChinatsu Aone of SRA Corp., Lois Childs ofLockheed Martin Corp., Jerry Hobbs of SRIInternational, Boyan Onyshkevych of the U.S.Dept.
of Defense, Marc Vilain of The MITRECorp., Takahiro Wakao of the Univ.
of Sheffield,and Ralph Weischedel of BBN Systems andTechnologies.
The author would also like to441acknowledge the critical behind-the-scenes computersupport rendered at NRaD by Tim Wadsworth, whopassed away suddenly in August 1995, leaving alasting empty spot in my work and my heart.REFERENCES[1] Proceedings of the Sixth Message UnderstandingConference (MUC-6), November 1995, SanFrancisco: Morgan Kaufmann.
[2] Proceedings of the Fourth MessageUnderstanding Conference (MUC-4), June 1992,San Mateo: Morgan Kaufmann.442
