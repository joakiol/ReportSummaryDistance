A Finite State and Data-Oriented Method for Grapheme toPhoneme ConversionGosse BoumaAl fa - in fo rmat icaR i jksun ivers i te i t  Gron ingenPostbus  7169700 AS Gron ingenThe  Nether landsgosse@let, rug.
nlAbst ractA finite-state method, based on leftmost longest-match replacement, is presented for segmentingwords into graphemes, and for converting raphemesinto phonemes.
A small set of hand-crafted conver-sion rules for Dutch achieves a phoneme accuracy ofover 93%.
The accuracy of the system is further im-proved by using transformation-based learning.
Thephoneme accuracy of the best system (using a largerule and a 'lazy' variant of Brill's algoritm), trainedon only 40K words, reaches 99%.1 In t roduct ionAutomatic grapheme to phoneme conversion (i.e.the conversion of a string of characters into a stringof phonemes) is essential for applications of textto speech synthesis dealing with unrestricted text,where the input may contain words which do notoccur in the system dictionary.
Furthermore, atransducer for grapheme to phoneme conversioncan be used to generate candidate replacements ina (pronunciation-sensitive) spelling correction sys-tem.
When given the pronunciation of a misspelledword, the inverse of the grapheme to phoneme trans-ducer will generate all identically pronounced words.Below, we present a method for developing suchgrapheme to phoneme transducers based on a com-bination of hand-crafted conversion rules, imple-mented using finite state calculus, and automaticallyinduced rules.The hand-crafted system is defined as a two-step procedure: segmentation of the input into asequence of graphemes (i.e.
sequences of one ormore characters typically corresponding to a sin-gle phoneme) and conversion of graphemes into (se-quences of) phonemes.
The composition of thetransducer which performs segmentation and thetransducer defined by the conversion rules, is atransducer which converts sequences of charactersinto sequences of phonemes.Specifying the conversion rules is a difficult task.Although segmentation of the input can in princi-ple be dispensed with, we found that writing con-version rules for segmented input substantially re-duces the context-sensitivity and order-dependenceof such rules.
We manually developed a grapheme tophoneme transducer for Dutch data obtained fromCELEX (Baayen et al, 1993) and achieved a word ac-curacy of 60.6% and a phoneme accuracy of 93.6%.To improve the performance of our system, weused transformation-based l arning (TBL) (Brill,1995).
Training data are obtained by aligning theoutput of the hand-crafted finite state transducerwith the correct phoneme strings.
These data canthen be used as input for TBL, provided that suit-able rule templates are available.
We performed sev-eral experiments, in which the amount of' trainingdata, the algorithm (Brill's original formulation and'lazy' variants (Samuel et al, 1998)), and the num-ber of rule templates varied.
The best experiment(40K words, using a 'lazy' strategy with a large setof rule templates) induces over 2000 transformationrules, leading to 92.6% word accuracy and 99.0%phoneme accuracy.
This result, obtained using arelatively small set of training data, compares wellwith that of other systems.2 F in i te  State CalculusAs argued in Kaplan and Kay (1994), Karttunen(1995), Karttunen et al (1997), and elsewhere,many of the rules used in phonology and morphol-ogy can be analysed as special cases of regular ex-pressions.
By extending the language of regular ex-pressions with operators which capture the interpre-tation of linguistic rule systems, high-level inguis-tic descriptions can be compiled into finite state au-tomata directly.
Furthermore, such automata can becombined with other finite state automata perform-ing low-level tasks such as tokenization or lexical-lookup, or more advanced tasks such as shallow pars-ing.
Composition of the individual components intoa single transducer may lead to highly efficient pro-cessing.The system described below was implemented us-ing FSA Utilities, 1 a package for implementing andmanipulating finite state automata, which providespossibilities for defining new regular expression oper-I www.
let.
rug.
n l / -vannoord/fs  a/303\[\]\[R~,..., R,,\]{R1,... ,Rn}R -ignore (A,B)AxBidentity(A)ToUmacro  (Term, R)the empty stringconcatenationdisjunctionoptionalityignore: A interspersed with elements of Bcross-product: he transducer which mapsall strings in A to all strings in B.identity: the transducer which maps eachelement, in A onto itself.composition of the transducers T and U.use Term as an abbreviation for R (where Term and R may contain variables).Figure 1: A fragment of FSA regular expression syntax.
A and B are regular expressions denoting recognizers,T and U transducers, and R can be either.ators.
The part of FSA's built-in regular expressionsyntax relevant o this paper, is listed in figure 1.One particular useful extension of the basic syn-tax of regular expressions i the replace-operator.Karttunen (1995) argues that many phonologicaland morphological rules can be interpreted as ruleswhich replace a certain portion of the input string.Although several implementations of the replace-operator are proposed, the most relevant case forour purposes is so-called 'leftmost longest-match' re-placement.
In case of overlapping rule targets in theinput, this operator will replace the leftmost arget,and in cases where a rule target contains a prefixwhich is also a potential target, the longer sequencewill be replaced.
Gerdemann and van Noord (1999)implement leftmost longest-match replacement inFSA as the operatorreplace(Target, LeftContext,RightContext),where Target is a transducer defining the actual re-placement, and LeftContext and RightContext areregular expressions defining the left- and rightcon-text of the rule, respectively.An example where leftmost replacement is use-ful is hyphenation.
Hyphenation of (non-compound)words in Dutch amounts to segmenting a wordinto syllables, separated by hyphens.
In caseswhere (the written form of) a word can in prin-ciple be segmented in several ways (i.e.
the se-quence a l fabet  can be segmented as a l - fa -bet ,a l - fab -e t ,  a l l -a -bet ,  or a l f -ab -e t ) ,  the seg-mentation which maximizes onsets is in general thecorrect one (i.e.
a l - fa -bet ) .
This property of hy-phenation is captured by leftmost replacement:macro(hyphenate,rep lace( \ [ \ ]  x - ,  sy l lab le ,  sy l lab le ) ) .Leftmost replacement ensures that hyphens are in-troduced 'eagerly', i.e.
as early as possible.
Givena suitable definition of sy l lab le ,  this ensures thatwherever a consonant can be final in a coda or initialin the next onset, it is in fact added to the onset.The segmentation task discussed below makes cru-cial use of longest match.3 A f in i te s tate  method forgrapheme to phoneme convers ionGrapheme to phoneme conversion is implemented asthe composition of four transducers:macro  (graph2phon,segmentat ion 7, segment he inputo mark_begin_end 7, add ' #'o convers ion 7. apply ru leso clean_up ) .
Z remove markersAn example of conversion including the in-termediate steps is given below for the wordaanknopingspunt (connection-point).input: aanknopingspunts: aa-n-k-n-o-p-i-ng-s-p-u-n-t-m: #-aa-n-k-n-o-p-i-ng-s-p-u-n-t-#co: #-a+N+k-n-o -p - I+N+s-p-}+n- t -#cl: aNknopINsp}ntThe first transducer (segmentation) takes asits input a sequence of characters and groupsthese into segments.
The second transducer(mark_begin_end) adds a marker ( '~')  to the be-ginning and end of the sequence of segments.
Thethird transducer (convers ion)  performs the actualconversion step.
It converts each segment into asequence of (zero or more) phonemes.
The finalstep (clean_up) removes all markers.
The outputis a list of phonemes in the notation used by CELEX(which can be easily translated into the more com-mon SAMPA-notation).3.1 Segmentat ionThe goal of segmentation is to divide a word into asequence of graphemes, providing a convenient input304level of representation for the actual grapheme tophoneme conversion rules.While there are many letter-combinations whichare realized as a single phoneme (ch, ng, aa, bb,.. ), it is only rarely the case that a single letter ismapped onto more than one phoneme (x), or that aletter receives no pronunciation at all (such as word-final n in Dutch, which is elided if it is proceeded bya schwa).
As the number of cases where multipleletters have to be mapped onto a single phonemeis relatively high, it is natural to model a letter tophoneme system as involving two subtasks: segmen-tation and conversion.
Segmentation splits an inputstring into graphemes, where each grapheme typ-ically, but not necessarily, corresponds to a singlephoneme.Segmentation is defined as:macro(segmentat ion,rep lace(\ [ ident i ty (graphemes) ,) .\[\]x - \ ] , \ [ \ ] , \ [ \ ] )The macro graphemes defines the set of graphemes.It contains 77 elements, some of which are:a, aa, au, ai, aai, e, ee, el, eu, eau,eeu, i, ie, lee, ieu, ij, o, oe, oei,..Segmentation attaches the marker ' - '  to eachgrapheme.
Segmentation, as it is defined here,is not context-sensitive, and thus the second andthird arguments of replace are simply empty.
Asthe set of graphemes contains many elements whichare substrings of other graphemes (i.e.
e is asubstring of ei, eau, etc.
), longest-match is es-sential: the segmentation of be iaard ie r  (caril-lon player) should be b -e i -aa - r -d - ie - r -  and notb -e - i -a -a - r -d - i -e - r - .
This effect can be ob-tained by making the segment itself part of the tar-get of the replace statement.
Targets are identi-fied using leftmost longest-match, and thus at eachpoint in the input, only the longest valid segment ismarked.The set of graphemes contains a number of ele-ments which might seem superfluous.
The graphemeaa?, for instance, translates as aj, a sequence whichcould also be derived on the basis of two graphemesaa and ?.
However, if we leave out the segmentaa?, segmentation (using leftmost longest match) ofwords such as waaien (to blow) would lead to thesegmentation w-aa- ie -n ,  which is unnatural, as itwould require an extra conversion rule for ?e.
Us-ing the grapheme aai  allows for two conversion ruleswhich always map aai  to aj and ?e goes to ?.Segmentation as defined above provides the in-tuitively correct result in almost all cases, given asuitably defined set of graphemes.
There are somecases which are less natural, but which do not nec-essarily lead to errors.
The grapheme u, for in-stance, almost always goes to ' l ' ,  but translatesas 'e , j ,} '  in (loan-) words such as museum andpetroleum.
One might argue that a segmentatione-u-  is therefore required, but a special conver-sion rule which covers these exceptional cases (i.e.eu followed by m) can easily be formulated.
Simi-larly, ng almost always translates as N, but in somecases actually represents the two graphemes n-g- ,as in aaneenges loten (connected), where it shouldbe translated as NG.
This case is harder to detect,and is a potential source of errors.3.2 The  Convers ion  Ru lesThe g2p operator is designed to facilitate the formu-lation of conversion rules for segmented input:macro(g2p(Target,LtCont,RtCont),replace(\[Target, - x +\],\[ignore(LtCont,{+,-}), {-,+}\],ignore(RtCont,{+,-}))).The g2p-operator implements a special pro:pose ver-sion of the replace-operator.
The replacement of themarker '-' by '+ '  in the target ensures that g2p-conversion rules cannot apply in sequence to thesame grapheme.
2 Second, each target of the g2p-operator must be a grapheme (and not some sub-string of it).
This is a consequence of the fact thatthe final element of the left-context must be a markerand the target itself ends in '-'.
Finally, the ig-nore statements in the left and right context implythat the rule contexts can abstract over the potentialpresence of markers.An overview of the conversion rules we used forDutch is given in Figure 2.
As the rules are ap-plied in sequence, exceptional rules can be orderedbefore the regular cases, thus allowing the regularcases to be specified with little or no context.
Thespecial_vowel_rules deal with exceptional trans-lations of graphemes such as eu or cases where i ori j  goes to '?'.
The short_vowel_rules treat sin-gle vowels preceding two consonants, or a word finalconsonant.
One problematic ase is e, which canbe translated either as 'E '  or '~ ' .
Here, an ap-proximation is attempted which specifies tile con-text where e goes 'E ' ,  and subsumes the othercase under the general rule for short vowels.
Tilespecia l_consonant_ru les address devoicing and afew other exceptional cases.
The de fau l t _ ru lessupply a default mapping for a large number of2Note that the input and output alphabet are not disjoint,and thus rules applying in sequence to the same part of theinput are not excluded in principle.305graphemes.
The target of this rule is a long disjunc-tion of grapheme-phoneme appings.
As this rule-set applies after all more specific: cases have beendealt with, no context restrictions need to be speci-fied.Depending somewhat on how one counts, the fullset of conversion rules for Dutch contains approxi-mately 80 conversion rules, more than 40 of whichare default mappings requiring no context.
3 Compi-lation of the complete system results in a (minimal,deterministic) transducer with 747 states and 20,123transitions.3.3 Test  resu l ts  and discussionThe accuracy of the hand-crafted system was evM-uated by testing it on all of tile words wihtout di-acritics in the CELEX lexical database which have aphonetic transcription.
After several developmentcycles, we achieved a word accuracy of 60.6% anda phonenle accuracy (measured as the edit distancebetween the phoneme string produced by the sys-tem and the correct string, divided by the numberof phonemes in the correct string) of 93.6%.There have been relatively few attempts at devel-oping grapheme to phoneme conversion systems us-ing finite state technology alone.
Williams (1994) re-ports on a system for Welsh, which uses no less than700 rules implemented in a rather estricted environ-ment.
The rules are also implemented in a two-levelsystem, PC-KIMMO, (Antworth, 1990), but this stillrequires over 400 rules.
MSbius et al (1997) reporton full-fledged text-to-speech system for German,containing around 200 rules (which are compiled intoa weighted finite state transducer) for the grapheme-to-phoneme conversion step.
These numbers uggestthat our implementation (which contains around 80rules in total) benefits considerably from the flexibil-ity and high-level of abstraction made available byfinite state calculus.One might suspect hat a two-level approach tographeme to phoneme conversion is more appropri-ate than the sequential approach used here.
Some-what surprisingly, however, Williams concludes thata sequential approach is preferable.
The formulationof rules in the latter approach is more intuitive, andrule ordering provides a way of dealing with excep-tional cases which is not easily available in a two-level system.While further improvements would definitely havebeen possible at this point, it becomes increasinglydifficult to do this on the basis of linguistic knowl-edge alone.
That is, most of the rules which haveto be added deal with highly idiosyncratic cases (of-ten related to loan-words) which can only be discov-3It should be noted that  we only considered words whichdo not contain diacritics.
Including those is unproblematic  nprinciple, but  would lead to a slight increase of the numberof rules.ered by browsing through the test results of previ-ous runs.
At this point, switching from a linguistics-oriented to a data-oriented methodology, seemed ap-propriate.4 T rans format ion-based  graphemeto  phoneme convers ionBrill (1995) demonstrates that accurate part-of-speech tagging can be learned by using a two-stepprocess.
First, a simple system is used which as-signs the most probable tag to each word.
The re-sults of the system are aligned with the correct agsfor some corpus of training data.
Next, (context-sensitive) transformation rules are selected from apool of rule patterns, which replace erroneous tagsby correct ags.
The rule with the largest benefit onthe training data (i.e.
the rule for which the numberof corrections minus the number of newly introducedmistakes, is the largest) is learned and applied tothe training data.
This process continues until nomore rules can be found which lead to improvement(above a certain threshold).Transformation-based l arning (TBL) can be ap-plied to the present problem as well.
4 In this case,the base-line system is the finite state transducer de-scribed above, which can be used to produce a setof phonemic transcriptions for a word list.
Next,these results are aligned with the correct transcrip-tions.
In combination with suitable rule patterns,these data can be used as input for a TBL process.4.1 AlignmentTBL requires aligned data for training and testing.While alignment is mostly trivial for part-of-speechtagging, this is not the case for the present task.Aligning data for grapheme-to-phoneme conversionamounts to aligning each part of the input (a se-quence of characters) with a part of the output (asequence of phonemes).
As the length of both se-quences is not guaranteed to be equal, it must bepossible to align more than one character with asingle phoneme (the usual case) or a single characterwith more than one phoneme (the exceptional case,i.e.
'x').
The alignment problem is often solved (Du-toit, 1997; Daelemans and van den Bosch, 1996) byallowing 'null' symbols in the phoneme string, andintroducing 'compound' phonemes, such as 'ks' toaccount for exceptional cases where a single charac-ter must be aligned with two phonemes.As our finite state system already segments theinput into graphemes, we have adopted a strategywhere graphemes instead of characters are alignedwith phoneme strings (see Lawrence and Kaye(1986) for a similar approach).
The correspondence4Hoste et al (2000b) compare TBL  to C5.0 (Quinlan,1993) on a similar task, i.e.
the mapping  of the pronunciat ionof one regional variant of Dutch into another.306macro(conversion, special_vowel_rules o short_vowel_ruleso special_consonant_rules o default_rulesmacro (special_vowel_rules,g2p( \ [e ,u \ ]  x \ [e , j ,} \ ] ,  \ [ \ ] ,  m)o g2p( i  x ?, \ [ \ ] ,  g)o g2p( \ [ i , j \ ]  x @, 1, k).
.
.
.
.macro (shor t_vowe l_ ru les ,g2p(e x 'E ' ,  \ [ \ ] ,  { \ [ t , t \ ] , \ [k ,k \ ] ,x , .
.
.}
)g2p({ a x 'h '  , e x 0, i x ' I '  , o x '0 ') .macro (special_consonant_rules,g2p(b x p, \ [ \ ] ,  {s , t ,#})o g2p(\[d,t ~\] x t ,  \[\], {s,g,k,j,v,h,z,#})o g2p({ f x v, s x z}, \[\], {b,d})o g2p(g x 'G', vowel, vowel)o g2p(n x 'N', \[\], {k,q})o g2p(n x \ [ \ ] ,  \ [?\] , \ [#\] ).
.
. )
.macro (de fau l t ru les ,%% museum%% moedig(st)%% mogelijkheid., u x ' } '  }, \ [ \ ] ,  \ [cons, {cons , #}\] )g2p({ \ [a ,a \ ]  x a, \ [a ,a , i \ ]  x \ [a , j \ ] ,  \ [a,u\]  x 'M', \ [e ,a ,u \ ]  x o .
.
.
.
.
,\ [b,b\]  x b, \[d,d\] x d .
.
.
.
.
.
\ [c,h\]  x ' x ' ,  \ [ s , c ,h \ ]  x I s ,x \ ] ,  \ [n,g\]  x 'N ' ,}, \[2, \[\] )) .Figure 2: Conversion Rulesbetween graphemes and phonemes is usually one toone, but it is no problem to align a grapheme withtwo or more phonemes.
Null symbols are only intro-duced in the output if a grapheme, such as word-final'n ' ,  is not realized phonologically.For TBL, the input actually has to be aligned bothwith the system output as well as with the correctphoneme string.
The first task can be solved triv-ially: since our finite state system proceeds by firstsegmenting the input into graphemes (sequences ofcharacters), and then transduces each grapheme intoa sequence of phonemes, we can obtain aligned databy simply aligning each grapheme with its con'e-sponding phoneme string.
The input is segmentedinto graphemes by doing the segmentation step ofthe finite state transducer only.
The correspondingphoneme strings can be identified by applying theconversion transducer to the segmented input, whilekeeping the boundary symbols '-' and '+' .
As a con-sequence of the design of the conversion-rules, theresulting sequence of separated phonemes equencesstands in a one-to-one relationship to the graphemes.An example is shown in figure 3, where GR representsthe grapheme segmented string, and sP the (system)phoneme strings produced by the finite state trans-ducer.
Note that the final sP cell contains only aboundary marker, indicating that the grapheme 'n'is translated into the null phoneme.For the alignment between graphemes (and, idi-Word aalbessen (currants)GR aa-  1-  b -  e - ss -  e - n -sP a + 1- b-  @+ s + ~@+cp a 1 b E s @Figure 3: Alignmentrectly, the system output) and the correct phonemestrings (as found in Celex), we used the 'hand-seeded' probabilistic alignment procedure describedby Black et al (1998) ~.
From the finite state conver-sion rules, a set of possible grapheme --+ phoneme se-quence mappings can be derived.
This allowables-setwas extended with (exceptional) mappings presentin the correct data, but not in the haml-crafted sys-tem.
We computed all possible aligmnents between(segmented) words and correct phoneme strings li-cenced by the allowables-set.
Next, probabilities forall allowed mappings were estimated on the basisof all possible alignments, and the data was parsedagain, now picking the most probable alignment foreach word.
To minimize the number of words thatcould not be aligned, a maximum of one unseen map-ping (which was assigned a low probability) was al-lowed per word.
With this modification, only oneout of 1000 words on average could not be aligned.
'~These words were discarded.The aJigned phoneme5Typical cases are loan words (umpires) and letter words(i.e.
abbreviations) (abe).307method training data phonenm word induced CPU time(words) accuracy accuracy rules (in minutes)Base-line 93.6 60.6Brill 20K 98.0 86.1 447 162Brill 40K 98.4 88.9 812 858lazy(5) 20K 97.6 83.5 337 43lazy(5) 40K 98.2 87.0 701 190lazy(5) 60K 98.4 88.3 922 397lazy(10) 20K 97.7 84.3 368 83lazy(10) 40K 98.2 87.5 738 335lazy(10) 60K 98.4 88.9 974 711lazy(5)+ 20K 98.6 89.8 1225 186lazy(5)+ 40K 99.0 92.6 2221 603Figure 4: Experimental Results using training data produced by graph2phonstring for the example in figure 3 is shown in thebottom line.
Note that the final cell is empty, rep-resenting the null phoneme.4.2 The  exper imentsFor the experiments with TBL we used the #-TBL-package (Lager, 1999).
This Prolog implementationof TBL is considerably more efficient (up to tentimes faster) than Brill's original (C) implementa-tion.
The speed-up results mainly from using Pro-log's first-argument indexing to access large quanti-ties of data efficiently.We constructed a set of 22 rule templates whichreplace a predicted phoneme with a (corrected)phoneme on the basis of the underlying segment,and a context consisting either of phoneme strings,with a maximum length of two on either side, ora context consisting of graphemes, with a maximallength of 1 on either side.
Using only 20K words(which corresponds to almost 180K segments), andBrill's algorithm, we achieved a phoneme accuracyof 98.0% (see figure 4) on a test set of 20K words ofunseen data.
6 Going to 40K words resulted in 98.4%phoneme accuracy.
Note, however, that in spite ofthe relative efficiency of the implementation, CPUtime also goes up sharply.The heavy computation costs of TBL are due tothe fact that for each error in the training data, allpossible instantiations of the rule templates whichcorrect his error are generated, and for each of theseinstantiated rules the score on the whole training sethas to be computed.
Samuel et al (1998) there-fore propose an efficient, 'lazy', alternative, basedon Monte Carlo sampling of the rules.
For each er-ror in the training set, only a sample of the rulesis considered which might correct it.
As rules whichcorrect a high number of errors have a higher chance6The statistics for less time consuming experiments wereobtained by 10-fold cross-validation a d for the more expen-sive experiments by 5-fold cross-validation.of being sampled at some point, higher scoring rulesare more likely to be generated than lower scoringrules, but no exhaustive search is required.
We ex-perimented with sampling sizes 5 and 10.
As CPUrequirements are more modest, we managed to per-form experiments on 60K words in this case, whichlead to results which are comparable with Brill's al-goritm applied to 40K words.Apart from being able to work with larger datasets, the 'lazy' strategy also has the advantage thatit can cope with larger sets of rule templates.
Brill'salgorithm slows down quickly when the set of ruletemplates i extended, but for an algorithm based onrule sampling, this effect is much less severe.
Thus,we also constructed a set of 500 rule templates, con-taining transformation rules which allowed up tothree graphemes or phoneme sequences as left orright context, and also allowed for disjunctive con-texts (i.e.
the context must contain an 'a '  at thefirst or second position to the right).
We used thisrule set in combination with a 'lazy' strategy withsampling size 5 (lazy(5)+ in figure 4).
This led to afurther improvement ofphoneme accuracy to 99.0%,and word accuracy of 92.6%, using only 40K wordsof training material.Finally, we investigated what the contribution wasof using a relatively accurate training set.
To thisend, we constructed an alternative training set, inwhich every segment was associated with its mostprobable phoneme (where frequencies were obtainedfrom the aligned CELEX data).
As shown in figure 5,the initial accuracy for such as system is much lowerthan that of the hand-crafted system.
The exper-imental results, for the 'lazy' algorithm with sam-pling size 5, show that the phoneme accuracy fortraining on 20K words is 0.3% less than for the cor-responding experiment in figure 4.
For 40K words,the difference is still 0.2%, which, in both cases, cor-responds to a difference in error rate of around 10%.As might be expected, the number of induced rules308method training data phoneme word induced CPU time(words) accuracy accuracy rules (i n minutes)Base-line 72.9 10.8lazy(5) 20K 97.3 81.6 691 133lazy(5) 40K 98.0 86.0 1075 705Figure 5: Experimental results using data based on frequency.is much higher now, and thus cPu-requirements al oincrease substantially.5 Conc lud ing  remarksWe have presented a method for grapheme tophoneme conversion, which combines a hand-craftedfinite state transducer with rules induced by atransformation-based learning.
An advantage ofthismethod is that it is able to achieve a high level ofaccuracy using relatively small training sets.
Busser(1998), for instance, uses a memory-based learningstrategy to achieve 90.1% word accuracy on the sametask, but used 90% of the CELEX data (over 300Kwords) as training set and a (character/phoneme)window size of 9.
Hoste et al (2000a) achieve aword accuracy of 95.7% and a phoneme accuracy of99.5% on the same task, using a combination ofma-chine learning techniques, as well as additional dataobtained from a second ictionary.Given the result of Roche and Schabes (1997), anobvious next step is to compile the induced rules intoan actual transducer, and to compose this with thehand-crafted transducer.
It should be noted, how-ever, that the number of induced rules is quite largein some of the experiments, sothat the compilationprocedure may require some attention.ReferencesEvan L. Antworth.
1990.
PC-KIMMO : a two-levelprocessor for morphological nalysis.
Summer In-stitute of Linguistics, Dallas, Tex.R.
H. Baayen, R. Piepenbrock, and H. van Rijn.1993.
The CELEX Lexical Database (CD-ROM).Linguistic Data Consortium, University of Penn-sylvania, Philadelphia, PA.Alan Black, Kevin Lenzo, and Vincent Pagel.
1998.Issues in building general letter to sound rules.
InProceedings of the 3rd ESCA/COCSADA Work-shop on Speech Synthesis, pages 77-81, JenolanCaves, Australia.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural anguage processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21:543-566.Bertjan Busser.
1998.
TreeTalk-D: a machine learn-ing approach to Dutch word pronunciation.
InProceedings TSD Conference, pages 3-8, MasarykUniversity, Czech Republic.W.
Daelemans and A. van den Bosch.
1996.Language-independent da a-oriented grapheme-to-phoneme conversion.
In Progress in SpeechSynthesis, pages 77-90, New York.
Springer Ver-lag.Thierry Dutoit.
1997.
An Introduction to Tezt-to-Speech Synthesis.
Kluwer, Dordrecht.Dale Gerdemann and Gertjan van Noord.
1999.Transducers from rewrite rules with backrefer-ences.
In Proceedings of the Ninth Conference o/the European Chapter of the Association for Com-putational Linguistics, pages 126-133, Bergen.Veronique Hoste, Walter Daelemans, ErikTjong Kim Sang, and Steven Gillis.
2000a.Meta-learning ofphonemic annotation ofcorpora.ms., University of Antwerp.Veronique Hoste, Steven Gillis, and Walter Daele-mans.
2000b.
A rule induction approach to mod-elling regional pronunciation variation, ms., Uni-versity of Antwerp.Ronald Kaplan and Martin Kay.
1994.
Regularmodels of phonological rule systems.
Computa-tional Linguistics, 20(3).L.
Karttunen, J.P. Chanod, G. Grefenstette, andA.
Schiller.
1997.
Regular expressions tbr lan-guage engineering.
Natural Larl.
'u, age Engineering,pages 1-24.Lauri Karttunen.
1995.
The replace operator.
In33th Annual Meeting o/ the Association/or Com-putational Linguistics, pages 16-23, Boston, Mas-sachusetts.Torbj6rn Lager.
1999.
The #-TBL System:Logic programming tools for transformation-based learning.
In Proceedings of the Th, ird In-ternational Workshop on Computational Natu-ral Language Learning (CoNLL '99), pages 33-42,Bergen.S.
C. G. Lawrence and G. Kaye.
1986.
Alignment ofphonemes with their corresponding orthography.Computer Speech and Language, 1(2):153 -165.Bernd MSbius, Richard Sproat, Jan van Santen, andJoseph Olive.
1997.
The Bell L~0~s German text-to-speech system: An overview.
In Proceedings ofthe European Conference on Speech Communica-tion and Technology, pages 2443-2446, Rhodes.J.
R. Quinlan.
1993.
C4.5: programs for machinelearning.
Morgan Kaufmann Publishers, San Ma-rco.309Emmanuel Roche and Yves Schabes.
1997.
Deter-ministic part-of-speech tagging with finite-statetransducers.
In Emmanuel Roche aml Yves Sdl-abes, editors, Finite state language processing,pages 205-239.
MIT Press, Cambridge, Mass.Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998.
Dialogue act tagging withtransformation-based learning.
In Proceedings ofthe 17th International Con\]erence on Computa-tional Linguistics (COLING-ACL '98), Montreal.Briony Williams.
1994.
Welsh letter-to-sound rules:rewrite rules and two-level rules compared.
Com-puter Speech and Language, 8:261-277.310
