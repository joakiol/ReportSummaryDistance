Proceedings of the 7th Workshop on Statistical Machine Translation, pages 338?344,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUPM system for WMT 2012Ver?nica L?pez-Lude?a, Rub?n San-Segundo and Juan M. MonteroGTH-IEL-ETSI Telecomunicaci?nUniversidad Polit?cnica de Madrid{veronicalopez, lapiz, juancho}@die.upm.esAbstractThis paper describes the UPM system for theSpanish-English translation task at theNAACL 2012 workshop on statistical ma-chine translation.
This system is based on Mo-ses.
We have used all available free corpora,cleaning and deleting some repetitions.
In thispaper, we also propose a technique for select-ing the sentences for tuning the system.
Thistechnique is based on the similarity with thesentences to translate.
With our approach, weimprove the BLEU score from 28.37% to28.57%.
And as a result of the WMT12 chal-lenge we have obtained a 31.80% BLEU withthe 2012 test set.
Finally, we explain differentexperiments that we have carried out after thecompetition.1 IntroductionThe Speech Technology Group at the TechnicalUniversity of Madrid has participated in the sev-enth workshop on statistical machine translation inthe Spanish-English translation task.Our submission is based on the state-of-the-artSMT toolkit Moses (Koehn et al, 2007).
Firstly,we have proved different corpora for training thesystem: cleaning the whole corpus and deletingsome repetitions in order to have a better perfor-mance of the translation model.There are several related works on filtering thetraining corpus by removing noisy data that use asimilarity measure based on the alignment score orbased on sentences length (Khadivi and Ney,2005).In this paper, we also propose a technique forselecting the most appropriate sentences for tuningthe system, based on the similarity with the Span-ish sentences to translate.
This technique is an up-date of the technique proposed by our group in thelast WMT11 challenge (L?pez-Lude?a and San-Segundo, 2011).
There are other works related toselect the development set (Hui et al, 2010) thatcombine different development sets in order to findthe more similar one with test set.There are also works related to select sentences,but for training instead of tuning, based on the sim-ilarity with the source test sentences.
Some of themare based on transductive learning: semi-supervised methods for the effective use of mono-lingual data from the source language in order toimprove translation quality (Ueffing, 2007); meth-ods using instance selection with feature decayalgorithms (Bicici and Yuret, 2011); or using TF-IDF algorithm (L?
et al, 2007).
There are alsoworks based on selecting training material withactive learning: using language model adaptation(Shinozaki et al, 2011); or perplexity-based meth-ods (Mandal et al, 2008).In this work, we have used the proposed selec-tion method only for tuning.The rest of the paper is organized as follows.Next section overviews the system.
Section 3 de-scribes the used corpora.
Section 4 explains theexperiments carried out before the competition.Section 5 describes the sentences selection tech-nique for tuning.
Section 6 summarizes the results:before the WMT12 challenge, the corresponding tothe competition and the last experiments.
Finally,section 7 shows the conclusions.2 Overall description of the systemThe translation system used is based on Moses, thesoftware released to support the translation task(http://www.statmt.org/wmt12/) at the NAACL2012 workshop on statistical machine translation.338The Moses decoder is used for the translationprocess (Koehn et al, 2007).
This program is abeam search decoder for phrase-based statisticalmachine translation models.We have used GIZA++ (Och and Ney, 2003) forthe word alignment computation.
In order to gen-erate the translation model, the parameter ?align-ment?
was fixed to ?grow-diag-final?
(defaultvalue), and the parameter ?reordering?
was fixed to?msd-bidirectional-fe?
as the best option, based onexperiments on the development set.In order to extract phrases (Koehn et al2003),the considered alignment was grow-diag-final.
Andthe parameter ?max-phrase-length?
was fixed to?7?
(default value), based on experiments on thedevelopment set.Finally, we have built a 5-gram language model,using the IRSTLM language modeling toolkit(Federico and Cettolo, 2007).Additionally, we have used the following toolsfor pre-processing the training corpus:tokenizer.perl, lowercase.perl, clean-corpus-n.perl.And the following ones for recasing, detokenizerand normalizing punctuation in the translation out-put: train-recaser.perl, recase.perl, detokenizer.perland normalize-punctuation.perl.In addition, we have used Freeling (Padr?
et al,2010) in some experiments, an open source libraryof natural language analyzers, but we did not im-prove our experiments by using Freeling.
We usedthis tool in order to extract factors for Spanishwords in order to train factored translation models.3 Corpora used in these experimentsFor the system development, only the free cor-pora distributed in the NAACL 2012 translationtask has been used, so any researcher can validatethese experiments easily.In order to train the translation model, we usedthe union of the Europarl corpus, the United Na-tions Organization (UNO) corpus and the NewsCommentary corpus.A 5-gram language model was built joining thefollowing monolingual corpora: Europarl, Newscommentary, United Nations and News Crawl.
Wehave not used the Gigaword corpus.In order to tune the model weights, the 2010 and2011 test set were used for development.
We didnot use the complete set, but a sentences selectionin order to improve the tuning process.
This selec-tion will be explained in section 5.The main characteristics of the corpora areshown in Table 1.
All the parallel corpora has beencleaned with clean-corpus-n.perl, lowercased withlowercase.perl and tokenized with tokenizer.perl.All these tools can be also free downloadedfrom http://www.statmt.org/wmt12/.We observed that the parallel corpora, speciallythe UNO corpus, have many repeated sentences.We noted that these repetitions can cause a badtraining.
So, after cleaning the parallel corporawith the clean-corpus-n.perl tool, we eliminated allrepetitions that appear more than 3 times in theparallel corpus.Table 1: Size of the corpora used in our experi-ments4 Previous experimentsSeveral experiments were carried out by usingdifferent number of sentences, as it is shown inTable 2.In these experiments, we used the 2010 test setfor tuning (news-test2010) and the 2011 test set fortest (news-test2011).
And a 5-gram language mod-el was built with the IRSTLM tool.
For evaluatingthe performance of the translation system, theBLEU (BiLingual Evaluation Understudy) metricOriginal sen-tencesTranslationModel (TM)Europarl (EU) 1,965,734UNO 11,196,913News commentary(NC) 157,302Total 13,319,949Total clean 9,530,335Total without repe-titions 4,907,778LanguageModel (LM)Europarl  2,218,201UNO 11,196,913News commentary(NC) 212,517News Crawl (NCR) 51,827,710Total 65,455,341Tuningnews-test2010 2,489news-test2011 3,003Total 5,492Total selected 4,500Test news-test2012 3,003339has been computed using the NIST tool (mteval.pl)(Papipeni et al, 2002).Firstly, we checked the contribution of UNOcorpus in the final result.
As it is shown in Table 2,the results improve when we add the UNO corpus,although this difference is small compared to theincreasing of number of sentences: with 1,643,597sentences we have a 28.24% BLEU and if we addaround other 8 million sentences more, the BLEUscore only increase 0.13 points (28.37%).Table 2: Previous experiments using news-test2010 for tuning and news-test2011 as test setWe observed that UNO corpus have a lot of re-peated sentences.
So, we decided to remove repeti-tions in the whole corpus.
With this action, weaimed to keep the UNO sentences that let us toimprove the BLEU score and, on the other hand, todelete the sentences that do not contribute in anyway, reducing the training time.We did some experiments deleting repetitions:allowing 5 repetitions, 3 repetitions and, finally, 1repetition (no repetitions).
Table 2 shows how theresults improve deleting more than 3 repetitions.So, finally, we improved the BLEU score from23.24% without UNO corpus to 28.37% adding theUNO and to 28.47% deleting all sentences repeat-ed more than 3 times.5 Selecting the development corpusWhen the system is trained, different modelweights must be tuned corresponding to the mainfour features of the system: translation model, lan-guage model, reordering model and word penalty.Initially, these weights are equal, but it is necessaryto optimize their values in order to get a better per-formance.
Development corpus is used to adapt thedifferent weights used in the translation process forcombining the different sources of information.The weight selection is performed by using theminimum error rate training (MERT) for log-linearmodel parameter estimation (Och, 2003).It is not demonstrated that the weights with bet-ter performance on the development set providebetter results on the unseen test set.
Because ofthis, this paper proposes a sentence selection tech-nique that allows selecting the sentences of thedevelopment set that have more similarity with thesentences to translate (source test set): if theweights are tuned with sentences more similar tothe sentence in the test set, the tuned weights willallow obtaining better translation results.We have considered two alternatives for compu-ting the similarity between a sentence and the testset.
As it will be shown, with these methods theresults improve.The first alternative consists of the similaritymethod proposed in (L?pez-Lude?a and San-Segundo, 2011), that computed a 3-gram languagemodel considering the source language sentencesfrom the test set.
After that, the system computesthe similarity of each source sentence in the valida-tion corpus considering the language model ob-tained in the first step and, finally, a threshold isdefined for selecting a subset with the higher simi-larity.The second method that we propose now is amodification of the first one.
With the formula ofthe first method, it was observed that, in some cas-es, the unigram probabilities had a relevant signifi-cance in the similarity, compared to 2-gram or 3-grams.
The system was selecting sentences thathave more unigrams that coincide with the sourcetest sentences.
However, these unigrams some-times were not part of ?good?
bigrams or trigrams.Moreover, it was detected that the previous strate-gy was selecting short sentences, leaving the longones out.Considering the previous aspects, a secondmethod was proposed and evaluated, trying to cor-rect these effects.
The proposal was to remove theunigram effect by normalizing the similarity meas-ure with the unigram probabilities of the word se-quence.
So, the similarity measure is computednow using the following equation:?
?==?=ninunignin PnPnsim1,1)log(1)log(1Training Deleting repetitionsNumberof sen-tencesBLEU(%)EU+NC NO 1,643,597 28.24EU+NC+UNO NO 9,530,335 28.37EU+NC+UNO YES (> 1) 2,112,968 28.12EU+NC+UNO YES (> 3) 4,907,778 28.47EU+NC+UNO YES (> 5) 6,270,441 28.28340Where Pn is the probability of the word ?n?
inthe sentence considering the language modeltrained with the source language sentences of thetest set.For example, if one sentence is ?A B C D?
(where each letter is a word of the validation sen-tence):))log()log()log()(log(41))log()log()log()(log(41_DCBABCDABCABAPPPPPPPPnormsim+++?+++=Each probability is extracted from the languagemodel calculated in the first step.
This similarity isthe negative of the source sentence perplexity giv-en the language model.With all the similarities organized in a sortedlist, it is possible to define a threshold selecting asubset with the higher similarity.
For example, cal-culating the similarity of all sentences in our de-velopment corpus (around 2,500 sentences) asimilarity histogram is obtained (Figure 1).0501001502002501 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97Figure 1: Similarity histogram of the source de-velopment sentences respect to the language modeltrained with the source language sentence of thetest setThis histogram indicates the number of sentenc-es inside each interval.
There are 100 different in-tervals: the minimum similarity is mapped into 0and the maximum one into 100.
As it is shown, thesimilarity distribution is very similar to a Gaussiandistribution.Finally, source development sentences with asimilarity lower than the threshold are eliminatedfrom the development set (the corresponding targetsentences are also removed).All the experiments have been carried out in theSpanish into English translation system, using thecorpora described in section 3 to generate thetranslation and language models.In order to evaluate the system, the test set of theEMNLP 2011 workshop on statistical machinetranslation (news-test2011) was considered.In order to adapt the different weights used inthe translation process, the test set of the ACL2010 workshop on statistical machine translation(news-test2010) has been used for weight tuning.The previous selection strategies allow filteringthis validation set, selecting the most similar sen-tences to the test set.Figure 2 and Table 3 show the different resultswith each number of selected sentences.Table 3: Results with different number of devel-opment sentences27,82828,228,428,628,829500 1000 1500 2000 2489NormalizedSimilarityORACLEBaselineBLEU (%)Figure 2: Results with different number of devel-opment sentencesFigure 2 shows that the BLEU score improveswhen the number of sentences of the developmentcorpus increases from 0 to around 1,500 sentenceswith both methods.
However, with more than1,500 sentences (selected with the first similaritycomputation method) and more than 2,000 (select-Sentences se-lected for de-velopmentBLEU results (%)NormalizedsimilaritySimilarity(L?pez-Lude?aand San-Segundo,2011)500 28.01 28.361,000 28.11 28.471,500 28.57 28.512,000 28.57 28.362,489 (Base-line) 28.47 28.47ORACLE 28.91 28.91341ed with the normalized similarity method), theBLEU score starts to decrease.
This decrementreveals that there is a subset of sentences that arequite different from the test sentences and they arenot appropriate for tuning the model weights.The best obtained result has been 28.57% BLEUwith 1,500 sentences of the development corpus,selected with the normalized similarity method.The improvement reached is 30% of the possibleimprovement (considering the ORACLE experi-ment).
This result is better than using the completedevelopment corpus (28.47% BLEU).When comparing both alternatives to computethe similarity between a sentence (from the valida-tion set) and a set of sentences (source sentencesfrom the test set), we can see that the normalizedsimilarity method allows a higher improvement.The main reason is that the similarity method se-lects sentences including information about similarunigrams, but sometimes, these unigrams are notpart of ?good?
bigrams or trigrams.
Moreover, thisstrategy selects short sentences, leaving the longones out.
When using the normalized similaritymethod, these two problems are reduced.6 ResultsTest set BLEU (%)BLEUcased(%)TER(%)Baseline news-test2011 28.37 25.76 59.9Best result news-test2011 28.57 25.98 59.8WMT12resultnews-test2012 31.80 28.90 57.9Table 4: Final results of the translation systemTable 4 shows the results with the 2011 test set:we have a 28.37% BLEU as baseline using thewhole corpora and finally we obtain a 28.57%BLEU with the deletion of repetitions and the sen-tences selection for tuning.With this configuration, we have obtained a31.8% BLEU with the 2012 test set as a result ofthe competition of this year.6.1 Other experimentsWe have carried out other experiments with the2012 test set: factored models, Minimum BayesRisk Decoding (MBR) and other sets for tuning.However, they did not finish before the competi-tion deadline.?
Factored models using FreelingFirstly, we have trained factored models inSpanish with Moses (Koehn and Hoang, 2007).We have only factored the source language (Span-ish) and, in order to obtain the factors for eachSpanish word, we have used Freeling(http://nlp.lsi.upc.edu/freeling/).When running the Freeling analyzer with aSpanish sentence and the output option ?tagged?,we obtain, for each word, an associated lemma, acoded tag with morphological and syntactic infor-mation, and a probability.
For instance, with thesentence ?la inflaci?n europea se desliz?
en losalimentos?, we obtain:word lemma tag probabilityla el DA0FS0 0.972inflaci?n inflaci?n NCFS000 1.000europea europeo AQ0FS0 0.900se se P00CN000 0.465desliz?
deslizar VMIS3S0 1.000en en SPS00 1.000los el DA0MP0 0.976alimentos alimento NCMP000 1.000Table 5: Freeling analyzer outputWe take advantage of the lemma (second col-umn) associated to each word and we use it as fac-tor.
So, the previous sentence is factorized as ?la|elinflaci?n|inflaci?n europea|europeo se|se des-liz?|deslizar en|en los|el alimentos|alimento?This way, two models are generated in the trans-lation process.
For the GIZA++ alignment we usedthe second factor (lemma) instead of the word.Results show that there is not improvement byusing Freeling.
BLEU score is a bit lower (30.95%in contrast to the 31.80% obtained withoutFreeling).
However, we want to continue doingexperiments with Freeling with other differentGIZA++ alignment options different to the defaultvalue ?grow-diag-final?.On the other hand, we want to prove differentsets for tuning.
When using factored models, thereare more weights to be adjusted and it is possiblethat 4,500 sentences are insufficient.342?
MBRThe use of Minumum Bayes Risk (MBR) (Ku-mar and Byrne, 2004) consists of, instead of select-ing the translation with the highest probability,minimum Bayes risk decoding selects the transla-tion that is most similar to the highest scoringtranslations.
The idea is to choose hypotheses thatminimize Bayes Risk as oppose to those that max-imize posterior probability.If we set up this option for decoding, the resultsimprove from 31.80% to 31.99%.?
Tuning with a 2008-2011 test set sen-tences selectionWe have also changed the set for tuning, includ-ing the 2008 and 2009 test set in addition to the2009 and 2010 sets.
With the four sets we havearound 10,000 sentences.
For tuning, we have se-lected 8,000 of these sentences with the normalizedsimilarity method explained in section 5.Table 6 shows that the results are worse.
How-ever, we have established the threshold based onprevious experiments with the 2010 and 2011 sets.Now, we should test different threshold with thefour sets in order to determine the best one.BLEU (%)BLEU cased(%)TER(%)WMT result 31.80 28.90 53.5Freeling 30.95 28.03 54.9MBR 31.99 29.06 53.4Tuning sets(2008-2011) 31.55 28.62 53.8Table 6: Results of the experiments after competi-tion7 ConclusionsThis paper has described the UPM statisticalmachine translation system for the Spanish-Englishtranslation task at the WMT12.
This system isbased on Moses.
We have checked that deletingrepetitions of the corpus, we can improve lightlythe results: we increase the BLEU score from28.37% with the whole corpora to 28.47% allow-ing only 3 repetitions of each sentence.
Althoughthis improvement is not significant (we have a con-fidence interval of ?0.35), we can say that we ob-tain a similar result by reducing very much thetraining time.We have also proposed a method for selectingthe sentences used for tuning the system.
This se-lection is based on the normalized similarity withthe source language test set.
With this techniquewe improve the BLEU score from 28.47% to28.57%.
Although this result is not significant, wecan appreciate an improving tendency by selectingthe training sentences.As a result of WMT12 challenge, we have ob-tained a 31.8% BLEU in Spanish-English transla-tion with the 2012 test set.
Our system takesaround 40 hours for training, 16 hours for tuning(with 5 minutes for the sentences selection) and 3hours to translate and to recase the test sentences inan 3.33 GHz Intel PC with 24 cores.Finally, we have presented other additional ex-periments after the competition.
We can improve abit more the results to 32% BLEU by using theMBR decoding option.AcknowledgmentsThe work leading to these results has receivedfunding from the European Union under grantagreement n?
287678.
It has also been supportedby TIMPANO (TIN2011-28169-C05-03),ITALIHA (CAM-UPM), INAPRA (MICINN,DPI2010-21247-C02-02), and MA2VICMR(Comunidad Aut?noma de Madrid, S2009/TIC-1542), Plan Avanza Consignos Exp N?
: TSI-020100-2010-489 and the European FEDER fundprojects.ReferencesE.
Bicici, D. Yuret, 2011.
Instance Selection forMachine Translation using Feature Decay Al-gorithms.
In Proceedings of the 6th Workshopon Statistical Machine Translation, pages 272?283.M.
Federico, M. Cettolo, 2007 Efficient Handlingof N-gram Language Models for Statistical Ma-chine Translation.
Proceedings of the SecondWorkshop on Statistical Machine Translation,pages 88?95.C.
Hui, H. Zhao, Y.
Song, B. Lu, 2010.
An Empiri-cal Study on Development Set Selection Strate-gy for Machine Translation Learning.
On FifthWorkshop on Statistical Machine Translation.343S.
Khadivi, H. Ney, 2005.
Automatic filtering ofbilingual corpora for statistical machine trans-lation.
In Natural Language Processing and In-formation Systems, 10th Int.
Conf.
onApplications of Natural Language to Infor-mation Systems, volume 3513 of Lecture Notesin Computer Science, pages 263?274, Alicante,Spain, June.
Springer.P.
Koehn and H. Hoang, 2007 Factored Transla-tion Models, Conference on Empirical Methodsin Natural Language Processing (EMNLP),Prague, Czech Republic.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin, E. Herbst, 2007.
Moses: OpenSource Toolkit for Statistical Machine Transla-tion, Annual Meeting of the Association forComputational Linguistics (ACL), demonstra-tion session, Prague, Czech Republic.P.
Koehn, F.J. Och, D. Marcu, 2003.
StatisticalPhrase-based translation.
Human LanguageTechnology Conference 2003 (HLT-NAACL2003), Edmonton, Canada, pp.
127-133, May2003.S.
Kumar and W. J. Byrne.
2004.
Minimum bayes-risk decoding for statistical machine transla-tion.
In HLT-NAACL, pages 169?176.V.
L?pez-Lude?a and R. San-Segundo.
2011.UPM system for the translation task.
In Pro-ceedings of the Sixth Workshop on StatisticalMachine Translation.Y.
L?, J. Huang, Q. Liu.
2007.
Improving statisti-cal machine translation performance by train-ing data selection and optimization.
InProceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Pro-cessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 343?350.A.
Mandal, D. Vergyri, W. Wang, J. Zheng, A.Stolcke, G. Tur, D.  Hakkani-Tur and N.F.Ayan.
2008.
Efficient data selection for ma-chine translation.
In Spoken Language Tech-nology Workshop.
SLT 2008.
IEEE, pages 261?264.F.
J. Och, 2003.
Minimum error rate training instatistical machine translation.
In Proceedingsof the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167,Sapporo, Japan, July.
Association for Computa-tional Linguistics.F.
J. Och, H. Ney, 2003.
A systematic comparisonof various alignment models.
ComputationalLinguistics, Vol.
29, No.
1 pp.
19-51, 2003.L.
Padr?, M. Collado, S. Reese, M. Lloberes,  I.Castell?n, 2010.
FreeLing 2.1: Five Years ofOpen-Source Language Processing ToolsProceedings of 7th Language Resources andEvaluation Conference (LREC 2010), ELRALa Valletta, Malta.
May.K.
Papineni, S. Roukos, T. Ward, W.J.
Zhu.
2002.BLEU: a method for automatic evaluation ofmachine translation.
40th Annual Meeting ofthe Association for Computational Linguistics(ACL), Philadelphia, PA, pp.
311-318.N.
Ueffing, G. Haffari, A. Sarkar, 2007.Transductive learning for statistical machinetranslation.
On ACL Second Workshop on Sta-tistical Machine Translation.344
