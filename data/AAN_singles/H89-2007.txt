Modelling Non-verbal Sounds for Speech RecognitionWayne Ward 1School of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTWhen speech understanding systems are used in real applications, they encounter incidental noise generated by thespeaker and the environment.
Such noises can cause serious problems for speech recognizers not designed to copewith them.
We attempt to model these noises by training HMM "noise words" to match classes of noises.
The noisewords were incorporated into the Sphinx system and performance compared to the system without noise words.Initial results uggest that the technique does increase system performance significantly.INTRODUCTIONRecent experiments performed by two groups of researchers at CMU have gathered ata on subjects using speechrecognizers in office-like environments (Rudnicky, et al, 1989, Stern & Acero, 1989).
These experiments arepresented by the authors in these proceedings.
Among other things, they show that non-verbal events (non-stationarynoises) do create serious problems for speech recognizers.
These sounds are generated both by the speaker and bythe environment.
Examples of noise generated by the speaker are breath noises, lip smacks, paper ustles, filledpauses, cough, clearing throat, etc.
Environmental noise can be phone rings, door slams, other speakers in thebackground, typing, etc.
We attempt to explicitly model classes of noise represented by these events in the contextof an HMM based speech recognizer (Sphinx).
Subjects were recorded performing the two tasks, spreadsheet andcensus data (alphanumeric).
A significant percentage (approx 10% overall in each task) of the utterances containphenomena of the type mentioned above.
The utterances were transcribed using a set of noise words to representnon-signal events in the recording.
Fourteen noise words were used."
AH, BEEP, BREATI-INOISE,CLEAR_THROAT, COUGH, DOOR_SLAM, MOUTFLNOISE, MUMBLE, RUSTLE, PHONE_RING, SNIFF,SNEEZE, TAP and THUMP.
For each of these noise classes, a phone was added to the phone set and a wordconsisting of only that phone was added to the lexicon.
The standard Sphinx training routines were then used totrain context dependent models for all phones except hose representing noise.
Context independent models wereused for the noise phones.
The simple word models for noise give no context since they are single tokens, and wedid not use between-word models.
For recognition, oise words are treated like Silence words.
They are allowed tooccur after any word, including themselves and other noise words.
We use the Sphinx recognizer with only minormodifications toimplement transitions to noise words and to allow utterances that are only noise or Silence.SPREADSHEET TASKAlex Rudnicky and Michelle Sakamoto gathered a large corpus of examples of users performing a spreadsheet taskusing voice (Rudnicky et al 1989).
They used an operational speech recognition system, not a PNAMBICparadigm.
The subjects poke in a spontaneous manner and were recorded using a Sennheiser close talkingmicrophone.
The input to their system was continuous, recognition wasn't started by pressing a key just before1This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No.5167, under contract number N00039-85-C-0163.
The views and conclusions contained in this document are thoseof the authors and should not be interpreted as representing the official policies, either expressed or implied, of theDefense Advanced Research Projects Agency or the US Government.47speaking.
The data used in the present experiment consists of 15 sessions each from 7 speakers (they subsequentlyrecorded more).
A session represents approximately 100 utterances.
These utterances were divided into a trainingset and testing set which were then transcribed using noise words.
Some "utterances" contained only real words,some contained words and noise, and some were noise alone.
Noises loud enough to pass background thresholdswere recognized as words even ff the subject were not speaking.
The recognizer used in their experiment wastrained on 4000 read utterances from spreadsheet and calculator tasks.
In order to avoid becomming speakerdependent, we used the 4000 speaker-independent read utterances along with 416 noise-word containingspontaneous tterances as our training set.
The test set was 7185 spontaneous tterances from the seven speakers(not including any from the training set).
The 416 noise utterances in the test set came from only five speakers, sotwo of the speakers in the test set had not been seen in the training.
The original recognizer, Sphinx (SPHX) and thenoise word recognizer, Phoenix (PHNX) were both run on the test set.
Table 1 shows the word and sentence rrorrates for each type of utterances.WORDS and NOISESPHX i PHNX % Reductionsent 47.2 26.9 43.0word 24.6 11.1 54.9sentNOISE AloneSPHX PHNX % Reduction81.4 3.8 95.3ALL Utterances Containing NoiseSPHX PHNX % Reductionsent 70.0 14.2 79.7sentwordTable 1: Error Rates for Spread Sheet TaskWELL-FORMEDSPHX PHNX % Reduction22.1 20.19.6 8.5The WORDS and NOISE results reflect hose utterances whose transcripts contained both real words and at leastone noise word.
In this condition, use of noise words reduced the sentence rror rate by 43 percent.
The NOISEresults are for those utterances whose transcripts consist solely of noise words.
The noise word models were veryeffective at discriminating these events from real speech.
Less than four percent produced real word hypotheses.
Forthe original system, 81.4 percent of these noises resulted in hypothesized words from the lexicon.
The two previouscategories are then combined to give errors for all utterances containing noise.
For this test set, the number ofsentence rrors for utterances containing noise was reduced by a factor of five by using noise words (290 vs 41).The WELL-FORMED condition are those utterances whose transcripts contained only real words from the lexicon.48This was run as a check that using noise words did not degrade performance on clean input.
As can be seen, thesystem with noise words performed at least as weU as the original system on clean input.Census Data TaskRichard Stern and Alejandro Acero gathered ata on subjects entering census data (Stern & Acero, 1989).
Subjectswere asked to speU their name and street address, etc.
This is largely an alphanumeric task.
The recordings weremade in a booth partitioned from the rest of the office, and a Sennheiser close talking mike was used.
In this task,subjects were prompted when to speak as opposed to the spread sheet ask where recording was continuous.
Thus,there were no utterances that contained only noise.
As before, the utterances were transcribed using noise words,and the system was trained using these models.
The error rate for the utterances containing words and noise isshown in Table 2.sentwordTable 2: Error Rates for Census Data TaskWORDS and NOISESPHX PHNX %Reduction82.9 63.4 23.533.9 20.91 38.3In this task also, the use of noise words significantly reduced error rate for noisyinput.
As before, there was also nodegredafion ofperformance for clean input.Conc lus ionThe experiments here were quick studies designed to test the feasibility of using HMM models in the standardframework to model non-stationary noise in the input.
The results suggest that the noises that are problematic forclose talking mikes in office environments can be modelled with these techniques.
We intend to extend and refinethese models to give better models of a wider range of events.
Much more can also be done with the way that themodels are used.
Currently, they are allowed to follow all words with no difference in probability.
Whileenvironmental noise probably is randomly interspersed throughout the signal, this is not true of user generated noise.These noises are more probable at some places than others.
Breath noises and rustles are far more common at thebeginning and end of utterances, for example.
Statistics on occurences of these events can be incorporated into thesearch as a part of the language model.
However, this ability requires that noise words be reliably distinguishedfrom each other.
In the data presented, noise words were stripped out for the analysis.
Insertions and substitutions ofnoise words were not counted as incorrect.
While noise words were not often confused with real words, they wereoften confused with other noise words.
Better modelling of these events will be required before their "languagemodel" probabilities can be reliably applied.49Referencesi.
Rudnicky, A.I., Sakamoto, M.H., and Polifroni, J.H.
Evaluat ingSpoken Language Interaction.
Proceedings of the DARPA Speech andNatural Language Workshop, 1987.2.
Stern, R.M.
and Acero, A. Acoustical  Pre-Processing for RobustSpeech Recognition.
Proceedings of the DARPA Speech and NaturalLanguage Workshop, 1987.50
