Use of Deep Linguistic Features for the Recognition and Labeling ofSemantic ArgumentsJohn ChenDepartment of Computer ScienceColumbia UniversityNew York, NY 10027jchen@cs.columbia.eduOwen RambowDepartment of Computer ScienceColumbia UniversityNew York, NY 10027rambow@cs.columbia.eduAbstractWe use deep linguistic features to predictsemantic roles on syntactic arguments,and show that these perform considerablybetter than surface-oriented features.
Wealso show that predicting labels from a?lightweight?
parser that generates deepsyntactic features performs comparably tousing a full parser that generates only sur-face syntactic features.1 IntroductionSyntax mediates between surface word order andmeaning.
The goal of parsing (syntactic analysis)is ultimately to provide the first step towards giv-ing a semantic interpretation of a string of words.So far, attention has focused on parsing, because thesemantically annotated corpora required for learn-ing semantic interpretation have not been available.The completion of the first phase of the PropBank(Kingsbury et al, 2002) represents an importantstep.
The PropBank superimposes an annotation ofsemantic predicate-argument structures on top of thePenn Treebank (PTB) (Marcus et al, 1993; Marcuset al, 1994).
The arc labels chosen for the argumentsare specific to the predicate, not universal.In this paper, we find that the use of deep lin-guistic representations to predict these semantic la-bels are more effective than the generally moresurface-syntax representations previously employed(Gildea and Palmer (2002)).
Specifically, we showthat the syntactic dependency structure that resultsloadarg0Johnarg2hayarg1truckFigure 1: PropBank-style semantic representationfor both John loaded the truck with hay and Johnloaded hay into the truckfrom the extraction of a Tree Adjoining Grammar(TAG) from the PTB, and the features that accom-pany this structure, form a better basis for determin-ing semantic role labels.
Crucially, the same struc-ture is also produced when parsing with TAG.
Wesuggest that the syntactic representation chosen inthe PTB is less well suited for semantic process-ing than the other, deeper syntactic representations.In fact, this deeper representation expresses syntac-tic notions that have achieved a wide acceptanceacross linguistic frameworks, unlike the very partic-ular surface-syntactic choices made by the linguistswho created the PTB syntactic annotation rules.The outline of this paper is as follows.
In Sec-tion 2 we introduce the PropBank and describe theproblem of predicting semantic tags.
Section 3presents an overview of our work and distinguishesit from previous work.
Section 4 describes themethod used to produce the TAGs that are the basisof our experiments.
Section 5 specifies how train-ing and test data that are used in our experimentsare derived from the PropBank.
Next, we give re-sults on two sets of experiments.
Those that predictsemantic tags given gold-standard linguistic infor-mation are described in Section 6.
Those that doprediction from raw text are described in Section 7.Finally, in Section 8 we present concluding remarks.2 The PropBank and the Labeling ofSemantic RolesThe PropBank (Kingsbury et al, 2002) annotatesthe PTB with dependency structures (or ?predicate-argument?
structures), using sense tags for eachword and local semantic labels for each argumentand adjunct.
Argument labels are numbered andused consistently across syntactic alternations forthe same verb meaning, as shown in Figure 1.
Ad-juncts are given special tags such as TMP (for tem-poral), or LOC (for locatives) derived from the orig-inal annotation of the Penn Treebank.
In additionto the annotated corpus, PropBank provides a lexi-con which lists, for each meaning of each annotatedverb, its roleset, i.e., the possible arguments in thepredicate and their labels.
As an example, the entryfor the verb kick, is given in Figure 2.
The notionof ?meaning?
used is fairly coarse-grained, typicallymotivated from differing syntactic behavior.
Sinceeach verb meaning corresponds to exactly one role-set, these terms are often used interchangeably.
Theroleset alo includes a ?descriptor?
field which is in-tended for use during annotation and as documenta-tion, but which does not have any theoretical stand-ing.
Each entry also includes examples.
Currentlythere are frames for about 1600 verbs in the corpus,with a total of 2402 rolesets.Since we did not yet have access to a corpus an-notated with rolesets, we concentrate in this paperon predicting the role labels for the arguments.
Itis only once we have both that we can interpret therelation between predicate and argument at a veryfine level (for example, truck in he kicked the truckwithhay as the destination of the loading action).
Wewill turn to the problem of assigning rolesets to pred-icates once the data is available.
We note though thatpreliminary investigations have shown that for about65% of predicates (tokens) in the WSJ, there is onlyone roleset.
In a further 7% of predicates (tokens),the set of semantic labels on the arguments of thatpredicate completely disambiguates the roleset.ID kick.01Name drive or impel with the footVN/Levin 11.4-2, 17.1, 18.1, 23.2classes 40.3.2, 49RolesNumber Description0 Kicker1 Thing kicked2 Instrument(defaults to foot)Example [John]i tried [*trace*i]ARG0 to kick [thefootball]ARG1Figure 2: The unique roleset for kick3 OverviewGildea and Palmer (2002) show that semantic rolelabels can be predicted given syntactic features de-rived from the PTB with fairly high accuracy.
Fur-thermore, they show that this method can be usedin conjunction with a parser to produce parses anno-tated with semantic labels, and that the parser out-performs a chunker.
The features they use in theirexperiments can be listed as follows.Head Word (HW.)
The predicate?s head word aswell as the argument?s head word is used.Phrase Type.
This feature represents the typeof phrase expressing the semantic role.
In Figure 3phrase type for the argument prices is NP.Path.
This feature captures the surface syntacticrelation between the argument?s constituent and thepredicate.
See Figure 3 for an example.Position.
This binary feature represents whetherthe argument occurs before or after the predicate inthe sentence.Voice.
This binary feature represents whether thepredicate is syntactically realized in either passive oractive voice.Notice that for the exception of voice, the fea-tures solely represent surface syntax aspects ofthe input parse tree.
This should not be takento mean that deep syntax features are not impor-tant.
For example, in their inclusion of voice,Gildea and Palmer (2002) note that this deep syntaxfeature plays an important role in connecting seman-tic role with surface grammatical function.Aside from voice, we posit that other deep lin-guistic features may be useful to predict semanticrole.
In this work, we explore the use of more gen-eral, deeper syntax features.
We also experimentwith semantic features derived from the PropBank.fallingarePricesNNSSNP VPVBP VPVBGFigure 3: In the predicate argument relationship be-tween the predicate falling and the argument prices,the path feature is VBG?VP?VP?S?NP.Our methodology is as follows.
The first stage en-tails generating features representing different lev-els of linguistic analysis.
This is done by first auto-matically extracting several kinds of TAG from thePropBank.
This may in itself generate useful fea-tures because TAG structures typically relate closelysyntactic arguments with their corresponding pred-icate.
Beyond this, our TAG extraction procedureproduces a set of features that relate TAG structureson both the surface-syntax as well as the deep-syntaxlevel.
Finally, because a TAG is extracted from thePropBank, we have a set of semantic features de-rived indirectly from the PropBank through TAG.The second stage of our methodology entails usingthese features to predict semantic roles.
We firstexperiment with prediction of semantic roles givengold-standard parses from the test corpus.
We sub-sequently experiment with their prediction given rawtext fed through a deterministic dependency parser.4 Extraction of TAGs from the PropBankOur experiments depend upon automatically extract-ing TAGs from the PropBank.
In doing so, we fol-low the work of others in extracting grammars ofvarious kinds from the PTB, whether it be TAG(Xia, 1999; Chen and Vijay-Shanker, 2000; Chi-ang, 2000), combinatory categorial grammar (Hock-enmaier and Steedman, 2002), or constraint depen-dency grammar (Wang and Harper, 2002).
We willdiscuss TAGs and an important principle guidingtheir formation, the extraction procedure from thePTB that is described in (Chen, 2001) including ex-tensions to extract a TAG from the PropBank, andfinally the extraction of deeper linguistic featuresVPVPareVBPSVPNPfallingVBGNPNNSPricesFigure 4: Parse tree associated with the sentencePrices are falling has been fragmented into threetree frames.from the resulting TAG.A TAG is defined to be a set of lexicalized el-ementary trees (Joshi and Schabes, 1991).
Theymay be composed by several well-defined opera-tions to form parse trees.
A lexicalized elementarytree where the lexical item is removed is called atree frame or a supertag.
The lexical item in thetree is called an anchor.
Although the TAG for-malism allows wide latitude in how elementary treesmay be defined, various linguistic principles gener-ally guide their formation.
An important principleis that dependencies, including long-distance depen-dencies, are typically localized the same elementarytree by appropriate grouping of syntactically or se-mantically related elements.The extraction procedure fragments a parse treefrom the PTB that is provided as input into elemen-tary trees.
See Figure 4.
These elementary trees canbe composed by TAG operations to form the origi-nal parse tree.
The extraction procedure determinesthe structure of each elementary tree by localizingdependencies through the use of heuristics.
Salientheuristics include the use of a head percolation ta-ble (Magerman, 1995), and another table that distin-guishes between complements and adjunct nodes inthe tree.
For our current work, we use the head per-colation table to determine heads of phrases.
Also,we treat a PropBank argument (ARG0 .
.
.
ARG9) asa complement and a PropBank adjunct (ARGM?s) asan adjunct when such annotation is available.1 Oth-erwise, we basically follow the approach of (Chen,2001).2Besides introducing one kind of TAG extraction1The version of the PropBank we are using is not fully an-notated with semantic role information, although the most com-mon predicates are.2Specifically, CA1.procedure, (Chen, 2001) introduces the notion ofgrouping linguistically-related extracted tree framestogether.
In one approach, each tree frame is decom-posed into a feature vector.
Each element of this vec-tor describes a single linguistically-motivated char-acteristic of the tree.The elements comprising a feature vector arelisted in Table 1.
Each elementary tree is decom-posed into a feature vector in a relatively straightfor-ward manner.
For example, the POS feature is ob-tained from the preterminal node of the elementarytree.
There are also features that specify the syntac-tic transformations that an elementary tree exhibits.Each such transformation is recognized by struc-tural pattern matching the elementary tree against apattern that identifies the transformation?s existence.For more details, see (Chen, 2001).Given a set of elementary trees which compose aTAG, and also the feature vector corresponding toeach tree, it is possible to annotate each node rep-resenting an argument in the tree with role informa-tion.
These are syntactic roles including for examplesubject and direct object.
Each argument node is la-beled with two kinds of roles: a surface syntacticrole and a deep syntactic role.
The former is ob-tained through determining the position of the nodewith respect to the anchor of the tree using the usu-ally positional rules for determining argument statusin English.
The latter is obtained from the formerand also from knowledge of the syntactic transfor-mations that have been applied to the tree.
For ex-ample, we determine the deep syntactic role of a wh-moved element by ?undoing?
the wh-movement byusing the trace information in the PTB.The PropBank contains all of the notation of thePenn Treebank as well as semantic notation.
For ourcurrent work, we extract two kinds of TAG from thePropBank.
One grammar, SEM-TAG, has elemen-tary trees annotated with the aforementioned syntac-tic information as well as semantic information.
Se-mantic information includes semantic role as well assemantic subcategorization information.
The othergrammar, SYNT-TAG, differs from SEM-TAG onlyby the absence of any semantic role information.Table 1: List of each feature in a feature vector andsome possible values.Feature ValuesPart of speech DT, NN, VB, RB, .
.
.Subcategorization NP , NP S , ?, .
.
.MaxProj S, NP, VP, .
.
.Modifyee NP, VP, S, .
.
.Direction LEFT, RIGHTCo-anchors { of }, { by }, ?, .
.
.Declarative TRUE, FALSEEmpty Subject TRUE, FALSEComplementizer TRUE, FALSEPassive TRUE, FALSEBy-Passive TRUE, FALSETopicalized-X TRUE, FALSEWh-movement-X-Y TRUE, FALSESubject-Aux Inversion TRUE, FALSERelative Clause TRUE, FALSE5 CorporaFor our experiments, we use a version of the Prop-Bank where the most commonly appearing predi-cates have been annotated, not all.
Our extractedTAGs are derived from Sections 02-21 of the PTB.Furthermore, training data for our experiments arealways derived from these sections.
Section 23 isused for test data.The entire set of semantic roles that are foundin the PropBank are not used in our experiments.In particular, we only include as semantic rolesthose instances in the propbank such that in the ex-tracted TAG they are localized in the same elemen-tary tree.
As a consequence, adjunct semantic roles(ARGM?s) are basically absent from our test cor-pus.
Furthermore, not all of the complement seman-tic roles are found in our test corpus.
For example,cases of subject-control PRO are ignored becausethe surface subject is found in a different tree framethan the predicate.
Still, a large majority of com-plement semantic roles are found in our test corpus(more than 87%).6 Semantic Roles from Gold-StandardLinguistic InformationThis section is devoted towards evaluating differentfeatures obtained from a gold-standard corpus in thetask of determining semantic role.
We use the fea-ture set mentioned in Section 3 as well as featuresderived from TAGs mentioned in Section 4.
In thissection, we detail the latter set of features.
We thendescribe the results of using different feature sets.These experiments are performed using the C4.5 de-cision tree machine learning algorithm.
The stan-dard settings are used.
Furthermore, results are al-ways given using unpruned decision trees becausewe find that these are the ones that performed thebest on a development set.These features are determined during the extrac-tion of a TAG:Supertag Path.
This is a path in a tree frame fromits preterminal to a particular argument node in a treeframe.
The supertag path of the subject of the right-most tree frame in Figure 4 is VBG?VP?S?NP.Supertag.
This can be the tree frame correspond-ing to either the predicate or the argument.Srole.
This is the surface-syntactic role of an ar-gument.
Example of values include 0 (subject) and1 (direct object).Ssubcat.
This is the surface-syntactic subcate-gorization frame.
For example, the ssubcat cor-responding to a transitive tree frame would beNP0 NP1.
PPs as arguments are always annotatedwith the preposition.
For example, the ssubcat forthe passive version of hit would be NP1 NP2(by).Drole.
This is the deep-syntactic role of an argu-ment.
Example of values include 0 (subject) and 1(direct object).Dsubcat.
This is the deep-syntactic subcate-gorization frame.
For example, the dsubcat cor-responding to a transitive tree frame would beNP0 NP1.
Generally, PPs as arguments are anno-tated with the preposition.
For example, the dsub-cat for load is NP0 NP1 NP2(into).
The exceptionis when the argument is not realized as a PP whenthe predicate is realized in a non-syntactically trans-formed way.
For example, the dsubcat for the pas-sive version of hit would be NP0 NP1.Semsubcat.
This is the semantic subcategoriza-tion frame.We first experiment with the set of features de-scribed in Gildea and Palmer (2002): Pred HW,Arg HW, Phrase Type, Position, Path, Voice.
Callthis feature set GP0.
The error rate, 10.0%, is lowerthan that reported by Gildea and Palmer (2002),17.2%.
This is presumably because our training andtest data has been assembled in a different manneras mentioned in Section 5.Our next experiment is on the same set of fea-tures, with the exception that Path has been replacedwith Supertag Path.
(Feature set GP1).
The er-ror rate is reduced from 10.0% to 9.7%.
This isstatistically significant (t-test, p < 0.05), albeit asmall improvement.
One explanation for the im-provement is that Path does not generalize as wellas Supertag path does.
For example, the path fea-ture value VBG?VP?VP?S?NP reflects surface sub-ject position in the sentence Prices are falling but sodoes VBG?VP?S?NP in the sentence Sellers regretprices falling.
Because TAG localizes dependencies,the corresponding values for Supertag path in thesesentences would be identical.We now experiment with our surface syntax fea-tures: Pred HW, Arg HW, Ssubcat, and Srole.
(Feature set SURFACE.)
Its performance on SEM-TAG is 8.2% whereas its performance on SYNT-TAG is 7.6%, a tangible improvement over previ-ous models.
One reason for the improvement couldbe that this model is assigning semantic labels withknowledge of the other roles the predicate assigns,unlike previous models.Our next experiment involves using deep syntaxfeatures: Pred HW, Arg HW, Dsubcat, and Drole.
(Feature set DEEP.)
Its performance on both SEM-TAG and SYNT-TAG is 6.5%, better than previousmodels.
Its performance is better than SURFACEpresumably because syntactic transformations aretaken to account by deep syntax features.
Note alsothat the transformations which are taken into ac-count are a superset of the transformations taken intoaccount by Gildea and Palmer (2002).This experiment considers use of semantic fea-tures: Pred HW, Arg HW, Semsubcat, and Drole.
(Feature set SEMANTIC.)
Of course, there are onlyresults for SEM-TAG, which turns out to be 1.9%.This is the best performance yet.In our final experiment, we use supertag features:Pred HW, Arg HW, Pred Supertag, Arg Su-Table 2: Error rates of models which label semanticroles on gold-standard parses.
Each model is basedon its own feature sets, with features coming from aparticular kind of extracted grammar.Feature Set SEM-TAG SYNT-TAGGP0 10.0 10.0GP1 9.7 9.7SURFACE 8.2 7.6DEEP 6.5 6.5SEMANTIC 1.9SUPERTAG 2.8 7.4pertag, Drole.
(Feature set SUPERTAG.)
The errorrates are 2.8% for SEM-TAG and 7.4% for SYNT-TAG.
Considering SEM-TAG only, this model per-forms better than its corresponding DEEP model,probably because supertag for SEM-TAG includecrucial semantic information.
Considering SYNT-TAG only, this model performs worse than its cor-responding DEEP model, presumably because ofsparse data problems when modeling supertags.This sparse data problem is also apparent by com-paring the model based on SEM-TAG with the cor-responding SEM-TAG SEMANTICmodel.7 Semantic Roles from Raw TextIn this section, we are concerned with the problem offinding semantic arguments and labeling them withtheir correct semantic role given raw text as input.
Inorder to perform this task, we parse this raw text us-ing a combination of supertagging and LDA, whichis a method that yields partial dependency parses an-notated with TAG structures.
We perform this taskusing both SEM-TAG and SYNT-TAG.
For the for-mer, after supertagging and LDA, the task is accom-plished because the TAG structures are already an-notated with semantic role information.
For the lat-ter, we use the best performing model from Section 6in order to find semantic roles given syntactic fea-tures from the parse.7.1 SupertaggingSupertagging (Bangalore and Joshi (1999)) is thetask of assigning a single supertag to each wordgiven raw text as input.
For example, given the sen-tence Prices are falling, a supertagger might returnthe supertagged sentence in Figure 4.
Supertaggingreturns an almost-parse in the sense that it is per-forming much parsing disambiguation.
The typi-cal technique to perform supertagging is the trigrammodel, akin to models of the same name for part-of-speech tagging.
This is the technique that we usehere.Data sparseness is a significant issuewhen supertagging with extracted grammar(Chen and Vijay-Shanker (2000)).
For this reason,we smooth the emit probabilities P (w|t) in thetrigram model using distributional similarity fol-lowing Chen (2001).
In particular, we use Jaccard?scoefficient as the similarity metric with a similaritythreshold of 0.04 and a radius of 25 because thesewere found to attain optimal results in Chen (2001).Training data for supertagging is Sections 02-21of the PropBank.
A supertagging model based onSEM-TAG performs with 76.32% accuracy on Sec-tion 23.
The corresponding model for SYNT-TAGperforms with 80.34% accuracy.
Accuracy is mea-sured for all words in the sentence including punc-tuation.
The SYNT-TAG model performs betterthan the SEM-TAG model, understandably, becauseSYNT-TAG is the simpler grammar.7.2 LDALDA is an acronym for Lightweight DependencyAnalyzer (Srinivas (1997)).
Given as input a su-pertagged sequence of words, it outputs a partial de-pendency parse.
It takes advantage of the fact thatsupertagging provides an almost-parse in order todependency parse the sentence in a simple, deter-ministic fashion.
Basic LDA is a two step procedure.The first step involves linking each word serving asa modifier with the word that it modifies.
The sec-ond step involves linking each word serving as an ar-gument with its predicate.
Linking always only oc-curs so that grammatical requirements as stipulatedby the supertags are satisfied.
The version of LDAthat is used in this work differs from Srinivas (1997)in that there are other constraints on the linking pro-cess.3 In particular, a link is not established if itsexistence would create crossing brackets or cyclesin the dependency tree for the sentence.We perform LDA on two versions of Section 23,3We thank Srinivas for the use of his LDA software.Table 3: Accuracy of dependency parsing usingLDA on supertagged input for different kinds of ex-tracted grammar.Grammar Recall Precision FSEM-TAG 66.16 74.95 70.58SYNT-TAG 74.79 80.35 77.47one supertagged with SEM-TAG and the other withSYNT-TAG.
The results are shown in Table 3.
Eval-uation is performed on dependencies excluding leaf-node punctuation.
Each dependency is evaluated ac-cording to both whether the correct head and depen-dent is related as well as whether they both receivethe correct part of speech tag.
The F-measure scores,in the 70% range, are relatively low compared toCollins (1999) which has a corresponding score ofaround 90%.
This is perhaps to be expected becauseCollins (1999) is based on a full parser.
Note alsothat the accuracy of LDA is highly dependent on theaccuracy of the supertagged input.
This explains, forexample, the fact that the accuracy on SEM-TAGsupertagged input is lower than the accuracy withSYNT-TAG supertagged input.7.3 Semantic Roles from LDA OutputThe output of LDA is a partial dependency parse an-notated with TAG structures.
We can use this outputto predict semantic roles of arguments.
The mannerin which this is done depends on the kind of gram-mar that is used.
The LDA output using SEM-TAGis already annotated with semantic role informationbecause it is encoded in the grammar itself.
On theother hand, the LDA output using SYNT-TAG con-tains strictly syntactic information.
In this case, weuse the highest performing model from Section 6 inorder to label arguments with semantic roles.Evaluation of prediction of semantic roles takesthe following form.
Each argument labeled by a se-mantic role in the test corpus is treated as one trial.Certain aspects of this trial are always checked forcorrectness.
These include checking that the seman-tic role and the dependency-link are correct.
Thereare other aspects which may or may not be checked,depending on the type of evaluation.
One aspect,?bnd,?
is whether or not the argument?s bracketingas specified in the dependency tree is correct.
An-Table 4: Evaluation of semantic argument recogni-tion on SEM-TAG corpus via supertag and LDA.Task: determine Recall Precision Fbase + arg 0.39 0.84 0.53base + bnd 0.28 0.61 0.38base + bnd + arg 0.28 0.61 0.38other aspect, ?arg,?
is whether or not the headwordof the argument is chosen to be correct.Table 4 show the results when we use SEM-TAGin order to supertag the input and perform LDA.When the boundaries are found, finding the headword additionally does not result in a decrease ofperformance.
However, correctly identifying thehead word instead of the boundaries leads to an im-portant increase in performance.
Furthermore, notethe low recall and high precision of the ?base +arg?
evaluation.
In part this is due to the natureof the PropBank corpus that we are using.
In par-ticular, because not all predicates in our version ofthe PropBank are annotated with semantic roles, thesupertagger for SEM-TAG will sometimes annotatetext without semantic roles when in fact it shouldcontain them.Table 5 shows the results of first supertaggingthe input with SYNT-TAG and then using a modeltrained on the DEEP feature set to annotate the re-sulting syntactic structure with semantic roles.
Thistwo-step approach greatly increases performanceover the corresponding SEM-TAG based approach.These results are comparable to the results fromGildea and Palmer (2002), but only roughly becauseof differences in corpora.
Gildea and Palmer (2002)achieve a recall of 0.50, a precision of 0.58, andan F-measure of 0.54 when using the full parser ofCollins (1999).
They also experiment with using achunker which yields a recall of 0.35, a precision of0.50, and an F-measure of 0.41.8 ConclusionsWe have presented various alternative approaches topredicting PropBank role labels using forms of lin-guistic information that are deeper than the PTB?ssurface-syntax labels.
These features may eitherbe directly derived from a TAG, such as Supertagpath, or indirectly via aspects of supertags, suchTable 5: Evaluation of semantic argument recogni-tion on SYNT-TAG corpus via supertag and LDA.Task: determine Recall Precision Fbase + arg 0.65 0.75 0.70base + bnd 0.48 0.55 0.51base + bnd + arg 0.48 0.55 0.51as deep syntactic features like Drole.
These arefound to produce substantial improvements in ac-curacy.
We believe that such improvement is dueto these features better capturing the syntactic infor-mation that is relevant for the task of semantic la-beling.
Also, these features represent syntactic cate-gories about which there is a broad consensus in theliterature.
Therefore, we believe that our results areportable to other frameworks and differently anno-tated corpora such as dependency corpora.We also show that predicting labels from a?lightweight?
parser that generates deep syntacticfeatures performs comparably to using a full parserthat generates only surface syntactic features.
Im-provements along this line may be attained by use ofa full TAG parser, such as Chiang (2000) for exam-ple.AcknowledgmentsThis paper is based upon work supported by the Na-tional Science Foundation under the KDD programthrough a supplement to Grant No.
IIS-98-17434.Any opinions, findings, and conclusions or recom-mendations expressed in this paper are those of theauthors and do not necessarily reflect the views ofthe National Science Foundation.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?266.John Chen and K. Vijay-Shanker.
2000.
Automated ex-traction of tags from the penn treebank.
In Proceed-ings of the Sixth International Workshop on ParsingTechnologies, pages 65?76.John Chen.
2001.
Towards Efficient Statistical ParsingUsing Lexicalized Grammatical Information.
Ph.D.thesis, University of Delaware.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of the the 38th Annual Meeting of the As-sociation for Computational Linguistics, pages 456?463, Hong Kong.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Daniel Gildea and Martha Palmer.
2002.
The neces-sity of parsing for predicate argument recognition.
Inacl02, pages 239?246, Philadelphia, PA.Julia Hockenmaier and Mark Steedman.
2002.
Acquir-ing compact lexicalized grammars from a cleaner tree-bank.
In Proceedings of the Third International Con-ference on Language Resources and Evaluation, LasPalmas.Aravind K. Joshi and Yves Schabes.
1991.
Tree-adjoining grammars and lexicalized grammars.
InMaurice Nivat and Andreas Podelski, editors, Defin-ability and Recognizability of Sets of Trees.
Elsevier.Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
In Proceedings of the Human Language Tech-nology Conference, San Diego, CA.David Magerman.
1995.
Statistical decision-tree modelsfor parsing.
In 33rd Meeting of the Association forComputational Linguistics (ACL?95).Mitchell M. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19.2:313?330, June.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn Treebank: Annotating predicate ar-gument structure.
In Proceedings of the ARPA HumanLanguage Technology Workshop.B.
Srinivas.
1997.
Performance evaluation of supertag-ging for partial parsing.
In Proceedings of the Fifth In-ternational Workshop on Parsing Technologies, pages187?198, Cambridge, MA.Wen Wang and Mary P. Harper.
2002.
The superarv lan-guage model: Investigating the effectiveness of tightlyintegrating multiple knowledge sources.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 238?247,Philadelphia, PA.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Fifth Natural Language Pro-cessing Pacific Rim Symposium (NLPRS-99), Beijing,China.
