Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1138?1147,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsFeature Optimization for Constituent Parsing via Neural NetworksZhiguo WangIBM Watson1101 KitchawanYorktown Heights, NY, USAzhigwang@us.ibm.comHaitao MiIBM Watson1101 KitchawanYorktown Heights, NY, USAhmi@us.ibm.comNianwen XueBrandeis University415 South StWaltham, MA, USAxuen@brandeis.eduAbstractThe performance of discriminative con-stituent parsing relies crucially on featureengineering, and effective features usu-ally have to be carefully selected througha painful manual process.
In this paper,we propose to automatically learn a setof effective features via neural networks.Specifically, we build a feedforward neu-ral network model, which takes as inputa few primitive units (words, POS tagsand certain contextual tokens) from the lo-cal context, induces the feature represen-tation in the hidden layer and makes pars-ing predictions in the output layer.
Thenetwork simultaneously learns the featurerepresentation and the prediction modelparameters using a back propagation al-gorithm.
By pre-training the model on alarge amount of automatically parsed data,and then fine-tuning on the manually an-notated Treebank data, our parser achievesthe highest F1score at 86.6% on Chi-nese Treebank 5.1, and a competitive F1score at 90.7% on English Treebank.
Moreimportantly, our parser generalizes wellon cross-domain test sets, where we sig-nificantly outperform Berkeley parser by3.4 points on average for Chinese and 2.5points for English.1 IntroductionConstituent parsing seeks to uncover the phrasestructure representation of sentences that can beused in a variety of natural language applicationssuch as machine translation, information extrac-tion and question answering (Jurafsky and Martin,2008).
One of the major challenges for this task isthat constituent parsers require an inference algo-rithm of high computational complexity in orderto search over their large structural space, whichmakes it very hard to efficiently train discrimina-tive models.
So, for a long time, the task wasmainly solved with generative models (Collins,1999; Charniak, 2000; Petrov et al, 2006).
Inthe last few years, however, with the use of ef-fective parsing strategies, approximate inferencealgorithms, and more efficient training methods,discriminative models began to surpass the gen-erative models (Carreras et al, 2008; Zhu et al,2013; Wang and Xue, 2014).Just like other NLP tasks, the performance ofdiscriminative constituent parsing crucially relieson feature engineering.
If the feature set is toosmall, it might underfit the model and leads to lowperformance.
On the other hand, too many fea-tures may result in an overfitting problem.
Usu-ally, an effective set of features have to be de-signed manually and selected through repeated ex-periments (Sagae and Lavie, 2005; Wang et al,2006; Zhang and Clark, 2009).
Not only doesthis procedure require a lot of expertise, but itis also tedious and time-consuming.
Even af-ter this painstaking process, it is still hard to saywhether the selected feature set is complete or op-timal to obtain the best possible results.
A moredesirable alternative is to learn features automat-ically with machine learning algorithms.
Lei etal.
(2014) proposed to learn features by represent-ing the cross-products of some primitive units withlow-rank tensors for dependency parsing.
How-ever, to achieve competitive performance, they hadto combine the learned features with the tradi-tional hand-crafted features.
For constituent pars-ing, Henderson (2003) employed a recurrent neu-ral network to induce features from an unboundedparsing history.
However, the final performancewas below the state of the art.In this work, we design a much simpler neu-ral network to automatically induce features fromjust the local context for constituent parsing.
Con-1138cretely, we choose the shift-reduce parsing strat-egy to build the constituent structure of a sentence,and train a feedforward neural network modelto jointly learn feature representations and makeparsing predictions.
The input layer of the net-work takes as input a few primitive units (words,POS tags and certain contextual tokens) from thelocal context, the hidden layer aims to inducea distributed feature representation by combiningall the primitive units with different weights, andthe output layer attempts to make parsing predic-tions based on the feature representation.
Dur-ing the training process, the model simultaneouslylearns the feature representation and predictionmodel parameters using a backpropagation algo-rithm.
Theoretically, the learned feature represen-tation is optimal (or at least locally optimal) forthe parsing predictions.
In practice, however, ourmodel does not work well if it is only trained onthe manually annotated Treebank data sets.
How-ever, when pre-trained on a large amount of auto-matically parsed data and then fine-tuned on theTreebank data sets, our model achieves a fairlylarge improvement in performance.
We evaluatedour model on both Chinese and English.
On stan-dard data sets, our model reaches F1= 86.6%for Chinese and outperforms all the state-of-the-art systems, and for English our final performanceis F1= 90.7% and this result surpasses that ofall the previous neural network based models andis comparable to the state-of-the-art systems.
Oncross-domain data sets, our model outperforms theBerkeley Parser1by 3.4 percentage points for Chi-nese and 2.5 percentage points for English.The remainder of this paper is organized as fol-lows: Section 2 introduces the shift-reduce con-stituent parsing approach.
Section 3 describes ourfeature optimization model and some parameterestimation techniques.
We discuss and analyzeour experimental results in Section 4.
Section 5discusses related work.
Finally, we conclude thispaper in Section 6.2 Shift-Reduce Constituent ParsingShift-reduce constituent parsing utilizes a series ofshift-reduce decisions to construct syntactic trees.Formally, the shift-reduce system is a quadrupleC = (S, T, s0, St), where S is a set of parserstates (sometimes called configurations), T is a fi-nite set of actions, s0is an initialization function1https://code.google.com/p/berkeleyparser/S..VPVPVPSVPPPNPNPPPNPPPNPNNSyearsCD10INoverNPNNinterestVBGincludingNPQPCDbillionCD166$CCor,,NPQPCDbillionCD50$INatNPPPNPNNbailoutDTtheINforNPNNspendingJJtotalDTtheVBGleaving,,SBARSVPVPVBNsoldVBPareNPNNSassetsDTtheINasPRTRPoffVBNpaidVBbeMDwouldNPNNdebtDTThat9Figure 1: An example of constituent tree.to map each input sentence into a unique initialstate, and St?
S is a set of terminal states.
Eachaction t ?
T is a transition function that maps astate into a new state.
A parser state s ?
S isdefined as a tuple s = (?, ?
), where ?
is a stackwhich is maintained to hold partial subtrees thatare already constructed, and ?
is a queue whichis used for storing remaining unprocessed words.In particular, the initial state has an empty stack ?and a queue ?
containing the entire input sentence,and the terminal states have an empty queue ?
anda stack ?
containing only one complete parse tree.The task of parsing is to scan the input sentencefrom left to right and perform a sequence of shift-reduce actions to transform the initial state into aterminal state.In order to jointly assign POS tags and constructa constituent structure for an input sentence, wedefine the following actions for the action set T ,following Wang and Xue (2014):?
SHIFT-X (sh-x): remove the first word from?, assign a POS tag X to the word and push itonto the top of ?;?
REDUCE-UNARY-X (ru-x): pop the topsubtree from ?, construct a new unary nodelabeled with X for the subtree, then push thenew subtree back onto ?.
The head of thenew subtree is inherited from its child;?
REDUCE-BINARY-{L/R}-X (rl/rr-x): popthe top two subtrees from ?, combine theminto a new tree with a node labeled with X,then push the new subtree back onto ?.
Theleft (L) and right (R) versions of the actionindicate whether the head of the new subtreeis inherited from its left or right child.With these actions, our parser can processtrees with unary and binary branches easily.For example, in Figure 1, for the sentence ?theassets are sold?, our parser can construct the1139parse tree by performing the action sequence{sh-DT, sh-NNS, rr-NP, sh-VBP,sh-VBN, ru-VP, rr-VP, rr-S}.
To pro-cess multi-branch trees, we employ binarizationand debinarization processes described in Zhangand Clark (2009) to transform multi-branch treesinto binary trees and restore the generated binarytrees back to their original forms.
For inference,we employ the beam search decoding algorithm(Zhang and Clark, 2009) to balance the tradeoffbetween accuracy and efficiency.3 Feature Optimization Model3.1 ModelTo determine which action t ?
T should be per-formed at a given state s ?
S, we need a modelto score each possible ?s, t?
combination.
In pre-vious approaches (Sagae and Lavie, 2005; Wanget al, 2006; Zhang and Clark, 2009), the model isusually defined as a linear model Score(s, t) =?
?w ?
?
(s, t), where ?
(s, t) is a vector of hand-crafted features for each state-action pair and?
?wis the weight vector for these features.
The hand-crafted features are usually constructed by com-pounding primitive units according to some fea-ture templates.
For example, almost all the pre-vious work employed the list of primitive units inTable 1(a), and constructed hand-crafted featuresby concatenating these primitive units accordingto the feature templates in Table 1(b).
Obviously,these feature templates are only a small subset ofthe cross products of all the primitive units.
Thisfeature set is the result of a large number of exper-iments through trial and error from previous work.Still we cannot say for sure that this is the optimalsubset of features for the parsing task.To cope with this problem, we propose to si-multaneously optimize feature representation andparsing accuracy via a neural network model.
Fig-ure 2 illustrates the architecture of our model.
Ourmodel consists of input, projection, hidden andoutput layers.
First, in the input layer, all primi-tive units (shown in Table 1(a)) are imported to thenetwork.
We also import the suffixes and prefixesof the first word in the queue, because these unitshave been shown to be very effective for predict-ing POS tags (Ratnaparkhi, 1996).
Then, in theprojection layer, each primitive unit is projectedinto a vector.
Specifically, word-type units arerepresented as word embeddings, and other unitsare transformed into one-hot representations.
The(1)p0w, p0t,p0c, p1w, p1t,p1c,p2w, p2t,p2c, p3w, p3t,p3c(2)p0lw, p0lc, p0rw, p0rc,p0uw, p0uc,p1lw, p1lc, p1rw, p1rc,p1uw, p1uc(3) q0w, q1w, q2w, q3w(a) Primitive Unitsunigramsp0tc, p0wc, p1tc, p1wc, p2tcp2wc, p3tc, p3wc, q0wt, q1wtq2wt, q3wt, p0lwc, p0rwcp0uwc, p1lwc, p1rwc, p1uwcbigramsp0wp1w, p0wp1c, p0cp1w, p0cp1cp0wq0w, p0wq0t, p0cq0w, p0cq0tq0wq1w, q0wq1t, q0tq1w, q0tq1tp1wq0w, p1wq0t, p1cq0w, p1cq0ttrigramsp0cp1cp2c, p0wp1cp2c, p0cp1wq0tp0cp1cp2w, p0cp1cq0t, p0wp1cq0tp0cp1wq0t, p0cp1cq0w(b) Feature TemplatesTable 1: Primitive units (a) and feature templates(b) for shift-reduce constituent parsing, where pirepresents the ithsubtree in the stack and qide-notes the ithword in the queue.
w refers to thehead word, t refers to the head POS, and c refersto the constituent label.
piland pirrefer to theleft and right child for a binary subtree pi, and piurefers to the child of a unary subtree pi.vectors of all primitive units are concatenated toform a holistic vector for the projection layer.
Thehidden layer corresponds to the feature representa-tion we want to learn.
Each dimension in the hid-den layer can be seen as an abstract factor of allprimitive units, and it calculates a weighted sumof all nodes from the projection layer and appliesa non-linear activation function to yield its acti-vation.
We choose the logistic sigmoid functionfor the hidden layer.
The output layer is used formaking parsing predictions.
Each node in the out-put layer corresponds to a shift-reduce action.
Wewant to interpret the activation of the output layeras a probability distribution over all possible shift-reduce actions, therefore we normalize the out-put activations (weighted summations of all nodesfrom the hidden layer) with the softmax function.3.2 Parameter EstimationOur model consists of three groups of parameters:(1) the word embedding for each word type unit,1140w0?wm?t0?tn?c0?cn??
??
??
?...
...p(t|s )sigmoidsoftmaxword  embedding one-hot representationdense real-valuedlow-dimensional vector...
suffix1?prefix4?...Figure 2: Neural network architecture for con-stituent parsing, where widenotes word type unit,tidenotes POS tag unit, cidenotes constituent la-bel unit, suffixiand prefixi(1 ?
i ?
4) de-notes i-character word suffix or prefix for the firstword in the queue.
(2) the connections between the projection layerand the hidden layer which are used for learningan optimal feature representation and (3) the con-nections between the hidden layer and the outputlayer which are used for making accurate pars-ing predictions.
We decided to learn word em-beddings separately, so that we can take advantageof a large amount of unlabeled data.
The remain-ing two groups of parameters can be trained si-multaneously by the back propagation algorithm(Rumelhart et al, 1988) to maximize the likeli-hood over the training data.We also employ three crucial techniques to seekmore effective parameters.
First, we utilize mini-batched AdaGrad (Duchi et al, 2011), in whichthe learning rate is adapted differently for differ-ent parameters at different training steps.
With thistechnique, we can start with a very large learningrate which decreases during training, and can thusperform a far more thorough search within the pa-rameter space.
In our experiments, we got a muchfaster convergence rate with slightly better accu-racy by using the learning rate ?
= 1 instead ofthe commonly-used ?
= 0.01.
Second, we initial-ize the model parameters by pre-training.
Unsu-pervised pre-training has demonstrated its effec-tiveness as a way of initializing neural networkmodels (Erhan et al, 2010).
Since our model re-quires many run-time primitive units (POS tagsand constituent labels), we employ an in-houseshift-reduce parser to parse a large amount of unla-beled sentences, and pre-train the model with theautomatically parsed data.
Third, we utilize theDropout strategy to address the overfitting prob-lem.
However, different from Hinton et al (2012),we only use Dropout during testing, because wefound that using Dropout during training did notimprove the parsing performance (on the dev set)while greatly slowing down the training process.4 Experiment4.1 Experimental SettingWe conducted experiments on the Penn ChineseTreebank (CTB) version 5.1 (Xue et al, 2005) andthe Wall Street Journal (WSJ) portion of Penn En-glish Treebank (Marcus et al, 1993).
To fairlycompare with other work, we follow the standarddata division.
For Chinese, we allocated Articles001-270 and 400-1151 as the training set, Articles301-325 as the development set, and Articles 271-300 as the testing set.
For English, we use sec-tions 2-21 for training, section 22 for developingand section 23 for testing.We also utilized some unlabeled corpora andused the word2vec2toolkit to train word em-beddings.
For Chinese, we used the unlabeledChinese Gigaword (LDC2003T09) and performedChinese word segmentation using our in-housesegmenter.
For English, we randomly selected 9million sentences from our in-house newswire cor-pus, which has no overlap with our training, test-ing and development sets.
We use Evalb3toolkitto evaluate parsing performance.4.2 Characteristics of Our ModelThere are several hyper-parameters in our model,e.g., the word embedding dimension (wordDim),the hidden layer node size (hiddenSize), theDropout ratio (dropRatio) and the beam size forinference (beamSize).
The choice of these hyper-parameters may affect the final performance.
Inthis subsection, we present some experiments todemonstrate the characteristics of our model, andselect a group of proper hyper-parameters that weuse to evaluate our final model.
All the experi-ments in this subsection were performed on Chi-nese data and the evaluation is performed on Chi-nese development set.First, we evaluated the effectiveness of vari-ous primitive units.
We set wordDim = 300,hiddenSize = 300, beamSize = 8, and did notapply Dropout (dropRatio = 0).
Table 2 presentsthe results.
By comparing numbers in other rows2https://code.google.com/p/word2vec/3http://nlp.cs.nyu.edu/evalb/114180818283848586878850 100 300 500 1000(a) word embedding dimension80818283848586878850 100 300 500 1000(b) hidden layer size76788082848688900 0.2 0.4 0.6 0.8(c) Dropout ratio83848586871 2 3 4 5 6 7 8 9 10(d) beam sizeFigure 3: Influence of hyper-parameters.with row ?All Units?, we found that ablating thePrefix and Suffix units (?w/o Prefix & Suffix?
)significantly hurts both POS tagging and parsingperformance.
Ablating POS units (?w/o POS?
)or constituent label units (?w/o NT?)
has little ef-fect on POS tagging accuracy, but hurts parsingperformance.
When only keeping the word typeunits (?Only Word?
), both the POS tagging andparsing accuracy drops drastically.
So the Prefixand Suffix units are crucial for POS tagging, andPOS units and constituent label units are helpfulfor parsing performance.
All these primitive unitsare indispensable to better performance.Second, we uncovered the effect of the dimen-sion of word embedding.
We set hiddenSize =300, beamSize = 8, dropRatio = 0 and var-ied wordDim among {50, 100, 300, 500, 1000}.Figure 3(a) draws the parsing performance curve.When increasing wordDim from 50 to 300, pars-ing performance improves more than 1.5 percent-age points.
After that, the curve flattens out, andparsing performance only gets marginal improve-ment.
Therefore, in the following experiments, wefixed wordDim = 300.Third, we tested the effect of hidden layer nodesize.
We varied hiddenSize among {50, 100,300, 500, 1000}.
Figure 3(b) draws the pars-ing performance curve.
We found increasinghiddenSize is helpful for parsing performance.However, higher hiddenSize would greatly in-crease the amount of computation.
To keep theefficiency of our model, we fixed hiddenSize =300 in the following experiments.Fourth, we applied Dropout and tuned theDropout ratio through experiments.
Figure 3(c)shows the results.
We found that the peakperformance occurred at dropRatio = 0.5,which brought about an improvement of morethan 1 percentage point over the model withoutDropout (dropRatio = 0).
Therefore, we fixedPrimitive Units F1POSAll Units 86.7 96.7w/o Prefix & Suffix 85.7 95.4w/o POS 86.0 96.7w/o NT 86.2 96.6Only Word 82.7 95.2Table 2: Influence of primitive units.dropRatio = 0.5.Finally, we investigated the effect of beam size.Figure 3(d) shows the curve.
We found increasingbeamSize greatly improves the performance ini-tially, but no further improvement is observed afterbeamSize is greater than 8.
Therefore, we fixedbeamSize = 8 in the following experiments.4.3 Semi-supervised TrainingIn this subsection, we investigated whether wecan train more effective models using automati-cally parsed data.
We randomly selected 200Ksentences from our unlabeled data sets for bothChinese and English.
Then, we used an in-houseshift-reduce parser4to parse these selected sen-tences.
The size of the automatically parsed dataset may have an impact on the final model.
Sowe trained many models with varying amounts ofautomatically parsed data.
We also designed twostrategies to exploit the automatically parsed data.The first strategy (Mix-Train) is to directly add theautomatically parsed data to the hand-annotatedtraining set and train models with the mixed dataset.
The second strategy (Pre-Train) is to first pre-train models with the automatically parsed data,and then fine-tune models with the hand-annotatedtraining set.Table 3 shows results of different experimen-tal configurations for Chinese.
For the Mix-Train4Its performance is F1=83.9 on Chinese and F1=90.8%on English.1142Mix-Train Pre-Train# Auto Sent F1POS F1POS0 87.8 97.0 ?
?50K 87.2 96.8 88.4 97.1100K 88.7 96.9 89.5 97.1200K 89.2 97.2 89.5 97.4Table 3: Semi-supervised training for Chinese.Mix-Train Pre-Train# Auto Sent F1POS F1POS0 89.7 96.6 ?
?50K 89.4 96.1 90.2 96.4100K 89.5 96.0 90.4 96.5200K 89.2 95.8 90.8 96.7Table 4: Semi-supervised training for English.strategy, when we only use 50K automaticallyparsed sentences, the performance drops in com-parison with the model trained without using anyautomatically parsed data.
When we increase theautomatically parsed data to 100K sentences, theparsing performance improves about 1 percent butthe POS tagging accuracy drops slightly.
Whenwe further increase the automatically parsed datato 200K sentences, both the parsing performanceand POS tagging accuracy improve.
For the Pre-Train strategy, the performance of all three config-urations improves performance against the modelthat does not use any automatically parsed data.The Pre-Train strategy consistently outperformsthe Mix-Train strategy when the same amount ofautomatically parsed data is used.
Therefore, forChinese, the Pre-Train strategy is much more help-ful, and the more automatically parsed data we usethe better performance we get.Table 4 presents results of different experimen-tal configurations for English.
The performancetrend for the Mix-Train strategy is different fromthat of Chinese.
Here, no matter how much auto-matically parsed data we use, there is a consistentdegradation in performance against the model thatdoes not use any automatically parsed data at all.And the more automatically parsed data we use,the larger the drop in accuracy.
For the Pre-Trainstrategy, the trend is similar to Chinese.
The pars-ing performance of the Pre-Train setting consis-tently improves as the size of automatically parseddata increases.Type System F1OursSupervised*?
83.2Pretrain-Finetune*?
86.6SIPetrov and Klein (2007) 83.3Wang and Xue (2014)?
83.6SEZhu et al (2013)?
85.6Wang and Xue (2014)?
86.3RECharniak and Johnson (2005) 82.3Wang and Zong (2011) 85.7Table 5: Comparison with the state-of-the-art sys-tems on Chinese test set.
* marks neural networkbased systems.
?
marks shift-reduce parsing sys-tems.4.4 Comparing With State-of-the-artSystemsIn this subsection, we present the performanceof our models on the testing sets.
We trainedtwo systems.
The first system (?Supervised?
)is trained only with the hand-annotated trainingset, and the second system (?Pretrain-Finetune?
)is trained with the Pre-Train strategy describedin subsection 4.3 using additional automaticallyparsed data.
The best parameters for the two sys-tems are set based on their performance on the de-velopment set.
To further illustrate the effective-ness of our systems, we also compare them withsome state-of-the-art systems.
We group parsingsystems into three categories: supervised singlesystems (SI), semi-supervised single systems (SE)and reranking systems (RE).
Both of our two mod-els belong to semi-supervised single systems, be-cause our ?Supervised?
system utilized word em-beddings in its input layer.Table 5 lists the performance of our systems aswell as the state-of-the-art systems on Chinese testset.
Comparing the performance of our two sys-tems, we see that our ?Pretrain-Finetune?
systemshows a fairly large gain over the ?Supervised?system.
One explanation is that our neural net-work model is a non-linear model, so the backpropagation algorithm can only reach a local op-timum.
In our ?Supervised?
system the startingpoints are randomly initialized in the parameterspace, so it only reaches local optimum.
In com-parison, our ?Pretrain-Finetune?
system gets tosee large amount of automatically parsed data, andinitializes the starting points with the pre-trained1143Type System F1OursSupervised*?
89.4Pretrain-Finetune*?
90.7SICollins (1999) 88.2Charniak (2000) 89.5Henderson (2003)* 88.8Petrov and Klein (2007) 90.1Carreras et al (2008) 91.1Zhu et al (2013)?
90.4SEHuang et al (2010) 91.6Collobert (2011)* 89.1Zhu et al (2013)?
91.3REHenderson (2004)* 90.1Charniak and Johnson (2005) 91.5McClosky et al (2006) 92.3Huang (2008) 91.7Socher et al (2013)* 90.4Table 6: Comparing with the state-of-the-art sys-tems on English test set.
* marks neural networkbased systems.
?
marks shift-reduce parsing sys-tems.parameters.
So it finds a much better local opti-mum than the ?Supervised?
system.
Comparingour ?Pretrain-Finetune?
system with all the state-of-the-art systems, we see our system surpass allthe other systems.
Although our system only uti-lizes some basic primitive units (in Table 1(a)),it still outperforms Wang and Xue (2014)?s shift-reduce parsing system which uses more complexstructural features and semi-supervised word clus-ter features.
Therefore, our model can simultane-ously learn an effective feature representation andmake accurate parsing predictions for Chinese.Table 6 presents the performance of our systemsas well as the state-of-the-art systems on the En-glish test set.
Our ?Pretrain-Finetune?
system stillachieves much better performance than the ?Su-pervised?
system, although the gap is smaller thanthat of Chinese.
Our ?Pretrain-Finetune?
systemalso outperforms all other neural network basedsystems (systems marked with *).
Although oursystem does not outperform all the state-of-the-artsystems, the performance is comparable to mostof them.
So our model is also effective for Englishparsing.4.5 Cross Domain EvaluationIn this subsection, we examined the robustness ofour model by evaluating it on data sets from var-ious domains.
We use the Berkeley Parser as ourbaseline parser, and trained it on our training set.For Chinese, we performed our experiments onthe cross domain data sets from Chinese Treebank8.0 (Xue et al, 2013).
It consists of six domains:newswire (nw), magazine articles (mz), broadcastnews (bn), broadcast conversation (bc), weblogs(wb) and discussion forums (df).
Since all of themz domain data is already included in our train-ing set, we only selected sample sentences fromthe other five domains as the test sets5, and madesure these test sets had no overlap with our tree-bank training, development and test sets.
Notethat we did not use any data from these five do-mains for training or development.
The modelsare still the ones described in the previous sub-section.
The results are presented in Table 7.
Al-though our ?Supervised?
model got slightly worseperformance than the Berkeley Parser (Petrov andKlein, 2007), as shown in Table 5, it outper-formed the Berkeley Parser on the cross-domaindata sets.
This suggests that the learned fea-tures can better adapt to cross-domain situations.Compared with the Berkeley Parser, on averageour ?Pretrain-Finetune?
model is 3.4 percentagepoints better in terms of parsing accuracy, and3.2 percentage points better in terms of POS tag-ging accuracy.
We also presented the performanceof our pre-trained model (?Only-Pretrain?).
Wefound the ?Only-Pretrain?
model performs poorlyon this cross-domain data sets.
But even pre-training based on this less than competitive model,our ?Pretrain-Finetune?
model achieves signifi-cant improvement over the ?Supervised?
model.So the Pre-Train strategy is crucial to our model.For English, we performed our experiments onthe cross-domain data sets from OntoNote 5.0(Weischedel et al, 2013), which consists of nw,mz, bn, bc, wb, df and telephone conversations(tc).
We also performed experiments on the SMSdomain, using data annotated by the LDC forthe DARPA BOLT Program.
We randomly se-lected 300 sentences for each domain as the testsets5.
Table 8 presents our experimental results.To save space, we only presented the results ofour ?Pretrain-Finetune?
model and the Berkeley5The selected sentences can be downloaded fromhttp://www.cs.brandeis.edu/ xuen/publications.html1144Only-Pretrain Supervised Pretrain-Finetune BerkeleyParserdomain F1POS F1POS F1POS F1POSbc 61.6 81.1 72.9 90.2 74.9 91.2 68.2 86.4bn ?
?
78.2 93.2 80.8 94.2 78.3 91.2df 65.6 84.5 76.2 91.7 78.5 92.6 75.9 90.3nw 72.0 86.1 82.1 95.2 85.0 95.8 82.9 93.6wb 65.4 81.5 74.6 89.5 76.9 90.2 73.8 86.7average 66.2 83.3 76.8 92.0 79.2 92.8 75.8 89.6Table 7: Cross-domain performance for Chinese.
The ?Only-Pretrain?
model cannot successfully parsesome sentences in bn domain, so we didn?t give the numbers.Pretrain-Finetune BerkeleyParserDomain F1POS F1POSbc 77.7 92.2 76.0 91.1bn 88.1 95.4 88.2 95.0df 82.5 93.3 79.4 92.4nw 89.6 95.3 86.2 94.6wb 83.3 93.1 82.0 91.2sms 79.2 85.8 74.6 85.3tc 74.2 88.0 71.1 87.6average 82.1 91.9 79.6 91.0Table 8: Cross-domain performance for English.Parser.
Except for the slightly worse performanceon the bn domain, our model outperformed theBerkeley Parser on all the other domains.
Whileour model is only 0.6 percentage point better thanthe Berkeley Parser (Petrov and Klein, 2007) whenevaluated on the standard Penn TreeBank test set(Table 6), our parser is 2.5 percentage points bet-ter on average on the cross domain data sets.
Soour parser is also very robust for English on cross-domain data sets.5 Related WorkThere has been some work on feature optimizationin dependency parsing, but most prior work in thisarea is limited to selecting an optimal subset offeatures from a set of candidate features (Nilssonand Nugues, 2010; Ballesteros and Bohnet, 2014).Lei et al (2014) proposed to learn features for de-pendency parsing automatically.
They first repre-sented all possible features with a multi-way ten-sor, and then transformed it into a low-rank tensoras the final features that are actually used by theirsystem.
However, to obtain competitive perfor-mance, they had to combine the learned featureswith traditional hand-crafted features.
Chen andManning (2014) proposed to learn a dense fea-ture vector for transition-based dependency pars-ing via neural networks.
Their model had to learnPOS tag embeddings and dependency label em-beddings first, and then induced the dense featurevector based on these embeddings.
Comparingwith their method, our model is much simpler.
Ourmodel learned features directly based on the orig-inal form of primitive units.There have also been some attempts to useneural networks for constituent parsing.
Hender-son (2003) presented the first neural network forbroad coverage parsing.
Later, he also proposedto rerank k-best parse trees with a neural net-work model which achieved state-of-the-art per-formance (Henderson, 2004).
Collobert (2011)designed a recurrent neural network model to con-struct parse tree by stacks of sequences labeling,but its final performance is significantly lower thanthe state-of-the-art performance.
Socher et al(2013) built a recursive neural network for con-stituent parsing.
However, rather than performingfull inference, their model can only score parsecandidates generated from another parser.
Ourmodel also requires a parser to generate trainingsamples for pre-training.
However, our system isdifferent in that, during testing, our model per-forms full inference with no need of other parsers.Vinyals et al (2014) employed a Long Short-TermMemory (LSTM) neural network for parsing.
Bytraining on a much larger hand-annotated data set,their performance reached 91.6% for English.6 ConclusionIn this paper, we proposed to learn features viaa neural network model.
By taking as input theprimitive units, our neural network model learns1145feature representations in the hidden layer andmade parsing predictions based on the learned fea-tures in the output layer.
By employing the back-propagation algorithm, our model simultaneouslyinduced features and learned prediction model pa-rameters.
We show that our model achieved signif-icant improvement from pretraining on a substan-tial amount of pre-parsed data.
Evaluated on stan-dard data sets, our model outperformed all state-of-the-art parsers on Chinese and all neural net-work based models on English.
We also showthat our model is particularly effective on cross-domain tasks for both Chinese and English.AcknowledgmentsWe thank the anonymous reviewers for comments.Haitao Mi is supported by DARPA HR0011-12-C-0015 (BOLT) and Nianwen Xue is supported byDAPRA HR0011-11-C-0145 (BOLT).
The viewsand findings in this paper are those of the authorsand are not endorsed by the DARPA.ReferencesMiguel Ballesteros and Bernd Bohnet.
2014.
Au-tomatic feature selection for agenda-based depen-dency parsing.Xavier Carreras, Michael Collins, and Terry Koo.2008.
Tag, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of the Twelfth Conference on ComputationalNatural Language Learning, pages 9?16.
Associa-tion for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics conference, pages 132?139.
Asso-ciation for Computational Linguistics.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 740?750.Michael Collins.
1999.
HEAD-DRIVEN STATISTI-CAL MODELS FOR NATURAL LANGUAGE PARS-ING.
Ph.D. thesis, University of Pennsylvania.Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In International Conference onArtificial Intelligence and Statistics, number EPFL-CONF-192374.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Dumitru Erhan, Yoshua Bengio, Aaron Courville,Pierre-Antoine Manzagol, Pascal Vincent, and SamyBengio.
2010.
Why does unsupervised pre-traininghelp deep learning?
The Journal of Machine Learn-ing Research, 11:625?660.James Henderson.
2003.
Neural network probabil-ity estimation for broad coverage parsing.
In Pro-ceedings of the tenth conference on European chap-ter of the Association for Computational Linguistics-Volume 1, pages 131?138.
Association for Compu-tational Linguistics.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 95.
Association for Com-putational Linguistics.Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Zhongqiang Huang, Mary Harper, and Slav Petrov.2010.
Self-training with products of latent vari-able grammars.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 12?22.
Association for Computa-tional Linguistics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL, pages 586?594.Daniel Jurafsky and James H Martin.
2008.
Speechand language processing.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics, volume 1, pages 1381?1391.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the main conference on human lan-guage technology conference of the North AmericanChapter of the Association of Computational Lin-guistics, pages 152?159.
Association for Computa-tional Linguistics.1146Peter Nilsson and Pierre Nugues.
2010.
Automaticdiscovery of feature sets for dependency parsing.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 824?832.
Associ-ation for Computational Linguistics.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL, pages404?411.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 433?440.
Association for Computational Linguistics.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof the conference on empirical methods in natu-ral language processing, volume 1, pages 133?142.Philadelphia, PA.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1988.
Learning representations by back-propagating errors.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnology, pages 125?132.
Association for Com-putational Linguistics.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In In Proceedings of theACL conference.
Citeseer.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2014.Grammar as a foreign language.
arXiv preprintarXiv:1412.7449.Zhiguo Wang and Nianwen Xue.
2014.
Joint pos tag-ging and transition-based constituent parsing in chi-nese with non-local features.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages733?742.
Association for Computational Linguis-tics.Zhiguo Wang and Chengqing Zong.
2011.
Parsereranking based on higher-order lexical dependen-cies.
In IJCNLP, pages 1251?1259.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.2006.
A fast, accurate deterministic parser for chi-nese.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for Computa-tional Linguistics, pages 425?432.
Association forComputational Linguistics.Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Nianwen Xue, Martha Palmer, MitchellMarcus, Ann Taylor, et al 2013.
Ontonotes release5.0.
Linguistic Data Consortium, Philadelphia.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(2):207?238.Nianwen Xue, Xiuhong Zhang, Zixin Jiang,Martha Palmer, Fei Xia, Fu-Dong Chiou, andMeiyu Chang.
2013.
Chinese treebank 8.0.Philadelphia: Linguistic Data Consortium, pagehttps://catalog.ldc.upenn.edu/LDC2013T21.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the chinese treebank using a global dis-criminative model.
In Proceedings of the 11th Inter-national Conference on Parsing Technologies, pages162?171.
Association for Computational Linguis-tics.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages434?443, Sofia, Bulgaria, August.
Association forComputational Linguistics.1147
