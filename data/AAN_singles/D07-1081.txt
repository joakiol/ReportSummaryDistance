Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
774?782, Prague, June 2007. c?2007 Association for Computational LinguisticsScalable Term Selection for Text CategorizationJingyang LiNational Lab of Intelligent Tech.
& Sys.Department of Computer Sci.
& Tech.Tsinghua University, Beijing, Chinalijingyang@gmail.comMaosong SunNational Lab of Intelligent Tech.
& Sys.Department of Computer Sci.
& Tech.Tsinghua University, Beijing, Chinasms@tsinghua.edu.cnAbstractIn text categorization, term selection is animportant step for the sake of both cate-gorization accuracy and computational ef-ficiency.
Different dimensionalities are ex-pected under different practical resource re-strictions of time or space.
Traditionallyin text categorization, the same scoring orranking criterion is adopted for all targetdimensionalities, which considers both thediscriminability and the coverage of a term,such as ?2 or IG.
In this paper, the poor ac-curacy at a low dimensionality is imputed tothe small average vector length of the docu-ments.
Scalable term selection is proposedto optimize the term set at a given dimen-sionality according to an expected averagevector length.
Discriminability and cover-age are separately measured; by adjustingthe ratio of their weights in a combined cri-terion, the expected average vector lengthcan be reached, which means a good com-promise between the specificity and the ex-haustivity of the term subset.
Experimentsshow that the accuracy is considerably im-proved at lower dimensionalities, and largerterm subsets have the possibility to lowerthe average vector length for a lower com-putational cost.
The interesting observationsmight inspire further investigations.1 IntroductionText categorization is a classical text informationprocessing task which has been studied adequately(Sebastiani, 2002).
A typical text categorization pro-cess usually involves these phases: document in-dexing, dimensionality reduction, classifier learn-ing, classification and evaluation.
The vector spacemodel is frequently used for text representation(document indexing); dimensions of the learningspace are called terms, or features in a general ma-chine learning context.
Term selection is often nec-essary because:?
Many irrelevant terms have detrimental effecton categorization accuracy due to overfitting(Sebastiani, 2002).?
Some text categorization tasks have many rel-evant but redundant features, which also hurtthe categorization accuracy (Gabrilovich andMarkovitch, 2004).?
Considerations on computational cost:(i) Many sophisticated learning machines arevery slow at high dimensionalities, such asLLSF (Yang and Chute, 1994) and SVMs.
(ii) In Asian languages, the term set is oftenvery large and redundant, which causes thelearning and the predicting to be really slow.
(iii) In some practical cases the computationalresources (time or space) are restricted, such ashand-held devices, real-time applications andfrequently retrained systems.
(iv) Some deeperanalysis or feature reconstruction techniquesrely on matrix factorization (e.g.
LSA basedon SVD), which might be computationally in-tractable while the dimensionality is large.Sometimes an aggressive term selection might beneeded particularly for (iii) and (iv).
But it is no-table that the dimensionality is not always directly774connected to the computational cost; this issue willbe touched on in Section 6.
Although we havemany general feature selection techniques, the do-main specified ones are preferred (Guyon and Elis-seeff, 2003).
Another reason for ad hoc term se-lection techniques is that many other pattern clas-sification tasks has no sparseness problem (in thisstudy the sparseness means a sample vector hasfew nonzero elements, but not the high-dimensionallearning space has few training samples).
As a ba-sic motivation of this study, we hypothesize that thelow accuracy at low dimensionalities is mainly dueto the sparseness problem.Many term selection techniques were presentedand some of them have been experimentally testedto be high-performing, such as Information Gain, ?2(Yang and Pedersen, 1997; Rogati and Yang, 2002)and Bi-Normal Separation (Forman, 2003).
Every-one of them adopt a criterion scoring and rankingthe terms; for a target dimensionality d, the term se-lection is simply done by picking out the top-d termsfrom the ranked term set.
These high performing cri-teria have a common characteristic ?
both discrim-inability and coverage are implicitly considered.?
discriminability: how unbalanced is the distri-bution of the term among the categories.?
coverage: how many documents does the termoccur in.
(Borrowing the terminologies from document index-ing, we can say the specificity of a term set corre-sponds to the discriminability of each term, and theexhaustivity of a term set corresponds to the cov-erage of each term.)
The main difference amongthese criteria is to what extent the discriminability isemphasized or the coverage is emphasized.
For in-stance, empirically IG prefers high frequency termsmore than ?2 does, which means IG emphasizes thecoverage more than ?2 does.The problem is, these criteria are nonparametricand do the same ranking for any target dimensional-ity.
Small term sets meet the specificity?exhaustivitydilemma.
If really the sparseness is the main rea-son of the low performance of a small term set, thespecificity should be moderately sacrificed to im-prove the exhaustivity for a small term set; that isto say, the term selection criterion should considercoverage more than discriminability.
Contrariwise,coverage could be less considered for a large termset, because we need worry little about the sparse-ness problem and the computational cost might de-crease.The remainder of this paper is organized as fol-lows: Section 2 describes the document collectionsused in this study, as well as other experiment set-tings; Section 3 investigates the relation betweensparseness (measured by average vector length) andcategorization accuracy; Section 4 explains the basicidea of scalable term selection and proposed a poten-tial approach; Section 5 carries out experiments toevaluate the approach, during which some empiricalrules are observed to complete the approach; Sec-tion 6 makes some further observations and discus-sions based on Section 5; Section 7 gives a conclud-ing remark.2 Experiment Settings2.1 Document CollectionsTwo document collections are used in this study.CE (Chinese Encyclopedia): This is from theelectronic version of the Chinese Encyclopedia.
Wechoose a Chinese corpus as the primary documentcollection because Chinese text (as well as otherAsian languages) has a very large term set and asatisfying subset is usually not smaller than 50000(Li et al, 2006); on the contrary, a dimensional-ity lower than 10000 suffices a general English textcategorization (Yang and Pedersen, 1997; Rogatiand Yang, 2002).
For computational cost reasonsmentioned in Section 1, Chinese text categorizationwould benefit more from an high-performing ag-gressive term selection.
This collection contains 55categories and 71674 documents (9:1 split to train-ing set and test set).
Each documents belongs toonly one category.
Each category contains 399?3374 documents.
This collection was also used byLi et al (2006).20NG (20 Newsgroups1): This classical Englishdocument collection is chosen as a secondary in thisstudy to testify the generality of the proposed ap-proach.
Some figures about this collection are notshown in this paper as the figures about CE, viz.
Fig-ure 1?4 because they are similar to CE?s.1http://people.csail.mit.edu/jrennie/20Newsgroups7752.2 Other SettingsFor CE collection, character bigrams are chosen tobe the indexing unit for its high performance (Li etal., 2006); but the bigram term set suffers from itshigh dimensionality.
This is exactly the case we tendto tackle.
For 20NG collection, the indexing unitsare stemmed2 words.
Both term set are df -cut bythe most conservative threshold (df ?
2).
The sizesof the two candidate term sets are |TCE| = 1067717and |T20NG| = 30220.Term weighting is done by tfidf (ti, dj) =log(tf (ti, dj) + 1) ?
log( df (ti)+1Nd)3, in which ti de-notes a term, dj denotes a document, Nd denotes thetotal document number.The classifiers used in this study are supportvector machines (Joachims, 1998; Gabrilovich andMarkovitch, 2004; Chang and Lin, 2001).
The ker-nel type is set to linear, which is fast and enoughfor text categorization.
Also, Brank et al (2002)pointed out that the complexity and sophistication ofthe criterion itself is more important to the successof the term selection method than its compatibilityin design with the classifier.Performance is evaluated by microaveraged F1-measure.
For single-label tasks, microaveraged pre-cision, recall and F1 have the same value.
?2 is used as the term selection baseline for itspopularity and high performance.
(IG was also re-ported to be good.
In our previous experiments, ?2is generally superior to IG.)
In this study, featuresare always selected globally, which means the maxi-mum are computed for category-specific values (Se-bastiani, 2002).3 Average Vector Length (AVL)In this study, vector length (how many differentterms does the document hold after term selection)is used as a straightforward sparseness measure for adocument (Brank et al, 2002).
Generally, documentsizes have a lognormal distribution (Mitzenmacher,2003).
In our experiment, vector lengths are alsofound to be nearly lognormal distributed, as shownin Figure 1.
If the correctly classified documents2Stemming by Porter?s Stemmer (http://www.tartarus.org/ martin/PorterStemmer/).3In our experiments this form of tfidf always outperformsthe basic tfidf (ti, dj) = tf (ti, dj) ?
log?df (ti)+1Nd?form.1 10 100 10000.000.010.02probdensityvector lengthcorrectwrongallFigure 1: Vector Length Distributions (smoothed),on CE Document Collection1 10 100 10000.00.10.20.30.40.50.60.7errorratevector lengthFigure 2: Error Rate vs. Vector Length (smoothed),on CE Collection, 5000 Dimensions by ?2and the wrongly classified documents are separatelyinvestigated, they both yield a nearly lognormal dis-tribution.Also in Figure 1, wrongly classified documentsshows a relatively large proportion at low dimen-sionalities.
Figure 2 demonstrates this with moreclarity.
Thus the hypothesis formed in Section 1 isconfirmed: there is a strong correlation between thesparseness degree and the categorization error rate.Therefore, it is quite straightforward a thought tomeasure the ?sparseness of a term subset?
(or moreprecisely, the exhaustivity) by the corresponding av-erage vector length (AVL) of all documents.4 In the4Due to the lognormal distribution of vector length, it seemsmore plausible to average the logarithmic vector length.
How-ever, for a fixed number of documents , logP |dj ||D| should holda nearly fixed ratio toP log |dj ||D| , in which |D| denotes the doc-ument number and |dj | denotes the document vector length.776remainder of this paper, (log) AVL is an importantmetric used to assess and control the sparseness of aterm subset.4 Scalable Term Selection (STS)Since the performance droping down at low dimen-sionalities is attributable to low AVLs in the previoussection, a scalable term selection criterion shouldautomatically accommodate its favor of high cov-erage to different target dimensionalities.4.1 Measuring Discriminability and CoverageThe first step is to separately measure the discrim-inability and the coverage of a term.
A basic guide-line is that these two metrics should not be highly(positive) correlated; intuitively, they should have aslight negative correlation.
The correlation of thetwo metrics can be visually estimated by the jointdistribution figure.
A bunch of term selection met-rics were explored by Forman (2003).
df (documentfrequency) is a straightforward choice to measurecoverage.
Since df follows the Zipf?s law (inversepower law), log(df ) is adopted.
High-performingterm selection criterion themselves might not begood candidates for the discriminability metric be-cause they take coverage into account.
For exam-ple, Figure 3 shows that ?2 is not satisfying.
(Forreadability, the grayness is proportional to the logprobability density in Figure 3, Figure 4 and Fig-ure 12.)
Relatively, probability ratio (Forman, 2003)is a more straight metric of discriminability.PR(ti, c) = P (ti|c+)P (ti|c?)
=df (ti, c+)/df (c+)df (ti, c?
)/df (c?
)It is a symmetric ratio, so log(PR) is likely to bemore appropriate.
For multi-class categorization,a global value can be assessed by PRmax(ti) =maxc PR(ti, c), like ?2max for ?2 (Yang and Ped-ersen, 1997; Rogati and Yang, 2002; Sebastiani,2002); for brief, PR denotes PRmax hereafter.
Thejoint distribution of log(PR) and log(df ) is shown inFigure 12.
We can see that the distribution is quiteeven and they have a slight negative correlation.4.2 Combined CriterionNow we have the two metrics: log(PR) for discrim-inability and log(df ) for coverage, and a parametriclog(df )?21.10 10.591.742033.0Figure 3: (log(df ), ?2) Distribution, on CElog(df )log(PR)1.10 10.590.409.46Figure 4: (log(df ), log(PR)) Distribution, on CEterm selection criterion comes forth:?(ti;?)
=( ?log(PR(ti)) +1?
?log(df (ti)))?1A weighted harmonic averaging is adopted here be-cause either metric?s being too small is a severedetriment.
?
?
[0, 1] is the weight for log(PR),which denotes how much the discriminability isemphasized.
When the dimensionality is fixed, asmaller ?
leads to a larger AVL and a larger ?
leadsto a smaller AVL.
The optimal ?
should be a function777of the expected dimensionality (k):??
(k) = argmax?F1(Sk(?
))in which the term subset Sk(?)
?
T is selected by?(?;?)
, |Sk| = k, and F1 is the default evaluationcriterion.
Naturally, this optimal ?
leads to a corre-sponding optimal AVL:AVL?
(k) ??
??
(k)For a concrete implementation, we should have an(empirical) function to estimate ??
or AVL?:AVL?
(k) .= AVL?
(k)In the next section, the values of AVL?
(as well as ??
)for some k-s are figured out by experimental search;then an empirical formula, AVL?
(k), comes forth.
Itis interesting and inspiring that by adding the ?cor-pus AVL?
as a parameter this formula is universalfor different document collections, which makes thewhole idea valuable.5 Experiments and Implementation5.1 ExperimentsThe expected dimensionalities (k) chosen for exper-imentation areCE: 500, 1000, 2000, 4000, .
.
.
, 32000, 64000;20NG: 500, 1000, 2000, .
.
.
, 16000, 30220.5For a given document collection and a given targetdimensionality, there is a corresponding AVL for a ?,and vice versa (for the possible value range of AVL).According to the observations in Section 5.2, AVLother than ?
is the direct concern because it is moreintrinsic, but ?
is the one that can be tuned directly.So, in the experiments, we vary AVL by tuning ?
toproduce it, which means to calculate ?(AVL).AVL(?)
is a monotone function and fast to cal-culate.
For a given AVL, the corresponding ?
canbe quickly found by a Newton iteration in [0,1].
Infact, AVL(?)
is not a continuous function, so ?
isonly tuned to get an acceptable match, e.g.
within?0.1.5STS is tested to the whole T on 20NG but not on CE, be-cause (i) TCE is too large and time consuming for training andtesting, and (ii) ?2 was previously tested on larger k and theperformance (F1) is not stable while k > 64000.For each k, by the above way of fitting ?,we manually adjust AVL (only in integers) untilF1(Sk(?
(AVL))) peaks.
By this way, Figure 5?11are manually tuned best-performing results as obser-vations for figuring out the empirical formulas.Figure 5 shows the F1 peaks at different dimen-sionalities.
Comparing to ?2, STS has a consid-erable potential superiority at low dimensionalities.The corresponding values of AVL?
are shown in Fig-ure 6, along with the AVLs of ?2-selected term sub-sets.
The dotted lines show the trend of AVL?
; at theoverall dimensionality, |TCE| = 1067717, they havethe same AVL = 898.5.
We can see that log(AVL?
)is almost proportional to log(k) when k is not toolarge.
The corresponding values of ??
are shown inFigure 7; the relation is nearly linear between ??
andlog(k).Now it is necessary to explain why an empiricalAVL?
(k) derived from the straight line in Figure 6can be used instead of AVL?
(k) in practice.
Oneimportant but not plotted property is that the per-formance of STS is not very sensitive to a smallvalue change of AVL.
For instance, at k = 4000,AVL?
= 120 and the F1 peak is 85.8824%, andfor AVL = 110 and 130 the corresponding F1 are85.8683% and 85.6583%; at the same k, the F1of ?2 selection is 82.3950%.
This characteristic ofSTS guarantee that the empirical AVL?
(k) has a veryclose performance to AVL?
(k); due to the limitedspace, the performance curve of AVL?
(k) will notbe plotted in Section 5.2.Same experiments are done on 20NG and the re-sults are shown in Figure 8, Figure 9 and Figure 10.The performance improvements is not as signifi-cant as on the CE collection; this will be discussedin Section 6.2.
The conspicuous relations betweenAVL?, ??
and k remain the same.5.2 Algorithm CompletionIn Figure 6 and Figure 9, the ratios of log(AVL?
(k))to log(k) are not the same on CE and 20NG.
Tak-ing into account the corpus AVL (the AVL producedby the whole term set): AVLTCE = 898.5286 andAVLT20NG = 82.1605, we guess log(AVL?
(k))log(AVLT ) is ca-pable of keeping the same ratio to log(k) for bothCE and 20NG.
This hypothesis is confirmed (not fortoo high dimensionalities) by Figure 11; Section 6.2778100 1000 10000 10000060657075808590F1(%)dimensionality (k)2STSFigure 5: Performance Comparison, on CE1 10 100 1000 10000 100000 100000011010010002STSAVL*dimensionality (k)Figure 6: AVL Comparison, on CE1 10 100 1000 10000 100000 10000000.000.020.040.060.080.100.12dimensionality (k)Figure 7: Optimal Weights of log(PR), on CE100 1000 10000 10000072747678808284862STSF1(%)dimensionality (k)Figure 8: Performance Comparison, on 20NG1 10 100 1000 10000 1000001101002STSAVL*dimensionality (k)Figure 9: AVL Comparison, on 20NG1 10 100 1000 10000 1000000.00.10.20.30.40.50.6dimensionality (k)Figure 10: Optimal Weights of log(PR), on 20NG7791 10 100 1000 10000 100000 10000000.00.20.40.60.81.0log(AVL*(k))/log(AVLT)dimensionality (k)CE20NGFigure 11: log(AVL?
(k))log(AVLT ) , on Both CE and 20NGcontains some discussion on this.From the figure, we get the value of this ratio (thebase of log is set to e):?
= log(AVL?
(k))/log(AVLT )log(k)?= 0.085which should be a universal constant for all text cat-egorization tasks.So the empirical estimation of AVL?
(k) is givenbyAVL?
(k) = exp(?
log(AVLT ) ?
log(k))= AVL ?
log(k)Tand the final STS criterion is?
(ti, k) = ?(ti;?(AVL?
(k)))= ?(ti;?
(AVL ?
log(k)T ))in which ?(?)
can be calculated as in Section 5.1.The target dimensionality, k, is involved as a param-eter, so the approach is named scalable term selec-tion.
As stated in Section 5.1, AVL?
(k) has a veryclose performance to AVL?
(k) and its performanceis not plotted here.6 Further Observation and Discussion6.1 Comparing the Selected SubsetsAn investigation shows that for a quite large rangeof ?, term rankings by ?(ti;?)
and ?2(ti) have astrong correlation (the Spearman?s rank correlationcoefficient is bigger than 0.999).
In order to com-log(df )log(PR)1.10 10.590.409.465001000200040008000160003200064000STS?2Figure 12: Selection Area Comparison of STS and?2 on Various Dimensionalities, on CElog(df )log(PR)1.10 9.140.118.04500100020004000800016000STS?2Figure 13: Selection Area Comparison of STS and?2 on Various Dimensionalities, on 20NGpare the two criteria?s preferences for discriminabil-ity and coverage, the selected subsets of differentdimensionalities are shown in Figure 12 (the cor-responding term density distribution was shown inFigure 4) and Figure 13.
For different dimension-780alities, the selection areas of STS are represented byboundary lines, and the selection areas of ?2 are rep-resented by different grayness.In Figure 12, STS shows its superiority at low di-mensionalities by more emphasis on the coverageof terms.
In Figure 13, STS shows its superior-ity at high dimensionalities by more emphasis onthe discriminability of terms; lower coverage yieldssmaller index size and lower computational cost.At any dimensionality, STS yields a relatively fixedbound for either discriminability or coverage, otherthan a compromise between them like ?2; this is at-tributable to the harmonic averaging.6.2 Adaptability of STSThere are actually two kinds of sparseness in a (vec-torized) document collection:collection sparseness: the high-dimensional learn-ing space contains few training samples;document sparseness: a document vector has fewnonzero dimensions.In this study, only the document sparseness is inves-tigated.
The collection sparseness might be a back-room factor influencing the actual performance ondifferent document collections.
This might explainwhy the explicit characteristics of STS are not thesame on CE to 20NG: (comparing with ?2, see Fig-ure 5, Figure 6, Figure 8 and Figure 9)CE.
The significant F1 improvements at low di-mensionalities sacrifice the short of AVL.
In somelearning process implementations, it is AVL otherthan k that determines the computational cost; inmany other cases, k is the determinant.
Furthermore, possible post-processing, like matrix factor-ization, might benefit from a low k.20NG.
The F1 improvements at low dimension-alities is not quite significant, but AVL remains alower level.
For higher k, there is less difference inF1, but the smaller AVL yield lower computationalcost than ?2.Nevertheless, STS shows a stable behavior forvarious dimensionalities and quite different docu-ment collections.
The existence of the universalconstant ?
empowers it to be adaptive and practi-cal.
As shown in Figure 11, STS draws the rela-tive log AVL?
(k) to the same straight line, ?
log(k),for different document collections.
This mightmeans that the relative AVL is an intrinsic demandfor the term subset size k.7 ConclusionIn this paper, Scalable Term Selection (STS) is pro-posed and supposed to be more adaptive than tra-ditional high-performing criteria, viz.
?2, IG, BNS,etc.
The basic idea of STS is to separately measurediscriminability and coverage, and adjust the relativeimportance between them to produce a optimal termsubset of a given size.
Empirically, the constant re-lation between target dimensionality and the optimalrelative average vector length is found, which turnedthe idea into implementation.STS showed considerable adaptivity and stabilityfor various dimensionalities and quite different doc-ument collections.
The categorization accuracy in-creasing at low dimensionalities and the computa-tional cost decreasing at high dimensionalities wereobserved.Some observations are notable: the loglinear rela-tion between optimal average vector length (AVL?
)and dimensionality (k), the semi-loglinear relationbetween weight ?
and dimensionality, and the uni-versal constant ?.
For a future work, STS needs to beconducted on more document collections to check if?
is really universal.In addition, there could be other implementationsof the general STS idea, via other metrics of discrim-inability and coverage, other weighted combinationforms, or other term subset evaluations.AcknowledgementThe research is supported by the National NaturalScience Foundation of China under grant number60573187, 60621062 and 60520130299.ReferencesJanez Brank, Marko Grobelnik, Natas?a Milic-Fraylingand, and Dunjia Mladenic.
2002.
Interactionof feature selection methods and linear classifica-tion models.
Workshop on Text Learning held atICML-2002.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.
Software avail-able at http://www.csie.ntu.edu.tw/?cjlin/libsvm.781George Forman.
2003.
An extensive empirical study offeature selection metrics for text classification.
Jour-nal of Machine Learning Research, 3:1289?1305.Evgeniy Gabrilovich and Shaul Markovitch.
2004.
Textcategorization with many redundant features: usingaggressive feature selection to make svms competitivewith c4.5.
In ICML ?04: Proceedings of the twenty-first international conference on Machine learning,page 41, New York, NY, USA.
ACM Press.Isabelle Guyon and Andre?
Elisseeff.
2003.
An intro-duction to variable and feature selection.
Journal ofMachine Learning Research, 3:1157?1182.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: learning with many relevant fea-tures.
In Proceedings of ECML ?98, number 1398,pages 137?142.
Springer Verlag, Heidelberg, DE.Jingyang Li, Maosong Sun, and Xian Zhang.
2006.
Acomparison and semi-quantitative analysis of wordsand character-bigrams as features in chinese text cat-egorization.
In Proceedings of COLING-ACL ?06,pages 545?552.
Association for Computational Lin-guistics, July.Michael Mitzenmacher.
2003.
A brief history of genera-tive models for power law and lognormal distributions.Internet Mathematics, 1:226?251.Monica Rogati and Yiming Yang.
2002.
High-performing feature selection for text classification.In Proceedings of CIKM ?02, pages 659?661.
ACMPress.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Computing Surveys(CSUR), 34(1):1?47.Yiming Yang and Christopher G. Chute.
1994.
Anexample-based mapping method for text categoriza-tion and retrieval.
ACM Transactions on InformationSystems (TOIS), 12(3):252?277.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Douglas H. Fisher, editor, Proceedings of ICML-97, 14th International Conference on Machine Learn-ing, pages 412?420, Nashville, US.
Morgan Kauf-mann Publishers, San Francisco, US.782
