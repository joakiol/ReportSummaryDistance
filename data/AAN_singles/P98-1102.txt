Unification-based Multimodal ParsingMichael JohnstonCenter for Human Computer CommunicationDepartment of  Computer Science and EngineeringOregon Graduate InstituteP.O.
Box 91000, Portland, OR 97291-1000johnston @ cse.ogi.eduAbstractIn order to realize their full potential, multimodal systemsneed to support not just input from multiple modes, butalso synchronized integration of modes.
Johnston et al(1997) model this integration using a unification opera-tion over typed feature structures.
This is an effective so-lution for a broad class of systems, but limits multimodalutterances to combinations of a single spoken phrase witha single gesture.
We show how the unification-based ap-proach can be scaled up to provide a full multimodalgrammar formalism.
In conjunction with a multidimen-sional chart parser, this approach supports integration ofmultiple lements distributed across the spatial, temporal,and acoustic dimensions of multimodal interaction.
In-tegration strategies are stated in a high level unification-based rule formalism supporting rapid prototyping and it-erative development of multimodal systems.1 IntroductionMultimodal interfaces enable more natural and effi-cient interaction between humans and machines byproviding multiple channels through which input oroutput may pass.
Our concern here is with multi-modal input, such as interfaces which support simul-taneous input from speech and pen.
Such interfaceshave clear task performance and user preference ad-vantages over speech only interfaces, in particularfor spatial tasks such as those involving maps (Ovi-att 1996).
Our focus here is on the integration of in-put from multiple modes and the role this plays in thesegmentation a d parsing of natural human input.
Inthe examples given here, the modes are speech andpen, but the architecture described is more generalin that it can support more than two input modes andmodes of other types such as 3D gestural input.Our multimodal interface technology is imple-mented in QuickSet (Cohen et al1997), a work-ing system which supports dynamic interaction withmaps and other complex visual displays.
The initialapplications of QuickSet are: setting up and inter-acting with distributed simulations (C urtemancheand Cercanowicz 1995), logistics planning, and nav-igation in virtual worlds.
The system is distributed;consisting of a series of agents (Figure 1) whichcommunicate hrough a shared blackboard (Cohenet al1994).
It runs on both desktop and handheldPCs, communicating over wired and wireless LANs.The user interacts with a map displayed on a wirelesshand-held unit (Figure 2).Figure 1: Multimodal Architecture~cm -~ ~Figure 2: User InterfaceThey can draw directly on the map and simultane-ously issue spoken commands.
Different kinds ofentities, lines, and areas may be created by drawingthe appropriate spatial features and speaking theirtype; for example, drawing an area and saying 'floodzone'.
Orders may also be specified; for example,by drawing a line and saying 'helicopterfollow thisroute'.
The speech signal is routed to an HMM-624based continuous peaker-independent r cognizer.The electronic 'ink' is routed to a neural net-basedgesture recognizer (Pittman 1991).
Both generateN-best lists of potential recognition results with as-sociated probabilities.
These results are assigned se-mantic interpretations by natural language process-ing and gesture interpretation agents respectively.A multimodal integrator agent fields input from thenatural anguage and gesture interpretation agentsand selects the appropriate multimodal or unimodalcommands to execute.
These are passed on to abridge agent which provides an API to the underly-ing applications the system is used to control.In the approach to multimodal integration pro-posed by Johnston et al1997, integration of spokenand gestural input is driven by a unification opera-tion over typed feature structures (Carpenter 1992)representing the semantic contributions ofthe differ-ent modes.
This approach overcomes the limitationsof previous approaches in that it allows for a fullrange of gestura~ input beyond simple deictic point-ing gestures.
Unlike speech-driven systems (Bolt1980, Neal and Shapiro 1991, Koons et al1993,Wauchope 1994), it is fully multimodal in that all el-ements of the content of a command can be in ei-ther mode.
Furthermore, compared to related frame-merging strategies (Vo and Wood 1996), it providesa well understood, generally applicable commonmeaning representation forthe different modes anda formally well defined mechanism for multimodalintegration.
However, while this approach providesan efficient solution for a broad class of multimodalsystems, there are significant limitations on the ex-pressivity and generality of the approach.A wide range of potential multimodal utterancesfall outside the expressive potential of the previousarchitecture.
Empirical studies of multimodal in-teraction (Oviatt 1996), utilizing wizard-of-oz tech-niques, have shown that when users are free to inter-act with any combination ofspeech and pen, a singlespoken utterance maybe associated with more thanone gesture.
For example, a number of deictic point-ing gestures may be associated with a single spo-ken utterance: ' calculate distance from here to bere','put that there', 'move this team to here and prepareto rescue residents from this building'.
Speech mayalso be combined with a series of gestures of differ-ent types: the user circles a vehicle on the map, says'follow this route', and draws an arrow indicatingthe route to be followed.In addition to more complex multipart multi-modal utterances, unimodal gestural utterances maycontain several component gestures which composeto yield a command.
For example, to create an entitywith a specific orientation, a user might draw the en-tity and then draw an arrow leading out from it (Fig-ure 3 (a)).
To specify a movement, the user mightdraw an arrow indicating the extent of the move andindicate departure and arrival times by writing ex-pressions at the base and head (Figure 3 (b)).
TheseI I z'?lFigure 3: Complex Unimodal Gesturesare specific examples of the more general problem ofvisual parsing, which has been a focus of attentionin research on visual programming and pen-basedinterfaces for the creation of complex graphical ob-jects such as mathematical equations and flowcharts(Lakin 1986, Wittenburg et al1991, Helm et al1991,Crimi et al1995).The approach of Johnston et al1997 also facesfundamental rchitectural problems.
The multi-modal integration strategy ishard-coded into the in-tegration agent and there is no isolatable statementof the rules and constraints independent of the codeitself.
As the range of multimodal utterances sup-ported is extended, it becomes essential that therebe a declarative statement of the grammar of multi-modal utterances, separate from the algorithms andmechanisms ofparsing.
This will enable system de-velopers to describe integration strategies in a highlevel representation, facilitating rapid prototypingand iterative development of multimodal systems.2 Pars ing in Mult id imensional  SpaceThe integrator inJohnston et al1997 does in essenceparse input, but the resulting structures can only beunary or binary trees one level deep; unimodal spo-ken or gestural commands and multimodal combina-tions consisting of a single spoken element and a sin-gle gesture.
In order to account for a broader angeof multimodal expressions, a more general parsingmechanism is needed.Chart parsing methods have proven effective forparsing strings and are commonplace in naturallanguage processing (Kay 1980).
Chart parsinginvolves population of a triangular matrix ofwell-formed constituents: chart(i, j ) ,  where i andj are numbered vertices delimiting the start andend of the string.
In its most basic formulation,chart parsing can be defined as follows, where .is an operator which combines two constituents inaccordance with the rules of the grammar.chart(i, j) = U chart(i, k) * chart(k, j)i<k<jCrucially, this requires the combining constituentsto be discrete and linearly ordered.
However,multimodal input does not meet hese requirements:625gestural input spans two (or three) spatial dimen-sions, there is an additional non-spatial acousticdimension of speech, and both gesture and speechare distributed across the temporal dimension.Unlike words in a string, speech and gesture mayoverlap temporally, and there is no single dimensionon which the input is linear and discrete.
So then,how can we parse in this multidimensional space ofspeech and gesture?
What is the rule for chart pars-ing in multi-dimensional space?
Our formulation ofmultidimensional parsing for multimodal systems(multichart) is as follows.multichart(X) = U multichart(Y) * multichart(Z)where X = Y uz ,  Y nZ  = O,Y ~ 0,2 ~In place of numerical spans within a singledimension (e.g.
chart(3,5)), edges in the mul-tidimensional chart are identified by sets (e.g.multichart({\[s, 4, 2\], \[g, 6, 1\]})) containing theidentifiers(IDs) of the terminal input elementsthey contain.
When two edges combine, the ID ofthe resulting edge is the union of their IDs.
Oneconstraint that linearity enforced, which we can stillmaintain, is that a given piece of input can only beused once within a single parse.
This is captured bya requirement of non-intersection between the IDsets associated with edges being combined.
Thisrequirement is especially important since a singlepiece of spoken or gestural input may have multipleinterpretations available in the chart.
To preventmultiple interpretations of a single signal beingused, they are assigned IDs which are identical withrespect o the the non-intersection constraint.
Themultichart statement enumerates all the possiblecombinations that need to be considered given a setof inputs whose IDs are contained in a set X.The multidimensional parsing algorithm (Figure4) runs bottom-up from the input elements, build-ing progressively arger constituents in accordancewith the ruleset.
An agenda is used to store edgesto be processed.
As a simplifying assumption, rulesare assumed to be binary.
It is straightforward to ex-tend the approach to allow for non-binary rules usingtechniques from active chart parsing (Earley 1970),but this step is of limited value given the availabilityof multimodal subcategorization (Section 4).while AGENDA ?
\[ \] doremove front edge from AGENDAand make it CURRENTEDGEfor each EDGE, EDGE E CHARTif CURRENTEDGE (1 EDGE =find set NEWEDGES = U ((U CURRENTEDGE * EDGE)(U EDGE * CURRENTEDGE))add NEWEDGES to end of AGENDAadd CURRENTEDGE to CHARTFigure 4: Multichart Parsing AlgorithmFor use in a multimodal interface, the multidi-mensional parsing algorithm needs to be embeddedinto the integration agent in such a way that inputcan be processed incrementally.
Each new input re-ceived is handled as follows.
First, to avoid unnec-essary computation, stale edges are removed fromthe chart.
A timeout feature indicates the shelf-life of an edge within the chart.
Second, the in-terpretations of the new input are treated as termi-nal edges, placed on the agenda, and combined withedges in the chart in accordance with the algorithmabove.
Third, complete dges are identified and ex-ecuted.
Unlike the typical case in string parsing, thegoal is not to find a single parse covering the wholechart; the chart may contain several complete non-overlapping edges which can be executed.
Theseare assigned to a category command as describedin the next section.
The complete dges are rankedwith respect to probability.
These probabilities area function of the recognition probabilities of the el-ements which make up the comrrrand.
The com-bination of probabilities is specified using declar-ative constraints, as described in the next section.The most probable complete dge is executed first,and all edges it intersects with are removed from thechart.
The next most probable complete dge re-maining is then executed and the procedure contin-ues until there are no complete dges left in the chart.This means that selection of higher probability com-plete edges eliminates overlapping complete dgesof lower probability from the list of edges to be ex-ecuted.
Lastly, the new chart is stored.
In ongoingwork, we are exploring the introduction of other fac-tors to the selection process.
For example, sets ofdisjoint complete dges which parse all of the termi-nal edges in the chart should likely be preferred overthose that do not.Under certain circumstances, anedge can be usedmore than once.
This capability supports multiplecreation of entities.
For example, the user can utter'multiple helicopters' point point point point in or-der to create a series of vehicles.
This significantlyspeeds up the creation process and limits relianceon speech recognition.
Multiple commands are per-sistent edges; they are not removed from the chartafter they have participated in the formation of anexecutable command.
They are assigned timeoutsand are removed when their alloted time runs out.These 'self-destruct' timers are zeroed each time an-other entity is created, allowing creations to chaintogether.3 Uni f icat ion-based Mu l t imoda lGrammar  RepresentationOur grammar representation formultimodal expres-sions draws on unification-based approaches tosyn-tax and semantics (Shieber 1986) such as Head-626driven phrase structure grammar (HPSG) (Pollardand Sag 1987,1994).
Spoken phrases and pen ges-tures, which are the terminal elements of the mul-timodal parsing process, are referred to as lexicaledges.
They are assigned grammatical representa-tions in the form of typed feature structures by thenatural anguage and gesture interpretation agentsrespectively.
For example, the spoken phrase "heli-copter is assigned the representation in Figure 5.cat  : unit.typef sTYPE  : unitcontent  : ob jec t  : type  : helicoptereche lon  : vehiclel ocat ion  : \[ f sTYPE  : point \]modal l ty  : speecht ime : interval(.., ..)prob  : 0 .85Figure 5: Spoken Input EdgeThe cat feature indicates the basic category of theelement, while content specifies the semantic on-tent.
In this case, it is a create_unit command inwhich the object o be created is a vehicle of typehelicopter, and the location is required to be a point.The remaining features pecify auxiliary informa-tion such as the modality, temporal interval, andprobability associated with the edge.
A point ges-ture has the representation in Figure 6.t r f sTYPE  : po in tconten  : L coord  : latlong(.., ..) \]modal i t \ ] t  : gesturet ime : interval(.,, ..)prob  : 0 .69Figure 6: Point Gesture EdgeMultimodal grammar rules are productions oftheform LHS --r DTR1 DTR2 where LHS, DTR1,and DTR2 are feature structures of the form indi-cated above.
Following HPSG, these are encodedas feature structure rule schemata.
One advantageof this is that rule schemata can be hierarchicallyordered, allowing for specific rules to inherit ba-sic constraints from general rule schemata.
The ba-sic multimodal integration strategy of Johnston et al1997 is now just one rule among many (Figure 7).content  : \[1\]l hs  : moda l i t~/  : \[2\]t ime : \[3 Iprob  : \ [4 \ ]content  : \ [ I \ ]  \[ l ocat ion  : \[51 \]d t r l  : moda l l t?
: \[6\]t ime : {7\]rhs  : p rob  : \ [8\]  cat:spatial .gesture "\[content  : \[5\] \]d t r2  : moda l i ty  : \[9\] \[ time: {,ol /prob  : \ [11 \ ]  J( lap(\[7\],\[lO\]) V \]ollow(\[7\],\[lO\],4) t .
.
.
.
total.tirne(\[7\],\[lOl, \ [3\] )const ra in ts :  combine-prob(Ial, \[I I\], {,1\])amsign.modahty(\[6\] ,\[9\],\[2\])Figure 7: Basic Integration Rule SchemaThe lhs,dtrl, and dtr2 features correspond toLHS, DTR1, and DTR2 in the rule above.
Theconstraints feature indicates an ordered series ofconstraints which must be satisfied in order for therule to apply.
Structure-sharing  the rule represen-tation is used to impose constraints on the input fea-ture structures, toconstruct the LHS category, andto instantiate he variables in the constraints.
For ex-ample, in Figure 7, the basic constraint that the lo-cation of a located command such as 'helicopter'needs to unify with the content of the gesture itcom-bines with is captured by the structure-sharing ta\[5\].
This also instantiates the location of the result-ing edge, whose content is inherited through tag \[1 \].The application of a rule involves unifying thetwo candidate dges for combination against dtr land dtr2.
Rules are indexed by their cat feature inorder to avoid unnecessary unification.
If the edgesunify with dtr l  and dtr2, then the constraints arechecked.
If they are satisfied then a new edge is cre-ated whose category is the value of lhs and whoseID set consists of the union of the ID sets assignedto the two input edges.Constraints require certain temporal and spatialrelationships tohold between edges.
Complex con-straints can be formed using the basic logical op-erators V ,  A, and =?,.
The temporal constraint inFigure 7, overlap(J7\], \[10\]) V follow(\[7\],\[lO\], 4),states that the time of the speech \[7\] must eitheroverlap with or start within four seconds of the timeof the gesture \[10\].
This temporal constraint isbased on empirical investigation of multimodal in-teraction (Oviatt et al1997).
Spatial constraints areused for combinations of gestural inputs.
For ex-ample, close_to(X, Y) requires two gestures to bea limited distance apart (See Figure 12 below) andcontact(X, Y) determines whether the regions oc-cupied by two objects are in contact.
The remainingconstraints in Figure 7 do not constrain the inputs perse, rather they are used to calculate the time, prob,and modality features for the resulting edge.
Forexample, the constraint combine_prob(\[8\], \[11\], \[4\])is used to combine the probabilities of two inputsand assign a joint probability to the resulting edge.In this case, the input probabilities are multiplied.The assign_modality(\[6\], \[9\],\[2\]) constraint deter-mines the modality of the resulting edge.
Auxiliaryfeatures and constraints which are not directly rele-vant to the discussion will be omitted.The constraints are interpreted using a prologmeta-interpreter.
This basic back-tracking con-straint satisfaction strategy issimplistic but adequatefor current purposes.
It could readily be substi-tuted with a more sophisticated constraint solvingstrategy allowing for more interaction among con-straints, default constraints, optimization among aseries of constraints, and so on.
The addition offunctional constraints is common in HPSG and otherunification grammar formalisms (Wittenburg 1993).6274 Multimodal SubcategorizationGiven that multimodal grammar rules are required tobe binary, how can the wide variety of commands inwhich speech combines with more than one gesturalelement be accounted for?
The solution to this prob-lem draws on the lexicalist treatment ofcomplemen-tation in HPSG.
HPSG utilizes a sophisticated the-ory of subcategorization t  account for the differentcomplementation patterns that verbs and other lexi-cal items require.
Just as a verb subcategorizes forits complements, we can think of a lexical edge inthe multimodal grammar as subcategorizing for theedges with which it needs to combine.
For example,spoken inputs such as 'calculate distance from hereto here' an d ' sandbag wall from here to here' (Figure8) result in edges which subcategorize for two ges-tures.
Their multimodal subcategorization is speci-fied in a list valued subcat feature, implemented us-ing a recursive first/rest feature structure (Shieber1986:27-32).
"eat  : subcat .command" f sTYPE  : create.l ine "lr f sTYPE  : wall.obj\]content  : ob jec t  : \ ] s ty le  : sand.bag |Lco lo r  : grey J?
r f sTYPE  : l ine \]l ocat ion  .
Lcoord l i s t  : \[\[I\],  \[2\]\]Jt ime : \[31r Feat  : spatial.ge#ture "~/ r f sTYPE  : point3 If irst: |content:  \[ .
.
.
.
d:\[1\]  J/Ltime : \[4\] Jconst ra in ts  : \[overlap(J3\], [4\]) V \]ollow(\[3\], \[4\],4)\]subcat  : 1 r teat : spatial.gesture ~ ~l\] \] \[ I" f sTYPE  : point1 I I/ |first : lcontent : \ [coord " f21 | | \[i rest: l t t ime:  \[,\] " "J /l lconstraints : \[lollo=(\[S\], \[41,S)\] /L Lres t  : end JFigure 8: 'Sandbag wall from here to here'The cat feature is subcat_comrnand, indicatingthat this is an edge with an unsaturated subcatego-rization list.
The first/rest structure indicates thetwo gestures the edge needs to combine with and ter-minates with rest: end.
The temporal constraintson expressions such as these are specific to the ex-pressions themselves and cannot be specified in therule constraints.
To support this, we allow for lexicaledges to carry their own specific lexical constraints,which are held in a constraints feature at each levelin the subeat list.
In this case, the first gesture isconstrained to overlap with the speech or come upto four seconds before it and the second gesture isrequired to follow the first gesture.
Lexical con-straints are inherited into the rule constraints in thecombinatory schemata described below.
Edges withsubcat features are combined with other elementsin the chart in accordance with general combinatoryschemata.
The first (Figure 9) applies to unsaturatededges which have more than one element on theirsubcat list.
It unifies the first element of the sub-cat list with an element in the chart and builds a newedge of category subcat_command whose subcat listis the value of rest.content  : \[1\]l hs  : subcat  :.\[2\]p rob  : \[31\[ content  : \[1\]/ I" f irst : \[4\]rhs: dtra : \[ subcat : \[ const .
.
.
.
ts: \[Sl/ L rest:J21| \]L prob  : \[6\]L d t r2  : \[41\[ p rob :  \[71 Jconstraints : { combine.prob(\[6\],\[7\], \[3\])I \[51 }Figure 9: Subcat Combination SchemaThe second schema (Figure 10) applies to unsat-urated (cat: subcat_command) e ges on whose sub-cat list only one element remains and generates sat-urated (cat: command) edges.content  : \[1\]lhs : subcat : endprob : \[2\]/ content : \[1\]rhs :  dt r l  : / .
.
.
.
.
t \[ cflor~ttr\[3\] L r:0 \[:5 \[ rest: en: tS :  \[4\] \]L dtr2 : \ [3\] \ [  prob : t61 \]constraints: { cornbir=e.prob(\[5\], \[O\], \[21) I \[4\] }Figure 10: Subcat Termination SchemaThis specification of combinatory information inthe lexical edges constitutes a shift from rules torepresentations.
The ruleset is simplified to a setof general schemata, and the lexical representa-tion is extended to express combinatorics.
How-ever, there is still a need for rules beyond thesegeneral schemata in order to account for construc-tional meaning (Goldberg 1995) in multimodal in-put, specifically with respect to complex unimodalgestures.5 Visual Parsing: Complex GesturesIn addition to combinations of speech with morethan one gesture, the architecture supports unimodalgestural commands consisting of several indepen-dently recognized gestural components.
For exam-ple, lines may be created using what we term gestu-ral diacritics.
If environmental noise or other fac-tors make speaking the type of a line infeasible, itmay be specified by drawing a simple gestural markor word over a line gesture.
To create abarbed wire,the user can draw a line specifying its spatial extentand then draw an alpha to indicate its type.Figure 1 1: Complex Gesture for Barbed WireThis gestural construction is licensed by the ruleschema in Figure 12.
It states that a line gesture628(dtrl) and an alpha gesture (dtr2) can be combined,resulting in a command to create abarbed wire.
Thelocation information is inherited from the line ges-ture.
There is nothing inherent about alpha thatmakes it mean 'barbed wire'.
That meaning is em-bodied only in its construction with a line gesture,which is captured in the rule schema.
The close_toconstraint requires that the centroid of the alpha bein proximity to the line.cat  : command "1 J f sTYPE  : wire.ob 3 lhs : content  : ob jec t  : co lo r  : red style : barbedl ocat ion  : \[I\]dt r l  : content  : \[1\] coord l l s t  : \[21rhs  : t ime : \[3\]F ca t  : spat ia l .gesture  1?
| content : \ [  f sTYPE:a lpha  \] l dt r2  .
| cent ro id  : \[41L t ime : \[5\]f Iollow(\[5\],\[3\],5)const ra in ts  : i, close.to(\[4\],\[2\])Figure 12: Rule Schema for Unimodal Barbed Wire6 ConclusionThe multimodal language processing architecturepresented here enables parsing and interpretation fnatural human input distributed across two or threespatial dimensions, time, and the acoustic dimensionof speech.
Multimodal integration strategies arestated eclaratively in a unification-based grammarformalism which is interpreted by an incrementalmultidimensional p rser.
We have shown how thisarchitecture supports multimodal (pen/voice) inter-faces to dynamic maps.
It has been implemented anddeployed as part of QuickSet (Cohen et al1997) andoperates in real time.
A broad range of multimodalutterances are supported including combination ofspeech with multiple gestures and visual parsing ofcollections of gestures into complex unimodal com-mands.
Combinatory information and constraintsmay be stated either in the lexical edges or in the ruleschemata, llowing individual phenomena to be de-scribed in the way that best suits their nature.
The ar-chitecture is sufficiently general to support other in-put modes and devices including 3D gestural input.The declarative statement ofmultimodal integrationstrategies nables rapid prototyping and iterative de-velopment of multimodal systems.The system has undergone a form of pro-activeevaluation i  that its design is informed by detailedpredictive modeling of how users interact multi-modally, and incorporates the results of empiricalstudies of multimodal interaction (Oviatt 1996, Ovi-att et al1997).
It is currently undergoing extensiveuser testing and evaluation (McGee et al1998).Previous work on grammars and parsing for mul-tidimensional languages has focused on two dimen-sional graphical expressions such as mathematicalequations, flowcharts, and visual programming lan-guages.
Lakin (1986) lays out many of the ini-tial issues in parsing for two-dimensional draw-ings and utilizes pecialized parsers implemented inLISP to parse specific graphical languages.
Helmet al(1991) employ a grammatical framework, con-strained set grammars, in which constituent s ruc-ture rules are augmented with spatial constraints.Visual language parsers are build by translation ofthese rules into a constraint logic programming lan-guage.
Crimi et al(1991) utilize a similar relationgrammar formalism in which a sentence consistsof a multiset of objects and relations among them.Their rules are also augmented with constraints andparsing is provided by a prolog axiomatization.
Wit-tenburg et al(1991) employ a unification-basedgrammar formalism augmented with functional con-straints (F-PATR, Wittenburg 1993), and a bottom-up, incremental, Earley-style (Earley 1970) tabularparsing algorithm.All of these approaches face significant difficul-ties in terms of computational complexity.
At worst,an exponential number of combinations of the in-put elements need to be considered, and the parsetable may be of exponential size (Wittenburg et al1991:365).
Efficiency concerns drive Helm et al(1991:111) to adopt a committed choice strategyunder which successfully applied productions can-not be backtracked over and complex negative andquantificational constraints are used to limit rule ap-plication.
Wittenburg et als parsing mechanism isdirected by expander relations in the grammar for-malism which filter out inappropriate combinationsbefore they are considered.
Wittenburg (1996) ad-dresses the complexity issue by adding top-downpredictive information to the parsing process.This work is fundamentally different from allof these approaches in that it focuses on multi-modal systems, and this has significant implicationsin terms of computational viability.
The task dif-fers greatly from parsing of mathematical equations,flowcharts, and other complex graphical expressionsin that the number of elements to be parsed is farsmaller.
Empirical investigation (Oviatt 1996, Ovi-att et al1997) has shown that multimodal utter-ances rarely contain more than two or three ele-ments.
Each of those elements may have multi-ple interpretations, but the overall number of lexi-cal edges remains ufficiently small to enable fastprocessing of all the potential combinations.
Also,the intersection constraint on combining edges lim-its the impact of the multiple interpretations of eachpiece of input.
The deployment of this architecturein an implemented system supporting real time spo-ken and gestural interaction with a dynamic mapprovides evidence of its computational viability forreal tasks.
Our approach is similar to Wittenburg et629al 1991 in its use of a unification-based grammar for-malism augmented with functional constraints anda chart parser adapted for multidimensional spaces.Our approach differs in that, given the nature of theinput, using spatial constraints and top-down predic-tive information to guide the parse is less of a con-cern, and as a result the parsing algorithm is signifi-cantly more straightforward and general.The evolution of multimodal systems is follow-ing a trajectory which has parallels in the historyof syntactic parsing.
Initial approaches to multi-modal integration were largely algorithmic in na-ture.
The next stage is the formulation of declarativeintegration rules (phrase structure rules), then comesa shift from rules to representations (lexicalism, cat-egorial and unification-based grammars).
The ap-proach outlined here is at representational stage, al-though rule schemata are still used for constructionalmeaning.
The next phase, which syntax is under-going, is the compilation of rules and representa-tions back into fast, low-powered finite state devices(Roche and Schabes 1997).
At this early stage in thedevelopment of multimodal systems, we need a highdegree of flexibility.
In the future, once it is clearerwhat needs to be accounted for, the next step will beto explore compilation of multimodal grammars intolower power devices.Our primary areas of future research include re-finement of the probability combination scheme formultimodal utterances, exploration of alternativeconstraint solving strategies, multiple inheritancefor rule schemata, maintenance of multimodal di-alogue history, and experimentation with 3D inputand other combinations of modes.ReferencesBolt, R.  A .
1980.
"Put-That-There":Voice and gesture atthe graphics interface.
ComputerGraphics, 14.3:262-270.Carpenter, R. 1992.
The logic of typed feature structures.Cambridge University Press, Cambridge, England.Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg.
1994.An open agent architecture.
In Working Notes of theAAAI Spring Symposium onSoftware Agents, 1-8.Cohen, P. R., M. Johnston, D. McGee, S. L. Oviatt, J.A.
Pittman, I. Smith, L. Chen, and J. Clow.
1997.?
QuickSet: Multimodal interaction for distributed ap-plications.
In Proceedings of the Fifth ACM Interna-tional Multimedia Conference.
31-40.Courtemanche, A. J., and A. Ceranowicz.
1995.
Mod-SAF development s atus.
In Proceedings of the 5thConference on Computer Generated Forces and Be-havioral Re_presentation, 3-13.Crimi, A, A. Guercio, G. Nota, G. Pacini, G. Tortora, andM.
Tucci.
1991.
Relation grammars and their applica-tion to multi-dimensionallanguages.
Journal of VisualLanguages and Computing, 2:333-346.Earley, J.
1970.
An efficient context-free parsing algo-rithm.
Communications f the ACM, 13, 94--102.Goldberg, A.
1995.
Constructions: A ConstructionGrammar Approach to Argument Structure.
Univer-sity of Chicago Press, Chicago.Helm, R., K. Marriott, and M. Odersky.
1991.
Buildingvisual language parsers.
In Proceedings ofConferenceon Human Factors in Computing Systems: CHI 91,ACM Press, New York, 105-112.Johnston, M., P. R. Cohen, D. McGee, S. L. Oviatt, J. A.Pittman, and I. Smith.
1997.
Unification-based multi-modal integration.
In Proceedings ofthe 35th AnnualMeeting of the Association for Computational Linguis-tics and 8th Conference ofthe European Chapter of theAssociation for Computational Linguistics, 281-288.Kay, M. 1980.
Algorithm schemata and data structuresIn syntactic processing.
In B. J. Grosz, K. S. Jones, andB.
L. Webber (eds.)
Readings in Natural LanguageProcessing, Morgan Kaufmann, 1986, 35-70.Koons, D. B., C. J.Sparrell, and K. R. Thorisson.
1993.Integrating simultaneous input from speech, gaze, andhand gestures.
In M. T. Maybury (ed.)
IntelligentMul-timedia Interfaces, MIT Press, 257-276.Lakin, E 1986.
Spatial parsing for visual languages.In S. K. Chang, T. Ichikawa, and E A. Ligomenides(ed.s), Ifsual Languages.
Plenum Press, 35-85.McGee, D., P. R. Co-hen, S. L. Oviatt.
1998.
Confirma-tion in multimodal systems.
In Proceedings ofl7th In-ternational Conference on Computational Linguisticsand 36th Annual Meeting of the Association for Com-putational Linguistics.Neal, J. G., and S. C. Shapiro.
1991.
Intelligent multi-media interface technology.
In J. W. Sullivan andS.
W. Tyler (eds.)
Intelligent User Interfaces, ACMPress, Addison Wesley, New York, 45-68.Oviatt, S.L.
1996.
Multimodal interfaces for dynamicinteractive maps.
In Proceedings of Conference onHuman Factors in Co.m.puting Systems, 95-102.Oviatt, S. L., A. DeAngeli, and K. Kuhn.
1997.
Integra-tion and synchronization f input modes during multi-modal human-computer interaction.
In Proceedings ofConference on Human Factors in Computing Systems,415-422.Pittman, J.A.
1991.
Recognizing handwritten text.In Proceedings of Conference on Human Factors inComputing Systems: CHI 91.271-275.Pollard, C. J., and I.
A.
Sag.
1987.
Information-basedsyntax and semantics: Volume L Fundamentals., CSLILecture Notes Volume 13.
CSLI, Stanford.Pollard, Carl and Ivan Sag.
1994.
Head-drivenhrase structure grammar.
University of Chicagoress.
Chicago.Roche, E. and Y. Schabes.
1997.
Finite state languageprocessing.
MIT Press, Cambridge.Shleber, S.M.
1986.
An Introauction to unification-based approaches togrammar.
CSLI Lecture NotesVolume 4.
CSLI, Stanford.Vo, M. T., and C. Wood.
1996.
Building an applica-tion framework for speech and pen input integrationin multimodal learning interfaces.
In Proceedmgs ofICASSP'96.Wauchope, K. 1994.
Eucalyptus: Integrating naturallanguage input with a graphical user interface.
NavalResearch Laboratory, Report NRL/FR/5510-94-9711.Wittenburg, K., L. Weitzman, and J. Talley.
1991.Unification-Based grammars and tabular parsing forgraphical languages.
Journal of Visual Languages andComputing 2:347-370.wmenburg, "K. L. 1993.
F-PATR: Functional con-straints for unification-based grammars.
Proceedingsof the 31st Annual Meeting of the Association for Com-putational Linguistics, 216-223.Wittenburg, K. 1996.
Predictive parsing for unorderedrelational languages.
In H. Bunt and M. Tomita (eds.
),Recent Advances in Parsing Technologies, Kluwer,Dordrecht, 385-407.630
