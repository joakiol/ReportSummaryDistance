Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55?63,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsImproving Summarization of Biomedical Documentsusing Word Sense DisambiguationLaura Plaza?lplazam@fdi.ucm.esMark Stevenson?m.stevenson@dcs.shef.ac.uk?
Universidad Complutense de Madrid, C/Prof.
Jose?
Garc?
?a Santesmases, 28040 Madrid, Spain?
University of Sheffield, Regent Court, 211 Portobello St., Sheffield, S1 4DP, UKAlberto D?
?az?albertodiaz@fdi.ucm.esAbstractWe describe a concept-based summariza-tion system for biomedical documents andshow that its performance can be improvedusing Word Sense Disambiguation.
Thesystem represents the documents as graphsformed from concepts and relations fromthe UMLS.
A degree-based clustering al-gorithm is applied to these graphs to dis-cover different themes or topics withinthe document.
To create the graphs, theMetaMap program is used to map thetext onto concepts in the UMLS Metathe-saurus.
This paper shows that applying agraph-based Word Sense Disambiguationalgorithm to the output of MetaMap im-proves the quality of the summaries thatare generated.1 IntroductionExtractive text summarization can be defined asthe process of determining salient sentences in atext.
These sentences are expected to condensethe relevant information regarding the main topiccovered in the text.
Automatic summarization ofbiomedical texts may benefit both health-care ser-vices and biomedical research (Reeve et al, 2007;Hunter and Cohen, 2006).
Providing physicianswith summaries of their patient records can helpto reduce the diagnosis time.
Researchers can usesummaries to quickly determine whether a docu-ment is of interest without having to read it all.Summarization systems usually work with arepresentation of the document consisting of in-formation that can be directly extracted from thedocument itself (Erkan and Radev, 2004; Mihalceaand Tarau, 2004).
However, recent studies havedemonstrated the benefit of summarization basedon richer representations that make use of externalknowledge sources (Plaza et al, 2008; Fiszman etal., 2004).
These approaches can represent seman-tic associations between the words and terms in thedocument (i.e.
synonymy, hypernymy, homonymyor co-occurrence) and use this information to im-prove the quality of the summaries.
In the biomed-ical domain the Unified Medical Language Sys-tem (UMLS) (Nelson et al, 2002) has proved tobe a useful knowledge source for summarization(Fiszman et al, 2004; Reeve et al, 2007; Plaza etal., 2008).
In order to access the information con-tained in the UMLS, the vocabulary of the doc-ument being summarized has to be mapped ontoit.
However, ambiguity is common in biomedi-cal documents (Weeber et al, 2001).
For exam-ple, the string ?cold?
is associated with seven pos-sible meanings in the UMLS Metathesuarus in-cluding ?common cold?, ?cold sensation?
, ?coldtemperature?
and ?Chronic Obstructive AirwayDisease?.
The majority of summarization sys-tems in the biomedical domain rely on MetaMap(Aronson, 2001) to map the text onto conceptsfrom the UMLS Metathesaurus (Fiszman et al,2004; Reeve et al, 2007).
However, MetaMap fre-quently fails to identify a unique mapping and, asa result, various concepts with the same score arereturned.
For instance, for the phrase ?tissues areoften cold?
MetaMap returns three equally scoredconcepts for the word ??cold?
: ?common cold?,?cold sensation?
and ?cold temperature?.The purpose of this paper is to study the ef-fect of lexical ambiguity in the knowledge sourceon semantic approaches to biomedical summariza-tion.
To this end, the paper describes a concept-based summarization system for biomedical doc-uments that uses the UMLS as an external knowl-edge source.
To address the word ambiguity prob-lem, we have adapted an existing WSD system(Agirre and Soroa, 2009) to assign concepts fromthe UMLS.
The system is applied to the summa-rization of 150 biomedical scientific articles fromthe BioMed Central corpus and it is found that55WSD improves the quality of the summaries.
Thispaper is, to our knowledge, the first to apply WSDto the summarization of biomedical documentsand also demonstrates that this leads to an im-provement in performance.The next section describes related work on sum-marization and WSD.
Section 3 introduces theUMLS resources used in the WSD and sum-marization systems.
Section 4 describes ourconcept-based summarization algorithm.
Section5 presents a graph-based WSD algorithm whichhas been adapted to assign concepts from theUMLS.
Section 6 describes the experiments car-ried out to evaluate the impact of WSD and dis-cusses the results.
The final section providesconcluding remarks and suggests future lines ofwork.2 Related workSummarization has been an active area withinNLP research since the 1950s and a variety of ap-proaches have been proposed (Mani, 2001; Afan-tenos et al, 2005).
Our focus is on graph-basedsummarization methods.
Graph-based approachestypically represent the document as a graph, wherethe nodes represent text units (i.e.
words, sen-tences or paragraphs), and the links represent co-hesion relations or similarity measures betweenthese units.
The best-known work in the area isLexRank (Erkan and Radev, 2004).
It assumes afully connected and undirected graph, where eachnode corresponds to a sentence, represented byits TF-IDF vector, and the edges are labeled withthe cosine similarity between the sentences.
Mi-halcea and Tarau (2004) present a similar methodwhere the similarity among sentences is measuredin terms of word overlaps.However, methods based on term frequenciesand syntactic representations do not exploit the se-mantic relations among the words in the text (i.e.synonymy, homonymy or co-occurrence).
Theycannot realize, for instance, that the phrases my-ocardial infarction and heart attack refer to thesame concepts, or that pneumococcal pneumoniaand mycoplasma pneumonia are two similar dis-eases that differ in the type of bacteria that causesthem.
This problem can be partially solved bydealing with concepts and semantic relations fromdomain-specific resources, rather than terms andlexical or syntactic relations.
Consequently, somerecent approaches have adapted existing methodsto represent the document at a conceptual level.
Inparticular, in the biomedical domain Reeve et al(2007) adapt the lexical chaining approach (Barzi-lay and Elhadad, 1997) to work with UMLS con-cepts, using the MetaMap Transfer Tool to anno-tate these concepts.
Yoo et al (2007) represent acorpus of documents as a graph, where the nodesare the MeSH descriptors found in the corpus, andthe edges represent hypernymy and co-occurrencerelations between them.
They cluster the MeSHconcepts in the corpus to identify sets of docu-ments dealing with the same topic and then gen-erate a summary from each document cluster.Word sense disambiguation attempts to solvelexical ambiguities by identifying the correctmeaning of a word based on its context.
Super-vised approaches have been shown to perform bet-ter than unsupervised ones (Agirre and Edmonds,2006) but need large amounts of manually-taggeddata, which are often unavailable or impractical tocreate.
Knowledge-based approaches are a goodalternative that do not require manually-taggeddata.Graph-based methods have recently been shownto be an effective approach for knowledge-basedWSD.
They typically build a graph for the text inwhich the nodes represent all possible senses ofthe words and the edges represent different kindsof relations between them (e.g.
lexico-semantic,co-occurrence).
Some algorithm for analyzingthese graphs is then applied from which a rank-ing of the senses of each word in the context isobtained and the highest-ranking one is chosen(Mihalcea and Tarau, 2004; Navigli and Velardi,2005; Agirre and Soroa, 2009).
These methodsfind globally optimal solutions and are suitable fordisambiguating all words in a text.One such method is Personalized PageRank(Agirre and Soroa, 2009) which makes use ofthe PageRank algorithm used by internet searchengines (Brin and Page, 1998).
PageRank as-signs weight to each node in a graph by analyz-ing its structure and prefers ones that are linked toby other nodes that are highly weighted.
Agirreand Soroa (2009) used WordNet as the lexicalknowledge base and creates graphs using the en-tire WordNet hierarchy.
The ambiguous words inthe document are added as nodes to this graph anddirected links are created from them to each oftheir possible meanings.
These nodes are assignedweight in the graph and the PageRank algorithm is56applied to distribute this information through thegraph.
The meaning of each word with the high-est weight is chosen.
We refer to this approachas ppr.
It is efficient since it allows all ambigu-ous words in a document to be disambiguated si-multaneously using the whole lexical knowledgebase, but can be misled when two of the possiblesenses for an ambiguous word are related to eachother in WordNet since the PageRank algorithmassigns weight to these senses rather than transfer-ring it to related words.
Agirre and Soroa (2009)also describe a variant of the approach, referredto as ?word to word?
(ppr w2w), in which a sep-arate graph is created for each ambiguous word.In these graphs no weight is assigned to the wordbeing disambiguated so that all of the informationused to assign weights to the possible senses of theword is obtained from the other words in the doc-ument.
The ppr w2w is more accurate but lessefficient due to the number of graphs that have tobe created and analyzed.
Agirre and Soroa (2009)show that the Personalized PageRank approachperforms well in comparison to other knowledge-based approaches to WSD and report an accuracyof around 58% on standard evaluation data sets.3 UMLSThe Unified Medical Language System (UMLS)(Humphreys et al, 1998) is a collection of con-trolled vocabularies related to biomedicine andcontains a wide range of information that canbe used for Natural Language Processing.
TheUMLS comprises of three parts: the SpecialistLexicon, the Semantic Network and the Metathe-saurus.The Metathesaurus forms the backbone of theUMLS and is created by unifying over 100 con-trolled vocabularies and classification systems.
Itis organized around concepts, each of which repre-sents a meaning and is assigned a Concept UniqueIdentifier (CUI).
For example, the following CUIsare all associated with the term ?cold?
: C0009443?Common Cold?, C0009264 ?Cold Temperature?and C0234192 ?Cold Sensation?.The MRREL table in the Metathesaurus lists re-lations between CUIs found in the various sourcesthat are used to form the Metathesaurus.
This ta-ble lists a range of different types of relations, in-cluding CHD (?child?
), PAR (?parent?
), QB (?canbe qualified by?
), RQ (?related and possibly syn-onymous?)
and RO (?other related?).
For exam-ple, the MRREL table states that C0009443 ?Com-mon Cold?
and C0027442 ?Nasopharynx?
are con-nected via the RO relation.The MRHIER table in the Metathesaurus liststhe hierarchies in which each CUI appears, andpresents the whole path to the top or root ofeach hierarchy for the CUI.
For example, theMRHIER table states that C0035243 ?RespiratoryTract Infections?
is a parent of C0009443 ?Com-mon Cold?.The Semantic Network consists of a set of cat-egories (or semantic types) that provides a consis-tent categorization of the concepts in the Metathe-saurus, along with a set of relationships (or seman-tic relations) that exist between the semantic types.For example, the CUI C0009443 ?Common Cold?is classified in the semantic type ?Disease or Syn-drome?.The SRSTR table in the Semantic Network de-scribes the structure of the network.
This tablelists a range of different relations between seman-tic types, including hierarchical relations (is a)and non hierarchical relations (e.g.
result of,associated with and co-occurs with).For example, the semantic types ?Disease or Syn-drome?
and ?Pathologic Function?
are connectedvia the is a relation in this table.4 Summarization systemThe method presented in this paper consists of 4main steps: (1) concept identification, (2) doc-ument representation, (3) concept clustering andtopic recognition, and (4) sentence selection.
Eachstep is discussed in detail in the following subsec-tions.4.1 Concept identificationThe first stage of our process is to map the doc-ument to concepts from the UMLS Metathesaurusand semantic types from the UMLS Semantic Net-work.We first run the MetaMap program over the textin the body section of the document1 MetaMap(Aronson, 2001) identifies all the phrases thatcould be mapped onto a UMLS CUI, retrievesand scores all possible CUI mappings for eachphrase, and returns all the candidates along with1We do not make use of the disambiguation algorithmprovided by MetaMap, which is invoked using the -y flag(Aronson, 2006), since our aim is to compare the effect ofWSD on the performance of our summarization system ratherthan comparing WSD algorithms.57their score.
The semantic type for each conceptmapping is also returned.
Table 1 shows this map-ping for the phrase tissues are often cold.
This ex-ample shows that MetaMap returns a single CUIfor two words (tissues and often) but also returnsthree equally scored CUIs for cold (C0234192,C0009443 and C0009264).
Section 5 describeshow concepts are selected when MetaMap is un-able to return a single CUI for a word.Phrase: ?Tissues?Meta Mapping (1000)1000 C0040300:Tissues (Body tissue)Phrase: ?are?Phrase: ?often cold?MetaMapping (888)694 C0332183:Often (Frequent)861 C0234192:Cold (Cold Sensation)MetaMapping (888)694 C0332183:Often (Frequent)861 C0009443:Cold (Common Cold)MetaMapping (888)694 C0332183:Often (Frequent)861 C0009264:Cold (cold temperature)Table 1: An example of MetaMap mapping for thephrase Tissues are often coldUMLS concepts belonging to very general se-mantic types are discarded, since they have beenfound to be excessively broad or unrelated to themain topic of the document.
These types areQuantitative Concept, Qualitative Concept, Tem-poral Concept, Functional Concept, Idea or Con-cept, Intellectual Product, Mental Process, SpatialConcept and Language.
Therefore, the conceptC0332183 ?Often?
in the previous example, whichbelongs to the semantic type Temporal Concept, isdiscarded.4.2 Document representationThe next step is to construct a graph-based repre-sentation of the document.
To this end, we first ex-tend the disambiguated UMLS concepts with theircomplete hierarchy of hypernyms and merge thehierarchies of all the concepts in the same sentenceto construct a graph representing it.
The two upperlevels of these hierarchies are removed, since theyrepresent concepts with excessively broad mean-ings and may introduce noise to later processing.Next, all the sentence graphs are merged intoa single document graph.
This graph is extendedwith more semantic relations to obtain a morecomplete representation of the document.
Vari-ous types of information from the UMLS can beused to extend the graph.
We experimented us-ing different sets of relations and finally used thehypernymy and other related relations betweenconcepts from the Metathesaurus, and the asso-ciated with relation between semantic types fromthe Semantic Network.
Hypernyms are extractedfrom the MRHIER table, RO (?other related?)
re-lations are extracted from the MRREL table, andassociated with relations are extracted fromthe SRSTR table (see Section 3).
Finally, eachedge is assigned a weight in [0, 1].
This weightis calculated as the ratio between the relative posi-tions in their corresponding hierarchies of the con-cepts linked by the edge.Figure 1 shows an example graph for a sim-plified document consisting of the two sentencesbelow.
Continuous lines represent hypernymy re-lations, dashed lines represent other related rela-tions and dotted lines represent associated with re-lations.1.
The goal of the trial was to assess cardiovascularmortality and morbidity for stroke, coronary heartdisease and congestive heart failure, as an evidence-based guide for clinicians who treat hypertension.2.
The trial was carried out in two groups: the firstgroup taking doxazosin, and the second group tak-ing chlorthalidone.4.3 Concept clustering and topic recognitionOur next step consists of clustering the UMLSconcepts in the document graph using a degree-based clustering method (Erkan and Radev, 2004).The aim is to construct sets of concepts stronglyrelated in meaning, based on the assumption thateach of these sets represents a different topic in thedocument.We assume that the document graph is an in-stance of a scale-free network (Barabasi and Al-bert, 1999).
A scale-free network is a complex net-work that (among other characteristics) presents aparticular type of node which are highly connectedto other nodes in the network, while the remain-ing nodes are quite unconnected.
These highest-degree nodes are often called hubs.
This scale-free power-law distribution has been empiricallyobserved in many large networks, including lin-guistic and semantic ones.To discover these prominent or hub nodes, wecompute the salience or prestige of each vertex58Figure 1: Example of a simplified document graphin the graph (Yoo et al, 2007), as shown in (1).Whenever an edge from vi to vj exists, a vote fromnode i to node j is added with the strength of thisvote depending on the weight of the edge.
Thisranks the nodes according to their structural im-portance in the graph.salience(vi) =?
?ej |?vk?ejconnect(vi,vk)weight(ej) (1)The n vertices with a highest salience arenamed Hub Vertices.
The clustering algorithmfirst groups the hub vertices into Hub VerticesSets (HVS).
These can be seen as set of conceptsstrongly related in meaning, and will represent thecentroids of the clusters.
To construct these HVS,the clustering algorithm first searches, iterativelyand for each hub vertex, the hub vertex most con-nected to it, and merges them into a single HVS.Second, the algorithm checks, for every pair ofHVS, if their internal connectivity is lower thanthe connectivity between them.
If so, both HVSare merged.
The remaining vertices (i.e.
thosenot included in the HVS) are iteratively assignedto the cluster to which they are more connected.This connectivity is computed as the sum of theweights of the edges that connect the target vertexto the other vertices in the cluster.4.4 Sentence selectionThe last step of the summarization process con-sists of computing the similarity between all sen-tences in the document and each of the clusters,and selecting the sentences for the summary basedon these similarities.
To compute the similarity be-tween a sentence graph and a cluster, we use a non-democratic vote mechanism (Yoo et al, 2007), sothat each vertex of a sentence assigns a vote toa cluster if the vertex belongs to its HVS, half avote if the vertex belongs to it but not to its HVS,and no votes otherwise.
Finally, the similarity be-tween the sentence and the cluster is computed asthe sum of the votes assigned by all the vertices inthe sentence to the cluster, as expressed in (2).similarity(Ci, Sj) =?vk|vk?Sjwk,j (2)where{wk,j=0 if vk 6?Ciwk,j=1 if vk?HV S(Ci)wk,j=0.5 if vk 6?HV S(Ci)Finally, we select the sentences for the sum-mary based on the similarity between them andthe clusters as defined above.
In previous work(blind reference), we experimented with differentheuristics for sentence selection.
In this paper, wejust present the one that reported the best results.For each sentence, we compute a single score, as59the sum of its similarity to each cluster adjustedto the cluster?s size (expression 3).
Then, the Nsentences with higher scores are selected for thesummary.Score(Sj) =?Cisimilarity(Ci, Sj)|Ci|(3)In addition to semantic-graph similarity(SemGr) we have also tested two further featuresfor computing the salience of sentences: sentencelocation (Location) and similarity with the titlesection (Title).
The sentence location featureassigns higher scores to the sentences close to thebeginning and the end of the document, whilethe similarity with the title feature assigns higherscores as the proportion of common concepts be-tween the title and the target sentence is increased.Despite their simplicity, these are well acceptedsummarization heuristics that are commonly used(Bawakid and Oussalah, 2008; Bossard et al,2008).The final selection of the sentences for the sum-mary is based on the weighted sum of these featurevalues, as stated in (4).
The values for the param-eters ?, ?
and ?
have been empirically set to 0.8,0.1, and 0.1 respectively.Score(Sj) = ??
SemGr(Sj) +?
?
Location(Sj) + ??
Title(Sj) (4)5 WSD for concept identificationSince our summarization system is based on theUMLS it is important to be able to accurately mapthe documents onto CUIs.
The example in Section4.1 shows that MetaMap does not always select asingle CUI and it is therefore necessary to havesome method for choosing between the ones thatare returned.
Summarization systems typicallytake the first mapping as returned by MetaMap,and no attempt is made to solve this ambiguity(Plaza et al, 2008).
This paper reports an alter-native approach that uses a WSD algorithm thatmakes use of the entire UMLS Metathesaurus.The Personalized PageRank algorithm (see Sec-tion 2) was adapted to use the UMLS Metathe-saurus and used to select a CUI from the MetaMapoutput2.
The UMLS is converted into a graphin which the CUIs are the nodes and the edges2We use a publicly available implementation of the Per-sonalized Page Rank algorithm (http://ixa2.si.ehu.es/ukb/) for the experiments described here.are derived from the MRREL table.
All possiblerelations in this table are included.
The outputfrom MetaMap is used to provide the list of pos-sible CUIs for each term in a document and theseare passed to the disambiguation algorithm.
Weuse both the standard (ppr) and ?word to word?
(ppr w2w) variants of the Personalized PageRankapproach.It is difficult to evaluate how well the Person-alized PageRank approach performs when usedin this way due to a lack of suitable data.
TheNLM-WSD corpus (Weeber et al, 2001) con-tains manually labeled examples of ambiguousterms in biomedical text but only provides exam-ples for 50 terms that were specifically chosen be-cause of their ambiguity.
To evaluate an approachsuch as Personalized PageRank we require doc-uments in which the sense of every ambiguousword has been identified.
Unfortunately no suchresource is available and creating one would beprohibitively expensive.
However, our main in-terest is in whether WSD can be used to improvethe summaries generated by our system rather thanits own performance and, consequently, decided toevaluate the WSD by comparing the output of thesummarization system with and without WSD.6 Experiments6.1 SetupThe ROUGE metrics (Lin, 2004) are used to eval-uate the system.
ROUGE compares automati-cally generated summaries (called peers) againsthuman-created summaries (called models), andcalculates a set of measures to estimate the con-tent quality of the summaries.
Results are re-ported for the ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-SU4 (R-SU) and ROUGE-W (R-W)metrics.
ROUGE-N (e.g.
ROUGE-1 and ROUGE-2) evaluates n-gram co-occurrences among thepeer and models summaries, where N stands forthe length of the n-grams.
ROUGE-SU4 allowsbi-gram to have intervening word gaps no longerthan four words.
Finally, ROUGE-W computesthe union of the longest common subsequences be-tween the candidate and the reference summariestaking into account the presence of consecutivematches.To the authors?
knowledge, no specific corpusfor biomedical summarization exists.
To evalu-ate our approach we use a collection of 150 doc-uments randomly selected from the BioMed Cen-60tral corpus3 for text mining research.
This collec-tion is large enough to ensure significant results inthe ROUGE evaluation (Lin, 2004) and allows usto work with the ppr w2w disambiguation soft-ware, which is quite time consuming.
We generateautomatic summaries by selecting sentences untilthe summary reaches a length of the 30% over theoriginal document size.
The abstract of the papers(i.e.
the authors?
summaries) are removed fromthe documents and used as model summaries.A separate development set was used to deter-mine the optimal values for the parameters in-volved in the algorithm.
This set consists of 10documents from the BioMed Central corpus.
Themodel summaries for these documents were man-ually created by medical students by selecting be-tween 20-30% of the sentences within the paper.The parameters to be estimated include the per-centage of vertices considered as hub vertices bythe clustering method (see Section 4.3) and thecombination of summarization features used tosentence selection (see Section 4.4).
As a result,the percentage of hub vertices was set to 15%, andno additional summarization features (apart fromthe semantic-graph similarity) were used.Two baselines were also implemented.
Thefirst, lead baseline, generate summaries by select-ing the first n sentences from each document.
Thesecond, random baseline, randomly selects n sen-tences from the document.
The n parameter isbased on the desired compression rate (i.e.
30%of the document size).6.2 ResultsVarious summarizers were created and evaluated.First, we generated summaries using our methodwithout performing word sense disambiguation(SemGr), but selecting the first CUI returned byMetaMap.
Second, we repeated these experimentsusing the Personalized Page Rank disambigua-tion algorithm (ppr) to disambiguate the CUIs re-turned by MetaMap (SemGr + ppr).
Finally, weuse the ?word to word?
variant of the PersonalizedPage Rank algorithm (ppr w2w) to perform thedisambiguation (SemGr + ppr w2w).Table 2 shows ROUGE scores for the differentconfigurations of our system together with the twobaselines.
All configurations significantly outper-form both baselines (Wilcoxon Signed Ranks Test,p < 0.01).3http://www.biomedcentral.com/info/about/datamining/Summarizer R-1 R-2 R-W R-SUrandom .5089 .1879 .1473 .2349lead .6483 .2566 .1621 .2646SemGr .7504 .3283 .1915 .3117SemGr+ppr .7737 .3419 .1937 .3178SemGr+ppr w2w .7804 .3530 .1966 .3262Table 2: ROUGE scores for two baselines andSemGr (with and without WSD).
Significant dif-ferences among the three versions of SemGr areindicated in bold font.The use of WSD improves the average ROUGEscore for the summarizer.
The ?standard?
(i.e.ppr) version of the WSD algorithm signifi-cantly improves ROUGE-1 and ROUGE-2 metrics(Wilcoxon Signed Ranks Test, p < 0.01), com-pared with no WSD (i.e.
SemGr).
The ?word toword?
variant (ppr w2w) significantly improvesall ROUGE metrics.
Performance using the ?wordto word?
variant is also higher than standard pprin all ROUGE scores.These results demonstrate that employing astate of the art WSD algorithm that has beenadapted to use the UMLS Metathesaurus improvesthe quality of the summaries generated by a sum-marization system.
To our knowledge this isthe first result to demonstrate that WSD can im-prove summarization systems.
However, this im-provement is less than expected and this is prob-ably due to errors made by the WSD system.The Personalized PageRank algorithms (ppr andppr w2w) have been reported to correctly dis-ambiguate around 58% of words in general text(see Section 2) and, although we were unable toquantify their performance when adapted for thebiomedical domain (see Section 5), it is highlylikely that they will still make errors.
However, theWSD performance they do achieve is good enoughto improve the summarization process.6.3 AnalysisThe results presented above demonstrate that us-ing WSD improves the performance of our sum-marizer.
The reason seems to be that, since the ac-curacy in the concept identification step increases,the document graph built in the following steps isa better approximation of the structure of the doc-ument, both in terms of concepts and relations.
Asa result, the clustering method succeeds in findingthe topics covered in the document, and the infor-mation in the sentences selected for the summary61is closer to that presented in the model summaries.We have observed that the clustering methodusually produces one big cluster along with a vari-able number of small clusters.
As a consequence,though the heuristic for sentence selection was de-signed to select sentences from all the clusters inthe document, the fact is that most of the sentencesare extracted from this single large cluster.
Thisallows our system to identify sentences that coverthe main topic of the document, while it occasion-ally fails to extract other ?satellite?
information.We have also observed that the ROUGE scoresdiffer considerably from one document to others.To understand the reasons of these differences weexamined the two documents with the highest andlowest ROUGE scores respectively.
The best caseis one of the largest document in the corpus, whilethe worst case is one of the shortest (6 versus 3pages).
This was expected, since according to ourhypothesis that the document graph is an instanceof a scale-free network (see Section 4.3), the sum-marization algorithm works better with larger doc-uments.
Both documents also differ in their under-lying subject matter.
The best case concerns thereactions of some kind of proteins over the brainsynaptic membranes; while the worst case regardsthe use of pattern matching for database searching.We have verified that UMLS covers the vocabu-lary contained in the first document better than inthe second one.
We have also observed that the usein the abstract of synonyms of terms presented inthe document body is quite frequent.
In particularthe worst case document uses different terms in theabstract and the body, for example ?pattern match-ing?
and ?string searching?.
Since the ROUGEmetrics rely on evaluating summaries based on thenumber of strings they have in common with themodel summaries the system?s output is unreason-ably penalised.Another problem is related to the use ofacronyms and abbreviations.
Most papers in thecorpus do not include an Abbreviations section butdefine them ad hoc in the document body.
Thesecontracted forms are usually non-standard and donot exist in the UMLS Metathesaurus.
This seri-ously affects the performance of both the disam-biguation and the summarization algorithms, es-pecially considering that it has been observed thatthe terms (or phrases) represented in an abbrevi-ated form frequently correspond to central con-cepts in the document.
For example, in a pa-per from the corpus that presents an analysis toolfor simple sequence repeat tracts in DNA, onlythe first occurrence of ?simple sequence repeat?is presented in its expanded form.
In the re-maining of the document, this phrase is namedby its acronym ?SSR?.
The same occurs in a pa-per that investigates the developmental expressionof survivin during embryonic submandibular sali-vary gland development, where ?embryonic sub-mandibular gland?
is always referred as ?SMG?.7 Conclusion and future workIn this paper we propose a graph-based approachto biomedical summarization.
Our algorithm rep-resents the document as a semantic graph, wherethe nodes are concepts from the UMLS Metathe-saurus and the links are different kinds of seman-tic relations between them.
This produces a richerrepresentation than the one provided by traditionalmodels based on terms.This approach relies on accurate mapping ofthe document being summarized into the conceptsin the UMLS Metathesaurus.
Three methods fordoing this were compared and evaluated.
Thefirst was to select the first mapping generated byMetaMap while the other two used a state of theart WSD algorithm.
This WSD algorithm wasadapted for the biomedical domain by using theUMLS Metathesaurus as a knowledge based andMetaMap as a pre-processor to identify the pos-sible CUIs for each term.
Results show that thesystem performs better when WSD is used.In future work we plan to make use of the dif-ferent types of information within the UMLS tocreate different configurations of the PersonalizedPageRank WSD algorithm and explore their ef-fect on the summarization system (i.e.
consider-ing different UMLS relations and assigning differ-ent weights to different relations).
It would alsobe interesting to test the system with other disam-biguation algorithms and use a state of the art al-gorithm for identifying and expanding acronymsand abbreviations.AcknowledgmentsThis research is funded by the Spanish Govern-ment through the FPU program and the projectsTIN2009-14659-C03-01 and TSI 020312-2009-44.
Mark Stevenson acknowledges the support ofthe Engineering and Physical Sciences ResearchCouncil (grant EP/D069548/1).62ReferencesS.D.
Afantenos, V. Karkaletsis, and P. Stamatopou-los.
2005.
Summarization from medical docu-ments: a survey.
Artificial Intelligence in Medicine,33(2):157?177.E.
Agirre and P. Edmonds, editors, 2006.
WordSense Disambiguation: Algorithms and Applica-tions.
Springer.E.
Agirre and A. Soroa.
2009.
Personalizing PageRankfor Word Sense Disambiguation.
In Proceedings ofEACL-09, pages 33?41, Athens, Greece.A.
Aronson.
2001.
Effective mapping of biomedi-cal text to the UMLS Metathesaurus: the MetaMapprogram.
In Proceedings of the AMIA Symposium,pages 17?21.A.
Aronson.
2006.
MetaMap: Mapping text to theUMLS Metathesaurus.
Technical report, U.S. Na-tional Library of Medicine.A.L.
Barabasi and R. Albert.
1999.
Emergence of scal-ing in random networks.
Science, 268:509?512.R.
Barzilay and M. Elhadad.
1997.
Using lexicalchains for text summarization.
In Proceedings of theACL Workshop on Intelligent Scalable Text Summa-rization, pages 10?17.A.
Bawakid and M. Oussalah.
2008.
A semanticsummarization system: University of Birminghamat TAC 2008.
In Proceedings of the First Text Anal-ysis Conference (TAC 2008).A.
Bossard, M. Gnreux, and T. Poibeau.
2008.
De-scription of the LIPN systems at TAC 2008: sum-marizing information and opinions.
In Proceedingsof the First Text Analysis Conference (TAC 2008).S.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertextual web search engine.
ComputerNetworks and ISDN Systems, 30:1?7.G.
Erkan and D. R. Radev.
2004.
LexRank: Graph-based lexical centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR), 22:457?479.M.
Fiszman, T. C. Rindflesch, and H. Kilicoglu.2004.
Abstraction summarization for managing thebiomedical research literature.
In Proceedings ofthe HLT-NAACL Workshop on Computational Lex-ical Semantics, pages 76?83.L.
Humphreys, D. Lindberg, H. Schoolman, andG.
Barnett.
1998.
The Unified Medical Lan-guage System: An informatics research collabora-tion.
Journal of the American Medical InformaticsAssociation, 1(5):1?11.L.
Hunter and K. B. Cohen.
2006.
BiomedicalLanguage Processing: Perspective Whats BeyondPubMed?
Mol Cell., 21(5):589?594.C.-Y.
Lin.
2004.
Rouge: A package for automatic eval-uation of summaries.
In Proceedings of the ACL-04 Workshop: Text Summarization Branches Out.,pages 74?81, Barcelona, Spain.I.
Mani.
2001.
Automatic summarization.
Jonh Ben-jamins Publishing Company.R.
Mihalcea and P. Tarau.
2004.
TextRank - Bringingorder into text.
In Proceedings of the ConferenceEMNLP 2004, pages 404?411.R.
Navigli and P. Velardi.
2005.
Structural seman-tic interconnections: A knowledge-based approachto word sense disambiguation.
IEEE Trans.
PatternAnal.
Mach.
Intell., 27(7):1075?1086.S.
Nelson, T. Powell, and B. Humphreys.
2002.
TheUnified Medical Language System (UMLS) Project.In Allen Kent and Carolyn M. Hall, editors, Ency-clopedia of Library and Information Science.
Mar-cel Dekker, Inc.L.
Plaza, A.
D?
?az, and P. Gerva?s.
2008.
Concept-graph based biomedical automatic summarizationusing ontologies.
In TextGraphs ?08: Proceedingsof the 3rd Textgraphs Workshop on Graph-Based Al-gorithms for Natural Language Processing, pages53?56.L.H.
Reeve, H. Han, and A.D. Brooks.
2007.
Theuse of domain-specific concepts in biomedical textsummarization.
Information Processing and Man-agement, 43:1765?1776.M.
Weeber, J. Mork, and A. Aronson.
2001.
Devel-oping a Test Collection for Biomedical Word SenseDisambiguation.
In Proceedings of AMIA Sympo-sium, pages 746?50, Washington, DC.I.
Yoo, X. Hu, and I-Y.
Song.
2007.
A coherentgraph-based semantic clustering and summarizationapproach for biomedical literature and a new sum-marization evaluation method.
BMC Bioinformat-ics, 8(9).63
