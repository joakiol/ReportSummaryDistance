Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71?79,Beijing, August 2010Fluency Constraints for Minimum Bayes-Risk Decodingof Statistical Machine Translation LatticesGraeme Blackwood and Adria` de Gispert and William ByrneMachine Intelligence Laboratory, Department of Engineering, Cambridge University{gwb24|ad465|wjb31}@cam.ac.ukAbstractA novel and robust approach to improv-ing statistical machine translation fluencyis developed within a minimum Bayes-risk decoding framework.
By segment-ing translation lattices according to con-fidence measures over the maximum like-lihood translation hypothesis we are ableto focus on regions with potential transla-tion errors.
Hypothesis space constraintsbased on monolingual coverage are ap-plied to the low confidence regions to im-prove overall translation fluency.1 Introduction and MotivationTranslation quality is often described in terms offluency and adequacy.
Fluency reflects the ?na-tiveness?
of the translation while adequacy indi-cates how well a translation captures the meaningof the original text (Ma and Cieri, 2006).From a purely utilitarian view, adequacy shouldbe more important than fluency.
But fluency andadequacy are subjective and not easy to tease apart(Callison-Burch et al, 2009; Vilar et al, 2007).There is a human tendency to rate less fluent trans-lations as less adequate.
One explanation is thaterrors in grammar cause readers to be more crit-ical.
A related phenomenon is that the nature oftranslation errors changes as fluency improves sothat any errors in fluent translations must be rel-atively subtle.
It is therefore not enough to fo-cus solely on adequacy.
SMT systems must alsobe fluent if they are to be accepted and trusted.It is possible that the reliance on automatic met-rics may have led SMT researchers to pay insuffi-cient attention to fluency: BLEU (Papineni et al,2002), TER (Snover et al, 2006), and METEOR(Lavie and Denkowski, 2009) show broad corre-lation with human rankings of MT quality, but areincapable of fine distinctions between fluency andadequacy.There is concern that the fluency of currentSMT is inadequate (Knight, 2007b).
SMT is ro-bust, in that a translation is nearly always pro-duced.
But unlike translators who should beskilled in at least one of the languages, SMT sys-tems are limited in both source and target lan-guage competence.
Fluency and accuracy there-fore tend to suffer together as translation qualitydegrades.
This should not be the case.
Ideally, anSMT system should never be any less fluent thanthe best stochastic text generation system avail-able in the target language (Oberlander and Brew,2000).
What is needed is a good way to enhancethe fluency of SMT hypotheses.The maximum likelihood (ML) formulation(Brown et al, 1990) of translation of source lan-guage sentence F to target language sentence E?E?
= argmaxEP (F |E)P (E) (1)makes it clear why improving SMT fluency is adifficult modelling problem.
The language modelP (E), the closest thing to a ?fluency component?in the original formulation, only affects candidateslikely under the translation model P (F |E).
Giventhe weakness of current translation models this isa severe limitation.
It often happens that SMT sys-tems assign P (F |E?)
= 0 to a correct referencetranslation E?
of F (see the discussion in Section9).
The problem is that in ML decoding the lan-guage model can only encourage the productionof fluent translations; it cannot easily enforce con-straints on fluency or introduce new hypotheses.In Hiero (Chiang, 2007) and syntax-based SMT(Knight and Graehl, 2005; Knight, 2007a), theprimary role of syntax is to drive the translationprocess.
Translations produced by these systemsrespect the syntax of their translation models, but71this does not force them to be grammatical in theway that a typical human sentence is grammati-cal; they produce many translations which are notfluent.
The problem is robustness.
Generatingfluent translations demands a tightly constrainingtarget language grammar but such a grammar is atodds with broad-coverage parsing needed for ro-bust translation.We have described two problems in transla-tion fluency: (1) SMT may fail to generate flu-ent hypotheses and there is no simple way to in-troduce them into the search; (2) SMT producesmany translations which are not fluent but enforc-ing constraints to improve fluency can hurt robust-ness.
Both problems are rooted in the ML decod-ing framework in which robustness and fluencyare conflicting objectives.We propose a novel framework to improve thefluency of any SMT system, whether syntactic orphrase-based.
We will perform Minimum Bayes-risk search (Kumar and Byrne, 2004) over a spaceof fluent hypotheses H:E?MBR = argminE??H?E?EL(E,E?
)P (E|F ) (2)In this approach the MBR evidence space E isgenerated by an SMT system as a k-best list or lat-tice.
The system runs in its best possible config-uration, ensuring both translation robustness andgood baselines.
Rather than decoding in the out-put of the baseline SMT system, translations willbe sought among a collection of fluent sentencesthat are close to the top SMT hypotheses as deter-mined by the loss function L(E,E?
).Decoupling the MBR hypothesis space fromfirst-pass translation offers great flexibility.
Hy-potheses in H may be arbitrarily constrained ac-cording to lexical, syntactic, semantic, or otherconsiderations, with no effect on translation ro-bustness.
This is because constraints on fluencydo not affect the production of the evidence spaceby the baseline system.
Robustness and fluencyare no longer conflicting objectives.
This frame-work also allows the MBR hypothesis space to beaugmented with hypotheses produced by an NLGsystem, although this is beyond the scope of thepresent paper.This paper focuses on searching out fluentstrings amongst the vast number of hypotheses en-coded in SMT lattices.
Oracle BLEU scores com-puted over k-best lists (Och et al, 2004) showthat many high quality hypotheses are producedby first-pass SMT decoding.
We propose reducingthe difficulty of enhancing the fluency of completehypotheses by first identifying regions of high-confidence in the ML translations and using theseto guide the fluency refinement process.
This hastwo advantages: (1) we keep portions of the base-line hypotheses that we trust and search for alter-natives elsewhere, and (2) the task is made mucheasier since the fluency of sentence fragments canbe refined in context.In what follows, we use posterior probabilitiesover SMT lattices to identify useful subsequencesin the ML translations (Sections 2 & 3).
Thesesubsequences drive the segmentation and transfor-mation of lattices into smaller subproblems (Sec-tions 4 & 5).
Subproblems are mined for fluentstrings (Section 6), resulting in improved transla-tion fluency (Sections 7 & 8).
Our results showthat, when guided by the careful selection of sub-problems, fluency can be improved with no realdegradation of the BLEU score.2 Lattice MBR DecodingThe formulation of the MBR decoder in Equation(2) separates the hypothesis space from the evi-dence space.
We apply the linearised lattice MBRdecision rule (Tromble et al, 2008)E?LMBR = argmaxE??H{?0|E?|+?u?N?u#u(E?
)p(u|E)},(3)whereH is the hypothesis space, E is the evidencespace, N is the set of all n-grams in H (typically,n = 1 .
.
.
4), and ?
are constants estimated onheld-out data.
The quantity p(u|E) is the path pos-terior probability of n-gram up(u|E) =?E?EuP (E|F ), (4)where Eu = {E ?
E : #u(E) > 0} is the sub-set of paths containing n-gram u at least once.The path posterior probabilities p(u|E) of Equa-tion (4) can be efficiently calculated (Blackwoodet al, 2010) using general purpose WFST opera-tions (Mohri et al, 2002).720 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Posterior probability threshold ?Averageper?sentenceprecisionp n,?1?gram2?gram3?gram4?gram0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1051015202530Posterior probability threshold ?Averageper?sentencen?gramcounts1?grams2?grams3?grams4?gramsFigure 1: Average n-gram precisions (left) and counts (right) for 2075 sentences of NISTArabic?English ML translations at a range of posterior probability thresholds 0 ?
?
?
1.
The leftplot shows at ?
= 0 the n-gram precisions used in the BLEU score of the ML baseline system.3 Posterior Probability ConfidenceMeasuresIn the formulation of Equations (3) and (4) thepath posterior n-gram probabilities play a crucialrole.
MBR decoding under the linear approxima-tion to BLEU is driven mainly by the presenceof high posterior n-grams in the lattice; the lowposterior n-grams contribute relatively little to theMBR decision criterion.
Here we investigate thepredictive power of these statistics.
We will showthat the n-gram posterior is a good predictor as towhether or not an n-gram is to be found in a set ofreference translations.Let Nn denote the set of n-grams of order nin the ML hypothesis E?, and let Rn denote theset of n-grams of order n in the union of the ref-erences.
For confidence threshold ?, let Nn,?
={u?Nn : p(u|E) ?
?}
denote the n-grams inNnwith posterior probability greater than or equal to?, where p(u|E) is computed using Equation (4).This is equivalent to identifying all substrings oflength n in the translation hypotheses for whichthe system assigns a posterior probability of ?
orhigher.
The precision at order n for threshold ?
isthe proportion of n-grams in Nn,?
also present inthe references:Pn,?
=|Rn ?
Nn,?||Nn,?|(5)The left plot in Figure 1 shows average per-sentence n-gram precisions Pn,?
at orders 1. .
.4for an Arabic?English translation task at a rangeof thresholds 0 ?
?
?
1.
Sentence start and endtokens are ignored when computing unigram pre-cisions.
We note that precision at all orders im-proves as the threshold ?
increases.
This confirmsthat these intrinsic measures of translation confi-dence have strong predictive power.The right-hand side of the figure shows the av-erage number of n-grams per sentence for thesame range of ?.
We see that for high ?, there arefew n-grams with p(u|E) ?
?
; this is as expected.However, even at a high threshold of ?
= 0.9there are still on average three 4-grams per sen-tence with posterior probabilities that exceed ?.Even at this very high confidence level, high pos-terior n-grams occur frequently enough that wecan expect them to be useful.These precision results motivate our use of pathposterior n-gram probabilities as a confidencemeasure.
We assign confidence p(E?ji |E) to sub-sequences E?i .
.
.
E?j of the ML hypothesis.Prior work focuses on word-level confidenceextracted from k-best lists and lattices (Ueffingand Ney, 2007), while Zens and Ney (2006)rescore k-best lists with n-gram posterior proba-bilities.
Similar experiments with a slightly dif-ferent motivation are reported by DeNero et al(2009); they show that expected n-gram counts ina lattice can be used to predict which n-grams ap-pear in the references.4 Lattice SegmentationWe have shown that current SMT systems, al-though flawed, can identify with confidence par-73the newspaper ?
constitution ?
quoted brigadier abdullah krishan , the chief of police in karak governorate ( 521 kmsouth @-@ west of amman ) as saying that the seizure took place after police received information that there wereattempts by the group to sell for more than $ 100 thousand dollars , the police rushed to the arrest in possession .0 11/313.692337/4.3574423/0.0683595358063723/0.00976568130093/4.6934104/5.7402115/4.75291223381300/2.91021323143580/2.248151300183580164/4.04881723/2.565413001923/1.25983/0.66016202323/1.5156323231300213/2.8027225/6.22564423241300234/2.5117 252330232623293580273/2.18952810/3.9199130013003/1.1123130023334/4.433631130034/1.64162335803/2.189533656143435803535803756144/1.859423392341561442561443123165512316575614565614691231649123165012/2.95025273595312272/1.92095142/1.044512/2.95024642/1.31054773594812272/1.9209548615/6.520558123164012/2.950242/1.3105735912272/1.92095942607359618615/3.65236212272/0.6650411258332/5.0586634/0.575211036/6.511764309/6.153365755/6.365266583326858332674/0.12109735912272/1.920942/1.04707359714258332724/0.5752583321234/0.12109735912272/0.665041231658332/3.2891747359758615/3.94637612272/0.916997858332774/0.69727791258332804/0.52637458332/4.6484132381515/4.131882755/2.994158332835158454853/4.281286309/3.6953 87515/3.818488755/3.90724755/3.9072893/4.281290309/3.695391515/3.818458332/3.1895755/4.78123515/3.967858332735912272/0.916993755/2.9941394121056512272/1.920942/1.31057373594/0.5752583329358332924/0.6972758332/0.08691412583324/0.526374755/3.90723/4.2812309/3.695396515/3.818458332/4.77643755/3.12395515/4.2607971056558332/3.18953515/3.96781397559859951005101309755102515/1.4756103515104755/1.0029309/2.402310551065755755/1.002910751510854755/3.9072515/3.8184 109309/3.69533755/3.12311155583324/1.54981133114309115311695850/0.212891389117554/2.911195850/0.44141118958503091193/0.6367212095850/1.582121309595850/0.441413093/0.63672515122395850/0.75391124309995850/1.7519585012587161263/0.2490212795850309/0.160161283129871695850/0.393551303091311008995850/0.9160230958332/0.8437595850/0.12402133913415/2.56641531630995850/0.90821695850/0.12402913630995850/0.074219141314213267157132671358716150514316/6.20713787162035/4.682615/2.56641614413267/5.4181614051468716147510814895850132671495108145309995850/6.0451688/2.2091513681543681703171121761001/1.965815/1.56931613267/3.381812 15910011605152 1565 16273751635108155 51617375/4.4355166737518115167158/4.058614226/5.498169368158 16412 172100117751657375/5.1436180737517315174151832017520121001/2.337954/0.94141 204/0.987320 17919763681861976 189131843/4.586916/6.084178154/0.9873201821976/5.3818 1851319231874854188313190714191485419633619571419331/1.312519757/1.45919848542051852043361994/5.2324200185/1.652320162620223671431/1.312557/1.45919463/6.7617206185207320813/3.381820924/1.8857626/0.5410221090/0.18164211309/1.8752125052036262145052151852173092163/0.7929721333621830922030921990/3.814522132225052233122550522490/1.12422690227185288903093/0.7929722824/5.6748626/4.769522990/4.746123350523290/4.071323130923090/3.802750550590/2.357423457235902369023750523990238312405/4.66412418242329782435052443150590/3.726624558/0.9511726657/1.7461247902463124850585/2.168982493/3.0986250505251132523253325530925490/4.82032569025750525832785025950526013261505309 262132638326430926530950557267882685/2.168926930927013271832723029/2.82912738274832752933763/1.94532761327713279330832803281471328283 6328329/0.66406284293292853029356302928683832873029/2.88575052893092903092911363/0.8457292923029293194362172963/1.56642952929663/0.290042981329913300833011730287303330410/0.214843061943071943098331083 2963/1.73543305140/5.428731210/0.2148431317294 310/0.65234140/6.06843141403151188/5.51863161403171188/3.1943318531987320873213096329/1.043631401188/2.3926323332410/0.12532683255/0.2724632711/2.59673281188/0.80762329140330533111/5.25593321188/2.54593331403341188335173361731113 32283 29/0.15527634001401188/2.40333381403413/2.90233421188/6.0127343399633911883402710/4.471740133445/4.94043458/1.1172346113473848/3.978534882710/4.2734349118835033848/3.3076402113518/0.44629835211/2.66735483534/0.53027335510/0.125310/0.2099611/2.803735753588/2.10643591188/1.204136053611188/2.40624/1.628936583633/5.52443645/3.475636611/1.30963671579/6.13283683848/3.566483695/1.551837011/1.703137115793848 374113723/3.13283739/3.780337581/4.08983761188/2.48933771704/4.78713782710/5.6816379157915793848/0.199223/0.01953138039963996/4.159238215793813/2.33013831649/4.83893843848/1.69533851139963512/3.42093996/2.55664/3.270538883863/3.74513875/2.58438911/3.01763901579/1.85063911649/5.44143923848/0.07324239315793/2.330139515793963848/1.695339439961579/0.65039397339939963988140171188118/0.446291579/6.53848/6.112339964063/5.680740315791579/1.11623848/1.316440434051649/2.50393996/4.21391649/4.893640734081579/0.0546884093848/1.7541011411113157941254139/2.705141411/2.57234151188/3.88484161704/4.0848142138/4.04422143/4.9678423188/6.507841711884182710/0.94434419569420775/0.1084714163775/2.3652425114249/0.6845742620/4.0977427130/1.07424281704/0.603524293431/4.08114308/0.405274312143294331704/1.22859/3.7803438114391704/4.7871775/5.4775443114415/4.51664429/1.304744420/5.0928445130/3.28224461704/1.43954401579447170481/3.7598449114489/2.5488450130/3.3754511704/1.544528246/3.0615813848/6.112339963/5.680745315791579/2.00783848/2.20845433/0.21191 3848/1.96194551579775/4.34575/3.6152114569/1.75394571704/1.4248458170481/5.1875114599/4.99614601704/4.542461511462157946339969/3.780381/5.39261704/4.787120/6.652346411118839968/1.
172111704/1.0518465113848/2.866246615791704467157946815795/4.51669/1.304720/5.09281704/1.439546911470130/3.2822119/2.54881704/1.54471130/3.3754728246/3.06158147338/3.3545474143/3.33598147538/4.7412476143/4.536181/0.9589854211884772710/2.8506478569/0.635744797758138/1.432648117044809/0.08105521775/1.76179/0.2548820/3.3711170448211/1.3154483130/0.80176111704/2.52544849/0.31934485114861148756148874489188/1.98634903250/0.0322274913745/1.80664924816/0.541994931434941438138/1.32134973/3.3994496775495569/0.539064983431814993/1.814550038/1.2832501143/0.495125023745/0.70996503850421/0.382815058813/1.266650638/0.38281815073/1.6318775/0.014648508569843454359/4.275443611/3.81054371188/3.629981/2.265611882710/2.850677581/0.1044950938130/0.470751117045109/0.0810558138/4.04512143/4.96782151355149/1.432651511/0.96685161704/2.21398138/2.57913/5.5254519143/3.0596775/0.79395815171704/0.10059775/1.0488518569520343181/1.75522385213/2.3906523143/0.81934524852521/1.231452688138/3.6436528775527569/0.3818481/0.98145383/0.779321/1.408252988138/1.1665303/0.90234531143/0.37305532561/0.85845/1.26959/0.99316115331704/1.46293848/3.75785341579536115359/1.30475371704/1.7061569775/0.43359539215388/0.4082215408/0.33887775569/0.3818421/0.6933654181188119/1.3047130/3.28221704/1.4395111704/1.545543118138/4.04188/6.5078143/4.9678385/2.4179/1.4287544115451704/2.7559511/0.9296920/4.699259/1.432611/0.96681704/2.2139546130/5.13098138/2.5791143/3.059681/1.7538143/0.819343881143/0.3730554756174/0.4511732504816/0.5097754856132504816/0.5097754974/0.71582775/2.454120/3.71781704/0.692383431/3.6172550955111/0.39551552130/0.372075539554115551155687758138/0.223633/2.15438138/1.2832143/0.495123745/0.709965573/0.82227775/0.9941456981/0.38965558108155938/1.59285605/2.4922561374556218856311915651191564143/0.6255661435671191/0.20703119156811915711157011572561881/0.56348573561574561188/1.955132503745/1.77444816/0.509775751438157738/0.382815763/2.0371815783/1.631881/0.09082385795615805615811158256121/0.382818775569/0.53906743250/0.0322274816/0.54199775/2.9178158311/3.3545584569/3.93365851704/2.4365775586569/0.6357481/0.104493/3.47953821/0.515625878588858911188/2.866232503745/0.820314816/1.421959074/0.55664591859356159256132503745/0.820314816/1.421981/1.48343/3.442438815943/0.8974638596115951181/0.714843859756132504816/0.5097737455985/0.0371095994816/0.43164218/0.63775/2.09679/1.4287111704/2.75598138/2.8457775/1.048860056921/0.964846018813/1.5813/2.350638/0.382818138/0.151378138/0.382818138/4.52738138/1.9385218/0.6377386023745374518860381/0.34082775/0.01464861056981/0.2558610813/2.140638/1.59288138/0.3828181/0.56348611561612233961356161418861514361611916173356619335661811916201191621335662233565691191623335681/0.2861310813/3.331138/1.5928562456254816/0.891637455/1.5254626119162856162756162956163037456315105/4.0293374581/0.56543386321121/1.716863386341181/1.15433881/1.48343881/0.255866351063638/3.79881886373250/1.4678 6383745/0.7581/0.8447338563937456405618110/0.13965813/1.793938/1.592854816/0.04882864118864211916431181/1.216838775604569/0.539068138/1.65043/3.93858138/1.28323745/0.70996606143/0.495126053/1.5205607561/3.85066083250/3.59086094593/3.88871191/3.284214316364411188/1.95513745/1.774464632506474816/0.5097781/0.5634864556137455/0.03710964814356491154816/0.89166509/1.6494737553745/0.523446521191651143/2.35556531191654335665510326563356657103265833566591032660103266110326622295/6.81356631881191/0.25391664143665335656664816/1.80765/3.1465374556673745/1.34471191/3.77441436681888110/1.382881/1.27153881/0.255866691067023396715611191/0.5625 672143143/1.819311911191/2.88281434816/2.059653745/1.34471191673335681/0.255866741081/0.286131038/3.82913745/2.312554816/0.89169/1.64941191143/0.6256754376/3.23241191676143/2.52056774376/2.5254678119181/0.5214810679459368081/2.3428681116/4.38386821886843356683119168533566861032687468946883/2.431669071/1.344769246913/1.811569343/1.81156944471/3.5833/1.811569546966/6.23246977/6.39066989/5.129969924/3.690470077/4.2246701119/3.6687021032143/1.9181191703119170410321431191/0.838871431191143/1.918705233957069/4.56157403745/1.183653745/0.5761770711917081032709233971010327111191712103271333567155717335671810323/3.603571/2.5166443/1.81157193720309/4.51377219331/2.3145309/2.1367723933172422194/2.8037722309725933139331/2.3145726309309/2.1367933122194/2.80377273/2.9238371/4.9521524/5.0078119/4.93163309/4.51379331/2.314522194/5.118272824/5.54348 72993317303731933173222194/1.971722194/1.324273393317343735373633563/1.8115453745/1.512773845931191/1.60351437393356453745/1.450274147423356 3/0.46582743474410321191143/0.666027161631191143/1.692481191143/1.53613/2.431671/1.34474471/3.5833/1.8115119/3.66874524/3.690474677/4.224674730974993317481899/1.5827508751309/4.406275293318309/4.40627533/4.74227548755875793317561899/4.460975830922194/0.544927599331760876130976283/0.423838763876430976530971/1.344780447665485437679331/0.0800784524/0.3681639331/0.0800784933193311899/4.302780393317682925/6.746177087692925309/1.9824771377287738309309309/2.19147743776877529257779331778877937809331309/0.552733309/0.9638737818/1.31157822121593319331783437678430978587878786171678881716/2.3975 78937902318/1.68757913097922318795379330979481716/2.454137962318/1.74417978309/1.13773798309799880023188018802103238059807231880923188081716/0.819348107/0.60848111618/3.6729812171681311070/2.513716111070/3.25181623188151716/0.81934814124/3.1658175763/4.48248187235/5.508881911744/5.4629171611070/2.513782023187/0.60848231618213/4.72858794/3.45518229/0.9267632318/1.7441171632318/1.7441824782623188253/2.91318274933182830051716/2.454132318/1.744180645/5.883882923181618307/0.7763783398327/0.30078831957/3.44348349835109/2.02738363005/0.50586837997/4.2412838161/4.261783991617/0.982428459/0.9267698437/0.300788424/3.31354/5.5439 8417/4.81458405844999/0.125846798484/3.55278497/2.38188803005881945/2.62798473005850212159/0.926768527/0.60848531618512318358846/0.257/0.557629945/4.8506300545/2.6006300557/2.309698553/1.57038565/0.349618573005/0.019531458583005/0.84277859635/2.6631645/0.205088603005945/1.24930058614545/0.48828862921/3.768645/5.558657/2.30969109/4.79793/3.07035/1.89263005/0.01953186463/5.46977/3.0781863986530053005965/0.9404367/0.579193005/2.10068669867998687/0.300789109/2.027398697/2.38188822870300535/3.1211668835/0.2460987195/0.8935568722318458733005/0.84277458743005/2.431687595/0.99414645453005/1.2275453005/1.523493005/0.01953195/2.06936161/2.6025 6876300587775/0.246096587830055/0.2871163/2.057657/1.9053956/0.25098H1 H2 H3 H4 H5 H6 H7 H8 H90 1123374235358063723 81300931041151223131300231435802513001541623173580130018233192323203213225 231300 2423234262327232832910303580312313001300313003213003233343456143 3535803635803756143856142343923401231633580415614425614431231644123164556144612474248735949122725012316511231652125342647359122721242735912272548615551231656427359122725786156358332583325843658332442735912272735959427359122725833260127359122724212272735958332618615 621258332735912272105655833212105650 14 23 3755 45 5309 69 0132958503871652035871645 958500 116 213267 35108 4368 512 61001 75 87375 915 1020 111976 1213 133 1448540 1312573714445185623673361018538336185918530 1309 2505 390 4130183230293296342953029619471788793101011140121713140141188151401611881751831910205218221123118824140255261127118828140291188301401188311403211883327103433511883639963733853984011413848428271043118844338484584611847114844981150551852118853554118845535655785811591579603848861562116315793848643659661167816811886917047027107115791579384837239963996733741579751649763848771139963996351247837958088111821579831649843848851579863996387157988384815798939089139961188399681111888119215791579384893394164915793848399695339961649963971579983848991110011315791015102910311104118810517041061188107271010856910977581110381111431121881131637759114111152011613011717041183431119812021121912217045123912411125118891261112717041281579775129513091311113220133130134170413517048191361113713013817041398246811401579157938483384839963338481411579775511917041704811117041429143511911130170411170414415793996981170420145111704146113848147157917041481579149157959201704150111511309111704152130153824681154381551438115638157143811581188159271056916077581381619162170421775920170416311164130111704165916611167111685611697417018817132501723745173481614317414381381753176343181177317838179143180374518181822188138381183377518456987758118538130918617048138143211875188918911190170477581191170477519256981383193143343181381943195143819621881388138321881381973198143199561591117041191704775569118858138188143385911200170451120130591117048138143813814338811432015617432504816202561325048162037477520170434311120492051309117752068813838138143374520737755698120810813820952103745188211119121214321311911191214143119121511912165618812175612185611883250374548161438132193881322056122111222561218775815691117047752235698133882182241118832503745481622574226561325037454816812273228561325048165481637458212293745374518823081775231569813250383745561232323314323445938138812355612362339237188238143266335623911912403356241119124233565481624355374524456151053745245118138246103250374518852475615481611911431632481124956118837452503250251481654816525292535 11911432541191267103226833562551032335625610322572295188537458125810259233948165374581381048163745591431191437611914376260459381188261116335644491032262233937455263958374552644593 265543760 13 2309 39331 4801324530171632318576372354124517166231871107081174492318359103114127139141611554916791741879797919300520957219221093730054530059234247254545972696321455739109300527530052864545293005635679930053123184532300533326345367516163579433 1 4 1 6 1 6860 1 76Figure 2: ML translation E?, word lattice E , and decomposition as a sequence of four string and fivesublattice regions H1 .
.
.H9 using n-gram posterior probability threshold p(u|E)?0.8.tial hypotheses that can be trusted.
We wish toconstrain MBR decoding to include these trustedpartial hypotheses but allow decoding to consideralternatives in regions of low confidence.
In thisway we aim to improve the best possible output ofthe best available systems.We use the path posterior n-gram probabilitiesof Equation (4) to segment lattice E into regions ofhigh and low confidence.
As shown in the exam-ple of Figure 2, the lattice segmentation processis performed relative to the ML hypothesis E?, i.e.relative to the best path through E .For confidence threshold ?, we find all 4-gramsu = E?i, .
.
.
, E?i+3 in the ML translation for whichp(u|E) > ?.
We then segment E?
into regionsof high and low confidence where the high confi-dence regions are identified by consecutive, over-lapping high confidence 4-grams.
The high confi-dence regions are contiguous strings of words forwhich there is consensus amongst the translationsin the lattice.
If we trust the path posterior n-gramprobabilities, any hypothesised translation shouldinclude these high confidence substrings.
This ap-proach differs from simple posterior-based prun-ing in that we discard paths, rather than wordsor n-grams, which are not consistent with high-confidence regions of the ML hypothesis.The hypothesis string E?
is in this way seg-mented into R alternating subsequences of highand low confidence.
The segment boundaries areir and jr so that E?jrir is either a high confidenceor a low confidence subsequence.
Each subse-quence is associated with an unweighted subspaceHr; this subspace has the form of a string for highconfidence regions and the form of a lattice forlow confidence regions.If the rth segment is a high confidence regionthen Hr accepts only the string E?jrir .
If the rthsegment is a region of low confidence, then Hris built to accept relevant substrings from E .
It isconstructed as follows.
The rth low confidenceregion E?jrir has a high confidence left context e?r?1and a high confidence right context e?r+1 formedfrom subsequences of the ML translation hypoth-esis E?
ase?r?1 = E?jr?1ir?1 , e?r+1 = E?jr+1ir+1Note that when r = 1 the left context e?r?1 is theempty string and when r = R the right contexte?r+1 is the empty string.
We build a transducer74Tr for the regular expression /.
?
e?r?1(.?)e?r+1.
?/\1/.1 Composition with E yieldsHr = E?Tr , sothat Hr contains all the reasonable alternatives toE?jrir in E consistent with the high confidence leftand right contexts e?r?1 and e?r+1.
IfHr is alignedto a high confidence subsequence of E?, we callit a string region since it contains a single path;if it is aligned to a low confidence region it is alattice and we call it a sublattice region.
The se-ries of high and low confidence subspace regionsH1, .
.
.
,HR defines the lattice segmentation.5 Hypothesis Space ConstructionWe now describe a general framework for improv-ing the fluency of the MBR hypothesis space.The segmentation of the lattice described inSection 4 considerably simplifies the problem ofimproving the fluency of its hypotheses since eachregion of low confidence may be considered in-dependently.
The low confidence regions can betransformed one-by-one and then reassembled toform a new MBR hypothesis space.In order to transform the hypothesis region Hrit is important to know the context in which it oc-curs, i.e.
the sequences of words that form its pre-fix and suffix.
Some transformations might needonly a short context; others may need a sentence-level context, i.e.
the full sequence of ML wordsE?jr?11 and E?Nir+1 to the left and right of the regionHr that is to be transformed.To put this formally, each low confidence sub-lattice region is transformed by the application ofsome function ?
:Hr ?
?
(E?jr?11 , Hr, E?Nir+1) (6)The hypothesis space is then constructed from theconcatenation of high confidence string and trans-formed low confidence sublattice regionsH = E ?
?1?r?RHr (7)The composition with the original lattice E dis-cards any new hypotheses that might be createdvia the unconstrained concatenation of stringsfrom theHr.
It may be that in some circumstances1In this notation parentheses indicate string matches sothat /.
?
y(a?)w.
?
/\1/ applied to xyaaawzz yields aaa.the introduction of new paths is good, but in whatfollows we test the ability to improve fluency bysearching among existing hypotheses, and this en-sures that nothing new is introduced.Size of the Hypothesis Space If no new hy-potheses are introduced by the operations ?, thesize of the hypothesis space H is determined bythe posterior probability threshold ?.
Only theML hypothesis remains at ?
= 0, since all itssubsequences are of high confidence, i.e.
can becovered by n-grams with non-zero path posteriorprobability.
At the other extreme, for ?
= 1, itfollows that H = E and no paths are removed,since any string regions created are formed fromsubsequences that occur on every path in E .We can therefore use ?
to tighten or relaxconstraints on the LMBR hypothesis space.
At?
= 0, LMBR returns only the ML hypothesis;at ?
= 1, LMBR is done over the full transla-tion lattice.
This is shown in Table 1, where theBLEU score approaches the BLEU score of un-constrained LMBR as ?
increases.Note also that the size of the resulting hypoth-esis space is the product of the number of se-quences in the sublattice regions.
For Figure 2 at?
= 0.8, this product is ?5.4 billion hypotheses.Even for fairly aggressive constraints on the hy-pothesis space, many hypotheses remain.6 Monolingual Coverage ConstraintsThis section describes one implementation of thetransformation function ?
that we will show leadsto improved fluency of machine translation out-put.
This transformation is based on n-gram cov-erage in a large target language text collection:where possible, we filter the sublattice regionsso that they contain only long-span n-grams ob-served in the text.
Our motivation is that largemonolingual text collections are good guides tofluency.
If a hypothesis is composed entirely ofpreviously seen high order n-grams, it is likely tobe fluent and should be favoured.Initial attempts to identify fluent hypotheses insublattice regions by ranking according to n-gramLM scores were ineffective.
Figure 3 shows thedifficulties.
We see that both the 4-gram Kneser-Ney and 5-gram stupid-backoff language models75LM Translation hypothesis E and n-gram orders used by the LM to score each word Score4g <s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 atomic3 bomb2 .3 </s>4 -22.59<s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 the4 atomic2 bomb3 .4 </s>4 -23.615g <s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 atomic5 bomb2 .3 </s>4 -16.04<s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 the4 atomic4 bomb5 .4 </s>5 -17.96Figure 3: Scores and n-gram orders for hypotheses using 4-gram Kneser-Ney and 5-gram stupid-backoff (estimated from 1.1B and 6.6B tokens, resp.)
LMs.
Low confidence regions are in italics.favour the shorter but disfluent hypothesis; nor-malising by length was not effective.
However,the stupid-backoff LM has better coverage and thebacking-off behaviour is a clue to the presenceof disfluency.
Similar cues have been observedin ASR analysis (Chase, 1997).
The shorter hy-pothesis backs off to a bigram for ?atomic bomb?,whereas the longer hypothesis covers the samewords with 4-grams and 5-grams.
We thereforedisregard the language model scores and focus onn-gram coverage.
This is an example where ro-bustness and fluency are at odds.
The n-grammodels are robust, but often favour less fluent hy-potheses.Let S denote the set of all n-grams in the mono-lingual training data.
To identify partial hypothe-ses in sublattice regions that have complete mono-lingual coverage at the maximum order n, webuild a coverage acceptor Cn with a similar formto the WFST representation of an n-gram backofflanguage model (Allauzen et al, 2003).
Cn as-signs a penalty to every n-gram not found in S .In Cn word arcs have no cost and backoff arcs areassigned a fixed cost of 1.
Firstly, arcs from thestart state are added for each unigram w ?
N1:ww/0?Then for n-grams u ?
S ?
{?ni=2 Ni}, whereu = wn1 consisting of history h = wn?11 and targetword wn, arcs are addedwn/0h h+where h+ = wn?12 if u has order n and h+ = wn1if u has order less than n. Backoff arcs are addedfor each u as?/1h h?where h?
= wn?12 if u has order > 2, and bi-grams backoff to the null history start state ?.For each sublattice region Hr, we wish to pe-nalise each path proportionally to the number ofits n-grams not found in the monolingual text col-lection S .
We wish to do this in context, so thatwe include the effect of the neighbouring highconfidence regions Hr?1 and Hr+1.
Given thatwe are counting n-grams at order n we form theleft context machine Lr which accepts the lastn ?
1 words in Hr?1; similarly, Rr accepts thefirst n ?
1 words of Hr+1.
The concatenationXr = Lr?Hr?Rr represents the partial transla-tion hypotheses inHr padded with n?1 words ofleft and right context from the neighbouring highconfidence regions.
Composing Xr ?
Cn assignseach partial hypothesis a cost equal to the numberof times it was necessary to back off to lower ordern-grams while reading the string.
Partial hypothe-ses with cost 0 did not back off at all and containonly maximum order n-grams.In the following experiments, we look at eachXn ?
Cn and if there are paths with cost 0, onlythese are kept and all others discarded.
We intro-duce this as a constraint on the hypothesis spacewhich we will evaluate for improvement on flu-ency.
Here the transformation function ?
returnsHr as Xr ?Cn after pruning.
If Xr ?Cn has no zerocost paths, the transformation function ?
returnsHr as we find it, since there is not enough mono-lingual coverage to guide the selection of fluenthypotheses.
After applying monolingual coverageconstraints to each region, the modified hypothe-sis space used for MBR search is formed by con-catenation using Equation (7).We note that Cn is a simplistic NLG system.
Itgenerates strings by concatenating n-grams foundin S .
We do not allow it to run ?open loop?
in theseexperiments, but instead use it to find the stringsin Xr with good n-gram coverage.7 LMBR Over Segmented LatticesThe effect of fluency constraints on LMBR de-coding is evaluated in the context of the NISTArabic?English MT task.
The set tune consists76ML ... view , especially with the open chinese economy to the world and ...+LMBR ... view , especially with the open chinese economy to the world and ...+LMBR+CC ... view , especially with the opening of the chinese economy to the world and ...ML ... revision of the constitution of the japanese public , which dates back ...+LMBR ... revision of the constitution of the japanese public , which dates back ...+LMBR+CC ... revision of the constitution of japan , which dates back ...Figure 4: Improved fluency through the application of monolingual coverage constraints to the hypoth-esis space in MBR decoding of NIST MT 08 Arabic?English newswire lattices.of the odd numbered sentences of the MT02?MT05 testsets; the even numbered sentences formtest.
MT08 performance on nw08 (newswire) andng08 (newsgroup) data is also reported.First-pass translation is performed using HiFST(Iglesias et al, 2009), a hierarchical phrase-baseddecoder.
The first-pass LM is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimatedover the English side of the parallel text and an881M word subset of the English GigaWord 3rdEdition.
Prior to LMBR, the first-pass lattices arerescored with zero-cutoff stupid-backoff 5-gramlanguage models (Brants et al, 2007) estimatedover more than 6B words of English text.
TheLMBR factors ?0, .
.
.
, ?4 are set as in Tromble etal.
(2008) using unigram precision p = 0.85 andrecall ratio r = 0.74.The effect of performing LMBR over the seg-mented hypothesis space is shown in Table 1.
Thehypothesis subspaces Hr are constructed at var-ious confidence thresholds as described in Sec-tion 4 with H formed via Equation (7); no cover-age constraints are applied yet.
Constraining thesearch space using ?
= 0.6 leads to little degra-dation in LMBR performance under BLEU.
Thisshows lattice segmentation works as intended.We next investigate the effect of monolingualcoverage constraints on BLEU.
We build accep-tors Cn as described in Section 6 with S con-sisting of all n-grams in the English GigaWord.At ?
= 0.6 we found 181 sentences with sub-lattices Hr spanned by maximum order n-gramsfrom S , i.e.
for which Xr ?
Cn have paths withcost 0; these are filtered as described.
LMBRover these coverage-constrained sublattices is de-noted LMBR+CC.
On nw08 the BLEU score forLMBR+CC is 52.0 which is +0.7 over the ML de-coder and only -0.2 BLEU below unconstrainedLMBR decoding.
Done in this way, constraininghypotheses to have 5-grams from the GigaWordtune test nw08 ng08ML 54.2 53.8 51.3 36.3?0.0 54.2 53.8 51.3 36.30.2 54.3 53.8 51.3 36.30.4 54.6 54.2 51.6 36.70.6 54.9 54.4 52.1 36.60.8 54.9 54.4 52.1 36.61.0 54.9 54.4 52.2 36.7LMBR 54.9 54.4 52.2 36.8Table 1: BLEU scores for ML hypotheses andLMBR decoding inH over 0 ?
?
?
1.has little impact on BLEU.At this value of ?, 116 of the 813 nw08 sen-tences have a low confidence region (1) com-pletely covered by 5-grams, and (2) within whichthe ML hypothesis and the LMBR+CC hypothe-sis differ.
It is these regions which we will inspectfor improved fluency.8 Human Fluency EvaluationWe asked 17 native speakers to judge the fluencyof sentence fragments from nw08.
We comparedhypotheses from the ML and the LMBR+CC de-coders.
Each fragment consisted of the partialtranslation hypothesis from a low confidence re-gion together with its left and right high confi-dence contexts (examples given in Figure 4).
Foreach sample, judges were asked: ?Could this frag-ment occur in a fluent sentence?
?The results are shown in Table 2.
Most of thetime, the ML and LMBR+CC sentence fragmentswere both judged to be fluent; it often happenedthat they differed by only a single noun or verbsubstitution which didn?t affect fluency.
In a smallnumber of cases, both ML and LMBR+CC werejudged to be disfluent.
We are most interested inthe ?off-diagonal?
cases.
In cases when one sys-tem was judged to be fluent and the other was not,LMBR+CC was preferred about twice as often asthe ML baseline (26.9% to 9.7%).
In other words,the monolingual fluency constraints were judged77LMBR+CCFluent Not FluentML Fluent 1175 (59.6%) 192 (9.7%)Not Fluent 530 (26.9%) 75 (3.8%)Table 2: Partial hypothesis fluency judgements.to have improved the fluency of the low confi-dence region more than twice as often as a fluenthypothesis was made disfluent.Some examples of improved fluency are shownin Figure 4.
Although both the ML and un-constrained LMBR hypotheses might satisfy ad-equacy, they lack the fluency of the LMBR+CChypotheses generated using monolingual fluencyconstraints.9 Summary and DiscussionWe have described a general framework for im-proving SMT fluency.
Decoupling the hypothesisspace from the evidence space allows for muchgreater flexibility in lattice MBR search.We have shown that high path posterior proba-bility n-grams in the ML translation can be used toguide the segmentation of a lattice into regions ofhigh and low confidence.
Segmenting the latticesimplifies the process of refining the hypothesisspace since low confidence regions can be refinedin the context of their high confidence neighbours.This can be done independently before reassem-bling the refined regions.
Lattice segmentationfacilitates the application of post-processing andrescoring techniques targeted to address particu-lar deficiencies in ML decoding.The techniques we presented are related to con-sensus decoding and system combination for SMT(Matusov et al, 2006; Sim et al, 2007), and tosegmental MBR for automatic speech recognition(Goel et al, 2004).
Mohit et al (2009) describean alternative approach to improving specific por-tions of translation hypotheses.
They use an SVMclassifier to identify a single phrase in each sourcelanguage sentence that is ?difficult to translate?
;such phrases are then translated using an adaptedlanguage model estimated from parallel data.
Incontrast to their approach, our approach is ableto exploit large collections of monolingual data torefine multiple low confidence regions using pos-terior probabilities obtained from a high-qualityevidence space of first-pass translations.Testset Sentences Reachabilitytune 2075 15%test 2040 14%nw08 813 11%ng08 547 9%Table 3: Arabic?English reference reachability.We applied hypothesis space constraints basedon monolingual coverage to low confidence re-gions resulting in improved fluency with no realdegradation in BLEU score relative to uncon-strained LMBR decoding.
This approach is lim-ited by the coverage of sublattices using monolin-gual text.
We expect this to improve with largertext collections or in tightly focused scenarioswhere in-domain text is less diverse.However, fluency will be best improved by inte-grating more sophisticated natural language gen-eration.
NLG systems capable of generating sen-tence fragments in context can be incorporated di-rectly into this framework.
If the MBR hypothe-sis spaceH contains a generated hypothesis E?
forwhich P (F |E?)
= 0, E?
could still be produced asa translation, since it can be ?voted for?
by nearbyhypotheses produced by the underlying system.Table 3 shows the proportion of NIST testsetsentences that can be aligned to any of the ref-erence translations using our high quality base-line hierarchical decoder with a powerful gram-mar.
The low level of reachability suggests thatNLG may be required to achieve high levels oftranslation quality and fluency.
Other rescoringapproaches (Kumar et al, 2009; Li et al, 2009)may also benefit from NLG when the baseline isincapable of generating the reference.We note that our approach could also be used toimprove the fluency of ASR, OCR and other lan-guage processing tasks where the goal is to pro-duce fluent natural language output.AcknowledgmentsWe would like to thank Matt Gibson and thehuman judges who participated in the evalua-tion.
This work was supported in part under theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-C-0022 and the European Union SeventhFramework Programme (FP7-ICT-2009-4) underGrant Agreement No.
247762.78ReferencesAllauzen, Cyril, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proceedings of ACL 2003.Blackwood, Graeme, Adria` de Gispert, and William Byrne.2010.
Efficient path counting transducers for minimumBayes-risk decoding of statistical machine translation lat-tices.
In Proceedings of ACL 2010.Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models in ma-chine translation.
In Proceedings of the EMNLP 2007.Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vin-cent J. Della Pietra, Fredrick Jelinek, John D. Lafferty,Robert L. Mercer, and Paul S. Roossin.
1990.
A sta-tistical approach to machine translation.
ComputationalLinguistics, 16(2):79?85.Callison-Burch, Chris, Philipp Koehn, Christof Monz, andJosh Schroeder.
2009.
Findings of the 2009 Workshopon Statistical Machine Translation.
In WMT 2009.Chase, Lin Lawrance.
1997.
Error-responsive feed-back mechanisms for speech recognizers, Ph.D. Thesis,Carnegie Mellon University.Chiang, David.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.DeNero, John, David Chiang, and Kevin Knight.
2009.
Fastconsensus decoding over translation forests.
In Proceed-ings of ACL-IJCNLP 2009.Goel, V., S. Kumar, and W. Byrne.
2004.
Segmental mini-mum Bayes-risk decoding for automatic speech recogni-tion.
IEEE Transactions on Speech and Audio Process-ing, 12:234?249.Iglesias, Gonzalo, Adria` de Gispert, Eduardo R. Banga, andWilliam Byrne.
2009.
Hierarchical phrase-based trans-lation with weighted finite state transducers.
In Proceed-ings of the 2009 Annual Conference of the NAACL.Kneser, R. and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Acoustics, Speech, andSignal Processing.Knight, K and J Graehl.
2005.
An overview of probabilis-tic tree transducers for natural language processing.
InProceedings of CICLING 2005.Knight, K. 2007a.
Capturing practical natural languagetransformations.
Machine Translation, 21(2).Knight, Kevin.
2007b.
Automatic language translation gen-eration help needs badly.
In MT Summit XI Workshop onUsing Corpora for NLG: Keynote Address.Kumar, Shankar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine translation.
InNAACL 2004.Kumar, Shankar, Wolfgang Macherey, Chris Dyer, and FranzOch.
2009.
Efficient minimum error rate training andminimum bayes-risk decoding for translation hypergraphsand lattices.
In Proceedings of ACL-IJCNLP 2009.Lavie, Alon and Michael J. Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machine trans-lation.
Machine Translation Journal.Li, Zhifei, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine translation.In Proceedings of ACL-IJCNLP 2009.Ma, Xiaoyi and Christopher Cieri.
2006.
Corpus support formachine translation at LDC.
In LREC 2006.Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.
2006.Computing consensus translation from multiple machinetranslation systems using enhanced hypotheses align-ment.
In 11th Conference of the EACL.Mohit, B., F. Liberato, and R. Hwa.
2009.
Language modeladaptation for difficult-to-translate phrases.
In Proceed-ings of the 13th Annual Conference of the EAMT.Mohri, Mehryar, Fernando Pereira, and Michael Riley.
2002.Weighted finite-state transducers in speech recognition.In CSL, volume 16, pages 69?88.Oberlander, Jon and Chris Brew.
2000.
Stochastic text gen-eration.
In Philosophical Transactions of the Royal Soci-ety.Och, F., D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,Z.
Jin, and D. Radev.
2004.
A smorgasbord of featuresfor statistical machine translation.
In Proceedings of theHLT Conference of the NAACL.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a method for automatic evaluation ofmachine translation.
In Proceedings of ACL 2002.Sim, K.-C., W. Byrne, M. Gales, H. Sahbi, and P.C.
Wood-land.
2007.
Consensus network decoding for statisti-cal machine translation system combination.
In ICASSP2007.Snover, Matthew, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, , and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In Pro-ceedings of AMTA.Tromble, Roy, Shankar Kumar, Franz Och, and WolfgangMacherey.
2008.
Lattice minimum Bayes-risk decodingfor statistical machine translation.
In Proceedings of the2008 Conference on EMNLP.Ueffing, Nicola and Hermann Ney.
2007.
Word-level confi-dence estimation for machine translation.
ComputationalLinguistics, 33(1):9?40.Vilar, D, G Leusch, H Ney, and R Banchs.
2007.
Humanevaluation of machine translation through binary systemcomparisons.
In Proceedings of WMT 2007.Zens, Richard and Hermann Ney.
2006.
N -gram posteriorprobabilities for statistical machine translation.
In Pro-ceedings of WMT 2006.79
