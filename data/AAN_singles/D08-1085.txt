Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 812?819,Honolulu, October 2008. c?2008 Association for Computational LinguisticsAttacking Decipherment Problems Optimally with Low-Order N-gramModelsSujith Ravi and Kevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292{sravi,knight}@isi.eduAbstractWe introduce a method for solving substi-tution ciphers using low-order letter n-grammodels.
This method enforces global con-straints using integer programming, and itguarantees that no decipherment key is over-looked.
We carry out extensive empirical ex-periments showing how decipherment accu-racy varies as a function of cipher length andn-gram order.
We also make an empirical in-vestigation of Shannon?s (1949) theory of un-certainty in decipherment.1 IntroductionA number of papers have explored algorithmsfor automatically solving letter-substitution ciphers.Some use heuristic methods to search for the best de-terministic key (Peleg and Rosenfeld, 1979; Gane-san and Sherman, 1993; Jakobsen, 1995; Olson,2007), often using word dictionaries to guide thatsearch.
Others use expectation-maximization (EM)to search for the best probabilistic key using lettern-gram models (Knight et al, 2006).
In this paper,we introduce an exact decipherment method basedon integer programming.
We carry out extensive de-cipherment experiments using letter n-gram models,and we find that our accuracy rates far exceed thoseof EM-based methods.We also empirically explore the concepts in Shan-non?s (1949) paper on information theory as appliedto cipher systems.
We provide quantitative plots foruncertainty in decipherment, including the famousunicity distance, which estimates how long a ciphermust be to virtually eliminate such uncertainty.We find the ideas in Shannon?s (1949) paper rel-evant to problems of statistical machine translationand transliteration.
When first exposed to the ideaof statistical machine translation, many people natu-rally ask: (1) how much data is needed to get a goodresult, and (2) can translation systems be trainedwithout parallel data?
These are tough questions byany stretch, and it is remarkable that Shannon wasalready in the 1940s tackling such questions in therealm of code-breaking, creating analytic formulasto estimate answers.Our novel contributions are as follows:?
We outline an exact letter-substitution deci-pherment method which:- guarantees that no key is overlooked, and- can be executed with standard integer pro-gramming solvers?
We present empirical results for deciphermentwhich:- plot search-error-free decipherment results atvarious cipher lengths, and- demonstrate accuracy rates superior to EM-based methods?
We carry out empirical testing of Shannon?sformulas for decipherment uncertainty2 Language ModelsWe work on letter substitution ciphers with spaces.We look for the key (among 26!
possible ones)that, when applied to the ciphertext, yields the mostEnglish-like result.
We take ?English-like?
to mean812most probable according to some statistical lan-guage model, whose job is to assign some proba-bility to any sequence of letters.
According to a 1-gram model of English, the probability of a plaintextp1...pn is given by:P (p1...pn) = P (p1) ?
P (p2) ?
... ?
P (pn)That is, we obtain the probability of a sequenceby multiplying together the probabilities of the in-dividual letters that make it up.
This model assignsa probability to any letter sequence, and the proba-bilities of all letter sequences sum to one.
We col-lect letter probabilities (including space) from 50million words of text available from the LinguisticData Consortium (Graff and Finch, 1994).
We alsoestimate 2- and 3-gram models using the same re-sources:P (p1...pn) = P (p1 | START ) ?
P (p2 | p1) ?
P (p3 | p2) ?...
?
P (pn | pn?1) ?
P (END | pn)P (p1...pn) = P (p1 | START ) ?
P (p2 | START p1) ?P (p3 | p1 p2) ?
... ?
P (pn | pn?2 pn?1) ?P (END | pn?1 pn)Unlike the 1-gram model, the 2-gram model willassign a low probability to the sequence ?ae?
be-cause the probability P (e | a) is low.
Of course, allthese models are fairly weak, as already known by(Shannon, 1949).
When we stochastically generatetext according to these models, we get, for example:1-gram: ... thdo detusar ii c ibt deg irn toihytrsen ...2-gram: ... itariaris s oriorcupunond rke uth ...3-gram: ... ind thnowelf jusision thad inat of ...4-gram: ... rece bence on but ther servier ...5-gram: ... mrs earned age im on d the perious ...6-gram: ... a party to possible upon rest of ...7-gram: ... t our general through approve the ...We can further estimate the probability of a wholeEnglish sentence or phrase.
For example, the prob-abilities of two plaintext phrases ?het oxf?
and?the fox?
(which have the same letter frequencydistribution) is shown below.
The 1-gram modelwhich counts only the frequency of occurrence ofeach letter in the phrase, estimates the same proba-bility for both the phrases ?het oxf?
and ?the fox?,since the same letters occur in both phrases.
On theother hand, the 2-gram and 3-gram models, whichtake context into account, are able to distinguish be-tween the English and non-English phrases better,and hence assign a higher probability to the Englishphrase ?the fox?.Model P(het oxf) P(the fox)1-gram 1.83?
10?9 1.83?
10?92-gram 3.26?
10?11 1.18?
10?73-gram 1.89?
10?13 1.04?
10?6Over a longer sequence X of length N , we canalso calculate ?log2(P (X))/N , which (per Shan-non) gives the compression rate permitted by themodel, in bits per character.
In our case, we get:11-gram: 4.192-gram: 3.513-gram: 2.933 DeciphermentGiven a ciphertext c1...cn, we search for the key thatyields the most probable plaintext p1...pn.
There are26!
possible keys, too many to enumerate.
How-ever, we can still find the best one in a guaranteedfashion.
We do this by taking our most-probable-plaintext problem and casting it as an integer pro-gramming problem.2Here is a sample integer programming problem:variables: x, yminimize:2x+ ysubject to:x+ y < 6.9y ?
x < 2.5y > 1.1We require that x and y take on integer values.
Asolution can be obtained by typing this integer pro-gram into the publicly available lp solve program,1Because spacing is fixed in our letter substitution ciphers,we normalize P (X) by the sum of probabilities of all Englishstrings that match the spacing pattern of X .2For an overview of integer and linear programming, see forexample (Schrijver, 1998).8131    2    3    4    5    6    7    8  ?_    Q    W    B    S    Q    W    _  ?a    a    a    a    a    a    a    a  ?b    b    b    b    b    b    b    b  ?c    c    c    c    c    c    c    c  ?d    d    d    d    d    d    d    d  ?e    e    e    e    e    e    e    e  ??
?
?
?
?
?
?
?
?z    z    z    z    z    z    z    z  ?_    _    _    _    _    _    _    _  ?ciphertextnetwork ofpossibleplaintextslink-2de link-5ad link-7e_Figure 1: A decipherment network.
The beginning of the ciphertext is shown at the top of the figure (underscoresrepresent spaces).
Any left-to-right path through the network constitutes a potential decipherment.
The bold pathcorresponds to the decipherment ?decade?.
The dotted path corresponds to the decipherment ?ababab?.
Given acipher length of n, the network has 27 ?
27 ?
(n ?
1) links and 27n paths.
Each link corresponds to a named variablein our integer program.
Three links are shown with their names in the figure.or the commercially available CPLEX program,which yields the result: x = 4, y = 2.Suppose we want to decipher with a 2-gram lan-guage model, i.e., we want to find the key that yieldsthe plaintext of highest 2-gram probability.
Giventhe ciphertext c1...cn, we create an integer program-ming problem as follows.
First, we set up a net-work of possible decipherments (Figure 1).
Eachof the 27 ?
27 ?
(n ?
1) links in the network is abinary variable in the integer program?it must beassigned a value of either 0 or 1.
We name thesevariables linkXY Z , where X indicates the columnof the link?s source, and Y and Z represent the rowsof the link?s source and destination (e.g.
variableslink1aa, link1ab, link5qu, ...).Each distinct left-to-right path through the net-work corresponds to a different decipherment.
Forexample, the bold path in Figure 1 corresponds tothe decipherment ?decade?.
Decipherment amountsto turning some links ?on?
(assigning value 1 to thelink variable) and others ?off?
(assigning value 0).Not all assignments of 0?s and 1?s to link variablesresult in a coherent left-to-right path, so we mustplace some ?subject to?
constraints in our integerprogram.We observe that a set of variables forms a path if,for every node in columns 2 through n?1 of the net-work, the following property holds: the sum of thevalues of the link variables entering the node equalsthe sum of the link variables leaving the node.
Fornodes along a chosen decipherment path, this sumwill be 1, and for others, it will be 0.3 Therefore,we create one ?subject to?
constraint for each node(?
?
stands for space).
For example, for the node incolumn 2, row e we have:subject to:link1ae + link1be + link1ce + ...+ link1 e= link2ea + link2eb + link2ec + ...+ link2eNow we set up an expression for the ?minimize?part of the integer program.
Recall that we wantto select the plaintext p1...pn of highest probability.For the 2-gram language model, the following areequivalent:(a) Maximize P (p1...pn)(b) Maximize log2 P (p1...pn)(c) Minimize ?log2 P (p1...pn)(d) Minimize ?log2 [ P (p1 |START )3Strictly speaking, this constraint over nodes still allowsmultiple decipherment paths to be active, but we can rely onthe rest of our integer program to select only one.814?P (p2 | p1)?
...?P (pn | pn?1)?P (END | pn) ](e) Minimize ?log2 P (p1 |START )?log2 P (p2 | p1)?
...?log2 P (pn | pn?1)?log2 P (END | pn)We can guarantee this last outcome if we con-struct our minimization function as a sum of 27 ?27 ?(n?
1) terms, each of which is a linkXY Z variablemultiplied by ?log2P (Z|Y ):Minimize link1aa ?
?log2 P (a | a)+ link1ab ?
?log2 P (b | a)+ link1ac ?
?log2 P (c | a)+ ...+ link5qu ?
?log2 P (u | q)+ ...When we assign value 1 to link variables alongsome decipherment path, and 0 to all others, thisfunction computes the negative log probability ofthat path.We must still add a few more ?subject to?
con-straints.
We need to ensure that the chosen path im-itates the repetition pattern of the ciphertext.
Whilethe bold path in Figure 1 represents the fine plain-text choice ?decade?, the dotted path represents thechoice ?ababab?, which is not consistent with therepetition pattern of the cipher ?QWBSQW?.
Tomake sure our substitutions obey a consistent key,we set up 27 ?
27 = 729 new keyxy variables torepresent the choice of key.
These new variablesare also binary, taking on values 0 or 1.
If variablekeyaQ = 1, that means the key maps plaintext a tociphertext Q.
Clearly, not all assignments to these729 variables represent valid keys, so we augmentthe ?subject to?
part of our integer program by re-quiring that for any letter x,subject to:keyxA + keyxB + ...+ keyxZ + keyx = 1keyAx + keyBx + ...+ keyZx + key x = 1That is, every plaintext letter must map to exactlyone ciphertext letter, and every ciphertext letter mustmap to exactly one plaintext letter.
We also add aconstraint to ensure that the ciphertext space charac-ter maps to the plaintext space character:subject to:key = 1Finally, we ensure that any chosen deciphermentpath of linkXY Z variables is consistent with thechosen key.
We know that for every node A alongthe decipherment path, exactly one active link hasA as its destination.
For all other nodes, zero activelinks lead in.
Suppose node A represents the de-cipherment of ciphertext letter ci as plaintext letterpj?for all such nodes, we stipulate that the sum ofvalues for link(i?1)xpj (for all x) equals the value ofkeypjci .
In other words, whether a node lies alongthe chosen decipherment path or not, the chosen keymust support that decision.Figure 2 summarizes the integer program that weconstruct from a given ciphertext c1...cn.
The com-puter code that transforms any given cipher into acorresponding integer program runs to about onepage.
Variations on the decipherment network yield1-gram and 3-gram decipherment capabilities.
Oncean integer program is generated by machine, weask the commercially-available CPLEX softwareto solve it, and then we note which keyXY variablesare assigned value 1.
Because CPLEX computesthe optimal key, the method is not fast?for ciphersof length 32, the number of variables and constraintsencoded in the integer program (IP) along with aver-age running times are shown below.
It is possible toobtain less-than-optimal keys faster by interruptingthe solver.Model # of IP # of IP Averagevariables constraints running time1-gram 1, 755 1, 083 0.01 seconds2-gram 27, 700 2, 054 50 seconds3-gram 211, 600 27, 326 450 seconds4 Decipherment ExperimentsWe create 50 ciphers each of lengths 2, 4, 8, ..., 256.We solve these with 1-gram, 2-gram, and 3-gramlanguage models.
We record the average percentageof ciphertext tokens decoded incorrectly.
50% errormeans half of the ciphertext tokens are decipheredwrong, while 0% means perfect decipherment.
Here815variables:linkipr 1 if the ith cipher letter is deciphered as plaintext letter p AND the (i+1)th cipher letter isdeciphered as plaintext letter r0 otherwisekeypq 1 if decipherment key maps plaintext letter p to ciphertext letter q0 otherwiseminimize:?n?1i=1?p,rlinkipr ?
?log P (r|p) (2-gram probability of chosen plaintext)subject to:for all p:?rkeypr = 1 (each plaintext letter maps to exactly one ciphertext letter)for all p:?rkeyrp = 1 (each ciphertext letter maps to exactly one plaintext letter)key = 1 (cipher space character maps to plain space character)for (i=1...n-2), for all r: [?plinkipr =?plink(i+1)rp ] (chosen links form a left-to-right path)for (i=1...n-1), for all p:?rlinkirp = keypci+1 (chosen links are consistent with chosen key)Figure 2: Summary of how to build an integer program for any given ciphertext c1...cn.
Solving the integer programwill yield the decipherment of highest probability.we illustrate some automatic decipherments with er-ror rates:42% error: the avelage ongrichman hal cy wiof asevesonme qus antizexty that he buprk lathes we blungthan soment - fotes mmasthes11% error: the average englishman has so week areference for antiality that he would rather be prong thanrecent - deter quarteur2% error: the average englishman has so keep areference for antiquity that he would rather be wrong thanrecent - peter mcarthur0% error: the average englishman has so deep areverence for antiquity that he would rather be wrongthan recent - peter mcarthurFigure 3 shows our automatic decipherment re-sults.
We note that the solution method is exact, notheuristic, so that decipherment error is not due tosearch error.
Our use of global key constraints alsoleads to accuracy that is superior to (Knight et al,2006).
With a 2-gram model, their EM algorithmgives 10% error for a 414-letter cipher, while ourmethod provides a solution with only 0.5% error.At shorter cipher lengths, we observe much higherimprovements when using our method.
For exam-0102030405060708090100Decipherment Error (%)Cipher Length (letters)2 4 8 16 32 64 128 256 512 10241-gram2-gram3-gramFigure 3: Average decipherment error using integer pro-gramming vs. cipher length, for 1-gram, 2-gram and 3-gram models of English.
Error bars indicate 95% confi-dence intervals.ple, on a 52-letter textbook cipher, using a 2-grammodel, the solution from our method resulted in 21%error as compared to 85% error given by the EM so-lution.We see that deciphering with 3-grams works wellon ciphers of length 64 or more.
This confirms816that such ciphers can be attacked with very limitedknowledge of English (no words or grammar) andlittle custom programming.The 1-gram model works badly in this scenario,which is consistent with Bauer?s (2006) observationthat for short texts, mechanical decryption on the ba-sis of individual letter frequencies does not work.
Ifwe had infinite amounts of ciphertext and plaintextdrawn from the same stochastic source, we wouldexpect the plain and cipher frequencies to eventuallyline up, allowing us to read off a correct key from thefrequency tables.
The upper curve in Figure 3 showsthat convergence to this end is slow.5 Shannon Equivocation and UnicityDistanceVery short ciphers are hard to solve accurately.Shannon (1949) pinpointed an inherent difficultywith short ciphers, one that is independent of the so-lution method or language model used; the cipheritself may not contain enough information for itsproper solution.
For example, given a short cipherlike XY Y X , we can never be sure if the answer ispeep, noon, anna, etc.
Shannon defined a mathemat-ical measure of our decipherment uncertainty, whichhe called equivocation (now called entropy).Let C be a cipher, M be the plaintext message itencodes, and K be the key by which the encodingtakes place.
Before even seeing C, we can computeour uncertainty about the key K by noting that thereare 26!
equiprobable keys:4H(K) = ?(26!)
?
(1/26!)
?
log2 (1/26!
)= 88.4 bitsThat is, any secret key can be revealed in 89 bits.When we actually receive a cipher C, our uncer-tainty about the key and the plaintext message is re-duced.
Shannon described our uncertainty about theplaintext message, letting m range over all decipher-ments:H(M |C) = equivocation of plaintext message= ?
?mP (m|C) ?
log2 P (m|C)4(Shannon, 1948) The entropy associated with a set of pos-sible events whose probabilities of occurrence are p1, p2, ..., pnis given by H = ?
?ni=1 pi ?
log2(pi).P (m|C) is probability of plaintext m (accordingto the language model) divided by the sum of proba-bilities of all plaintext messages that obey the repeti-tion pattern of C. While integer programming givesus a method to find the most probable deciphermentwithout enumerating all keys, we do not know of asimilar method to compute a full equivocation with-out enumerating all keys.
Therefore, we sample upto 100,000 plaintext messages in the neighborhoodof the most probably decipherment5 and computeH(M |C) over that subset.6Shannon also described H(K|C), the equivoca-tion of key.
This uncertainty is typically larger thanH(M |C), because a given message M may be de-rived from C via more than one key, in case C doesnot contain all 26 letters of the alphabet.We compute H(K|C) by letting r(C) be thenumber of distinct letters in C, and letting q(C) be(26 ?
r(C))!.
Letting i range over our sample ofplaintext messages, we get:H(K|C) = equivocation of key= ?
?iq(C) ?
(P (i)/q(C)) ?
log2 (P (i)/q(C))= ?
?iP (i) ?
log2 (P (i)/q(C))= ?
?iP (i) ?
(log2 P (i)?
log2 q(C))= ?
?iP (i) ?
log2 P (i) +?iP (i) ?
log2 q(C)= H(M |C) + log2 q(C)Shannon (1949) used analytic means to roughlysketch the curves for H(K|C) and H(M |C), whichwe reproduce in Figure 4.
Shannon?s curve is drawnfor a human-level language model, and the y-axis isgiven in ?decimal digits?
instead of bits.5The sampling used to compute H(M |C) starts with theoptimal key and expands out a frontier, by swapping letters inthe key, and recursing to generate new keys (and correspondingplaintext message decipherments).
The plaintext messages areremembered so that the frontier expands efficiently.
The sam-pling stops if 100,000 different messages are found.6Interestingly, as we grow our sample out from the mostprobable plaintext, we do not guarantee that any intermediateresult is a lower bound on the equivocation.
An example is pro-vided by the growing sample (0.12, 0.01, 0.01, 0.01, 0.01, 0.01,0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01), whose entropy steadilyincreases.
However, if we add a 14th item whose P (m) is 0.12,the entropy suddenly decreases from 2.79 to 2.78.817Unicity DistanceKey EquivocationMessage EquivocationFigure 4: Equivocation for simple substitution on English (Shannon, 1949).01020304050607080900  10  20  30  40  50  60  70  80  90  100110120130140150160170180190200210220230240250260Equivocationofkey (bits)Cipher Length1-gram2-gram3-gramFigure 5: Average key equivocation observed (bits) vs.cipher length (letters), for 1-gram, 2-gram and 3-grammodels of English.For comparison, we plot in Figures 5 and 6 the av-erage equivocations as we empirically observe themusing our 1-, 2-, and 3-gram language models.The shape of the key equivocation curve followsShannon, except that it is curved from the start,rather than straight.The message equivocation curve follows Shan-non?s prediction, rising then falling.
Because veryshort ciphers have relatively few solutions (for ex-01020304050607080900  10  20  30  40  50  60  70  80  90  100110120130140150160170180190200210220230240250260Equivocationofmessage(bits)Cipher Length1-gram2-gram3-gramFigure 6: Average message equivocation observed (bits)vs. cipher length (letters), for 1-gram, 2-gram and 3-grammodels of English.ample, a one-letter cipher has only 26), the overalluncertainty is not that great.7 As the cipher getslonger, message equivocation rises.
At some point,it then decreases, as the cipher begins to reveal itssecret through patterns of repetition.Shannon?s analytic model also predicts a sharpdecline of message equivocation towards zero.
He7Uncertainty is only loosely related to accuracy?even if weare quite certain about a solution, it may still be wrong.818defines the unicity distance (U ) as the cipher lengthat which we have virtually no more uncertaintyabout the plaintext.
Using analytic means (and vari-ous approximations), he gives:U = H(K)/(A?B)where:A = bits per character of a 0-gram model (4.7)B = bits per character of the model used to decipherFor a human-level language model (B ?
1.2), heconcludes U ?
25, which is confirmed by practice.For our language models, the formula gives:U = 173 (1-gram)U = 74 (2-gram)U = 50 (3-gram)These numbers are in the same ballpark asBauer (2006), who gives 167, 74, and 59.
We notethat these predicted unicity distances are a bit toorosy, according to our empirical message equivoca-tion curves.
Our experience confirms this as well, as1-gram frequency counts over a 173-letter cipher aregenerally insufficient to pin down a solution.6 ConclusionWe provide a method for deciphering letter substi-tution ciphers with low-order models of English.This method, based on integer programming, re-quires very little coding and can perform an opti-mal search over the key space.
We conclude by not-ing that English language models currently used inspeech recognition (Chelba and Jelinek, 1999) andautomated language translation (Brants et al, 2007)are much more powerful, employing, for example,7-gram word models (not letter models) trained ontrillions of words.
Obtaining optimal keys accord-ing to such models will permit the automatic deci-pherment of shorter ciphers, but this requires morespecialized search than what is provided by gen-eral integer programming solvers.
Methods suchas these should also be useful for natural languagedecipherment problems such as character code con-version, phonetic decipherment, and word substitu-tion ciphers with applications in machine translation(Knight et al, 2006).7 AcknowledgementsThe authors wish to gratefully acknowledgeJonathan Graehl, for providing a proof to supportthe argument that taking a larger number of samplesdoes not necessarily increase the equivocation.
Thisresearch was supported by the Defense AdvancedResearch Projects Agency under SRI International?sprime Contract Number NBCHD040058.ReferencesFriedrich L. Bauer.
2006.
Decrypted Secrets: Methodsand Maxims of Cryptology.
Springer-Verlag.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of EMNLP-CoNLL.Ciprian Chelba and Frederick Jelinek.
1999.
Structuredlanguage modeling for speech recognition.
In Pro-ceedings of NLDB: 4th International Conference onApplications of Natural Language to Information Sys-tems.Ravi Ganesan and Alan T. Sherman.
1993.
Statisticaltechniques for language recognition: An introductionand guide for cryptanalysts.
Cryptologia, 17(4):321?366.David Graff and Rebecca Finch.
1994.
Multilingual textresources at the linguistic data consortium.
In Pro-ceedings of the HLT Workshop on Human LanguageTechnology.Thomas Jakobsen.
1995.
A fast method for cryptanalysisof substitution ciphers.
Cryptologia, 19(3):265?274.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-mada.
2006.
Unsupervised analysis for deciphermentproblems.
In Proceedings of the COLING/ACL.Edwin Olson.
2007.
Robust dictionary attack of shortsimple substitution ciphers.
Cryptologia, 31(4):332?342.Shmuel Peleg and Azriel Rosenfeld.
1979.
Break-ing substitution ciphers using a relaxation algorithm.Comm.
ACM, 22(11):598?605.Alexander Schrijver.
1998.
Theory of Linear and IntegerProgramming.
John Wiley & Sons.Claude E. Shannon.
1948.
A mathematical theoryof communication.
Bell System Technical Journal,27:379?423 and 623?656.Claude E. Shannon.
1949.
Communication theoryof secrecy systems.
Bell System Technical Journal,28:656?715.819
