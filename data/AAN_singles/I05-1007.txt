Parsing the Penn Chinese Treebankwith Semantic KnowledgeDeyi Xiong1,2, Shuanglong Li1,3,Qun Liu1, Shouxun Lin1, and Yueliang Qian11 Institute of Computing Technology, Chinese Academy of Sciences,P.O.
Box 2704, Beijing 100080, China{dyxiong, liuqun, sxlin}@ict.ac.cn2 Graduate School of Chinese Academy of Sciences3 University of Science and Technology BeijingAbstract.
We build a class-based selection preference sub-model to in-corporate external semantic knowledge from two Chinese electronic se-mantic dictionaries.
This sub-model is combined with modifier-head gen-eration sub-model.
After being optimized on the held out data by theEM algorithm, our improved parser achieves 79.4% (F1 measure), as wellas a 4.4% relative decrease in error rate on the Penn Chinese Treebank(CTB).
Further analysis of performance improvement indicates that se-mantic knowledge is helpful for nominal compounds, coordination, andNV tagging disambiguation, as well as alleviating the sparseness of in-formation available in treebank.1 IntroductionIn the recent development of full parsing technology, semantic knowledge is sel-dom used, though it is known to be useful for resolving syntactic ambiguities.The reasons for this may be twofold.
The first one is that it can be very difficultto add additional features which are not available in treebanks to generativemodels like Collins (see [1]), which are very popular for full parsing.
For smallertasks, like prepositional phrase attachment disambiguation, semantic knowledgecan be incorporated flexibly using different learning algorithms (see [2,3,4,5]).For full parsing with generative models, however, incorporating semantic knowl-edge may involve great changes of model structures.
The second reason is thatsemantic knowledge from external dictionaries seems to be noisy, ambiguous andnot available in explicit forms, compared with the information from treebanks.Given these two reasons, it seems to be difficult to combine the two differentinformation sources?treebank and semantic knowledge?into one integrated sta-tistical parsing model.One feasible way to solve this problem is to keep the original parsing modelunchanged and build an additional sub-model to incorporate semantic knowledgefrom external dictionaries.
The modularity afforded by this approach makesit easier to expand or update semantic knowledge sources with the treebankR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
70?81, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Parsing the Penn Chinese Treebank with Semantic Knowledge 71unchanged or vice versa.
Further, the combination of the semantic sub-modeland the original parsing model can be optimized automatically.In this paper, we build a class-based selection preference sub-model, whichis embedded in our lexicalized parsing model, to incorporate external seman-tic knowledge.
We use two Chinese electronic dictionaries and their combi-nation as our semantic information sources.
Several experiments are carriedout on the Penn Chinese Treebank to test our hypotheses.
The results indi-cate that a significant improvement in performance is achieved when seman-tic knowledge is incorporated into parsing model.
Further improvement analy-sis is made.
We confirm that semantic knowledge is indeed useful for nominalcompounds and coordination ambiguity resolution.
And surprisingly, semanticknowledge is also helpful to correct Chinese NV mistagging errors mentionedby Levy and Manning (see [12]).
Yet another great benefit to incorporatingsemantic knowledge is to alleviate the sparseness of information available intreebank.2 The Baseline ParserOur baseline parsing model is similar to the history-based, generative and lexical-ized Model 1 of Collins (see [1]).
In this model, the right hand side of lexicalizedrules is decomposed into smaller linguistic objects as follows:P (h) ?
#Ln(ln)...L1(l1)H(h)R1(r1)...Rm(rm)# .The uppercase letters are delexicalized nonterminals, while the lowercase lettersare lexical items, e.g.
head word and head tag (part-of-speech tag of the headword), corresponding to delexicalized nonterminals.
H(h) is the head constituentof the rule from which the head lexical item h is derived according to some headpercolation rules.1 The special termination symbol # indicates that there is nomore symbols to the left/right.
Accordingly, the rule probability is factored intothree distributions.
The first distribution is the probability of generating thesyntactic label of the head constituent of a parent node with label P , head wordHhw and head tag Hht:PrH(H |P, Hht, Hhw) .Then each left/right modifier of head constituent is generated in two steps: firstits syntactic label Mi and corresponding head tag Miht are chosen given contextfeatures from the parent (P ), head constituent (H, Hht, Hhw), previously gen-erated modifier (Mi?1, Mi?1ht) and other context information like the direction(dir) and distance2 (dis) to the head constituent:1 Here we use the modified head percolation table for Chinese from Xia (see [6]).2 Our distance definitions are different for termination symbol and non-terminationsymbol, which are similar to Klein and Manning (see [7]).72 D. Xiong et alPrM (Mi, Miht|HCM ) .where the history context HCM is defined as the joint event ofP, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis .Then the new modifier?s head word Mihw is also generated with the probability:PrMw(Mihw|HCMw ) .where the history context HCMw is defined as the joint event ofP, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis, Mi, Miht .All the three distributions are smoothed through Witten-Bell interpolationjust like Collins (see [1]).
For the distribution PrM , we build back-off struc-tures with six levels, which are different from Collins?
since we find our back-offstructures work better than the three-level back-off structures of Collins.
Forthe distribution PrMw , the parsing model backs off to the history context withhead word Hhw removed, then to the modifier head tag Miht, just like Collins.Gildea (see [9]) and Bikel (see [10]) both observed that the effect of bilexical de-pendencies is greatly impaired due to the sparseness of bilexical statistics.
Bikeleven found that the parser only received an estimate that made use of bilexi-cal statistics a mere 1.49% of the time.
However, according to the wisdom ofthe parsing community, lexical bigrams, the word pairs (Mihw, Hhw) are veryinformative with semantic constraints.
Along this line, in this paper, we buildan additional class-based selectional preference sub-model, which is describedin section 3, to make good use of this semantic information through selectionalrestrictions between head and modifier words.Our parser takes segmented but untagged sentences as input.
The probabilityof unknown words, Pr(uword|tag), is estimated based on the first character ofthe word and if the first characters are unseen, the probability is estimated byabsolute discounting.We do some linguistically motivated re-annotations for the baseline parser.The first one is marking non-recursive noun phrases from other common nounphrases without introducing any extra unary levels (see [1,8]).
We find this basicNP re-annotation very helpful for the performance.
We think it is because of theannotation style of the Upenn Chinese Treebank (CTB).
According to Xue et al(see [11]), noun-noun compounds formed by an uninterrupted sequence of wordsPOS-tagged as NNs are always left flat because of difficulties in determiningwhich modifies which.
The second re-annotation is marking basic VPs, which wethink is beneficial for reducing multilevel VP adjunction ambiguities (see [12]).To speed up parsing, we use the beam thresholding techniques in Xiong etal.
(see [13]).
In all cases, the thresholding for completed edges is set at ct = 9and incomplete edges at it = 7.
The performance of the baseline parser is 78.5%in terms of F1 measure of labeled parse constituents on the same CTB trainingand test sets with Bikel et al (see [14])Parsing the Penn Chinese Treebank with Semantic Knowledge 733 Incorporating Semantic KnowledgeIn this section, we describe how to incorporate semantic knowledge from externalsemantic dictionaries into parsing model to improve the performance.
Firstly, weextract semantic categories through two Chinese electronic semantic dictionariesand some heuristic rules.
Then we build a selection preference sub-model basedon extracted semantic categories.
In section 3.3, we present our experimentsand results in detail.
And finally, we compare parses from baseline parser withthose from the new parser incorporated with semantic knowledge.
We empiricallyconfirm that semantic knowledge is helpful for nominal compound, coordinationand POS tagging ambiguity resolution.
Additionally, we also find that semanticknowledge can greatly alleviate problems caused by data sparseness.3.1 Extracting Semantic CategoriesSemantic knowledge is not presented in treebanks and therefore has to be ex-tracted from external knowledge sources.
We have two Chinese electronic se-mantic dictionaries, both are good knowledge sources for us to extract semanticcategories.
One is the HowNet dictionary3, which covers 67,440 words definedby 2112 different sememes.
The other is the ?TongYiCi CiLin?
expanded version(henceforth CiLin)4, which represents 77,343 words in a dendrogram.HowNet (HN): Each sememe defined by the HowNet is regarded as a semanticcategory.
And through the hypernym-hyponym relation between different cat-egories, we can extract semantic categories at various granularity levels.
Sincewords may have different senses, and therefore different definitions in HowNet,we just use the first definition of words in HowNet.
At the first level HN1, we ex-tract the first definitions and use them as semantic categories of words.
Throughthe hypernym ladders, we can get HN2, HN3, by replacing categories at lowerlevel with their hypernyms at higher level.
Table 1 shows information aboutwords and extracted categories at different levels.CiLin (CL): CL is a branching diagram, where each node represents a semanticcategory.
There are three levels in total, and from the top down, 12 categories inthe first level (CL1), 97 categories in the second level (CL2), 1400 categories inthe third level (CL3).
We extract semantic categories at level CL1, CL2 and CL3.HowNet+CiLin: Since the two dictionaries have different ontologies and rep-resentations of semantic categories, we establish a strategy to combine them:HowNet is used as a primary dictionary, and CiLin as a secondary dictionary.If a word is not found in HowNet but found in Cilin, we will look up otherwords from its synset defined by CiLin in HowNet.
If HowNet query succeeds,the corresponding semantic category in HowNet will be assigned to this word.3 http://www.keenage.com/.4 The dictionary is recorded and expanded by Information Retrieval Laboratory,Harbin Institute of Technology.74 D. Xiong et alTable 1.
Sizes and coverage of words and semantic categories from different semanticknowledge sourcesData HN1 HN2 HN3 CL1 CL2 CL3words in train 9522 6040 6469words in test 1824 1538 1581words in both 1412 1293 1310classes in train - 1054 381 118 12 92 1033classes in test - 520 251 93 12 79 569classes in both - 504 248 93 12 79 552According to our experimental results, we choose HN2 as the primary semanticcategory set and combine it with CL1, CL2 and CL3.Heuristic Rules (HR): Numbers and time expressions are recognized usingsimple heuristic rules.
For a better recognition, one can define accurate regularexpressions.
However, we just collect suffixes and feature characters to matchstrings.
For example, Chinese numbers are strings whose characters all comefrom a predefined set.
These two classes are merged into HowNet and labelledby semantic categories from HowNet.In our experiments, we combine HN2, CL1/2/3, and HR as our externalsources.
In these combinations {HN2+CL1/2/3+HR}, all semantic classes comefrom the primary semantic category set HN2, therefore we get the same classcoverage that we obtain from the single source HN2 but a bigger word coverage.The number of covered words of these combinations in {train, test, both} is{7911, 1672, 1372} respectively.3.2 Building Class-Based Selection Preference Sub-modelThere are several ways to incorporate semantic knowledge into parsing model.Bikel (see [15]) suggested a way to capture semantic preferences by employingbilexical-class statistics, in other words, dependencies among head-modifier wordclasses.
Bikel did not carry it out and therefore greater details are not available.However, the key point, we think, is to use classes extracted from semanticdictionary, instead of words, to model semantic dependencies between head andmodifier.
Accordingly, we build a similar bilexical-class sub-model as follows:Prclass(CMihw|CHhw, Hht, Miht, dir) .where CMihw and CHhw represent semantic categories of words Mihw and Hhw,respectively.
This model is combined with sub-model PrMw to form a mixturemodel Pmix:Prmix = ?PrMw + (1 ?
?
)Prclass .
(1)?
is hand-optimized, and an improvement of about 0.5% in terms of F1 measure isgained.
However, even a very slight change in the value of ?, e.g.
0.001, will havea great effect on the performance.
Besides, it seems that the connection betweenParsing the Penn Chinese Treebank with Semantic Knowledge 75entropy, i.e.
the total negative logarithm of the inside probability of trees, and F1measure, is lost, while this relation is observed in many experiments.
Therefore,automatic optimization algorithms, like EM, can not work in this mixture model.The reason, we guess, is that biclass dependencies among head-modifier wordclasses seem too coarse-grained to capture semantic preferences between headand modifier.
In most cases, a head word has a strong semantic constraints onthe concept ?
of mw, one of its modifier words, but that doesn?t mean otherwords in the same class with the head word has the same semantic preferenceson the concept ?.
For example, the verb eat impose a selection restriction onits object modifier5: it has to be solid food.
On the other hand, the verb drinkspecifies its object modifier to be liquid beverage.
At the level HN2, verb eatand drink have the same semantic category metabolize.
However, they imposedifferent selection preferences on their PATIENT roles.To sum up, bilexical dependencies are too fine-grained when being used tocapture semantic preferences and therefore lead to serious data sparseness.
Bi-class dependencies, which result in an unstable performance improvement, on theother hand, seem to be too coarse-grained for semantic preferences.
We build aclass-based selection preference model:Prsel(CMihw|Hhw, P ) .This model is similar to Resnik (see [2]).
We use the parent node label P torepresent the grammatical relation between head and modifier.
Besides, in thismodel, only modifier word is replaced with its semantic category.
The depen-dencies between head word and modifier word class seem to be just right forcapturing these semantic preferences.The final mixture model is the combination of the class-based selection pref-erence sub-model Prsel and modifier-head generation sub-model PrMw :Prmix = ?PrMw + (1 ?
?
)Prsel .
(2)Since the connection between entropy and F1 measure is observed again, EMalgorithm is used to optimize ?.
Just like Levy (see [12]), we set aside articles 1-25 in CTB as held out data for EM algorithm and use articles 26-270 as trainingdata during ?
optimization.3.3 Experimental ResultsWe have designed several experiments to check the power of our class-based se-lection preference model with different semantic data sources.
In all experiments,we first use the EM algorithm to optimize the parameter ?.
As mentioned above,during parameter optimization, articles 1-25 are used as held out data and ar-ticles 26-270 are used as training data.
Then we test our mixture model withoptimized parameter ?
using the training data of articles 1-270 and test data ofarticles 271-300 of length at most 40 words.5 According to Thematic Role theory, this modifier has a PATIENT role.76 D. Xiong et alTable 2.
Results for incorporating different semantic knowledge sources.
The baselineparser is described in Sect.
2. in detail.Baseline HN1 HN2 HN3 CL1 CL2 CL3F1(%) 78.5 78.6 79.1 78.9 77.5 78.7 78.8Table 3.
Results for combinations of different semantic knowledge sourcesBaseline HN2+CL1+HR HN2+CL2+HR HN2+CL3+HRF1(%) 78.5 79.2 79.4 79.3Firstly, we carry out experiments on HowNet and CiLin, separately.
Exper-imental results are presented in Table 2.
As can be seen, CiLin has a greatercoverage of words than that of HowNet, however, it works worse than HowNet.And at the level CL1, coarse-grained classes even yield degraded results.
It?s dif-ficult to explain this, but the main reason may be that HowNet has a fine-grainedand substantial ontology while CiLin is designed only as a synset container.Since HowNet has a better semantic representation and CiLin better cov-erage, we want to combine them.
The combination is described in Sect.
3.1,where HN2 is used as the primary semantic category set.
Words found by CiLinand heuristic rules are labelled by semantic categories from HN2.
Results areshown in Table 3.
Although external sources HN2+CL1/2/3+HR have the iden-tical word coverage and yield exactly the same number of classes, the differentword-class distributions in them lead to the different results.Due to the combination of HN2, CL2 and HR, we see that our new parserwith external semantic knowledge outperforms the baseline parser by 0.9% inF1 measure.
Given we are already at the 78% level of accuracy, an improve-ment of 0.9% is well worth obtaining and confirms the importance of semanticdependencies on parsing.
Further, we do the significance test using Bikel?s sig-nificance tester6 which is modified to output p-value for F1.
The significancelevel for F-score is at most (43376+1)/(1048576+1) = 0.041.
A second 1048576iteration produces the similar result.
Therefore the improvement is statisticallysignificant.3.4 Performance Improvement AnalysisWe manually analyze parsing errors of the baseline parser (BP ) as well as per-formance improvement of the new parser (IP ) with semantic knowledge fromthe combination of HN2, CL2 and HR.
Improvement analysis can provide anadditional valuable perspective: how semantic knowledge helps to resolve someambiguities.
We compare BP and IP on the test data parse by parse.
There are299 sentences of length at most 40 words among the total 348 test sentences.
Thetwo parsers BP and IP found different parses for 102 sentences, among which6 See http://www.cis.upenn.edu/ dbikel/software.htmlParsing the Penn Chinese Treebank with Semantic Knowledge 77Table 4.
Frequency of parsing improvement types.
AR represents ambiguity resolution.Type Count Percent(%)Nominal Compound AR 19 38Coordination AR 9 18NV AR in NV+noun 6 12Other AR 16 32IP yields better parse trees for 47 sentences according to the gold standard trees.We have concentrated on these 47 sentences and compared parse trees found byIP with those found by BP .
Frequencies of major types of parsing improvementis presented in Table 4.
Levy and Manning (see [12])(henceforth L&M) observedthe top three parsing error types: NP-NP modification, Coordination and NVmistagging, which are also common in our baseline parser.
As can be seen, ourimproved parser can address these types of ambiguities to some extent throughsemantic knowledge.Nominal Compounds (NCs) Disambiguation: Nominal compounds are no-torious ?every way ambiguous?
constructions.7 The different semantic interpre-tations have different dependency structures.
According to L&M, this ambiguitywill be addressed by the dependency model when word frequencies are largeenough to be reliable.
However, even for the treebank central to a certain topic,many very plausible dependencies occur only once.8 A good technique for re-solving this conflict is to generalize the dependencies from word pairs to word-class pairs.
Such generalized dependencies, as noted in section 3.2, can capturesemantic preferences, as well as alleviate the data sparseness associated withstandard bilexical statistics.
In our class-based selection preference model, if thefrequency of pair [CMhw, Hhw]9 is large enough, the parser can interpret nominalcompounds correctly, that is, it can tell which modify which.NCs are always parsed as flatter structures by our baseline parser, just likethe tree a. in Figure 1.
This is partly because of the annotation style of CTB,where there is no NP-internal structure.
For these NCs without internal analysis,we re-annotated them as basic NPs with label NPB, as mentioned in section 2.This re-annotation really helps.
Another reason is that the baseline parser, orthe modifier word generating sub-model PMw , can not capture hierarchical se-mantic dependencies of internal structures of NCs due to the sparseness of bilex-ical dependencies.
In our new parser, however, the selection preference model isable to build semantically preferable structures through word-class dependencystatistics.
For NCs like (n1, n2, n3), where ni is a noun, dependency structures7 ?Every way ambiguous?
constructions are those for which the number of analy-ses is the number of binary trees over the terminal elements.
Prepositional phraseattachment, coordination, and nominal compounds are all ?every way ambiguous?constructions.8 Just as Klein et al (see [8]) said, one million words of training data just isn?t enough.9 Henceforth, [s1, s2] denotes a dependency structure, where s1 is a modifier word orits semantic class (C), and s2 is the head word.78 D. Xiong et ala.
NPBNR??NN??NN??b.
NPNPNPBNR??NPBNN??NPBNN??Fig.
1.
Nominal Compounds: The North Korean government?s special envoy.
a. is theincorrect flat parse, b. is the right one in corpus{[Cn1 , n2], [Cn1 , n3], [Cn2 , n3]} will be checked in terms of semantic acceptabilityand semantically preferable structures will be built finally.
For more complicatedNCs, similar analysis follows.In our example (see Fig.
1.
), the counts of word dependencies [?
?/NorthKorea, ?
?/government] and [?
?/North Korea,?
?/special envoy] in thetraining data both are 0.
Therefore, it is impossible for the baseline parser tohave a preference between these two dependency structures.
On the other hand,the counts of word-class dependencies [???,?
?/government], where ??
?is the semantic category of??
in HN2, is much larger than the counts of [???,?
?/special envoy] and [??,?
?/special envoy], where??
is the semanticcategory of ??
in the training data.
Therefore, the dependency structure of[?
?/North Korea, ?
?/government] will be built.Coordination Disambiguation: Coordination is another kind of ?every wayambiguous?
construction.
For coordination structures, the head word is meaning-less.
But that doesn?t matter, since semantic dependency between the spurioushead and modifier will be used to measure the meaning similarity of coordinatedstructures.
Therefore, our selection preference model still works in coordinationconstructions.
We have also found VP coordination ambiguity, which is similarto that observed by L&M.
The latter VP in coordinated VPs is often parsed asan IP due to pro-drop by the baseline parser.
That is, the coordinated structureVP is parsed as: V P 0 ?
V P 1IP 2.
This parse will be penalized by the selectionpreference model because the hypothesis that the head word of IP 2 has a sim-ilar meaning to the head word of V P 1 under the grammatical relation V P 0 isinfrequent.NV-ambiguous Tagging Disambiguation: The lack of overt morphologicalmarking for transforming verbal words to nominal words in Chinese results inambiguity between these two categories.
L&M argued that the way to resolvethis ambiguity is to look at more external context, like some function words,e.g.
adverbial or prenominal modifiers, co-occurring with NV-ambiguous words.However, in some cases, NV-ambiguous words can be tagged correctly withoutexternal context.
Chen et al (see [16]) studied the pattern of NV+noun, whichwill be analyzed as a predicate-object structure if NV is a verb and a modifier-noun structure if NV is a noun.
They found that in most cases, this pattern canParsing the Penn Chinese Treebank with Semantic Knowledge 79a.
VPVPBVV??NPBNN??b.
NPBNN??NN??Fig.
2.
NV-ambiguity: a. implement plans (incorrect parse) versus b. implementationplans (corpus)Table 5.
Previous Results on CTB parsing for sentences of length at most 40 wordsLP LR F1Bikel and Chiang 2000 77.2 76.2 76.7Levy and Manning 2003 78.4 79.2 78.8Present work 80.1 78.7 79.4Bikel Thesis 2004 81.2 78.0 79.6Chiang and Bikel 2002 81.1 78.8 79.9be parsed correctly without any external context.
Furthermore, they argued thatsemantic preferences are helpful for the resolution of ambiguity between thesetwo different structures.
In our selection preference model, semantic preferencesinterweave with grammatical relations.
These semantic dependencies impose con-straints on the structure of the pattern NV+noun and therefore on the POStag of NV.
Figure 2 shows our new parser can correct NV mistagging errorsoccurring in the pattern of NV+noun.Smoothing:Besides the three ambiguity resolution noted above, semantic knowl-edge indeed helps alleviate the fundamental sparseness of the lexical dependencyinformation available in the CTB.
For many word pairs [mod,head], whose countinformation is not available in the training data, the dependency statistics of headand modifier can still work through the semantic category of mod.
During our man-ual analysis of performance improvement, many other structural ambiguities areaddressed due to the smoothing function of semantic knowledge.4 Related Work on CTB ParsingPrevious work on CTB parsing and their results are shown in table 5.
Bikel andChiang (see [14]) used two different models on CTB, one based on the modi-fied BBN model which is very similar to our baseline model, the other on TreeInsertion Grammar (TIG).
While our baseline model used the same unknownword threshold with Bikel and Chiang but smaller beam width, our result out-performs theirs due to other features like distance, basic NP re-annotation usedby our baseline model.
Levy and Manning (see [12]) used a factored model withrich re-annotations guided by error analysis.
In the baseline model, we also usedseveral re-annotations but find most re-annotations they suggested do not fit80 D. Xiong et alour model.
The three parsing error types expounded above are also found byL&M.
However, we used more efficient measures to keep our improved modelfrom these errors.The work of Bikel thesis (see [10]) emulated Collins?
model and created alanguage package to Chinese parsing.
He used subcat frames and an additionalPOS tagger for unseen words.
Chiang and Bikel (see [17]) used the EM algorithmon the same TIG-parser to improve the head percolation table for Chinese pars-ing.
Both these two parsers used fine-tuned features recovered from the treebankthat our model does not use.
This leads to better results and indicates that thereis still room of improvement for our model.5 ConclusionsWe have shown that how semantic knowledge may be incorporated into a gener-ative model for full parsing, which reaches 79.4% in CTB.
Experimental resultsare quite consistent with our intuition.
After the manual analysis of performanceimprovement, the working mechanism of semantic knowledge in the selectionpreference model is quite clear:1.
Using semantic categories extracted from external dictionaries, the class-based selection preference model first generalizes standard bilexical depen-dencies, some of which are not available in training data, to word-class de-pendencies.
These dependencies are neither too fine-grained nor too coarse-grained compared with bilexical and biclass dependencies, and really help toalleviate fundamental information sparseness in treebank.2.
Based on the generalized word-class pairs, semantic dependencies are cap-tured and used to address different kinds of ambiguities, like nominal com-pounds, coordination construction, even NV-ambiguous words tagging.Our experiments show that generative models have room for improvementby employing semantic knowledge.
And that may be also true for discrimina-tive models, since these models can easily incorporate richer features in a well-founded fashion.
This is the subject of our future work.AcknowledgementsThis work was supported in part byNational HighTechnologyResearch andDevel-opment Program under grant #2001AA114010 and #2003AA111010.
We wouldlike to acknowledge anonymous reviewers who provided helpful suggestions.References1.
Michael Collins.
1999.
Head-Driven Statistical Models for Natural Language Pars-ing.
PhD thesis, University of Pennsylvania.2.
Philip Stuart Resnik.
1993.
Selection and Information: A Class-Based Approach toLexical Relationships.
PhD thesis, University of Pennsylvania, Philadelphia, PA,USA.Parsing the Penn Chinese Treebank with Semantic Knowledge 813.
Sanda Harabagiu.
1996.
An Application of WordNet to Prepositional Attachement.In Proceedings of ACL-96, June 1996, Santa Cruz CA, pages 360-363.4.
Yuval Krymolowski and Dan Roth.
1998.
Incorporating Knowledge in Natural Lan-guage Learning: A Case Study.
In COLING-ACL?98 Workshop on Usage of Word-Net in Natural Language Processing Systems,Montreal, Canada.5.
Mark McLauchlan.
2004.
Thesauruses for Prepositional Phrase Attachment.
InProceedings of CoNLL-2004,Boston, MA, USA, 2004, pp.
73-80.6.
Fei Xia.
1999.
Automatic Grammar Generation from Two Different Perspectives.PhD thesis, University of Pennsylvania.7.
Dan Klein and Christopher D. Manning.
2002.
Fast Exact Natural Language Pars-ing with a Factored Model.
In Advances in Neural Information Processing Systems15 (NIPS-2002).8.
Dan Klein and Christopher D. Manning.
2003.
Accurate Unlexicalized Parsing.
InProceedings of ACL-03.9.
Daniel Gildea.
2001.
Corpus variation and parser performance.
In Proceedings ofEMNLP-01, Pittsburgh, Pennsylvania.10.
Daniel M. Bikel.
2004a.
On the Parameter Space of Generative Lexicalized Statis-tical Parsing Models.
PhD thesis, University of Pennsylvania.11.
Nianwen Xue and Fei Xia.
2000.
The Bracketing Guidelines for Chinese TreebankProject.
Technical Report IRCS 00-08, University of Pennsylvania.12.
Roger Levy and Christopher Manning.
2003.
Is it harder to parse Chinese, or theChinese Treebank?
In Proceedings of ACL-03.13.
Deyi Xiong, Qun Liu and Shouxun Lin.
2005.
Lexicalized Beam Thresholding Pars-ing with Prior and Boundary Estimates.
In Proceedings of the 6th Conference onIntelligent Text Processing and Computational Linguistics (CICLing), Mexico City,Mexico, 2005.14.
Daniel M. Bikel and David Chiang.
2000.
Two statistical parsing models appliedto the chinese treebank.
In Proceedings of the Second Chinese Language ProcessingWorkshop, pages 1-6.15.
Daniel M. Bikel.
2004b.
Intricacies of Collins?
Parsing Model.
to appear in Com-putational Linguistics.16.
Kejian Chen and Weimei Hong.
1996.
Resolving Ambiguities of Predicate-objectand Modifier-noun Structures for Chinese V-N Patterns.
in Chinese.
In Communi-cation of COLIPS, Vol.6, #2, pages 73-79.17.
David Chiang and Daniel M. Bikel.
2002.
Recovering Latent Information in Tree-banks.
In proceedings of COLING,2002.
