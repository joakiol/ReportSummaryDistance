Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 123?128,Dublin, Ireland, August 23-24, 2014.Biocom Usp: Tweet Sentiment Analysis with Adaptive BoostingEnsembleNa?dia F. F. Silva, Eduardo R. HruschkaUniversity of S?ao Paulo, USPS?ao Carlos, SP, Brazilnadia, erh@icmc.usp.brEstevam Rafael Hruschka Jr.Department of Computer ScienceFederal University of Sao Carlos.S?ao Carlos, SP, Brazilestevam@dc.ufscar.brAbstractWe describe our approach for theSemEval-2014 task 9: Sentiment Analy-sis in Twitter.
We make use of an en-semble learning method for sentimentclassification of tweets that relies onvaried features such as feature hash-ing, part-of-speech, and lexical fea-tures.
Our system was evaluated inthe Twitter message-level task.1 IntroductionThe sentiment analysis is a field of study thatinvestigates feelings present in texts.
Thisfield of study has become important, espe-cially due to the internet growth, the contentgenerated by its users, and the emergence ofthe social networks.
In the social networkssuch as Twitter people post their opinions in acolloquial and compact language, and it is be-coming a large dataset, which can be used asa source of information for various automatictools of sentiment inference.
There is an enor-mous interest in sentiment analysis of Twit-ter messages, known as tweets, with applica-tions in several segments, such as (i) directingmarketing campaigns, extracting consumer re-views of services and products (Jansen et al.,2009); (ii) identifying manifestations of bully-ing (Xu et al., 2012); (iii) predicting to fore-cast box-office revenues for movies (Asur andHuberman, 2010); and (iv) predicting accep-tance or rejection of presidential candidates(Diakopoulos and Shamma, 2010; O?Connoret al., 2010).This work is licensed under a CreativeCommons Attribution 4.0 International Li-cence.
Page numbers and proceedings footerare added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/One of the problems encountered by re-searchers in tweet sentiment analysis is thescarcity of public datasets.
Although Twit-ter sentiment datasets have already been cre-ated, they are either small ?
such as Obama-McCain Debate corpus (Shamma et al., 2009)and Health Care Reform corpus (Speriosu etal., 2011) or big and proprietary such as in(Lin and Kolcz, 2012).
Others rely on noisylabels obtained from emoticons and hashtags(Go et al., 2009).
The SemEval-2014 task 9: Sen-timent Analysis in Twitter (Nakov et al., 2013)provides a public dataset to be used to com-pare the accuracy of different approaches.In this paper, we propose to analyse tweetsentiment with the use of Adaptive Boost-ing (Freund and Schapire, 1997), makinguse of the well-known Multinomial Classi-fier.
Boosting is an approach to machinelearning that is based on the idea of creat-ing a highly accurate prediction rule by com-bining many relatively weak and inaccuraterules.
The AdaBoost algorithm (Freund andSchapire, 1997) was the first practical boost-ing algorithm, and remains one of the mostwidely used and studied, with applications innumerous fields.
Therefore, it has potential tobe very useful for tweet sentiment analysis, aswe address in this paper.2 Related WorkClassifier ensembles for tweet sentiment anal-ysis have been underexplored in the literature?
a few exceptions are (Lin and Kolcz, 2012;Clark and Wicentwoski, 2013; Rodriguez etal., 2013; Hassan et al., 2013).Lin and Kolcz (2012) used logistic regres-sion classifiers learned from hashed byte 4-grams as features ?
The feature extractor con-siders the tweet as a raw byte array.
It movesa four-byte sliding window along the array,123and hashes the contents of the bytes, the valueof which was taken as the feature id.
Here the4-grams refers to four characters (and not tofour words).
They made no attempt to per-form any linguistic processing, not even wordtokenization.
For each of the (proprietary)datasets, they experimented with ensemblesof different sizes.
The ensembles were formedby different models, obtained from differenttraining sets, but with the same learning algo-rithm (logistic regression).
Their results showthat the ensembles lead to more accurate clas-sifiers.Rodr?
?gues et al.
(2013) and Clark et al.
(2013) proposed the use of classifier ensem-bles at the expression-level, which is relatedto Contextual Polarity Disambiguation.
In thisperspective, the sentiment label (positive,negative, or neutral) is applied to a specificphrase or word within the tweet and does notnecessarily match the sentiment of the entiretweet.Finally, another type of ensemble frame-work has been recently proposed by Hassanet al.
(2013), who deal with class imbalance,sparsity, and representational issues.
The au-thors propose to enrich the corpus using mul-tiple additional datasets related to the task ofsentiment classification.
Differently from pre-vious works, the authors use a combination ofunigrams and bigrams of simple words, part-of-speech, and semantic features.None of the previous works used AdaBoost(Freund and Schapire, 1996).
Also, lexiconsand/or part-of-speech in combination withfeature hashing, like in (Lin and Kolcz, 2012)have not been addressed in the literature.3 AdaBoost EnsembleBoosting is a relatively young, yet extremelypowerful, machine learning technique.
Themain idea behind boosting algorithms is tocombine multiple weak learners ?
classifi-cation algorithms that perform only slightlybetter than random guessing ?
into a power-ful composite classifier.
Our focus is on thewell known AdaBoost algorithm (Freund andSchapire, 1997) based on Multinomial NaiveBayes as base classifiers (Figure 1).AdaBoost and its variants have been ap-plied to diverse domains with great success,owing to their solid theoretical foundation,accurate prediction, and great simplicity (Fre-und and Schapire, 1997).
For example, Violaand Jones (2001) used AdaBoost to face de-tection, Hao and Luo (2006) dealt with im-age segmentation, recognition of handwrittendigits, and outdoor scene classification prob-lems.
In (Bloehdorn and Hotho, 2004) textclassification is explored.Figure 1: AdaBoost Approach4 Feature EngineeringThe most commonly used text representationmethod adopted in the literature is known asBag of Words (BOW) technique, where a doc-ument is considered as a BOW, and is repre-sented by a feature vector containing all thewords appearing in the corpus.
In spite ofBOW being simple and very effective in textclassification, a large amount of informationfrom the original document is not considered,word order is ruptured, and syntactic struc-tures are broken.
Therefore, sophisticated fea-ture extraction methods with a deeper under-standing of the documents are required forsentiment classification tasks.
Instead of us-ing only BOW, alternative ways to representtext, including Part of Speech (PoS) based fea-tures, feature hashing, and lexicons have beenaddressed in the literature.We implemented an ensemble of classifiersthat receive as input data a combination ofthree features sets: i) lexicon features that cap-tures the semantic aspect of a tweet; ii) fea-ture hashing that captures the surface-form asabbreviations, slang terms from this type ofsocial network, elongated words (for exam-ple, loveeeee), sentences with words withouta space between them (for instance, Ilovveap-ple!
), and so on; iii) and a specific syntactic fea-tures for tweets.
Technical details of each fea-ture set are provided in the sequel.Lexicon FeaturesWe use the sentimental lexicon provided by(Thelwall et al., 2010) and (Hu and Liu, 2004).The former is known as SentiStrength and124provides: an emotion vocabulary, an emoti-cons list (with positive, negative, and neutralicons), a negation list, and a booster word list.We use the negative list in cases where thenext term in a sentence is an opinion word(either positive or negative).
In such caseswe have polarity inversion.
For example, inthe sentence ?The house is not beautiful?, thenegative word ?not?
invert the polarity of theopinion word beautiful.
The booster word listis composed by adverbs that suggest more orless emphasis in the sentiment.
For exam-ple, in the sentence ?He was incredibly rude.
?the term ?incredibly?
is an adverb that lay em-phasis on the opinion word ?rude?.
Besidesusing SentiStrength, we use the lexicon ap-proach proposed by (Hu and Liu, 2004).
Intheir approach, a list of words and associa-tions with positive and negative sentimentshas been provided that are very useful forsentiment analysis.These two lexicons were used to build thefirst feature set according to Table 1, where itis presented an example of tweet representa-tion for the tweet1: ?The soccer team didn?tplay extremely bad last Wednesday.?
Theword ?bad?
exists in the lexicon list of (Huand Liu, 2004), and it is a negative word.The word ?bad?
also exists in the negationlist provided by (Thelwall et al., 2010).
Theterm ?didn?t?
is a negative word according toSentiStrength (Thelwall et al., 2010) and thereis a polarity inversion of the opinion wordsahead.
Finally, the term ?extremely?
belongsthe booster word list and this word suggestsmore emphasis to the opinion words existingahead.positive negative neutral classtweet13 0 0 positiveTable 1: Representing Twitter messages withlexicons.Feature hashingFeature hashing has been introduced for textclassification in (Shi et al., 2009), (Wein-berger et al., 2009), (Forman and Kirshen-baum, 2008), (Langford et al., 2007), (Carageaet al., 2011).
In the context of tweet classi-fication, feature hashing offers an approachto reducing the number of features providedas input to a learning algorithm.
The origi-nal high-dimensional space is ?reduced?
byhashing the features into a lower-dimensionalspace, i.e., mapping features to hash keys.Thus, multiple features can be mapped to thesame hash key, thereby ?aggregating?
theircounts.We used the MurmurHash3 function(SMHasher, 2010), that is a non-cryptographichash function suitable for general hash-basedlookup tables.
It has been used for manypurposes, and a recent approach that hasemerged is its use for feature hashing orhashing trick.
Instead of building and storingan explicit traditional bag-of-words withn-grams, the feature hashing uses a hashfunction to reduce the dimensionality of theoutput space and the length of this space(features) is explicitly fixed in advance.
Forthis paper, we used this code (in Python):Code Listing 1: Murmurhash:from sklearn.utils.murmurhashimport murmurhash3_bytes_u32for w in "i loveee apple".split():print("{0} => {1}".format(w,murmurhash3_bytes_u32(w,0)%2**10))The dimensionality is 2 ?
?10, i.e 210fea-tures.
In this code the output is a hash codefor each word ?w?
in the phrase ?i loveeeapple?, i.e.
i => 43, loveee => 381 andapple => 144.
Table 2 shows an example offeature hashing representation.1 2 3 4 ?
?
?
1024 classtweet10 0 1 1 ?
?
?
0 positivetweet20 1 0 3 ?
?
?
0 negativetweet32 0 0 0 ?
?
?
0 positive............... ?
?
?......tweetn0 0 2 1 ?
?
?
0 neutralTable 2: Representing Twitter messages withfeature hashing.Specific syntactic (PoS) featuresWe used the Part of Speech (PoS) tagged fortweets with the Twitter NLP tool (Gimpel etal., 2011).
It encompasses 25 tags includingNominal, Nominal plus Verbal, Other open-class words like adjectives, adverbs and in-terjection, Twitter specific tags such as hash-tags, mention, discourse marker, just to name125a few.
Table 3 shows an example of syntacticfeatures representation.tag1tag2tag3tag4?
?
?
tag25classtweet10 0 3 1 ?
?
?
0 positivetweet20 2 0 1 ?
?
?
0 negativetweet31 0 0 0 ?
?
?
0 positive............... ?
?
?......tweetn0 0 1 1 ?
?
?
0 neutralTable 3: Representing Twitter messages withsyntactic features.A combination of lexicons, feature hashing,and part-of-speech is used to train the ensem-ble classifiers, thereby resulting in 1024 fea-tures from feature hashing, 3 features fromlexicons, and 25 features from PoS.5 Experimental Setup and ResultsWe conducted experiments by using theWEKA platform1.
Table 4 shows the class dis-tributions in training, development, and test-ing sets.
Table 5 presents the results for posi-tive and negative classes with the classifiersused in training set, and Table 6 shows thecomputed results by SemEval organizers inthe test sets.Training SetSet Positive Negative Neutral TotalTrain 3,640 (37%) 1,458 (15%) 4,586 (48%) 9,684Development SetSet Positive Negative Neutral TotalDev 575 (35%) 340(20%) 739 (45%) 1,654Testing SetsSet Positive Negative Neutral TotalLiveJournal 427 (37%) 304 (27%) 411 (36%) 1,142SMS2013 492 (23%) 394(19%) 1,207 (58%) 2,093Twitter2013 1,572 (41%) 601 (16%) 1,640 (43%) 3,813Twitter2014 982 (53%) 202 (11%) 669 (36%) 1,853Twitter2014Sar 33 (38%) 40 (47%) 13 (15%) 86Table 4: Class distributions in the training set(Train), development set (Dev) and testing set(Test).6 Concluding RemarksFrom our results, we conclude that the use ofAdaBoost provides good performance in thesentiment analysis (message-level subtask).In the cross-validation process, MultinomialNaive Bayes (MNB) has shown better resultsthan Support Vector Machines (SVM) as acomponent for AdaBoost.
However, we feel1http://www.cs.waikato.ac.nz/ml/weka/Set Algorithm F-MeasurePositiveF-MeasureNegativeAverageTrain MNB 63.40 49.40 56.40Train SVM 64.00 44.50 54.20Train AdaBoost w/ SVM 62.50 44.50 53.50Train AdaBoost w/ MNB 65.10 49.60 57.35Table 5: Results from 10-fold cross validationin the training set with default parameters ofWeka.
MNB and SVM stand for MultinomialNaive Bayes and Support Vector Machine, re-spectively.Scoring LiveJournal2014class precision recall F-measurepositive 69.79 64.92 67.27negative 76.64 61.64 68.33neutral 51.82 69.84 59.50overall score : 67.80Scoring SMS2013positive 61.99 46.78 53.32negative 72.34 42.86 53.82neutral 53.85 83.76 65.56overall score : 53.57Scoring Twitter2013positive 68.07 66.13 67.08negative 48.09 50.00 49.02neutral 67.20 68.15 67.67overall score : 58.05Scoring Twitter2014positive 65.17 70.48 67.72negative 53.47 48.21 50.70neutral 59.94 55.62 57.70overall score : 59.21Scoring Twitter2014Sarcasmpositive 63.64 44.68 52.50negative 22.50 75.00, 34.62neutral 76.92 37.04 50.00overall score : 43.56Table 6: Results in the test sets ?
AdaBoostplus Multinomial Naive Bayes, which was thebest algorithm in cross validation.that further investigations are necessary be-fore making strong claims about this result.Overall, the SemEval Tasks have make evi-dent the usual challenges when mining opin-ions from Social Media channels: noisy text,irregular grammar and orthography, highlyspecific lingo, and others.
Moreover, tempo-ral dependencies can affect the performance ifthe training and test data have been gatheredat different.AcknowledgementsThe authors would like to thank the Re-search Agencies CAPES, FAPESP, and CNPqfor their financial support.ReferencesSitaram Asur and Bernardo A. Huberman.
2010.Predicting the future with social media.
In Pro-ceedings of the 2010 International Conference on126Web Intelligence and Intelligent Agent Technology- Volume 01, WI-IAT ?10, pages 492?499, Wash-ington, DC, USA.
IEEE Computer Society.Stephan Bloehdorn and Andreas Hotho.
2004.Text classification by boosting weak learnersbased on terms and concepts.
In Proceedings ofthe Fourth IEEE International Conference on DataMining, pages 331?334.
IEEE Computer SocietyPress, November.Cornelia Caragea, Adrian Silvescu, and Prasen-jit Mitra.
2011.
Protein sequence classifica-tion using feature hashing.
In Fang-Xiang Wu,Mohammed Javeed Zaki, Shinichi Morishita,Yi Pan, Stephen Wong, Anastasia Christianson,and Xiaohua Hu, editors, BIBM, pages 538?543.IEEE.Sam Clark and Rich Wicentwoski.
2013.
Swatcs:Combining simple classifiers with estimatedaccuracy.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 2:Proceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013), pages425?429, Atlanta, Georgia, USA, June.Nicholas A. Diakopoulos and David A. Shamma.2010.
Characterizing debate performance viaaggregated twitter sentiment.
In Proceedings ofthe SIGCHI Conference on Human Factors in Com-puting Systems, CHI ?10, pages 1195?1198, NewYork, NY, USA.
ACM.George Forman and Evan Kirshenbaum.
2008.Extremely fast text feature extraction for clas-sification and indexing.
In CIKM ?08: Proceed-ing of the 17th ACM conference on Information andknowledge management, pages 1221?1230, NewYork, NY, USA.
ACM.Yoav Freund and Robert E. Schapire.
1996.
Ex-periments with a new boosting algorithm.
InThirteenth International Conference on MachineLearning, pages 148?156, San Francisco.
MorganKaufmann.Yoav Freund and Robert E Schapire.
1997.A decision-theoretic generalization of on-linelearning and an application to boosting.
Jour-nal of Computer and System Sciences, 55(1):119 ?139.Kevin Gimpel, Nathan Schneider, BrendanO?Connor, Dipanjan Das, Daniel Mills, JacobEisenstein, Michael Heilman, Dani Yogatama,Jeffrey Flanigan, and Noah A. Smith.
2011.Part-of-speech tagging for twitter: Annotation,features, and experiments.
In Proceedings ofthe 49th Annual Meeting of the Association forComputational Linguistics ?
Short Papers - Volume2, HLT ?11, pages 42?47, Stroudsburg, PA, USA.Alec Go, Richa Bhayani, and Lei Huang.
2009.Twitter sentiment classification using distantsupervision.
Processing, pages 1?6.Wei Hao and Jiebo Luo.
2006.
GeneralizedMulticlass AdaBoost and Its Applications toMultimedia Classification.
In Computer Visionand Pattern Recognition Workshop, 2006.
CVPRW&#039;06.
Conference on, page 113, Washington,DC, USA, June.
IEEE.Ammar Hassan, Ahmed Abbasi, and DanielZeng.
2013.
Twitter sentiment analysis: Abootstrap ensemble framework.
In SocialCom,pages 357?364.
IEEE.Minqing Hu and Bing Liu.
2004.
Mining andsummarizing customer reviews.
In Proceed-ings of the tenth ACM SIGKDD international con-ference on Knowledge discovery and data mining,KDD ?04, pages 168?177, New York, NY, USA.ACM.Bernard J. Jansen, Mimi Zhang, Kate Sobel, andAbdur Chowdury.
2009.
Twitter power:Tweets as electronic word of mouth.
J.
Am.
Soc.Inf.
Sci.
Technol., 60(11):2169?2188, nov.John Langford, Alex Strehl, and Lihong Li.
2007.Vowpal wabbit online learning project.
http://mloss.org/software/view/53/.Jimmy Lin and Alek Kolcz.
2012.
Large-scale ma-chine learning at twitter.
In Proceedings of the2012 ACM SIGMOD International Conference onManagement of Data, SIGMOD ?12, pages 793?804, New York, NY, USA.
ACM.Preslav Nakov, Sara Rosenthal, ZornitsaKozareva, Veselin Stoyanov, Alan Ritter,and Theresa Wilson.
2013.
Semeval-2013 task2: Sentiment analysis in twitter.
In SecondJoint Conference on Lexical and ComputationalSemantics (*SEM), Volume 2: Proceedings of theSeventh International Workshop on Semantic Eval-uation (SemEval 2013), pages 312?320, Atlanta,Georgia, USA, June.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In ICWSM?10, pages1?1.Penagos Carlos Rodriguez, Jordi Atserias, JoanCodina-Filba, David Garc?a-Narbona, JensGrivolla, Patrik Lambert, and Roser Saur?.2013.
Fbm: Combining lexicon-based ml andheuristics for social media polarities.
In Pro-ceedings of SemEval-2013 ?
International Work-shop on Semantic Evaluation Co-located with *Semand NAACL, Atlanta, Georgia.
Url date at 2013-10-10.David A. Shamma, Lyndon Kennedy, and Eliz-abeth F. Churchill.
2009.
Tweet the debates:Understanding community annotation of un-collected sources.
In In WSM ?09: Proceedingsof the international workshop on Workshop on So-cial.127Qinfeng Shi, James Petterson, Gideon Dror,John Langford, Alex Smola, and S.V.N.
Vish-wanathan.
2009.
Hash kernels for structureddata.
J. Mach.
Learn.
Res., 10:2615?2637.SMHasher.
2010.
The murmurhash family ofhash functions.Michael Speriosu, Nikita Sudan, Sid Upadhyay,and Jason Baldridge.
2011.
Twitter polarityclassification with label propagation over lexi-cal links and the follower graph.
In Proceedingsof the First Workshop on Unsupervised Learning inNLP, pages 53?63, Stroudsburg, PA, USA.Mike Thelwall, Kevan Buckley, Georgios Pal-toglou, Di Cai, and Arvid Kappas.
2010.
Senti-ment in short strength detection informal text.J.
Am.
Soc.
Inf.
Sci.
Technol., 61(12):2544?2558,December.Paul Viola and Michael Jones.
2001.
Robust real-time object detection.
In International Journal ofComputer Vision.Kilian Q. Weinberger, Anirban Dasgupta, JohnLangford, Alexander J. Smola, and Josh Atten-berg.
2009.
Feature hashing for large scale mul-titask learning.
In Andrea Pohoreckyj Dany-luk, L Bottou, and Michael L. Littman, editors,ICML, volume 382 of ACM International Confer-ence Proceeding Series, page 140.
ACM.Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu, andAmy Bellmore.
2012.
Learning from bullyingtraces in social media.
In HLT-NAACL, pages656?666.128
