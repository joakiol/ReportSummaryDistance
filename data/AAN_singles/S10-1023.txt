Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 112?116,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsFCC: Modeling Probabilities with GIZA++ for Task #2 and #3 ofSemEval-2Darnes Vilarin?o, Carlos Balderas, David Pinto, Miguel Rodr?
?guez, Saul Leo?nFaculty of Computer Science, BUAPPuebla, Mexico{darnes,mrodriguez,dpinto}@cs.buap.mxAbstractIn this paper we present a na?
?ve approachto tackle the problem of cross-lingualWSD and cross-lingual lexical substitu-tion which correspond to the Task #2 and#3 of the SemEval-2 competition.
We useda bilingual statistical dictionary, which iscalculated with Giza++ by using the EU-ROPARL parallel corpus, in order to cal-culate the probability of a source word tobe translated to a target word (which is as-sumed to be the correct sense of the sourceword but in a different language).
Two ver-sions of the probabilistic model are tested:unweighted and weighted.
The obtainedvalues show that the unweighted versionperforms better thant the weighted one.1 IntroductionWord Sense Disambiguation (WSD) is con-sidered one of the most important prob-lems in Natural Language Processing(Agirre and Edmonds, 2006).
It is claimedthat WSD is essential for those applications thatrequire of language comprehension modulessuch as search engines, machine translationsystems, automatic answer machines, second lifeagents, etc.
Moreover, with the huge amountsof information in Internet and the fact that thisinformation is continuosly growing in differentlanguages, we are encourage to deal with cross-lingual scenarios where WSD systems are alsoneeded.
Despite the WSD task has been studiedfor a long time, the expected feeling is that WSDshould be integrated into real applications such asmono and multi-lingual search engines, machinetranslation systems, automatic answer machines,etc (Agirre and Edmonds, 2006).
Different stud-ies on this issue have demonstrated that thoseapplications benefit from WSD, such as in thecase of machine translation (Chan et al, 2007;Carpuat and Wu., 2007).
On the other hand,Lexical Substitution (LS) refers to the processof finding a substitute word for a source wordin a given sentence.
The LS task needs to beapproached by firstly disambiguating the sourceword, therefore, these two tasks (WSD and LS)are somehow related.Since we are describing the modules of oursystem, we did not provide information of thedatasets used.
For details about the corpora,see the task description paper for both tasks (#2and #3) in this volume (Mihalcea et al, 2010;Lefever and Hoste, 2010).
Description about theother teams are also described in the same papers.2 A Na?
?ve Approach to WSD and LSIn this section it is presented an overview of thepresented system, but also we further discuss theparticularities of the general approach for eachtask evaluated.
We will start this section byexplaining the manner we deal with the Cross-Lingual Word Sense Disambiguation (C-WSD)problem.2.1 Cross-Lingual Word SenseDisambiguationWe have approached the cross-lingual word sensedisambiguation task by means of a probabilisticsystem which considers the probability of a wordsense (in a target language), given a sentence (in asource language) containing the ambiguous word.In particular, we used the Naive Bayes classifierin two different ways.
First, we calculated theprobability of each word in the source languageof being associated/translated to the correspondingword (in the target language).
The probabilitieswere estimated by means of a bilingual statisticaldictionary which is calculated using the Giza++system over the EUROPARL parallel corpus.
Wefiltered this corpus by selecting only those sen-112tences which included some senses of the ambigu-ous word which were obtained by translating thisambiguous word on the Google search engine.In Figure 1 we may see the complete process forapproaching the problem of cross-lingual WSD.The second approach considered a weightedprobability for each word in the source sentence.The closer a word of the sentence to the ambigu-ous word, the higher the weight given to it.In other words, given an English sentence S ={w1, w2, ?
?
?
, wk, ?
?
?
, wk+1, ?
?
? }
with the am-biguous word wkin position k. Let us considerN candidate translations of wk, {tk1, tk2, ?
?
?
, tkN}obtained somehow (we will further discuss aboutthis issue in this section).
We are insterested onfinding the most probable candidate translationsfor the polysemous word wk.
Therefore, we mayuse a Na?
?ve Bayes classifier which considers theprobability of tkigiven wk.
A formal descriptionof the classifier is given as follows.p(tki|S) = p(tki|w1, w2, ?
?
?
, wk, ?
?
? )
(1)p(tki|S) =p(tki)p(w1, w2, ?
?
?
, wk, ?
?
?
|tki)p(w1, w2, ?
?
?
, wk, ?
?
?
)(2)We are interested on finding the argument thatmaximizes p(tki|S), therefore, we may to calculatethe denominator.
Moreover, if we assume that allthe different translations are equally distributed,then Eq.
(2) may be approximated by Eq.
(3).p(tki|w1, w2, ?
?
?
, wk, ?
?
? )
?
p(w1, w2, ?
?
?
, wk, ?
?
?
|tki)(3)The complete calculation of Eq.
(3) requires toapply the chain rule.
However, if we assumed thatthe words of the sentence are independent, then wemay rewrite Eq.
(3) as Eq.
(4).p(tki|w1, w2, ?
?
?
, wk, ?
?
? )
?|S|?j=1p(wj|tki) (4)The best translation is obtained as shown in Eq.(5).
Nevertheless the position of the ambiguousword, we are only considering a product of theprobabilites of translation.
Thus, we named thisapproach, the unweighted version.
Algorithm 1provides details about the implementation.BestSenseu(wk) = arg maxtki|S|?j=1p(wj|tki) (5)with i = 1, ?
?
?
, N .Algorithm 1: An unweighted na?
?ve Bayes ap-proach to cross-lingual WSDInput: A set Q of sentences:Q = {S1, S2, ?
?
?
};Dictionary = p(w|t): A bilingual statisticaldictionary;Output: The best word/sense for eachambiguous word wj?
Slfor l = 1 to |Q| do1for i = 1 to N do2Pl,i= 1;3for j = 1 to |Sl| do4foreach wj?
Sldo5if wj?
Dictionary then6Pl,i= Pl,i?
p(wj|tki);7else8Pl,i= Pl,i?
?
;9end10end11end12end13end14return arg maxtki?|S|j=1p(wj|tki)15A second approach (weighted version) is alsoproposed as shown in Eq.
(6).
Algorithm 2 pro-vides details about its implementation.BestSensew(wk) =arg maxtki|S|?j=1p(wj|tki) ?1k ?
j + 1(6)With respect to the N candidate translationsof the polysemous word wk, {tk1, tk2, ?
?
?
, tkN}, wehave used of the Google translator1 .
Google pro-vides all the possible translations for wkwiththe corresponding grammatical category.
There-fore, we are able to use those translations thatmatch with the same grammatical category of the1http://translate.google.com.mx/113Figure 1: An overview of the presented approach for cross-lingual word sense disambiguationAlgorithm 2: A weighted na?
?ve Bayes ap-proach to cross-lingual WSDInput: A set Q of sentences:Q = {S1, S2, ?
?
?
};Dictionary = p(w|t): A bilingual statisticaldictionary;Output: The best word/sense for eachambiguous word wj?
Slfor l = 1 to |Q| do1for i = 1 to N do2Pl,i= 1;3for j = 1 to |Sl| do4foreach wj?
Sldo5if wj?
Dictionary then6Pl,i=7Pl,i?
p(wj|tki) ?1k?j+1;else8Pl,i= Pl,i?
?
;9end10end11end12end13end14return arg maxtki?|S|j=1p(wj|tki) ?1k?j+115ambiguous word.
Even if we attempted otherapproaches such as selecting the most probabletranslations from the statistical dictionary, we con-firmed that by using the Google online transla-tor we obtain the best results.
We consider thatthis result is derived from the fact that Google hasa better language model than we have, becauseour bilingual statistical dictionary was trained onlywith the EUROPARL parallel corpus.The experimental results of both, the un-weighted and the weighted versions of the pre-sented approach for cross-lingual word sense dis-ambiguation are given in Section 3.2.2 Cross-Lingual Lexical SubstitutionThis module is based on the cross-lingual wordsense disambiguation system.
Once we knewthe best word/sense (Spanish) for the ambigu-ous word(English), we lemmatized the Spanishword.
Thereafter, we searched, at WordNet, thesynonyms of this word (sense) that agree withthe grammatical category (noun, verb, etc) of thequery (source polysemous word), and we returnthose synonyms as possible lexical substitutes.Notice again that this task is complemented by theWSD solver.In Figure 2 we may see the complete process ofapproaching the problem of cross-lingual lexicalsubstitution.114Figure 2: An overview of the presented approach for cross-lingual lexical substitution3 Experimental ResultsIn this section we present the obtained results forboth, the cross-lingual word sense disambiguationtask and the cross-lingual lexical substitution task.3.1 Cross-Lingual Word SenseDisambiguationIn Table 2 we may see the results we have ob-tained with the different versions of the presentedapproach.
In the same Table we can find a com-parison of our runs with others presented at theSemEval-2 competition.
In particular, we havetested four different runs which correspond to twoevaluations for each different version of the prob-abilistic classifier.
The description of each run isgiven in Table 1.We obtained a better performance with thoseruns that were evaluated with the five best trans-lations (oof) than with those that were evaluatedwith only the best ones.
This fact lead us to con-sider in further work to improve the ranking of thetranslations found by our system.
On other hand,the unweighted version of the proposed classifierimproved the weighted one.
This behavior was un-expected, because in the development dataset, theresults were opposite.
We consider that the prob-lem comes from taking into account the entire sen-tence instead of a neighborhood (windows) aroundthe ambiguous word.
We will further investigateabout this issue.
We got a better performance thanother systems, and those runs that outperformedour system runs did it by around 3% of precisionand recall in the case of the oof evaluation.3.2 Cross-Lingual Lexical SubstitutionIn Table 3 we may see the obtained results forthe cross-lingual lexical substitution task.
The ob-tained results are low in comparison with the bestone.
Since this task relies on the C-WSD task, thena lower performance on the C-WSD task will con-duct to a even lower performance in C-LS.
Firstly,we need to improve the C-WSD solver.
In partic-ular, we need to improve the ranking procedure inorder to obtain a better translation of the sourceambiguous word.
Moreover, we consider that theuse of language modeling would be of high ben-efit, since we could test whether or not a giventranslation together with the terms in its contextwould have high probability in the target language.115Run name DescriptionFCC-WSD1 : Best translation (one target word) / unweighted versionFCC-WSD2 : Five best translations (five target words - oof) / unweighted versionFCC-WSD3 : Best translation (one target word) / weighted versionFCC-WSD4 : Five best translations (five target words - oof) / weighted versionTable 1: Description of runsSystem name Precision (%) Recall (%)UvT-v 23.42 23.42UvT-g 19.92 19.92FCC-WSD1 15.09 15.09FCC-WSD3 14.43 14.43UHD-1 20.48 16.33UHD-2 20.2 16.09T3-COLEUR 19.78 19.59System name Precision (%) Recall (%)UvT-v 42.17 42.17UvT-g 43.12 43.12FCC-WSD2 40.76 40.76FCC-WSD4 38.46 38.46UHD-1 38.78 31.81UHD-2 37.74 31.3T3-COLEUR 35.84 35.46a) Best translation b) Five best translations (oof)Table 2: Evaluation of the cross-lingual word sense disambiguation taskSystem name Precision (%) Recall (%)SWAT-E 174.59 174.59SWAT-S 97.98 97.98UvT-v 58.91 58.91UvT-g 55.29 55.29UBA-W 52.75 52.75WLVUSP 48.48 48.48UBA-T 47.99 47.99USPWLV 47.6 47.6ColSlm 43.91 46.61ColEur 41.72 44.77TYO 34.54 35.46IRST-1 31.48 33.14FCC-LS 23.9 23.9IRSTbs 8.33 29.74DICT 44.04 44.04DICTCORP 42.65 42.65Table 3: Evaluation of the cross-lingual lexicalsubstitution task (the ten best results - oot)4 Conclusions and Further WorkIn this paper we have presented a system for cross-lingual word sense disambiguation and cross-lingual lexical substitution.
The approach uses aNa?
?ve Bayes classifier which is fed with the prob-abilities obtained from a bilingual statistical dic-tionary.
Two different versions of the classifier,unweighted and weighted were tested.
The resultswere compared with those of an international com-petition, obtaining a good performance.
As fur-ther work, we need to improve the ranking mod-ule of the cross-lingual WSD classifier.
Moreover,we consider that the use of a language model forSpanish would highly improve the results on thecross-lingual lexical substitution task.AcknowledgmentsThis work has been partially supported by CONA-CYT (Project #106625) and PROMEP (Grant#103.5/09/4213).References[Agirre and Edmonds2006] E. Agirre and P. Edmonds.2006.
Word Sense Disambiguation, Text, Speechand Language Technology.
Springer.
[Carpuat and Wu.2007] M. Carpuat and D. Wu.
2007.Improving statistical machine translation using wordsense disambiguation.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLPCoNLL), pages 61?72.
[Chan et al2007] Y.S.
Chan, H.T.
Ng, and D. Chiang.2007.
Word sense disambiguation improves statisti-cal machine translation.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 33?40.
[Lefever and Hoste2010] E. Lefever and V. Hoste.2010.
Semeval-2010 task3:cross-lingual wordsense disambiguation.
In Proceedings of theFifth International Workshop on Semantic Evalu-ations (SemEval-2010).
Association for Computa-tional Linguistics.
[Mihalcea et al2010] R. Mihalcea, R. Sinha, andD.
McCarthy.
2010.
Semeval-2010 task2:cross-lingual lexical substitution.
In Proceedings of theFifth International Workshop on Semantic Evalu-ations (SemEval-2010).
Association for Computa-tional Linguistics.116
