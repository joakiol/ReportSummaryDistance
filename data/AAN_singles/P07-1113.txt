Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 896?903,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Sequencing Model for Situation Entity ClassificationAlexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota SmithDepartment of LinguisticsUniversity of Texas at Austin{alexispalmer,ponvert,jbaldrid,carlotasmith}@mail.utexas.eduAbstractSituation entities (SEs) are the events, states,generic statements, and embedded facts andpropositions introduced to a discourse byclauses of text.
We report on the first data-driven models for labeling clauses accordingto the type of SE they introduce.
SE classifi-cation is important for discourse mode iden-tification and for tracking the temporal pro-gression of a discourse.
We show that (a)linguistically-motivated cooccurrence fea-tures and grammatical relation informationfrom deep syntactic analysis improve clas-sification accuracy and (b) using a sequenc-ing model provides improvements over as-signing labels based on the utterance alone.We report on genre effects which support theanalysis of discourse modes having charac-teristic distributions and sequences of SEs.1 IntroductionUnderstanding discourse requires identifying theparticipants in the discourse, the situations they par-ticipate in, and the various relationships between andamong both participants and situations.
Coreferenceresolution, for example, is concerned with under-standing the relationships between references to dis-course participants.
This paper addresses the prob-lem of identifying and classifying references to situ-ations expressed in written English texts.Situation entities (SEs) are the events, states,generic statements, and embedded facts and propo-sitions which clauses introduce (Vendler, 1967;Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher,1993; Carlson and Pelletier, 1995).
Consider thetext passage below, which introduces an event-typeentity in (1), a report-type entity in (2), and a state-type entity in (3).
(1) Sony Corp. has heavily promoted the VideoWalkmansince the product?s introduction last summer ,(2) but Bob Gerson , video editor of This Week in Con-sumer Electronics , says(3) Sony conceives of 8mm as a ?family of products ,camcorders and VCR decks , ?SE classification is a fundamental component in de-termining the discourse mode of texts (Smith, 2003)and, along with aspectual classification, for tempo-ral interpretation (Moens and Steedman, 1988).
Itmay be useful for discourse relation projection anddiscourse parsing.Though situation entities are well-studied in lin-guistics, they have received very little computationaltreatment.
This paper presents the first data-drivenmodels for SE classification.
Our two main strate-gies are (a) the use of linguistically-motivated fea-tures and (b) the implementation of SE classificationas a sequencing task.
Our results also provide empir-ical support for the very notion of discourse modes,as we see clear genre effects in SE classification.We begin by discussing SEs in more detail.
Sec-tion 3 describes our two annotated data sets and pro-vides examples of each SE type.
Section 4 discussesfeature sets, and sections 5 and 6 present models,experiments, and results.8962 Discourse modes and situation entitiesIn this section, we discuss some of the linguistic mo-tivation for SE classification and the relation of SEclassification to discourse mode identification.2.1 Situation entitiesThe categorization of SEs into aspectual classes ismotivated by patterns in their linguistic behavior.We adopt an expanded version of a paradigm relat-ing SEs to discourse mode (Smith, 2003) and char-acterize SEs with four broad categories:1.
Eventualities.
Events (E), particular states (S),and reports (R).
R is a sub-type of E for SEsintroduced by verbs of speech (e.g., say).2.
General statives.
Generics (G) and generaliz-ing sentences (GS).
The former are utterancespredicated of a general class or kind rather thanof any specific individual.
The latter are habit-ual utterances that refer to ongoing actions orproperties predicated of specific individuals.3.
Abstract entities.
Facts (F) and proposi-tions (P).14.
Speech-act types.
Questions (Q) and impera-tives (IMP).Examples of each SE type are given in section 3.2.There are a number of linguistic tests for iden-tifying situation entities (Smith, 2003).
The termlinguistic test refers to a rule which correlates anSE type to particular linguistic forms.
For exam-ple, event-type verbs in simple present tense are alinguistic correlate of GS-type SEs.These linguistic tests vary in their precision anddifferent tests may predict different SE types forthe same clause.
A rule-based implementation us-ing them to classify SEs would require careful ruleordering or mediation of rule conflicts.
However,since these rules are exactly the sort of informationextracted as features in data-driven classifiers, they1In our system these two SE types are identified largely ascomplements of factive and propositional verbs as discussedin Peterson (1997).
Fact and propositional complements havesome linguistic as well as some notional differences.
Facts mayhave causal effects, and facts are in the world.
Neither of theseis true for propositions.
In addition, the two have somewhatdifferent semantic consequences of a presuppositional nature.can be cleanly integrated by assigning them empiri-cally determined weights.
We use maximum entropymodels (Berger et al, 1996), which are particularlywell-suited for tasks (like ours) with many overlap-ping features, to harness these linguistic insights byusing features in our models which encode, directlyor indirectly, the linguistic correlates to SE types.The features are described in detail in section 4.2.2 Basic and derived situation typesSituation entities each have a basic situation type,determined by the verb plus its arguments, the verbconstellation.
The verb itself plays a key role in de-termining basic situation type but it is not the onlyfactor.
Changes in the arguments or tense of the verbsometimes change the basic situation types:(4) Mickey painted the house.
(E)(5) Mickey paints houses.
(GS)If SE type could be determined solely by the verbconstellation, automatic classification of SEs wouldbe a relatively straightforward task.
However, otherparts of the clause often override the basic situationtype, resulting in aspectual coercion and a derivedsituation type.
For example, a modal adverb cantrigger aspectual coercion:(6) Mickey probably paints houses.
(P)Serious challenges for SE classification arise fromthe aspectual ambiguity and flexibility of manypredicates as well as from aspectual coercion.2.3 Discourse modesMuch of the motivation of SE classification istoward the broader goal of identifying discoursemodes, which provide a linguistic characterizationof textual passages according to the situation enti-ties introduced.
They correspond to intuitions as tothe rhetorical or semantic character of a text.
Pas-sages of written text can be classified into modesof discourse ?
Narrative, Description, Argument, In-formation, and Report ?
by examining concrete lin-guistic cues in the text (Smith, 2003).
These cuesare of two forms: the distribution of situation entitytypes and the mode of progression (either temporalor metaphorical) through the text.897For example, the Narration and Report modesboth contain mainly events and temporally boundedstates; they differ in their principles of temporal pro-gression.
Report passages progress with respect to(deictic) speech time, whereas Narrative passagesprogress with respect to (anaphoric) reference time.Passages in the Description mode are predominantlystative, and Argument mode passages tend to becharacterized by propositions and Information modepassages by facts and states.3 DataThis section describes the data sets used in the ex-periments, the process for creating annotated train-ing data, and preprocessing steps.
Also, we give ex-amples of the ten SE types.There are no established data sets for SE classifi-cation, so we created annotated training data to testour models.
We have annotated two data sets, onefrom the Brown corpus and one based on data fromthe Message Understanding Conference 6 (MUC6).3.1 SegmentationThe Brown texts were segmented according to SE-containing clausal boundaries, and each clause waslabeled with an SE label.
Segmentation is itself adifficult task, and we made some simplifications.In general, clausal complements of verbs like saywhich have clausal direct objects were treated asseparate clauses and given an SE label.
Clausal com-plements of verbs which have an entity as a directobject and second clausal complement (such as no-tify) were not treated as separate clauses.
In addi-tion, some modifying and adjunct clauses were notassigned separate SE labels.The MUC texts came to us segmented into ele-mentary discourse units (EDUs), and each EDU waslabeled by the annotators.
The two data sets weresegmented according to slightly different conven-tions, and we did not normalize the segmentation.The inconsistencies in segmentation introduce someerror to the otherwise gold-standard segmentations.3.2 AnnotationEach text was independently annotated by two ex-perts and reviewed by a third.
Each clause was as-signed precisely one SE label from the set of tenpossible labels.
For clauses which introduce moreSE TextS That compares with roughly paperback-bookdimensions for VHS.G Accordingly, most VHS camcorders are usuallybulky and weigh around eight pounds or more.S ?Carl is a tenacious fellow,?R said a source close to USAir.GS ?He doesn?t give up easilyGS and one should never underestimate what he canor will do.
?S For Jenks knewF that Bari?s defenses were made of paper.E Mr. Icahn then proposedP that USAir buy TWA,IMP ?Fermate?
!R Musmanno bellowed to his Italian crewmen.Q What?s her name?S Quite seriously, the names mentioned as possibilitieswere three male apparatchiks from the Beltway?sDemocratic political machineN By Andrew B. Cohen Staff Reporter of The WSJTable 1: Example clauses and their SE annota-tion.
Horizontal lines separate extracts from differ-ent texts.than one SE, the annotators selected the most salientone.
This situation arose primarily when comple-ment clauses were not treated as distinct clauses, inwhich case the SE selected was the one introducedby the main verb.
The label N was used for clauseswhich do not introduce any situation entity.The Brown data set consists of 20 ?popular lore?texts from section cf of the Brown corpus.
Seg-mentation of these texts resulted in a total of 4390clauses.
Of these, 3604 were used for training anddevelopment, and 786 were held out as final test-ing data.
The MUC data set consists of 50 WallStreet Journal newspaper articles segmented to a to-tal of 1675 clauses.
137 MUC clauses were heldout for testing.
The Brown texts are longer thanthe MUC texts, with an average of 219.5 clausesper document as compared to MUC?s average of33.5 clauses.
The average clause in the Brown datacontains 12.6 words, slightly longer than the MUCtexts?
average of 10.9 words.Table 1 provides examples of the ten SE types aswell as showing how clauses were segmented.
EachSE-containing example is a sequence of EDUs fromthe data sets used in this study.898WWORDS words & punctuationWTW (see above)POSONLY POS tag for each wordWORD/POS word/POS pair for each wordWTLWT (see above)FORCEPRED T if clause (or preceding clause)contains force predicatePROPPRED T if clause (or preceding clause)contains propositional verbFACTPRED T if clause (or preceding clause)contains factive verbGENPRED T if clause contains generic predicateHASFIN T if clause contains finite verbHASMODAL T if clause contains modal verbFREQADV T if clause contains frequency adverbMODALADV T if clause contains modal adverbVOLADV T if clause contains volitional adverbFIRSTVB lexical item and POS tag for first verbWTLGWTL (see above)VERBS all verbs in clauseVERBTAGS POS tags for all verbsMAINVB main verb of clauseSUBJ subject of clause (lexical item)SUPER CCG supertagTable 2: Feature sets for SE classification3.3 PreprocessingThe linguistic tests for SE classification appeal tomultiple levels of linguistic information; there arelexical, morphological, syntactic, categorial, andstructural tests.
In order to access categorial andstructural information, we used the C&C2 toolkit(Clark and Curran, 2004).
It provides part-of-speechtags and Combinatory Categorial Grammar (CCG)(Steedman, 2000) categories for words and syntac-tic dependencies across words.4 FeaturesOne of our goals in undertaking this study was toexplore the use of linguistically-motivated featuresand deep syntactic features in probabilistic modelsfor SE classification.
The nature of the task requiresfeatures characterizing the entire clause.
Here, wedescribe our four feature sets, summarized in table 2.The feature sets are additive, extending very basicfeature sets first with linguistically-motivated fea-tures and then with deep syntactic features.2svn.ask.it.usyd.edu.ap/trac/candc/wiki4.1 Basic feature sets: W and WTThe WORDS (W) feature set looks only at the wordsand punctuation in the clause.
These features areobtained with no linguistic processing.WORDS/TAGS (WT) incorporates part-of-speech(POS) tags for each word, number, and punctuationmark in the clause and the word/tag pairs for eachelement of the clause.
POS tags provide valuable in-formation about syntactic category as well as certainkinds of shallow semantic information (such as verbtense).
The tags are useful for identifying verbs,nouns, and adverbs, and the words themselves repre-sent lexico-semantic information in the feature sets.4.2 Linguistically-motivated feature set: WTLThe WORDS/TAGS/LINGUISTIC CORRELATES(WTL) feature set introduces linguistically-motivated features gleaned from the literatureon SEs; each feature encodes a linguistic cue thatmay correlate to one or more SE types.
Thesefeatures are not directly annotated; instead they areextracted by comparing words and their tags forthe current and immediately preceding clauses tolists containing appropriate triggers.
The lists arecompiled from the literature on SEs.For example, clauses embedded under predicateslike force generally introduce E-type SEs:(7) I forced [John to run the race with me].
(8) * I forced [John to know French].The feature force-PREV is extracted if a memberof the force-type predicate word list occurs in theprevious clause.Some of the correlations discussed in the litera-ture rely on a level of syntactic analysis not availablein the WTL feature set.
For example, stativity of themain verb is one feature used to distinguish betweenevent and state SEs, and particular verbs and verbtenses have tendencies with respect to stativity.
Toapproximate the main verb without syntactic analy-sis, WTL uses the lexical item of the first verb in theclause and the POS tags of all verbs in the clause.These linguistic tests are non-absolute, makingthem inappropriate for a rule-based model.
Ourmodels handle the defeasibility of these correlationsprobabilistically, as is standard for machine learningfor natural language processing.8994.3 Addition of deep features: WTLGThe WORDS/TAGS/LINGUISTIC CORRE-LATES/GRAMMATICAL RELATIONS (WTLG)feature set uses a deeper level of syntactic analysisvia features extracted from CCG parse representa-tions for each clause.
This feature set requires anadditional step of linguistic processing but providesa basis for more accurate classification.WTL approximated the main verb by sloppily tak-ing the first verb in the clause; in contrast, WTLGuses the main verb identified by the parser.
Theparser also reliably identifies the subject, which isused as a feature.Supertags ?CCG categories assigned to words?provide an interesting class of features in WTLG.They succinctly encode richer grammatical informa-tion than simple POS tags, especially subcategoriza-tion and argument types.
For example, the tag S\NPdenotes an intransitive verb, whereas (S\NP)/NPdenotes a transitive verb.
As such, they can be seenas a way of encoding the verbal constellation and itseffect on aspectual classification.5 ModelsWe consider two types of models for the automaticclassification of situation entities.
The first, a la-beling model, utilizes a maximum entropy modelto predict SE labels based on clause-level linguisticfeatures as discussed above.
This model ignores thediscourse patterns that link multiple utterances.
Be-cause these patterns recur, a sequencing model maybe better suited to the SE classification task.
Oursecond model thus extends the first by incorporatingthe previous n (0 ?
n ?
6) labels as features.Sequencing is standardly used for tasks like part-of-speech tagging, which generally assume smallerunits to be both tagged and considered as contextfor tagging.
We are tagging at the clause level ratherthan at the word level, but the structure of the prob-lem is essentially the same.
We thus adapted theOpenNLP maximum entropy part-of-speech tagger3(Hockenmaier et al, 2004) to extract features fromutterances and to tag sequences of utterances insteadof words.
This allows the use of features of adjacentclauses as well as previously-predicted labels whenmaking classification decisions.3http://opennlp.sourceforge.net.6 ExperimentsIn this section we give results for testing on Browndata.
All results are reported in terms of accu-racy, defined as the percentage of correctly-labeledclauses.
Standard 10-fold cross-validation on thetraining data was used to develop models and fea-ture sets.
The optimized models were then tested onthe held-out Brown and MUC data.The baseline was determined by assigning S(state), the most frequent label in both training sets,to each clause.
Baseline accuracy was 38.5% and36.2% for Brown and MUC, respectively.In general, accuracy figures for MUC are muchhigher than for Brown.
This is likely due to the factthat the MUC texts are more consistent: they are allnewswire texts of a fairly consistent tone and genre.The Brown texts, in contrast, are from the ?popularlore?
section of the corpus and span a wide rangeof topics and text types.
Nonetheless, the patternsbetween the feature sets and use of sequence predic-tion hold across both data sets; here, we focus ourdiscussion on the results for the Brown data.6.1 Labeling resultsThe results for the labeling model appear in the twocolumns labeled ?n=0?
in table 3.
On Brown, thesimple W feature set beats the baseline by 6.9% withan accuracy of 45.4%.
Adding POS information(WT) boosts accuracy 4.5% to 49.9%.
We did notsee the expected increase in performance from thelinguistically motivated WTL features, but rather aslight decrease in accuracy to 48.9%.
These featuresmay require a greater amount of training material tobe effective.
Addition of deep linguistic informationwith WTLG improved performance to 50.6%, a gainof 5.2% over words alone.6.2 Oracle resultsTo determine the potential effectiveness of sequenceprediction, we performed oracle experiments onBrown by including previous gold-standard labels asfeatures.
Figure 1 illustrates the results from ora-cle experiments incorporating from zero to six pre-vious gold-standard SE labels (the lookback).
Theincrease in performance illustrates the importance ofcontext in the identification of SEs and motivates theuse of sequence prediction.900424446485052545658600 1 2 3 4 5 6AccLookbackWWTWTLWTLGFigure 1: Oracle results on Brown data.6.3 Sequencing resultsTable 3 gives the results of classification with the se-quencing model on the Brown data.
As with the la-beling model, accuracy is boosted by WT and WTLGfeature sets.
We see an unexpected degradation inperformance in the transition from WT to WTL.The most interesting results here, though, are thegains in accuracy from use of previously-predictedlabels as features for classification.
When labelingperformance is relatively poor, as with feature set W,previous labels help very little, but as labeling accu-racy increases, previous labels begin to effect notice-able increases in accuracy.
For the best two featuresets, considering the previous two labels raises theaccuracy 2.0% and 2.5%, respectively.In most cases, though, performance starts to de-grade as the model incorporates more than two pre-vious labels.
This degradation is illustrated in Fig-ure 2.
The explanation for this is that the model isstill very weak, with an accuracy of less than 54%for the Brown data.
The more previous predicted la-bels the model conditions on, the greater the likeli-hood that one or more of the labels is incorrect.
Withgold-standard labels, we see a steady increase in ac-curacy as we look further back, and we would needa better performing model to fully take advantage ofknowledge of SE patterns in discourse.The sequencing model plays a crucial role, partic-ularly with such a small amount of training material,and our results indicate the importance of local con-text in discourse analysis.424446485052540 1 2 3 4 5 6WWTWTLWTLGFigure 2: Sequencing results on Brown data.BROWN Lookback (n)0 1 2 3 4 5 6W 45.4 45.2 46.1 46.6 42.8 43.0 42.4WT 49.9 52.4 51.9 49.2 47.2 46.2 44.8WTL 48.9 50.5 50.1 48.9 46.7 44.9 45.0WTLG 50.6 52.9 53.1 48.1 46.4 45.9 45.7Baseline 38.5Table 3: SE classification results with sequencingon Brown test set.
Bold cell indicates accuracy at-tained by model parameters that performed best ondevelopment data.6.4 Error analysisGiven that a single one of the ten possible labelsoccurs for more than 35% of clauses in both datasets, it is useful to look at the distribution of er-rors over the labels.
Table 4 is a confusion matrixfor the held-out Brown data using the best featureset.4 The first column gives the label and numberof occurrences of that label, and the second columnis the accuracy achieved for that label.
The nexttwo columns show the percentage of erroneous la-bels taken by the labels S and GS.
These two labelsare the most common labels in the development set(38.5% and 32.5%).
The final column sums the per-centages of errors assigned to the remaining sevenlabels.
As one would expect, the model learns thepredominance of these two labels.
There are a fewinteresting points to make about this data.First, 66% of G-type clauses are mistakenly as-signed the label GS.
This is interesting becausethese two SE-types constitute the broader SE cat-4Thanks to the anonymous reviewer who suggested this use-ful way of looking at the data.901% Correct % IncorrectLabel Label S GS OtherS(278) 72.7 n/a 14.0 13.3E(203) 50.7 37.0 11.8 0.5GS(203) 44.8 46.3 n/a 8.9R(26) 38.5 30.8 11.5 19.2N(47) 23.4 31.9 23.4 21.3G(12) 0.0 25.0 66.7 8.3IMP(8) 0.0 75.0 25.0 0.0P(7) 0.0 71.4 28.6 0.0F(2) 0.0 100.0 0.0 0.0Table 4: Confusion matrix for Brown held-out testdata, WTLG feature set, lookback n = 2.
Numbersin parentheses indicate how many clauses have theassociated gold standard label.egory of generalizing statives.
The distribution oferrors for R-type clauses points out another interest-ing classification difficulty.5 Unlike the other cat-egories, the percentage of false-other labels for R-type clauses is higher than that of false-GS labels.80% of these false-other labels are of type E. Theexplanation for this is that R-type clauses are a sub-type of the event class.6.5 Genre effects in classificationDifferent text domains frequently have differentcharacteristic properties.
Discourse modes are oneway of analyzing these differences.
It is thus in-teresting to compare SE classification when trainingand testing material come from different domains.Table 5 shows the performance on Brown whentraining on Brown and/or MUC using the WTLGfeature set with simple labeling and with sequenceprediction with a lookback of two.
A number ofthings are suggested by these figures.
First, the la-beling model (lookback of zero), beats the baselineeven when training on out-of-domain texts (43.1%vs.
38.5%), but this is unsurprisingly far belowtraining on in-domain texts (43.1% vs. 50.6%).Second, while sequence prediction helps with in-domain training (53.1% vs 50.6%), it makes nodifference with out-of-domain training (42.9% vs43.1%).
This indicates that the patterns of SEs in atext do indeed correlate with domains and their dis-course modes, in line with case-studies in the dis-course modes theory (Smith, 2003).
Finally, mix-5Thanks to an anonymous reviewer for bringing this to ourattention.lookback Brown test setWTLGtrain:Brown 0 50.62 53.1train:MUC 0 43.12 42.9train:all 0 50.42 49.5Table 5: Cross-domain SE classificationing out-of-domain training material with in-domainmaterial does not hurt labelling accuracy (50.4% vs50.6%), but it does take away the gains from se-quencing (49.5% vs 53.1%).These genre effects are suggestive, but inconclu-sive.
A similar setup with much larger training andtesting sets would be necessary to provide a clearerpicture of the effect of mixed domain training.7 Related workThough we are aware of no previous work in SEclassification, others have focused on automatic de-tection of aspectual and temporal data.Klavans and Chodorow (1992) laid the founda-tion for probabilistic verb classification with theirinterpretation of aspectual properties as gradient andtheir use of statistics to model the gradience.
Theyimplement a single linguistic test for stativity, treat-ing lexical properties of verbs as tendencies ratherthan absolute characteristics.Linguistic indicators for aspectual classificationare also used by Siegel (1999), who evaluates 14 in-dicators to test verbs for stativity and telicity.
Manyof his indicators overlap with our features.Siegel and McKeown (2001) address classifica-tion of verbs for stativity (event vs. state) andfor completedness (culminated vs. non-culminatedevents).
They compare three supervised and one un-supervised machine learning systems.
The systemsobtain relatively high accuracy figures, but they aredomain-specific, require extensive human supervi-sion, and do not address aspectual coercion.Merlo and Stevenson (2001) use corpus-basedthematic role information to identify and classifyunergative, unaccusative, and object-drop verbs.Stevenson and Merlo note that statistical analysiscannot and should not be separated from deeper lin-guistic analysis, and our results support that claim.902The advantages of our approach are the broadenedconception of the classification task and the use ofsequence prediction to capture a wider context.8 ConclusionsSituation entity classification is a little-studied butimportant classification task for the analysis of dis-course.
We have presented the first data-driven mod-els for SE classification, motivating the treatment ofSE classification as a sequencing task.We have shown that linguistic correlations to sit-uation entity type are useful features for proba-bilistic models, as are grammatical relations andCCG supertags derived from syntactic analysis ofclauses.
Models for the task perform poorly givenvery basic feature sets, but minimal linguistic pro-cessing in the form of part-of-speech tagging im-proves performance even on small data sets used forthis study.
Performance improves even more whenwe move beyond simple feature sets and incorpo-rate linguistically-motivated features and grammat-ical relations from deep syntactic analysis.
Finally,using sequence prediction by adapting a POS-taggerfurther improves results.The tagger we adapted uses beam search; this al-lows tractable use of maximum entropy for each la-beling decision but forgoes the ability to find theoptimal label sequence using dynamic programmingtechniques.
In contrast, Conditional Random Fields(CRFs) (Lafferty et al, 2001) allow the use of max-imum entropy to set feature weights with efficientrecovery of the optimal sequence.
Though CRFs aremore computationally intensive, the small set of SElabels should make the task tractable for CRFs.In future, we intend to test the utility of SEs in dis-course parsing, discourse mode identification, anddiscourse relation projection.AcknowledgmentsThis work was supported by the Morris MemorialTrust Grant from the New York Community Trust.The authors would like to thank Nicholas Asher,Pascal Denis, Katrin Erk, Garrett Heifrin, JulieHunter, Jonas Kuhn, Ray Mooney, Brian Reese, andthe anonymous reviewers.ReferencesN.
Asher.
1993.
Reference to Abstract objects in Dis-course.
Kluwer Academic Publishers.A.
Berger, S. Della Pietra, and V. Della Pietra.
1996.
Amaximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.G.
Carlson and F. J. Pelletier, editors.
1995.
The GenericBook.
University of Chicago Press, Chicago.S.
Clark and J. R. Curran.
2004.
Parsing the WSJ usingCCG and log?linear models.
In Proceedings of ACL?04, pages 104?111, Barcelona, Spain.D.
Dowty.
1979.
Word Meaning and Montague Gram-mar.
Reidel, Dordrecht.J.
Hockenmaier, G. Bierner, and J. Baldridge.
2004.
Ex-tending the coverage of a CCG system.
Research onLanguage and Computation, 2:165?208.J.
L. Klavans and M. S. Chodorow.
1992.
Degrees ofstativity: The lexical representation of verb aspect.
InProceedings of COLING 14, Nantes, France.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labelling sequence data.
In Proceedingsof ICML, pages 282?289, Williamstown, USA.P.
Merlo and S. Stevenson.
2001.
Automatic verb clas-sification based on statistical distributions of argumentstructure.
Computational Linguistics.M.
Moens and M. Steedman.
1988.
Temporal ontol-ogy and temporal reference.
Computational Linguis-tics, 14(2):15?28.P.
Peterson.
1997.
Fact Proposition Event.
Kluwer.E.
V. Siegel and K. R. McKeown.
2001.
Learning meth-ods to combine linguistic indicators: Improving as-pectual classification and revealing linguistic insights.Computational Linguistics, 26(4):595?628.E.
V. Siegel.
1999.
Corpus-based linguistic indicatorsfor aspectual classification.
In Proceedings of ACL37,University of Maryland, College Park.C.
S. Smith.
1991.
The Parameter of Aspect.
Kluwer.C.
S. Smith.
2003.
Modes of Discourse.
CambridgeUniversity Press.M.
Steedman.
2000.
The Syntactic Process.
MITPress/Bradford Books.Z.
Vendler, 1967.
Linguistics in Philosophy, chapterVerbs and Times, pages 97?121.
Cornell UniversityPress, Ithaca, New York.H.
Verkuyl.
1972.
On the Compositional Nature of theAspects.
Reidel, Dordrecht.903
