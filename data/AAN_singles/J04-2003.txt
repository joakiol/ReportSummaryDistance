c?
2004 Association for Computational LinguisticsStatistical Machine Translation withScarce Resources Using Morpho-syntacticInformationSonja Nie?en?
Hermann Ney?RWTH Aachen RWTH AachenIn statistical machine translation, correspondences between the words in the source and thetarget language are learned from parallel corpora, and often little or no linguistic knowledge isused to structure the underlying models.
In particular, existing statistical systems for machinetranslation often treat different inflected forms of the same lemma as if they were independent of oneanother.
The bilingual training data can be better exploited by explicitly taking into account theinterdependencies of related inflected forms.
We propose the construction of hierarchical lexiconmodels on the basis of equivalence classes of words.
In addition, we introduce sentence-levelrestructuring transformations which aim at the assimilation of word order in related sentences.We have systematically investigated the amount of bilingual training data required to maintainan acceptable quality of machine translation.
The combination of the suggested methods forimproving translation quality in frameworks with scarce resources has been successfully tested:We were able to reduce the amount of bilingual training data to less than 10% of the originalcorpus, while losing only 1.6% in translation quality.
The improvement of the translation resultsis demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole!task.1.
IntroductionThe statistical approach to machine translation has proved successful in various com-parative evaluations since its revival by the work of the IBM research group morethan a decade ago.
The IBM group dispensed with linguistic analysis, at least in itsearliest publications.
Although the IBM group finally made use of morphological andsyntactic information to enhance translation quality (Brown et al 1992; Berger et al1996), most of today?s statistical machine translation systems still consider only surfaceforms and use no linguistic knowledge about the structure of the languages involved.In many applications only small amounts of bilingual training data are availablefor the desired domain and language pair, and it is highly desirable to avoid at leastparts of the costly data collection process.
The main objective of the work reported inthis article is to introduce morphological knowledge in order to reduce the amountof bilingual data necessary to sufficiently cover the vocabulary expected in testing.This is achieved by explicitly taking into account the interdependencies of relatedinflected forms.
In this work, a hierarchy of equivalence classes at different levels ofabstraction is proposed.
Features from those hierarchy levels are combined to formhierarchical lexicon models, which can replace the standard probabilistic lexicon used?
Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,D-52056 Aachen, Germany.
E-mail: sonja.niessen@gmx.de; ney@cs.rwth-aachen.de.182Computational Linguistics Volume 30, Number 2in most statistical machine translation systems.
Apart from the improved coverage,the proposed lexicon models enable the disambiguation of ambiguous word forms bymeans of annotation with morpho-syntactic tags.1.1 OverviewThe article is organized as follows.
After briefly reviewing the basic concepts of thestatistical approach to machine translation, we discuss the state of the art and relatedwork as regards the incorporation of morphological and syntactic information intosystems for natural language processing.
Section 2 describes the information providedby morpho-syntactic analysis and introduces a suitable representation of the analyzedcorpus.
Section 3 suggests solutions for two specific aspects of structural difference,namely, question inversion and separated verb prefixes.
Section 4 is dedicated to hi-erarchical lexicon models.
These models are able to infer translations of word formsfrom the translations of other word forms of the same lemma.
Furthermore, they usemorpho-syntactic information to resolve categorial ambiguity.
In Section 5, we describehow disambiguation between different readings and their corresponding translationscan be performed when no context is available, as is typically the case for conven-tional electronic dictionaries.
Section 6 provides an overview of our procedure fortraining model parameters for statistical machine translation with scarce resources.Experimental results are reported in Section 7.
Section 8 concludes the presentationwith a discussion of the achievements of this work.1.2 Statistical Machine TranslationIn statistical machine translation, every target language string eI1 = e1 ?
?
?
eI is assigneda probability Pr(eI1) of being a valid word sequence in the target language and aprobability Pr(eI1|fJ1) of being a translation for the given source language string fJ1 =f1 ?
?
?
fJ.
According to Bayes?
decision rule, the optimal translation for f J1 is the targetstring that maximizes the product of the target language model Pr(eI1) and the stringtranslation model Pr(f J1 |eI1).
Many existing systems for statistical machine translation(Garc?
?a-Varea and Casacuberta 2001; Germann et al 2001; Nie?en et al 1998; Och,Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, DellaPietra, and Mercer (1993): The correspondence between the words in the source andthe target strings is described by alignments that assign target word positions to eachsource word position.
The probability that a certain target language word will occurin the target string is assumed to depend basically only on the source words alignedwith it.1.3 Related Work1.3.1 Morphology.
Some publications have already dealt with the treatment of mor-phology in the framework of language modeling and speech recognition: Kanevsky,Roukos, and Sedivy (1997) propose a statistical language model for inflected languages.They decompose word forms into stems and affixes.
Maltese and Mancini (1992) re-port that a linear interpolation of word n-grams, part of speech n-grams, and lemman-grams yields lower perplexity than pure word-based models.
Larson et al (2000)apply a data-driven algorithm for decomposing compound words in compoundinglanguages as well as for recombining phrases to enhance the pronunciation lexiconand the language model for large-vocabulary speech recognition systems.As regards machine translation, the treatment of morphology is part of the analysisand generation step in virtually every symbolic machine translation system.
For thispurpose, the lexicon should contain base forms of words and the grammatical category,183Nie?en and Ney SMT with Scarce Resourcessubcategorization features, and semantic information in order to enable the size of thelexicon to be reduced and in order to account for unknown word forms, that is, wordforms not present explicitly in the dictionary.Today?s statistical machine translation systems build upon the work of P. F. Brownand his colleagues at IBM.
The translation models they presented in various papersbetween 1988 and 1993 (Brown et al 1988; Brown et al 1990; Brown, Della Pietra, DellaPietra, and Mercer 1993) are commonly referred to as IBM models 1?5, based on thenumbering in Brown, Della Pietra, Della Pietra, and Mercer (1993).
The underlying(probabilistic) lexicon contains only pairs of full forms.
On the other hand, Brownet al (1992) had already suggested word forms be annotated with morpho-syntacticinformation, but they did not perform any investigation on the effects.1.3.2 Translation with Scarce Resources.
Some recent publications, like Al-Onaizanet al (2000), have dealt with the problem of translation with scarce resources.
Al-Onaizan et al report on an experiment involving Tetun-to-English translation by dif-ferent groups, including one using statistical machine translation.
Al-Onaizan et alassume the absence of linguistic knowledge sources such as morphological analyzersand dictionaries.
Nevertheless, they found that the human mind is very well capableof deriving dependencies such as morphology, cognates, proper names, and spellingvariations and that this capability was finally at the basis of the better results producedby humans compared to corpus-based machine translation.
The additional informationresults from complex reasoning, and it is not directly accessible from the full-word-form representation in the data.This article takes a different point of view: Even if full bilingual training dataare scarce, monolingual knowledge sources like morphological analyzers and data fortraining the target language model as well as conventional dictionaries (one wordand its translation[s] per entry) may be available and of substantial usefulness forimproving the performance of statistical translation systems.
This is especially the casefor more-inflecting major languages like German.
The use of dictionaries to augmentor replace parallel corpora has already been examined by Brown, Della Pietra, DellaPietra, and Goldsmith (1993) and Koehn and Knight (2001), for instance.2.
Morpho-syntactic InformationA prerequisite for the methods for improving the quality of statistical machine trans-lation described in this article is the availability of various kinds of morphologicaland syntactic information.
This section describes the output resulting from morpho-syntactic analysis and explains which parts of the analysis are used and how theoutput is represented for further processing.2.1 Description of the Analysis ResultsFor obtaining the required morpho-syntactic information, the following analyzers forGerman and English were applied: gertwol and engtwol for lexical analysis and gercgand engcg for morphological and syntactic disambiguation.
For a description of theunderlying approach, the reader is referred to Karlsson (1990).
Tables 1 and 2 giveexamples of the information provided by these tools.2.2 Treatment of AmbiguityThe examples in Tables 1 and 2 demonstrate the capability of the tools to disambiguateamong different readings: For instance, they infer that the word wollen is a verb in theindicative present first-person plural form.
Without any context taken into account,184Computational Linguistics Volume 30, Number 2Table 1Sample analysis of a German sentence.
Input: Wir wollen nach demAbendessen nach Essen aufbrechen.
(In English: We want to start for Essenafter dinner.
)Original Base form TagsWir wir personal-pronoun plural first nominativewollen wollen verb indicative present plural firstnach nach preposition dativedem das definite-article singular dative neuterAbendessen Abend#essen noun neuter singular dativenach nach preposition dativeEssen Essen noun name neuter singular dativeEsse noun feminine plural dativeEssen noun neuter plural dativeEssen noun neuter singular dativeaufbrechen auf|brechen verb separable infinitiveTable 2Sample analysis of an English sentence.
Input: Do we have to reserverooms?.Original Base form TagsDo do verb present not-singular-third finite auxiliarywe we personal-pronoun nominative plural first subjecthave have verb infinitive not-finite mainto to infinitive-markerreserve reserve verb infinitive not-finite mainrooms room noun nominative plural objectwollen has other readings.
It can even be interpreted as derived from an adjectivewith the meaning ?made of wool.?
The inflected word forms on the German part ofthe Verbmobil (cf.
Section 7.1.1) corpus have on average 2.85 readings (1.86 for theEnglish corpus), 58% of which can be eliminated by the syntactic analyzers on thebasis of sentence context.Common bilingual corpora normally contain full sentences, which provide enoughcontext information for ruling out all but one reading for an inflected word form.
Toreduce the remaining uncertainty, preference rules have been implemented.
For in-stance, it is assumed that the corpus is correctly true-case-converted beforehand, andas a consequence, non-noun readings of uppercase words are dropped.
Furthermore,indicative verb readings are preferred to subjunctive or imperative.
In addition, somesimple domain-specific heuristics are applied.
The reading ?plural of Esse?
for theGerman word form Essen, for instance, is much less likely in the domain of appoint-ment scheduling and travel arrangements than the readings ?proper name of the townEssen?
or the German equivalent of the English word meal.
As can be seen in Table 3,the reduction in the number of readings resulting from these preference rules is fairlysmall in the case of the Verbmobil corpus.The remaining ambiguity often lies in those parts of the information which arenot used or which are not relevant to the translation task.
For example, the analyzerscannot tell accusative from dative case in German, but the case information is notessential for the translation task (see also Table 4).
Section 2.4 describes a method185Nie?en and Ney SMT with Scarce ResourcesTable 3Resolution of ambiguity on the Verbmobil corpus.Number of readings per word formDisambiguation German EnglishNone 2.85 1.86By context 1.20 1.02By preference 1.19 1.02By selecting relevant tags 1.06 1.01By resorting to unambiguous part 1.00 1.00for selecting morpho-syntactic tags considered relevant for the translation task, whichresults in a further reduction in the number of readings per word form to 1.06 forGerman and 1.01 for English.
In these rare cases of ambiguity it is admissible to resortto the unambiguous parts of the readings, that is, to drop all tags causing mixedinterpretations.
Table 3 summarizes the gradual resolution of ambiguity.The analysis of conventional dictionaries poses some special problems, becausethey do not provide enough context to enable effective disambiguation.
For handlingthis special situation, dedicated methods have been implemented; these are presentedin Section 5.1.2.3 The Lemma-Tag RepresentationA full word form is represented by the information provided by the morpho-syntacticanalysis: from the interpretation gehen verb indicative present first singular, thatis, the base form plus part of speech plus the other tags, the word form gehe can berestored.
It has already been mentioned that the analyzers can disambiguate amongdifferent readings on the basis of context information.
In this sense, the informationinherent in the original word forms is augmented by the disambiguating analyzer.This can be useful for choosing the correct translation of ambiguous words.
Of course,these disambiguation clues result in an enlarged vocabulary.
The vocabulary of the newrepresentation of the German part of the Verbmobil corpus, for example, in which fullword forms are replaced by base form plus morphological and syntactic tags (lemma-tag representation), is one and a half times as large as the vocabulary of the originalcorpus.
On the other hand, the information in the lemma-tag representation can beaccessed gradually and ultimately reduced: For example, certain instances of wordscan be considered equivalent.
This fact is used to better exploit the bilingual trainingdata along two directions: detecting and omitting unimportant information (see Section2.4) and constructing hierarchical translation models (see Section 4).
To summarize,the lemma-tag representation of a corpus has the following main advantages: It makescontext information locally available, and it allows information to be explicitly accessedat different levels of abstraction.2.4 Equivalence Classes of Words with Similar TranslationInflected word forms in the input language often contain information that is not rel-evant for translation.
This is especially true for the task of translating from a moreinflecting language like German into English, for instance: In parallel German/Englishcorpora, the German part contains many more distinct word forms than the Englishpart (see, for example, Table 5).
It is useful for the process of statistical machine trans-lation to define equivalence classes of word forms which tend to be translated bythe same target language word: The resulting statistical translation lexicon becomes186Computational Linguistics Volume 30, Number 2Table 4Candidates for equivalence classes.Part of speech CandidatesNoun Gender (masculine, feminine, neuter)and case (nominative, dative, accusative)Verb Number (singular, plural) and person (first, second, third)Adjective Gender, case, and numberNumber Casesmoother, and the coverage is considerably improved.
Such equivalence classes areconstructed by omitting those items of information from morpho-syntactic analysiswhich are not relevant for translation.The lemma-tag representation of the corpus helps to identify the unimportantinformation.
The definition of relevant and unimportant information, respectively, de-pends on many factors like the languages involved, the translation direction, and thechoice of the models.
We detect candidates for equivalence classes of words automat-ically from the probabilistic lexicon trained for translation from German to English.For this purpose, those inflected forms of the same base form which result in the sametranslation are inspected.
For each set of tags T, the algorithm counts how often anadditional tag t1 can be replaced with a certain other tag t2 without effect on the trans-lation.
As an example, let T = ?blau-adjective?, t1 =?masculine?
and t2 =?feminine?.The two entries (?blau-adjective-masculine?|?blue?)
and (?blau-adjective-feminine?|?blue?
)are hints for detecting gender as nonrelevant when translating adjectives into English.Table 4 lists some of the most frequently identified candidates to be ignored whiletranslating: The gender of nouns is irrelevant for their translation (which is straight-forward, as the gender of a noun is unambiguous), as are the cases nominative, dative,accusative.
(For the genitive forms, the translation in English differs.)
For verbs thecandidates number and person were found: The translation of the first-person singularform of a verb, for example, is often the same as the translation of the third-personplural form.
Ignoring (dropping) those tags most often identified as irrelevant fortranslation results in the building of equivalence classes of words.
Doing so results ina smaller vocabulary, one about 65.5% the size of the vocabulary of the full lemma-tag representation of the Verbmobil corpus, for example?it is even smaller than thevocabulary of the original full-form corpus.The information described in this section is used to improve the quality of statis-tical machine translation and to better exploit the available bilingual resources.3.
Treatment of Structural DifferencesDifference in sentence structure is one of the main sources of errors in machine trans-lation.
It is thus promising to ?harmonize?
the word order in corresponding sentences.The presentation in this section focuses on the following aspects: question inversionand separated verb prefixes.
For a more detailed discussion of restructuring for statis-tical machine translation the reader is referred to Nie?en and Ney (2000, 2001).3.1 Question InversionIn many languages, the sentence structure of questions differs from the structure indeclarative sentences in that the order of the subject and the corresponding finite verbis inverted.
From the perspective of statistical translation, this behavior has some dis-187Nie?en and Ney SMT with Scarce Resourcesadvantages: The algorithm for training the parameters of the target language modelPr(eI1), which is typically a standard n-gram model, cannot deduce the probabilityof a word sequence in an interrogative sentence from the corresponding declarativeform.
The same reasoning is valid for the lexical translation probabilities of multiword-phrase pairs.
To harmonize the word order of questions with the word order in declar-ative sentences, the order of the subject (including the appendant articles, adjectivesetc.)
and the corresponding finite verb is inverted.
In English questions supportingdos are removed.
The application of the described preprocessing step in the bilingualtraining corpus implies the necessity of restoring the correct forms of the translationsproduced by the machine translation algorithm.
This procedure was suggested byBrown et al (1992) for the language pair English and French, but they did not re-port on experimental results revealing the effect of the restructuring on the translationquality.3.2 Separated Verb PrefixesGerman prefix verbs consist of a main part and a detachable prefix, which can beshifted to the end of the clause.
For the automatic alignment process, it is often dif-ficult to associate one English word with more than one word in the correspondingGerman sentence, namely, the main part of the verb and the separated prefix.
To solvethe problem of separated prefixes, all separable word forms of verbs are extractedfrom the training corpus.
The resulting list contains entries of the form prefix|main.In all clauses containing a word matching a main part and a word matching the cor-responding prefix part occurring at the end of the clause, the prefix is prepended tothe beginning of the main part.4.
Hierarchical Lexicon ModelsIn general, the probabilistic lexicon resulting from training the translation model con-tains all word forms occurring in the training corpus as separate entries, not takinginto account whether or not they are inflected forms of the same lemma.
Bearing inmind that typically more than 40% of the word forms are seen only once in training(see, for example, Table 5), it is obvious that for many words, learning the correcttranslations is difficult.
Furthermore, new input sentences are expected to contain un-known word forms, for which no translation can be retrieved from the lexicon.
Thisproblem is especially relevant for more-inflecting languages like German: Texts in Ger-man contain many more distinct word forms than their English translations.
Table 5also reveals that these words are often generated via inflection from a smaller set ofbase forms.4.1 A Hierarchy of Equivalence Classes of Inflected Word FormsAs mentioned in Section 2.3, the lemma-tag representation of the information frommorpho-syntactic analysis makes it possible to gradually access information with dif-ferent grades of abstraction.
Consider, for example, the German verb form ankomme,which is the indicative present first-person singular form of the lemma ankommen andcan be translated into English by arrive.
The lemma-tag representation provides an?observation tuple?
consisting of?
the original full word form (e.g., ankomme),?
morphological and syntactic tags (part of speech, tense, person, case, .
.
.
)(e.g., verb, indicative, present tense, 1st person singular), and188Computational Linguistics Volume 30, Number 2?
the base form (e.g., ankommen).In the following, ti0 = t0, .
.
.
, ti denotes the representation of a word where the baseform t0 and i additional tags are taken into account.
For the example above, t0 =ankommen, t1 = verb, and so on.
The hierarchy of equivalence classes F0, .
.
.
,Fn is asfollows:Fn = F(tn0) = ankommen verb indicative present singular 1Fn?1 = F(tn?10 ) = ankommen verb indicative present singularFn?2 = F(tn?20 ) = ankommen verb indicative present...F0 = F(t0) = ankommenwhere n is the maximum number of morpho-syntactic tags.
The mapping from thefull lemma-tag representation back to inflected word forms is generally unambigu-ous; thus Fn contains only one element, namely, ankomme.
Fn?1 contains the formsankomme, ankommst, and ankommt; in Fn?2 the number (singular or plural) is ig-nored, and so on.
The largest equivalence class contains all inflected forms of thebase form ankommen.1 Section 4.2 introduces the concept of combining information atdifferent levels of abstraction.4.2 Log-Linear CombinationIn modeling for statistical machine translation, a hidden variable aJ1, denoting thehidden alignment between the words in the source and target languages, is usuallyintroduced into the string translation probability:Pr(f J1 |eI1) =?aJ1Pr(f J1, aJ1|eI1) =?aJ1Pr(aJ1|eI1) ?
Pr(fJ1 |aJ1, eI1) (1)In the following, Tj =(tn0)j denotes the lemma-tag representation of the jth word inthe input sentence.
The sequence TJ1 stands for the sequence of readings for the wordsequence f J1 and can be introduced as a new hidden variable:Pr(f J1 |aJ1, eI1) =?TJ1Pr(f J1, TJ1|aJ1, eI1) (2)which can be decomposed intoPr(f J1 |aJ1, eI1) =?TJ1J?j=1Pr(fj, Tj|f j?11 , Tj?11 , aJ1, eI1) (3)1 The order of omitting tags can be defined in a natural way depending on the part of speech.
Inprinciple this decision can also be left to the maximum-entropy training, when features for all possiblesets of tags are defined, but this would cause the number of parameters to explode.
As the experimentsin this work have been carried out only with up to three levels of abstraction as defined in Section 4.2,the set of tags of the intermediate level is fixed, and thus the priority of the tags needs not be specified.The relation between this equivalence class hierarchy and the suggestions in Section 2.4 is clear:Choosing candidates for morpho-syntactic tags not relevant for translation amounts to fixing a level inthe hierarchy.
This is exactly what has been done to define the intermediate level in Section 4.2.189Nie?en and Ney SMT with Scarce ResourcesLet T (fj) be the set of interpretations which are regarded valid readings of fj by themorpho-syntactic analyzers on the basis of the whole-sentence context f J1.
We assumethat the probability functions defined above yield zero for all other readings, that is,when Tj ?
T (fj).
Under the usual independence assumption, which states that theprobability of the translation of words depends only on the identity of the wordsassociated with each other by the word alignment, we getPr(f J1 |aJ1, eI1) =?TJ1Tj ?
T (fj)J?j=1p(fj, Tj|eaj) (4)As has been argued in Section 2.2, the number of readings |T (fj)| per word form canbe reduced to one for the tasks for which experimental results are reported here.The elements in equation (4) are the joint probabilities p(f , T|e) of f and the read-ings T of f given the target language word e. The maximum-entropy principle rec-ommends choosing for p the distribution which preserves as much uncertainty aspossible in terms of maximizing the entropy, while requiring p to satisfy constraintswhich represent facts known from the data.
These constraints are encoded on the basisof feature functions hm(x), and the expectation of each feature hm over the model p isrequired to be equal to the observed expectation.
The maximum-entropy model canbe shown to be unique and to have an exponential form involving a weighted sumover the feature functions hm (Ratnaparkhi 1997).
In equation (5), the notation tn0 isused again for the lemma-tag representation of an input word (this was denoted by Tin equations (2)?
(4) for notational simplicity):p(f , T|e) = p?
(f , tn0 |e) =exp[?m?mhm(e, f , tn0)]?f?
,?tn0exp[?m?mhm(e, f?
, t?n0)] (5)where ?
= {?m} is the set of model parameters with one weight ?m for each featurefunction hm.
These model parameters can be trained using converging iterative trainingprocedures like the ones described by Darroch and Ratcliff (1972) or Della Pietra, DellaPietra, and Lafferty (1995).In the experiments presented in this article, the sum over the word forms f?
andthe readings t?n0 in the denominator of equation (5) is restricted to the readings of wordforms having the same base form and partial reading as a word form f ??
aligned atleast once to e.The new lexicon model p?
(f , tn0 |e) can now replace the usual lexicon model p(f |e),over which it has the following main advantages:?
The decomposition of the modeled events into feature functions allowsmeaningful probabilities to be provided for word forms that have notoccurred during training as long as the feature functions involved arewell-defined.
(See also the argument later in the article and the definitionof first-level and second-level feature functions presented inSection 4.2.1.)?
Introducing the hidden variable T = tn0 and constraining the lexiconprobability to be zero for interpretations considered nonvalid readings of190Computational Linguistics Volume 30, Number 2f (that is, for tn0 ?
T (f )) amounts to making context information from thecomplete sentence f J1 locally available: The sentence context was takeninto account by the morpho-syntactic analyzer, which chose the validreadings T (f ).4.2.1 Definition of Feature Functions.
There are numerous possibilities for definingfeature functions.
We do not need to require that they all have the same parametricform or that the components be disjoint and statistically independent.
Still, it is nec-essary to restrict the number of parameters so that optimizing them is practical.
Weused the following types of feature functions, which have been defined on the basisof the lemma-tag representation (see Section 2.3):First level: m = {L, e?
}, where L is the base form:h1L,?e(e, f , tn0) ={1 if e = e?
and t0 = L and f ?
F(tn0) (?
)0 otherwiseSecond level: m = {T, L, e?
}, with subsets T of cardinality ?
n of morpho-syntactictags considered relevant (see Section 2.4 for a description of the detectionof relevant tags):h2T,L,?e(e, f , tn0) ={1 if (?)
and T ?
tn1 (??
)0 otherwiseThird level: m = {F, T, L, e?
}, with the fully inflected original word form F:h3F,T,L,?e(e, f , tn0) ={1 if (??)
and F = f0 otherwiseIn terms of the hierarchy introduced in Section 4.1, this means that information at threedifferent levels in the hierarchy is combined.
The subsets T of relevant tags mentionedpreviously fix the intermediate level.2 This choice of the types of features as well asthe choice of the subsets T is reasonable but somewhat arbitrary.
Alternatively onecan think of defining a much more general set of features and applying some methodof feature selection, as has been done, for example, by Foster (2000), who compareddifferent methods for feature selection within the task of translation modeling forstatistical machine translation.
Note that the log-linear model introduced here uses oneparameter per feature.
For the Verbmobil task, for example, there are approximately162, 000 parameters: 47,800 for the first-order features, 55,700 for the second-orderfeatures, and 58,500 for the third-order features.
No feature selection or threshold wasapplied: All features seen in training were used.4.2.2 Training Procedure.
The overall process of training and testing with hierarchicallexicon models is depicted in Figure 1.
This figure includes the possibility of usingrestructuring operations as suggested in Section 3 in order to deal with structural dif-ferences between the languages involved.
This can be especially advantageous in thecase of multiword phrases which jointly fulfill a syntactic function: Not merging them2 Of course, there is not only one set of relevant tags, but at least one per part of speech.
In order tokeep the notation as simple as possible, this fact is not accounted for in the formulas and the textualdescriptions.191Nie?en and Ney SMT with Scarce Resourcestraining ofalignmentrestructuringtraining oflanguage modellanguagemodelalignmentmodelalignmentextractevent countslexiconmodelME trainingsourcesentencesearch foroptimal translationoutputsentenceanalyzeannotationall readingsof vocabularyrestructuringrestructuring inverserestructuringannotationvocabularysupported in testannotationsourceparttargetparttraining corpusFigure 1Training and test with hierarchical lexicon.
?
(Inverse) restructuring,?
?analyze,?
and?annotation?
all require morpho-syntactic analysis of the transformed sentences.would raise the question of how to distribute the syntactic tags which have been asso-ciated with the whole phrase.
In Section 5.2 we describe a method of learning multi-word phrases using conventional dictionaries.
The alignment on the training corpusis trained using the original source language corpus containing inflected word forms.This alignment is then used to count the co-occurrences of the annotated ?words?
inthe lemma-tag representation of the source language corpus with the words in the tar-get language corpus.
These event counts are used for the maximum-entropy trainingof the model parameters ?.The probability mass is distributed over (all readings of) the source languageword forms to be supported for test (not necessarily restricted to those occurring dur-ing training).
The only precondition is that the firing features for these unseen eventsare known.
This ?vocabulary supported in test,?
as it is called in Figure 1, can be apredefined closed vocabulary, as is the case in Verbmobil, in which the output of aspeech recognizer with limited output vocabulary is to be translated.
In the easiestcase it is identical to the vocabulary found in the source language part of the trainingcorpus.
The other extreme would be an extended vocabulary containing all automati-cally generated inflected forms of all base forms occurring in the training corpus.
Thisvocabulary is annotated with morpho-syntactic tags, ideally under consideration of allpossible readings of all word forms.192Computational Linguistics Volume 30, Number 2To enable the application of the hierarchical lexicon model, the source languageinput sentences in test have to be analyzed and annotated with their lemma-tag rep-resentation before the actual translation process.
So far, the sum over the readings inequation (4) has been ignored, because when the techniques for reducing the amountof ambiguity described in Section 2.2 and the disambiguated conventional dictionariesresulting from the approach presented in Section 5.1 are applied, there remains almostalways only one reading per word form.5.
Conventional DictionariesConventional dictionaries are often used as additional evidence to better train themodel parameters in statistical machine translation.
The expression conventional dictio-nary here denotes bilingual collections of word or phrase pairs predominantly collected?by hand,?
usually by lexicographers, as opposed to the probabilistic lexica, which arelearned automatically.
Apart from the theoretical problem of how to incorporate ex-ternal dictionaries in a mathematically sound way into a statistical framework formachine translation (Brown, Della Pietra, Della Pietra, and Goldsmith 1993) there arealso some pragmatic difficulties: As discussed in Section 2.2, one of the disadvantagesof these conventional dictionaries as compared to full bilingual corpora is that theirentries typically contain single words or short phrases on each language side.
Conse-quently, it is not possible to distinguish among the translations for different readingsof a word.
In normal bilingual corpora, the words can often be disambiguated bytaking into account the sentence context in which they occur.
For example, from thecontext in the sentence Ich werde die Zimmer buchen, it is possible to infer that Zimmerin this sentence is plural and has to be translated by rooms in English, whereas thecorrect translation of Zimmer in the sentence Ich ha?tte gerne ein Zimmer is the singularform room.
The dictionary used by our research group for augmenting the bilingualdata contains two entries for Zimmer: (?Zimmer?|?room?)
and (?Zimmer?|?rooms?
).5.1 Disambiguation without ContextThe approach described in this section is based on the observation that in many of thecases of ambiguous entries in dictionaries, the second part of the entry?that is, theother-language side?contains the information necessary to decide upon the interpre-tation.
In some other cases, the same kind of ambiguity is present in both languages,and it would be possible and desirable to associate the (semantically) correspondingreadings with one another.
The method proposed here takes advantage of these factsin order to disambiguate dictionary entries.Figure 2 sketches the procedure for the disambiguation of a conventional dictio-nary D. In addition to D, a bilingual corpus C1 of the same language pair is requiredto train the probability model for tag sequence translations.
The word forms in C1need not match those in D. C1 is not necessarily the training corpus for the translationtask in which the disambiguated version of D will be used.
It does not even have tobe taken from the same domain.A word alignment between the sentences in C1 is trained with some automaticalignment algorithm.
Then the words in the bilingual corpus are replaced by a reducedform of their lemma-tag representation, in which only a subset of their morpho-syntactictags is retained?even the base form is dropped.
The remaining subset of tags, inthe following denoted by Tf for the source language and Te for the target language,consists of tags considered relevant for the task of aligning corresponding readings.This is not necessarily the same set of tags considered relevant for the task of translationwhich was used, for example, to fix the intermediate level for the log-linear lexicon193Nie?en and Ney SMT with Scarce Resourcesmerge phraseslearn phrasesconventionaldictionaryanalyzeannotationall readingsof entriesalign correspondingreadingstag translationprobabilitiesdisambiguateddictionarytraining ofalignmentalignmentcount tagco-occurencesannotationbilingual corpusFigure 2Disambiguation of conventional dictionaries.
?Learn phrases,?
?analyze,?
and ?annotation?require morpho-syntactic analysis of the transformed sentences.combination in Section 4.2.1.
In the case of the Verbmobil corpus, the maximum lengthof a tag sequence is five.The alignment is used to count the frequency of a certain tag sequence tf in thesource language to be associated with another tag sequence te in the target languageand to compute the tag sequence translation probabilities p(tf |te) as relative frequen-cies.
For the time being, these tag sequence translation probabilities associate readingsof words in one language with readings of words in the other language: Multiwordsequences are not accounted for.To alleviate this shortcoming it is possible and advisable to automatically detectand merge multiword phrases.
As will be described in Section 5.2, the conventionalbilingual dictionary itself can be used to learn and validate these phrases.
The resultingmultiword phrases Pe for the target language and Pf for the source language areafterwards concatenated within D to form entries consisting of pairs of ?units.
?The next step is to analyze the word forms in D and generate all possible readingsof all entries.
It is also possible to ignore those readings that are considered unlikelyfor the task under consideration by applying the domain-specific preference rulesproposed in Section 2.2.
The process of generating all readings includes replacing wordforms with their lemma-tag representation, which is thereafter reduced by droppingall morpho-syntactic tags not contained in the tag sets Tf and Te.Using the tag sequence translation probabilities p(tf |te), the readings in one lan-guage are aligned with readings in the other language.
These alignments are applied tothe full lemma-tag representation (not only tags in Tf and Te) of the expanded dictio-nary containing one entry per reading of the original word forms.
The highest-rankingaligned readings according to p(tf |te) for each lemma are preserved.194Computational Linguistics Volume 30, Number 2The resulting disambiguated dictionary contains two entries for the German wordZimmer: (?Zimmer-noun-sg.?|?room-noun-sg.?)
and (?Zimmer-noun-pl.?|?room-noun-pl.?).
The target language part is then reduced to the surface forms: (?Zimmer-noun-sg.?|?room?)
and (?Zimmer-noun-pl.?|?rooms?).
Note that this augmented dictionary, in thefollowing denoted by D?, has more entries than D as a result of the step of generatingall readings.
The two entries (?beabsichtigt?|?intends?)
and (?beabsichtigt?|?intended?
),for example, produce three new entries: (?beabsichtigt-verb-ind.-pres.-sg.-3rd?|?intends?
), (?beabsichtigt-verb-past-part.?|?intended?
), and (?beabsichtigt-adjective-pos.?|?intended?
).5.2 Multiword PhrasesSome recent publications deal with the automatic detection of multiword phrases (Ochand Weber 1998; Tillmann and Ney 2000).
These methods are very useful, but they haveone drawback: They rely on sufficiently large training corpora, because they detectthe phrases from automatically learned word alignments.
In this section a method fordetecting multiword phrases is suggested which merely requires monolingual syntacticanalyzers and a conventional dictionary.Some multiword phrases which jointly fulfill a syntactic function are providedby the analyzers.
The phrase irgend etwas (?anything?
), for example, may form eitheran indefinite determiner or an indefinite pronoun.
irgend=etwas is merged by theanalyzer in order to form one single vocabulary entry.
In the German part of the Verb-mobil training corpus 26 different, nonidiomatic multiword phrases are merged, whilethere are 318 phrases suggested for the English part.
In addition, syntactic informa-tion like the identification of infinitive markers, determiners, modifying adjectives (forexample, single room), premodifying adverbials (more comfortable), and premodifyingnouns (account number) are used for detecting multiword phrases.
When applied tothe English part of the Verbmobil training corpus, these hints suggest 7,225 differentphrases.Altogether, 26 phrases for German and about 7,500 phrases for English are detectedin this way.
It is quite natural that there are more multiword phrases found for English,as German, unlike English, uses compounding.
But the experiments show that it is notadvantageous to use all these phrases for English.
Electronic dictionaries can be usefulfor detecting those phrases which are important in a statistical machine translationcontext: A multiword phrase is considered useful if it is translated into a single wordor a distinct multiword phrase (suggested in a similar way by syntactic analysis) inanother language.
There are 290 phrases chosen in this way for the English language.6.
Overall Procedure for Training with Scarce ResourcesTaking into account the interdependencies of inflected forms of the same base formis especially relevant when inflected languages like German are involved and whentraining data are sparse.
In this situation many of the inflected word forms to accountfor in test do not occur during training.
Sparse bilingual training data also make ad-ditional conventional dictionaries especially important.
Enriching the dictionaries byaligning corresponding readings is particularly useful when the dictionaries are usedin conjunction with a hierarchical lexicon, which can access the information neces-sary to distinguish readings via morpho-syntactic tags.
The restructuring operationsdescribed in Section 3 also help in coping with the data sparseness problem, becausethey make corresponding sentences more similar.
This section proposes a procedurefor combining all these methods in order to improve the translation quality despitesparseness of data.
Figure 3 sketches the proposed procedure.195Nie?en and Ney SMT with Scarce Resourceslearn phrasesdisambiguateddictionary D?conventionaldictionary Dbil.
corpusC1disambiguatedictionaryrestructuring restructuringmonolingualcorpusLMtraininglanguagemodelrestructuringannotationtrainalignmentcombinedcorpustrain hierarch.lexicon alignmenton combinedcorpusalignmentmodellexiconmodelbil.
corpusC2Figure 3Training with scarce resources.
?Restructuring,?
?learn phrases,?
and ?annotation?
all requiremorpho-syntactic analysis of the transformed sentences.Two different bilingual corpora C1 and C2, one monolingual target language cor-pus, and a conventional bilingual dictionary D can contribute in various ways to theoverall result.
It is important to note here that C1 and C2 can, but need not, be dis-tinct, and that the monolingual corpus can be identical to the target language part ofC2.
Furthermore these corpora can be taken from different domains, and C1 can be(very) small.
Only C2 has to represent the domain and the vocabulary for which thetranslation system is built, and only the size of C2 and the monolingual corpus havea substantial effect on the translation quality.
It is interesting to note, though, thata basic statistical machine translation system with an accuracy near 50% can be builtwithout any domain-specific bilingual corpus C2, solely on the basis of a disambiguateddictionary and the hierarchical lexicon models, as Table 9 shows.?
In the first step, multiword phrases are learned and validated on thedictionary D in the way described in Section 5.2.
These multiwordphrases are concatenated in D. Then an alignment is trained on the firstbilingual corpus C1.
On the basis of this alignment, the tag sequencetranslation probabilities which are needed to align correspondingreadings in the dictionary are extracted, as proposed in Section 5.1.
Theresult of this step is an expanded and disambiguated dictionary D?.
Forthis purpose, C1 does not have to cover the vocabulary of D. Besides C1196Computational Linguistics Volume 30, Number 2can be comparatively small, given the limited number of tag sequencepairs (tf |te) for which translation probabilities must be provided: In theVerbmobil training corpus, for example, there are only 261 differentGerman and 110 different English tag sequences.?
In the next step, the second bilingual corpus C2 and D?
are combined,and a word alignment A for both is trained.
C2, D?, and A are presentedas input to the maximum-entropy training of a hierarchical lexiconmodel as described in Section 4.2.?
The language model can be trained on a separate monolingual corpus.As monolingual data are much easier and cheaper to compile, thiscorpus might be (substantially) larger than the target language part of C2.7.
Experimental Results7.1 The Tasks and the CorporaTests were carried out on Verbmobil data and on Nespole!
data.
As usual, the sentencesfrom the test sets were not used for training.
The training corpora were used fortraining the parameters of IBM model 4.7.1.1 Verbmobil.
Verbmobil was a project for automatic translation of spontaneouslyspoken dialogues.
A detailed description of the statistical translation system withinVerbmobil is given by Ney et al (2000) and by Och (2002).
Table 5 summarizes thecharacteristics of the English and German parallel corpus used for training the param-eters of IBM model 4.
A conventional dictionary complements the training corpus (seeTable 6 for the statistics).
The vocabulary in Verbmobil was considered closed: Thereare official lists of word forms which can be produced by the speech recognizers.
Suchlists exist for German and English (see Table 7).
Table 8 lists the characteristics of thetwo test sets Test and Develop taken from the end-to-end evaluation in Verbmobil, thedevelopment part being meant to tune system parameters on a held-out corpus dif-ferent from the training as well as the test corpus.
As no parameters are optimized onthe development set for the methods described in this article, most of the experimentswere carried out on a joint set containing both test sets.Table 5Statistics of corpora for training: Verbmobil and Nespole!
Singletons are types occurring onlyonce in training.Verbmobil Nespole!English German English GermanNumber of sentences 58,073 58,073 3,182 3,182Number of distinct sentences 57,731 57,771 1,758 1,767Number of running word forms 549,921 519,523 15,568 14,992Number of running word forms without punctuation 453,612 418,974 12,461 11,672Number of word forms 4,673 7,940 1,034 1,363Number of singleton word forms 1,698 3,453 403 641Number of base forms 3,639 6,063 1,072 870Number of singleton base forms 1,236 2,546 461 326197Nie?en and Ney SMT with Scarce ResourcesTable 6Conventional dictionary used to complement thetraining corpus.English GermanNumber of entries 10,498 10,498Number of running word forms 15,305 12,784Number of word forms 5,161 7,021Number of base forms 3,666 5,479Table 7The official vocabularies in Verbmobil.English GermanNumber of word forms 6,871 10,157Number of base forms 3,268 6,667Table 8Statistics for the test sets for German to English translation: VerbmobilEval-2000 (Test and Develop) and Nespole!Verbmobil Nespole!Test DevelopNumber of sentences 251 276 70Number of running word forms in German part 2,628 3,159 456Number of word forms in German part 429 434 180Trigram LM perplexity of reference translation 30.5 28.1 76.97.1.2 Nespole!.
Nespole!
is a research project that ran from January 2000 to June 2002.It aimed to provide multimodel support for negotiation (Nespole!
2000; Lavie et al2001).
Table 5 summarizes the corpus statistics of the Nespole!
training set.
Table 8provides the corresponding figures for the test set used in this work.7.2 The Translation SystemFor testing we used the alignment template translation system, described in Och,Tillmann, and Ney (1999).
Training the parameters for this system entails training ofIBM model 4 parameters in both translation directions and combining the resultingalignments into one symmetrized alignment.
From this symmetrized alignment, thelexicon probabilities as well as the so-called alignment templates are extracted.
Thelatter are translation patterns which capture phrase-level translation pairs.7.3 Performance MeasuresThe following evaluation criteria were used in the experiments:BLEU (Bilingual Evaluation Understudy): This score, proposed by Papineni etal.
(2001), is based on the notion of modified n-gram precision, withn ?
{1, .
.
.
, 4}: All candidate unigram, bigram, trigram, and four-gramcounts are collected and clipped against their corresponding maximumreference counts.
The reference n-gram counts are calculated on a corpus198Computational Linguistics Volume 30, Number 2of reference translations for each input sentence.
The clipped candidatecounts are summed and normalized by the total number of candidate n-grams.
The geometric mean of the modified precision scores for a testcorpus is calculated and multiplied by an exponential brevity penalty fac-tor to penalize too-short translations.
BLEU is an accuracy measure, whilethe others are error measures.m-WER (multireference word error rate): For each test sentence there is a set ofreference translations.
For each translation hypothesis, the edit distance(number of substitutions, deletions, and insertions) to the most similarreference is calculated.SSER (subjective sentence error rate): Each translated sentence is judged by ahuman examiner according to an error scale from 0.0 (semantically andsyntactically correct) to 1.0 (completely wrong).ISER (information item semantic error rate): The test sentences are segmentedinto information items; for each of these items, the translation candidatesare assigned either ?OK?
or an error class.
If the intended informationis conveyed, the translation of an information item is considered correct,even if there are slight syntactic errors which do not seriously deterioratethe intelligibility.For evaluating the SSER and the ISER, we have used the evaluation tool EvalTrans(Nie?en and Leusch 2000), which is designed to facilitate the work of manually judgingevaluation quality and to ensure consistency over time and across evaluators.7.4 Impact of the Corpus SizeIt is a costly and time-consuming task to compile large texts and have them translatedto form bilingual corpora suitable for training the model parameters for statisticalmachine translation.
As a consequence, it is important to investigate the amount ofdata necessary to sufficiently cover the vocabulary expected in testing.
Furthermore,we want to examine to what extent the incorporation of morphological knowledgesources can reduce this amount of necessary data.
Figure 4 shows the relation betweenthe size of a typical German corpus and the corresponding number of different fullforms.
At the size of 520,000 words, the size of the Verbmobil corpus used for training,this curve still has a high growth rate.To investigate the impact of the size of the bilingual corpus available for train-ing, on translation quality three different setups for training the statistical lexicon onVerbmobil data have been defined:?
using the full training corpus as described in Table 5, comprising 58,000sentences?
restricting the corpus to 5,000 sentences (approximately every 11thsentence)?
using no bilingual training corpus at all (only a bilingual dictionary; seesubsequent discussion)The language model is always trained on the full English corpus.
The argument forthis is that monolingual corpora are always easier and less expensive to obtain thanbilingual corpora.
A conventional dictionary is used in all three setups to complement199Nie?en and Ney SMT with Scarce Resources0123456780 100 200 300 400 500corpus size [1000 words]vocabularysize[1000 wordforms]Figure 4Impact of corpus size (measured in number of running words in the corpus) on vocabularysize (measured in number of different full-form words found in the corpus) for the Germanpart of the Verbmobil corpus.the bilingual corpus.
In the last setup, the lexicon probabilities are trained exclusivelyon this dictionaryAs Table 9 shows, the quality of translation drops significantly when the amountof bilingual data available during training is reduced: When the training corpus isrestricted to 5,000 sentences, the SSER increases by about 7% and the ISER by about3%.
As could be expected, the translations produced by the system trained exclusivelyon a conventional dictionary are very poor: The SSER jumps over 60%.7.5 Results for Log-Linear Lexicon Combination7.5.1 Results on the Verbmobil Task.
As was pointed out in Section 4, the hierarchi-cal lexicon is expected to be especially useful in cases in which many of the inflectedword forms to be accounted for in test do not occur during training.
To systematicallyinvestigate the model?s generalization capability, it has been applied on the three dif-ferent setups described in Section 7.4.
The training procedure was the one proposedin Section 6, which includes restructuring transformations in training and test.
Table 9summarizes the improvement achieved for all three setups.Training on 58,000 sentences plus conventional dictionary: Compared to the ef-fect of restructuring, the additional improvement achieved with the hier-archical lexicon is relatively small in this setup.
The combination of allmethods results in a relative improvement in terms of SSER of almost13% and in terms of information ISER of more than 16% as compared tothe baseline.Training on 5,000 sentences plus conventional dictionary: Restructuring alonecan improve the translation quality from 37.3% to 33.6%.
The benefit fromthe hierarchical lexicon is larger in this setup, and the resulting in SSER is31.8%.
This is a relative improvement of almost 15%.
The relative improve-ment in terms of ISER is almost 22%.
Note that by applying the methods200Computational Linguistics Volume 30, Number 2Table 9Results for hierarchical lexicon models and translation with scarce resources.?Restructuring?
entails treatment of question inversion and separated verb prefixes aswell as merging of phrases in both languages.
A conventional dictionary is available inall three setups.
The language model is always trained on the full monolingual Englishcorpus.
Task: Verbmobil.
Testing on 527 sentences (Test and Develop).Number of sentencesfor training BLEU m-WER SSER ISER58,000 Baseline 53.7% 34.1% 30.2% 14.1%Restructuring 56.3 32.5 26.6 12.8+ dictionary disambiguated+ hierarchical lexicon 57.1 31.8 26.3 11.85,000 Baseline 47.4 38.0 37.3 17.4Restructuring 52.1 34.7 33.6 15.2+ dictionary disambiguated+ hierarchical lexicon 52.9 33.9 31.8 13.70 Baseline 23.3 53.6 60.4 29.8Restructuring 29.1 50.2 57.8 30.0+ dictionary disambiguated+ hierarchical lexicon 32.6 48.0 52.8 24.1proposed here, the corpus for training can be reduced to less than 10%of the original size while increasing the SSER only from 30.2% to 31.8%compared to the baseline when using the full corpus.Training only on conventional dictionary: In this setup the impact of the hierar-chical lexicon is clearly larger than the effect of the restructuring methods,because here the data sparseness problem is much more important thanthe word order problem.
The overall relative reduction in terms of SSERis 13.7% and in terms of ISER 19.1%.
An error rate of about 52% is stillvery poor, but it is close to what might be acceptable when only the gistof the translated document is needed, as is the case in the framework ofdocument classification or multilingual information retrieval.Examples taken from the Verbmobil Eval-2000 test set are given in Table 10.Smoothing the lexicon probabilities over the inflected forms of the same lemma en-ables the translation of sind as would instead of are.
The smoothed lexicon contains thetranslation convenient for any inflected form of bequem.
The comparative more conve-nient would be the completely correct translation.
The last two examples in the tabledemonstrate the effect of the disambiguating analyzer, which on the basis of the sen-tence context identifies Zimmer as plural (it has been translated into the singular formroom by the baseline system) and das as an article to be translated by the instead of apronoun which would be translated as that.
The last example demonstrates that over-fitting on domain-specific training can be problematic in some cases: Generally, becauseis a good translation for the co-ordinating conjunction denn, but in the appointment-scheduling domain, denn is often an adverb, and it often occurs in the same sentenceas dann, as in Wie wa?re es denn dann?.
The translation for this sentence is somethinglike How about then?.
Because of the frequency of this domain-specific language use,the word form denn is often aligned to then in the training corpus.
The hierarchical201Nie?en and Ney SMT with Scarce ResourcesTable 10Examples of the effect of the hierarchical lexicon.Input sind Sie mit einem Doppelzimmer einverstanden?Baseline are you agree with a double room?Hierarchical lexicon would you agree with a double room?Input mit dem Zug ist es bequemer.Baseline by train it is UNKNOWN-bequemer.Hierarchical lexicon by train it is convenient.Input wir haben zwei Zimmer.Baseline we have two room.Hierarchical lexicon we have two rooms.Input ich wu?rde das Hilton vorschlagen denn es ist das beste.Baseline I would suggest that Hilton then it is the best.Hierarchical lexicon I would suggest the Hilton because it is the best.lexicon distinguishes the adverb reading and the conjunction reading, and the correcttranslation because is the highest-ranking one for the conjunction.7.5.2 Results on the Nespole!
Task.
We were provided with a small German-Englishcorpus from the Nespole!
project (see Section 7.1 for a description).
From Table 5 itis obvious that this task is an example of very scarce training data, and it is thusinteresting to test the performance of the methods proposed in this article on thistask.
The same conventional dictionary as was used for the experiments on Verbmobildata (cf.
Table 6) complemented the small bilingual training corpus.
Furthermore, the(monolingual) English part of the Verbmobil corpus was used in addition to the Englishpart of the Nespole!
corpus for training the language model.
Table 11 summarizes theresults.
Information items have not been defined for this test set.
An overall relativeimprovement of 16.5% in the SSER can be achieved.8.
ConclusionIn this article we have proposed methods of incorporating morphological and syntacticinformation into systems for statistical machine translation.
The overall goal was toimprove translation quality and to reduce the amount of parallel text necessary toTable 11Results for hierarchical lexicon model Nespole!?Restructuring?
entails treatment of questioninversion and separated verb prefixes as well asmerging of phrases in both languages.
The sameconventional dictionary was used as in theexperiments the Verbmobil.
The language modelwas trained on a combination of the English partsof the Nespole!
corpus and the Verbmobil corpus.BLEU m-WER SSERBaseline 31.6% 50.2% 41.1%Restructuring 33.7 45.9 38.1+ hierarchical lexicon 36.5 44.1 34.3202Computational Linguistics Volume 30, Number 2train the model parameters.
Substantial improvements on the Verbmobil task and theNespole!
task were achieved.Some sentence-level restructuring transformations have been introduced whichare motivated by knowledge about the sentence structure in the languages involved.These transformations aim at the assimilation of word orders in related sentences.A hierarchy of equivalence classes has been defined on the basis of morpholog-ical and syntactic information beyond the surface forms.
The study of the effect ofusing information from either degree of abstraction led to the construction of hier-archical lexicon models, which combine different items of information in a log-linearway.
The benefit from these combined models is twofold: First, the lexical coverage isimproved, because the translation of unseen word forms can be derived by consider-ing information from lower levels in the hierarchy.
Second, category ambiguity can beresolved, because syntactical context information is made locally accessible by meansof annotation with morpho-syntactic tags.
As a side effect of the preparative work forsetting up the underlying hierarchy of morpho-syntactic information, those pieces ofinformation inherent in fully inflected word forms that are not relevant for translationare detected.A method for aligning corresponding readings in conventional dictionaries con-taining pairs of fully inflected word forms has been proposed.
The approach usesinformation deduced from one language side to resolve category ambiguity in thecorresponding entry in the other language.
The resulting disambiguated dictionar-ies have proven to be better suited for improving the quality of machine translation,especially if they are used in combination with the hierarchical lexicon models.The amount of bilingual training data required to achieve an acceptable quality ofmachine translation has been systematically investigated.
All the methods mentionedpreviously contribute to a better exploitation of the available bilingual data and thusto improving translation quality in frameworks with scarce resources.
Three setups fortraining the parameters of the statistical lexicon on Verbmobil data have been exam-ined: (1) Using the full 58,000 sentences comprising the bilingual training corpus, (2)restricting the corpus to 5,000 sentences, and (3) using only a conventional dictionary.For each of these setups, a relative improvement in terms of subjective sentence errorrate between 13% and 15% as compared to the baseline could be obtained using combi-nations of the methods described in this article.
The amount of bilingual training datacould be reduced to less than 10% of the original corpus, while losing only 1.6% inaccuracy as measured by the subjective sentence error rate.
A relative improvement of16.5% in terms of subjective sentence error rate could also be achieved on the Nespole!task.AcknowledgmentsThis work has been partially supported aspart of the Verbmobil project (contractnumber 01 IV 701 T4) by the GermanFederal Ministry of Education, Science,Research and Technology and as part of theEuTrans project (project number 30268) bythe European Union.
For the provision ofthe Nespole!
data we thank the Nespole!consortium, listed on the project?s homepage (Nespole!
2000).
Special thanks toAlon Lavie, Lori Levin, Stephan Vogel, andAlex Waibel (in alphabetical order).ReferencesAl-Onaizan, Yaser, Ulrich Germann, UlfHermjakob, Kevin Knight, Philipp Koehn,Daniel Marcu, and Kenji Yamada.
2000.Translating with scarce resources.
InProceedings of the 17th National Conference onArtificial Intelligence (AAAI), pages672?678, Austin, TX, August.Berger, Adam L., Peter F. Brown, Stephen A.Della Pietra, Vincent J. Della Pietra, J. R.Gillett, and A. S. Kehler.
1996.
Languagetranslation apparatus and method ofusing context-based translation models.United States Patent, Patent Number5510981, April.203Nie?en and Ney SMT with Scarce ResourcesBrown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and M. J.Goldsmith.
1993.
But dictionaries are datatoo.
In Proceedings of the ARPA HumanLanguage Technology Workshop ?93, pages202?205, Princeton, NJ, March.Brown, Peter F., John Cocke, Stephen A.Della Pietra, Vincent J. Della Pietra,Frederick Jelinek, John D. Lafferty,Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machinetranslation.
Computational Linguistics,16(2):79?85.Brown, Peter F., John Cocke, Stephen A.Della Pietra, Vincent J. Della Pietra,Frederick Jelinek, Robert L. Mercer, andPaul S. Roossin.
1988.
A statisticalapproach to language translation.
InProceedings of COLING 1988: The 12thInternational Conference on ComputationalLinguistics, pages 71?76, Budapest,August.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, John D. Lafferty,and Robert L. Mercer.
1992.
Analysis,statistical transfer, and synthesis inmachine translation.
In Proceedings of TMI1992: Fourth International Conference onTheoretical and Methodological Issues in MT,pages 83?100, Montreal, Quebec, Canada,June.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
Mathematics of statisticalmachine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Darroch, J. N. and D. Ratcliff.
1972.Generalized iterative scaling for log-linearmodels.
Annals of Mathematical Statistics,43:1470?1480.Della Pietra, Stephen A., Vincent J.Della Pietra, and John D. Lafferty.
1995.Inducing features of random fields.
Tech-nical Report CMU-CS-95-144, CarnegieMellon University, Pittsburgh, PA.Foster, George.
2000.
A maximumentropy/minimum divergence translationmodel.
In Proceedings of ACL 2000: The 38thAnnual Meeting of the Association forComputational Linguistics, pages 37?44,Hong Kong, October.Garc?
?a-Varea, Ismael and FranciscoCasacuberta.
2001.
Search algorithms forstatistical machine translation based ondynamic programming and pruningtechniques.
In Proceedings of the MTSummit VIII, pages 115?120, Santiago deCompostela, Spain, September.Germann, Ulrich, Michael Jahr, KevinKnight, Daniel Marcu, and Kenji Yamada.2001.
Fast decoding and optimal decodingfor machine translation.
In Proceedings ofACL-EACL 2001: The 39th Annual Meetingof the Association for ComputationalLinguistics (joint with EACL 2001), pages228?235, Toulouse, France, July.Kanevsky, Dimitri, Salim Roukos, and JanSedivy.
1997.
Statistical language modelfor inflected languages.
United StatesPatent, Patent Number 5835888.Karlsson, Fred.
1990.
Constraint grammar asa framework for parsing running text.
InProceedings of COLING 1990: The 13thInternational Conference on ComputationalLinguistics, volume 3, pages 168?173,Helsinki, August.Koehn, Philipp and Kevin Knight.
2001.Knowledge sources for word-leveltranslation models.
In Lillian Lee andDonna Harman, editors, Proceedings ofEMNLP 2001: Conference on EmpiricalMethods in Natural Language Processing,pages 27?35, Pittsburgh, PA, June.Larson, Martha, Daniel Willett, JoachimKo?hler, and Gerhard Rigoll.
2000.Compound splitting and lexical unitrecombination for improved performanceof a speech recognition system forGerman parliamentary speeches.
InProceedings ICSLP 2000: Sixth InternationalConference on Spoken Language Processing,volume 3, pages 945?948, Beijing,February.Lavie, Alon, Chad Langley, Alex Waibel,Fabio Pianesi, Gianni Lazzari, PaoloColetti, Loredana Taddei, and FrancoBalducci.
2001.
Architecture and designconsiderations in NESPOLE!
A speechtranslation system for e-commerceapplications.
In James Allan, editor,Proceedings of HLT 2001: First InternationalConference on Human Language TechnologyResearch, pages 31?39, San Diego, March.Maltese, G., and F. Mancini.
1992.
Anautomatic technique to includegrammatical and morphologicalinformation in a trigram-based statisticallanguage model.
In Proceedings of ICASSP1992: International Conference on Acoustics,Speech and Signal Processing, pages157?160, San Francisco, March.NESPOLE!
(NEgotiating through SPOkenLanguage in e-commerce).
2000 Projecthomepage.
Available at http://nespole.itc.it/.Ney, Hermann, Sonja Nie?en, Franz JosefOch, Hassan Sawaf, Christoph Tillmann,and Stephan Vogel.
2000.
Algorithms forstatistical translation of spoken language.IEEE Transactions on Speech and AudioProcessing, 8(1):24?36.204Computational Linguistics Volume 30, Number 2Nie?en, Sonja and Gregor Leusch.
2000.EvalTrans, a tool for semi-automaticevaluation of machine translation.
InProceedings of LREC 2000, Athens.
Tool isavailable at http://www-i6.Informatik.RWTH-Aachen.DE/?niessen/Evaluation/.Nie?en, Sonja and Hermann Ney.
2000.Improving SMT quality withmorpho-syntactic analysis.
In Proceedingsof COLING 2000: The 18th InternationalConference on Computational Linguistics,pages 1081?1085, Saarbru?cken, Germany,July.Nie?en, Sonja and Hermann Ney.
2001.Morpho-syntactic analysis for reorderingin statistical machine translation.
InProceedings of MT Summit VIII, pages247?252, Santiago de Compostela, Spain,September.Nie?en, Sonja, Stephan Vogel, HermannNey, and Christoph Tillmann.
1998.
A DPbased search algorithm for statisticalmachine translation.
In Proceedings ofCOLING-ACL 1998: The 36th AnnualMeeting of the Association for ComputationalLinguistics and the 17th InternationalConference on Computational Linguistics,pages 960?967, Montreal, Quebec,Canada, August.Och, Franz Josef.
2002.
Machine Translation:From Single-Word Models to AlignmentTemplates.
Ph.D. thesis, Computer ScienceDepartment, RWTH?University ofTechnology, Aachen, Germany.Och, Franz Josef, Christoph Tillmann, andHermann Ney.
1999.
Improved alignmentmodels for statistical machine translation.In Proceedings of EMNLP 1999: Conferenceon Empirical Methods in Natural LanguageProcessing, pages 20?28, University ofMaryland, College Park, June.Och, Franz Josef and Hans Weber.
1998.Improving statistical natural languagetranslation with categories and rules.
InProceedings of COLING-ACL 1998: The 36thAnnual Meeting of the Association forComputational Linguistics and the 17thInternational Conference on ComputationalLinguistics, pages 985?989, Montreal,Quebec, Canada, August.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2001.
Bleu: Amethod for automatic evaluation ofmachine translation.
Technical ReportRC22176 (W0109-022), IBM ResearchDivision, Yorktown Heights, NY,September.Ratnaparkhi, Adwait.
1997.
A simpleintroduction to maximum entropy modelsfor natural language processing.
TechnicalReport 97?08, Institute for Research inCognitive Science, University ofPennsylvania, Philadelphia, May.Tillmann, Christoph and Hermann Ney.2000.
Word re-ordering and DP-basedsearch in statistical machine translation.In Proceedings of COLING 2000: The 18thInternational Conference on ComputationalLinguistics, pages 850?856, Saarbru?cken,Germany, August.
