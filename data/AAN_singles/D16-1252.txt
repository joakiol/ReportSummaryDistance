Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2300?2305,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCreating a Large Benchmark for Open Information ExtractionGabriel Stanovsky and Ido DaganComputer Science Department,Bar-Ilan University, Ramat Gan, Israelgabriel.satanovsky@gmail.comdagan@cs.biu.ac.ilAbstractOpen information extraction (Open IE) waspresented as an unrestricted variant of tra-ditional information extraction.
It has beengaining substantial attention, manifested by alarge number of automatic Open IE extractorsand downstream applications.
In spite of thisbroad attention, the Open IE task definitionhas been lacking ?
there are no formal guide-lines and no large scale gold standard annota-tion.
Subsequently, the various implementa-tions of Open IE resorted to small scale post-hoc evaluations, inhibiting an objective and re-producible cross-system comparison.
In thiswork, we develop a methodology that lever-ages the recent QA-SRL annotation to createa first independent and large scale Open IE an-notation,1 and use it to automatically comparethe most prominent Open IE systems.1 IntroductionOpen Information Extraction (Open IE) was origi-nally formulated as a function from a document to aset of tuples indicating a semantic relation betweena predicate phrase and its arguments (Banko et al,2007).
Wu and Weld (2008) further defined that anOpen IE extractor should ?produce one triple for ev-ery relation stated explicitly in the text, but is notrequired to infer implicit facts?.
For example, giventhe sentence ?John managed to open the door?
anOpen IE extractor should produce the tuple (John;managed to open; the door) but is not required to pro-duce the extraction (John; opened; the door).1Publicly available at http://www.cs.biu.ac.il/nlp/resources/downloadsFollowing this initial presentation of the task,Open IE has gained substantial and consistent atten-tion.
Many automatic extractors were created (e.g.,(Fader et al, 2011; Mausam et al, 2012; Del Corroand Gemulla, 2013)) and were put to use in variousdownstream applications.In spite of this wide attention, Open IE?s for-mal definition is lacking.
There are no clear guide-lines as to what constitutes a valid proposition to beextracted, and subsequently there is no large scalebenchmark annotation.
Open IE evaluations there-fore usually consist of a post-hoc manual evaluationof a small output sample.This evaluation practice lacks in several respects:(1) Most works provide a precision oriented metric,whereas recall is often not measured, (2) the num-bers are not comparable across systems, as they usedifferent guidelines and datasets, and (3) the experi-ments are hard to replicate.In this work, we aim to contribute to the standard-ization of Open IE evaluation by providing a largegold benchmark corpus.
For that end, we first iden-tify consensual guiding principles across prominentOpen IE systems, resulting in a clearer formulationof the Open IE task.
Following, we find that the re-cent formulation of QA-SRL (He et al, 2015) in factsubsumes these requirements for Open IE.
This en-ables us to automatically convert the annotations ofQA-SRL to a high-quality Open IE corpus of morethan 10K extractions, 13 times larger than the previ-ous largest Open IE annotation.Finally, we automatically evaluate the perfor-mance of various Open IE systems against our cor-pus, using a soft matching criterion.
This is the first2300time such a comparative evaluation is performed ona large scale gold corpus.Future Open IE systems (and its applicative users)can use this large benchmark, along with the auto-matic evaluation measure, to easily compare theirperformance against previous baselines, alleviatingthe current need for ad-hoc evaluation.2 Background2.1 Open IEOpen Information Extraction (Open IE) was intro-duced as an open variant of traditional InformationExtraction (Etzioni et al, 2008).
As mentioned inthe Introduction, its primary goal is to extract coher-ent propositions from a sentence, each comprising ofa relation phrase and two or more argument phrases(e.g., (Barack Obama, born in, Hawaii)).
Since itsinception, Open IE has gained consistent attention,mostly used as a component within larger frame-works (Christensen et al, 2013; Balasubramanian etal., 2013).In parallel, many Open IE extractors were de-veloped.
TextRunner (Banko et al, 2007) andWOE (Wu and Weld, 2010) take a self-supervisedapproach over automatically produced dependencyparses.
Perhaps more dominant is the rule based ap-proach taken by ReVerb (Fader et al, 2011), OLLIE(Mausam et al, 2012), KrakeN (Akbik and Lo?ser,2012) and ClausIE (Del Corro and Gemulla, 2013).Two recent systems take a semantically-orientedapproach.
Open IE-42 uses semantic role labeling toextract tuples, while Stanford Open Information Ex-traction (Angeli et al, 2015) uses natural logic infer-ence to arrive at shorter, more salient, arguments.Recently, Stanovsky et al (2016b) presentedPropS, a proposition oriented representation, ob-tained via conversion rules from dependency trees.Performing Open IE extraction over PropS struc-tures is straightforward ?
follow the clearly markedpredicated nodes to their direct arguments.Contrary to the vast interest in Open IE, its taskformulation has been largely overlooked.
There arecurrently no common guidelines defining a valid ex-traction, which consequently hinders the creation ofan evaluation benchmark for the task.
Most Open2https://github.com/knowitall/openieIE extractors3 evaluate performance by manually ex-amining a small sample of their output.
Table 1 sum-marizes the evaluations taken by the most prominentOpen IE systems.2.2 QA-SRLSemantic Role Labeling (SRL) (Carreras andMa`rquez, 2005) is typically perceived as answer-ing argument role questions, such as who, what, towhom, when, or where, regarding a target predicate.For instance, PropBank?s ARG0 for the predicatesay answers the question ?who said something?
?.QA-SRL (He et al, 2015) suggests that answeringexplicit role questions is an intuitive means to solicitpredicate-argument structures from non-expert an-notators.
Annotators are presented with a sentencein which a target predicate4 was marked, and are re-quested to annotate argument role questions and cor-responding answers.Consider the sentence ?Giles Pearman, Mi-crosoft?s director of marketing, left his job?
and thetarget predicate left.
The QA-SRL annotation con-sists of the following pairs: (1) Who left something?
{Giles Pearman; Microsoft?s director of market-ing} and (2) what did someone leave?
his job.5He et al assessed the validity of QA-SRLby annotating 3200 sentences from PropBank andWikipedia, showing high agreement with the Prop-Bank annotations.
In the following section we au-tomatically derive an Open IE benchmark from thisQA-SRL annotation.3 Creating an Open IE Benchmark3.1 Open IE GuidelinesBefore creating a generic benchmark for evaluat-ing Open IE systems, it is first needed to obtain aclearer specification of the common task that theyaddress.
Despite some nuances, we identified thefollowing core aspects of the Open IE task as con-sensual across all systems mentioned in Section 2:3Except for (Wu and Weld, 2010) who evaluated recall.4Currently consisting of automatically annotated verbs.5Three cases give rise to multiple answers for the same ques-tion: appositives (as illustrated in this example), co-reference(?Jimmy Hendrix played the guitar, he was really good at it?
),and distributive coordinations (?Bob and Mary were born inAmerica?
).2301System #Sentences Genre Metric #Annot.
AgreementTextRunner 400 Web % Correct 3 -WOE 300 Web, Wiki, News Precision / Recall 5 -ReVerb 500 Web Precision / AUC 2 86%, .68 kKrakeN 500 Web % Correct 2 87%Ollie 300 News, Wiki, Biology Precision/YieldAUC 2 96%ClauseIE 300 Web, Wiki, News Precision/Yield 2 57% / 68% / 63%Table 1: The post-hoc evaluation metrics taken by the different systems described in Section 2.
In contrast,Stanford Open IE and PropS took an extrinsic evaluation approach.Assertedness Extracted propositions should beasserted by the original sentence.
For example,given the sentence ?Sam succeeded in convincingJohn?, ReVerb and ClausIE produce the extraction:(Sam; succeeded in convincing; John).
Most Open IEsystems do not attempt to recover implied embeddedpropositions (e.g., (Sam; convinced; John)), but ratherinclude matrix verbs (e.g., succeeded) in the predi-cate slot.
Other elements that affect assertedness,like negations and modals, are typically included inthe predicate slot as well (e.g.
(John; could not join;the band)).Minimal propositions Open IE systems aim to?break down?
a sentence into a set of small isolatedpropositions.
Accordingly, the span of each individ-ual proposition, and hence the span of each of itspredicate and argument slots, should be as minimalas possible, as long as the original information (truthconditions) is preserved.
For example, this leadsto splitting distributive coordination in the sentence?Bell distributes electronic and building products?,for which ClausIE produces: (Bell, distributes, elec-tronic products) and (Bell, distributes, building prod-ucts).
Having shorter entities as Open IE argumentswas further found to be useful in several semantictasks (Angeli et al, 2015; Stanovsky et al, 2015).Completeness and open lexicon Open IE systemsaim to extract all asserted propositions from a sen-tence.
In practice, most current Open IE systemslimit their scope to extracting verbal predicates, butconsider all possible verbs without being bound to apre-specified lexicon.3.2 From QA-SRL to Open IESRL and Open IE have been defined with differ-ent objectives.
Particularly, SRL identifies argumentrole labels, which is not addressed in Open IE.
Yet,the two tasks overlap as they both need to recoverpredicate-argument structures in sentences.
We nowexamine the above Open IE requirements and sug-gest that while they are only partly embedded withinSRL structures, they can be fully recovered fromQA-SRL.Asserted (matrix) propositions appear in SRL asnon-embedded predicates (e.g., succeeded in the?Sam succeeded to convince John?).
However,SRL?s predicates are grounded to a lexicon such asPropBank (Palmer et al, 2005) or FrameNet (Bakeret al, 1998), which violates the completeness andopen lexicon principle.
Further, in contrast to theminimal propositions principle, arguments in SRLannotations are inclusive, each marked as full sub-trees in a syntactic parse.Yet, QA-SRL seems to bridge this gap betweentraditional SRL structures and Open IE require-ments.
Its predicate vocabulary is open, and itsquestion-answer format solicits minimal proposi-tions, as was found in a recent study by (Stanovskyet al, 2016a).
This correlation suggests that the QA-SRL methodology is in fact also an attractive meansfor soliciting Open IE extractions from non-expertsannotators.
Evidently, it enables automatically de-riving high quality Open IE annotations from (cur-rent or future) QA-SRL gold annotations, as de-scribed in the following section23023.3 Generating Open-IE ExtractionsFormally, we extract an Open-IE dataset from theQA-SRL dataset by the following algorithm, whichis illustrated in more detail further below:1.
Given:?
s - a sentence from the QA-SRL dataset.?
p - a predicate in s.?
tq1, ..., qnu - a list of questions over p.?
tta1,1, ..., a1,l1u, ...tan,1, ..., an,lnuu - alist of sets of corresponding answers,where question qi has li answers.2.
If p is a non-embedded (matrix) verb:(a) Remove answers which are composedonly of pronouns, as these are not ex-pected to be extracted by Open-IE (and ac-cordingly adjust the li?s).
(b) Return extractions composed of pand every combination of answers intta1,1, ..., a1,l1u ?
... ?
tan,1, ..., an,lnuu(the Cartesian product of the answers).This process results in a list of l1 ?
l2 ?
... ?
lnOpen IE extractions.For example, consider the sentence: ?BarackObama, the U.S. president, was determined to winthe majority vote in Washington and Arizona?.
Thequestions corresponding to the predicate determineare: {who was determined?, what was someone de-termined to do?
}, and the corresponding answersets are: {{?Barack Obama?, ?the U.S president?
},{?win the majority vote in Washington?, ?win themajority vote in Arizona?
}}.Following, our algorithm will produce these OpenIE extractions: (Barack Obama; was determined; towin the majority vote in Washington), (the U.S. presi-dent; was determined; to win the majority vote in Wash-ington), (Barack Obama; was determined; to win themajority vote in Arizona), and (the U.S. president; wasdetermined; to win the majority vote in Arizona).Note that we do not produce extractions for em-bedded predicates (e.g., win) to conform with theassertedness principle, as discussed earlier.With respect to pronoun removal (step 2(a)), wewould remove the pronoun ?he?
as the answer to thequestion who was tired?
in ?John went home, he wasCorpus WSJ WIKI ALL#Sentences 1241 1959 3200#Predicates 2020 5690 7710#Questions 8112 10798 18910#Extractions 4481 5878 10359Table 2: Corpus statistics.System #ExtractionsWSJ WIKI ALLStanford 6423 14104 20527ClausIE 5295 8265 13560Open IE4 3634 5113 8747OLLIE 2976 5250 8226PropS 2852 4990 7842ReVerb 1624 2552 4716Table 3: The yield of the different Open IE systems.tired?.
Notice that in this sentence ?John?
would bea second answer for the above question, yielding theextraction (John; was tired).
When the only answer toa question is a pronoun this question will be ignoredin the extraction process, since the QA-SRL corpusdoes not address cross-sentence co-references.
Thisissue may be addressed in future work.Applying this process to the QA-SRL corpusyielded a total of 10,359 Open IE extractionsover 3200 sentences from 2 domains (see Ta-ble 2).
This corpus is about 13 times largerthan the previous largest annotated Open IE cor-pus (Fader et al, 2011).
The corpus is avail-able at: http://www.cs.biu.ac.il/nlp/resources/downloads.Corpus validation We assess the validity of ourdataset by performing expert annotation6 of OpenIE extractions, following the principles discussed inSection 3.1, for 100 random sentences.
We findthat our benchmark extractions, derived automati-cally from QA-SRL, highly agree with the expertannotation, reaching 95.8 F1 by the head-agreementcriterion defined in the next section.23030.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0RecallPrecisionOLLIEReVerbClausIEOpenIE-4PropSStanfordFigure 1: Precision-recall curve for the differentOpen IE systems on our corpus (see discussion inSection 4).4 Comparative EvaluationIn this section, we illustrate the utility of our newcorpus by testing the performance of 6 promi-nent Open IE systems: OpenIE-4, ClausIE, OLLIE,PropS, Stanford, and ReVerb (see Section 2).7In order to evaluate these systems in terms of pre-cision and recall, we need to match between their au-tomated extractions and the benchmark extractions.To allow some flexibility (e.g., omissions of preposi-tions or auxiliaries), we follow (He et al, 2015) andmatch an automated extraction with a gold proposi-tion if both agree on the grammatical head of all oftheir elements (predicate and arguments).
We thenanalyze the recall and precision of Open IE systemson different confidence thresholds (Figure 1).
Fur-thermore, we calculate the area under the PR curvefor each of the different corpora (Figure 2) and theexplicit yield per system (Table 3).To the best of our knowledge, this is the first ob-jective comparative evaluation of prominent Open-IE systems, over a large and independently createddataset.
This comparison gives rise to several ob-servations; which can be useful for future researchand for choosing a preferred system for a particularapplication setting, such as:6Carried by the first author.7Currently, we test only the common case of verbal predi-cates.Figure 2: Area Under the PR Curve (AUC) measurefor the evaluated systems.1.
Open IE-4 achieves best precision above 3% re-call (?
78.67) and best AUC score (54.02),2.
ClausIE is best at recall (81.38), and3.
Stanford Open IE assigns confidence of 1 to94% of its extractions, explaining its low pre-cision.5 ConclusionsWe presented the first independent and large scaleOpen IE benchmark annotation, and tested the mostprominent systems against it.
We hope that futureOpen IE systems can make use of this new resourceto easily and objectively measure and compare theirperformance.AcknowledgmentsWe would like to thank Mausam for fruitful discus-sions, and the anonymous reviewers for their helpfulcomments.This work was supported in part by grants fromthe MAGNET program of the Israeli Office of theChief Scientist (OCS), the Israel Science Founda-tion grant 880/12, and the German Research Foun-dation through the German-Israeli Project Coopera-tion (DIP, grant DA 1600/1-1).ReferencesAlan Akbik and Alexander Lo?ser.
2012.
Kraken: N-aryfacts in open information extraction.
In NAACL-HLT2012: Proceedings of the The Knowledge ExtractionWorkshop.Gabor Angeli, Melvin Johnson Premkumar, and Christo-pher D. Manning.
2015.
Leveraging linguistic struc-ture for open domain information extraction.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2015).2304Collin F Baker, Charles J Fillmore, and John B Lowe.1998.
The berkeley framenet project.
In Proceedingsof ACL, pages 86?90.
Association for ComputationalLinguistics.Niranjan Balasubramanian, Stephen Soderland, Oren Et-zioni Mausam, and Oren Etzioni.
2013.
Generatingcoherent event schemas at scale.
In EMNLP, pages1721?1731.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction from the web.
In IJCAI 2007,Proceedings of the 20th International Joint Confer-ence on Artificial Intelligence, Hyderabad, India, Jan-uary 6-12, 2007, pages 2670?2676.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the conll-2005 shared task: Semantic role labeling.In Proceedings of CONLL, pages 152?164.Janara Christensen, Stephen Soderland Mausam, StephenSoderland, and Oren Etzioni.
2013.
Towards coher-ent multi-document summarization.
In HLT-NAACL,pages 1163?1173.
Citeseer.Luciano Del Corro and Rainer Gemulla.
2013.
Clausie:clause-based open information extraction.
In Proceed-ings of the 22nd international conference on WorldWide Web, pages 355?366.
International World WideWeb Conferences Steering Committee.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld.
2008.
Open information extrac-tion from the Web.
Communications of the ACM,51(12):68?74.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1535?1545.
Association for Computational Linguis-tics.Luheng He, Mike Lewis, and Luke Zettlemoyer.
2015.Question-answer driven semantic role labeling: Us-ing natural language to annotate natural language.
Inthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Mausam, Michael Schmitz, Stephen Soderland, RobertBart, and Oren Etzioni.
2012.
Open language learn-ing for information extraction.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 523?534, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational linguistics, 31(1):71?106.Gabriel Stanovsky, Ido Dagan, and Mausam.
2015.Open IE as an intermediate structure for semantictasks.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics (ACL2015).Gabriel Stanovsky, Ido Dagan, and Meni Adler.
2016a.Specifying and annotating reduced argument span viaqa-srl.
In Proceedings of the 54rd Annual Meetingof the Association for Computational Linguistics (ACL2015).Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and YoavGoldberg.
2016b.
Getting more out of syntax withprops.
arXiv preprint.Fei Wu and Daniel S. Weld.
2010.
Open information ex-traction using wikipedia.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 118?127, Uppsala, Sweden, July.Association for Computational Linguistics.Fei Wu, Raphael Hoffmann, and Daniel S Weld.
2008.Information extraction from wikipedia: Moving downthe long tail.
In Proceedings of the 14th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 731?739.
ACM.2305
