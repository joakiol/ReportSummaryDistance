The University of Amsterdam at Senseval-3:Semantic Roles and Logic FormsDavid Ahn Sisay Fissaha Valentin Jijkoun Maarten de RijkeInformatics Institute, University of AmsterdamKruislaan 4031098 SJ AmsterdamThe Netherlands{ahn,sfissaha,jijkoun,mdr}@science.uva.nlAbstractWe describe our participation in two of the tasks or-ganized within Senseval-3: Automatic Labeling ofSemantic Roles and Identification of Logic Formsin English.1 IntroductionThis year (2004), Senseval, a well-established fo-rum for the evaluation and comparison of wordsense disambiguation (WSD) systems, introducedtwo tasks aimed at building semantic representa-tions of natural language sentences.
One task, Auto-matic Labeling of Semantic Roles (SR), takes as itstheoretical foundation Frame Semantics (Fillmore,1977) and uses FrameNet (Johnson et al, 2003) asa data resource for evaluation and system develop-ment.
The definition of the task is simple: givena natural language sentence and a target word inthe sentence, find other fragments (continuous wordsequences) of the sentence that correspond to ele-ments of the semantic frame, that is, that serve asarguments of the predicate introduced by the targetword.For this task, the systems receive a sentence, atarget word, and a semantic frame (one target wordmay belong to multiple frames; hence, for real-world applications, a preliminary WSD step mightbe needed to select an appropriate frame).
The out-put of a system is a list of frame elements, with theirnames and character positions in the sentence.
Theevaluation of the SR task is based on precision andrecall.
For this year?s task, the organizers chose 40frames from FrameNet 1.1, with 32,560 annnotatedsentences, 8,002 of which formed the test set.The second task, Identification of Logic Formsin English (LF), is based on the LF formalism de-scribed in (Rus, 2002).
The LF formalism is a sim-ple logical form language for natural language se-mantics with only predicates and variables; thereis no quantification or negation, and atomic predi-cations are implicitly conjoined.
Predicates corre-spond directly to words and are composed of thebase form of the word, the part of speech tag, and asense number (corresponding to the WordNet senseof the word as used).
For the task, the system isgiven sentences and must produce LFs.
Word sensedisambiguation is not part of the task, so the pred-icates need not specify WordNet senses.
Systemevaluation is based on precision and recall of pred-icates and predicates together with all their argu-ments as compared to a gold standard.2 Syntactic ProcessingFor both tasks, SR and LF, the core of our systemswas the syntactic analysis module described in de-tail in (Jijkoun and de Rijke, 2004).
We only havespace here to give a short overview of the module.Every sentence was part-of-speech tagged usinga maximum entropy tagger (Ratnaparkhi, 1996) andparsed using a state-of-the-art wide coverage phrasestructure parser (Collins, 1999).
Both the tagger andthe parser are trained on the Penn Treebank WallStreet Journal Corpus (WSJ in the rest of this paper)and thus produce structures similar to those in thePenn Treebank.
Unfortunately, the parser does notdeliver some of the information available in WSJthat is potentially useful for our two applications:Penn functional tags (e.g., subject, temporal, closelyrelated, logical subject in passive) and non-local de-pendencies (e.g., subject and object control, argu-ment extraction in relative clauses).
Our syntacticmodule tries to compensate for this and make thisinformation explicit in the resulting syntactic analy-ses.As a first step, we converted phrase trees pro-duced by the parser to dependency structures, bydetecting heads of constituents and then propagat-ing the lexical head information up the syntactictree, similarly to (Collins, 1999).
The resulting de-pendency structures were labeled with dependencylabels derived from corresponding Penn phrase la-bels: e.g., a verb phrase (VP) modified by a prepo-sitional phrase (PP) resulted in a dependency withlabel ?VP|PP?.Then, the information available in the WSJ (func-Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsVPto seek NPseatsVPplannedSdirectorsthis monthNPNP  SplanneddirectorsVP|SS|NPS|NPmonththisNP|DT toseekseatsVP|NPVP|TOplanneddirectorsVP|SS|NP?SBJS|NP?TMPS|NP?SBJmonththisNP|DT toseekseatsVP|NPVP|TO(a) (b) (c)Figure 1: Stages of the syntactic processing: (a) the parser?s output, (b) the result of conversion to a depen-dency structure, (c) final output of our syntactic moduletional tags, non-local dependencies) was added todependency structures using Memory-Based Learn-ing (Daelemans et al, 2003): we trained the learnerto change dependency labels, or add new nodes orarcs to dependency structures.
Trained and testedon WSJ, our system achieves state-of-the-art perfor-mance for recovery of Penn functional tags and non-local dependencies (Jijkoun and de Rijke, 2004).Figure 1 shows three stages of the syntactic anal-ysis of the sentence Directors this month planned toseek seats (a simplified actual sentence from WSJ):(a) the phrase structure tree produced by Collins?parser, (b) the phrase structure tree converted to adependency structure and (c) the transformed de-pendency structure with added functional tags and anon-local dependency?the final output of our syn-tactic module.
Dependencies are shown as arcsfrom heads to dependents.3 Automatic Labeling of Semantic RolesFor the SR task, we applied a method very similar tothe one used in (Jijkoun and de Rijke, 2004) for re-covering syntactic structures and somewhat similarto the first method for automatic semantic role iden-tification described in (Gildea and Jurafsky, 2002).Essentially, our method consists of extracting possi-ble syntactic patterns (paths in syntactic dependencystructures), introducing semantic relations from atraining corpus, and then using a machine learn-ing classifier to predict which syntactic paths cor-respond to which frame elements.Our main assumption was that frame elements,as annotated in FrameNet, correspond directly toconstituents (constituents being complete subtreesof dependency structures).
Similarly to (Gildea andJurafsky, 2002), our own evaluation showed thatabout 15% of frame elements in FrameNet 1.1 donot correspond to constituents, even when applyingsome straighforward heuristics (see below) to com-pensate for this mismatch.
This observation puts anupper bound of around 85% on the accuracy of oursystem (with strict evaluation, i.e., if frame elementboundaries must match the gold standard exactly).Note, though, that these 15% of ?erroneous?
con-stituents also include parsing errors.Since the core of our SR system operates onwords, constituents, and dependencies, two im-portant steps are the conversion of FrameNet el-ements (continuous sequences of characters) intohead words of constituents, and vice versa.
The con-version of FrameNet elements is straightforward:we take the head of a frame element to be the wordthat dominates the most words of this element inthe dependency graph of the sentence.
In the otherdirection, when converting a subgraph of a depen-dency graph dominated by a word w into a contin-uous sequence of words, we take all (i.e., not onlyimmediate) dependents of w, ignoring non-local de-pendencies, unless w is the target word of the sen-tence, in which case we take the word w alone.
Thislatter heuristic helps us to handle cases when a nountarget word is a semantic argument of itself.
Sev-eral other simple heristics were also found helpful:e.g., if the result of the conversion of a constituent toa word sequence contains the target word, we takeonly the words to the right of the target word.With this conversion between frame elements andconstituents, the rest of our system only needs tooperate on words and labeled dependencies.3.1 Training: the major stepsFirst, we extract from the training corpus(dependency-parsed FrameNet sentences, withwords marked as targets and frame elements) allshortest undirected paths in dependency graphs thatconnect target words with their semantic arguments.In this way, we collect all ?interesting?
syntacticpaths from the training corpus.In the second step, for all extracted syntacticpaths and again for all training sentences, we extractall occurences of the paths (i.e., paths, starting froma target word, that actually exist in the dependencygraph), recording for each such occurrence whetherit connects a target word to one of its semantic ar-guments.
For performance reasons, we consider foreach target word only syntactic paths extracted fromsentences annotated with respect to the same frame,and we ignore all paths of length more than 3.For every extracted occurrence, we record thefeatures describing the occurrence of a path in moredetail: the frame name, the path itself, the wordsalong the path (including the target word and thepossible head of a frame element?first and lastnode of the path, respectively), their POS tags andsemantic classes.
For nouns, the semantic classof a word is defined as the hypernym of the firstsense of the noun in WordNet, one of 19 manu-ally selected terms (animal, person, social group,clothes, feeling, property, phenomenon, etc.)
Forlexical adverbs and prepositions, the semantic classis one of the 6 clusters obtained automatically usingthe K-mean clustering algorithm on data extractedfrom FrameNet.
Examples of the clusters are:(abruptly, ironically, slowly, .
.
.
), (above, beneath,inside, .
.
.
), (entirely, enough, better, .
.
.
).
The listof WordNet hypernyms and the number of clusterswere chosen experimentally.
We also added featuresdescribing the subcategorization frame of the tar-get word; this information is straightforwardly ex-tracted from the dependency graph.
In total, the sys-tem used 22 features.The set of path occurrences obtained in the sec-ond step, with all the extracted features, is a pool ofpositive and negative examples of whether certainsyntactic patterns correspond to any semantic argu-ments.
The pool is used as an instance base to trainTiMBL, a memory-based learner (Daelemans et al,2003), to predict whether the endpoint of a syntac-tic path starting at a target word corresponds to asemantic argument, and if so, what its name is.We chose TiMBL for this task because we hadpreviously found that it deals successfully withcomplex feature spaces and data sparseness (in ourcase, in the presence of many lexical features) (Ji-jkoun and de Rijke, 2004).
Moreover, TiMBL isvery flexible and implements many variants of thebasic k-nearest neighbor algorithm.
We found thattuning various parameters (the number of neigh-bors, weighting and voting schemes) made substan-tial differences in the performance of our system.3.2 Applying the systemOnce the training is complete, the system can beapplied to new sentences (with the indicated targetword and its frame) as follows.
A sentence is parsedand its dependency structure is built, as described inSection 2.
All occurences of ?interesting?
syntac-tic paths are extracted, along with their features asdescribed above.
The resulting feature vectors arefed to TiMBL to determine whether the endpointsof the syntactic paths correspond to semantic argu-ments of the target word.
For the path occurencesclassified positively, the constituents of their end-points are converted to continuous word sequences,as described earlier; in this case the system has de-tected a frame element.3.3 ResultsDuring the development of our system, we usedonly the 24,558 sentences from FrameNet set asidefor training by the SR task organizers.
To tune thesystem, this corpus was randomly split into trainingand development sets (70% and 30%, resp.
), evenlyfor all target words.
The official test set (8002 sen-tences) was used only once to produce the submittedrun, with the whole training set (24,558 sentences)used for training.We submitted one run of the system (with iden-tification of both element boundaries and elementnames).
Our official scores are: precision 86.9%,recall 75.2% and overlap 84.7%.
Our own evalua-tion of the submitted run with the strict measures,i.e., an element is considered correct only if both itsname and boundaries match the gold standard, gaveprecision 73.5% and recall 63.6%.4 Logic Forms4.1 MethodFor the LF task, it was straightforward to turn de-pendency structures into LFs.
Since the LF for-malism does not attempt to represent the more sub-tle aspects of semantics, such as quantification, in-tensionality, modality, or temporality (Rus, 2002),the primary information encoded in a LF is basedon argument structure, which is already well cap-tured by the dependency parses.
Our LF genera-tor traverses the dependency structure, turning POS-tagged lexical items into LF predicates, creating ref-erential variables for nouns and verbs, and usingdependency labels to order the arguments for eachpredicate.
We make one change to the dependencygraphs originally produced by the parser.
Instead oftaking coordinators, such as and, to modify the con-stituents they coordinate, we take the coordinatedconstituents to be arguments of the coordinator.Our LF generator builds a labeled directed graphfrom a dependency structure and traverses thisgraph depth-first.
In general, a well-formed depen-dency graph has exactly one root node, which cor-responds to the main verb of the sentence.
Sen-tences with multiple independent clauses may haveone root per clause.
The generator begins traversingthe graph at one of these root nodes; if there is morethan one, it completes traversal of the subgraph con-nected to the first node before going on to the nextnode.The first step in processing a node?producing anLF predicate from the node?s lexical item?is takencare of in the graph-building stage.
We use a baseform dictionary to get the base form of the lexicalitem and a simple mapping of Penn Treebank tagsinto ?n?, ?v?, ?a?, and ?r?
to get the suffix.
For wordsthat are not tagged as nouns, verbs, adjectives, oradverbs, the LF predicate is simply the word itself.As the graph is traversed, the processing of a nodedepends on its type.
The greatest amount of pro-cessing is required for a node corresponding to averb.
First, a fresh referential variable is generatedas the event argument of the verbal predication.
Theout-edges are then searched for nodes to process.Since the order of arguments in an LF predicationis important and some sentence constitutents are ig-nored for the purposes of LF, the out-edges are cho-sen in order by label: first particles (?VP|PRT?
),then arguments (?S|NP-SBJ?, ?VP|NP?, etc.
), andfinally adjuncts.
We attempt to follow the argu-ment order implicit in the description of LF givenin (Rus, 2002), and as the formalism requires, weignore auxiliary verbs and negation.
The processingof each of these arguments or adjuncts is handled re-cursively and returns a set of predications.
For mod-ifiers, the event variable also has to be passed down.For referential arguments and adjuncts, a referen-tial variable also is returned to serve as an argumentfor the verb?s LF predicate.
Once all the argumentsand adjuncts have been processed, a new predica-tion is generated, in which the verb?s LF predicateis applied to the event variable and the recursivelygenerated referential variables.
This new predica-tion, along with the recursively generated ones, isreturned.The processing of a nominal node proceeds sim-ilarly.
A fresh referential variable is generated?since determiners are ignored in the LF formalism,it is simply assumed that all noun phrases corre-spond to a (possibly composite) individual.
Out-edges are examined for modifiers and recursivelyprocessed.
Both the referential variable and the setof new predications are returned.
Noun compoundsintroduce some additional complexity; each modi-fying noun introduces two additional variables, onefor the modifying noun and one for composite indi-vidual realizing the compound.
This latter variablethen replaces the referential variable for the headnoun.Processing of other types of nodes proceeds in asimilar fashion.
For modifiers such as adjectives,adverbs, and prepositional phrases, a variable (cor-responding to the individual or event being modi-fied) is passed in, and the LF predicate of the nodeis applied to this variable, rather than to a freshvariable.
In the case of prepositional phrases, thepredicate is applied to this variable and to the vari-able corresponding to the object of the preposition,which must be processed, as well.
The latter vari-able is then returned along with the new predica-tions.
For other modifiers, just the predications arereturned.4.2 Development and resultsThe rules for handling dependency labels were writ-ten by hand.
Of the roughly 1100 dependency la-bels that the parser assigns (see Section 2), our sys-tem handles 45 labels, all of which fall within themost frequent 135 labels.
About 50 of these 135labels are dependencies that can be ignored in thegeneration of LFs (labels involving punctuation, de-terminers, auxiliary verbs, etc.
); of the remaining85 labels, the 45 labels handled were chosen to pro-vide reasonable coverage over the sample corpusprovided by the task organizers.
Extending the sys-tem is straightforward; to handle a dependency labellinking two node types, a rule matching the labeland invoking the dependent node handler is addedto the head node handler.On the sample corpus of 50 sentences to whichour system was tuned, predicate identification, com-pared to the provided LFs, including POS-tags, wasperformed with 89.1% precision and 87.1% recall.Argument identification was performed with 78.9%precision and 77.4% recall.
On the test corpus of300 sentences, our official results, which excludePOS-tags, were 82.0% precision and 78.4% recallfor predicate identification and 73.0% precision and69.1% recall for argument identification.We did not get the gold standard for the test cor-pus in time to perform error analysis for our officialsubmission, but we did examine the errors in theLFs we generated for the trial corpus.
Most couldbe traced to errors in the dependency parses, whichis unsurprising, since the generation of LFs from de-pendency parses is relatively straightforward.
A fewerrors resulted from the fact that our system does nottry to identify multi-word compounds.Some discrepancies between our LFs and the LFsprovided for the trial corpus arose from apparentinconsistencies in the provided LFs.
Verbs withparticles were a particular problem.
Sometimes,as in sentences 12 and 13 of the trial corpus, averb-particle combination such as look forward tois treated as a single predicate (look forward to); inother cases, such as in sentence 35, the verb and itsparticle (go out) are treated as separate predicates.Other inconsistencies in the provided LFs includemissing arguments (direct object in sentence 24),and verbs not reduced to base form (felt, saw, andfound in sentences 34, 48, 50).5 ConclusionsOur main finding during the development of the sys-tems for the two Senseval tasks was that semanticrelations are indeed very close to syntactic depen-dencies.
Using deep dependency structures helpedto keep the manual rules for the LF task simpleand made the learning for the SR task easier.
Alsowe found that memory-based learning can be effi-ciently applied to complex, highly structured prob-lems such as the identification of semantic roles.Our future work includes more accurate fine-tuning of the learner for the SR task, extending thecoverage of the LF generator, and experimentingwith the generated LFs for question answering.6 AcknowledgmentsAhn and De Rijke were supported by a grant fromthe Netherlands Organization for Scientific Re-search (NWO) under project number 612.066.302.Fissaha, Jijkoun, and De Rijke were supported by agrant from NWO under project number 220-80-001.De Rijke was also supported by grants from NWO,under project numbers 365-20-005, 612.069.006,612.000.106, and 612.000.207.ReferencesM.
Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch, 2003.
TiMBL: Tilburg Mem-ory Based Learner, version 5.0, Reference Guide.ILK Technical Report 03-10.
Available fromhttp://ilk.kub.nl/downloads/pub/papers/ilk0310.pdf.C.
J. Fillmore.
1977.
The need for a frame semantics inlinguistics.
Statistical Methods in Linguistics, 12:5?29.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.V.
Jijkoun and M. de Rijke.
2004.
Enriching the outputof a parser using memory-based learning.
In Proceed-ings of ACL 2004.C.
Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-penhofer, and C. Fillmore.
2003.
Framenet: Theoryand practice.
http://www.icsi.berkeley.edu/ framenet.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the Empirical Meth-ods in Natural Language Processing Conference.V.
Rus.
2002.
Logic Form for WordNet Glosses.
Ph.D.thesis, Southern Methodist University.
