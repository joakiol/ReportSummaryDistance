Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 122?127,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMunich-Edinburgh-Stuttgart Submissions of OSM Systems at WMT13Nadir Durrani1, Helmut Schmid2, Alexander Fraser2,Hassan Sajjad3, Richa?rd Farkas41University of Edinburgh ?
dnadir@inf.ed.ac.uk2Ludwig Maximilian University Munich ?
schmid,fraser@cis.uni-muenchen.de3Qatar Computing Research Institute ?
hsajjad@qf.org.qa4University of Szeged ?
rfarkas@inf.u-szeged.huAbstractThis paper describes Munich-Edinburgh-Stuttgart?s submissions to the EighthWorkshop on Statistical Machine Transla-tion.
We report results of the translationtasks from German, Spanish, Czech andRussian into English and from English toGerman, Spanish, Czech, French and Rus-sian.
The systems described in this paperuse OSM (Operation Sequence Model).We explain different pre-/post-processingsteps that we carried out for differentlanguage pairs.
For German-English weused constituent parsing for reorderingand compound splitting as preprocessingsteps.
For Russian-English we transliter-ated the unknown words.
The translitera-tion system is learned with the help of anunsupervised transliteration mining algo-rithm.1 IntroductionIn this paper we describe Munich-Edinburgh-Stuttgart?s1 joint submissions to the Eighth Work-shop on Statistical Machine Translation.
We useour in-house OSM decoder which is based onthe operation sequence N-gram model (Durraniet al 2011).
The N-gram-based SMT frame-work (Marin?o et al 2006) memorizes Markovchains over sequences of minimal translation units(MTUs or tuples) composed of bilingual transla-tion units.
The OSM model integrates reorderingoperations within the tuple sequences to form aheterogeneous mixture of lexical translation and1Qatar Computing Research Institute and University ofSzeged were partnered for RU-EN and DE-EN language pairsrespectively.reordering operations and learns a Markov modelover a sequence of operations.Our decoder uses the beam search algorithm ina stack-based decoder like most sequence-basedSMT frameworks.
Although the model is basedon minimal translation units, we use phrases dur-ing search because they improve the search accu-racy of our system.
The earlier decoder (Durraniet al 2011) was based on minimal units.
But werecently showed that using phrases during searchgives better coverage of translation, better futurecost estimation and lesser search errors (Durraniet al 2013a) than MTU-based decoding.
We havetherefore shifted to phrase-based search on top ofthe OSM model.This paper is organized as follows.
Section 2gives a short description of the model and searchas used in the OSM decoder.
In Section 3 wegive a description of the POS-based operation se-quence model that we test for our German-Englishand English-German experiments.
Section 4 de-scribes our processing of the German and Englishdata for German-English and English-German ex-periments.
In Section 5 we describe the unsuper-vised transliteration mining that has been done forthe Russian-English and English-Russian experi-ments.
In Section 6 we describe the sub-samplingtechnique that we have used for several languagepairs.
In Section 7 we describe the experimentalsetup followed by the results.
Finally we summa-rize the paper in Section 8.2 System Description2.1 ModelOur systems are based on the OSM (Operation Se-quence Model) that simultaneously learns trans-lation and reordering by representing a bilingual122Figure 1: Bilingual Sentence with Alignmentssentence pair and its alignments as a unique se-quence of operations.
An operation either jointlygenerates source and target words, or it performsreordering by inserting gaps or jumping to gaps.We then learn a Markov model over a sequence ofoperations o1, o2, .
.
.
, oJ that encapsulate MTUsand reordering information as:posm(o1, ..., oJ) =J?j=1p(oj |oj?n+1, ..., oj?1)By coupling reordering with lexical generation,each (translation or reordering) decision dependson n?
1 previous (translation and reordering) de-cisions spanning across phrasal boundaries.
Thereordering decisions therefore influence lexical se-lection and vice versa.
A heterogeneous mixtureof translation and reordering operations enables usto memorize reordering patterns and lexicalizedtriggers unlike the classic N-gram model wheretranslation and reordering are modeled separately.2.2 TrainingDuring training, each bilingual sentence pair is de-terministically converted to a unique sequence ofoperations.2 The example in Figure 1(a) is con-verted to the following sequence of operations:Generate(Beide, Both)?
Generate(La?nder, coun-tries)?
Generate(haben, have)?
Insert Gap?Generate(investiert, invested)At this point, the (partial) German and Englishsentences look as follows:Beide La?nder haben investiertBoth countries have investedThe translator then jumps back and covers theskipped German words through the following se-quence of operations:Jump Back(1)?Generate(Millionen, millions)?Generate(von, of)?
Generate(Dollar, dollars)2Please refer to Durrani et al(2011) for a list of opera-tions and the conversion algorithm.The generative story of the OSM model alsosupports discontinuous source-side cepts andsource-word deletion.
However, it doesn?t providea mechanism to deal with unaligned and discon-tinuous target cepts.
These are handled througha 3-step process3 in which we modify the align-ments to remove discontinuous and unaligned tar-get MTUs.
Please see Durrani et al(2011) fordetails.
After modifying the alignments, we con-vert each bilingual sentence pair and its align-ments into a sequence of operations as describedabove and learn an OSM model.
To this end,a Kneser-Ney (Kneser and Ney, 1995) smoothed9-gram model is trained with SRILM (Stolcke,2002) while KenLM (Heafield, 2011) is used atruntime.2.3 Feature FunctionsWe use additional features for our model and em-ploy the standard log-linear approach (Och andNey, 2004) to combine and tune them.
We searchfor a target string E which maximizes a linearcombination of feature functions:E?
= argmaxE??
?J?j=1?jhj(o1, ..., oJ)??
?where ?j is the weight associated with the fea-ture hj(o1, ..., oj).
Apart from the main OSMfeature we train 9 additional features: A target-language model (see Section 7 for details), 2 lex-ical weighting features, gap and open gap penaltyfeatures, two distance-based distortion models and2 length-based penalty features.
Please refer toDurrani et al(2011) for details.2.4 Phrase ExtractionPhrases are extracted in the following way: Thealigned training corpus is first converted to an op-eration sequence.
Each subsequence of operationsthat starts and ends with a translation operation, isconsidered a ?phrase?.
The translation operationsinclude Generate Source Only (X) operation whichdeletes unaligned source word.
Such phrases maybe discontinuous if they include reordering opera-tions.
We replace each subsequence of reorderingoperations by a discontinuity marker.3Durrani et al(2013b) recently showed that our post-processing of alignments hurt the performance of the MosesPhrase-based system in several language pairs.
The solu-tion they proposed has not been incorporated into the currentOSM decoder yet.123During decoding, we match the source tokensof the phrase with the input.
Whenever there isa discontinuity in the phrase, the next source to-ken can be matched at any position of the inputstring.
If there is no discontinuity marker, the nextsource token in the phrase must be to the right ofthe previous one.
Finally we compute the numberof uncovered input tokens within the source spanof the hypothesized phrase and reject the phraseif the number is above a threshold.
We use athreshold value of 2 which had worked well ininitial experiments.
Once the positions of all thesource words of a phrase are known, we can com-pute the necessary reordering operations (whichmay be different from the ones that appeared inthe training corpus).
This usage of phrases al-lows the decoder to generalize from a seen trans-lation ?scored a goal ?
ein Tor schoss?
(wherescored/a/goal and schoss/ein/Tor are aligned, re-spectively) to ?scored a goal ?
schoss ein Tor?.The phrase can even be used to translate ?er schossheute ein Tor ?
he scored a goal today?
although?heute?
appears within the source span of thephrase ?ein Tor schoss?.
Without phrase-baseddecoding, the unusual word translations ?schoss?scored?
and ?Tor?goal?
(at least outside of the soc-cer literature) are likely to be pruned.The phrase tables are further filtered withthreshold pruning.
The translation options witha frequency less than x times the frequency ofthe most frequent translation are deleted.
We usex = 0.02.
We use additional settings to increasethis threshold for longer phrases.
The phrase fil-tering heuristic was used to speed up decoding.
Itdid not lower the BLEU score in our small scaleexperiments (Durrani et al 2013a), however wecould not test whether this result holds in a largescale evaluation.2.5 DecoderThe decoding framework used in the operation se-quence model is based on Pharaoh (Koehn, 2004).The decoder uses beam search to build up thetranslation from left to right.
The hypotheses arearranged in m stacks such that stack i maintainshypotheses that have already translated imany for-eign words.
The ultimate goal is to find the bestscoring hypothesis, that translates all the wordsin the foreign sentence.
During the hypothesisextension each extracted phrase is translated intoa sequence of operations.
The reordering opera-tions (gaps and jumps) are generated by looking atthe position of the translator, the last foreign wordgenerated etc.
(Please refer to Algorithm 1 in Dur-rani et al(2011)).
The probability of an opera-tion depends on the n?1 previous operations.
Themodel is smoothed with Kneser-Ney smoothing.3 POS-based OSM ModelPart-of-speech information is often relevant fortranslation.
The word ?stores?
e.g.
should betranslated to ?La?den?
if it is a noun and to ?spei-chert?
when it is a verb.
The sentence ?The smallchild cries?
might be incorrectly translated to ?Diekleinen Kind weint?
where the first three wordslack number, gender and case agreement.In order to better learn such constraints whichare best expressed in terms of part of speech, weadd another OSM model as a new feature to thelog-linear model of our decoder, which is identi-cal to the regular OSM except that all the wordshave been replaced by their POS tags.
The inputof the decoder consists of the input sentence withautomatically assigned part-of-speech tags.
Thesource and target part of the training data are alsoautomatically tagged and phrases with words andPOS tags on both sides are extracted.
The POS-based OSM model is only used in the German-to-English and English-to-German experiments.4 Sofar, we only used coarse POS tags without genderand case information.4 Constituent Parse ReorderingOur German-to-English system used constituentparses for pre-ordering of the input.
We parsed allof the parallel German to English data available,and the tuning, test and blind-test sets.
We thenapplied reordering rules to these parses.
We usedthe rules for reordering German constituent parsesof Collins et al(2005) together with the additionalrules described by Fraser (2009).
These are ap-plied as a preprocess to all German data (training,tuning and test data).
To produce the parses, westarted with the generative BitPar parser trained onthe Tiger treebank with optimizations of the gram-mar, as described by (Fraser et al 2013).
We thenperformed self-training using the high quality Eu-roparl corpus - we parsed it, and then retrained theparser on the output.4This work is ongoing and we will present detailed exper-iments in the future.124Following this, we performed linguistically-informed compound splitting, using the system ofFritzinger and Fraser (2010), which disambiguatescompeting analyses from the high-recall StuttgartMorphological Analyzer SMOR (Schmid et al2004) using corpus statistics (Koehn and Knight,2003).
We also split portmanteaus like German?zum?
formed from ?zu dem?
meaning ?to the?.Due to time constraints, we did not address Ger-man inflection.
See Weller et al(2013) for furtherdetails of the linguistic processing involved in ourGerman-to-English system.5 Transliteration Mining/HandlingOOVsThe machine translation system fails to translateout-of-vocabulary words (OOVs) as they are un-known to the training data.
Most of the OOVsare named entities and simply passing them tothe output often produces correct translations ifsource and target language use the same script.If the scripts are different transliterating them tothe target language script could solve this prob-lem.
However, building a transliteration systemrequires a list of transliteration pairs for training.We do not have such a list and making one is acumbersome process.
Instead, we use the unsu-pervised transliteration mining system of Sajjad etal.
(2012) that takes a list of word pairs for train-ing and extracts transliteration pairs that can beused for the training of the transliteration system.The procedure of mining transliteration pairs andtransliterating OOVs is described as follows:We word-align the parallel corpus usingGIZA++ in both direction and symmetrize thealignments using the grow-diag-final-and heuris-tic.
We extract all word pairs which occur as 1-to-1 alignments (like Sajjad et al(2011)) and laterrefer to them as the list of word pairs.
We train theunsupervised transliteration mining system on thelist of word pairs and extract transliteration pairs.We use these mined pairs to build a transliterationsystem using the Moses toolkit.
The translitera-tion system is applied in a post-processing stepto transliterate OOVs.
Please refer to Sajjad etal.
(2013) for further details on our transliterationwork.6 Sub-samplingBecause of scalability problems we were not ableto use the entire data made available for build-ing the translation model in some cases.
We usedmodified Moore-Lewis sampling (Axelrod et al2011) for the language pairs es-en, en-es, en-fr,and en-cs.
In each case we included the News-Commentary and Europarl corpora in their en-tirety, and scored the sentences in the remainingcorpora (the selection corpus) using a filtering cri-terion, adding 10% of the selection corpus tothe training data.
We can not say with certaintywhether using the entire data will produce betterresults with the OSM decoder.
However, we knowthat the same data used with the state-of-the-artMoses produced worse results in some cases.
Theexperiments in Durrani et al(2013c) showed thatMML filtering decreases the BLEU scores in es-en (news-test13: Table 19) and en-cs (news-test12:Table 14).
We can therefore speculate that beingable to use all of the data may improve our resultssomewhat.7 ExperimentsParallel Corpus: The amount of bitext used forthe estimation of the translation models is: de?en?
4.5M and ru?en ?
2M parallel sentences.
Wewere able to use all the available data for cs-to-en(?
15.6M sentences).
However, sub-sampled datawas used for en-to-cs (?
3M sentences), en-to-fr(?
7.8M sentences) and es?en (?
3M sentences).Monolingual Language Model: We used allthe available training data (including LDC Giga-word data) for the estimation of monolingual lan-guage models: en?
287.3M sentences, fr?
91M,es ?
65.7M, cs ?
43.4M and ru ?
21.7M sen-tences.
All data except for ru-en and en-ru wastrue-cased.
We followed the approach of Schwenkand Koehn (2008) by training language modelsfrom each sub-corpus separately and then linearlyinterpolated them using SRILM with weights op-timized on the held-out dev-set.
We concatenatedthe news-test sets from four years (2008-2011) toobtain a large dev-set5 in order to obtain more sta-ble weights (Koehn and Haddow, 2012).Decoder Settings: For each extracted inputphrase only 15-best translation options were usedduring decoding.6 We used a hard reordering limit5For Russian-English and English-Russian languagepairs, we divided the tuning-set news-test 2012 into twohalves and used the first half for tuning and second for test.6We could not experiment with higher n-best translationoptions due to a bug that was not fixed in time and hinderedus from scaling.125of 16 words which disallows a jump beyond 16source words.
A stack size of 100 was used duringtuning and 200 for decoding the test set.Results: Table 1 shows the uncased BLEUscores along with the rank obtained on the sub-mission matrix.7 We also show the results fromhuman evaluation.Lang EvaluationAutomatic HumanBLEU Rank Win Ratio Rankde-en 27.6 9/31 0.562 6-8es-en 30.4 6/12 0.569 3-5cs-en 26.4 3/11 0.581 2-3ru-en 24.5 8/22 0.534 7-9en-de 20.0 6/18en-es 29.5 3/13 0.544 5-6en-cs 17.6 14/22 0.517 4-6en-ru 18.1 6/15 0.456 9-10en-fr 30.0 7/26 0.541 5-9Table 1: Translating into and from English8 ConclusionIn this paper, we described our submissions toWMT 13 in all the shared-task language pairs(except for fr-en).
We used an OSM-decoder,which implements a model on n-gram of opera-tions encapsulating lexical generation and reorder-ing.
For German-to-English we used constituentparsing and applied linguistically motivated rulesto these parses, followed by compound splitting.We additionally used a POS-based OSM model forGerman-to-English and English-to-German exper-iments.
For Russian-English language pairs weused unsupervised transliteration mining.
Becauseof scalability issues we could not use the entiredata in some language pairs and used only sub-sampled data.
Our Czech-to-English system thatwas built from the entire data did better in bothautomatic and human evaluation compared to thesystems that used sub-sampled data.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
Wewould like to thank Philipp Koehn and Barry Had-dow for providing data and alignments.
Nadir7http://matrix.statmt.org/Durrani was funded by the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement n ?
287658.
AlexanderFraser was funded by Deutsche Forschungsge-meinschaft grant Models of Morphosyntax forStatistical Machine Translation.
Helmut Schmidwas supported by Deutsche Forschungsgemein-schaft grant SFB 732.
Richa?rd Farkas waspartially funded by the Hungarian National Ex-cellence Program (TA?MOP 4.2.4.A/2-11-1-2012-0001).
This publication only reflects the authors?views.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 355?362, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In ACL05, pages 531?540, Ann Arbor,MI.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A Joint Sequence Translation Model with In-tegrated Reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1045?1054, Portland, Oregon, USA, June.Nadir Durrani, Alexander Fraser, and Helmut Schmid.2013a.
Model With Minimal Translation Units, ButDecode With Phrases.
In The 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Atlanta, Georgia, USA, June.
Associationfor Computational Linguistics.Nadir Durrani, Alexander Fraser, Helmut Schmid,Hieu Hoang, and Philipp Koehn.
2013b.
CanMarkov Models Over Minimal Translation UnitsHelp Phrase-Based SMT?
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013c.
Edinburgh?s Machine Trans-lation Systems for European Language Pairs.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Alexander Fraser, Helmut Schmid, Richa?rd Farkas,Renjing Wang, and Hinrich Schu?tze.
2013.
Knowl-edge sources for constituent parsing of German, amorphologically rich and less-configurational lan-guage.
Computational Linguistics - to appear.126Alexander Fraser.
2009.
Experiments in Morphosyn-tactic Processing for Translating to and from Ger-man.
In Proceedings of the EACL 2009 FourthWorkshop on Statistical Machine Translation, pages115?119, Athens, Greece, March.Fabienne Fritzinger and Alexander Fraser.
2010.
Howto Avoid Burning Ducks: Combining LinguisticAnalysis and Corpus Statistics for German Com-pound Processing.
In Proceedings of the ACL 2010Fifth Workshop on Statistical Machine Translation,Uppsala, Sweden.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland, United King-dom, 7.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In InProceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing, vol-ume I, pages 181?184, Detroit, Michigan, May.Philipp Koehn and Barry Haddow.
2012.
Towards Ef-fective Use of Training Data in Statistical MachineTranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 317?321, Montre?al, Canada, June.
Association for Com-putational Linguistics.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proceedings ofthe 10th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 187?193, Morristown, NJ.Philipp Koehn.
2004.
Pharaoh: A Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In AMTA, pages 115?124.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego,Adria` de Gispert, Patrik Lambert, Jose?
A. R. Fonol-losa, and Marta R. Costa-jussa`.
2006.
N-gram-Based Machine Translation.
Computational Lin-guistics, 32(4):527?549.Franz J. Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Transla-tion.
Computational Linguistics, 30(1):417?449.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2011.
An algorithm for unsupervised transliterationmining with an application to word alignment.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, Portland, USA.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2012.
A statistical model for unsupervised andsemi-supervised transliteration mining.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers - Vol-ume 1, Jeju, Korea.Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,Alexander Fraser, and Helmut Schmid.
2013.QCRI-MES Submission at WMT13: Using Translit-eration Mining to Improve Statistical MachineTranslation.
In Proceedings of the Eighth Workshopon Statistical Machine Translation, Sofia, Bulgaria,August.
Association for Computational Linguistics.Helmut Schmid, Arne Fitschen, and Ulrich Heid.2004.
SMOR: A German Computational Morphol-ogy Covering Derivation, Composition, and Inflec-tion.
In Proceedings of the Fourth InternationalConference on Language Resources and Evaluation(LREC).Holger Schwenk and Philipp Koehn.
2008.
Large andDiverse Language Models for Statistical MachineTranslation.
In International Joint Conference onNatural Language Processing, pages 661?666, Jan-uary 2008.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Intl.
Conf.
Spoken Lan-guage Processing, Denver, Colorado.Marion Weller, Max Kisselew, Svetlana Smekalova,Alexander Fraser, Helmut Schmid, Nadir Durrani,Hassan Sajjad, and Richa?rd Farkas.
2013.
Munich-Edinburgh-Stuttgart Submissions at WMT13: Mor-phological and Syntactic Processing for SMT.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.127
