Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 241?245, New York City, June 2006. c?2006 Association for Computational LinguisticsThe Exploration of Deterministic and Efficient Dependency ParsingYu-Chieh Wu Yue-Shi Lee Jie-Chi YangDept.
of Computer Science andInformation EngineeringDept.
of Computer Science andInformation EngineeringGraduate Institute of Net-work Learning TechnologyNational Central University Ming Chuan University National Central UniversityTaoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwanbcbb@db.csie.ncu.edu.tw lees@mcu.edu.tw yang@cl.ncu.edu.twAbstractIn this paper, we propose a three-stepmultilingual dependency parser, whichgeneralizes an efficient parsing algorithmat first phase, a root parser and post-processor at the second and third stages.The main focus of our work is to providean efficient parser that is practical to usewith combining only lexical and part-of-speech features toward language inde-pendent parsing.
The experimental resultsshow that our method outperforms Malt-parser in 13 languages.
We expect thatsuch an efficient model is applicable formost languages.1 IntroductionThe target of dependency parsing is to automati-cally recognize the head-modifier relationshipsbetween words in natural language sentences.
Usu-ally, a dependency parser can construct a similargrammar tree with the dependency graph.
In thisyear, CoNLL-X shared task (Buchholz et al, 2006)focuses on multilingual dependency parsing with-out taking the language-specific knowledge intoaccount.
The ultimate goal of this task is to designan ideal multilingual portable dependency parsingsystem.To accomplish the shared task, we present a verylight-weight and efficient parsing model to the 13distinct treebanks (Haji?
et al, 2004; Simov et al,2005; Simov and Osenova, 2003; Chen et al, 2003;B?hmov?
et al, 2003; Kromann 2003; van derBeek et al, 2002; Brants et al, 2002; Kawata andBartels, 2000; Afonso et al, 2002; D?eroski et al,2006; Civit and Mart?
2002; Nivre et al, 2005;Oflazer et al, 2003; Atalay et al, 2003) with athree-step process, Nivre?s algorithm (Nivre, 2003),root parser, and post-processing.
Our method isquite different from the conventional three-passprocessing, which usually exhaustively processesthe whole dataset three times, while our methodfavors examining the ?un-parsed?
tokens, whichincrementally shrink.
At the beginning, we slightlymodify the original parsing algorithm (proposed by(Nivre, 2003)) to construct the initial dependencygraph.
A root parser is then used to recognize rootwords, which were not parsed during the previousstep.
At the third phase, the post-processor (whichis another learner) recognizes the still un-parsedwords.
However, in this paper, we aim to build amultilingual portable parsing model without em-ploying deep language-specific knowledge, such aslemmatization, morphologic analyzer etc.
Instead,we only make use of surface lexical and part-of-speech (POS) information.
Combining these shal-low features, our parser achieves a satisfactory re-sult for most languages, especially Japanese.In the remainder of this paper, Section 2 describesthe proposed parsing model, and Section 3 lists theexperimental settings and results.
Section 4 pre-sents the discussion and analysis of our parser withthree selected languages.
In Section 5, we draw thefuture direction and conclusion.2 System Description241Over the past decades, many state-of-the-art pars-ing algorithm were proposed, such as head-wordlexicalized PCFG (Collins, 1998), Maximum En-tropy (Charniak, 2000), Maximum/Minimumspanning tree (MST) (McDonald et al, 2005), Bot-tom-up deterministic parsing (Yamada and Ma-tsumoto, 2003), and Constant-time deterministicparsing (Nivre, 2003).
Among them, the Nivre?salgorithm (Nivre, 2003) was shown to be most ef-ficient method, which only costs at most 2n transi-tion actions to parse a sentence (O(n3) for thebottom-up or MST approaches).
Nivre?s method ismainly consists of four transition actions,Left/Right/Reduce/Shift.
We further extend thesefour actions by dividing the ?reduce?
into ?reduce?and ?sleep (reduce-but-shift)?
two actions.
Becausethe too early reduce action makes the followingwords difficult to find the parents.
Thus, duringtraining, if a word which is the child of the top ofthe stack, it is then assigned to the ?sleep?
categoryand pushed into stack, otherwise, the conventionalreduce action is applied.
Besides, we do not ar-range these transition actions with priority order,instead, the decision is made by the classifier.
Theoverall parsing model can be found in Figure 1.Table 1 lists the detail system spec of our model.Figure 1: System architectureTable 1: Overall parsing system summary?.
Parsing Algorithm: 1.
Nivre's Algorithm (Nivre, 2003)2.
Root Parser3.
Exhaustive-based Post-processing?.
Parser Characteris-tics:1.
Top-down + Bottom-up2.
Deterministic + Exhaustive3.
Labeling integrated4.
Non-Projective?.
Learner: SVMLight (Joachims, 1998)(1) One-versus-One(2) Linear Kernel?.
Feature Set: 1.
Lexical (Unigram/Bigram)2.
Fine-grained POS and Coarse grainedBiCPOS.?
Post-Processing: Another learner is used to re-recognizeheads in stacks.?
Additional/External Resources: Non-Used2.1 Constant-time Parser and AnalysisThe Nivre?s algorithm makes use of a stack and aninput list to model the word dependency relationsvia identifying the transition action of the top tokenon the stack (Top) and the next token of the inputlist (Next).
Typically a learning algorithm can beused to recognize these actions via encoding fea-tures of the two terms (Top and Next).
The ?Left?and ?Reduce?
pops the Top from stack whereas the?Right?, ?Reduce-But-Shift?, and ?Shift?
push to-ken Next into the top of stack.
Nivre (Nivre, 2003)had proved that this algorithm can accomplish de-pendency parsing at most 2n transition actions.Although, the Nivre?s algorithm is much moreefficient than the others, it produces three problems.1.
It does not explicitly indicate which words arethe roots.2.
Some of the terms in the stack do not belongto the root but still should be parsed.3.
It always only compares the Top and Nextwords.The problem (2) and (3) are complement with eachother.
A straightforward way resolution is to adoptthe exhaustive parsing strategy (Covington, 2001).Unfortunately, such a brute-force way may causeexponential training and testing spaces, which isimpractical to apply to the large-scale corpus, forexample, the Czech Treebank (1.3 million words).To overcome this and keep the efficiency, we de-sign a post-processor that re-cycles the residuum inthe stack and re-identify the heads of them.
Sincemost of the terms (90-95%) of the terms had beprocessed in previous stages, the post-processorjust exhaustively parses a small part.
In addition,for problem (1), we propose a root parser based onthe parsed result of the Nivre?s algorithm.
We dis-cuss the root-parser and post-processor in the nexttwo subsections.2.2 Root ParserAfter the first stage, the stack may contain root andun-parsed words.
The root parser identifies the rootword in the stack.
The main advantage of thisstrategy could avoid sequential classification proc-ess, which only focuses on terms in the stack.We build a classifier, which learns to find rootword based on encoding context and children fea-tures.
However, most of the dependency relationswere constructed at the first stage.
Thus, we havemore sufficient head-modifier information rather242than only taking the contexts into account.
Theused features are listed as follows.Neighbor terms,bigrams,POS,BiCPOS (+/-2 window)Left most child term, POS, Bigram, BiCPOSRight most child term, POS, Bigram, BiCPOS2.3 Post-ProcessingBefore post-processing, we remove the root wordsfrom stack, which were identified by root-parser.The remaining un-parsed words in stack were usedto construct the actual dependency graph via ex-haustive comparing with parsed-words.
It is neces-sary to build a post-processor since there are about10% un-parsed words in each training set.
We pro-vide the un-parsed rate of each language in Table 2(the r.h.s.
part).By applying previous two steps (constant-timeparser and root parser) to the training data, the re-maining un-parsed tokens were recorded.
Not onlyusing the forward parsing direction, the backwarddirection is also taken into account in this statistics.Averagely, the un-parsed rates of the forward andbackward directions are 13% and 4% respectively.The back ward parsing often achieves lower un-parsed rate among all languages (except for Japa-nese and Turkish).To find the heads of the un-parsed words, wecopy the whole sentence into the word list again,and re-compare the un-parsed tokens (in stack) andall of the words in the input list.
Comparing withthe same words is disallowed.
The comparingprocess is going on until the actual head is found.Acquiescently, we use the nearest root words as itshead.
Although such a brute force way is time-consuming.
However, it only parses a small part ofun-parsed tokens (usually, 2 or 3 words per sen-tence).2.4 Features and LearnersFor the constant-time parser of the first stage, weemploy the features as follows.Basic features:Top.word,Top.pos,Top.lchild.pos,Top.lchild.relation,Top.rchild.pos, Top.rchild.relation,Top.head.pos,Top.head.relation,Next.word, Next.pos, Next.lchild.pos,Next.lchild.relation, Next+1.pos, Next+2.pos, Next+3.posEnhanced features:Top.bigram,Top.bicpos,Next.bigram,Next.bicpos,Next+1.word,Next+2.word,Next+3.wordIn this paper, we use the support vector machines(SVM) (Joachims, 1998) as the learner.
SVM iswidely used in many natural language processing(NLP) areas, for example, POS tagging (Wu et al,2006).
However, the SVM is a binary classifierwhich only recognizes true or false.
For multiclassproblem, we use the so-called one-versus-one(OVO) method with linear kernel to combine theresults of each pairwise subclassifier.
The finalclass in testing phase is mainly determined by ma-jority voting.For all languages, our parser uses the same set-tings and features.
For all the languages (exceptJapanese and Turkish), we use backward parsingdirection to keep the un-parsed token rate low.3 Experimental Result3.1 Dataset and Evaluation MetricsThe testing data is provided by the (Buchholz et al,2006) which consists of 13 language treebanks.The experimental results are mainly evaluated bythe unlabeled and labeled attachment scores.
TheCoNLL also provided a perl-scripter to automaticcompute these rates.3.2 System ResultsTable 2 presents the overall parsing performanceof the 13 languages.
As shown in Table 2, we listtwo parsing results at the second and third columns(new and old).
It is worth to note that the result Bis produced by removing the enhanced features andthe post-processing step from our parser, while theresult A is the complete use of the enhanced fea-tures and the overall three-step parsing.
In this year,we submit result B to the CoNLL shared task dueto the time limitation.In addition, we also apply the Maltparser, whichis implemented with the Nivre?s algorithm (Nivre,2003) to be compared.
The Maltpaser also includesthe SVM and memory-based learner (MBL).
Nev-ertheless, it does not optimize the SVM where thetraining and testing times are too long to be com-pared even the linear kernel is used.
Therefore weuse the default MBL and feature model 3 (M3) inthis experiment.
We also perform the significanttest to evaluate the statistical difference among thethree results.
If the answer is ?Yes?, it means thetwo systems are significant difference under atleast 95% confidence score (p < 0.05).243Table 2: A general statistical table of labeled attachment score, test and un-parsed rate (percentage)Statistic test Un-Parsed Rate A(New result)B(Old result)C(Maltparser) A vs. B B vs. C A vs. C Forward BackwardArabic 63.75 63.81 54.11 No Yes Yes 10.3 1.4Chinese 81.25 74.81 73.92 Yes No Yes 4.01 2.3Czech 71.24 59.36 59.36 Yes No Yes 16.1 5.6Danish 79.52 78.38 77.31 No No No 12.8 2.5Dutch 68.45 68.45 63.61 No Yes Yes 18.4 9.8German 79.57 76.52 76.52 Yes No Yes 12.7 9.2Japanese 91.43 90.11 89.07 Yes No Yes 1.1 4.4Portugese 81.33 81.47 75.38 No Yes Yes 24.3 3.17Slovene 68.41 67.83 55.04 No Yes Yes 14.9 5.5Spanish 74.65 72.99 72.81 Yes No Yes 20 0.5Swedish 79.53 71.72 76.28 Yes Yes Yes 19.1 2.8Turkish 55.33 55.09 52.18 No Yes Yes 2.5 4Bulgarian 81.23 79.73 79.73 No No No 15.7 1.2AVG 75.05 72.32 69.64 13.22 4.024 Discussion4.1 Analysis of Overview AspectAlthough our method is efficient for parsing thatachieves satisfactory result, it is still away from thestate-of-the-art performance.
Many problems giverise to not only the language-specific characteris-tics, but also the parsing strategy.
We found thatour method is weak to the large-scale training sizeand large dependency class datasets, for example,German (Brants et al, 2002) and Czech.
For Dutch,we observe that the large non-projective tokensand relations in this set.
Overall, we conclude thefour main limitations of our parsing model.1.Unbalanced and large dependency relationclasses2.Too fine or coarse POS tag3.Long sentences and non-projective token rates4.Feature engineering and root accuracyThe main reason of the first problem is still causedby the unbalanced distribution of the training data.Usually, the right-action categories obtain muchfewer training examples.
For example, in the Turk-ish data, 50 % of the categories receive less than0.1% of the training examples, 2/3 are the rightdependency group.
For the Czech, 74.6% of thecategories receive less than 0.1% of the trainingexamples.Second, the too fine grained size of POS tag  setoften cause the features too specific that is difficultto be generalized by the learner.
Although wefound the grained size is not the critical factor ofour parser, it is closely related to the fourth prob-lem, feature engineering.
For example, in Chinese(Chen et al, 2003), there are 303 fine grained POStypes which achieves better result on the labeledattachment score is higher than the coarse grained(81.25 vs. 81.17).
Intuitively, the feature combina-tions deeply affect the system performance (see Avs.
C where we extend more features than theoriginal Nivre?s algorithm).Problem 3 exposes the disadvantage of ourmethod, which is weak to identify the long dis-tance dependency.
The main reason is resultedfrom the Nivre?s algorithm in step 1.
This methodis quite sensitive and non error-recovered since it isa deterministic parsing strategy.
Abnormal orwrong push or pop actions usually cause the errorpropagation to the remaining words in the list.
Forexample, there are large parts of errors are causedby too early reduce or missed left arc makes somewords could not find the actual heads.
On the con-trary, one can use an N-best selection to choose theoptimal dependency graph or applying MST orexhaustive parsing schema.
Usually, these ap-proaches are quite inefficient which requires atleast O(n3).Finally, in this paper, we only take the surfacelexical word and POS tag into account withoutemploying the language-specific features, such asLemma, Morph?etc.
Actually, it is an open ques-tion to compile and investigate the feature engi-neering.
On the other hand, we also find theperformance of the root parser in some languagesis poor.
For example, for Dutch the root precisionrate is only 38.52, while the recall rate is 76.07.
Itindicates most of the words in stack were wronglyrecognized as root.
This is because there are sub-stantially un-parsed rate that left many un-parsedwords remain in stack.
One way to remedy theproblem can adjust the root parser to independentlyidentify root word by sequential word classifica-tion at first step and then apply the Nivre?s algo-rithm.
We left the comparison of the issue as futurework.2444.2 Analysis of Specific ViewWe select three languages, Arabic, Japanese, andTurkish to be more detail analysis.
Figure 2 illus-trates the learning curve of the three languages andTable 3 summarizes the comparisons of ?fine vs.coarse?
POS types and ?forward vs. backward?parsing directions.For the three languages, we found that most of theerrors frequently appear to the noun POS tagswhich often denominate half of the training set.
InTurkish, the lower performance on the noun POSattachment rate deeply influents the overall parsing.For example, the error rate of Noun in Turkish is39% which is the highest error rate.
On the con-trary, the head error rates fall in the middle rankfor the other two languages.?????????????????
??
??
??
??
??
??
??
??
??????????????????????????????????????????????????????????????
??????
??????
?Figure 2: Learning curve of the three datasetsTable 3: Parsing performance of different grainedPOS tags and forward/backward parsing directionsParsingdirection LA-ScorePOSgrained LA-ScoreJa Forward 91.35 Fine 91.35Backward 85.75ForwardCoarse 91.25Ar Forward 60.62 Fine 63.55Backward 63.55BackwardCoarse 63.63Tu Forward 55.47 Fine 55.47Backward 55.59ForwardCoarse 55.59In Turkish, we also find an interesting resultwhere the recall rate of the distance=2 parsing(56.87) is lower than distance=3-6, and >7 (62.65,57.83).
In other words, for Turkish, our parserfailed to recognize the distance=2 dependency rela-tions.
For the other languages, usually the identifi-cation rate of the longer distance parsing should belower than the smaller distance.
Thus, a futurework to parsing Turkish, should put more emphasison improving not only the noun POS type, but alsothe distance=2 parsing.Besides, the root parsing accuracy is also an im-portant factor to most languages.
In Japanese, al-though our parser achieves more than 97%left/right arc rates.
However, for the root word pre-cision rate is quite lower (85.97).
Among all de-pendency relation classification rates, the root classusually locates in the lowest rank for the three lan-guages.5 Conclusion and Future RemarksDependency parsing is one of the most importantissues in NLP community.
This paper presents andanalyzes the impact of the efficient parsing modelthat only combines with lexical and part-of-speechinformation.
To go language-independent, we didnot tune any parameter settings in our model andexclude most of the language-dependent feature set,which provided by the CoNLL (Buchholz et al,2006).
The main focus of our work coincides withthe target goal of the CoNLL shared task, i.e., gomultilingual dependency parsing without takingthe language-specific knowledge into account.
Afuture work on the deterministic parsing strategy isto convert the existing model toward N-best pars-ing.ReferencesS.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006.CoNLL-X Shared Task on Multilingual Dependency Pars-ing, In Proceedings of the Tenth Conf.
on ComputationalNatural Language Learning CoNLL-X.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In NAACL, pages 132-139.Michael Collins.
1998.
Head-driven statistical models fornatural language processing.
Ph.D. thesis.
University ofPennsylvania.Michael A. Covington.
2001.
A fundamental Algorithm forDependency Parsing.
In Proceedings of the Annual ACMSoutheast Conference, pages 95-102.Jason M. Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In COLING, pages340-345.Thornsten Joachims.
1998.
Text categorization with supportvector machines: learning with many relevant features.
InECML, pages 137-142.Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005.Online Large-Margin Training of Dependency Parsers, InACL, pages 91-98.Joakim Nivre.
2003.
An Efficient Algorithm for ProjectiveDependency Parsing.
In Proceedings of the InternationalWorkshop on Parsing Technology, pages 149-160.Yu C. Wu, Chia H. Chang, and Yue S. Lee.
2006.
A Generaland Multi-lingual Phrase Chunking Model based on Mask-ing Method.
In CICLING, pages 144-155.Hiroyasu Yamada, and Yuji Matsumoto.
2003.
StatisticalDependency Analysis with Support Vector Machines.
InProceedings of the International Workshop on ParsingTechnology, pages 195-206.245
