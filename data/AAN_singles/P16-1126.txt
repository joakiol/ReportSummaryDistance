Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1330?1340,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsThe Creation and Analysis of a Website Privacy Policy CorpusShomir Wilson1, Florian Schaub1, Aswarth Abhilash Dara1, Frederick Liu1,Sushain Cherivirala1, Pedro Giovanni Leon2, Mads Schaarup Andersen1,Sebastian Zimmeck3, Kanthashree Mysore Sathyendra1, N. Cameron Russell4,Thomas B. Norton4, Eduard Hovy1, Joel Reidenberg4and Norman Sadeh11Carnegie Mellon University, School of Computer Science, Pittsburgh, PA, USA2Stanford University, Center for Internet and Society, Stanford, CA, USA3Columbia University, Department of Computer Science, New York, NY, USA4Fordham University, Law School, New York, NY, USAshomir@cs.cmu.edu, sadeh@cs.cmu.eduAbstractWebsite privacy policies are often ignoredby Internet users, because these docu-ments tend to be long and difficult to un-derstand.
However, the significance of pri-vacy policies greatly exceeds the attentionpaid to them: these documents are bindinglegal agreements between website opera-tors and their users, and their opaquenessis a challenge not only to Internet users butalso to policy regulators.
One proposed al-ternative to the status quo is to automate orsemi-automate the extraction of salient de-tails from privacy policy text, using a com-bination of crowdsourcing, natural lan-guage processing, and machine learning.However, there has been a relative dearthof datasets appropriate for identifying datapractices in privacy policies.
To remedythis problem, we introduce a corpus of 115privacy policies (267K words) with man-ual annotations for 23K fine-grained datapractices.
We describe the process of us-ing skilled annotators and a purpose-builtannotation tool to produce the data.
Weprovide findings based on a census of theannotations and show results toward au-tomating the annotation procedure.
Fi-nally, we describe challenges and oppor-tunities for the research community to usethis corpus to advance research in both pri-vacy and language technologies.1 IntroductionPrivacy policies written in natural language area nearly pervasive feature of websites and mo-bile applications.
The ?notice and choice?
le-gal regimes of many countries require that web-site operators post a notice of how they gatherand process users?
information.
In theory, usersthen choose whether to accept those practices orto abstain from using the website or service.
Inpractice, however, the average Internet user strug-gles to understand the contents of privacy poli-cies (McDonald and Cranor, 2008) and generallydoes not read them (Federal Trade Commission,2012; President?s Concil of Advisors on Scienceand Technology, 2014).
This disconnect betweenInternet users and the data practices that affectthem has led to the assessment that the notice andchoice model is ineffective in the status quo (Rei-denberg et al, 2015b; Cate, 2010).Thus, an opening exists for language technolo-gies to help ?bridge the gap?
between privacy poli-cies in their current form and representations thatserve the needs of Internet users.
Such a bridgewould also serve unmet needs of policy regula-tors, who do not have the means to assess privacypolicies in large numbers.
Legal text is a familiardomain for natural language processing, and thelegal community has demonstrated some recipro-cal interest (Mahler, 2015).
However, the scaleof the problem and its significance?i.e., to vir-tually any Internet user, as well as to website op-erators and policy regulators?distinguishes it andprovides immense motivation (Sadeh et al, 2013).To this end, we introduce a corpus of 115 web-site privacy policies annotated with detailed in-formation about the data practices that they de-scribe.1This information consists of 23K datapractices, 128K practice attributes, and 103K an-notated text spans, all produced by skilled anno-1The dataset is available for download atwww.usableprivacy.org/data.1330tators.
To the best of our knowledge, this is thefirst large-scale effort to annotate privacy policiesat such a fine level of detail.
It exceeds prior ef-forts to annotate sentence-level fragments of pol-icy text (Breaux and Schaub, 2014), answer simpleoverarching questions about privacy policy con-tents (Wilson et al, 2016; Zimmeck and Bellovin,2014), or analyze the readability of privacy poli-cies (Massey et al, 2013).
We further present anal-ysis that demonstrates the richness of the corpusand the feasibility of partly automating the anno-tation of privacy policies.The remainder of this paper is structured as fol-lows.
We discuss related work and contextualizethe corpus we have created in Section 2.
In Section3 we describe the creation of the corpus, includingthe collection of a diverse set of policies and thecreation of a privacy policy annotation tool.
Sec-tion 4 presents analysis that illustrates the diver-sity and complexity of the corpus, and Section 5shows results on the prediction of policy structure.Finally, in Section 6 we describe some promisingavenues for future work.2 Related WorkPrior attempts on analyzing privacy policies fo-cused largely on manually assessing their usabil-ity (Jensen and Potts, 2004) or compliance withself-regulatory requirements (Hoke et al, 2015).Breaux et al proposed a description logic to ana-lyze and reason about data sharing properties inprivacy policies (2013), but rely on a small setof manually annotated privacy policies to instan-tiate their language.
Automated assessments havelargely focused on readability scores (Massey etal., 2013; Meiselwitz, 2013; Ermakova et al,2015).
Cranor et al leveraged the standardizedformat of privacy notices in the U.S. financial in-dustry to automatically analyze privacy polices offinancial institutions (2013).
However, in spite ofnotable efforts such as P3P (Wenning et al, 2006),the majority of privacy policies are unstructuredand do not follow standardized formats.Costante et al (2012) proposed a supervisedlearning approach to determine which data prac-tice categories are covered in a privacy policy.Rule-based extraction techniques have been pro-posed to extract some of a website?s data collec-tion practices from its privacy policy (Costante etal., 2013) or to answer certain binary questionsabout a privacy policy (Zimmeck and Bellovin,2014).
Other approaches leverage topic mod-eling (Chundi and Subramaniam, 2014; Stameyand Rossi, 2009) or sequence alignment tech-niques (Liu et al, 2014; Ramanath et al, 2014)to analyze privacy policies or identify similar pol-icy sections and paragraphs.
However, the com-plexity and vagueness of privacy policies makesit difficult to automatically extract complex datapractices from privacy policies without substantialgold standard data.Crowdsourcing has been proposed as a potentialapproach to obtain annotations for privacy poli-cies (Sadeh et al, 2013; Breaux and Schaub, 2014;Wilson et al, 2016).
However, crowdworkers arenot trained in understanding and interpreting legaldocuments, which may result in interpretation dis-crepancies compared to experts (Reidenberg et al,2015a).
Our policy annotation tool shares somecommon features with GATE (Bontcheva et al,2013), although the interface for our tool is sim-pler to fit the specific requirements of the task.Few prior efforts, aside from those we citeabove, have applied natural language processingto privacy policies or other legal documents pur-ported for the general public to regularly read.More generally, legal text has a history of atten-tion from natural language processing (Bach etal., 2013; Galgani et al, 2012; Francesconi et al,2010) and from artificial intelligence (Sartor andRotolo, 2013; Bench-Capon et al, 2012).
Classi-fying legal text into categories has received someinterest (?Savelka and Ashley, 2015; Mickevicius etal., 2015), as well as making the contents of legaltexts more accessible (Boella et al, 2015; Curtottiand McCreath, 2013).Compared to prior efforts, our data set is notablefor its combination of size, input from experts (forthe label scheme) and skilled annotators (for theannotation procedure), and fine-grained detail.3 Corpus Creation and StructureIn this section we describe our procedure for se-lecting a diverse set of privacy policies, our anno-tation scheme, how we obtained annotations, andthe structure of the corpus.3.1 Privacy Policy SelectionPrivacy policies vary in length, complexity, legalsophistication, and coverage of services.
For in-stance, privacy policies of large companies maycover multiple services, websites, apps, and evenphysical stores; such policies are often crafted bylegal teams and frequently updated.
Privacy poli-1331cies of smaller or less popular companies mayhave narrower focus or vary in employed lan-guage, and they may be updated less frequently.To reflect this diversity, we used a two-step process for policy selection: (1) relevance-based website pre-selection and (2) sector-basedsub-sampling.
First, we monitored GoogleTrends (Google, 2015) for one month (May 2015)to collect the top five search queries for each trend.Then, for each query we retrieved the first fivewebsites listed on each of the first 10 pages of re-sults.
This process produced a diverse sample of1,799 unique websites.Second, we sub-sampled from this websitedataset according to DMOZ.org?s top-level web-site sectors.2More specifically, we organized thedataset into 15 sectors (e.g., Arts, Shopping, Busi-ness, News).
We excluded the ?World?
sector andlimited the ?Regional?
sector to the ?U.S.?
sub-sector in order to ensure that all privacy policies inour corpus are subject to the same legal and regula-tory requirements.
We ranked the websites in eachsector according to their frequency in the retrievedsearch results.
Then we selected eight websitesfrom each sector by randomly chosing two web-sites from each rank quartile.For each selected website, we manually verifiedthat it had an English-language privacy policy andthat it pertained to a US company (based on con-tact information and WHOIS entry) before down-loading its privacy policy.
Excluded websites werereplaced with random re-draws from the same sec-tor rank quartile.
Some privacy policies coveredmore than one of the selected websites (e.g., theDisney privacy policy covered disney.go.com andespn.go.com), resulting in a final dataset of 115privacy policies across 15 sectors.3.2 Annotation Scheme and ProcessWe developed a policy annotation scheme to cap-ture the data practices specified by privacy poli-cies.
To ensure the scheme reflected actual policycontents, development occurred as an iterative re-finement process, in which a small group of do-main experts (privacy experts, public policy ex-perts, and legal scholars) identified different datapractice categories and their descriptive attributesfrom multiple privacy policies.
The annotationscheme was then applied to additional policies andrefined over multiple iterations during discussions2The DMOZ.org website sectors are notable for their useby Alexa.com.among the experts.The final annotation scheme consists of ten datapractice categories:1.
First Party Collection/Use: how and why aservice provider collects user information.2.
Third Party Sharing/Collection: how user in-formation may be shared with or collected bythird parties.3.
User Choice/Control: choices and controloptions available to users.4.
User Access, Edit, & Deletion: if and howusers may access, edit, or delete their infor-mation.5.
Data Retention: how long user information isstored.6.
Data Security: how user information is pro-tected.7.
Policy Change: if and how users will be in-formed about changes to the privacy policy.8.
Do Not Track: if and how Do Not Track sig-nals3for online tracking and advertising arehonored.9.
International & Specific Audiences: practicesthat pertain only to a specific group of users(e.g., children, Europeans, or California resi-dents).10.
Other: additional sub-labels for introduc-tory or general text, contact information, andpractices not covered by the other categories.An individual data practice belongs to one ofthe ten categories above, and it is articulated bya category-specific set of attributes.
For exam-ple, a User Choice/Control data practice is associ-ated with four mandatory attributes (Choice Type,Choice Scope, Personal Information Type, Pur-pose) and one optional attribute (User Type).
Theannotation scheme defines a set of potential valuesfor each attribute.
To ground the data practice inthe policy text, each attribute also may be associ-ated with a text span in the privacy policy.The set of mandatory and optional attributes re-flects the potential level of specificity with whicha data practice of a given category may be de-scribed.
Optional attributes are less common,while mandatory attributes are necessary to rep-resent a data practice.
However, privacy policiesare often vague or ambiguous on many of these at-tributes.
Therefore, a valid value for each attributeis Unspecified, allowing annotators to express anabsence of information.3www.w3.org/2011/tracking-protection1332Documents 115Words 266,713Annotated Data Practices 23,194Annotated Attributes 128,347Annotated Text Spans 102,576Annotators Per Document 3Annotators Total 10Table 1: Totalized statistics on the corpus.We developed a web-based annotation tool,shown in Figure 1, for skilled annotators to ap-ply our annotation scheme to the selected privacypolicies.4In preparation, privacy policies were di-vided into paragraph-length segments for annota-tors to read in the tool, one at a time in sequence.For each segment, an annotator may label zero ormore data practices from each category.
To createa data practice, an annotator first selects a practicecategory and then specifies values and text spansfor each of its attributes.
Annotators can see a listof data practices they have created for a segmentand selectively duplicate and edit them to annotatepractices that differ only slightly, though we omitthese features from the figure for brevity.4 Composition of the OPP-115 CorpusThe annotation process produced a nuanced anddiverse dataset, which we describe in detail below.We name the dataset the OPP-115 Corpus (OnlinePrivacy Policies, set of 115) for convenience.4.1 Policy ContentsTable 1 shows some descriptive statistics for thecorpus as a whole.
Each privacy policy was readby three skilled annotators, who worked indepen-dently, and a total of ten annotators participated inthe process.
All annotators were law students andwere compensated for their work at rates appropri-ate for student employees at their respective uni-versities.
They required a mean of 72 minutes perpolicy, though this number is slightly inflated byoutliers when they stepped away from in-progresssessions for extended periods of time.
The anno-tators produced a total of 23K data practices, al-though this number contains some redundanciesbetween annotators?
efforts.5In aggregate, these4Our experts for the annotation scheme development andour skilled annotators were mutually exclusive groups.5We describe a method to consolidate annotations (i.e., toeliminate redundancies between annotators?
data) in Section4.2.
Here, we analyze policy contents pre-consolidation toavoid propagating the effects of nontrivial assumptions nec-data practices are associated with 128K values forattributes and 103K selected spans of policy text.Note that the annotation tool required the selec-tion of a text span for mandatory attributes, but didnot require a text-based justification for optionalattributes or attributes marked as ?Unspecified?.The corpus allows us to investigate the compo-sition of typical privacy policies in terms of datapractices.
Privacy policies are known for theirlength and complexity, but those notions do notnecessarily entail a density of pertinent informa-tion.
Table 2 shows the pre-consolidation quanti-ties of practices that we collected in each of theten annotation categories, along with the meanand median counts of practices per privacy policy.Intuitively, First Party Collection/Use and ThirdParty Sharing/Collection dominated the rankingsby frequency: the collection, usage, and sharingof user data are the primary concerns that compelthe production of privacy policies.
Data practicesin the Other category, while frequent, were mostlystatements that were ostensibly not about userdata; 57% were introductory, contact, or genericinformation.
Means were above medians for allcategories, reflecting rightward skews for all thedistributions.Table 2 also contains statistics on segment-levelcategory coverage and annotator agreement.
Here,coverage is meant in an ipso facto sense: a prac-tice category covers a policy segment if two ofthree annotators each identified at least one prac-tice from that category in the segment text.
Differ-ences in the category rankings by frequency andby coverage reveal that practices in some cate-gories are less tightly clustered than others.
In par-ticular, Data Retention is the second rarest prac-tice category but ranks fourth by segment cov-erage.
Since Kappa is applied here to an artifi-cial task (annotators were not asked to label en-tire segments) the common conventions for its in-terpretation (Carletta, 1996; Viera and Garrett,2005) are not directly applicable.
However, DoNot Track and International and Specific Audi-ences remain standout categories with the great-est segment-level agreement.
We hypothesize thatthese two categories have the most easily recog-nizable cues for annotation.
Do Not Track prac-tices, for example, are associated with the epony-mous phrase.Finally, the pre-consolidation mean and medianessary for consolidation.1333Figure 1: Web-based tool for our skilled annotators of privacy policies.Category Freq.
Mean Median Coverage Fleiss?
KappaFirst Party Collection/Use 8,956 78 74 .27 .76Third Party Sharing/Collection 5,230 45 39 .21 .76Other 3,551 31 25 .24 .49User Choice/Control 1,791 16 13 .08 .61Data Security 1,009 9 7 .05 .67International and Specific Audiences 941 8 6 .07 .87User Access, Edit and Deletion 747 6 5 .03 .74Policy Change 550 5 4 .03 .73Data Retention 370 3 2 .20 .55Do Not Track 90 1 0 .01 .91Table 2: By-category descriptive statistics for the data practices in the corpus.
These statistics are cal-culated prior to consolidating multiple annotators?
work.
Means and medians are calculated across thepopulation of policies in the corpus.
Coverage and Kappa are calculated in terms of by-segment contents.0 5 10 15 20 25600550500450400350300250200150100<50FrequencyData PracticesFigure 2: Distribution of data practices per policy.quantities of data practices per policy were 202and 200, respectively.
These do not correspondto columnar totals that may be calculated fromTable 2 because the categories were not equallydistributed among the privacy policies.
Figure 2shows the distribution of quantities of data prac-tices per policy.
The distribution exhibits a skewtoward larger numbers of data practices per pol-icy.
Importantly, differences in the number ofdata practices should not be interpreted as vary-ing levels of data protection or privacy.
A privacypolicy that contains many data practices may ex-hibit substantial redundancy among them, and aprivacy policy with relatively few data practicescould merely be concise.
In either case, the datapractices may be responsive to users concerns orat odds with them.13344.2 Consolidating Annotators?
WorkIn this section we discuss the problem of consol-idation, or merging data practices from multipleannotators if those practices refer to the same un-derlying practice expressed by the text.
The ambi-guity and vagueness of privacy policies (Reiden-berg et al, 2016) and the sophistication of the an-notation scheme are natural limitations on anno-tator agreement.
With that in mind, we present aconsolidation procedure to collapse redundant an-notations with the proviso that practices labeled byonly one or two skilled annotators also have sub-stantial value and merit retention.First, we institute some basic requirementsabout locality and topicality to determine whichdata practices are eligible for consolidation.
Givena segment, if annotators A1, ..., An(for n = 2 orn = 3 in our dataset) respectively produce setsof data practices P1, ..., Pn, then a selection ofdata practices p1?
P1, ..., pn?
Pnis eligible tobe consolidated into a single data practice only ifall of them belong to the same category.
Addition-ally, three implicit assumptions in this requirementare that (1) at least two annotators contribute prac-tices to a consolidation set, (2) all the practices arelocated in the same policy segment, and (3) eachpractice must belong to a unique annotator.For each segment we create an exhaustive listof eligible combinations of data practices to con-solidate, score and rank each combination using amethod detailed below, prune the list with a scorethreshold, and finally perform consolidations inorder of ranking until no further consolidations arepossible.
Consolidation sets containing three an-notators?
practices are considered prior to sets con-taining practices from only two annotators.
Thedata practices in a chosen consolidation set areremoved and replaced by a single ?master?
datapractice.
To do this, it is necessary to merge setsof values and sets of text spans respectively associ-ated with each attribute.
Sets of values are mergedusing a majority vote if possible and set to Unspec-ified if otherwise; the latter case occurs in approxi-mately a third of all mergers.
Sets of text spans aremerged with a strong bias toward recall, by creat-ing a new text span that begins and ends with re-spectively the first and last indexes in the set.Our scoring method is based on the summativeoverlap between the sets of text spans associatedwith attributes of data practices, with normaliza-tion to account for longer spans.
Since the text??????????????????????????????????????
????
????
????
????
?????????????????????????????????????????????????????????
?Figure 3: Consolidation threshold value versus theaverage number of data practices per segment pro-duced by consolidation.spans connect data practices to the policy text, weuse their overlaps as evidence that two annotators?data practices refer to the same underlying practicein the text.
Thus, a score for two data practices thatare associated with roughly the same policy text isrelatively high, and a score for two data practicesthat are associated with different text is low.Figure 3 shows the effect of the consolidationthreshold on the average number of practices pro-duced by consolidation per policy segment (i.e.,excluding those original data practices that wereretained because they were not subject to consoli-dation).
Past a threshold value of approximately0.2, the number of practices steadily decreases.Notably the average number of practices producedby consolidation is substantially less than the av-erage practices per annotator per segment (2.04)at any point on the curve, indicating a relative lackof agreement between annotators in terms of textspan selections.
As part of the corpus, we releaseconsolidated datasets at threshold values of 0.5,0.75, and 1.4.3 Data Exploration WebsiteThe data practice annotations are difficult for hu-man readers to interpret without visual connec-tions to the policy text.
To help researchers, pol-icy regulators, and the general public understandthe structure and utility of the data set, we cre-ated a data exploration website6that visually inte-grates the data practice annotations with the textsof privacy policies.
Site interactivity allows usersto search for websites in the dataset or browse byDMOZ sectors.The website also allows users to compare pri-vacy policies by categorical structure, data prac-6explore.usableprivacy.org1335Figure 4: Comparing five policies on the data exploration website.tice quantities, and reading level.7Figure 4 showsa sample comparison between five websites.
Eachpolicy?s segments are depicted in order from leftto right.
Segments are colored according to thepractice categories that annotators labeled withinthem.
Qualitative patterns are discernible; for ex-ample, several of these policies have large blocksof First Party Collection/Use toward the beginningor large blocks of Third Party Sharing/Collectionfurther inward.
We discuss exploiting such recur-ring structures in the next section.5 Prediction of Policy StructureThe current human annotation procedure is im-practical for covering the entire Internet or ac-counting for changes in privacy policies.
Thisraises the question of whether the process can bepartly automated.
In this section we describe ourexperiments to automatically assign category la-bels to policy segments, which would enable thesimplification of the annotation task.5.1 ExperimentsOur dataset consisted of 3,792 segments from 115privacy policies.
We represented the text of eachsegment as a dense vector using Paragraph2Vec(Le and Mikolov, 2014) and the GENSIM toolkit(?Reh?u?rek and Sojka, 2010).
This approach ex-ploited semantic similarities between words inthe vocabulary of privacy policies, acknowledg-ing that the vocabulary in this domain is special-ized but not completely standardized.
We assignedeach policy segment a binary vector of category-specific labels, with each element in the vectorcorresponding to the presence or absence of a datapractice category in the segment.
We considereda vector with twelve elements, with nine of them7explore.usableprivacy.org/comparecoming from existing practice categories (all ex-cept Other).
The remaining three came from el-evating three attributes of Other to category sta-tus: Introductory/Generic, Practice Not Covered,and Privacy Contact Information.
We created goldstandard data for this problem using a simplifiedconsolidation approach: if two or more annotatorsagreed that a category is present in a segment, thenwe labeled that segment with the category.To predict the category labels of privacy policysegments, we tried three approaches.
Two werelogistic regression and SVM models, for whichwe treated this as a multi-class classification prob-lem.
Since 212unique category vectors exist, wetrimmed the label space to only those that occurin the training set.
The third was a sequence la-beling approach inspired by prior work to applyhidden Markov models (HMMs) to privacy pol-icy text (Ramanath et al, 2014).
Our work dif-fers from this prior work by using labels froman annotation scheme constructed by privacy ex-perts rather than topics developed from an un-supervised method.
Additionally, in our formu-lation, each hidden state corresponds to one ofthe unique binary vectors that represent classes ofcategory combinations in the training data.
TheHMM?s transition probabilities capture the ten-dency of privacy policy authors to organize topics(i.e., practice categories in our annotation scheme)in similar sequences.
Since each segment is repre-sented by a unique real-valued vector from Para-graph2Vec, it was not possible to directly obtainan emission probability distribution from the train-ing data.
Therefore, we ran the K-Means++ algo-rithm using the scikit-learn toolkit (Pedregosa etal., 2011) on the segment vector representationsand assigned each segment to a cluster.
The emis-sion probability distribution then captured the ten-dencies of a given class and generated the segment1336that is represented as a cluster.
These two distribu-tions are estimated empirically from the trainingdata, and we used Viterbi decoding to obtain thebest labeling sequence during the prediction.5.2 ResultsWe split the set of 115 policies into subsets of 75for training and 40 for testing.
The number of clus-ters in the HMM approach8is set to 100 and the re-sults are shown in Table 3 as means across 10 runs.The standard deviations for these performance fig-ures are generally between 0.01 and 0.05; the oneexception is Do Not Track (the least frequent cat-egory) with a standard deviation of 0.2.
As the ta-ble shows, although the HMM does not reach thesame performance as SVM, it performs similarlyto logistic regression and meets or exceeds its F1-score for five categories.We interpret the strength of the SVM as indi-cating the strong potential to partly automate thepolicy labeling procedure, especially for two cate-gories: First Party Collection/Use (a standout per-formance and the category for which the most la-beled data exists) and Do Not Track (a perfectperformance, likely due to the limited vocabularyused to describe practices in this category).
Addi-tionally, while the HMM did not perform as welloverall, we note that its micro-average F1 was aslight improvement over logistic regression.
Withrelatively little data to train this HMM, we expectthat the accumulation of more labeled instancescan improve its performance substantially.6 Future DirectionsThe OPP-115 Corpus enables research in sev-eral directions of interest to natural language pro-cessing and usable privacy.
We sketch some op-portunites for future work below.A central challenge for this research is the scal-ability of policy annotation.
Although it was nec-essary to annotate the first 115 policies manu-ally, to ensure the annotations were responsive tothe annotation scheme, a less labor-intensive ap-proach will be required for large-scale Web cov-erage.
The OPP-115 Corpus is a valuable datasetfor this move toward automated methods.
Addi-tionally, a strong potential exists for a combina-tion of automated annotation of coarse informa-tion and human annotation of finer details.
For8We tuned the parameters of the HMM approach andSVM after performing a five-fold cross validation on thetraining data.example, automated category labeling of policysegments is feasible, as demonstrated in Section5.
Asking a human to label practices in a singlecategory would be a reduction in effort, especiallyif they are shown text that is relevant to the cat-egory.
Crowdsourcing also becomes a possibilitywhen the complexity of the task is reduced.An ambitious goal will be to eliminate humanannotators altogether.
Our preliminary analysishas shown that the policy vocabularies associatedwith certain annotations are very distinctive (e.g.,the Do Not Track category or financial informationas a data type, for example), lending themselves toautomatic identification.
By producing confidenceratings alongside data practice predictions, an au-tomated system could mitigate its shortcomings.Separately, data practices must be presented toInternet users in a way that is responsive to theirconcerns.
Text summarization is a possibility, us-ing the annotations as a guide for important detailsto retain.
Internet users have already demonstratedlimited patience with text-based privacy policies,which adds a nuance to this challenge and sug-gests the need for a combination of text and picto-rial representations (or chiefly pictorial represen-tations) to communicate data practices (Schaub etal., 2015).Additional questions of interest include:?
How can the data practice annotations fora policy be combined into a cohesive inter-pretation?
The relationships between datapractices are not straightforward.
Vague-ness, contradictions, and unclear scope are allproblems for constructing a knowledge baseof them.?
How can the balance between human andautomated methods for annotation be opti-mized?
The model for the ideal combina-tion is subject to parameters such as the avail-ability of resources and the necessary level ofconfidence for annotations.?
How can sectoral norms and outliers be iden-tified automatically?
A bank website thatcollects users?
health information, for exam-ple, deserves scrutiny.
It seems appropriateto address this question with clustering tech-niques, using features from the data practicesand from the policy text.1337LR SVM HMMCategory P R F P R F P R FFirst Party Collection/Use 0.73 0.67 0.70 0.76 0.73 0.75 0.69 0.76 0.72Third Party Sharing/Collection 0.64 0.63 0.63 0.67 0.73 0.70 0.63 0.61 0.62User Choice/Control 0.45 0.62 0.52 0.65 0.58 0.61 0.47 0.33 0.39Introductory/Generic* 0.51 0.50 0.50 0.58 0.49 0.53 0.54 0.49 0.51Data Security 0.48 0.75 0.59 0.66 0.67 0.67 0.67 0.53 0.59Internat?l and Specific Audiences 0.49 0.69 0.57 0.70 0.70 0.70 0.67 0.66 0.66Privacy Contact Information* 0.34 0.72 0.46 0.60 0.68 0.64 0.48 0.59 0.53User Access, Edit, and Deletion 0.47 0.71 0.57 0.67 0.56 0.61 0.48 0.42 0.45Practice Not Covered* 0.20 0.47 0.28 0.19 0.26 0.22 0.15 0.12 0.13Policy Change 0.59 0.83 0.69 0.66 0.88 0.75 0.52 0.68 0.59Data Retention 0.10 0.35 0.16 0.12 0.12 0.12 0.08 0.12 0.09Do Not Track 0.45 1.0 0.62 1.0 1.0 1.0 0.45 0.40 0.41Micro-Average 0.53 0.65 0.58 0.66 0.66 0.66 0.60 0.59 0.60Table 3: Precision/Recall/F1 for the three models.
The three starred categories resulted from the decom-position of the original Other category, which is excluded here.
Categories are ordered in this table indescending order by frequency in the dataset.7 ConclusionWe have described the motivation, creation, andanalysis of a unique corpus of 115 privacy policiesand 23K fine-grained data practice annotations,and we have demonstrated the feasibility of partlyautomating the annotation process.
The annota-tions reveal the structure and complexity of thesedocuments, which Internet users are expected tounderstand and accept.
This corpus should serveas a resource for language technologies researchto help Internet users understand the privacy prac-tices of businesses and other entities that they in-teract with online.AcknowledgementsThis work is funded by the National Sci-ence Foundation under grants CNS-1330596 andCNS-1330214.
The authors would like to ac-knowledge the law students at Fordham Uni-versity and the University of Pittsburgh whoworked as annotators to make this corpus possi-ble.
The authors also wish to acknowledge allmembers of the Usable Privacy Policy Project(www.usableprivacy.org) for their contri-butions.ReferencesNgo Xuan Bach, Nguyen Le Minh, Tran Thi Oanh, andAkira Shimazu.
2013.
A two-phase framework forlearning logical structures of paragraphs in legal ar-ticles.
ACM Transactions on Asian Language Infor-mation Processing (TALIP), 12(1):3:1?3:32, March.Trevor Bench-Capon, Micha?
Araszkiewicz, KevinAshley, Katie Atkinson, Floris Bex, Filipe Borges,Daniele Bourcier, Paul Bourgine, Jack G Conrad,Enrico Francesconi, et al 2012.
A History of AIand Law in 50 Papers: 25 Years of the InternationalConference on AI and Law.
Artificial Intelligenceand Law, 20(3):215?319.Guido Boella, Luigi Di Caro, Michele Graziadei,Loredana Cupi, Carlo Emilio Salaroglio, LlioHumphreys, Hristo Konstantinov, Kornel Marko,Livio Robaldo, Claudio Ruffini, et al 2015.
Link-ing legal open data: Breaking the accessibility andlanguage barrier in European legislation and caselaw.
In Proceedings of the 15th International Con-ference on Artificial Intelligence and Law, ICAIL?15, pages 171?175.
ACM.Kalina Bontcheva, Hamish Cunningham, Ian Roberts,Angus Roberts, Valentin Tablan, Niraj Aswani, andGenevieve Gorrell.
2013.
GATE teamware: aweb-based, collaborative text annotation framework.Language Resources and Evaluation, 47(4):1007?1029.Travis D. Breaux and Florian Schaub.
2014.
Scal-ing requirements extraction to the crowd.
In RE?14:Proceedings of the 22nd IEEE International Re-quirements Engineering Conference, RE?
14, pages163?172, Washington, DC, USA, August.
IEEE So-ciety Press.Travis D. Breaux, Hanan Hibshi, and Ashwini Rao.2013.
Eddy, a formal language for specifying andanalyzing data flow specifications for conflictingprivacy requirements.
Requirements Engineering,19(3):281?307, September.1338Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254, June.F.H.
Cate.
2010.
The limits of notice and choice.
IEEESecurity Privacy, 8(2):59?62, March.Parvathi Chundi and Pranav M. Subramaniam.
2014.An approach to analyze web privacy policy docu-ments.
In KDD Workshop on Data Mining for So-cial Good.Elisa Costante, Yuanhao Sun, Milan Petkovi?c, andJerry den Hartog.
2012.
A machine learning solu-tion to assess privacy policy completeness.
In Pro-ceedings of the 2012 ACM Workshop on Privacyin the Electronic Society, WPES ?12, pages 91?96.ACM.Elisa Costante, Jerry den Hartog, and Milan Petkovi?c.2013.
What websites know about you: Privacypolicy analysis using information extraction.
InRoberto Di Pietro, Javier Herranz, Ernesto Dami-ani, and Radu State, editors, Data Privacy Manage-ment and Autonomous Spontaneous Security, vol-ume 7731 of Lecture Notes in Computer Science,pages 146?159.
Springer.Lorrie Faith Cranor, Kelly Idouchi, Pedro GiovanniLeon, Manya Sleeper, and Blase Ur.
2013.
Are theyactually any different?
Comparing thousands of fi-nancial institutions?
privacy practices.
In Workshopon the Economics of Information Security, WEIS?13, June.Michael Curtotti and Eric McCreath.
2013.
A rightto access implies a right to know: An open onlineplatform for research on the readability of law.
J.Open Access L., 1:1?56.Tatiana Ermakova, Benjamin Fabian, and EleonoraBabina.
2015.
Readability of privacy policies ofhealthcare websites.
In 12.
Internationale TagungWirtschaftsinformatik (Wirtschaftsinformatik 2015).Federal Trade Commission.
2012.
Protecting con-sumer privacy in an era of rapid change: Rec-ommendations for businesses and policymakers.http://www.ftc.gov/reports.
Last ac-cessed: June 22, 2016.Enrico Francesconi, Simonetta Montemagni, Wim Pe-ters, and Daniela Tiscornia.
2010.
Semantic pro-cessing of legal texts: Where the language of lawmeets the law of language, volume 6036 of LectureNotes in Artificial Intelligence.
Springer.Filippo Galgani, Paul Compton, and Achim Hoffmann.2012.
Combining different summarization tech-niques for legal text.
In Proceedings of the Work-shop on Innovative Hybrid Approaches to the Pro-cessing of Textual Data, HYBRID ?12, pages 115?123.
ACL.Google.
2015.
Google Trends.
https://www.google.com/trends/.
Last accessed: June 22,2016.Candice Hoke, Lorrie Cranor, Pedro Leon, and AlyssaAu.
2015.
Are they worth reading?
An in-depthanalysis of online tracker?s privacy policies.
I/S: AJournal of Law and Policy for the Information Soci-ety, 11(2):325?404, April.Carlos Jensen and Colin Potts.
2004.
Privacy policiesas decision-making tools: An evaluation of onlineprivacy notices.
In Proceedings of the SIGCHI Con-ference on Human Factors in Computing Systems,CHI ?04, pages 471?478.
ACM.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
arXivpreprint arXiv:1405.4053.Fei Liu, Rohan Ramanath, Norman Sadeh, andNoah A. Smith.
2014.
A step towards usable pri-vacy policy: Automatic alignment of privacy state-ments.
In Proceedings of COLING 2014, the 25thInternational Conference on Computational Lin-guistics: Technical Papers, COLING ?14, pages884?894.
ANLP, August.Lars Mahler.
2015.
What is NLP and whyshould lawyers care?
http://www.lawpracticetoday.org/article/nlp-lawyers/, February.
Last accessed:June 22, 2016.Aaron K. Massey, Jacob Eisenstein, Annie I. Ant?on,and Peter P. Swire.
2013.
Automated text miningfor requirements analysis of policy documents.
In21st IEEE International Requirements EngineeringConference, RE ?13, pages 4?13.
IEEE.Aleecia M. McDonald and Lorrie Faith Cranor.
2008.The cost of reading privacy policies.
I/S: J Law &Policy Info.
Soc., 4(3):540?561.Gabriele Meiselwitz.
2013.
Readability assessment ofpolicies and procedures of social networking sites.In A. Ant Ozok and Panayiotis Zaphiris, editors, 5thInternational conference, OCSC 2013, Held as Partof HCI International 2013, Lecture Notes in Com-puter Science, pages 67?75.
Springer, January.Vytautas Mickevicius, Tomas Krilavicius, and VaidasMorkevicius.
2015.
Classification of short legalLithuanian texts.
In The 5th Workshop on Balto-Slavic Natural Language Processing, BSNLP ?15,pages 106?111.
ACM.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.President?s Concil of Advisors on Science and Tech-nology.
2014.
Big data and privacy: a technologicalperspective.
Report to the President, Executive Of-fice of the President, May.1339Rohan Ramanath, Fei Liu, Norman Sadeh, andNoah A. Smith.
2014.
Unsupervised alignment ofprivacy policies using hidden markov models.
InProceedings of the Annual Meeting of the Associ-ation of Computational Linguistics, ACL ?14, pages605?610.
ACL, June.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50.
ELRA, May.Joel R. Reidenberg, Travis Breaux, Lorrie Faith Cranor,Brian French, Amanda Grannis, James T. Graves,Fei Liu, Aleecia McDonald, Thomas B. Norton, Ro-han Ramanath, N. Cameron Russell, Norman Sadeh,and Florian Schaub.
2015a.
Disagreeable privacypolicies: Mismatches between meaning and users?understanding.
Berkeley Technology Law Journal,30(1):39?88.Joel R. Reidenberg, N. Cameron Russell, Alexan-der J. Callen, Sophia Qasir, and Thomas B. Norton.2015b.
Privacy harms and the effectiveness of thenotice and choice framework.
I/S: J Law & PolicyInfo.
Soc., 11(2).Joel R. Reidenberg, Jaspreet Bhatia, Travis Breaux, andThomas B. Norton.
2016.
Automated comparisonsof ambiguity in privacy policies and the impact ofregulation.
Social Science Research Network Work-ing Paper Series, January.Norman Sadeh, Alessandro Acquisti, Travis D. Breaux,Lorrie Faith Cranor, Aleecia M. Mcdonald, Joel R.Reidenberg, Noah A. Smith, Fei Liu, N. CameronRussell, Florian Schaub, and Shomir Wilson.
2013.The usable privacy policy project: Combiningcrowdsourcing, machine learning and natural lan-guage processing to semi-automatically answerthose privacy questions users care about.
TechnicalReport CMU-ISR-13-119, Carnegie Mellon Univer-sity.Giovanni Sartor and Antonino Rotolo.
2013.
AI andlaw.
In Agreement Technologies, pages 199?207.Springer.Jarom?
?r?Savelka and Kevin D Ashley.
2015.
Transferof predictive models for classification of statutorytexts in multi-jurisdictional settings.
In Proceedingsof the 15th International Conference on ArtificialIntelligence and Law, ICAIL ?15, pages 216?220.ACM.Florian Schaub, Rebecca Balebako, Adam L. Durity,and Lorrie Faith Cranor.
2015.
A design space foreffective privacy notices.
In Eleventh SymposiumOn Usable Privacy and Security, SOUPS ?15, pages1?17.
USENIX Association, July.John W. Stamey and Ryan A. Rossi.
2009.
Auto-matically identifying relations in privacy policies.In 27th ACM International Conference on Designof Communication, SIGDOC ?09, pages 233?238.ACM.Anthony J. Viera and Joanne M. Garrett.
2005.
Under-standing interobserver agreement: the kappa statis-tic.
Family Medicine, 37(5):360?363, 5.Rigo Wenning, Matthias Schunter, Lorrie Cranor,B.
Dobbs, S. Egelman, G. Hogben, J. Humphrey,M.
Langheinrich, M. Marchiori, M. Presler-Marshall, J. Reagle, and D. A. Stampley.
2006.
Theplatform for privacy preferences 1.1 (P3P 1.1) spec-ification.Shomir Wilson, Florian Schaub, Rohan Ramanath,Norman Sadeh, Fei Liu, Noah A. Smith, and Freder-ick Liu.
2016.
Crowdsourcing annotations for web-sites?
privacy policies: Can it really work?
In Pro-ceedings of the 25th World Wide Web Conference,WWW ?13, pages 133?143.Sebastian Zimmeck and Steven M. Bellovin.
2014.Privee: An architecture for automatically analyz-ing web privacy policies.
In 23rd USENIX Secu-rity Symposium, USENIX Security ?14, pages 1?16.USENIX Association, August.1340
