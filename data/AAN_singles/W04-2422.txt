Learning Transformation Rulesfor Semantic Role LabelingKen Williams Christopher DozierResearch & DevelopmentThomson Legal and RegulatoryEagan, MN 55123 USA{ken.williams|chris.dozier|andrew.mcculloh}@thomson.comAndrew McCullohAbstractThis paper presents our work on Semantic RoleLabeling using a Transformation-Based Error-Driven approach in the style of Eric Brill (Brill,1995).
Our approach achieved an overall F1score of 43.48 on non-verb annotations.
Webelieve our approach is noteworthy because ofits novelty in this area and because it producesshort lists of human-understandable transfor-mation rules as its output.1 Introduction to Transformation-BasedError-Driven LearningFor the 2004 Conference on Computational Natural Lan-guage Learning (CoNLL), our team has applied themethodology popularized by Eric Brill for part-of-speechtagging and linguistic parsing (Brill, 1995; Brill, 1993).In this methodology, illustrated in Figure 1, a systemlearns a sequence of rules that best labels training data.These rules are then used to annotate previously unseendata.According to (Brill, 1995), a Transformation-BasedError-Driven learning application is defined by:1.
The initial annotation scheme2.
The space of allowable transformations3.
The iterative algorithm for choosing a transforma-tion sequenceThe initial annotation may be extremely simple.
Forexample, in a part-of-speech tagging task, the initial an-notation may assign each token its most likely tag withoutany regard to context (Brill, 1995).The iterative learning algorithm typically consists ofsimply searching for a rule that maximizes the increasein some objective function using a greedy hill-climbingUnannotated textInitialannotationLearnedannotationTruthannotationLearnerTransformationrulesFigure 1: Overview of general Transformation-BasedError-Driven learningstrategy.
For the CoNLL shared task, since participantsare evaluated by their F1 scores, it is reasonable to usethe F1 score as an objective function.
We also imple-mented some extensions to the hill-climbing strategy thatwe describe in Section 2.3.2 Experimental SettingIn our approach, we used three successive learningstages?the first stage tags the verb region V, the secondtags the A0 and A1 arguments, and the third tags all re-maining arguments.
The output of each stage becomesthe initial annotation for the following stage.
Therefore,our system only defines an explicit initial annotation forthe verb-tagging phase: for each proposition, we initiallytag only the single token containing the verb as V.The search for new transformation templates is ter-minated when no new transformation can be found thatwould improve the objective function by at least 0.03%.2.1 Transformation templatesFor the first stage, transformations are generated from thefollowing eight transformation templates:Lengthen [shorten] the end of region1 V by one token if:a,b) followed by chunk with tag=Xc,d) followed by token with POS2=Xe,f) followed by chunk with tag=X and token withPOS=Yg,h) the verb token?s lemma is XIn this formulation, ?chunk?
refers to the IOB2 chunks,and ?clause?
refers to the nested clause structure (S re-gions) given as task input.
?Lemma?
refers to the in-finitive form of the verb, identified in the task input andcoreferenced in the PropBank data.
X and Y are vari-ables that range over all types of chunks, POS tags, orlemmas.
The rule-learning system must determine whichvalues for these variables will produce the most effec-tive transformations.
For example, a rule that the systemmight produce from template ?e?
is:Lengthen the end of region V by one token ifthe region is followed by chunk with tag=PRTand token with POS=RP.Based on the observation that all V regions in the train-ing data were either one or two tokens in length, an ad-ditional constraint was added to the first stage, requiringthat lengthening-rules only apply to regions of length one,and shortening-rules only apply to regions of length two.The second and third stages use a common set ofeleven transformation templates, but in the second stagethe learner is restricted to adding or altering only A0 andA1 regions.
The transformation templates are as follows:A,B) If chunk with tag=X is followed [preceded] directlyby region with tag=Y , mark chunk as Z.C,D) If token with POS=X is followed [preceded] di-rectly by region with tag=Y , mark token as Z.E,F) If chunk with tag=X is followed [preceded] (per-haps indirectly) by region with tag=Y , mark chunkas Z.G,H) If region with tag=X is followed [preceded] bychunk with tag=PP, which is in turn followed [pre-ceded] by chunk with tag=Y , extend X forward[backward] through Y .1In this paper, we use the term ?region?
to refer to a sectionof corpus text that has been labeled in the output as a verb oras a verb argument.
We also use the term in rule definitions torefer to the type of label assigned to that section of text.2part-of-speechI,J) If verb?s first token has POS=X [and is preceded byPOS=Y ], switch A0 and A1.K) If region with tag=X is contained in a clause-starting verb phrase, and this is preceded by aclause-starting token with POS=Y , mark token Y asZ.Templates ?A-H?
are meant to capture structural rela-tionships among arguments, such as the fact that A1 re-gions usually follow V regions, or that arguments mayconsist of several NP chunks joined by PP chunks.
Tem-plates ?I?
and ?J?
were written to discover passive verbrelationships.
Template ?K?
was an explicit (admittedlyad hoc) attempt to recognize R-A0 and R-A1 arguments.To avoid creating tagged regions that overlap, we usea first-tag-wins strategy: if a transformation would tag anew region that overlaps an existing tagged region, thenew region is trimmed until any overlaps vanish.Notice that unlike the templates in the first stage, thesetemplates make no reference to lexical information.
Inparticular, no rule takes advantage of PropBank data in itstagging process3.
We anticipate that using PropBank datawould potentially improve performance, but we have notyet experimented with it.
Also, without any lexical infor-mation in these templates, we are capturing only generalpatterns of argument structure within the training corpus,not the statistical patterns of particular verb frames.
Infuture experiments we expect to incorporate lexical datainto transformation rules.2.2 Arguments and clause structureIn order to gain some traction on the problem, we an-alyzed the relationship between semantic arguments andclause boundaries.
To investigate this, we labeled each ar-gument with the smallest clause containing it as a propersubset.
We then tallied the number of each type of ar-gument labeled with the same clause as its verb, and thenumber labeled with a different clause.
The results areshown in Table 1.Note that for almost all argument types, the over-whelming majority of arguments are found in the sameclause as the verb.
This motivated us to add an additionalconstraint to the transformation templates A-J: only cre-ate arguments in the same clause as the verb.
This sim-plification necessarily will miss any legitimate argumentsoutside the clause (most notably 20% of A0 arguments).2.3 Reordering of learned rulesIn observing the sequence of transformations learned bythe system, it became apparent that the system?s strict3We actually do use PropBank in a limited way: no trans-formation will assign an argument A0-A5 to a verb unless thatargument is listed for one of the verb?s senses in PropBank.Same Different PercentTag clause clause sameA1 16896 1150 93.6%A0 10134 2575 79.7%A2 4022 201 95.2%AM-TMP 3329 238 93.3%AM-MOD 1752 1 99.9%AM-ADV 1675 52 97.0%AM-MNR 1277 60 95.5%AM-LOC 1184 95 92.6%AM-DIS 1042 35 96.8%A3 758 26 96.7%AM-NEG 687 0 100.0%A4 625 1 99.8%AM-PNC 432 14 96.9%C-A1 313 129 70.8%AM-CAU 261 22 92.2%AM-DIR 228 3 98.7%AM-EXT 150 2 98.7%R-A0 10 728 1.4%R-A1 7 353 1.9%Table 1: Verb-argument clause agreement on trainingdata (arguments with fewer than 50 examples omitted)greedy-hill-climbing strategy often learned a non-optimalordering of rules.
This is because the system has no look-ahead capability to check whether a sequence of multiplerules applied in succession might produce a good finalresult despite providing little or no initial improvement.The addition of a look-ahead searcher has been sug-gested (Brill, 1995), but we have not seen it implementedin a research context, likely due to the fact that a straight-forward implementation of the concept would at mini-mum square the amount of time required for training.Instead, we implemented a look-behind search strat-egy, which allows rules to be reordered after discovery.
Itis meant to address the case in which the system learnsa set of rules that each produce improvements in the tar-get function, but interact with each other in a non-optimalway.
Whenever our system discovers a new rule, ratherthan simply applying it and searching for the next rule,it is allowed to try all permutations of the last n discov-ered rules to see whether performance would improve byusing a different ordering.
If so, the rules are re-ordered.To our knowledge, this strategy has not been employedin Transformation-Based Error-Driven learning settings.In our experiments, the strategy discovered transforma-tion sequences that better annotated the input data with-out using more rules, and therefore seems to produce alabeler less likely to overfit the training data.
In our test-ing, the technique seems to have increased the overall F1score by between 0.5% and 1.0%?we caution, however,Precision Recall F?=1Overall 57.73% 34.35% 43.07A0 60.92% 52.98% 56.67A1 53.90% 45.18% 49.16AM-MOD 99.81% 58.59% 73.83AM-NEG 44.89% 77.29% 56.79AM-TMP 38.34% 6.36% 10.92R-A0 64.61% 76.69% 70.14V 99.19% 99.19% 99.19Table 2: Annotation agreement on training data (rowswith all-zero entries omitted)that we have not undertaken a rigorous comparative studyof the technique.3 ResultsThe quality of our transformation rules on the training setis shown in Table 2, and the results on the test set areshown in Table 3.
The rules that generated these resultsare shown in Table 4, along with the iterative F1 scoreson the training set as the rules are learned.4 DiscussionFirst, note that only one rule was learned in the verb-tagging phase: Lengthen region V if followed by chunkwith tag=PRT.
With earlier releases of the data the sys-tem did learn multiple rules, including lexically-basedrules, but in later releases only this one rule was learned.Second, observe that the system actually did reorderrules after discovering them, as evidenced by the non-monotonic ?discovery order?
column.
To attain this re-sult, we used a look-behind of 2, i.e.
the last 3 ruleslearned were candidates for reordering.Third, several of the rules in the sequence are identical.In some cases, this seems to be because multiple applica-tions of a rule were necessary to achieve full results (e.g.rule ?H?, which extended an A0 or A1 region throughjoined NP chunks several times).
In other cases, thisseems to be one rule re-applying itself after another rulemodified the results of its earlier application (e.g.
rule?E?, which was affected by applications of rule ?H?
).Finally, note that only 23 transformations were found.The last few rules begin dealing with lesser-representedargument types like R-A0 and AM-NEG, but many typesremain completely unaddressed by the system.
We maybe able to increase performance on those types by addingadditional rule templates, or by decreasing the learningtermination threshold for the system.
Rule ?K?
was cre-ated as an explicit attempt to recognize R-A0 and similarargument types, and seems to have been reasonably suc-cessful.
There may be other relatively simple templateswe can create to recognize other arguments.Precision Recall F?=1Overall 58.08% 34.75% 43.48A0 60.26% 52.73% 56.24A1 54.53% 44.56% 49.05A2 0.00% 0.00% 0.00A3 0.00% 0.00% 0.00A4 0.00% 0.00% 0.00A5 0.00% 0.00% 0.00AM-ADV 0.00% 0.00% 0.00AM-CAU 0.00% 0.00% 0.00AM-DIR 0.00% 0.00% 0.00AM-DIS 0.00% 0.00% 0.00AM-EXT 0.00% 0.00% 0.00AM-LOC 0.00% 0.00% 0.00AM-MNR 0.00% 0.00% 0.00AM-MOD 100.00% 56.68% 72.35AM-NEG 48.34% 80.31% 60.36AM-PNC 0.00% 0.00% 0.00AM-PRD 0.00% 0.00% 0.00AM-TMP 40.48% 6.83% 11.68R-A0 66.45% 64.78% 65.61R-A1 0.00% 0.00% 0.00R-A2 0.00% 0.00% 0.00R-A3 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 0.00% 0.00% 0.00V 98.21% 98.21% 98.21Table 3: Results on test dataIn future work, there are several avenues we would liketo explore.
Our first-tag-wins assignment strategy men-tioned above is not grounded in research into alternatestrategies, and in fact we have not yet tried any others.We also experimented with isolating common verbtypes into their own corpus?for example, if we train sep-arately on the verb ?say,?
which represents nearly 10% ofthe target verbs in the training set and exhibits differentargument patterns from other verbs, we achieve an F1value of about 82% on this subset using only five learnedrules.
It may be possible to leverage this work by group-ing other less common verbs by their VerbNet class(es).5 ConclusionWe have described a Transformation-Based Error-Drivenlearning approach to the CoNLL shared task on seman-tic role labeling.
Although we are relative newcomers tothis task and this approach has not to our knowledge beenapplied to it before, we believe our results are of generalinterest for the following reasons.First, the learned output of the system is highlyDiscoveryRule Parameters order F1initial annotation 0.00a PRT 1 0.00E NP, V, A0 2 20.20I VBN 5 24.60B NP, V, A1 3 31.60B S, V, A1 4 35.82H A0, NP 6 36.44D NN, V, A1 9 36.67D NNS, V, A1 10 36.86E NP, V, A1 8 37.36J VBZ, ?
11 37.46E S, V, A1 7 38.55H A1, NP 12 38.64H A0, NP 13 38.71H A1, NP 14 38.78J VBD, ?
15 38.83E S, V, A1 16 39.07G A0, NP 17 39.09H A1, NP 18 39.11C MD, V, AM-MOD 19 41.23K V, WDT, R-A0 20 41.78K V, WP, R-A0 21 42.21A ADVP, V, AM-TMP 22 42.46C RB, V, AM-NEG 23 43.16Table 4: Rules learned for semantic role labelingscrutable, in the sense that the transformation rules caneasily be reviewed and understood by a human supervi-sor.
This may benefit real-world application of the tech-nique as rules may be manually reordered, switched onor off, or modified.
It also allows a developer to closelymonitor changes in the system, creating new rules as heor she identifies areas of the data that are being under-served by the current set of transformation templates.Second, as alluded to above, there are several appeal-ing directions to direct future research, and we believe theresults obtained here can be significantly improved.Third, we know of no previous work using our look-behind reordering technique in conjunction with rule-based learning, and the technique may have broad appli-cability beyond semantic role labeling.ReferencesEric Brill.
1993.
A Corpus-Based Approach to LanguageLearning.
Ph.D. thesis, Philadelpha, PA.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A case studyin part-of-speech tagging.
Computational Linguistics,21(4):543?565.
