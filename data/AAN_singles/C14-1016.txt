Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 151?160, Dublin, Ireland, August 23-29 2014.A Probabilistic Model for Learning Multi-Prototype Word EmbeddingsFei Tian?, Hanjun Dai?, Jiang Bian?, Bin Gao?,Rui Zhang?, Enhong Chen?, Tie-Yan Liu?
?University of Science and Technology of China, Hefei, P.R.China?Fudan University, Shanghai, P.R.China?Microsoft Research, Building 2, No.
5 Danling Street, Beijing, P.R.China?Sun Yat-Sen University, Guangzhou, P.R.China?tianfei@mail.ustc.edu.cn,?cheneh@ustc.edu.cn,?daihanjun@gmail.com,?
{jibian, bingao, tyliu}@microsoft.com,?rayz0620@hotmail.comAbstractDistributed word representations have been widely used and proven to be useful in quite a fewnatural language processing and text mining tasks.
Most of existing word embedding models aimat generating only one embedding vector for each individual word, which, however, limits theireffectiveness because huge amounts of words are polysemous (such as bank and star).
To addressthis problem, it is necessary to build multi embedding vectors to represent different meanings ofa word respectively.
Some recent studies attempted to train multi-prototype word embeddingsthrough clustering context window features of the word.
However, due to a large number ofparameters to train, these methods yield limited scalability and are inefficient to be trained withbig data.
In this paper, we introduce a much more efficient method for learning multi embeddingvectors for polysemous words.
In particular, we first propose to model word polysemy from aprobabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model.Under this framework, we design an Expectation-Maximization algorithm to learn the word?smulti embedding vectors.
With much less parameters to train, our model can achieve comparableor even better results on word-similarity tasks compared with conventional methods.1 IntroductionDistributed word representations usually refer to low dimensional and dense real value vectors (a.k.a.word embeddings) to represent words, which are assumed to convey semantic information contained inwords.
With the exploding text data on the Web and fast development of deep neural network technolo-gies, distributed word embeddings have been effectively trained and widely used in a lot of text miningtasks (Bengio et al., 2003) (Morin and Bengio, 2005) (Mnih and Hinton, 2007) (Collobert et al., 2011)(Mikolov et al., 2010) (Mikolov et al., 2013b).While word embedding plays an increasingly important role in many tasks, most of word embeddingmodels, which assume one embedding vector for each individual word, suffer from a critical limitationfor modeling tremendous polysemous words (e.g.
bank, left, doctor).
Using the same embedding vec-tor to represent the different meanings (we will call prototype of a word in the rest of the paper) of apolysemous word is somehow unreasonable and sometimes it even hurts the model?s expression ability.To address this problem, some recent efforts, such as (Reisinger and Mooney, 2010) (Huang et al.,2012), have investigated how to obtain multi embedding vectors for the respective different prototypesof a polysemous word.
Specifically, these works usually take a two-step approach: they first train singleprototype word representations through a multi-layer neural network with the assumption that one wordonly yields single word embedding; then, they identify multi word embeddings for each polysemousword by clustering all its context window features, which are usually computed as the average of singleprototype embeddings of its neighboring words in the context window.Compared with traditional single prototype model, these models have demonstrated significant im-provements in many semantic natural language processing (NLP) tasks.
However, they suffer from aThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/151crucial restriction in terms of scalability when facing exploding training text corpus, mainly due to thedeep layers and huge amounts of parameters in the neural networks in these models.
Moreover, theperformance of these multi-prototype models is quite sensitive to the clustering algorithm and requiresmuch effort in clustering implementation and parameter tuning.
The lack of probabilistic explanationalso refrains clustering based methods from being applied to many text mining tasks, such as languagemodeling.To address these challenges, in this work, we propose a new probabilistic multi-prototype model andintegrate it into a highly efficient continuous Skip-Gram model, which was recently introduced in thewell-known Word2Vec toolkit (Mikolov et al., 2013b).
Compared with conventional neural networklanguage models which usually set up a multi-layer neural network, Word2Vec merely leverages a three-layer neural network to learn word embeddings, resulting in greatly decreased number of parameters andlargely increased scalability.
However, similar to most of existing word embedding models, Word2Vecalso assumes one embedding for one word.
We break this limitation by introducing a new probabilisticframework which employs hidden variables to indicate which prototype each word belongs to in the con-text.
In this framework, the conditional probability of observing word wOconditioned on the presenceof neighboring word wI(i.e.
P(wO|wI)) can be formulated as a mixture model, where mixtures corre-sponds to wI?s different prototypes.
This is a more natural way to define P(wO|wI), since it has taken thepolysemy of word wIinto consideration.
After defining the model, we design an efficient Expectation-Maximization (EM) algorithm to learn various word embedding vectors corresponding to each of wI?sprototypes.
Evaluations on widely used word similarity tasks demonstrate that our algorithm producescomparable or even better word embeddings compared with either clustering-based multi-prototype mod-els or the original Skip-Gram model.
Furthermore, as a unified way to obtain multi word embeddings,our proposed method can effectively avoid the sensitivity to the clustering algorithm applied by previousmulti-prototype word embedding approach.The following of the paper is organized as follows: we introduce related work in Section 2.
Then,Section 3 describes our new model and algorithm in details and conducts a comparison in terms ofcomplexity between our algorithm and the previous method.
We present our experimental results inSection 4.
The paper is concluded in Section 5.2 Related WorkSince the initial work (Bengio et al., 2003), there have been quite a lot of neural network based modelsto obtain distributed word representations (Morin and Bengio, 2005) (Mnih and Hinton, 2007) (Mikolovet al., 2010) (Collobert et al., 2011) (Mikolov et al., 2013b).
Most of these models assume that oneword has only one embedding, except the work of Eric Huang (Huang et al., 2012), in which the authorspropose to leverage global context information and multi-prototype embeddings to achieve performancegains in word similarity task.
To obtain multi-prototype word embeddings, this work conducts clusteringon a word?s all context words?
features in the corpus.
The features are the embedding vectors trainedpreviously via a three-layer neural network.
Each cluster?s centroid is regarded as the embedding vectorfor each prototype.
Their reported experimental results verify the importance of considering multi-prototype models.Note that (Reisinger and Mooney, 2010) also proposes to deal with the word polysemy problem byassigning to each prototype a real value vector.
However their embedding vectors are obtained througha tf-idf counting model, which is usually called as distributional representations (Turian et al., 2010),rather than through a neural network.
Therefore, we do not regard their paper as very related to ourwork.
The similar statement holds for other works on vector model for word meaning in context such as(Erk and Pad?o, 2008) (Thater et al., 2011) (Reddy et al., 2011) (Van de Cruys et al., 2011).Our model is mainly based on the recent proposed Word2Vec model, more concretely, the continuousSkip-Gram model (Mikolov et al., 2013a) (Mikolov et al., 2013b).
The continuous Skip-Gram modelspecifies the probability of observing the context words conditioned on the central word wIin the win-dow via a three-layer neural network.
With less parameters to train (thus higher scalability), Word2Vecdiscovers interesting analogical semantic relations between words like Japan - Tokyo = France - Paris.1523 Model DescriptionIn this section, we introduce our algorithm for learning multi-prototype embeddings in details.
In partic-ular, since our new model is based on the continuous Skip-Gram model, we first make a brief introductionto the Skip-Gram model.
Then, we present our new multi-prototype algorithm and how we integrate itinto the Skip-Gram model.
After that, we propose an EM algorithm to conduct the training process.
Wealso conduct a comparison on the number of parameters between the new EM algorithm and the state-of-the-art multi-prototype model proposed in (Huang et al., 2012), which can illustrate the efficiencysuperior of our algorithm.3.1 Multi-Prototype Skip-Gram ModelIn contrast to the conventional ways of using context words to predict the next word or the centralword, the Skip-Gram model (Mikolov et al., 2013b) aims to leverage the central word to predict itscontext words.
Specifically, assuming that the central word is wIand one of its neighboring word is wO,P(wO|wI) is modeled in the following way:P(wO|wI) =exp(VTwIUwO)?w?Wexp(VTwIUw), (1)where W denotes the dictionary consisting of all words, Uw?Rdand Vw?Rdrepresent the d-dimensional?output?
and ?input?
embedding vectors of word w, respectively.
Note that all the parameters to be learnedare the input and output embedding vectors of all words, i.e.
U = {Uw|w ?W} and V = {Vw|w ?W}.This corresponds to a three-layer neural network, in which U and V denote the two parameter matrices ofthe neural network.
Compared with the conventional neural networks employed in the literature whichyield at least four layers (including the look-up table layer), the Skip-Gram model greatly reduces thenumber of parameters and thus gives rise to a significant improvement in terms of training efficiency.Our proposed Multi-Prototype Skip-Gram model is similar to the original Skip-Gram model in that italso aims to model P(wO|wI) and uses two matrices (the input and output embedding matrices) as theparameters.
The difference lies in that given word wI, the occurrence of word wOis described as a finitemixture model, in which each mixture corresponds to a prototype of word wI.
To be specific, supposethat word w has Nwprototypes and it appears in its hw-th prototype, i.e., hw?
{1, ?
?
?
,Nw} is the index ofw?s prototype.
Then P(wO|wI) is expanded as:p(wO|wI) =NwI?i=1P(wO|hwI= i,wI)P(hwI= i|wI) (2)=NwI?i=1exp(UTwOVwI,i)?w?Wexp(UTwVwI,i)P(hwI= i|wI), (3)where VwI,i?
Rdrefers to the embedding vector of wI?s i-th prototype.
This equation states that P(wO|wI)is a weighted average of the probabilities of observing wOconditioned on the appearance of wI?s everyprototype.
The probability P(wO|hwI= i,wI) takes the similar softmax form to equation (1) and theweight is specified as a prior probability of word wIfalls in its every prototype.The general idea behind the Multi-Prototype Skip-Gram model is very intuitive: the surrounding wordsunder different prototypes of the same word are usually different.
For example, when the word bankrefers to the side of a river, it is very possible to observe the corresponding context words such asriver, water, and slope; however, when bank falls into the meaning of the financial organization, thesurrounding word set is likely to be comprised of quite different words, such as money, account, andinvestment.The probability formulation in (3) brings much computation cost because of the linear dependency of|W | in the denominator?w?Wexp(UTwVwI,i).
To address this issue, several efficient methods have beenproposed such as Hierarchical Softmax Tree (Morin and Bengio, 2005) (Mnih and Kavukcuoglu, 2013)and Negative Sampling (Mnih and Kavukcuoglu, 2013) (Mikolov et al., 2013b).
Taking Hierarchical153Softmax Tree as an example, through a binary tree in which every word is a leaf node, word wOisassociated with a binary vector b(wO)?
{?1,+1}LwOspecifying a path from the root of the tree to leafwO, where LwOis the length of vector b(wO).
Then the conditional probability is described asP(wO|hwI= i,wI) =LwO?t=1P(b(wO)t|wI,hwI= i)=LwO?t=1?
(b(wO)tUTwO,tVwI,i), (4)where ?
(x) = 1/(1+ exp(?x)) is the sigmoid function, and UwO,tspecifies the d-dimensional parametervector associated with the t-th node in the path from the root to the leaf node wO.
Substituting (4) into(2) to replace the large softmax operator in (3) leads to a much more efficient probability form.3.2 EM AlgorithmIn this section, we describe the EM algorithm adopted to train the Multi-Prototype Skip-Gram model.Without loss of generality, we will focus on obtaining multi embeddings for a specified word w ?Wwith Nwprototypes.
Word w?s embedding vectors are denoted as Vw?
Rd?Nw.
Suppose there are Mword pairs for training: {(w1,w),(w2,w), ?
?
?
,(wM,w)}, where all the inputs words (i.e., word w) are thesame, and the set of output words to be predicted are denoted as X= {w1,w2, ?
?
?
,wM}.
That is, X are Msurrounding words of w in the training corpus.For ease of reference and without loss of generality, we make some changes to the notations in Section3.1.
We will use hmas the index of w?s prototype in the pair (wm,w), m ?
{1,2, ?
?
?
,M}.
Besides,some new notations are introduced: P(hw= i|wI) is simplified as pii, and ?m,k, where m ?
{1,2, ?
?
?
,M},k ?
{1,2, ?
?
?Nw}, are the hidden binary variables indicating whether the m-th presence of word w is inits k-th prototype, i.e.
?m,k= 1hm=k, where 1 is the indicator function.
Other notations are the same asbefore: Vw,i?
Rdis the embedding vector for word w?s i-th prototype, Uw,t?
Rdis the embedding vectorfor the t-th node on the path from the tree root to the leaf node representing word w, and b(w)t?
{?1,1}is the t-th bit of the binary coding vector of word w along its corresponding path on the HierarchicalSoftmax Tree.Then the parameter set we aim to learn is ?
= {pi1, ?
?
?
,piNw;U ;Vw}.
The hidden variable set is ?
={?m,k|m ?
(1,2, ?
?
?
,M),k ?
(1,2, ?
?
?
,Nw)}.
Considering equation (2) and (4), we have the log likelihoodof X as below:logP(X,?|?)
=M?m=1Nw?k=1?m,k(logpik+ logP(wm|hm= k,w))=M?m=1Nw?k=1?m,k(logpik+Lwm?t=1log?(b(wm)tUTwm,tVw,k)).
(5)With equation (5), the E-Step and M-Step are:E-Step:The conditional expectation of hidden variable ?m,k, denoted as?
?m,k, is:?
?m,k= P(?m,k= 1|X,?)
=pikP(wm|hm= k,w)?Nwi=1piiP(wm|hm= i,w).
(6)The Q function w.r.t.
the parameters at the i-th iteration ?
(i)is written as:Q(?
,?
(i)) =Nw?k=1M?m=1?
?m,k(logpik+ logP(wm|hm= k,w))=M?m=1Nw?k=1??m,k(logpik+Lwm?t=1log?(b(wm)tUTwm,tVw,k)).
(7)M-Step:pi can be updated by154pik=?Mm=1?
?m,kM, k = 1,2 ?
?
?
,Nw.
(8)We leave the detailed derivations for equation (6), (7), and (8) to the appendix of the paper.
Then wediscuss how we obtain the update of the embedding parameters Uwm,tand Vw,k.
Note that the optimizationproblem is non-convex, and it is hard to compute the exact solution of?Q?Uwm,t= 0 and?Q?Vw,k= 0.
Therefore,we use gradient ascent to optimize in the M-step.
The gradients of Q function w.r.t.
embedding vectorsare given by:?Q?Uwm,t=Nw?k=1??m,kb(wm)t(1?
?(b(wm)tUTwm,tVw,k))Vw,k,(9)?Q?Vw,k=M?m=1??m,kLwm?t=1b(wm)t(1?
?(b(wm)tUTwm,tVw,k))Uwm,t.
(10)Iterating between E-Step and M-Step till the convergence of the value of function Q makes the EMalgorithm complete.In order to enhance the scalability of our approach, we propose a fast computing method to boostthe implementation of the EM algorithm.
Note that the most expensive computing operations in boththe E-Step and M-Step are the inner product of the input and output embedding vectors, as well as thesigmoid function.
However, if we take the Hierarchical Softmax Tree form as shown in Equation (4) tomodel P(wm|hm= i,w), and perform only one step gradient ascent in M-Step, the aforementioned twoexpensive operations in M-Step will be avoided by leveraging the pre-computed results in the E-Step.Specifically, since the gradient of the function f (x) = log?
(x) is given by f?
(x) = 1?
?
(x), the sigmoidvalues computed in the E-Step to obtain P(wm|hm= i,w) (i.e.
the term ?
(b(wm)tUTwm,tVw,k) in equation (5),(9), and (10)) can be re-used to derive the gradients in the M-Step.However, such enhanced computation method cannot benefit the second order optimization methodsin the M-Step such as L-BFGS and Conjugate Gradient, since they usually rely on multiple iterations toconverge.
In fact, we tried these two optimization methods in our experiments but they have brought noimprovement compared with simple one-step gradient ascent method.3.3 Model ComparisonTo show that our model is more scalable than the former multi-prototype model in (Huang et al., 2012)(We denote it as EHModel in the rest of the paper), we conduct a comparison on the number of parameterswith respect to each of these two models in this subsection.We use nembeddingand nwindowto denote the numbers of all word embedding vectors and context win-dow words, respectively.
It is clear that nembeddings=?w?WNw.
EHModel aims to compute two scores,i.e., the local score and the global score, both with hidden layer node activations.
We denote the hiddenlayer node number as hland hgfor these two scores.
The parameter numbers are listed in Table 1.Model EHModel Our Model#parameters dnwords+dnembeddings+(dnwindow+1)hl+(2d +1)hgdnwords+dnembeddingsTable 1: Comparison of parameter numbers of two modelsNote that d in Table 1 denotes the embedding vector size.
It can be observed that EHModel has(dnwindow+1)hl+(2d +1)hgmore parameters than our model, which is mainly because EHModel hasone more layer in the neural network and it considers global context.
In previous study (Huang et al.,2012), d, nwindow, hl, and hgare set to be 50, 10, 100, 100, respectively, which greatly increases the gapof parameter numbers between the two models.1554 ExperimentsIn this section, we will present our experimental settings and results.
Particularly, we first describe thedata collection and the training configuration we used in the experiments; then, we conduct a qualitativecase study followed by quantitative evaluation results on a public word similarity task to demonstrate theperformance of our proposed model.4.1 Experimental SetupDataset: To make a fair comparison with the state-of-the-art methods, we employ a publicly availabledataset, which is used in (Huang et al., 2012), to train word embeddings in our experiments.
Particularly,this training corpus is a snapshot of Wikipedia at April, 2010 (Shaoul, 2010), which contains about 990million tokens.
We removed the infrequent words from this corpus and kept a dictionary of about 1million most frequent words.
Similar to Word2Vec, we removed pure digit words such as 2014 as wellas about 100 stop words like how, for, and we.Training Configuration: In order to boost the training speed, we take advantage of the HierarchicalSoftmax Tree structure.
More concretely, we use the Huffman tree structure, as introduced in Word2Vec,to further increase the training speed.
All the embedding size, including both word embedding vectorsand the Huffman tree node embedding vectors, are set to be 50, which is the same as the size used in(Huang et al., 2012).
To train word embedding, we set the context window size as 10, i.e., for a word w,10 of the closest neighboring words to w are regarded as ws contexts.
For the numbers of word prototypes,i.e., Nwintroduced in Section 3.2, we set the top 7 thousand frequent words as multi-prototype words byexperience, with all of them having 10 prototypes (i.e.
Nw= 10).During the training process, we used the same strategy to set the learning rate as what Word2Vec did.Specifically, we set the initial learning rate to 0.025 and diminished the value linearly along with theincreasing number of training words.
Our experimental results illustrate that this learning rate strategycan lead to the best results for our algorithm.For the hyper parameters of the EM algorithm, we set the batch size to 1, i.e.
M = 1 in Section 3.2,since our experimental results reveal that smaller batch size can result in better experimental results.
Thereason is explained as the following.
Our optimization problem is highly non-convex.
Smaller batch sizeyields more frequent updates of parameters, and thus avoids trapping in local optima, while larger batchsize, associated with more infrequent parameter updating, may cause higher probability to encounterlocal optima.
In our experiments, we observe that only one iteration of E-Step and M-Step can reach theembedding vectors with good enough performance on the word similarity task, whereas increasing theiteration number just leads to slight performance improvement with much longer training time.
Underthe above configuration, our model runs about three times faster than EHModel.4.2 Case StudyThis section gives some qualitative evaluations of our model by demonstrating how our model can ef-fectively identify multi-prototype word embeddings on some specific cases.
In Table 2, we list severalpolysemous words.
For each word, we pick some of their prototypes learned by our model, includingthe prototype prior probability (i.e.
piiintroduced in Section 3.2) and three of the most similar wordswith each prototype, respectively.
The similarity is calculated by the cosine similarity score between theembedding vectors.From the table we can observe some interesting results of the multi-prototype embedding vectorsproduced by our model:?
For a polysemous word, its different embedding vectors represent its different semantic meanings.For example, the first embedding vector of the word apple corresponds to its sense as a kind of fruit,whereas the second one represents its meaning as an IT company.?
The prior probability reflects the likelihood of the occurrence of various prototypes to some extent.For example, the word cell is more likely to represent the meaning of the smallest part of livingstructure (with probability 0.81), than to be used as the meaning of cellphone (with probability156Word Prior Probability Most Similar Wordsapple 1 0.82 strawberry, cherry, blueberryapple 2 0.17 iphone, macintosh, microsoftbank 1 0.15 river, canal, waterwaybank 2 0.6 citibank, jpmorgan, bancorpbank 3 0.25 stock, exchange, bankingcell 1 0.09 phones, cellphones, mobilecell 2 0.81 protein, tissues, lysiscell 3 0.01 locked, escape, handcuffedTable 2: Most similar words with different prototypes of the same word0.09) or prisoned (with probability 0.01).
Note that the three prior probability scores of cell do notsum to 1.
The reason is that there are some other embeddings not presented in the table which arefound to have high similarities with the three embeddings.
We do not present them due to the spacelimitation.?
By setting the prototype number to a fairly large value (e.g.
Nw= 10), the model tends to learnmore fine-grained separations of the word?s different meanings.
For example, we can observe fromTable 2 that the second and the third prototypes of the word bank seem similar to each other as bothof them denote a financial concept.
However, there are subtle differences between them: the secondprototype represents concrete banks, such as citibank and jpmorgan, whereas the third one denoteswhat is done in the banks, since it is most similar to the words stock, exchange, and banking.
Webelieve that such a fine-grained separation will bring more expressiveness to the multi-prototypeword embeddings learned by our model.4.3 Results on Word Similarity in Context DatasetIn this subsection, we give quantitative comparison of our method with conventional word embeddingmodels, including Word2Vec and EHModel (Huang et al., 2012).The task we perform is the word similarity evaluation introduced in (Huang et al., 2012).
Word simi-larity tasks evaluate a model?s performance by calculating the Spearman?s rank correlation between theranking of ground truth similarity scores (given by human labeling) and the ranking based on the simi-larity scores produced by the model.
Traditional word similarity tasks such as WordSim353 (Finkelsteinet al., 2001) and RG (Rubenstein and Goodenough, 1965) are not suitable for evaluating multi-prototypemodels since there is neither enough number of polysemous words in these datasets nor context infor-mation to infer the prototype index.
To address this issue, a new word similarity benchmark datasetincluding context information was released in (Huang et al., 2012).
Following (Luong et al., 2013), weuse SCWS to denote this dataset.
Similar to WordSim353, SCWS contains some word pairs (concretely,2003 pairs), together with human labeled similarity scores for these word pairs.
What makes SCWSdifferent from WS353 is that the words in SCWS are contained in sentences, i.e., there are 2003 pairs ofsentences containing these words, while words in WS353 are not associated with sentences.
Therefore,the human labeled scores are based on the meanings of the words in the context.
Given the presenceof the context, the word similarity scores, especially those scores depending on polysemous words, aremuch more convincing for evaluating different models?
performance in our experiments.Then, we propose a method to compute the similarity score for a pair of words {w1,w2} in the contextbased on our model.
Suppose that the context of a word w is defined as all its neighboring words in aT +1 sized window, where w is the central word in the window.
We use Context1= {c11,c12, ?
?
?
,c1T} andContext2= {c21,c22, ?
?
?
,c2T} to separately denote the context of w1and w2, where c1tand c2tare the t-thcontext word of w1and w2, respectively.
According to Bayesian rule, we have that for i?
{1,2, ?
?
?
,Nw1}:P(hw1= i|Context1,w1) ?
P(Context1|hw1= i,w1)P(hw1= i|w1)=T?t=1P(c1t|hw1= i,w1)P(hw1= i|w1),(11)157where P(c1t|hw1= i,w1) can be calculated by equation (4) and P(hw1= i|w1) is the prior probabilitywe learned in the EM algorithm (equation (8)).
The similar equation holds for word w2as well.
Herewe make an assumption that the context words are independent with each other given the central word.Furthermore, suppose that the most likely prototype index for w1given Context1is?hw1, i.e., we de-note?hw1= argmaxi?{1,2,???
,Nw1}P(hw1= i|Context1,w1).
Similarly,?hw2is denoted as the correspondingmeaning for w2.We calculate two similarity scores base on equation (11), i.e., MaxSim Score and WeightedSim Score:MaxSim(w1,w2) =Cosine(Vw1,?hw1,Vw2,?hw2), (12)WeightedSim(w1,w2) =Nw1?i=1Nw2?j=1P(hw1= i|Context1,w1)P(hw2= j|Context2,w2)Cosine(Vw1,i,Vw2, j).
(13)In the above similarity scores, Cosine(x,y) denotes the cosine similarity score of vector x and y, andVw,i?
Rdis the embedding vector for the word w?s i-th prototype.The detailed experimental results are listed in Table 3, where ?
refers to the Spearman?s rank cor-relation.
The higher value of ?
indicates the better performance.
The performance score of EHModelis borrowed from its original paper (Huang et al., 2012).
For Word2Vec model, we use HierarchicalHuffman Tree rather than Negative Sampling to do the acceleration.
Our Model M uses the MaxSimscore in testing and our Model W uses the WeightedSim score.
All of these models are run on the sameaforementioned Wikipedia corpus, with the dimension of the embedding space to be 50.From the table, we can observe that our Model W (65.4%) outperforms the original Word2Vec model(61.7%), and achieves almost the same performance with the state-of-the-art EHModel (65.7%).
Amongthe two similarity measures used in testing, the WeightedSim score performs better (65.4%) than theMaxSim score (63.6%), indicating that the overall consideration of all prototype probabilities are moreeffective.Model ?
?100Word2Vec 61.7EHModel 65.7Model M 63.6Model W 65.4Table 3: Spearman?s rank correlations on SCWS dataset.5 ConclusionIn this paper, we introduce a fast and probabilistic method to generate multiple embedding vectors forpolysemous words, based on the continuous Skip-Gram model.
On one hand, our method addressesthe drawbacks of the original Word2Vec model by leveraging multi-prototype word embeddings; on theother hand, our model yields much less complexity without performance loss compared with the formerclustering based multi-prototype algorithms.
In addition, the probabilistic framework of our methodavoids the extra efforts to perform clustering besides training word embeddings.For the future work, we plan to apply the proposed probabilistic framework to other neural networklanguage models.
Moreover, we would like to apply the multi-prototype embeddings to more real worldtext mining tasks, such as information retrieval and knowledge mining, with the expectation that themulti-prototype embeddings produced by our model will benefit these tasks.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin.
2003.
A neural probabilistic languagemodel.
In Journal of Machine Learning Research, pages 1137?1155.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493?2537.158Katrin Erk and Sebastian Pad?o.
2008.
A structured vector space model for word meaning in context.
In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,Stroudsburg, PA, USA.
Association for Computational Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2001.
Placing search in context: The concept revisited.
In Proceedings of the 10th international conference onWorld Wide Web, pages 406?414.
ACM.Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng.
2012.
Improving word representationsvia global context and multiple word prototypes.
In Proceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Association for ComputationalLinguistics.Minh-Thang Luong, Richard Socher, and Christopher D Manning.
2013.
Better word representations with recur-sive neural networks for morphology.
CoNLL-2013, 104.Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur.
2010.
Recurrent neuralnetwork based language model.
In INTERSPEECH, pages 1045?1048.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013a.
Efficient estimation of word representationsin vector space.
arXiv preprint arXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean.
2013b.
Distributed representationsof words and phrases and their compositionality.
In C.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahramani, andK.Q.
Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111?3119.Andriy Mnih and Geoffrey Hinton.
2007.
Three new graphical models for statistical language modelling.
InProceedings of the 24th international conference on Machine learning, pages 641?648.
ACM.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learning word embeddings efficiently with noise-contrastive esti-mation.
In C.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q.
Weinberger, editors, Advances inNeural Information Processing Systems 26, pages 2265?2273.Frederic Morin and Yoshua Bengio.
2005.
Hierarchical probabilistic neural network language model.
In Proceed-ings of the international workshop on artificial intelligence and statistics, pages 246?252.Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, and Suresh Manandhar.
2011.
Dynamic and static prototypevectors for semantic composition.
In IJCNLP, pages 705?713.Joseph Reisinger and Raymond J Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the Associationfor Computational Linguistics, pages 109?117.
Association for Computational Linguistics.Herbert Rubenstein and John B Goodenough.
1965.
Contextual correlates of synonymy.
Communications of theACM, 8(10):627?633.Westbury C Shaoul, C. 2010.
The westbury lab wikipedia corpus.Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2011.
Word meaning in context: A simple and effectivevector model.
In IJCNLP, pages 1134?1143.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, pages 384?394.
Association for Computational Linguistics.Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2011.
Latent vector weighting for word meaning incontext.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP?11, pages 1012?1022, Stroudsburg, PA, USA.
Association for Computational Linguistics.6 Appendix6.1 Derivations for the EM AlgorithmWe give detailed derivations for the updating rules used in the EM algorithms in Section 3.2., i.e., thederivations for equation (6), (7), and (8).159According to the properties of conditional probability, we have?
?m,k= P(?m,k= 1|X,?)
=P(?m,k= 1,X|?
)?Nwi=1P(?m,i= 1,X|?
)=P(?m,k= 1|?
)P(X|?m,k= 1,?
)?Nwi=1P(?m,i= 1|?
)P(X|?m,i= 1,?
)=pikP(wm|hm= k,w)?Nwi=1piiP(wm|hm= i,w).
(14)From equation (7), the Q function is calculated as:Q(?
,?
(i)) = E[logP(X,?|?)|?(i)]=Nw?k=1M?m=1E[?m,k|?(i)](logpik+Lwm?t=1log?(b(wm)tUTwm,tVw,k))=Nw?k=1M?m=1??m,k(logpik+Lwm?t=1log?(b(wm)tUTwm,tVw,k))=M?m=1Nw?k=1??m,k(logpik+Lwm?t=1log?(b(wm)tUTwm,tVw,k)).
(15)Then we give the derivations for pi?s updating rule, i.e., equation (8).
Note that for parameters pik,k = {1,2, ?
?
?
,Nw}, they need to satisfy the condition that?Nwk=1pik= 1.
From equation (7) (or equivalentlyequation (15)), the loss with regard to pi is:L[pi]=M?m=1Nw?k=1??m,klogpik+?
(Nw?k=1pik?1), (16)where ?
is the Language multiplier.
Letting?L[pi]?pi= 0, we obtain:pik?M?m=1??m,k.
(17)Further considering the fact that?Nwk=1?Mm=1?
?m,k= M, we have pik=?Mm=1?
?m,kM.160
