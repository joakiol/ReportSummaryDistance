Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 21?29,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsExploring Measures of ?Readability?
for Spoken Language:Analyzing linguistic features of subtitlesto identify age-specific TV programsSowmya Vajjala and Detmar MeurersLEAD Graduate School, Department of LinguisticsUniversity of T?ubingen{sowmya,dm}@sfs.uni-tuebingen.deAbstractWe investigate whether measures of read-ability can be used to identify age-specificTV programs.
Based on a corpus of BBCTV subtitles, we employ a range of lin-guistic readability features motivated bySecond Language Acquisition and Psy-cholinguistics research.Our hypothesis that such readability fea-tures can successfully distinguish betweenspoken language targeting different agegroups is fully confirmed.
The classifierswe trained on the basis of these readabilityfeatures achieve a classification accuracyof 95.9%.
Investigating several featuresubsets, we show that the authentic mate-rial targeting specific age groups exhibitsa broad range of linguistics and psycholin-guistic characteristics that are indicative ofthe complexity of the language used.1 IntroductionReading, listening, and watching television pro-grams are all ways to obtain information partly en-coded in language.
Just like books are written fordifferent target groups, current TV programs targetparticular audiences, which differ in their interestsand ability to understand language.
For books andtext in general, a wide range of readability mea-sures have been developed to determine for whichaudience the information encoded in the languageused is accessible.
Different audiences are com-monly distinguished in terms of the age or schoollevel targeted by a given text.While for TV programs the nature of the inter-action between the audio-visual presentation andthe language used is a relevant factor, in this pa-per we want to explore whether the language byitself is equally characteristic of the particular agegroups targeted by a given TV program.
We thusfocused on the language content of the programas encoded in TV subtitles and explored the roleof text complexity in predicting the intended agegroup of the different programs.The paper is organized as follows.
Section 2introduces the corpus we used, and section 3 thereadability features employed and their motiva-tion.
Section 4 discusses the experimental setup,the experiments we conducted and their results.Section 5 puts our research into the context of re-lated work, before section 6 concludes and pro-vides pointers to future research directions.2 CorpusThe BBC started subtitling all the scheduled pro-grams on its main channels in 2008, implement-ing UK regulations designed to help the hearingimpaired.
Van Heuven et al.
(2014) constructed acorpus of subtitles from the programs run by nineTV channels of the BBC, collected over a periodof three years, January 2010 to December 2012.They used this corpus to compile an English wordfrequencies database SUBTLEX-UK1, as a part ofthe British Lexicon Project (Keuleers et al., 2012).The subtitles of four channels (CBeebies, CBBC,BBC News and BBC Parliament) were annotatedwith the channel names.While CBeebies targets children aged under 6years, CBBC telecasts programs for children 6?12years old.
The other two channels (News, Parlia-ment) are not assigned to a specific age-group, butit seems safe to assume that they target a broader,adult audience.
In sum, we used the BBC subtitlecorpus with a three-way categorization: CBeebies,CBBC, Adults.Table 1 shows the basic statistics for the overallcorpus.
For our machine learning experiments, weuse a balanced subcorpus with 3776 instances foreach class.
As shown in the table, the programs for1http://crr.ugent.be/archives/142321Program Category Age group # texts avg.
tokens avg.
sentence lengthper text (in words)CBEEBIES < 6 years 4846 1144 4.9CBBC 6?12 years 4840 2710 6.7Adults (News + Parliament) > 12 years 3776 4182 12.9Table 1: BBC Subtitles Corpus Descriptionthe older age-groups tend to be longer (i.e., morewords per text) and have longer sentences.
Whiletext length and sentence length seem to constituteinformative features for predicting the age-group,we hypothesized that other linguistic properties ofthe language used may be at least as informative asthose superficial (and easily manipulated) proper-ties.
Hence, we explored a broad linguistic featureset encoding various aspects of complexity.3 FeaturesThe feature set we experimented with consists of152 lexical and syntactic features that are primar-ily derived from the research on text complexityin Second Language Acquisition (SLA) and Psy-cholinguistics.
There are four types of features:Lexical richness features (LEX): This groupconsists of various part-of-speech (POS) tag den-sities, lexical richness features from SLA research,and the average number of senses per word.Concretely, the POS tag features are: the pro-portion of words belonging to different parts ofspeech (nouns, proper nouns, pronouns, determin-ers, adjectives, verbs, adverbs, conjunctions, in-terjections, and prepositions) and different verbforms (VBG, VBD, VBN, VBP in the Penn Tree-bank tagset; Santorini 1990) per document.The SLA-based lexical richness features weused are: type-token ratio and corrected type-token ratio, lexical density, ratio of nouns, verbs,adjectives and adverbs to the number of lexicalwords in a document, as described in Lu (2012).The POS information required to extract thesefeatures was obtained using Stanford Tagger(Toutanova et al., 2003).
The average number ofsenses for a non-function word was obtained byusing the MIT WordNet API2(Finlayson, 2014).Syntactic complexity features (SYNTAX): Thisgroup of features encodes the syntactic complex-ity of a text derived from the constituent struc-ture of the sentences.
Some of these features are2http://projects.csail.mit.edu/jwiderived from SLA research (Lu, 2010), specif-ically: mean lengths of production units (sen-tence, clause, t-unit), sentence complexity ratio(# clauses/sentence), subordination in a sentence(# clauses per t-unit, # complex t-units per t-unit,# dependent clauses per clause and t-unit), co-ordination in a sentence (# co-ordinate phrasesper clause and t-unit, # t-units/sentence), and spe-cific syntactic structures (# complex nominals perclause and t-unit, # VP per t-unit).
Other syntacticcomplexity features we made use of are the num-ber of NPs, VPs, PPs, and SBARs per sentenceand their average length (in terms of # words), theaverage parse tree height and the average numberof constituents per sub-tree.All of these features were extracted using theBerkeley Parser (Petrov and Klein, 2007) and theTregex pattern matcher (Levy and Andrew, 2006).While the selection of features for these twoclasses is based on Vajjala and Meurers (2012), forthe following two sets of features, we explored fur-ther information available through psycholinguis-tic resources.Psycholinguistic features (PSYCH): This groupof features includes an encoding of the averageAge-of-acquisition (AoA) of words according todifferent norms as provided by Kuperman et al.
(2012), including their own AoA rating obtainedthrough crowd sourcing.
It also includes mea-sures of word familiarity, concreteness, imageabil-ity, meaningfulness and AoA as assigned in theMRC Psycholinguistic database3(Wilson, 1988).For each feature, the value per text we computedis the average of the values for all the words in thetext that had an entry in the database.While these measures were not developed withreadability analysis in mind, we came across onepaper using such features as measures of worddifficulty in an approach to lexical simplification(Jauhar and Specia, 2012).3http://www.psych.rl.ac.uk/22Celex features (CELEX): The Celex lexicaldatabase (Baayen et al., 1995) for English con-sists of annotations for the morphological, syntac-tic, orthographic and phonological properties formore than 50k words and lemmas.
We includedall the morphological and syntactic properties thatwere encoded using character or numeric codes inour feature set.
We did not use frequency informa-tion from this database.In all, this feature set consists of 35 morpholog-ical and 49 syntactic properties per lemma.
Theset includes: proportion of morphologically com-plex words, attributive nouns, predicative adjec-tives, etc.
in the text.
A detailed description ofall the properties of the words and lemmas in thisdatabase can be found in the Celex English Lin-guistic Guide4.For both the PSYCH and CELEX features,we encode the average value for a given text.Words which were not included in the respec-tive databases were ignored for this computation.On average, around 40% of the words from textsfor covered by CELEX, 75% by Kuperman et al.
(2012) and 77% by the MRC database.We do not use any features encoding the occur-rence or frequency of specific words or n-grams ina document.4 Experiments and Results4.1 Experimental SetupWe used the WEKA toolkit (Hall et al., 2009) toperform our classification experiments and evalu-ated the classification accuracy using 10-fold crossvalidation.
As classification algorithm, we usedthe Sequential Minimal Optimization (SMO) im-plementation in WEKA, which marginally outper-formed (1?1.5%) some other classification algo-rithms (J48 Decision tree, Logistic Regression andRandom Forest) we tried in initial experiments.4.2 Classification accuracy with variousfeature groupsWe discussed in the context of Table 1 that sen-tence length may be a good surface indicator ofthe age-group.
So, we first constructed a classifi-cation model with only one feature.
This yieldeda classification accuracy of 71.4%, which we con-sider as our baseline (instead of a basic randombaseline of 33%).4http://catalog.ldc.upenn.edu/docs/LDC96L14/eug_a4.pdfWe then constructed a model with all the fea-tures we introduced in section 3.
This modelachieves a classification accuracy of 95.9%, whichis a 23.7% improvement over the sentence lengthbaseline in terms of classification accuracy.In order to understand what features contributethe most to classification accuracy, we applied fea-ture selection on the entire set, using two algo-rithms available in WEKA, which differ in the waythey select feature subsets:?
InfoGainAttributeEval evaluates the featuresindividually based on their Information Gain(IG) with respect to the class.?
CfsSubsetEval (Hall, 1999) chooses a featuresubset considering the correlations betweenfeatures in addition to their predictive power.Both feature selection algorithms use methodsthat are independent of the classification algorithmas such to select the feature subsets.Information Gain-based feature selection re-sults in a ranked list of features, which are inde-pendent of each other.
The Top-10 features ac-cording to this algorithm are listed in Table 2.Feature Groupavg.
AoA (Kuperman et al., 2012) PSYCHavg.
# PPs in a sentence SYNTAXavg.
# instances where the lemmahas stem and affixCELEX?
avg.
parse tree height SYNTAX?
avg.
# NPs in a sentence SYNTAXavg.
# instances of affix substitution CELEX?
avg.
# prep.
in a sentence LEXavg.
# instances where a lemma isnot a count nounCELEXavg.
# clauses per sentence SYNTAX?
sentence length SYNTAXTable 2: Ranked list of Top-10 features using IGAs is clear from their description, all Top-10features encode different linguistic aspects of atext.
While there are more syntactic features fol-lowed by Celex features in these Top-10 features,the most predictive feature is a psycholinguisticfeature encoding the average age of acquisition ofwords.
A classifier using only the Top-10 IG fea-tures achieves an accuracy of 84.5%.Applying CfsSubsetEval to these Top-10 fea-tures set selects the six features not prefixed by a23hyphen in the table, indicating that these featuresdo not correlate with each other (much).
A clas-sifier using only this subset of 6 features achievesan accuracy of 84.1%.We also explored the use of CfsSubsetEval fea-ture selection on the entire feature set instead ofusing only the Top 10 features.
From the total of152 features, CfsSubsetEval selected a set of 41features.
Building a classification model with onlythese features resulted in a classification accuracyof 93.9% which is only 2% less than the modelincluding all the features.Table 3 shows the specific feature subset se-lected by the CfsSubsetEval method, including# preposition phrases# t-units# co-ordinate phrases per t-unit# lexical words in total words# interjections# conjunctive phrases# word senses# verbs# verbs, past participle (VBN)# proper nouns# plural nounsavg.
corrected type-token ratioavg.
AoA acc.
to ratings of Kuperman et al.
(2012)avg.
AoA acc.
to ratings of Cortese and Khanna (2008)avg.
word imageability rating (MRC)avg.
AoA according to MRC# morph.
complex words (e.g., sandbank)# morph.
conversion (e.g., abandon)# morph.
irrelevant (e.g., meow)# morph.
obscure (e.g., dedicate)# morph.
may include root (e.g., imprimatur)# foreign words (e.g., eureka)# words with multiple analyses (e.g., treasurer)# noun verb affix compounds (e.g., stockholder)# lemmas with stem and affix (e.g., abundant=abound+ant)# flectional forms (e.g., bagpipes)# clipping allomorphy (e.g., phone vs. telephone)# deriv.
allomorphy (e.g., clarify?clarification)# flectional allomorphy (e.g., verb bear 7?
adjective born)# conversion allomorphy (e.g., halve?half )# lemmas with affix substitution (e.g., active=action+ive)# words with reversion (e.g., downpour)# uncountable nouns# collective, countable nouns# collective, uncountable nouns# post positive nouns.# verb, expression (e.g., bell the cat)# adverb, expression (e.g., run amok)# reflexive pronouns# wh pronouns# determinative pronounsTable 3: CfsSubsetEval feature subsetsome examples illustrating the morphological fea-tures.
The method does not provide a ranked list,so the features here simply appear in the order inwhich they are included in the feature vector.All of these features except for the psycholinguis-tic features encode the number of occurrences av-eraged across the text (e.g., average number ofprepositions/sentence in a text) unless explicitlystated otherwise.
The psycholinguistic featuresencode the average ratings of words for a givenproperty (e.g., average AoA of words in a text).Table 4 summarizes the classification accura-cies with the different feature subsets seen so far,with the feature count shown in parentheses.Feature Subset (#) Accuracy SDAll Features (152) 95.9% 0.37Cfs on all features (41) 93.9% 0.59Top-10 IG features (10) 84.5% 0.70Cfs on IG (6) 84.1% 0.55Table 4: Accuracy with various feature subsetsWe performed statistical significance tests be-tween the feature subsets using the Paired T-tester(corrected), provided with WEKA and all the dif-ferences in accuracy were found to be statisticallysignificant at p < 0.001.
We also provide the Stan-dard Deviation (SD) of the test set accuracy in the10 folds of CV per dataset, to make it possible tocompare these experiments with future research onthis dataset in terms of statistical significance.Table 5 presents the classification accuracies ofindividual features from the Top-10 features list(introduced in Table 2).Feature AccuracyAoA Kup Lem 82.4%# pp 74.0%# stem & affix 77.7%avg.
parse tree height 73.4%# np 73.0%# substitution 74.3%# prep 72.0%# uncountable nouns 68.3%# clauses 72.5%sentence length 71.4%Table 5: Accuracies of Top-10 individual featuresThe table shows that all but one of the featuresindividually achieves a classification accuracyabove 70%.
The first feature (AoA Kup Lem)24alone resulted in an accuracy of 82.4%, which isquite close to the accuracy obtained by all the Top-10 features together (84.5%).To obtain a fuller picture of the impact of dif-ferent feature groups, we also performed ablationtests removing some groups of features at a time.Table 6 shows the results of these tests along withthe SD of the 10 fold CV.
All the results that arestatistically different at p < 0.001 from the modelwith all features (95.9% accuracy, 0.37 SD) are in-dicated with a *.Features Acc.
SDAll ?
AoA Kup Lem 95.9% 0.37All ?
All AoA Features 95.6% 0.58All ?
PSYCH 95.8% 0.31All ?
CELEX 94.7%* 0.51All ?
CELEX?PSYCH 93.6%* 0.66All ?
CELEX?PSYCH?LEX(= SYNTAX only) 77.5%* 0.99LEX 93.1%* 0.70CELEX 90.0%* 0.79PSYCH 84.5%* 1.12Table 6: Ablation test accuraciesInterestingly, removing the most predictive in-dividual feature (AoA Kup Lem) from the featureset did not change the overall classification accu-racy at all.
Removing all of the AoA features orall of the psycholinguistic features also resulted inonly a very small drop.
The combination of thelinguistic features, covering lexical and syntacticcharacteristics as well as the morphological, syn-tactic, orthographic, and phonological propertiesfrom Celex, thus seem to be equally characteristicof the texts targeting different age-groups as thepsycholinguistic properties, even though the fea-tures are quite different in nature.In terms of separate groups of features, syntac-tic features alone performed the worst (77.5%) andlexical richness features the best (93.1%).To investigate which classes were mixed up bythe classifier, consider Table 7 showing the con-fusion matrix for the model with all features on a10-fold CV experiment.We find that CBeebies is more often con-fused with the CBBC program for older chil-dren (156+214) and very rarely with the programfor adults (1+2).
The older children programs(CBBC) are more commonly confused with pro-grams for adults (36+58) compared to CBeebiesclassified as?
CBeebies CBBC AdultsCBeebies (0?6) 3619 156 1CBBC (6?12) 214 3526 36Adults (12+) 2 58 3716Table 7: Confusion Matrix(1+2), which is expected given that the CBBC au-dience is closer in age to adults than the CBeebiesaudience.Summing up, we can conclude from these ex-periments that the classification of transcripts intoage groups can be informed by a wide range of lin-guistics and psycholinguistic features.
While forsome practical tasks a few features may be enoughto obtain a classification of sufficient accuracy, themore general take-home message is that authentictexts targeting specific age groups exhibit a broadrange of linguistics characteristics that are indica-tive of the complexity of the language used.4.3 Effect of text size and training data sizeWhen we first introduced the properties of the cor-pus in Table 1, it appeared that sentence lengthand the overall text length could be important pre-dictors of the target age-groups.
However, the listof Top-10 features based on information gain wasdominated by more linguistically oriented syntac-tic and psycholinguistic features.Sentence length was only the tenth best featureby information gain and did not figure at all in the43 features chosen by the CfsSubsetEval methodselecting features that are highly correlated withthe class prediction while having low correlationbetween themselves.
As mentioned above, sen-tence length as an individual feature only achieveda classification accuracy of 71.4%.The text length is not a part of any feature set weused, but considering the global corpus propertieswe wanted to verify how well it would performand thus trained a model with only text length(#sentences per text) as a feature.
This achieveda classification accuracy of only 56.7%.The corpus consists of transcripts of whole TVprograms and hence an individual transcript texttypically is longer than the texts commonly used inreadability classification experiments.
This raisesthe question whether the high classification accu-racies we obtained are the consequences of thelarger text size.As a second issue, the training size available forthe 10-fold cross-validation experiments is com-25paratively large, given the 3776 text per levelavailable in the overall corpus.
We thus alsowanted to study the impact of the training size onthe classification accuracy achieved.Pulling these threads together, we comparedthe classification accuracy against text length andtraining set size to better understand their impact.For this, we trained models with different textsizes (by considering the first 25%, 50%, 75% or100% of the sentences from each text) and withdifferent training set sizes (from 10% to 100%).Figure 1 presents the resulting classification ac-curacy in relation to training set size for the dif-ferent text sizes.
All models were trained with thefull feature set (152 features), using 10-fold cross-validation as before.9091929394959610  20  30  40  50  60  70  80  90  100classification accuracy (in percent)training set size (in percent)Variation of Classification Accuracy with training set size and text sample size25% text size50% text size75% text size100% text sizeFigure 1: Classification accuracy for different textsizes and training set sizesAs expected, both the training set size and thetext size affect the classification accuracy.
How-ever, the classification accuracy even for the small-est text and training set size is always above 90%,which means that the unusually large text andtraining size is not the main factor behind the veryhigh accuracy rates.In all four cases of text size, there was a smalleffect of training set size on the classification ac-curacy.
But the effect reduced as the text size in-creased.
At 25% text size, for example, the clas-sification accuracy ranged 90?93% (mean 92.1%,SD 0.9) as the training set size increased from 10%to 100%.
However, at 100% text size, the rangewas only 94.8?96% (mean 95.6%, SD 0.4).Comparing the results in terms of text sizealone, larger text size resulted in better classifica-tion accuracy in all cases, irrespective of the train-ing set size.
A longer text will simply providemore information for the various linguistic fea-tures, enabling the model to deliver better judg-ments about the text.
However, despite the textlength being reduced to one fourth of its size, themodels built with our feature set always collectenough information to ensure a classification ac-curacy of at least 90%.In the above experiments, we varied the text sizefrom 10% to 100%.
But since these are percent-ages, texts from CBBC and Adults on average stillare longer than CBEEBIES texts.
While this re-flects the fact that TV transcripts in real life are ofdifferent length, we also wanted to see what hap-pens when we eliminate such length differences.We thus trained classification models fixing thelength of all documents to a concrete absolutelength, starting from 100 words (rounded off to thenearest sentence boundary) increasing the text sizeuntil we achieve the best overall performance.
Fig-ure 2 displays the classification accuracy we ob-tained for the different (maximum) text sizes, forall features and feature subsets.65707580859095100100  200  300  400  500  600  700  800  900classification accuracy (in percent)max.
text size (in number of words)Variation of Classification Accuracy with text sample size in wordsAll FeaturesPSYCHLEXSYNCELEXFigure 2: Classification accuracy for different ab-solute text sizes (in words)The plot shows that the classification accuracyalready reaches 80% accuracy for short texts, 100words in length, for the model with all features.
Itrises to above 90% for texts which are 300 wordslong and reaches the best overall accuracy of al-most 96% for texts which are 900 words in length.All the feature subsets too follow the same trend,with varying degrees of accuracy that is alwayslower than the model with all features.While in this paper, we focus on documents,the issue whether the data can be reduced further26to perform readability at the sentence level is dis-cussed in Vajjala and Meurers (2014a).5 Related WorkAnalyzing the complexity of written texts andchoosing suitable texts for various target groupsincluding children is widely studied in computa-tional linguistics.
Some of the popular approachesinclude the use of language models and machinelearning approaches (e.g., Collins-Thompson andCallan, 2005; Feng, 2010).
Web-based tools suchas REAP5and TextEvaluator6are some examplesof real-life applications for selecting English textsby grade level.In terms of analyzing spoken language, researchin language assessment has analyzed spoken tran-scripts in terms of syntactic complexity (Chen andZechner, 2011) and other textual characteristics(Crossley and McNamara, 2013).In the domain of readability assessment,the Common Core Standards (http://www.corestandards.org) guideline texts wereused as a standard test set in the recent past (Nel-son et al., 2012; Flor et al., 2013).
This test setcontains some transcribed speech.
However, tothe best of our knowledge, the process of select-ing suitable TV programs for children as exploredin this paper has not been considered as a case ofreadability assessment of spoken language before.Subtitle corpora have been created and usedin computational linguistics for various pur-poses.
Some of them include video classifica-tion (Katsiouli et al., 2007), machine translation(Petukhova et al., 2012), and simplification fordeaf people (Daelemans et al., 2004).
But, we arenot aware of any such subtitle research studyingthe problem of automatically identifying TV pro-grams for various age-groups.This paper thus can be seen as connecting sev-eral threads of research, from the analysis of textcomplexity and readability, via the research onmeasuring SLA proficiency that many of the lin-guistic features we used stem from, to the com-putational analysis of speech as encoded in subti-tles.
The range of linguistic characteristics whichturn out to be relevant and the very high preci-sion with which the age-group classification canbe performed, even when restricting the input to5http://reap.cs.cmu.edu6https://texteval-pilot.ets.org/TextEvaluatorartificially shortened transcripts, confirm the use-fulness of connecting these research threads.6 ConclusionsIn this paper, we described a classification ap-proach identifying TV programs for differentage-groups based on a range of linguistically-motivated features derived from research on textreadability, proficiency in SLA, and psycholin-guistic research.
Using a collection of subtitledocuments classified into three groups based onthe targeted age-group, we explored different clas-sification models with our feature set.The experiments showed that our linguisticallymotivated features perform very well, achievinga classification accuracy of 95.9% (section 4.2).Apart from the entire feature set, we also exper-imented with small groups of features by apply-ing feature selection algorithms.
As it turns out,the single most predictive feature was the age-of-acquisition feature of Kuperman et al.
(2012),with an accuracy of 82.4%.
Yet when this fea-ture is removed from the overall feature set, theclassification accuracy is not reduced, highlightingthat such age-group classification is informed by arange of different characteristics, not just a single,dominating one.
Authentic texts targeting specificage groups exhibit a broad range of linguistics andpsycholinguistic characteristics that are indicativeof the complexity of the language used.While an information gain-based feature subsetconsisting of 10 features resulted in an accuracy of84.5%, a feature set chosen using the CfsSubsetE-val method in WEKA gave an accuracy of 93.9%.Any of the feature groups we tested exceeded therandom baseline (33%) and a baseline using thepopular sentence length feature (71.4%) by a largemargin.
Individual feature groups also performedwell at over 90% accurately in most of the cases.The analysis thus supports multiple, equally validperspectives on a given text, each view encoding adifferent linguistic aspect.Apart from the features explored, we also stud-ied the effect of the training set size and the lengthof the text considered for feature extraction onclassification accuracy (Section 4.3).
The size oftraining set mattered more when the text size wassmaller.
Text size, which did not work well as anindividual feature, clearly influences classificationaccuracy by providing more information for modelbuilding and testing.27In terms of the practical relevance of the re-sults, one question that needs some attention ishow well the features and trained models gener-alize across different type of TV programs or lan-guages.
While we have not yet investigated thisfor TV subtitles, in experiments investigating thecross-corpus performance of a model using thesame feature set, we found that the approach per-forms well for a range of corpora composed ofreading materials for language learners (Vajjalaand Meurers, 2014b).
The very high classificationaccuracies of the experiments we presented in thecurrent paper thus seem to support the assumptionthat the approach can be useful in practice for au-tomatically identifying TV programs for viewersof different age groups.Regarding the three class distinctions and theclassifier setup we used in this paper, the approachcan also be generalized to other scales and a re-gression setup (Vajjala and Meurers, 2013).6.1 OutlookThe current work focused mostly on modeling andstudying different feature groups in terms of theirclassification accuracy.
Performing error analysisand looking at the texts where the approach failedmay yield further insights into the problem.
Someaspects of the text that we did not consider in-clude discourse coherence or topic effects.
Study-ing these two aspects can provide more insightsinto the nature of the language used in TV pro-grams directed at viewers of different ages.
Across-genre evaluation between written and spo-ken language complexity across age-groups couldalso be insightful.On the technical side, it would also be usefulto explore the possibility of using a parser tunedto spoken language, to check if this helps improvethe classification accuracy of syntactic features.While in this paper we focused on English, arelated readability model also performed well forGerman (Hancke et al., 2012) so that we expectthe general approach to be applicable to other lan-guages, subject to the availability of the relevantresources and tools.AcknowledgementsWe would like to thank Marc Brysbaert and hiscolleagues for making their excellent resourcesavailable to the research community.
We alsothank the anonymous reviewers for their usefulfeedback.
This research was funded by LEADGraduate School (GSC 1028, http://purl.org/lead), a project of the Excellence Initiative of theGerman federal and state governments.ReferencesHarald R. Baayen, Richard Piepenbrock, and Leon Gu-likers.
1995.
The CELEX lexical database.
http://catalog.ldc.upenn.edu/LDC96L14.Maio Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features for auto-mated scoring of spontaneous non-native speech.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages722?731, Portland, Oregon, June.Kevyn Collins-Thompson and Jamie Callan.
2005.Predicting reading difficulty with statistical lan-guage models.
Journal of the American Society forInformation Science and Technology, 56(13):1448?1462.Michael J. Cortese and Maya M. Khanna.
2008.
Ageof acquisition ratings for 3,000 monosyllabic words.Behavior Research Methods, 43:791?794.Scott Crossley and Danielle McNamara.
2013.
Ap-plications of text analysis tools for spoken re-sponse grading.
Language Learning & Technology,17:171?192.Walter Daelemans, Anja Hoethker, and Erik F.Tjong Kim Sang.
2004.
Automatic sentence sim-plification for subtitling in Dutch and English.
InFourth International Conference on Language Re-sources And Evaluation (LREC), pages 1045?1048.Lijun Feng.
2010.
Automatic Readability Assessment.Ph.D.
thesis, City University of New York (CUNY).Mark Alan Finlayson.
2014.
Java libraries for access-ing the princeton wordnet: Comparison and evalua-tion.
In Proceedings of the 7th Global Wordnet Con-ference, pages 78?85.Michael Flor, Beata Beigman Klebanov, and Kath-leen M. Sheehan.
2013.
Lexical tightness and textcomplexity.
In Proceedings of the Second Workshopon Natural Language Processing for Improving Tex-tual Accessibility (PITR) held at ACL, pages 29?38,Sofia, Bulgaria.
ACL.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.The SIGKDD Explorations, 11:10?18.Mark A.
Hall.
1999.
Correlation-based Feature Selec-tion for Machine Learning.
Ph.D. thesis, The Uni-versity of Waikato, Hamilton, NewZealand.28Julia Hancke, Detmar Meurers, and Sowmya Vajjala.2012.
Readability classification for german usinglexical, syntactic, and morphological features.
InProceedings of the 24th International Conferenceon Computational Linguistics (COLING): TechnicalPapers, pages 1063?1080, Mumbai, India.Sujay Kumar Jauhar and Lucia Specia.
2012.
Uow-shef: Simplex ?
lexical simplicity ranking based oncontextual and psycholinguistic features.
In In pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics (SEM).Polyxeni Katsiouli, Vassileios Tsetsos, and StathesHadjiefthymiades.
2007.
Semantic video classifi-cation based on subtitles and domain terminologies.In Proceedings of the 1st International Workshopon Knowledge Acquisition from Multimedia Content(KAMC).Emmanuel Keuleers, Paula Lacey, Kathleen Rastle,and Marc Brysbaert.
2012.
The british lexiconproject: Lexical decision data for 28,730 monosyl-labic and disyllabic english words.
Behavior Re-search Methods, 44:287?304.Victor Kuperman, Hans Stadthagen-Gonzalez, andMarc Brysbaert.
2012.
Age-of-acquisition ratingsfor 30,000 english words.
Behavior Research Meth-ods, 44(4):978?990.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In 5th International Conference on Lan-guage Resources and Evaluation, pages 2231?2234,Genoa, Italy.
European Language Resources Asso-ciation (ELRA).Xiaofei Lu.
2010.
Automatic analysis of syntac-tic complexity in second language writing.
Inter-national Journal of Corpus Linguistics, 15(4):474?496.Xiaofei Lu.
2012.
The relationship of lexical richnessto the quality of ESL learners?
oral narratives.
TheModern Languages Journal, pages 190?208.Jessica Nelson, Charles Perfetti, David Liben, andMeredith Liben.
2012.
Measures of text difficulty:Testing their predictive value for grade levels andstudent performance.
Technical report, The Coun-cil of Chief State School Officers.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Volha Petukhova, Rodrigo Agerri, Mark Fishel, YotaGeorgakopoulou, Sergio Penkale, Arantza del Pozo,Mirjam Sepesy Maucec, Martin Volk, and AndyWay.
2012.
Sumat: Data collection and parallelcorpus compilation for machine translation of sub-titles.
In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation(LREC-2012), pages 21?28, Istanbul, Turkey.
Euro-pean Language Resources Association (ELRA).Beatrice Santorini.
1990.
Part-of-speech taggingguidelines for the Penn Treebank, 3rd revision, 2ndprinting.
Technical report, Department of ComputerScience, University of Pennsylvania.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-ofspeech tagging with a cyclic dependency net-work.
In HLT-NAACL, pages 252?259, Edmonton,Canada.Sowmya Vajjala and Detmar Meurers.
2012.
On im-proving the accuracy of readability classification us-ing insights from second language acquisition.
InIn Proceedings of the 7th Workshop on InnovativeUse of NLP for Building Educational Applications(BEA) at NAACL-HLT, pages 163?-173, Montr?eal,Canada.
ACL.Sowmya Vajjala and Detmar Meurers.
2013.
Onthe applicability of readability models to web texts.In Proceedings of the Second Workshop on NaturalLanguage Processing for Improving Textual Acces-sibility (PITR) held at ACL, pages 59?-68, Sofia,Bulgaria.
ACL.Sowmya Vajjala and Detmar Meurers.
2014a.
Assess-ing the relative reading level of sentence pairs fortext simplification.
In Proceedings of the 14th Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL).
ACL.Sowmya Vajjala and Detmar Meurers.
2014b.
Read-ability assessment for text simplification: From an-alyzing documents to identifying sentential simplifi-cations.
International Journal of Applied Linguis-tics, Special Issue on Current Research in Read-ability and Text Simplification, edited by ThomasFranc?ois and Delphine Bernhard.Walter J.B. Van Heuven, Pawel Mandera, EmmanuelKeuleers, and Marc Brysbaert.
2014.
Subtlex-UK:A new and improved word frequency database forBritish English.
The Quarterly Journal of Experi-mental Psychology, pages 1?15.Michael D. Wilson.
1988.
The mrc psycholinguis-tic database: Machine readable dictionary, version2.
Behavioural Research Methods, Instruments andComputers, 20(1):6?11.29
