Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 78?86,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsSemi-supervised learning of concatenative morphologyOskar Kohonen and Sami Virpioja and Krista LagusAalto University School of Science and TechnologyAdaptive Informatics Research CentreP.O.
Box 15400, FI-00076 AALTO, Finland{oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fiAbstractWe consider morphology learning in asemi-supervised setting, where a smallset of linguistic gold standard analyses isavailable.
We extend Morfessor Base-line, which is a method for unsupervisedmorphological segmentation, to this task.We show that known linguistic segmenta-tions can be exploited by adding them intothe data likelihood function and optimiz-ing separate weights for unlabeled and la-beled data.
Experiments on English andFinnish are presented with varying amountof labeled data.
Results of the linguis-tic evaluation of Morpho Challenge im-prove rapidly already with small amountsof labeled data, surpassing the state-of-the-art unsupervised methods at 1000 la-beled words for English and at 100 labeledwords for Finnish.1 IntroductionMorphological analysis is required in many natu-ral language processing problems.
Especially, inagglutinative and compounding languages, whereeach word form consists of a combination of stemsand affixes, the number of unique word forms ina corpus is very large.
This leads to problems inword-based statistical language modeling: Evenwith a large training corpus, many of the words en-countered when applying the model did not occurin the training corpus, and thus there is no infor-mation available on how to process them.
Usingmorphological units, such as stems and affixes, in-stead of complete word forms alleviates this prob-lem.
Unfortunately, for many languages morpho-logical analysis tools either do not exist or theyare not freely available.
In many cases, the prob-lems of availability also apply to morphologicallyannotated corpora, making supervised learning in-feasible.In consequence, there has been a need for ap-proaches for morphological processing that wouldrequire little language-dependent resources.
Dueto this need, as well as the general interest inlanguage acquisition and unsupervised languagelearning, the research on unsupervised learningof morphology has been active during the pastten years.
Especially, methods that perform mor-phological segmentation have been studied exten-sively (Goldsmith, 2001; Creutz and Lagus, 2002;Monson et al, 2004; Bernhard, 2006; Dasguptaand Ng, 2007; Snyder and Barzilay, 2008b; Poonet al, 2009).
These methods have shown to pro-duce results that improve performance in severalapplications, such as speech recognition and in-formation retrieval (Creutz et al, 2007; Kurimo etal., 2008).While unsupervised methods often work quitewell across different languages, it is difficult toavoid biases toward certain kinds of languages andanalyses.
For example, in isolating languages, theaverage amount of morphemes per word is low,whereas in synthetic languages the amount may bevery high.
Also, different applications may needa particular bias, for example, not analyzing fre-quent compound words as consisting of smallerparts could be beneficial in information retrieval.In many cases, even a small amount of labeled datacan be used to adapt a method to a particular lan-guage and task.
Methodologically, this is referredto as semi-supervised learning.In semi-supervised learning, the learning sys-tem has access to both labeled and unlabeled data.Typically, the labeled data set is too small for su-pervised methods to be effective, but there is alarge amount of unlabeled data available.
Thereare many different approaches to this class ofproblems, as presented by Zhu (2005).
One ap-proach is to use generative models, which spec-ify a join distribution over all variables in themodel.
They can be utilized both in unsupervised78and supervised learning.
In contrast, discrimina-tive models only specify the conditional distribu-tion between input data and labels, and thereforerequire labeled data.
Both, however, can be ex-tended to the semi-supervised case.
For generativemodels, it is, in principle, very easy to use both la-beled and unlabeled data.
For unsupervised learn-ing one can consider the labels as missing data andestimate their values using the Expectation Maxi-mization (EM) algorithm (Dempster et al, 1977).In the semi-supervised case, some labels are avail-able, and the rest are considered missing and esti-mated with EM.In this paper, we extend the Morfessor Base-line method for the semi-supervised case.
Morfes-sor (Creutz and Lagus, 2002; Creutz and Lagus,2005; Creutz and Lagus, 2007, etc.)
is one of thewell-established methods for morphological seg-mentation.
It applies a simple generative model.The basic idea, inspired by the Minimum Descrip-tion Length principle (Rissanen, 1989), is to en-code the words in the training data with a lexiconof morphs, that are segments of the words.
Thenumber of bits needed to encode both the morphlexicon and the data using the lexicon should beminimized.
Morfessor does not limit the num-ber of morphemes per word form, making it suit-able for modeling a large variety of agglutinativelanguages irrespective of them being more isolat-ing or synthetic.
We show that the model can betrained in a similar fashion in the semi-supervisedcase as in the unsupervised case.
However, witha large set of unlabeled data, the effect of the su-pervision on the results tends to be small.
Thus,we add a discriminative weighting scheme, wherea small set of word forms with gold standard ana-lyzes are used for tuning the respective weights ofthe labeled and unlabeled data.The paper is organized as follows: First, wediscuss related work on semi-supervised learning.Then we describe the Morfessor Baseline modeland the unsupervised algorithm, followed by oursemi-supervised extension.
Finally, we present ex-perimental results for English and Finnish usingthe Morpho Challenge data sets (Kurimo et al,2009).1.1 Related workThere is surprisingly little work that consider im-proving the unsupervised models of morphologywith small amounts of annotated data.
In therelated tasks that deal with sequential labeling(word segmentation, POS tagging, shallow pars-ing, named-entity recognition), semi-supervisedlearning is more common.Snyder and Barzilay (2008a; 2008b) considerlearning morphological segmentation with non-parametric Bayesian model from multilingualdata.
For multilingual settings, they extract 6 139parallel short phrases from the Hebrew, Arabic,Aramaic and English bible.
Using the alignedphrase pairs, the model can learn the segmen-tations for two languages at the same time.
Inone of the papers (2008a), they consider alsosemi-supervised scenarios, where annotated datais available either in only one language or both ofthe languages.
However, the amount of annotateddata is fixed to the half of the full data.
This differsfrom our experimental setting, where the amountof unlabeled data is very large and the amount oflabeled data relatively small.Poon et al (2009) apply a log-linear, undi-rected generative model for learning the morphol-ogy of Arabic and Hebrew.
They report resultsfor the same small data set as Snyder and Barzilay(2008a) in both unsupervised and semi-supervisedsettings.
For the latter, they use somewhat smallerproportions of annotated data, varying from 25%to 100% of the total data, but the amount of unla-beled data is still very small.
Results are reportedalso for a larger 120 000 word Arabic data set, butonly for unsupervised learning.A problem similar to morphological segmen-tation is word segmentation for the languageswhere orthography does not specify word bound-aries.
However, the amount of labeled data isusually large, and unlabeled data is just an addi-tional source of information.
Li and McCallum(2005) apply a semi-supervised approach to Chi-nese word segmentation where unlabeled data isutilized for forming word clusters, which are thenused as features for a supervised classifier.
Xuet al (2008) adapt a Chinese word segmentationspecifically to a machine translation task, by usingthe indirect supervision from a parallel corpus.2 MethodWe present an extension of the Morfessor Baselinemethod to the semi-supervised setting.
Morfes-sor Baseline is based on a generative probabilis-tic model.
It is a method for modeling concatena-tive morphology, where the morphs?i.e., the sur-79face forms of morphemes?of a word are its non-overlapping segments.
The model parameters ?encode a morph lexicon, which includes the prop-erties of the morphs, such as their string represen-tations.
Each morph m in the lexicon has a proba-bility of occurring in a word, P (M = m |?
).1 Theprobabilities are assumed to be independent.
Themodel uses a prior P (?
), derived using the Min-imum Description Length (MDL) principle, thatcontrols the complexity of the model.
Intuitively,the prior assigns higher probability to models thatstore fewer morphs, where a morph is consideredstored if P (M = m |?)
> 0.
During model learn-ing, ?
is optimized to maximize the posterior prob-ability:?MAP= argmax?P (?|DW)= argmax?
{P (?
)P (DW|?
)}, (1)where DWincludes the words in the trainingdata.
In this section, we first consider sepa-rately the likelihood P (DW|?)
and the prior P (?
)used in Morfessor Baseline.
Then we describethe algorithms, first unsupervised and then semi-supervised, for finding optimal model parameters.Last, we shortly discuss the algorithm for seg-menting new words after the model training.2.1 LikelihoodThe latent variable of the model, Z =(Z1, .
.
.
, Z|DW|), contains the analyses of thewords in the training data DW.
An instance ofa single analysis for the j:th word is a sequence ofmorphs, zj= (mj1, .
.
.
,mj|zj|).
During training,each word wjis assumed to have only one possibleanalysis.
Thus, instead of using the joint distribu-tion P (DW,Z |?
), we need to use the likelihoodfunction only conditioned on the analyses of theobserved words, P (DW|Z,?).
The conditionallikelihood isP (DW|Z = z,?
)=|DW|?j=1P (W = wj|Z = z,?
)=|DW|?j=1|zj|?i=1P (M = mji|?
), (2)where mijis the i:th morph in word wj.1We denote variables with uppercase letters and their in-stances with lowercase letters.2.2 PriorsMorfessor applies Maximum A Posteriori (MAP)estimation, so priors for the model parametersneed to be defined.
The parameters ?
of the modelare:?
Morph type count, or the size of the morphlexicon, ?
?
Z+?
Morph token count, or the number of morphstokens in the observed data, ?
?
Z+?
Morph strings (?1, .
.
.
, ??
), ?i?
???
Morph counts (?1, .
.
.
, ??
), ?i?
{1, .
.
.
, ?
},?i?i= ?.
Normalized with ?, these givethe probabilities of the morphs.MDL-inspired and non-informative priors havebeen preferred.
When using such priors, morphtype count and morph token counts can be ne-glected when optimizing the model.
The morphstring prior is based on length distribution P (L)and distribution P (C) of characters over the char-acter set ?, both assumed to be known:P (?i) = P (L = |?i|)|?i|?j=1P (C = ?ij) (3)We use the implicit length prior (Creutz and La-gus, 2005), which is obtained by removing P (L)and using end-of-word mark as an additional char-acter in P (C).
For morph counts, the non-informative priorP (?1, .
.
.
, ??)
= 1/(?
?
1??
1)(4)gives equal probability to each possible combina-tion of the counts when ?
and ?
are known, asthere are(??1?
?1)possible ways to choose ?
positiveintegers that sum up to ?.2.3 Unsupervised learningIn principle, unsupervised learning can be per-formed by looking for the MAP estimate with theEM-algorithm.
In the case of Morfessor Baseline,this is problematic, because the prior only assignshigher probability to lexicons where fewer morphshave nonzero probabilities.
The EM-algorithm hasthe property that it will not assign a zero probabil-ity to any morph, that has a nonzero likelihood inthe previous step, and this will hold for all morphs80that initially have a nonzero probability.
In con-sequence, Morfessor Baseline instead uses a localsearch algorithm, which will assign zero probabil-ity to a large part of the potential morphs.
Thisis memory-efficient, since only the morphs withnonzero probabilities need to be stored in mem-ory.
The training algorithm of Morfessor Base-line, described by Creutz and Lagus (2005), triesto minimize the cost functionL(?, z,DW) = ?
lnP (?)?
lnP (DW| z,?
)(5)by testing local changes to z, modifying the pa-rameters according to each change, and selectingthe best one.
More specifically, one word is pro-cessed at a time, and the segmentation that min-imizes the cost function with the optimal modelparameters is selected:z(t+1)j= argminzj{min?L(?, z(t),DW)}.
(6)Next, the parameters are updated:?
(t+1)= argmin?
{L(?, z(t+1),DW)}.
(7)As neither of the steps can increase the cost func-tion, this will converge to a local optimum.
Theinitial parameters are obtained by adding all thewords into the morph lexicon.
Due to the contextindependence of the morphs within a word, the op-timal analysis for a segment does not depend onin which context the segment appears.
Thus, it ispossible to encode z as a binary tree-like graph,where the words are the top nodes and morphs theleaf nodes.
For each word, every possible split intotwo morphs is tested in addition to no split.
If theword is split, the same test is applied recursivelyto its parts.
See, e.g., Creutz and Lagus (2005) formore details and pseudo-code.2.4 Semi-supervised learningA straightforward way to do semi-supervisedlearning is to fix the analyses z for the labeled ex-amples.
Early experiments indicated that this haslittle effect on the results.
The Morfessor Baselinemodel only contains local parameters for morphs,and relies on the bias given by its prior to guidethe amount of segmentation.
Therefore, it may notbe well suited for semi-supervised learning.
Thelabeled data affects only the morphs that are foundin the labeled data, and even their analyses can beoverwhelmed by a large amount of unsuperviseddata and the bias of the prior.We suggest a fairly simple solution to this byintroducing extra parameters that guide the moregeneral behavior of the model.
The amount ofsegmentation is mostly affected by the balancebetween the prior and the model.
The Morfes-sor Baseline model has been developed to ensurethis balance is sensible.
However, the labeleddata gives a strong source of information regardingthe amount of segmentation preferred by the goldstandard.
We can utilize this information by intro-ducing the weight ?
on the likelihood.
To addressthe problem of labeled data being overwhelmed bythe large amount of unlabeled data we introduce asecond weight ?
on the likelihood for the labeleddata.
These weights are optimized on a separateheld-out set.
Thus, instead of optimizing the MAPestimate, we minimize the following function:L(?, z,DW,DW 7?A) =?
lnP (?)?
??
lnP (DW| z,?)?
?
?
lnP (DW 7?A| z,?)
(8)The labeled training set DW 7?Amay include al-ternative analyses for some of the words.
LetA(wj) = {aj1, .
.
.
, ajk} be the set of known anal-yses for word wj.
Assuming the training samplesare independent, and giving equal weight for eachanalysis, the likelihood of the labeled data wouldbeP (DW 7?A|?
)=|DW 7?A|?j=1?ajk?A(wj)|ajk|?i=1P (M = mjki|?).
(9)However, when the analyses of the words arefixed, the product over alternative analyses in Ais problematic, because the model cannot selectseveral of them at the same time.
A sum overA(wj):s would avoid this problem, but then thelogarithm of the likelihood function becomes non-trivial (i.e., logarithm of sum of products) and tooslow to calculate during the training.
Instead, weuse the hidden variable Z to select only one anal-ysis also for the labeled samples, but now with therestriction that Zj?
A(wj).
The likelihood func-tion for DW 7?Ais then equivalent to Equation 2.Because the recursive algorithm search assumesthat a string is segmented in the same way irre-spective of its context, the labeled data can still81get zero probabilities.
In practice, zero probabil-ities in the labeled data likelihood are treated asvery large, but not infinite, costs.2.5 Segmenting new wordsAfter training the model, a Viterbi-like algorithmcan be applied to find the optimal segmentationof each word.
As proposed by Virpioja and Ko-honen (2009), also new morph types can be al-lowed by utilizing an approximate cost of addingthem to the lexicon.
As this enables reasonable re-sults also when the training data is small, we use asimilar technique.
The cost is calculated from thedecrease in the probabilities given in Equations 3and 4 when a new morph is assumed to be in thelexicon.3 ExperimentsIn the experiments, we compare six different vari-ants of the Morfessor Baseline algorithm:?
Unsupervised: The classic, unsupervisedMorfessor baseline.?
Unsupervised + weighting: A held-out setis used for adjusting the weight of the likeli-hood ?.
When ?
= 1 the method is equiva-lent to the unsupervised baseline.
The maineffect of adjusting ?
is to control how manysegments per word the algorithm prefers.Higher ?
leads to fewer and lower ?
to moresegments per word.?
Supervised: The semi-supervised methodtrained with only the labeled data.?
Supervised + weighting: As above, but theweight of the likelihood ?
is optimized onthe held-out set.
The weight can only af-fect which segmentations are selected fromthe possible alternative segmentations in thelabeled data.?
Semi-supervised: The semi-supervisedmethod trained with both labeled andunlabeled data.?
Semi-supervised + weighting: As above,but the parameters ?
and ?
are optimized us-ing the the held-out set.All variations are evaluated using the linguisticgold standard evaluation of Morpho Challenge2009.
For supervised and semi-supervised meth-ods, the amount of labeled data is varied be-tween 100 and 10 000 words, whereas the held-out set has 500 gold standard analyzes.
To obtainprecision-recall curves, we calculated weightedF0.5 and F2 scores in addition to the normal F1score.
The parameters ?
and ?
were optimizedalso for those.3.1 Data and evaluationWe used the English and Finnish data sets fromCompetition 1 of Morpho Challenge 2009 (Ku-rimo et al, 2009).
Both are extracted from athree million sentence corpora.
For English, therewere 62 185 728 word tokens and 384 903 wordtypes.
For Finnish, there were 36 207 308 wordtokens and 2 206 719 word types.
The complexityof Finnish morphology is indicated by the almostten times larger number of word types than in En-glish, while the number of word tokens is smaller.We applied also the evaluation method of theMorpho Challenge 2009: The results of the mor-phological segmentation were compared to a lin-guistic gold standard analysis.
Precision measureswhether the words that share morphemes in theproposed analysis have common morphemes alsoin the gold standard, and recall measures the op-posite.
The final score to optimize was F-measure,i.e, the harmonic mean of the precision and re-call.2 In addition to the unweighted F1 score, wehave applied F2 and F0.5 scores, which give moreweight to recall and precision, respectively.Finnish gold standards are based on FINT-WOL morphological analyzer from Lingsoft, Inc.,that applies the two-level model by Koskenniemi(1983).
English gold standards are from theCELEX English database.
The final test sets arethe same as in Morpho Challenge, based on 10 000English word forms and 200 000 Finnish wordforms.
The test sets are divided into ten parts forcalculating deviations and statistical significances.For parameter tuning, we applied a small held-outset containing 500 word forms that were not in-cluded in the test set.For supervised and semi-supervised training,we created sets of five different sizes: 100, 300,1 000, 3 000, and 10 000.
They did not contain anyof the word forms in the final test set, but wereotherwise randomly selected from the words for2Both the data sets and evaluation scripts are availablefrom the Morpho Challenge 2009 web page: http://www.cis.hut.fi/morphochallenge2009/82Figure 1: The F-measure for English as a functionof the number of labeled training samples.which the gold standard analyses were available.In order to use them for training Morfessor, themorpheme analyses were converted to segmenta-tions using the Hutmegs package by Creutz andLinde?n (2004).3.2 ResultsFigure 1 shows a comparison of the unsupervised,supervised and semi-supervised Morfessor Base-line for English.
It can be seen that optimiz-ing the likelihood weight ?
alone does not im-prove much over the unsupervised case, imply-ing that the Morfessor Baseline is well suited forEnglish morphology.
Without weighting of thelikelihood function, semi-supervised training im-proves the results somewhat, but it outperformsweighted unsupervised model only barely.
Withweighting, however, semi-supervised training im-proves the results significantly already for only100 labeled training samples.
For comparison,in Morpho Challenges (Kurimo et al, 2009), theunsupervised Morfessor Baseline and MorfessorCategories-MAP by Creutz and Lagus (2007) haveachieved F-measures of 59.84% and 50.50%, re-spectively, and the all time best unsupervised re-sult by a method that does not provide alternativeanalyses for words is 66.24%, obtained by Bern-hard (2008).3 This best unsupervised result is sur-passed by the semi-supervised algorithm at 1000labeled samples.As shown in Figure 1, the supervised methodobtains inconsistent scores for English with the3Better results (68.71%) have been achieved by Monsonet al (2008), but as they were obtained by combining oftwo systems as alternative analyses, the comparison is not asmeaningful.Figure 2: The F-measure for Finnish as a functionof the number of labeled training samples.
Thesemi-supervised and unsupervised lines overlap.smallest training data sizes.
The supervised al-gorithm only knows the morphs in the trainingset, and therefore is crucially dependent on theViterbi segmentation algorithm for analyzing newdata.
Thus, overfitting to some small data sets isnot surprising.
At 10 000 labeled training samplesit clearly outperforms the unsupervised algorithm.The improvement obtained from tuning the weight?
in the supervised case is small.Figure 2 shows the corresponding results forFinnish.
The optimization of the likelihood weightgives a large improvement to the F-measure al-ready in the unsupervised case.
This is mainly be-cause the standard unsupervised Morfessor Base-line method does not, on average, segment wordsinto as many segments as would be appropriate forFinnish.
Without weighting, the semi-supervisedmethod does not improve over the unsupervisedone: The unlabeled training data is so much largerthat the labeled data has no real effect.For Finnish, the unsupervised Morfessor Base-line and Categories-MAP obtain F-measures of26.75% and 44.61%, respectively (Kurimo et al,2009).
The all time best for an unsupervisedmethod is 52.45% by Bernhard (2008).
With op-timized likelihood weights, the semi-supervisedMorfessor Baseline achieves higher F-measureswith only 100 labeled training samples.
Fur-thermore, the largest improvement for the semi-supervised method is achieved already from 1000labeled training samples.
Unlike English, the su-pervised method is quite a lot worse than the un-supervised one for small training data.
This isnatural because of the more complex morphology83Figure 3: Precision-recall graph for English withvarying amount of labeled training data.
Parame-ters ?
and ?
have been optimized for three differ-ent measures: F0.5, F1 and F2 on the held-out set.Precision and recall values are from the final testset, error bars indicate one standard deviation.in Finnish; good results are not achieved just byknowing the few most common suffixes.Figures 3 and 4 show precision-recall graphsof the performance of the semi-supervised methodfor English and Finnish.
The parameters ?
and ?have been optimized for three differently weightedF-measures (F0.5, F1, and F2) on the held-out set.The weight tells how much recall is emphasized;F1 is the symmetric F-measure that emphasizesprecision and recall alike.
The graphs show thatthe more there are labeled training data, the moreconstrained the model parameters are: With manylabeled examples, the model cannot be forced toachieve high precision or recall only.
The phe-nomenon is more evident in the Finnish data (Fig-ure 3), where the same amount of words containsmore information (morphemes) than in the En-glish data.
Table 1 shows the F0.5, F1 and F2measures numerically.Table 2 shows the values for the F1-optimalweights ?
and ?
that were chosen for differentamounts of labeled data using the held-out set.
Aseven the largest labeled sets are much smaller thanthe unlabeled training set, it is natural that ?
?
?.The small optimal ?
for Finnish explains why thedifference between unsupervised unweighted andweighted versions in Figure 2 was so large.
Gener-ally, the more there is labeled data, the smaller ?
isneeded.
A possible increase in overall likelihoodcost is compensated by a smaller ?.
Finnish with100 labeled words is an exception; probably a veryFigure 4: Precision-recall graph for Finnish withvarying amount of labeled training data.
Param-eters ?
and ?
have been optimized for three dif-ferent measures: F0.5, F1 and F2 on the held-outset.
Precision and recall values are from the finaltest set, error bars indicate one standard deviation,which here is very small.high ?
would end in overlearning of the small setwords at the cost of overall performance.4 DiscussionThe method developed in this paper is a straight-forward extension of Morfessor Baseline.
In thesemi-supervised setting, it should be possible todevelop a generative model that would not requireany discriminative reweighting, but could learn,e.g., the amount of segmentation from the labeleddata.
Moreover, it would be possible to learn themorpheme labels instead of just the segmentationinto morphs, either within the current model or asa separate step after the segmentation.
We madeinitial experiment with a trivial context-free label-ing: A mapping between the segments and mor-pheme labels was extracted from the labeled train-ing data.
If some label did not have a correspond-ing segment, it was appended to the previous la-bel.
E.g., if the labels for ?found?
are ?find V+PAST?, ?found?
was mapped to both labels.
Af-ter segmentation, each segment in the test data wasreplaced by the most common label or label se-quence whenever such was available.
The resultsusing training data with 1 000 and 10 000 labeledsamples are shown in Table 3.
Although preci-sions decrease somewhat, recalls improve consid-erably, and significant gains in F-measure are ob-tained.
A more advanced, context-sensitive label-ing should perform much better.84Englishlabeled data F0.5 F1 F20 69.16 61.05 62.70100 73.23 65.18 68.30300 72.98 65.63 68.811000 71.86 68.29 69.683000 74.34 69.13 72.0110000 76.04 72.85 73.89Finnishlabeled data F0.5 F1 F20 56.81 49.07 53.95100 58.96 52.66 57.01300 59.33 54.92 57.161000 61.75 56.38 58.243000 63.72 58.21 58.9010000 66.58 60.26 57.24Table 1: The F0.5, F1 and F2 measures for thesemi-supervised + weighting method.English Finnishlabeled data ?
?
?
?0 0.75 - 0.01 -100 0.75 750 0.01 500300 1 500 0.005 50001000 1 500 0.05 25003000 1.75 350 0.1 100010000 1.75 175 0.1 500Table 2: The values for the weights ?
and ?that the semisupervised algorithm chose for differ-ent amounts of labeled data when optimizing F1-measure.The semi-supervised extension could easily beapplied to the other versions and extensions ofMorfessor, such as Morfessor Categories-MAP(Creutz and Lagus, 2007) and Allomorfessor (Vir-pioja and Kohonen, 2009).
Especially the model-ing of allomorphy might benefit from even smallamounts of labeled data, because those allomorphsthat are hardest to find (affixes, stems with irregu-lar orthographic changes) are often more commonthan the easy cases, and thus likely to be foundeven from a small labeled data set.Even without labeling, it will be interestingto see how well the semi-supervised morphologylearning works in applications such as informationretrieval.
Compared to unsupervised learning, weobtained much higher recall for reasonably goodlevels of precision, which should be beneficial tomost applications.Segmented LabeledEnglish, D = 1000Precision 69.72% 69.30%Recall 66.92% 72.21%F-measure 68.29% 70.72%English, D = 10 000Precision 77.35% 77.07%Recall 68.85% 77.78%F-measure 72.86% 77.42%Finnish, D = 1000Precision 61.03% 58.96%Recall 52.38% 66.55%F-measure 56.38% 62.53%Finnish, D = 10 000Precision 69.14% 66.90%Recall 53.40% 74.08%F-measure 60.26% 70.31%Table 3: Results of a simple morph labeling aftersegmentation with semi-supervised Morfessor.5 ConclusionsWe have evaluated an extension of the MorfessorBaseline method to semi-supervised morphologi-cal segmentation.
Even with our simple method,the scores improve far beyond the best unsuper-vised results.
Moreover, already one hundredknown segmentations give significant gain overthe unsupervised method even with the optimizeddata likelihood weight.AcknowledgmentsThis work was funded by Academy of Finland andGraduate School of Language Technology in Fin-land.
We thank Mikko Kurimo and Tiina Lindh-Knuutila for comments on the manuscript, andNokia foundation for financial support.ReferencesDelphine Bernhard.
2006.
Unsupervised morpholog-ical segmentation based on segment predictabilityand word segments alignment.
In Proceedings of thePASCAL Challenge Workshop on Unsupervised seg-mentation of words into morphemes, Venice, Italy.PASCAL European Network of Excellence.Delphine Bernhard.
2008.
Simple morpheme labellingin unsupervised morpheme analysis.
In Advances inMultilingual and Multimodal Information Retrieval,8th Workshop of the CLEF, volume 5152 of Lec-ture Notes in Computer Science, pages 873?880.Springer Berlin / Heidelberg.85Mathias Creutz and Krista Lagus.
2002.
Unsuper-vised discovery of morphemes.
In Proceedings ofthe Workshop on Morphological and PhonologicalLearning of ACL?02, pages 21?30, Philadelphia,Pennsylvania, USA.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using Morfessor 1.0.
TechnicalReport A81, Publications in Computer and Informa-tion Science, Helsinki University of Technology.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1), January.Mathias Creutz and Krister Linde?n.
2004.
Morphemesegmentation gold standards for Finnish and En-glish.
Technical Report A77, Publications in Com-puter and Information Science, Helsinki Universityof Technology.Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo,Antti Puurula, Janne Pylkko?nen, Vesa Siivola, MattiVarjokallio, Ebru Arisoy, Murat Sarac?lar, and An-dreas Stolcke.
2007.
Morph-based speech recog-nition and modeling of out-of-vocabulary wordsacross languages.
ACM Transactions on Speech andLanguage Processing, 5(1):1?29.Sajib Dasgupta and Vincent Ng.
2007.
High-performance, language-independent morphologicalsegmentation.
In the annual conference of the NorthAmerican Chapter of the ACL (NAACL-HLT).Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the em algorithm.
Journal of the Royal Sta-tistical Society, Series B (Methodological), 39(1):1?38.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?189.Kimmo Koskenniemi.
1983.
Two-level morphology: Ageneral computational model for word-form recog-nition and production.
Ph.D. thesis, University ofHelsinki.Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.2008.
Morpho Challenge evaluation using a linguis-tic Gold Standard.
In Advances in Multilingual andMultiModal Information Retrieval, 8th Workshop ofthe Cross-Language Evaluation Forum, CLEF 2007,Budapest, Hungary, September 19-21, 2007, Re-vised Selected Papers, Lecture Notes in ComputerScience , Vol.
5152, pages 864?873.
Springer.Mikko Kurimo, Sami Virpioja, Ville T. Turunen,Graeme W. Blackwood, and William Byrne.
2009.Overview and results of Morpho Challenge 2009.
InWorking Notes for the CLEF 2009 Workshop, Corfu,Greece, September.Wei Li and Andrew McCallum.
2005.
Semi-supervised sequence modeling with syntactic topicmodels.
In AAAI?05: Proceedings of the 20th na-tional conference on Artificial intelligence, pages813?818.
AAAI Press.Christian Monson, Alon Lavie, Jaime Carbonell, andLori Levin.
2004.
Unsupervised induction of natu-ral language morphology inflection classes.
In Pro-ceedings of the Workshop of the ACL Special InterestGroup in Computational Phonology (SIGPHON).Christian Monson, Jaime Carbonell, Alon Lavie, andLori Levin.
2008.
ParaMor: Finding paradigmsacross morphology.
In Advances in Multilingualand MultiModal Information Retrieval, 8th Work-shop of the Cross-Language Evaluation Forum,CLEF 2007, Budapest, Hungary, September 19-21,2007, Revised Selected Papers, Lecture Notes inComputer Science , Vol.
5152.
Springer.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In NAACL ?09: Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 209?217.
Association for Computational Lin-guistics.Jorma Rissanen.
1989.
Stochastic Complexity in Sta-tistical Inquiry, volume 15.
World Scientific Seriesin Computer Science, Singapore.Benjamin Snyder and Regina Barzilay.
2008a.
Cross-lingual propagation for morphological analysis.
InAAAI?08: Proceedings of the 23rd national con-ference on Artificial intelligence, pages 848?854.AAAI Press.Benjamin Snyder and Regina Barzilay.
2008b.
Un-supervised multilingual learning for morphologicalsegmentation.
In Proceedings of ACL-08: HLT,pages 737?745, Columbus, Ohio, June.
Associationfor Computational Linguistics.Sami Virpioja and Oskar Kohonen.
2009.
Unsuper-vised morpheme analysis with Allomorfessor.
InWorking notes for the CLEF 2009 Workshop, Corfu,Greece.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervised chineseword segmentation for statistical machine transla-tion.
In COLING ?08: Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics, pages 1017?1024, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Xiaojin Zhu.
2005.
Semi-supervised Learning withGraphs.
Ph.D. thesis, CMU.
Chapter 11, Semi-supervised learning literature survey (updated onlineversion).86
