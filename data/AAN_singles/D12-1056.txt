Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 609?618, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Unified Approach to Transliteration-based Text Inputwith Online Spelling CorrectionHisami Suzuki              Jianfeng GaoMicrosoft ResearchOne Microsoft Way, Redmond WA 98052 USA{hisamis,jfgao}@microsoft.comAbstractThis paper presents an integrated, end-to-endapproach to online spelling correction for textinput.
Online spelling correction refers to thespelling correction as you type, as opposed topost-editing.
The online scenario isparticularly important for languages thatroutinely use transliteration-based text inputmethods, such as Chinese and Japanese,because the desired target characters cannotbe input at all unless they are in the list ofcandidates provided by an input method, andspelling errors prevent them from appearingin the list.
For example, a user might typesuesheng by mistake to mean xuesheng ??
'student' in Chinese; existing input methodsfail to convert this misspelled input to thedesired target Chinese characters.
In thispaper, we propose a unified approach to theproblem of spelling correction andtransliteration-based character conversionusing an approach inspired by the phrase-based statistical machine translationframework.
At the phrase (substring) level, kmost probable pinyin (Romanized Chinese)corrections are generated using a monotonedecoder; at the sentence level, input pinyinstrings are directly transliterated into targetChinese characters by a decoder using a log-linear model that refer to the features of bothlevels.
A new method of automaticallyderiving parallel training data from userkeystroke logs is also presented.
Experimentson Chinese pinyin conversion show that ourintegrated method reduces the character errorrate by 20% (from 8.9% to 7.12%) over theprevious state-of-the art based on a noisychannel model.1 IntroductionThis paper addresses the problem of onlinespelling correction, which tries to correct users'misspellings as they type, rather than post-editingthem after they have already been input.
Thisonline scenario is particularly important forlanguages that routinely use transliteration-basedtext input methods, including Chinese andJapanese: in these languages, characters (calledhanzi in Chinese and kanji/kana in Japanese) aretypically input by typing how they are pronouncedin Roman alphabet (called pinyin in Chinese,romaji in Japanese), and selecting a conversioncandidate among those that are offered by an inputmethod system, often referred to as IMEs or inputmethod editors.
One big challenge posed byspelling mistakes is that they prevent the desiredcandidates from appearing as conversioncandidates, as in Figure 1: suesheng is likely to bea spelling error of xuesheng??
'student', but it isnot included as one of the candidates.Figure 1: Spelling mistake prevents the desired output(??)
from appearing in the list of candidatesThis severely limits the utility of an IME, asspelling errors are extremely common.
Speakers ofa non-standard dialect and non-native speakershave a particularly hard time, because they maynot know the standard pronunciation of the word tobegin with, preventing them from inputting theword altogether.
Error-tolerant word completionand next word prediction are also highly desirablefeatures for text input on software (onscreen)keyboards for any language, making the currentwork relevant beyond Chinese and Japanese.In this paper, we propose a novel, unifiedsystem of text input with spelling correction, using609Chinese pinyin-to-hanzi conversion as an example.We first formulate the task of pinyin spellingcorrection as a substring-based monotonetranslation problem, inspired by phrase-basedstatistical machine translation (SMT) systems(Koehn et al2003; Och and Ney, 2004): weconsider the pinyin input (potentially with errors)as the source language and the error-correctedpinyin as the target, and build a log-linear modelfor spelling correction.
In doing so, we alsopropose a novel, unsupervised method ofcollecting parallel training data from user inputlogs.
We then build an integrated end-to-end textinput system that directly converts a potentiallyerroneous input pinyin sequence into a desiredhanzi sequence, also formulated as a monotonephrase-based SMT problem, in which the featurefunctions of the substring-based error correctioncomponent are integrated and jointly optimizedwith the sentence-level feature functions forcharacter conversionOur method generalizes and improves over theprevious state-of-the-art methods for the task oferror correction and text input in several crucialrespects.
First, our error correction model isdesigned and implemented as a substring-based,fully trainable system based on a log-linear model,which has been shown effective for related taskssuch as transliteration and letter-to-phoneconversion, but has not been attempted for the taskof spelling correction.
Second, we build an end-to-end pinyin-to-hanzi conversion system bycombining all the feature functions used in theerror correction and character conversioncomponents in an SMT-style log-linear model,where the feature weights are traineddiscriminatively for the end-to-end task.
Thisintegration method generalizes the previousapproach based on a noisy channel model (Chenand Lee, 2000; Zheng et al011b), in which onlythe error model and the conversion modelprobabilities are used and combined with equalweights.
Finally, like other statistical systems, theamount and quality of training data control thequality of the outcome; we thus propose a new,language-independent method of deriving paralleldata for spelling correction from user keystrokelogs.We performed experiments on various methodsof integrating the error correction and characterconversion sub-components.
Our best system, afully integrated SMT-based approach, reduces thecharacter error rate by 35% on test data that iscompletely independent of the creation of errorcorrection and character conversion models.In what follows, we first give the background ofthis research in Section 2.
We then describe ourapproach to the spelling correction task (Section 3)and the end-to-end conversion task (Section 4).
Wesummarize our contribution and conclude withremarks for future directions in Section 5.2 Related WorkThe current work builds on many previous workson the task of monotone substring-basedtransduction, including spelling correction, letter-to-phone conversion and transliteration betweendifferent scripts.
In particular, our substring-basedapproach to spelling correction is motivated by thesuccess on transliteration (e.g., Sherif and Kondrak,2007; Cherry and Suzuki, 2009) and letter-to-phoneme conversion (e.g., Jiampojamarn et al2007; Rama et al2009).
One big challenge of thespelling correction research is the general lack ofnaturally occurring paired data of contextualspelling errors and their correction.
Previous workhas therefore either focused on the task ofcorrecting out-of-vocabulary words out of context(e.g., Brill and Moore, 2000; Toutanova andMoore, 2002), or has resorted to innovativemethods of data collection.
For example, Bankoand Brill (2001) generate data artificially bysubstituting words from a confusion word set intext for building a contextual speller; Whitelaw etal.
(2009) use word frequency and edit distanceinformation to harvest error pairs from a webcorpus in an unsupervised manner; Bertoldi et al(2010) intentionally corrupt clean text by addingnoise to the data.
Another approach to spellingerror data collection uses web search query logs,available in large quantity (albeit to limitedinstitutions), and limit its focus on the task ofcorrecting misspelled queries (e.g., Cucerzan andBrill, 2004; Gao et al2010; Sun et al2010;Duan and Hsu, 2011).
The problem of datacollection is particularly difficult for pinyin errorcorrection, as pinyin is not a final form of text inChinese, so it is not recorded in final text.
Zheng etal.
(2011a) study a log of pinyin input method anduse the backspace key to learn the user mistypingbehavior, but they do so only for the purpose of610data analysis, and do not build a statistical modelfrom this data.Text input methods have been commerciallyavailable for decades for inputting Chinese andJapanese, but have also recently become availablefor other non-Roman script languages includingArabic and the languages of India.1 Early researchwork on text input methods includes e.g., Mori etal.
(1998), Chen and Lee (2000) and Gao et al(2002), all of which approach the problem using anoisy channel model.
Discriminative approacheshave also been proposed, e.g., Suzuki and Gao(2005); Tokunaga et al2011).
There is only avery limited amount of work that deals withspelling correction in the context of text input:Zheng et al2011b) represents a recent workbased on a noisy channel model, which defines ourbaseline.
Their work is strictly word-based andonly handles the correction of out-of-vocabularypinyin words into in-vocabulary pinyin words,while our substring-based model is not limited bythese constraints.The current work also has an affinity to the taskof speech translation in that the parallel databetween the input (speech signal) and the output(text in foreign language) is not directly available,but is mediated by a corrected (transcribed) formof input.
Zhang et al2011) is thus relevant to ourstudy, though their approach differs from ours inthat we build an integrated system that include thefeature functions of both error correction andcharacter conversion sub-systems.3 Substring-based Spelling Correctionusing a Log-linear ModelIn this section, we describe our approach to pinyinerror correction within a log-linear framework.Though our current target is pinyin error correction,the method described in this section is applicableto any language of interest.The spelling correction problem has beenstandardly formulated within the framework ofnoisy channel model (e.g., Kernighan et al1990).Let A be the input phonetic string in pinyin.
Thetask of spelling correction is to search for the best1 A few examples include Google Transliterate(http://www.google.com/transliterate/) and Microsoft Maren(http://www.microsoft.com/middleeast/egypt/cmic/maren/) /ILIT (http://specials.msn.co.in/ilit/Hindi.aspx).
Quillpad(http://quillpad.in/) is also popularly used in India.correction candidate in pinyin C* among allpossible corrections for each potentially misspelledpinyin A:|Applying Bayes' Rule and dropping the constantdenominator, we have|where the error model    |  models thetranslation probability from C to A, and thelanguage model      models how likely theoutput C is a correctly spelled pinyin sequence.Many variations on the error model have beenproposed, including substring-based (Brill andMoore, 2000) and pronunciation-based (Toutanovaand Moore, 2002) models.Our model is inspired by the SMT framework,in which the error correction probability    |   ofEquation (1) is directly modeled using a log-linearmodel of the following form:|?where Z(A) is the normalization factor, hi is afeature function and ?i is the feature weight.Similarly to phrase-based SMT, many featurefunctions are derived from the translation andlanguage models, where the translation model-derived features are trained using a parallel corpusof original pinyin and correction pairs.
The argmaxof Equation (1) defines the search operation: weuse a left-to-right beam search decoder to seek foreach input pinyin the best correction according toEquation (3).We first describe how the paired data forderiving the error model probabilities is generatedfrom user logs in Section 3.1, and then how themodels are trained and the model weights arelearned in Section 3.2.
We discuss the results ofpinyin error correction as an independent task inSection 3.3.3.1 Generating error correction pairs fromkeystroke logsUnlike English text, which includes instances ofmisspelled words explicitly, pinyin spelling errorsare not found in a corpus, because pinyin is used asa means of inputting text, and is not part of the611final written form of the language.
Therefore,pinyin error correction pairs must be createdintentionally.
We chose the method ofimplementing a version of an input method whichrecords the keystrokes of users while they areasked to type a particular Chinese text in hanzi; indoing so, we captured each keystroke issued by theuser behind the scene.
Such keystroke logs includethe use of the backspace key, from which wecompute the pinyin strings after the usage of thebackspace keys as well as the putative pinyin stringhad the user not corrected it using the backspacekey.2 Table 1 shows a few examples of the entriesin the keystroke log, along with the computedpinyin strings before and after correction.
Eachentry (or phrase) in the log represents the unit thatcorresponds to the sequence the user input at once,at the end of which the user committed to aconversion candidate, which typically consists ofone or more words.
While the post-correctionstring can be straightforwardly derived by deletingthe same number of characters preceding thebackspaces, the computation of the pre-correctionstring is trickier and ambiguous, because thebackspace key is used for the purpose of bothdeletion and substitution (delete and replace)operations.
In Table 1, a backspace usage isindicated by _ in the original keystroke sequencethat is logged.
In the second example, a deletioninterpretation will generate zhonguo as a pre-correction string, while substitution interpretationwill generate zhonguoo.
In order to recover thedesired pre-correcting string, we compared theprefix of the backspace usage (zhonguo) with thesubstrings after error correction (zhong, zhongg,zhonggu?).
We considered that the prefix wasspell-corrected into the substring which is thelongest and with the smallest edit distance: in thiscase, zhonguo is considered an error forzhongguo, therefore recovering the pre-correctionstring of the whole sequence as zhonguo.
Notethat this method of error data extraction is general2 Zheng et al2011a) also uses the backspace key in the IMElog to generate error-correction pairs, but they focus on theusage of a backspace after the desired hanzi characters havebeen input, i.e., the backspace key is used to delete one ormore hanzi characters.
In contrast, our method focuses on theuse of backspace to delete one or more pinyin charactersbefore conversion.
This simulates the scenario of online errorcorrection more truthfully, and can collect paired data in largequantity faster.and is language-independent.
Since paired errorcorrection data do not exist naturally and isexpensive to collect for any language, we believethat the proposed method is useful beyond the caseof Chinese text input and applicable to the datacollection of the spelling correction task in general.In a related work (Baba and Suzuki, 2012), wecollected such keystroke data using Amazon'sMechanical Turk for English and Japanese, andreleased the error-correction pairs for researchpurposes.3The extracted pairs are still quite noisy, becauseone error correction behavior might not completelyeliminate the errors in typing a word.
For example,in trying to type women ??
'we', a user mightfirst type wmen, hit the backspaces key four times,retype womeen, and commit to a conversioncandidate by mistake.
We extract the pair (wmen,womeen) from this log incorrectly, which is one ofthe causes of the noise in the data.
Despite theseremaining errors, we use the data without furthercleaning, as we expect our approach to be robustagainst a certain amount of noise.Keystroke data was collected for three textdomains (chat, blog and online forum) from 60users, resulting in 86,783 pairs after removingduplicates.
The data includes the pairs with thesame source and target, with about 41%representing the case of correction.
We used 5,000pairs for testing, 1,000 pairs for tuning the log-linear model weights (see the next subsection), andthe remaining portion for training the errorcorrection component.3.2 Training the log-linear modelThe translation model captures substring-basedspelling error patterns and their transformationprobabilities.
The model is learned from largeamounts of pinyin-correction pairs mined fromuser keystroke logs discussed above.
Take the3 Available at http://research.microsoft.com/en-us/downloads/4eb8d4a0-9c4e-4891-8846-7437d9dbd869/default.aspx.keystroke pre-correctionpost-correctionn a n s _ r e n nansen nanrenz h o n g u o _ _ g u o zhonguo(*zhonguoo)zhongguoTable 1: Computation of pre- and post-correctionstrings from keystroke log612following pinyin-correction pair as an example,where the input pinyin and its correction arealigned at the character level: given a pair (A,C),we align the letters in A with those in C so as tominimize the edit distance between A and C basedon single character insertions, deletions andsubstitutions.From this pair, we learn a set of error patterns thatare consistent with the character alignment,4 eachof which is a pair of substrings indicating how thespelling is transformed from one to another.
Someexamples of extracted phrases are (wanmian,waimian) and (andshi, andeshi).
In ourimplementation, we extract all patterns with asubstring length of up to 9 characters.
We thenlearn the translation probabilities for each pairusing maximum likelihood estimation (MLE).
Let(a,c) denote a pair.
For each pair, we learn thetranslation probabilities P(c|a) and P(a|c),estimated using MLE, as well as lexical weights intwo directions following Koehn et al2003).
Ourerror correction model is completely substring-based and does not use a word-based lexicon,which gives us the flexibility of generating unseencorrection targets as well as supporting pinyininput consisting of multiple words at a time.
Forthe language model, we use a character 9-grammodel to capture the knowledge of correctlyspelled pinyin words and phrases.
We trained thelanguage model using the target portion of theparallel data described in Section 3.1, though it ispossible to train it with an arbitrary text in pinyinwhen such data is available.In addition to the feature functions derived fromthe error and language models, we also use wordand phrase penalties as feature functions, which arecommonly used in SMT.
These features also makesense in the current context, as using fewer phrasemeans encouraging longer ones with more context,and the target character length can capturetendencies to delete or insert words in errors.4 Consistency here implies two things.
First, there must be atleast one aligned character pair in the aligned phrase.
Second,there must not be any alignments from characters inside thealigned phrase to characters outside the phrase.
That is, we donot extract a phrase pair if there is an alignment from withinthe phrase pair to outside the phrase pair.Overall, the log-linear model uses 7 featurefunctions: 4 derived from the translation models,word and phrase penalties, and the language model.The model weights were trained using theminimum error rate training algorithm (MERT,Och, 2003).
We tried MERT with two objectivefunctions: one that uses the 4-gram BLEU score asstraightforwardly adapted from SMT, and the otherthat minimizes the character error rate (CER).
CERis based on the edit distance between the referenceand system output, which is used for evaluating theIME accuracy (Section 4.3).
It is more directlyrelated with the word/phrase-level accuracy, whichwe used to evaluate the error correction module inisolation, than the BLEU metric.
As we will showbelow, however, using different objectivefunctions turned out to have only a minimal impacton the spelling correction accuracy.3.3 Experiments and resultsThe performance of pinyin error correction wasevaluated on two data sets: (1) log-test: the test setof the data in Section 3.1, which is derived in thesame way as the training data but is noisy,consisting of 5,000 phrases of which 2,020 aremisspelled; (2) CHIME: the gold standard from theCHIME data set made available by Zheng et al(2011b), 5  which is also used in the end-to-endevaluation in Section 4.
This data set consists of2,000 sentence pairs of pinyin input with errorsand the target hanzi characters, constructed bycollecting actual user typing logs of the Lancastercorpus (McEnery and Xiao, 2004), which includestext from newspaper, fiction, and essays.
6  TheCHIME data set does not include the correctedpinyin string; we therefore generated this byrunning a text-to-pinyin utility, 7  and created thepairs before and after error correction forevaluating our pinyin spelling correction module.The set contains 11,968 words of which 908 aremisspelled.The results of the evaluation are given in Table2.
They are for phrase/word-level accuracy, as thelog-derived data set is for each phrase (a user-5 Available from http://chime.ics.uci.edu/6 Details on the Lancaster corpus are found athttp://www.lancs.ac.uk/fass/projects/corpus/LCMC/.7 We used an in-house tool, but many tools are availableonline.
Unlike pinyin-to-hanzi, hanzi-to-pinyin is relativelystraightforward as most characters have a uniquepronunciation.613defined unit of conversion, consisting of one to afew words), while the CHIME data set is word-segmented.
The baseline accuracy is the accuracyof not correcting any error, which is very strong inthis task: 59.6% and 92.41% for the two data sets,respectively.
The accuracy on the log-test data isgenerally much lower than the CHIME data,presumably because the latter is cleaner, containsless errors to begin with, and the unit of evaluationis smaller (word) than the log-test (phrase).Though CHIME is an out-of-domain data set, theproposed model works very well on this set,achieving more than 93% accuracy with the bestoutput, significantly (at p<0.001 using McNemar'stest) improving on the strong baseline of notcorrecting any error.
The proposed log-linearapproach is also compared against the noisychannel model baseline, which is simulated byonly using one error model-derived featurefunction    |   and the language model, weightedequally, using the same beam search decoder.Somewhat surprisingly, the noisy channel modelresults fall below the baseline in both data sets,while the log-linear model improves over thebaseline, especially on the 1-best accuracy: alldifferences between the noisy channel model andthe log-linear model outputs are significant.
Finally,regarding the effect of using the CER as theobjective function of MERT, we only observeminimal impact: none of the differences inaccuracy between the BLEU and CER objectives isstatistically significant on either data set.
For amonotone decoding task such as spellingcorrection, using either objective function thereforeseems to suffice, even though BLEU is moreindirect and redundant in capturing the phrase-level accuracy.4 A Unified Model of CharacterConversion with Spelling CorrectionIn this section we describe our unified model ofspelling correction and transliteration-basedcharacter conversion.
Analogous to the spellingcorrection task, the character conversion problemcan also be considered as a substring-basedtranslation problem.
The novelty of our approachlies in the fact that we take advantage of theparallelism between these tasks, and build anintegrated model that performs spelling correctionand character conversion at the same time, withinthe log-linear framework.
This allows us tooptimize the feature weights directly for the endgoal, from which from we can expect a betteroverall conversion accuracy.4.1 Noisy channel model approach toincorporating error correction incharacter conversionThe task of pinyin-to-hanzi conversion consists ofconverting the input phonetic strings provided bythe user into the appropriate word string usingideographic characters.
This has been formulatedwithin the noisy channel model (Chen and Lee,2000), in exactly the same manner as the spellingcorrection, as describe in Equations (1) and (2) inSection 3.
Given the pinyin input A, the task is tofind the best output hanzi sequence W*:||In traditional conversion systems which do notconsider spelling errors, P(A|W) is usually set to 1if the word is found in a dictionary of word-pronunciation pairs, which also defines GEN(A).Therefore, the ranking of the candidates reliesexclusively on the language model probabilityP(W).An extension of this formulation to handlespelling errors can be achieved by incorporating anactual error model P(A|W).
Assuming a conditionalindependence of A and W given the error-correctedpinyin sequence C, Equation (4) can be re-writtenas:1-best 3-best 20-bestlog-test: No correction  59.6log-test: Noisy Channel 49.5 67.86 84.8log-test: Proposed (BLEU) 62.46 74.58 86.66log-test: Proposed (CER) 62.82 75.06 86.8CHIME: No correction 92.41CHIME: Noisy Channel 91.29 95.75 98.82CHIME: Proposed (BLEU) 93.51 97.38 99.06CHIME: Proposed (CER) 93.49 97.29 99.08Table 2: Pinyin error correction accuracy (in %)614|?
|     |?
|         |Here, P(C|W) corresponds to the channel model oftraditional input methods, P(W) the languagemodel, and P(C|A) the pinyin error correctionmodel.
There have been attempts to use thisformulation in text input: for example, Chen andLee (2000) trained a syllable-based model forP(C|A) with user keystroke data,8 and Zheng et al(2011b) used a model based on a weightedcharacter edit distance whose weights are manuallyassigned.
This noisy channel integration of errorcorrection and character conversion is the state-of-the-art in the task of error-correcting text input,and will serve as our baseline.4.2 Log-linear model for error-correctingcharacter conversionSimilar to the formulation of our error correctionmodel in Section 3, we adopt the log-linear modelfor modeling the character conversion probabilityin (4):|?where A = a1,?,an is a sequence of phrases inpinyin, and W = w1,?,wn is the correspondingsequence in hanzi.
A unique challenge of thecurrent task is that the parallel data for A and W donot exist directly.
Therefore, we generated thetranslation phrase table offline by merging the8 No detail of this data is available in Chen and Lee (2000).substring-based phrase table generated for thepinyin error correction task in Section 3 with theresults of character conversion.
This process isdescribed in detail in Figure 2: k-best candidatesfor each input pinyin phrase a are generated by theerror model in Section 3, which are then submittedoffline to an IME system to obtain n-bestconversion candidates with probabilities.
For theIME system, we used an in-house conversionsystem, which only uses a word trigram languagemodel for ranking.
In the resulting translation table,defined for each (a, w) pair, the feature functionsand their values are inherited from the pinyin errorcorrection translation table mediated by thecorrection candidates c1?k for a, plus the functionthat defines the IME conversion probability for (cj,w).
Note that in this final phrase table, thecorrection candidates for a are latent, onlyaffecting the values of the feature functions.9 Thefinal end-to-end system uses the following 11features:- 7 error correction model features at the phraselevel- IME conversion probability at the phrase level- language model probability at the sentence level- word/phrase penalty features at the sentencelevelThe language model at the sentence level is trainedon a large monolingual corpus of Chinese in hanzi,consisting of about 13 million sentences (176million words).
The IME conversion probability9 The final phrase table needs to be unique for each phrase pair(a, w), though the process described here results in multipleentries with the same pair having different feature values,because the generation of (a, w) is mediated by multiplecorrection candidates c1?k.
These entries need to be added upto remove duplicates; we used a heuristic approximation oftaking the pair where a equals cj (i.e., no spelling correction)when multiple entries are found.c1  xuesheng f1 ... f7c2  xueshereng   f1 ... f7c3  xueshusheng  f1 ... f7...+c1  xuesheng    w1 ??
1c2  xueshereng w1 ???
0.103w2 ???
0.101w3 ???
0.101...c3  xueshusheng  w1 ???
0.102w2 ???
0.101w3 ???
0.101......?w11 xueshseng ??
f1 ... f7 1w21 xueshseng ???
f1 ... f7 0.103w22 xueshseng ???
f1 ... f7 0.101w23 xueshseng ???
f1 ... f7 0.101...w31 xueshseng ???
f1 ... f7 0.102w32 xueshseng ???
f1 ... f7 0.101w33 xueshseng ???
f1 ... f7 0.101...k-best error correction candidates c1...kn-best IME conversioncandidates w1...n for c1...kcombined translation table w11...knFigure 2: Generation of integrated translation table for the pinyin input a = xueshseng615also uses a word trigram model, but it is trained ona different data set which we did not have accessto; we therefore used both of these models.
Thevalues for k and n can be determined empirically;we used 20 for both of them.
10  This generatesmaximally 400 conversion candidates for eachinput pinyin.The feature weights of the log-linear model aretuned using MERT.
As running MERT on a CER-based target criterion on the similar, monotonetranslation task of spelling correction did not leadto a significant improvement (Section 3.3), wesimply report the results of using the 4-gramBLEU as the training criterion in this task.4.3 Experiments and resultsFor the evaluation of the end-to-end conversiontask, we used the CHIME corpus mentioned above.In order to use the word trigram language modelthat is built in-house, we re-segmented the CHIMEcorpus using our word-breaker, resulting in 12,102words in 2,000 sentences.
We then divided thesentences in the corpus randomly into two halves,and performed a two-fold cross validationevaluation.
The development portion of the data isused to tune the weights of the feature functions inMERT-style training.
We measured our resultsusing character error rate (CER), which is based onthe longest common subsequence match incharacters between the reference and the bestsystem output.
This is a standard metric used inevaluating IME systems (e.g., Mori et al1998;Gao et al2002).
Let NREF be the number ofcharacters in a reference sentence, NSYS be thecharacter length of a system output, and NLCS bethe length of the longest common subsequencebetween them.
Then the character-level recall isdefined as NLCS/NREF, and the precision as NLCS/NSYS.The CER based on recall and on precision are thendefined as 1 ?
recall and 1 ?
precision, respectively.We report the harmonic mean of these values,similarly to the widely used F1-measure.As our goal is to show the effectiveness of theunified approach, we used simpler methods ofintegrating pinyin error correction with characterconversion to create baselines.
The simplest10 From Table 2, we observe that the accuracy of the 20-bestoutput of the spelling correction component is over 99%.
Anoffline run with the IME system on an independent data setalso showed that the accuracy of the 20-best IME output isover 99%.baseline is a pre-processing approach: we use thepinyin error correction model to convert A into asingle best candidate C, and run an IME system onC.
Another more realistic baseline is the noisychannel integration discussed in Section 4.1.
Weapproximated this integration method by re-ranking all the candidates generated by theproposed log-linear model with only the channeland language model probabilities, equallyweighted.The results are shown Table 3.
5-best results aswell as the 1-best results are shown, because in anIME application, providing the correct candidate inthe candidate list is particularly important even if itis not the best candidate.
Let us first discuss the 1-best results.
The CER of this test corpus using thein-house IME system without correcting any errorsis 10.91.
The oracle CER, which is the result ofapplying the IME on the gold standard pinyin inputderived from the reference text using a hanzi-to-pinyin converter (as mentioned in Section 3.3), is4.08, which is the upper-bound imposed by theIME conversion accuracy.
The simple pipelineapproach of concatenating the pinyin correctioncomponent with the character conversioncomponent improves the CER by 1% to 9.93.Assuming that there are on average 20 words in asentence, and each word consists of 2 characters,1% CER reduction means one improvement every2.5 sentences.
Noisy channel integration improvesover this quite substantially, achieving a CER of7.92, demonstrating the power of the wordlanguage model in character conversion.Incidentally, the CER of the output by Zheng et al(2011b)'s model is 8.90.11 Their results are not asgood as our noisy channel integration, as theirsystem uses a manually defined error model and aword bigram language model.
With the use ofadditional feature functions weighteddiscriminatively for the final conversion task, the11 Available at http://chime.ics.uci.edu/.CER on1-bestCER on5-bestBaseline: No correction 10.91 7.76Baseline: Pre-processing 9.93 6.75Baseline: Zheng et al2011b) 8.90Baseline: Noisy channel 7.92 3.93Proposed: SMT model 7.12 3.63Oracle 4.08 1.51Table 3: CER results for the conversion task (%)616proposed method outperforms all these baselines toreduce the CER to 7.12, a 35% relative error ratereduction compared with the no correction baseline,a 20% reduction against Zheng et al011b) and a10% reduction from our noisy channel baseline.The 5-best results follow the same trend of steadyimprovement as we use a more integrated system.In order to understand the characteristics of theerrors and remaining issues, we ran an erroranalysis on the 1-best results of the proposedsystem.
For each word in the test data (all 2,000sentences) for which the system output had anerror, we classified the reasons of failure into oneof the four categories: (1) character conversionerror: correct pinyin was input to the IME but theconversion failed; (2) over-correction of pinyininput: the system corrected the pinyin input whenit should not have; (3) under-correction of pinyininput: the system did not correct an error in theinput pinyin when it should have; (4) wrongcorrection: input pinyin string had a spelling errorbut it was corrected incorrectly.Table 4 shows the results of the error analysis.We find that somewhat contrary to our expectation,over-correction of the spelling mistakes was not aconspicuous problem, even though the pinyincorrection rate of the training data is much higherthan that of the test data.
We therefore concludethat the error correction model adapts very well tothe characteristics of the test data in our integratedSMT-based approach, which trains the unifiedfeature weights to optimize the end goal.5 Conclusion and Future WorkIn this paper we have presented a unified approachto error-tolerant text input, inspired by the phrase-based SMT framework, and demonstrated itseffectiveness over the traditional method based onthe noisy channel model.
We have also presented anew method of automatically collecting paralleldata for spelling correction from user keystrokelogs, and showed that the log-linear model workswell on the task of spelling correction in isolationas well.In this study, we isolated the problem of spellingerrors and studied the effectiveness of errorcorrection over a basic IME system that does notinclude advanced features such as abbreviatedinput (e.g., typing only "py" for ??
pengyou'friend' or ??
pinyin in Chinese) and auto-completion (e.g., typing only "ari" for ????
?arigatou 'thank you' in Japanese).
Integrating data-driven error correction feature with these advancedfeatures for the benefit of users is the challenge weface in the next step.AcknowledgementsWe are indebted to many colleagues at Microsoftand MSR for their help in conducting this research,particularly to Xi Chen, Pallavi Choudhury, ChrisQuirk, Mei-Yuh Hwang and Kristina Toutanova.We are also grateful for the comments we receivedfrom the reviewers of this paper.ReferencesBaba, Y. and H. Suzuki.
2012.
How are spelling errorsgenerated and corrected?
A study of corrected anduncorrected spelling errors using keystroke logs.
InProceedings of ACL.Banko, M. and E. Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
InProceedings of ACL.Bertoldi, N., M. Cettolo, and M. Federico.
2010.Statistical machine translation of texts withmisspelled words.
In Proceedings of HLT-NAACL.Brill, E., and R. C. Moore.
2000.
An improved errormodel for noisy channel spelling correction.
InProceedings of ACL.Chen, Z., and K. F. Lee.
2000.
A new statisticalapproach to Chinese Pinyin input.
In Proceedings ofACL.Cherry, C., and H. Suzuki.
2009.
Discriminativesubstring decoding for transliteration.
In Proceedingsof EMNLP.Cucerzan, S., and E. Brill.
2004.
Spelling correction asan iterative process that exploits the collectiveknowledge of web users.
In Proceedings of EMNLP.Duan, H., and P. Hsu.
2011.
Online spelling correctionfor query completion.
In Proceedings of WWW.Gao, J., J. Goodman, M. Li and K.-F. Lee.
2002.Toward a unified approach to statistical languagemodeling for Chinese.
In ACM Transactions onOverall errors (words) 1,074 / 12,102Conversion 646 (60.14%)Over-corrections 155 (14.43%)Under-correction 161 (14.99%)Wrong correction 112 (10.42%)Table 4: Classification of errors617Asian Language Information Processing, Vol.
1, No.1, pp 3-33.Gao, J., X. Li, D. Micol, C. Quirk and X.
Sun.
2010.
Alarge scale ranker-based system for search queryspelling correction.
In Proceedings of COLING.Jiampojamarn, S., G. Kondrak and T. Sherif, 2007.Applying many-to-many alignments and hiddenmarkov models to letter-to-phoneme conversion.
InProceedings of HLT/NAACL.Kernighan, M., K. Church, and W. Gale.
1990.
Aspelling correction program based on a noisy channelmodel.
In Proceedings of COLING.Koehn, P., F. Och and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of HLT-NAACL.McEnery, A. and Xiao, Z.
2004.
The Lancaster Corpusof Mandarin Chinese: A Corpus for Monolingual andContrastive Language Study.
In Proceedings ofLREC.Mori, S., M. Tsuchiya, O. Yamaji and M. Nagao.
1998.Kana-kanji conversion by a stochastic model.
InProceedings of Information Processing Society ofJapan, SIG-NL-125-10 (in Japanese).Och, F. J.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL.Och, F., and Ney, H. 2004.
The alignment templateapproach to statistical machine translation.Computational Linguistics, 30(4): 417-449.Rama, T., A. K. Singh and S. Kolachina.
2009.Modeling letter-to-phoneme conversion as a phrasebased statistical machine translation problem withminimum error rate training.
In Proceedings of theNAACL HLT Student Research Workshop andDoctoral Consortium.Sherif, T. and G. Kondrak.
2007.
Substring-basedtransliteration.
In Proceedings of ACL.Sun, X., J. Gao, D. Micol and C. Quirk.
2010.
Learningphrase-based spelling error models from clickthroughdata.
In Proceedings of ACL.Suzuki, H. and J. Gao.
2005.
A comparative study onlanguage model adaptation using new evaluationmetrics.
In Proceedings of EMNLP.Toutanova, K., and R. C. Moore.
2002.
Pronunciationmodeling for improved spelling correction.
InProceedings of ACL.Tokunaga, H., D. Okanohara and S. Mori.
2011.Discriminative method for Japanese kana-kanji inputmethod.
In Proceedings of the Workshop onAdvances in Text Input Methods (WTIM 2011).Whitelaw, C., B. Hutchinson, G. Y. Chung, and G.Ellis.
2009.
Using the web for language independentspellchecking and autocorrection.
In Proceedings ofACL.Zhang, Y., L. Deng, X.
He and A. Acero.
2011.
A noveldecision function and the associated decision-feedback learning for speech translation.
InProceedings of ICASSP.Zheng, Y., L. Xie, Z. Liu, M. Sun.
Y. Zhang and L. Ru.2011a.
Why press backspace?
Understanding userinput behaviors in Chinese pinyin input method.
InProceedings of ACL.Zheng, Y., C. Li and M. Sun.
2011b.
CHIME: Anefficient error-tolerant Chinese pinyin input method.In Proceedings of IJCAI.618
