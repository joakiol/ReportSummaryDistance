Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
486?495, Prague, June 2007. c?2007 Association for Computational LinguisticsAn Approach to Text Corpus Construction which Cuts Annotation Costsand Maintains Reusability of Annotated DataKatrin Tomanek Joachim Wermter Udo HahnJena University Language & Information Engineering (JULIE) LabFu?rstengraben 30D-07743 Jena, Germany{tomanek|wermter|hahn}@coling-uni-jena.deAbstractWe consider the impact Active Learning(AL) has on effective and efficient text cor-pus annotation, and report on reduction ratesfor annotation efforts ranging up until 72%.We also address the issue whether a corpusannotated by means of AL ?
using a particu-lar classifier and a particular feature set ?
canbe re-used to train classifiers different fromthe ones employed by AL, supplying alter-native feature sets as well.
We, finally, reporton our experience with the AL paradigm un-der real-world conditions, i.e., the annota-tion of large-scale document corpora for thelife sciences.1 IntroductionThe annotation of corpora has become a crucial pre-requisite for NLP utilities which rely on (semi-) su-pervised machine learning (ML) techniques.
Whilestability, by and large, has been reached for tagsetsup until the syntax layer, semantic annotations interms of (named) entities, semantic roles, proposi-tions, events, etc.
reveal a high degree of variabilitydue to the inherent domain-dependence of the under-lying tagsets.
This diversity fuels a continuous needfor creating semantic annotation data anew.Accordingly, annotation activities will persist andeven increase in number as HLT is expanding onvarious technical and scientific domains (e.g., thelife sciences) outside the classical general-languagenewspaper genre.
Since the provision of annota-tions is a costly, labor-intensive and error-prone pro-cess the amount of work and time this activity re-quires should be minimized to the extent that corpusdata could still be used to effectively train ML-basedNLP components on them.
The approach we ad-vocate does exactly this and yields reduction gains(compared with standard procedures) ranging be-tween 48% to 72%, without seriously sacrificing an-notation quality.Various techniques to minimize the necessaryamount of annotated training material have al-ready been investigated.
In co-training (Blum andMitchell, 1998), e.g., from a small initial set of la-beled data multiple learners mutually provide newtraining material for each other by labeling unseenexamples.
Pierce and Cardie (2001) have shown,however, that for tasks which require large numbersof labeled examples ?
such as most NLP tasks ?
co-training might be inadequate because it tends to gen-erate noisy data.
Furthermore, a well compiled ini-tial training set is a crucial prerequisite for success-ful co-training.
As another alternative for minimiz-ing annotation work, active learning (AL) is basedon the idea to let the learner have control over the ex-amples to be manually labeled so as to optimize theprediction accuracy.
Accordingly, AL aims at select-ing those examples with high utility for the model.AL (as well as semi-supervised methods) is typi-cally considered as a learning protocol, i.e., to traina particular classifier.
In contrast, we here proposeto employ AL as a corpus annotation method.
Acorpus built on these premises must, however, stillbe reusable in a flexible way so that, e.g., train-ing with modified or improved classifiers is feasibleand reasonable on AL-generated corpora.
Baldridgeand Osborne (2004) have already argued that this isa highly critical requirement because the examplesselected by AL are tuned to one particular classi-fier.
The second major contribution of this paper ad-486dresses this issue and provides empirical evidencethat corpora built with one type of classifier (basedon Maximum Entropy) can reasonably be reused byanother, methodologically related type of classifier(based on Conditional Random Fields) without re-quiring changes of the corpus data.
We also showthat feature sets being used for training classifierscan be enhanced without invalidating corpus annota-tions generated on the basis of AL and, hence, witha poorer feature set.2 Related WorkThere are mainly two methodological strands ofAL research, viz.
optimization approaches whichaim at selecting those examples that optimize some(algorithm-dependent) objective function, such asprediction variance (Cohn et al, 1996), and heuris-tic methods with uncertainty sampling (Lewis andCatlett, 1994) and query-by-committee (QBC) (Se-ung et al, 1992) just to name the most prominentones.
AL has already been applied to several NLPtasks, such as document classification (Schohn andCohn, 2000), POS tagging (Engelson and Dagan,1996), chunking (Ngai and Yarowsky, 2000), statis-tical parsing (Thompson et al, 1999; Hwa, 2000),and information extraction (Lewis and Catlett, 1994;Thompson et al, 1999).In a more recent study, Shen et al (2004) considerAL for entity recognition based on Support VectorMachines.
Here, the informativeness of an exam-ple is estimated by the distance to the hyperplane ofthe currently learned SVM.
It is assumed that an ex-ample which lies close to the hyperplane has highchances to have an effect on training.
This approachis essentially limited to the SVM learning scheme asit solely relies on SVM-internal selection criteria.Hachey et al (2005) propose a committee-basedAL approach where the committee?s classifiers con-stitute multiple views on the data by employing dif-ferent feature subsets.
The authors focus on (pos-sible) negative side effects of AL on the annota-tions.
They argue that AL annotations are cogni-tively more difficult to deal with for the annota-tors (because of the increased complexity of the se-lected sentences).
Hence, lower annotation qualityand higher per-sentence annotation times might be aconcern.There are controversial findings on the reusabil-ity of data annotated by means of AL for the prob-lem of parse tree selection.
Whereas Hwa (2001) re-ports positive results, Baldridge and Osborne (2004)argue that AL based on uncertainty sampling mayface serious performance degradation when labeleddata is reused for training a classifier different fromthe one employed during AL.
For committee-basedAL, however, there is a lack of work on reusabil-ity.
Our experiments of committee-based AL for en-tity recognition, however, reveal that for this task atleast, reusability can be guaranteed to a very largeextent.3 AL for Corpus Annotation -Requirements for Practical UseAL frameworks for real-world corpus annotationshould meet the following requirements:fast selection time cycles ?
AL-based corpus an-notation is an interactive process in which bsentences are selected by the AL engine for hu-man annotation.
Once the annotated data issupplied, the AL engine retrains its underly-ing classifier(s) on all available annotations andthen re-classifies all unseen corpus items.
Afterthat the most informative (i.e., deviant) b sen-tences from the set of newly classified data areselected for the next iteration round.
In this ap-proach the time needed to select the next exam-ples (which is the idle time of the human an-notators) has to be kept at an acceptable limitof a few minutes only.
There are various ALstrategies which ?
although they yield theoreti-cally near-optimal sample selection ?
turn outto be actually impractible for real-world usebecause of excessively high computation times(cf.
Cohn et al (1996)).
Thus, AL-based an-notation should be based on a computationallytractable and task-wise feasible and acceptableselection strategy (even if this might imply asuboptimal reduction of annotation costs).reusability ?
The examples AL selects for man-ual annotation are dependent on the model be-ing used, up to a certain extent (Baldridge andOsborne, 2004).
During annotation time, how-ever, the best model might not be known and487model tuning (especially the choice of features)is typically performed once a training corpusis available.
Hence, from a practical point ofview, the resulting corpus should be reusablewith modified classifiers as well.adaptive stopping criterion ?
An explicit andadaptive stopping criterion which is sensitivetowards the already achieved level of quality ofthe annotated corpus is clearly preferred overstopping after an a priori fixed number of an-notation iterations.If these requirements, especially the first and thesecond one, cannot be guaranteed for a specific an-notation task one should refrain from using AL.
Theefficiency of AL-driven annotation (in terms of thetime needed to compile high quality training mate-rial) might be worse compared to the annotation ofrandomly (or subjectively) selected examples.4 Framework for AL-based Named EntityAnnotationFor named entity recognition (NER), each changeof the application domain requires a more or lessprofound change of the types of semantic categories(tags) being used for corpus annotation.
Hence, onemay encounter a lack of training material for variousrelevant (sub)domains.
Once this data is available,however, one might want to modify the features ofthe final classifier with respect to the specific entitytypes.
Thus, a corpus annotated by means of AL hasto provide the flexibility to modify the features ofthe final classifier.To meet the requirements from above under theconstraints of a real-world annotation task, wedecided for QBC-based AL, a heuristic AL ap-proach, which is computationally less complex andresource-greedy than objective function AL meth-ods (the latter explicitly quantify the differences be-tween the current and an ideal classifier in termsof some objective function).
Accordingly, we ruledout uncertainty sampling, another heuristic AL ap-proach, because it was shown before that QBC ismore efficient and robust (Freund et al, 1997).QBC is based on the idea to select those examplesfor manual annotation on which a committee of clas-sifiers disagree most in their predictions (Engelsonand Dagan, 1996).
A committee consists of a num-ber of k classifiers of the same type (same learningalgorithm, parameters, and features) but trained ondifferent subsets of the training data.
QBC-basedAL is also iterative.
In each AL round the com-mittee?s k classifiers are trained on the already an-notated data C, then a pool of unannotated data Pis predicted with each classifier resulting in n au-tomatically labeled versions of P .
These are thencompared according to their labels.
Those with thehighest variance are selected for manual annotation.4.1 Selection StrategyIn each iteration, a batch of b examples is selectedfor manual annotation.
The informativeness of anexample is estimated in terms of the disagreement,i.e., the uncertainty among the committee?s classi-fiers on classifying a particular example.
This ismeasured by the vote entropy (Engelson and Dagan,1996), i.e., the entropy of the distribution of classi-fications assigned to an example by the classifiers.Vote entropy is defined on the token level t as:Dtok(t) := ?1log k?liV (li, t)k logV (li, t)kwhere V (li,t)k is the ratio of k classifiers where thelabel li is assigned to a token t. As (named) en-tities often span more than a single text token weconsider complete sentences as a reasonable exam-ple size unit1 for AL and calculate the disagreementof a sentence Dsent as the mean vote entropy ofits single tokens.
Since the vote entropy is mini-mal when all classifiers agree in their vote, sentenceswith high disagreement are preferred for manual an-notation.
With informed decisions of human anno-tators made available, the potential for future dis-agreement of the classifier committee on conflictinginstances should decrease.
Thus, each AL iterationselects the b sentences with the highest disagreementto focus on the most controversial decision prob-lems.Besides informativeness, additional criteria canbe envisaged for the selection of examples, e.g., di-1Sentence-level examples are but one conceivable grain size?
lower grains (such as clauses or phrases) as well as highergrains (e.g., paragraphs or abstracts) are equally possible, withdifferent implications for the AL process.488feature class descriptionorthographical based on regular expressions (e.g.
Has-Dash, IsGreek, ...), token transforma-tion rule: capital letters replaced by ?A?,lowercase letters by ?a?, digits by ?0?,etc.
(e.g., IL2 ?
AA0, have ?
aaaa)lexical andmorphologicalprefix and suffix of length 3, stemmedversion of each tokensyntactic the token?s part-of-speech tagcontextual features of neighboring tokensTable 1: Features used for ALversity of a batch and representativeness of the re-spective example (to avoid outliers) (Shen et al,2004).
We experimented with these more sophis-ticated selection strategies but preliminary experi-ments did not reveal any significant improvement ofthe AL performance.
Engelson and Dagan (1996)confirm this observation that, in general, different(and even more refined) selection methods still yieldsimilar results.
Moreover, strategies incorporatingmore selection criteria often require more parame-ters to be set.
However, proper parametrization ishard to achieve in real-world applications.
Usingdisagreement exclusively for selection requires onlyone parameter, viz.
the batch size b, to be specified.4.2 Classifier and FeaturesFor our AL framework we decided to employ a Max-imum Entropy (ME) classifier (Berger et al, 1996).We employ a rich set of features (see Table 1) whichare general enough to be used in most (sub)domainsfor entity recognition.
We intentionally avoided us-ing features such as semantic triggers or externaldictionary look-ups because they depend a lot onthe specific subdomain and entity types being used.However, one might add them to fin- tune the finalclassifier, if needed.
ME classifiers outperform theirgenerative counterparts (e.g., Na?
?ve Bayesian clas-sifiers) because they can easily handle overlapping,probably dependent features which might be con-tained in rich feature sets.
We also favored an MEclassifier over an SVM one because the latter is com-putationally much more complex on rich feature setsand multiple classes and is thus not so well suited foran interactive process like AL.It has been shown that Conditional RandomFields (CRF) (Lafferty et al, 2001) achieve higherperformance on many NLP tasks, such as NER, buton the other hand they are computionally more com-plex than an ME classifier making them also im-practical for the interactive AL process.
Thus, inour committee we employ ME classifiers to meet re-quirement 1 (fast selection time cycles).
However,in the end we want to use the annotated corpora totrain a CRF and will thus examine the reusabilityof such an ME-annotated AL corpus for CRFs (cf.Subsection 5.2).4.3 Stopping CriterionA question hardly addressed up until now is when toactually terminate the AL process.
Usually, it getsstopped when the supervized learning performanceof the specific classifier is achieved.
The problemwith such an approach is, however, that in prac-tice one does not know the performance level whichcould possibly be achieved on an unannotated cor-pus.An apparent way to monitor the progress of theannotation process is to periodically (e.g., after eachAL iteration) train a classifier on the data annotatedso far and evaluate it against some randomly se-lected gold standard.
When the relative performancegrowth of each AL iteration falls below a certainthreshold this might be a good reason to stop the an-notation.
Though this is probably the most reliableway, it is impractical for many scenarios since as-sembling and manually annotating a representativegold standard may already be quite a laborious task.Thus, a measure from which we can predict the de-velopment of the learning curve would be beneficial.One way to achieve this goal is to monitor the rateof disagreement among the different classifiers aftereach iteration.
This rate will descend as the classi-fiers get more and more robust in their predictionson unseen data.
Thus, an average disagreement ap-proaching zero can be interpreted as an indicationthat additional annotations will not render any fur-ther improvement.
In our experiments, we will showthat this is a valid stopping criterion, indeed.5 Experiments and ResultsFor our experiments, we specified the followingthree parameters: the batch size b (i.e., the num-ber of sentences to be selected for each AL itera-tion), the size and composition of the initial train-489ing set, and the number of k classifiers in a com-mittee.
The smaller the batch size, the higher theAL performance turns out to be.
In the special caseof batch size of b = 1 only that example with thehighest disagreement is selected.
This is certainlyimpractical since after each AL iteration a new com-mittee of classifiers has to be trained causing unwar-ranted annotation idle time.
We found b = 20 tobe a good compromise between the annotators?
idletime and AL performance.
The initial training setalso contains 20 sentences which are randomly se-lected though.
Our committee consists of k = 3classifiers, which is a good trade-off between com-putational complexity and diversity.
Although theAL iterations were performed on the sentence level,we report on the number of annotated tokens.
Sincesentences may considerably vary in their length thenumber of tokens constitutes a better measure for an-notation costs.We ran our experiments on two common entity-annotated corpora from two different domains (seeTable 2).
From the general-language newspaper do-main, we used the English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meul-der, 2003).
It consists of a collection of newswirearticles from the Reuters Corpus,2 which comesannotated with three entity types: persons, loca-tions, and organizations.
From the sublanguagebiology domain we used the oncology part of thePENNBIOIE corpus which consists of some 1150PubMed abstracts.
Originally, this corpus containsgene, variation event, and malignancy entity annota-tions.
Manual annotation after each AL round wassimulated by moving the selected sentences fromthe pool of unannotated sentences P to the train-ing corpus T .
For our simulations, we built twosubcorpora by filtering out entity annotations: thePENNBIOIE gene corpus (PBgene), including thethree gene entity subtypes generic, protein, and rna,and the PENNBIOIE variation events corpus (PB-var) corpus including the variation entity subtypestype, event, location, state-altered, state-generic,and state-original.
We split all three corpora intotwo subsets, viz.
AL simulation data and gold stan-dard data on which we evaluate3 a classifier in terms2http://trec.nist.gov/3We use a strict evaluation criterion which only counts exactmatches as true positives because annotations having incorrectcorpus data set sentences tokensCONLL AL 14,040 203,6173 entities Gold 3,453 46,435PBGENE AL 10,050 249,4903 entities Gold 1,114 27,563PBVAR AL 10,050 249,4906 entities Gold 1,114 27,563Table 2: Corpora used in the Experimentsof f-score trained on the annotated corpus after eachAL iteration (learning curve).
As far as the CoNLLcorpus is concerned, we have used CoNLL?s trainingset for AL and CoNLL?s test set as gold standard.
Asfor PBgene and PBvar, we randomly split the cor-pora into 90% for AL and 10% as gold standard.In the following experiments we will refer to theclassifiers used in the AL committee as selectors,and the classifier used for evaluation as the tester.5.1 Efficiency of AL and the Applicability ofthe Stopping CriterionIn a first series of experiments, we evaluated whetherAL-based annotations can significantly reduce thehuman effort compared to the standard annotationprocedure where sentences are selected randomly(or subjectively).
We also show that disagreementis an accurate stopping criterion.
As described inSection 4.2, we here employed a committee of MEclassifiers for AL; a CRF was used as tester for boththe AL and the random selection.
Figures 1, 2, and 3depict the learning curves for AL selection and ran-dom selection (upper two curves) and the respectivedisagreement curves (lower curve).
The random se-lection curves contained in these plots are averagedover three random selection runs.With AL, we get a maximum f-score of ?
84.5%on the CoNLL corpus after about 118,000 tokens.
Atabout the same number of tokens the disagreementcurve drops down to values of around Dsent = 0.Comparing AL and random selection, an f-score of?
84% is reached after 86,000 and 165,000 tokens,respectively, which means a reduction of annotationcosts of about 48%.
On PBgene, the effect of AL iscomparable: a maximum value of 83.5% f-score isreached first after about 124,000 tokens, a data pointwhere hardly any disagreement between the com-mittee?s classifiers occurs.
For, e.g., an f-score ofboundaries are insufficient for manual corpus annotation.490Figure 1: CoNLL Corpus: Learning/Disagreement CurvesFigure 2: PBgene Corpus: Learning/Disagreement CurvesFigure 3: PBvar Corpus: Learning/Disagreement Curvescorpus selection F tokens reductionCONLL random 84.0 165,000AL 84.0 86,000 ?
48%PBGENE random 83.0 101,000AL 83.0 213,000 ?
53%PBVAR random 80.0 56,000AL 80.0 200,000 ?
72%Table 3: Reduction of Annotation Costs Achievedwith AL-based Annotation83%, the annotation effort can be reduced by about53% using AL.
On PBvar, an f-score of about 80%is reached after ?
56,000 tokens when using AL se-lection, while 200,000 tokens are needed with ran-dom selection.
For this task, AL reduces the an-notation effort by of 72%.
Here, the disagreementcurve approaches values of zero after approximately80,000 tokens.
At about this point the learning curvereaches its maximum of about 81% f-score.
Ta-ble 3 summarizes the reduction of annotation costsachieved on all three corpora.Comparing both PENNBIOIE simulations, obvi-ously, the reduction of annotation costs through ALis much higher for the variation type entities than forthe gene entities.
We hypothesize this to be mainlydue to incomparable entity densities.
Whereas thegene entities are quite frequent (about 1.3 per sen-tence on average), the variation entities are rathersparse (0.62 per sentence on average) making it anideal playground for AL-based annotation.
Our ex-periments also reveal that disagreement approachingvalues of zero is a valid stopping criterion.
This is,under all circumstances, definitely the point whenAL-based annotation should stop because then allclassifiers of the committee vote consistently.
Anyfurther selection ?
even though AL selection is used?
is then, actually, a random selection.
If, due toreasons whatsoever, further annotations are wanted,a direct switch to random selection is advisable be-cause this is computationally less expensive thanAL-based selection.5.2 ReusabilityTo evaluate whether the proposed AL framework fornamed entity annotation allows for flexible re-useof the annotated data, we performed experimentswhere we varied both the learning algorithms andthe features of the selectors.4910.60.650.70.750.80.850.914012010080604020F-scoreK tokensAL (CRF committee)AL (ME committee)AL (NB committee)random selectionFigure 4: Algorithm Flexibility on PBvar0.60.650.70.750.80.850.914012010080604020F-scoreK tokensAL (CRF committee)AL (ME committee)AL (NB committee)random selectionFigure 5: AlgorithmFflexibility on CoNLLFirst, we analyzed the effect of different proba-bilistic classifiers as selectors on the resulting learn-ing curve of the CRF tester.
Figures 4 and 5 showthe learning curves on our original ME committee,a CRF committee, and also a committee of Na?
?veBayes (NB) classifiers.
It is not surprising that self-reuse (CRF selectors and CRF tester) yields the bestresults.
Switching from CRF selectors to ME selec-tors has almost no negative effect.
Even with a com-mittee of NB selectors (an ML approach which isessentially less well suited for the NER task), AL-based selection is still substantially more efficientthan random selection on both corpora.
This showsthat our approach to use the less complex ME clas-sifiers for the AL selection process has the positiveeffect of fast selection cycle times at almost no costs.This is especially interesting as the performance of0.60.650.70.750.80.850.914012010080604020F-scoreK tokensall featuressub1sub2sub3random selectionFigure 6: Feature Flexibility on PBvar0.60.650.70.750.80.850.914012010080604020F-scoreK tokensall featuressub1sub2sub3random selectionFigure 7: Feature Flexibility on ConLLan ME classifier trained in supervized manner onthe complete corpus is significantly worse (severalpercentage points of f-measure) than a CRF.
Thatmeans, even though an ME classifier is less wellsuited as the final classifier, it works well as a se-lector for CRFs.4Second, we ran experiments on selectors withonly some features and our CRF tester with all fea-tures (cf.
Table 1).
Feature subset 1 (sub1) containsall but the syntactic features.
In the second subset(sub2), also morphological and lexical features aremissing.
The third set (sub3) only contains ortho-graphical features.
We ran an AL simulation for4We have also conducted experiments where we varied thelearning algorithms of the tester (we experimented with NB,ME, MEMM, and CRFs) ?
with comparable results.
In a real-istic scenario, however, on would rather choose a CRF as finaltester over, e.g., a NB.492each feature subset with a committee of CRF se-lectors.5 Figures 6 and 7 show the various learningcurves.
Here we see that a corpus that was producedwith AL on sub1 can easily be re-used by a testerwith little more features.
This is probably the mostrealistic scenario: the core features are kept andonly a few specific features (e.g., POS, a dictionarylook-up, chunk information, etc.)
are added.
Whenadding substantially more features to the tester thanwere available during AL time, the respective learn-ing curves drop down towards the learning curve forrandom selection.
But even with a selector whichhas only orthographical features and a tester withmany more features ?
which is actually quite an ex-treme example and a rather unrealistic scenario fora real-world application ?
AL is more efficient thanrandom selection.
However, the limits of reusabilityare taking shape: on PBvar, the AL selection withsub3 converges with the random selection curve af-ter about 100,000 tokens.5.3 Findings with Real AL AnnotationWe currently perform AL entity mention annotationsfor an information extraction project in the biomedi-cal subdomain of immunogenetics.
For this purpose,we retrieved about 200,000 abstracts (?
2,000,000sentences) as our document pool of unlabeled exam-ples from PUBMED.
By means of random subsam-pling, only about 40,000 sentences are considered ineach round of AL selection.
To regularly monitorclassifier performance, we also perform gold stan-dard (GS) annotations on 250 randomly chosen ab-stracts (?
2,200 sentences).
In all our annotations ofdifferent entity types so far, we found AL learningcurves similar to the ones reported in our simula-tion experiments, with classifier performance level-ling off at around 75% - 85% f-score (depending onthe entity type).Our annotations also reveal that AL is especiallybeneficial when entity mentions are very sparse.Figure 8 shows the cumulated entity density on ALand gold standard annotations of cytokine receptors(specialized proteins for which we annotated six dif-ferent entity subtypes) ?
very sparse entity typeswith less than one entity mention per PUBMED ab-stract on the average.
As can be seen, after 2,0005Here, we employed CRF instead of ME selectors to isolatethe effect of feature re-usability.050010001500200025003000200  400  600  800  1000  1200  1400  1600  1800  2000entitymentionessentencesGS annotationAL annotationFigure 8: Cumulated Entity Density on AL and GSAnnotations of Cytokine Receptorssentences the entity density in our AL corpus is al-most 15 times higher than in our GS corpus.
Such adense corpus may be more appropriate for classifiertraining than a sparse one yielded by random or se-quential annotations, which may just contain lots ofnegative training examples.
We have observed com-parable effects with other entity types, too, and thusconclude that the sparser entity mentions of a spe-cific type are in texts, the more beneficial AL-basedannotation is.
We report on other aspects of AL forreal annotation projects in Tomanek et al (2007).6 Discussion and ConclusionsWe have shown, for the annotation of (named) en-tities, that AL is well-suited to speed up annotationwork under realistic conditions.
In our simulationswe yielded gains (in the number of tokens) up to72%.
We collected evidence that an average dis-agreement approaching zero may serve as an adap-tive stopping criterion for AL-driven annotation andthat a corpus compiled by means of QBC-based ALis to a large extent reusable by modified classifiers.These findings stand in contrast to those suppliedby Baldridge and Osborne (2004) who focused onparse selection.
Their research indicates that AL onselectors with different learning algorithms and fea-ture sets then used by the tester can easily get worsethan random selection.
They conclude that it mightnot be be advisable to employ AL in environmentswhere the final classifier is not very stable.Our evidence leads us to a re-assessment of AL-493based annotations.
First, we employed a committee-based (QBC) while Baldridge and Osborne per-formed uncertainty sampling AL.
Committee-basedapproaches calculate the uncertainty on an exam-ple in a more implicit way, i.e., by the disagree-ment among the committee?s classifiers.
With uncer-tainty sampling, however, the labeling uncertaintyof one classifier is considered directly.
In futurework we will directly compare QBC and uncertaintysampling with respect to data reusability.
Second,whereas Baldridge and Osborne employed AL on ascoring or ranking problem we focused on classifica-tion problems.
Further research is needed to inves-tigate whether the problem class (classification witha fixed and moderate number of classes vs. rankinglarge numbers of possible candidates) is responsiblefor limited data reusability.On the basis of our experiments we stipulate thatthe proposed AL approach might be applicable withcomparable results to a wider range of corpus anno-tation tasks, which otherwise would require substan-tially larger amounts of annotation efforts.AcknowledgementsThis research was funded by the EC within theBOOTStrep project (FP6-028099), and by the Ger-man Ministry of Education and Research within theStemNet project (01DS001A to 1C).ReferencesJason Baldridge and Miles Osborne.
2004.
Active learn-ing and the total cost of annotation.
In Dekang Linand Dekai Wu, editors, EMNLP 2004 ?
Proceedings ofthe 2004 Conference on Empirical Methods in NaturalLanguage Processing, pages 9?16.
Barcelona, Spain,July 25-26, 2004.
Association for Computational Lin-guistics.Adam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71.Avrim Blum and Tom Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.
InCOLT?98 ?
Proceedings of the 11th Annual Confer-ence on Computational Learning Theory, pages 92?100.
Madison, Wisconsin, USA, July 24-26, 1998.New York, NY: ACM Press.David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-dan.
1996.
Active learning with statistical models.Journal of Artifical Intelligence Research, 4:129?145.Sean Engelson and Ido Dagan.
1996.
Minimizing man-ual annotation cost in supervised training from cor-pora.
In ACL?96 ?
Proceedings of the 34th AnnualMeeting of the Association for Computational Linguis-tics, pages 319?326.
University of California at SantaCruz, California, U.S.A., 24-27 June 1996.
San Fran-cisco, CA: Morgan Kaufmann.Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-tali Tishby.
1997.
Selective sampling using the queryby committee algorithm.
Machine Learning, 28(2-3):133?168.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In CoNLL-2005 ?
Proceedings of the9th Conference on Computational Natural LanguageLearning, pages 144?151.
Ann Arbor, MI, USA, June2005.
Association for Computational Linguistics.Rebecca Hwa.
2000.
Sample selection for statisticalgrammar induction.
In EMNLP/VLC-2000 ?
Proceed-ings of the Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing and VeryLarge Corpora, pages 45?52.
Hong Kong, China, Oc-tober 7-8, 2000.. Association for Computational Lin-guistics.Rebecca Hwa.
2001.
On minimizing training corpus forparser acquisition.
In Walter Daelemans and Re?miZajac, editors, CoNLL-2001 ?
Proceedings of the5th Natural Language Learning Workshop.
Toulouse,France, 6-7 July 2001.
Association for ComputationalLinguistics.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In ICML-2001 ?
Proceedings of the 18th In-ternational Conference on Machine Learning, pages282?289.
Williams College, MA, USA, June 28 - July1, 2001.
San Francisco, CA: Morgan Kaufmann.David D. Lewis and Jason Catlett.
1994.
Heteroge-neous uncertainty sampling for supervised learning.In William W. Cohen and Haym Hirsh, editors, ICML?94: Proceedings of the 11th International Conferenceon Machine Learning, pages 148?156.
San Francisco,CA: Morgan Kaufmann.Grace Ngai and David Yarowsky.
2000.
Rule writingor annotation: Cost-efficient resource usage for basenoun phrase chunking.
In ACL?00 ?
Proceedings of the38th Annual Meeting of the Association for Computa-tional Linguistics, pages 117?125.
Hong Kong, China,1-8 August 2000.
San Francisco, CA: Morgan Kauf-mann.494David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Lillian Lee and Donna Harman, editors,EMNLP 2001 ?
Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1?9.
Pittsburgh, PA, USA, June 3-4, 2001.Association for Computational Linguistics.Greg Schohn and David Cohn.
2000.
Less is more: Ac-tive learning with support vector machines.
In ICML?00: Proceedings of the 17th International Conferenceon Machine Learning, pages 839?846.
San Francisco,CA: Morgan Kaufmann.H.
Sebastian Seung, Manfred Opper, and Haim Som-polinsky.
1992.
Query by committee.
In COLT?92 ?Proceedings of the 5th Annual Conference on Compu-tational Learning Theory, pages 287?294.
Pittsburgh,PA, USA, July 27-29, 1992.
New York, NY: ACMPress.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, andChew Lim Tan.
2004.
Multi-criteria-based activelearning for named entity recognition.
In ACL?04 ?Proceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 589?596.Barcelona, Spain, July 21-26, 2004.
San Francisco,CA: Morgan Kaufmann.Cynthia A. Thompson, Mary Elaine Califf, and Ray-mond J. Mooney.
1999.
Active learning for naturallanguage parsing and information extraction.
In ICML?99: Proceedings of the 16th International Conferenceon Machine Learning, pages 406?414.
Bled, Slovenia,June 1999.
San Francisco, CA: Morgan Kaufmann.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CONLL-2003 sharedtask: Language-independent named entity recogni-tion.
In Walter Daelemans and Miles Osborne, edi-tors, CoNLL-2003 ?
Proceedings of the 7th Confer-ence on Computational Natural Language Learning,pages 142?147.
Edmonton, Canada, 2003.
Associationfor Computational Linguistics.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
Efficient annotation with the Jena ANnota-tion Environment (JANE).
In Proceedings of the ACL2007 ?Linguistic Annotation Workshop ?
A Merger ofNLPXML 2007 and FLAC 2007?.
Prague, Czech Re-public, June 28-29, 2007.
Association for Computa-tional Linguistics (ACL).495
