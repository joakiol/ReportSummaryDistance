Unsupervised Acquisition of PredominantWord SensesDiana McCarthyUniversity of SussexRob KoelingUniversity of SussexJulie WeedsUniversity of SussexJohn Carroll?University of SussexThere has been a great deal of recent research into word sense disambiguation, particularlysince the inception of the Senseval evaluation exercises.
Because a word often has more thanone meaning, resolving word sense ambiguity could benefit applications that need some levelof semantic interpretation of language input.
A major problem is that the accuracy of wordsense disambiguation systems is strongly dependent on the quantity of manually sense-taggeddata available, and even the best systems, when tagging every word token in a document,perform little better than a simple heuristic that guesses the first, or predominant, sense of aword in all contexts.
The success of this heuristic is due to the skewed nature of word sensedistributions.
Data for the heuristic can come from either dictionaries or a sample of sense-tagged data.
However, there is a limited supply of the latter, and the sense distributions andpredominant sense of a word can depend on the domain or source of a document.
(The firstsense of ?star?
for example would be different in the popular press and scientific journals).In this article, we expand on a previously proposed method for determining the predominantsense of a word automatically from raw text.
We look at a number of different data sources andparameterizations of the method, using evaluation results and error analyses to identify wherethe method performs well and also where it does not.
In particular, we find that the methoddoes not work as well for verbs and adverbs as nouns and adjectives, but produces more accuratepredominant sense information than the widely used SemCor corpus for nouns with low coveragein that corpus.
We further show that the method is able to adapt successfully to domains whenusing domain specific corpora as input and where the input can either be hand-labeled for domainor automatically classified.?
Department of Informatics, Brighton BN1 9QH, UK.
E-mail: {dianam,robk,juliewe,johnca}@sussex.ac.uk.Submission received: 16 November 2005; revised submission received: 12 July 2006; accepted for publication16 February 2007.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 41.
IntroductionIn word sense disambiguation, the ?first sense?
heuristic (choosing the first, or predom-inant sense of a word) is used by most state-of-the-art systems as a back-off methodwhen information from the context is not sufficient to make a more informed choice.In this article, we present an in-depth study of a method for automatically acquiringpredominant senses for words from raw text (McCarthy et al 2004a).The method uses distributionally similar words listed as ?nearest neighbors?in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of theobservation that the more prevalent a sense of a word, the more neighbors will relateto that sense, and the higher their distributional similarity scores will be.
The sensesof a word are defined in a sense inventory.
We use WordNet (Fellbaum 1998) becausethis is widely used, is publicly available, and has plenty of gold-standard evaluationdata available (Miller et al 1993; Cotton et al 2001; Preiss and Yarowsky 2001; Mihalceaand Edmonds 2004).
The distributional strength of the neighbors is associated with thesenses of a word using a measure of semantic similarity which relies on the relationshipsbetween word senses, such as hyponyms (available in an inventory such as WordNet)or overlap in the definitions of word senses (available in most dictionaries), or both.In this article we provide a detailed discussion and quantitative analysis of themotivation behind the first sense heuristic, and a full description of our method.
Weextend previously reported work in a number of different directions: We evaluate the method on all parts of speech (PoS) on SemCor (Milleret al 1993).
Previous experiments (McCarthy et al 2004c) evaluated onlynouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al 2001)and Senseval-3 (Mihalcea and Edmonds 2004) data.
The evaluation on allPoS is much more extensive because the SemCor corpus is composed of220,000 words in contrast to the 6 documents in the Senseval-2 and -3English all words data (10,000 words). We compare two WordNet similarity measures in our evaluation onnouns, and also contrast performance using two publicly availablethesauruses, both produced from the same NEWSWIRE corpus, but onederived using a proximity-based approach and the other usingdependency relations from a parser.
It turns out that the results from theproximity-based thesaurus are comparable to those from the dependency-based thesaurus; this is encouraging for applying the method to languageswithout sophisticated analysis tools. We manually analyze a sample of errors from the SemCor evaluation.
Asmall number of errors can be traced back to inherent shortcomings of ourmethod, but the main source of error is due to noise from related senses.This is a common problem for all WSD systems (Ide and Wilks 2006) butone which is only recently starting to be addressed by the WSDcommunity (Navigli, Litkowski, and Hargraves 2007). One motivation for an automatic method for acquiring predominantsenses is that there will always be words for which there are insufficientdata available in manually sense-tagged resources.
We compare theperformance of our automatic method with the first sense heuristicderived from SemCor on nouns in the Senseval-2 data.
We find that the554McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Sensesautomatic method outperforms the one obtained from manual annotationsin SemCor for nouns with fewer than five occurrences in SemCor. Aside from the lack of coverage of manually annotated data, there is aneed for first sense heuristics to be specific to domain.
We explore thepotential for applying the method with domain-specific text for all PoS inan experiment using a gold-standard domain-specific resource (Magniniand Cavaglia` 2000) which we have used previously only with nouns.
Weshow that although there is a little mileage to be had from domain-specificfirst sense heuristics for verbs, nouns benefit greatly from domain-specifictraining. In previous work (Koeling, McCarthy, and Carroll 2005) we producedmanually sense-annotated domain-specific test corpora for a lexicalsample, and demonstrated that predominant senses acquired (fromhand-classified corpora) in the same domain as the test data outperformedthe SemCor first sense.
We further this exploration by contrasting withresults from training on automatically categorized text from the EnglishGigaword Corpus and show that the results are comparable to those usinghand-classified domain data.The article is organized as follows.
In the next section we motivate the use of pre-dominant sense information in WSD systems and the need for acquiring this informationautomatically.
In Section 3 we give an overview of related work in WSD, focusing on theacquisition of prior sense distributions and domain-specific sense information.
Section 4describes our acquisition method.
Section 5 describes the experimental setup for thework reported in this article.
Section 6 describes four experiments.
The first evaluatesthe first sense heuristic using predominant sense information acquired for all PoS onSemCor; for nouns we compare two semantic similarity methods and three differenttypes of distributional thesaurus.
We also report an error analysis for all PoS of ourmethod.
The second experiment compares the performance of the automatic methodto the manually produced data in SemCor, on nouns in the Senseval-2 data, lookingparticularly at nouns which have a low frequency in SemCor.
The third uses corpora inrestricted domains and the subject field code gold standard of Magnini and Cavaglia`(2000) to investigate the potential for domain-specific rankings for different PoS.
Thefourth compares results when we train and test on domain-specific corpora, wherethe training data is (1) manually categorized for domain and from the same corpusas the test data, and (2) where the training data is harvested automatically from anothercorpus which is categorized automatically.
Finally, we conclude (Section 7) and discussdirections for future work (Section 8).2.
MotivationThe problem of disambiguating the meanings of words in text has received muchattention recently, particularly since the inception of the Senseval evaluation exercises(Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004).One of the standard Senseval tasks (the ?all words?
task) is to tag each open class wordwith one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum1998).
The most accurate word sense disambiguation (WSD) systems use supervisedmachine learning approaches (Stevenson and Wilks 2001), trained on text which hasbeen sense tagged by hand.
However, the performance of these systems is strongly555Computational Linguistics Volume 33, Number 4dependent on the quantity of training data available (Yarowsky and Florian 2002),and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998).
Thelargest all words sense tagged corpus is SemCor, which is 220,000 words taken from103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kuc?era1979) and the complete text of a 19th-century American novel, The Red Badge of Courage,which totals 45,600 words (Landes, Leacock, and Tengi 1998).
Approximately half of thewords in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) andthese have been linked to WordNet senses by human taggers using a software interface.The shortage of training data due to the high costs of tagging texts has motivatedresearch into unsupervised methods for WSD.
But in the English all-words tasks inSenseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not makeuse of hand-tagged data (in some form or other) performed substantially worse thanthose that did.
Table 1 summarizes the situation.
It gives the precision and recall ofthe best1 two supervised (S) and unsupervised (U)2 systems for the English all wordsand English lexical sample for Senseval-23 and -3, along with the first sense baseline(FS) reported by the task organizers.4 This is a simple application of the ?first sense?heuristic?that is, using the most common sense of a word for every instance of it in thetest corpus, regardless of context.
Although contextual WSD is of course preferable, thebaseline is a very powerful one and unsupervised systems find it surprisingly hard tobeat (indeed, some of the systems that report themselves as unsupervised actually makesome use of a manually obtained first-sense heuristic).
Considering both precision andrecall, only 5 of 26 systems in the Senseval-3 English all-words task beat the first senseheuristic as derived from SemCor (61.5%5), and then by only a few percentage points(the top system scoring 65% precision and recall) despite using hand-tagged trainingdata available from SemCor and previous Senseval data sets, large sets of contextualfeatures, and sophisticated machine learning algorithms.The performance of WSD systems, at least for all-words tasks, seems to haveplateaued at a level just above the first sense heuristic (Snyder and Palmer 2004).
This isdue to the shortage of training data and the often fine granularity of sense distinctions.Ide and Wilks (2006) argue that it is best to concentrate effort on distinctions whichare useful for applications and where systems can be confident of high precision.
Incases where systems are less confident, but word senses, rather than words, are needed,the first sense heuristic is a powerful back-off strategy.
This strategy is dependent oninformation provided in dictionaries.
Two dictionaries that have been used by EnglishWSD systems are the Longman Dictionary of Contemporary English (LDOCE) (Procter1 We rank the systems by the recall scores, because this is the accuracy over the entire test set regardless ofhow many items were attempted.2 Note that the classification of systems as unsupervised is not straightforward.
Systems reported asunsupervised in the Senseval proceedings sometimes make use of some manual annotations.
Forexample, the top scoring system that reported itself unsupervised in the Senseval-3 lexical sample taskused manually sense-tagged training data for constructing glosses.3 The verb lexical sample was done as a separate exercise for Senseval-2, and for brevity we have notincluded the results from this task.4 The all-words task organizers used the first sense as listed in WordNet.
This is based on the SemCor firstsense because WordNet senses are ordered according to the frequency data in SemCor.
However, wheresenses are not found in WordNet, the ordering is arbitrarily determined as a function of the ?grind?program (see http://wordnet.princeton.edu/man/grind.1WN.htm).
The lexical sample task organizersstate that they use the ?most frequent sense?
but do not stipulate if this is taken from WordNet, ordirectly from SemCor.5 This figure is the arithmetic mean of two published estimates (Snyder and Palmer 2004), the differencebeing due to the treatment of multiwords.556McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesTable 1The best two performing systems of each type (according to fine-grained recall) in Senseval-2and -3.All words Lexical samplePrecision (%) Recall (%) Precision (%) Recall (%)Senseval-2 S 69.0 69.0 64.2 64.2Senseval-2 S 63.6 63.6 63.8 63.8Senseval-2 U 45.1 45.1 40.2 40.1Senseval-2 U 36.0 36.0 58.1 31.9FS baseline 57.0 57.0 47.6 47.6Senseval-3 S 65.1 65.1 72.9 72.9Senseval-3 S 65.1 64.2 72.6 72.6Senseval-3 U 58.3 58.2 66.1 65.7Senseval-3 U 55.7 54.6 56.3 56.3FS baseline 61.5 61.5 55.2 55.21978) and WordNet (Fellbaum 1998).
These both provide a ranking of senses accord-ing to their predominance.
The sense ordering in LDOCE is based on lexicographerintuition, whereas in WordNet the senses are ordered according to their frequency inSemCor (Miller et al 1993).There are two major problems with deriving a first sense heuristic from these typesof resources.
The first is that the predominant sense of a word varies according tothe source of the document (McCarthy and Carroll 2003) and with the domain.
Forexample, the first sense of star as derived from SemCor is celestial body, but if one weredisambiguating popular news stories then celebrity would be more likely.
Domain,topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al 2002)and the sense-frequency distributions of words depend on all of these factors.
Anydictionary will provide only a single sense ranking, whether this is derived from sense-tagged data as in WordNet, lexicographer intuition as in LDOCE, or inspection of corpusdata as in the Oxford Advanced Learner?s Dictionary (Hornby 1989).
A fixed order ofsenses may not reflect the data that an NLP system is dealing with.The second problem with obtaining predominant sense information applies to theuse of hand-tagged resources, such as SemCor.
Such resources are relatively small dueto the cost of manual tagging (Kilgarriff 1998).
Many words will simply not be covered,or occur only a few times.
For many words in WordNet the ordering of word senses isbased on a very small number of occurrences in SemCor.
For example, the first senseof tiger is an audacious person whereas most people would assume the carnivorousanimal sense is more prevalent.
This is because the two senses each occur exactly oncein SemCor, and when there is no frequency information to break the tie the WordNetsense ordering is assigned arbitrarily.
There are many fairly common words (such asthe noun crane) which do not occur at all in SemCor.
Table 2 gives the number andpercentage of words6 in WordNet and the BNC which do not occur in SemCor.
As onewould expect from Zipf?s law, a substantial number of words do not occur in SemCor,even when we do not consider multiwords.
Many of these words are extremely rare, but6 Here and elsewhere in this article we give figures only for words without embedded spaces, that is, notmultiwords.557Computational Linguistics Volume 33, Number 4Table 2Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor.WordNet types BNC typesPoS No.
% No.
%noun 43,781 81.9 360,535 97.5verb 4,741 56.4 25,292 87.6adjective 14,991 72.3 95,908 95.4adverb 2,405 64.4 10,223 89.2Table 3Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with nodata in SemCor (0 columns), or with very little data (?
1 and ?
5 occurrences).
Note that thereare no annotations for adverbs in the Senseval-3 documents.Senseval-2 Senseval-30 ?
1 ?
5 0 ?
1 ?
5PoS No.
% No.
% No.
% No.
% No.
% No.
%noun 12 3.2 28 7.4 49 12.9 13 3.1 26 6.3 69 16.7verb 7 2.1 11 3.4 28 8.6 3 0.9 10 2.9 36 10.4adjective 9 4.2 16 7.4 50 23.1 8 4.7 15 8.9 33 19.5adverb 1 0.9 1 0.9 2 1.8 ?
?
?
?
?
?in any given document it is likely that there will be at least some words without SemCordata.
Table 3 quantifies this, for the Senseval-2 and -3 all-words tasks test data, showingthe percentage of polysemous word types with no frequency information in SemCor, thepercentage with zero or one occurrences, and the percentage with up to five occurrences.
(For example, the table indicates that 12.9% of nouns in the Senseval-2 data, and 16.7%in Senseval-3, have five or fewer occurrences in SemCor.)
Thus, although SemCor maycover many frequently occurring word types in a given document, there are likely to bea substantial proportion for which there is very little or no information available.Tables 4 and 5 present an analysis of the actual ambiguity of polysemous wordswithin the six documents making up the Senseval-2 and -3 all-words test data.
Theyshow the extent to which these words are used in a predominant sense, within adocument, and the extent to which this is the same as that given by SemCor.
The twotables share a common format: columns 2?5 give percentages over all ?document/wordtype?
combinations.
The second column shows the percentage of the ?document/wordtype?
combinations where the word is used in the document in only one of its senses.The fourth column shows the same percentage but for ?document/word type?
combi-nations where the word is used in more than one sense in the document.
The third andfifth columns give the percentage of the words in the preceding columns (second andfourth, respectively) where the first sense for the word in the document is the same as inSemCor (FS = SC FS).
For the third column, this is the only sense that this word appearsin within the document.
(Note that for any row, columns 2 and 4 account for all possibil-ities so will always add up to 100.)
The sixth column gives the mean degree of polysemy,according to WordNet, for the set of words that these figures are calculated for.558McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesTable 4Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more thanonce in a document (adverb data is only from Senseval-2).1 sense > 1 sensePoS % FS = SC FS % % FS = SC FS % Mean polysemynoun 72.2 52.2 27.8 7.3 5.9verb 45.6 25.1 54.4 16.9 12.7adjective 62.9 40.5 37.1 10.3 4.8adverb 64.7 50.0 35.3 17.6 4.7The figures in Table 4 are for words occurring more than once in a given Sensevaltest document.
The tendency for words to be used in only one sense in any givendocument7 is strongest for nouns, although adverbs and adjectives also tend towardsone sense.
Verbs are on average much more polysemous than the other parts of speechyet still 45.6% of polysemous verbs which occur more than once are used in only a singlesense.
However, because verbs are in general more polysemous, it makes it less likelythat if a verb occurs in only one sense in a document then it will be the one indicated bySemCor.The figures in Table 5 are for all words in the Senseval documents (not just those oc-curring more than once), showing the accuracy of a SemCor-derived first-sense heuristicfor words with a frequency below a specified threshold (column 1) in SemCor.
The tableshows that although having a first sense from SemCor is certainly useful, when lookingat figures for all the words in the Senseval documents a good proportion have firstsenses other than the one indicated by SemCor.
Furthermore, the lower the frequencyin SemCor the more likely that the first sense indicated by SemCor is wrong.
(However,the situation is slightly different for adverbs because there are not many with lowfrequency in SemCor and they are on average not very polysemous, so for them a firstsense derived from a resource like SemCor?where one exists?is possibly sufficient.
)These results show that although SemCor is a useful resource, there will always bewords for which its coverage is inadequate.
In addition, few languages have extensivehand-tagged resources or sense orderings produced by lexicographers.
Moreover, gen-eral resources containing word sense information are not likely to be appropriate whenprocessing language for a wider variety of domains, topics, and genres.
What is neededis a means to find predominant senses automatically.3.
Related WorkMost research in WSD to date has concentrated on using contextual features, typicallyneighboring words, to help infer the correct sense of a target word.
In contrast, ourwork is aimed at discovering the predominant sense of a word from raw text because7 The tendency for words to be used in only one sense in a given discourse is weaker for fine-graineddistinctions (Krovetz 1998) compared to coarse-grained distinctions (Gale, Church, and Yarowsky 1992).Nevertheless, even with a fine-grained inventory the first sense heuristic is certainly powerful, as shownin Table 1.559Computational Linguistics Volume 33, Number 4Table 5Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data,broken down by their frequencies of occurrence in SemCor (adverb data is only fromSenseval-2).1 sense > 1 senseFrequency % FS = SC FS % % FS = SC FS % Mean polysemynoun?
1 (54) 96.3 24.1 3.7 0.0 2.8?
5 (118) 96.6 43.2 3.4 0.0 3.2?
10 (191) 96.9 48.7 3.1 0.0 3.3all (792) 88.8 51.6 11.2 2.5 5.5verb?
1 (21) 100.0 33.3 0.0 0.0 2.4?
5 (64) 98.4 35.9 1.6 1.6 3.2?
10 (110) 98.2 38.2 1.8 1.8 3.5all (671) 82.6 39.3 17.4 5.1 9.0adjective?
1 (31) 93.5 19.4 6.5 0.0 2.5?
5 (83) 95.2 34.9 4.8 1.2 2.7?
10 (120) 90.8 40.8 9.2 1.7 2.8all (385) 82.6 46.2 17.4 3.6 5.1adverb?
1 (1) 0.0 0.0 100.0 0.0 2.0?
5 (2) 50.0 50.0 50.0 0.0 2.0?
10 (8) 87.5 62.5 12.5 0.0 2.3all (111) 82.9 62.2 17.1 5.4 4.0the first sense heuristic is so powerful, and because manually sense-tagged data is notalways available.Lapata and Brew (2004) highlighted the importance of a good prior in WSD.
Theyused syntactic evidence to find a prior distribution for Levin (1993) verb classes, andincorporated this in a WSD system.
Lapata and Brew obtained their priors for verbclasses directly from subcategorization evidence in a parsed corpus, whereas we useparsed data to find distributionally similar words (nearest neighbors) to the targetword which reflect the different senses of the word and have associated distributionalsimilarity scores which can be used for ranking the senses according to prevalence.We would, however, agree that subcategorization evidence should be very useful fordisambiguating verbs, and would hope to combine such evidence with our rankingmodels for context-based WSD.A major benefit of our work is that this method permits us to produce predominantsenses for any desired domain and text type.
Buitelaar and Sacaleanu (2001) exploredranking and selection of synsets in GermaNet for specific domains using the wordsin a given synset, and those related by hyponymy, and a term relevance measuretaken from information retrieval.
Buitelaar and Sacaleanu evaluated their method onidentifying domain-specific concepts using human judgments on 100 items.
We evaluate560McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Sensesour method using publicly available resources for balanced text, and, for domain-specific investigations, resources we have developed ourselves (Koeling, McCarthy,and Carroll 2005).
Magnini and Cavaglia` (2000) associated WordNet word senses withparticular domains, and this has proved useful for high precision WSD (Magnini etal.
2001); indeed, we have used their domain labels (or subject field codes, SFCs) forevaluation (Section 6.3).
Identification of these SFCs for word senses was semi-automaticand required a considerable amount of hand-labeling.
Our approach requires only rawtext from the given domain and because of this it can easily be applied to a new domainor sense inventory, as long as there is enough appropriate text.There are other approaches aimed at gleaning domain-specific information fromraw data.
Gliozzo, Giuliano, and Strapparava (2005) induced domain models from rawdata using unsupervised latent semantic models and then fed this into a supervisedWSD model and evaluated on Senseval-3 lexical sample data in four languages.
Chanand Ng (2005) obtained probability distributions to feed into their supervised WSD mod-els.
They used multilingual parallel corpus data to provide probability estimates for asubset of 22 nouns from the lexical sample task.
They then fed this into a supervised WSDmodel and verified that the estimates for prior distributions improved performance forsupervised WSD.
We intend eventually to use our prevalence scores to feed into un-supervised WSD models.
Although unsupervised models seem to be beaten wheneverthere is training data to be had, we anticipate that unsupervised models with improvedpriors from the ranking might outperform supervised systems in situations where thereis little training data available.
Whereas this article is about finding predominant sensesfor back-off in a WSD system, the method could be applied to finding a prior distributionover all word senses of each target word.
It is our intention that the back-off models pro-duced by our prevalence ranking, either as predominant senses or prior distributionsover word senses, could be combined with contextual information for WSD.Mohammad and Hirst (2006) describe an approach to acquiring predominant sensesfrom corpora which makes use of the category information in the Macquarie Thesaurus.Evaluation is performed on an artificially constructed test set from unambiguous wordsin the same category as the 27 test words (nouns, verbs, and adjectives).
The senses ofthe words are the categories of the thesaurus and the experiment uses only two sensesof each word, the two most predominant ones.
The predominance of the two senses isaltered systematically.
The results are encouraging because a much smaller amount ofcorpus data is needed compared to our approach.
However, their method has only beenapplied to an artificially constructed test set, rather than a publicly available corpus, andhas yet to be applied in a domain-specific setting, which is the chief motivation of ourwork.The work of Pantel and Lin (2002) is probably the most closely related studythat predates ours, although their ultimate goal is different.
Pantel and Lin deviseda method called CBC (clustering by committee) where the 10 nearest neighbors ofa word in a distributional thesaurus are clustered to identify the various senses ofthe word.
Pantel and Lin use a measure of semantic similarity (Lin 1997) to evaluatethe discovered classes with respect to WordNet as a gold standard.
The CBC methodobtained a precision of 61% (the percentage of senses discovered that did exist inWordNet) and a recall of 51% (the percentage of senses discovered from the union ofthose discovered with different clustering algorithms that they tried).88 The calculation of recall was over the union of senses discovered automatically, rather than over thesenses in WordNet, because senses in WordNet may be unattested in the data.561Computational Linguistics Volume 33, Number 4Pantel and Lin?s approach is related to ours in that, in their sense discovery pro-cedure, predominant senses have more of a chance of being found than other senses,although their algorithm is specifically tailored to look for senses regardless of fre-quency.
To do this the algorithm removes neighbors of the target word once theyare assigned to a cluster so that less frequent senses can be discovered.
Our method,described in detail in Section 4, associates the nearest neighbors to the senses of thetarget in a predefined inventory (we use WordNet).
We rank the senses using a measurewhich sums over the distributional similarity of neighbors weighted by the strength ofthe association between the neighbors and the sense.
This is done on the assumptionthat more prevalent senses will have strong associations with more nearest neighborsbecause they have occurred in more contexts in the corpus used for producing thethesaurus.
Both the number and the distributional similarity of the neighbors are usedin our prevalence ranking measure.
Pantel and Lin process the possible clusters in orderof their average distributional similarity and number of neighbors but do not take thenumber of neighbors into account in the scores given for the clusters.
The measuresthat Pantel and Lin associate with their clusters are determined by the cohesivenessof the cluster with the target word because their aim is one of sense discovery.
Theirmeasure is the similarity between the cluster and the target word and does not retainthe distributional similarity of the neighbors within the cluster.
It is quite possible thatthere is a low frequency sense of a target word with synonyms that form a nice cohesivegroup.Although the number of neighbors assigned to a cluster may correlate with ourranking score, intuition suggests that a combination of the quantity and distributionalsimilarity of neighbors to the target word sense is best for determining the relativepredominance of senses.
In Section 6 we test this hypothesis using a simplified versionof our method which only uses the number of neighbors, and assigns each to onesense.
Comparisons with the CBC algorithm as it stands would be difficult becausein order to evaluate acquisition of predominance information we have used publiclyavailable gold-standard sense-tagged corpora, and these have WordNet senses.
CBCwill not always find WordNet senses.
For example, using the on-line demonstration ofCBC,9 several common senses from nouns from the Senseval-2 lexical sample are notdiscovered, including the upright object sense of post, the block of something senseof bar, the daytime sense of day and the meaning of the word sense of the word sense.Automatic acquisition of sense inventories is an important endeavor, and we hope tolook at ways of combining our method for detecting predominance with automaticallyinduced inventories such as those produced by CBC.
Evaluation of induced inventoriesshould be done in the context of an application, because the senses will be keyed to theacquisition corpus and not to WordNet.Induction of senses allows coverage of senses appearing in the data that are notpresent in a predefined inventory.
Although we could adapt our method for use withan automatically induced inventory, our method which uses WordNet might also becombined with one that can automatically find new senses from text and then relatethese to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do withunknown nouns.9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with theoption to include all corpora (TREC-2002, TREC-9, and COSMOS).562McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses4.
MethodIn our method, the predominant sense for a target word is determined from a preva-lence ranking of the possible senses for that word.
The senses come from a predefinedinventory (which might be a dictionary or WordNet-like resource).
The ranking isderived using a distributional thesaurus automatically produced from a large corpus,and a semantic similarity measure defined over the sense inventory.
The distributionalthesaurus contains a set of words that are ?nearest neighbors?
to the target word withrespect to similarity of the way in which they are distributed.
(Distributional similarityis based on the hypothesis of Harris, 1968, that words which occur in similar contextshave related meanings.)
The thesaurus assigns a distributional similarity score to eachneighbor word, indicating its closeness to the target word.
For example, the nearest10neighbors of sandwich might be:salad, pizza, bread, soup...and the nearest neighbors of the polysemous noun star11 might be:actor, footballer, planet, circle...These neighbors reflect the various senses of the word, which for star might be: a celebrity a celestial body a shape a sign of the zodiac12We assume that the number and distributional similarity scores of neighbors pertainingto a given sense of a target word will reflect the prevalence of that sense in the corpusfrom which the thesaurus was derived.
This is because the more prevalent senses of theword will appear more frequently and in more contexts than other, less prevalent senses.The neighbors of the target word relate to its senses, but are themselves word formsrather than senses.
The senses of the target word are predefined in a sense inventoryand we use a semantic similarity score defined over the sense inventory to relate theneighbors to the various senses of the target word.
The two semantic similarity scoresthat we use in this article are implemented in the WordNet similarity package.
One usesthe overlap in definitions of word senses, based on Lesk (1986), and the other uses acombination of corpus statistics and the WordNet hyponym hierarchy, based on Jiangand Conrath (1997).
We describe these fully in Section 4.2.
We now describe intuitively10 In this and other examples we restrict ourselves to four neighbors for brevity.11 In this example we assume that the sense inventory assigns four senses to star, but the inventory couldassign fewer or more depending on its level of granularity and level of detail.12 Note that this zodiac or horoscope sense of star usually occurs as part of the multiword star sign (e.g.,your star sign secrets revealed) or in plural form (your stars today?free online).563Computational Linguistics Volume 33, Number 4the measure for ranking the senses according to predominance, and then give a moreformal definition.The measure uses the sum total of the distributional similarity scores of the k nearestneighbors.
This total is divided between the senses of the target word by apportioningthe distributional similarity of each neighbor to the senses.
The contribution of eachneighbor is measured in terms of its distributional similarity score so that ?nearer?neighbors count for more.
The distributional similarity score of each neighbor is dividedbetween the various senses rather than attributing the neighbor to only one sense.
Thisis done because neighbors can relate to more than one sense due to relationships suchas systematic polysemy.
For example, in the thesaurus we describe subsequently inSection 4.1 acquired from the BNC, chicken has neighbors duck and goose which relate toboth the meat and animal senses.
We apportion the contribution of a neighbor to eachof the word senses according to a weight which is the normalized semantic similarityscore between the sense and the neighbor.
We normalize the semantic similarity scoresbecause some of the semantic similarity scores that we use, described in Section 4.2,can get disproportionately large.
Because we normalize the semantic similarity scores,the sum of the ranking scores for a word equals the sum of the distributional similarityscores.
To summarize, we rank the senses of the target word, such as star, by apportion-ing the distributional similarity scores of the top k neighbors between the senses.
Eachdistributional similarity score (dss) is weighted by a normalized semantic similarityscore (sss) between the sense and the neighbor.
This process is illustrated in Figure 1.More formally, to find the predominant sense of a word (w) we take each sensein turn and obtain a prevalence score.
Let Nw = {n1, n2...nk} be the ordered set of thetop scoring k neighbors of w from the distributional thesaurus with associated scores{dss(w, n1), dss(w, n2), ...dss(w, nk)}.
Let senses(w) be the set of senses of w in the senseinventory.
For each sense of w (si ?
senses(w)) we obtain a prevalence score by summingover the dss(w, nj) of each neighbor (nj ?
Nw) multiplied by a weight.
This weight is thesss between the target sense (si) and nj divided by the sum of all sss scores for senses(w)Figure 1The prevalence ranking process for the noun star.564McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Sensesand nj.
sss is the maximum WordNet similarity score (sss?)
between si and the senses ofnj (sx ?
senses(nj)).13 Each sense si ?
senses(w) is therefore assigned a score as follows:Prevalence Score(w, si) =?nj?Nwdss(w, nj) ?sss(si, nj)?si?
?senses(w) sss(si?
, nj)(1)wheresss(si, nj) = maxsx?senses(nj )sss?
(si, sx) (2)We describe dss and sss?
in Sections 4.1 and 4.2.
Note that the dss for a given neighboris shared between the different senses of w depending on the weight given by thenormalized sss.4.1 The Distributional Similarity ScoreMeasures of distributional similarity take into account the shared contexts of the twowords.
Several measures of distributional similarity have been described in the litera-ture.
In our experiments, dss is computed using Lin?s similarity measure (Lin 1998a).We set the number of nearest neighbors to equal 50.14 We use three different sources ofdata for our first two experiments, resulting in three distributional thesauruses.
Theseare described in the next section.
We use domain-specific data for our third and fourthexperiments.
The data sources for these are described in Sections 6.3 and 6.4.A word, w, is described by a set of features, f , each with an associated frequency,where each feature is a pair ?r, x?
consisting of a grammatical relation name and theother word in the relation.
We computed distributional similarity scores for every pair ofwords of the same PoS where each word?s total feature frequency was at least 10.
A the-saurus entry of size k for a target word w is then defined as the k most similar words to w.A large number of distributional similarity measures have been proposed in theliterature (see Weeds 2003 for a review) and comparing them is outside the scope of thiswork.
However, the study of Weeds and Weir (2005) provides interesting insights intowhat makes a ?good?
distributional similarity measure in the contexts of semantic simi-larity prediction and language modeling.
In particular, weighting features by pointwisemutual information (Church and Hanks 1989) appears to be beneficial.
The pointwisemutual information (I(w, f )) between a word and a feature is calculated asI(w, f ) = logP( f |w)P( f )(3)Intuitively, this means that the occurrence of a less-common feature is more importantin describing a word than a more-common feature.
For example, the verb eat is moreselective and tells us more about the meaning of its arguments than the verb be.13 We use sss for the semantic similarity between a WordNet sense and another word, the neighbor.
We usesss?
for the semantic similarity between two WordNet senses, si and a sense of the neighbor (sx).14 From previous work (McCarthy et al 2004b), the value of k has a minimal effect on finding thepredominant sense; however, we will continue experimentation with this in the future for using ourranking score for estimating probability distributions of senses, because a sufficiently large value of k willbe needed to include neighbors for rarer senses.565Computational Linguistics Volume 33, Number 4We chose to use the distributional similarity score described by Lin (1998a) becauseit is an unparameterized measure which uses pointwise mutual information to weightfeatures and which has been shown (Weeds 2003) to be highly competitive in makingpredictions of semantic similarity.
This measure is based on Lin?s information-theoreticsimilarity theorem (Lin 1997):The similarity between A and B is measured by the ratio between the amount ofinformation needed to state the commonality of A and B and the information needed tofully describe what A and B are.In our application, if T(w) is the set of features f such that I(w, f ) is positive, then thesimilarity between two words, w and n, isdss(w, n) =?f?T(w)?T(n)(I(w, f ) + I(n, f ))?f?T(w) I(w, f ) +?f?T(n) I(n, f )(4)However, due to this choice of dss and the openness of the domain, we restrict ourselvesto only considering words with a total feature frequency of at least 10.
Weeds et al (2005)do show that distributional similarity can be computed for lower frequency words butthis is using a highly specialized corpus of 400,000 words from the biomedical domain.Further, it has been shown (Weeds et al 2005; Weeds and Weir 2005) that performanceof Lin?s distributional similarity score decreases more significantly than other measuresfor low frequency nouns.
We leave the investigation of other distributional similarityscores and the application to smaller corpora as areas for further study.4.2 The Semantic Similarity ScoresWordNet is widely used for research in WSD because it is publicly available and thereare a number of associated sense-tagged corpora (Miller et al 1993; Cotton et al 2001;Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes.Several semantic similarity scores have been proposed that leverage the structure ofWordNet; for sss?
we experiment with two of these, as implemented in the WordNetSimilarity Package (Patwardhan and Pedersen 2003).The WordNet Similarity Package implements a range of similarity scores.
McCarthyet al (2004b) experimented with six of these for the sss?
used in the prevalence score,Equation (2).
In the experiments reported here we use the two scores that performedbest in that previous work.
We briefly summarize them here; Patwardhan, Banerjee,and Pedersen (2003) give a more detailed discussion.
The scores measure the similaritybetween two WordNet senses (s1 and s2).lesk This measure (Banerjee and Pedersen 2002) maximizes the number of overlap-ping words in the gloss, or definition, of the senses.
It uses the glosses of semanti-cally related (according to WordNet) senses too.
We use the default version of themeasure in the package with no normalizing for gloss length, and the default setof relations:lesk(s1, s2) = |{W1 ?
definition(s1)}| ?
|{W2 ?
definition(s2)}| (5)where definitions(s) is the gloss definition of sense s concatenated with the glossdefinitions of the senses related to s where the relationships are defined by the de-566McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Sensesfault set of relations in the relations.dat file supplied with the WordNet Similaritypackage.
W ?
definition(s) is the set of words from the concatenated definitions.jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes(synsets) in the WordNet hierarchy with frequency counts.
Each synset is incre-mented with the frequency counts (from the corpus) of all words belonging tothat synset, directly or via the hyponymy relation.
The frequency data is used tocalculate the ?information content?
(IC; Resnik 1995) of a class as follows:IC(s) = ?log(p(s)) (6)Jiang and Conrath specify a distance measure:Djcn(s1, s2) = IC(s1) + IC(s2) ?
2 ?
IC(s3) (7)where the third class (s3) is the most informative, or most specific, superordinatesynset of the two senses s1 and s2.
This is converted to a similarity measurein the WordNet Similarity package by taking the reciprocal as in Equation (8)(which follows).
For this reason, the jcn values can get very large indeed whenthe distances are negligible, for example where the neighbor has a sense which isa synonym.
This is a motivation for our normalizing the sss in Equation (1).jcn(s1, s2) = 1/Djcn(s1, s2) (8)The IC data required for the jcn measure can be acquired automatically from rawtext.
We used raw data from the BNC to create the IC files.
There are various parametersthat can be set in the WordNet Similarity Package when creating these files; we usedthe RESNIK method of counting frequencies in WordNet (Resnik 1995), the stop wordsprovided with the package, and no smoothing.The lesk score is applicable to all parts of speech, whereas the jcn is applicableonly to nouns and verbs because it relies on IC counts which are obtained using thehyponym links and these only exist for nouns and verbs.15 However, we did not usejcn for verbs because in previous experiments (McCarthy et al 2004c) the lesk measureoutperformed jcn because the structure of the hyponym hierarchy is very shallow forverbs and the measure is therefore considerably less informative for verbs than it is fornouns.4.3 An ExampleWe illustrate the application of our measure with an example.
For star, if we set16 k = 4and have the dss for the previously given neighbors as in the first row of Table 6, and15 For verbs these pointers actually encode troponymy, which is a particular kind of entailment relation,rather than hyponymy.16 In this example, as before, we set k to 4 for the sake of brevity.567Computational Linguistics Volume 33, Number 4Table 6Example dss and sss scores for star and its neighbors.Neighbors of star (dss)Senses actor (0.22) footballer (0.12) planet (0.08) circle (0.03)celebrity 0.42 0.53 0.02 0.01celestial body 0.01 0.01 0.68 0.10shape 0.0 0.0 0.02 0.78zodiac 0.03 0.03 0.21 0.01Total 0.46 0.57 0.93 0.90the sss between the senses and the neighbors as in the remaining rows, the prevalencescore for celebrity would be:= 0.22 ?
0.420.46 + 0.12 ?
0.530.57 + 0.08 ?
0.020.93 + 0.03 ?
0.010.90= 0.2009 + 0.1116 + 0.0017 + 0.0003= 0.3145The prevalence score for each of the senses would be:prevalence score(celebrity) = 0.3145prevalence score(celestial body) = 0.0687prevalence score(shape) = 0.0277prevalence score(zodiac) = 0.0390so the method would select celebrity as the predominant sense.5.
Experimental Setup5.1 The Distributional ThesaurusesThe three thesauruses used in our first two experiments were all created automaticallyfrom raw corpus data, based either on grammatical relations between words computedby syntactic parsers or alternatively on word proximity relations.We created the first thesaurus, which we call BNC, from grammatical relation outputproduced by the RASP system (Briscoe and Carroll 2002) applied to the 90M words ofthe ?written?
portion of the British National Corpus (Leech 1992), for all polysemousnouns, verbs, adjectives, and adverbs in WordNet.
For each word we considered co-occurring words in the grammatical contexts listed in Table 7.In the first two experiments, we also use two further automatically computeddistributional thesauruses, produced by Dekang Lin from 125M words of text from theWall Street Journal, San Jose Mercury News, and AP Newswire, using the same similaritymeasure.
The thesauruses are publicly available.17 One was constructed based on word17 The thesauruses are available for download from http://www.cs.ualberta.ca/~lindek/downloads.htm.568McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesTable 7Grammatical contexts used for acquiring the BNC thesaurus.PoS Grammatical contextsnoun verb in direct object or subject relation, adjective or noun modifierverb noun as direct object or subjectadjective modified noun, modifying adverbadverb modified adjective or verbTable 8Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6.PoS Thesaurus types NISC NITHnoun BNC 7,090 2,436 115noun DEP 6,583 2,176 217noun PROX 6,582 2,176 217verb BNC 2,958 553 45adjective BNC 3,659 1,208 123adverb BNC 505 132 38similarities computed from syntactic dependencies produced by MINIPAR (Lin 1998b),and the other was constructed based on textual proximity relationships between words.We refer below to the original corpus as NEWSWIRE, and these two thesauruses as DEPand PROX, respectively.
We restricted our experiments to the nouns in these thesauruses.Table 8 contains details of the numbers of polysemous (according to WordNet 1.6)words contained in these thesauruses, the number of words in SemCor that were notfound in these thesauruses (NITH) and the number of words in the thesauruses thatwere not in SemCor (NISC).For the experiments described in Sections 6.3 and 6.4 we use exactly the samemethod as that proposed for the BNC thesaurus, however the data source is differentand is described in those sections.5.2 The Sense InventoryWe use WordNet version 1.6 as the sense inventory for our first three experiments, and1.7.1 for our last experiment.18For sss?
we use the WordNet Similarity Package version 0.05 (Patwardhan andPedersen 2003).18 We use 1.6 which is a rather old version of WordNet so that we can directly evaluate on the SemCor datareleased with this version; we also use it to enable comparison with the results of McCarthy et al (2004a).We use WordNet 1.7.1 for the fourth experiment, because this is the version that was used for annotatingthe test data in that experiment.
We plan to move to more recent versions of WordNet and experimentwith other sense inventories in the future.569Computational Linguistics Volume 33, Number 46.
ExperimentsIn this section we describe four experiments using our method for acquiring predomi-nant sense information.The first experiment evaluates automatically acquired predominant senses for allparts of speech, using SemCor as the test corpus.
This extends previous work whichhad only evaluated all PoS on Senseval-2 (Cotton et al 2001) and Senseval-3 (Mihalceaand Edmonds 2004) data.
The SemCor corpus is composed of 220,000 words, in contrastto the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words).
Weexamine the effects of using the two different semantic similarity scores that performedwell in previous work: jcn is quick to compute but lesk has the advantage that it isapplicable to all PoS and can be implemented for any dictionary with sense defini-tions.
We compare three thesauruses: one is derived from the BNC and two from theNEWSWIRE corpus.
The two from the NEWSWIRE corpus examine the requirement fora parser by contrasting results obtained when the thesaurus is built using parsed datacompared to a proximity approach.
We contrast the results of the BNC thesaurus witha simplified version of the prevalence score which uses the number of the k neighborsclosest to a sense for ranking without using the dss and without sharing the credit fora neighbor between senses.
We also perform an error analysis on a random sampleof words for which a predominant sense was found that differed from that given bySemCor, identifying and giving an indication of the frequencies of the main sources oferror.The second experiment is on nouns in the Senseval-2 all-words data, again usingpredominant senses acquired using each of the three distributional thesauruses, butin this experiment we explore the benefits of an automatic first sense heuristic whenthere is inadequate data in available resources.
Although McCarthy et al (2004c) showthat on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCoroutperforms the automatic method, we look at whether the method?s performance isrelatively stronger on words for which there is little data in SemCor.
This is importantbecause, as we have shown in Table 5, low frequency words are used often in sensesother than the sense that is ranked first according to SemCor.In addition to the issue of lack of coverage of manually annotated resources, sensefrequency will depend on the domain of the data.
In the third experiment, we revisitsome previous work on noun senses and domain (McCarthy et al 2004a) using corporaof news text about sports and finance.
Using distributional thesauruses computed fromthese corpora and a gold standard domain labeling of word senses we look at thepotential for computing domain-specific predominant senses for parts of speech otherthan nouns.Continuing the line of research on automatic acquisition of domain-specific pre-dominant senses, the fourth experiment compares results when we train and test ondomain-specific corpora, where the training data is (1) manually categorized for domainand from the same corpus as the gold-standard test data, and (2) where the training datais harvested automatically from another corpus which is categorized automatically.6.1 Experiment 1: All Parts of SpeechIn this experiment, we evaluate the accuracy of automatically acquired predominantsenses for all open class parts of speech, taking SemCor as the gold standard.
For nounswe use the semantic similarity measures lesk and jcn, and for other parts of speech, lesk.We use the three distributional thesauruses BNC, DEP, and PROX.570McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesThe gold standard is derived from the Brown Corpus files publicly released as partof SemCor, rather than the processed data provided in the cntlist file in the WordNetdistribution.
The released SemCor files contain only the tagged data from the BrownCorpus and do not include data from The Red Badge of Courage.
We use the released datarather than that in cntlist because this includes the actual tagged examples which aremarked for genre by the Brown files.
We envisage the possibility of further experimentswith these genre markers.
We only evaluate on instances where a single, unique senseis supplied by the annotators.
So, for example, we ignore instances like the followingwith multiple wnsn values:<wf cmd=done pos=NN lemma=tooth wnsn=3;1 lexsn=1:05:02::;1:08:00::>tooth</wf>We also only evaluate on polysemous words (according to WordNet) having one sensein SemCor which is more frequent than any other, and for which both SemCor and ourthesauruses have at least a minimal amount of data.
Specifically, a word must occurthree or more times in SemCor; it must also occur in ten or more grammatical relationsin the parsed version of the BNC and have neighbors in the distributional thesaurus, orbe present in Dekang Lin?s thesaurus.19We evaluate on nouns, verbs, adjectives, and adverbs separately, computing a num-ber of accuracy measures, both type-based and token-based.
PSacc is calculated overword types in SemCor which have one sense which occurs more than any other.
It is theaccuracy of identifying the predominant sense in SemCor.
If the automatic ranking hasa tie for the top ranked sense then we score that word as incorrect.20 So we havePSacc =|correcttyp||typesmf |?
100 (9)where typesmf are the types in SemCor such that one sense is more frequent thanany other, the word has occurred at least three times in SemCor and has an entryin the thesaurus.
|correcttyp| is the number of these where the automatically acquiredpredominant sense matches the first sense in SemCor.PSaccBL is the predominant sense random baseline, obtained as follows:PSaccBL =?w?typesmf1|senses(w)||typesmf |?
100 (10)WSDsc is a token-based measure.
It is the WSD accuracy that would be obtainedby using the first sense heuristic with the automatically acquired predominant senseinformation, in cases where there was a unique automatic top ranked sense:WSDsc =|correcttok||SCtokensafs|?
100 (11)19 Although we do not evaluate words for which there were no neighbors in the thesaurus, we could extendthe thesaurus to include some of these by widening the range of grammatical relations covered andcompensating for some systematic PoS tagging errors.20 If we exclude these words with joint top ranking from the automatic method (precision rather than recall)then we obtain marginally higher accuracy for the jcn measure but no difference for lesk.571Computational Linguistics Volume 33, Number 4where |correcttok| is the number of tokens disambiguated correctly out of the tokens inSemCor having an automatically acquired first sense (SCtokensafs).SC FS is the WSD accuracy of the SemCor first sense heuristic on the same set oftokens (SCtokensafs), which is the upper bound because the information it uses is derivedfrom the test data itself.
RBL is the random baseline for the WSD task, calculated bysplitting the credit for each token to be tagged in the test data evenly between all of theword?s senses.RBL =?w?SCtokensafs1|senses(w)||SCtokensafs|?
100 (12)The results are shown in Table 9.
We examined differences between the semanticsimilarity measures (lesk and jcn), the BNC and DEP thesauruses, and the DEP andPROX thesauruses using the ?2 test of significance with one degree of freedom (Siegeland Castellan 1988).
None of the differences between the different combinations ofsimilarity measures and thesauruses for the type-based measure PSacc are significant.The differences between lesk and jcn are significant for the token-based measure WSDscfor both the BNC and PROX thesauruses (both p < .001), however not when comparinglesk and jcn for the DEP thesaurus.
Although lesk is more accurate than jcn, at least onthe WSD task, jcn is much faster because of the precompilation of IC in the WordNetsimilarity package; however, lesk has the additional benefit of being applicable to otherparts of speech.
The method gives particularly good results for adjectives, given thatthey have a similar random baseline to nouns.
It does not do so well for adverbs andverbs, but still performs well above the random baseline which is low for verbs dueto their high degree of polysemy.
Given that the first sense heuristic from SemCor isparticularly strong for adverbs, it is disappointing that the automatic method does notperform as well as it does on adjectives.
One possible reason for this might be thatadverbs are often less strongly associated to the verbs that they modify than adjectivesare to the nouns that they modify, so the distributional thesaurus information is lessreliable.
Another reason may be that less data are available for adverbs, both in thethesaurus and also in WordNet.Table 9Evaluation on SemCor, polysemous words only.Type TokenPoS Settings No.
PSacc PSaccBL No.
WSDsc SC FS RBLnoun lesk BNC 2,555 54.5 32.3 53,468 48.7 68.6 24.7noun lesk DEP 2,437 56.3 32.1 52,158 49.2 68.4 24.6noun lesk PROX 2,437 55.9 32.1 52,158 49.0 68.4 24.6noun jcn BNC 2,555 54.0 32.3 53,429 46.1 68.6 24.7noun jcn DEP 2,436 56.4 32.1 52,122 48.8 68.4 24.6noun jcn PROX 2,436 55.9 32.1 52,117 47.7 68.4 24.6verb lesk BNC 1,149 45.6 27.1 31,182 36.1 57.1 17.1adjective lesk BNC 1,154 60.4 32.8 18,216 56.8 73.8 24.9adverb lesk BNC 230 52.2 39.9 8,810 43.2 76.1 33.0572McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesComparing the results for the DEP and the PROX thesauruses, we see that althoughthere is no significant difference in PSacc (with either lesk or jcn), there is for WSDscwhen using jcn (p < .001), but not when comparing the lesk values for these thesauruses.Even though the differences between jcn DEP and jcn PROX are significant, the absolutedifferences are nevertheless relatively small; this bodes well for applying the automaticpredominant sense method to languages less well resourced than English, becausethe PROX thesaurus was produced without using a parser.
The differences in resultsbetween jcn BNC and jcn DEP for nouns are statistically significant (p < .001).21 Thebetter accuracy with DEP may be because the NEWSWIRE corpus is larger than theBNC.
We intend to investigate the effects of corpus size in the future.
The differences inresults between lesk BNC and lesk DEP for nouns are not significant.6.1.1 Results Using Simplified Prevalence Score.
A simple variation of our method is just toassociate each neighbor with just one sense and use the number of neighbors associatedwith a sense for the prevalence score.
This gives a modified version of Equation (1)where each sense si ?
senses(w) is assigned a score as follows:Simplified Prevalence Score(w, si) = |{nj ?
Nw} : arg maxsk?senses(w)(sss(sk, nj)) = si| (13)wheresss(sk, nj) = maxsx?senses(nj )sss?
(sk, sx) (14)For the example in Table 6, celebrity would get the top score of 2 (due to it havingthe highest sss for actor and footballer), celestial body would get a score of 1 (due to itssss with planet), shape would get 1 (due to circle), and zodiac would obtain a SimplifiedPrevalence Score of 0 because it does not have the highest sss for any of the neighbors.As the results from Table 10 show, we do not get such good results with this score.This supports our intuition that a combination of both the number of neighbors andtheir distributional similarity scores is important for determining predominance.
Therest of the article gives results and analysis for our original prevalence score as given inEquation (1).6.1.2 Error Analysis.
We took a random sample of 80 words that occurred more than fivetimes in SemCor, 20 words for each PoS, from those where the automatically identifiedpredominant sense was different from the SemCor first sense when using the lesk sssand BNC thesaurus and our ranking score as defined in Equation (1) (i.e., the datarepresented by the first result line and the last three result lines of Table 9).
Herein, wecall the automatically identified sense AUTO FS, and the SemCor sense SemCor FS.
We21 The coverage of the SemCor data by the DEP and PROX thesauruses is slightly lower than that of theBNC-derived thesaurus due to mismatches in spelling and capitalization and also probably because theNEWSWIRE corpus is narrower in genre and domain than the BNC.573Computational Linguistics Volume 33, Number 4Table 10Simplified prevalence score, evaluation on SemCor, polysemous words only.Type TokenPoS Settings No.
PSacc PSaccBL No.
WSDsc SC FS RBLnoun lesk BNC 2,555 52.9 32.3 53,175 47.2 68.6 24.7noun jcn BNC 2,555 50.1 32.3 52,033 46.7 69.2 24.8verb lesk BNC 1,149 45.1 27.1 30,364 36.7 58.0 17.4adjective lesk BNC 1,154 58.3 32.8 18,136 56.0 73.7 24.8adverb lesk BNC 230 50.0 39.9 8,802 42.2 76.1 33.0manually inspected the data for each of the words to find the source of the problem.We did not have the (substantial) resources that would be required to sense tag alloccurrences of these words in the BNC to see what their actual first senses were.
Instead,we examined the parses, grammatical relations, and sense definitions for the words tosee why the AUTO FS was ranked above the SemCor FS.
We found the following maintypes of error:22corpora The difference appears to be due to genuine divergence between the BNCand SemCor.
For this error type we looked at the BNC parses to see if the acquiredpredominant sense was clearly due to differences in the corpus data.
There maybe other errors that should have been assigned this category, but without accessto sense tagged BNC data we could not be sure of this, so we used this categoryconservatively.
An example of this error is the adjective solid which has the goodquality first sense in the Brown files in SemCor, but the firm sense according toour BNC automatic ranking.related The automatic predominant sense is closely related to the SemCor first sense.Although many word senses are related to some extent, the category was pickedwhere a close relationship seemed to be the main cause of the error.
An exampleis the noun straw which has two senses in WordNet 1.6, fibre used for hats andfodder and plant material.
The SemCor FS was the former whereas our AUTO FSwas the latter.competing Two or more related senses are ranked highly but they are overtaken byan unrelated sense.
For example, the ranking and scores for the noun transmissionare:WordNet sense Description Prevalence score5 gears 1.792 communication 1.201 act of sending a message 1.193 fraction of radiant energy 0.484 infection 0.1522 There were a few other, less numerous types of error, for example systematic PoS mis-tagging of particles(such as down) as adverbs.574McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesThe act of sending a message sense is overtaken by the gears sense because thecredit from shared distributional neighbors is split between it and the communi-cation sense.neighbors There are not many neighbors related to the sense.
There can be variousreasons for this, such as the sense having restricted contexts of occurrence or onlya small number of near synonyms existing for the sense.
An example of this isthe adjective live where the SemCor FS unrecorded sense seems to occur in theBNC corpus more than the alive sense; there are plenty of grammatical relationspertaining to this sense, but there are few distributional neighbors near in meaningto unrecorded.23spurious similarity The WordNet similarity scores were misled by spurious relation-ships to neighbors; this can occur in dense areas such as the ?physical object?region of the noun hyponym hierarchy.
An example of this is the verb tap whichhas neighbors push and press which are related to the AUTO FS (solicit) as well asthe SemCor FS (strike lightly).The results of the error analysis are shown in Table 11.
The analysis shows thatdifferences between the training (BNC) and testing (SemCor) corpora are not a majorsource of error.
Although SemCor itself (the released files from the Brown corpuscomprising only 200,000 words) is not large enough to build a thesaurus with entriesfor a reasonable portion of the words, we did build a thesaurus from the entire Browncorpus (1 million words) to see the effect of corpus data.
The results are comparedto those from the BNC in Table 12 on the set of words which had thesaurus entriesin the Brown data (to make the results more comparable, because the corpora are ofsuch different sizes).
We also show the average results for 10 random selections of a1 million word random sample of the BNC.
To do this we randomly selected 190 th of thetuples.24 The differences in the WSDsc for the BNC 190 sample and the Brown corpus aresignificant (p < .01 on the ?2), but the differences in PSacc are not significant.
Althoughthe entire BNC produced better results than the Brown data, this is undoubtedly due tothe difference in size of the corpus.
Taking a comparably sized sample, the results areslightly better from Brown which is the corpus from which SemCor is taken.For nouns, it was apparent that in two cases less-prevalent senses were receiving ahigher ranking simply because the credit for some neighbors associated with anothermeaning was split between related senses (error type competing).
This was not ob-served for other parts of speech, possibly because the AUTO FS was rarely unrelated tothe SemCor FS.There were some problems arising from spurious similarity.
One possible sourceof such problems is due to the ambiguity of the neighbor; in the future we will lookat reducing this source of error by removing neighbors which have a value for sx inEquation (2) which is not the same as that preferred by the other senses of the targetword (w).
For adverbs, all the cases that were categorized as spurious similarity werealso noted to be related to the SemCor FS, though they were not categorized as relatedas this was not considered the primary cause of the error.The analysis was hardest for verbs.
Verbs are on average highly polysemous, andoften have senses that are related.
Furthermore, the structure of the WordNet verb tro-ponym hierarchy is very shallow compared to the noun hyponymy hierarchy, so there23 The closest neighbors to the adjective live are adult, forthcoming, lively, solo, excellent, stuffed, living, dead,and australian weekly.24 The variance for the 190 sample for PSacc was 0.46 and for WSDsc it was 0.49.575Computational Linguistics Volume 33, Number 4Table 11Results of the error analysis for the sample of 80 words.PoSnoun verb adjective adverb All PoScorpora 1 2 1 1 5related 8 12 13 8 41competing 2 0 0 0 2neighbors 4 3 2 2 11spurious similarity 5 3 4 9 21Table 12SemCor results for Nouns using jcn.Thesaurus PSacc% WSDsc %full BNC 53.8 44.9190 BNC 46.6 40.8Brown 47.2 41.7are more possibilities for spurious similarities from overlap of glosses.
So, although wetried to identify the main problem source, for verbs the problems usually arose from acombination of factors and the relatedness of the senses was usually one of these.Relatedness of senses and fine-grained distinctions are major sources of error.
Therehave been various attempts to group WordNet senses both manually and automati-cally (Agirre and Lopez de Lacalle 2003; McCarthy 2006; Palmer, Dang, and Fellbaum2007).
Indeed, McCarthy demonstrated that distributional and semantic similarity canbe used for relating word senses and that such methods increase accuracy of firstsense heuristics, including the automatic method proposed here.
WSD is improved withcoarser-grained inventories but ultimately, performance depends on the application.For example, the noun bar has 11 senses in WordNet 1.6.
These include the pub senseas well as the counter sense and these are related to a certain extent.
One might wantto group them when acquiring predominant senses, but there may be situations wherethey should be distinguished.
For example, if one were to ask a robot to ?go to the bar?one would hope it could use the context to go get the drinks rather than replying that itis already there!
Even in cases where fine-grained distinctions are ultimately required, itmay be helpful to have a coarse-grained prior and then use contextual features to teaseapart subtle sense distinctions.From our error analysis, the problem of semantically isolated senses (identified asneighbors) was not a major source of error, but still causes some problems.
One possibleremedy might be to identify these cases by looking for neighbors which relate stronglyto a sense which none of the other neighbors relate to and weighting the contributionfrom these neighbors more.
This may however give rise to further errors because of thenoise introduced by focusing on individual neighbors.
We will explore such directionsin future work.576McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesIn this experiment we did not assign any credit for near misses.
In many casesof error the SemCor FS nonetheless received a high prevalence score.
In the futurewe hope to use the score for probability estimation, and combine this with contextualinformation for WSD as in related work by Lapata and Brew (2004) and Chan and Ng(2005).6.2 Experiment 2: Frequency and the SemCor First Sense HeuristicIn the previous section we described an evaluation of the accuracy of automaticallyacquired predominant sense information.
We carried out the evaluation with respect toSemCor in order to have as much test data as possible.
To obtain reasonably reliablegold-standard first-sense data and first-sense heuristic upper bounds, we limited theevaluation to words occurring at least three times in SemCor.
Clearly this scenario isunrealistic.
For many words, and particularly for nouns, there is very little or no data inSemCor; Table 2 shows that 81.9% of nouns (excluding multiwords) listed in WordNetdo not occur at all in SemCor.
Thus, even for English, which has substantial manuallysense-tagged resources, coverage is severely limited for many words.For a more realistic comparison of automatic and manual heuristics, we thereforenow change to a different test corpus, the Senseval-2 English all-words task data set.
Wefocus on nouns and evaluate using all words regardless of their frequencies in SemCor.We examine the effect of frequency in SemCor on performance of a SemCor-derivedheuristic in comparison to results from our automatic method on the same words.
Ourhypothesis is that although automatically acquired predominant sense information maynot outperform first-sense data obtained from a hand-tagged resource over all words ina text, the information may well be more accurate for low frequency items.We use a mapping between different WordNet versions25 (Daude?, Padro?, and Rigau2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sensenumbers) with 1.6 sense numbers.
As well as examining the performance of our methodin contrast to the SemCor heuristic, we calculate an upper bound for this using thefirst sense heuristic from the Senseval-2 all-words data itself.
This is obtained for nounswith two or more occurrences in the Senseval-2 data and where one sense occurs morethan any of the others.
We calculate type, precision, and recall, using this Senseval-2first-sense as the gold standard.
The recall measure is the same as PSacc describedpreviously, except that we include items which do not have entries in the thesaurus,scoring them incorrect.
Precision only includes items where there is a sense rankedhigher than any other for that word with the prevalence score, that is, it does not includeitems with a joint automatic ranking.
We also calculate token precision and recall (WSD).These measures relate to WSDsc, but again, recall includes words not in the thesauruswhich are scored incorrect, and precision does not include items with a joint automaticranking.
We also separately compute WSD precision for words not in SemCor (NISC).The results are shown in Table 13.26The automatically acquired predominant sense results (the first six lines of resultsin the table) are approaching the SemCor-derived results (third line from the bottom ofthe table).
The NISC results are particularly encouraging, but with the caveat that thereare only 17 such words in the data.
The precision for these items is higher than the25 This mapping is available at http://www.lsi.upc.es/~nlp/tools/mapping.html.26 Note that these figures are lower than those of McCarthy et al (2004a) in a similar experiment becausethe evaluation here is only on polysemous words.577Computational Linguistics Volume 33, Number 4Table 13Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-wordstask data.Type WSD/tokenSettings Precision (%) Recall (%) Precision (%) Recall (%) Precision NISC (%)lesk BNC 56.3 53.7 54.6 53.4 58.3lesk DEP 52.0 47.2 52.6 48.7 58.3lesk PROX 52.0 47.2 52.3 48.5 58.3jcn BNC 52.4 50.0 51.8 50.6 66.7jcn DEP 52.0 47.2 58.0 53.7 83.3jcn PROX 53.1 48.1 57.3 53.1 83.3SemCor 64.8 63.0 58.5 57.3 0.0Senseval-2 ?
?
90.8 60.1 100.0RBL 26.5 26.5 26.0 26.0 50.0overall figure.
This is because the nouns involved are less frequent so tend to be lesspolysemous and consequently have a higher random baseline.
There are a few nounsthat are not in the automatic ranking, but this is due to the fact that neighbors werenot collected for these nouns in the thesaurus because of tagging or parser errors orthe particular set of grammatical relations used.
It should be possible to extend therange of grammatical relations, or use proximity-based relations, so that neighbors canbe obtained in these cases.
It would also be possible to assign some credit in the case ofjoint top ranked senses to increase coverage.Looking at Table 13 in more detail, it seems to be the case that although the BNCthesaurus does well in identifying the first sense of a word (the type results), the PROXand DEP thesauruses from the NEWSWIRE corpus return better WSD results whenused with the jcn measure.
This is possibly because jcn works well for more frequentitems due to its incorporation of frequency information, and the NEWSWIRE corpushas more data for frequent words, although coverage is not as good as the BNC asseen by the bigger differences in precision and recall and the figures in Table 8.
Thelower coverage may be due to the narrower domain and genre of the NEWSWIREcorpus, though spelling and capitalization differences probably also account for somedifferences.Table 14 shows results on the Senseval-2 nouns for the best similarity measureand thesaurus combinations in Table 13 for nouns at or below various frequenciesin SemCor.
(The differences between the DEP and PROX thesauruses are negligible atfrequencies of 10 or below, so for those we report only the results for DEP.)
As weanticipated, for low frequency words the automatic methods do give more accuratepredominant sense information than SemCor.
The low number of test items at frequencyfive or less means that results for jcn with the BNC thesaurus are not significantly betterwhen compared with SemCor (p = .05); however the lesk WSD results are significantlybetter (p < .01 for the ?
1 threshold and p < .05 for the ?
5 threshold).
On the whole, wesee that the automatic method, using either jcn or lesk and any of the three thesauruses,tend to give better results than SemCor on nouns which have low coverage in SemCor.Figures 2 and 3 show the precision for type and token (WSD) evaluation where theitems have a frequency at or below given thresholds in SemCor.
Although the manually578McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesTable 14Senseval-2 results, polysemous nouns only, broken down by their frequencies of occurrence inSemCor.Type WSD/tokenNo.
of occurrences in Precision Recall Precision RecallSemCor (no.
of words) Settings (%) (%) (%) (%)jcn BNC 100.0 33.3 66.7 47.1lesk BNC 100.0 33.3 58.3 41.20 (17) jcn DEP 100.0 33.3 83.3 58.8lesk DEP 100.0 33.3 58.3 41.2SemCor 0.0 0.0 0.0 0.0Senseval-2 ?
?
100.0 52.9RBL 38.9 38.9 46.1 46.1jcn BNC 66.7 44.4 54.1 45.5lesk BNC 83.3 55.6 67.6 56.8jcn DEP 50.0 22.2 51.7 34.1?
1 (44) lesk DEP 75.0 33.3 69.0 45.5SemCor 50.0 33.3 33.3 20.5Senseval-2 ?
?
93.3 63.6RBL 40.7 40.7 42.8 42.8jcn BNC 80.0 61.5 63.0 57.5lesk BNC 90.0 69.2 71.2 65.0?
5 (80) jcn DEP 71.4 38.5 56.7 42.5lesk DEP 85.7 46.2 70.0 52.5SemCor 60.0 46.2 54.0 42.5Senseval-2 ?
?
95.9 58.8RBL 38.1 38.1 39.1 39.1jcn BNC 75.0 63.2 59.3 55.8lesk BNC 68.8 57.9 62.8 59.2?
10 (120) jcn DEP 66.7 42.1 56.8 45.0lesk DEP 58.3 36.8 58.9 46.7SemCor 68.8 57.9 57.3 49.2Senseval-2 ?
?
96.8 50.8RBL 37.5 37.5 38.0 38.0jcn BNC 76.0 67.9 66.7 64.8lesk BNC 64.0 57.1 71.6 69.6?
15 (250) jcn DEP 60.0 42.9 68.8 55.6lesk DEP 55.0 39.3 67.3 54.4jcn PROX 70.0 50.0 72.3 58.4lesk PROX 55.0 39.3 66.8 54.0SemCor 64.0 57.1 66.5 62.0Senseval-2 ?
?
98.8 68.4RBL 32.9 32.9 30.4 30.4jcn BNC 52.4 50.0 51.8 50.6lesk BNC 56.3 53.7 54.6 53.4all (786) jcn DEP 52.0 47.2 58.0 53.7lesk DEP 52.0 47.2 52.6 48.7jcn PROX 53.1 48.1 57.3 53.1lesk PROX 52.0 47.2 52.3 48.5SemCor 64.8 63.0 58.5 57.3Senseval-2 ?
?
90.8 60.1RBL 26.5 26.5 26.0 26.0579Computational Linguistics Volume 33, Number 4Figure 2?TYPE?
precision on finding the predominant sense for the Senseval-2 English all-words testdata for nouns having a frequency less than or equal to various thresholds.produced SemCor first-sense heuristic outperforms the automatic methods over all thetest items (see the ?all?
results in Table 14), when items are below a frequency thresholdof five the automatic methods give better results.
Indeed, as the threshold is movedup to 20 and even 30, more nouns are covered, and the automatic methods are stillcomparable and in some cases competitive with the SemCor heuristic.6.3 Experiment 3: The Influence of DomainIn this experiment, we investigate the potential of the automatic ranking method forcomputing predominant senses with respect to particular domains.
We have previ-ously demonstrated that the method produces intuitive domain-specific models fornouns (McCarthy et al 2004a), and that these can be more accurate than first senses de-rived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005).Here we investigate the behavior for other parts of speech, using a similar experimentalsetup to that of McCarthy et al That work used the subject field codes (SFC) (Magniniand Cavaglia` 2000)27 as a gold standard.
In SFC the Princeton English WordNet isaugmented with some domain labels.
Every synset in WordNet?s sense inventory isannotated with at least one domain label, selected from a set of about 200 labels.
Theselabels are organized in a tree structure.
Each synset of WordNet 1.6 is labeled with oneor more labels.
The label factotum is assigned if any other is inadequate.
The first levelconsists of five main categories (e.g., doctrines and social science) and factotum.doctrines27 More recently referred to as WordNet Domains (WN-DOMAINS).580McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesFigure 3WSD precision on the Senseval-2 English all-words test data for nouns having a frequency lessthan or equal to various thresholds.has subcategories such as art, religion, and psychology.
Some subcategories are furtherdivided in subcategories (e.g., dance, music, and theatre are subcategories of art).McCarthy et al (2004a) used two domain-specific corpora for input to the methodfor finding predominant senses.
The corpora were obtained from the Reuters Corpus,Volume 1 (RCV1; Rose, Stevenson, and Whitehead 2002) using the Reuters topic codes.The two domain-specific corpora were:SPORTS (Reuters topic code GSPO), 9.1 million wordsFINANCE (Reuters topic codes ECAT and MCAT), 32.5 million wordsIn that work we produced sense rankings for a set of 38 nouns which have atleast one synset with an economy SFC label and one with a sport SFC label.
We thendemonstrated that there were more sport labels assigned to the predominant sensesacquired from the SPORTS corpus and more economy labels assigned to those from theFINANCE corpus.
The predominant senses from both domains had a similarly highpercentage of factotum (domain-independent) labels.
We reproduce the results here (inFigure 4) for ease of reference, and for comparison with other results presented in thissection.
The y-axis in this figure shows the percentage of the predominant sense labelsfor these 38 nouns that have the SFC label indicated by the x-axis.We envisaged running the same experiment with verbs, adjectives, and adverbs,although we suspected that these would show less domain-specific tendencies andthere would be fewer candidate words to work with.
The SFC labels for all senses ofpolysemous words (excluding multiwords) in the various parts of speech are shown inTable 15.
We see from the distribution of factotum labels across the parts of speech thatnouns are certainly the PoS most likely to be influenced by domain.To produce results like Figure 4 for each PoS, we needed words having at least onesynset with a sport label and one with an economy label.
There were 20 such verbs but581Computational Linguistics Volume 33, Number 4Figure 4Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using theSPORTS and FINANCE corpora.only two adjectives and no adverbs meeting this condition.
We therefore performedthe experiment only with verbs.
To do this we used the SPORTS and FINANCE corporaas before, computing thesauruses for verbs using the grammatical relations specifiedin Table 7.
The results for the distribution of domain labels of the predominant sensesTable 15Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.Domain % Domain %noun biology 29.3 verb factotum 67.0factotum 20.7 psychology 3.5art 6.2 sport 2.9sport 3.1 art 2.5medicine 3.1 biology 2.5other 37.6 other 21.6adjective factotum 67.8 adverb factotum 81.4biology 6.5 psychology 7.5art 3.2 art 1.8psychology 2.7 physics 1.1physics 1.9 economy 1.1other 17.9 other 7.1582McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Sensesacquired from the SPORTS and FINANCE corpora are shown in Figure 5.
We see the sametendency for sport labels for predominant senses from the SPORTS corpus and economylabels for the predominant senses from the FINANCE corpus, but the relationship isless marked compared with nouns because of the high proportions of factotum sensesin both corpora for verbs.
We believe that acquisition of domain-specific predominantsenses should be focused on those words which show domain-specific tendencies.
Wehope to put more work into automatic detection of these tendencies using indicatorssuch as domain salience and words that have different sense rankings in a given domaincompared to the BNC (as discussed by Koeling, McCarthy, and Carroll 2005).6.4 Experiment 4: Domain-Specific Predominant Sense AcquisitionIn the final set of experiments we evaluate the acquired predominant senses for domain-specific corpora.
The first of the two experiments was reported by Koeling, McCarthy,and Carroll (2005), but we extend it by the second experiment reported subsequently.Because there are no publicly available domain-specific manually sense-tagged corpora,we created our own gold standard.
The two chosen domains (SPORTS and FINANCE) andthe domain-neutral corpus (BNC) are the same as we used in the previous experiment.We selected 40 words and we sampled (randomly) sentences containing these wordsfrom the three corpora and asked annotators to choose the correct sense for the targetwords.
The set consists of 17 words which have at least one sense assigned an economydomain label and at least one sense assigned a sports label: club, manager, record, right,bill, check, competition, conversion, crew, delivery, division, fishing, reserve, return, score,receiver, running; eight words that are particular salient in the SPORTS domain: fan,star, transfer, striker, goal, title, tie, coach; eight words that are particular salient in theFigure 5Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using theSPORTS and FINANCE corpora.583Computational Linguistics Volume 33, Number 4Table 16WSD using predominant senses, training, and testing on all domain combinations(hand-classified corpora).TestingTraining BNC FINANCE SPORTSBNC 40.7 43.3 33.2FINANCE 39.1 49.9 24.0SPORTS 25.7 19.7 43.7Random BL 19.8 19.6 19.4SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)FINANCE domain: package, chip, bond, market, strike, bank, share, target; and seven wordsthat are equally salient in both domains: will, phase, half, top, performance, level, country.Koeling, McCarthy, and Carroll (2005) give further details of the construction of the goldstandard.In the first experiment, we train on a corpus of documents with manually assigneddomain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test ondata from the same source.
In a second experiment we build a text classifier, use thetext classifier to obtain SPORTS and FINANCE corpora (using general newswire text fromthe English Gigaword Corpus; Graff 2003) and test on the gold-standard data from theReuters corpus.
The second experiment eliminates issues about dependencies betweentraining and test data and will shed light on the question of how robust the acquiredpredominant sense method is with respect to noise in the input data.
At the same time,the second experiment paves the way towards creating predominant sense inventoriesfor any conceivable domain.6.4.1 Experiment Using Hand-Labeled Data.
In this section we focus on the predominantsense evaluation of the experiments described by Koeling, McCarthy, and Carroll (2005).After running the predominant sense finding algorithms on the raw text of the two do-main corpora (SPORTS and FINANCE) and the domain-neutral corpus (BNC), we evaluatethe accuracy of performing WSD on the sample of 40 words purely with the first senseheuristic using all nine combinations of training and test corpora.
The results (as givenin Table 16) are compared with a random baseline (?Random BL?
)28 and the accuracyusing the first sense heuristic from SemCor (?SemCor FS?
).29The results in Table 16 show that the best results are obtained when the predominantsenses are acquired using the appropriate domain (i.e., test and training data from thesame domain).
Moreover, when trained on the domain-relevant corpora, the randombaseline as well as the baseline provided by SemCor are comfortably beaten.
It can beobserved from these results that apparently the BNC is more similar to the FINANCEcorpus than it is to the SPORTS corpus.
The results for the SPORTS domain lag behind theresults for the FINANCE domain by almost 6 percentage points.
This could be because28 The random baseline is?i?tokens1#senses(i) .29 The precision is given alongside in brackets because a predominant sense for the word striker is notsupplied by SemCor.
The automatic method proposes a predominant sense in every case.584McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesTable 17WSD using predominant senses, training, and testing on all domain combinations (automaticallyclassified corpora).TestingTraining BNC FINANCE SPORTSBNC 40.7 43.3 33.2FINANCE 38.2 44.0 29.0SPORTS 27.0 23.4 45.0Random BL 19.8 19.6 19.4SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)of the smaller amount of training data available (32M words versus 9M words), but itcould also be an artifact of this particular selection of words.6.4.2 Experiment Using Automatically Classified Data.
Although the previous experimentshows that it is possible to acquire domain-specific predominant senses successfully, theusefulness of doing this will be far greater if there is no need to classify corpora withrespect to domain by hand.
There is no such thing as a standard domain specificationbecause the definition of a domain depends on user and application.
It would beadvantageous if we could automatically obtain a user-/application-specific corpus fromwhich to acquire predominant senses.In this section we describe an experiment where we build a text classifier usingWordNet as a sense inventory and the SFC domain extension (see Section 6.3).
Weextracted bags of domain-specific words from WordNet for all the defined domains bycollecting all the word senses (synsets) and corresponding glosses associated with eachdomain label.
These bags of words are the fingerprints for the domains and we usedthem to train a Support Vector Machine (SVM) text classifier using TwentyOne.30The classifier distinguishes between 48 classes (the first and second levels of theSFC hierarchy).
When a document is evaluated by the classifier, it returns a list ofall the classes (domains) it recognizes and an associated confidence score reflecting thecertainty that the document belongs to that particular domain.
We classified 10 months?worth of data from the English Gigaword Corpus using this classifier and assigned eachdocument to the corpus belonging to the highest scoring class of the classifier?s output.The level of confidence was ignored at this stage.This resulted in a SPORTS corpus comprising about 11M words and a FINANCEcorpus of about 27M words.
The predominant sense finding algorithm was run on theraw text of these two corpora and we followed exactly the same evaluation strategy asin the previous section.
The results are summarized in Table 17 and are very similarto those based on hand-labeled corpora.
Again, the best results are obtained when testand training data are derived from the same domain.
The FINANCE?FINANCE resultis slightly worse, but is still well above both Random and the SemCor baseline.
TheSPORTS?SPORTS result has slightly improved over the result reported in the previous30 TwentyOne Classifier is an Irion Technologies product: www.irion.ml/products/english/products classify.html.585Computational Linguistics Volume 33, Number 4section.
The reason for these differences may well be because the FINANCE corpus usedfor this experiment is smaller and the SPORTS corpus is slightly larger than those used inthe hand-labeled experiment.Automatically classifying documents inherently introduces noise in the trainingcorpora.
This experiment to test the robustness of our method for finding predominantsenses suggests that it deals well with the noise.
Further experiments that take theconfidence levels of the classifier into account will allow us to create corpora with lessnoise and will allow us to find the right balance between corpus size and corpus quality.7.
ConclusionsIn this article we have argued that information on the predominant sense of words isimportant, and that it is desirable to be able to infer this automatically from unlabeledtext.
We presented a number of evaluations investigating various facets of a previouslyproposed method for automatically acquiring this information (McCarthy et al 2004a).The evaluations extend ones in previous publications in a number of ways: they uselarger, balanced test data sets, and they compare alternative semantic similarity scoresand distributional thesauruses derived from different corpora and based on differentkinds of relations.
We also looked in detail at areas where the method performs welland also where it does not, and carried out a manual error analysis to identify the typesof mistakes it makes.Our main results are: The predominant sense acquisition method produces promising resultsoverall for all open class parts of speech, when evaluated on SemCor, alarge balanced corpus. The highest accuracies are for nouns and adjectives; overall accuracy forverbs is lower, but they have the lowest random baseline; adverbs have thelowest average polysemy but gains over the random baseline are lowerthan for other PoS. Using a thesaurus computed from proximity-based relations producesalmost as good results as using an otherwise identical one computed fromsyntactic dependency-based relations. Lesk?s semantic similarity score (Banerjee and Pedersen 2002, lesk)produces particularly good results for nouns which have low corpusfrequencies; Jiang and Conrath?s (1997, jcn) score does well on higherfrequency words.31 For low frequency nouns in SemCor, the method, using any combinationof automatically acquired thesaurus and semantic similarity score that wetried, produces more accurate predominant sense information thanSemCor.
In particular, for nouns with a frequency of five or less (12.9% ofthe polysemous nouns in the Senseval-2 data) it outperforms the SemCorfirst sense heuristic.
As the threshold is increased, the SemCor first sense31 The lesk score has wider applicability than jcn since it can be applied to all parts of speech.
It can also beused with any sense inventory which has textual definitions for its senses even if the inventory does notcontain WordNet-like semantic relations.586McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Sensesheuristic becomes more competitive, but some of the automatic methodsare still outperforming it for nouns occurring 20 or fewer times in SemCor. Nouns show a stronger tendency for domain-specific meanings than otherparts of speech, but predominant senses for verbs acquired automaticallywith respect to domain-specific corpora also correlate with the appropriatedomain labeling for those senses. Predominant senses acquired using domain-specific corpora outperformthose from SemCor in a WSD task, for a selection of nouns, using corporaconsisting of either hand-classified or automatically-classified documents.8.
Further WorkWe are continuing to work on automatic ranking of word senses for WSD.
Our next stepwill be to use the numeric values of sense prevalence scores to compare the skews inthe distributions of word senses across different corpora and see if this enables us todetect automatically words for which a domain- or genre-specific ranking is warranted.Looking at skews should also help in predicting words for which contextual WSD islikely to be particularly powerful, for example when more than one sense is scoredas being highly prevalent.
In such situations we will combine our method with anapproach to unsupervised context-based WSD which uses the collocates of the distri-butional neighbors associated with each of the senses as contextual features.Our error analysis shows that many errors in identifying predominant senses arecaused by the sense distinctions in WordNet being particularly fine-grained.
We haverecently (Koeling and McCarthy 2007) evaluated our method on the coarse-grainedEnglish all words task at SemEval (Navigli, Litkowski, and Hargraves 2007).
We will fol-low work on finding relationships between WordNet senses to induce coarser-grainedclasses (McCarthy 2006), and on automatic induction of senses (Pantel and Lin 2002)and adapt our method to acquire prevalence rankings for these.
The granularity of theinventory will depend on the application and we plan to apply rankings over suchinventories for WSD within the context of a task, such as lexical substitution (McCarthyand Navigli 2007).To date we have only applied our methods to English.
We plan to apply ourapproach to other languages for which sense tagged resources of the size of SemCor arenot available.
Given the good results with Lin?s proximity based thesaurus we believeour method should work even for languages which do not have high quality parsersavailable.AcknowledgmentsThis work was supported by the UK EPSRCproject EP/C537262 ?Ranking Word Sensesfor Disambiguation: Models andApplications,?
and a UK Royal SocietyDorothy Hodgkin Fellowship to the firstauthor.
We are grateful to Dekang Lin formaking his thesaurus data publicly availableand to Siddharth Patwardhan and TedPedersen for the WordNet SimilarityPackage.
We thank the anonymous reviewersfor the many helpful comments andsuggestions they made.ReferencesAgirre, Eneko and Oier Lopez de Lacalle.2003.
Clustering WordNet word senses.
InRecent Advances in Natural LanguageProcessing, pages 121?130, Borovets,Bulgaria.Banerjee, Satanjeev and Ted Pedersen.
2002.An adapted Lesk algorithm for word sensedisambiguation using WordNet.
InProceedings of the Third InternationalConference on Intelligent Text Processing andComputational Linguistics (CICLing-02),pages 136?145, Mexico City.587Computational Linguistics Volume 33, Number 4Briscoe, Edward and John Carroll.
2002.Robust accurate statistical annotation ofgeneral text.
In Proceedings of the ThirdInternational Conference on LanguageResources and Evaluation (LREC),pages 1499?1504, Las Palmas, CanaryIslands, Spain.Buitelaar, Paul and Bogdan Sacaleanu.2001.
Ranking and selecting synsets bydomain relevance.
In Proceedings ofWordNet and Other Lexical Resources:Applications, Extensions and Customizations,NAACL 2001 Workshop, pages 119?124,Pittsburgh, PA.Chan, Yee Seng and Hwee Tou Ng.
2005.Word sense disambiguation withdistribution estimation.
In Proceedings ofthe 19th International Joint Conference onArtificial Intelligence (IJCAI 2005),pages 1010?1015, Edinburgh, UK.Church, Kenneth W. and Patrick Hanks.1989.
Word association norms, mutualinformation and lexicography.
InProceedings of the 27th Annual Conference ofthe Association for Computational Linguistics(ACL-89), pages 76?82, Vancouver, BritishColumbia, Canada.Ciaramita, Massimiliano and Mark Johnson.2003.
Supersense tagging of unknownnouns in WordNet.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing (EMNLP 2003),pages 168?175, Sapporo, Japan.Cotton, Scott, Phil Edmonds, AdamKilgarriff, and Martha Palmer.
2001.Senseval-2.
http://www.sle.sharp.co.uk/senseval2.Curran, James.
2005.
Supersense taggingof unknown nouns using semanticsimilarity.
In Proceedings of the 43rdAnnual Meeting of the Association forComputational Linguistics (ACL?05), pages26?33, Ann Arbor, MI.Daude?, Jordi, Lluis Padro?, and GermanRigau.
2000.
Mapping WordNets usingstructural information.
In Proceedings of the38th Annual Meeting of the Association forComputational Linguistics, pages 504?511,Hong Kong.Fellbaum, Christiane, editor.
1998.
WordNet,An Electronic Lexical Database.
The MITPress, Cambridge, MA.Francis, W. Nelson and Henry Kuc?era, 1979.Manual of Information to Accompany aStandard Corpus of Present-Day EditedAmerican English, for Use with DigitalComputers.
Department of Linguistics,Brown University, Providence, RI.
Revisedand amplified ed.Gale, William, Kenneth Church, and DavidYarowsky.
1992.
One sense per discourse.In Proceedings of the 4th DARPA Speech andNatural Language Workshop, pages 233?237,Harriman, NY.Gliozzo, Alfio, Claudio Giuliano, andCarlo Strapparava.
2005.
Domain kernelsfor word sense disambiguation.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics,pages 403?410, Ann Arbor, MI.Graff, David.
2003.
English Gigaword.Linguistic Data Consortium, Philadelphia,PA.Harris, Zellig S. 1968.
Mathematical Structuresof Languages.
Wiley, New York, NY.Hornby, A. S. 1989.
Oxford Advanced Learner?sDictionary of Current English.
OxfordUniversity Press, Oxford, UK.Ide, Nancy and Yorick Wilks.
2006.
Makingsense about sense.
In Eneko Agirre andPhil Edmonds, editors, Word SenseDisambiguation, Algorithms and Applications.Springer, Dordrecht, The Netherlands,pages 47?73.Jiang, Jay and David Conrath.
1997.Semantic similarity based on corpusstatistics and lexical taxonomy.
In 10thInternational Conference on Research inComputational Linguistics, pages 19?33,Taiwan.Kilgarriff, Adam.
1998.
Gold standarddatasets for evaluating word sensedisambiguation programs.
ComputerSpeech and Language, 12(3):453?472.Kilgarriff, Adam and Martha Palmer, editors.2000.
Senseval: Special Issue of the JournalComputers and the Humanities, volume34(1?2).
Kluwer, Dordrecht, TheNetherlands.Koeling, Rob and Diana McCarthy.
2007.Sussx: WSD using automatically acquiredpredominant senses.
In Proceedings ofACL/SIGLEX SemEval-2007, pages 314?317,Prague, Czech Republic.Koeling, Rob, Diana McCarthy, and JohnCarroll.
2005.
Domain-specific sensedistributions and predominant senseacquisition.
In Proceedings of the HumanLanguage Technology Conference andEMNLP, pages 419?426, Vancouver, BritishColumbia, Canada.Krovetz, Robert.
1998.
More than one senseper discourse.
In Proceedings of theACL-SIGLEX Senseval Workshop.http://www.itri.bton.ac.uk/events/senseval/ARCHIVE/PROCEEDINGS/.Landes, Shari, Claudia Leacock, andRandee I. Tengi, editors.
1998.
Building588McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word SensesSemantic Concordances.
The MIT Press,Cambridge, MA.Lapata, Mirella and Chris Brew.
2004.
Verbclass disambiguation using informativepriors.
Computational Linguistics,30(1):45?75.Leech, Geoffrey.
1992.
100 million words ofEnglish: The British National Corpus.Language Research, 28(1):1?13.Lesk, Michael.
1986.
Automatic sensedisambiguation using machine readabledictionaries: How to tell a pine cone froman ice cream cone.
In Proceedings of theACM SIGDOC Conference, pages 24?26,Toronto, Canada.Levin, Beth.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago andLondon.Lin, Dekang.
1997.
Using syntacticdependency as local context to resolveword sense ambiguity.
In Proceedings of the35th Annual Meeting of the Association forComputational Linguistics and 8th Conferenceof the European Chapter of the Associationfor Computational Linguistics (ACL-97),pages 64?71, Madrid, Spain.Lin, Dekang.
1998a.
Automatic retrieval andclustering of similar words.
In Proceedingsof COLING-ACL?98, pages 768?774,Montreal, Canada.Lin, Dekang.
1998b.
Dependency-basedevaluation of MINIPAR.
In Proceedings ofthe Workshop on the Evaluation of ParsingSystems, pages 48?56, Granada, Spain.http://www.cs.ualberta.ca/~lindek/minipar.htm.Magnini, Bernardo and Gabriela Cavaglia`.2000.
Integrating subject field codes intoWordNet.
In Proceedings of LREC-2000,pages 1413?1418, Athens, Greece.Magnini, Bernardo, Carlo Strapparava,Giovanni Pezzuli, and Alfio Gliozzo.
2001.Using domain information for word sensedisambiguation.
In Proceedings of theSenseval-2 Workshop, pages 111?114,Toulouse, France.Magnini, Bernardo, Carlo Strapparava,Giovanni Pezzulo, and Alfio Gliozzo.
2002.The role of domain information in wordsense disambiguation.
Natural LanguageEngineering, 8(4):359?373.Martinez, David and Eneko Agirre.
2000.One sense per collocation and genre/topicvariations.
In Proceedings of the JointSIGDAT Conference on Empirical Methods inNatural Language Processing and Very LargeCorpora, pages 207?215, Hong Kong.McCarthy, Diana.
2006.
Relating WordNetsenses for word sense disambiguation.
InProceedings of the EACL 06 Workshop:Making Sense of Sense: BringingPsycholinguistics and ComputationalLinguistics Together, pages 17?24, Trento,Italy.McCarthy, Diana and John Carroll.
2003.Disambiguating nouns, verbs, andadjectives using automatically acquiredselectional preferences.
ComputationalLinguistics, 29(4):639?654.McCarthy, Diana, Rob Koeling, Julie Weeds,and John Carroll.
2004a.
Findingpredominant senses in untagged text.
InProceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics,pages 280?287, Barcelona, Spain.McCarthy, Diana, Rob Koeling, Julie Weeds,and John Carroll.
2004b.
Ranking WordNetsenses automatically.
CSRP 569,Department of Informatics, University ofSussex, January.McCarthy, Diana, Rob Koeling, Julie Weeds,and John Carroll.
2004c.
Usingautomatically acquired predominantsenses for word sense disambiguation.
InProceedings of the ACL Senseval-3 Workshop,pages 151?154, Barcelona, Spain.McCarthy, Diana and Roberto Navigli.
2007.SemEval-2007 task 10: English lexicalsubstitution task.
In Proceedings ofACL/SIGLEX SemEval-2007, pages 48?53,Prague, Czech Republic.Mihalcea, Rada and Phil Edmonds, editors.2004.
Proceedings Senseval-3 3rdInternational Workshop on Evaluating WordSense Disambiguation Systems.
ACL,Barcelona, Spain.Miller, George A., Claudia Leacock, RandeeTengi, and Ross T. Bunker.
1993.
Asemantic concordance.
In Proceedings of theARPA Workshop on Human LanguageTechnology, pages 303?308, San Francisco,CA.Mohammad, Saif and Graeme Hirst.2006.
Determining word sense dominanceusing a thesaurus.
In Proceedings ofthe 11th Conference of the European Chapterof the Association for ComputationalLinguistics (EACL-2006), pages 121?128,Trento, Italy.Navigli, Roberto, Ken Litkowski, andOrin Hargraves.
2007.
SemEval-2007task 7: Coarse-grained English all-wordstask.
In Proceedings of ACL/SIGLEXSemEval-2007, pages 30?35, Prague,Czech Republic.589Computational Linguistics Volume 33, Number 4Palmer, Martha, Hoa Trang Dang, andChristiane Fellbaum.
2007.
Makingfine-grained and coarse-grained sensedistinctions, both manually andautomatically.
Natural LanguageEngineering, 13(02):137?163.Pantel, Patrick and Dekang Lin.2002.
Discovering word senses fromtext.
In Proceedings of ACM SIGKDDConference on Knowledge Discovery andData Mining, pages 613?619, Edmonton,Alberta, Canada.Patwardhan, Siddharth, Satanjeev Banerjee,and Ted Pedersen.
2003.
Using measures ofsemantic relatedness for word sensedisambiguation.
In Proceedings of the FourthInternational Conference on Intelligent TextProcessing and Computational Linguistics(CICLing 2003), pages 241?257, MexicoCity, Mexico.Patwardhan, Siddharth and Ted Pedersen.2003.
The CPAN WordNet::SimilarityPackage.
http://search.cpan.org/~sid/WordNet-Similarity-0.05/.Preiss, Judita and David Yarowsky, editors.2001.
Proceedings of Senseval-2 SecondInternational Workshop on Evaluating WordSense Disambiguation Systems.
ACL,Toulouse, France.Procter, Paul, editor.
1978.
LongmanDictionary of Contemporary English.Longman Group Ltd., Harlow, UK.Resnik, Philip.
1995.
Using informationcontent to evaluate semantic similarityin a taxonomy.
In 14th InternationalJoint Conference on Artificial Intelligence,pages 448?453, Montreal, Canada.Rose, Tony G., Mary Stevenson, and MilesWhitehead.
2002.
The Reuters Corpusvolume 1?From yesterday?s news totomorrow?s language resources.
InProceedings of the 3rd InternationalConference on Language Resources andEvaluation, pages 827?833, Las Palmas,Canary Islands, Spain.Siegel, Sidney and N. John Castellan.1988.
Non-Parametric Statistics for theBehavioral Sciences.
McGraw-Hill,New York, NY.Snyder, Benjamin and Martha Palmer.2004.
The English all-words task.In Proceedings of the ACL Senseval-3Workshop, pages 41?43, Barcelona,Spain.Stevenson, Mark and Yorick Wilks.
2001.The interaction of knowledge sources forword sense disambiguation.
ComputationalLinguistics, 27(3):321?350.Weeds, Julie.
2003.
Measures andApplications of Lexical DistributionalSimilarity.
Ph.D. thesis, Department ofInformatics, University of Sussex,Brighton, UK.Weeds, Julie, James Dowdall, GeroldSchneider, Bill Keller, and David Weir.2005.
Using distributional similarity toorganise biomedical terminology.Terminology, 11(1):107?141.Weeds, Julie and David Weir.
2005.Co-occurrence Retrieval: A flexibleframework for lexical distributionalsimilarity.
Computational Linguistics,31(4):439?476.Yarowsky, David and Radu Florian.
2002.Evaluating sense disambiguationperformance across diverse parameterspaces.
Natural Language Engineering,8(4):293?310.590
