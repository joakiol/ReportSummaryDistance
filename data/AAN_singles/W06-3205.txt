Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 32?40,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsExploring variant definitions of pointer length in MDLAris XanthosDepartment of LinguisticsUniversity of ChicagoChicago IL 60637axanthos@uchicago.eduYu HuDepartment ofComputer ScienceUniversity of ChicagoChicago IL 60637yuhu@uchicago.eduJohn GoldsmithDepartments of Linguistics andComputer ScienceUniversity of ChicagoChicago IL 60637goldsmith@uchicago.eduAbstractWithin the information-theoretical frame-work described by (Rissanen, 1989; deMarcken, 1996; Goldsmith, 2001), point-ers are used to avoid repetition of phono-logical material.
Work with which weare familiar has assumed that there is onlyone way in which items could be pointedto.
The purpose of this paper is to de-scribe and compare several different meth-ods, each of which satisfies MDL?s ba-sic requirements, but which have differentconsequences for the treatment of linguis-tic phenomena.
In particular, we assessthe conditions under which these differentways of pointing yield more compact de-scriptions of the data, both from a theoret-ical and an empirical perspective.1 IntroductionThe fundamental hypothesis underlying the Mini-mum Description Length (MDL) framework (Rissa-nen, 1989; de Marcken, 1996; Goldsmith, 2001) isthat the selection of a model for explaining a set ofdata should aim at satisfying two constraints: on theone hand, it is desirable to select a model that can bedescribed in a highly compact fashion; on the otherhand, the selected model should make it possible tomodel the data well, which is interpreted as beingable to describe the data in a maximally compactfashion.
In order to turn this principle into an op-erational procedure, it is necessary to make explicitthe notion of compactness.
This is not a trivial prob-lem, as the compactness (or conversely, the length)of a description depends not only on the complexityof the object being described (in this case, either amodel or a set of data given a model), but also onthe ?language?
that is used for the description.Consider, for instance, the model of morphologydescribed in Goldsmith (2001).
In this work, thedata consist in a (symbolically transcribed) corpussegmented into words, and the ?language?
used todescribe the data contains essentially three objects:a list of stems, a list of suffixes, and a list of sig-natures, i.e.
structures specifying which stems asso-ciate with which suffixes to form the words found inthe corpus.
The length of a particular model (or mor-phology) is defined as the sum of the lengths of thethree lists that compose it; the length of each list is inturn defined as the sum of the lengths of elements init, plus a small cost for the list structure itself1.
Thelength of an individual morpheme (stem or suffix) istaken to be proportional to the number of symbols init.Calculating the length of a signature involves thenotion of pointer, with which this paper is primar-ily concerned.
The function of a signature is to re-late a number of stems with a number of suffixes.Since each of these morphemes is spelled once inthe corresponding list, there is no need to spell itagain in a signature that contains it.
Rather, eachsignature comprises a list of pointers to stems anda list of pointers to suffixes.
A pointer is a sym-bol that stands for a particular morpheme, and therecourse to pointers relies on the assumption that1More on this in section 2.1 below32their length is lesser than that of the morphemesthey replace.
Following information-theoretic prin-ciples (Shannon, 1948), the length of a pointer to amorpheme (under some optimal encoding scheme)is equal to -1 times the binary logarithm of that mor-pheme?s probability.
The length of a signature is thesum of the lengths of the two lists it contains, andthe length of each list is the sum of the lengths ofthe pointers it contains (plus a small cost for the listitself).This work and related approaches to unsupervisedlanguage learning have assumed that there is onlyone way in which items could be pointed to, or iden-tified.
The purpose of this paper is to describe, com-pare and evaluate several different methods, each ofwhich satisfies MDL?s basic requirements, but whichhave different consequences for the treatment of lin-guistic phenomena.
One the one hand, we contrastthe expected description length of ?standard?
lists ofpointers with polarized lists of pointers, which arespecified as either (i) pointing to the relevant mor-phemes (those that belong to a signature, or undergoa morpho-phonological rule, for instance) or (ii)pointing to their complement (those that do not be-long to a signature, or do not undergo a rule).
On theother hand, we compare (polarized) lists of pointerswith a method based on binary strings specifyingeach morpheme as relevant or not (for a given sig-nature, rule, etc.).
In particular, we discuss the con-ditions under which these different ways of pointingare expected to yield more compact descriptions ofthe data.The remainder of this paper is organized as fol-lows.
In the next section, we give a formal reviewof the standard treatment of lists of pointers as de-scribed in (Goldsmith, 2001); then we successivelyintroduce polarized lists of pointers and the methodof binary strings, and make a first, theoretical com-parison of them.
Section three is devoted to an em-pirical comparison of these methods on a large nat-ural language corpus.
In conclusion, we discuss theimplications of our results in the broader context ofunsupervised language learning.2 Variant definitions of pointersIn order to simplify the following theoretical discus-sion, we temporarily abstract away from the com-plexity of a full-blown model of morphology.
Givena set of N stems and their distribution, we considerthe general problem of pointing to a subset of Mstems (with 0 < M ?
N ), first by means of ?stan-dard?
lists of pointers, then by means of polarizedones, and finally by means of binary strings.2.1 Expected length of lists of pointersLet ?
denote a set of N stems; we assume that thelength of a pointer to a specific stem t ?
?
is itsinverse log probability ?
log pr(t).2 Now, let {M}denote the set of all subsets of ?
that contain exactly0 < M ?
N stems.
The description length of alist of pointers to a particular subset ?
?
{M} isdefined as the sum of the lengths of the M pointersit contains, plus a small cost of for specifying the liststructure itself, defined as ?
(M) := 0 if M = 0 andlogM bits otherwise3:DLptr(?)
:= ?(M)??t?
?log pr(t)The expected length of a pointer is equal to theentropy over the distribution of stems:hstems := Et??
[?
log pr(t)] = ??t?
?pr(t) log pr(t)Thus, the expected description length of a list ofpointers to M stems (over all subsets ?
?
{M})is:E??
{M} [DLptr(?)]
= 1|{M}|???{M}DLptr(?
)= ?
(M) +Mhstems(1)This value increases as a function of both the num-ber of stems which are pointed to and the entropyover the distribution of stems.
Since 0 ?
hstems ?logN , the following bounds hold:0 ?
hstems ?
E??
{M} [DLptr(?)]?
logN +Nhstems ?
(N + 1) logN2Here and throughout the paper, we use the notation log xto refer to the binary logarithm of x; thus entropy and otherinformation-theoretic quantities are expressed in terms of bits.3Cases where the argument of this function can have thevalue 0 will arise in the next section.332.2 PolarizationConsider a set of N = 3 equiprobable stems, andsuppose that we need to specify that a given morpho-phonological rule applies to one of them.
In this con-text, a list with a single pointer to a stem requireslog 1 ?
log 13 = 1.58 bits.
Suppose now that therule is more general and applies to two of the threestems.
The length of the new list of pointers is thuslog 2 ?
2 log 13 = 4.17 bits.
It appears that for sucha general rule, it is more compact to list the stems towhich it does not apply, and mark the list with a flagthat indicates the ?negative?
meaning of the point-ers.
Since the flag signals a binary choice (either thelist points to stems that undergo the rule, or to thosethat do not), log 2 = 1 bit suffices to encode it, sothat the length of the new list is 1.58 + 1 = 2.58bits.We propose to use the term polarized to refer tolists of pointers bearing a such flag.
If it is useful todistinguish between specific settings of the flag, wemay speak of positive versus negative lists of point-ers (the latter being the case of our last example).The expected description length of a polarized listof M pointers is:E??
{M} [DLpol(?)]
= 1 + ?(M?)
+ M?hstemswith M?
:= min(M,N ?M)(2)From (1) and (2), we find that in general, the ex-pected gain in description length by polarizing a listof M pointers is:E??
{M} [DLptr(?)?DLpol(?)]=?????
?1 iff M ?
N2?1 + ?(M)?
?
(N ?M) + (2M ?N)hstemsotherwiseThus, if the number of stems pointed to is lesser thanor equal to half the total number of stems, using apolarized list rather than a non-polarized one meanswasting exactly 1 bit for encoding the superfluousflag.
If the number of stems pointed to is larger thanthat, we still pay 1 bit for the flag, but the reducednumber of pointers results in an expected saving of?
(M) ?
?
(N ?
M) bits for the list structure, plus(2M ?N) ?
hstems bits for the pointers themselves.Now, let us assume that we have no informa-tion regarding the number M of elements which are0 500 1000 1500 20000100030005000Polarized vs. non?polarized listsTotal number of stems NDescriptionlengthgain (in bits)s=0s=1s=2s=10Figure 1: Expected gain in description length by us-ing polarized rather than non-polarized lists of point-ers.pointed to, i.e.
that it has a uniform distribution be-tween 1 and N (M ?
U [1, N ]).
Let us further as-sume that stems follow a Zipfian distribution of pa-rameter s, so that the probability of the k-th mostfrequent stem is defined as:f(k,N, s) := 1/ksHN,s with HN,s :=N?n=11/nswhere HN,s stands for the harmonic number of orderN of s. The entropy over this distribution is:hZipfN,s :=sHN,sN?k=1log kks + logHN,sArmed with these assumptions, we may now com-pute the expected description length gain of polar-ization (over all values of M ) as a function of Nand s:EM(E??
{M} [DLptr(?)?DLpol(?
)])=?1+ 1N?NM=1 ?(M)?
?(M?)
+ (M ?
M?
)hZipfN,sFigure 1 shows the gain calculated for N = 1,400, 800, 1200, 1600 and 2000, and s = 0, 1, 2and 10.
In general, it increases with N , with aslope that depends on s: the greater the value of s,the lesser the entropy over the distribution of stems;since the entropy corresponds to the expected length34Figure 2: Two ways of pointings to stems: by meansof a polarized list of pointers, or a binary string.of a pointer, its decrease entails a decrease in thenumber of bits that can be saved by using polarizedlists (which generally use less pointers).
However,even for an aberrantly skewed distribution of stems4,the expected gain of polarization remains positive.Since the value of s is usually taken to be slightlygreater than 1 for natural languages (Mandelbrot,1953), it seems that polarized lists generally entaila considerable gain in description length.2.3 Binary stringsConsider again the problem of pointing to one outof three equiprobable stems.
Suppose that the list ofstems is ordered, and that we want to point to thefirst one, for instance.
An alternative to the recourseto a list of pointers consists in using a binary string(in this case 100) where the i-th symbol is set to 1(or +) if the i-th stem is being pointed to, and to 0(or -) otherwise.
Figure 2 gives a schematic view ofthese two ways of pointing to items.There are two main differences between thismethod and the previous one.
On the one hand,the number of symbols in the string is constant andequal to the total number N of stems, regardless ofthe number M of stems that are pointed to.
On theother hand, the compressed length of the string de-pends on the distribution of symbols in it, and not onthe distribution of stems.
Thus, by comparison withthe description length of a list of pointers, there is aloss due to the larger number of encoded symbols,and a gain due to the use of an encoding specifically4In the case s = 10, the probability of the most frequentstem is .999 for N = 2000.tailored for the relevant distribution of pointed ver-sus ?unpointed?
elements.The entropy associated with a binary string is en-tirely determined by the number of 1?s it contains,i.e.
the number M of stems which are pointed to,and the length N of the string:hbinN,M := ?MN logMN ?N ?MN logN ?MNThe compressed length of a binary string pointing toM stems is thus:DLbin(M) := NhbinN,M (3)It is maximal and equal to N bits when M = N2 ,and minimal and equal to 0 when M = N , i.e.
whenall stems have a pointer on them.
Notice that binarystrings are intrinsically polarized, so that intervert-ing 0?s and 1?s results in the same description lengthregardless of their distribution.5The question naturally arises, under which con-ditions would binary strings be more or less com-pact than polarized lists of pointers.
If we assumeagain that the distribution of the number of elementspointed to is uniform and the distribution of stems isZipfian of parameter s, (2) and (3) justify the follow-ing expression for the expected description lengthgain by using binary strings rather than polarizedlists (as a function of N and s):EM[E??{M}[DLpol(?
)]?DLbin(M)]= 1 + 1N?NM=1 ?(M?)
+ M?hZipfN,s ?NhbinN,MFigure 3 shows the gain calculated for N = 1, 400,800, 1200, 1600 and 2000, and s = 0, 1, 2 and 3.For s small, i.e.
when the entropy over the distri-bution of stems is greater or not much lesser thanthat of natural languages, the description length ofbinary strings is considerably lesser than that of po-larized lists.
The difference decreases as s increases,5As one the reviewers has indicated to us, the binary stringsapproach is actually very similar to the method of combinato-rial codes described by (Rissanen, 1989).
This method con-sists in pointing to one among  NM possible combinations ofM stems out of N .
Under the assumption that these combi-nations have a uniform probability, the cost for pointing to Mstems is log  NM bits, which is in general slightly lesser thanthe description length of the corresponding binary string (thedifference being maximal for M = N/2, i.e.
when the binarystring encoding cannot take advantage of any compression).350 500 1000 1500 2000?1000010003000Binary strings vs. polarized lists(uniform distribution of M)Total number of stems NDescriptionlengthgain (in bits)s=0s=1s=2s=3Figure 3: Expected gain in description length by us-ing binary strings rather than polarized lists underthe assumption that M ?
U [1, N ].until at some point (around s = 2), the situation re-verses and polarized lists become more compact.
Inboth cases, the trend increases with the number Nof stems (within the range of values observed).By contrast, it is instructive to consider a casewhere the distribution of the number of elementspointed to departs from uniformity.
For instance, wecan make the assumption that M follows a binomialdistribution (M ?
B[N, p]).6 Under this assump-tion (and, as always, that of a Zipfian distribution ofstems), the expected description length gain by us-ing binary strings rather than polarized lists is:EM[E??{M}[DLptr(?
)]?DLbin(M)]= ?NM=1 pr(M)(1+?(M?
)+M?hZipfN,s?NhbinN,M)with pr(M) = (NM)pM (1?
p)N?MLetting N and s vary as in the previous computation,we set the probability for a stem to have a pointer onit to p = 0.01, so that the distribution of pointed ver-sus ?unpointed?
elements is considerably skewed.76This model predicts that most of the time, the number Mof elements pointed to is equal to N ?
p (where p denotes theprobability for a stem to have a pointer on it), and that the prob-ability pr(M) of other values of M decreases as they divergefrom N ?
p.7By symmetry, the same results would be found with p =0.99.0 500 1000 1500 2000?150?50050Binary strings vs. polarized lists(binomial distribution of M, p = 0.01)Total number of stems NDescriptionlengthgain (in bits) s=0s=1s=2s=3Figure 4: Expected gain in description length by us-ing binary strings rather than polarized lists underthe assumption that M ?
B[N, 0.01].As shown on figure 4, under these conditions, the ab-solute value of the gain of using binary strings getsmuch smaller in general, and the value of s for whichthe gain becomes negative for N large gets close to 1(for this particular value, it becomes positive at somepoint between N = 1200 and N = 1600).Altogether, under the assumptions that we haveused, these theoretical considerations suggest thatbinary strings generally yield shorter descriptionlengths than polarized lists of pointers.
Of course,data for which these assumptions do not hold couldarise.
In the perspective of unsupervised learning,it would be particularily interesting to observe thatsuch data drive the learner to induce a differentmodel depending on the representation of pointersbeing adopted.It should be noted that nothing prevents binarystrings and lists of pointers from coexisting in a sin-gle system, which would select the most compactone for each particular case.
On the other hand, it isa logical necessity that all lists of pointers be of thesame kind, either polarized or not.3 ExperimentsIn the previous section, by assuming frequencies ofstems and possible distributions of M (the num-ber of stems per signature), we have explored the-oretically the differences between several encoding360 500 1000 1500 2000 25000.00000.00100.0020Frequency as a function of rank(English corpus)RankFrequencyFigure 5: Frequency versus rank (stems) in Englishcorpus.methods in the MDL framework.
In this section, weapply these methods to the problem of suffix discov-ery in natural language corpora, in order to verify thetheoretical predictions we made previously.
Thus,the purpose of these experiments is not to state thatone encoding is preferable to the others; rather, wewant to answer the three following questions:1.
Are our assumptions on the frequency of stemsand size of signatures appropriate for naturallanguage corpora?2.
Given these assumptions, do our theoreticalanalyses correctly predict the difference in de-scription length of two encodings?3.
What is the relationship between the gain in de-scription length and the size of the corpus?3.1 Experimental methodologyIn this experiment, for the purpose of calculatingdistinct description lengths while using different en-coding methods, we modified Linguistica8 by imple-menting list of pointers and binary strings as alter-native means to encode the pointers from signaturesto their associated stems9.
As a result, given a set8The source and binary files can be freely downloaded athttp://linguistica.uchicago.edu.9Pointers to suffixes are not considered here.0 50 100 150 2000.00.10.20.30.40.5Distribution of the number of stemsper signature (English corpus)Number of stemsProportionofsignaturesFigure 6: Distribution of number of stems per signa-ture (English corpus)of signatures, we are able to compute a descriptionlength for each encoding methods.Within Linguistica, the morphology learning pro-cess can be divided into a sequence of heuristics,each of which searches for possible incrementalmodifications to the current morphology.
For exam-ple, in the suffix-discovery procedure, ten heuristicsare carried out successively; thus, we have a dis-tinct set of signatures after applying each of the tenheuristics.
Then, for each of these sets, we encodethe pointers from each signature to its correspond-ing stems in three rival ways: as a list of pointers(polarized or not), as traditionally understood, andas a binary string.
This way, we can compute the to-tal description length of the signature-stem-linkagefor each of the ten sets of signatures and for each ofthree two ways of encoding the pointers.
We alsocollect statistics on word frequencies and on the dis-tribution of the size of signatures M , i.e.
the numberM of stems which are are pointed to, both of whichare important parametric components in our theoret-ical analysis.Experiments are carried out on two orthographiccorpora (English and French), each of which has100,000 word tokens.3.2 Frequency of stems and size of signaturesThe frequency of stems as a function of their rankand the distribution of the size of signatures are plot-370 100 200 300 400 500 6000.00000.00100.0020Frequency as a function of rank(French corpus)RankFrequencyFigure 7: Frequency versus rank (stems) in Frenchcorpus.ted in figures 5 and 6 for the English corpus, and infigures 7 and 8 for the French corpus.
These graphsshow that in both the English and the French cor-pora, stems appear to have a distribution similar to aZipfian one.
In addition, in both corpora, M followsa distribution whose character we are not sure of, butwhich appears more similar to a binomial distribu-tion.
To some extent, these observations are consis-tent with the assumptions we made in the previoustheoretical analysis.3.3 Description length of each encodingThe description length obtained with each encodingmethod is displayed in figures 9 (English corpus)and 10 (French corpus), in which the x-axis refers tothe set of signatures resulting from the applicationof each successive heuristics, and the y-axis corre-sponds to the description length in bits.
Note thatwe only plot description lengths of non-polarizedlists of pointers, because the number of stems persignature is always less than half the total number ofstems in these data (and we expect that this wouldbe true for other languages as well).10These two plots show that in both corpora, there isalways a gain in description length by using binarystrings rather than lists of pointers for encoding thepointers from signatures to stems.
This observationis consistent with our conclusion in section 2.3, but10See figures 6 and 8 as well as section 2.2 above.0 50 100 1500.00.10.20.30.40.5Distribution of the number of stemsper signature (French corpus)Number of stemsProportionofsignaturesFigure 8: Distribution of number of stems per signa-ture (French corpus)it is important to emphasize again that for other data(or other applications), lists of pointers might turnout to be more compact.3.4 Description length gain as a function ofcorpus sizeIn order to evaluate the effect of corpus size onthe gain in description length by using binary stringrather than lists of variable-length pointers, we ap-plied Linguistica to a number of English corpora ofdifferent sizes ranging between 5,000 to 200,000 to-kens.
For the final set of signatures obtained witheach corpus, we then compute the gain of binarystrings encoding over lists of pointers as we did inthe previous experiments.
The results are plotted infigure 11.This graph shows a strong positive correlation be-tween description length gain and corpus size.
Thisis reminiscent of the results of our theoretical simu-lations displayed in figures 3 and 4.
As before, weinterpret the match between the experimental resultsand the theoretical expectations as evidence support-ing the validity of our theoretical predictions.3.5 Discussion of experimentsThese experiments are actually a number of casestudies, in which we verify the applicability of ourtheoretical analysis on variant definitions of pointerlengths in the MDL framework.
For the particu-382 4 6 8 1002000060000DL of lists and binary strings(English corpus)HeuristicsDescriptionlength(inbits)ListsBinary strings1 3 5 7 9Figure 9: Comparison of DL of 10 successive mor-phologies using pointers versus binary strings (En-glish corpus).lar application we considered, learning morphologywith Linguistica, binary strings encoding proves tobe more compact than lists of variable-length point-ers.
However, the purpose of this paper is not topredict that one variant is always better, but rather toexplore the mathematics behind different encodings.Armed with the mathematical analysis of differentencodings, we hope to be better capable of makingthe right choice under specific conditions.
In partic-ular, in the suffix-discovery application (and for thelanguages we examined), our results are consistentwith the assumptions we made and the predictionswe derived from them.4 ConclusionThe overall purpose of this paper has been to illus-trate what was for us an unexpected aspect of us-ing Minimum Description Length theory: not onlydoes MDL not specify the form of a grammar (ormorphology), but it does not even specify the pre-cise form in which the description of the abstractlinkages between concepts (such as stems and sig-natures) should be encoded and quantitatively eval-uated.
We have seen that in a range of cases, us-ing binary strings instead of the more traditionalfrequency-based pointers leads to a smaller overallgrammar length, and there is no guarantee that wewill not find an even shorter way to accomplish the2 4 6 8 10050001000015000DL of lists and binary strings(French corpus)HeuristicsDescriptionlength(inbits)ListsBinary strings1 3 5 7 9Figure 10: Comparison of DL of 10 successivemorphologies using pointers versus binary strings(French corpus)same thing tomorrow11.
Simply put, MDL is em-phatically an evaluation procedure, and not a discov-ery procedure.We hope to have shown, as well, that a system-atic exploration of the nature of the difference be-tween standard frequency-based pointer lengths andbinary string based representations is possible, andwe can develop reasonably accurate predictions orexpectations as to which type of description will beless costly in any given case.AcknowledgementsThis research was supported by a grant of the SwissNational Science Foundation to the first author.ReferencesC.
de Marcken.
1996.
Unsupervised Language Acquisi-tion.
Ph.D. thesis, MIT, Cambridge, MA.J.
Goldsmith.
2001.
The unsupervised learning of natu-ral language morphology.
Computational Linguistics,27(2):153?198.B.
Mandelbrot.
1953.
An informational theory of thestatistical structure of language.
In Willis Jackson, ed-itor, Communication Theory, the Second London Sym-posium, pages 486?502.
Butterworth: London.11See note 5.390 50000 100000 150000 2000000200004000060000DL gain of binary strings vs. lists(English corpus)Corpus sizeDescriptionlengthgain (in bits)Figure 11: DL gain from using binary string versussize of corpus (English corpus)J. Rissanen.
1989.
Stochastic Complexity in StatisticalInquiry.
World Scientific Publishing Co, Singapore.C.E.
Shannon.
1948.
A mathematical theory of commu-nication.
Bell Systems Technical Journal, 27:379?423.40
