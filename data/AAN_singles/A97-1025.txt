Contextual Spelling Correction Using Latent Semantic AnalysisMichae l  P.  Jones  and James  H .
Mar t inDept .
of Computer  Sc ience and  Ins t i tu te  of Cogn i t ive  Sc ienceUn ivers i ty  of Co lo radoBou lder ,  CO 80309-0430{mj ones, mart in}@cs, colorado, eduAbstractContextual spelling errors are defined asthe use of an incorrect, though valid, wordin a particular sentence or context.
Tra-ditional spelling checkers flag misspelledwords, but they do not typically attempt toidentify words that are used incorrectly in asentence.
We explore the use of Latent Se-mantic Analysis for correcting these incor-rectly used words and the results are com-pared to earlier work based on a Bayesianclassifier.1 In t roduct ionSpelling checkers are now available for all majorword processing systems.
However, these spellingcheckers only catch errors that result in misspelledwords.
If an error results in a different, but incor-rect word, it will go undetected.
For example, quitemay easily be mistyped as quiet.
Another type of er-ror occurs when a writer simply doesn't know whichword of a set of homophones 1 (or near homophones)is the proper one for a particular context.
For ex-ample, the usage of affect and effect is commonlyconfused.Though the cause is different for the two typesof errors, we can treat them similarly by examiningthe contexts in which they appear.
Consequently,no effort is made to distinguish between the two er-ror types and both are called contextual spelling er-rors.
Kukich (1992a; 1992b) reports that 40% to45% of observed spelling errors are contextual er-rors.
Sets of words which are frequently misused ormistyped for one another are identified as confusionsets.
Thus, from our earlier examples, {quiet, quite}and { affect, effect} are two separate confusion sets.In this paper, we introduce Latent Semantic Anal-ysis (LSA) as a method for correcting contextualspelling errors for a given collection of confusion sets.1 Homophones are words that sound the same, but arespelled differently.LSA was originally developed as a model for infor-mation retrieval (Dumais et al, 1988; Deerwester etal., 1990), but it has proven useful in other taskstoo.
Some examples include an expert Expert lo-cator (Streeter and Lochbaum, 1988) and a confer-ence proceedings indexer (Foltz, 1995) which per-forms better than a simple keyword-based index.Recently, LSA has been proposed as a theory of se-mantic learning (Landauer and Dumais, (In press)).Our motivation in using LSA was to test its effec-tiveness at predicting words based on a given sen-tence and to compare it to a Bayesian classifier.
LSAmakes predictions by building a high-dimensional,"semantic" space which is used to compare the sim-ilarity of the words from a confusion set to a givencontext.
The experimental results from LSA predic-tion are then compared to both a baseline predic-tor and a hybrid predictor based on trigrams and aBayesian classifier.2 Re la ted  WorkLatent Semantic Analysis has been applied to theproblem of spelling correction previously (Kukich,1992b).
However, this work focused on detect-ing misspelled words, not contextual spelling errors.The approach taken used letter n-grams to build thesemantic space.
In this work, we use the words di-rectly.Yarowsky (1994) notes that conceptual spellingcorrection is part of a closely related class of prob-lems which include word sense disambiguation, wordchoice selection in machine translation, and accentand capitalization restoration.
This class of prob-lems has been attacked by many others.
A numberof feature-based methods have been tried, includingBayesian classifiers (Gale, Church, and Yarowsky,1992; Golding, 1995), decision lists (Yarowsky,1994), and knowledge-based approaches (McRoy,1992).
Recently, Golding and Schabes (1996) de-scribed a system, Tribayes, that combines a trigrammodel of the words' parts of speech with a Bayesianclassifier.
The trigram component of the system isused to make decisions for those confusion sets that166te rmsdocumentsX TOrx rD ~Orxdtxd  tx rFigure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D'.contain words with different parts of speech.
TheBayesian component is used to predict the  correctword from among same part-of-speech words.Golding and Schabes selected 18 confusion setsfrom a list of commonly confused words plus a fewthat represent ypographical errors.
They trainedtheir system using a random 80% of the Brow\[/cor-pus (Ku~era and Francis, 1967).
The remaining 20%of the corpus was used to test how well the systemperformed.
We have chosen to use the same 18 con-fusion sets and the Brown corpus in order to compareLSA to Tribayes.3 Latent  Semant ic  AnalysisLatent Semantic Analysis (LSA) was developed atBellcore for use in information retrieval tasks (forwhich it is also known as LSI) (Dumais et al, 1988;Deerwester et al, 1990).
The premise of the LSAmodel is that an author begins with some idea orinformation to be communicated.
The selection ofparticular lexical items in a collection of texts issimply evidence for the underlying ideas or informa-tion being presented.
The goal of LSA, then, is totake the "evidence" (i.e., words) presented and un-cover the underlying semantics of the text passage.Because many words are polysemous (have multi-ple meanings) and synonymous (have meanings incommon with other words), the evidence availablein the text tends to be somewhat "noisy."
LSA at-tempts to eliminate the noise from the data by firstrepresenting the texts in a high-dimensional spaceand then reducing the dimensionality of the spaceto only the most important dimensions.
This pro-cess is described in more detail in Dumais (1988)or Deerwester (1990), but a brief description is pro-vided here.A collection of texts is represented in matrix for-mat.
The rows of the matrix correspond to termsand the columns represent documents.
The indi-vidual cell values are based on some function of theterm's frequency in the corresponding document and167its frequency in the whole collection.
The func-tion for selecting cell values will be discussed in sec-tion 4.2.
A singular value decomposition (SVD) isperformed on this matrix.
SVD factors the origi-nal matrix into the product of three matrices.
We'llidentify these matrices as T, S, and D'(see Figure 1).The T matrix is a representation f the original termvectors as vectors of derived orthogonal factor val-ues.
D' is a similar representation for the originaldocument vectors.
S is a diagonal matrix 2of rank r.It is also called the singular value matrix.
The sin-gular values are sorted in decreasing order along thediagonal.
They represent a scaling factor for eachdimension in the T and D' matrices.Multiplying T, S, and D'together perfectly repro-duces the original representation f the text collec-tion.
Recall, however, that the original representa-tion is expected to be noisy.
What we really wantis an approximation of the original space that elim-inates the majority of the noise and captures themost important ideas or semantics of the texts.An approximation of the original matrix is createdby eliminating some number of the least importantsingular values in S. They correspond to the leastimportant (and hopefully, most noisy) dimensionsin the space.
This step leaves a new matrix (So) ofrank k. 3 A similar reduction is made in T and Dby retaining the first k columns of T and the firstk rows of D' as depicted in Figure 2.
The productof the resulting To, So, and D'o matrices is a leastsquares best fit reconstruction of the original matrix(Eckart and Young, 1939).
The reconstructed ma-trix defines a space that represents or predicts thefrequency with which each term in the space wouldappear in a given document or text segment givenan infinite sample of semantically similar texts (Lan-2A diagonal matrix is a square matrix that containsnon-zero values only along the diagonal running from theupper left to the lower right.3The number of factors k to be retained is generallyselected empirically.te rmsdocumentsAX.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
iiiiiiii !
D: I ::::::::::::::::::::::::::::::::::: !:~:~,~:i:~:i:i,i:~:i:i:!
:i:~:~:~ +:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.
:* :::::::::::::::::::::::::::::::::::: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::~:~:~:~:~:~:~:~:~:~:~:~:~:~:!
:~:~:~ k x k k x d ~: :~: :~:~:i:!
:i:i:i:i:i: :i:i:i:~txd  txkFigure 2: Results of reducing the T, S and D' matrices produced by SVD to rank k. Recombiningthe reduced matrices gives X, a least squares best fit reconstruction of the original matrix.dauer and Dumais, (In press)).New text passages can be projected into the spaceby computing a weighted average of the term vectorswhich correspond to the words in the new text.
Inthe contextual spelling correction task, we can gen-erate a vector representation for each text passagein which a confusion word appears.
The similaritybetween this text passage vector and the confusionword vectors can be used to predict the most likelyword given the context or text in which it will ap-pear.
'4 Exper imenta l  Method4.1 DataSeparate corpora for training and testing LSA's abil-ity to correct contextual word usage errors were cre-ated from the Brown corpus (Ku~era and Francis,1967).
The Brown corpus was parsed into individ-ual sentences which are randomly assigned to eithera training corpus or a test corpus.
Roughly 80%of the original corpus was assigned as the trainingcorpus and the other 20% was reserved as the testcorpus.
For each confusion set, only those sentencesin the training corpus which contained words in theconfusion set were extracted for construction of anLSA space.
Similarly, the sentences used to test theLSA space's predictions were those extracted fromthe test corpus which contained words from the con-fusion set being examined.
The details of the spaceconstruction and testing method are described be-low.4.2 TrainingTraining the system consists of processing the train-ing sentences and constructing an LSA space fromthem.
LSA requires the corpus to be segmented intodocuments.
For a given confusion set, an LSA spaceis constructed by treating each training sentence asa document.
In other words, each training sentenceis used as a column in the LSA matrix.
Before be-.
168ing processed by LSA, each sentence undergoes thefollowing transformations: context reduction, stem-ming, bigram creation, and term weighting.Context  reduct ion  is a step in which the sen-tence is reduced in size to the confusion word plusthe seven words on either side of the word or up tothe sentence boundary.
The average sentence lengthin the corpus is 28 words, so this step has the effectof reducing the size of the data to approximatelyhalf the original.
Intuitively, the reduction ought toimprove performance by disallowing the distantly lo-cated words in long sentences to have any influenceon the prediction of the confusion word because theyusually have little or nothing to do with the selec-tion of the proper word.
In practice, however, thereduction we use had little effect on the predictionsobtained from the LSA space.We ran some experiments in which we built LSAspaces using the whole sentence as well as other con-text window sizes.
Smaller context sizes didn't seemto contain enough information to produce good pre-dictions.
Larger context sizes (up to the size of theentire sentence) produced results which were not sig-nificantly different from the results reported here.However, using a smaller context size reduces thetotal number of unique terms by an average of 13%.Correspondingly, using fewer terms in the initial ma-trix reduces the average running t ime and storagespace requirements by 17% and 10% respectively.S temming is the process of reducing each word toits morphological root.
The goal is to treat the dif-ferent morphological variants of a word as the sameentity.
For example, the words smile, smiled, smiles,smiling, and smilingly (all from the corpus) are re-duced to the root smile and treated equally.
Wetried different stemming algorithms and all improvedthe predictive performance of LSA.
The results pre-sented in this paper are based on Porter's (Porter,1980) algorithm.B igram creat ion  is performed for the words thatwere not removed in the context reduction step.Bigrams are formed between all adjacent pairs ofwords.
The bigrams are treated as additional termsduring the LSA space construction process.
In otherwords, the bigrams fill their own row in the LSA ma-trix.Term we ight ing  is an effort to increase theweight or importance of certain terms in the highdimensional space.
A local and global weightingis given to each term in each sentence.
The localweight is a combination of the raw count of the par-ticular term in the sentence and the term's prox-imity to the confusion word.
Terms located nearerto the confusion word are given additional weightin a linearly decreasing manner.
The local weightof each term is then flattened by taking its log2.The global weight given to each term is an attemptto measure its predictive power in the corpus as awhole.
We found that entropy (see also (Lochbaumand Streeter, 1989)) performed best as a global mea-sure.
Furthermore, terms which did not appear inmore than one sentence in the training corpus wereremoved.While LSA can be used to quickly obtain satisfac-tory results, some tuning of the parameters involvedcan improve its performance.
For example, we chose(somewhat arbitrarily) to retain 100 factors for eachLSA space.
We wanted to fix this variable for allconfusion sets and this number gives a good averageperformance.
However, tuning the number of factorsto select the "best" number for each space shows anaverage of 2% improvement over all the results andup to 8% for some confusion sets.4.3 Test ingOnce the LSA space for a confusion set has been cre-ated, it can be used to predict the word (from theconfusion set) most likely to appear in a given sen-tence.
We tested the predictive accuracy of the LSAspace in the following manner.
A sentence from thetest corpus is selected and the location of the confu-sion word in the sentence is treated as an unknownword which must be predicted.
One at a time, thewords from the confusion set are inserted into thesentence at the location of the word to be predictedand the same transformations that the training sen-tences undergo are applied to the test sentence.
Theinserted confusion word is then removed from thesentence (but not the bigrams of which it is a part)because its presence biases the comparison which oc-Curs later.
A vector in LSA space is constructed fromthe resulting terms.The word predicted most likely to appear in a sen-tence is determined by comparing the similarity ofeach test sentence vector to each confusion word vec-tor from the LSA space.
Vector similarity is evalu-ated by computing the cosine between two vectors.The pair of sentence and confusion word vectors withthe largest cosine is identified and the correspondingconfusion word is chosen as the most likely word for169the test sentence.
The predicted word is comparedto the correct word and a tally of correct predictionsis kept.5 Resu l tsThe results described in this section are based on the18 confusion sets selected by Golding (1995; 1996).Seven of the 18 confusion sets contain words that areall the same part of speech and the remaining 11 con-tain words with different parts of speech.
Goldingand Schabes (1996) have already shown that using atrigram model to predict words from a confusion setbased on the expected part of speech is very effec-tive.
Consequently, we will focus most of our atten-tion on the seven confusion sets containing words ofthe same part of speech.
These seven sets are listedfirst in all of our tables and figures.
We also showthe results for the remaining 11 confusion sets forcomparison purposes, but as expected, these aren'tas good.
We, therefore, consider our system com-plementary to one (such as Tribayes) that predictsbased on part of speech when possible.5.1 Baseline Predict ion SystemWe describe our results in terms of a baseline predic-tion system that ignores the context contained in thetest sentence and always predicts the confusion wordthat occurred most frequently in the training corpus.Table 1 shows the performance of this baseline pre-dictor.
The left half of the table lists the various con-fusion sets.
The next two columns how the trainingand testing corpus sentence counts for each confu-sion set.
Because the sentences in the Brown corpusare not tagged with a markup language, we identi-fied individual sentences automatically based on asmall set of heuristics.
Consequently, our sentencecounts for the various confusion sets differ slightlyfrom the counts reported in (Golding and Schabes,1996).The right half of Table 1 shows the most frequentword in the training corpus from each confusion set.Following the most frequent word is the baselineperformance data.
Baseline performance is the per-centage of correct predictions made by choosing thegiven (most frequent) word.
The percentage of cor-rect predictions also represents the frequency of sen-tences in the test corpus that contain the given word.The final column lists the training corpus frequencyof the given word.
The difference between the base-line performance column and the training corpusfrequency column gives some indication about howevenly distributed the words are between the twocorpora.For example, there are 158 training sentences forthe confusion set {principal, principle} and 34 testsentences.
Since the word principle is listed in theright half of the table, it must.
have appeared morefrequently in the training set.
From the final column,Confusion Set Train Testprincipal principle 158 34raise rise 117 36affect effect 193 53peace piece 257 62country county 389 91amount number 480 122among between 853 203accept except 189 62begin being 623 161lead led 197 63passed past 353 81quiet quite 280 76weather whether 267 67cite sight site 128 32it's its 1577 391than then 2497 578you're your 734 220their there they're 4176 978Most Freq.
Base (Train Freq.
)principle 41.2 (57.6)rise 72.2 (65.0)effect 88.7 (85.0)peace 58.1 (59.5)country 59.3 (71.0)number 75.4 (73.8)between 62.1 (66.7)except 67.7 (73.5)being 88.8 (89.4)led 50.8 (52.3)past 64.2 (63.2)quite 88.2 (76.1)whether 73.1 (79.0)sight 62.5 (54.7)its 84.7 (84.9)than 58.8 (55.3)your 86.8 (84.5)there 53.4 (53.1)Table 1: Baseline performance for 18 confusion sets.
The table is divided into confusion setscontaining words of the same part of speech and those which have different parts of speech.we can see that it occurred in almost 58% of thetraining sentences.
However, it occurs in only 41%of the test sentences and thus the baseline predictorscores only 41% for this confusion set.5.2 Latent Semantic AnalysisTable 2 shows the performance of LSA on the con-textual spelling correction task.
The table providesthe baseline performance information for compari-son to LSA.
In all but the case of {amount, number},LSA improves upon the baseline performance.
Theimprovement provided by LSA averaged over all con-fusion sets is about 14% and for the sets with thesame part of speech, the average improvement is16%.Table 2 also gives the results obtained by Tribayesas reported in (Golding and Schabes, 1996).
Thebaseline performance given in connection with Trib-ayes corresponds to the partitioning of the Browncorpus used to test Tribayes.
It.
should be noted that.we did not implement Tribayes nor did we use thesame partitioning of the Brown corpus as Tribayes.Thus, the comparison between LSA and Tribayes isan indirect one.The differences in the baseline predictor for eachsystem are a result of different partitions of theBrown corpus.
Both systems randomly split thedata such that roughly 80% is allocated to the train-ing corpus and the remaining 20% is reserved forthe test corpus.
Due to the random nature of thisprocess, however, the corpora must differ betweenthe two systems.
The baseline predictor presentedin this paper and in (Golding and Schabes, 1996)are based on the same method so the correspond-170ing columns in Table 2 can be compared to get anidea of the distribution of sentences that contain themost frequent word for each confusion set.Examination of Table 2 reveals that it is difficultto make a direct comparison between the results ofLSA and Tribayes due to the differences in the par-titioning of the Brown corpus.
Each system shouldperform well on the most frequent confusion wordin the training data.
Thus, the distribution of themost frequent word between the the training andthe test corpus will affect the performance of thesystem.
Because the baseline score captures infor-mation about the percentage of the test corpus thatshould be easily predicted (i.e., the portion that con-tains the most frequent word), we propose a com-parison of the results by examination of the respec-tive systems' improvement over the baseline scorereported for each.
The results of this comparisonare charted in Figure 3.
The horizontal axis in thefigure represents the baseline predictor performancefor each system (even though it varies between thetwo systems).
The vertical bar thus represents theperformance above (or below) the baseline predictorfor each system on each confusion set.LSA performs slightly better, on average, thanTribayes for those confusion sets which containwords of the same part of speech.
Tribayes clearlyout-performs LSA for those words of a different partof speech.
Thus, LSA is doing better than theBayesian component of Tribayes, but it doesn't in-clude part of speech information and is therefore notcapable of performing as well as the part of speechtrigram component of Tribayes.
Consequently, webelieve that LSA is a competitive alternative toLSA TribayesConfusion Set Baseline LSA Baseline Tribayesprincipal principleraise riseaffect effectpeace piececountry countyamount numberamong betweenaccept exceptbegin beinglead ledpassed pastquiet quiteweather whethercite sight siteit's itsthan thenyou're yourtheir there they're41.2 91.272.2 80.688.7 94.358.1 83.959.3 81.375.4 56.662.1 80.867.7 82.388.8 93.250.8 73.064.2 80.388.2 90.873.1 85.162.5 78.184.7 92.858.8 90.586.8 91.453.4 73.958.8 88.264.1 76.991.8 95.944.0 90.091.9 85.571.5 82.971.5 75.370.0 82.093.2 97.346.9 83.768.9 95.983.3 95.586.9 93.464.7 70.691.3 98.163.4 94.989.3 98.956.8 97.6Table 2: LSA performance for 18 confusion sets.
The results of Tribayes (Golding and Schabes,1996) are also given.5040oo20~10-10-20Tribayes and LSA performance compared to baseline predictor\ [ \ ]  Tribayes\ [ \ ]  LSA:2!!i!
~ ...=!!
!i !:!::!
:5: :.:.
:.-fill i:~:~:>:.0 "0  0 ~ " - -  t -  O'J " - -  0 (I) - -  ?.~ ~ .
- -  t '-"0 ?
Z~ X ~ ~ ~ ~ '~ ~ ?- .__=_ "~ o_ o *~ ?
= *"o_  oFigure 3: Comparison of Tribayes vs. LSA performance above the baseline metric.171a Bayesian classifier for making predictions amongwords of the same part of speech.5.3 Per fo rmance  Tun ingThe results that have been presented here are basedon uniform treatment for each confusion set.
That  is,the initial data processing steps and LSA space con-struction parameters have all been the same.
How-ever, the model does not require equivalent reat-ment of all confusion sets.
In theory, we should beable to increase the performance for each confusionset by tuning the various parameters for each confu-sion set.In order to explore this idea further, we selectedthe confusion set {amount, number} as a testbedfor performance tuning to a particular confusion set.As previously mentioned, we can tune the numberof factors to a particular confusion set.
In the caseof this confusion set, using 120 factors increases theperformance by 6%.
However, tuning this param-eter alone still leaves the performance short of thebaseline predictor.A quick examination of the context in which bothwords appear reveals that a significant percentage(82%) of all training instances contain either the bi-gram of the confusion word preceded by the, fol-lowed by of, or in some cases, both.
For exam-ple, there are many instances of the collocationthe+humber+of in the training data.
However, thereare only one third as many training instances foramount (the less frequent word) as there are fornumber.
This situation leads LSA to believe that thebigrams the+amount and amount+of have more dis-crimination power than the corresponding bigramswhich contain number.
As a result, LSA gives thema higher weight and LSA almost always predictsamount when the confusion word in the test sen-tence appears in this context.
This local context isa poor predictor of the confusion word and its pres-ence tends to dominate the decision made by LSA.By eliminating the words the and of from the train-ing and testing process, we permit the remainingcontext to be used for prediction.
The eliminationof the poor local context combined with the largernumber of factors increases the performance of LSAto 13% above the baseline predictor (compared to11% for Tribayes).
This is a net increase in perfor-mance of 32%!6 Conc lus ionWe've shown that LSA can be used to attack theproblem of identifying contextual misuses of words,particularly when those words are the same part ofspeech.
It has proven to be an effective alternativeto Bayesian classifiers.
Confusions ets whose wordsare different parts of speech are more effectively han-dled using a method which incorporates the word'spart of speech as a feature.
We are exploring tech-niques for introducing part of speech informationinto the LSA space so that the system can makebetter predictions for those sets on which it doesn'tyet measure up to Tribayes.
We've also shown thatfor the cost of experimentation with different param-eter combinations, LSA's performance can be tunedfor individual confusion sets.While the results of this experiment look very nice,they still don't tell us anything about how useful thetechnique is when applied to unedited text.
Thetesting procedure assumes that a confusion wordmust be predicted as if the author of the text hadn'tsupplied a word or that writers misuse the confusionwords nearly 50% of the time.
For example, considerthe case of the confusion set {principal, principle}.The LSA prediction accuracy for this set is 91%.However, it might be the case that, in practice, peo-ple tend to use the Correct word 95% of the time.LSA has thus introduced a 4% error into the writingprocess.
Our continuing work is to explore the errorrate that occurs in unedited text as a means of as-sessing the "true" performance of contextual spellingcorrection systems.7 AcknowledgmentsThe first author is supported under DARPA con-tract SOL BAA95-10.
We gratefully acknowledgethe comments and suggestions of Thomas Landauerand the anonymous reviewers.ReferencesScott Deerwester, Susan T. Dumais, George W. Fur-haS, Thomas K. Landauer, and Richard A. Harsh-man.
1990.
Indexing by Latent Semantic Analy-sis.
Journal of the American Society for Informa-tion Science, 41(6):391-407, September.Susan T. Dumais, George W. Furnas, Thomas K.Landauer, Scott Deerwester, and Richard Harsh-man.
1988.
Using Latent Semantic Analysis toimprove access to textual information.
In HumanFactors in Computing Systems, CHI'88 Confer-ence Proceedings (Washington, D.C.), pages 281-285, New York, May.
ACM.Carl Eckart and Gale Young.
1939.
A principleaxis transformation for non-hermitian matrices.American Mathematical Society Bulletin, 45:118-121.Peter W. Foltz.
1995.
Improving human-proceedings interaction: Indexing the CHI index.In Human Factors in Computing Systems: CHI'95Conference Companion, pages 101-102.
Associa-tions for Computing Machinery (ACM), May.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
A method for disambiguatingword senses in a large corpus.
Computers and theHumanities, 26(5-6):415-439, Dec.172Andrew R. Golding.
1995.
A Bayesian hybridmethod for context-sensitive spelling correction.In Proceedings of the Third Workshop on VeryLarge Corpora, Cambridge, MA.Andrew R. Golding and Yves Schabes.
1996.
Com-bining trigram-based and feature-based methodsfor context-sensitive spelling correction.
In Pro-ceedings of the 34th Annual Meeting of the Associ-ation for Computational Linguistics, Santa Clara,CA, June.
Association for Computational Linguis-tics.Karen Kukich.
1992a.
Spelling correction for thetelecommunications network for the deaf.
Com-munications of the ACM, 35(5):80-90, May.Karen Kukich.
1992b.
Techniques for automaticallycorrecting words in text.
A CM Computing Sur-veys, 24(4):377-439, Dec.Henry KuSera and W. Nelson Francis.
1967.
Com-putational Analysis of Present-Day American En-glish.
Brown University Press, Providence, RI.Thomas K. Landauer and Susan T. Dumais.
(Inpress).
A solution to Plato's problem: The La-tent Semantic Analysis theory of acquisition, in-duction, and representation f knowledge.
Psy-chological Review.Karen E. Lochbaum and Lynn A. Streeter.
1989.Comparing and combining the effectiveness ofLa-tent Semantic Indexing and the ordinary vectorspace model for information retrieval.
Informa-tion Processing CJ Management, 25(6):665-676.Susan W. McRoy.
1992.
Using multiple knowledgesources for word sense disambiguation.
Computa-tional Linguistics, 18(1):1-30, March.M.
F. Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3):130-137, July.Lynn A. Streeter and Karen E. Lochbaum.
1988.An expert/expert-locating system based on au-tomatic representation f semantic structure.
InProceedings of the Fourth Conference on Artifi-cial Intelligence Applications, pages 345-350, SanDiego, CA, March.
IEEE.David Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in Spanish and French.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, pages 88-95, Las Cruces,NM, June.
Association for Computational Lin-guistics.173
