Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 266?277,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsGo Climb a Dependency Tree and Correct the Grammatical ErrorsLongkai Zhang Houfeng WangKey Laboratory of Computational Linguistics (Peking University)Ministry of Education, Chinazhlongk@qq.com, wanghf@pku.edu.cnAbstractState-of-art systems for grammar errorcorrection often correct errors based onword sequences or phrases.
In this paper,we describe a grammar error correctionsystem which corrects grammatical errorsat tree level directly.
We cluster all errorinto two groups and divide our system intotwo modules correspondingly: the generalmodule and the special module.
In thegeneral module, we propose a TreeNodeLanguage Model to correct errors relatedto verbs and nouns.
The TreeNode Lan-guage Model is easy to train and the de-coding is efficient.
In the special module,two extra classification models are trainedto correct errors related to determiners andprepositions.
Experiments show that oursystem outperforms the state-of-art sys-tems and improves the F1score.1 IntroductionThe task of grammar error correction is difficultyet important.
An automatic grammar error cor-rection system can help second language (L2)learners improve the quality of their writing.
In re-cent years, there are various competitions devotedto grammar error correction, such as the HOO-2011(Dale and Kilgarriff, 2011), HOO-2012(Daleet al., 2012) and the CoNLL-2013 shared task (Nget al., 2013).
There has been a lot of work ad-dressing errors made by L2 learners.
A significantproportion of the systems for grammar error cor-rection train individual statistical models to cor-rect each special kind of error word by word andignore error interactions.
These methods assumeno interactions between different kinds of gram-matical errors.
In real problem settings errors arecorrelated, which makes grammar error correctionmuch more difficult.Recent research begins to focus on the errorinteraction problem.
For example, Wu and Ng(2013) decodes a global optimized result basedon the individual correction confidence of eachkind of errors.
The individual correction confi-dence is still based on the noisy context.
Ro-zovskaya and Roth (2013) uses a joint modelingapproach, which considers corrections in phrasestructures instead of words.
For dependencies thatare not covered by the joint learning model, Ro-zovskaya and Roth (2013) uses the results of Illi-nois system in the joint inference.
These resultsare still at word level and are based on the noisycontext.
These systems can consider error inter-actions, however, the systems are complex andinefficient.
In both Wu and Ng (2013) and Ro-zovskaya and Roth (2013), Integer Linear Pro-gramming (ILP) is used for decoding a global op-timized result.
In the worst case, the time com-plexity of ILP can be exponent.In contrast, we think a better grammar error cor-rection system should correct grammatical errorsat sentence level directly and efficiently.
The sys-tem should correct as many kinds of errors as pos-sible in a generalized framework, while allowingspecial models for some kinds of errors that weneed to take special care.
We cluster all error intotwo groups and correspondingly divide our sys-tem into two modules: the general module and thespecial module.
In the general module, our sys-tem views each parsed sentence as a dependencytree.
The system generates correction candidatesfor each node on the dependency tree.
The cor-rection can be made on the dependency tree glob-ally.
In this module, nearly all replacement errorsrelated to verb form, noun form and subject-verbagreement errors can be considered.
In the spe-cial module, two extra classification models areused to correct the determiner errors and preposi-tion errors .
The classifiers are also trained at treenode level.
We take special care of these two kinds266of errors because these errors not only include re-placement errors, but also include insertion anddeletion errors.
A classification model is moresuitable for handling insertion and deletion errors.Besides, they are the most common errors madeby English as a Second Language (ESL) learnersand are much easier to be incorporated into a clas-sification framework.We propose a TreeNode Language Model(TNLM) to efficiently measure the correctness ofselecting a correction candidate of a node in thegeneral module.
Similar to the existing statisticallanguage models which assign a probability to alinear chain of words, our TNLM assigns correct-ness scores directly on each node on the depen-dency tree.
We select candidates for each nodeto maximize the global correctness score and usethese candidates to form the corrected sentence.The global optimized inference can be tackled ef-ficiently using dynamic programming.
Becausethe decoding is based on the whole sentence, errorinteractions can be considered.
Our TNLM onlyneeds to use context words related to each nodeon the dependency tree.
Training a TreeNode lan-guage model costs no more than training ordinarylanguage models on the same corpus.
Experimentsshow that our system can outperform the state-of-art systems.The paper is structured as follows.
Section 1gives the introduction.
In section 2 we describe thetask and give an overview of the system.
In section3 we describe the general module and in section 4we describe the special module.
Experiments aredescribed in section 5.
In section 6 related worksare introduced, and the paper is concluded in thelast section.2 Task and System Overview2.1 Task DescriptionThe task of grammar error correction aims to cor-rect grammatical errors in sentences.
There arevarious competitions devoted to the grammar er-ror correction task for L2 learners.
The CoNLL-2013 shared task is one of the most famous, whichfocuses on correcting five types of errors thatare commonly made by non-native speakers ofEnglish, including determiner, preposition, nounnumber, subject-verb agreement and verb form er-rors.
The training data released by the task orga-nizers come from the NUCLE corpus(Dahlmeieret al., 2013).
This corpus contains essays writ-ten by ESL learners, which are then corrected byEnglish teachers.
The test data are 50 student es-says.
Details of the corpus are described in Nget al.
(2013).2.2 System ArchitectureIn our system, lists of correction candidates arefirst generated for each word.
We generate can-didates for nouns based on their plurality.
We gen-erate candidates for verbs based on their tenses.Then we select the correction candidates that max-imize the overall correctness score.
An exampleprocess of correcting figure 1(a) is shown in table1.Correcting grammatical errors using local sta-tistical models on word sequence is insufficient.The local models can only consider the contextsin a fixed window.
In the example of figure 1(a),the context of the verb ?is?
is ?that boy is on the?,which sounds reasonable at first glance but is in-correct when considering the whole sentence.
Thelimitation of local classifiers is that long distancesyntax information cannot be incorporated withinthe local context.
In order to effectively use thesyntax information to get a more accurate correct-ing result, we think a better way is to tackle theproblem directly at tree level to view the sentenceas a whole.
From figure 1(a) we can see that thenode ?is?
has two children on the dependency tree:?books?
and ?on?.
When we consider the node?is?, its context is ?books is on?, which sounds in-correct.
Therefore, we can make better correctionsusing such context information on nodes.Therefore, our system corrects grammatical er-rors on dependency trees directly.
Because thecorrelated of words are more linked on trees thanin a word sequence, the errors are more easier tobe corrected on the trees and the agreement of dif-ferent error types is guaranteed by the edges.
Wefollow the strategy of treating different kinds oferrors differently, which is used by lots of gram-mar error correction systems.
We cluster the fivetypes of errors considered in CoNLL-2013 intotwo groups and divide our system into two mod-ules correspondingly.?
The general module, which is responsiblefor the verb form errors, noun number errorsand subject-verb agreement errors.
These er-rors are all replacement errors, which canbe corrected by replacing the wrongly usedword with a reasonable candidate word.267Figure 1: Dependency parsing results of (a) the original sentence ?The books of that boy is on the desk.?
(b) the corrected sentence.Position Original Correction Candidates Corrected1 The The The2 books books, book books3 of of of4 that that that5 boy boy, boys boy6 is is,are,am,was,were,be,being,been are7 on on on8 the the the9 desk desk, desks desk10 .
.
.Table 1: An example of the ?correction candidate generation and candidate selection?
framework.?
The special module, where two classifica-tion models are used to correct the determinererrors and preposition errors at tree level.
Wetake special care of these two kinds of errorsbecause these errors include both replace-ment errors and insertion/deletion errors.
Be-sides, they are the most common errors madeby ESL learners and is much easier to be in-corporated into a classification framework.We should make it clear that we are not the firstto use tree level correction models on ungrammat-ical sentences.
Yoshimoto et al.
(2013) uses aTreelet Language model (Pauls and Klein, 2012)to correct agreement errors.
However, the perfor-mance of Treelet language model is not that goodcompared with the top-ranked system in CoNLL-2013.
The reason is that the production rules in theTreelet language model are based on complex con-texts, which will exacerbate the data sparsenessproblem.
The ?context?
in Treelet language modelalso include words ahead of treelets, which aresometimes unrelated to the current node.
In con-trast, our TreeNode Language model only needs toconsider useful context words related to each nodeon the dependency tree.
To train a TreeNode lan-guage model costs no more than training ordinarylanguage models on the same corpus.2.3 Data PreparationOur system corrects grammatical errors on de-pendency trees directly, therefore the sentencesin training and testing data should have beenparsed before being corrected.
In our system, weuse the Stanford parser1to parse the New YorkTimes source of the Gigaword corpus2, and use theparsed sentences as our training data.
We use theoriginal training data provided by CoNLL-2013 asthe develop set to tune all parameters.Some sentences in the news texts use a differ-ent writing style against the sentences written byESL learners.
For example, sentences written byESL learners seldom include dialogues betweenpeople, while very often news texts include para-graphs such as ?
?I am frightened!?
cried Tom?.
Weuse heuristic rules to eliminate the sentences in theGigaword corpus that are less likely to appear inthe ESL writing.
The heuristic rules include delet-1http://nlp.stanford.edu/software/lex-parser.shtml2https://catalog.ldc.upenn.edu/LDC2003T05268ing sentences that are too short or too long3, delet-ing sentences that contains certain punctuationssuch as quotation marks, or deleting sentences thatare not ended with a period.In total we select and parse 5 million sen-tences of the New York Times source of Englishnewswire in the Gigaword corpus.
We build thesystem and experiment based on these sentences.3 The General Module3.1 OverviewThe general module aims to correct verb form er-rors, noun number errors and subject-verb agree-ment errors.
Other replacement errors such asspelling errors can also be incorporated into thegeneral module.
Here we focus on verb form er-rors, noun number errors and subject-verb agree-ment errors only.
Our general module views eachsentence as a dependency tree.
All words in thesentence form the nodes of the tree.
Nodes arelinked through directed edges, annotated with thedependency relations.Before correcting the grammatical errors, thegeneral module should generate correction candi-dates for each node first.
For each node we usethe word itself as its first candidate.
Because thegeneral module considers errors related to verbsand nouns, we generate extra correction candi-dates only for verbs and nouns.
For verbs we useall its verb forms as its extra candidates.
For ex-ample when considering the word ?speaks?, weuse itself and {speak, spoke, spoken, speaking}as its correction candidates.
For nouns we useits singular form and plural form as its extra cor-rection candidates.
For example when consider-ing the word ?dog?, we use itself and ?dogs?
asits correction candidate.
If the system selects theoriginal word as the final correction, the sentenceremains unchanged.
But for convenience we stillcall the newly generated sentence ?the correctedsentence?.In a dependency tree, the whole sentences can be formulized as a list of productionrules r1, ..., rLof the form: [r = head ?modifier1,modifier2...].
An example of allproduction rules of figure 1(a) is shown in table2.
Because the production rules are made up ofwords, selecting a different correction candidatefor only one node will result in a list of different3In our experiment, no less than 5 words and no more than30 words.production rules.
For example, figure 1(b) selectsthe correction candidate ?is?
to replace the origi-nal ?are?.
Therefore the production rules of figure1(b) include [are ?
books, on], instead of [is ?books, on] in figure 1(a).books?
The, ofof?
boyboy?
thatis?
books, onon?
deskdesk?
theTable 2: All the production rules in the example offigure 1(a)The overall correctness score of s, whichis score(s), can be further decomposed into?Li=0score(ri).
A reasonable score functionshould score the correct candidate higher than theincorrect one.
Consider the node ?is?
in Figure1(a), the production rule with head ?is?
is [is ?books, on].
Because the correction of ?is?
is ?are?,a reasonable scorer should have score([is ?books, on]) < score([are?
books, on]).Given the formulation of sentence s =[r1, ..., rL] and the candidates for each node, weare faced with two problems:1.
Score Function.
Given a fixed selection ofcandidate for each node, how to computethe overall score of the dependency tree, i.e.,score(s).
Because score(s) is decomposedinto?Li=0score(ri), the problem becomesfinding a score function to measure the cor-rectness of each r given a fixed selection ofcandidates.2.
Decoding.
Given each node a list of correc-tion candidates and a reasonable score func-tion score(r) for the production rules, how tofind the selection of candidates that maximizethe overall score of the dependency tree.For the first problem, we propose a TreeNodeLanguage Model as the correctness measure of afixed candidate selection.
For the decoding prob-lem, we use a dynamic programming method toefficiently find the correction candidates that max-imize the overall score.
We will describe the de-tails in the following sections.One concern is whether the automaticallyparsed trees are reliable for grammar error cor-rection.
We define ?reliable?
as follows.
If we269change some words in original sentence into theirreasonable correction candidates (e.g.
change ?is?to ?are?)
but the structure of the dependency treedoes not change (except the replaced word andits corresponding POS tag, which are definitelychanged), then we say the dependency tree is reli-able for this sentence.
To verify this we randomlyselected 1000 sentences parsed by the StanfordParser.
We randomly select the verbs and nounsand replace them with a wrong form.
We parsedthe modified sentences again and asked 2 annota-tors to examine whether the dependency trees arereliable for grammar error correction.
We find that99% of the dependency trees are reliable.
There-fore we can see that the dependency tree can beused as the structure for grammar error correctiondirectly.3.2 TreeNode Language ModelIn our system we use the score of TreeNode Lan-guage Model (TNLM) as the scoring function.Consider a node n on a dependency tree and as-sume n has K modifiers C1, ..., CKas its childnodes.
We define Seq(n) = [C1, ..., n, ..., CK]as an ordered sub-sequence of nodes that includesthe node n itself and all its child nodes.
The or-der of the sub-sequence in Seq(n) is sorted basedon their position in the sentence.
In this formula-tion, we can score the correctness of a productionrule r by scoring the correctness of Seq(n).
Be-cause Seq(n) is a word sequence, we can use alanguage model to measure its correctness.
Thesub-sequences are not identical to the originaltext.
Therefore instead of using ordinary languagemodels, we should train special language modelsusing the sub-sequences to measure the correct-ness of a production rule.Take the sentence in figure 2 as an example.When considering the node ?is?
in the word se-quence, it is likely to be corrected into ?are?
be-cause it appear directly after the plural noun ?par-ents?.
However, by the definition above, the sub-sequence corresponding to the node ?damaged?
is?car is damaged by ?.
In such context, the word?is?
is less likely to be changed to ?are?.
Fromthe example we can see that the sub-sequence issuitable to be used to measure the correctness ofa production rule.
From this example we can alsofind that the sub-sequences are different with or-dinary sentences, because ordinary sentences areless likely to end with ?by?.Table 3 shows all the sub-sequences in the ex-ample of figure 2.
If we collect all the sub-sequences in the corpus to form a new sub-sequence corpus, we can train a language modelbased on the new sub-sequence corpus.
This isour TreeNode Language Model.
One advantageof TLM is that once we have generated the sub-sequences, we can train the TLM in the sameway as we train ordinary language models.
Be-sides, the TLM is not limited to a fixed smoothingmethod.
Any smoothing methods for ordinary lan-guage models are applicable for TLM.Node Sub-sentenceThe Thecar The car ofof of parentsmy myparents my parentsis isdamaged car is damaged byby by stormthe thestorm the stormTable 3: All the sub-sentences in the example offigure 2In our system we train the TLM using the sameway as training tri-gram language model.
For asub-sequence S = w0...wL, we calculate P (S) =?Li=0P (wi|wi?1wi?2).
The smoothing methodwe use is interpolation, which assumes the finalP?
(wi|wi?1wi?2) of the tri-gram language modelfollows the following decomposition:P?
(wi|wi?1wi?2) =?1P (wi|wi?1wi?2)+?2P (wi|wi?1)+?3P (wi)(1)where ?1, ?2and ?3are parameters sum to1.
The parameters ?1, ?2and ?3are estimatedthrough EM algorithm(Baum et al., 1970; Demp-ster et al., 1977; Jelinek, 1980).3.3 DecodingThe decoding problem is to select one correctioncandidate for each node that maximizes the over-all score of the corrected sentence.
When the sen-tence is long and contains many verbs and nouns,enumerating all possible candidate selections istime-consuming.
We use a bottom-up dynamic270Figure 2: A illustrative sentence for TreeNode Language Model.programming approach to find the maximized cor-rections within polynomial time complexity.For a node n with L correction candidatesn1, ...nLand K child nodes C1, ..., CK, we definen.scores[i] as the maximum score if we choosethe ith candidate nifor n. Because we decodefrom leaves to the root, C1.scores, ..., CK.scoreshave already been calculated before we calculaten.scores.We assume the sub-sequence Seq(ni) =[C1, ..., CM, ni, CM+1, ..., CK] without loss ofgenerality, where C1, .., CMare the nodes beforeniand CM+1, ..., CKare the nodes after ni.We define ci,jas the jth correction can-didate of child node Ci.
Given a se-lection of candidates for each child nodeseq = [c1,j1, ..., cM,jM, ni, cM+1,jM+1, ..., cK,jK],we can calculate score(seq) as:score(seq) = TNLM(seq)K?i=1Ci.scores[ji](2)where TNLM(seq) is the TreeNode LanguageModel score of seq.
Then, n.scores[i] is calcu-lated as:n.scores[i] = max?seqscore(seq) (3)Because seq is a word sequence, the maxi-mization can be efficiently calculated using Viterbialgorithm (Forney Jr, 1973).
To be specific,the Viterbi algorithm uses the transition scoresand emission scores as its input.
The transitionscores in our model are the tri-gram probabilitiesfrom our tri-gram TNLM.
The emission scoresin our model are the candidate scores of eachchild: C1.scores, ..., CK.scores, which have al-ready been calculated.After the bottom-up calculation, we only needto look into the ?ROOT?
node to find the maxi-mum score of the whole tree.
Similar to the Viterbialgorithm, back pointers should be kept to findwhich candidate is selected for the final correctedsentence.
Detailed decoding algorithm is shown intable 4.Function decode(Node n)if n is leafset n.scores uniformlyreturnfor each child c of ndecode(c)calculating n.scores using ViterbiEnd FunctionBEGINdecode(ROOT )find the maximum score for the tree and back-track all candidatesENDTable 4: The Decoding algorithmIn the real world implementations, we add acontrolling parameter for the confidence of thecorrectness of the inputs.
We multiply ?
onP (w0|w?2w?1) of the tri-gram TNLM if the cor-recting candidate w0is the same word in the orig-inal input.
?
is larger than 1 to ?emphasis?
theconfidence of the original word because the mostof the words in the inputs are correct.
The value of?
can be set using the development data.3.4 The Special ModuleThe special module is designed for determiner er-rors and preposition errors.
We take special careof these two kinds of errors because these errorsinclude insertion and deletion errors, which can-not be corrected in the general module.
Becausethere is a fixed number of prepositions and deter-miners, these two kinds of errors are much easierto be incorporated into a classification framework.Besides, they are the most common errors made byESL learners and there are lots of previous worksthat leave valuable guidance for us to follow.Similar to many previous state-of-art systems,we treat the correction of determiner errors andpreposition errors as a classification problem.
Al-though some previous works (e.g.
Rozovskayaet al.
(2013)) use NPs and the head of NPs as271features, they are basically local classifiers mak-ing predictions on word sequences.
Difference tothe local classifier approaches, we make predic-tions on the nodes of the dependency tree directly.In our system we correct determiner errors andpreposition errors separately.For the determiner errors, we consider the in-sertion, deletion and replacement of articles (i.e.
?a?, ?an?
and ?the?).
Because the articles are usedto modify nouns in the dependency trees, we canclassify based on noun nodes.
We give each nounnode (node whose POS tag is noun) a label to in-dicate which article it should take.
We use left po-sition (LP) and right position (RP) to specify theposition of the article.
The article therefore lo-cates between LP and RP.
If a noun node alreadyhas an article as its modifier, then LP will be theposition directly ahead of the article.
In this case,RP = LP + 2.
If an insertion is needed, the RPis the position of the first child node of the nounnode.
In this case LP = RP ?
1.
With this no-tation, detailed feature templates we use to correctdeterminer errors are listed in table 5.
In our modelwe use 3 labels: ?a?, ?the?
and ???.
We use ?a?, ?the?to represent a noun node should be modified with?a?
or ??the?
correspondingly.
We use ????
to in-dicate that no article is needed for the noun node.We use rule-based method to distinguish between?a?
and ?an?
as a post-process.For the preposition errors, we only considerdeletion and replacement of an existing preposi-tion.
The classification framework is similar todeterminer errors.
We consider classification onpreposition nodes (nodes whose POS tag is prepo-sition).
We use prepositions as labels to indicatewhich preposition should be used.
and use ??
?to denote that the preposition should be deleted.We use the same definition of LP and RP as thecorrection of determiner errors.
Detailed featuretemplates we use to correct preposition errors arelisted in table 6.
Similar to the previous work(Xinget al., 2013), we find that adding more preposi-tions will not improve the performance in our ex-periments.
Thus we only consider a fixed set ofprepositions: {in, for, to, of, on}.Previous works such as Rozovskaya et al.
(2013) show that Naive Bayes model and averagedperceptron model show better results than otherclassification models.
These classifiers can give areasonably good performance when there are lim-ited amount of training data.
In our system, we uselarge amount of automatically generated trainingdata based on the parsed Gigaword corpus insteadof the limited training data provided by CoNLL-2013.Take generating training data for determiner er-rors as an example.
We generate training databased on the parsed Gigaword corpus C describedin section 2.
Each sentence S in C is a depen-dency tree T .
We use each noun node N on T asone training instance.
If N is modified by ?the?,its label will be ?THE?.
If N is modified by ?a?or ?an?, its label will be ?A?.
Otherwise its labelwill be ?NULL?.
Then we just omit the determinermodifier and generate features based on table 5.Generating training data for preposition errors isthe same, except we use preposition nodes insteadof noun nodes.By generating training instances in this way, wecan get large amount of training data.
Thereforewe think it is a good time to try different classifi-cation models with enough training data.
We ex-periment on Naive Bayes, Averaged Perceptron,SVM and Maximum Entropy models (ME) in a 5-fold cross validation on the training data.
We findME achieves the highest accuracy.
Therefore weuse ME as the classification model in our system.4 Experiment4.1 Experiment SettingsIn the experiments, we use our parsed Gigawordcorpus as the training data, use the training dataprovided by CoNLL-2013 as the develop data, anduse the test data of CoNLL-2013 as test data di-rectly.
In the general module, the training datais used for the training of TreeNode LanguageModel.
In the special module, the training data isused for training individual classification models.We use the M2 scorer (Dahlmeier and Ng,2012b) provided by the organizer of CoNLL-2013for the evaluation of our system.
The M2 scoreris widely used as a standard scorer in previoussystems.
Because we make comparison with thestate-of-art systems on the CoNLL-2013 corpus,we use the same evaluation metric F1score of M2scorer as the evaluation metric.In reality, some sentences may have more thanone kind of possible correction.
As the example in?The books of that boy is on the desk.
?, the cor-responding correction can be either ?The books ofthat boy are on the desk.?
or ?The book of that boyis on the desk.?.
The gold test data can only con-272Word Features wLP, wLP?1, wLP?2, wRP, wRP+1, wRP+2, wLP?2wLP?1, wLP?1wLP,wLPwRP, wRPwRP+1, wRP+1wRP+2, wLP?2wLP?1wLP, wLP?1wLPwRP,wLPwRPwRP+1, wRPwRP+1wRP+2Noun NodeFeaturesNN , wLPNN , wLP?1wLPNN , wLP?2wLP?1wLPNNFather/ChildNode FeaturesFa, wRPFa, wRPwRP+1Fa, wRPwRP+1wRP+2Fa, Fa&ChTable 5: Feature templates for the determiner errors.
wiis the word at the ith position.
NN is the currentnoun node.
Fa is the father node of the current noun node.
Ch is a child node of the current noun node.Word Features wLP, wLP?1, wLP?2, wRP, wRP+1, wRP+2, wLP?2wLP?1, wLP?1wLP,wLPwRP, wRPwRP+1, wRP+1wRP+2, wLP?2wLP?1wLP, wLP?1wLPwRP,wLPwRPwRP+1, wRPwRP+1wRP+2Father/ChildNode FeaturesFa, wRPFa, wRPwRP+1Fa, wRPwRP+1wRP+2Fa, Fa&ChTable 6: Feature templates for preposition errors.
wiis the word at the ith position.
Fa is the father nodeof the current preposition node.
Ch is a child node of the current preposition node.sider a small portion of possible answers.
To re-lieve this, the CoNLL-2013 shared task allows allparticipating teams to provide alterative answersif they believe their system outputs are also cor-rect.
These alterative answers form the ?RevisedData?
in the shared task, which indeed help evalu-ate the outputs of the participating systems.
How-ever, the revised data only include alterative cor-rections from the participating teams.
Thereforethe evaluation is not that fair for future systems.
Inour experiment we only use the original test dataas the evaluation dataset.4.2 Experiment ResultsWe first show the performance of each stage of oursystem.
In our system, the general module andthe special module correct grammar errors conse-quently.
Therefore in table 7 we show the perfor-mance when each component is added to the sys-tem.Method P R F1 scoreTNLM 33.96% 17.71% 23.28%+Det 32.83% 38.28% 35.35%+Prep 32.64% 39.20% 35.62%Table 7: Results of each stage in our system.TNLM is the general module.
?+Det?
is the sys-tem containing the general module and determinerpart of special module.?+Prep?
is the final systemWe evaluate the effect of using TreeNode lan-guage model for the general module.
We comparethe TNLM with ordinary tri-gram language model.We use the same amount of training data and thesame smoothing strategy (i.e.
interpolation) forboth of them.
Table 8 shows the comparison.
TheTNLM can improve the F1by +2.1%.Method P R F1 scoreOrdinary LM 29.27% 16.68% 21.27%Our TNLM 33.96% 17.71% 23.28%Table 8: Comparison for the general modulebetween TNLM and ordinary tri-gram languagemodel on the test data.Based on the result of the general module usingTNLM, we compare our tree level special mod-ule against the local classification approach.
Thespecial module of our system makes predictionson the dependency tree directly, while local clas-sification approaches make predictions on linearchain of words and decide the article of a nounPhrase or the preposition of a preposition phrase.We use the same word level features for the twoapproaches except for the local classifiers we donot add tree level features.
Table 9 shows the com-parison.When using the parsed Gigaword texts as train-ing data, the quality of the sentences we selectwill influence the result.
For comparison, we ran-domly select the same amount of sentences fromthe same source of Gigaword and parse them asa alterative training set.
Table 10 shows the com-parison between random chosen training data and273Method P R F1 scoreLocal Classifier 26.38% 39.14% 31.51%Our Tree-based 32.64% 39.20% 35.62%Table 9: Comparison for the special module on thetest data.
The input of the special module is thesentences corrected by the TNLM in the generalmodule.the selected training data of our system.
We cansee that the data selection (cleaning) procedure isimportant for the improvement of system F1.Method P R F1 scoreRandom 31.89% 35.85% 33.75%Selected 32.64% 39.20% 35.62%Table 10: Comparison of training using randomchosen sentences and selected sentences.Method F1 scoreRozovskaya et al.
(2013) 31.20%Kao et al.
(2013) 25.01%Yoshimoto et al.
(2013) 22.17%Rozovskaya and Roth (2013) 35.20%Our method 35.62%Table 11: Comparison of F1 of different systemson the test data .4.3 Comparison With Other SystemsWe also compare our system with the state-of-art systems.
The first two are the top-2 systemsat CoNLL-2013 shared task : Rozovskaya et al.
(2013) and Kao et al.
(2013).
The third one isthe Treelet Language Model in Yoshimoto et al.(2013).
The fourth one is Rozovskaya and Roth(2013), which until now shows the best perfor-mance.
The comparison on the test data is shownin table 11.In CoNLL-2013 only 5 kinds of errors are con-sidered.
Our system can be slightly modified tohandle the case where other errors such as spellingerrors should be considered.
In that case, we canmodify the candidate generation of the generalmodule.
We only need to let the generate cor-rection candidates be any possible words that aresimilar to the original word, and run the same de-coding algorithm to get the corrected sentence.
Asa comparison, the ILP systems should add extrascoring system to score extra kind of errors.5 Related WorksEarly grammatical error correction systems usethe knowledge engineering approach (Murata andNagao, 1994; Bond et al., 1996; Bond and Ikehara,1996; Heine, 1998).
However, manually designedrules usually have exceptions.
Therefore, the ma-chine learning approach has become the dominantapproach recently.
Previous machine learning ap-proaches typically formulates the task as a clas-sification problem.
Of all the errors, determinerand preposition errors are the two main researchtopics (Knight and Chander, 1994; AEHAN et al.,2006; Tetreault and Chodorow, 2008; Dahlmeierand Ng, 2011).
Features used in the classifica-tion models include the context words, POS tags,language model scores (Gamon, 2010), and treelevel features (Tetreault et al., 2010).
Models usedinclude maximum entropy (AEHAN et al., 2006;Tetreault and Chodorow, 2008), averaged percep-tron, Naive Bayes (Rozovskaya and Roth, 2011),etc.
Other errors such as verb form and noun num-ber errors also attract some attention recently (Liuet al., 2010; Tajiri et al., 2012).Recent research efforts have started to deal withcorrecting different errors jointly (Gamon, 2011;Park and Levy, 2011; Dahlmeier and Ng, 2012a;Wu and Ng, 2013; Rozovskaya and Roth, 2013).Gamon (2011) uses a high-order sequential label-ing model to detect various errors.
Park and Levy(2011) models grammatical error correction usinga noisy channel model.
Dahlmeier and Ng (2012a)uses a beam search decoder, which iteratively cor-rects to produce the best corrected output.
Wu andNg (2013) and Rozovskaya and Roth (2013) useILP to decode a global optimized result.
The jointlearning and joint inference are still at word/phraselevel and are based on the noisy context.
In theworst case, the time complexity of ILP can reachexponent.
In contrast, our system corrects gram-mar errors at tree level directly, and the decodingis finished with polynomial time complexity.6 Conclusion and Future workIn this paper we describe our grammar error cor-rection system which corrects errors at tree leveldirectly.
We propose a TreeNode Language Modeland use it in the general module to correct errorsrelated to verbs and nouns.
The TNLM is easy totrain and the decoding of corrected sentence is ef-ficient.
In the special module, two extra classifica-tion models are trained to correct errors related to274determiners and prepositions at tree level directly.Because our current method depends on an auto-matically parsed corpus, future work may includeapplying some additional filtering (e.g.
Mejer andCrammer (2012)) of the extended training set ac-cording to some confidence measure of parsing ac-curacy.AcknowledgmentsThis research was partly supported by Na-tional Natural Science Foundation of China(No.61370117,61333018), Major National SocialScience Fund of China (No.12&ZD227) and Na-tional High Technology Research and Devel-opment Program of China (863 Program) (No.2012AA011101).
The contact author of this pa-per, according to the meaning given to this roleby Key Laboratory of Computational Linguistics,Ministry of Education, School of Electronics En-gineering and Computer Science, Peking Univer-sity, is Houfeng Wang.
We thank Longyue Wangand the reviewers for their comments and sugges-tions.ReferencesAEHAN, N., Chodorow, M., and LEACOCK,C.
L. (2006).
Detecting errors in english arti-cle usage by non-native speakers.Baum, L. E., Petrie, T., Soules, G., and Weiss, N.(1970).
A maximization technique occurring inthe statistical analysis of probabilistic functionsof markov chains.
The annals of mathematicalstatistics, pages 164?171.Bond, F. and Ikehara, S. (1996).
When and how todisambiguate?
?countability in machine trans-lation?.
In International Seminar on Multi-modal Interactive Disambiguation: MIDDIM-96, pages 29?40.
Citeseer.Bond, F., Ogura, K., and Kawaoka, T. (1996).Noun phrase reference in japanese-to-englishmachine translation.
arXiv preprint cmp-lg/9601008.Dahlmeier, D. and Ng, H. T. (2011).
Grammaticalerror correction with alternating structure opti-mization.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume 1, pages 915?923.
Association forComputational Linguistics.Dahlmeier, D. and Ng, H. T. (2012a).
A beam-search decoder for grammatical error correc-tion.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning, pages 568?578.
Associa-tion for Computational Linguistics.Dahlmeier, D. and Ng, H. T. (2012b).
Better eval-uation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics: Human Language Tech-nologies, pages 568?572.
Association for Com-putational Linguistics.Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).Building a large annotated corpus of learner en-glish: The nus corpus of learner english.
In Pro-ceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applica-tions, pages 22?31.Dale, R., Anisimoff, I., and Narroway, G. (2012).Hoo 2012: A report on the preposition and de-terminer error correction shared task.
In Pro-ceedings of the Seventh Workshop on Build-ing Educational Applications Using NLP, pages54?62.
Association for Computational Linguis-tics.Dale, R. and Kilgarriff, A.
(2011).
Helping ourown: The hoo 2011 pilot shared task.
In Pro-ceedings of the 13th European Workshop onNatural Language Generation, pages 242?249.Association for Computational Linguistics.Dempster, A. P., Laird, N. M., Rubin, D. B., et al.(1977).
Maximum likelihood from incompletedata via the em algorithm.
Journal of the Royalstatistical Society, 39(1):1?38.Forney Jr, G. D. (1973).
The viterbi algorithm.Proceedings of the IEEE, 61(3):268?278.Gamon, M. (2010).
Using mostly native datato correct errors in learners?
writing: a meta-classifier approach.
In Human Language Tech-nologies: The 2010 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, pages 163?171.
As-sociation for Computational Linguistics.Gamon, M. (2011).
High-order sequence model-ing for language learner error detection.
In Pro-ceedings of the 6th Workshop on Innovative Useof NLP for Building Educational Applications,275pages 180?189.
Association for ComputationalLinguistics.Heine, J. E. (1998).
Definiteness predictionsfor japanese noun phrases.
In Proceedingsof the 36th Annual Meeting of the Associa-tion for Computational Linguistics and 17thInternational Conference on ComputationalLinguistics-Volume 1, pages 519?525.
Associ-ation for Computational Linguistics.Jelinek, F. (1980).
Interpolated estimation ofmarkov source parameters from sparse data.Pattern recognition in practice.Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,Boisson, J., Wu, J.-c., and Chang, J. S. (2013).Conll-2013 shared task: Grammatical error cor-rection nthu system description.
In Proceed-ings of the Seventeenth Conference on Compu-tational Natural Language Learning: SharedTask, pages 20?25, Sofia, Bulgaria.
Associationfor Computational Linguistics.Knight, K. and Chander, I.
(1994).
Automatedpostediting of documents.
In AAAI, volume 94,pages 779?784.Liu, X., Han, B., Li, K., Stiller, S. H., and Zhou,M.
(2010).
Srl-based verb selection for esl.In Proceedings of the 2010 conference on em-pirical methods in natural language process-ing, pages 1068?1076.
Association for Compu-tational Linguistics.Mejer, A. and Crammer, K. (2012).
Are yousure?
confidence in prediction of dependencytree edges.
In Proceedings of the 2012 Con-ference of the North American Chapter of theAssociation for Computational Linguistics: Hu-man Language Technologies, pages 573?576,Montr?eal, Canada.
Association for Computa-tional Linguistics.Murata, M. and Nagao, M. (1994).
Determinationof referential property and number of nouns injapanese sentences for machine translation intoenglish.
arXiv preprint cmp-lg/9405019.Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., andTetreault, J.
(2013).
The conll-2013 shared taskon grammatical error correction.
In Proceed-ings of the Seventeenth Conference on Compu-tational Natural Language Learning: SharedTask, pages 1?12, Sofia, Bulgaria.
Associationfor Computational Linguistics.Park, Y.
A. and Levy, R. (2011).
Automatedwhole sentence grammar correction using anoisy channel model.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies-Volume 1, pages 934?944.
Asso-ciation for Computational Linguistics.Pauls, A. and Klein, D. (2012).
Large-scale syn-tactic language modeling with treelets.
In Pro-ceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics: LongPapers-Volume 1, pages 959?968.
Associationfor Computational Linguistics.Rozovskaya, A., Chang, K.-W., Sammons, M.,and Roth, D. (2013).
The university of illi-nois system in the conll-2013 shared task.In Proceedings of the Seventeenth Conferenceon Computational Natural Language Learning:Shared Task, pages 13?19, Sofia, Bulgaria.
As-sociation for Computational Linguistics.Rozovskaya, A. and Roth, D. (2011).
Algorithmselection and model adaptation for esl correc-tion tasks.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume 1, pages 924?933.
Association forComputational Linguistics.Rozovskaya, A. and Roth, D. (2013).
Joint learn-ing and inference for grammatical error correc-tion.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Pro-cessing, pages 791?802, Seattle, Washington,USA.
Association for Computational Linguis-tics.Tajiri, T., Komachi, M., and Matsumoto, Y.(2012).
Tense and aspect error correction foresl learners using global context.
In Proceed-ings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: ShortPapers-Volume 2, pages 198?202.
Associationfor Computational Linguistics.Tetreault, J., Foster, J., and Chodorow, M. (2010).Using parse features for preposition selectionand error detection.
In Proceedings of the acl2010 conference short papers, pages 353?358.Association for Computational Linguistics.Tetreault, J. R. and Chodorow, M. (2008).
Theups and downs of preposition error detec-tion in esl writing.
In Proceedings of the22nd International Conference on Computa-276tional Linguistics-Volume 1, pages 865?872.Association for Computational Linguistics.Wu, Y. and Ng, H. T. (2013).
Grammatical errorcorrection using integer linear programming.
InProceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Vol-ume 1: Long Papers), pages 1456?1465, Sofia,Bulgaria.
Association for Computational Lin-guistics.Xing, J., Wang, L., Wong, D. F., Chao, L. S., andZeng, X.
(2013).
Um-checker: A hybrid sys-tem for english grammatical error correction.In Proceedings of the Seventeenth Conferenceon Computational Natural Language Learning:Shared Task, pages 34?42, Sofia, Bulgaria.
As-sociation for Computational Linguistics.Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-machi, M., and Matsumoto, Y.
(2013).
Naist at2013 conll grammatical error correction sharedtask.
CoNLL-2013, 26.277
