Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1?8Manchester, August 2008Two-Phased Event Relation Acquisition:Coupling the Relation-Oriented and Argument-Oriented ApproachesShuya Abe Kentaro Inui Yuji MatsumotoGraduate School of Information Science,Nara Institute of Science and Technology{shuya-a,inui,matsu}@is.naist.jpAbstractAddressing the task of acquiring semanticrelations between events from a large cor-pus, we first argue the complementarity be-tween the pattern-based relation-orientedapproach and the anchor-based argument-oriented approach.
We then propose a two-phased approach, which first uses lexico-syntactic patterns to acquire predicate pairsand then uses two types of anchors to iden-tify shared arguments.
The present resultsof our empirical evaluation on a large-scaleJapanese Web corpus have shown that (a)the anchor-based filtering extensively im-proves the accuracy of predicate pair ac-quisition, (b) the two types of anchors arealmost equally contributive and combiningthem improves recall without losing accu-racy, and (c) the anchor-based method alsoachieves high accuracy in shared argumentidentification.1 IntroductionThe growing interest in practical NLP applicationssuch as question answering, information extrac-tion and multi-document summarization places in-creasing demands on the processing of relationsbetween textual fragments such as entailment andcausal relations.
Such applications often need torely on a large amount of lexical semantic knowl-edge.
For example, a causal (and entailment) rela-tion holds between the verb phrases wash some-thing and something is clean, which reflects thecommonsense notion that if someone has washedc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.something, this object is clean as a result of thewashing event.
A crucial issue is how to ob-tain and maintain a potentially huge collection ofsuch event relation instances.
This paper addressesthe issue of how to automatically acquire such in-stances of relations between events (henceforth,event relation instances) from a large-scale textcollection.Motivated by this issue, several research groupshave reported their experiments on automatic ac-quisition of causal, temporal and entailment rela-tions between event expressions (typically verbsor verb phrases) (Lin and Pantel, 2001; Inui etal., 2003; Chklovski and Pantel, 2005; Torisawa,2006; Pekar, 2006; Zanzotto et al, 2006; Abe etal., 2008, etc.).
As we explain below, however,none of these studies fully achieves the goal wepursue in this paper.An important aspect to consider in event relationacquisition is that each event has arguments.
Forexample, the causal relation between wash some-thing and something is clean can be representednaturally as:(1) wash(obj:X) ?causeis clean(subj:X),where X is a logical variable denoting that thefiller of the object slot of the wash event should beshared (i.e.
identical) with the filler of the subjectslot of the is clean event.To be more general, an instance of a given rela-tion R can be represented as:(2) pred1(arg1:X) ?Rpred2(arg2:X),where prediis a natural language predicate, typ-ically a verb or adjective, and X is a logical vari-able denoting which argument of one predicate andwhich argument of the other are shared.
The goalwe pursue in this paper is therefore not only (a)to find predicate pairs that are of a given relation1type, but also (b) to identify the arguments sharedbetween the predicates if any.
We call the for-mer subtask predicate pair acquisition and the lat-ter shared argument identification.
As we reviewin the next section, however, existing state-of-the-art methods for event relation acquisition are de-signed to achieve only either of these two subtasksbut not both.
In this paper, we propose a two-phased method, which first uses lexico-syntacticpatterns to acquire predicate pairs for a given re-lation type and then uses two kinds of anchors toidentify shared arguments.2 Previous workExisting methods for event relation acquisition canbe classified into two approaches, which we callthe pattern-based approach and anchor-based ap-proach in this paper.The common idea behind the pattern-based ap-proach is to use a small number of manually se-lected generic lexico-syntactic co-occurrence pat-terns (LSPs or simply patterns).
Perhaps the sim-plest way of using LSPs for event relation acqui-sition can be seen in the method Chklovski andPantel (2005) employ to develop their knowledgeresource called VerbOcean.
Their method uses asmall number of manually selected generic LSPssuch as to ?Verb-X?
and then ?Verb-Y?1to obtainsix types of semantic relations including strength(e.g.
taint ?
poison) and happens-before (e.g.marry ?
divorce).
The use of such generic patterns,however, tends to be high recall but low precision.Chklovski and Pantel (2005), for example, reportthat their method obtains about 29,000 verb pairswith 65.5% precision.This low-precision problem requires an addi-tional component for pruning extracted relations.This issue has been addressed from a variety ofangles.
For example, some devise heuristic sta-tistical scores and report their impact on preci-sion (Chklovski and Pantel, 2005; Torisawa, 2006;Zanzotto et al, 2006).
Another way is to incor-porate a classifier trained with supervision.
Inuiet al (2003), for example, use a Japanese genericcausal connective marker tame (because) and asupervised classifier learner to separately obtainfour types of causal relations: cause, precondi-tion, effect and means.
More recently, Abe etal.
(2008) propose to extend Pantel and Pennac-1A ??
included in an LSP denotes, throughout this paper,a variable slot to be filled with an event expression.
The fillerof ??
denotes either the lexical or syntactic constraints on theslot or an example that is to fill the slot.chiotti (2006)?s Espresso algorithm, which inducesspecific reliable LSPs in a bootstrapping man-ner for entity-entity relation extraction, so thatthe extended algorithm can apply to event rela-tions.
Their method learns a large number of rel-atively specific patterns such as cannot ?find out(something)?
due to the lack of ?investigation?
in aboot-strapping fashion, which produces a remark-able improvement on precision.The anchor-based approach, on the other hand,has emerged mainly in the context of paraphraseand entailment acquisition.
This approach uses in-formation of argument fillers (i.e.
anchors) of eachevent expression as a useful clue for identifyingevent relations.
A popular way of using such ar-gument information relies on the distributional hy-pothesis (Harris, 1968) and identifies synonymousevent expressions by seeking a set of event expres-sions whose argument fillers have a similar distri-bution.
Such algorithms as DIRT (Lin and Pantel,2001) and TE/ASE (Szpektor et al, 2004) repre-sent this line of research.Another way of using argument information isproposed by Pekar (2006), which identifies candi-date verb pairs for the entailment relation by im-posing criteria: (a) the two verbs must appear inthe same local discourse-related context and (b)their arguments need to refer to the same par-ticipant, i.e.
anchor.
For example, if a pair ofclauses Mary bought a house.
and The house be-longs to Mary.
appear in a single local discourse-related context, two pairs of verbs, buy(obj:X) ?belong(subj:X) and buy(subj:X) ?
belong(to:X) areidentified as candidate entailment pairs.It is by now clear that the above two approaches,which apparently have emerged somewhat inde-pendently, could play a complementary role witheach other.
Pattern-based methods, on the onehand, are designed to be capable of discriminat-ing relatively fine-grained relation types.
For ex-ample, the patterns used by Chklovski and Pan-tel (2005) identify six relation types, while Abeet al (2008) identify two of the four causal rela-tion types defined by Inui et al (2003).
However,these methods are severely limited for the purposeof shared argument identification because lexico-syntactic patterns are not a good indication ofargument-shared structure in general.
The anchor-based approach, on the other hand, works well foridentifying shared arguments simply because it re-lies on argument information in identifying syn-onymous or entailment verb pairs.
However, it hasno direct means to discriminate more fine-grained2specific relations such as causality and backwardpresupposition.
To sum up, the pattern-based ap-proach tends to be rather relation-oriented whilethe anchor-based approach tends to be argument-oriented.In spite of this complementarity, however, to ourbest knowledge, the issue of how to benefit fromboth approaches has never been paid enough at-tention.
An interesting exception could be foundin Torisawa (2006)?s method of combining verbpairs extracted with a highly generic connectivepattern ?Verb-X?
and ?Verb-Y?
together with theco-occurrence statistics between verbs and their ar-guments.
While the reported results for inferencerules with temporal ordering look promising, it isnot clear yet, however, whether the method ap-plies to other types of relations because it relieson relation-specific heuristics.3 Two-phased event relation acquisition3.1 The basic ideaThe complementarity between the pattern-basedrelation-oriented approach and the anchor-basedargument-oriented approach as discussed abovenaturally leads us to consider combining them.The method we explore in this paper is illustratedin Figure 1.
The overall process has two phrases:predicate pair acquisition followed by shared ar-gument identification.
Given a relation type for ac-quisition, we first acquire candidate predicate pairsthat are likely to be of the given relation exploitinga state-of-the-art pattern-based method.
We then,in the second phase, seek anchors indicative of theshared argument for each acquired predicate pair.We consider two kinds of anchors: instance-basedanchors and type-based anchors.
If anchors arefound, the predicate pair is verified and the asso-ciated argument pair is identified as the shared ar-gument; otherwise, the predicate pair is discarded.As we demonstrate in the section for empiricalevaluation, this verification process boosts the ac-curacy as well as identifying shared arguments.3.2 Predicate pair acquisitionFor predicate pair acquisition, we can chooseone from a range of state-of-the-art pattern-basedmethods.
Among others, in our experiments, weadopted Abe et al (2008)?s method because it hadan advantage in that it was capable of learning pat-terns as well as relation instances.Abe et al (2008)?s method is based on Pan-tel and Pennacchiotti (2006)?s Espresso algorithm,which is originally designed to acquire relationsbetween entities.
Espresso takes as input a smallnumber of seed instances of a given target rela-tion and iteratively learns co-occurrence patternsand relation instances in a bootstrapping manner.Abe et al have made several extensions to it sothat it can be applied to event relations.
Since thedetails of this phase are not the focus of this paper,we refer the reader to (Abe et al, 2008) for furtherinformation.3.3 Shared argument identificationFor each of the predicate pairs acquired in the pre-vious phase, in shared argument identification, weuse anchors to identify which argument is sharedbetween the predicate pair.
To find anchors indica-tive of shared arguments, we have so far examinedtwo methods.
We detail each below.3.3.1 Instance-based anchorsInspired by Pekar (2006)?s way of using an-chors for verb entailment acquisition, we assumethat if two related predicates have a shared argu-ment, they must tend to appear in the same localdiscourse-related context with the shared argumentfilled with the same noun phrase (i.e.
anchor).As an example, let us consider discourse (2a) inFigure 1.
In this local discourse context, the nounbread appears twice, and one bread fills the subjectslot of burn while the other fills the object slot ofbake.
In such a case, we assume the two breads re-fer to the same object, namely anchor, and the sub-ject of burn and the object of bake are shared witheach other.
We call such anchors instance-basedanchors for the sake of contrast with type-basedanchors, which we describe in 3.3.2.We implement this assumption in the followingway.
Given a pair of predicates Pred1and Pred2,we search a corpus for tuples ?Pred1-Arg1; Pred2,Arg2; Anc?
satisfying the following conditions:(a) Anchor word Anc is the head of a noun phrasefilling argument Arg1of Pred1appearing in aWeb page.
(b) Anc also fills argument Arg2of Pred2appear-ing in the same Web page as above.
(c) Anc must not be any of those in the stop list.
(d) pmi(Predi, Argi) ?
?1.0 for i ?
{1, 2}For our experiments, we manually created the stoplist, which contained 219 words including pro-nouns, numerals and highly generic nouns such as3Figure 1: Two-phased event relation acquisition4???
(thing)?, ???
(thing)?
and ???
(time)?.pmi(Predi, Argi) in condition (d) is the point-wisemutual information between Prediand Argi.
Thiscondition is imposed for pruning wrong anchorsmisidentified due to parsing errors.While Pekar carefully defines boundaries of lo-cal discourse-related context, we simply assumethat every pair of predicates sharing an anchor ina Web page is somewhat related ?
unlike Pekar,we do not impose such constraints as paragraphboundaries.
Nevertheless, as we show later inthe evaluation section, our assumption works pre-cisely enough because the looseness of our dis-course boundary constraint is compensated by theconstraints imposed by lexico-syntactic patterns.We finally calculate an anchor set for each argu-ment pair Pred1-Arg1and Pred2-Arg2by accumu-lating the obtained tuples:AnchorSet(Pred1-Arg1, Pred2-Arg2)= {Arg|?Pred1-Arg1; Pred2-Arg2; Anc?
}.3.3.2 Type-based anchorsLet us consider sentences (3a) and (3b) inFigure 1.
These two sentences both contain pred-icates bake and burn.
In (3a), the noun bread fillsthe object slot of bake, while in (3b) the same nounbread fills the subject slot of burn.
In such a case,we assume the noun bread to be an anchor indi-cating that the object of bake and the subject ofburn are shared with each other.
We call such an-chors type-based anchors because bread in (3a)and bread in (3b) do not refer to the same objectbut are identical just as type.Given a pair of predicates Pred1and Pred2, wesearch a corpus for sentences where Pred1andPred2co-occur, and calculate the frequency countsof their argument fillers appearing in those sen-tences:?
If argument Arg1of Pred1is filled by nounAnc, increment the count of ?Pred1-Arg1;Pred2; Anc?.?
If argument Arg2of Pred2is filled by nounAnc, increment the count of ?Pred1; Pred2-Arg2; Anc?.We then identify the intersection between the fillersets of Pred1-Arg1and Pred2-Arg2as the anchorset of that argument pair.
Namely,AnchSet(Pred1-Arg1, Pred2-Arg2) = S1?
S2,whereS1= {Arg|?Pred1-Arg1; Pred2; Anc?
},S2= {Arg|?Pred1; Pred2-Arg2; Anc?
}.3.3.3 Application of anchor setsWe say an argument pair covered by anchorsonly if any anchor is found for it.
Analogously,we say a predicate pair covered by anchors only ifany argument pair associated with it is covered byanchors.
In the phase of shared argument identifi-cation, for each given predicate pair, we carry outthe following procedure:1.
Discard the predicate pair if it is not coveredby anchors.2.
Choose maximally k-most frequent argumentpairs associated with the predicate pair (k = 3in our experiments).3.
Choose maximally l-most frequent anchorsfor each chosen argument pair (l = 3).4 Experiments4.1 SettingsFor an empirical evaluation, we used a sampleof approximately 500M sentences taken from theWeb corpus collected by Kawahara and Kuro-hashi (2006).
The sentences were dependency-parsed with CaboCha (Kudo and Matsumoto,2002), and co-occurrence samples of event men-tions were extracted.
Event mentions with patternswhose frequency was less than 20 were discardedin order to reduce computational costs.In our experiments, we considered two of Inui etal.
(2003)?s four types of causal relations: action-effect relations (Effect in Inui et al?s terminology)and action-means relations (Means).
An action-effect relation holds between events x and y if andonly if non-volitional event y is likely to happen aseither a direct or indirect effect of volitional actionx.
For example, the action X-ga undou-suru (X ex-ercises) and the event X-ga ase-o-kaku (X sweats)are considered to be in this type of relation.
Wedid not require the necessity for an effect.
For ex-ample, while nomu (drink) does not necessarily re-sult in futsukayoi-ni naru (have a hangover), theassessors judged this pair correct because one canat least say that the latter sometimes happens as aresult of the former.
An action-means relation, onthe other hand, holds between events x and y if andonly if volitional action y is likely to be done as apart/means of volitional action x.
For example, if5case a event-pair is X-ga hashiru (X runs) is con-sidered as a typical action that is often done as apart of the action X-ga undou-suru (X exercises).For our experiments, we manually built a lex-icon of over 12,000 verbs with volitionality la-bels, obtaining 8,968 volitional verbs, 3,597 non-volitional and 547 ambiguous.
Volitional verbsinclude taberu (eat) and kenkyu-suru (research),while non-volitional verbs include atatamaru (getwarm), kowareru (to break-vi) and kanashimu (besad).
Volitionality information was used as a fea-ture of predicate slots in pattern-based predicatepair acquisition.4.2 Results and discussion4.2.1 Predicate pair acquisitionWe ran the extended Espresso algorithm start-ing with 25 positive and 4 negative seed rela-tion instances for the action-effect relation and 174positive and 131 negative seed relations for theaction-means relation.
As a result, we obtained9,511 patterns with 22,489 relation instances foraction-effect and 14,119 co-occurrence patternswith 13,121 relation instances for action-meansafter 40 iterations of pattern and instance rank-ing/selection.
The threshold parameters for select-ing patterns and instances were decided in a pre-liminary trial.
Some of the acquired instances areshown in Table 1.We next randomly sampled 100 predicate pairsfrom each of four sections (1?500, 501?1500,1501?3500 and 3500?7500) of the ranks of the ac-quired pairs for each relation class.
Two annotatorswere asked to judge the correctness of each pred-icate pair (i.e.
800 pairs in total).
They judged apredicate pair to be correct if they could producean appropriate relation instance from that pair byadding some shared argument.
For example, thepair???
(hang/put/call) and????
(connect)was judged correct because it could constitute sucha relation instance as:(3) ???
(?
:X) ?effect????
(?
:X)(X ?
{??
})make(obj:X) ?effectgo-through(subj:X)(X ?
{phone-call})Unfortunately, the two annotators did not agreewith each other very much.
out of the 400 sam-ples, they agreed only on 294 for action-effect and297 for action-means.
However, a closer look atthe results revealed that the judgements of the oneannotator were considerably but very consistentlyTable 2: Accuracy and recall of relation classifica-tionLSPs covered by anchorsall top-N instance type combinedaction-effect 400 254 175 169 254269 185 144 143 206(accuracy) (0.67) (0.72) (0.82) (0.84) (0.81)(recall) (1.00) (0.68) (0.53) (0.53) (0.76)action-means 400 254 178 176 254280 193 143 140 200(accuracy) (0.70) (0.75) (0.80) (0.79) (0.78)(recall) (1.00) (0.68) (0.51) (0.50) (0.71)more tolerant than the other.
Assuming that thejudgements of the latter correct, the precision andrecall of those of the former would be 0.71 and0.97 for action-effect, and 0.75 and 0.99 for action-means.
These figures indicate that the two annota-tors agreed quite well with respect to the ?good-ness?
of a sample, while having different criteriafor strictness.
For our evaluation, we decided tolean to the strict side and considered a sample cor-rect only if it was judged correct by both anno-tators.
The accuracy and recall achieved by thepattern-based model is shown in the column ?all?under ?LSPs?
in Table 2.We then applied the anchor-based methods de-scribed in Section 3.3 to the above 800 sampledpredicate pairs.
The results are shown in the col-umn ?covered by anchors?
of Table 2.
Since thetendency for both relation classes is more or lessthe same, let us focus only on the results for action-effect.As shown in the column ?all?
under ?LSPs?
inthe table, the pattern-based method covered 269out of the 400 predicate pairs sampled above.
Theinstance-based anchors (?instance?)
covered 175out of the 400 predicate pairs sampled above, and144 of them were correct with respect to relationtype.
We calculate its accuracy by dividing 144by 175 and recall by dividing 144 by 269.
Thesefigures indicate that the instance-based anchorschose correct predicate pairs at a very high accu-racy while sacrificing recall.
The recall, however,can be extensively improved without losing accu-racy by combining the instance-based and type-based anchors, where we considered a predicatepair covered if it was covered by either of theinstance-based and type-based anchors.
The re-sults are shown in the column ?combined?
under?covered by anchors?
in the same table.
While thetype-based anchors exhibited the same tendency asthe instance-based anchors (namely, high accuracy6Table 1: ExamplesPred1 Arg1 Pred2 Arg2 Ancaction-effect begin(????)
obj(?)
finish(????)
subj(?)
installation(?
?
?
?
?
?),transaction(????????
)action-effect design(??????)
obj(?)
be pretty(????)
subj(?)
logotype(??
)action-effect sleep(??)
in(?)
be sleep(???)
in(?)
bed(???
), futon(??
)action-means cure(????)
by(?)
prescribe(????)
obj(?)
medicine(?
)action-means cure(????)
obj(?)
prescribe(????)
for(?)
patient(??
)action-means go home(????)
by(?)
drive(????)
obj(?)
car(?
), car(???
)action-means use(????)
obj(?)
copy(?????)
obj(?)
file(????
), data(???
)and low recall), their coverage reasonably differedfrom each other, which contributed to the improve-ment of recall.To summarize so far, the pattern-based methodwe adopted in the experiment generated a sub-stantial number of predicate pairs with a accuracycomparative to the state of the art.
The accuracywas, however, further boosted by applying bothinstance-based and type-based anchors.
This ef-fect is particularly important because, to our bestknowledge, very few pattern-based relation acqui-sition models have been reported to achieve as higha accuracy as what we achieved.
In the case of ourpattern-based model, for reference, the 254 highlyranked pairs of the 400 samples included only 185correct pairs, which is worse than the 206 pairscovered by anchors for both accuracy and recall(see the ?top-N?
column under ?LSPs?
in Table 2.This difference also leads us to consider incor-porating our anchor-based filtering into the boot-strapping cycles of pattern-based predicate pair ac-quisition.4.2.2 Shared argument identificationWe next investigated the accuracy of shared ar-gument identification.
For each of the aforemen-tioned predicate pairs covered by anchors (the 254pairs for action-effect and 254 for action-means),we asked the same two annotators as above tojudge the correctness of the shared argument in-formation.
The results of combination are shownin Table 3.?arg-strict?
shows the results of the strict judg-ments where the shared argument was consideredto be correctly identified only when the most fre-quent argument pair was judged correct, while?arg-lenient?
shows the results of the lenient judg-ments where the shared argument was consideredto be correctly identified when either of the threemost frequent argument pairs was judged correct.For judging the correctness of an argument pair,we had three degrees of strictness.
In the moststrict criterion (?anc-strict?
), an argument pair wasjudged correct only when its maximally three an-chor words were all correct, while in ?anc-lenient?,an argument pair was judged correct when any ofthe three most frequent anchor words was correct.In ?anc-any?, an argument pair was judged correctas far as an annotator could think of any appropri-ate anchor word for it.
While the inter-annotatoragreement was not very high, with the kappa co-efficient in the ?arg-strict?
and ?anc-any?
setting0.47 for action-effect and 0.42 for action-effect),one was again consistently more tolerant than theother.
For the same reason as argued in 4.2.1, weconsidered an acquired relation correct only if bothannotators judged it correct.In this experiment, predicate pairs that had beenjudged wrong with respect to relation types wereall considered wrong in all the settings.
The upperbounds of accuracy, therefore, are given by thosein Table 2.
For ?arg-??
with the ?combined?
an-chors, for example, the upper bound of accuracyis 0.81.
Since ?arg-lenient?
with ?combined?
and?anc-lenient?
achieved 0.76 accuracy, our methodturned out to be reasonably precise in identifyingargument pairs and their fillers.
Paying attentionto ?arg-strict?
and ?anc-strict?, on the other hand,one can see a considerable drop from the lenientcase, which needs to be further investigated.5 Conclusion and future workMotivated by the complementarity between thepattern-based relation-oriented approach and theanchor-based argument-oriented approach to eventrelation acquisition, we have explored a two-phased approach, which first uses patterns to ac-quire predicate pairs and then uses two types ofanchors to identify shared arguments, reporting onthe present results of our empirical evaluation.
Theresults have shown that (a) the anchor-based fil-tering extensively improves the accuracy of pred-icate pair acquisition, (b) the instance-based andtype-based anchors are almost equally contributiveand combining them improves recall without los-7Table 3: Accuracy of shared argument identificationaction-effect action-meansanc-strict anc-lenient anc-any anc-strict anc-lenient anc-anyinstance 0.64 0.71 0.71 0.61 0.66 0.66arg-strict type 0.60 0.63 0.65 0.61 0.65 0.67combined 0.60 0.65 0.66 0.58 0.62 0.64instance 0.78 0.80 0.80 0.73 0.75 0.76arg-lenient type 0.68 0.71 0.72 0.67 0.69 0.71combined 0.74 0.76 0.77 0.71 0.73 0.74ing accuracy, and (c) the anchor-based method alsoachieves high accuracy in shared argument identi-fication.Our future direction will be two-fold.
One isevaluation.
Clearly, more comprehensive evalu-ation needs to be done.
For example, the ac-quired relation instances should be evaluated insome task-oriented manner.
The other intriguingissue is how our anchor-based method for sharedargument identification can benefit from recent ad-vances in coreference and zero-anaphora resolu-tion (Iida et al, 2006; Komachi et al, 2007, etc.
).ReferencesAbe, Shuya, Kentaro Inui, and Yuji Matsumoto.
2008.Acquiring event relation knowledge by learningcooccurrence patterns and fertilizing cooccurrencesamples with verbal nouns.
In Proceedings of the3rd International Joint Conference on Natural Lan-guage Processing, pages 497?504.Chklovski, Timothy and Patrick Pantel.
2005.
Globalpath-based refinement of noisy graphs applied toverb semantics.
In Proceedings of Joint Conferenceon Natural Language Processing.Harris, Zelling.
1968.
Mathematical structures oflanguage.
Interscience Tracts in Pure and AppliedMathematics.Iida, Ryu, Kentaro Inui, and Yuji Matsumoto.
2006.Exploiting syntactic patterns as clues in zero-anaphora resolution.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and the 44th annual meeting of the ACL, pages625?632.
Association for Computational Linguis-tics.Inui, Takashi, Kentaro Inui, and Yuji Matsumoto.
2003.What kinds and amounts of causal knowledge canbe acquired from text by using connective markersas clues?
In Proceedings of the 6th InternationalConference on Discovery Science,, pages 180?193.Kawahara, Daisuke and Sadao Kurohashi.
2006.
Afully-lexicalized probabilistic model for japanesesyntactic and case structure analysis.
In Proceedingsof the Human Language Technology Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 176?183.Komachi, Mamoru, Ryu Iida, Kentaro Inui, and YujiMatsumoto.
2007.
Learning based argument struc-ture analysis of event-nouns in japanese.
In Proceed-ings of the Conference of the Pacific Association forComputational Linguistics, pages 120?128.Kudo, Taku and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of the 6th Conference on Natural LanguageLearning 2002 (COLING 2002 Post-ConferenceWorkshops), pages 63?69.Lin, Dekang and Patrick Pantel.
2001.
Dirt: discoveryof inference rules from text.
In Proceedings of theseventh ACM SIGKDD international conference onKnowledge discovery and data mining, pages 323?328.Pantel, Patrick and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automat-ically harvesting semantic relations.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theACL, pages 113?120.Pekar, Viktor.
2006.
Acquisition of verb entailmentfrom text.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Main Confer-ence, pages 49?56.Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisitionof entailment relations.
In Lin, Dekang and DekaiWu, editors, Proceedings of EMNLP 2004, pages41?48, Barcelona, Spain.
Association for Computa-tional Linguistics.Torisawa, Kentaro.
2006.
Acquiring inference ruleswith temporal constraints by using japanese coordi-nated sentences and noun-verb co-occurrences.
InProceedings of Human Language Technology Con-ference/North American chapter of the Associationfor Computational Linguistics annual meeting (HLT-NAACL06), pages 57?64.Zanzotto, Fabio Massimo, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using se-lectional preferences.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 849?856.8
