Improved Iterative Scaling can yield multiple globally optimalmodels with radically diering performance levelsIain Bancarz and Miles Osbornefiainrb,osborneg@cogsci.ed.ac.ukDivision of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWScotlandAbstractLog-linear models can be e?ciently estimated us-ing algorithms such as Improved Iterative Scaling(IIS)(Laerty et al, 1997).
Under certain conditionsand for a particular class of problems, IIS is guaran-teed to approach both the maximum-likelihood andmaximum entropy solution.
This solution, in like-lihood space, is unique.
Unfortunately, in realisticsituations, multiple solutions may exist, all of whichare equivalent to each other in terms of likelihood,but radically dierent from each other in terms ofperformance.
We show that this behaviour can occurwhen a model contains overlapping features and thetraining material is sparse.
Experimental results,from the domain of parse selection for stochastic at-tribute value grammars, shows the wide variationin performance that can be found when estimatingmodels using IIS.
Further results show that the in-uence of the initial model can be diminished byselecting either uniform weights, or else by modelaveraging.1 BackgroundWhen statistically modelling linguistic phenomenaof one sort or another, researchers typically t log-linear models to the data (for example (Johnson etal., 1999)).
There are (at least) three reasons forthe popularity of such models: they do not make un-warranted independence assumptions, the maximumlikelihood solution of such models coincides with themaximum entropy solution, and nally, they can bee?ciently estimated (using algorithms such as Im-proved Iterative Scaling (IIS) (Laerty et al, 1997)).Now, the solution found by IIS is guaranteed toapproach a global maximum for both likelihood andentropy under certain conditions.
Although this isappealing, in realistic situations it turns out thatmultiple models exist, all of which are equivalent interms of likelihood but dierent from each other interms of their performance at some task.
In particu-lar, the initial weight settings can inuence the qual-ity of the nal model, even though this nal modelis the maximum entropy solution (as found by IIS).At rst glance, this seems very strange.
The IISalgorithm is guaranteed to converge to a globally op-timal solution regardless of the initial parameters.If the initial weights assigned to some of the fea-tures are wildly inappropriate then the algorithmmay take longer to converge, but one would expectthe nal destination to remain the same.
However,as we show later, what is unique in terms of likeli-hood need not be unique in terms of performance,and so IIS can be sensitive to the initial weight set-tings.Some of the reason for this behaviour may liein a relatively subtle eect which we call overlap-ping features.1If some features behave identically inthe training set (but not necessarily in future sam-ples), IIS cannot distinguish between dierent setsof weights for those features unless the sum of allweights in each set is dierent.
In fact, the nalweights assigned to such features will be dependenton their initial values.
Under these conditions, therewill be a family of models, all of which are identicalas far as IIS is concerned, but distinguishable fromeach other in terms of performance.
This means thatin terms of performance space, the landscape willcontain local maxima.
In terms of likelihood space,the landscape will continue to contain just the single(global) maximum.This indeterminacy is clearly undesirable.
How-ever, there are (at least) two practical ways of deal-ing with it: one either initialises IIS with weightsthat are identical (0 is a good choice), or else onetakes a Bayesian approach, and averages over thespace of models.
In this paper, we show that theperformance of IIS is sensitive to the initial weightsettings.
We show that setting the weights to zeroyields performance that is better than a large setof randomly initialised models.
Also, we show thatmodel averaging can also reduce indeterminacies in-troduced through the choice of initial weights.The rest of this paper is as follows.
Section2 is a restatement of the theory behind IIS.
Sec-tion 3 shows how sparse training material, coupled1We do not claim that overlapping features are the solereason for our observed eects.
As an anonymous reviewernoted, sparse statistics may also yield similar ndings.with large models can result in situations wherebyIIS produces suboptimal results.
We then moveon to experimental support for our theoretical re-sults.
Our domain is parse selection for broad-coverage stochastic attribute-value grammars.
Sec-tion 4 shows how we model the broad coverageattribute-value grammar (mentioned in section 5used in our experiments), and then present our re-sults.
The rst set of experiments (section 7) dealswith how well IIS, with uniform initial settings.
out-performs models with randomised initial settings.The second set of experiments shows how modelaverging can deal with the problem of initialising themodel.
We conclude the paper with some commentson our work.2 The Duality LemmaHow can we be certain that IIS seeks out a globalmaximum for both likelihood and entropy?
The an-swer is that for the class of problems under consider-ation, there is only a single maximum - which is thennecessarily the global one.
The IIS algorithm sim-ply `hill-climbs' and seeks to increase the likelihoodof the model being trained.
Its success is guaranteedbecause of a result which we refer to as the DualityLemma2.The proof of the Duality Lemma is contained in(Laerty et al, 1997) but will be omitted here.
Inorder to state the lemma, we rst dene the settingand establish some notation.Suppose that we have a probability measure space(;F ;P), where as usualis a set made up of ele-ments !, F is a sigma-eld on, and P is a prob-ability measure on F .
Suppose further that X is asimple random variable on- that is to say, it is areal-valued function on, having nite range, andsuch that [!
: X(!)
= x] 2 F .
As usual, we will omitthe argument !, so that X indicates a general valueof the function as well as the function itself, and xdenotes [!
: X(!)
= x].
We will also abuse notationby letting X denote the set of possible values of x;the meaning should be clear from context.We now consider a stochastic process on (;F ; P ).We are given a sample of past outputs (data points)from this process which make up the training data.We again abuse notation by using ~p to refer to boththe set of data points and the distribution dened bythat set.
Let  denote the set of all possible proba-bility distributions over X ; we seek a model q2 which is in some sense the best possible probabilitydistribution over future outputs.
We specically ex-amine generalized Gibbs distributions, which are ofthe form:qh(x) =1(Zq(h))exph(x)q(x) (1)2This result was called Proposition 4 (Laerty et al, 1997).In this case, h is a real-valued function on X , q is aninitial probability distribution overX (which may bethe uniform distribution), and Zq(h) is a normalisingconstant, taking a value such thatPx2Xqh(x) = 1.The function h here takes the form:h(x) =nXi=1ifi(x) = (  f)(x) (2)where the fi(x) are integer feature functions.
Thereal numbers iare adjustable parameters.Suppose that we are given the data ~p, an initialmodel q0, and a set of features f .
Laerty et al(1997) describe two natural sets of models.
The rstis the set P(f; ~p) of all distributions that agree with~p as to the expected value of the feature function f :P(f; ~p) = [p 2  : p [f ] = ~p [f ]] (3)where, as usual, p [f ] denotes the expectation of thefunction f under the distribution p.The second is Q(f; q0), the set of generalizedGibbs distributions based on q0and with feature setf :Q(f; q0) = [(  f) ?
q0] (4)Let Q denote the closure of Q in , with respectto the topology  inherits as a subset of Euclideanspace.These in turn determine two natural candidatesfor the `best' model q.
Let D(pkq) denote theKullback-Leibler divergence between two distribu-tions p and q.
The suitable models are: Maximum Likelihood Gibbs Distribution.
A dis-tribution in Q with maximum likelihood withrespect to ~p: qML= argminq2QD(~pkq). Maximum Entropy Constrained Distribution.
Adistribution in P with maximum entropy rela-tive to q0: qME= argminq2P(f;~p)D(pkq0)The key result of Laerty et al (1997) is thatthere is a unique qsatisfying q= qML= qME.In Appendix 1 of that paper, the following result isproved:The Duality Lemma.
Suppose that D(~pkq0) <1.
Then there exists a unique q2  satisfying:1. q2 P \ Q2.
D(pkq) = D(pkq) +D(qkq) 8p 2 P; q 2 Q3.
q= argminq2QD(~pkq)4. q= argminq2P(f;~p)D(pkq0)The Duality Lemma is a very useful result, funda-mental to the ability of IIS to converge upon the sin-gle maximum likelihood, maximum entropy solution.The IIS algorithm itself uses a fairly straightforwardtechnique to look for any maximum of the likelihoodfunction; because of the lemma, IIS is guaranteed toapproach the solution q.3 Limitations With Sparse TrainingData3.1 Overlapping FeaturesIn this paper, all models have the same training dataand a uniform initial distribution q0.
This ensuresthat D(pkq0)  1 and that, for all models, the IISalgorithm approaches the same optimal distributionq.
However, our experiments show that not all dis-tributions obtained by running IIS (to convergence)are equally good at modelling a set of test data.
Per-formance appears to depend (at least) on the start-ing values for the weights i.
This may be a resultof the following situation:Consider two features, fiand fjwith i < j,with weights iand jrespectively.
Suppose thatfi(x) = fj(x) for all values of x in ~p, but there existvalues of x outside ~p such that fi(x) 6= fj(x); thatis, the two functions take exactly the same valueson the set of training data but dier outside of it.This phenomenon can be called overlapping.3Over-lapping features are commonly found in maximumentropy models, and are one of the main reasons fortheir popularity.
In fact, one could argue that allmaximum entropy models found in natural languageapplications contain overlapping features.
Overlap-ping features may be present by explicit design (forexample when emulating backing-o smoothing), orelse naturally, for example when using features totreat words co-occurring with each other.We assign initial weights (0)iand (0)jto the fea-tures, and after n iterations of the algorithm, theyhave been adjusted to (n)iand (n)jrespectively.Now consider the target solution q, as determinedby q0and p. Let us assume for convenience that qis of the form:q=1Zexp (nXk=1kfk) (5)where Zis the usual normalising constant.
Noticethat qmay not belong to the family of exponentialmodels Q, but instead may be part of the largerset Q.
Indeed, della Pietra et al state that P \Q may be empty.
This is not a serious limitationas one can come arbitrarily close to any element ofQ while remaining inside Q.
Thus, if q=2 Q, wemay consider the above expression to be a very closeapproximation to q, such as could be obtained byrunning IIS to convergence.Because the ith and jth features are equal on thetraining data, the exact values of iand jhaveno eect on the likelihood of the model as long as3We can relax this denition of overlapping and allow fea-tures to largely co-occur together.
Depending upon the de-gree of co-occurrence, we would expect to continue to nd ourresults.their sum remains the same.
In this instance, qis not a single model at all, but rather a family ofmodels satisfying the condition i+ j= ijfora particular ij.4All models in this family assignequal likelihood to the training data, and so the IISalgorithm is unable to distinguish between them.Under these circumstances we should ensure thati= j.
Since we have no way to distinguish be-tween the two features, our model should assignthem equal importance.
However, this is not guar-anteed by the IIS algorithm.
In particular, it is lesslikely to occur if the initial weights (0)iand (0)jarenot equal.3.2 IIS and Overlapping Features - ASimple ExampleSuppose that our model has a vector of parameters = [i: i = 1 : : : n] and we wish to change it to + ?, where ?
= [?i: i = 1 : : : n] The IIS algorithmconsiders each iin turn.
It chooses ?ito maximizean auxiliary function B(?j), which provides a lowerbound on the change in log-likelihood of the model.Each adjustment ii+ ?iis guaranteed to in-crease the likelihood of the model and thus approachour ideal solution q.
This process is repeated un-til convergence.
There are no inherent restrictionson the value of ?i; it may be positive or negative,and large or small compared to the values taken fordierent features.Now, suppose that the features fiand fjover-lap and we halt the algorithm after t iterations.Clearly, the algorithm cannot guarantee that the -nal weights tiand tjwill be equal.
The followinghighly simplied example should demonstrate whythis is the case.Example 1: Imagine that our model has twooverlapping features f1and f2, and qis the fam-ily of models in which 1+ 2= 5.
Suppose thatthe initial weights are (0)1= 5; 02= 0.
Recall thatat the ith step of an iteration, the IIS algorithmconsiders the change in log-likelihood of the modelwhich can be made by only adjusting the ith pa-rameter.
In this case no change is possible as thesum 1+ 2is already at its optimum value, so thealgorithm terminates with t1= 5 and t2= 0.
Wehave assigned far greater importance to f1than tof2with no justication for doing so.
Clearly, thisassumption might not be warranted.3.3 IIS and Overlapping Features inPracticeThe situation will obviously be much more compli-cated in practice.
Most importantly, there is no such4The number ijis not a `constant' as such, since anyvector of weights is in a sense unique only up to multiplicationby a positive constant.
This will be examined in greater detailwhen we consider overlapping features in practice.thing as an absolute \optimum" vector of weights.
As a result, no one parameter can be regardedas xed until the algorithm has converged for all pa-rameters.
In the above example, our \true" set oftarget solutions is those for which 1+ 2is a con-stant.
Since there are no other features, IIS would infact terminate at once for any pair of initial weights.Suppose that we added a third feature f3whichdid not overlap the rst two.
Our set of target solu-tions would then be of the form 1+2= 12k; 3=3k for some xed 12and 3and for any k > 0.The adjustments made to 1and 2will depend onthose made to 3, and the algorithm will not neces-sarily terminate after the rst pass.The following example will describe what happensin this more realistic situation.
It will also explainthe possible benets of setting all initial weights tothe same value.Example 2: Let the model have the three fea-tures described above, with initial weights 01=5; 02= 0; 03= 1 and a family of target solutions qdened by 1+ 2= 5k; 3= k for any k > 0.
IISagain terminates at once, because the initial modelis already part of the family q.
Again there is anunjustied dierence between the nal weights forf1and f2.Now suppose that all three initial weights areset to zero.
Imagine for simplicity that we haverestricted the algorithm to adjust weights only byzero or 1.
On the rst pass, all three weights arechanged to 1.
On the second, 1and 2are increasedto 2; 3remains at 1.
In the third iteration, 1is setto 3, 2remains at 2 and 3at 1, and the algorithmterminates.5Notice that, although there is still a dierence be-tween the nal weights for f1and f2, it is much lessthan before.
This more closely approaches the idealsituation in which the weights are equal.This concludes our theoretical treatment of IIS.We now show experimentally the inuence that theinitial weight settings have, and how it can beminimised.
Our strategy is to use plausible fea-tures, as found in a realistic domain (parse selec-tion for stochastic attribute-value grammars), andrstly show what happens when the initial weightsettings are set uniformly (to zero).
We then showwhat happens when these initial settings are ran-domly set, and nally, what happens when we av-erage over randomly initialised maximum likelihoodsolutions.5The exact behaviour of the algorithm will depend on thetraining data, so this is not the only imaginable outcome, butit is certainly a plausible one.4 Log-linear Modelling ofAttribute-Value GrammarsHere we show how attribute-value grammars may bemodelled using log-linear models.
Abney gives fullerdetails (Abney, 1997).Let G be an attribute-value grammar, and D a setof sentences within the string-set dened by L(G).A log-linear model, M , consist of two components:a set of features, F and a set of weights, .The (unnormalised) total weight of a parse x,(x), is a function of the k features that are `active'on a parse:(x) = exp(kXi=1ifi(x)) (6)The probability of a parse, P (x j M), is simplythe result of normalising the total weight associatedwith that parse:P (x jM) =1Z(x) (7)Z =Xy2(y) (8)is the union of the set of parses assigned to eachsentence in D by the grammar G, such that eachparse inis unique in terms of the features that areactive on it.
Normally a parse can be viewed as theset of features that are active on it.The interpretation of this probability (equation 7)depends upon the application of the model.
Here,we use parse probabilities to reect preferences forparses.5 The GrammarThe grammar we model with log-linear models(called the Tag Sequence Grammar (Briscoe andCarroll, 1996), or TSG for short) was manually de-veloped with regard to coverage, and when compiledconsists of 455 Denite Clause Grammar (DCG)rules.
It does not parse sequences of words directly,but instead assigns derivations to sequences of part-of-speech tags (using the CLAWS2 tagset).
Thegrammar is relatively shallow (for example, it doesnot fully analyse unbounded dependencies).6 Modelling the GrammarModelling the TSG with respect to the parsed WallStreet Journal consists of two steps: creation of afeature set and denition of a reference distribution(the target model, ~p).6.1 Feature SetModelling the TSG with respect to the parsed WallStreet Journal consists of two steps: creation of aAP/a1:unimpededA1/app1:unimpededaaa!!
!unimpeded PP/p1:byP1/pn1:bybb""by N1/n:tra?ctra?cFigure 1: TSG Parse Fragmentfeature set and denition of the reference distribu-tion.Our feature set is created by parsing sentencesin the training set, and using each parse to instan-tiate templates.
Each template denes a family offeatures.
Our templates are motivated by the ob-servations that linguistically-stipulated units (DCGrules) are informative, and that many DCG appli-cations in preferred parses can be predicted usinglexical information.The rst template creates features that countthe number of times a DCG instantiationis presentwithin a parse.6For example, suppose we parsedthe Wall Street Journal AP:1 unimpeded by tra?cA parse tree generated by TSG might be as shownin gure 1.
Here, to save on space, we have labelledeach interior node in the parse tree with TSG rulenames, and not attribute-value bundles.
Further-more, we have annotated each node with the headword of the phrase in question.
Within our gram-mar, heads are (usually) explicitly marked.
Thismeans we do not have to make any guesses whenidentifying the head of a local tree.
With head in-formation, we are able to lexicalise models.
We havesuppressed tagging information.For example, a feature dened using this templatemight count the number of times the we saw:AP/a1A1/app1in a parse.
Such features record some of the contextof the rule application, in that rule applications that6Note, all our features suppress any terminals that appearin a local tree.
Lexical information is included when we decideto lexicalise features.dier in terms of how attributes are bound will bemodelled by dierent features.Our second template creates features that are par-tially lexicalised.
For each local tree (of depth one)that has a PP daughter, we create a feature thatcounts the number of times that local tree, decoratedwith the head-word of the PP, was seen in a parse.An example of such a lexicalised feature would be:A1/app1PP/p1:byThese features are designed to model PP attach-ments that can be resolved using the head of thePP.The third and nal template creates features thatare again partially lexicalised.
This time, we createlocal trees of depth one that are decorated with thehead word.
For example, here is one such feature:AP/a1:unimpededA1/app1Note the second and third templates result in fea-tures that overlap with features resulting from ap-plications of the rst template.6.2 Reference DistributionWe create the reference distribution R (an associa-tion of probabilities with TSG parses of sentences,such that the probabilities reect parse preferences)using the following process:1.
Take the training set of parses and for eachparse, compare the structural dierences be-tween it and a reference treebank parse.2.
Map these tree similarity scores into probabili-ties (where the sum of all reference probabilitiesfor all parses sums to one).Again, see Anon for more details.This concludes our discussion of how we modelgrammars.
We now go on to present our experimen-tal investigation of the inuence of the initial weightsettings.7 ExperimentsHere we present three sets of experiments.
Therst set shows the performance of maximum entropywhen the initial weight setting are zero.
The sec-ond set show the eects of randomised initial setting,and so establishes (an estimate of) the variation inperformance space.
The third set of experimentsshowed how the inuence of the initial weight set-tings could be minimised by averaging over manyhmodels.Throughout, we used the same training set.
Thisconsisted of a sample of 53795 parses (produced fromsentences at most 15 tokens long, with at most 15parses per sentence).
The sentences were drawnfrom the parsed Wall Street Journal, and all couldbe parsed using our grammar.
The motivation forthis choice of training set came from the fact thatwhen the sample of sentences is too small, the result-ing model will tend to undert. Likewise, when thetraining set is too large, the model will tend to over-t. A sample of appropriate size (which can be foundusing a simple search, as Osborne (2000) demon-strated) will therefore neither signicantly undertnor overt. Quite apart from estimation issues re-lated to sample size, because we repeatedly estimatemodels, using a sample that is just su?ciently large(and no larger) allows us to make signicant compu-tational savings.We used a disjoint development set and testingset.
The development set consisted of 2620 parses,derived from parsing sentences at most 30 tokenslong, with at most 100 parses per sentence.
The test-ing set was randomly sampled from the Wall StreetJournal, and consisted of 469 sentences, with eachsentence at most 30 tokens long, with at most 100parses per sentence.
Each sentence has on average60:0 parses per sentence.The model we used throughout contained 75171features.For all experiments, we ran IIS for the same num-ber of iterations (75).
This number was selected asthe number of iterations that produced the best per-formance for maximum entropy (our yardstick).Evaluation was in terms of exact match: for eachsentence in the test set, we awarded ourselves apoint if the estimated model ranked highest the sameparse that was ranked highest using the referenceprobabilities.7Ties were randomly broken.7.1 Maximum Entropy Results usingUniform InitialisationA model trained using weights all initialised to zeroyielded an exact match score of 52:0%.7.2 Randomised ModelsA pool of 10; 000 models was created by randomlysetting the initial weights to values in the range [-0.3,0.3] and then estimating the nal weights usingthe training set.The histogram in gure 2 shows the number ofmodels that produced the same results on the test-ing material.
As can be seen, performance is roughlynormally distributed, with some local minima hav-ing a much wider basin of attraction than other min-ima.
Also, note that all of these models underper-7We use exact match as an arbitrary evaluation metric.The particular choice of metric is not crucial to the results ofthis paper.forms a model crated using uniform initialisation.8The performance of the worst model was 24:1%.This is our lower bound, and is less than half thatof basic maxent.
The best randomly selected modelhad a performance of 46:5%.05010015020025030035040045020 25 30 35 40 45 50Number of ModelsExact Match"rand-hist04"Figure 2: Distribution of models with randomly ini-tialised starting conditions7.3 Averaging over modelsTo see whether an ensemble of such randomisedmodels could cancel-out the inuence of the ini-tial weight settings, we created a pool of 600 ran-domised models, and then combined then togetherusing equation 9:P (x jM1: : :Mn) =Qni=1P (x jMi)PyQnj=1P (y jMj)(9)Because it is possible that some subset of the mod-els outperforms an ensemble using all models, weuniformly sampled, with replacement, from this poolmodels for inclusion into the nal ensemble.
Ran-dom selection introduces variation, so we repeatedour results ten time and averaged the results.Figure 3 shows our results (N is the number ofmodels in each ensemble, x is the mean performanceand 2is the standard deviation).
The nal entry(marked all) shows the performance obtained usingan ensemble consisting of all models in the model,equally weighted.As can be seen, increasing the number of modelsin the ensemble reduces the inuence of the initialweight settings.
However, even with a large poolof models, we still marginally underperform uniforminitialisation.8Running IIS until convergence did not narrow the gap,so our ndings cannot be attributed to dierential rates ofconvergence.N x 2N x 21 38.78 0.12 50 49.94 1.242 41.41 1.04 100 50.55 0.703 44.24 1.63 150 50.62 0.885 46.57 1.69 200 50.66 0.7510 47.21 1.71 300 50.81 0.7320 49.23 1.19 600 51.04 0.36all 51.39 -Figure 3: Model averaging resultsNote that we have not carried out the control ex-periment of repeating our runs using features whichdo not overlap with each other.
Unfortunately, doingthis would probably mean having to create modelsthat are unnatural.
We therefore cannot be abso-lutely certain that the sole reason for the existanceof local optima is the presence of overlapping fea-tures.
However, we can be sure that they do exist,and that varying the initial weight settings will re-veal them.8 CommentsWe have established that, in the presence of overlap-ping features, the values of the initial weights canaect the utility of the nal model.
Our experi-mental results support this nding.
In general, onemight encounter a similar problem (overlapping fea-tures) with what might be called `semi-overlappingfeatures': features which are very similar (but notidentical) on the training data and very dierentoutside of it.IIS could be made more robust to the choice ofinitial parameters in a number of ways: The simplest course of action is to set al ini-tial weights to the same value.
Although zerois often a convenient initial value, in principleany real number would do, since the IIS algo-rithm can reach a given optimal solution fromany starting point in the space of initial param-eters. We could also examine all the features to de-termine which ones overlap, and force it to bal-ance the nal weights of these features.
For verylarge models, this may be prohibitively di?cultand time-consuming. Model averaging can also cancel out variationscaused by a particular choice of initial settings.However, this implies a greater computationalburden as IIS will need to be run many times inorder to gain a representative sample of models. The number of features in the model could bereduced using feature selection methods (for ex-ample (Mullen and Osborne, 2000)).Although IIS is a useful tool for estimating log-linear models, we have since moved-on to estimatingmodels using limited-memory variable-metric meth-ods (Malouf, 2002).
Our ndings show that conver-gence, for a range of problems, is faster.
An inter-esting question is seeing the extent to which othernumerical methods for estimating log-linear modelsare sensitive to initial parameter values.
Finally, itshould be noted that our theoretical results apply toa more general setting than that of log-linear mod-els trained using the IIS algorithm.
The problem ofoverlapping features could in principle occur in anysituation in which a model has a linear combinationof features, and a `hill-climbing' algorithm is used toseek a maximum-likelihood solution.AcknowledgementsWe wish to thank Steve Clark for useful discussionsabout IIS, Rob Malouf for supplying the IIS imple-mentation and the two anonymous reviewers.
IainBancarz was supported by the EPSRC grant POEM.ReferencesSteven P. Abney.
1997.
Stochastic Attribute-Value Grammars.
Computational Linguistics,23(4):597{618, December.Ted Briscoe and John Carroll.
1996.
AutomaticExtraction of Subcategorization from Corpora.In Proceedings of the 5thConference on AppliedNLP, pages 356{363, Washington, DC.Mark Johnson, Stuart Geman, Stephen Cannon,Zhiyi Chi, and Stephan Riezler.
1999.
Estimatorsfor Stochastic \Unication-Based" Grammars.
In37thAnnual Meeting of the ACL.J.
Laerty, S. Della Pietra, and V. Della Pietra.1997.
Inducing features of random elds.
IEEETransactions on Pattern Analysis and MachineIntelligence, 19(4):380{393, April.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
InProceedings of the joint CoNLL-WVLC Meeting,Taipei, Taiwan.
ACL.
To appear.Tony Mullen and Miles Osborne.
2000.
OverttingAvoidance for Stochastic Modeling of Attribute-Value Grammars.
In Claire Cardie, Walter Daele-mans, Claire Nedellec, and Erik Tjong Kim Sang,editors, Proceedings of the Computational NaturalLanguage learning 2000, pages 49{54.
ACL, Lis-bon, Portugal.Kamal Nigam, John Laerty, , and Andrew Mc-Callum.
1999.
Using maximum entropy for textclassication.
In IJCAI-99 Workshop on MachineLearning for Information Filtering,.Miles Osborne.
2000.
Estimation of StochasticAttribute-Value Grammars using an InformativeSample.
In The 18thInternational Conference onComputational Linguistics, Saarbrucken, August.
