Linear-Time Dependency Analysis for JapaneseManabu SassanoFujitsu Laboratories, Ltd.4-1-1, Kamikodanaka, Nakahara-ku,Kawasaki 211-8588, Japansassano@jp.fujitsu.comAbstractWe present a novel algorithm for Japanese dependencyanalysis.
The algorithm allows us to analyze dependencystructures of a sentence in linear-time while keeping astate-of-the-art accuracy.
In this paper, we show a formaldescription of the algorithm and discuss it theoreticallywith respect to time complexity.
In addition, we eval-uate its efficiency and performance empirically againstthe Kyoto University Corpus.
The proposed algorithmwith improved models for dependency yields the best ac-curacy in the previously published results on the KyotoUniversity Corpus.1 IntroductionEfficiency in parsing as well as accuracy is one ofvery important issues in natural languages process-ing.
Although we often focus much on parsing ac-curacy, studies of its efficiency are also important,especially for practical NLP applications.
Improv-ing efficiency without loss of accuracy is a really bigchallenge.The main purpose of this study is to propose anefficient algorithm to analyze dependency structuresof head final languages such as Japanese and toprove its efficiency both theoretically and empiri-cally.
In this paper, we present a novel efficient al-gorithm for Japanese dependency analysis.
The al-gorithm allows us to analyze dependency structuresof a sentence in linear-time while keeping a state-of-the-art accuracy.
We show a formal descriptionof the algorithm and discuss it theoretically withrespect to time complexity.
In addition to this,we evaluate its efficiency and performance empir-ically against the Kyoto University Corpus (Kuro-hashi and Nagao, 1998), which is a parsed corpusof news paper articles in Japanese.The remainder of the paper is organized as fol-lows.
Section 2 describes the syntactic characteris-tics of Japanese and the typical sentence processingof Japanese.
In Section 3 previous work of depen-dency analysis of Japanese as well as of English isbriefly reviewed.
After these introductory sections,our proposed algorithm is described in Section 4.Next, improved models for estimating dependencyof two syntactic chunks called bunsetsus are pro-posed in Section 5.
Section 6 describes experimen-tal results and discussion.
Finally, in Section 7 weconclude this paper by summarizing our contribu-tions and pointing out some future directions.2 Parsing Japanese2.1 Syntactic Properties of JapaneseThe Japanese language is basically an SOV lan-guage.
Word order is relatively free.
In English thesyntactic function of each word is represented withword order, while in Japanese postpositions repre-sent the syntactic function of each word.
For ex-ample, one or more postpositions following a nounplay a similar role to declension of nouns in Ger-man, which indicates a grammatical case.Based on such properties, a bunsetsu1 was de-vised and has been used to analyze syntactically asentence in Japanese.
A bunsetsu consists of oneor more content words followed by zero or morefunction words.
By defining a bunsetsu like that,we can analyze a sentence in a similar way that isused when analyzing a grammatical role of wordsin inflecting languages like German.Thus, strictly speaking, bunsetsu order ratherthan word order is free except the bunsetsu that con-tains a main verb of a sentence.
Such bunsetsu mustbe placed at the end of the sentence.
For example,the following two sentences have an identical mean-ing: (1) Ken-ga kanojo-ni hon-wo age-ta.
(2) Ken-ga hon-wo kanojo-ni age-ta.
(-ga: subject marker,-ni: dative case particle, -wo: accusative case par-ticle.
English translation: Ken gave a book to her.
)Note that the rightmost bunsetsu ?age-ta,?
which iscomposed of a verb stem and a past tense marker,has to be placed at the end of the sentence.1?Bunsetsu?
is composed of two Chinese characters, i.e.,?bun?
and ?setsu.?
?Bun?
means a sentence and ?setsu?
meansa segment.
A ?bunsetsu?
is considered to be a small syntacticsegment in a sentence.
A eojeol in Korean (Yoon et al, 1999)is almost the same concept as a bunsetsu.
Chunks defined in(Abney, 1991) for English are also very similar to bunsetsus.We here list the constraints of Japanese depen-dency including ones mentioned above.C1.
Each bunsetsu has only one head except therightmost one.C2.
Each head bunsetsu is always placed at theright hand side of its modifier.C3.
Dependencies do not cross one another.These properties are basically shared also with Ko-rean and Mongolian.2.2 Typical Steps of Parsing JapaneseSince Japanese has the properties above, the follow-ing steps are very common in parsing Japanese:1.
Break a sentence into morphemes (i.e.
mor-phological analysis).2.
Chunk them into bunsetsus.3.
Analyze dependencies between these bunset-sus.4.
Label each dependency with a semantic rolesuch as agent, object, location, etc.We focus on dependency analysis in Step 3.3 Previous WorkWe review here previous work, mainly focusing ontime complexity.
In English as well as in Japanese,dependency analysis has been studied (e.g., (Laf-ferty et al, 1992; Collins, 1996; Eisner, 1996)).
Theparsing algorithms in their papers require time where  is the number of words.2In dependency analysis of Japanese it is verycommon to use probabilities of dependencies be-tween each two bunsetsus in a sentence.
Harunoet al (1998) used decision trees to estimate thedependency probabilities.
Fujio and Matsumoto(1998) applied a modified version of Collins?
model(Collins, 1996) to Japanese dependency analysis.Both Haruno et al, and Fujio and Matsumoto usedthe CYK algorithm, which requires  time,where  is a sentence length, i.e., the number ofbunsetsus.
Sekine et al (2000) used MaximumEntropy (ME) Modeling for dependency probabili-ties and proposed a backward beam search to findthe best parse.
This beam search algorithm re-quires  time.
Kudo and Matsumoto (2000)also used the same backward beam search togetherwith SVMs rather than ME.There are few statistical methods that do not usedependency probabilities of each two bunsetsus.2Nivre (2003) proposes a deterministic algorithm for pro-jective dependency parsing, the running time of which is linear.The algorithm has been evaluated on Swedish text.Ken-ga kanojo-ni ano hon-wo age-ta.Ken-subj to her that book-acc gave.ID 0 1 2 3 4Head 4 4 3 4 -Figure 3: Sample SentenceSekine (2000) observed that 98.7% of the head lo-cations are covered by five candidates in a sentence.Maruyama and Ogino (Maruyama and Ogino, 1992)also observed similar phenomena.
Based on this ob-servation, Sekine (2000) proposed an efficient anal-ysis algorithm using deterministic finite state trans-ducers.
This algorithm, in which the limited num-ber of bunsetsus are considered in order to avoidexhaustive search, takes  time.
However, hisparser achieved an accuracy of 77.97% on the Ky-oto University Corpus, which is considerably lowerthan the state-of-the-art accuracy around 89%.Another interesting method that does not use de-pendency probabilities between each two bunsetsusis the cascaded chunking model by Kudo and Mat-sumoto (2002) based on the idea in (Abney, 1991;Ratnaparkhi, 1997).
They used the model withSVMs and achieved an accuracy of 89.29%, whichis the best result on the Kyoto University Corpus.Although the number of dependencies that are es-timated in parsing are significantly fewer than thateither in CYK or the backward beam search, the up-per bound of time complexity is still .Thus, it is still an open question as to how we an-alyze dependencies for Japanese in linear time witha state-of-the-art accuracy.
The algorithm describedbelow will be an answer to this question.4 Algorithm4.1 Algorithm to Parse a SentenceThe pseudo code for our algorithm of dependencyanalysis is shown in Figure 1.
This algorithm is usedwith any estimator that decides whether a bunsetsumodifies another bunsetsu.
A trainable classifier,such as an SVM, a decision tree, etc., is a typicalchoice for the estimator.
We assume that we havesome classifier to estimate the dependency betweentwo bunsetsus in a sentence and the time complex-ity of the classifier is not affected by the sentencelength.Apart from the estimator, variables used for pars-ing are only two data structures.
One is for inputand the other is for output.
The former is a stack forkeeping IDs of modifier bunsetsus to be checked.The latter is an array of integers that stores head IDsthat have already been analyzed.Following the presented algorithm, let us parse a// Input: N: the number of bunsetsus in a sentence.// w[]: an array that keeps a sequence of bunsetsus in the sentence.// Output: outdep[]: an integer array that stores an analysis result, i.e., dependencies between// the bunsetsus.
For example, the head of w[j] is outdep[j].//// stack: a stack that holds IDs of modifier bunsetsus in the sentence.
If it is empty, the pop// method returns EMPTY ().// function estimate dependency(j, i, w[]):// a function that returns non-zero when the j-th bunsetsu should// modify the i-th bunsetsu.
Otherwise returns zero.function analyze(w[], N, outdep[])stack.push(0); // Push 0 on the stack.for (int i = 1; i  N; i++)  // Variable i for a head and j for a modifier.int j = stack.pop(); // Pop a value off the stack.while (j != EMPTY && (i == N  1  estimate dependency(j, i, w))) outdep[j] = i; // The j-th bunsetsu modifies the i-th bunsetsu.j = stack.pop(); // Pop a value off the stack to update j.if (j != EMPTY)stack.push(j);stack.push(i);Figure 1: Pseudo Code for Analyzing Dependencies.
Note that ?i == N - 1?
means the i-th bunsetsu is therightmost one in the sentence.// indep[]: an integer array that holds correct dependencies given in a training corpus.//// function estimate dependency(j, i, w[], indep[]):// a function that returns non-zero if indep[j] == i, otherwise returns zero.// It also prints a feature vector (i.e., an encoded example) with a label which is decided to be// 1 (modify) or -1 (not modify) depending on whether the j-th bunsetsu modifies the i-th.function generate examples(w[], N, indep[])stack.push(0);for (int i = 1; i  N; i++) int j = stack.pop();while (j != EMPTY && (i == N  1  estimate dependency(j, i, w, indep))) j = stack.pop();if (j != EMPTY)stack.push(j);stack.push(i);Figure 2: Pseudo Code for Generating Training Examples.
Variables w[], N, and stack are the same as inFigure 1.sample sentence in Figure 3.
For explanation, wehere assume that we have a perfect classifier as esti-mate dependency() in Figure 1, which can return acorrect decision for the sample sentence.First, we push 0 (Ken-ga) on the stack for the bun-setsu ID at the top of the sentence.
After this initial-ization, let us see how analysis proceeds at each iter-ation of the for loop.
At the first iteration we checkthe dependency between the zero-th bunsetsu andthe 1st (kanojo-ni).
We push 0 and 1 because thezero-th bunsetsu does not modify the 1st.
Note thatthe bottom of the stack is 0 rather than 1.
SmallerIDs are always stored at lower levels of the stack.Due to this, we do not break the non-crossing con-straint (C3.
in Section 2.1).At the second iteration we pop 1 off the stack andcheck the dependency between the 1st bunsetsu andthe 2nd (ano).
Since the 1st does not modify the2nd, we again push 1 and 2.At the third iteration we pop 2 off the stack andcheck the dependency for the 2nd and the 3rd (hon-wo).
Since the 2nd modifies the 3rd, the dependencyis stored in outdep[].
The value of outdep[j] repre-sents the head of the -th bunsetsu.
For example,outdep[2] = 3 means the head of the 2nd bunsetsuis the 3rd.
Then we pop 1 off the stack and check thedependency between the 1st and the 3rd.
We pushagain 1 since the 1st does not modify the 3rd.
Afterthat, we push 3 on the stack.
The stack now has 3, 1and 0 in top-to-bottom order.At the fourth iteration we pop 3 off the stack.
Wedo not have to check the dependency between the3rd and the 4th (age-ta) because the 4th bunsetsu isthe last bunsetsu in the sentence.
Now we set out-dep[3] = 4.
Next, we pop 1 off the stack.
Also inthis case, we do not have to check the dependencybetween the 1st and the 4th.
Similarly the zero-thbunsetsu modifies the 4th.
As a result we set out-dep[1] = 4 and outdep[0] = 4.
Now the stack isempty and we finish the analysis function.
Finally,we have obtained a dependency structure throughthe array outdep[].4.2 Time ComplexityAt first glance, the upper bound of the time com-plexity of this algorithm seems to be  becauseit involves a double loop; however, it is not.
We willshow that the upper bound is  by consideringhow many times the condition part of the while loopin Figure 1 is executed.
The condition part of thewhile loop fails    times because the outer forloop will be executed from 1 to  .
On the otherhand, the same condition part successes  timesbecause outdep[j] = i is executed    times.
Foreach bunsetsu ID , outdep[j] = i is surely executedonce because by executing j = stack.pop() the valueof  is lost and it is never pushed on the stack again.That is the body of the while loop will be executedat most    times which is equal to the numberof the bunsetsus except the last one.
Therefore thetotal number of execution of the condition part ofthe while loop is  , which is obtained by sum-ming up  and .
This means that the upperbound of time complexity is .4.3 Algorithm to Generate Training ExamplesWhen we prepare training examples for the train-able classifier used with this algorithm, we use thealgorithm shown in Figure 2.
It is almost the sameas the algorithm for analyzing in Figure 1.
The dif-ferences are that we give correct dependencies to es-timate dependency() through indep[] and we obvi-ously do not have to store the head IDs to outdep[].4.4 Summary and Theoretical Comparisonwith Related WorkThe algorithm presented here has the following fea-tures:F1.
It is independent on specific machine learningmethodologies.
Any trainable classifiers canbe used.F2.
It scans a sentence just once in a left-to-rightmanner.F3.
The upper bound of time complexity is .The number of the classifier call, which is mosttime consuming, is at most   .F4.
The flow and the used data structures are verysimple.
Therefore, it is easy to implement.One of the most related models is the cascadedchunking model by (Kudo and Matsumoto, 2002).Their model and our algorithm share many fea-tures including F1.3 The big difference betweentheirs and ours is how many times the input sen-tence has to be scanned (F2).
With their model wehave to scan it several times, which leads to somecomputational inefficiency, i.e., at the worst case computation is required.
Our strict left-to-right parsing is more suitable also for practical ap-plications such as real time speech recognition.
Inaddition, the flow and the data strucutres are muchsimpler (F4) than those of the cascaded chunkingmodel where an array for chunk tags is used and itmust be updated while scanning the sentence sev-eral times.Our parsing method can be considered to be oneof the simplest forms of shift-reduce parsing.
Thedifference from typical use of shift-reduce parsingis that we do not need several types of actions andonly the top of the stack is inspected.
The reason forthese simplicities is that Japanese has the C2 con-straint (Sec.
2.1) and the target task is dependencyanalysis rather than CFG parsing.5 Models for Estimating DependencyIn order to evaluate the proposed algorithm empir-ically, we use SVMs (Vapnik, 1995) for estimatingdependencies between two bunsetsus because theyhave excellent properties.
One of them is that com-binations of features in an example are automati-cally considered with polynomial kernels.
Excellentperformances have been reported for many classifi-cation tasks.
Please see (Vapnik, 1995) for formaldescriptions of SVMs.3Kudo and Matsumoto (2002) give more comprehensivecomparison with the probabilistic models as used in (Uchimotoet al, 1999).At estimate dependency() in Figure 1, we encodean example with features described below.
Then wegive it to the SVM and receive the estimated deci-sion as to whether a bunsetsu modifies the other.5.1 Standard FeaturesBy the ?standard features?
here we mean the fea-ture set commonly used in (Uchimoto et al, 1999;Sekine et al, 2000; Kudo and Matsumoto, 2000;Kudo and Matsumoto, 2002).
We employ the fea-tures below for each bunsetsu:1.
Rightmost Content Word - major POS, minorPOS, conjugation type, conjugation form, sur-face form (lexicalized form)2.
Rightmost Function Word - major POS, minorPOS, conjugation type, conjugation form, sur-face form (lexicalized form)3.
Punctuation (periods, and commas)4.
Open parentheses and close parentheses5.
Location - at the beginning of the sentence orat the end of the sentence.In addition, features as to the gap between two bun-setsus are also used.
They include: distance, parti-cles, parentheses, and punctuation.5.2 Local Contexts of the Current BunsetsusLocal contexts of a modifier and its possible headwould be useful because they may represent fixedexpressions, case frames, or other collocational re-lations.
Assume that the -th bunsetsu is a modifierand the -th one is a possible head.
We considerthree bunsetsus in the local contexts of the -th andthe -th: the (  )-th bunsetsu if it modifies the-th, the ( )-th one, and the ( )-th one.
Notethat in our algorithm the (  )-th always modifiesthe -th when checking the dependency between the-th bunsetsu and the -th where    .
In orderto keep the data structure simple in the proposed al-gorithm, we did not consider more distant bunsetsusfrom both the -th and the -th.
It is easy to checkwhether the (  )-th bunsetsus modifies the -thone through outdep[].
Note that this use of localcontexts is similar to the dynamic features in (Kudoand Matsumoto, 2002)4.5.3 Richer Features Inside a BunsetsuWith the standard features we will miss some caseparticles if the bunsetsu has two or more functionwords.
Suppose that a bunsetsu has a topic marker4Their model extracts three types of dynamic features frommodifiers of the -th bunsetsu (Type B), modifiers of the -thbunsetsu (Type A), and heads of the -th bunsetsu (Type C).Since in our proposed algorithm analysis proceeds in a left-to-right manner, we have to use stacking (Wolpert, 1992) or othertechniques to employ the type C features.as well as a case particle.
In this case the case par-ticle is followed by the topic marker.
Thus we missthe case particle since in the standard features onlythe rightmost function word is employed.
In orderto capture this information, we use as features alsoall the particles in each bunsetsu.Another important features missed in the stan-dard are ones of the leftmost word of a possiblehead bunsetsu, which often has a strong association,e.g., an idiomatic fixed expression, with the right-most word of its modifier.
Furthermore, we use as afeature the surface form of the leftmost word of thebunsetsu that follows a possible head.
This featureis used with ones in Section 5.2.5.4 Features for Conjunctive StructuresDetecting conjunctive structures is one of hard tasksin parsing long sentences correctly.
Kurohashi andNagao (1994) proposed a method to detect conjunc-tive structures by calculating similarity scores be-tween two sequences of bunsetsus.So far few attempts have been made to explorefeatures for detecting conjunctive structures.
As afirst step we tried two preliminary features for con-junctive structures.
If the current modifier bunsetsuis a distinctive key bunsetsu (Kurohashi and Nagao,1994, page 510), these features are triggered.
Oneis a feature which is activated when a modifier bun-setsu is a distinctive key bunsetsu.
The other is afeature which is activated when a modifier is a dis-tinctive key bunsetsu and the content words of boththe modifier and its possible head are equal to eachother.
For simplicity, we limit the POS of these con-tent words to nouns.6 Experimental Results and DiscussionWe implemented a parser and SVM tools in C++and used them for experiments.6.1 CorpusWe used the Kyoto University Corpus Version 2(Kurohashi and Nagao, 1998) to evaluate the pro-posed algorithm.
Our parser was trained on the ar-ticles on January 1st through 8th (7,958 sentences)and tested on the article on January 9th (1,246 sen-tences).
The article on January 10th were used fordevelopment.
The usage of these articles is the sameas in (Uchimoto et al, 1999; Sekine et al, 2000;Kudo and Matsumoto, 2002).6.2 SVM settingPolynomial kernels with the degree of 3 are usedand the misclassification cost is set to 1 unless statedotherwise.6.3 ResultsAccuracy.
Performances of our parser on the testset is shown in Table 1.
For comparison to pre-Dependency SentenceAcc.
(%) Acc.
(%)Standard 88.72 45.88Full 89.56 48.35w/o Context 88.91 46.33w/o Rich 89.19 47.05w/o Conj 89.41 47.86Table 1: Performance on Test Set.
Context, Rich,and Conj mean the features in Sec.
5.2, 5.3, and5.4, respectively.024681012140 5 10 15 20 25 30 35 40 45SecondsSentence Length (Number of Bunsetsus)Figure 4: Observed Running Timevious work we use the standard measures for theKyoto University Corpus: dependency accuracy andsentence accuracy.
The dependency accuracy is thepercentage of correct dependencies and the sentenceaccuracy is the percentage of sentences, all the de-pendencies in which are correctly analyzed.The accuracy with the standard feature set is rel-atively good.
Actually, this accuracy is almost thesame as that of the cascaded chunking model with-out dynamic features (Kudo and Matsumoto, 2002).Our parser with the full feature set yields an accu-racy of 89.56%, which is the best in the previouslypublished results.Asymptotic Time Complexity.
Figure 4 showsthe running time of our parser on the test set usinga workstation (Ultra SPARC II 450 MHz with 1GBmemory).
It clearly shows that the running time isproportional to the sentence length and this obser-vation is consistent with our theoretical analysis inSection 4.2.One might think that although the upper boundof time complexity is lower than those of previouswork, actual processing of our parser is not so fast.Slowness of our parser is mainly due to a huge com-putation of kernel evaluations in SVMs.
The SVM00.0020.0040.0060.0080.010.0120.0140.0160.0180.020 5 10 15 20 25 30 35 40 45SecondsSentence Length (Number of Bunsetsus)Figure 5: Observed Running Time with Linear Ker-nel.
The misclassification cost is set to 0.0056.classifiers in our experiments have about forty thou-sand support vectors.
Therefore, for every deci-sion of dependency also a huge computation of dotproducts is required.
Fortunately, solutions to thisproblem have already been given by Kudo and Mat-sumoto (2003).
They proposed methods to converta polynomial kernel with higher degrees to a simplelinear kernel and reported a new classifier with theconverted kernel was about 30 to 300 times fasterthan the original one while keeping the accuracy.
Byapplying their methods to our parser, its processingtime would be enough practical.In order to roughly estimate the improved speedof our parser, we built a parser with a linear kerneland ran it on the same test set.
Figure 5 shows theobserved time of the parser with a linear kernel us-ing the same machine.
The parser runs fast enough.It can parse a very long sentence within 0.02 sec-onds.
Furthermore, accuracy as well as speed ofthis parser was much better than we expected.
Itachieves a dependency accuracy of 87.36% and asentence accuracy of 40.60%.
These accuracies areslightly better than those in (Uchimoto et al, 1999),where combinations of features are manually se-lected.6.4 Comparison to Related WorkWe compare our parser to those in related work.
Asummary of the comparison is shown in Table 2.It clearly shows that our proposed algorithm withSVMs has a good property with regard to timecomplexity and in addition our parser successfullyachieves a state-of-the-art accuracy.Theoretical comparison with (Kudo and Mat-sumoto, 2002) is described in Section 4.4.
Uchi-moto et al (1999) used the backward beam searchwith ME.
According to (Sekine et al, 2000), the an-alyzing time followed a quadratic curve.
In contrast,Algorithm/Model Time Acc.
(%)ComplexityThis paper Stack Dependency Analysis (SVMs)  89.56Stack Dependency Analysis (linear SVMs)  87.36KM02 Cascaded Chunking (SVMs)  89.29KM00 Backward Beam Search (SVMs)  89.09USI99 Backward Beam Search (ME)  87.14Seki00 Deterministic Finite State Transducer  77.97Table 2: Comparison to Related Work.
KM02 = Kudo and Matsumoto 2002, KM00 = Kudo and Matsumoto2000, USI99 = Uchimoto et al 1999, and Seki00 = Sekine 2000.our parser analyzes a sentence in linear time keep-ing a better accuracy.
Sekine (2000) also proposeda very fast parser that runs in linear time; however,accuracy is greatly sacrificed.7 Conclusion and Future DirectionsWe have presented a novel algorithm for Japanesedependency analysis.
The algorithm allows usto analyze dependency structures of a sentence inlinear-time while keeping a state-of-the-art accu-racy.
We have shown a formal description of thealgorithm and discussed it theoretically in terms oftime complexity.
In addition, we have evaluated itsefficiency and performance empirically against theKyoto University Corpus.
Our parser gives the bestaccuracy, 89.56%, in the previously published re-sults.In the future, it would be interesting to applythis algorithm to speech recognition in which it ismore desirable to analyze a sentence in a left-to-right manner.
Another interesting direction wouldbe to explore features for conjunctive structures.
Al-though we found some useful features, they werenot enough to improve the performance much.
Weexpect stacking would be useful.ReferencesS.
P. Abney.
1991.
Parsing by chunks.
In R. C. Berwick,S.
P. Abney, and C. Tenny, editors, Principle-BasedParsing: Computation and Psycholinguistics, pages257?278.
Kluwer Academic Publishers.M.
Collins.
1996.
A new statistical parser based on bi-gram lexical dependencies.
In Proc.
of ACL-96, pages184?191.J.
M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proc.
ofCOLING-96, pages 340?345.M.
Fujio and Y. Matsumoto.
1998.
Japanese depen-dency structure analysis based on lexicalized statis-tics.
In Proc.
of EMNLP-1998, pages 88?96.M.
Haruno, S. Shirai, and Y. Ooyama.
1998.
Using de-cision trees to construct a practical parser.
In Proc.
ofCOLING/ACL-98, pages 505?511.T.
Kudo and Y. Matsumoto.
2000.
Japanese dependencystructure analysis based on support vector machines.In Proc.
of EMNLP/VLC 2000, pages 18?25.T.
Kudo and Y. Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In Proc.
ofCoNLL-2002, pages 63?69.T.
Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proc.
of ACL-03, pages24?31.S.
Kurohashi and M. Nagao.
1994.
A syntactic analysismethod of long Japanese sentences based on the de-tection of conjunctive structures.
Computational Lin-guistics, 20(4):507?534.S.
Kurohashi and M. Nagao.
1998.
Building a Japaneseparsed corpus while improving the parsing system.
InProc.
of the 1st LREC, pages 719?724.J.
Lafferty, D. Sleator, and D. Temperley.
1992.
Gram-matical trigrams: A probabilistic model of link gram-mar.
In Proc.
of the AAAI Fall Symp.
on ProbabilisticApproaches to Natural Language, pages 89?97.H.
Maruyama and S. Ogino.
1992.
A statistical propertyof Japanese phrase-to-phrase modifications.
Mathe-matical Linguistics, 18(7):348?352.J.
Nivre.
2003.
An efficient algorithm for projective de-pendency parsing.
In Proc.
of IWPT-03, pages 149?160.A.
Ratnaparkhi.
1997.
A linear observed time statisticalparser based on maximum entropy models.
In Proc.of EMNLP-1997, pages 1?10.S.
Sekine, K. Uchimoto, and H. Isahara.
2000.
Back-ward beam search algorithm for dependency analysisof Japanese.
In Proc.
of COLING-00, pages 754?760.S.
Sekine.
2000.
Japanese dependency analysis usinga deterministic finite state transducer.
In Proc.
ofCOLING-00, pages 761?767.K.
Uchimoto, S. Sekine, and H. Isahara.
1999.
Japanesedependency structure analysis based on maximum en-tropy models.
In Proc.
of EACL-99, pages 196?203.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer-Verlag.D.
H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5:241?259.J.
Yoon, K. Choi, and M. Song.
1999.
Three types ofchunking in Korean and dependency analysis based onlexical association.
In Proc.
of the 18th Int.
Conf.
onComputer Processing of Oriental Languages, pages59?65.
