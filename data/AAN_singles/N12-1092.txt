2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 742?751,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMind the Gap: Learning to Choose Gaps for Question GenerationLee BeckerDepartment of Computer ScienceUniversity of Colorado BoulderBoulder, CO 80309lee.becker@colorado.eduSumit Basu and Lucy VanderwendeMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{sumitb,lucyv}@microsoft.comAbstractNot all learning takes place in an educationalsetting: more and more self-motivated learnersare turning to on-line text to learn about newtopics.
Our goal is to provide such learnerswith the well-known benefits of testing by au-tomatically generating quiz questions for on-line text.
Prior work on question generationhas focused on the grammaticality of generat-ed questions and generating effective multi-ple-choice distractors for individual questiontargets, both key parts of this problem.
Ourwork focuses on the complementary aspect ofdetermining what part of a sentence we shouldbe asking about in the first place; we call this?gap selection.?
We address this problem byasking human judges about the quality ofquestions generated from a Wikipedia-basedcorpus, and then training a model to effective-ly replicate these judgments.
Our data showsthat good gaps are of variable length and spanall semantic roles, i.e., nouns as well as verbs,and that a majority of good questions do notfocus on named entities.
Our resulting systemcan generate fill-in-the-blank (cloze) ques-tions from generic source materials.1 IntroductionAssessment is a fundamental part of teaching, bothto measure a student?s mastery of the material andto identify areas where she may need reinforce-ment or additional instruction.
Assessment has alsobeen shown an important part of learning, as test-ing assists retention and can be used to guide learn-ing (Anderson & Biddle, 1975).
Thus, as learnersmove on from an educational setting to unstruc-tured self-learning settings, they would still benefitfrom having the means for assessment available.Even in traditional educational settings, there is aneed for automated test generation, as teacherswant multiple tests for topics to give to differentstudents, and students want different tests withwhich to study and practice the material.One possible solution to providing quizzes fornew source material is the automatic generation ofquestions.
This is a task the NLP community hasalready embraced, and significant progress hasbeen made in recent years with the introduction ofa shared task (Rus et al, 2010).
However, thus farthe research community has focused on the prob-lem of generating grammatical questions (as inHeilman and Smith (2010a)) or generating effec-tive distractors for multiple-choice questions(Agarwal and Mannem, 2011).While both of these research threads are of crit-ical importance, there is another key issue thatmust be addressed ?
which questions should we beasking in the first place?
We have highlighted thisaspect of the problem in the past (seeVanderwende (2008)) and begin to address it inthis work, postulating that we can both collect hu-man judgments on what makes a good questionand train a machine learning model that can repli-cate these judgments.
The resulting learned modelcan then be applied to new material for automatedquestion generation.
We see this effort as comple-mentary to the earlier progress.In our approach, we factor the problem of gen-erating good questions into two parts: first, the se-lection of sentences to ask about, and second, theidentification of which part of the resulting sen-tences the question should address.
Because wewant to focus on these aspects of the problem andnot the surface form of the questions, we have cho-sen to generate simple gap-fill (cloze) questions,742though our results can also be used to trigger Wh-questions or multiple-choice questions by leverag-ing prior work.
For sentence selection, we turn tomethods in summarization and use the simple buteffective SumBasic (Nenkova et al, 2006) algo-rithm to prioritize and choose important sentencesfrom the article.
We cast the second part, gap se-lection, as a learning problem.
To do this, we firstselect a corpus of sentences from a very generalbody of instructional material (a range of populartopics from Wikipedia).
We then generate a con-strained subset of all possible gaps via NLP heuris-tics, and pair each gap with a broad variety offeatures pertaining to how it was generated.
Wethen solicit a large number of human judgments viacrowdsourcing to help us rate the quality of variousgaps.
With that data in hand, we train a machinelearning model to replicate these judgments.
Theresults are promising, with one possible operatingpoint producing a true positive rate of 83% with acorresponding false positive rate of 19% (83% ofthe possible Good gaps are kept, and 19% of thenot-Good gaps are incorrectly marked); see Figure6 for the full ROC curve and Section 4 for an ex-planation of the labels.
As the final model has onlyminimal dependence on Wikipedia-specific fea-tures, we expect that it can be applied to an evenwider variety of material (blogs, news articles,health sites, etc.
).2 Background and Related WorkThere already exists a large body of work in auto-matic question generation (QG) for educationalpurposes dating back to the Autoquest system(Wolfe, 1976), which used an entirely syntacticapproach to generate Wh-Questions from individu-al sentences.
In addition to Autoquest, several oth-ers have created systems for Wh-questiongeneration using approaches including transfor-mation rules (Mitkov and Ha, 2003), template-based generation (Chen et al, 2009; Curto et al,2011), and overgenerate-and-rank (Heilman andSmith, 2010a).
The work in this area has largelyfocused on the surface form of the questions, withan emphasis on grammaticality.Alternatively, generation of gap-fill style ques-tions (a.k.a.
cloze questions) avoids these issues ofgrammaticality by blanking out words or spans in aknown good sentence.
There is a large body of ex-isting work that has focused on generation of thistype of question, most of which has focused onvocabulary and language learning.
The recent workof Agarwal and Mannem (2011) is closer to ourpurposes; they generated fill-in-the-blank questionsand distractor answers for reading comprehensiontests using heuristic scoring measures and a smallevaluation set.
Our work has similar aims but em-ploys a data-driven approach.The Question-Generation Shared Task andEvaluation Challenge (QG-STEC) (Rus et al,2010) marks a first attempt at creating a commontask and corpus for empirical evaluation of ques-tion generation components.
However, evaluationin this task was manual and the number of instanc-es in both the development and training set weresmall.
As there exists no other dataset for questiongeneration, we created a new corpus using AmazonMechanical Turk by soliciting judgments fromnon-experts.
Snow et al (2008) have validatedAMT as a valid data source by comparing non-expert with gold-standard expert judgments.
Cor-pus creation using AMT has numerous precedentsnow; see i.e.
Callison-Burch and Dredze (2010)and Heilman and Smith (2010b).
We have madeour corpus (see Section 4) available online to ena-ble others to continue research on the gap-selectionproblem we address here.3 Question GenerationTo achieve our goal of selecting better gap-fillquestions, we have broken down the task into stag-es similar to those proposed by Nielsen (2008): 1)sentence selection, 2) question construction, and 3)classification/scoring.
Specifically, we utilize sum-marization to identify key sentences from a pas-sage.
We then apply semantic and syntacticconstraints to construct multiple candidate ques-tion/answer pairs from a given source sentence.Lastly we extract features from these hypothesesfor use with a question quality classification mod-el.
To train this final question-scoring component,we made use of crowdsourcing to collect ratingsfor a corpus of candidate questions.
While thispipeline currently produces gap-fill questions, weenvision these components can be used as input formore complex surface generation such as Wh-forms or distractor selection.7433.1 Sentence SelectionWhen learning about a new subject, a student willmost likely want to learn about key concepts be-fore moving onto more obscure details.
As such, itis necessary to order target sentences in terms oftheir importance.
This is fortunately very similar tothe goals of automatic summarization, in which theselected sentences should be ordered by how cen-tral they are to the article.As a result, we make use of our own implemen-tation of SumBasic (Nenkova et al, 2006), a sim-ple but competitive document summarization al-gorithm motivated by the assumption thatsentences containing the article?s most frequentlyoccurring words are the most important.
We thususe the SumBasic score for each sentence to orderthem as candidates for question construction.3.2 Question ConstructionWe seek to empirically determine how to choosequestions instead of relying on heuristics and rulesfor evaluating candidate surface forms.
To do this,we cast question construction as a generate-and-filter problem: we overgenerate potential ques-tion/answer pairs from each sentence and train adiscriminative classifier on human judgments ofquality for those pairs.
With the trained classifier,we can then apply this approach on unseen sen-tences to return the highest-scoring ques-tion/answer, all question/answer pairs scoringabove a threshold, and so on.GenerationAlthough it would be possible to select every wordor phrase as a candidate gap, this tactic would pro-duce a skewed dataset composed mostly of unusa-ble questions, which would subsequently requiremuch more annotation to discriminate good ques-tions from bad ones.
Instead we rely on syntacticand semantic constraints to reduce the number ofquestions that need annotation.To generate questions we first run the sourcesentence through a constituency parser and a se-mantic role labeler (components of a state-of-the-art natural language toolkit from (Quirk et al,2012)), with the rationale that important parts ofthe sentence will occur within a semantic role.Each verb predicate found within the roles thenautomatically becomes a candidate gap.
From eve-ry argument to the predicate, we extract all childnoun phrases (NP) and adjectival phrases (ADJP)as candidate gaps as well.
Figure 1 illustrates thisgeneration process.ClassificationTo train the classifier for question quality, we ag-gregated per-question ratings into a single label(see Section 4 for details).
Questions with an aver-age rating of 0.67 or greater were considered aspositive examples.
This outcome was then pairedwith a vector of features (see Section 5) extractedfrom the source sentence and the generated ques-tion.Because our goal is to score each candidatequestion in a meaningful way, we use a calibratedlearner, namely L2-regularized logistic regression(Bishop 2006).
This model?s output is(|	); in our case this is the posteri-or probability of a candidate receiving a positivelabel based on its features.4 Corpus ConstructionWe downloaded 105 articles from Wikipedia?slisting of vital articles/popular pages.1 These arti-cles represent a cross section of historical, social,1http://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Popular_pagesIn 1874 R?ntgen              a lecturer at the University of Strassburg.In          R?ntgen became a lecturer at the University of Strassburg.In 1874               became a lecturer at the University of Strassburg.In 1874 R?ntgen became a              at the University of Strassburg.In 1874 R?ntgen became a lecturer at                         of Strassburg.In 1874 R?ntgen became a lecturer at the University of                  .In 1874 R?ntgen became a lecturer at                                              .Figure 1 An example of the question generation process.744and scientific topics.
From each article we sampled10 sentences using the sentence selection algorithmdescribed in Section 3.1 for 50% of the sentences;the other 50% were chosen at random to preventany possible overfitting to the selection method.These sentences were then processed using thecandidate generation method from Section 3.2.To collect training data outcomes for our ques-tion classifier, we used Amazon?s MechanicalTurk (AMT) service to obtain human judgments ofquality for each question.
We initially consideredasking about the quality of individual ques-tion/answer pairs in isolation, but in pilot studieswe found poor agreement in this case; we noticedthat the inability to compare with other possiblequestions actually made the task seem difficult andarbitrary.
We thus instead presented raters with asource sentence and a list of up to ten candidatequestions along with their corresponding answers(see Figure 2).
Raters were asked to rate questions?quality as Good, Okay, or Bad.
The task instruc-tions defined a Good question as ?one that testskey concepts from the sentence and would be rea-sonable to answer.?
An Okay question was definedas ?one that tests key concepts but might be diffi-cult to answer (the answer is too lengthy, the an-swer is ambiguous, etc.).?
A Bad question was?one that asks about an unimportant aspect of thesentence or has an uninteresting answer that can befigured out from the context of the sentence.?
The-se ratings were binarized into a score of one forGood and zero for not-Good (Okay or Bad), as ourgoal was to find the probability of a question beingtruly Good (and not just Okay).22Heilman and Smith (2010a and b) asked raters to identifyquestion deficiencies, including vague or obvious, but raterswere not asked to differentiate between Good and Okay.
Thusquestions considered Good in their study would include Okay.Thus far we have run 300 HITs with 4 judgesper HIT.
Each HIT consisted of up to 10 candidatequestions generated from a single sentence.
In totalthis yielded 2252 candidate questions with 4 rat-ings per question from 85 unique judges.
We thenwished to eliminate judges who were gaming thesystem or otherwise performing poorly on the task.It is common to do such filtering when usingcrowdsourced data by using the majority or medianvote as the final judgment or to calibrate judgesusing expert judgments (Snow et al 2008).
Otherapproaches to annotator quality control includeusing EM-based algorithms for estimating annota-tor bias (Wiebe et al 1999, Ipeirotis et al 2010).In our case, we computed the distance for eachjudge from the median judgment (from all judges)on each question, then took the mean of this dis-tance over all questions they rated.
We removedjudges with a mean distance two standard devia-tions above the mean distance, which eliminatedthe five judges who disagreed most with others.In addition to filtering judges, we wanted to fur-ther constrain the data to those questions on whichthe human annotators had reasonable agreement, asit would not make sense to attempt to train a modelto replicate judgments on which the annotatorsthemselves could not agree.
To do this, we com-puted the variance of the judgments for each ques-tion.
By limiting the variance to 0.3, we kept ques-tions on which up to 1 judge (out of 4) disagreed;this eliminated 431 questions and retained the 1821with the highest agreement.
Of these filtered ques-tions, 700 were judged to be Good (38%).To formally assess inter-rater reliability wecomputed Krippendorff?s alpha (Krippendorff,2004), a statistical measure of agreement applica-ble for situations with multiple raters and incom-plete data (in our case not all raters providedratings for all items).
An alpha value of 1.0 indi-cates perfect agreement, and an alpha value of 0.0Source Sentence:The large scale production of chemicals was an important development during the Industrial Revolution.Question Answer RatingsThe _ _ _ _ _ _ _ _ _ _ _ _  of chemicals was an importantdevelopment during the Industrial Revolution.large scale production ?
Good   ?
Okay   ?
BadThe large scale production of _ _ _ _ _ _was an importantdevelopment during the Industrial Revolution.chemicals ?
Good   ?
Okay   ?
BadThe large scale production of chemicals was an importantdevelopment during the _ _ _ _ _ _ _ _ _ _ _ .Industrial Revolution ?
Good   ?
Okay   ?
BadFigure 2: Example question rating HIT745indicates no agreement.
Our original data yieldedan alpha of 0.34, whereas after filtering judges andquestions the alpha was 0.51.
It should be notedthat because Krippendorff?s Alpha accounts forvariability due to multiple raters and sample size,its values tend to be more pessimistic than manyKappa values commonly used to measure inter-rater reliability.For others interested in working with this data,we have made our corpus of questions and ratingsavailable for download at the following location:http://research.microsoft.com/~sumitb/questiongeneration.5 Model FeaturesWhile intuition would suggest that selecting high-quality gaps for cloze questions should be astraightforward task, analysis of our features im-plies that identifying important knowledge dependson more complex interactions between syntax, se-mantics, and other constraints.
In designing fea-tures, we focused on using commonly extractedNLP information to profile the answer (gap), thesource sentence, and the relation between the two.To enable extraction of these features, we usedthe MSR Statistical Parsing and Linguistic Analy-sis Toolkit (MSR SPLAT)3, a state-of-the-art, web-based service for natural language processing(Quirk et al, 2012).
Table 1 shows a breakdown ofour feature categories and their relative proportionof the feature space.
In the subsections below, wedescribe the intuitions behind our choice of fea-tures and highlight example features from each ofthese categories.
An exhaustive list of the featurescan be found at the corpus URL listed in Section 4.5.1 Token Count FeaturesA good question gives the user sufficient context toanswer correctly without making the answer obvi-ous.
At the same time, gaps with too many wordsmay be impossible to answer.
Figure 3 shows thedistributions of number of tokens in the answer(i.e., the gap) for Good and not-Good questions.
Asintuition would predict, the not-Good class hashigher likelihoods for the longer answer lengths.
Inaddition to the number and percentage of tokens inthe answer features, we also included other token3http://research.microsoft.com/projects/msrsplatcount features such as the number of tokens in thesentence, and the number of overlapping tokensbetween the answer and the remainder of the sen-tence.Feature Category Number of FeaturesToken Count 5Lexical 11Syntactic 112Semantic 40Named Entity 11Wikipedia link 3Total 182Table 1: Breakdown of features by category5.2 Lexical featuresAlthough lexical features play an important role insystem performance for several NLP tasks likeparsing, and semantic role labeling, they require alarge number of examples to provide practical ben-efit.
Furthermore, because most sentences in Wik-ipedia articles feature numerous domain-specificterms and names, we cannot rely on lexical fea-tures to generalize across the variety of possiblearticles in our corpus.
Instead we approximate lex-icalization by computing densities of word catego-ries found within the answer.
The intuition behindthese features is that an answer composed primari-ly of pronouns and stopwords will make for a badquestion while an answer consisting of specificentities may make for a better question.
Examplesof our semi-lexical features include answer pro-noun density, answer abbreviation density, answercapitalized word density, and answer stopworddensity.5.3 Syntactic FeaturesThe answer?s structure relative to the sentence?sstructure provides information as to where betterspans for the gap may exist.
Similarly, part-of-speech (POS) tags give a topic-independent repre-sentation of the composition and makeup of thequestions and answers.
The collection of syntacticfeatures includes the answer?s depth with the sen-tence?s constituent parse, the answer?s locationrelative to head verb (before/after), the POS tagbefore the answer, the POS tag after the answer,and the answer bag-of-POS tags.746Figure 3: Distribution of number of tokens in the answerfor Good and not-Good questions.5.4 Semantic Role Label FeaturesBeyond syntactic constraints, semantics can yieldadditional cues in identifying the important spansfor questioning.
Shallow-semantic parses like thosefound in Propbank (Palmer et al, 2005) provide aconcise representation for linking predicates(verbs) to their arguments.
Because these semanticrole labels (SRLs) often correspond to the ?who,what, where, and when?
of a sentence, they natu-rally lend themselves for use as features for ratingquestion quality.
To compute SRL features, weused the MSR SPLAT?s semantic role labeler tofind the SRLs whose spans cover the question?sanswer, the SRLs whose spans are contained with-in the answer, and the answer?s constituent parsedepth within the closest covering SRL node.To investigate whether judges keyed in on spe-cific roles or modifiers when rating questions, weplotted the distribution of the answer-coveringSRLs (Figure 4).
This graph indicates that goodanswers are not associated with only a single labelbut are actually spread across all SRL classes.While the bulk of questions came from the argu-ments often corresponding to subjects and objects(ARG0-2, shown as A0-A2), we see that good andbad questions have mostly similar distributionsover SRL classes.
However, a notable exceptionare answers covered by verb predicates (shown as?predicate?
), which were highly skewed with 190of the 216 (88.0%) question/answer pairs exhibit-ing this feature labeled as Bad.
Together these dis-tributions may suggest that judges are more likelyto rate gap-fill questions as Good if they corre-spond   to  questions  of   ?who,  what,  where,  andFigure 4: Distribution of semantic role labels forGood and not-Good questions.when?
over questions pertaining to ?why andhow.
?5.5 Named Entity FeaturesFor many topics, especially in the social sciences,knowing the relevant people and places marks thefirst step toward comprehending new material.
Toreflect these concerns we use the named-entitytagger in the toolkit to identify the spans of textthat refer to persons, locations, or organizations,which are then used to derive additional featuresfor distinguishing between candidate questions.Example named-entity features include: answernamed entity density, answer named entity typefrequency (LOC, ORG, PER), and sentence namedentity frequency.Figure 5 shows the distribution of named entitytypes found within the answers for Good and not-Good questions.
From this graph, we see that Goodquestions have a higher class-conditional probabil-ity of containing a named entity.
Furthermore, wesee that Good questions are not confined to a sin-gle named entity type, but are spread across alltypes.
Together, these distributions indicate thatwhile named entities can help to identify importantgaps, the majority of questions labeled Good donot contain any named entity (515/700, i.e.
74%).This provides substantial evidence for generatingquestions for more than only named entities.747Figure 5: Distribution of answer named entity type forGood and not-Good questions.5.6 Wikipedia Link FeaturesWikipedia?s markup language allows spans of textto link to other articles.
This annotation inherentlyindicates a span of text as noteworthy, and canserve as evidence of an answer?s importance.
Weuse the presence of this markup to compute fea-tures such as answer link density, sentence linkdensity, and the ratio of the number of linkedwords in the answer to the ratio of linked words inthe sentence.6 Model and TrainingWe chose logistic regression as our classifier be-cause of its calibrated output of the class posterior;we combined it with an L2 regularizer to preventoverfitting.
As the data likelihood is convex in themodel parameters, we trained the model to maxim-ize this quantity along with the regularization termusing the L-BFGS algorithm for Quasi-Newtonoptimization (Nocedal, 1980).
Evaluation wasconducted with 10-fold cross validation, takingcare to stratify folds so that all questions generatedfrom the same source sentence are placed in thesame fold.
Results are shown in Section 7 below.To ensure that we were not overly narrow inthis model choice, we tested two other more pow-erful classifiers that do not have calibrated outputs,a linear SVM and a boosted mixture of decisiontrees (Caruana and Niculescu-Mizil, 2006); bothproduced accuracies within a percentage point ofour model at the equal error rate.7 Results and DiscussionFigure 6 shows ROC curves for our question quali-ty classifier produced by sweeping the threshold onthe output probability, using the raw collected data,our filtered version as described above, and a fur-ther filtered version keeping only those questionswhere judges agreed perfectly; the benefits of fil-tering can be seen in the improved performance.
Inthis context, the true positive rate refers to the frac-tion of Good questions that were correctly identi-fied, and the false positive rate refers to thefraction of not-Good questions that were incorrect-ly marked.
At the equal error rate, the true positiverate was 0.83 and the false positive rate was 0.19.Figure 6: ROC for our model using unfiltered data(green dots), our filtered version (red dashes), and fil-tered for perfect agreement (blue line).Choosing the appropriate operating point dependson the final application.
By tuning the classifier?strue positive and false positive rates, we can cus-tomize the system for several uses.
For example, ina relatively structured scenario like compliancetraining, it may be better to reduce any possibilityof confusion by eliminating false positives.
On theother hand, a self-motivated learner attempting toexplore a new topic may tolerate a higher falsepositive rate in exchange for a broader diversity ofquestions.
The balance is subtle, though, as ill-formed and irrelevant questions could leave thelearner bored or frustrated, but alternatively, overlyconservative question classification could poten-tially eliminate all but the most trivial questions.TruePositiveRate748Figure 7: ROC for our model with (red dash) and with-out (blue line) Wikipedia-specific features.Figure 8: Classifier learning curve; each point repre-sents mean accuracy over 40 folds.We next wanted to get a sense of how well themodel would generalize to other text, and as suchran an analysis of training the classifier without thebenefit of the Wikipedia-specific features (Figure7).
The resulting model performs about the same asthe original on average over the ROC, slightly bet-ter in some places and slightly worse in others.
Wehypothesize the effect is small because these fea-tures relate only to Wikipedia entities, and the oth-er named entity features likely make themredundant.Finally, to understand the sensitivity of ourmodel to the amount of training data, we plot alearning curve of the question classifier?s accuracyby training it against fractions of the available data(Figure 8).
While the curve starts to level outaround 1200 data points, the accuracy is still risingslightly, which suggests the system could achievesome small benefits in accuracy from more data.7.1 Error AnalysisTo explore the nature of our system?s misclassifi-cations we examine the errors that occur at theequal error rate operating point.
For our system,false positive errors occur when the system labels aquestion as Good when the raters considered it not-Good.
Table 2 lists three examples of this type oferror.
The incorrect high score in example 1(?Greeks declared ___?)
suggests that system per-formance can be improved via language modeling,as such features would penalize questions with an-swers that could be predicted mostly by word tran-sition probabilities.
Similarly, when classifyingquestions like example 2 (?such as ____ for amathematical function?
), the system could benefitfrom some measure of word frequency or answernovelty.
While our model included a feature for thenumber of overlapping words between the questionand the answer, the high classifier score for exam-ple 3 (?atop ______, the volcano?
), suggests thatthis can be solved by explicitly filtering out suchquestions at generation time.With false negative errors the judges rated thequestion as Good, whereas the system classified itas Bad.
The question and answer pairs listed inTable 3 demonstrate some of these errors.
In ex-ample 1 (?where Pompey was soon ___?
), the sys-tem was likely incorrect because a majority ofquestions with verb-predicate answers had Badratings (only 12% are Good).
Conversely, classifi-cation of example 2 (?Over the course of dec-ades??)
could be improved with a featureindicating the novelty of the words in the answers.Example 3 (?About 7.5% of the...?)
appears tocome from rater error or rater confusion as thequestion does little to test the understanding of thematerial.While the raters considered the answer to ex-ample 4 as Good, the low classifier score arguesfor different handling of answers derived fromlong coordinated phrases.
One alternative approachwould be to generate questions that use multiplegaps.
Conversely, one may argue that a learnermay be better off answering any one of the nounphrases like palm oil or cocoa in isolation.accuracy749Question Answer Confidence1 In 1821 the Greeksdeclared ___ on thesultan.war 0.7322 He also introducedmuch of the modernmathematical terminol-ogy and notation, par-ticularly formathematical analysis,such as _________ of amathematical function.the notion 0.5273 Not only is there muchice atop ________, thevolcano is also beingweakened by hydro-thermal activity.the volcano 0.790Table 2: Example false positives (human judges ratedthese as not-Good)Question Answer Confidence1 Caesar then pursuedPompey to Egypt,where Pompey wassoon  ____.murdered 0.4712 Over the course of dec-ades, individual wellsdraw down local tem-peratures and waterlevels until _______ isreached with naturalflows.a newequilibrium0.3063 About 7.5% of worldsea trade is carried viathe canal ____.today 0.1194 Asante and Dahomeyconcentrated on thedevelopment of ?legiti-mate commerce?
in__________, formingthe bedrock of WestAfrica?s modern exporttrade,the form ofpalm oil,cocoa, tim-ber, andgold.0.029Table 3: Example false negatives (human judges ratedthese Good)7.2 Feature AnalysisTo ensure that all of the gain of the classifier wasnot coming from only a handful of isolated fea-tures, we examined the mean values for each fea-ture?s learned weight in the model over the courseof 10 cross-validation folds, and then sorted themeans for greater clarity (Figure 8).
The weightsindeed seem to be well distributed across manyfeatures.Figure 8: Feature weight means and standard deviations.8 Discussion and Future WorkWe have presented a method that determineswhich gaps in a sentence to ask questions about bytraining a classifier that largely agrees with humanjudgments on question quality.
We feel this effortis complementary to the past work on questiongeneration, and represents another step towardshelping self-motivated learners with automaticallygenerated tests.In our future work, we hope to expand the set offeatures as described in Section 7.
We additionallyintend to cast the sentence selection problem as aseparate learning problem that can also be trainedfrom human judgments.ReferencesManish Agarwal and Prashanth Mannem.
2011.
Auto-matic Gap-fill Question Generation from TextBooks.
In Proceedings of the 6th Workshop on In-novative Use of NLP for Building Educational Ap-plications.
Portland, OR, USA.
pages 56-64.Richard C. Anderson and W. Barry Biddle.
1975.
Onasking people questions about what they are read-ing.
In G. Bower (Ed.)
Psychology of Learning andMotivation, 9:90-132.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
New York: Springer, 2006.Jonathan C. Brown, Gwen A. Frishkoff, and MaxineEskenazi.
2005.
Automatic Question Generation forVocabulary Assessment.
In Proceedings ofHLT/EMNLP 2005.
Vancouver, Canada: Associa-tion for Computational Linguistics.
pages 819-826.Rich Caruana and Alexandru Niculescu-Mizil.
2006.
AnEmpirical Comparison of Supervised Learning Al-gorithms.
In Proceedings of ICML 2006.meanandstd.dev.
of featureweight750Chris Callison-Burch and Mark Dredze.
2010.
CreatingSpeech and Language Data with Amazon's Me-chanical Turk.
In Proceedings of NAACL 2010Workshop on Creating Speech and Language Datawith Amazon's Mechanical Turk.
Los Angeles, CA.pages 1-12.Wei Chen, Gregory Aist, and Jack Mostow.
2009.
Gen-erating Questions Automatically from Information-al Text.
In S. Craig & S. Dicheva (Ed.
),Proceedings of the 2nd Workshop on QuestionGeneration.S?rgio Curto, Ana Cristina Mendes, and Lu?sa Coheur.2011.
Exploring linguistically-rich patterns forquestion generation.
In Proceedings of the UCNLG+ eval: Language Generation and EvaluationWorkshop.
Edinburgh, Scotland: Association forComputational Linguistics.
Pages 33-38.Michael Heilman and Noah A. Smith.
2010a.
GoodQuestion!
Statistical Ranking for Question Genera-tion.
In Proceedings of NAACL/HLT 2010. pages609-617.Michael Heilman and Noah A. Smith.
2010b.
RatingComputer-Generated Questions with MechanicalTurk.
In Proceedings of NAACL 2010 Workshop onCreating Speech and Language Data with Ama-zon's Mechanical Turk.
Los Angeles, CA.
pages 35-40.Ayako Hoshino and Hiroshi Nakagawa.
2005.
A real-time multiple-choice question generation for lan-guage testing - a preliminary study -.
In Proceed-ings of the 2nd Workshop on Building EducationalApplications Using NLP.
Ann Arbor, MI, USA:Association for Computational Linguistics.
pages17-20.Panagiotis G. Ipeirotis, Foster Provost, Jing Wang .2010.
In Proceedings of the ACM SIGKDD Work-shop on Human Computation (HCOMP?10).Klaus Krippendorff.
2004.
Content Analysis: An Intro-duction to Its Methodology.
Thousand Oaks, CA:Sage.Ruslan Mitkov and Le An Ha.
2003.
Computer-AidedGeneration of Multiple-Choice Tests.
Proceedingsof the HLT-NAACL 2003 Workshop on BuildingEducational Applications Using Natural LanguageProcessing, pages 17-22.Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.2006.
A computer-aided environment for generat-ing multiple choice test items.
Natural LanguageEngineering, 12(2): 177-194.Ani Nenkova, Lucy Vanderwende, and KathleenMcKeown.
2006.
A Compositional Context Sensi-tive Multidocument Summarizer.
In Proceedings ofSIGIR 2006. pages 573-580.Rodney D. Nielsen.
2008.
Question Generation: Pro-posed Challenge Tasks and Their Evaluation.
In V.Rus, & A. Graesser (Ed.
), In Proceedings of theWorkshop on the Question Generation Shared Taskand Evaluation Challenge.
Arlington, VA.Jorge Nocedal.
1980.
Updating Quasi-Newton Matriceswith Limited Storage.
Mathematics of Computa-tion, 35:773-782.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An Annotated Corpus ofSemantic Roles.
Computational Linguistics, vol.31, no.
1, pp.
71--106.Chris Quirk, Pallavi Choudhury, Jianfeng Gao, HisamiSuzuki, Kristina Toutanova, Michael Gamon, Wen-tau Yih, and Lucy Vanderwende.
2012.
MSRSPLAT, a language analysis toolkit.
In Proceedingsof NAACL HLT 2012 Demonstration Session.http://research.microsoft.com/projects/msrsplat .Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,Svetlana Stoyanchev and Cristian Moldovan.
2010.Overview of The First Question Generation SharedTask Evaluation Challenge.
In Proceedings of theThird Workshop on Question Generation.
Pitts-burgh, PA, USA.
pages 45-57.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and Fast?but is itGood?
Evaluating non-Expert Annotations for Nat-ural Language Tasks.
In Proceedings ofEMNLP?08.
pages 254-263.Lucy Vanderwende.
2008.
The Importance of BeingImportant: Question Generation.
In Proceedings ofthe 1st Workshop on the Question GenerationShared Task Evaluation Challenge, Arlington, VA.Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.O?Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.
InProceedings of ACL 1999.John H. Wolfe.
1976.
Automatic question generationfrom text - an aid to independent study.
In Proceed-ings of the ACM SIGCSE-SIGCUE technical sym-posium on Computer science and education.
NewYork, NY, USA: ACM.
pages 104-112.751
