Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562?570,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsCooooooooooooooollllllllllllll!!!!!!!!!!!!!
!Using Word Lengthening to Detect Sentiment in MicroblogsSamuel BrodySchool of Communicationand InformationRutgers Universitysdbrody@gmail.comNicholas DiakopoulosSchool of Communicationand InformationRutgers Universitydiakop@rutgers.eduAbstractWe present an automatic method which lever-ages word lengthening to adapt a sentimentlexicon specifically for Twitter and similar so-cial messaging networks.
The contributions ofthe paper are as follows.
First, we call at-tention to lengthening as a widespread phe-nomenon in microblogs and social messag-ing, and demonstrate the importance of han-dling it correctly.
We then show that lengthen-ing is strongly associated with subjectivity andsentiment.
Finally, we present an automaticmethod which leverages this association to de-tect domain-specific sentiment- and emotion-bearing words.
We evaluate our method bycomparison to human judgments, and analyzeits strengths and weaknesses.
Our results areof interest to anyone analyzing sentiment inmicroblogs and social networks, whether forresearch or commercial purposes.1 IntroductionRecently, there has been a surge of interest in sen-timent analysis of Twitter messages.
Many re-searchers (e.g., Bollen et al 2011; Kivran-Swaineand Naaman 2011) are interested in studying struc-ture and interactions in social networks, where senti-ment can play an important role.
Others use Twitteras a barometer for public mood and opinion in di-verse areas such as entertainment, politics and eco-nomics.
For example, Diakopoulos and Shamma(2010) use Twitter messages posted in conjunctionwith the live presidential debate between BarackObama and John McCain to gauge public opinion,Bollen et al (2010) measure public mood on Twitterand use it to predict upcoming stock market fluc-tuations, and O?Connor et al (2010) connect pub-lic opinion data from polls to sentiment expressed inTwitter messages along a timeline.A prerequisite of all such research is an effec-tive method for measuring the sentiment of a postor tweet.
Due to the extremely informal nature ofthe medium, and the length restriction1, the lan-guage and jargon which is used in Twitter varies sig-nificantly from that of commonly studied text cor-pora.
In addition, Twitter is a quickly evolvingdomain, and new terms are constantly being intro-duced.
These factors pose difficulties to methodsdesigned for conventional domains, such as news.One solution is to use human annotation.
For exam-ple, Kivran-Swaine and Naaman (2011) use manualcoding of tweets in several emotion categories (e.g.,joy, sadness) for their research.
Diakopoulos andShamma (2010) use crowd sourcing via Amazon?sMechanical Turk.
Manual encoding usually offersa deeper understanding and correspondingly higheraccuracy than shallow automatic methods.
However,it is expensive and labor intensive and cannot be ap-plied in real time.
Crowd-sourcing carries additionalcaveats of its own, such as issues of annotator exper-tise and reliability (see Diakopoulos and Shamma2010).The automatic approach to sentiment analysis iscommonly used for processing data from social net-works and microblogs, where there is often a hugequantity of information and a need for low latency.Many automatic approaches (including all thoseused in the work mentioned above) have at their corea sentiment lexicon, containing a list of words la-1Messages in Twitter are limited to 140 characters, for com-patibility with SMS messaging562beled with specific associated emotions (joy, hap-piness) or a polarity value (positive, neutral, nega-tive).
The overall sentiment of a piece of text is cal-culated as a function of the labels of the componentwords.
Because Twitter messages are short, shal-low approaches are sometimes considered sufficient(Bermingham and Smeaton, 2010).
There are alsoapproaches that use deeper machine learning tech-niques to train sentiment classifiers on examples thathave been labeled for sentiment, either manually orautomatically, as described above.
Recent examplesof this approach are Barbosa and Feng (2010) andPak and Paroubek (2010).Most established sentiment lexicons (e.g., Wilsonet al 2005, see Section 5) were created for a gen-eral domain, and suffer from limited coverage andinaccuracies when applied to the highly informal do-main of social networks communication.
By cre-ating a sentiment lexicon which is specifically tai-lored to the microblogging domain, or adapting anexisting one, we can expect to achieve higher accu-racy and increased coverage.
Recent work in thisarea includes Velikovich et al (2010), who devel-oped a method for automatically deriving an exten-sive sentiment lexicon from the web as a whole.The resulting lexicon has greatly increased cover-age compared to existing dictionaries and can handlespelling errors and web-specific jargon.
Bollen et al(2010) expand an existing well-vetted psychometricinstrument - Profile of Mood States (POMS) (Mc-Nair et al, 1971) that associates terms with moods(e.g.
calm, happy).
The authors use co-occurrenceinformation from the Google n-gram corpus (Brantsand Franz, 2006) to enlarge the original list of 72terms to 964.
They use this expanded emotion lexi-con (named GPOS) in conjunction with the lexiconof Wilson et al (2005) to estimate public mood fromTwitter posts2.The method we present in this paper leverages aphenomenon that is specific to informal social com-munication to enable the extension of an existinglexicon in a domain specific manner.2Although the authors state that all data and methods willbe made available on a public website, it was not present at thetime of the writing of this article.2 MethodologyProsodic indicators (such as high pitch, prolongedduration, intensity, vowel quality, etc.)
have longbeen know (Bolinger, 1965) as ways for a speaker toemphasize or accent an important word.
The waysin which they are used in speech are the subject ofongoing linguistic research (see, for example, Cal-houn 2010).
In written text, many of these indica-tors are lost.
However, there exist some orthographicconventions which are used to mark or substitutefor prosody, including punctuation and typographicstyling (italic, bold, and underlined text).
In purelytext-based domains, such as Twitter, styling is notalways available, and is replaced by capitalizationor other conventions (e.g., enclosing the word in as-terisks).
Additionally, the informal nature of the do-main leads to an orthographic style which is muchcloser to the spoken form than in other, more formal,domains.
In this work, we hypothesize that the com-monly observed phenomenon of lengthening wordsby repeating letters is a substitute for prosodic em-phasis (increased duration or change of pitch).
Assuch, it can be used as an indicator of importantwords and, in particular, ones that bear strong in-dication of sentiment.Our experiments are designed to analyze the phe-nomenon of lengthening and its implications to sen-timent detection.
First, in Experiment I, we showthe pervasiveness of the phenomenon in our dataset,and measure the potential gains in coverage that canbe achieved by considering lengthening when pro-cessing Twitter data.
Experiment II substantiatesthe claim that word lengthening is not arbitrary, andis used for emphasis of important words, includingthose conveying sentiment and emotion.
In the firstpart of Experiment III we demonstrate the implica-tions of this connection for the purpose of sentimentdetection using an existing sentiment lexicon.
Inthe second part, we present an unsupervised methodfor using the lengthening phenomenon to expand anexisting sentiment lexicon and tailor it to our do-main.
We evaluate the method through compari-son to human judgments, analyze our results, anddemonstrate some of the benefits of our automaticmethod.5631.
For every word in the vocabulary, extract the condensed form, where sequences of a repeatedletter are replaced with a single instance of that letter.E.g., niiiice?
nice, realllly ?
realy ...2.
Create sets of words sharing the same condensed form.E.g., {nice, niiice, niccccceee...}, {realy, really, realllly, ...} ...3.
Remove sets which do not contain at least one repeat of length three.E.g.,{committee, committe, commitee}4.
Find the most frequently occurring form in the group, and mark it as the canonical form.E.g., {nice, niiice, niccccceee...}, {realy, really, realllly, ...} ...Figure 1: Procedure for detecting lengthened words and associating them with a canonical form.3 DataHalf a million tweets were sampled from the Twit-ter Streaming API on March 9th 2011.
The tweetswere sampled to cover a diverse geographic distri-bution within the U.S. such that regional variation inlanguage use should not bias the data.
Some tweetswere also sampled from Britain to provide a morediverse sampling of English.
We restricted our sam-ple to tweets from accounts which indicated theirprimary language as English.
However, there maybe some foreign language messages in our dataset,since multi-lingual users may tweet in other lan-guages even though their account is marked as ?En-glish?.The tweets were tokenized and converted tolower-case.
Punctuation, as well as links, hash-tags, and username mentions were removed.
Theresulting corpus consists of approximately 6.5 mil-lion words, with a vocabulary of 22 thousand wordsoccurring 10 times or more.4 Experiment I - DetectionTo detect and analyze lengthened words, we employthe procedure described in Figure 1.
We find setsof words in our data which share a common formand differ only in the number of times each letter isrepeated (Steps 1 & 2).
In Step 3 we remove setswhere all the different forms are likely to be the re-sult of misspelling, rather than lengthening.
Finally,in Step 4, we associate all the forms in a single setwith a canonical form, which is the most commonone observed in the data.The procedure resulted in 4,359 sets of size > 1.To reduce noise resulting from typos and mis-spellings, we do not consider words containing non-alphabetic characters, or sets where the canonicalform is a single character or occurs less than 10times.
This left us with 3,727 sets.Analysis Table 1 lists the canonical forms of the20 largest sets in our list (in terms of the number ofvariations).
Most of the examples are used to ex-press emotion or emphasis.
Onomatopoeic wordsexpressing emotion (e.g., ow, ugh, yay) are oftenlengthened and, for some, the combined frequencyof the different lengthened forms is actually greaterthan that of the canonical (single most frequent) one.Lengthening is a common phenomenon in ourdataset.
Out of half-a-million tweets, containingroughly 6.5 million words, our procedure identifies108,762 word occurrences which are lengtheningsof a canonical form.
These words occur in 87,187tweets (17.44% or approximately one out of everysix, on average).
The wide-spread use of length-ening is surprising in light of the length restrictionof Twitter.
Grinter and Eldridge (2003) point outseveral conventions that are used in text messagesspecifically to deal with this restriction.
The fact thatlengthening is used in spite of the need for brevitysuggests that it conveys important information.Canonical Assumption We validate the assump-tion that the most frequent form in the set is thecanonical form by examining sets containing one ormore word forms that were identified in a standard564Can.
Form Card.
# Can.
# Non-Can.nice 76 3847 348ugh 75 1912 1057lmao 70 10085 3727lmfao 67 2615 1619ah 61 767 1603love 59 16360 359crazy 59 3530 253yeah 57 4562 373sheesh 56 247 131damn 52 5706 299shit 51 10332 372really 51 9142 142oh 51 7114 1617yay 45 1370 375wow 45 3767 223good 45 21042 3171ow 44 116 499mad 44 3627 827hey 44 4669 445please 43 4014 157Table 1: The canonical forms of the 20 largest sets (interms of cardinality), with the number of occurrences ofthe canonical and non-canonical forms.English dictionary3.
This was the case for 2,092 ofthe sets (56.13%).
Of these, in only 55 (2.63%) themost frequent form was not recognized by the dic-tionary.
This indicates that the strategy of choosingthe most frequent form as the canonical one is reli-able and highly accurate (> 97%).Implications for NLP To examine the effects oflengthening on analyzing Twitter data, we look atthe difference in coverage of a standard English dic-tionary when we explicitly handle lengthened wordsby mapping them to the canonical form.
Cov-erage with a standard dictionary is important formany NLP applications, such as information re-trieval, translation, part-of-speech tagging and pars-ing.
The canonical form for 2,037 word-sets areidentified by our dictionary.
We searched for oc-currences of these words which were lengthened bytwo or more characters, meaning they would notbe identified using standard lemmatization methodsor spell-correction techniques that are based on edit3We use the standard dictionary for U.S. English included inthe Aspell Unix utility.distance.
We detected 25,101 occurrences of these,appearing in 22,064 (4.4%) tweets.
This implies thata lengthening-aware stemming method can be usedto increase coverage substantially.5 Experiment II - Relation to SentimentAt the beginning of Section 2 we presented the hy-pothesis that lengthening represents a textual substi-tute for prosodic indicators in speech.
As such, it isnot used arbitrarily, but rather applied to subjectivewords to strengthen the sentiment or emotion theyconvey.
The examples presented in Table 1 in theprevious section appear to support this hypothesis.In this section we wish to provide experimental evi-dence for our hypothesis, by demonstrating a signif-icant degree of association between lengthening andsubjectivity.For this purpose we use an existing sentiment lex-icon (Wilson et al, 2005), which is commonly usedin the literature (see Section 1) and is at the coreof OpinionFinder4, a popular sentiment analysis tooldesigned to determine opinion in a general domain.The lexicon provides a list of subjective words, eachannotated with its degree of subjectivity (stronglysubjective, weakly subjective), as well as its sen-timent polarity (positive, negative, or neutral).
Inthese experiments, we use the presence of a word(canonical form) in the lexicon as an indicator ofsubjectivity.
It should be noted that the reverse isnot true, i.e., the fact that a word is absent from thelexicon does not indicate it is objective.As a measure of tendency to lengthen a word, welook at the number of distinct forms of that word ap-pearing in our dataset (the cardinality of the set towhich it belongs).
We group the words according tothis statistic, and compare to the vocabulary of ourdataset (all words appearing in our data ten timesor more, and consisting of two or more alphabeticcharacters, see Section 4).
Figure 2 shows the per-centage of subjective words (those in the lexicon) ineach of the groups.
As noted previously, this is alower bound, since it is possible (in fact, very likely)that other words in the group are subjective, despitebeing absent from the lexicon.
The graph shows aclear trend - the more lengthening forms a word has,4http://www.cs.pitt.edu/mpqa/opinionfinderrelease/565the more likely it is to be subjective (as measured bythe percentage of words in the lexicon).The reverse also holds - if a word is used to con-vey sentiment, it is more likely to be lengthened.
Wecan verify this by calculating the average number ofdistinct forms for words in our data that are sub-jective and comparing to the rest.
This calculationyields an average of 2.41 forms for words appearingin our sentiment lexicon (our proxy for subjectiv-ity), compared to an average of 1.79 for those thataren?t5.
This difference is statistically significant atp < 0.01%, using a student t-test.The lexicon we use was designed for a generaldomain, and suffers from limited coverage (see be-low) and inaccuracies (see O?Connor et al 2010 andbelow Section 6.2 for examples), due to the domainshift.
The sentiment lexicon contains 6,878 words,but only 4,939 occur in our data, and only 2,446 ap-pear more than 10 times.
Of those appearing in ourdata, only 485 words (7% of the lexicon vocabulary)are lengthened (the bar for group 2+ in Figure 2),but these are extremely salient.
They encompass701,607 instances (79% of total instances of wordsfrom the lexicon), and 339,895 tweets.
This pro-vides further evidence that lengthening is used withsalient sentiment words.These results also demonstrates the limitations ofusing a sentiment lexicon which is not tailored tothe domain.
Only a small fraction of the lexicon isrepresented in our data, and it is likely that there aremany sentiment words that are commonly used butare absent from it.
We address this issue in the nextsection.6 Experiment III - Adapting the SentimentLexiconThe previous experiment showed the connection be-tween lengthening and sentiment-bearing words.
Italso demonstrated some of the shortcomings of alexicon which is not specifically tailored to our do-main.
There are two steps we can take to usethe lengthening phenomenon to adapt an existingsentiment lexicon.
The first of these is simplyto take lengthening into account when identifyingsentiment-bearing words in our corpus.
The second5This, too, is a conservative estimate, since the later groupalso includes subjective words, as mentioned.0%5%10%15%20%25%30%All 2+ 3+ 4+ 5+ 6+ 7+ 8+ 9+ 10+% SubjectiveNumber of VariationsAll 2+ 3+ 4+ 5+ 6+ 7+ 8+ 9+ 10+18,817 3,727 2,451 1,540 1,077 778 615 487 406 335Figure 2: The percentage of subjective word-sets (thosewhose canonical form appears in the lexicon) as a func-tion of cardinality (number of lengthening variations).The accompanying table provides the total number of setsin each cardinality group.is to exploit the connection between lengthening andsentiment to expand the lexicon itself.6.1 Expanding Coverage of Existing WordsWe can assess the effect of specifically consider-ing lengthening in our domain by measuring theincrease of coverage of the existing sentiment lex-icon.
Similarly to Experiment I (Section 4), wesearched for occurrences of words from the lexi-con which were lengthened by two or more charac-ters, and would therefore not be detected using edit-distance.
We found 12,105 instances, occurring in11,439 tweets (2.29% of the total).
This increase incoverage is relatively small6, but comes at almost nocost, by simply considering lengthening in the anal-ysis.A much greater benefit of lengthening, however,results from using it as an aid in expanding the sen-timent lexicon and detecting new sentiment-bearingwords.
This is the subject of the following section.6.2 Expanding the Sentiment VocabularyIn Experiment II (Section 5) we showed that length-ening is strongly associated with sentiment.
There-fore, words which are lengthened can provide uswith good candidates for inclusion in the lexicon.We can employ existing sentiment-detection meth-6Note that almost half of the increase in coverage calculatedin Experiment I (Section 4) comes from subjective words!566ods to decide which candidates to include, and de-termine their polarity.Choosing a Candidate Set The first step in ex-panding the lexicon is to choose a set of candidatewords for inclusion.
For this purpose we start withwords that have 5 or more distinct forms.
Thereare 1,077 of these, of which only 217 (20.15%)are currently in our lexicon (see Figure 2).
Sincewe are looking for commonly lengthened words,we disregard those where the combined frequencyof the non-canonical forms is less than 1% that ofthe canonical one.
We also remove stop words,even though some are often lengthened for emphasis(e.g., me, and, so), since they are too frequent, andintroduce many spurious edges in our co-occurrencegraph.
Finally, we filter words based on weight, asdescribed below.
This leaves us with 720 candidatewords.Graph Approach We examine two methods forsentiment detection - that of Brody and Elhadad(2010) for detecting sentiment in reviews, and that ofVelikovich et al (2010) for finding sentiment termsin a giga-scale web corpus.
Both of these employa graph-based approach, where candidate terms arenodes, and sentiments is propagated from a set ofseed words of known sentiment polarity.
We calcu-lated the frequency in our corpus of all strongly pos-itive and strongly negative words in the Wilson et al(2005) lexicon, and chose the 100 most frequent ineach category as our seed sets.Graph Construction Brody and Elhadad (2010)considered all frequent adjectives as candidates andweighted the edge between two adjectives by a func-tion of the number of times they both modified asingle noun.
Velikovich et al (2010) constructeda graph where the nodes were 20 million candidatewords or phrases, selected using a set of heuristicsincluding frequency and mutual information of wordboundaries.
Context vectors were constructed foreach candidate from all its mentions in a corpus of4 billion documents, and the edge between two can-didates was weighted by the cosine similarity be-tween their context vectors.Due to the nature of the domain, which is highlyinformal and unstructured, accurate parsing is dif-ficult.
Therefore we cannot employ the exact con-struction method of Brody and Elhadad (2010).
Onthe other hand, the method of Velikovich et al(2010) is based on huge amounts of data, and takesadvantage of the abundance of contextual informa-tion available in full documents, whereas our do-main is closer to that of Brody and Elhadad (2010),who dealt with a small number of candidates andshort documents typical to online reviews.
There-fore, we adapt their construction method.
We con-sider all our candidates words as nodes, along withthe words in our positive and negative seed sets.
As aproxy for syntactic relationship, edges are weightedas a function of the number of times two words oc-curred within a three-word window of each other inour dataset.
We remove nodes whose neighboringedges have a combined weight of less than 20, mean-ing they participate in relatively few co-occurrencerelations with the other words in the graph.Algorithm Once the graph is constructed, we canuse either of the propagation algorithms of Brodyand Elhadad (2010) and Velikovich et al (2010),which we will denote Reviews and Web, respec-tively.
The Reviews propagation method is based onZhu and Ghahramani (2002).
The words in the posi-tive and negative seed groups are assigned a polarityscore of 1 and 0, respectively.
All the rest start witha score of 0.5.
Then, an update step is repeated.
Inupdate iteration t, for each word x that is not in theseed, the following update rule is applied:pt(x) =?y?N(x)w(y, x) ?
pt?1(y)?y?N(x)w(y, x)(1)Where pt(x) is the polarity of word x at step t,N(x)is the set of the neighbors of x, and w(y, x) is theweight of the edge connecting x and y. FollowingBrody and Elhadad (2010), we set this weight to be1 + log(#co(y, x)), where #co(y, x) is the numberof times y and x co-occurred within a three-wordwindow.
The update step is repeated to convergence.Velikovich et al (2010) employed a differentlabel propagation method, as described in Fig-ure 3.
Rather than relying on diffusion along thewhole graph, this method considers only the sin-gle strongest path between each candidate and eachseed word.
In their paper, the authors claim thattheir algorithm is more suitable than that of Zhu andGhahramani (2002) to a web-based dataset, which567Input: G = (V,E), wij ?
[0, 1]P,N, ?
?
R, T ?
NOutput: poli ?
R|V |Initialize: poli, pol+i , pol-i = 0 for all ipol+i = 1.0 for all vi ?
P andpol-i = 1.0 for all vi ?
N1: ?ij = 0 for all i 6= j, ?ii = 1 for all i2: for vi ?
P3: F = {vi}4: for t : 1...T5: for (vk, vj) ?
E such that vk ?
F6: ?ij = max(?ij , ?ik ?
wk,j)F = F ?
{vj}7: for vj ?
V8: pol+j =?vi?P ?ij9: Repeat steps 1-8 using N to compute pol-10: ?
=?i pol+i /?i pol-i11: poli = pol+i ?
?pol-i , for all i12: if |poli| < ?
then poli = 0.0 for all iFigure 3: Web algorithm from Velikovich et al (2010).P and N are the positive and negative seed sets, respec-tively, wij are the weights, and T and ?
are parameters9.contained many dense subgraphs and unreliable as-sociations based only on co-occurrence statistics.We ran both algorithms in our experiment7, andcompared the results.Evaluation We evaluated the output of the algo-rithms by comparison to human judgments.
Forwords appearing in the sentiment lexicon, we usedthe polarity label provided.
For the rest, similarlyto Brody and Elhadad (2010), we asked volunteersto rate the words on a five-point scale: strongly-negative, weakly-negative, neutral, weakly-positive,or strongly-positive.
We also provided a N/A optionif the meaning of the word was unknown.
Each wordwas rated by two volunteers.
Words which were la-beled N/A by one or more annotators were consid-ered unknown.
For the rest, exact inter-rater agree-7We normalize the weights described above when using theWeb algorithm.9In Velikovich et al (2010), the parameters T and ?
weretuned on a held out dataset.
Since our graphs are comparativelysmall, we do not need to limit the path length T in our search.We do not use the threshold ?, but rather employ a simple cutoffof the top 50 words.Human JudgmentPos.
Neg.
Neu.
Unk.Web Pos.
18 2 26 2Neg.
8 19 17 8Reviews Pos.
21 6 21 2Neg.
9 14 11 16Table 2: Evaluation of the top 50 positive and negativewords retrieved by the two algorithms through compari-son to human judgment.Webpos.
neg.see shitwin niggasway disgotta gettinsummer smhlets tighthaha fuckinbirthday fucktomorrow sickever holyschool smfhpeace outtasoon odeestuff wackcanes niggaReviewspos.
neg.kidding relljustin whorewin rocksfeel uggfinale nawtotally yeaawh headacheboys whackpls yuckever shawtyyer yeahlord susmike sleepythree hunniagreed sickTable 3: Top fifteen negative and positive words for thealgorithms of Brody and Elhadad (2010) (Reviews) andVelikovich et al (2010) (Web).ment was 67.6%, but rose to 93% when consideringadjacent ratings as equivalent10.
This is compara-ble with the agreement reported by Brody and El-hadad (2010).
We assigned values 1 (strongly nega-tive) to 5 (strongly positive) to the ratings, and cal-culated the average between the two ratings for eachword.
Words with an average rating of 3 were con-sidered neutral, and those with lower and higher rat-ings were considered negative and positive, respec-tively.Results Table 2 shows the distribution of the hu-man labels among the top 50 most positive and mostnegative words as determined by the two algorithms.Table 3 lists the top 15 of these as examples.10Cohen?s Kappa ?
= 0.853568From Table 2 we can see that both algorithms dobetter on positive words (fewer words with reversedpolarity)11, and that the Web algorithm is more accu-rate than the Reviews method.
The difference in per-formance can be explained by the associations usedby the algorithms.
The Web algorithm takes into ac-count the strongest path to every seed word, whilethe Reviews algorithm propagates from the eachseed to its neighbors and then onward.
This makesthe Reviews algorithm sensitive to strong associa-tions between a word and a single seed.
Because ourgraph is constructed with co-occurrence edges be-tween words, rather than syntactic relations betweenadjectives, noisy edges are introduced, causing mis-taken associations.
The Web algorithm, on the otherhand, finds words that have a strong association withthe positive or negative seed group as a whole, thusmaking it more robust.
This explains some of the ex-amples in Table 3.
The words yeah and yea, whichoften follow the negative seed word hell, are consid-ered negative by the Reviews algorithm.
The wordJustin refers to Justin Beiber, and is closely associ-ated with the positive seed word love.
Although theWeb algorithm is more robust to associations witha single seed, it still misclassifies the word holy asnegative, presumably because it appears frequentlybefore several different expletives.Detailed analysis shows that the numbers reportedin Table 2 are only rough estimates of performance.For instance, several of the words in the unknowncategory were correctly identified by the algorithm.Examples include sm(f)h, which stands for ?shak-ing my (fucking) head?
and expresses disgust or dis-dain, sus, which is short for suspicious (as in ?i hatesusssss ass cars that follow me/us when i?m/we walk-inggg?
), and odee, which means overdose and isusually negative (though it does not always refersto drugs, and is sometimes used as an intensifier,e.g., ?aint shit on tv odee bored?
).There were also cases were the human labels wereincorrect in the context of our domain.
For exam-ple, the word bull is listed as positive in the sen-timent lexicon, presumably because of its financialsense.
In our domain it is (usually) short for bull-shit.
The word canes was rated as negative by one of11This trend is not apparent from the top 15 results presentedin Table 3, but becomes noticeable when considering the largergroup.the annotators, but in our data it refers to the MiamiHurricanes, who won a game on the day our datasetwas sampled, and were the subject of many posi-tive tweets.
This example also demonstrates that ourmethod is capable of detecting terms which are asso-ciated with sentiment at different time points, some-thing that is not possible with a fixed lexicon.7 ConclusionIn this paper we explored the phenomenon of length-ening words by repeating a single letter.
We showedthat this is a common phenomenon in Twitter, oc-curring in one of every six tweets, on average, in ourdataset.
Correctly detecting these cases is importantfor comprehensive coverage.
We also demonstratedthat lengthening is not arbitrary, and is often usedwith subjective words, presumably to emphasize thesentiment they convey.
This finding leads us to de-velop an unsupervised method based on lengtheningfor detecting new sentiment bearing words that arenot in the existing lexicon, and discovering their po-larity.
In the rapidly-changing domain of microblog-ging and net-speak, such a method is essential forup-to-date sentiment detection.8 Future WorkThis paper examined one aspect of the lengtheningphenomenon.
There are other aspects of lengthen-ing that merit research, such as the connection be-tween the amount of lengthening and the strength ofemphasis in individual instances of a word.
In addi-tion to sentiment-bearing words, we saw other wordclasses that were commonly lengthened, includingintensifiers (e.g., very, so, odee), and named enti-ties associated with sentiment (e.g., Justin, ?Canes).These present interesting targets for further study.Also, in this work we focused on data in English,and it would be interesting to examine the phe-nomenon in other languages.
Another direction ofresearch is the connection between lengthening andother orthographic conventions associated with sen-timent and emphasis, such as emoticons, punctua-tion, and capitalization.
Finally, we plan to integratelengthening and its related phenomena into an accu-rate, Twitter-specific, sentiment classifier.569AcknowledgementsThe authors would like to thank Paul Kantor andMor Naaman for their support and assistance in thisproject.
We would also like to thank Mark Steedmanfor his help, and the anonymous reviewers for theircomments and suggestions.ReferencesBarbosa, Luciano and Junlan Feng.
2010.
Robustsentiment detection on twitter from biased andnoisy data.
In Chu-Ren Huang and Dan Jurafsky,editors, COLING (Posters).
Chinese InformationProcessing Society of China, pages 36?44.Bermingham, Adam and Alan F. Smeaton.
2010.Classifying sentiment in microblogs: is brevity anadvantage?
In Proceedings of the 19th ACM in-ternational conference on Information and knowl-edge management.
ACM, New York, NY, USA,CIKM ?10, pages 1833?1836.Bolinger, Dwight.
1965.
Forms of English: Accent,Morpheme, Order.
Harvard University Press,Cambridge, Massachusetts, USA.Bollen, J., H. Mao, and X.-J.
Zeng.
2010.
Twittermood predicts the stock market.
ArXiv e-prints .Bollen, Johan, Bruno Gonalves, Guangchen Ruan,and Huina Mao.
2011.
Happiness is assortative inonline social networks.
Artificial Life 0(0):1?15.Brants, Thorsten and Alex Franz.
2006.
Google web1T 5-gram corpus, version 1.
Linguistic DataConsortium, Catalog Number LDC2006T13.Brody, Samuel and Noemie Elhadad.
2010.
An un-supervised aspect-sentiment model for online re-views.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL 2010).
ACL, Los Angeles,CA, pages 804?812.Calhoun, Sasha.
2010.
The centrality of metricalstructure in signaling information structure: Aprobabilistic perspective.
Language 86:1?42.Diakopoulos, Nicholas A. and David A. Shamma.2010.
Characterizing debate performance via ag-gregated twitter sentiment.
In Proceedings ofthe 28th international conference on Human fac-tors in computing systems.
ACM, New York, NY,USA, CHI ?10, pages 1195?1198.Grinter, Rebecca and Margery Eldridge.
2003.Wan2tlk?
: everyday text messaging.
In Proceed-ings of the SIGCHI conference on Human fac-tors in computing systems.
ACM, New York, NY,USA, CHI ?03, pages 441?448.Kivran-Swaine, Funda and Mor Naaman.
2011.Network properties and social sharing of emo-tions in social awareness streams.
In Proceed-ings of the 2011 ACM Conference on Com-puter Supported Cooperative Work (CSCW 2011).Hangzhou, China.McNair, D. M., M. Lorr, and L. F. Droppleman.1971.
Profile of Mood States (POMS).
Educa-tional and Industrial Testing Service.O?Connor, Brendan, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theInternational AAAI Conference on Weblogs andSocial Media.Pak, Alexander and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Proceedings of the Seventh conferenceon International Language Resources and Evalu-ation (LREC?10).
ELRA, Valletta, Malta.Velikovich, Leonid, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The via-bility of web-derived polarity lexicons.
In Hu-man Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics.
ACL,Stroudsburg, PA, USA, HLT ?10, pages 777?785.Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of theconference on Human Language Technology andEmpirical Methods in Natural Language Process-ing.
ACL, Stroudsburg, PA, USA, HLT ?05, pages347?354.Zhu, X. and Z. Ghahramani.
2002.
Learning fromlabeled and unlabeled data with label propagation.Technical report, CMU-CALD-02.570
