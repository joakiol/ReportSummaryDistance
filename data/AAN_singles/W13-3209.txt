Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74?82,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics?Not not bad?
is not ?bad?
: A distributional account of negationKarl Moritz Hermann Edward GrefenstetteUniversity of Oxford Department of Computer ScienceWolfson Building, Parks RoadOxford OX1 3QD, United Kingdomfirstname.lastname@cs.ox.ac.ukPhil BlunsomAbstractWith the increasing empirical success ofdistributional models of compositional se-mantics, it is timely to consider the typesof textual logic that such models are ca-pable of capturing.
In this paper, we ad-dress shortcomings in the ability of cur-rent models to capture logical operationssuch as negation.
As a solution we pro-pose a tripartite formulation for a continu-ous vector space representation of seman-tics and subsequently use this representa-tion to develop a formal compositional no-tion of negation within such models.1 IntroductionDistributional models of semantics characterizethe meanings of words as a function of the wordsthey co-occur with (Firth, 1957).
These models,mathematically instantiated as sets of vectors inhigh dimensional vector spaces, have been appliedto tasks such as thesaurus extraction (Grefenstette,1994; Curran, 2004), word-sense discrimination(Schu?tze, 1998), automated essay marking (Lan-dauer and Dumais, 1997), and so on.During the past few years, research has shiftedfrom using distributional methods for modellingthe semantics of words to using them for mod-elling the semantics of larger linguistic units suchas phrases or entire sentences.
This move fromword to sentence has yielded models applied totasks such as paraphrase detection (Mitchell andLapata, 2008; Mitchell and Lapata, 2010; Grefen-stette and Sadrzadeh, 2011; Blacoe and Lapata,2012), sentiment analysis (Socher et al 2012;Hermann and Blunsom, 2013), and semantic re-lation classification (ibid.).
Most efforts approachthe problem of modelling phrase meaning throughvector composition using linear algebraic vectoroperations (Mitchell and Lapata, 2008; Mitchelland Lapata, 2010; Zanzotto et al 2010), matrixor tensor-based approaches (Baroni and Zampar-elli, 2010; Coecke et al 2010; Grefenstette et al2013; Kartsaklis et al 2012), or through the useof recursive auto-encoding (Socher et al 2011;Hermann and Blunsom, 2013) or neural-networks(Socher et al 2012).
On the non-compositionalfront, Erk and Pado?
(2008) keep word vectors sep-arate, using syntactic information from sentencesto disambiguate words in context; likewise Turney(2012) treats the compositional aspect of phrasesand sentences as a matter of similarity measurecomposition rather than vector composition.These compositional distributional approachesoften portray themselves as attempts to recon-cile the empirical aspects of distributional seman-tics with the structured aspects of formal seman-tics.
However, they in fact only principally co-optthe syntax-sensitivity of formal semantics, whilemostly eschewing the logical aspects.Expressing the effect of logical operations inhigh dimensional distributional semantic modelsis a very different task than in boolean logic.
Forexample, whereas predicates such as ?red?
are seenin predicate calculi as functions mapping elementsof some set Mred to > (and all other domain ele-ments to ?
), in compositional distributional mod-els we give the meaning of ?red?
a vector-likerepresentation, and devise some combination op-eration with noun representations to obtain therepresentation for an adjective-noun pair.
Underthe logical view, negation of a predicate thereforeyields a new truth-function mapping elements ofthe complement of Mred to > (and all other do-main elements to?
), but the effect of negation andother logical operations in distributional models isnot so sharp: we expect the representation for ?notred?
to remain close to other objects of the samedomain of discourse (i.e.
other colours) while be-ing sufficiently different from the representation of?red?
in some manner.
Exactly how textual logic74would best be represented in a continuous vectorspace model remains an open problem.In this paper we propose one possible formu-lation for a continuous vector space based repre-sentation of semantics.
We use this formulationas the basis for providing an account of logicaloperations for distributional models.
In particu-lar, we focus on the case of negation and how itmight work in higher dimensional distributionalmodels.
Our formulation separates domain, valueand functional representation in such a way as toallow negation to be handled naturally.
We ex-plain the linguistic and model-related impacts ofthis mode of representation and discuss how thisapproach could be generalised to other semanticfunctions.In Section 2, we provide an overview of workrelating to that presented in this paper, coveringthe integration of logical elements in distributionalmodels, and the integration of distributional el-ements in logical models.
In Section 3, we in-troduce and argue for a tripartite representationin distributional semantics, and discuss the issuesrelating to providing a linguistically sensible no-tion of negation for such representations.
In Sec-tion 4, we present matrix-vector models similar tothat of Socher et al(2012) as a good candidatefor expressing this tripartite representation.
Weargue for the elimination of non-linearities fromsuch models, and thus show that negation cannotadequately be captured.
In Section 5, we presenta short analysis of the limitation of these matrix-vector models with regard to the task of modellingnon-boolean logical operations, and present an im-proved model bypassing these limitations in Sec-tion 6.
Finally, in Section 7, we conclude by sug-gesting future work which will extend and buildupon the theoretical foundations presented in thispaper.2 Motivation and Related WorkThe various approaches to combining logic withdistributional semantics can broadly be put intothree categories: those approaches which usedistributional models to enhance existing logicaltools; those which seek to replicate logic with themathematical constructs of distributional models;and those which provide new mathematical defini-tions of logical operations within distributional se-mantics.
The work presented in this paper is in thethird category, but in this section we will also pro-vide a brief overview of related work in the othertwo in order to better situate the work this paperwill describe in the literature.Vector-assisted logic The first class of ap-proaches seeks to use distributional models ofword semantics to enhance logic-based models oftextual inference.
The work which best exempli-fies this strand of research is found in the efforts ofGarrette et al(2011) and, more recently, Beltagyet al(2013).
This line of research converts logi-cal representations obtained from syntactic parsesusing Bos?
Boxer (Bos, 2008) into Markov LogicNetworks (Richardson and Domingos, 2006), anduses distributional semantics-based models suchas that of Erk and Pado?
(2008) to deal with issuespolysemy and ambiguity.As this class of approaches deals with improv-ing logic-based models rather than giving a dis-tributional account of logical function words, weview such models as orthogonal to the effort pre-sented in this paper.Logic with vectors The second class of ap-proaches seeks to integrate boolean-like logicaloperations into distributional semantic models us-ing existing mechanisms for representing andcomposing semantic vectors.
Coecke et al(2010)postulate a mathematical framework generalisingthe syntax-semantic passage of Montague Gram-mar (Montague, 1974) to other forms of syntac-tic and semantic representation.
They show thatthe parses yielded by syntactic calculi satisfyingcertain structural constraints can be canonicallymapped to vector combination operations in dis-tributional semantic models.
They illustrate theirframework by demonstrating how the truth-valueof sentences can be obtained from the combina-tion of vector representations of words and multi-linear maps standing for logical predicates and re-lations.
They furthermore give a matrix interpre-tation of negation as a ?swap?
matrix which in-verts the truth-value of vectorial sentence repre-sentations, and show how it can be embedded insentence structure.Recently, Grefenstette (2013) showed that theexamples from this framework could be extendedto model a full quantifier-free predicate logic usingtensors of rank 3 or lower.
In parallel, Socher etal.
(2012) showed that propositional logic can belearned using tensors of rank 2 or lower (i.e.
onlymatrices and vectors) through the use of non-linear75activation functions in recursive neural networks.The work of Coecke et al(2010) and Grefen-stette (2013) limits itself to defining, rather thanlearning, distributional representations of logicaloperators for distributional models that simulatelogic, and makes no pretense to the provision ofoperations which generalise to higher-dimensionaldistributional semantic representations.
As forthe non-linear approach of Socher et al(2012),we will discuss, in Section 4 below, the limita-tions with this model with regard to the task ofmodelling logic for higher dimensional represen-tations.Logic for vectors The third and final class ofapproaches is the one the work presented herebelongs to.
This class includes attempts to de-fine representations for logical operators in highdimensional semantic vector spaces.
Such ap-proaches do not seek to retrieve boolean logic andtruth values, but to define what logical operatorsmean when applied to distributional representa-tions.
The seminal work in this area is found in thework of Widdows and Peters (2003), who definenegation and other logical operators algebraicallyfor high dimensional semantic vectors.
Negation,under this approach, is effectively a binary rela-tion rather than a unary relation: it expresses thesemantics of statements such as ?A NOT B?
ratherthan merely ?NOT B?, and does so by projectingthe vector for A into the orthogonal subspace ofthe vector for B.
This approach to negation is use-ful for vector-based information retrieval models,but does not necessarily capture all the aspects ofnegation we wish to take into consideration, aswill be discussed in Section 3.3 Logic in textIn order to model logical operations over semanticvectors, we propose a tripartite meaning represen-tation, which combines the separate and distincttreatment of domain-related and value-related as-pects of semantic vectors with a domain-drivensyntactic functional representation.
This is a unifi-cation of various recent approaches to the problemof semantic representation in continuous distribu-tional semantic modelling (Socher et al 2012;Turney, 2012; Hermann and Blunsom, 2013).We borrow from Socher et al(2012) and oth-ers (Baroni and Zamparelli, 2010; Coecke et al2010) the idea that the information words refer tois of two sorts: first the semantic content of theword, which can be seen as the sense or referenceto the concept the word stands for, and is typi-cally modelled as a semantic vector; and second,the function the word has, which models the effectthe word has on other words it combines with inphrases and sentences, and is typically modelledas a matrix or higher-order tensor.
We borrowfrom Turney (2012) the idea that the semantic as-pect of a word should not be modelled as a singlevector where everything is equally important, butideally as two or more vectors (or, as we do here,two or more regions of a vector) which stand forthe aspects of a word relating to its domain, andthose relating to its value.We therefore effectively suggest a tripartite rep-resentation of the semantics of words: a word?smeaning is modelled by elements representing itsvalue, domain, and function, respectively.The tripartite representation We argue that thetripartite representation suggested above allows usto explicitly capture several aspects of semantics.Further, while there may be additional distinct as-pects of semantics, we argue that this is a minimalviable representation.First of all, the differentiation between do-main and value is useful for establishing similar-ity within subspaces of meaning.
For instance,the words blue and red share a common domain(colours) while having very different values.
Wehypothesise that making this distinction explicitwill allow for the definition of more sophisticatedand fine-grained semantics of logical operations,as discussed below.
Although we will representdomain and value as two regions of a vector, thereis no reason for these not to be treated as separatevectors at the time of comparison, as done by Tur-ney (2012).Through the third part, the functional repre-sentation, we capture the compositional aspect ofsemantics: the functional representation governshow a term interacts with its environment.
In-spired by the distributional interpretation (Baroniand Zamparelli, 2010; Coecke et al 2010) ofsyntactically-paramatrized semantic compositionfunctions from Montogovian semantics (Mon-tague, 1974), we will also assume the function partof our representation to be parametrized princi-pally by syntax and domain rather than value.
Theintuition behind taking domain into account in ad-dition to syntactic class being that all members ofa domain largely interact with their environment76in the same fashion.Modeling negation The tripartite representationproposed above allows us to define logical opera-tions in more detail than competing approaches.To exemplify this, we focus on the case of nega-tion.We define negation for semantic vectors to bethe absolute complement of a term in its domain.This implies that negation will not affect the do-main of a term but only its value.
Thus, blue andnot blue are assumed to share a common domain.We call this naive form of negation the inversionof a term A, which we idealise as the partial inver-sion Ainv of the region associated with the valueof the word in its vector representation A.??dvv????dv?v????dv??v??
[f] [f] [f]W Winv ?WFigure 1: The semantic representations of a wordW , its inverse W inv and its negation ?W .
Thedomain part of the representation remains un-changed, while the value part will partially be in-verted (inverse), or inverted and scaled (negation)with 0 < ?
< 1.
The (separate) functional repre-sentation also remains unchanged.Additionally, we expect negation to have adiminutive effect.
This diminutive effect is bestexemplified in the case of sentiment: good is morepositive than not bad, even though good and badare antonyms of each other.
By extension not notgood and not not not bad end up somewhere in themiddle?qualitative statements still, but void ofany significant polarity.
To reflect this diminutiveeffect of negation and double negation commonlyfound in language, we define the idealised diminu-tive negation ?A of a semantic vectorA as a scalarinversion over a segment of the value region of itsrepresentation with the scalar ?
: 0 < ?
< 1, asshown in Figure 1.As we defined the functional part of our rep-resentation to be predominately parametrized bysyntax and domain, it will remain constant undernegation and inversion.4 A general matrix-vector modelHaving discussed, above, how the vector compo-nent of a word can be partitioned into domain andvalue, we now turn to the partition between se-mantic content and function.
A good candidate formodelling this partition would be a dual-space rep-resentation similar to that of Socher et al(2012).In this section, we show that this sort of represen-tation is not well adapted to the modelling of nega-tion.Models using dual-space representations havebeen proposed in several recent publications, no-tably in Turney (2012) and Socher et al(2012).We use the class of recursive matrix-vector mod-els as the basis for our investigation; for a detailedintroduction see the MV-RNN model described inSocher et al(2012).We begin by describing composition for a gen-eral dual-space model, and apply this model to thenotion of compositional logic in a tripartite repre-sentation discussed earlier.
We identify the short-comings of the general model and subsequentlydiscuss alternative composition models and mod-ifications that allow us to better capture logic invector space models of meaning.Assume a basic model of compositionality forsuch a tripartite representation as follows.
Eachterm is encoded by a semantic vector v captur-ing its domain and value, as well as a matrix Mcapturing its function.
Thus, composition consistsof two separate operations to learn semantics andfunction of the composed term:vp = fv(va,vb,Ma,Mb) (1)Mp = fM (Ma,Mb)As we defined the functional representation to beparametrized by syntax and domain, its compo-sition function does not require va and vb as in-puts, with all relevant information already beingcontained in Ma,Mb.
In the case of Socher et al(2012) these functions are as follows:Mp =WM[MaMb](2)vp = g(Wv[MavbMbva])(3)where g is a non-linearity.4.1 The question of non-linearitiesWhile the non-linearity g could be equipped withgreater expressive power, such as in the boolean77logic experiment in Socher et al(2012)), the aimof this paper is to place the burden of composition-ality on the atomic representations instead.
Forthis reason we treat g as an identity function, andWM , Wv as simple additive matrices in this inves-tigation, by settingg = I Wv =WM = [I I]where I is an identity matrix.
This simplificationis justified for several reasons.A simple non-linearity such as the commonlyused hyperbolic tangent or sigmoid function willnot add sufficient power to overcome the issuesoutlined in this paper.
Only a highly complex non-linear function would be able to satisfy the require-ments for vector space based logic as discussedabove.
Such a function would defeat the pointhowever, by pushing the ?heavy-lifting?
from themodel structure into a separate function.Furthermore, a non-linearity effectively en-codes a scattergun approach: While it may havethe power to learn a desired behaviour, it similarlyhas a lot of power to learn undesired behavioursand side effects.
From a formal perspective itwould therefore seem more prudent to explicitlyencode desired behaviour in a model?s structurerather than relying on a non-linearity.4.2 NegationWe have outlined our formal requirements fornegation in the previous section.
From these re-quirements we can deduce four equalities, con-cerning the effect of negation and double nega-tion on the semantic representation and functionof a term.
The matrices J?
and J?
(illustrated in??????????1.
.
.
01??0.
.
.???????????
?Figure 2: A partially scaled and inverted identitymatrix J?.
Such a matrix can be used to trans-form a vector storing a domain and value repre-sentation into one containing the same domain buta partially inverted value, such as W and ?W de-scribed in Figure 1.Figure 2) describe a partially scaled and invertedidentity matrix, where 0 < ?, ?
< 1.fv(not, a) = J?va (4)fM (not, a) ?Ma (5)fv(not, fv(not, a)) = J?J?va (6)fM (not, fM (not, a)) ?Ma (7)Based on our assumption about the constant do-main and interaction across negation, we can re-place the approximate equality with a strict equal-ity in Equations 5 and 7.
Further, we assume thatboth Ma 6= I and Ma 6= 0, i.e.
that A has a spe-cific and non-zero functional representation.
Wemake a similar assumption for the semantic repre-sentation va 6= 0.Thus, to satisfy the equalities in Equations 4through 7, we can deduce the values of vnot andMnot as discussed below.Value and Domain in Negation Under the sim-plifications of the model explained earlier, weknow that the following is true:fv(a, b) = g(Wv[MavbMbva])= I([I I][MavbMbva])=Mavb +MbvaI.e.
the domain and value representation of a par-ent is the sum of the two Mv multiplications ofits children.
The matrix Wv could re-weight thisaddition, but that would not affect the rest of thisanalysis.Given the idea that the domain stays constantunder negation and that a part of the value is in-verted and scaled, we further know that these twoequations hold:?a ?
A : fv(not, a) = J?va?a ?
A : fv(not, fv(not, a)) = J?J?vaAssuming that both semantic and functionalrepresentation across all A varies and is non-zero,these equalities imply the following conditions forthe representation of not:Mnot = J?
= J?vnot = 0These two equations suggest that the term not hasno inherent value (vnot = 0), but merely acts as afunction, inverting part of another terms semanticrepresentation (Mnot = J?
).78Functional Representation in Negation Wecan apply the same method to the functional rep-resentation.
Here, we know that:fM (a, b) =WM[MaMb]=[I I][MaMb]=Ma +MbFurther, as defined in our discussion of nega-tion, we require the functional representation toremain unchanged under negation:?a ?
A : fM (not, a) =Ma?a ?
A : fM (not, fM (not, a)) =MaThese requirements combined leave us to con-clude that Mnot = 0.
Combined with the resultfrom the first part of the analysis, this causes acontradiction:Mnot = 0Mnot = J?=?
J?
= 0This demonstrates that the MV-RNN as de-scribed in this paper is not capable of modellingsemantic logic according to the principles we out-lined.
The fact that we would require Mnot = 0further supports the points made earlier about thenon-linearities and setting WM to[I I].
Even aspecific WM and non-linearity would not be ableto ensure that the functional representation staysconstant under negation given a non-zero Mnot.Clearly, any other complex semantic represen-tation would suffer from the same issue here?thefailure of double-negation to revert a representa-tion to its (diminutive) original.5 AnalysisThe issue identified with the MV-RNN style mod-els described above extends to a number of othermodels of vector spaced compositionality.
It canbe viewed as a problem of uninformed composi-tion caused by a composition function that fails toaccount for syntax and thus for scope.Of course, identifying the scope of negation is ahard problem in its own right?see e.g.
the *SEM2012 shared task (Morante and Blanco, 2012).However, at least for simple cases, we can deducescope by considering the parse tree of a sentence:SVPADJPJJblueRBnotVBZisNPNcarDetThisFigure 3: The parse tree for This car is not blue,highlighting the limited scope of the negation.If we consider the parse tree for this car is not blue,it is clear that the scope of the negation expressedincludes the colour but not the car (Figure 3).While the MV-RNN model in Socher et al(2012) incorporates parse trees to guide the orderof its composition steps, it uses a single composi-tion function across all steps.
Thus, the functionalrepresentation of not will to some extent propagateoutside of its scope, leading to a vector capturingsomething that is not blue, but also not quite a car.There are several possibilities for addressingthis issue.
One possibility is to give greater weightto syntax, for instance by parametrizing the com-position functions fv and fM on the parse struc-ture.
This could be achieved by using specificweight matrices Wv and WM for each possibletag.
While the power of this approach is limitedby the complexity of the parse structure, it wouldbe better able to capture effects such as the scopingand propagation of functional representations.Another approach, which we describe in greaterdetail in the next section, pushes the issue ofpropagation onto the word level.
While both ap-proaches could easily be combined, this secondoption is more consistent with our aim of avoid-ing the implicit encoding of logic into fixed modelparameters in favour of the explicit encoding inmodel structure.6 An improved modelAs we outlined in this paper, a key requirementfor a compositional model motivated by formal se-mantics is the ability to propagate functional rep-resentations, but also to not propagate these repre-sentations when doing so is not semantically ap-propriate.
Here, we propose a modification of theMV-RNN class of models that can capture this dis-79tinction without the need to move the compositionlogic into the non-linearity.We add a parameter ?
to the representation ofeach word, controlling the degree to which itsfunctional representation propagates after havingbeen applied in its own composition step.Thus, the composition step of the new modelrequires three equations:Mp =WM[?a?a+?bMa?b?a+?bMb](8)vp = g(Wv[MavbMbva])(9)?p = max(?a, ?b) (10)Going back to the discussion of negation, thismodel has the clear advantage of being able to cap-ture negation in the way we defined it.
As fv(a, b)is unchanged, these two equations still hold:Mnot = J?
= J?vnot = 0However, as fM (a, b) is changed, the secondset of equations changes.
We use Z as the ?-denominator (Z = ?a + ?B) for simplification:fM (a, b) =WM[?aZ Ma?bZ Mb]=[II] [?aZ Ma?bZ Mb]=?aZMa +?bZMbFurther, we still require the functional representa-tion to remain constant under negation:?a ?
A : fM (not, a) =Ma?a ?
A : fM (not, fM (not, a)) =MaThus, we can infer the following two conditionson the new model:?notZMnot ?
0?aZMa ?MaFrom our previous investigation we already knowthat Mnot = J?
6= 0, i.e.
that not has a non-zero functional representation.
While this causeda contradiction for the original MV-RNN model,the design of the improved model can resolve thisissue through the ?-parameter:?not = 0Thus, we can use this modified MV-RNN modelto represent negation according to the principlesoutlined in this paper.
The result ?not = 0 is inaccordance with our intuition about the propaga-tion of functional aspects of a term: We commonlyexpect negation to directly affect the things un-der its scope (not blue) by choosing their semanticcomplement.
However, this behaviour should notpropagate outside of the scope of the negation.
Anot blue car is still very much a car, and when afilm is not good, it is still very much a film.7 Discussion and Further WorkIn this paper, we investigated the capability of con-tinuous vector space models to capture the seman-tics of logical operations in non-boolean cases.Recursive and recurrent vector models of meaninghave enjoyed a considerable amount of success inrecent years, and have been shown to work well ona number of tasks.
However, the complexity andsubsequent power of these models comes at theprice that it can be difficult to establish which as-pect of a model is responsible for what behaviour.This issue was recently highlighted by an inves-tigation into recursive autoencoders for sentimentanalysis (Scheible and Schu?tze, 2013).
Thus, oneof the key challenges in this area of research is thequestion of how to control the power of these mod-els.
This challenge motivated the work in this pa-per.
By removing non-linearities and other param-eters that could disguise model weaknesses, we fo-cused our work on the basic model design.
Whilesuch features enhance model power, they shouldnot be used to compensate for inherently flawedmodel designs.As a prerequisite for our investigation we estab-lished a suitable encoding of textual logic.
Distri-butional representations have been well explainedon the word level, but less clarity exists as to thesemantic content of compositional vectors.
Withthe tripartite meaning representation we proposedone possible approach in that direction, which wesubsequently expanded by discussing how nega-tion should be captured in this representation.Having established a suitable and rigorous sys-tem for encoding meaning in compositional vec-tors, we were thus able to investigate the repre-80sentative power of the MV-RNN model.
We fo-cused this paper on the case of negation, whichhas the advantage that it does not require manyadditional assumptions about the underlying se-mantics.
Our investigation showed that the basicMV-RNN model is incompatible with our notionof negation and thus with any textual logic build-ing on this proposal.Subsequently, we analysed the reasons for thisfailure.
We explained how the issue of nega-tion affects the general class of MV-RNN models.Through the issue of double-negation we furthershowed how this issue is largely independent onthe particular semantic encoding used.
Based onthis analysis we proposed an improved model thatis able to capture such textual logic.In summary, this paper has two key contribu-tions.
First, we developed a tripartite represen-tation for vector space based models of seman-tics, incorporating multiple previous approachesto this topic.
Based on this representation, thesecond contribution of this paper was a modifiedMV-RNN model that can capture effects such asnegation in its inherent structure.In future work, we would like to build on theproposals in this paper, both by extending ourwork on textual logic to include formulations fore.g.
function words, quantifiers, or locative words.Similarly, we plan to experimentally validate theseideas.
Possible tasks for this include sentimentanalysis and relation extraction tasks such as inSocher et al(2012) but also more specific taskssuch as the *SEM shared task on negation scopeand reversal (Morante and Blanco, 2012).AcknowledgementsThe first author is supported by the UK Engineer-ing and Physical Sciences Research Council (EP-SRC).
The second author is supported by EPSRCGrant EP/I03808X/1.ReferencesM.
Baroni and R. Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.
Association for Computational Linguis-tics.I.
Beltagy, C. Chau, G. Boleda, D. Garrette, E. Erk, andR.
Mooney.
2013.
Montague meets markov: Deepsemantics with probabilistic logical form.
June.W.
Blacoe and M. Lapata.
2012.
A comparison ofvector-based representations for semantic composi-tion.
Proceedings of the 2012 Conference on Empir-ical Methods in Natural Language Processing.J.
Bos.
2008.
Wide-coverage semantic analysis withboxer.
In Proceedings of the 2008 Conference onSemantics in Text Processing, pages 277?286.
Asso-ciation for Computational Linguistics.B.
Coecke, M. Sadrzadeh, and S. Clark.
2010.
Math-ematical Foundations for a Compositional Distribu-tional Model of Meaning.
March.J.
R. Curran.
2004.
From distributional to semanticsimilarity.
Ph.D. thesis.K.
Erk and S. Pado?.
2008.
A structured vector spacemodel for word meaning in context.
Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing - EMNLP ?08, (October):897.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in linguistic analysis.D.
Garrette, K. Erk, and R. Mooney.
2011.
Integratinglogical representations with probabilistic informa-tion using markov logic.
In Proceedings of the NinthInternational Conference on Computational Seman-tics, pages 105?114.
Association for ComputationalLinguistics.E.
Grefenstette and M. Sadrzadeh.
2011.
Experi-mental support for a categorical compositional dis-tributional model of meaning.
In Proceedings ofEMNLP, pages 1394?1404.E.
Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, andM.
Baroni.
2013.
Multi-step regression learningfor compositional distributional semantics.
In Pro-ceedings of the Tenth International Conference onComputational Semantics.
Association for Compu-tational Linguistics.E.
Grefenstette.
2013.
Towards a formal distributionalsemantics: Simulating logical calculi with tensors.Proceedings of the Second Joint Conference on Lex-ical and Computational Semantics.G.
Grefenstette.
1994.
Explorations in automatic the-saurus discovery.K.
M. Hermann and P. Blunsom.
2013.
The role ofsyntax in vector space models of compositional se-mantics.
In Proceedings of ACL, Sofia, Bulgaria,August.
Association for Computational Linguistics.D.
Kartsaklis, M. Sadrzadeh, and S. Pulman.
2012.
Aunified sentence space for categorical distributional-compositional semantics: Theory and experiments.In Proceedings of 24th International Conferenceon Computational Linguistics (COLING 2012):Posters, pages 549?558, Mumbai, India, December.81T.
K. Landauer and S. T. Dumais.
1997.
A solution toPlato?s problem: The latent semantic analysis the-ory of acquisition, induction, and representation ofknowledge.
Psychological review.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL,volume 8.J.
Mitchell and M. Lapata.
2010.
Composition in Dis-tributional Models of Semantics.
Cognitive Science.R.
Montague.
1974.
English as a Formal Language.Formal Semantics: The Essential Readings.R.
Morante and E. Blanco.
2012.
*SEM 2012 sharedtask: resolving the scope and focus of negation.
InProceedings of the First Joint Conference on Lexi-cal and Computational Semantics - Volume 1: Pro-ceedings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation, SemEval?12, pages 265?274, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine learning, 62(1-2):107?136.C.
Scheible and H. Schu?tze.
2013.
Cutting recursiveautoencoder trees.
In Proceedings of the Interna-tional Conference on Learning Representations.H.
Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational linguistics, 24(1):97?123.R.
Socher, E. H. Huang, J. Pennington, A. Y. Ng, andC.
D. Manning.
2011.
Dynamic pooling and un-folding recursive autoencoders for paraphrase detec-tion.
Advances in Neural Information ProcessingSystems, 24:801?809.R.
Socher, B. Huval, C. D. Manning, and A. Y. Ng.2012.
Semantic compositionality through recursivematrix-vector spaces.
Proceedings of the 2012 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1201?1211.P.
D. Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.D.
Widdows and S. Peters.
2003.
Word vectors andquantum logic: Experiments with negation and dis-junction.
Mathematics of language, 8(141-154).F.
M. Zanzotto, I. Korkontzelos, F. Fallucchi, andS.
Manandhar.
2010.
Estimating linear models forcompositional distributional semantics.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 1263?1271.
Associa-tion for Computational Linguistics.82
