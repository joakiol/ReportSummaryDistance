Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 89?93,Gothenburg, Sweden, 26-27 April 2014. c?2014 Association for Computational LinguisticsDetecting change and emergence for multiword expressionsMartin EmmsDepartment of Computer ScienceTrinity College, DublinIrelandMartin.Emms@tcd.ieArun JayapalDepartment of Computer ScienceTrinity College, DublinIrelandjayapala@tcd.ieAbstractThis work looks at a temporal aspect ofmultiword expressions (MWEs), namelythat the behaviour of a given n-gram andits status as a MWE change over time.
Wepropose a model in which context wordshave particular probabilities given a us-age choice for an n-gram, and those us-age choices have time dependent probabil-ities, and we put forward an expectation-maximisation technique for estimating theparameters from data with no annotationof usage choice.
For a range of MWEusages of recent coinage, we evaluatewhether the technique is able to detect theemerging usage.1 IntroductionWhen an n-gram is designated a ?multiword ex-pression?, or MWE, its because it possesses prop-erties which are not straightforwardly predictablegiven the component words of the n-gram ?
thatred tape can refer to bureaucratic regulation wouldbe a simple example.
A further aspect is that whilesome tokens of the n-gram type may be examplesof the irregular MWE usage, others may not be ?so red tape can certainly also be used in a fash-ion which is transparent relative to its parts.
Afurther aspect is temporal: that tokens of the n-gram can be sought in language samples from dif-ferent times.
It seems reasonable to assume thatthe irregular MWE usage of red tape at some timeemerged, and was predated by the more transpar-ent usage.
This paper concerns the possibility offinding automatic, unsupervised means to detectthe emergence of a MWE usage of a given n-gram.To illustrate further, consider the following ex-amples (these are all taken from the data set onwhich we worked)(a) the wind lifted his three-car garage andsmashed it to the ground.
(1995)(a?)
sensational group CEO, totally smashedit in the BGT (Britain Got Talent) (2013)(b) my schedule gave me time to get ad-justed (1990)(b?)
it?s important to set time out and enjoysome me time (2013)(1)(a) and (a?)
feature the n-gram smashed it.
(a)uses the standard destructive sense of smashed,and it refers to an object undergoing the destruc-tive transformation.
In (a?)
the n-gram is used dif-ferently and is roughly replaceable by ?excelled?,a usage not via the standard sense of smashed, norone where it refers to any object at all.
Where inboth (a) and (a?)
the n-gram would be regarded asa phrase, (b) and (b?)
involving the n-gram me timeshow another possibility.
In (b), me and time arestraightforward dependants of gave.
In (b?
), thetwo words form a noun-phrase, meaning some-thing like ?personal time?.
The usage is arguablymore acceptable than would be the case with otherobject pronouns, and if addressed to a particularperson, the me would refer to the addressee, whichis not the usual function of a first-person pronoun.For smashed it and me time, the second (primed)example illustrates an irregular usage-variant ofthe n-gram, whilst the first illustrates a regularusage-variant, and the irregular example is drawnfrom a later time than the regular usage.
Lan-guage is a dynamic phenomenon, with the rangeof ways a given n-gram might contribute subjectto change over time, and for these n-grams, itwould seem to be the case that the availabilityof the ?me time?
= ?personal time?
and ?smashedit = ?excelled?
usage-variants is a relatively re-cent innovation1 , predated by the regular usage-variants.
It seems that in work on multiword ex-1That is to say, recent in British English according to the89pressions, there has been little attention paid tothis dynamic aspect, whereby a particular multi-word usage starts to play a role in a language at aparticular point in time.
Building on earlier work(Emms, 2013), we present some work concerningunsupervised means to detect this.
Section 2 de-scribes our data, section 3 our EM-based methodand section 4 discusses the results obtained.2 DataTo investigate such emergence phenomena somekind of time-stamped corpus is required.
The ap-proach we took to this was to exploit a search fa-cility that Google has offered for some time ?
cus-tom date range ?
whereby it is possible to specifya time period for text matching the searched item.To obtain data for a given n-gram, we repeatedlyset different year-long time spans and saved thefirst 100 returned ?hits?
as potential examples ofthe n-gram?s use.
Each ?hit?
has a text snippet andan anchor text for a link to the online source fromwhich the snippet comes.
If the text snippet or an-chor string contains the n-gram it can furnish anexample of its use, and the longer of the two istaken if both feature the n-gram.A number of n-grams were chosen having theproperties that they have an irregular, MWE usagealongside a regular one, with the MWE usage a re-cent innovation.
These were smashed it, me time(illustrated in (1)) and going forward, and biolog-ical clock, illustrated below.
(c) Going forward from the entrance,you?ll come to a large room.
(1995)(c?)
Going forward BJP should engage inpeople?s movements (2009)(d) A biological clock present in most eu-karyotes imposes daily rhythms (1995)(d?)
How To Stop Worrying About Your Bi-ological Clock .
.
.
Pressure to have ababy before 35 (2009)(2)Alongside the plain movement usage-variant seenin (c), going forward has the more opaque usage-variant in which it is roughly replaceable by ?inthe future?, seen in (c?).
Alongside a technical usein biology seen in (d), biological clock has cometo be used in a wider context to refer to a sense ofexpiring time within which people may be able tohave a child, seen in (d?
).first author?s intuitions.
It is not easy to find sources to cor-roborate such intuitionsFor each n-gram data was downloaded for suc-cessive year-long time-spans from 1990 to 2013,retaining the first 100 hits for each year.
For someof the earlier years there are less than 100 hits, butmostly there are more than 100.
This gives on theorder of 2000 examples for each n-gram, each witha date stamp, but otherwise with no other annota-tion.
See Section 4 for some discussion of thismethod of obtaining data.3 AlgorithmFor an n-gram with usage variants (as illustratedby (1) and (2)), we take the Bayesian approachthat each variant gives different probabilities tothe words in its immediate vicinity, as has beendone in unsupervised word-sense disambiguation(Manning and Schu?tze, 2003; de Marneffe andDupont, 2004).
In those approaches, which ignoreany temporal dimension, it is also assumed thatthere are prior probabilities on the usage-variants.We bring in language change by having a succes-sion of priors, one for each time period.To make this more precise, where T is an oc-currence of a particular n-gram, with W the se-quence of words around T , let Y represent itstime-stamp.
If we suppose there are k differentusage-variants of the n-gram, we simply modelthis with a discrete variable S which can take on kvalues.
So S can be thought of as ranging over po-sitions in an enumeration of the different ways thatthe n-gram can contribute to the semantics.
Withthese variables we can say that we are consider-ing a probability model for p(Y, S,W ).
Apply-ing the chain-rule this may be re-expresssed with-out loss of generality as p(Y )p(S|Y )p(W |S, Y ).We then make some assumptions: (i) that Wis conditionally independent of Y given S, sop(W |S, Y ) = p(W |S), (ii) that p(W |S) may betreated as?i(p(Wi|S) , and (iii) that p(Y ) is uni-form.
This then givesp(Y, S,W ) = p(Y )p(S|Y )?i(p(Wi|S) (3)The term p(S|Y ) directly models the fact that ausage variant can vary its likelihood over time,possibly having zero probability on some earlyrange of times.
While (i) make context wordsand times indepedent given a usage variant, con-text words are still time-dependent: the sum?S[p(S|Y )p(W |S)] varies with time Y due to90p(S|Y ).
Assumption (i) reflects a plausibile ideathat given a concept being conveyed, the expectedaccompanying vocabulary is substantially time-independent.
Moreover (i) drastically reduces thenumber of parameters to be estimated: with 20time spans and a 2-way usage choice, the wordprobabilities are conditioned on 2 settings ratherthan 40.The parameters of the model in (3) have to beestimated from data which is labelled only fortime ?
the usage-variant variable is a hidden vari-able ?
and we tackle this with an EM procedure(Dempster et al., 1977).
Space precludes givingthe derivations of the update formulae but in out-line there is an iteration of an E and an M step, asfollows:(E step) based on current parameters, a table, ?,is populated, such that for each data point d, andpossible S value s, ?
[d][s] stores P (S = s|Y =yd,W = wd).
(M step) based on ?, fresh parameter values arere-estimated according to:P (S = s|Y = y) =?d(if Y d=y then ?
[d][s] else 0)?d(if Y d=y then 1 else 0)P (w|S = s) =?d(?
[d][s]?freq(w?W d))?d(?
[d][s]?length(W d))These updates can be shown to increase thedata probability, where the usage variable S issummed-out.4 Results and DiscussionRunning the above-outlined EM procedure on thedownloaded data for a particular n-gram gener-ates unsupervised estimates for p(S|Y ) ?
inferredusage distributions for each time span.
To ob-tain a reference with which to compare these in-ferred distributions, approximately 10% of thedata per time-span was manually annotated andused to give simple relative-frequency estimates ofp(S|Y ) ?
which we will call empirical estimates.Although the data was downloaded for year-longtime spans, it was decided to group the data intosuccessive spans of 3 year duration.
This was tomake the empirical p(S|Y ) less brittle as they areotherwise based on too small a quantity of data.Figure 1 shows the outcomes, as usage-variantprobabilities in a succession of time spans, boththe empirical estimates obtained on a subset, andthe unsupervised estimates obtained on all thedata.
The EM method can seek any numberof usage variants, and the results show the casewhere 2 variants were sought.
Where the man-ually annotated subset used more variants thesewere grouped to facilitate a comparison.For smashed it, biological clock and going for-ward, the ?
line in the empirical plot is for theMWE usage, and for me time it is the ?
line, andit has an upward trend.
In the unsupervised case,there is inevitable indeterminacy about which Svalues may come to be associated with any objec-tively real usage.
Modulo this the unsupervisedand supervised graphs broadly concur.One can also inspect the context-words whichcome to have high probability in one semanticvariant relative to their probability in another.
Forexample, for smashed it, for the semantic usagewhich is inferred to have an increasing proba-bility in recent years, a selection from the mostfavoured tokens includes !
!, guys, really, com-pletely, They, !, whilst for the other usage they in-clude smithereens, bits, bottle, onto, phone.
Forbiological clock, a similar exercise gives for theapparently increasing usage, tokens such as Ticks,Ticking?, Health, Fertility and for the other usagerunning, 24-hour, controlled, mammalian, mecha-nisms.
These associations would seem to be con-sistent with the inferred semantic-usages being inbroad correspondence with the annotated usages.As noted in section 2, as a means to obtaindata on relatively recent n-gram usages, we usedthe custom date range search facility of Google.One of the issues with such data is the potentialfor the time-stamping (inferred by Google) to beinnaccurate.
Though its not possible to exhaus-tively verify the time-stamping, some inspectionwas done, which revealed that although there aresome cases of documents which were incorrectlystamped, this was tolerably infrequent.
Then thereis the question of the representativeness of thesample obtained.
The mechanism we used givesthe first 100 from the at most 1000 ?hits?
whichGoogle will return from amongst all index docu-ments which match the n-gram and the date range,so an uncontrollable factor is the ranking mech-anism according to which these hits are selectedand ordered.
The fact that the empirical usagedistributions accord reasonably well with prior in-tuition is a modest indicator that the data is notunusably unrepresentative.
One could also arguethat for an initial test of the algorithms it suf-fices for the methods to recover an apparent trend91smashed it (empirical and unsupervised)2000 2005 20100.00.20.40.60.81.0 smashed itYearSenseProp2000 2004 2008 20120.00.20.40.60.81.0YearSensePropbiological clock (empirical and unsupervised)1995 20050.00.40.8?YearSenseProp1995 20050.00.40.8bio clockYearSensePropme time (empirical and unsupervised)1995 20050.00.40.8me timeYearSenseProp1995 20050.00.40.8me timeYearSensePropgoing forward (empirical and unsupervised)1995 20050.00.40.8going forwardYearSenseProp1995 20050.00.40.8going forwardYearSensePropFigure 1: For each n-gram the plots show the empirical usage-variant distributions per time-period in thelabelled subset and unsupervised usage-varaint distributions per time-period in the entire data setin the downloaded data, even if the data is un-representative.
This being said, one direction forfurther work will be to consider other sources oftime-stamped language use, such as the Google n-grams corpus (Brants and Franz, 2012), or variousnewswire corpora (Graff et al., 2007).There does not seem to have been that muchwork on unsupervised means to identify emer-gence of new usage of a given expression ?
thereis more work which groups all tokens of a typetogether and uses change of context words to indi-cate an evolving single meaning (Sagi et al., 2008;Gulordava and Baroni, 2011).
Lau et al.
(2012)though they do not address MWEs do look at theemergence of new word senses, applying a word-sense induction technique.
Their testing was be-tween two corpora taken to represent two differenttime periods, the BNC and ukWac corpus, takento represent the late 20th century and 2007, re-spectively, and they reported promising results on5 words.
The unsupervised method they used isbased on a Hierarchical Dirichlet Process model(Yao and Van Durme, 2011), and a direction forfuture work will be a closer comparison of the al-gorithm presented here to that algorithm and otherrelated LDA-based methods in word sense induc-tion (Brody and Lapata, 2009).
Also the bag-of-tokens model of the context words which weadopted is a very simple one, and we wish to con-sider more sophisticated models involving for ex-ample part-of-tagging or syntactic structures.The results are indicative at least that MWEusage of an n-gram can be detected by unsuper-vised means to be preceded by the other usages ofthe n-gram.
There has been some work on algo-rithms which seek to quantify the degree of com-positionality of particular n-grams (Maldonado-Guerra and Emms, 2011; Biemann and Gies-brecht, 2011) and it is hoped in future work toconsider the possible integration of some of thesetechniques with those reported here.
For a givenn-gram, it would be interesting to know if the col-lection of its occurrences which the techniques ofthe current paper suggest to belong to a more re-cently emerging usage, are also a corpus of occur-rences relative to which a compositionality mea-sure would report the n-gram as being of low com-positionality, and conversely for the apparentlyless recent usage.AcknowledgementsThis research is supported by the Science Foun-dation Ireland (Grant 12/CE/I2267) as part ofthe Centre for Next Generation Localisation(www.cngl.ie) at Trinity College Dublin.92ReferencesChris Biemann and Eugenie Giesbrecht, editors.
2011.Proceedings of the Workshop on Distributional Se-mantics and Compositionality.Thorsten Brants and Alex Franz.
2012.
Google booksn-grams.
ngrams.googlelabs.com.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In EACL 09: Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,pages 103?111, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Marie-Catharine de Marneffe and Pierre Dupont.
2004.Comparative study of statistical word sense discrim-ination.
In Ge?rald Purnelle, Ce?dric Fairon, andAnne Dister, editors, Proceedings of JADT 2004 7thInternational Conference on the Statistical Analysisof Textual Data, pages 270?281.
UCL Presses Uni-versitaire de Louvain.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
J. Royal Statistical Society, B 39:1?38.Martin Emms.
2013.
Dynamic EM in neologism evo-lution.
In Hujun Yin, Ke Tang, Yang Gao, FrankKlawonn, Minho Lee, Thomas Weise, Bin Li, andXin Yao, editors, Proceedings of IDEAL 2013, vol-ume 8206 of Lecture Notes in Computer Science,pages 286?293.
Springer.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2007.
English gigaword corpus.
Linguis-tic Data Consortium.Kristina Gulordava and Marco Baroni.
2011.
A distri-butional similarity approach to the detection of se-mantic change in the google books ngram corpus.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,pages 67?71, Edinburgh, UK, July.
Association forComputational Linguistics.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense in-duction for novel sense detection.
In Proceedings ofthe 13th Conference of the European Chapter of theAssociation for Computational Linguistics, EACL?12, pages 591?601, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Alfredo Maldonado-Guerra and Martin Emms.
2011.Measuring the compositionality of collocations viaword co-occurrence vectors: Shared task system de-scription.
In Proceedings of the Workshop on Dis-tributional Semantics and Compositionality, pages48?53, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Christopher Manning and Hinrich Schu?tze, 2003.Foundations of Statistical Language Processing,chapter Word Sense Disambiguation, pages 229?264.
MIT Press, 6 edition.Eyal Sagi, Stefan Kaufmann, and Brady Clark.
2008.Tracing semantic change with latent semantic anal-ysis.
In Proceedings of ICEHL 2008.Xuchen Yao and Benjamin Van Durme.
2011.
Non-parametric bayesian word sense induction.
In Pro-ceedings of TextGraphs-6: Graph-based Methodsfor Natural Language Processing, pages 10?14.
As-sociation for Computational Linguistics.93
