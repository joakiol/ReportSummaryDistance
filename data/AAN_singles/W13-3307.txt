Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 51?59,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsTranslation of ?It?
in a Deep Syntax FrameworkMichal Nova?k, Anna Nedoluzhko and Zdene?k ?Zabokrtsky?Charles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?me?st??
25, CZ-11800{mnovak,nedoluzko,zabokrtsky}@ufal.mff.cuni.czAbstractWe present a novel approach to the trans-lation of the English personal pronoun itto Czech.
We conduct a linguistic analysison how the distinct categories of it are usu-ally mapped to their Czech counterparts.Armed with these observations, we designa discriminative translation model of it,which is then integrated into the TectoMTdeep syntax MT framework.
Features inthe model take advantage of rich syntac-tic annotation TectoMT is based on, exter-nal tools for anaphoricity resolution, lex-ical co-occurrence frequencies measuredon a large parallel corpus and gold coref-erence annotation.
Even though the newmodel for it exhibits no improvement interms of BLEU, manual evaluation showsthat it outperforms the original solution in8.5% sentences containing it.1 IntroductionAfter it has long been neglected, retaining cohe-sion of a text larger than a single sentence in Ma-chine Translation (MT) has recently become a dis-cussed topic.
Correct translation of referential ex-pressions is in many cases essential for humans tograsp the meaning of a translated text.Especially, the translation of pronouns attractsa higher rate of interest.
In the previous worksof Le Nagard and Koehn (2010), Hardmeier andFederico (2010) and Guillou (2012), it has beenshown that current MT systems perform poorly inproducing the correct forms of pronouns.
As re-gards English, the personal pronoun it is the mostcomplicated case.
Not only can it corefer with al-most any noun phrase (making it hard to pick thecorrect gender and number if the target language ismorphologically rich), but it can also corefer witha larger discourse segment or play the role of afiller in certain grammatical constructions.In this work, we turn our attention to the transla-tion of the English personal pronoun it into Czech.Even if we ignore morphology and merge all re-lated surface forms into one, we cannot find asingle Czech expression that would comprise allfunctions of the English it.
Moreover, there is nosimple one-to-one mapping from categories of itto Czech expressions.
For instance, one would ex-pect that the translation of it which is coreferen-tial with a noun phrase has to agree in number andgender with the translation of its antecedent.
How-ever, there are cases when it is more suitable totranslate it as the demonstrative pronoun to, whosegender is always neuter.The aim of this work is to build an English-to-Czech translation model for the personal pronounit within the TectoMT framework ( ?Zabokrtsky?
etal., 2008).
TectoMT is a tree-to-tree translationsystem with transfer via tectogrammatical layer,a deep syntactic layer which follows the Praguetectogrammatics theory (Sgall, 1967; Sgall et al1986) Therefore, its translation model outputs thedeep syntactic representation of a Czech expres-sion.
Selecting the correct grammatical categoriesand thus producing a concrete surface form of adeep syntactic representation is provided by thetranslation synthesis stage, which we do not focuson in this work.The mapping between it and correspondingCzech expressions depends on many aspects.
Weaddress them by introducing features based onsyntactic annotation and anaphoricity resolver out-put.
Furthermore, we make use of lexical co-occurrence counts aggregated on a large auto-matically annotated Czech-English parallel corpusCzEng 1.0 (Bojar et al 2012).
Coreference linksalso appear to be a source of valuable features.1In contrast to the related work, we prefer a dis-criminative model to a commonly used generative1However, we excluded them from the final model usedin MT as they originate from gold standard annotation.51model.
The former allows us to feed it with manysyntactic and lexical features that may affect theoutput, which would hardly be possible in the lat-ter.2 Related WorkOur work addresses a similar issue that has beenexplored by Le Nagard and Koehn (2010), Hard-meier and Federico (2010) and Guillou (2012).These works attempted to incorporate informa-tion on coreference relations into MT, aiming toimprove the translation of English pronouns intomorphologically richer languages.
The poor re-sults in the first two works were mainly due to im-perfect automatic coreference annotation.The work of Guillou (2012) is of special interestto this work because it is also focused on Englishto Czech translation and makes an extensive useof the Prague Czech-English Dependency Tree-bank 2.0 (PCEDT).
Instead of automatic corefer-ence links, they employed gold annotation, reveal-ing further reasons of small improvements ?
thenumber of occurrences in the tranining data weak-ened by including grammatical number and gen-der in the annotation and availability of only a sin-gle reference translation.The first issue is a consequence of the assump-tion that a Czech pronoun must agree in gen-der and number with its antecedent.
There arecases, though, when demonstrative pronoun to fitsbetter and grammatical categories are not propa-gated.
Keeping grammatical information on itsantecedent may in this case result in probably notharmful but still superfluous partitioning the train-ing data.Our work deals also with the second issue, how-ever, at the cost of partial manual annotating.The most significant difference of our workcompared to the abovementioned ones lies in theMT systems used.
Whereas they tackle the issueof pronoun translation within the Moses phrase-based system (Koehn et al 2003), we rely on thetranslation via deep syntax with TectoMT system( ?Zabokrtsky?
et al 2008).
Our approach is morelinguistically oriented, working with deep syntac-tic representations and postponing the decisionsabout the concrete forms to the synthesis stage.3 Linguistic AnalysisIn English, three main coarse-grained types ofit are traditionally distinguished.
Referential itpoints to a noun phrase in the preceding or the fol-lowing context:(1) Peter has finished writing an article andshowed it to his supervisor.Anaphoric it refers to a verbal phrase or larger dis-course segments (so-called discourse deixis).
(2) Peter has discussed the issue with his su-pervisor and it helped him to finish the ar-ticle.Pleonastic it has no antecedent in the preced-ing/following context and its presence is imposedonly by the syntactic rules of English.
(3) It is difficult to give a good example.From the perspective of Czech, there are alsothree prevailing types of how it can be translated.The most frequent are personal pronouns or zeroforms.2 In Prague tectogrammatics theory zeroanaphors are reconstructed on the tectogrammat-ical layer.
Same as expressed personal pronouns,they are represented by a node with the #PersPronsymbol, e.g.
(4) Bushova vla?da ozna?mila, z?e se svu?j pla?n#PersPron pokus??
vzkr??
?sit.The Bush administration has said it willtry to resurrect its plan.The second typical possibility is the Czech demon-strative pronoun to (= it, this), which is a form ofa pronoun ten in its neuter singular form, e.g.
(5) Analytik r?ekl, z?e to byla tato moz?nostpoz?adavku, ktera?
pevne?js??
?m cena?m po-mohla.The analyst said that it was the possibilityof this demand that helped firm prices.In many cases, it has no lexical counterpart inthe Czech translation, the English and Czech sen-tences thus having a different syntactic structure.These are cases like, for instance:(6) Obchodn?
?ci uvedli, z?e je obt??z?ne?
nove?emise REMIC strukturovat, kdyz?
se cenytolik me?n?
?.Dealers noted that it?s difficult to struc-ture new Remics when prices are movingwidely.2Czech is a pro-drop language.52Figure 1: The mapping of the types of English itto Czech translations.There are also some other possibilities of how itcan be translated into Czech, such as the repeti-tion of the antecedent noun, different genders ofthe demonstrative ten (=it, this) in the anaphoricposition, using synonyms and hyperomyns.
How-ever, these cases are not so frequent and they rarelycannot be converted to one of the three broadercategories.The correspondence between the course-grained types of English it and its possible Czechtranslations is not one-to-one.
As seen fromFigure 1, a personal pronoun/zero anaphoratranslates to the referential it (see example 4) andno lexical counterpart is used when translating thepleonastic it (see example 6).However, all types of it can be translated as aneuter demonstrative to.
The typical case ?it refer-ring to VPs/larger discourse segments = to?
wasdemonstrated in (5).The mapping ?referential it = to?
is common forcases where the referent is attributed some furthercharacteristics, mostly in constructions with a verbto be like ?It is something.
?, such as (7).3 Thisis an interesting case for Czech, because a gen-der and number agreement between the antecedentand the anaphoric to is generally absent.
(7) Some investors say Friday?s sell-off was agood thing.
?It was a healthy cleansing,?says Michael Holland.Ne?kter???
investor?i r???kaj?
?, z?e pa?tec?n?
?vy?prodej byla dobra?
ve?c.
?Byla to zdrava?oc?ista,?
r???ka?
Michael Holland.The ?cleft sentences?
(see example 8) and someother syntactic constructions are the case whenpleonastic it is translated into Czech with thedemonstrative to.3We suspect that it holds also for he/she/they but such aclaim is not yet empirically supported.
For the sake of sim-plicity, we conduct our research only for it.
(8) But it is Mr. Lane, as movie director, whohas been obsessed with refitting Chaplin?sLittle Tramp in a contemporary way.Ale je to Lane jako filmovy?
rez?ise?r, kdo jeposedly?
t?
?m, z?e zmodernizuje Chaplinu?vfilm ?Little Tramp (Maly?
tula?k)?.In some cases, both translations of pleonastic itare possible: neuter demonstrative to or a differentsyntactic construction with no lexical counterpartof it.
Compare the examples from PCEDT whereit with similar syntactic function was translated bychanging the syntactic structure in (9) and using aneuter to in (10):(9) ?It was great to have the luxury of time,?Mr.
Rawls said.
?Bylo skve?le?, z?e jsme me?li dostatek c?asu,?r?ekl Rawls.
(10) ?On days that I?m really busy,?
says Ms.Foster, ?it seems decadent to take time offfor a massage.?
?Ve dnech, kdy ma?m opravdu mnohopra?ce,?
r???ka?
pan??
Fosterova?, ?to vypada?zvrhle, kdyz?
si vyhrad?
?m c?as na masa?z?.
?4 Translation via Deep SyntaxFollowing a phrase-based statistical MT approach,it may be demanding to tackle issues that arisewhen translating between typologically differentlanguages.
Translation from English to Czech is atypical example.
One has to deal with a rich mor-phology, less constrained word order, changes inclauses bindings, pro-drops etc.In this work, we make use of the English toCzech translation implemented within the Tec-toMT system, first introduced by ?Zabokrtsky?
et al(2008).
In contrast to the phrase-based approach,TectoMT performs a tree-to-tree machine transla-tion.
Given an input English sentence, the trans-lation process is divided into three stages: analy-sis, transfer and synthesis.
TectoMT at first con-ducts an automatic analysis including POS tag-ging, named entity recognition, syntactic parsing,semantic role labeling, coreference resolution etc.This results in a deep syntactic representation ofthe English sentence, which is subsequently trans-ferred into Czech, with the translation of lexicaland grammatical information being provided viaseveral factors.
The process proceeds with a rule-53based synthesis stage, when a surface Czech sen-tence is generated from its deep syntactic struc-ture.Deep syntactic representation of a sentence fol-lows the Prague tectogrammatics theory (Sgall,1967; Sgall et al 1986).
It is a dependencytree whose nodes correspond to the content wordsin the sentence.
Personal pronouns missing onthe surface are reconstructed in special nodes.Nodes are assigned semantic roles (called func-tors) and grammatical information is comprised inso called grammatemes.
Furthermore, tectogram-matical representation is a place where corefer-ence relations are annotated.4.1 Model of it within TectoMTThe transfer stage, which maps an English tec-togrammatical tree to a Czech one, is a placewhere the translation model of it is applied.
Forevery English node corresponding to it, a featurevector is extracted and fed into a discriminative re-solver that assigns one of the three classes to it ?PersPron, To and Null, corresponding to themain Czech types introduced in Section 3.If labeled as PersPron, the English nodeis mapped to a Czech #PersPron node and theEnglish coreference link is projected.
Duringthe synthesis, it is decided whether the pronounshould be expressed on a surface, its gender andnumber are copied from the antecedent?s head andfinally the correct form (if any) is generated.Obtaining class To makes things easier.
TheEnglish node is only mapped to a Czech node con-taining the pronoun ten with its gender and num-ber set to neuter singular, so that later the correctform to will be generated.Last, if it is assigned Null, no correspondingnode on the Czech side is generated, but the Czechcounterpart of the governing verb is forced to be inneuter singular.5 Prague Czech-English DependencyTreebank as a source of dataThe Prague Czech-English Dependency Treebank(Hajic?
et al 2011, PCEDT) is a manually parsedCzech-English parallel corpus comprising over 1.2million words for each language in almost 50,000sentence pairs.
The English part contains the en-tire Penn Treebank?Wall Street Journal Section(Linguistic Data Consortium, 1999).
The Czechpart consists of translations of all the texts fromthe English part.
The data from both parts areannotated on three layers following the theory ofPrague tectogrammatics ?
the morphological layer(where each token from the sentence gets a lemmaand a POS tag), the analytical layer (surface syn-tax in the form of a dependency tree, where eachnode corresponds to a token in the sentence) andthe tectogrammatical representation (see Section4).Sentences of PCEDT have been automaticallymorphologically annotated and parsed into ana-lytical dependency trees.4 The tectogrammaticaltrees in both language parts have been annotatedmanually (Hajic?
et al 2012).
The nodes of Czechand English trees have been automatically alignedon analytical as well as tectogrammatical layer(Marec?ek et al 2008).5.1 Extraction of ClassesThe shortcomings of the automatic alignmentis particularly harmful for pronouns and zeroanaphors, which can replace a whole range of con-tent words and their meaning is inferred mainlyfrom the context.
The situation is better for verbsas their usual parents in dependency trees: sincethey carry meaning in a greater extent, their auto-matic alignment is of a higher quality.Thus, we did not search for a Czech counterpartof it by following the alignment of it itself.
Usingthe fact that the verb alignment is more reliableand functors in tectogrammatical trees have beenmanually corrected, we followed the alignment ofthe parent of it (a verb) and selected the Czech sub-tree with the same tectogrammatical functor as ithad on the English side.
If the obtained subtreeis a single node of type #PersPron or ten, we as-signed class PersPron or To, respectively, to thecorresponding it.
This approach relies also on theassumption that semantic roles do not change inthe translation.The automatic acquisition of classes coveredmore than 60% of instances, the rest had to be la-beled manually.
During the annotation, we obeyedthe following rules:1.
If a demonstrative pronoun to is present in theCzech sentence or if a personal pronoun iseither present or unexpressed, assign the in-stance to the corresponding class.4The English dependency trees were built by automati-cally transforming the original phrase-structure annotation ofthe Penn Treebank.542.
Otherwise, ignore the Czech translation pro-vided in the corpus and follow the most sim-plistic possible translation which would stillbe correct.
Assign the instance to the classwhich fits it the best.Note that it may happen that none of the threeoptions fits, because it is either an idiomatic ex-pression or larger structural modifications are re-quired.
Such cases are very rare and we left themout of the data.The manual annotation was a bottleneck.
Wemanaged to tag the complete testing data, but wereonly able to annotate more than just 1/6 of thetraining data due to time reasons.
We only usea corresponding proportion of the automaticallylabeled training instances in order to respect theoverall distribution.5.2 Extraction of FeaturesGiven the linguistically supported observation onboth manually and automatically annotated tree-banks, we designed features to differentiate be-tween the ways it is translated.Since this work focuses on MT with transfer viadeep-syntactic layer, it is possible for the proposedfeatures to exploit morphological, syntactic and alittle of semantic information present on variousannotation layers.Unlike the target classes, which have to be as-signed as accurately as possible, extracted fea-tures must follow the real-world scenario of MT?
the only information that is given is the sourcesentence.
Thus, whereas extracting classes mayexploit the gold standard linguistic annotation, itcannot be employed in feature extraction.
We ex-tract them from text automatically annotated bythe same pipeline that is used in the TectoMT anal-ysis stage.However, there is an exception where we violatethis approach ?
coreference.
Performance of state-of-the-art coreference resolvers is still far from theideal, especially for distinguishing between pro-nouns referring to noun phrases and those refer-ring to clauses or wider discourse segments.
Sim-ilarly to the work of Guillou (2012) we wantedto isolate the problem of translating referentialexpressions from the task of resolving the entitythey refer to.
Therefore, we opted for extractingthe coreferential features from the gold annotationprojected onto automatically analyzed trees.
Notethat the results achieved using these features haveto be considered an upper bound for a given set-ting.Although the mapping between Czech transla-tion of it and English categories of it does not al-low to translate it directly, the category of it es-timated by an anaphoricity resolver might be apromising feature.
We therefore constructed a bi-nary feature based on the output of a system iden-tifying whether a pronoun it is coreferential ornot.
We employed the NADA resolver (Bergsmaand Yarowsky, 2011)5 exploiting the web-scale n-gram data and its tree-based extension presentedin (Veselovska?
et al 2012).Some verbs are more likely to bind with it thatrefers to a longer utterance.
Such it is quite con-sistently translated as a demonstrative to.
Thismotivated incorporating a parent lemma of an oc-currence of it into the feature set.
However, thetraining data is too small to be a sufficient samplefrom a distribution over lexical properties.
Hence,we took advantage of the automatically annotated6Czech-English corpus CzEng 1.0 (Bojar et al2012) that comprises more than 15 million sen-tence pairs.
In the manner described in Section5.1, we collected co-occurrence counts betweena functor that the given it possesses concatenatedwith a lemma of its verbal parent and a Czechcounterpart having the same functor (denoted ascsit).
We filtered out all occurrences where csitwas neither #PersPron nor ten.
Then, for both val-ues of csit a feature is constructed by looking upcounts for a concrete occurrence in the collectedcounts and quantized into 4-5 bins (Bansal andKlein, 2012) following the formula:bin(log(count(functor : parent ?
csit)count(functor : parent)count(csit))).Linguistic analysis carried out in Section 3 sug-gests the following syntax-oriented features re-lated to the verb to be.
Some nominal predicatestend to be translated as to, even though it is usuallycoreferential in such expressions (see example 7).So the corresponding binary feature fires if it is asubject and its parent is the verb to be having anobject (Figure 2a).Similarly, adjectival predicates that are not fol-lowed by a subordinating clause connected with5A probability value returned by this tool was binarized ata threshold 0.56Using the same annotation layers as in PCEDT and Tec-toMT, i.e.
in accordance with the Prague tectogrammaticstheory.55Figure 2: Syntactic features capturing typical con-structions with a verb be.the main clause by the English connectives to orthat are usually referential and translated as to,too.
We proposed a feature describing these cases,illustrated in Figure 2b.In contrast, if an adjectival predicate is followedby a subordinating clause with the verb being finiteand connected to the main clause by a conjunctionthat, in majority of cases it is a pleonastic usage ofit translated as a null subject (see example 6).
Aschema of the feature is depicted in Figure 2c.Being definitely pleonastic, it in cleft sentencesis expressed in Czech either by to or by sentencerearranging (see example 8).
We target this phe-nomenon by another feature being fired if it is asubject of the verb to be and if this verb has an ob-ject and is followed by a relative clause (see Figure2d).Finally, we designed two features exploitingcoreference relations.
The first one simply indi-cates if it has an antecedent, while the second firesif any of the antecedents in the coreferential chainis a verb phrase.
As we noted above, these fea-tures are based on the gold standard annotation ofcoreference.5.3 Data DescriptionThe data for training and testing a discriminativetranslation model of the personal pronoun it wereextracted from PCEDT with classes and featuresobtained as described in Section 5.1 and 5.2, re-spectively.
Due to the limited amount of manuallyannotated training data, the training set extractedfrom sections 00 ?
19 was reduced from 5841 to940 instances, though.
The testing set was an-notated thoroughly, thus containing 543 instancesextracted from sections 20 ?
21.
Every instancerepresents an occurrence of it in PCEDT.
The dis-Class Train TestPersPron 576 322To 231 138Null 133 83Table 1: Distribution of classes in the data sets.tribution of target classes in the data is shown inTable 1.6 ExperimentsExperiments were conducted in two settings thatdiffer in the usage of features extracted from goldcoreferential relations.To mitigate a possible error caused by a wrongclassifier choice, we built several models based onvarious Machine Learning classification methods.If not explicitly mentioned, the methods below areapplied with default parameters:?
Vowpal Wabbit (Langford, 2012).
Binarylogistic regression with one-against-all strat-egy for handling multiple classes.
The opti-mum has been found using the online method(Stochastic Gradient Descent).
We varied theparameters of the number of passes over thedata and the L2 regularization weight.?
AI::MaxEntropy.7 Multiclass logistic re-gression.8 The optimum has been found us-ing the batch method (L-BFGS).?
sklearn.neighbors.9 k-nearest neighborsclassifier with the parameter k being varied.?
sklearn.tree.
Decision tree classifier.?
sklearn.SVC.
Support Vector Machines withone-against-one strategy to handle multipleclasses.
We varied the choice of a kernel.The accuracy evaluated on both training and testsets is shown in Table 2 (columns Acc:Train andAcc:Test).
The baseline resolver simply picks themost frequent class in the training set, which isPersPron.
For both experimental settings, thestandard deviation measured on the test set is lessthan 1% in total, if the method?s best configurationof parameters is taken and the result on decisiontrees, which we did not tune, is excluded.
Thisshows that all classifiers are consistent in their de-cisions.7http://search.cpan.org/?laye/AI-MaxEntropy-0.20/8In the field of NLP also called Maximum Entropy.9All classifiers labeled as sklearn.
* are implemented inthe Scikit-learn Python library (Pedregosa et al 2011).56all feats all feats + corefML Method Acc:Train Acc:Test BLEU Acc:Train Acc:TestBaseline 60.70 59.30 0.1401 60.70 59.30Original TectoMT ?
?
0.1404 ?
?Vowpal Wabbit (passes=30) 90.62 75.69 ?
90.83 75.87Vowpal Wabbit (passes=20) 89.99 76.43 0.1403 90.20 76.98Vowpal Wabbit (passes=10) 87.78 76.24 ?
87.78 76.61Vowpal Wabbit (passes=30, l2=0.001) 71.23 66.11 ?
83.03 77.16Vowpal Wabbit (passes=20, l2=0.001) 82.19 74.95 ?
78.19 74.40Vowpal Wabbit (passes=10, l2=0.001) 75.03 70.17 ?
72.81 70.17Vowpal Wabbit (passes=30, l2=0.00001) 90.52 75.69 ?
90.94 76.06Vowpal Wabbit (passes=20, l2=0.00001) 89.99 76.43 ?
90.09 76.98Vowpal Wabbit (passes=10, l2=0.00001) 87.67 76.24 ?
87.67 76.61AI::MaxEntropy 85.99 76.61 0.1403 86.09 76.98sklearn.neighbors (k=1) 91.57 71.64 ?
93.36 72.19sklearn.neighbors (k=3) 84.62 72.01 ?
84.93 71.82sklearn.neighbors (k=5) 84.93 74.77 0.1403 84.72 75.87sklearn.neighbors (k=10) 82.51 73.30 ?
83.14 75.87sklearn.tree 93.36 73.66 0.1403 94.10 71.82sklearn.SVC (kernel=linear) 90.83 75.51 0.1402 91.15 76.80sklearn.SVC (kernel=poly) 60.70 59.30 ?
60.70 59.30sklearn.SVC (kernel=rbf) 71.23 68.69 ?
73.76 71.27Table 2: Intrinsic (accuracy on the training and test data) and extrinsic (BLEU score) evaluation oftranslation model of it in configuration with (all feats) and without gold coreferential features (all feats+ coref).By introducing linguistically motivated featuresexploiting the deep-syntactic description of thesentence, we gained 17% in total over the base-line.
Moreover, adding features based on the goldcoreference annotation results in a further 0.5%improvement.7 Evaluation on MTAlthough intrinsic evaluation as performed in Sec-tion 6 can give us a picture of how accurate thetranslation model might be, the main purpose ofthis work is to integrate it in a full-fledged MTsystem.
As explained in Section 4, this componentis tailored for TectoMT ?
an MT system where thetransfer is provided through a deep-syntactic layer.The extrinsic evaluation of the proposed methodwas carried out on the English-Czech test set forWMT 2011 Shared Translation Task (Callison-Burch et al 2011).10 This data set contains 3,003English sentences with one Czech reference trans-lation, out of which 430 contain at least one occur-rence of it.Since this test set is provided with no annota-tion of coreferential links, the model of it that isinvolved in experiments on the end-to-end transla-tion was trained on a complete feature set exclud-10http://www.statmt.org/wmt11/test.tgzing the coreferential features using the MachineLearning method that performed best in the intrin-sic test, i.e.
AI::MaxEntropy (see Section 6).The new method was compared to the rule-based approach originally used in TectoMT, whichworks as follows.
In the transfer stage, all occur-rences of it are translated to a demonstrative ten.In the synthesis stage, another rule is fired, whichdetermines whether ten is omitted on the surface.Then, omitting it corresponds either to a structuralchange (Null class) or an unexpressed personalpronoun (a subset of PersPron class).
It makesthis original approach difficult to compare with thescores in Table 2, as the translation model of itis applied in the transfer stage, where we do notknow yet if a personal pronoun is to be expressedor not.
Thus, we consider it the most appropriateto use final translated sentences produced by twoversions of TectoMT in order to compare the dif-ferent way they handle it.The shift from the original settings to a newmodel for it results in 166 changed sentences.
Interms of BLEU score, we observe a marginal dropfrom 0.1404 to 0.1403 when using the new ap-proach.11 Other classifiers achieved the same or11For comparison, the best system so far ?
Chimera (Bojaret al 2013) achieves 0.1994 on the same test set.
Chimeracombines Moses, TectoMT and rule-based corrections.57new better than old 24old better than new 13both equally wrong 9both equally correct 4Table 3: The results of manual evaluation con-ducted on 50 sentences translated by TectoMT inthe original settings (old) and with the new trans-lation model for it (new)similar score which correlates with the findingsfrom intrinsic evaluation (see Table 2).
It accordswith a similar experience of Le Nagard and Koehn(2010) and Guillou (2012) and gives another evi-dence that the BLEU metric is inaccurate for mea-suring pronoun translation.Manual evaluation gives a more realistic view.We randomly sampled 50 out of the 166 sentencesthat differ and one annotator assessed which ofthe two systems gave a better translation.
Table3 shows that in almost half of the cases the changewas an improvement.
Including the sentences thatare acceptable for both settings, the new approachpicked the correct Czech counterpart of it in 22%more sentences than the original approach.
Sincethe proportion of the changed sentences accountsfor almost 39% of all sentences containing it, theoverall proportion of improved sentences with it isaround 8.5% in total.8 DiscussionInspecting the manually evaluated translation fortypes of improvements and losses, we have foundthat in none of the changed sentences the originalsystem decided to omit ten (obtained by the rule)on the surface.
It shows that the new approachagrees with the original one on the way of omit-ting personal pronouns and mainly addresses theoverly simplistic assignment of the demonstrativeten.The distribution of target classes over cor-rected sentences is almost uniform.
In 13 outof 24 improvements, the new system succeededin correctly resolving the Null class while inthe remaining 11 cases, the corrected class wasPersPron.
It took advantage mostly of thesyntax-based features in the former and sugges-tions given by the NADA anaphoricity resolver inthe latter.Examining the errors, we observed that the ma-jority of them are incurred in the structures with?it is?.
These errors stem mostly from incorrectactivation of syntactic features due to parsing andPOS tagging errors.
Example 11 (the Czech sen-tence is an MT output) shows the latter, when thePOS tagger erroneously labeled the word soy as anadjective.
That resulted in activating the featurefor adjectival predicates followed by that (Figure2c) instead of a feature indicating cleft structures(Figure 2d), thus preferring the label Null to thecorrect To.
(11) SOURCE: It is just soy that all well-knownmanufacturers use now.TECTOMT: Je to jen so?jove?, z?e zna?m?
?vy?robci vs?ech pouz???vaj??
te?d.9 ConclusionIn this work we presented a novel approach todealing with the translation of the English personalpronoun it.
We have shown that the mapping be-tween the categories of it and the ways of trans-lating it to Czech is not one-to-one.
In order todeal with this, we designed a discriminative trans-lation model of it for the TectoMT deep syntax MTframework.We have built a system that outperforms its pre-decessor in 8.5% sentences containing it, takingadvantage of the features based on rich syntacticannotation the MT system provides, external toolsfor anaphoricity resolution and features capturinglexical co-occurrence in a massive parallel corpus,The main bottleneck that hampered bigger im-provements is the manual annotation of the train-ing data.
We managed to accomplish it just on 1/6of the data, which did not provide sufficient evi-dence for some specific features.Our main objective of the future work is thusto reduce a need for manual annotation by dis-covering ways of automatic extraction of reliableclasses from a semi-manually annotated corpussuch as PCEDT.AcknowledgmentsThis work has been supported by the GrantAgency of the Czech Republic (grantsP406/12/0658 and P406/2010/0875), the grantGAUK 4226/2011 and EU FP7 project Khresmoi(contract no.
257528).
This work has been usinglanguage resources developed and/or stored and/ordistributed by the LINDAT-Clarin project of theMinistry of Education of the Czech Republic(project LM2010013).58ReferencesMohit Bansal and Dan Klein.
2012.
Coreference Se-mantics from Web Features.
In Proceedings of the50th Annual Meeting of the ACL: Long Papers ?
Vol-ume 1, pages 389?398, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Shane Bergsma and David Yarowsky.
2011.
NADA:A Robust System for Non-Referential Pronoun De-tection.
In DAARC, pages 12?23, Faro, Portugal,October.Ondr?ej Bojar, Zdene?k ?Zabokrtsky?, Ondr?ej Dus?ek, Pe-tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???Mars??
?k, Michal Nova?k, Martin Popel, and Ales?
Tam-chyna.
2012.
The Joy of Parallelism with CzEng1.0.
In Proceedings of LREC 2012, Istanbul, Turkey,May.
ELRA, European Language Resources Associ-ation.Ondr?ej Bojar, Rudolf Rosa, and Ales?
Tamchyna.
2013.Chimera ?
Three Heads for English-to-Czech Trans-lation.
In Proceedings of the Eight Workshop on Sta-tistical Machine Translation.
Under review.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011Workshop on Statistical Machine Translation.
InProceedings of the Sixth Workshop on Statisti-cal Machine Translation, pages 22?64, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.Liane Guillou.
2012.
Improving Pronoun Translationfor Statistical Machine Translation.
In Proceedingsof the Student Research Workshop at the 13th Con-ference of the EACL, pages 1?10, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,Silvie Cinkova?, Eva Fuc??
?kova?, Marie Mikulova?, PetrPajas, Jan Popelka, Jir???
Semecky?, Jana ?Sindlerova?,Jan ?Ste?pa?nek, Josef Toman, Zden?ka Ures?ova?, andZdene?k ?Zabokrtsky?.
2011.
Prague Czech-EnglishDependency Treebank 2.0.Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, PetrSgall, Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc??
?kova?,Marie Mikulova?, Petr Pajas, Jan Popelka, Jir??
?Semecky?, Jana ?Sindlerova?, Jan ?Ste?pa?nek, JosefToman, Zden?ka Ures?ova?, and Zdene?k ?Zabokrtsky?.2012.
Announcing Prague Czech-English Depen-dency Treebank 2.0.
In Proceedings of the 8th In-ternational Conference on Language Resources andEvaluation (LREC 2012), pages 3153?3160.
ELRA.Christian Hardmeier and Marcello Federico.
2010.Modelling Pronominal Anaphora in Statistical Ma-chine Translation.
In Marcello Federico, Ian Lane,Michael Paul, and Franc?ois Yvon, editors, Proceed-ings of the seventh International Workshop on Spo-ken Language Translation (IWSLT), pages 283?289.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of the 2003 Conference of the NAACL HLT?
Volume 1, pages 48?54, Stroudsburg, PA, USA.Association for Computational Linguistics.John Langford.
2012.
Vowpal Wabbit.Ronan Le Nagard and Philipp Koehn.
2010.
Aid-ing Pronoun Translation with Co-Reference Resolu-tion.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 252?261, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Linguistic Data Consortium.
1999.
Penn Treebank 3.LDC99T42.David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clavNova?k.
2008.
Automatic Alignment of Czech andEnglish Deep Syntactic Dependency Trees.
In Pro-ceedings of the Twelfth EAMT Conference, pages102?111.Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and ?Edouard Duchesnay.
2011.Scikit-learn: Machine Learning in Python.
Jour-nal of Machine Learning Research, 12:2825?2830,November.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Semantic andPragmatic Aspects.
D. Reidel Publishing Company,Dordrecht.Petr Sgall.
1967.
Generativn??
popis jazyka a c?eska?deklinace.
Academia, Prague, Czech Republic.Kater?ina Veselovska?, Giang Linh Nguy, and MichalNova?k.
2012.
Using Czech-English Parallel Cor-pora in Automatic Identification of It.
In The FifthWorkshop on Building and Using Comparable Cor-pora, pages 112?120.Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas.
2008.TectoMT: Highly Modular MT System with Tec-togrammatics Used as Transfer Layer.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 167?170, Stroudsburg, PA, USA.Association for Computational Linguistics.59
