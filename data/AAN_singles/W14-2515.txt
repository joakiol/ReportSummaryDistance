Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50?55,Baltimore, Maryland, USA, June 26, 2014.c?2014 Association for Computational LinguisticsPredicting Fine-grained Social Roles with Selectional PreferencesCharley Beller Craig Harman Benjamin Van Durmecharleybeller@jhu.edu craig@craigharman.net vandurme@cs.jhu.eduHuman Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, MD USAAbstractSelectional preferences, the tendencies ofpredicates to select for certain semanticclasses of arguments, have been success-fully applied to a number of tasks incomputational linguistics including wordsense disambiguation, semantic role label-ing, relation extraction, and textual infer-ence.
Here we leverage the informationencoded in selectional preferences to thetask of predicting fine-grained categoriesof authors on the social media platformTwitter.
First person uses of verbs that se-lect for a given social role as subject (e.g.I teach ... for teacher) are used to quicklybuild up binary classifiers for that role.1 IntroductionIt has long been recognized that linguistic pred-icates preferentially select arguments that meetcertain semantic criteria (Katz and Fodor, 1963;Chomsky, 1965).
The verb eat for example se-lects for an animate subject and a comestible ob-ject.
While the information encoded by selectionalpreferences can and has been used to support nat-ural language processing tasks such as word sensedisambiguation (Resnik, 1997), syntactic disam-biguation (Li and Abe, 1998) and semantic rolelabeling (Gildea and Jurafsky, 2002), much ofthe work on the topic revolves around developingmethods to induce selectional preferences fromdata.
In this setting, end-tasks can be used forevaluation of the resulting collection.
Ritter et al.
(2010) gave a recent overview of this work, break-ing it down into class-based approaches (Resnik,1996; Li and Abe, 1998; Clark and Weir, 2002;Pantel et al., 2007), similarity-based approaches(Dagan et al., 1999; Erk, 2007), and approachesusing discriminative (Bergsma et al., 2008) or gen-erative probabilistic models (Rooth et al., 1999)like their own.One of our contributions here is to show thatthe literature on selectional preferences relates tothe analysis of the first person content transmittedthrough social media.
We make use of a ?quickand dirty?
method for inducing selectional pref-erences and apply the resulting collections to thetask of predicting fine-grained latent author at-tributes on Twitter.
Our method for inducing se-lectional preferences is most similar to class-basedapproaches, though unlike approaches such as byResnik (1996) we do not require a WordNet-likeontology.The vast quantity of informal, first-person textdata made available by the rise of social me-dia platforms has encouraged researchers to de-velop models that predict broad user categorieslike age, gender, and political preference (Gareraand Yarowsky, 2009; Rao et al., 2010; Burger etal., 2011; Van Durme, 2012b; Zamal et al., 2012).Such information is useful for large scale demo-graphic research that can fuel computational socialscience advertising.Similarly to Beller et al.
(2014), we are inter-ested in classification that is finer-grained thangender or political affiliation, seeking instead topredict social roles like smoker, student, andartist.
We make use of a light-weight, unsuper-vised method to identify selectional preferencesand use the resulting information to rapidly boot-strap classification models.2 Inducing Selectional PreferencesConsider the task of predicting social roles in moredetail: For a given role, e.g.
artist, we want a wayto distinguish role-bearing from non-role-bearingusers.
We can view each social role as being afine-grained version of a semantic class of the sortrequired by class-based approaches to selectionalpreferences (e.g.
the work by Resnik (1996) andthose reviewed by Light and Greiff (2002)).
Thegoal then is to identify a set of verbs that preferen-50tially select that particular class as argument.
Oncewe have a set of verbs for a given role, simple pat-tern matches against first person subject templateslike I can be used to identify authors that bearthat social role.In order to identify verbs that select for a givenrole r as subject we use an unsupervised methodinspired by Bergsma and Van Durme (2013) thatextracts features from third-person content (i.e.newswire) to build classifiers on first-person con-tent (i.e.
tweets).
For example, if we read in anews article that an artist drew ..., we can take atweet saying I drew ... as potential evidence thatthe author bears the artist social role.We first count all verbs v that appear with role ras subject in the web-scale, part-of-speech taggedn-gram corpus, Google V2 (Lin et al., 2010).The resulting collection of verbs is then rankedby computing their pointwise mutual information(Church and Hanks, 1990) with the subject role r.The PMI of a given role r and a verb v that takesr as subject is given as:PMI(r, v) = logP (r, v)P (r)P (v)Probabilities are estimated from counts of therole-verb pairs along with counts matching thegeneric subject patterns he and she whichserve as general background cases.
This gives us aset of verbs that preferentially select for the subsetof persons filling the given role.The output of the PMI ranking is a high-recalllist of verbs that preferentially select the given so-cial role as subject over a background population.Each such list then underwent a manual filteringstep to rapidly remove non-discriminative verbsand corpus artifacts.
One such artifact from ourcorpus was the term wannabe which was spuri-ously elevated in the PMI ranking based on therelative frequency of the bigram artist wannabe ascompared to she wannabe.
Note that in the firstcase wannabe is best analyzed as a noun, while inthe second case a verbal analysis is more plausi-ble.
The filtering was performed by one of the au-thors and generally took less than two minutes perlist.
The rapidity of the filtering step is in line withfindings such as by Jacoby et al.
(1979) that rele-vance based filtering involves less cognitive effortthan generation.
After filtering the lists containedfewer than 40 verbs selecting each social role.In part because of the pivot from third- to first-person text we performed a precision test on theremaining verbs to identify which of them arelikely to be useful in classifying twitter users.
Foreach remaining verb we extracted all tweets thatcontained the first person subject pattern I froma small corpus of tweets drawn from the free pub-lic 1% sample of the Twitter Firehose over a singlemonth in 2013.
Verbs that had no matches whichappeared to be composed by a member of the as-sociated social role were discarded.
Using thissmaller high-precision set of verbs, we collectedtweets from a much larger corpus drawn from 1%sample over the period 2011-2013.One notable feature of the written English insocial media is that sentence subjects can be op-tionally omitted.
Subject-drop is a recognized fea-ture of other informal spoken and written registersof English, particularly ?diary dialects?
(Thrasher,1977; Napoli, 1982; Haegeman and Ihsane, 2001;Weir, 2012; Haegeman, 2013; Scott, 2013).
Be-cause of the prevalence of subjectless cases wecollected two sets of tweets: those matching thefirst person subject pattern I and those wherethe verb was tweet initial.
Example tweets for eachof our social roles can be seen in Table 2.3 Classification via selectionalpreferencesWe conducted a set of experiments to gauge thestrength of the selectional preference indicatorsfor each social role.
For each experiment we usedbalanced datasets for training and testing with halfof the users taken from a random background sam-ple and half from a collection of users identifiedas belonging to the social role.
Base accuracy wasthus 50%.To curate the collections of positively identifiedusers we crowdsourced a manual verification pro-cedure.
We use the popular crowdsourcing plat-form Mechanical Turk1to judge whether, for atweet containing a given verb, the author held therole that verb prefers as subject.
Each tweet wasjudged using 5-way redundancy.Mechanical Turk judges (?Turkers?)
were pre-sented with a tweet and the prompt: Based on thistweet, would you think this person is a ARTIST?along with four response options: Yes, Maybe,Hard to tell, and No.
An example is shown in Fig-ure 1.We piloted this labeling task with a goal of20 tweets per verb over a variety of social roles.1https://www.mturk.com/mturk/51Artistdraw Yeaa this a be the first time I draw myshit onnAthleteplay @[user] @[user] i have got the night offtonight because I played last night and Iam going out for dinner so won?t be ableto come?Bloggerblogged @[user] I decided not to renew.
Iblogged about it on the fan club.
a bitshocked no neg comments back to meCheerleadercheer I really dont wanna cheer for this gameI have soo much to doChristianthank Had my bday yesterday 3011 nd had agood night with my friends.
I thank God4 His blessings in my life nd praise Him4 adding another year.DJspin Quick cut session before I spin tonightFilmmakerfilm @[user] apparently there was no au-dio on the volleyball game I filmedso...there will be no ?NAT sound?
causeI have no audio at allMedia Hostinterview Oh.
I interviewed her on the @[user] .You should listen to the interview.
Itsawesome!
@[user] @[user] @[user]Performerperform I perform the flute... kareem shocked...Producerproduce RT @[user]: Wow 2 films in Urban-world this year-1 I produced ... [URL]Smokersmoke I smoke , i drank .. was my shit bra !Stonerpuff I?m a cigarello fiend smokin weed likeits oxygen Puff pass, nigga I puff grasstill I pass outStudentfinish I finish school in March and my friendbirthday in March ...Teacherteach @[user] home schooled I really wannafind out wat it?s like n making newfriends but home schooling is cool Iteach myself mums illTable 1: Example verbs and sample tweets collected usingthem in the first person subject pattern (I ).Each answer was associated with a score (Yes = 1,Maybe = .5, Hard to tell = No = 0) and aggregatedacross the five judges, leading to a range of pos-sible scores from 0.0 to 5.0 per tweet.
We foundin development that an aggregate score of 4.0 ledto an acceptable agreement rate between the Turk-ers and the experimenters, when the tweets wererandomly sampled and judged internally.Verbs were discarded for being either insuffi-ciently accurate or insufficiently prevalent in thecorpus.
From the remaining verbs, we identifiedusers with tweets scoring 4.0 or better as the posi-tive examples of the associated social roles.
Thesepositively identified user?s tweets were scraped us-ing the Twitter API in order to construct user-specific corpora of positive examples for each role.Figure 1: Mechanical Turk presentation0.50.60.70.8ArtistAthleteBloggerCheerleaderChristianDJFilmmakerHostPerformerProducerSmokerStonerStudentTeacherAccuracyFigure 2: Accuracy of classifier trained and tested on bal-anced set contrasting agreed upon Twitter users of a givenrole, against users pulled at random from the 1% stream.3.1 General ClassificationThe positively annotated examples were balancedwith data from a background set of Twitter usersto produce training and test sets.
These test setswere usually of size 40 (20 positive, 20 back-ground), with a few classes being sparser (thesmallest test set had only 28 instances).
We usedthe Jerboa (Van Durme, 2012a) platform to con-vert our data to binary feature vectors over a uni-gram vocabulary filtered such that the minimumfrequency was 5 (across unique users).
Trainingand testing was done with a log-linear model viaLibLinear (Fan et al., 2008).
Results are shownin Figure 2.
As can be seen, a variety of classes inthis balanced setup can be predicted with accura-cies in the range of 80%.
This shows that the in-formation encoded in selectional preferences con-tains discriminating signal for a variety of thesesocial roles.3.2 Conditional ClassificationHow accurately can we predict membership in agiven class when a Twitter user sends a tweetmatching one of the collected verbs?
For exam-ple, if one sends a tweet saying I race ..., then howlikely is it that the author is an athlete?520.50.60.70.8Artist :drawAthlete: raceAthlete: runBlogger : bloggedCheerleader : cheerChristian :prayChristian :serveChristian :thankDJ: spinFilmmaker: filmHost :interviewPerformer :performProducer :produceSmoker :smokeStoner: puffStoner: sparkStudent : enrollStudent : finishTeacher : teachAccuracyFigure 3: Results of positive vs negative by verb.
Giventhat a user writes a tweet containing I interview .
.
.
or Inter-viewing .
.
.
we are about 75% accurate in identifying whetheror not the user is a Radio/Podcast Host.# Users # labeled # Pos # Neg Attribute199022 516 63 238 Artist-draw45162 566 40 284 Athlete-race1074289 1000 54 731 Athlete-run9960 15 14 0 Blogger-blog2204 140 57 18 College Student-enroll247231 1000 85 564 College Student-finish60486 845 61 524 Cheerleader-cheer448738 561 133 95 Christian-pray92223 286 59 180 Christian-serve428337 307 78 135 Christian-thank17408 246 17 151 DJ-spin153793 621 53 332 Filmmaker-film36991 554 42 223 Radio Host-interview43997 297 81 97 Performer-perform69463 315 71 100 Producer-produce513096 144 74 8 Smoker-smoke5542 124 49 15 Stoner-puff5526 229 59 51 Stoner-spark149244 495 133 208 Teacher-teachTable 2: Numbers of positively and negatively identifiedusers by indicative verb.Using the same collection as the previous ex-periment, we trained classifiers conditioned on agiven verb term.
Positive instances were taken tobe those with a score of 4.0 or higher, with nega-tive instances taken to be those with scores of 1.0or lower (strong agreement by judges that the orig-inal tweet did not provide evidence of the givenrole).
Classification results are shown in figure 3.Note that for a number of verb terms these thresh-olds left very sparse collections of users.
Therewere only 8 users, for example, that tweeted thephrase I smoke ... but were labeled as negative in-stances of Smokers.
Counts are given in Table 2.Despite the sparsity of some of these classes,many of the features learned by our classifiersmake intuitive sense.
Highlights of the mosthighly weighted unigrams from the classificationVerb Feature ( Rank)draw drawing, art, book4, sketch14, paper19race race, hard, winter, won11, training16, run17run awesome, nike6, fast9, marathon20blog notes, boom, hacked4, perspective9cheer cheer, pictures, omg, text, literallypray through, jesus3, prayers7, lord14, thank17serve lord, jesus, church, blessed, pray, gracethank [ ], blessed, lord, trust11, pray12enroll fall, fat, carry, job, spend, fail15finish hey, wrong, may8, move9, officially14spin show, dj, music, dude, ladies, posted, listenfilm please, wow, youtube, send, music8perform [ ], stuck, act, song, tickets7, support16produce follow, video8, listen10, single11, studio13,interview fan, latest, awesome, seemssmoke weakness, runs, ti, simplypuff bout, $7, smh9, weed10spark dont, fat5, blunt6, smoke11teach forward, amazing, students, great, teacher7Table 3: Most-highly indicative features that a user holdsthe associated role given that they used the phrase I VERBalong with select features within the top 20.experiments are shown in Table 3.
Taken togetherthese features suggest that several of our roles canbe distinguished from the background populationby focussing on typical language use.
The use ofterms like, e.g., sketch by artists, training by ath-letes, jesus by Chrisitians, and students by teach-ers conforms to expected pattern of language use.4 ConclusionWe have shown that verb-argument selectionalpreferences relates to the content-based classifica-tion strategy for latent author attributes.
In particu-lar, we have presented initial studies showing thatmining selectional preferences from third-personcontent, such as newswire, can be used to informlatent author attribute prediction based on first-person content, such as that appearing in socialmedia services like Twitter.Future work should consider the question ofpriors.
Our study here relied on balanced classexperiments, but the more fine-grained the socialrole, the smaller the subset of the population wemight expect will possess that role.
Estimatingthese priors is thus an important point for futurework, especially if we wish to couple such demo-graphic predictions within a larger automatic sys-tem, such as the aggregate prediction of targetedsentiment (Jiang et al., 2011).Acknowledgements This material is partially basedon research sponsored by the NSF under grant IIS-1249516and by DARPA under agreement number FA8750-13-2-0017(DEFT).53ReferencesCharley Beller, Rebecca Knowles, Craig Harman,Shane Bergsma, Margaret Mitchell, and BenjaminVan Durme.
2014.
I?m a belieber: Social roles viaself-identification and conceptual attributes.
In Pro-ceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics.Shane Bergsma and Benjamin Van Durme.
2013.
Us-ing Conceptual Class Attributes to Characterize So-cial Media Users.
In Proceedings of ACL.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Discriminative learning of selectional pref-erence from unlabeled text.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 59?68.
Association forComputational Linguistics.John D. Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender onTwitter.
In Proceedings of EMNLP.Noam Chomsky.
1965.
Aspects of the Theory of Syn-tax.
Number 11.
MIT press.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2).Ido Dagan, Lillian Lee, and Fernando CN Pereira.1999.
Similarity-based models of word cooccur-rence probabilities.
Machine Learning, 34(1-3):43?69.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceeding of the45th Annual Meeting of the Association for Compu-tational Linguistics, volume 45, page 216.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, (9).Nikesh Garera and David Yarowsky.
2009.
Modelinglatent biographic attributes in conversational genres.In Proceedings of ACL.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational linguis-tics, 28(3):245?288.Liliane Haegeman and Tabea Ihsane.
2001.
Adult nullsubjects in the non-pro-drop languages: Two diarydialects.
Language acquisition, 9(4):329?346.Liliane Haegeman.
2013.
The syntax of registers: Di-ary subject omission and the privilege of the root.Lingua, 130:88?110.Larry L Jacoby, Fergus IM Craik, and Ian Begg.
1979.Effects of decision difficulty on recognition and re-call.
Journal of Verbal Learning and Verbal Behav-ior, 18(5):585?600.Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao.2011.
Target-dependent twitter sentiment classifi-cation.
In Proceedings of ACL.Jerrold J Katz and Jerry A Fodor.
1963.
The structureof a semantic theory.
language, pages 170?210.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the MDL principle.Computational linguistics, 24(2):217?244.Marc Light and Warren Greiff.
2002.
Statistical mod-els for the induction and use of selectional prefer-ences.
Cognitive Science, 26(3):269?281.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil,Emily Pitler, Rachel Lathbury, Vikram Rao, KapilDalwani, and Sushant Narsale.
2010.
New tools forweb-scale n-grams.
In Proc.
LREC, pages 2221?2227.Donna Jo Napoli.
1982.
Initial material deletion inEnglish.
Glossa, 16(1):5?111.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard H Hovy.
2007.ISP: Learning inferential selectional preferences.
InHLT-NAACL, pages 564?571.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In Proceedings of the Work-shop on Search and Mining User-generated Con-tents (SMUC).Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61(1):127?159.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How, pages 52?57.
Washington,DC.Alan Ritter, Masaum, and Oren Etzioni.
2010.
A la-tent dirichlet allocation method for selectional pref-erences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 424?434.
Association for ComputationalLinguistics.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via EM-based clustering.
InProceedings of the 37th annual meeting of the Asso-ciation for Computational Linguistics, pages 104?111.
Association for Computational Linguistics.54Kate Scott.
2013.
Pragmatically motivated null sub-jects in English: A relevance theory perspective.Journal of Pragmatics, 53:68?83.Randolph Thrasher.
1977.
One way to say more bysaying less: A study of so-called subjectless sen-tences.
Kwansei Gakuin University Monograph Se-ries, 11.Benjamin Van Durme.
2012a.
Jerboa: A toolkit forrandomized and streaming algorithms.
TechnicalReport 7, Human Language Technology Center ofExcellence, Johns Hopkins University.Benjamin Van Durme.
2012b.
Streaming analysis ofdiscourse participants.
In Proceedings of EMNLP.Andrew Weir.
2012.
Left-edge deletion in English andsubject omission in diaries.
English Language andLinguistics, 16(01):105?129.Faiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of Twitter users from neighbors.
InProceedings of ICWSM.55
