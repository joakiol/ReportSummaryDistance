Proceedings of NAACL-HLT 2015, pages 61?65,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsWOLFE: An NLP-friendly Declarative Machine Learning StackSameer Singh?Tim Rockt?aschel?Luke Hewitt?Jason Naradowsky?Sebastian Riedel?
?University of Washington?University College LondonSeattle WA London UKsameer@cs.washington.edu {t.rocktaschel,luke.hewitt.10,j.narad,s.riedel}@cs.ucl.ac.ukAbstractDeveloping machine learning algorithms fornatural language processing (NLP) applica-tions is inherently an iterative process, involv-ing a continuous refinement of the choice ofmodel, engineering of features, selection of in-ference algorithms, search for the right hyper-parameters, and error analysis.
Existing proba-bilistic program languages (PPLs) only providepartial solutions; most of them do not supportcommonly used models such as matrix factor-ization or neural networks, and do not facilitateinteractive and iterative programming that iscrucial for rapid development of these models.In this demo we introduce WOLFE, a stack de-signed to facilitate the development of NLP ap-plications: (1) the WOLFE language allows theuser to concisely define complex models, en-abling easy modification and extension, (2) theWOLFE interpreter transforms declarative ma-chine learning code into automatically differ-entiable terms or, where applicable, into factorgraphs that allow for complex models to beapplied to real-world applications, and (3) theWOLFE IDE provides a number of differentvisual and interactive elements, allowing intu-itive exploration and editing of the data rep-resentations, the underlying graphical models,and the execution of the inference algorithms.1 IntroductionMachine learning has become a critical componentof practical NLP systems, however designing andtraining an appropriate, accurate model is an itera-tive and time-consuming process for a number of rea-sons.
First, initial intuitions that inform model design(such as which features to use) are often inaccurate,requiring incremental model tweaking based on per-formance.
Even if the model is accurate, the finalperformance depends quite critically on the choice ofthe algorithms and their hyper-parameters.
Further,bugs that are introduced by the user may not even bereflected directly in the performance (such as a fea-ture computation bug may not degrade performance).All these concerns are further compounded due to thevariety of approaches commonly used in NLP, suchas conditional random fields (Sutton and McCallum,2007), Markov random networks (Poon and Domin-gos, 2007), Bayesian networks (Haghighi and Klein,2010), matrix factorization (Riedel et al, 2013), andDeep learning (Socher et al, 2013).Probabilistic programming languages (PPLs), byclosing the gap between traditional programmingand probabilistic modeling, go a long way in aidingquick design and modification of expressive models1.However, creating accurate machine learning modelsusing these languages remains challenging.
Of theprobabilistic programming languages that exist today,no language can easily express the variety of modelsused in NLP, focusing instead on a restricted set ofmodeling paradigms, for example, Markov logic net-works can be models by Alchemy (Richardson andDomingos, 2006), Bayesian generative networks byChurch (Goodman et al, 2008), undirected graphi-cal models by Factorie (McCallum et al, 2009), andso on.
Further, these toolkits are only restricted tomodel design and inference execution, and do notprovide the appropriate debugging and interactive1For a comprehensive list of PPLs, see http://probabilistic-programming.org/.61UserREPL InterfaceLanguageVisualizationInterpreterEfficient ImplementationFigure 1: Overview of the WOLFE Stack.visualization tools required for developing such mod-els in practice.
Due to these concerns, probabilis-tic programming has not found significant adoptionin natural language processing, and application ofmachine learning to NLP still consists either of ar-duously designing, debugging, and iterating over avariety of models, or more commonly, giving up andusing the first model that is ?good enough?.In this demo, we introduce our probabilistic pro-gramming toolkit WOLFE (Riedel et al, 2014) thataids in the iterative design of machine learning mod-els for NLP applications.
The underlying proba-bilistic programming language can be used to con-cisely express a wide range of models, includingundirected graphical models, matrix factorization,Bayesian networks, neural networks, and further, itsmodular nature allows combinations of these model-ing paradigms.
We additionally present an easy-to-use IDE for the interactive designing of NLP models,consisting of an interactive and visual presentationof structured data, graphical models, and inferenceexecution.
Using the WOLFE language and IDE canthus enable the users to quickly create, debug, anditerate on complex models and inference.2 Overview of the WOLFE StackThe overall goal of the demo will be to guide usersin creating complex graphical models using an easy-to-use mathematical language for defining models,and in performing learning and inference for the cre-ated model using an IDE.
Figure 1 summarizes theoverview of the WOLFE stack, consisting of the lan-guage and the visualization that form the user-facinginterface, with the interpreter and efficient learningand inference engine as the back-end.Figure 2: Implementation of various matrix and ten-sor factorization models in WOLFE.2.1 Declarative Modeling LanguageExisting PPLs primarily focus on a single represen-tation for the probabilistic models, and either do notsupport, or provide only inefficient implementationsfor other kinds of machine learning models.
Thus apractitioner either has to write her own customizedimplementation of the models she is trying to explore,or decide apriori on the family of models she will berestricted to; both undesirable options.
Instead, weintroduce a probabilistic programming language thatis universal in its expression of models, yet alowsfor efficient implementations of these models.The design of the WOLFE language is inspiredby the observation that most machine learning algo-rithms can be formulated in terms of scalar functions(such as distributions and objectives/losses), searchspaces (such as the universe of possible labels) and asmall set of mathematical operations such as maxi-mization, summation and expectations that operateon these functions and spaces.
Using this insight, aprogram in WOLFE consists of a declarative descrip-tion of the machine learning algorithm in terms ofimplementations of these scalar functions, definitionsof the search spaces, and the use of appropriate opera-tors on these.
For example, named-entity recognitiontagging using conditional random fields consists ofa scalar function that defines the model score usinga dot product between the parameters and the sumof node and edge features, while inference using thismodel involves finding the label sequence that hasthe maximum model score over all label sequences.The focus on scalar functions as building blocksallows for rapid prototyping of a large range of ma-62chine learning models.
For instance, there exist avariety of matrix and tensor factorization methods forknowledge base population that have a succinct, uni-fied mathematical formulation (Nickel et al, 2015).In WOLFE these models can be easily implementedwith a few lines of code.
See Figure 2 for examplesof a Tucker2 decomposition, TransE (Bordes et al,2013), and Riedel et al (2013)?s feature model (F),entity model (E), and combination of the two (FE),either based on a log likelihood or Bayesian Person-alized Ranking (Rendle et al, 2009) objective.2.2 Interpreter, and Efficient ImplementationsIn WOLFE users write models using a domain-specific-language that supports a wide range of math-ematical expressions.
The WOLFE interpreter thenevaluates these expressions.
This is non-trivial asexpressions usually contain operators such as theargmax functions which are, in general, intractableto compute.
For efficient evaluation of WOLFE pro-grams our interpreter compiles WOLFE expressionsinto representations that enable efficient computa-tion in many cases.
For example, for terms that in-volve maximization over continuous search spacesWOLFE generates a computation tree that supportsefficient forward and back-propagation for automaticdifferentiation.
Likewise, when maximizing over dis-crete search spaces, WOLFE constructs factor graphsthat support efficient message passing algorithm suchas Max-Product or Dual Decomposition.
Crucially,due to the compositional nature of WOLFE, discreteand continuous optimization problems can be nestedto support a rich class of structured prediction ob-jectives.
In such cases the interpreter constructsnested computational structures, such as a factorgraph within a back-propagation graph.2.3 Visual and Interactive IDEIn this demonstration, we present an integrated de-veloping, debugging and visualization toolkit for ma-chine learning for NLP.
The IDE is based on the read-eval-print loop (REPL) to allow quick iterations ofwriting and debugging, and consists of the followingelements: (1) Editor (read): Users define the modeland inference in the declarative, math-like languagedescribed in Section 2.1 using a syntax highlightedcode editor.
(2) Build Automation (eval): The use ofthe interpreter as described in the previous sectionto provide efficient code that is executed.
(3) De-bugging/Visualization (print): Our tool presents theunderlying factor graph as an interactive UI elementthat supports clicking, drag and drop, hover, etc.
toexplore the structure and the factors of the model.We visualize the results of inference in a graphicalmanner that adapts to the type of the result (bar chartsfor simple distributions, shaded maps for matrix-likeobjects, circles/arrows for NLP data types, etc.).
Forfurther fine-grained debugging, we can also surfaceintermediate results from inference, for example, vi-sualizing the messages in belief propagation for eachedge in the factor graph.3 Demo OutlineThe overall objective of the demo is for users to de-sign, debug, and modify a machine learning modelfor an NLP application, starting from scratch.
Thedemo takes the user through all the steps of loadingdata, creating an initial model, observing the out-put errors, modifying the model accordingly, andrerunning to see the errors fixed: the complete set ofsteps often involved in real-life application of ML forNLP.
We provide pre-built functions for the menialtasks, such as data loading and feature computationfunctions, leaving the more interesting aspects ofmodel design to the user.
Further, we include an?open-ended?
option for interested users to developarbitrary models.
Based on their interest or area of ex-pertise, the user has an option of investigating any (orall) of the following applications: (1) sequence tag-ging using CRFs, (2) relational learning using MLNs,(3) matrix factorization for relation extraction, and(4) dependency parsing (for advanced users).
Eachof these are similar in the overall ?script?, differingin the data, models, and inference algorithms used;we describe the steps of the demo using the CRFexample.
All of the demo applications are availableonline at http://wolfe.ml/demos/nlp.1.
The first step of the demo allows the user toread as input a standard dataset of the task, andvisualize instances in an easy-to-read manner.
InFigure 3a for example, we show two sentencesread for the purpose of sequence tagging.2.
The user then defines an initial model for thetask, which is visualized as a factor graph for63(a) Data Loading(b) Initial Model(c) Error in PredictionFigure 3: Model Creation and Evaluation: An exam-ple instance of the demo showing the creation steps, in-cluding the loading and visualization of the sentences,designing and presentation of a linear chain CRF, andViterbi decoding for the sentences.the purpose of debugging the model definition.The initial model for sequence tagging is a sim-ple linear chain, defined and visualized for asentence in Figure 3b.3.
The user writes the declarative definition of in-ference, and makes predictions of the input data.The predictions are appropriately visualized, al-(a) Modify the Model (add skip edge)(b) Fixed PredictionFigure 4: Debugging Loop: The remaining steps of theiterative development, consisting of modification of themodel to fix the error from Figure 3c by adding a skip-factor to the original model, and confirming the inferencein the skip-chain model results in the correct prediction.lowing the user to detect mistakes (for example,the incorrect NER tag of location to ?Denver?in Figure 3c).4.
The user then modifies the model (adding a skip-factor in Figure 4a) that will likely correct themistake.
The modified model is then visualizedto confirm it is correct.
(Optionally, the user can,at any point, visualize the execution of the in-ference to confirm the modifications as well, forexample Figure 4a shows the state of messagesin belief propagation.)5.
On the execution of the model, the user con-firms that the original error has been fixed, forexample the skip factor allows the correct tag ofperson for ?Denver?
in Figure 4b.644 ConclusionsThis demo describes WOLFE, a language, interpreter,and an IDE for easy, iterative development of com-plex machine learning models for NLP applications.The language allows concise definition of the mod-els and inference by using universal, mathematicalsyntax.
The interpreter performs program analysison the user code to automatically generate efficientlow-level code.
The easy-to-use IDE allows the userto iteratively write and execute such programs, butmost importantly supports intuitive visualizations ofstructured data, models, and inference to enable usersto understand and debug their code.
The demo thusallows a user to design, debug, evaluate, and modifycomplex machine learning models for a variety ofNLP applications.AcknowledgmentsWe would like to thank Larysa Visengeriyeva, JanNoessner, and Vivek Srikumar for contributions toearly versions of WOLFE.
This work was supportedin part by Microsoft Research through its PhD Schol-arship Programme, an Allen Distinguished Investiga-tor Award, a Marie Curie Career Integration Grant,and in part by the TerraSwarm Research Center, oneof six centers supported by the STARnet phase of theFocus Center Research Program (FCRP) a Semicon-ductor Research Corporation program sponsored byMARCO and DARPA.ReferencesAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,Jason Weston, and Oksana Yakhnenko.
2013.
Trans-lating embeddings for modeling multi-relational data.In Advances in Neural Information Processing Systems,pages 2787?2795.Noah D. Goodman, Vikash K. Mansinghka, Daniel Roy,Keith Bonawitz, and Joshua B. Tenenbaum.
2008.Church: a language for generative models.
In Un-certainty in Artificial Intelligence (UAI).Aria Haghighi and Dan Klein.
2010.
Coreference reso-lution in a modular, entity-centered model.
In NorthAmerican Chapter of the Association for ComputationalLinguistics - Human Language Technologies (NAACLHLT), pages 385?393.Andrew McCallum, Karl Schultz, and Sameer Singh.2009.
FACTORIE: Probabilistic programming via im-peratively defined factor graphs.
In Neural InformationProcessing Systems (NIPS).Maximilian Nickel, Kevin Murphy, Volker Tresp, andEvgeniy Gabrilovich.
2015.
A review of relationalmachine learning for knowledge graphs: From multi-relational link prediction to automated knowledgegraph construction.
arXiv preprint arXiv:1503.00759.Hoifung Poon and Pedro Domingos.
2007.
Joint inferencein information extraction.
In Proceedings of the 22ndAAAI Conference on Artificial Intelligence (AAAI ?07),pages 913?918.Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,and Lars Schmidt-Thieme.
2009.
BPR: Bayesian per-sonalized ranking from implicit feedback.
In Uncer-tainty in Artificial Intelligence (UAI).Matthew Richardson and Pedro Domingos.
2006.
Markovlogic networks.
Machine Learning, 62(1-2):107?136.Sebastian Riedel, Limin Yao, Benjamin M. Marlin, andAndrew McCallum.
2013.
Relation extraction with ma-trix factorization and universal schemas.
In Joint Hu-man Language Technology Conference/Annual Meetingof the North American Chapter of the Association forComputational Linguistics (HLT-NAACL ?13), June.Sebastian Riedel, Sameer Singh, Vivek Srikumar, TimRocktaschel, Larysa Visengeriyeva, and Jan Noessner.2014.
Wolfe: Strength reduction and approximate pro-gramming for probabilistic programming.
In Interna-tional Workshop on Statistical Relational AI (StarAI).Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Empirical Methods in Natural Language Pro-cessing (EMNLP).Charles Sutton and Andrew McCallum.
2007.
An intro-duction to conditional random fields for relational learn-ing.
In Introduction to Statistical Relational Learning.65
