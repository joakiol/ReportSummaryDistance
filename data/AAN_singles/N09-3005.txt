Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 25?30,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing Language Modeling to Select Useful Annotation DataDmitriy DligachDepartment ofComputer ScienceUniversity of Coloradoat BoulderDmitriy.Dligach@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Coloradoat BoulderMartha.Palmer@colorado.eduAbstractAn annotation project typically has an abun-dant supply of unlabeled data that can bedrawn from some corpus, but because thelabeling process is expensive, it is helpful topre-screen the pool of the candidate instancesbased on some criterion of future usefulness.In many cases, that criterion is to improve thepresence of the rare classes in the data to beannotated.
We propose a novel method forsolving this problem and show that it com-pares favorably to a random sampling baselineand a clustering algorithm.1 IntroductionA data set is imbalanced when the distributionof classes in it is dominated by a single class.
InWord Sense Disambiguation (WSD), the classesare word senses.
The problem of imbalanced datais painfully familiar to WSD researchers: wordsenses are particularly well known for their skeweddistributions that are also highly domain and cor-pus dependent.
Most polysemous words have asense that occurs in a disproportionately highnumber of cases and another sense that is seen veryinfrequently.
For example, the OntoNotes (Hovy etal., 2006) sense inventory defines two senses forthe verb to add.
Of all the instances of this verb inthe OntoNotes sense-tagged corpus, 93% are theinstances of the predominant sense (not the arith-metic sense!).
Another fact: there are 4,554 totalsenses in the OntoNotes sense inventory for 1,713recently released verbs.
Only 3,498 of them arepresent in the actual annotated data.
More than1,000 senses (23%) are so rare that they are miss-ing from the corpus altogether.
More than a thirdof the released verbs are missing representativeinstances of at least one sense.
In fact many of theverbs are pseudo-monosemous: even though thesense inventory defines multiple senses, only themost frequent sense is present in the actual anno-tated data.
For example, only 1 out of 8 senses ofto rip is present in the data.The skewed nature of sense distributions is afact of life.
At the same time, a large-scale annota-tion project like OntoNotes, whose goal is the crea-tion of a comprehensive linguistic resource, cannotsimply ignore it.
That a sense is rare in a corpusdoes not mean that it is less important to annotate asufficient number of instances of that sense: in adifferent domain it can be more common and nothaving enough annotated instances of that sensecould jeopardize the success of an automatic cross-domain WSD system.
For example, sense 8 of torip ("to import an audio file directly from CD") isextremely popular on the web but it does not existat all in the OntoNotes data.
Only the traditionalsense of to swap exists in the data but not the com-puter science sense ("to move a piece of programinto memory"), while the latter can conceivably besignificantly more popular in technical domains.In general, class imbalance complicates super-vised learning.
This contention certainly holds forWSD.
As an illustration, consider the verb to call,for which the OntoNotes sense inventory defines11 senses.
Senses 3 and 5 are the most frequent:together they constitute 84% of the data.
To inves-tigate which classes are problematic for a classifi-25er, we conducted 50 supervised learning experi-ments.
In each experiment one instance of this verbwas selected at random and used for testing whilethe rest was used for training a maximum entropymodel.
The resulting confusion matrix shows thatthe model correctly classified most of the instancesof the two predominant senses while misclassify-ing the other classes.
The vast majority of the er-rors came from confusing other senses with sense 5which is the most frequent sense of to call.
Clearly,the data imbalance problem has a significant nega-tive effect on performance.Let us now envision the following realistic sce-nario: An annotation project receives funds tosense-tag a set of verbs in a corpus.
It may be thecase that some annotated data is already availablefor these verbs and the goal is to improve sensecoverage, or no annotated data is available at all.But it turns out there are only enough funds to an-notate a portion (e.g.
half) of the total instances.The question arises how to pre-select the instancesfrom the corpus in a way that would ensure that allthe senses are as well represented as possible.
Be-cause some senses of these verbs are very rare, thepool of instances pre-selected for the annotationshould include as many as possible instances of therare senses.
Random sampling ?
the simplest ap-proach ?
will clearly not work: the pre-selecteddata will contain roughly the same proportion ofthe rare sense instances as the original set.If random sampling is not the answer, the datamust be selected in some non-uniform way, i.e.using selective sampling.
Active learning (e.g.Chen et al, 2006) is one approach to this problem.Some evidence is available (Zhu and Hovy, 2007)that active learning outperforms random samplingin finding the instances of rare senses.
However,active learning has several shortcomings: (1) it re-quires some annotated data to start the process; (2)it is problematic when the initial training set onlycontains the data for a single class (e.g.
the pseudo-monosemous verbs); (3) it is not always efficient inpractice: In the OntoNotes project, the data is an-notated by two human taggers and the disagree-ments are adjudicated by the third.
In classic activelearning a single instance is labeled on each itera-tion  This means the human taggers would have towait on each other to tag the instance, on the adju-dicator for the resolution of a possible disagree-ment, and finally on the system which still needs tobe-retrained to select the next instance to be la-beled, a time sink much greater than tagging addi-tional instances; (4) finally, active learning maynot be an option if the data selected needs to bemanually pre-processed (e.g.
sentence segmented,tokenized, and treebanked ?
as was the case withsome of the OntoNotes data).
In this setting, oneach iteration of the algorithm, the taggers have toalso wait for the selected instance to be manuallypre-processed before they can label it.Thus, it would be significantly more convenientif all the data to be annotated could be pre-selectedin advance.
In this paper we turn to two unsuper-vised methods which have the potential to achievethat goal.
We propose a simple language modeling-based sampling method (abbreviated as LMS) thatincreases the likelihood of seeing rare senses in thepre-selected data.
The basic approach is as follow:using language modeling we can rank the instancesof the ambiguous verb according to their probabili-ty of occurrence in the corpus.
Because the in-stances of the rare senses are less frequent than theinstances of the predominant sense, we can expectthat there will be a higher than usual concentrationof the rare sense instances among the instances thathave low probabilities.
The method is completelyunsupervised and the only resource that it requiresis a Language Modeling toolkit such as SRILM(Stolcke, 2002), which we used in our experiments.We compare this method with a random samplingbaseline and semi-supervised clustering, which canserve the same purpose.
We show that our methodoutperforms both of the competing approaches.
Wereview the relevant literature in section 2, explainthe details of LMS in section 3, evaluate LMS insection 4, discuss the results in section 5, and de-scribe our plans for future work in section 6.2 Relevant WorkThe problem of imbalanced data has recently re-ceived much attention in the machine learningcommunity.
Rare classes can be of higher impor-tance than frequent classes, as in medical diagnosiswhen one is interested in correctly identifying arare disease.
Network intrusion detection faces asimilar problem: a malicious activity, although ofcrucial importance, is a very rare event comparedto the large volumes of routine network traffic.
Atthe same time, imbalanced data poses difficultiesfor an automatic learner in that rare classes have amuch higher misclassification rate than common26ones (Weiss, 1995; Japkowicz, 2001).
Learningfrom imbalanced sets can also be problematic if thedata is noisy: given a sufficiently high level ofbackground noise, a learner may not distinguishbetween true exceptions (i.e.
rare cases) and noise(Kubat and Matwin, 1997; Weiss, 2004).In the realm of supervised learning, cost-sensitive learning has been recommended as a so-lution to the problem of learning from imbalanceddata (e.g.
Weiss, 2004).
However, the costs of mis-classifying the senses are highly domain specificand hard to estimate.
Several studies recently ap-peared that attempted to apply active learning prin-ciples to rare category detection (Pelleg andMoore, 2004; He and Carbonell, 2007).
In additionto the issues with active learning outlined in theintroduction, the algorithm described in (He andCarbonell, 2007) requires the knowledge of thepriors, which is hard to obtain for word senses.WSD has a long history of experiments withunsupervised learning (e.g.
Schutze, 1998; Puran-dare and Peterson, 2004).
McCarthy et al (2004)propose a method for automatically identifying thepredominant sense in a given domain.
Erk (2006)describes an application of an outlier detection al-gorithm to the task of identifying the instances ofunknown senses.
Our task differs from the lattertwo works in that it is aimed at finding the in-stances of the rare senses.Finally, the idea of LMS is similar to the tech-niques for sentence selection based on rare n-gramco-occurrences used in machine translation (Eck etal., 2005) and syntactic parsing (Hwa, 2004).3 Language Modeling for Data SelectionOur method is outlined in Figure 1:InputA large corpus that contains T candidate instancesfrom which S instances are to be selected for anno-tationBasic Steps1.
Compute the language model for the corpus2.
Compute the probability distribution over the Tcandidate instances of the target verb3.
Rank the T candidate instances by their proba-bilities4.
Form a cluster by selecting S instances with thelowest probabilityFigure 1.
Basic steps of LMSLet us now clarify a few practical points.
Al-though an instance of the target verb can berepresented as the entire sentence containing theverb, from the experiments with automatic WSD(e.g.
Dligach and Palmer, 2008), it is known thathaving access to just a few words in the neighbor-hood of the target verb is sufficient in many casesto predict the sense.
For the purpose of LMS werepresent an instance as the chunk of text centeredupon the target verb plus the surrounding words onboth sides within a three-word window.
Althoughthe size of the window around the target verb isfixed, the actual number of words in each chunkmay vary when the target verb is close to the be-ginning or the end of sentence.
Therefore, we needsome form of length normalization.
We normalizethe log probability of each chunk by the actualnumber of words to make sure we do not favorshorter chunks (SRILM operates in log space).
Theresulting metric is related to perplexity: for a se-quence of words W = w1w2 ?
wN  the perplexity isNNwwwPWPP121 )...()(?=The log of perplexity is)]...(log[1)](log[ 21 NwwwPNWPP ?=Thus, the quantity we use for ranking is nega-tive perplexity.4 EvaluationFor the evaluation, we selected two-sense verbsfrom the OntoNotes data that have at least 100 in-stances and where the share of the rare sense is lessthan 20%.
There were 11 such verbs (2,230 in-stances total) with the average share of the raresense 11%.Our task consists of clustering the instances of averb into two clusters, one of which is expected tohave a higher concentration of the rare senses thanthe other.
Since the rare sense cluster is of primaryinterest to us, we report two metrics: (1) precision:the ratio of the number of instances of the raresense in the cluster and the total number of in-stances in the cluster; (2) recall: the ratio of thenumber of instances of the rare sense in the clusterand the total number of the rare sense instances inboth clusters.
Note that precision is not of primaryimportance for this task because the goal is not toreliably identify the instances of the rare sense but27rather to group them into a cluster where the raresenses will have a higher concentration than in theoriginal set of the candidate instances.
At the sametime achieving high recall is important since wewant to ensure that most, if not all, of the raresenses that were present among the candidate in-stances are captured in the rare sense cluster.4.1 Plausibility of LMSThe goal of our first set of experiments is to illu-strate the plausibility of LMS.
Due to space con-straints, we examine only two verbs: compare andadd.
The remaining experiments will focus on amore comprehensive evaluation that will involveall 11 verbs.
We computed the normalized logprobability for each instance of a verb.
We thenordered these candidate instances by their norma-lized log probability and computed the recall of therare sense at various levels of the size of the raresense cluster.
We express the size of the rare sensecluster as a share of the total number of instances.We depict recall vs. cluster size with a dottedcurve.
The graphs are in Figures 2 and 3.Figure 2.
Rare sense recall for compareFigure 3.
Rare sense recall for addThe diagonal line on these figures correspondsto the random sampling baseline.
A successfulLMS would correspond to the dotted curve lyingabove the random sampling baseline, which hap-pens to be the case for both of these verbs.
Forcompare we can capture all of the rare sense in-stances in a cluster containing less than half of thecandidate instances.
While verbs like compare re-flect the best-case scenario, the technique we pro-posed still works for the other verbs although notalways as well.
For example, for add we can recallmore than 70% of the rare sense instances in acluster that contains only half of all instances.
Thisis more than 20 percentage points better than therandom sampling baseline where the recall of therare sense instances would be approximately 50%.4.2 LMS vs. Random Sampling BaselineIn this experiment we evaluated the performanceof LMS for all 11 verbs.
For each verb, we rankedthe instances by their normalized log probabilityand placed the bottom half in the rare sense cluster.The results are in Table 2.
The second columnshows the share of the rare sense instances in theentire corpus for each verb.
Thus, it represents theprecision that would be obtained by random sam-pling.
The recall for random sampling in this set-ting would be 0.5.Ten verbs outperformed the random samplingbaseline both with respect to precision and recall(although recall is much more important for thistask) and one verb performed as well.
On averagethese verbs showed a recall figure that was 22 per-centage points better than random sampling.
Twoof the 11 verbs (compare and point) were able torecall all of the rare sense instances.Verb Rare Inst Precision Recallaccount 0.12 0.21 0.93add 0.07 0.10 0.73admit 0.18 0.18 0.50allow 0.06 0.07 0.62compare 0.08 0.16 1.00explain 0.10 0.12 0.60maintain 0.11 0.11 0.53point 0.15 0.29 1.00receive 0.07 0.08 0.60remain 0.15 0.20 0.65worry 0.15 0.22 0.73average 0.11 0.16 0.72Table 2.
LMS results for 11 verbs284.3 LMS vs. K-means ClusteringSince LMS is a form of clustering one way to eva-luate its performance is by comparing it with anestablished clustering algorithm such as K-means(Hastie et al, 2001).
There are several issues re-lated to this evaluation.
First, K-means producesclusters and which cluster represents which class isa moot question.
Since for the purpose of the eval-uation we need to know which cluster is mostclosely associated with a rare sense, we turn K-means into a semi-supervised algorithm by seedingthe clusters.
This puts LMS at a slight disadvan-tage since LMS is a completely unsupervised algo-rithm, while the new version of K-means willrequire an annotated instance of each sense.
How-ever, this disadvantage is not very significant: in areal-world application, the examples from a dictio-nary can be used to seed the clusters.
For the pur-pose of this experiment, we simulated theexamples from a dictionary by simply taking theseeds from the pool of the annotated instances weidentified for the evaluation.
K-means is known tobe highly sensitive to the choice of the initialseeds.
Therefore, to make the comparison fair, weperform the clustering ten times and pick the seedsat random for each iteration.
The results are aver-aged.Second, K-means generates clusters of a fixedsize while the size of the LMS-produced clusterscan be easily varied.
This advantage of the LMSmethod has to be sacrificed to compare its perfor-mance to K-means.
We compare LMS to K-meansby counting the number of instances that K-meansplaced in the cluster that represents the rare senseand selecting the same number of instances thathave the lowest normalized probability.
Thus, weend up with the two methods producing clusters ofthe same size (with k-means dictating the clustersize).Third, K-means operates on vectors and there-fore the instances of the target verb need to berepresented as vectors.
We replicate lexical, syn-tactic, and semantic features from a verb sense dis-ambiguation system that showed state-of-the-artperformance on the OntoNotes data (Dligach andPalmer, 2008).The results of the performance comparison areshown in Table 3.
The fourth column shows therelative size of the K-means cluster that wasseeded with the rare sense.
Therefore it also de-fines the share of the instances with the lowestnormalized log probability that are to be includedin the LMS-produced rare sense clusters.
On aver-age, LMS showed 3% better recall than K-meansclustering.K-means LMSverb precision recall size precision recallaccount 0.21 1.00 0.58 0.20 1.00add 0.06 0.54 0.50 0.10 0.73admit 0.21 0.31 0.29 0.09 0.15allow 0.08 0.36 0.31 0.06 0.31compare 0.22 0.42 0.18 0.19 0.43explain 0.16 0.61 0.44 0.14 0.60maintain 0.13 0.91 0.80 0.11 0.82point 0.27 0.66 0.42 0.31 0.89receive 0.11 0.68 0.72 0.08 0.80remain 0.10 0.41 0.44 0.21 0.61worry 0.81 0.51 0.13 0.38 0.33average 0.21 0.58 0.44 0.17 0.61Table 3.
LMS vs. K-means5 Discussion and ConclusionIn this paper we proposed a novel method wetermed LMS for pre-selecting instances for annota-tion.
This method is based on computing the prob-ability distribution over the instances and selectingthe ones that have the lowest probability.
The ex-pectation is that instances selected in this fashionwill capture more of the instances of the rareclasses than would have been captured by randomsampling.
We evaluated LMS by comparing it torandom sampling and showed that LMS outper-forms it.
We also demonstrated that LMS com-pares favorably to K-means clustering.
This isdespite the fact that the cluster sizes were dictatedby K-means and that K-means had at its disposalmuch richer linguistic representations and someannotated data.Thus, we conclude that LMS is a promising me-thod for data selection.
It is simple to use since oneonly needs the basic functionality that any lan-guage modeling toolkit offers.
It is flexible in thatthe number of the instances to be selected can bespecified by the user, unlike, for example, whenclustering using k-means.296 Future WorkFirst, we would like to investigate the effect of se-lective sampling methods (including LMS) on theperformance of WSD models learned from the se-lected data.
Next, we plan to apply LMS for Do-main adaptation.
Unlike the scenario we dealt within this paper, the language model would have to belearned from and applied to different corpora: itwould be trained on the source corpus and used tocompute probabilities for the instances in the targetcorpus that needs to be adapted.
We will also expe-riment with various outlier detection techniques todetermine their applicability to data selection.Another promising direction is a simplified activelearning approach in which a classifier is trainedon the labeled data and applied to unlabeled data;the instances with a low classifier's confidence areselected for annotation (i.e.
this is active learningconducted over a single iteration).
This approach ismore practical than the standard active learning forthe reasons mentioned in Section 1 and should becompared to LMS.
Finally, we will explore theutility of LMS-selected data as the initial trainingset for active learning (especially in the cases ofthe pseudo-monosemous verbs).AcknowledgmentsWe gratefully acknowledge the support of the Na-tional Science Foundation Grant NSF-0715078,Consistent Criteria for Word Sense Disambigua-tion, and the GALE program of the Defense Ad-vanced Research Projects Agency, Contract No.HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.
Any opinions, findings, and con-clusions or recommendations expressed in this ma-terial are those of the authors and do notnecessarily reflect the views of the NationalScience Foundation.ReferencesJinying Chen, Andrew Schein, Lyle Ungar, and MarthaPalmer.
2006.
An Empirical Study of the Behavior ofActive Learning for Word Sense Disambiguation.
InProceedings of the HLT-NAACL.Dmitriy Dligach and Martha Palmer.
2008.
Novel Se-mantic Features for Verb Sense Disambiguation.
InProceedings of ACL-HLT.Matthias Eck, Stephan Vogel, and Alex Waibel.
2005.Low Cost Portability for Statistical Machine Transla-tion Based on N-gram Frequency and TF-IDF.
Pro-ceedings of IWSLT 2005.Katrin Erk.
Unknown Word Sense Detection as OutlierDetection.
2006.
In Proceedings of HLT-NAACL.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.The Elements of Statistical Learning.
Data Mining,Inference, and Prediction.
2001.
Springer.Jingrui He and Jaime Carbonell.
2007.
Nearest-Neighbor-Based Active Learning for Rare CategoryDetection.
NIPS.Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L.Ramshaw, and R. Weischedel.
2006.
OntoNotes: The90% Solution.
In Proceedings of the HLT-NAACL.Eduard Hovy and Jingbo Zhu.
2007.
Active Learningfor Word Sense Disambiguation with Methods forAddressing the Class Imbalance Problem.
In Pro-ceedings of EMNLP.Rebecca Hwa.
2004.
Sample Selection for StatisticalParsing.
Computational Linguistics.
Volume 30.
Is-sue 3.Natalie Japkowicz.
2001.
Concept Learning in the Pres-ence of Between-Class and Within-Class Imbalances.Proceedings of the Fourteenth Conference of the Ca-nadian Society for Computational Studies of Intelli-gence, Springer-Verlag.Miroslav Kubat and Stan Matwin.
1997.
Addressing thecurse of imbalanced training sets: one-sided selec-tion.
In Proceedings of the Fourteenth InternationalConference on Machine Learning.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding Predominant Word Senses inUntagged Text.
In Proceedings of 42nd AnnualMeeting of Association for Computational Linguis-tics.Dan Pelleg and Andrew Moore.
2004.
Active Learningfor Anomaly and Rare-Category Detection.
NIPS.Amruta Purandare and Ted Pedersen.
Word Sense Dis-crimination by Clustering Contexts in Vector andSimilarity Spaces.
2004.
In Proceedings of the Con-ference on CoNLL.Hinrich Schutze.
1998 Automatic Word Sense Discrim-ination.
Computational Linguistics.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proc.
Intl.
Conf.
SpokenLanguage Processing, Denver, Colorado.Gary M. Weiss.
1995.
Learning with Rare Cases andSmall Disjuncts.
Proceedings of the Twelfth Interna-tional Conference on Machine Learning, MorganKaufmann.Gary M. Weiss.
2004.
Mining with Rarity: A UnifyingFramework.
SIGKDD Explorations, special issue onlearning from imbalanced datasets.30
