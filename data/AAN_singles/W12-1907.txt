NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 47?54,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Part of Speech Inference with Particle FiltersGregory Dubbin and Phil BlunsomDepartment of Computer ScienceUniversity of OxfordWolfson Building, Parks RoadOxford, OX1 3QD, United KingdomGregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.ukAbstractAs linguistic models incorporate more subtlenuances of language and its structure, stan-dard inference techniques can fall behind.
Of-ten, such models are tightly coupled such thatthey defy clever dynamic programming tricks.However, Sequential Monte Carlo (SMC) ap-proaches, i.e.
particle filters, are well suitedto approximating such models, resolving theirmulti-modal nature at the cost of generatingadditional samples.
We implement two par-ticle filters, which jointly sample either sen-tences or word types, and incorporate theminto a Gibbs sampler for part-of-speech (PoS)inference.
We analyze the behavior of the par-ticle filters, and compare them to a block sen-tence sampler, a local token sampler, and aheuristic sampler, which constrains inferenceto a single PoS per word type.
Our findingsshow that particle filters can closely approx-imate a difficult or even intractable samplerquickly.
However, we found that high poste-rior likelihood do not necessarily correspondto better Many-to-One accuracy.
The resultssuggest that the approach has potential andmore advanced particle filters are likely to leadto stronger performance.1 IntroductionModern research is steadily revealing more of thesubtle structure of natural language to create in-creasingly intricate models.
Many modern problemsin computational linguistics require or benefit frommodeling the long range correlations between latentvariables, e.g.
part of speech (PoS) induction (Lianget al, 2010), dependency parsing (Smith and Eis-ner, 2008), and coreference resolution (Denis andBaldridge, 2007).
These correlations make infer-ence difficult because they reflect the complicatedeffect variables have on each other in such tightlycoupled models.Sequential Monte Carlo (SMC) methods, like par-ticle filters, are particularly well suited to estimatingtightly coupled distributions (Andrieu et al, 2010).Particle filters sample sequences of latent variableassignments by concurrently generating several rep-resentative sequences consistent with a model?s con-ditional dependencies.
The sequential nature of thesampling simplifies inference by ignoring ambigu-ous correlations with unsampled variables at thecost of sampling the sequence multiple times.
Thefew applications of particle filters in computationallinguistics generally focus on the online nature ofSMC (Canini et al, 2009; Borschinger and John-son, 2011).
However, batch applications still benefitfrom the power of SMC to generate samples fromtightly coupled distributions that would otherwiseneed to be approximated.
Furthermore, the time costof the additional samples generated by SMC can bemitigated by generating them in parallel.This report presents an initial approach to the inte-gration of SMC and block sampling, sometimes ref-fered to as Particle Gibbs (PG) sampling (Andrieuet al, 2010).
Unsupervised PoS induction servesas a motivating example for future extensions toother problems.
Section 3 reviews the PYP-HMMmodel used for PoS inference.
Section 4 explains theSequential Importance Sampling (SIS) algorithm, abasic SMC method that generates samples for the47block sampler.
This approach yields two implemen-tations: a simple sentence-based block sampler (4.1)and a more complicated type-based sampler (4.2).Finally, section 5 evaluates both implementations ona variety of unsupervised PoS inference tasks, ana-lyzing the behavior of the SMC inference and com-paring them to state-of-the-art approaches.2 BackgroundSMC was introduced in 1993 as a Bayesian esti-mator for signal processing problems with strongnon-linear conditional dependencies (Gordon et al,1993).
Since then, SMC methods have been adoptedby many fields, including statistics, biology, eco-nomics, etc.
(Jasra et al, 2008; Beaumont, 2003;Fernandez-Villaverde and Rubio-Ramirez, 2007).The SMC approach is the probabilisitic analogue ofthe beam search heuristic, where the beam width canbe compared to the number of particles and pruningis analogous to resampling.The basic SMC approach serves as the basis forseveral variants.
Many SMC implementations re-sample the population of particles to create a newpopulation that minimizes the effect of increasingsample variance with increasing sequence length(Kitagawa, 1996).
Particle smoothing variants ofSMC reduce the relative variance of marginals earlyin the sequence, as well improving the diversityof the final sample (Fearnhead et al, 2008).
Par-ticle Markov chain Monte Carlo (PMCMC) for-mally augments classic Markov chain Monte Carlo(MCMC) approaches, like Gibbs sampling, withsamples generated by particle filters (Andrieu et al,2010).3 The PYP-HMMThe PYP-HMM model of PoS generation demon-strates the tightly coupled correlations that com-plicate many standard inference methods (Blunsomand Cohn, 2011).
The model applies a hierarchicalPitman-Yor process (PYP) prior to a trigram hiddenMarkov model (HMM) to jointly model the distri-bution of a sequence of latent word classes, t, andword tokens, w. This model performs well on cor-pora in multiple languages, but the lack of a closedform solution for the sample probabilities makes it astrong canditate for PG sampling.
The joint proba-bility defined by a trigram HMM isP?
(t,w) =N+1?n=1P?
(tl|tn?1, tn?2)P?
(wn|tn)where N = |t| = |w| and the special tag $ is addedto the boundaries on the sentence.
The model de-fines transition and emission distributions,tn|tn?1, tn?2, T ?
Ttn?1,tn?2wn|tn, E ?
EtnThe PYP-HMM smoothes these distributions by ap-plying hierarchical PYP priors to them.
The hierar-chical PYP describes a back-off path of simpler PYPpriors,Tij |aT , bT , Bi ?
PYP(aT , bT , Bi)Bi|aB, bB, U ?
PYP(aB, bb, U)U |aU , bU ?
PYP(aU , bU ,Uniform).Ei|aE , bE , C ?
PYP(aE , bE , Ci),where Tij , Bi, and U are trigram, bigram, and un-igram transition distributions respectively and Ci iseither a uniform distribution (PYP-HMM) or a bi-gram character language model emission distribu-tion (PYP-HMM+LM, intended to model basic mor-phology).Draws from the posterior of the hierarchicalPYP can be calculated with a variant of the Chi-nese Restaraunt Process (CRP) called the ChineseRestaurant Franchise (CRF) (Teh, 2006; Goldwateret al, 2006).
In the CRP analogy, each latent vari-able in a sequence is represented by a customer en-tering a restaurant and sitting at one of an infinitenumber of tables.
A customer chooses to sit at a ta-ble in a restaurant according to the probabilityP (zn = k|z1:n?1) ={c?k ?an?1+b 1 ?
k ?
K?K?a+bn?1+b k = K?
+ 1(1)where zn is the index of the table chosen by the nthcustomer to the restaurant, z1:n?1 is the seating ar-rangement of the previous n?
1 customers to enter,c?k is the count of the customers at table k, and K?is the total number of tables chosen by the previ-ous n?
1 customers.
All customers at a table sharethe same dish, representing the value assigned to the48latent variables.
When customers sit at an empty ta-ble, a new dish is assigned to that table according tothe base distribution of the PYP.
To expand the CRPanalogy to the CRF for hierarchical PYPs, when acustomer sits at a new table, a new customer entersthe restaurant representing the PYP of the base dis-tribution.4 Sequential Monte CarloWhile MCMC approximates a distribution as the av-erage of a sequence of samples taken from the poste-rior of the distribution, SMC approximates a distri-bution as the importance weighted sum of several se-quentially generated samples, called particles.
Thisarticle describes two SMC samplers that jointly sam-ple multiple tag assignments: a sentence based blocksampler (sent) and a word type based block sam-pler (type).
The basics of particle filtering are out-lined below, while the implementation specifics ofthe sent and type particle filters are described insecions 4.1 and 4.2, respectively.SMC is essentially the probabilistic analogue ofthe beam search heuristic.
SMC stores P sequences,analogous to beam width, and extends each incre-mentally according to a proposal distribution qn,similar to the heuristic cost function in beam search.Many particle filtering implementations also includea resampling step which acts like pruning by reduc-ing the number of unlikely sequences.We implemented Sequential ImportanceSampling (SIS), detailed by Doucet and Jo-hansen (2009), to approximate joint samplesfrom the sentence and word type distributions.This approach approximates a target distribution,pin(x1:n) =?n(x1:n)Zn, of the sequence, x1:n, of nrandom variables, that is ?n(x1:n) calculates theunnormalized density of x1:n.SIS initilizes each particle p ?
[1, P ] by samplingfrom the initial proposal distribution q1(xp1), wherexpn is the value assigned to the n-th latent variable forparticle p. The algorithm then sequentially extendseach particle according to the conditional proposaldistribution qn(xpn|xp1:n), where xp1:n is the sequenceof values assigned to the first n latent variables inparticle p. After extending a particle p, SIS updatesthe importance weight ?pn = ?pn?1 ?
?n(xp1:n).
Theweight update, defined as?n(x1:n) =?n(x1:n)?n?1(x1:n?1)qn(xn|x1:n?1), (2)accounts for the discrepancy between the proposaldistribution, qn, and the target distribution, pin,without normalizing over x1:n, which becomes in-tractable for longer sequences even in discretedomains.
The normalizing constant of the tar-get distribution is approximately Zn ?
?Pp=1 ?pnand the unnormalized density is ?n(x1:n) ?
?Pp=1 ?pnifxp1:n = x1:n. The particles can also beused to generate an unbiased sample from pin bychoosing a particle p proportional to its weight ?pn.Andrieu et al (2010) shows that to ensure thesamples generated by SMC for a Gibbs sampler hasthe target distribution as the invariant density, theparticle filter must be modified to perform a con-ditional SMC update.
This means that the particlefilter guarantees that one of the final particles is as-signed the same values as the previous Gibbs iter-ation.
Our implementation of the conditional SMCupdate reserves one special particle, 0, for which theproposal distribution always chooses the previous it-eration?s value at that site.4.1 Sentence SamplingThe sent particle filter samples blocks of tag as-signments tS1:n for a sentence, S, composed of to-kens, wS1:n. Sampling an entire sentence minimizesthe risk of assigning a tag with a high probabil-ity given its local context but minimal probabilitygiven the entire sentence.
Sentences can be sampledby ignoring table counts while sampling a proposalsentence, incorporating them after the fact with aMetropolis-Hastings acceptance test (Gao and John-son, 2008).
The Metropolis-Hastings step simplifiesthe sentence block particle filter further by not re-quiring the conditional SMC update.While there is already a tractable dynamic pro-gramming approach to sampling an entire sentencebased on the Forward-Backward algorithm, parti-cle filtering the sentences PYP-HMM model shouldprove beneficial.
For the trigram HMM definedby the model, the forward-backward sampling ap-proach has time complexity in O(NT 3) for a sen-tence of length N with T possible tag assignmentsat each site.
Particle filters with P particles can ap-proximate these samples in O(NTP ) time, which49becomes much faster as the number of tags, T , in-creases.Sampling of sentence S begins by removing allof the transitions and emitions in S from the tablecounts, z, resulting in the table counts z?S of tag as-signments t?S the values assigned to the variablesoutside of S. For each site index n ?
[1, N ] in thesentence, the particle filter chooses the new tag as-signment, tS,pn , for each particle p ?
[1, P ] from thesentence proposal distribution,qSn (tS,pn |tS,p1:n?1) ?
P (tS,pn |tS,pn?2, tS,pn?1, t?S , z?S)?
P (wS,pn |tS,pn , t?S , z?S ,w?S).After each new tag is assigned, the particle?s weightis updated according to equation (2).
The simplic-ity of the proposal density hints at the advantage ofparticle filtering over forward-backward sampling:it tracks only P histories and their weights ratherthan tracking the probability of over all possible his-tories.
Once each particle has assigned a value toeach site in the sentence, one tag sequence is chosenproportional to its particle weight, ?S,pN .4.2 Type SamplingThe type sampling case for the PYP-HMM is morecomplicated than the sent sampler.
The long-rangecouplings defined by the hierarchical PYP priorsstrongly influence the joint distribution of tags as-signed to tokens of the same word type (Liang etal., 2010).
Therefore, the affects of the seating de-cisions of new customers cannot be postponed dur-ing filtering as in sentence sampling.
To accountfor this, the type particle filter samples sequencesof seating arrangements and tag assignments jointly,xW1:n = (tW1:n, zW1:n), for the word-type, W .
The fi-nal table counts are resampled once a tag assignmenthas been chosen from the particles.Tracking the seating arrangement history for eachparticle adds an additional complication to the typeparticle filter.
The exchangeability of seating deci-sions means that only counts of customers are nec-essary to represent the history.
Each particle repre-sents both a tag sequence, tW,p1:n , and the count deltas,zW,p1:n .
The count deltas of each particle are stored in ahash table that maps a dish in one of the CRF restau-rants to the number of tables serving that dish andthe total number of customers seated at those tables.The count delta hash table ensures that it has suffi-cient data to calculate the correct probabilities (perequation (1)) by storing any counts that are differentfrom the base counts, z?W , and defering to the basecounts for any counts it does not have stored.At each token occurence n, the next tag assign-ment, tW,pn for each particle p ?
[1, P ] is chosen firstaccording to the word type proposal distributionqWn (tW,pn |tW,p1:n?1, zW,p1:n?1) ?P (tW,pn |c?2n , c?1n , t?W,p1:n?1, z?W,p1:n?1)?
P (c+1n |c?1n , tW,pn , t?W,p1:n?1, z?W,p1:n?1)?
P (c+2n |tW,pn , c+1n , t?W,p1:n?1, z?W,p1:n?1)?
P (wWn |tW,pn , t?W,p1:n?1, z?W,p1:n?1,w?W,p1:n?1).In this case, c?kn represents a tag in the contextof site tWn offset by k, while t?W,p1:n?1, zW,p1:n?1, andw?W,p1:n?1 represent the tag assignments, table counts,and word token values chosen by particle p as wellas the values at all of the sites where a word tokenof type W does not appear.
This proposal distribu-tion ignores changes to the seating arrangement be-tween the three transitions involving the site n. Thespecific seating arrangement of a particle is chosenafter the tag choice, at which point the weights areupdated by the result of equation (2).
As with thesent sampler, once all of the particles have beensampled, one of them is sampled with probabilityproportional to its weight.
This final sample is asample from the true target probability.As mentioned earlier, the sequence of particle ap-proximations do not have the target distribution asinvariant unless they use the conditional SMC up-date.
Therefore, a the special 0 particle is automat-ically assigned the value from the prior iteration ofthe Gibbs sampler at each site n, though the proposalprobability qWn (tW,0n |tW,p1:n?1, zW,p1:n?1) still has to becalculated to update the weight ?W,pn properly.
Thisensures that the type sampler has a non-zero prob-ability of reverting to the prior iteration?s sequence.5 Experiments and ResultsWe take two approaches to evaluating the SMCbased samplers.
The first approach is an analysis ofthe samplers as inference algorithms.
The samplersshould tend to maximize the posterior likelihood ofthe model over iterations, eventually converging to50the mode.
Section 5.1 analyzes the particle filterbased samplers with various numbers of particles inan effort to understand how they behave.Then, section 5.2 evaluates each of the proposedapproaches on PoS inference tasks from several lan-guages.
These results allow a practical comparisonwith other PoS inference approaches.5.1 SMC AnalysisBefore comparing the performance of the SMCblock samplers to other inference methods, we wishto learn more about the approaches themselves.
Itis not clear how well the benefits of block samplingtransfer to SMC based approaches.
Both the sentand type samplers are novel approaches to com-putational linguistics, and many of their propertiesare unclear.
For example, the samples generatedfrom the particle filter should have a higher vari-ance than the target distribution.
If the variance istoo high, the sampler will be slower to converge.While additional particles lower the relative vari-ance, they also increase the run time linearly.
It ispossible that there is a threshold of particles nec-essary to ensure that some are high likelihood se-quences, beyond which inference gains are minimalthe additional computational expense is wasted.
Allof the experiments in this section were run on theArabic corpus from the CoNLL-X shared languagetask, which is small enough to quickly experimentwith these issues (Buchholz and Marsi, 2006).The sentence based sampler, sent, samples froma distribution that can be exactly computed, facilitat-ing comparisons between the exact sampler and theSMC approach.
Figure 5.1 compares the posteriorlog-likelihoods of the sent sampler and the exactsentence sampler over 200 iterations.
As expected,the likelihoods of the particle filters approach that ofthe exact sentence sampler as the number of particlesincreases from 25 to 100, which completely overlapsthe performance of the exact sampler by the 50th it-eration.
This is impressive, because even with 99additional sequences sampled (one for each particle)each iteration the SMC approach is still faster thanthe exact sampler.
Furthermore, the Arabic tagsethas only 20 distinct tags, while other data sets, e.g.the WSJ and Bulgarian, use tagsets more than twiceas large.
The particle filter, which is linear in thenumber of tags, should take twice as long per token0 50 100 150 200Iteration6.66.46.26.05.85.65.45.25.04.8Log-Likelihood1e5Sent: ExactSent: 25 ParticlesSent: 50 ParticlesSent: 100 ParticlesLog-Likelihood of Sentence Samplers over 200 IterationsFigure 1: Posterior Log-Likelihood of PYP-HMM infer-ence with exact as well as SMC sentence sampler withvarious numbers of particles.
Error bars represent onstandard deviation over three runs.sampled on those data, relative to the arabic data.On the other hand, the forward-backward per tokensample time, which is cubic in tagset size, shouldincrease at least eightfold.
So the time savings im-prove dramatically as the size of the tagset increases.Figure 2 compares the table configuration log-likelihood of the 1HMM approximation imple-mented by Blunsom and Cohn (2011) with the typeparticle filter based sampler as well as the local,token-based sampler and the exact block sentencesampler.
Unlike the sentence based block sampler,type sampler cannot be exactly calculated, evenwith the 1HMM approach of constraining inferenceto only consider sequences that assign the same tagto every token of the same word type.
The 1HMMsampler approximates these probabilities using ex-pected table counts.
Theoretically, the type sam-pler should be a better approximation, being guar-anteed to approach the true distribution as the num-ber of particles increases.
However, the type sam-pler does not constrain inference as the 1HMM does,slowing convergence by wasting particles on lesslikely tag sequences.
As expected, the type sam-pler converges with the 1HMM sampler with suffi-ciently many particles in a few iterations.
The exactblock sentence sampler surpases approaches by iter-ation 75 and does not seem to have converged by theend of 200 iterations.The local sampler samples a single site at a timewith a standard, token-based Gibbs sampler.
The510 50 100 150 200Iteration6.66.46.26.05.85.65.45.25.04.8Log-Likelihood1e51HMMLocalSent: ExactType: 100 ParticlesLog-Likelihood Comparison of Samplers over 200 IterationsFigure 2: Posterior Log-Likelihood of PYP-HMM infer-ence with particle filters, 1HMM approximation, and lo-cal samplers.
Error bars represent one standard deviationover three runs.local sampler performs surprisingly well, averaginga slightly higher likelihood than both the 1HMMand the type samplers.
This may be an indicationthat the PYP-HMM model is not too tightly coupledfor the local sampler to eventually migrate towardmore likely modes.
Note that both the type and1HMM samplers initially take much larger steps,before eventually hitting a plateau.
This suggeststhat some sort of mixed sampler may outperformits component samplers by only occasionally takinglarge steps.5.2 Unsupervised Part-of-Speech TaggingThe samplers evaluated in section 5.1 induce syn-tactic categories analogous to PoS tags.
However,to induce PoS tags, each syntactic category must beassigned to a PoS tag in the tagset.
Each site in acorpus is assigned the most commonly visited syn-tactic category at that site over all iterations.
Themany-to-one (M-1) assignment for a category is themost common gold standard PoS tag of the tokensassigned to that category.
While this assignmenttheoretically allows a perfect accuracy if each tokenis assigned to its own category, these experimentslimit the number of induced categories to the size ofthe tagset.
Table 1 compares the M-1 accuracy ofthe sent and type particle filter samplers, fromsections 4.1 and 4.2, with 100 particles each.
Theparticle filter based samplers rarely score a higheraccuracy than even the local sampler, which com-pletes 500 iterations before the particle filters com-plete 200.While figure 2 shows that the sentence basedblock sampler eventually surpasses the 1HMM sam-pler in likelihood, the accuracies of the 1HMM and1HMM-LM approximations remain well above theother approaches.
The 1HMM sampler and the 100particle type sampler have approximately the samelikelihood, yet the M-1 accuracy of the 1HMM sam-pler is much higher.
This suggests that there arehigh-likelihood assignments that produce lower ac-curacy results, presumably related to the fact that thetype sampler is not restricted to assignments withexactly one tag for each word type.
If the modelassigns equal likelihood to these assignments, infer-ence will not be able to distinguish between them.Perhaps a model that assigned higher likelihoods totag sequences with fewer tags per word type wouldhave a stronger correlation between likelihood andaccuracy.6 Future WorkWhile the results leave much room for improvement,the approach presented here is the most basic of par-ticle methods.
There has been considerable researchin improvements to particle methods since their in-troduction in 1993 (Gordon et al, 1993).
Two com-mon approaches to improving particle filters are re-sampling and particle smoothing (Doucet and Jo-hansen, 2009; Godsill and Clapp, 2001; Pitt, 2002).Resampling ensures that particles aren?t wasted onunlikely sequences.
Particle smoothing reduces thevariability of the marginal distributions by combin-ing the final particles.7 ConclusionThis paper presented a preliminary approach to in-corporating particle methods into computational lin-guistic inference applications.
Such approachesshow great potential for inference even in highly de-pendent distributions, but at a serious computationalcost.
However, the type particle filter itself can belargely run in parallel, only bottlenecking when theparticle weights need to be normalized.
Further ex-pansion of the basic ideas presented will enable scal-able inference in otherwise intractable models.52Language Sent-100 Type-100 Local 1HMM 1HMM-LM Tokens Tag typesWSJ 69.8% 70.1% 70.2% 75.6% 77.5% 1,173,766 45Arabic 53.5% 57.6% 56.2% 61.9% 62.0% 54,379 20Bulgarian 64.8% 67.8% 67.6% 71.4% 76.2% 190,217 54Czech 59.8% 61.6% 64.5% 65.4% 67.9% 1,249,408 12cDanish 65.0% 70.3% 69.1% 70.6% 74.6% 94,386 25Dutch 61.6% 71.6% 64.1% 73.2% 72.9% 195,069 13cHungarian 61.8% 61.8% 64.8% 69.6% 73.2% 131,799 43Portuguese 59.4% 71.1% 68.1% 72.0% 77.1% 206,678 22Table 1: Many-to-1 accuracies on CoNLL and Penn-Treebank Wall Street Journal corpora for sentence- (Sent) andtype- (Type) based filtering.
The table lists the average M-1 accuracy measured according to the maximum marginaltag assignments over 3 seperate runs after 200 iterations for the sent, type, 1HMM and 1HMM-LM samplers,and 500 iterations for the HMM local sampler.
The 1HMM-LM model has been shown to achieve state-of-the-artunsupervised M-1 accuracies on these datasets.
and thus represents the limit of unsupervised M-1 accuracy.ReferencesChristophe Andrieu, Arnaud Doucet, and Roman Holen-stein.
2010.
Particle markov chain monte carlo meth-ods.
Journal Of The Royal Statistical Society Series B,72(3):269?342.Mark A. Beaumont.
2003.
Estimation of PopulationGrowth or Decline in Genetically Monitored Popula-tions.
Genetics, 164(3):1139?1160, July.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised part ofspeech induction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 865?874,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Benjamin Borschinger and Mark Johnson.
2011.
A parti-cle filter algorithm for bayesian wordsegmentation.
InProceedings of the Australasian Language TechnologyAssociation Workshop 2011, pages 10?18, Canberra,Australia, December.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning, CoNLL-X ?06, pages 149?164, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Kevin R. Canini, Lei Shi, and Thomas L. Griffiths.
2009.Online inference of topics with latent Dirichlet aloca-tion.
In David van Dyk and Max Welling, editors, Pro-ceeings of the 12th International Conference on Arti-ficial Intelligence and Statistics (AISTATS*09), pages65?72.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Human Language Tech-nologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages236?243, Rochester, New York, April.
Association forComputational Linguistics.Arnaud Doucet and Adam M. Johansen, 2009.
A Tuto-rial on Particle Filtering and Smoothing: Fifteen YearsLater.
Oxford University Press.Paul Fearnhead, David Wyncoll, and Jonathan Tawn.2008.
A sequential smoothing algorithm with linearcomputational cost.
Technical report, Department ofMathematics and Statistics, Lancaster University.Jesus Fernandez-Villaverde and Juan F. Rubio-Ramirez.2007.
Estimating macroeconomic models: A like-lihood approach.
Review of Economic Studies,74(4):1059?1087, October.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofbayesian estimators for unsupervised hidden markovmodel pos taggers.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?08, pages 344?352, Morristown, NJ,USA.
Association for Computational Linguistics.Simon Godsill and Tim Clapp.
2001.
Improvementstrategies for monte carlo particle filters.
In A. Doucet,J.
F. G. de Freitas, and N.J. Gordon, editors, SE-QUENTIAL MONTE CARLO METHODS IN PRAC-TICE, pages 139?158.
Springer-Verlag.Sharon Goldwater, Tom Griffiths, and Mark John-son.
2006.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466.MIT Press, Cambridge, MA.N.
J. Gordon, D. J. Salmond, and A. F. M. Smith.
1993.Novel approach to nonlinear/non-Gaussian Bayesianstate estimation.
Radar and Signal Processing, IEEProceedings F, 140(2):107?113, April.A.
Jasra, A. Doucet, D. Stephens, and C. Holmes.
2008.Interacting sequential Monte Carlo samplers for trans-dimensional simulation.
Computational Statistics &Data Analysis, 52(4):1765?1791, January.53G Kitagawa.
1996.
Monte carlo filter and smoother fornon-gaussian nonlinear state space models.
Journal OfComputational And Graphical Statistics, 5(1):1?25.P.
Liang, M. I. Jordan, and D. Klein.
2010.
Type-basedMCMC.
In North American Association for Compu-tational Linguistics (NAACL).Michael K Pitt.
2002.
Smooth particle filters for likeli-hood evaluation and maximisation.
The Warwick Eco-nomics Research Paper Series (TWERPS) 651, Uni-versity of Warwick, Department of Economics, July.David A. Smith and Jason Eisner.
2008.
Depen-dency parsing by belief propagation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?08, pages 145?156,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Yee Whye Teh.
2006.
A hierarchical bayesian languagemodel based on pitman-yor processes.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, ACL-44, pages985?992, Morristown, NJ, USA.
Association for Com-putational Linguistics.54
