Automatic Selection of HPSG-Parsed Sentences forTreebank ConstructionMontserrat Marimon?Universitat de BarcelonaNu?ria Bel?
?Universitat Pompeu FabraLlu?
?s Padro?
?Universitat Polite`cnica de CatalunyaThis article presents an ensemble parse approach to detecting and selecting high-quality lin-guistic analyses output by a hand-crafted HPSG grammar of Spanish implemented in the LKBsystem.
The approach uses full agreement (i.e., exact syntactic match) along with a MaxEnt parseselection model and a statistical dependency parser trained on the same data.
The ultimate goalis to develop a hybrid corpus annotation methodology that combines fully automatic annotationand manual parse selection, in order to make the annotation task more efficient while maintaininghigh accuracy and the high degree of consistency necessary for any foreseen uses of a treebank.1.
IntroductionTreebanks constitute a crucial resource for theoretical linguistic investigations as wellas for NLP applications.
Thus, in the past decades, there has been increasing interest intheir construction and both theory-neutral and theory-grounded treebanks have beendeveloped for a great variety of languages.
Descriptions of available annotated corporacan be found in Abeille?
(2003) and in the proceedings from the annual editions of theInternational Workshop on Treebanks and Linguistic Theories.Quantity and quality are two very important objectives when building a treebank,but speed and low labor costs are also required.
In addition, guaranteeing consis-tency, that is, that the same phenomena receive the same annotation through the corpus,is crucial for any of the possible uses of the treebank.
The first attempts at treebankprojects used manual annotation mainly and devoted many hours of human laborto their construction.
Human annotation is not only slow and expensive, but it alsointroduces errors and inconsistencies because of the difficulty and tiring nature of the?
Gran Via de les Corts Catalanes 585, 08007-Barcelona.
E-mail: montserrat.marimon@ub.edu.??
Roc Boronat 138, 08018-Barcelona.
E-mail: nuria.bel@upf.edu.?
Jordi Girona 1-3, 08034-Barcelona.
E-mail: padro@lsi.upc.edu.Submission received: 16 October 2012; revised submission received: 20 October 2013; accepted for publication:5 December 2013.doi:10.1162/COLI a 00190?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 3task.1 Therefore, automating parts of the annotation process aims to leverage effective-ness, producing a larger number of high-quality and consistent analyses in shorter timeand using fewer resources.This article presents research that attempts to increase the degree of automation inthe annotation process when constructing a large treebank for Spanish (the IULA Span-ish LSP Treebank) in the framework of the European project METANET4U (Enhancingthe European Linguistic Infrastructure, GA 270893GA).2The treebank was developed using the following bootstrapping approach, detailsof which are presented in Sections 3 and 4: First, we annotated the sentences using the DELPH-IN developmentframework, in which the annotation process is effected by manuallyselecting the correct parses from among all the analyses produced by ahand-built symbolic grammar. Second, when a number of human-validated parsed sentences wereavailable, we trained a MaxEnt ranker. Third, we trained a dependency parser with the human-validated parsedsentences converted to the CoNLL format. Fourth, we provided a fully automated chain based on an ensemblemethod that compared the parse delivered by the dependency parser andthe one delivered by the MaxEnt ranker, and then accepted theautomatically proposed analysis, but only if both were identical. Fifth, sentences rejected by the ensemble were given to human annotatorsfor manual disambiguation.Obviously, using fully automatic parsing would have been the best solution forspeed and consistency, but no statistical parsers for Spanish are good enough yet, andwhen using symbolic parsers, there is no way to separate good parses from incorrectones.
The ensemble method we propose is a way of avoiding monitoring automaticparsing; the error is more than acceptable and recall is expected to be augmented byre-training and the refinement of the different parses.After this introduction, Section 2 presents an overview of related work on automaticparse selection, Section 3 summarizes the set-up, Section 4 presents our experiments andresults and, finally, Section 5 concludes.2.
Related WorkIn the broadest sense, this work is situated with respect to research into automaticparse selection.
Such projects have had a variety of different goals as well as dif-ferent approaches, based on (i) semantic filtering techniques (Yates, Schoenmackers,and Etzioni 2006), (ii) sentence-level features (e.g., length; Kawahara and Uchimoto1 In order to control errors, a common strategy is to control inter-annotator agreement by making twoannotators work on the same sentences.
This makes the task even slower and more expensive.2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among differentdomains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths(ranging from 4 to 30 words).
The treebank is publicly available at http://metashare.upf.edu.524Marimon, Bel, and Padro?
Automatic Selection of HPSG-Parsed Sentences2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart andRappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007;Sagae and Tsujii 2007; Baldridge and Osborne 2003).
Here, we focus on ensembleapproaches.Reichart and Rappoport (2007) selected high-quality constituency parses byusing the level of agreement among 20 copies of the same parser, trained on dif-ferent subsets of a training corpus.
Experiments using training and test data for thesame domain and in the parser-adaptation scenario showed improvements over severalbaselines.Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses.They compared the outputs of two statistical shift-reduce LR models and selected onlyidentical parses, in their case to retrain the MaxEnt model.
Following this procedure,they achieved the highest score in the domain adaptation track of the CoNLL 2007shared task.Finally, Baldridge and Osborne (2003) used an ensemble of parsers in the con-text of HPSG grammars applied to committee-based active learning, that is, to selectthe most informative sentences to be hand-annotated and used as training materialto improve the statistical parser and to minimize the required amount of such sen-tences.
Using the English Resource Grammar (Flickinger 2002) and the Redwoodstreebank (Oepen et al.
2002), they showed that sample selection according to preferredparse disagreement between two different machine learning algorithms (log-linearand perceptron), or between the same algorithm trained on two independent featuresets (configurational and ngram sets, based on the HPSG derivation trees), reduced theamount of human-annotated material needed to train an HPSG parse selection modelcompared with a certainty-based method based on tree entropy and several baselineselection metrics.Like Baldridge and Osborne (2003), we investigate ensemble parsing in the contextof HPSG grammars; however, our goal does not involve selecting the most informativesentences to retrain the parser, but rather to select those sentences most reliably parsed,in order to enlarge the treebank automatically.
Thus, rather than selecting sentences onwhich two models disagree, we select those where they agree completely.
In addition,we present two important contributions, going beyond what has been done in previouswork.
First, although parsing ensembles have previously been proposed only for closelyrelated language models (i.e., parsers that use algorithms under the machine-learningparadigm, varying only the feature set or training data), the presented work is thefirst to combine parsers from different paradigms: stochastic dependency parsing andMaxEnt parse selection over parses produced by a symbolic grammar.
Second, thecurrent work is the first to propose such a methodology for parse selection as a way ofovercoming the seemingly impossible task of automatically selecting good parses fromautomatic parsing to speed treebank production and, more importantly, to meet therequirements of high precision and high consistency that are good for all of the uses ofthe treebank.3.
Set-upWe select high-quality HPSG analyses using full agreement among a MaxEnt parseselection model and a dependency parser.
A comparison between the two is performedon the dependency structures that we obtain converting the parse tree produced by asymbolic grammar to the CoNLL format.525Computational Linguistics Volume 40, Number 33.1 HPSG Parsing and DisambiguationOur investigation uses the Deep Linguistic Processing with HPSG Initiative (DELPH-IN),3 an open-source processing framework also used in several treebank projectswithin this international initiative (Oepen et al.
2002; Flickinger et al.
2012).
Using thisframework, the annotation process is divided into two parts: (1) the corpus is parsedusing a hand-built HPSG (Pollard and Sag 1994); (2) the grammar output is ranked bya MaxEnt-based parse ranker (Toutanova et al.
2005), and the best parse is manuallyselected.The grammar applied in parsing is a broad-coverage, open-source Spanish gram-mar implemented in the Linguistic Knowledge Builder (LKB) system (Copestake 2002),the Spanish Resource Grammar (SRG) (Marimon 2013).The manual selection task is performed with an interface provided as part of the[incr tstb()] grammar profiling environment (Oepen and Carroll 2000) that allows theannotator to reduce the set of parses incrementally by choosing so-called discriminants(Carter 1997); that is, by selecting the features that distinguish between the differentparses, until the appropriate parse is left or, if none of the displayed parses is the correctone, all parses are rejected.As always the case with symbolic grammars, the SRG produces several hundredsof analyses for a sentence.
The DELPH-IN framework, however, provides a MaxEnt-based ranker that sorts the parses produced by the grammar.
Although this stochasticranker cannot be used to select automatically the correct parse without introducing aconsiderable number of errors (as we will show, it only achieves accuracy of about 61%),it nevertheless allows the annotator to reduce the forest to the n-best trees, typically the500 top readings.
The statistics that form the model of the MaxEnt ranker are gatheredfrom disambiguated parses and can be updated as the number of annotated sentencesincreases.3.2 Conversion to the CoNLL FormatThe linguistic analysis produced by the LKB system for each parsed sentence provides,together with a constituent structure and a Minimal Recursion Semantics (MRS) seman-tic representation (Copestake et al.
2005), a derivation tree, obtained from a completesyntactico-semantic analysis represented in a parse tree with standard HPSG-typedfeature structures at each node.The derivation tree is encoded in a nested, parenthesized structure whose ele-ments correspond to the identifiers of grammatical rules and the lexical items usedin parsing.
Phrase structure rules?marked by the suffix ?
c?
(for ?construction?
)?identify the daughter sequence, separated by a hyphen, and, in headed-phrase con-structions, a basic dependency relation between sentence constituents (e.g., subject-head (sb-hd) and head-complement (hd-cmp)).
Lexical items are annotated withpart-of-speech information according to the EAGLES tag set for Spanish4 and theirlexical entry identifier, and they optionally include a lexical rule identifier.
Figure 1shows an example.In order to compare the first-best trees selected by the MaxEnt selection model andthe outputs of the dependency parser, we convert the derivation trees to a dependency3 http://www.delph-in.net/.4 See http://www.ilc.cnr.it/EAGLES96/annotate/annotate.html.526Marimon, Bel, and Padro?
Automatic Selection of HPSG-Parsed SentencesFigure 1Derivation tree and dependency graph of Conceder licencias, cuando as??
lo dispongan las ordenanzas[To grant licences, when so stipulated by ordinances].format, also illustrated in Figure 1.
In this target annotation, lexical elements are linkedby asymmetrical dependency relations in which one of the elements is consideredthe head of the relation and the other one is its dependant.
The conversion is a fullyautomatic and unambiguous process that produces the dependency structure in theCoNLL format (Buchholz and Marsi 2006).
A deterministic conversion algorithmmakes use of the identifiers of the phrase structure rules mentioned previously, inorder to identify the heads, dependants, and some dependency types that are directlytransferred onto the dependency structure (e.g., subject, specifier, and modifier).
Theidentifiers of the lexical entries, which include the syntactic category of the sub-categorized elements, enable the identification of the argument-related dependencyfunctions.53.3 Dependency ParsingFor dependency parsing, we use MaltParser (Nivre et al.
2007).
To train it, we usemanually disambiguated parses among those parses produced by the HPSG grammar,converted to the dependency format we describe earlier.5 An alternative proposal for projecting HPSG trees to CoNLL is described in Ivanova et al.
(2012).527Computational Linguistics Volume 40, Number 3Table 1Results of the MaxEnt model and MaltParser as labeled attachment scores, unlabeledattachment scores, labeled accuracy score, and exact syntactic match.LAS UAS Label Accur Score Exact Synt MatchMaxEnt model 95.4% 96.8% 97.6% 61.0%MaltParser 92.0% 95.0% 94.5% 43.1%4.
Experiments and ResultsIn our experiments, we tested the ability of the ensemble approach to select only correctparses.
The experiment proceeded as follows: We divided a set of 15,329 sentences into a training and test set (13,901 and1,428 sentences, respectively).
Sentence length ranged from 4 to 20 words(longer sentences had not been annotated yet). We trained the MaxEnt model and MaltParser and ran each of the modelson the test set.
The results we achieved are displayed in Table 1. We compared the outputs of the two models and selected those sentenceswhere both parses produced identical analyses.The performance of our parser ensemble approach was measured through precisionand recall on the task of selecting those sentences for which the first tree proposed bythe MaxEnt model was the correct one.
Table 2 shows the confusion matrix resultingfrom the experiment.
The row predicted ok counts the number of sentences selectedby our ensemble method (Malt and MaxEnt delivered parses are identical), and therow predicted nok contains the number of sentences not selected because the parsersdisagreed.
Columns gold present the manual evaluation of a MaxEnt model first rankedparse.
From this table, we can compute precision and recall of our sentence selector:445 sentences were selected out of the 1,428 sentences in the test set (31.2%).
Precision(number of correctly selected sentences among all the selected sentences) stood at 90.6%(403/445), and recall (number of correctly selected sentences among all the actuallycorrectly ranked first sentences) was 46.6% (403/864).We compared the results of our ensemble method with two parse selection methodsbased on: (i) a simple probability-based threshold (baseline) and (ii) a parser uncertaintymeasure computed as tree entropy as used by Baldridge and Osborne (2003).
The baselineconsisted of selecting sentences for which the ratio between the probabilities of the twohighest ranked analyses delivered by the MaxEnt model was over a given threshold.Table 2Confusion matrix used to assess the results in terms of precision and recall.goldok nok totalpredicted ok 403 42 445nok 461 522 983total 864 564 1,428528Marimon, Bel, and Padro?
Automatic Selection of HPSG-Parsed SentencesThe idea was that a very high ratio would indicate that the parse ranked first had alarge advantage over the others, whereas if the ratio was close to 1, both the first andthe second analyses would have similar probabilities, indicating lower confidence of themodel in the decision.
Tree entropy takes into account not just the two highest rankedanalyses, but all trees proposed by the parser for that sentence.
The rationale is thathigh entropy indicates a scattered probability distribution among possible trees (andthus less certainty of the model in the prediction), whereas low entropy should indicatethat one tree (or a few) gets most of the probability mass.Results for different thresholds (both for the baseline and tree entropy) are shownin Table 3 (top).
As we can see, setting a high threshold for the baseline, we can select asmall subset of 20% of the sentences with precision similar to that achieved by our parseensemble approach.
To select 31% of the sentences (i.e., about the same proportion weobtained with the ensemble approach) we need to set a threshold of 4.5, obtaining aprecision of 84%, which is lower than the 90% obtained with the ensemble method.Tree entropy exhibits similar behavior, in that a restrictive threshold can selectabout 15% of sentences with precision over 90%, while setting a threshold such thatabout 31% of sentences are selected, we obtain precision of about 75%.Note that although the baseline has an F1 score slightly higher than the ensemble,our goal is a high precision filter that can be utilized to select correctly parsed sentences.From this point of view, our approach beats both baselines.The fact that tree entropy yields worse values than the baseline is somehow pre-dictable: Given a sentence with n possible trees (note that n may be in the order ofdozens or even hundreds), if a small number m of those analyses (1 < m << n) concen-trate a large portion of probability mass but exhibit small differences between them, thesentence will be rejected by the baseline (because there is not enough distance betweenthe first and second analyses) but will be accepted by tree entropy (because entropy willbe relatively low, given the large value of n).
Thus, tree entropy is a good measure forBaldridge and Osborne (2003), whose purpose is to select sentences where the modelis less confident, but our simple baseline seems to be better when the goal is to selectsentences where the first parse is the correct one.Table 3Top: Comparative results using different threshold values for the baselines.
Bottom: Results persentence length when selecting about 31% over all sentences.
Thr = threshold; %sel = percentageof selected sentences; P = precision; R = recall; Len = sentence length.Baseline Tree entropy EnsembleThr.
%sel P R Thr.
%sel P R %sel P R2 50.9% 67.6% 70.2% 0.2 59.6% 60.1% 73.2%3 38.4% 77.5% 60.8% 0.15 38.2% 71.7% 56.0%4.5 31.0% 84.1% 53.3% 0.133 31.3% 75.7% 48.3% 31.2% 90.6% 46.6%10 20.8% 91.1% 38.6% 0.1 21.4% 82.0% 35.8%20 12.1% 97.3% 24.0% 0.075 15.1% 91.5% 28.2%30 9.9% 98.7% 19.9% 0.05 11.2% 96.6% 22.2%Len.
Baseline Tree entropy Ensemble%sel P R %sel P R %sel P R1-10 56.0% 96.7% 70.6% 42.8% 96.6% 53.9% 43.6% 97.7% 70.4%11-20 19.8% 68.2% 36.9% 26.1% 60.3% 43.0% 10.3% 83.7% 33.8%All 31.0% 84.1% 53.3% 31.3% 75.7% 48.3% 31.2% 90.6% 46.6%529Computational Linguistics Volume 40, Number 3As shown in Table 3 (bottom), behavior is different for sentences of up to 10 wordsthan for longer sentences.
All three systems have a bias towards selecting short ratherthan long sentences (because short sentences are more often correctly analyzed by theparser).
The results for short sentences are similar in all three cases, but the ensembleapproach is clearly more precise for long sentences, with only a moderate loss in recall.5.
ConclusionWe have described research that aims to increase the degree of automation whenbuilding annotated corpora.
We propose a parser ensemble approach based on fullagreement between a MaxEnt model and a dependency parser to select correct linguisticanalyses output by an HPSG grammar.
This enables a hybrid annotation methodologythat combines fully automatic annotation and manual parse selection, which makes theannotation task more efficient while maintaining high accuracy and the high degree ofconsistency necessary for a useful treebank.
Our approach is grammar-independent andcan be used by any DELPH-IN-style treebank.
In the future, we plan to investigate theimpact of automatic treebank enlargement on the performance of statistical parsers.AcknowledgmentsThis work was supported by grant Ramo?n yCajal from Spanish MICINN and the projectMETANET4U.
We thank the reviewers fortheir comments and Carlos Morell for hissupport.ReferencesAbeille?, Anne (editor).
2003.
Treebanks:Building and Using Parsed Corpora.
Kluwer,Amsterdam.Baldridge, Jason and Miles Osborne.2003.
Active learning for HPSGparse selection.
In Proceedings of the7th Conference on Computational NaturalLanguage Learning, pages 17?24,Edmonton.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings of the10th Conference on Computational NaturalLanguage Learning, pages 149?164,New York, NY.Carter, David.
1997.
The TreeBanker: A toolfor supervised training of parsed corpora.In Proceedings of the 14th National Conferenceon Artificial Intelligence, pages 598?603,Providence, RI.Copestake, Ann.
2002.
Implementing TypedFeature Structure Grammars.
CSLIPublications, Stanford, CA.Copestake, Ann, Dan Flickinger, CarlPollard, and Ivan A.
Sag.
2005.
Minimalrecursion semantics: An introduction.Research on Language and Computation,3(4):281?332.Flickinger, Dan.
2002.
On building a moreefficient grammar by exploiting types.In Natural Language Engineering (6)1?Special Issue: Efficiency Processing withHPSG: Methods, Systems, Evaluation,16(1):1?17.Flickinger, Dan, Valia Kordoni, Yi Zhang,Anto?nio Branco, Kiril Simov, PetyaOsenova, Catarina Carvalheiro, FranciscoCosta, and Se?rgio Castro.
2012.ParDeepBank: Multiple parallel deeptreebanking.
In Proceedings of the 11thWorkshop on Treebanks and LinguisticTheories, pages 97?108, Lisbon.Ivanova, Angelina, Stephan Oepen, LiljaOvrelid, and Dan Flickinger.
2012.Who did what to whom?
A contrastivestudy of syntacto-semantic dependencies.In Proceedings of the 6th LinguisticAnnotation Workshop, pages 2?11,Jeju Island.Kawahara, Daisuke and KiyotakaUchimoto.
2008.
Learning reliabilityof parses for domain adaptation ofdependency parsing.
In Proceedings of the3rd International Joint Conference on NaturalLanguage Processing, pages 709?714,Hyderabad.Marimon, Montserrat.
2013.
The SpanishDELPH-IN grammar.
Language Resourcesand Evaluation, 47(2):371?397.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gu?lsen Eryigit, SandraKu?bler, Svetoslav Marinov, and ErwinMars.
2007.
Maltparser: A language-independent system for data-drivendependency parsing.
Natural LanguageEngineering, 13(2):95?135.530Marimon, Bel, and Padro?
Automatic Selection of HPSG-Parsed SentencesOepen, Stephan and John Carroll.
2000.Performance profiling for parserengineering.
In Natural LanguageEngineering (6)1?Special Issue: EfficiencyProcessing with HPSG: Methods, Systems,and Evaluation, 16(1):81?97.Oepen, Stephan, Dan Flickinger,K.
Toutanova, and C. D. Manning.
2002.LinGo Redwoods.
A rich and dynamictreebank for HPSG.
In Proceedingsof the 1st Workshop on Treebanks andLinguistic Theories, pages 139?149,Sozopol.Pollard, Carl and Ivan A.
Sag.
1994.Head-driven Phrase Structure Grammar.The University of Chicago Press andCSLI Publications, Chicago.Reichart, Roi and Ari Rappoport.
2007.An ensemble method for selection ofhigh quality parses.
In Proceedings of the45th Annual Meeting of the Association forComputational Linguistics, pages 408?415,Prague.Reichart, Roi and Ari Rappoport.
2009.Automatic selection of high qualityparses created by a fully unsupervisedparser.
In Proceedings of the 13thConference on Computational NaturalLanguage Learning, pages 156?164,Boulder, CO.Sagae, Kenji and Jun-Ichi Tsujii.
2007.Dependency parsing and domainadaptation with LR models and parserensembles.
In Proceedings of the JointMeeting of the Conference on EmpiricalMethods in Natural Language Processing andConference on Computational NaturalLanguage Learning, pages 1,044?1,050,Prague.Toutanova, Kristina, Christoper D. Manning,Dan Flickinger, and Stephan Oepen.
2005.Stochastic HPSG parse disambiguationusing the Redwoods corpus.
Research onLanguage and Computation, 3(1):83?105.Yates, Alexander, Stefan Schoenmackers,and Oren Etzioni.
2006.
Detecting parsererrors using Web-based semantic filters.In Proceedings of the 11th Conference ofEmpirical Methods in Natural LanguageProcessing, pages 27?34, Sydney.531This article has been cited by:1.
Montserrat Marimon, N?ria Bel.
2014.
Dependency structure annotation in the IULA SpanishLSP Treebank.
Language Resources and Evaluation .
[CrossRef]
