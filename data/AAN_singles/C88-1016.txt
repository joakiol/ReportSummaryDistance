A STATISTICAL APPROACH TO LANGUAGE TRANSLAT IONP.
BROWN,  J. COCKE,  S. DEL I ,A  PIETRA,  V. DELLA P IETRA,F.
JEL INEK,  R, MF, RCF, R, and P. ROOSSINIBM Research Divis ionT.J.
Watson  Research CenterDepar tment  of Computer  ScienceP.O.
Box 218York town Heights,  N.Y. 10598ABSTRACTAn approach to automatic translation is outlined that utilizestechnklues of statistical inl'ormatiml extraction from large databases.
The method is based on the availability of pairs of largecorresponding texts that are translations of each other.
In ourcase, the iexts are in English and French.Fundamental to the technique is a complex glossary ofcorrespondence of fixed locutions.
The steps of the proposedtranslation process are: (1) Partition the source text into a setof fixed locutioris.
(2) Use the glossary plus coutextualinformation to select im corresponding set of fixed Ioctttions intoa sequen{e forming the target sentence.
(3) Arrange the wordsof the talget fixed locutions into a sequence forming the targetsentence.We have developed stalistical techniques facilitating both tileautonlatic reation of the glossary, and the performance of tilethree translation steps, all on the basis of an aliglnncllt ofcorresponding sentences in tile two texts.While wc are not yet able to provide examples of French /English tcanslation, we present some encouraging intermediateresults concerning lossary creation and the arrangement of targetWOl'd seq  l ie) lees .1.
INTRODUCTIONIn this paper we will outline an approach to automatic translationthat utilizes techniques of statistical information extraction fromlarge data bases.
These self-organizing techniques have provensuccessful in the field of automatic speech recognition \[1,2,3\].Statistical approaches have also been used recently inlexicography \[41 and natural anguage processing \[3,5,6\].
The ideaof automatic translation by statistical (information thco,'etic)methods was proposed many years ago by Warren Weaver \[711.As will be seen in the body of tile paper, tile suggested techniqueis based on the availability of pairs of large corresponding textsthat are Iranslations of each other.
I l l  particular, we have chosento work with the English and French languages because we wereable to obtain the biqingual l lansard corpus of proceedings of theCanadian parliament containing 30 million words of text \[8\].
Wealso prefer to apply our ideas initially to two languages whoseword orcter is similar, a condition that French and English satisfy.Our approach eschews the use of an internmdiate ,nechalfism(language) that would encode the "meaning" of tile source text.The proposal will seem especially radical since very little will besakl about employment of conventional grammars.
Thisomissiol\], however, is not essential, and may only rcl'lect ourrelative lack of tools as well as our uncertainty about tile degreeof grammar sophistication required.
We are keeping an openmind!Ill what follows we will not be able to give actual results el  French/ English translation: our less than a year old project is not I'arenongh ahmg.
Rather, we will outline our current hinking, sketchcertain techniqttes, and substantiate our Ol)timism by presenting:some intermediate quantitative data.
We wrote this solnewhatspecttlativc paper hoping to stimulate interest in applications elstatistics to transhttion and to seek cooperation in achieving thisdifficult task.2.
A I IEURIST|C OUTLINE OF 'FILE BASIC PHI I ,OSOPttYFigure I juxtaposes a rather typical pair of corresponding Englishmid \]:rench selltenees, as they appear in the Ih.nlsard corpus.They arc arranged graphically so as to make evident thai (a) theliteral word order is on the whole preserved, (b) the chulsal (andperhaps phrasal) structure is preserved, and (c) the sentence pairscontain stretches of essentially literal correspondence interruptedby fixed locutions.
In the latter category arc \[I rise on = iesouleve\], \]affecting = apropos\] ,  and \[one which reflects o n =i/our mettre cn doutc\].It can thus be argued that translation ought to bc based on acomplex glossary of correspondence el' fixed locutions.
Inch~dedwould be single words as well as phrases consisting el  contiguousor tuna--contiguous words.
E.g., I word = mot l, I word = proposl.\[not = ne ... pasl, \[no = ne ... pas\[, \[scat belt = ccmturc\[, late =a mangel and even (perhaps} lone which reflects Oil = \[)()ill"mcttrc ell doute\], etc.Transhttion call he sotnewhat naively regarded as a tht'cc slag?process:( 1 ) Partition the source text into a set of fixed locutions(2) Use the glossary plus contextual information to select thecorresponding set of fixed locutions in the target language.
(3) Arrange the words of the target fixed locutions into asequence that forms the target sentence.This naive approach forms the basis of our work.
In fact, we havedeveloped statistical techniques facilitating the creation of theglossary, and the performance of the three translation steps.While the only way to refute the many weighty objections to ourideas woukl be to construct a machine that actually carries outsatisfactory translation, some mitigating comments are ill order,7 lWe do not hope to partition uniquely the source sentence intolocutions.
In most cases, many partitions will be possible, eachhaving a probability attached to it.Whether "affccting" is to be translated as "apropos"  or"cuncernant," or, as  our dictionary has it, "touchant" or"cmouvant," or in a variety of other ways, depends on the restof the sentence.
However, a statistical indication may beobtained from the presence or absence of particular guide wordsin that scntcncc.
Tile statistical technique of decision trees \[9\]can be used to determine the guide word set, and to estimate theln'obability to be attached to cach possible translate.The sequential arrangement of target words obtained from theglossary inay depend on an analysis of the source sentence?
Forinstance, clause corrcspondence may be insisted upon, in whichcase only permutations of words which originate in the samesource clause wotdd be possible.
Furthermore, the character ofthe source clause may affect the probability of use of certainfunctioll words in the target clause.
There is, of course, nothingto prcvent the use of more detailed information about thestructure of the parse of the source sentence.
However,preliminary experilnents presented below indicate that only a verycrude grammar may be needed (see Section 6).3.
CREATING THE GLOSSARY,'FIRST ATTEMPTWe have already indicated in the previous ection why creating aglossary is not just a matter of copying some currently availabledictiouary into the computer, in fact, in the paired sentences ofFigure 1, "affecting" was translated as "apropos , "  acorrespondence that is riot ordinarily available?
Laying aside forthe time being the desirability of (idiomatic) word cluster - to -word cluster translation, what we are'after at first is to find foreach word f in the (French) source language the list of words{e~, e2 ..... e,} of the (English) target language into which f cantranslate, and the probability P(e, I f  ) that such a translation takesplace.A first approach to a solution that takes advantage ofa large databasc of paired sentences (referred to as 'training text') may be asfollows.
Suppose for a moment hat in every French / Englishsentence pair each French wordftranslates into one and only oneEnglish word e ,  and that this word is somehow revealed to thecomputer.
Then we could proceed by!'I.
Establish a counter C(e,,f) for each word e~ of the Englishw~cabulary.
Initially set C(e~,f) = 0 for words et.
Set J = 1.2.
Find the Jth occurrence of the word f in  the French text.
Letit take place in the Kth sentence, and let its translate be the qthword in the Kth English sentence E = e~,, e~ .
.
.
.
.
e~,.
Thenincrement by 1 the counter C(e,,?f).3.
Increase J by 1 and repeat steps 2 and 3.Setting M(f  ) equal to the sum of all the counters C(e,, f )  at theconclusion of the above operation (in fact, it is easy to see thatM(f)  is the number of occurrences off in  the total French text),we could then estimate the probability P(e, J f ) of translating theword f by the word e, by the fraction C(e, , f ) /M(f) .The problem with the above approach is that it relies on correctidentification of the translates of French words, i.e., on thesolution of a significant part of tile translation problem.
In theabsence of such identification, the obvious recourse is to professcomplete ignorance, beyond knowing that the translate is one ofthe words of the corresponding English sentence, each of itswords being equally likely.
Step 2 of the above algorithm thenmust be changed to2'.
Find the Jth occurrence of the word f in  the French text.
Letit take place in the Kth sentence, and let the Kth English sentenceconsist of words e,,, e,~, ..., e,?.
Then increment he countersC(e,,,f), C(e,,,f) ..... C(o,o,f) by tire fraction 1/n.This second approach is based on tile faith that in a large corpus,the frequency of occurrence of true translates of f incorresponding English sentences would overwhelm that of othercandidates whose appearance in those sentences i accidental?This belief is obviously flawed.
In particular, the article "the"would get the highest count since it would appear multiply inpractically every English sentence, and similar problems wouldexist with other function words as well.What needs to bedone is to introduce some sort of normalizationthat would appropriately discount for the expected frequency ofoccurrence of words.
Let P(e~) denote the probability (based onttle above procedure) that the word e, is a translate of a randomlychosen French'word.
P(e~) is given byPie i) = ~f P(e i l f ' ) r ( f ' )  = ~f P(e~ l f ' )M( f ' ) /M  (3.i)where M is the total length of the French text, and M(f ' )  is thenumber of occurrences o f f  t in that text (as before).
The fractionP(e, I f )  / P(e,) is an indicator of the strength of association of e,with f, since P(e, I f )  is normalized by the frequency P(e,) ofassociating e~ with an average word.
Thus it is reasonable toconsider e, a likely translate of f if P(e, I f ) is sufficiently large?The above normalization may seem arbitrary, but it has a soundunderpinning from the field of Information Theory \[ 10\].
In fact,the quantityP(eilf) l(ei; f )  = log (3.2) P(e,)is the mutual information between the French word f and theEnglish word e,.Unfortunately, while normalization yields ordered lists of likelyEnglish word translates of French words, it does not provide uswith the desired probability values.
Furthermore, we get noguidance as to the size of a threshold T such that e, would be acandidate translate of f if and only ifl (~ ; f )  > T (3.3)Various ad hoe modifications exist to circumvent he twoproblems?
One might, for instance, find the pair e, f with thehighest mutual information, criminate e~ and f from allcorresponding sentences in which they occur (i.e.
decide onceand for all that in those sentences e,is tile translate of f !
), thenre-compute all the quantities over the shortened texts, determinethe new maximizing pair e~,f ~ and continue the process untilsome arbitrary stopping rule is invoked?Before the next section introduces a better approach that yieldsprobabilities, we present in Figure 2 a list of high mutual72information English words for some selected French words.
Thereader will agree that even tire flawed technique isquite powerful.4.
A SIMPLE GI,OSSARY BASED ON A MODE\[,O1" TIlE TRANSI,ATION PROCESSWe will now revert to our original ambition of derivingprobabilities of translation, P(e,\[f).
Let us start by observingthat tlm algorithm of the previous ection has the following flaw:Shonld it be "decided" that the qth word, e,, , of the Englishsentence is Ihc translate of the rth word, ~r, of the Frenchsentence, that process makes no provision for removing e,.
fromeonskk ratiou as a candidate translate of any of tile remainingFrench words (those not in the rth position)!
We need to find amctho0 to decide (probabilistically !)
which English word wasgeneral ed by which l.'rench one, and then estimate P(e, t f  ) by therelative frequency with whiehfgave rise to e, as "observed" ira tiretexts of paired French / English sentence transhttcs.
Ourprocedure will be based on a model (an admittedly crude one) ofhow Ertgtish words are generated from their French counterparts.With a slight additional refinement to be specified in the nextsection (see the discussion on position distortion), the followingmodel will do the trick.
Augment he English vocabulary by theNULl, vcord eo that leaves no trace in tile English text.
Then eachFrench word f will prodnce exactly one 'primary' English word(which may be, however, invisible).
Furthermore, primaryEnglish words can produce a number of secondary ones.The provisions for the null word and for tile production ofsecondary words will account for the unequal length ofcorresponding French and English sentences.
It would beexpected that some (but not all) French function words wouldbe killed by producing null words, and that English ones wouldbe crealed by secondary production.
In particular, in the exampleof Figme l, one would expect hat "reflects" woakl generate both"which" and "on" by secondary production, and "rise" wouldsimilarly generate "on."
On tbc other hand, the article 'T" of'TOrat( ur" and the preposition "a" of "apropos" wotfld bothbe expected to generate a null word in the primary process.This model of generation ofEnglish words from French ones thenrequires the specification of the following quantities:1.
The probabilities P(e, l f )  that the ith word of the Englishdictionary was generated by the French word f.2.
The probabilities Q(% l e,) that the jth English word isgenerated from tile ith one in a secondary generation process.3.
The probabilities R (k I e~) that the ith English word generatesexactly k other words in the secondary process.
By convention,we set R(0 \[ e0) = 1 to assure that the null word does not generateany other words.The lnollel probability that the word f generates e,,in tile primaryprocess, and e~:,...,e~, in the secondary one, is equal to the productP(ei, l f  ) R(k - 11%) Q(ei2lei,) Q(%lei~)... Q(%leq) (4.1)Given a pair of English and French sentences E and F, by theterm generation pattern $ we understand the specification ofwhich English words were generated from which French ones,and which~secondary words from which primary ones.
Therefore,the probability P(E,$IF) of generating the words of E ira apattern $ from those of F is given simply by a product of factorslike (4.1), one for each French word.
We can then think ofestimating the probabilities P(e, l f ) ,  R(k l e,), and Q(e:l?)
by thefollowing algorithm at tile start of which all counters are set to0:1.
For a sentence pair E,F of the texts, find that pattern $ thatgives the maximal value of P(E,$IF), and then make the(somewhat impulsive) decision that that pattern $ actually tookplace.2.
If in the pattern $, f gave rise to e,, augment counterCP(e,,f) by l; if e, gave rise to k sccoudary English words,augment counter CR(k, e,) by 1 ; if e~ is any (secondary) word thatwas given rise to by e,, augment counter CQ(e~, e,) by 1.3.
Carry out steps 1 and 2 for all sentence pairs of tile trainingtext.4.
Estimate the model probabilities by nornmlizing thecorrespnndiug counters, i.e.,P(e,\]f) = CP(ei, f ) /CP( f )  where CP(f) = ECP(e, f )iR(k\] e i) = CR(k, ei)/CR(e,) where CR(ei) = E CR(k, ei)kQ(ejl e i) = CQ(e 1, e,)/CQ(e i) where CQ(e,) = ECQ(ei, e,)JThe problem with the above algorithm is that it is circular: ilaorder to evalnate P(E,$ \] F) one needs to know the probabilitiesP(e, I)c), R(kl e,), and Q(ejle,) in the first place!
Forttmately, thedifficulty can be alleviated by use of itcrative re-estimation, whichis a technique that starts out by guessing the values of unknownquantities and gradually re-adjusts them so as to account betterand better for given data \[ 11 \].More precisely, given any specification of the probabiliticsP(e, l f ) ,  R(k l e,), and Q(%le,) , we compute the probabilitiesP(E,$ \[ F) needed in step 1, and after carrying out step 4, wct, scthe freshly obtained probabilities P(e, I f ) ,  R(k \]e,), and Q(e, I e,)to repeat the process fi'om step I again, etc.
We hah thecomputation when the obtained estimates top changing fromiteration to iteration.While it can be shown that tile probability estimates obtained inthe above process will converge \[11,12\], it cannot be proven thatthe values obtained will be the desired ones.
A heuristic argumentcan be formulated making it plausible that a more complex butcomputationally excessive version \[13\] will succceC Its truncatedmodification leads to a glossary that seems a very satisfactoryone.
We present some interesting examples of its P(e, I f )  entriesin Figure 3.Two important aspects of this process have not yet been dealtwith: the initial selection of values of P(e, l f ) ,  R(kle,) , andQ(51e,), and a method of finding the pattern $ maximizingP(E,$ \[ F).A good starting point is as follows:A.
Make Q(ejle,) = l /K,  where K is the size of the Englishvocabulary.73g.
l.et R(I  \[e,) = 0 .8 ,  R(01?)
= 0.1, R(2 I<)  = R(31<) =R(4 I g) = R(5 I e,) = 0.025 for all words e, except he null wordell l,et R(0 le0) = 1.0.C.
To determine the initial distribution P(e, l f )  proceed asI'ollows:(i) Estimate first P(< If ) by tile algorithm of Section 3.
(ii) Compute the mutual information values l(e,; f )  by formula(Y2), and for each f find the 20 words e, for which I(e,;f) islargest.
(iii) I.ct P(<~I./') = P(< l f )  = ( I /21)  - e for all words<on thelist obtained in OiL whEre e is some small positive number.l)istributc the remaining probability e uniformly over all theI nglish words not on the list.I:inding tile maximizing pattern $ for a given sentence pair E, Fis ~ well-studiEd technical problem with a variety of,'{mHmtatiomdly feasible solutions that arc suboptimal in somepractically uuimportant respects I 14\].
Not to interrupt he flowt,l imuiti\e ideas, we omit the discussion of the correspondingd 1~2',11 i l lnns.5.
TOWARD A COMPLEX G1,OfSSARYIn the previous section we have introduced a techniqne thatderives a word - to - word translation glossary.
We will nowreline tile model to make the probabilities a better reflection ofreality, and then outline an approach for including in tile glossaryIhe /ixEd locations discussed in Section 2.It should be noted that while English / French translation is quitek)cal (as illustrated by the alignment of Figure 1), the modelleading to (4.1) did not take advantage of this affinity of the twolanguages: tile relative position of the word translate pairs ill theirrespective selltences was not taken into account.
If m and ndenote the respective lengths of corresponding French andl:.nglish sentences, then the probability that 6~ (the kth word in theEnglish sentencE) is a primary translate of.f~0 (the hth word in the\[:rench sentence) shoukl more accurately be given by theprobability P(e,,kl .f,,,h,m,n) that depends both on wordpositions and sentence lengths.
'1'o keep the formulation as simpleas possiblE, WE can restrict ourselves to tile functional forml'(ei ,k I /i,,,h,m,n) = PW(e,~ I fh) PD(k l h,m,n) (5.1)In (5.1) we make thc 'distortion' distribution PD(klh,m,n)indcpcndcnt o1' the identity of the words whose positionaldiscrepancy it dcscribcs.As far as secondary generation is concerned, it is first clear thatthe production of preceding words differs from that of those thatIollow.
So the R and Q probabilities hould be split into left andright probabilities RL and QL, and RR and QR.
Furthermore,\re shnuld provide the Q -probabilities with their own distortioncomponents that would depend on the distance of the secondaryword from its primary 'parent'.
As a result of thesecons!derations, the probability that f~, generates (for instance) theprimary words e,~ and preceding and following secondary words<~ ,, <~ ,, e,.~ would be given byf'W(6~ I .\[i~,) PD(k l h,m,n) RL(2 l eiA) RR(I  e~a)QL(G_:~, 3 I G) QL(% ,,11%) QR(e,~+2,2lei ,)(5.2)Obviously, other distortion formulations are possible.
Thepurpose of any is to sharpen the derivation process by restrictingthe choice of translates to the positinnally likely candidates in thecorresponding sen tencc.To find fixed locutions in English, we can use the finalprobabilities QL and QR obtained by tile method of the previoussection to compute mutual informations between primary andsecondary word pairs,QR(e' I e) IR(e;e r) = log- - - -  (5.3)P(e')andQL(e' I e) 1L(e~;e) = logP(e')where P(e') = C(e')/N is the relative frequency of occurrence ofthe secondary word e' in the English text (C(e') denotes thenumber of occurrences of e ' in the text of size N), and QR andQL are the average secondary generation probabilities,QR(e'\]e) = ZQR(e ' ,  i\] e) (5,4)iandQl.
(e'\]e) = EQR(e ' ,  i l e)iWE can then establish an experimentally appropriate threshold71, and iuchulc in the glossary all pairs (e, e') and (e', e) whosemutual information exceeds 7'.While tile process above results in two-word fixed locutions,longer locutions can be obtained iteratively in the next round afterthe two-word variety had been included in the glossary and in theformulation of its creation.To obtain French locutions, one must simply reverse the directionof the translation process, making English and French the sourceand target languages, respectively.With two-word locutions present in both the English and Frenchparts of the glossary, it is necessary to reformulate the generationprocess (4.1).
The change would be minimal if we could decideto treat the words of a locution ( , / ; f ' )  as a single word f* =U, f ' )  rather than as two separate words f and f '  whenever bothare found in a sentence.
In such a case nothing more than areceding of the French text would be required.
However, such aradical step would almost certainly be wrong: it could wellconnect auxiliaries and participles that were not part of a singlepast construction.
Clearly then, the choice between separatenessand unity should be statistical, with probabilities estimated in theoverall glossary construction process and initialized according tothe frequencies with which elements of the pair f , f~ wereassociated o1 not by secondary generation when they appeared inthe same sentence.Since the approach of this section was not yet used to obtain anyresults, we will leave its complete mathematical specification to afuture report.746.
GF, I~,IERATIION ()F TRANSLATED 'I'EXTWe have pointed out in Sectk)u 2 that translation can besomewhat  xaively regarded as a lhrec stage process:( I ) Partit ion the source text into a set of fixed locutions.
(2) Use the glossary plus contextual information to select thecorresponding set of fixed lomttious in the target language.
(3) At range the words of thc target fixed locutkms into aseqtteltce forming the target sentence.We have just fitfished arguin b, itt Section 5 that the parti l ioningof sottrcc +ext ili1O locutions is SOIIIUWIIHt conlplex, and that itmust be approached statistically.
The basic idea of usingcotltextttal iilfOl'l?lation tO select the correct 'sense' of a Iocutiollis to eonsh uct a contextual glossary based on a probabil ity of theform P(el  J; g'IFI ) where e and f are English anrl Frenchlocutions, ;tnd q, ilq denotes a 'lexical' equivalence class of thescalence F The tu,;t of class membership woukl typically dependon tilt pre~:ence of SOIIIC contbination of words in F. The choiceof an app;opr iate qtfivalcncc htssification schenlc would, ofcourse, be .
'+he subject of research based on yet another statisticalformulation.
The estimate of P(el ./'; ~11"1 ) would be derivedfrom courtts o1' locttlion alignments in sentmlce translate pai,s, theal ignments being dstimated based on non-contextual  glossaryprobabilitit+s of the form (5.2).The last stop in our translation scheme is the re-arrangement ofthe words o1" the generated English locutions into an appropriatesequence.
To see whc'ther this can be douc statistically, weexplored what would happen in the ilnpossibly optimistic casewhere the words generated in (2) were exactly those of thel inglish sczttencc (only their order would be unknown):From a large f'+'uglish corpus we derived estimates of tr igramprobabilities, P(e3let, e:~), that the word el follows immediatelythe sequencc pair e~, % A model of 13,nglish sentence productionbased on a trigram estimate would conclude that a sentencee~, ca, ..J e,, is generated with probabil ityP(el, e2) P(e3 Iet, e2) P(e41 e2, e3) ..- P(e,, I e,,+ 2, e, I) (6.1)We then rook other l:';nglish sentences (not included in thetraining COrlmS) and deterntined which of the n t differentarrangements el + their n words was most likely, using the l'ormula(6.1).
We found that in 63% of sentences of 10 words or less,the most likely arrangement was the original English sentence.I ;urthermore, the most likely arrangement preserved the meaningof the original sentence in 79% of the cases.Figure, 4. shows examples hi' synonymous and non-.synonymousre-al'rangelnenL'~.We realize that very little hope exists of the glossary ielding thewords and only the words of an English seutence translating theoriginal French one, and that, furthermore, Euglish sentences arctypically longer than 10 words.
Nevertheless, we feel that theabow: result is a hopeful one for fnture statistical translationmethods incorporat ing the use of appropriate syntactic structureinformation.REFERENCES111 L.R.
Bahl, F. Jclinek, and R.l,.
Mercer: A maximum likelihoodapproach to contimlous speech recognition, IEEE Traosaclioos onPattern Analysis and Machine Intelligence, PAM I-5 (2): 179-190, March1983.12\] .I.K.
Baker: Stochastic modeling for automatic speechtmdcrstanding.
In R.A. P, eddy, editor, Speech Recognition, pages521-541, Academic Press, New York, 1979.131 J.D.
Ferguson: llidden Markov analysis: An introduction.
In J.D.Fcrguson, Ed., ltldden Marker Models for Speech.
Princeton, NewJersey, IDA-CRD, Oct. 1980, pp.
8-1514\] J. Metl.
Sinclair: "Lcxicogral~hic F.vidence" in, I)ielionarie,~,Lexicography and Langaage l,earniog (l!1+'F Doeoments: 1211), editolR.
llson, NewYork: Pergamon Press, pp.
81-94, 1985.151 P,.G.
Garsidc, G.N.
Lccch and (\].P,.
Sampson, The ComlmlationalAnalysis of l(l,glish: a Corpus-Based AI)l)roach, I.ongman 19871161 G.R.
Sampson, "A Stochastic Approach to Parsing" itl.
lh'oceeding+,of tile I lth lnlernalional Corfferenee oil (k+mputaliotml l,inguintics(COl ,IN(\] '86) Bonn 151-155, 1986.171 W. Weaver: Translalion (194.9).
Reproduced in: I,ocke.
WN.
&Booth, A,D.
eds.
: Maelnine Iranslalimn of hmguages.
Calnbrid,ee, MA.
:MIT Press, 1955.18\] I\[lansards: Official l)roeeedings of the liouse of Cemlnons of Canada,19"I4+.78, CanadialJ Government Printie~ Bureau, Ihtll, Quebec( ~ ~111~/(Ja.19\[ I+.
IIrciman, J.ll.
Friedtnall, R.A. Olshen, and ( J .
Stone:Classification and Regression Trees, Wadsworth alld t~rooks, M(mtcrey.CA, \[ 984.\[10\] R.G.
Gallager: Informalion Theory aad reliahle (ommuniealion,John Wiley and Sons, Ii1c,, New York, 1968.\[I I1 A.P.
Dcmpstcr, N.M.l.aird, al/d It.B.
ll.ubin: Maximum likelihoodfrom ineolnpletc data via tile I"M algorithm, Journal of Ihe RoyalS|atist ical Society,  39(B) :  1-38, 1977.1121 A.J, Viterbi: Error bounds Ior conw)httional codes and anasylntotically optimum decoding algorithm, 11,'1.
;1,~ Transactions onInformation Theory, 1T-13:2611-267, 19fi7.\[ 13\] L.E.
Bauln: All inequality and associated inaxilnization tcc\]miqucin statistical estimatkm of probabilistic functions o1 a Maikov process,lneqoalities, 3:1-8, 1972.\[ 14\] F. Jclinek: A fast sequential decoding algorithm using a stack, IBMT.
a. Watson Research Development, vol.
13, pp.
6754~85, No\.
19(?)Mr.
Speaker, I rise on a question of privilegeMonsieur l 'Orateur, je souleve la qoestion de privilegeaffecting the rights and prerogatives of pmliamentary committeesa propos des droits et des prerogatives des eomites parlenmnlairesand o11o which reflects oii tile wol'd of two ininisterset i)otlr nlettre en d<mte les i)ro\])os tie detlX illhlistlesof the Crown.tic la Cotlronne.F IGURE IAI,IGNMENT OF A FRENCII AND ENGHSH SI;,NTENCE PAIR75eau waterlait milkbanque bankbanques bankshier yesterdayjanvier Januaryjours daysvotre yourcufants childrentrop tootoujours alwaystrois threemonde worldpourquoi whyaujord'bui todaysans withoutlui himmais butsuis amseulemeot onlypeut cannotceintures seatceinturcs beltsbravo !FIGURE 2A LIST OF HIGH MUTUAL INFORMATION FRENCH-ENGLISHWORD PAIRSWHICH QUII.
qui 0.380 who 0.1882. que 0.177 which 0.1613. dont 0.082 that 0.0844, de 0.060 0.0385. d' 0.035 to 0.0326. laquclle 0.
(131 of 0.0277. ou 0.027 the 0.0268. ct 0.022 what 0.018THEREFORE DONC1.
donc 0.514 therefore 0.3222. consequent 0.075 so 0.1473. pat" 0.074 is 0.0344. ce 0.066 then 0.0245. pourquoi 0.064 thus 0.0226. alors 0.025 the 0.0187. il 0.025 that 0.0138. aussi 0.015 us 0.012STILL ENCORE1.
encore 0.435 still 0,1812. toujours 0.230 again 0.1743. reste 0.027 yet 0.1484.
*** 0.020 even 0.0555. quand 0.018 more 0.0466. meme 0.017 another 0,0307. de 0.015 further 0.0218. de 0.014 once 0.013FIGURE 3 (PART I)EXAMPLES OF PARTIAL GLOSSARY LISTS OF MOST LIKELYWORD TRANSLATES AND THEIR PROBABILITIESNote: *** denotes miscellaneous words not belonging to the lexicon.PEOPLE GENS1.
les 0.267 people 0.7812. gens 0.244 they 0.0133. personnes 0.100 those 0.0094. population 0.055 individuals 0.0085. peuple 0.035 persons 0.0056. canadiens 0.031 people's 0.0047. habitants 0.024 men 0.0048. ceux 0.023 person 0.003OBTAIN OBTENIRl.
obtenir 0.457 get 0.3012. pour 0.050 obtain 0.1083. les 0.033 have 0.0364. de 0.031 getting 0.0325. trouver 0.026 seeking 0.0236. se 0.025 available 0.0217. obtenu 0.020 obtaining 0.0218. procurer 0.020 information 0.016QUICKLY RAPIDEMENT1.
rapidement 0.508 quickly 0.3892. vite 0.130 rapidly 0.1473. tot 0.042 fast 0.0524. rapide 0.021 quick 0.0425. brievement 0.019 soon 0.0366. aussitot 0.013 faster 0.0357. plus 0.012 speedy 0.0268. bientot 0.012 briefly 0.025FIGURE 3 (PART II)EXAMPLES OF PARTIAL GLOSSARY LISTS OF MOST LIKELYWORD TRANSLATES AND THEIR PROBABILITIESEXAMPLES OF RECONSTRUCTION TttAT PRESERVEMEANING:would I report directly to you?I would report directly to you?now let me mention some of the disadvantages.let me mention some of the disadvantages now,he did this several hours later.this he did several hours later.EXAMPLES OF RECONSTRUCTION THAT DO NOT PRESERVEMEANINGthese people have a fairly large rate of turnover.of these people have a fairly large turnover rate.in our organization research as two missions.in our missions research organization has two.exactly how this might be done is not clear.clear is not exactly how this might be done.FIGURE 4STATISTICAL ARRANGEMENT OF WORDS BELONGING TOENGLISH SENTENCES76
