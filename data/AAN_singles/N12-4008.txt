T8: Predicting Structures in NLP:Constrained Conditional Models and IntegerLinear Programming NLPDan Goldwasser, Vivek Srikumar, Dan RothABSTRACTMaking decisions in natural language processing problems often involves assigningvalues to sets of interdependent variables where the expressive dependency structurecan influence, or even dictate what assignments are possible.
This setting includes abroad range of structured prediction problems such as semantic role labeling, namedentity and relation recognition, co-reference resolution, dependency parsing andsemantic parsing.
The setting is also appropriate for cases that may require makingglobal decisions that involve multiple components, possibly pre-designed or pre-learned, as in summarization, paraphrasing, textual entailment and question answering.In all these cases, it is natural to formulate the decision problem as a constrainedoptimization problem, with an objective function that is composed of learned models,subject to domain or problem specific constraints.Constrained Conditional Models (CCM) formulation of NLP problems (also known as:Integer Linear Programming for NLP) is a learning and inference framework thataugments the learning of conditional (probabilistic or discriminative) models withdeclarative constraints (written, for example, using a first-order representation).
The keyadvantage of the CCM formulation is its support for making decisions in an expressiveoutput space while maintaining modularity and tractability of training and inference.
Inmost applications of this framework in NLP, following [Roth & Yih, CoNLL'04], integerlinear programming (ILP) has been used as the inference framework, although otheralgorithms can be used.This framework has attracted much attention within the NLP community over the lastfew years, with multiple papers in all the recent major conferences.
Formulatingstructured prediction as a constrained optimization problem over the output of learnedmodels has several advantages.
It allows the incorporation of problem specific globalconstraints using a first order language ???
thus freeing the developer from (much ofthe) low level feature engineering ???
and guarantees exact inference.
Importantly, itprovides also the freedom of decoupling model generation (learning) from theconstrained inference stage, often simplifying the learning stage as well as theengineering aspect of building an NLP system, while improving the quality of thesolutions.
These advantages and the availability of off-the-shelf solvers have led to alarge variety of NLP tasks being formulated within it, including semantic role labeling,syntactic parsing, co-reference resolution, summarization, transliteration and jointinformation extraction.The goal of this tutorial is to introduce the framework of Constrained Conditional Modelsto the broader ACL community, motivate it as a generic framework for structuredinference in global NLP decision problems, present some of the key theoretical andpractical issues involved in using CCMs and survey some of the existing applications ofit as a way to promote further development of the framework and additionalapplications.
The tutorial will be useful for senior and junior researchers who areinterested in structured prediction and global decision problems in NLP, providing aconcise overview of recent perspectives and research results.OUTLINEAfter briefly motivating and introducing the general framework, the main part of thetutorial is a methodological presentation of some of the key computational issuesstudied within CCMs that we will present by looking at case studies published in theNLP literature.
In the last part of the tutorial, we will discuss engineering issues thatarise in using CCMs.1.
MotivationWe will begin by introducing structured prediction with various NLP examples.We will motivate the framework of Constrained Conditional Models usingexamples from sequential inference, sentence compression and semantic rolelabeling.
[30 min]2.
Examples of Existing CCM ApplicationsWe will present several applications that use CCMs ???
including co-referenceresolution, sentence compression and information extraction and use these toexplain several of the key advantages the framework offers.
In this context, wewill discuss several ways in which expressive constraints can be introduced intoan application.
[30 min]3.
Modeling CCMs: Inference methods and ConstraintsWe will present and discuss several possibilities for modeling inference in CCMs.We will discuss ways to model problems as structured prediction problems andthe use of hard and soft constraints to represent prior knowledge.
We will look atways in which Constrained Conditional Models can be used to augmentprobabilistic models with declarative knowledge based constraints and how thesesupport expressive global decisions.
We will also mention various possibilities forperforming the inference, from commercial Integer Linear Programmingpackages to search techniques to Lagrangian relaxation approximation methods.
[30 min]4.
Training ParadigmsThe objective function used by CCMs can be decomposed and learned in severalways, ranging from ???standard??
structured learning, i.e., a complete jointtraining of the model along with the constraints, to a complete decouplingbetween the learning and the inference stage.
We will present the advantagesand disadvantages offered by different training paradigms and provide theoreticaland experimental understanding.
In this part, we will also compare CCMs toother approaches studied in the literature.
[30 min]5.
Beyond Supervised LearningIn the standard supervised setting, we require a corpus that is annotated with thestructures of interest, which is expensive and often impractical.
In this part of thetutorial, we will show how to go beyond the supervised setting using CCMs,which provide ways to use declarative constraints to guide supervised and semi-supervised training in the presence of partial and indirect supervision.
We willalso present recent advances in training structured predictors indirectly using adata for a companion binary task.
We will review several successful applicationsof these methods in diverse tasks such as information extraction and textualentailment.
[30 min]6.
Developing CCMs ApplicationsWe will present a ??
?cookbook approach??
for developing applications withinthe CCM framework and discuss templates for possible applications.
[30 min]BIOSDan GoldwasserComputer Science Department, University of Illinois at Urbana-Champaign, IL, 61801Email: goldwas1--AT--illinois.eduPhone: +(217) 333-2584Dan Goldwasser is a PhD candidate in the Department of Computer Science at theUniversity of Illinois at Urbana-Champaign.
He is the recipient of the 2010 C.L.
andJane Liu Award for research promise.
He published several papers in natural languageprocessing, machine learning and semantic interpretation.
His research work studiesthe role that an external context, such as the real world or a simulated world, plays insemantic interpretation and learning protocols.Vivek SrikumarComputer Science Department, University of Illinois at Urbana-Champaign, IL.
61801Email: vsrikum2--AT--illinois.eduPhone: +(217) 333-2584Vivek Srikumar is a Ph.D. candidate at the University of Illinois, Urbana-Champaign.
Hehas worked on Machine Learning in the context of Natural Language Processing andhas published papers in several conferences.
His research deals with applying structurelearning and prediction formalized as Constrained Conditional Models to semantic rolelabeling and its extensions.Dan RothComputer Science Department, University of Illinois at Urbana-Champaign, IL, 61801Email: danr--AT--cs.uiuc.eduPhone: +(217) 244-7068Dan Roth is a Professor in the Department of Computer Science at the University ofIllinois at Urbana-Champaign and the Beckman Institute of Advanced Science andTechnology (UIUC) and a University of Illinois Scholar.
He is a fellow of AAAI and theACM.
Roth has published broadly in machine learning, natural language processing,knowledge representation and reasoning and received several paper, teaching andresearch awards.
He has developed several machine learning based natural languageprocessing systems that are widely used in the computational linguistics community andin industry and has presented invited talks and tutorials in several major conferences.Dan Roth has written the first paper on formulating global NLP decisions as ILPproblems with his student Scott Yih, presented in CoNLL'04, and since then has workedon further developing Constrained Conditional Models, on learning and inference issueswithin this framework and on applying it to several NLP problems, including SemanticRole Labeling, Information and Relation Extraction and Transliteration.
