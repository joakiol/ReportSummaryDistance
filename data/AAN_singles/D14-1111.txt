Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036?1046,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsReducing Dimensions of Tensors in Type-Driven Distributional SemanticsTamara Polajnar Luana F?ag?ar?as?an Stephen ClarkComputer LaboratoryUniversity of CambridgeCambridge, UKfirst.last@cl.cam.ac.ukAbstractCompositional distributional semantics isa subfield of Computational Linguisticswhich investigates methods for represent-ing the meanings of phrases and sen-tences.
In this paper, we explore im-plementations of a framework based onCombinatory Categorial Grammar (CCG),in which words with certain grammaticaltypes have meanings represented by multi-linear maps (i.e.
multi-dimensional arrays,or tensors).
An obstacle to full implemen-tation of the framework is the size of thesetensors.
We examine the performance oflower dimensional approximations of tran-sitive verb tensors on a sentence plausi-bility/selectional preference task.
We findthat the matrices perform as well as, andsometimes even better than, full tensors,allowing a reduction in the number of pa-rameters needed to model the framework.1 IntroductionAn emerging subfield of computational linguis-tics is concerned with learning compositional dis-tributional representations of meaning (Mitchelland Lapata, 2008; Baroni and Zamparelli, 2010;Coecke et al., 2010; Grefenstette and Sadrzadeh,2011; Clarke, 2012; Socher et al., 2012; Clark,2013).
The advantage of such representations liesin their potential to combine the benefits of dis-tributional approachs to word meaning (Sch?utze,1998; Turney and Pantel, 2010) with the more tra-ditional compositional methods from formal se-mantics (Dowty et al., 1981).
Distributional repre-sentations have the properties of robustness, learn-ability from data, ease of handling ambiguity,and the ability to represent gradations of mean-ing; whereas compositional models handle the un-bounded nature of natural language, as well asproviding established accounts of logical words,quantification, and inference.One promising approach which attempts tocombine elements of compositional and distribu-tional semantics is by Coecke et al.
(2010).
Theunderlying idea is to take the type-driven approachfrom formal semantics ?
in particular the ideathat the meanings of complex grammatical typesshould be represented as functions ?
and ap-ply it to distributional representations.
Since themathematics of distributional semantics is pro-vided by linear algebra, a natural set of functionsto consider is the set of linear maps.
Coecke etal.
recognize that there is a natural correspon-dence from complex grammatical types to tensors(multi-linear maps), so that the meaning of an ad-jective, for example, is represented by a matrix (a2nd-order tensor)1and the meaning of a transitiveverb is represented by a 3rd-order tensor.Coecke et al.
use the grammar of pregroupsas the syntactic machinery to construct distribu-tional meaning representations, since both pre-groups and vector spaces can be seen as exam-ples of the same abstract structure, which leadsto a particularly clean mathematical description ofthe compositional process.
However, the approachapplies more generally, for example to other formsof categorial grammar, such as Combinatory Cate-gorial Grammar (Steedman, 2000; Maillard et al.,2014), and also to phrase-structure grammars in away that a formal linguist would recognize (Ba-roni et al., 2014).
Clark (2013) provides a descrip-tion of the tensor-based framework aimed more atcomputational linguists, relying only on the math-ematics of multi-linear algebra rather than the cat-egory theory used in Coecke et al.
(2010).
Sec-tion 2 repeats some of this description.A major open question associated with thetensor-based semantic framework is how to learn1This same insight lies behind the work of Baroni andZamparelli (2010).1036the tensors representing the meanings of wordswith complex types, such as verbs and adjec-tives.
The framework is essentially a composi-tional framework, providing a recipe for how tocombine distributional representations, but leav-ing open what the underlying vector spaces are andhow they can be acquired.
One significant chal-lenge is an engineering one: in a wide-coveragegrammar, which is able to handle naturally occur-ring text, there will be a) a large lexicon with manyword-category pairs requiring tensor representa-tions; and b) many higher-order tensors with largenumbers of parameters which need to be learned.In this paper we take a first step towards learningsuch representations, by learning tensors for tran-sitive verbs.One feature of the tensor-based framework isthat it allows the meanings of words and phraseswith different basic types, for example nouns andsentences, to live in different vector spaces.
Thismeans that the sentence space is task specific, andmust be defined in advance.
For example, to calcu-late sentence similarity, we would have to learn avector space where distances between vectors rep-resenting the meanings of sentences reflect simi-larity scores assigned by human annotators.In this paper we describe an initial investi-gation into the learning of word meanings withcomplex syntactic types, together with a simplesentence space.
The space we consider is the?plausibility space?
described by Clark (2013), to-gether with sentences of the form subject-verb-object.
This space is defined to distinguish se-mantically plausible sentences (e.g.
Animals eatplants) from implausible ones (e.g.
Animals eatplanets).
Plausibility can be either representedas a single continuous variable between 0 and 1,or as a two-dimensional probability distributionover the classes plausible (>) and implausible (?
).Whether we consider a one- or two-dimensionalsentence space depends on the architecture of thelogistic regression classifier that is used to learnthe verb (Section 3).We begin with this simple plausibility sentencespace to determine if, in fact, the tensor-based rep-resentation can be learned to a sufficiently usefuldegree.
Other simple sentence spaces which canperhaps be represented using one or two variablesinclude a ?sentence space?
for the sentiment anal-ysis task (Socher et al., 2013), where one variablerepresents positive sentiment and the other nega-tive.
We also expect that the insights gained fromresearch on this task can be applied to more com-plex sentence spaces, for example a semantic simi-larity space which will require more than two vari-ables.2 Syntactic Types to TensorsThe syntactic type of a transitive verb in Englishis (S\NP)/NP (using notation from Steedman(2000)), meaning that a transitive verb is a func-tion which takes an NP argument to the right, anNP argument to the left, and results in a sentenceS .
Such categories with slashes are complex cate-gories; S and NP are basic or atomic categories.Interpreting such categories under the Coecke etal.
framework is straightforward.
First, for eachatomic category there is a corresponding vectorspace; in this case the sentence space S and thenoun space N.2Hence the meaning of a noun ornoun phrase, for example people, will be a vectorin the noun space:???
?people ?
N. In order to obtainthe meaning of a transitive verb, each slash is re-placed with a tensor product operator, so that themeaning of eat, for example, is a 3rd-order tensor:eat ?
S?N?N.
Just as in the syntactic case,the meaning of a transitive verb is a function (amulti-linear map) which takes two noun vectors asarguments and returns a sentence vector.Meanings combine using tensor contraction,which can be thought of as a multi-linear gen-eralisation of matrix multiplication (Grefenstette,2013).
Consider first the adjective-noun case, forexample black cat.
The syntactic type of blackis N /N ; hence its meaning is a 2nd-order tensor(matrix): black ?
N?N.
In the syntax, N /Ncombines with N using the rule of forward appli-cation (N /N N ?
N ), which is an instance offunction application.
Function application is alsoused in the tensor-based semantics, which, for amatrix and vector argument, corresponds to ma-trix multiplication.Figure 1 shows how the syntactic types com-bine with a transitive verb, and the correspondingtensor-based semantic types.
Note that, after theverb has combined with its object NP , the typeof the verb phrase is S\NP , with a correspond-ing meaning tensor (matrix) in S ?N.
This ma-trix then combines with the subject vector, through2In practice, for example using the CCG parser of Clarkand Curran (2007), there will be additional atomic categories,such as PP , but not many more.1037people eat fishNP (S\NP)/NP NPN S?N?N N>S\NPS?N<SSFigure 1: Syntactic reduction and tensor-based se-mantic types for a transitive verb sentencematrix multiplication, to give a sentence vector.In practice, using for example the wide-coverage grammar from CCGbank (Hockenmaierand Steedman, 2007), there will be many typeswith more than 3 slashes, with correspondinghigher-order tensors.
For example, a com-mon category for a preposition is the follow-ing: ((S\NP)\(S\NP))/NP , which would beassigned to WITH in eat WITH a fork.
(The wayto read the syntactic type is as follows: with re-quires an NP argument to the right ?
a fork inthis example ?
and then a verb phrase to theleft ?
eat with type S\NP ?
resulting in a verbphrase S\NP .)
The corresponding meaning ten-sor lives in the tensor space S?N?S?N?N,i.e.
a 5th-order tensor.
Categories with evenmore slashes are not uncommon, for example((N /N )/(N /N ))/((N /N )/(N /N )).
Clearlylearning parameters for such tensors is highlychallenging, and it is likely that lower dimensionalapproximations will be required.3 MethodsIn this paper we compare five different methodsfor modelling the type-driven semantic represen-tation of subject-verb-object sentences.
The ten-sor is a function that encodes the meaning of averb.
It takes two vectors from the K-dimensionalnoun space as input, and produces a representa-tion of the sentence in the S-dimensional sentencespace.
In this paper, we consider a plausibilityspace where S is either a single variable or a two-dimensional space over two classes: plausible (>)and implausible (?
).The first method (Tensor) follows Krishna-murthy and Mitchell (2013) by learning a tensor asparameters in a softmax classifier.
We introducethree related methods (2Mat, SKMat, KKMat),all of which model the verb as a matrix or a pair ofmatrices (Figure 2).
Table 1 gives the number ofTensor 2Mat SKMat KKMat DMatV 2K24K 2K K2K2?
4 8 4 0 0Table 1: Number of parameters per method.parameters for each method.
Tensor, 2Mat, andSKMat all have a two-dimensional S space, whileKKMat produces a scalar value.
In all of theselearning-based methods the derivatives were ob-tained via the chain rule with respect to each setof parameters and gradient descent was performedusing the Adagrad algorithm (Duchi et al., 2011).We also reimplement a distributional method(DMat), which was previously used in SVOexperiments with the type-driven framework(Grefenstette and Sadrzadeh, 2011).
While theother methods are trained as plausibility classi-fiers, in DMat we estimate the class boundaryfrom cosine similarity via training data (see expla-nation below).Tensor If subject (ns) and object (no) nouns areK-dimensional vectors and the plausibility vec-tor is S-dimensional with S = 2, we can learnthe values of the K ?
K ?
S tensor represent-ing the verb as parameters (V) of a regression al-gorithm.
To represent this space as a distributionover two classes (>,?)
we apply a sigmoid func-tion (?)
to restrict the output to the [0,1] range andthe softmax activation function (g) to balance theclass probabilities.
The full parameter set whichneeds to be optimised for is B = {V,?
}, where?
= {?>, ??}
are the softmax parameters forthe two classes.
For each verb we optimise theKL-divergence L between the training labels tiand classifier predictions using the following reg-ularised objective:O(B) =N?i=1L(ti, g(?
(hV(nis, nio)),?
))+?2||B||2(1)where nisand nioare the subject and object ofthe training instance i ?
N , and hV(nis, nio)=(nis)V(nio)Tdescribes tensor contraction.
Thefunction hVis described diagrammatically in Fig-ure 2-(a), where the verb tensor parameters aredrawn as a cube with the subject and object nounvectors as operands on either side.
The outputis a two-dimensional vector which is then trans-formed using the sigmoid and softmax functions.1038KSKKKS(a) KK SKSK2*SxSSx(b)KKSKK000 000 K SSxx(c) K KKKxx(d)Figure 2: Illustrations of the hVfunction for the regression-based methods (a)-Tensor, (b)-2Mat, (c)-SKMat, (d)-KKMat.
The operation in (a) is tensor contraction, T denotes transpose, and ?
denotesmatrix multiplication.The gold-standard distribution over training labelsis defined as (1, 0) or (0, 1), depending on whetherthe training instance is a positive (plausible) ornegative (implausible) example.
Tensor contrac-tion is implemented using the Matlab Tensor Tool-box (Bader et al., 2012).2Mat An alternative approach is to decouplethe interaction between the object and subject bylearning a pair of S ?
K (S = 2) matrices (Vs,Vo) for each of the input noun vectors (one ma-trix for the subject slot of the verb and one for theobject slot).
The resulting S-vectors are concate-nated, after the subject and object nouns have beencombined with their matrices, and combined withthe softmax component to produce the output dis-tribution.
Therefore the objective function is thesame as in Equation 1, but hVis defined as:hV(nis, nio)=((nis)VTs)||(Vo(nio)T)Twhere || represents vector concatenation.
The in-tention is to test whether we can learn the verbwithout directly multiplying subject and objectfeatures, nisand njo.
The function hVis shown inFigure 2-(b), where the verb tensor parameters aredrawn as two 2?K matrices, one of which inter-acts with the subject and the other with the objectnoun vector.
The output is a four-dimensional vec-tor whose values are then restricted to [0,1] usingthe sigmoid function and then transformed into atwo-dimensional distribution over the classes us-ing the softmax function.SKMat A third option for generating a sentencevector with S = 2 dimensions is to consider theverb as an S ?K matrix.
If we transform the ob-ject vector into a K ?K matrix with the noun onthe diagonal and zeroes elsewhere, we can com-bine the verb and object to produce a new S ?Kmatrix, which is encoding the meaning of the verbphrase.
We can then complete the sentence re-duction by multiplying the subject vector with thisverb phrase vector to produce an S-dimensionalsentence vector.
Formally, we define SKMat as:hV(nis, nio)= nis(Vdiag(nio))Tand use it in Equation 1.
The function hVisdescribed in Figure 2-(c), where the verb ten-sor parameters are drawn as a matrix, the sub-ject as a vector, and the object as a diagonal ma-1039trix.
The graphic demonstrates the two-step com-bination and the intermediate S ?
K verb phrasematrix, as well as the the noun vector productthat results in a two-dimensional vector which isthen transformed using the sigmoid and softmaxfunctions.
Whilst the tensor method captures theinteractions between all pairs of context features(nsi?
noj), SKMat only captures the interactionsbetween matching features (nsi?
noi).KKMat Given a two-class problem, such asplausibility classification, the softmax implemen-tation is overparameterised because the classmembership can be estimated with a single vari-able.
To produce a scalar output, we can learn theparameters for a single K ?
K matrix (V) usingstandard logistic regression with the mean squarederror cost function:O(V) = ?1m[N?i=1tilog hV(nis, nio)+ (1?
ti) log hV(nis, nio)]where hV(nis, nio)= (nis)V(nio)Tand the objec-tive is regularised: O(V) +?2||V||2.
This functionis shown in Figure 2-(d), where the verb parame-ters are shown as a matrix, while the subject andobject are vectors.
The output is a single scalar,which is then transformed with the sigmoid func-tion.
Values over 0.5 are considered plausible.DMat The final method produces a scalar as inKKMat, but is distributional and based on corpuscounts rather than regression-based.
Grefenstetteand Sadrzadeh (2011) introduced a corpus-basedapproach for generating a K ?K matrix for eachverb from an average of Kronecker products of thesubject and object vectors from the positively la-belled subset of the training data.
The intuition isthat, for example, the matrix for eat may have ahigh value for the contextual topic pair describinganimate subjects and edible objects.
To determinethe plausibility of a new subject-object pair for aparticular verb, we calculate the Kronecker prod-uct of the subject and object noun vectors for thispair, and compare the resulting matrix with the av-erage verb matrix using cosine similarity.For label prediction, we calculate the similar-ity between each of the training data pairs and thelearned average matrix.
Unlike for KKmat, theclass cutoff is estimated at the break-even pointof the receiver operator characteristic (ROC) gen-erated by comparing the training labels with thiscosine similarity value.
The break-even point iswhen the true positive rate is equal to the false pos-itive rate.
In practice it would be more accurateto estimate the cutoff on a validation dataset, butsome of the verbs have so few training instancesthat this was not possible.4 ExperimentsIn order to examine the quality of learning we runseveral experiments where we compare the differ-ent methods.
In these experiments we considerthe DMat method as the baseline.
Some of theexperiments employ cross-validation, in particularfive repetitions of 2-fold cross validation (5x2cv),which has been shown to be statistically more ro-bust than the traditional 10-fold cross validation(Alpaydin, 1999; Ulas?
et al., 2012).
The results of5x2cv experiments can be compared using the reg-ular paired t-test, but the specially designed 5x2cvF-test has been proven to produce fewer statisticalerrors (Ulas?
et al., 2012).The performance was evaluated using the areaunder the ROC (AUC) and the F1measure (basedon precision and recall over the plausible class).The AUC evaluates whether a method is rankingpositive examples above negative ones, regardlessof the class cutoff value.
F1shows how accuratelya method assigns the correct class label.
Anotherway to interpret the results is to consider the AUCas the measure of the quality of the parameters inthe verb matrix or tensor, while the F-score indi-cates how well the softmax, the sigmoid, and theDMat cutoff algorithm are estimating class partic-ipation.Ex-1.
In the first experiment, we compare thedifferent transitive verb representations by running5x2cv experiments on ten verbs chosen to cover arange of concreteness and frequency values (Sec-tion 4.2).Ex-2.
In the initial experiments we found thatsome models had low performance, so we appliedthe column normalisation technique, which is of-ten used with regression learning to standardisethe numerical range of features:~x :=~x?min(~x)max(~x)?min(~x)(2)This preserves the relative values of features be-tween training samples, while moving the valuesto the [0,1] range.1040Ex-3.
There are varying numbers of training ex-amples for each of the verbs, so we repeated the5x2cv with datasets of 52 training points for eachverb, since this is the size of the smallest dataset ofthe verb CENSOR.
The points were randomly sam-pled from the datasets used in the first experiment.Finally, the four verbs with the largest datasetswere used to examine how the performance of themethods changes as the amount of training dataincreases.
The 4,000 training samples were ran-domised and half were used for testing.
We sam-pled between 10 and 1000 training triples from theother half (Figure 4).4.1 Noun vectorsDistributional semantic models (Turney and Pan-tel, 2010) encode word meaning in a vector for-mat by counting co-occurrences with other wordswithin a specified context window.
We con-structed the vectors from the October 2013 dumpof Wikipedia articles, which was tokenised us-ing the Stanford NLP tools3, lemmatised with theMorpha lemmatiser (Minnen et al., 2001), andparsed with the C&C parser (Clark and Curran,2007).
In this paper we use sentence boundaries todefine context windows and the top 10,000 mostfrequent lemmatised words in the whole corpus(excluding stopwords) as context words.
The rawco-occurrence counts are re-weighted using thestandard tTest weighting scheme (Curran, 2004),where fwicjis the number of times target noun wioccurs with context word cj:tTest( ~wi, cj) =p(wi, cj)?
p(wi)p(cj)?p(wi)p(cj)(3)where p(wi) =?jfwicj?k?lfwkcl, p(cj) =?ifwicj?k?lfwkcl,and p(wi, cj) =fwicj?k?lfwkcl.Using all 10,000 context words would result ina large number of parameters for each verb ten-sor, and so we apply singular value decomposition(SVD) (Turney and Pantel, 2010) with 40 latentdimensions to the target-context word matrix.
Weuse context selection (with N = 140) and rownormalisation as described in Polajnar and Clark(2014) to markedly improve the performance ofSVD on smaller dimensions (K) and enable us totrain the verb tensors using very low-dimensional3http://nlp.stanford.edu/software/index.shtmlVerb Concreteness # of Positive FrequencyAPPLY 2.5 5618 47361762CENSOR 3 26 278525COMB 5 164 644447DEPOSE 2.5 118 874463EAT 4.44 5067 26396728IDEALIZE 1.17 99 485580INCUBATE 3.5 82 833621JUSTIFY 1.45 5636 10517616REDUCE 2 26917 40336784WIPE 4 1090 6348595Table 2: The 10 chosen verbs together with theirconcreteness scores.
The number of positive SVOexamples was capped at 2000.
Frequency is thefrequency of the verb in the GSN corpus.noun vectors.
Performance of the noun vectorswas measured on standard word similarity datasetsand the results were comparable to those reportedby Polajnar and Clark (2014).4.2 Training dataIn order to generate training data we made useof two large corpora: the Google Syntactic N-grams (GSN) (Goldberg and Orwant, 2013) andthe Wikipedia October 2013 dump.
We first choseten transitive verbs with different concretenessscores (Brysbaert et al., 2013) and frequencies, inorder to obtain a variety of verb types.
Then thepositive (plausible) SVO examples were extractedfrom the GSN corpus.
More precisely, we col-lected all distinct syntactic trigrams of the formnsubj ROOT dobj, where the root of the phrase wasone of our target verbs.
We lemmatised the wordsusing the NLTK4lemmatiser and filtered these ex-amples to retain only the ones that contain nounsthat also occur in Wikipedia, obtaining the countsreported in Table 2.For every positive training example, we con-structed a negative (implausible) one by replac-ing both the subject and the object with a con-founder, using a standard technique from the se-lectional preference literature (Chambers and Ju-rafsky, 2010).
A confounder was generated bychoosing a random noun from the same frequencybucket as the original noun.5Frequency bucketsof size 10 were constructed by collecting noun fre-quency counts from the Wikipedia corpus.
For ex-4http://nltk.org/5Note that the random selection of the confounder couldresult in a plausible negative example by chance, but man-ual inspection of a subset of the data suggests this happensinfrequently for those verbs which select strongly for theirarguments, but more often for those verbs that don?t.1041Verb Tensor DMat KKMat SKMat 2MatAUCAPPLY 85.68?
81.46?
88.88??
68.02 88.92?
?CENSOR 79.40 85.54 80.55 78.52 83.19COMB 89.41 85.65 88.38 69.20??
89.56DEPOSE 92.70 94.44 93.12 84.47?
93.20EAT 94.62 93.81 95.17 67.92 95.88?IDEALIZE 69.56 75.84 72.46 61.19 70.23INCUBATE 89.33 85.53 88.61 70.59 91.40JUSTIFY 85.27?
88.70?
89.97?
73.56 90.10?REDUCE 96.13 95.48 96.69?
79.32 97.21WIPE 85.19 84.47 87.84?
64.93??
81.29MEAN 86.93 87.29 88.37 71.96 88.30Tensor DMat KKMat SKMat 2MatF179.27 64.00 81.24?
54.06 80.80?70.66 47.93 73.52 37.86 71.0781.15 45.02 81.38 39.67 82.3684.60 54.77 84.79 43.79 86.1588.91 52.45 88.83 56.22 89.9566.53 48.28 68.39 31.03 67.4380.30 50.84 80.90 31.99 84.5579.73 73.71 81.10 54.09 82.5291.24 71.24?
87.46 76.67?
92.2278.57 47.62 80.65 39.50 78.9080.30 55.79 81.03 46.69 81.79Table 3: The best AUC and F1results for all the verbs, where ?
denotes statistical significance comparedto DMat and ?
denotes significance compared to Tensor according to the 5x2cv F-test with p < 0.05.ample, for the plausible triple animal EAT plant,we generate the implausible triple mountain EATproduct.
Some verbs were well represented in thecorpus, so we used up to the top 2,000 most fre-quent triples for training.00.51AUCAPPLYCENSOR COMBDEPOSE EATIDEALIZEINCUBATEJUSTIFYREDUCE WIPETensorTensor*SKMatSKMat*?0.200.20.40.60.811.2F?ScoreAPPLYCENSOR COMBDEPOSE EATIDEALIZEINCUBATEJUSTIFYREDUCE WIPETensorTensor*SKMatSKMat*Figure 3: The effect of column normalisation (*)on Tensor and SKMat.
Top table shows AUC andthe bottom F1-score, while the error bars indicatestandard deviation.5 ResultsThe results from Ex-1 are summarised in Ta-ble 3.
We can see that linear regression can leadto models that are able to distinguish betweenplausible and implausible SVO triples.
The Ten-sor method outperforms DMat, which was pre-viously shown to produce reasonable verb repre-sentations in related experiments (Grefenstette andSadrzadeh, 2011).
2Mat and KKMat, in turn,outperform Tensor demonstrating that it is pos-sible to learn lower dimensional approximationsof the tensor-based framework.
2Mat is an appro-priate approximation for functions with two inputsand a sentence space of any dimensionality, whileKKMat is only appropriate for a single valuedsentence space, such as the plausibility or senti-ment space.
Due to method variance and datasetsize there are very few AUC results that are sig-nificantly better than DMat and even fewer thatoutperform Tensor.
All methods perform poorlyon the verb IDEALIZE, probably because it hasthe lowest concreteness value and is in one of thesmallest datasets.
This verb is also particularly dif-ficult because it does not select strongly for eitherits subject or object, and so some of the pseudo-negative examples are in fact somewhat plausible(e.g.
town IDEALIZE authority or child IDEALIZEracehorse).
In general, this would indicate thatmore concrete verbs are easier to learn, as theyhave a clearer pattern of preferred property types,but there is no distinct correlation.The results of the normalisation experiments(Ex-2) are shown in Table 4.
We can see that theSKMat method, which performed poorly in Ex-1 notably improves with normalisation.
TensorAUC scores also improve through normalisation,but the F-scores decrease.
The rest of the methods,and in particular DMat are negatively affected bycolumn normalisation.
The results from Ex-1 andEx-2 for SKMat and Tensor are summarised in1042Verb Tensor DMat KKMat SKMat 2MatAUCAPPLY 86.16?
48.63?
82.63??
85.73?
85.65?CENSOR 75.74 71.20 78.00 82.77 78.64COMB 91.67?
62.42?
90.85?
89.79?
91.42?DEPOSE 93.96?
54.93?
93.56?
93.87?
93.81?EAT 95.64?
47.68?
92.92?
94.99??
94.76?IDEALIZE 69.64 55.98 72.20??
76.71??
71.85?INCUBATE 90.97?
61.31?
89.69?
90.19?
90.05?JUSTIFY 89.76?
54.87?
87.26??
89.64?
89.05?REDUCE 96.63?
59.58?
94.99??
96.14?
96.53?WIPE 86.82?
58.02?
84.18?
83.65?
86.02?MEAN 87.90 57.66 86.83 88.55 87.98Tensor DMat KKMat SKMat 2MatF145.57 46.99 46.17 60.86 76.60?30.43 55.16 65.19 49.59 44.2233.37 61.05 71.20 64.56 75.9642.73 39.71 73.07 54.51 56.5460.42 47.42 58.80 69.05 87.44?39.14 49.16 41.75 31.57 50.5946.35 53.33 70.45 41.57 63.6147.38 51.40 41.91 63.96 80.55?51.63 54.27 69.18 69.76 90.77?44.04 55.19 47.84 49.89 75.8044.31 51.57 58.76 55.73 70.41Table 4: The best AUC and F1results for all the verbs with normalised vectors, where ?
denotes statisticalsignificance compared to DMat and ?
denotes significance compared to Tensor according to the 5x2cvF-test with p < 0.05.Figure 3.
This figure also shows that AUC valueshave much lower variance, but that high variancein F-score leads to results that are not statisticallysignificant.When considering the size of the datasets (Ex-3), it would seem from Table 5 that 2Mat is able tolearn from less data than DMat or Tensor.
Whilethis may be true over a 5x2cv experiment on smalldata, Figure 4 shows that this view may be overlysimplistic and that different training examples caninfluence learning.
Analysis of errors shows thatthe baseline method mostly generates false nega-tive errors (i.e.
predicting implausible when thegold standard label is plausible).
In contrast, Ten-sor produces almost equal numbers of false posi-tives and false negatives, but sometimes producesfalse negatives with low frequency nouns (e.g.bourgeoisie IDEALIZE work), presumably becausethere is not enough information in the noun vec-tor to decide on the correct class.
It also producessome false positive errors when either of the nounsis plausible (but the triple is implausible), whichwould suggest results may be improved by train-ing with data where only one noun is confoundedor by treating negative data as possibly positive(Lee and Liu, 2003).6 DiscussionCurrent methods which derive distributed repre-sentations for phrases, for example the work ofSocher et al.
(2012), typically use only matrix rep-resentations, and also assume that words, phrasesand sentences all live in the same vector space.The tensor-based semantic framework is moreflexible, in that it allows different spaces for dif-ferent grammatical types, which results from it be-Verb Tensor DMat 2MatAPPLY 95.76 86.50 86.31CENSOR 82.97 84.09 77.79COMB 90.13 92.93 95.18DEPOSE 92.41 91.27 95.61EAT 99.64 98.25 99.58IDEALIZE 75.03 76.68 88.98INCUBATE 91.10 87.20 96.42JUSTIFY 88.96 88.99 87.31REDUCE 100.0 99.87 99.46WIPE 97.20 91.63 96.36MEAN 91.52 89.94 92.50Table 5: Results show average of 5x2cv AUC onsmall data (26 positive + 26 negative per verb).None of the results are significant.ing tied more closely to a type-driven syntactic de-scription; however, this flexibility comes at a cost,since there are many more paramaters to learn.Various communities are beginning to recog-nize the additional power that tensor representa-tions can provide, through the capturing of interac-tions that are difficult to represent with vectors andmatrices (see e.g.
(Ranzato et al., 2010; Sutskeveret al., 2009; Van de Cruys et al., 2012)).
Hierar-chical recursive structures in language potentiallyrepresent a large number of such interactions ?
theobvious example for this paper being the interac-tion between a transitive verb?s subject and object?
and present a significant challenge for machinelearning.This paper is a practical extension of the workin Krishnamurthy and Mitchell (2013), which in-troduced learning of CCG-based function tensorswith logistic regression on a compositional se-mantics task, but was implemented as a proof-of-concept with vectors of length 2 and on small,manually created datasets based on propositional104310 20 40 80 150 300 600 800 1000 20000.650.70.750.80.850.90.951# Training ExamplesAUCapplyDMatTensor2Mat10 20 40 80 150 300 600 800 1000 20000.70.750.80.850.90.951# Training ExamplesAUCeatDMatTensor2Mat10 20 40 80 150 300 600 800 1000 20000.650.70.750.80.850.90.951# Training ExamplesAUCjustifyDMatTensor2Mat10 20 40 80 150 300 600 800 1000 20000.80.850.90.951# Training ExamplesAUCreduceDMatTensor2MatFigure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances in-creases.logic examples.
Here, we go beyond this by learn-ing tensors using corpus data and by deriving sev-eral different matrix representations for the verb inthe subject-verb-object (SVO) sentence.This work can also be thought of as applyingneural network learning techniques to the clas-sic problem of selectional preference acquisition,since the design of the pseudo-disambiguation ex-periments is taken from the literature on selec-tional preferences (Clark and Weir, 2002; Cham-bers and Jurafsky, 2010).
We do not compare di-rectly with methods from this literature, e.g.
thosebased on WordNet (Resnik, 1996; Clark and Weir,2002) or topic modelling techniques (Seaghdha,2010), since our goal in this paper is not to ex-tend the state-of-the-art in that area, but rather touse selectional preference acquisition as a test bedfor the tensor-based semantic framework.7 ConclusionIn this paper we introduced three dimensionallyreduced representations of the transitive verb ten-sor defined in the type-driven framework for com-positional distributional semantics (Coecke et al.,2010).
In a comprehensive experiment on ten dif-ferent verbs we find no significant difference be-tween the full tensor representation and the re-duced representations.
The SKMat and 2Mat rep-resentations have the lowest number of parame-ters and offer a promising avenue of research formore complex sentence structures and sentencespaces.
KKMat and DMat also had high scoreson some verbs, but these representations are appli-cable only in spaces where a single-value output isappropriate.In experiments where we varied the amount oftraining data, we found that in general more con-crete verbs can learn from less data.
Low con-creteness verbs require particular care with datasetdesign, since some of the seemingly random ex-amples can be plausible.
This problem may becircumvented by using semi-supervised learningtechniques.We also found that simple numerical tech-niques, such as column normalisation, canmarkedly alter the values and quality of learning.On our data, column normalisation has a side-effect of removing the negative values that wereintroduced by the use of tTest weighting measure.The use of the PPMI weighting scheme and non-negative matrix factorisation (NMF) (Grefenstetteet al., 2013; Van de Cruys, 2010) could lead to asimilar effect, and should be investigated.
Furthernumerical techniques for improving the estimationof the class decision boundary, and consequentlythe F-score, will also constitute future work.1044ReferencesEthem Alpaydin.
1999.
Combined 5x2 CV F-testfor comparing supervised classification learning al-gorithms.
Neural Computation, 11(8):1885?1892,November.Brett W. Bader, Tamara G. Kolda, et al.
2012.
Matlabtensor toolbox version 2.5.
Available online, Jan.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-10), pages 1183?1193,Cambridge, MA.Marco Baroni, Raffaella Bernardi, and Roberto Zam-parelli.
2014.
Frege in space: A program for com-positional distributional semantics.
Linguistic Is-sues in Language Technology, 9:5?110.Marc Brysbaert, Amy Beth Warriner, and Victor Ku-perman.
2013.
Concreteness ratings for 40 thou-sand generally known English word lemmas.
Be-havior research methods, pages 1?8.Nathanael Chambers and Dan Jurafsky.
2010.
Im-proving the use of pseudo-words for evaluating se-lectional preferences.
In Proceedings of ACL 2010,Uppsala, Sweden.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.Stephen Clark.
2013.
Type-driven syntax and seman-tics for composing meaning vectors.
In Chris He-unen, Mehrnoosh Sadrzadeh, and Edward Grefen-stette, editors, Quantum Physics and Linguistics:A Compositional, Diagrammatic Discourse, pages359?377.
Oxford University Press.Daoud Clarke.
2012.
A context-theoretic frame-work for compositionality in distributional seman-tics.
Computational Linguistics, 38(1):41?71.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
In J. vanBentham, M. Moortgat, and W. Buszkowski, edi-tors, Linguistic Analysis (Lambek Festschrift), vol-ume 36, pages 345?384.James R. Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, University of Edinburgh.David R. Dowty, Robert E. Wall, and Stanley Peters.1981.
Introduction to Montague Semantics.
Dor-drecht.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159, July.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof English books.
In Second Joint Conference onLexical and Computational Semantics, pages 241?247, Atlanta,Georgia.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing, pages 1394?1404,Edinburgh, Scotland, UK, July.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for compo-sitional distributional semantics.
Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013).Edward Grefenstette.
2013.
Category-TheoreticQuantitative Compositional Distributional Modelsof Natural Language Semantics.
Ph.D. thesis, Uni-versity of Oxford.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Jayant Krishnamurthy and Tom M Mitchell.
2013.Vector space semantic parsing: A framework forcompositional vector space models.
In Proceed-ings of the 2013 ACL Workshop on Continuous Vec-tor Space Models and their Compositionality, Sofia,Bulgaria.Wee Sun Lee and Bing Liu.
2003.
Learning with posi-tive and unlabeled examples using weighted logisticregression.
In Proceedings of the Twentieth Interna-tional Conference on Machine Learning (ICML).Jean Maillard, Stephen Clark, and Edward Grefen-stette.
2014.
A type-driven tensor-based semanticsfor CCG.
In Proceedings of the EACL 2014 TypeTheory and Natural Language Semantics Workshop(TTNLS), Gothenburg, Sweden.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08, pages 236?244, Columbus, OH.Tamara Polajnar and Stephen Clark.
2014.
Improvingdistributional semantic vectors through context se-lection and normalisation.
In 14th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, EACL?14, Gothenburg, Sweden.1045M.
Ranzato, A. Krizhevsky, and G. E. Hinton.
2010.Factored 3-way restricted boltzmann machines formodeling natural images.
In Proceedings of theThirteenth International Conference on Artificial In-telligence and Statistics (AISTATS), Sardinia, Italy.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61:127?159.Hinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Diarmuid O Seaghdha.
2010.
Latent variable mod-els of selectional preference.
In Proceedings of ACL2010, Uppsala, Sweden.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 1201?1211, Jeju, Korea.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Chris Manning, Andrew Ng, and ChrisPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP 2013), Seat-tle, USA.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, MA.I.
Sutskever, R. Salakhutdinov, and J.
B. Tenenbaum.2009.
Modelling relational data using bayesian clus-tered tensor factorization.
In Proceedings of Ad-vances in Neural Information Processing Systems(NIPS 2009), Vancouver, Canada.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Ayd?n Ulas?, Olcay Taner Y?ld?z, and Ethem Alpayd?n.2012.
Cost-conscious comparison of supervisedlearning algorithms over multiple data sets.
PatternRecognition, 45(4):1772?1781, April.Tim Van de Cruys, Laura Rimell, Thierry Poibeau, andAnna Korhonen.
2012.
Multi-way tensor factor-ization for unsupervised lexical acquisition.
In Pro-ceedings of COLING 2012, Mumbai, India.Tim Van de Cruys.
2010.
A non-negative tensor fac-torization model for selectional preference induc-tion.
Journal of Natural Language Engineering,16(4):417?437.1046
