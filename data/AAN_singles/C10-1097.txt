Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 860?868,Beijing, August 2010Latent Mixture of Discriminative Expertsfor Multimodal Prediction ModelingDerya Ozkan, Kenji Sagae and Louis-Philippe MorencyUSC Institute for Creative Technologies{ozkan,sagae,morency}@ict.usc.eduAbstractDuring face-to-face conversation, peoplenaturally integrate speech, gestures andhigher level language interpretations topredict the right time to start talking orto give backchannel feedback.
In thispaper we introduce a new model calledLatent Mixture of Discriminative Expertswhich addresses some of the key issueswith multimodal language processing: (1)temporal synchrony/asynchrony betweenmodalities, (2) micro dynamics and (3) in-tegration of different levels of interpreta-tion.
We present an empirical evaluationon listener nonverbal feedback prediction(e.g., head nod), based on observable be-haviors of the speaker.
We confirm the im-portance of combining four types of mul-timodal features: lexical, syntactic struc-ture, eye gaze, and prosody.
We showthat our Latent Mixture of DiscriminativeExperts model outperforms previous ap-proaches based on Conditional RandomFields (CRFs) and Latent-Dynamic CRFs.1 IntroductionFace-to-face communication is highly interactive.Even when only one person speaks at a time,other participants exchange information continu-ously amongst themselves and with the speakerthrough gestures, gaze and prosody.
These differ-ent channels contain complementary informationessential to interpretation and understanding ofhuman behaviors (Oviatt, 1999).
Psycholinguisticstudies also suggest that gesture and speech comefrom a single underlying mental process, and theyPitchWordsGazeTimeP(nod)Look  at listenerSpeakerListenerPredictionFigure 1: Example of multimodal predictionmodel: listener nonverbal backchannel predictionbased on speaker?s speech and eye gaze.
As thespeaker says the word her, which is the end of theclause (her is also the object of the verb bother-ing), and lowers the pitch while looking back atthe listener and eventually pausing, the listeneris then very likely to head nod (i.e., nonverbalbackchannel).are related both temporally and semantically (Mc-Neill, 1992; Cassell and Stone, 1999; Kendon,2004).A good example of such complementarity ishow people naturally integrate speech, gesturesand higher level language to predict when to givebackchannel feedback.
Building computationalmodels of such a predictive process is challeng-ing since it involves micro dynamics and temporalrelationship between cues from different modali-ties (Quek, 2003).
Figure 1 shows an example ofbackchannel prediction where a listener head nod860is more likely.
For example, a temporal sequencefrom the speaker where he/she reaches the end ofsegment (syntactic feature) with a low pitch andlooks at the listener before pausing is a good op-portunity for the listener to give nonverbal feed-back (e.g., head nod).
These prediction modelshave broad applicability, including the improve-ment of nonverbal behavior recognition, the syn-thesis of natural animations for robots and virtualhumans, the training of cultural-specific nonver-bal behaviors, and the diagnoses of social disor-ders (e.g., autism spectrum disorder).In this paper we introduce a new modelcalled Latent Mixture of Discriminative Experts(LMDE) which addresses some of the key issueswith multimodal language processing: (1) tempo-ral synchrony/asynchrony between modalities, (2)micro dynamics and (3) integration of differentlevels of interpretation.
We present an empiricalevaluation on nonverbal feedback prediction (e.g.,head nod) confirming the importance of combin-ing different types of multimodal features.
Weshow that our LMDE model outperforms previ-ous approaches based Conditional Random Fields(CRFs) and Latent-Dynamic CRFs.2 Related WorkEarlier work in multimodal language processingfocused on multimodal dialogue systems wherethe gestures and speech may be constrained (John-ston, 1998; Jurafsky et al, 1998).
Most ofthe research in multimodal language processingover the past decade fits within two main trendsthat have emerged: (1) recognition of individ-ual multimodal actions such as speech and ges-tures (e.g, (Eisenstein et al, 2008; Frampton etal., 2009; Gravano et al, 2007)), and (2) recog-nition/summarization of the social interaction be-tween more than one participants (e.g., meetinganalysis (Heylen and op den Akker, 2007; Moore,2007; Murray and Carenini, 2009; Jovanovic etal., 2006)).The work described in this paper can be seenfrom a third intermediate category where multi-modal cues from one person is used to predictthe social behavior of another participant.
Thistype of predictive models has been mostly stud-ied in the context of embodied conversationalagents (Nakano et al, 2003; Nakano et al, 2007).In particular, backchannel feedback (the nods andparaverbals such as ?uh-hu?
and ?mm-hmm?
thatlisteners produce as someone is speaking) has re-ceived considerable interest due to its pervasive-ness across languages and conversational contextsand this paper addresses the problem of how topredict and generate this important class of dyadicnonverbal behavior.Several researchers have developed models topredict when backchannel should happen.
In gen-eral, these results are difficult to compare as theyutilize different corpora and present varying eval-uation metrics.
Ward and Tsukahara (2000) pro-pose a unimodal approach where backchannelsare associated with a region of low pitch last-ing 110ms during speech.
Models were pro-duced manually through an analysis of Englishand Japanese conversational data.
Nishimuraet al (2007) present a unimodal decision-treeapproach for producing backchannels based onprosodic features.
Cathcart et al (2003) propose aunimodal model based on pause duration and tri-gram part-of-speech frequency.
The model wasconstructed by identifying, from the HCRC MapTask Corpus (Anderson et al, 1991), trigramsending with a backchannel.
Fujie et al (2004)used Hidden Markov Models to perform head nodrecognition.
In their paper, they combined headgesture detection with prosodic low-level featuresfrom the same person to determine strongly pos-itive, weak positive and negative responses toyes/no type utterances.In recent years, great research has shown thestrength of latent variable models for natural lan-guage processing (Blunsom et al, 2008).
One ofthe most relevant works is that of Eisenstein andDavis (2007), which presents a latent conditionalmodel for fusion of multiple modalities (speechand gestures).
One of the key difference of ourwork is that we are explicitly modeling the mi-cro dynamics and temporal relationship betweenmodalities.3 Multimodal Prediction ModelsHuman face-to-face communication is a little likea dance, in that participants continuously adjusttheir behaviors based on verbal and nonverbal dis-861plays and signals.
A topic of central interest inmodeling such behaviors is the patterning of in-terlocutor actions and interactions, moment-by-moment, and one of the key challenges is iden-tifying the patterns that best predict specific ac-tions.
Thus we are interested in developing pre-dictive models of communication dynamics thatintegrate previous and current actions from all in-terlocutors to anticipate the most likely next ac-tions of one or all interlocutors.
Humans are goodat this: they have an amazing ability to predict, ata micro-level, the actions of an interlocutor (Bave-las et al, 2000); and we know that better predic-tions can correlate with more empathy and betteroutcomes (Goldberg, 2005; Fuchs, 1987).With turn-taking being perhaps the best-knownexample, we now know a fair amount about someaspects of communication dynamics, but muchless about others.
However, recent advances inmachine learning and experimental methods, andrecent findings from a variety of perspectives, in-cluding conversation analysis, social signal pro-cessing, adaptation, corpus analysis and model-ing, perceptual experiments, and dialog systems-building and experimentation, mean that the timeis ripe to start working towards more comprehen-sive predictive models.The study of multimodal prediction modelsbring a new series of research challenges:MULTIMODAL ASYNCHRONY While speechand gestures seem to come from a single under-lying mental process (McNeill, 1992), they notalways happen at the same time, making it hardfor earlier multimodal fusion approaches basedon synchrony.
A multimodal prediction modelneeds to be able to learn automatically the tempo-ral relationship (and relative importance) betweenmodalities.MICRO DYNAMICS The dynamic between mul-timodal signals should be taken at a micro levelsince many of the interactions between speech andgesture happen at the sub-gesture level or sub-word level (Quek, 2003).
Typical word-basedsampling may not be sufficient and instead ahigher sampling rate should be used.LIMITED ANNOTATED DATA Given the time re-quirement to correctly annotate multimodal data,Figure 2: Latent Mixture of Discriminative Ex-perts: a new dynamic model for multimodal fu-sion.
In this graphical model, xj represents thejth multimodal observation, hj is a hidden stateassigned to xj , and yj the class label of xj .
Graycircles are latent variables.
The micro dynamicsand multimodal temporal relationships are auto-matically learned by the hidden states hj duringthe learning phase.most multimodal datasets contain only a limitednumber of labeled examples.
Since many ma-chine learning algorithms rely on a large trainingcorpus, effective training of a predictive model onmultimodal datasets is challenging.4 Latent Mixture of DiscriminativeExpertsIn this paper we present a multimodal fusion al-gorithm, called Latent Mixture of DiscriminativeExperts (shown in Figure 2), that addresses thethree challenges discussed in the previous section.The hidden states of LMDE automatically learnthe temporal asynchrony between modalities.
Byusing a constant sample rate of 30Hz in our ex-periments, we can model the micro dynamics ofspeech and prosody (e.g., change of intonationin the middle of a word).
And finally, by train-ing separate experts for each modalities, we im-prove the prediction performance even with lim-ited datasets.The task of our LMDE model is to learn a map-ping between a sequence of multimodal observa-tions x = {x1, x2, ..., xm} and a sequence of la-bels y = {y1, y2, ..., ym}.
Each yj is a class la-bel for the jth frame of a video sequence and is amember of a set Y of possible class labels, for ex-ample, Y = {head-nod,other-gesture}.862Each frame observation xj is represented by a fea-ture vector ?
(xj) ?
Rd, for example, the prosodicfeatures at each sample.
For each sequence, wealso assume a vector of ?sub-structure?
variablesh = {h1, h2, ..., hm}.
These variables are not ob-served in the training examples and will thereforeform a set of hidden variables in the model.Following Morency et al (2007), we define ourLMDE model as follows:P (y | x, ?)
= ?hP (y | h, x, ?
)P (h | x, ?)
(1)where ?
is the model parameters that is to be esti-mated from training data.To keep training and inference tractable,Morency et al (2007) restrict the model to havedisjoint sets of hidden states associated with eachclass label.
Each hj is a member of a set Hyjof possible hidden states for the class label yj .H, the set of all possible hidden states, is definedto be the union of all Hy sets.
Since sequenceswhich have any hj /?
Hyj will by definition haveP (y | h, x, ?)
= 0, latent conditional model be-comes:P (y | x, ?)
= ?h:?hj?HyjP (h | x, ?).
(2)What differentiates our LMDE model from theoriginal work of Morency et al is the definition ofP (h|x, ?
):P (h| x, ?)
=exp( ?l ?l ?
Tl(h, x)+??
??
?
P?
(y|x, ??
))Z(x, ?)
,(3)whereZ is the partition function and P?
(y|x) isthe conditional distribution of the expert indexedby ?.
The expert conditional distributions are de-fined P?
(y|x, ??)
using the usual conditional ran-dom field formulation:P?
(y| x, ??)
= exp (?k ?
?,k ?
F?,k(y, x))Z?
(x, ??)
, (4)F?,k is defined asF?,k(y, x) =m?j=1f?,k(yj?1, yj , x, j),and each feature function f?,k(yj?1, yj , x, j) iseither a state function sk(yj , x, j) or a transitionfunction tk(yj?1, yj , x, j).
State functions sk de-pend on a single hidden variable in the modelwhile transition functions tk can depend on pairsof hidden variables.
Tl(h, x), defined in Equa-tion 3, is a special case, summing only overthe transition feature functions tl(hl?1, hl, x, l).Each expert ?
contains a different subset off?,k(yj?1, yj , x, j).
These feature functions aredefined in Section 5.2.4.1 Learning Model ParametersGiven a training set consisting of n labeled se-quences (xi,yi) for i = 1...n, training is done ina two step process.
First each expert ?
is trainedfollowing (Kumar and Herbert., 2003; Lafferty etal., 2001) objective function to learn the parame-ter ???:L(??)
=n?i=1logP?
(yi | xi, ??)?
12?2 ||?
?||2(5)The first term in Eq.
5 is the conditional log-likelihood of the training data.
The second termis the log of a Gaussian prior with variance ?2,i.e., P (??)
?
exp( 12?2 ||?
?||2).Then the marginal probabilities P?
(yj =a | y, x, ???
), are computed using belief prop-agation and used as input for Equation 3.
Theoptimal parameter ??
was learned using the log-likelyhood of the conditional probability definedin Equation 2 (i.e., no regularization).4.2 InferenceFor testing, given a new test sequence x, we wantto estimate the most probable sequence of labelsy?
that maximizes our LMDE model:y?
= argmaxy?h:?hi?HyiP (h | x, ??)
(6)5 Experimental SetupWe evaluate our Latent Mixture of Discrimina-tive Experts on the multimodal task of predictinglistener nonverbal backchannel (i.e., head nods).Backchannel feedback (the nods and paraverbalssuch as ?uh-hu?
and ?mm-hmm?
that listeners863produce as some is speaking) has received con-siderable interest due to its pervasiveness acrosslanguages and conversational contexts.5.1 DatasetWe are using the RAPPORT dataset from (Maat-man et al, 2005), which contains 47 dyadic inter-actions between a speaker and a listener.
Data isdrawn from a study of face-to-face narrative dis-course (?quasi-monologic?
storytelling).
In thisdataset, participants in groups of two were toldthey were participating in a study to evaluate acommunicative technology.
Subjects were ran-domly assigned the role of speaker and listener.The speaker viewed a short segment of a videoclip taken from the Edge Training Systems, Inc.Sexual Harassment Awareness video.
After thespeaker finished viewing the video, the listenerwas led back into the computer room, where thespeaker was instructed to retell the stories por-trayed in the clips to the listener.
The listenerwas asked to not talk during the story retelling.Elicited stories were approximately two minutesin length on average.
Participants sat approxi-mately 8 feet apart.
Video sequences were manu-ally annotated to determine the ground truth headnod labels.
A total of 587 head nods occured overall video sequences.5.2 Multimodal FeaturesThis section describes the different multimodalfeatures used to create our five experts.PROSODY Prosody refers to the rhythm, pitch andintonation of speech.
Several studies have demon-strated that listener feedback is correlated with aspeaker?s prosody (Nishimura et al, 2007; Wardand Tsukahara, 2000; Cathcart et al, 2003).
Forexample, Ward and Tsukahara (2000) show thatshort listener backchannels (listener utteranceslike ?ok?
or ?uh-huh?
given during a speaker?s ut-terance) are associated with a lowering of pitchover some interval.
Listener feedback often fol-lows speaker pauses or filled pauses such as?um?
(see (Cathcart et al, 2003)).
Using openS-MILE (Eyben et al, 2009) toolbox, we extract thefollowing prosodic features, including standardlinguistic annotations and the prosodic featuressuggested by Ward and Tsukhara: downslopes inpitch continuing for at least 40ms, regions of pitchlower than the 26th percentile continuing for atleast 110ms (i.e., lowness), drop or rise in energyof speech (i.e., energy edge), Fast drop or rise inenergy of speech (i.e., energy fast edge), vowelvolume (i.e., vowels are usually spoken softer)and Pause in speech (i.e., no speech).VISUAL GESTURES Gestures performed by thespeaker are often correlated with listener feed-back (Burgoon et al, 1995).
Eye gaze, in particu-lar, has often been implicated as eliciting listenerfeedback.
Thus, we manually annotate the follow-ing contextual feature: speaker looking at the lis-tener.LEXICAL Some studies have suggested an asso-ciation between lexical features and listener feed-back (Cathcart et al, 2003).
Using the transcrip-tions, we included all individual words (i.e., uni-grams) spoken by the speaker during the interac-tions.SYNTACTIC STRUCTURE Finally, we attemptto capture syntactic information that may pro-vide relevant cues by extracting four types of fea-tures from a syntactic dependency structure cor-responding to the utterance.
The syntactic struc-ture is produced automatically using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shift-reduce dependency parser (Sagae andTsujii, 2007), both trained on POS tags and de-pendency trees extracted from the Switchboardsection of the Penn Treebank (Marcus et al,1994), converted to dependency trees using thePenn2Malt tool1.
The four syntactic features are:?
Part-of-speech tags for each word (e.g.
noun,verb, etc.
), taken from the output of the POStagger?
Grammatical function for each word (e.g.subject, object, etc.
), taken directly from thedependency labels produced by the parser?
Part-of-speech of the syntactic head of eachword, taken from the dependency links pro-duced by the parser?
Distance and direction from each word to itssyntactic head, computed from the depen-dency links produced by the parser1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html864Figure 3: Baseline Models: a) Conditional Random Fields (CRF), b) Latent Dynamic ConditionalRandom Fields(LDCRF), c) CRF Mixture of Experts (no latent variable)Although our current method for extractingthese features requires that the entire utterancebe available for processing, this provides us witha first step towards integrating information aboutsyntactic structure in multimodal prediction mod-els.
Many of these features could in principle becomputed incrementally with only a slight degra-dation in accuracy, with the exception of featuresthat require dependency links where a word?s syn-tactic head is to the right of the word itself.
Weleave an investigation that examines only syntac-tic features that can be produced incrementally inreal time as future work.5.3 Baseline ModelsINDIVIDUAL EXPERTS Our first baseline modelconsists of a set of CRF chain models, eachtrained with different set of multimodel features(as described in the previous section).
In otherwords, only visual, prosodic, lexical or syntacticfeatures are used to train a single CRF expert.
Inone CRF chain model, each gesture class corre-sponds to a state label.
(See Figure 3a).MULTIMODAL CLASSIFIERS (EARLY FUSION)Our second baseline consists of two models: CRFand LDCRF (Morency et al, 2007).
To train thesemodels, we concatenate all multimodal features(lexical, syntactic, prosodic and visual) in one in-put vector.
Graphical representation of these base-line models are given in Figure 3.CRF MIXTURE OF EXPERTS To show the im-portance of latent variable in our LMDE model,we trained a CRF-based mixture of discriminativeexperts.
This model is similar to the LogarithmicOpinion Pool (LOP) CRF suggested by Smith etal.
(2005).
The training is performed in two steps.A graphical representation of a CRF Mixture ofexperts is given in the last graph of Figure 3.5.4 MethodologyWe performed held-out testing by randomly se-lecting a subset of 11 interactions (out of 47) forthe test set.
The training set contains the remain-ing 36 dyadic interactions.
All models in this pa-per were evaluated with the same training and testsets.
Validation of all model parameters (regular-ization term and number of hidden states) was per-formed using a 3-fold cross-validation strategy onthe training set.
The regularization term was vali-dated with values 10k, k = ?1..3.
Three differentnumber of hidden states were tested for the LMDEmodels: 2, 3 and 4.The performance is measured by using the F-measure.
This is the weighted harmonic meanof precision and recall.
Precision is the proba-bility that predicted backchannels correspond toactual listener behavior.
Recall is the probabil-ity that a backchannel produced by a listener inour test set was predicted by the model.
We usethe same weight for both precision and recall, so-called F1.
During validation we find all the peaks(i.e., local maxima) from the marginal probabil-ities.
These backchannel hypotheses are filteredusing the optimal threshold from the validationset.
A backchannel (i.e., head nod) is predictedcorrectly if a peak happens during an actual lis-tener backchannel with high enough probability.The same evaluation measurement is applied to allmodels.The training of all CRFs and LDCRFs weredone using the hCRF library2.
The LMDE modelwas implemented in Matlab3 based on the hCRF2http://sourceforge.net/projects/hrcf/3The source code is available at:http://projects.ict.usc.edu/multicomp/.865Table 1: Comparison of individual experts withour Latent Mixture of Discriminative Experts(LMDE).Expert Precision Recall f1Lexical 0.1647 0.3305 0.2198Prosody 0.1396 0.9112 0.2421Syntactic 0.1833 0.4663 0.2632POS 0.1935 0.4514 0.2709Eye Gaze 0.1573 0.1741 0.1653LMDE 0.2295 0.5677 0.32680 0.2 0.4 0.6 0.8 100.10.20.30.40.50.60.7RecallPrecisionLatent MixtureLexicalProsodySyntacticPOSEyeGazeFigure 4: Comparison of individual experts withour LMDE model.library.6 Results and DiscussionIn this section we present the results of our empiri-cal evaluation designed to test the three main char-acteristics of the LMDE model: (1) integration ofmultiple sources of information, (2) late fusion ap-proach and (3) latent variable which models thehidden dynamic between experts.
We also presentan analysis of the output probabilities from theLMDE model and individual experts.INDIVIDUAL EXPERTS We trained one individ-ual expert for each feature types: visual, prosodic,lexical and syntactic features (both part-of speechand syntactic structure).
Precision, recall and F1values for each individual expert and our LMDEmodel are shown in Table 1 and Figure 4.Pairwise two-tailed t-test comparison betweenour LMDE model and individual experts shows aTable 2: Comparison of our Latent Mixture ofDiscriminative Experts (LMDE) with two earlyfusion technique (CRF vs LDCRF) and the CRFMixture of Experts (Smith et al, 2005).model Precision Recall f1LMDE 0.2295 0.5677 0.3268Early CRF 0.13958 0.9245 0.2425Early LDCRF 0.1826 0.2484 0.2105Mixture CRF 0.1502 0.2712 0.1934significant difference for Lexical, Prosody, Syn-tactic and Eye gaze, with respective p-values of0.0037, 0.0379, 0.0400 and 0.0233.
Even thoughsome experts may not perform well individually(e.g., eye gaze), they can bring important informa-tion once merged with others.
Table 1 shows thatour LMDE model was able to take advantage ofthe complementary information from each expert.LATE FUSION We compare our approach withtwo early fusion models: CRF and Latent-dynamic CRF (see Figure 3).
Table 2 summarizesthe results.
The CRF model learns direct weightsbetween input features and the gesture labels.
TheLDCRF is able to model more complex dynam-ics between input features with the latent variable.We can see that our LMDE model outperformsboth early fusion approaches because of its latefusion approach.
Pairwise two-tailed t-test analy-sis gives p-values of 0.0481 and 0.0748, for CRFand LDCRF respectively.LATENT VARIABLE The CRF Mixture of Ex-perts (2005) directly merges the expert outputswhile our model uses a latent variable to model thehidden dynamic between experts (see Figure 3).Table 2 summarizes the results.
Pairwise two-tailed t-test comparison between these two mod-els shows a significant difference with a p-valueof 0.0062.
This result is important since it showsthat our LMDE model does learn the hidden inter-action between experts.MODEL ANALYSIS To understand the multi-modal integration which happens at the latentvariable level in our LMDE model, Figure 5shows the output probabilities for all five individ-ual experts as well as our model.
The strength ofthe latent variable is to enable different weigting866Speaker input featuresExpertsLatentMixture Ground truth labelsWordsGaze ...Time(a) (b)5.6s 7.7s 10.3s...15.6s 17.0s 18.7s 20.5sFigure 5: Output probabilities from LMDE and individual experts for two different sub-sequences.
Thegray areas in the graph corresponds to ground truth backchannel feedbacks of the listener.of the experts at different point in time.By analyzing the sequence (a), we observe thatboth the POS and Syntactic experts learned thatwhen no words are present (i.e., pause) there isa high likelihood of backchennel feedback fromthe listener (shown at 5.6s and 10.3s).
These twoexperts are highly weighted (by one of the hid-den state) during this part of the sequence.
Also,both the Lexical and POS experts learned that theword ??that??
(and its part-of-speech) are impor-tant but since the speaker is not looking at thelistener when saying it, the output from LMDEmodel is low (see Figure 5, Sequence (a), 7.7s).By analyzing sequence (b), we see that the Lex-ical and POS experts learned the importance of the??and??
at 15.6s and 20.5s.
More importantly, wecan see at 17.0s and 18.7s that the influence ofthe POS and Syntactic experts have been reducedin the LMDE output probability.
This differenceof weighting shows that a different hidden state isactive during Sequence (b).7 ConclusionIn this paper we introduced a new modelcalled Latent Mixture of Discriminative Experts(LMDE) for learning predictive models of humancommunication behaviors.
Many of the interac-tions between speech and gesture happen at thesub-gesture or sub-word level.
LMDE learns au-tomatically the temporal relationship between dif-ferent modalities.
Since, we train separate expertsfor each modality, LMDE is capable of improv-ing the prediction performance even with limiteddatasets.We evaluated our model on the task of non-verbal feedback prediction (e.g., head nod).
Ourexperiments confirm the importance of combin-ing the four types of multimodal features: lexical,syntactic structure, eye gaze, and prosody.
LMDEis a generic model that can be applied to a widerange of problems.
As future work, we are plan-ning to test our model on dialog act classificationand multimodal behavior recognition tasks.AcknowledgementsThis material is based upon work supported bythe National Science Foundation under Grant No.0917321 and the U.S. Army Research, Develop-ment, and Engineering Command (RDECOM).The content does not necessarily reflect the posi-tion or the policy of the Government, and no offi-cial endorsement should be inferred.ReferencesAnderson, H., M. Bader, E.G.
Bard, G. Doherty, S. Garrod,S.
Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo,867H.
Thompson, and R. Weinert.
1991.
The mcrc map taskcorpus.
Language and Speech, 34(4):351?366.Bavelas, J.B., L. Coates, and T. Johnson.
2000.
Listeners asco-narrators.
JPSP, 79(6):941?952.Blunsom, P., T. Cohn, and M. Osborne.
2008.
A discrimi-native latent variable model for statistical machine trans-lation.
In ACL: HLT, pages 200?208.Burgoon, Judee K., Lesa A. Stern, and Leesa Dillman.
1995.Interpersonal adaptation: Dyadic interaction patterns.Cambridge University Press, Cambridge.Cassell, J. and M. Stone.
1999.
Living hand to mouth: Psy-chological theories about speech and gesture in interactivedialogue systems.
In AAAI.Cathcart, N., Jean Carletta, and Ewan Klein.
2003.
A shal-low model of backchannel continuers in spoken dialogue.In EACL, pages 51?58.Eisenstein, J., R. Barzilay, and R. Davis.
2008.
Gestural co-hesion for topic segmentation.
In ACL: HLT, pages 852?860.Eisentein, J. and R. Davis.
2007.
Conditional modality fu-sion for coreference.
In ACL, pages 352?359.Eyben, Florian, Martin Wo?llmer, and Bjo?rn Schuller.
2009.openEAR - Introducing the Munich Open-Source Emo-tion and Affect Recognition Toolkit.
In ACII, pages 576?581.Frampton, M., J. Huang, T. Bui, and S. Peters.
2009.Real-time decision detection in multi-party dialogue.
InEMNLP, pages 1133?1141.Fuchs, D. 1987.
Examiner familiarity effects on test perfor-mance: implications for training and practice.
Topics inEarly Childhood Special Education, 7:90?104.Fujie, Shinya, Yasuhi Ejiri, Kei Nakajima, Yosuke Mat-susaka, and Tetsunori Kobayashi.
2004.
A conversationrobot using head gesture recognition as para-linguistic in-formation.
In RO-MAN, pages 159?164.Goldberg, S.B.
2005.
The secrets of successful mediators.Negotiation Journal, 21(3):365?376.Gravano, A., S. Benus, H. Chavez, J. Hirschberg, andL.
Wilcox.
2007.
On the role of context and prosodyin the interpretation and ?okay?.
In ACL, pages 800?807.Heylen, D. and R. op den Akker.
2007.
Computingbackchannel distributions in multi-party conversations.
InACL:EmbodiedNLP, pages 17?24.Johnston, M. 1998.
Multimodal language processing.
InICSLP.Jovanovic, N., R. op den Akker, and A. Nijholt.
2006.Adressee identification in face-to-face meetings.
InEACL.Jurafsky, D., E. Shriberg, B.
Fox, and T. Curl.
1998.
Lexical,prosodic and syntactic cures for dialog acts.
In Workshopon Discourse Relations, pages 114?120.Kendon, A.
2004.
Gesture: Visible Action as Utterance.Cambridge University Press.Kumar, S. and M. Herbert.
2003.
Discriminative randomfields: A framework for contextual interaction in classifi-cation.
In ICCV.Lafferty, J., A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: probabilistic models for segmenting andlabelling sequence data.
In ICML.Maatman, M., J. Gratch, and S. Marsella.
2005.
Naturalbehavior of a listening agent.
In IVA.Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
The penn treebank: anno-tating predicate argument structure.
In ACL:HLT, pages114?119.McNeill, D. 1992.
Hand and Mind: What Gestures Revealabout Thought.
Univ.
Chicago Press.Moore, P.-Y.
Hsueh J.
2007.
What decisions have you made:Automatic decision detection in conversational speech.
InNAACL-HLT, pages 25?32.Morency, Louis-Philippe, Ariadna Quattoni, and Trevor Dar-rell.
2007.
Latent-dynamic discriminative models forcontinuous gesture recognition.
In CVPR.Murray, G. and G. Carenini.
2009.
Predicting subjectivity inmultimodal conversations.
In EMNLP, pages 1348?1357.Nakano, Reinstein, Stocky, and Justine Cassell.
2003.
To-wards a model of face-to-face grounding.
In ACL.Nakano, Y., K. Murata, M. Enomoto, Y. Arimoto, Y. Asa,and H. Sagawa.
2007.
Predicting evidence of understand-ing by monitoring user?s task manipulation in multimodalconversations.
In ACL, pages 121?124.Nishimura, Ryota, Norihide Kitaoka, and Seiichi Nakagawa.2007.
A spoken dialog system for chat-like conversationsconsidering response timing.
LNCS, 4629:599?606.Oviatt, S. 1999.
Ten myths of multimodal interaction.
Com-munications of the ACM.Quek, F. 2003.
The catchment feature model for multimodallanguage analysis.
In ICCV.Sagae, Kenji and Jun?ichi Tsujii.
2007.
Dependency parsingand domain adaptation with LRmodels and parser ensem-bles.
In ACL, pages 1044?1050.Smith, A., T. Cohn, and M. Osborne.
2005.
Logarithmicopinion pools for conditional random fields.
In ACL,pages 18?25.Ward, N. and W. Tsukahara.
2000.
Prosodic features whichcue back-channel responses in english and japanese.Journal of Pragmatics, 23:1177?1207.868
