A Compar i son  of  A l ignment  Mode ls  for S ta t i s t i ca l  Mach ineTrans la t ionFranz Josef Och and Hermann NeyLehrstuhl fiir Informatik VI, Comlmter Science DepartmentRWTH Aachen - University of TechnologyD-52056 Aachen, Germany{och, ney}~inf ormat ik.
ruth-aachen, deAbst ractIn this paper, we t)resent and compare various align-nmnt models for statistical machine translation.
Wepropose to measure tile quality of an aligmnentmodel using the quality of the Viterbi alignmentcomt)ared to a manually-produced alignment and de-scribe a refined mmotation scheme to produce suit-able reference alignments.
We also con,pare the im-pact of different; alignment models on tile translationquality of a statistical machine translation system.1 I n t roduct ionIn statistical machine translation (SMT) it is neces-sm'y to model the translation probability P r ( f l  a Ic~).Here .fi' = f denotes tile (15'ench) source and e{ = edenotes the (English) target string.
Most SMTmodels (Brown et al, 1993; Vogel et al, 1996)try to model word-to-word corresl)ondences betweensource and target words using an alignment nmpl)ingfrom source l)osition j to target position i = aj.We can rewrite tim t)robal)ility Pr(fille~) t) 3, in-troducing the 'hidden' alignments ai 1 := al ...aj...a.l(aj C {0 , .
.
.
, /} ) :Pr(f~lel) = ~Pr(f i ' ,a~le{).1?
j -1  I~ = E H Pr(fj 'ajlf i '-"al 'e l )q, j=lTo allow fbr French words wlfich do not directly cor-respond to any English word an artificial 'empty'word c0 is added to the target sentence at positioni=0.The different alignment models we present pro-vide different decoInt)ositions of Pr(f~,a~le().
Analignnlent 5~ for which holdsa~ = argmax Pr(fi' , a'l'\[eI)atfor a specific model is called V i terb i  al ignment of"this model.In this paper we will describe extensions to tileHidden-Markov alignment model froln (Vogel et al,1.996) and compare tlmse to Models 1 - 4 of (Brownet al, 1993).
We t)roI)ose to measure the quality ofan alignment nlodel using the quality of tlle Viterbialignment compared to a manually-produced align-ment.
This has the advantage that once having pro-duced a reference alignlnent, the evaluation itself canbe performed automatically.
In addition, it results ina very precise and relia.ble valuation criterion whichis well suited to assess various design decisions inmodeling and training of statistical alignment mod-els.It, is well known that manually pertbrming a wordaligmnent is a COlnplicated and ambiguous task(Melamed, 1998).
Therefore, to produce tlle refer-ence alignment we use a relined annotation schemewhich reduces the complications and mnbiguities oc-curring in the immual construction of a word align-ment.
As we use tile alignment models for machinetranslation purposes, we also evahlate the resultingtranslation quality of different nlodels.2 Al ignment  w i th  HMMIn the Hidden-Markov alignment model we assumea first-order dependence for tim aligmnents aj andthat the translation probability depends Olfly on ajand not  Oil (tj_l:- ~-' el) =p(ajl.
j-,,Z)p(J~l%) Pr(fj,(glf~ ',% ,Later, we will describe a refinement with a depen-dence on e,,j_, iu the alignment model.
Puttingeverything together, we have the following basicHMM-based modeh.1*'(flJl~I) = ~ I I  \[~,(-jla~.-,, z).
p(fj l%)\] (1)at j= lwith the alignment I)robability p(ili',I ) and thetranslation probability p(fle).
To find a Viterbialigninent for the HMM-based model we resort todynamic progralnming (Vogel et al, 1996).The training of tlm HMM is done by the EM-algorithm.
In the E-step the lexical and alignment1086counts for one sentenee-i)air (f, e) are calculated:c(flc; f, e) = E P"(a l f '  e) ~ 5(f, f~)5(e, c~)a i,j,.
:(ill', z; f, e) = E / ' , ' (a i r ,  e) aj)a jIn the M-step the lexicon and translation probabili-ties are:p(f le) o< ~-~c(fle;f('~),e (~))8P( i l i ' , I )  o (Ec ( i l i ' , I ; fO) ,e (~) )8To avoid the smlunation ov(;r all possible aligmnentsa, (Vogel et el., 1996) use the maximum apllroxima-tion where only the Viterbi alignlnent )ath is used tocollect counts.
We used the Baron-Welch-algorithm(Baum, 1972) to train the model parameters in out'ext)eriments.
Theret/y it is possible to t)erti)rm anefl-iciellt training using; all aligmnents.To make the alignlnenl; t)arameters indo,1)en(lentt'ronl absolute word i)ositions we assmne that thealignment i)robabilities p(i\[i', I )  (lel)end only Oil thejmnp width (i - i').
Using a set of non-negativet)arameters {c(i - i ' )} ,  we can write the alignmentprobabilities ill the fl)rm:~'(i - i') (2) p(i l i ' ,  I)  =c(,,:" - i ' )This form ensures that for eadl word posilion it,i' = 1, ..., I , the aligmnent probat)ilities atis(y th(,normalization constraint.Extension:  refined a l igmnent mode lThe count table e(i - i') has only 2.1  ......... - 1 en-tries.
This might be suitable for small corpora, butfi)r large corpora it is possil)le to make a more re-fine(1 model of Pr (a j  ~i-I  i - I  Ji ,% ,c'~).
Est)ecially, weanalyzed the effect of a det)endence on c,b_ ~ or .fj.As a dependence on all English words wouht resultill a huge mmflmr of aligmnent 1)arameters we use as(Brown et el., 1993) equivalence classes G over tlleEnglish and the French words.
Here G is a mallpingof words to (:lasses.
This real)ping is trained au-tonmtically using a modification of the method de-scrilled ill (Kneser and Ney, 1991.).
We use 50 classesin our exlmriments.
The most general form of align-ment distribution that we consider in the ItMM isp(aj - a.+_, la(%), G(f~), h -Extension:  empty  wordIn the original formulation of the HMM alignmentmodel there ix no 'empty' word which generatesFren(:h words having no directly aligned Englishword.
A direct inchlsion of an eml/ty wor(t ill theHMM model by adding all c o as in (Brown et al,1.993) is not 1)ossit)le if we want to model the j un lpdistances i - i', as the I)osition i = 0 of tim emt)tyword is chosen arbitrarily.
Therefore, to introducethe eml)ty word we extend the HMM network by Iempty words ci+ 1.
'2I The English word ci has a co lrest)onding eml)ty word el+ I.
The I)osition of theeml)ty word encodes the previously visited Englishword.We enforce the following constraints for the tran-sitions in the HMM network (i _< I, i' _< I):p(i  + I l i ' , I )  = pff .
5( i , i ' )V(i + I l l '  + I, I )  = J J .
5( i , i ' )p(i l i '  + I, 1) = p(iIi ' ,1)The parameter pff is the 1)robability of a transitionto the emt)ty word.
In our extleriments we set pIl =0.2.Smooth ingFor a t)etter estimation of infrequent events we in-troduce the following smoothing of alignment )rob-abilities:1F(a j I~ j - , ,~)  = ~" ~- + (1 - , , ) .p (a j la j _ l  , I )in our exlleriments we use (t = 0.4.3 Mode l  1 and  Mode l  2l~cl)lacing the (l(~,t)endence on aj - l  in the HMMalignment mo(M I)y a del)endence on j, we olltaina model wlfich (:an lie seen as a zero-order Hid(l(m-Markov Model which is similar to Model 2 1)rot)ose(tt/y (Brown et al, 1993).
Assmning a mfiform align-ment prol)ability p(i l j ,  I )  = 1/1, we obtain Model1.Assuming that the dominating factor in the align-ment model of Model 2 is the distance relative to thediagonal line of the (j, i) plane the too(tel p(i l j  , I)  can1)e structured as tbllows (Vogel et al, 1996):,'(i -,- (3) v(ilj, 5 = Ei,=t r ( ' i '  lThis model will be referred to as diagonal-orientedModel 2.4 Mode l  3 and  Mode l  4Model:  The fertility models of (Brown et el., 1993)explicitly model the probability l,(?lc) that the En-glish word c~ is aligned to4,, = EJ\]~rench words.1087Model 3 of (Brown et al, 1993) is a zero-orderalignment model like Model 2 including in addi-tion fertility paranmters.
Model 4 of (Brown et al,1993) is also a first-order alignment model (alongthe source positions) like the HMM, trot includesalso fertilities.
In Model 4 the alignment positionj of an English word depends on the alignment po-sition of tile previous English word (with non-zerofertility) j ' .
It models a jump distance j - j '  (for con-secutive English words) while in the HMM a jumpdistance i - i '  (for consecutive French words) is mod-eled.
Tile full description of Model 4 (Brown et al,1993) is rather complica.ted as there have to be con-sidered tile cases that English words have fertilitylarger than one and that English words have fertil-ity zero.For training of Model 3 and Model 4, we use anextension of the program GlZA (A1-Onaizan et al,1999).
Since there is no efficient way in these mod-els to avoid tile explicit summation over all align-ments in the EM-algorithin, the counts are collectedonly over a subset of promising alignments.
It is notknown an efficient algorithm to compute the Viterbialignment for the Models 3 and 4.
Therefore, theViterbi alignment is comlmted only approximatelyusing the method described in (Brown et al, 1993).The models 1-4 are trained in succession with thetinal parameter values of one model serving as thestarting point tbr the next.A special problein in Model 3 and Model 4 con-cerns the deficiency of tile model.
This results inproblems in re-estimation of the parameter whichdescribes the fertility of the empty word.
In nor-real EM-training, this parameter is steadily decreas-ing, producing too many aligmnents with tile emptyword.
Therefore we set tile prot)ability for aligninga source word with tile emt)ty word at a suitablychosen constant value.As in tile HMM we easily can extend the depen-dencies in the alignment model of Model 4 easilyusing the word class of the previous English wordE = G(ci,), or the word class of the French wordF = G(I j)  (Brown et al, 1993).5 Inc lud ing  a Manual DictionaryWe propose here a simple method to make use ofa bilingual dictionary as an additional knowledgesource in the training process by extending the train-ing corpus with the dictionary entries.
Thereby, thedictionary is used already in EM-training and canimprove not only the alignment fox" words which arein the dictionary but indirectly also for other words.The additional sentences in the training cortms areweighted with a factor Fl~x during the EM-trainingof the lexicon probabilities.We assign tile dictionary entries which really co-occur in the training corpus a high weight Fle.~.
andthe remaining entries a vex'y low weight.
In our ex-periments we use Flex = 10 for the co-occurring dic-tionary entries which is equivalent to adding everydictionary entry ten times to the training cortms.6 The Al ignment Template  SystemThe statistical machine-translation method descri-bed in (Och et al, 1999) is based on a word alignedtraiifing corIms and thereby makes use of single-word based alignment models.
Tile key element oftiffs apt/roach are the alignment emplates which arepairs of phrases together with an alignment betweenthe words within tile phrases.
The advantage ofthe alignment emplate approach over word basedstatistical translation models is that word contextand local re-orderings are explicitly taken into ac-count.
We typically observe that this approach pro-duces better translations than the single-word basedmodels.
The alignment templates are automaticallytrailmd using a parallel trailxing corlms.
For moreinformation about the alignment template approachsee (Och et at., 1999).7 Resu l tsWe present results on the Verbmobil Task which isa speech translation task ill the donmin of appoint-nxent scheduling, travel planning, and hotel reserva-tion (Wahlster, 1993).We measure the quality of tile al)ove inentionedaligmnent models with x'espect to alignment qualityand translation quality.To obtain a refereuce aligmnent for evaluatingalignlnent quality, we manually aligned about 1.4percent of onr training corpus.
We allowed the hu-mans who pertbrmed the alignment o specify twodifferent kinds of alignments: an S (sure) a, lignmentwhich is used for alignmelxts which are unambigu-ously and a P (possible) alignment which is usedfor alignments which might or might not exist.
TheP relation is used especially to align words withinidiomatic expressions, free translations, and missingfunction words.
It is guaranteed that S C P. Figure1 shows all example of a manually aligned sentencewith S and P relations.
The hunxan-annotated align-ment does not prefer rely translation direction andlnay therefore contain many-to-one and one-to-manyrelationships.
The mmotation has been performedby two annotators, producing sets $1, 1~, S2, P2.Tile reference aliglunent is produced by forming theintersection of the sure aligmnents (S = $1 rqS2) andthe ration of the possible atignumnts (P = P1 U P'2).Tim quality of an alignment A = { (j, aj) } is mea-sured using the following alignment error rate:AER(S, P; A) = 1 - IA o Sl + IA o PlIAI + ISl1088that  .
.
.
.
.
.
.
.
.
\ [ \ ]at  .
.
.
.
.
.
.
.
.
\ [ \ ].
.
.
.
.
.
.
V1V1.l eave  .
.
.
.
.
.
.
\[---'l \ [ - "~ ".
.
.
.
.
.
.
l i E \ ] .l e t  .
.
.
.
.
.
.
C l l -1  "e .
.
.
.
.
.
?
.
.
.
.say  .
.
.
.
.
?
.
.
.
.
.would " ?
.
.
.
.
.
.
.T .
.
.
.
?
.
.
.
.
.
.then"  " ?
.
.
.
.
.
.
.
.?
\ [ \ ]  .
.
.
.
.
.
.
.
oyes  ?
.
.
.
.
.
.
.
.
.
.-rn I:I '13 O ?
?
-~t ~1J~oFigure i: Exmnple of a manually annotated align-ment with sure (filled dots) and possible commotions.Obviously, if we colnpare the sure alignnlents of ev-ery sitigle annotator with the reference a.ligmnent weobtain an AEI{ of zero percent.~\[ifl)le l.: Cort)us characteristics for alignment qualityexperiments.Train Sente iH : ( i sWordsVocalmlaryDictionary EntriesWordsTest SentencesWordsGerman I English34 446329 625 / 343 0765 936 \] 3 5054 1834 533 I 5 3243543 109 I 3 233Tal)le 1 shows the characteristics of training andtest corlms used in the alignment quality ext)eri-inents.
The test cortms for these ext)eriments (notfor the translation exl)eriments) is 1)art of the train-ing corpus.Table 2 shows the aligmnent quality of differentalignment models.
Here the alignment models ofIIMM and Model 4 do not include a dependenceon word classes.
We conclude that more sophisti-cated alignment lnodels are crtlcial tbr good align-ment quality.
Consistently, the use of a first-orderaligmnent model, modeling an elnpty word and fer-tilities result in better alignments.
Interestingly, thesiinl)ler HMM aligninent model outt)erforms Model3 which shows the importance of first-order align-ment models.
The best t)erformanee is achievedwith Model 4.
The improvement by using a dictio-nary is small eomI)ared to the effect of using 1)ettera.lignmellt models.
We see a significant dill'erencein alignment quality if we exchange source and tar-get languages.
This is due to the restriction in allalignment models that a source language word can1)e aligned to at most one target language word.
IfGerman is source language the t'requelltly occurringGerman word coml)ounds, camlot be aligned cor-rectly, as they typically correspond to two or moreEnglish words.WaNe 3 shows the effect of including a det)endenceon word classes in the aligmnent model of ItMM orModel 4.
By using word classes the results can beTable 3: Eft'cot of including a det)endence on wordclasses in the aligmnent model.AER \[%\]Det)endencies -IIMM I Model 4no 8.0 6.5source 7.5 6.0target 7.1 6.1source ?
target 7.6 6.1improved by 0.9% when using the ItMM and by 0.5%when using Model 4.For the translation experiments we used a differ-ent training and an illdetmndent test corpus (Table4).Table 4: Corlms characteristics for translation (tual-it;.
), exlmriments.TrainS ~e,tSentencesWordsVocabularySe l l te l leesWordsPP (trigram LM)I German English58332519523 5499217 940 4 6731471968 2173(40.3) 28.8For tile evMuation of the translation quality weused the automatically comlmtable Word Error Rate(WEll.)
and the Subjective Sentence Error Rate(SSEll,) (Niefien et al, 2000).
The WEll, corre-spomls to the edit distance t)etween the producedtranslation and one t)redefined reference translation.To obtain the SSER the translations are classified byhuman experts into a small number of quality classesranging from "l)ertbet" to "at)solutely wrong".
Incomparison to the WEll,, this criterion is more mean-ingflfl, but it is also very exl)ensive to measure.
Thetranslations are produced by the aligmnent templatesystem mentioned in the previous ection.1089Table 2: Alignment error rate (AER \[%\]) of ditl~rent alignment models tbr the translations directions Englishinto German (German words have fertilities) and German into English.English -+ German German -~ EnglishDictionary no yes no yesEmpty Word no lYes yes no l yes yesModel 1 17.8 16.9 16.0 22.9 21.7 20.3Model 2 12.8 12.5 11.7 17.5 17.1 15.7Model 2(diag) 11.8 10.5 9.8 16.4 15.1 13.3Mode l  3 10.5 9.3 8.5 15.7 14.5 12.1HMM 10.5 9.2 8.0 14.1 12.9 11.5Model 4 9.0 7.8 6.5 14.0 12.5 10.8Table 5: Effect of different alignment models ontranslation quality.Alignlnent Modelin Training WER\[%\] SSER\[%\]Model 1 49.8 22.2HMM 47.7 19.3Model 4 48.6 16.8The results are shown in Table 5.
We see a clearimprovement in translation quality as measured bySSER whereas WER is inore or less the same for allmodels.
The imwovement is due to better lexiconsand better alignment templates extracted from theresulting aliglunents.8 ConclusionWe have evaluated vm'ious statistical alignmentmodels by conlparing the Viterbi alignment of themodel with a human-made alignment.
We haveshown that by using inore sophisticated models thequality of the alignments improves ignificantly.
Fur-ther improvements in producing better alignmentsare expected from using the HMM alignment modelto bootstrap the fertility models, fronl making use ofcognates, and from statistical lignment models thatare based on word groups rather than single words.AcknowledgmentThis article has been partially supported aspart of the Verbmobil project (contract nmnber01 IV 701 T4) by the German Federal Ministry ofEducation, Science, Research and Technology.ReferencesY.
A1-Onaizan, J. Cur\]n, M. Jahr, K. Knight, J. Laf-ferty, I. D. Melamed, F. a. Och, D. Purdy, N. A.Smith, and D. Yarowsky.
1999.
Statistical ina-chine translation, final report, JHU workshop.http ://www.
clsp.
j hu.
edu/ws99/proj ects/mt/f inal_report/mr- f inal-report, ps.L.E.
Baum.
1972.
An Inequality and AssociatedMaximization Technique in Statistical Estimationfor Probabilistie Functions of Markov Processes.Inequalities, 3:1 8.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
The mathenlatics ofsta-tistical machine trmlslation: Parameter estima-tion.
Computational Linguistics, 19(2):263-311.R.
Kneser and H. Ney.
1991.
Forming Word Classesby Statistical Clustering for Statistical Langm~geModelling.
In 1.
Quantitative Linguistics Conf.I.
D. Melamed.
1998.
Manual mmotation of transla-tional equivalence: The Blinker project.
TechnicalReport 98-07, IRCS.S.
Niegen, F. J.
()ch, G. Leusch, and H. Ney.2000.
An evaluation tool \]'or machine translation:Fast evaluation for mt research.
In Proceedings ofthe Second International Conference on LanguageResources and Evaluation, pages 39-45, Athens,Greece, May June.F.
J. Och, C. Tilhnalm, mid H. Ney.
1999.
Improvedalignment models for statistical machine transla-tion.
In In Prec.
of the Joint SIGDAT Co~?
onEmpirical Methods in Natural Language Process-ing and Very LaTye Corpora, pages 20-28, Univer-sity of Marylmld, College Park, MD, USA, June.S.
Vogel, H. Ney, and C. Tilhnann.
1996.
HMM-based word alignment in statistical translation.In COLING '96: The 16th Int.
Conf.
on Compu-tational Linguistics, pages 836-841, Copenhagen,August.W.
Wahlster.
1993.
Verbmobil: Translation of face-to-face dialogs.
In P~vc.
of the MT Summit IV,pages 127-135, Kobe, Jat)an.1090
