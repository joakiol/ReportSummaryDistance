Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 36?44,Athens, Greece, 31 March, 2009. c?2009 Association for Computational LinguisticsUnsupervised Concept Discovery In Hebrew Using Simple UnsupervisedWord Prefix Segmentation for Hebrew and ArabicElad Dinur1 Dmitry Davidov21Institute of Computer Science2ICNCThe Hebrew University of JerusalemAri Rappoport1AbstractFully unsupervised pattern-based methodsfor discovery of word categories have beenproven to be useful in several languages.The majority of these methods rely on theexistence of function words as separatetext units.
However, in morphology-richlanguages, in particular Semitic languagessuch as Hebrew and Arabic, the equiva-lents of such function words are usuallywritten as morphemes attached as prefixesto other words.
As a result, they are missedby word-based pattern discovery methods,causing many useful patterns to be unde-tected and a drastic deterioration in per-formance.
To enable high quality lexicalcategory acquisition, we propose a sim-ple unsupervised word segmentation algo-rithm that separates these morphemes.
Westudy the performance of the algorithm forHebrew and Arabic, and show that it in-deed improves a state-of-art unsupervisedconcept acquisition algorithm in Hebrew.1 IntroductionIn many NLP tasks, we wish to extract informa-tion or perform processing on text using minimalknowledge on the input natural language.
Towardsthis goal, we sometimes find it useful to divide theset of words in natural language to function wordsand content words, a division that applies in thevast majority of languages.
Function words (orgrammatical words, e.g., a, an, the, in, of, etc) arewords that have little or highly ambiguous lexi-cal meaning, and serve to express grammatical orsemantic relationships with the other words in asentence.In some morphologically-rich languages, im-portant function words are not written as space-separated units but as morphemes attached as pre-fixes to other words.
This fact can cause prob-lems when statistically analyzing text in these lan-guages, for two main reasons: (1) the vocabularyof the language grows, as our lexical knowledgecomes solely from a corpus (words appear withand without the function morphemes); (2) infor-mation derived from the presence of these mor-phemes in the sentence is usually lost.In this paper we address the important task ofa fully unsupervised acquisition of Hebrew lexicalcategories (or concepts ?
words sharing a signifi-cant aspect of their meaning).
We are not aware ofany previous work on this task for Hebrew.
Dueto the problem above, the performance of manyacquisition algorithms deteriorates unacceptably.This happens, for example, in the (Davidov andRappoport, 2006) algorithm that utilizes automati-cally detected function words as the main buildingblock for pattern construction.In order to overcome this problem, one shouldseparate such prefixes from the compound words(words consisting of function morphemes attachedto content words) in the input corpus.
Whenwe consider some particular word, there are fre-quently many options to split it to smaller strings.Fortunately, the set of function words is small andclosed, and the set of grammatical sequences offunction prefixes is also small.
Hence we assumeit does not cost us much to know in advance whatare the possible sequences for a specific language.Even when considering the small number ofpossible function words, the task of separatingthem is not simple, as some words may be ambigu-ous.
When reading a word that starts with a prefixknown to be a function morpheme, the word may36be a compound word, or it may be a meaningfulword by itself.
For example, the word ?hsws?
inHebrew1 can be interpreted as ?hsws?
(hesitation),or ?h sws?
(the horse).
The segmentation of theword is context dependent ?
the same string maybe segmented differently in different contexts.One way of doing such word prefix segmenta-tion is to perform a complete morphological dis-ambiguation of the sentence.
The disambigua-tion algorithm finds for each word its morpho-logical attributes (POS tag, gender, etc.
), and de-cides whether a word is a compound word or aword without prefixes.
A disambiguation algo-rithm generally relies on a language-specific mor-phological analyzer.
It may also require a largemanually tagged corpus, construction of which forsome particular language or domain requires sub-stantial human labor.
We avoid the utilization ofsuch costly and language-specific disambiguationalgorithms and manually annotated data.In this paper we present a novel method to sep-arate function word prefixes, and evaluate it us-ing manually labeled gold standards in Hebrewand Arabic.
We incorporate the method into apattern-based Hebrew concept acquisition frame-work and show that it greatly improves state-of-artresults for unsupervised lexical category acquisi-tion.
This improvement allows the pattern-basedunsupervised framework to use one-tenth of theHebrew data in order to reach a similar level ofresults.Section 2 discusses related work, and Section 3reviews the word categories discovery algorithm.Section 4 presents the word prefix segmentationalgorithm.
Results are given in Section 5.2 Related WorkIn this paper we develop an unsupervised frame-work for segmentation of the function words forlanguages where context is important for correctsegmentation.
Our main target language is He-brew, and we experimented with Arabic as well.As far as we know, there is no work on unsu-pervised segmentation of words in Hebrew whichdoes not utilize language-specific tools such asmorphological analyzers.Lee et al (2003) addressed supervised wordsegmentation in Arabic and have some aspectssimilar to our approach.
As in their study, we1Transcription is according to (Ornan, 2005), except forShin which is denoted by ?$?.also have a pre-supplied list of possible prefixsequences and assume a trigram model in orderto find the most probable morpheme sequence.Both studies evaluate performance on a segmentedtext, and not just on words in the lexicon.
How-ever, their algorithm, while achieving good per-formance (97% accuracy), relies on a training set?
a manually segmented corpus of about 110,000words, while our unsupervised framework doesnot require any annotation and is thus easier to im-plement and to apply to different domains and lan-guages.Snyder and Barzilay (2008) study the task of un-supervised morphological segmentation of multi-ple languages.
Their algorithm automatically in-duces a segmentation and morpheme alignment ofshort parallel phrases from a multilingual corpus.Their corpus (The Hebrew Bible and translations)contains parallel phrases in English, Arabic, He-brew and Aramaic.
They obtain 63.87 F-Scorefor Hebrew words segmentation (prefix and suf-fix), where recall and precision is calculated basedon all possible segmentation points.Another type of segmentation algorithms in-volves utilization of language-specific morpholog-ical analyzers for complete morphological disam-biguation.
In Hebrew each word usually has morethan one possible POS (along with other attributes,such as gender, number, etc.).
Assuming we havea morphological analyzer (producing the set ofpossible analyses for a given word), we can try todiscover the correct segmentation of each word.Levinger et al (1995) developed a method fordisambiguation of the results provided by a mor-phological analyzer for Hebrew.
Adler and El-hadad (2006) proposed an unsupervised algorithmfor word segmentation.
They estimate an initiallanguage model (using (Levinger et al, 1995))and improve this model with EM.
Direct compar-ison to their work is problematic, however, sincewe avoid utilization of a language-specific mor-phology/POS analyzer.
There are also studies ofthis type that utilize labeled data (Bar-Haim et al,2005), where the language model is learned fromthe training data.Extensive research has been done on word seg-mentation, where, unlike in our study, the segmen-tation is evaluated for every word, regardless of itscontext.
Creutz (2003) presents an algorithm forunsupervised segmentation under these assump-tions.
He proposes a probabilistic model which37utilizes the distributions of morpheme length andfrequency to estimate the quality of the inducedmorphemes.
Dasgupta and Ng (2007) improvesover (Creutz, 2003) by suggesting a simpler ap-proach.
They segment a prefix using the wordfrequency with and without a prefix.
Other re-cent studies that follow the context-independentsetup include (Creutz and Lagus, 2005; Keshavaand Pitler, 2005; Demberg, 2007).
They testtheir methods on English, Finnish and Turkish.All of these studies, however, assume context-independency of segmentation, disregarding theambiguity that may come from context.
Thismakes it problematic to apply the proposed meth-ods to context-dependent morphology types as inHebrew and Arabic.The guiding goal in the present paper is the con-cept acquisition problem.
Concept acquisition ofdifferent kinds has been studied extensively.
Thetwo main classification axes for this task are thetype of human input and annotation, and the basicalgorithmic approach used.
The two main algo-rithmic approaches are clustering of context fea-ture vectors and pattern-based discovery.The first approach is to map each word to a fea-ture vector and cluster these vectors.
Example ofsuch algorithms are (Pereira et al, 1993) and (Lin,1998) that use syntactic features in the vector def-inition.
Pantel and Lin (2002) improves on the lat-ter by clustering by committee.Recently, there is a growing interest in the sec-ond main algorithmic approach, usage of lexico-syntactic patterns.
Patterns have been shown toproduce more accurate results than feature vectors,at a lower computational cost on large corpora(Pantel et al, 2004).
Thus (Dorow et al, 2005)discover categories using two basic pre-specifiedpatterns (?x and y?, ?x or y?
).Some recent studies have proposed frameworksthat attempt to avoid any implicit or explicit pre-specification of patterns.
Davidov and Rappoport(2006) proposed a method that detects functionwords by their high frequency, and utilizes thesewords for the discovery of symmetric patterns.Their method is based on two assumptions: (1)some function words in the language symmetri-cally connect words belonging to the same cat-egory; (2) such function words can be detectedas the most frequent words in language.
Whilethese assumptions are reasonable for many lan-guages, for some morphologically rich languagesthe second assumption may fail.
This is due tothe fact that some languages like Hebrew and Ara-bic may express relationships not by isolated func-tion words but by morphemes attached in writingto other words.As an example, consider the English word?and?, which was shown to be very useful in con-cept acquisition (Dorow et al, 2005).
In Hebrewthis word is usually expressed as the morpheme?w?
attached to the second word in a conjunc-tion (?...
wsws?
?
?...
and horse?).
Patterns dis-covered by such automatic pattern discovery al-gorithms are based on isolated words, and hencefail to capture ?and?-based relationships that arevery useful for detection of words belonging to thesame concept.
Davidov and Rappoport (2006) re-ports very good results for English and Russian.However, no previous work applies a fully unsu-pervised concept acquisition for Hebrew.In our study we combine their concept ac-quisition framework with a simple unsupervisedword segmentation technique.
Our evaluation con-firms the weakness of word-based frameworks formorphology-rich languages such as Hebrew, andshows that utilizing the proposed word segmen-tation can overcome this weakness while keepingthe concept acquisition approach fully unsuper-vised.3 Unsupervised Discovery of WordCategoriesIn this study we use word segmentation to improvethe (Davidov and Rappoport, 2006) method fordiscovery of word categories, sets of words shar-ing a significant aspect of their meaning.
An ex-ample for such a discovered category is the set ofverbs {dive, snorkel, swim, float, surf, sail, drift,...}.
Below we briefly describe this category ac-quisition algorithm.The algorithm consists of three stages as fol-lows.
First, it discovers a set of pattern candidates,which are defined by a combination of high fre-quency words (denoted by H) and slots for lowfrequency (content) words (denoted by C).
An ex-ample for such a pattern candidate is ?x belongs toy?, where ?x?
and ?y?
stand for content word slots.The patterns are found according to a predefinedset of possible meta-patterns.
The meta-patternsare language-independent2 and consist of up to 42They do not include any specific words, only a relativeorder of high/low frequency words, and hence can be used on38words in total, from which two are (non-adjacent)content words.
Four meta-patterns are used: CHC,CHCH, CHHC, HCHC.Second, those patterns which give rise to sym-metric lexical relationships are identified.
Themeaning of phrases constructed from those pat-terns is (almost) invariant to the order of the con-tent words contained in them.
An example forsuch a pattern is ?x and y?.
In order to iden-tify such useful patterns, for each pattern we builda graph following (Widdows and Dorow, 2002).The graph is constructed from a node for each con-tent word, and a directed arc from the node ?x?
to?y?
if the corresponding content words appear inthe pattern such that ?x?
precedes ?y?.
Then wecalculate several symmetry measures on the graphstructure and select the patterns with best valuesfor these measures.The third stage is the generation of categories.We extract tightly connected sets of words fromthe unified graph which combines all graphs of se-lected patterns.
Such sets of words define the de-sired categories.The patterns which include the ?x and y?
sub-string are among the most useful patterns for gen-eration of categories (they were used in (Dorow etal., 2005) and discovered in all 5 languages testedin (Davidov and Rappoport, 2006)).
However, inHebrew such patterns can not be found in the sameway, since the function word ?and?
is the prefix ?w?and not a standalone high frequency word.Another popular set of patterns are ones includ-ing ?x or y?.
Such patterns can be identified inHebrew, as ?or?
in Hebrew is a separate word.However, even in this case, the content word rep-resented by ?x?
or ?y?
may appear with a pre-fix.
This damages the construction of the patterngraph, since two different nodes may be createdinstead of one ?
one for a regular content word,the other for the same word with a prefix.
Conse-quently, it is reasonable to assume that segmentingthe corpus in advance should improve the resultsof discovery of word categories.4 Word Segmentation AlgorithmWe assume we know the small and closed set ofgrammatical function word prefix sequences in thelanguage3.
Our input is a sentence, and our ob-any languages with explicit word segmentation.3Unlike development of labeled training data, handcraft-ing such a closed set is straightforward for many languagesand does not requires any significant time/human laborjective is to return the correct segmentation of thesentence.
A sentence L is a sequence of words{w1, w2, ..., wn}.
A segmentation Si of L is a se-quence of morphemes {m1, m2, ..., mk} and l(Si)is the number of morphemes in the sequence.
Notethat l(Si) may be different for each segmentation.The best segmentation S will be calculated by:P (Si) = p(m1)p(m2|m1)l(Si)?i=3p(mi|mi?1mi?2)S = arg maxSiP (Si)Calculation of joint probabilities requires a tri-gram model of the language.
Below we describethe construction of the trigram model and then wedetail the algorithm for efficient calculation of S.4.1 Construction of trigram modelCreating the trigram language model is done intwo stages: (1) we segment a corpus automati-cally, and (2) we learn a trigram language modelfrom the segmented corpus.4.1.1 Initial corpus segmentationFor initial corpus segmentation, we define a sta-tistical measure for the segmentation of individualwords.
Let wx be a word, such that w is the pre-fix of the word composed of a sequence of func-tion word prefixes and x is a string of letters.
Letf(x) be the frequency of the word x in the cor-pus.
Denote by al the average length of the strings(with prefixes) in the language.
This can be eas-ily estimated from the corpus ?
every string thatappears in the corpus is counted once.
l(x) is thenumber of characters in the word x.
We utilizetwo parameters G, H , where G < H (we usedG = 2.5, H = 3.5) and define the following func-tions :factor(x) ={al?G?l(x)al?H l(x) < al ?
G0 otherwiseRank(wx) =f(wx)f(wx) + f(x)+ factor(x)Note that the expression f(wx)f(wx)+f(x) is a numberin (0, 1], inversely correlated with the frequency ofthe prefixed word.
Thus higher Rank(wx) valuesindicate that the word is less likely to be composedof the prefix w followed by the word x.39The expression al?G?l(x)al?H is a number in (0, 1],therefore factor(x) ?
[0, 1].
H is G ?
1 in orderto keep the expression smaller than 1.
The termfactor(x) is greater as x is shorter.
The factoris meant to express the fact that short words areless likely to have a prefix.
We have examinedthis in Hebrew ?
as there are no words of length1, two letter words have no prefix.
We have ana-lyzed 102 randomly chosen three letter words, andfound that only 19 of them were prefixed words.We have analyzed 100 randomly chosen four let-ter words, and found that 40 of them were pre-fixed words.
The result was about the same forfive letter words.
In order to decide whether aword needs to be separated, we define a thresh-old T ?
[0, 1].
We allow word separation onlywhen Rank(wx) is lower than T .
When thereare more than two possible sequences of functionword prefixes (?mhsws?,?m hsws?, ?mh sws?
),we choose the segmentation with the lower rank.4.1.2 Learning the trigram modelThe learning of the language model is based oncounts of the corpus, assigning a special symbol,?u/k?
(unknown) for all words that do not appearin the corpus.
As estimated by (Lee et al, 2003),we set the probability of ?u/k?
to be 1E ?
9.
Thevalue of the symbol ?u/k?
was observed to be sig-nificant.
We found that the value proposed by (Leeet al, 2003) for Arabic gives good results also forHebrew.4.2 Dynamic programming approach forword segmentationThe naive method to find S is to iterate overall possible segmentations of the sentence.
Thismethod may fail to handle long sentences, asthe number of segmentations grows exponentiallywith the length of the sentence.
To overcome thisproblem, we use dynamic programming.Each morpheme has an index i to its place in asegmentation sequence.
Iteratively, for index i, forevery morpheme which appears in some segmen-tation in index i, we calculate the best segmen-tation of the sequence m1 .
.
.mi.
Two problemsarise here: (1) we need to calculate which mor-phemes may appear in a given index; (2) we needto constrain the calculation, such that only validsegmentations would be considered.To calculate which morphemes can appear in agiven index we define the object Morpheme.
Itcontains the morpheme (string), the index of aword in the sentence the morpheme belongs to,reference to the preceding Morpheme in the sameword, and indication whether it is the last mor-pheme in the word.
For each index of the sen-tence segmentation, we create a list of Morphemes(index-list).For each word wi, and for segmentationm1i , .., mki , we create Morphemes M1i , .., Mki .
Wetraverse sequentially the words in the sentence,and for each segmentation we add the sequence ofMorphemes to all possible index-lists.
The index-list for the first Morpheme M1i is the combinationof successors of all the index-lists that contain aMorpheme Mki?1.
The constraints are enforcedeasily ?
if a Morpheme M ji is the first in a word,the preceding Morpheme in the sequence must bethe last Morpheme of the previous word.
Oth-erwise, the preceding Morpheme must be M j?1i ,which is referenced by M ji .4.3 LimitationsWhile our model handles the majority of cases, itdoes not fully comply with a linguistic analysis ofHebrew, as there are a few minor exceptions.
Weassumed that there is no ambiguity in the functionword prefixes.
This is not entirely correct, as inHebrew we have two different kinds of exceptionsfor this rule.
For example, the prefix ?k$?
(when),can also be interpreted as the prefix ?k?
(as) fol-lowed by the prefix ?$?
(that).
As the second in-terpretation is rare, we always assumed it is theprefix ?k$?.
This rule was applied wherever anambiguity exists.
However, we did not treat thisproblem as it is very rare, and in the developmentset and test set it did not appear even once.A harder problem is encountered when process-ing the word ?bbyt?.
Two interpretations couldbe considered here: ?b byt?
(?in a house?
), and?b h byt?
(?in the house?).
Whether this actu-ally poses a problem or not depends on the ap-plication.
We assume that the correct segmenta-tion here is ?b byt?.
Without any additional lin-guistic knowledge (for example, diacritical vowelsymbols should suffice in Hebrew), solving theseproblems requires some prior discriminative data.5 Evaluation and ResultsWe evaluate our algorithm in two stages.
First wetest the quality of our unsupervised word segmen-tation framework on Hebrew and Arabic, compar-ing our segmentation results to a manually anno-40With factor(x) Without factor(x)T Prec.
Recall F-Measure Accuracy Prec.
Recall F-Measure Accuracy0.70 0.844 0.798 0.820 0.875 0.811 0.851 0.830 0.8810.73 0.841 0.828 0.834 0.883 0.808 0.866 0.836 0.8840.76 0.837 0.846 0.841 0.886 0.806 0.882 0.842 0.8870.79 0.834 0.870 0.851 0.893 0.803 0.897 0.847 0.8900.82 0.826 0.881 0.852 0.892 0.795 0.904 0.846 0.8880.85 0.820 0.893 0.854 0.892 0.787 0.911 0.844 0.8860.88 0.811 0.904 0.855 0.891 0.778 0.917 0.841 0.882Table 1: Ranks vs.
Threshold T for Hebrew.With factor(x) Without factor(x)T Prec.
Recall F-Measure Accuracy Prec.
Recall F-Measure Accuracy0.91 0.940 0.771 0.846 0.892 0.903 0.803 0.850 0.8910.93 0.930 0.797 0.858 0.898 0.903 0.840 0.870 0.9040.95 0.931 0.810 0.866 0.904 0.902 0.856 0.878 0.9090.97 0.927 0.823 0.872 0.906 0.896 0.869 0.882 0.9110.99 0.925 0.848 0.872 0.915 0.878 0.896 0.886 0.9131.00 0.923 0.852 0.886 0.915 0.841 0.896 0.867 0.895Table 2: Ranks vs.
Threshold T for Arabic.Algorithm P R F ARank seg.
0.834 0.870 0.851 0.893Baseline 0.561 0.491 0.523 0.69Morfessor 0.630 0.689 0.658 0.814Table 3: Segmentation results comparison.tated gold standard.
Then we incorporate wordsegmentation into a concept acquisition frame-work and compare the performance of this frame-work with and without word segmentation.5.1 Corpora and annotationFor our experiments in Hebrew we used a 19MBHebrew corpus obtained from the ?Mila?
Knowl-edge Center for Processing Hebrew4.
The cor-pus consists of 143,689 different words, and atotal of 1,512,737 word tokens.
A sample textof size about 24,000 words was taken from thecorpus, manually segmented by human annotatorsand used as a gold standard in our segmentationevaluation.
In order to estimate the quality of ouralgorithm for Arabic, we used a 7MB Arabic newsitems corpus, and a similarly manually annotatedtest text of 4715 words.
The Arabic corpus is toosmall for meaningful category discovery, so weused it only in the segmentation evaluation.5.2 Evaluation of segmentation frameworkIn order to estimate the performance of word seg-mentation as a standalone algorithm we appliedour algorithm on the Hebrew and Arabic corpora,4http://mila.cs.technion.ac.il.using different parameter settings.
We first cal-culated the word frequencies, then applied initialsegmentation as described in Section 4.
Then weused SRILM (Stolcke, 2002) to learn the trigrammodel from the segmented corpus.
We utilizedGood-Turing discounting with Katz backoff, andwe gave words that were not in the training set theconstant probability 1E ?
9.
Finally we utilizedthe obtained trigram model to select sentence seg-mentations.
To test the influence of the factor(x)component of the Rank value, we repeated ourexperiment with and without usage of this com-ponent.
We also ran our algorithm with a set ofdifferent threshold T values in order to study theinfluence of this parameter.Tables 1 and 2 show the obtained results for He-brew and Arabic respectively.
Precision is the ra-tio of correct prefixes to the total number of de-tected prefixes in the text.
Recall is the ratio of pre-fixes that were split correctly to the total numberof prefixes.
Accuracy is the number of correctlysegmented words divided by the total number ofwords.As can be seen from the results, the best F-scorewith and without usage of the factor(x) compo-nent are about the same, but usage of this compo-nent gives higher precision for the same F-score.From comparison of Arabic and Hebrew perfor-mance we can also see that segmentation decisionsfor the task in Arabic are likely to be easier, sincethe accuracy for T=1 is very high.
It means that,unlike in Hebrew (where the best results were ob-tained for T=0.79), a word which starts with a pre-41Method us k-means randomavg ?shared meaning?
(%) 85 24.61 10avg triplet score(1-4) 1.57 2.32 3.71avg category score(1-10) 9.35 6.62 3.5Table 4: Human evaluation results.abuse, robbery, murder, assault, extortiongood, cheap, beautiful, comfortableson, daughter, brother, parentwhen, how, whereessential, important, central, urgentTable 5: A sample from the lexical categories dis-covered in Hebrew (translated to English).fix should generally be segmented.We also compared our best results to the base-line and to previous work.
The baseline draws asegmentation uniformly for each word, from thepossible segmentations of the word.
In an at-tempt to partially reproduce (Creutz and Lagus,2005) on our data, we also compared our resultsto the results obtained from Morfessor Categories-MAP, version 0.9.1 (Described in (Creutz and La-gus, 2005)).
The Morfessor Categories-MAP al-gorithm gets a list of words and their frequen-cies, and returns the segmentation for every word.Since Morfessor may segment words with prefixeswhich do not exist in our predefined list of validprefixes, we did not segment the words that hadillegal prefixes as segmented by Morfessor.Results for this comparison are shown in Table3.
Our method significantly outperforms both thebaseline and Morfessor-based segmentation.
Wehave also tried to improve the language model bya self training scheme on the same corpus but weobserved only a slight improvement, giving 0.848Precision and 0.872 Recall.5.3 Discovery of word categoriesWe divide the evaluation of the word categoriesdiscovery into two parts.
The first is evaluatingthe improvement in the quantity of found lexicalcategories.
The second is evaluating the qualityof these categories.
We have applied the algo-rithm to a Hebrew corpus of size 130MB5, whichis sufficient for a proof of concept.
We comparedthe output of the categories discovery on two dif-ferent settings, with function word separation andwithout such separation.
In both settings we omit-5Again obtained from the ?Mila?
Knowledge Center forProcessing Hebrew.N A JWith Separation 148 4.1 1No Separation 36 2.9 0Table 6: Lexical categories discovery results com-parison.
N: number of categories.
A: average cat-egory size.
J: ?junk?
words.ted all punctuation symbols.
In both runs of thealgorithm we used the same parameters.
Eightsymmetric patterns were automatically chosen foreach run.
Two of the patterns that were chosenby the algorithm in the unseparated case were alsochosen in the separated case.5.3.1 Manual estimation of category qualityEvaluating category quality is challenging sinceno exhaustive lists or gold standards are widelyaccepted even in English, certainly so in resource-poor languages such as Hebrew.
Hence we followthe human judgment evaluation scheme presentedin (Davidov and Rappoport, 2006), for the cate-gories obtained from the segmented corpus.We compared three methods of word categoriesdiscovery.
The first is random sampling of wordsinto categories.
The second is k-means, whereeach word is mapped to a vector, and similarity iscalculated as described in (Pantel and Lin, 2002).We applied k-means to the set of vectors, with sim-ilarity as a distance function.
If a vector had lowsimilarity with all means, we leave it unattached.Therefore some clusters contained only one vec-tor.
Running the algorithm 10 times, with differentinitial means each time, produced 60 clusters withthree or more words.
An interesting phenomenonwe observed is that this method produces very niceclusters of named entities.
The last method is theone in (Davidov and Rappoport, 2006).The experiment contained two parts.
In PartI, subjects were given 40 triplets of words andwere asked to rank them using the following scale:(1) the words definitely share a significant partof their meaning; (2) the words have a sharedmeaning but only in some context; (3) the wordshave a shared meaning only under a very un-usual context/situation; (4) the words do not shareany meaning; (5) I am not familiar enough withsome/all of the words.The 40 triplets were obtained as follows.
20 ofour categories were selected at random from thenon-overlapping categories we have discovered,and three words were selected from each of these42at random.
10 triplets were selected in the samemanner from the categories produced by k-means,and 10 triplets were selected at random from con-tent words in the same document.In Part II, subjects were given the full categoriesrepresented by the triplets that were graded as 1 or2 in Part I (the full ?good?
categories in terms ofsharing of meaning).
Subjects were asked to gradethe categories from 1 (worst) to 10 (best) accord-ing to how much the full category had met the ex-pectations they had when seeing only the triplet.Nine people participated in the evaluation.
Asummary of the results is given in Table 4.The categories obtained from the unsegmentedcorpus are too few and too small for a significantevaluation.
Therefore we applied the evaluationscheme only for the segmented corpus.The results from the segmented corpus containsome interesting categories, with a 100% preci-sion, like colors, Arab leaders, family membersand cities.
An interesting category is {Arabic, En-glish, Russian, French, German, Yiddish, Polish,Math}.
A sample of some other interesting cate-gories can be seen in Table 5.5.3.2 Segmentation effect on categorydiscoveryIn Table 6, we find that there is a major improve-ment in the number of acquired categories, and aninteresting improvement in the average categorysize.
One might expect that as a consequence ofan incorrect segmentation of a word, ?junk?
wordsmay appear in the discovered categories.
As canbe seen, only one ?junk?
word was categorized.Throughout this paper we have assumed thatfunction word properties of languages such as He-brew and Arabic decrease performance of whole-word pattern-based concept acquisition methods.To check this assumption, we have applied theconcept acquisition algorithm on several web-based corpora of several languages, while choos-ing corpora size to be exactly equal to the size ofthe Hebrew corpus (130Mb) and utilizing exactlythe same parameters.
We did not perform qualityevaluation6, but measured the number of conceptsand concept size.
Indeed the number of categorieswas (190, 170, 159, 162, 150, 29) for Russian, En-glish, Spanish, French, Turkish and Arabic respec-tively, clearly inferior for Arabic in comparison tothese European and Slavic languages.
A similar6Brief manual examination suggests no significant dropsin concept quality.tendency was observed for average concept size.At the same time prefix separation does help to ex-tract 148 concepts for Hebrew, making it nearly in-line with other languages.
In contrast, our prelim-inary experiments on English and Russian suggestthat the effect of applying similar morphologicalsegmentation on these languages in insignificant.In order to test whether more data can substi-tute segmentation even for Hebrew, we have ob-tained by means of crawling and web queries alarger (while potentially much more noisy) web-based 2GB Hebrew corpus which is based on fo-rum and news contents.
Our goal was to estimatewhich unsegmented corpus size (if any) can bringsimilar performance (in terms of concept number,size and quality).
We gradually increased corpussize and applied the concept acquisition algorithmon this corpus.
Finally, we have obtained similar,nearly matching, results to our 130MB corpus fora 1.2GB Hebrew subcorpus of the 2GB Hebrewcorpus.
The results remain stable for 4 different1.2GB subsets taken from the same 2GB corpus.This suggests that while segmentation can be sub-stituted with more data, it may take roughly x10more data for Hebrew to obtain the same resultswithout segmentation as with it.6 SummaryWe presented a simple method for separating func-tion word prefixes from words.
The method re-quires very little language-specific knowledge (theprefixes), and it can be applied to any morpholog-ically rich language.
We showed that this segmen-tation dramatically improves lexical acquisition inHebrew, where nearly ?10 data is required to ob-tain the same number of concepts without segmen-tation.While in this paper we evaluated our frameworkon the discovery of concepts, we have recentlyproposed fully unsupervised frameworks for thediscovery of different relationship types (Davidovet al, 2007; Davidov and Rappoport, 2008a; Davi-dov and Rappoport, 2008b).
Many of these meth-ods are mostly based on function words, and maygreatly benefit from the proposed segmentationframework.ReferencesMeni Adler, Michael Elhadad, 2006.
An UnsupervisedMorpheme-Based HMM for Hebrew MorphologicalDisambiguation.
ACL ?06.43Roy Bar-Haim, Khalil Simaan, Yoad Winter, 2005.Choosing an Optimal Architecture for Segmentationand POS-Tagging of Modern Hebrew.
ACL Work-shop on Computational Approaches to Semitic Lan-guages ?05.Mathias Creutz, 2003.
Unsupervised Segmentation ofWords Using Prior Distributions of Morph Lengthand Frequency.
ACL ?03.Mathias Creutz and Krista Lagus, 2005.
Unsuper-vised Morpheme Segmentation and Morphology In-duction from Text Corpora Using Morfessor 1.0.In Computer and Information Science, Report A81,Helsinki University of Technology.Sajib Dasgupta and Vincent Ng, 2007.
High Perfor-mance, Language-Independent Morphological Seg-mentation.
NAACL/HLT ?07.Dmitry Davidov, Ari Rappoport, 2006.
EfficientUnsupervised Discovery of Word Categories Us-ing Symmetric Patterns and High Frequency Words.ACL ?06.Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.Fully Unsupervised Discovery of Concept-SpecificRelationships by Web Mining.
ACL ?07.Dmitry Davidov, Ari Rappoport, 2008a.
Classificationof Semantic Relationships between Nominals UsingPattern Clusters.
ACL ?08.Dmitry Davidov, Ari Rappoport, 2008b.
UnsupervisedDiscovery of Generic Relationships Using PatternClusters and its Evaluation by Automatically Gen-erated SAT Analogy Questions.
ACL ?08.Vera Demberg, 2007.
A Language-Independent Un-supervised Model for Morphological Segmentation.ACL ?07.Beate Dorow, Dominic Widdows, Katarina Ling, Jean-Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.Using Curvature and Markov Clustering in Graphsfor Lexical Acquisition and Word Sense Discrimi-nation.
MEANING ?05.Samarth Keshava, Emily Pitler, 2006.
A Simpler, Intu-itive Approach to Morpheme Induction.
In Proceed-ings of 2nd Pascal Challenges Workshop, Venice,Italy.Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-sama Emam, Hany Hassan, 2003.
Language ModelBased Arabic Word Segmentation.
ACL ?03.Moshe Levinger, Uzzi Ornan, Alon Itai, 1995.
Learn-ing Morpho-Lexical Probabilities from an UntaggedCorpus with an Application to Hebrew.
Comput.Linguistics, 21:383:404.Dekang Lin, 1998.
Automatic Retrieval and Cluster-ing of Similar Words.
COLING ?98.Uzzi Ornan, 2005.
Hebrew in Latin Script.
Lesonenu,LXIV:137:151 (in Hebrew).Patrick Pantel, Dekang Lin, 2002.
Discovering WordSenses from Text.
SIGKDD ?02.Patrick Pantel, Deepak Ravichandran, Eduard Hovy,2004.
Towards Terascale Knowledge Acquisition.COLING ?04.Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.Distributional Clustering of English Words.
ACL?93.Benjamin Snyder, Regina Bazilay, 2008.
Unsuper-vised Multilingual Learning for Morphological Seg-mentation.
ACL/HLT ?08.Andreas Stolcke, 2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
ICSLP, pages 901-904,Denver, Colorado.Dominic Widdows, Beate Dorow, 2002.
A GraphModel for Unsupervised Lexical Acquisition.
COL-ING ?02.44
