Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 114?118,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsUnsupervised Authorial Clustering Based on Syntactic StructureAlon Daks and Aidan ClarkComputer Science DivisionUniversity of California, BerkeleyBerkeley, CA 94720{alon.daks, aidanbclark}@berkeley.eduAbstractThis paper proposes a new unsupervisedtechnique for clustering a collection ofdocuments written by distinct individu-als into authorial components.
We high-light the importance of utilizing syntacticstructure to cluster documents by author,and demonstrate experimental results thatshow the method we outline performs onpar with state-of-the-art techniques.
Addi-tionally, we argue that this feature set out-performs previous methods in cases whereauthors consciously emulate each other?sstyle or are otherwise rhetorically similar.1 IntroductionUnsupervised authorial clustering is the process ofpartitioning n documents written by k distinct au-thors into k groups of documents segmented byauthorship.
Nothing is assumed about each docu-ment except that it was written by a single author.Koppel et al (2011) formulated this problem in apaper focused on clustering five books from theHebrew Bible.
They also consider a ?multi-authordocument?
version of the problem: decomposingsentences from a single composite document gen-erated by merging randomly sampled chunks oftext from k authors.
Akiva and Koppel (2013) fol-lowed that work with an expanded method, andAldebei et al (2015) have since presented an im-proved technique in the ?multi-author document?context by exploiting posterior probabilities of aNaive-Bayesian Model.
We consider only the caseof clustering n documents written by k authorsbecause we believe that, in most cases of autho-rial decomposition, there is some minimum size oftext (a ?document?
), for which it can be reliably as-serted that only a single author is present.
Further-more, this formulation precludes results dependenton a random document generation procedure.In this paper, we argue that the biblical cluster-ing done by Koppel et al (2011) and by Aldebeiet al (2015) do not represent a grouping aroundtrue authorship within the Bible, but rather aroundcommon topics or shared style.
We demonstratea general technique that can accurately discernmultiple authors contained within the Books ofEzekiel and Jeremiah.
Prior work assumes thateach prophetic book reflects a single source, anddoes not consider the consensus among modernbiblical scholars that the books of Ezekiel andJeremiah were written by multiple individuals.To cluster documents by true authorship, wepropose that considering part-of-speech (POS) n-grams as features most distinctly identifies an indi-vidual writer.
The use of syntactic structure in au-thorial research has been studied before.
Baayen etal.
(1996) introduced syntactic information mea-sures for authorship attribution and Stamatatos(2009) argued that POS information could reflecta more reliable authorial fingerprint than lexicalinformation.
Both Zheng et al (2006) and Lay-ton et al (2013) propose that syntactic featuresets are reliable predictors for authorial attribu-tion, and Tschuggnall and Specht (2014) demon-strates, with modest success, authorial decompo-sition using pq-grams extracted from sentences?syntax trees.
We found that by combining the fea-ture set of POS n-grams with a clustering approachsimilar to the one presented by Akiva (2013), ourmethod of decomposition attains higher accuracythan Tschuggnall?s method, which also considersgrammatical style.
Additionally, in cases whereauthors are rhetorically similar, our frameworkoutperforms techniques outlined by Akiva (2013)and Aldebei (2015), which both rely on word oc-currences as features.This paper is organized as follows: section 2outlines our proposed framework, section 3 clari-114fies our method in detail through an example, sec-tion 4 contains results, section 5 tests an expla-nation of our results, and section 6 concludes ourfindings and discusses future work.2 Our FrameworkGiven n documents written by k distinct authors,where it is assumed that each document is writtenentirely by one of the k authors, our method pro-ceeds in the following way:First, represent each document as a frequencyvector reflecting all n-grams occurring in the?POS-translated?
document.Second, cluster documents into k groups usingan unsupervised clustering algorithm.Third, determine ?core elements?, documentsthat most strongly represent authorship attributesof their respective clusters.Fourth, use ?core elements?
to train a supervisedclassifier in order to improve accuracies of docu-ments that were not central to any cluster.A key improvement our framework presentsover prior techniques is in step one, where werepresent documents in terms of POS n-grams.Specifically, each document, xi, is transformedinto a ?POS-translated?
version, x?i, such that everyword or punctuation symbol from the original doc-ument is replaced with its respective POS or punc-tuation token in the translated version.
Considerthe following sentences from a New York Times(NYT) column written by Paul Krugman: ?Lastweek the Federal Reserve chose not to raise inter-est rates.
It was the right decision.?
In the ?POS-translated?
version these sentences appear as ?JJNN DT NNP NNP NN RB TO VB NN NNS PE-RIOD PRP VBD DT JJ NN PERIOD?.1We usea POS tagger from the Natural Language Toolkitto translate English documents (Bird et al, 2009)and use hand annotations for the Hebrew Bible.Our framework will work with any text for whichPOS-annotations are obtainable.
The requirementthat k is a fixed parameter is a limitation of the setof unsupervised clustering algorithms available instep two.3 Clarifying Details with NYT ColumnsWe shall describe a clustering of New York Timescolumns to clarify our framework.
The NYT cor-1A list of POS tags and explanations:http://www.ling.upenn.edu/courses/Fall 2003/ling001/penn treebank pos.htmlAuthors 1st 2nd 3rdTF-PK 4 - 4 5 - 5 3 - 4TF-GC 3 - 5 3 - 4 4 - 4TF-MD 5 - 5 3 - 4 3 - 5GC-PK 4 - 4 3 - 5 3 - 4MD-PK 3 - 5 3 - 4 4 - 4GC-MD 3 - 5 3 - 4 4 - 4Table 1: The top three ranges for n-grams by F1 accuracy foreach two-way split of NYT columnists.
Here, TF = ThomasFriedman, GC = Gail Collins, MD = Maureen Dowd, PK =Paul Krugman.pus is used both because the author of each doc-ument is known with certainty and because it is acanonical dataset that has served as a benchmarkfor both Akiva and Koppel (2013) and Aldebei etal.
(2015).
The corpus is comprised of texts fromfour columnists: Gail Collins (274 documents),Maureen Dowd (298 documents), Thomas Fried-man (279 documents), and Paul Krugman (331documents).
Each document is approximately thesame length and the columnists discuss a varietyof topics.
Here we consider the binary (k = 2)case of clustering the set of 629 Dowd and Krug-man documents into two groups.In step one, the documents are converted intotheir ?POS-translated?
form as previously outlined.Each document is represented as a frequency vec-tor that reflects all 3, 4, and 5-grams that appearin the ?POS-translated?
corpus.
This range of n-grams was determined through validation of dif-ferent values for n across several datasets.
Re-sults of this validation for the two way split overNYT columnists is displayed in Table 1.
These re-sults are consistent when validating against otherdatasets.
Using 3, 4, and 5-grams, the resultingdesign matrix has dimension 629 by 302,395.
Were-weight every element in the design matrix ac-cording to its term frequency?inverse documentfrequency.In step two, we apply spectral clustering to thedesign matrix to partition the documents into twoclusters.
This is implemented with the Shi andMalik (2000) algorithm, which solves a convexrelaxation of the normalized cuts problem on theaffinity graph (Pedregosa et al, 2011).
Edge-weights of the affinity graph are computed usinga linear kernel.
In the case of clustering several(k > 2) authors, we apply the Yu and Shi (2003)algorithm to perform multiclass spectral cluster-ing.In step three, we calculate the centroid of eachcluster produced by step two.
For each document115Columnist Cluster I Cluster IIDowd 294 4Krugman 3 328Table 2: Results when clustering 629 documents written byMaureen Dowd and Paul Krugman into two clusters.x?i, we determine ?i, the angle between that docu-ment and the centroid of its cluster, and call a doc-ument a ?core element?
if ?iis within 2 standarddeviations of the average of ?iin x?i?s cluster.In step four, ?core elements?
are used to train a500 tree random forest where at each split the stan-dard heuristic of?p features are considered (herep = 302, 395).
Finally, we reclassify all 629 doc-uments according to this random forest to produceour final class labels, summarized in Table 2.
Thefinal accuracy of the Dowd-Krugman clustering,measured as an F1-score, is 98.8%.4 ResultsAll accuracy scores given in the rest of this pa-per are calculated using the F1-score.
Because ourtechnique contains stochastic elements, results re-flect an average of 20 runs.4.1 NYT ColumnsWhen clustering over all six binary-pairs of NYTcolumnists, our framework achieves an averageaccuracy of 94.5%, ranging from 90.0% to 98.8%.Aldebei et al (2015) addresses the slightly dif-ferent problem of decomposing artificially mergedNYT documents, and acknowledging the distinc-tion between the two problems, our results arecomparable to their accuracies which range from93.3% to 96.1%.4.2 Sanditon: An Uncompleted NovelAnother canonical authorship test is that of thenovel Sanditon, a text left incomplete at the deathof its author, Jane Austen, and finished some yearslater by an anonymous person known as ?AnotherLady.?
She closely emulated Austen?s style andadded 19 chapters to Austen?s original 11.
Re-searchers have long examined this text and mostrecently Moon et al (2006) analyzed Sanditon us-ing supervised techniques in the context of author-ship attribution.
Much progress has been madein the field since then, but examining Sanditonhas fallen out of style.
Our framework clus-ters Austen?s chapters from Another Lady?s with93.8% accuracy, only mislabeling two documents.4.3 Obama-McCain & Ezekiel-JeremiahIn order to confirm our framework is accurate overa variety of documents, we considered campaignspeeches from the 2008 presidential election.
Col-lecting 27 speeches from President Obama and 20from Senator McCain, we expected our techniqueto excel in this context.
We found instead that ourmethod performed exceptionally poorly, cluster-ing these speeches with only 74.2% accuracy.
In-deed, we were further surprised to discover that byadjusting our framework to be similar to that pre-sented in Akiva and Koppel (2013) and Aldebeiet al (2015) ?
by replacing POS n-grams with or-dinary word occurrences in step one ?
our frame-work performed very well, clustering at 95.3%.Similarly, our framework performed poorly onthe Books of Ezekiel and Jeremiah from the He-brew Bible.
Using the English-translated KingJames Version, and considering each chapter asan individual document, our framework clustersthe 48 chapters of Ezekiel and the 52 chapters ofJeremiah at 54.7%.
Aldebei et al (2015) reports98.0% on this dataset, and when considering theoriginal English text instead of the POS-translatedtext, our framework achieves 99.0%.
The simulta-neous success of word features and failure of POSfeatures on these two datasets seemed to com-pletely contradict our previous results.We propose two explanations.
First, perhaps toomuch syntactic structure is lost during translation.This could certainly be a factor, but does not ex-plain the Obama-McCain results.
The second ex-planation comes from the wide consensus amongbiblical scholars that there was no single ?Ezekiel?or ?Jeremiah?
entirely responsible for each book.Instead, the books are composites from a num-ber of authors, sometimes written over the spanof hundreds of years (McKane, 1986; Zimmerli,1979; Mowinckel, 1914).
Koppel et al (2011) ac-knowledges this shortcoming in their original pa-per, and suggest that in this authorial interpretationtheir clustering is one of style, not authorship.
Wehypothesize that in both failed cases, accuracy islow because our assumption that only two authorswere represented among the documents is incor-rect.
This theory holds for the Obama-McCaindataset, because Obama had up to three primaryspeechwriters during the ?08 election and McCainlikely had a similar number (Parker, 2008).
Per-haps emulating syntactic patterns is more difficultthan emulating word choice.
If so, using word fea-116Author Cluster I Cluster IIEzekiel 1 37 2Ezekiel 2 1 8Table 3: Results when clustering the Hebrew text of theBook of Ezekiel split over the two authors.Author Cluster I Cluster IIJeremiah 1 21 2Jeremiah 2 0 14Table 4: Results when clustering the Hebrew text of theBook of Jeremiah split over the two primary authors.tures, a model can discern Obama?s rhetoric fromthat of McCain.
However, since the syntax ofmore than two individuals is present in the text,POS features cannot accurately cluster the docu-ments into two groups.
Our goal is for POS fea-tures to cluster more accurately than word featureswhen the true authorship of the documents is cor-rectly considered.5 Testing Our TheoryWe first attempt to cluster the Ezekiel andJeremiah texts in the original Hebrew in order totest if too much syntactic structure is lost dur-ing translation.
For the Hebrew text, we usehand-tagged POS information because a reliableautomatic tagger was not available (van Peursenet al, 2015; Roorda, 2015).
Clustering Ezekieland Jeremiah using Hebrew POS features obtains62.5% accuracy.
This is an improvement over theEnglish text, but still performs far worse than lex-ical feature sets.We next attempt to cluster the Ezekiel andJeremiah texts according to the authorial stratawithin each book that is widely agreed upon bybiblical scholars, in order to test if incorrect au-thorial assumptions were causing the decrease inaccuracy.
Unfortunately, there is no public break-down of Obama and McCain speeches by speech-writer, so testing our hypothesis is limited here tothe biblical dataset.We therefore cluster the Book of Ezekiel assum-ing there are two nested authors, which accord-ing to modern scholarship are Ezekiel 1 (chap-ters 1?39) and Ezekiel 2 (chapters 40?48) (Zim-merli, 1979).
Summarized in Table 3, accord-ing to this division our framework clusters theEzekiel chapters with 93.6% accuracy, mislabel-ing only three documents.
We also consider theBook of Jeremiah, which is composed of two pri-mary authors with four secondary authors.
In clus-Author C I C II C III C IVEzekiel 1 32 2 5 0Ezekiel 2 1 8 0 0Jeremiah 1 0 0 21 2Jeremiah 2 0 0 0 14Table 5: Results when clustering Ezekiel 1 and 2 andJeremiah 1 and 2 simultaneously with k = 4.tering a corpus containing Jeremiah 1 (23 non-contiguous chapters) and Jeremiah 2 (14 non-contiguous chapters) (McKane, 1986), our frame-work divides the 37 chapters into two groups with94.5% accuracy, mislabeling only two documents.These results are summarized in Table 4.
Whenconsidering the 4-way split between Ezekiel 1,Ezekiel 2, Jeremiah 1 and Jeremiah 2, our methodachieves 87.5% accuracy as summarized in Ta-ble 5.When comparing these results with those ob-tained by looking at word frequencies in the orig-inal Hebrew texts partitioned into the four cor-rect authors, we find that our approach performssignificantly better.
With word frequencies asfeatures, our framework clusters Ezekiel 1 fromEzekiel 2 with only 76.3% accuracy, Jeremiah1 from Jeremiah 2 with only 74.9% accuracy,and crucially, clusters the four-way between bothEzekiels and both Jeremiahs with only 47.9% ac-curacy.
While lexical features outperform syntac-tic features when considering incorrect authorship,syntactic features substantially outperform lexicalfeatures when considering the true authorial divi-sions of Ezekiel and Jeremiah.6 Conclusion and Future WorkWe have demonstrated a new framework for au-thorial clustering that not only clusters canon-ical datasets with state-of-the-art accuracy, butalso discerns nested authorship within the HebrewBible more accurately than prior work.
While webelieve it is possible for an author to emulate an-other author?s word choice, it is much more dif-ficult to emulate unconscious syntactic structure.These syntactic patterns, rather than lexical fre-quencies, may therefore be key to understandingauthorial fingerprints.
Finding testing data for thisproblem is difficult, since documents for whichauthorship is misconstrued or obfuscated but forwhich true authorship is known with certainty arerare.
However, when clustering texts for which au-thorship is not known, one would wish to have aframework which most accurately discerns author-117ship, rather than rhetorical similarity.
We believethat our framework, and syntactic feature sets inparticular, clusters documents based on authorshipmore accurately than prior work.
While we haveshown that POS feature sets can succeed indepen-dently, future work should examine augmentingsyntactic and lexical feature sets in order to utilizethe benefits of each.Finally, authorial clustering performs poorlywhen the number of true and expected authorswithin a corpus do not match.
An important nextstep is to automatically identify the number of au-thors contained within a set of documents.
Webelieve that a more reliable method of generating?core elements?
is essential, and should not be re-liant on a predetermined number of authors.AcknowledgmentsWe thank Laurent El Ghaoui, Professor of EECSand IEOR, UC Berkeley, and Ronald Hendel,Norma and Sam Dabby Professor of Hebrew Bibleand Jewish Studies, UC Berkeley, for commentsthat greatly improved the paper.ReferencesNavot Akiva and Moshe Koppel.
2013.
A genericunsupervised method for decomposing multi-authordocuments.
Journal of the American Society forInformation Science and Technology, pages 2256?2264.Khaled Aldebei, Xiangjian He, and Jie Yang.
2015.Unsupervised decomposition of a multi-author doc-ument based on naive-bayesian model.
Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Short Papers), pages 501?505.Harald Baayen, Hans Van Halteren, and FionaTweedie.
1996.
Outside the cave of shadows: Us-ing syntactic annotation to enhance authorship attri-bution.
Literary and Linguistic Computing, 11:121?131.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Moshe Koppel, Navot Akiva, Idan Dershowitz, andNachum Dershowitz.
2011.
Unsupervised decom-position of a document into authorial components.Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1356?1364.Robert Layton, Paul Watters, and Richard Dazeley.2013.
Automated unsupervised authorship analy-sis using evidence accumulation clustering.
NaturalLanguage Engineering.William McKane.
1986.
A Critical and ExegeticalCommentary on Jeremiah.
Edinburgh, Edinburgh,UK.Todd K. Moon, Peg Howland, and Jacob H Gunther.2006.
Document author classification using general-ized discriminant analysis.
Proc.
Workshop on TextMining, SIAM Int?l Conf.
on Data Mining.Sigmund Mowinckel.
1914.
Zur Komposition desBuches Jeremia.
Kristiania.Ashley Parker.
2008.
What would obama say?
NewYork Times, January 20.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Dirk Roorda.
2015.
Laf-fabric software.https://github.com/ETCBC/laf-fabric.
GitHubrepository.Jianbo Shi and Jitendra Malik.
2000.
Normalizedcuts and image segmentation.
IEE Transactions OnPattern Analysis And Machine Intelligence, 22:888?905.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for Information Science and Technology,60:538?556.Michael Tschuggnall and Gunther Specht.
2014.
En-hancing authorship attribution by utilizing syntaxtree profiles.
Proceedings of the 14th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 195?199.W.T.
van Peursen, M. Sc.
C. Sikkel, and D. Ro-orda.
2015.
Hebrew text database etcbc4b.http://dx.doi.org/10.17026/dans-z6y-skyh.
DANS.Stella X. Yu and Jianbo Shi.
2003.
Multiclass spectralclustering.
Proceedings of the Ninth IEEE Interna-tional Conference on Computer Vision, 1:313?319.Rong Zheng, Jiexun Li, Hsinchun Chen, and ZanHuang.
2006.
A framework for authorship identi-fication of online messages: Writing-style featuresand classification techniques.
J.
Am.
Soc.
Inf.
Sci.Technol., 57(3):378?393, February.Walther Zimmerli.
1979.
Ezekiel 1-2: A Commentaryon the Book of the Prophet Ezekiel.
Fortress Pressof Philadelphia, Philadelphia, US.118
