Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 9?16,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue SystemsIoannis Klasinas1, Elias Iosif2,4, Katerina Louka3, Alexandros Potamianos2,41School of ECE, Technical University of Crete, Chania 73100, Greece2School of ECE, National Technical University of Athens, Zografou 15780, Greece3Voiceweb S.A., Athens 15124, Greece4Athena Research Center, Marousi 15125, Greeceiklasinas@isc.tuc.gr,{iosife,potam}@telecom.tuc.gr,klouka@voiceweb.euAbstractIn this paper we present the SemEval-2014 Task 2 on spoken dialogue gram-mar induction.
The task is to classifya lexical fragment to the appropriate se-mantic category (grammar rule) in orderto construct a grammar for spoken dia-logue systems.
We describe four sub-tasks covering two languages, English andGreek, and three speech application do-mains, travel reservation, tourism and fi-nance.
The classification results are com-pared against the groundtruth.
Weightedand unweighted precision, recall and f-measure are reported.
Three sites partic-ipated in the task with five systems, em-ploying a variety of features and in somecases using external resources for training.The submissions manage to significantlybeat the baseline, achieving a f-measure of0.69 in comparison to 0.56 for the base-line, averaged across all subtasks.1 IntroductionThis task aims to foster the application of com-putational models of lexical semantics to the fieldof spoken dialogue systems (SDS) for the problemof grammar induction.
Grammars constitute a vi-tal component of SDS representing the semanticsof the domain of interest and allowing the systemto correctly respond to a user?s utterance.The task has been developed in tight collabo-ration between the research community and com-mercial SDS grammar developers, under the aus-pices of the EU-IST PortDial project1.
Among theThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://www.portdial.eu/project aims is to help automate the grammar de-velopment and localization process.
Unlike previ-ous approaches (Wang and Acero, 2006; Cramer,2007) that have focused on full automation, Port-Dial adopts a human-in-the-loop approach werea developer bootstraps each grammar rule or re-quest type with a few examples (use cases) andthen machine learning algorithms are used to pro-pose grammar rule enhancements to the developer.The enhancements are post-edited by the devel-oper and new grammar rule suggestions are pro-posed by the system, in an iterative fashion un-til a grammar of sufficient quality is achieved.
Inthis task, we focus on a snapshot of this process,where a portion of the grammar is already inducedand post-edited by the developer and new candi-date fragments are rolling in order to be classifiedto an existing rule (or rejected).
The goal is to de-velop machine learning algorithms for classifyingcandidate lexical fragments to the correct grammarrule (semantic category).
The task is equally rel-evant for both finite-state machine and statisticalgrammar induction.In this task the semantic hierarchy of SDSgrammars has two layers, namely, low- and high-level.
Low-level rules are similar to gazetteersreferring to terminal concepts that can be as rep-resented as sets of lexical entries.
For example,the concept of city name can be represented as<CITY> = (?London?, ?Paris?, ...).
High-levelrules are defined on top of low-level rules, whilethey can be lexicalized as textual fragments (orchunks), e.g., <TOCITY> = (?fly to <CITY>?,...).
Using the above examples the sentence ?Iwant to fly to Paris?
will be first parsed as ?Iwant to fly to <CITY>?
and finally as ?I want to<TOCITY>?.In this task, we focus exclusively on high-levelrule induction, assuming that the low-level rulesare known.
The problem of fragment extractionand selection is simplified by investigating the9binary classification of (already extracted) frag-ments into valid and non-valid.
The task boilsdown mainly to a semantic similarity estimationproblem for the assignment of valid fragments intohigh-level rules.2 Prior WorkThe manual development of grammars is a time-consuming and tedious process that requires hu-man expertise, posing an obstacle to the rapid port-ing of SDS to new domains and languages.
Asemantically coherent workflow for SDS gram-mar development starts from the definition of low-level rules and proceeds to high-level ones.
Thisprocess is also valid for the case of inductionalgorithms.
Automatic or machine-aided gram-mar creation for spoken dialogue systems canbe broadly divided in two categories (Wang andAcero, 2006): knowledge-based (or top-down)and data-driven (or bottom-up) approaches.Knowledge-based approaches rely on the man-ual or semi-automatic development of domain-specific grammars.
They start from the domain on-tology (or taxonomy), often in the form of seman-tic frames.
First, terminal concepts in the ontology(that correspond to low-level grammar rules) getpopulated with values, e.g., <CITY>, and thenhigh-level concepts (that correspond to high-levelgrammar rules) get lexicalized creating grammarfragments.
Finally, phrase headers and trailers areadded to create full sentences.
The resulting gram-mars often suffer from limited coverage (poor re-call).
In order to improve coverage, regular ex-pressions and word/phrase order permutations areused, however at the cost of over-generalization(poor precision).
Moreover, knowledge-basedgrammars are costly to create and maintain, asthey require domain and engineering expertise,and they are not easily portable to new domains.This led to the development of grammar authoringtools that aim at facilitating the creation and adap-tation of grammars.
SGStudio (Semantic Gram-mar Studio), (Wang and Acero, 2006), for exam-ple, enables 1) example-based grammar learning,2) grammar controls, i.e., building blocks and op-erators for building more complex grammar frag-ments (regular expressions, lists of concepts), and3) configurable grammar structures, allowing fordomain-adaptation and word-spotting grammars.The Grammatical Framework Resource GrammarLibrary (GFRGL) (Ranta, 2004) enables the cre-ation of multilingual grammars adopting an ab-straction formalism, which aims to hide the lin-guistic details (e.g., morphology) from the gram-mar developer.Data-driven approaches rely solely on corpora(bottom-up) of transcribed utterances (Meng andSiu, 2002; Pargellis et al., 2004).
The inductionof low-level rules consists of two steps dealingwith the 1) identification of terms, and 2) assign-ment of terms into rules.
Standard tokenizationtechniques can be used for the first step, however,different approaches are required for the case ofmultiword terms, e.g., ?New York?.
In such cases,gazetteer lookup and named entity recognition canbe employed (if the respective resources and toolsare available), as well as corpus-based colloca-tion metrics (Frantzi and Ananiadou, 1997).
Typ-ically, the identified terms are assigned into low-level rules via clustering algorithms operating overa feature space that is built according to the termsemantic similarity.
The distributional hypothe-sis of meaning (Harris, 1954) is a widely-used ap-proach for estimating term similarity.
A compar-ative study of similarity metrics for the inductionof SDS low-level rules is presented in (Pargelliset al., 2004), while the combination of metricswas investigated in (Iosif et al., 2006).
Differentclustering algorithms have been applied includ-ing hard- (Meng and Siu, 2002) and soft-decision(Iosif and Potamianos, 2007) agglomerative clus-tering.High-level rule induction is a less researchedarea that consists of two main sub-problems: 1)the extraction and selection of candidate frag-ments from a corpus, and 2) assignment of termsinto rules.
Regarding the first sub-problem,consider the fragments ?I want to depart from<CITY> on?
and ?depart from <CITY>?
for theair travel domain.
Both express the meaning of de-parture city, however, the (semantics of the) latterfragment are more concise and generalize better.The application of syntactic parsers for segmentextraction is not straightforward since the outputis a full parse tree.
Moreover, such parsers aretypically trained over annotated corpora of formallanguage usage, while the SDS corpora often areungrammatical due to spontaneous speech.
Thereare few statistical parsing algorithms that rely onlyon plain lexical features (Ponvert et al., 2011; Biskand Hockenmaier, 2012) however, as other algo-rithms, one needs to decide where to prune the10parse tree.
In (Georgiladakis et al., 2014), the ex-plicit extraction and selection of fragments is in-vestigated following an example-driven approachwhere few rule seeds are provided by the gram-mar developer.
The second sub-problem of high-level rule induction deals with the formulationof rules using the selected fragments.
Each ruleis meant to consist of semantically similar frag-ments.
For this purpose, clustering algorithms canbe employed exploiting the semantic similarity be-tween fragments as features.
This is a challengingproblem since the fragments are multi-word struc-tures whose overall meaning is composed accord-ing to semantics of the individual constituents.
Re-cently, several models have been proposed regard-ing phrase (Mitchell and Lapata, 2010) and sen-tence similarity (Agirre et al., 2012), while anapproach towards addressing the issue of seman-tic compositionality is presented in (Milajevs andPurver, 2014).The main drawback of data-driven approachesis the problem of data sparseness, which may af-fect the coverage of the grammar.
A popular so-lution to the data sparseness bottleneck is to har-vest in-domain data from the web.
Recently, thishas been an active research area both for SDSsystems and language modeling in general.
Dataharvesting is performed in two steps: (i) queryformulation, and (ii) selection of relevant docu-ments or sentences (Klasinas et al., 2013).
Posingthe appropriate queries is important both for ob-taining in-domain and linguistically diverse sen-tences.
In (Sethy et al., 2007), an in-domain lan-guage model was used to identify the most ap-propriate n-grams to use as web queries.
An in-domain language model was used in (Klasinas etal., 2013) for the selection of relevant sentences.A more sophisticated query formulation was pro-posed in (Sarikaya, 2008), where from each in-domain utterance a set of queries of varying lengthand complexity was generated.
These approachesassume the availability of in-domain data (even iflimited) for the successful formulation of queries;this dependency is also not eliminated when us-ing a mildly lexicalized domain ontology to for-mulate the queries, as in (Misu and Kawahara,2006).
Selecting the most relevant sentences thatget returned from web queries is typically doneusing statistical similarity metrics between in do-main data and retrieved documents, for examplethe BLEU metric (Papineni et al., 2002) of n-gram similarity in (Sarikaya, 2008) and a metricof relative entropy (Kullback-Leibler) in (Sethy etal., 2007).
In cases where in-domain data is notavailable, cf.
(Misu and Kawahara, 2006), heuris-tics (pronouns, sentence length, wh-questions) andmatches with out-of-domain language models canbe used to identify sentences for training SDSgrammars.
In (Sarikaya, 2008), the producedgrammar fragments are also parsed and attachedto the domain ontology.
Harvesting web data canproduce high-quality grammars while requiring upto 10 times less in-domain data (Sarikaya, 2008).Further, data-driven approaches induce syntac-tic grammars but do not learn their correspondingmeanings, for this purpose an additional step is re-quired of parsing the grammar fragments and at-taching them to the domain ontology (Sarikaya,2008).
Also, in many cases it was observedthat the fully automated bottom-up paradigm re-sults to grammars of moderate quality (Wangand Acero, 2006), especially on corpora con-taining longer sentences and more lexical vari-ety (Cramer, 2007).
Finally, algorithms focusingon crosslingual grammar induction, like CLIoS(Kuhn, 2004), are often even more resource-intensive, as they require training corpora of par-allel text and sometimes also a grammar for one ofthe languages.
Grammar quality can be improvedby introducing a human in the loop of grammar in-duction (Portdial, 2014a); an expert that validatesthe automatically created results (Meng and Siu,2002).3 Task DescriptionNext we describe in detail the candidate grammarfragment classification SemEval task.
This taskis part of a grammar rule induction scenario forhigh-level rules.
The evaluation focuses in spokendialogue system grammars for multiple domainsand languages.3.1 Task DesignThe goal of the task is to classify a number frag-ment to the rules available in the grammar.
Foreach grammar we provide a training and develop-ment set, i.e., a set of rules with the associatedfragments and the test set which is composed ofplain fragments.
An excerpt of the train set for therule ?<TOCITY>?
is ?ARRIVE AT <CITY>,ARRIVES AT <CITY>, GOING TO <CITY>?and of the test set ?GOING INTO <CITY>, AR-11RIVES INTO <CITY>?.In preliminary experiments during the task de-sign we noticed that if the test set consists of validfragments only, good classification performance isachieved, even when using the naive baseline sys-tem described later in this paper.
To make the taskmore realistic we have included a set of ?junk?fragments not corresponding to any specific rule.Junk fragments were added both in the train setwhere they are annotated as such and in the testset.
For this task we have artificially created thejunk fragments by removing or adding words fromlegitimate fragments.
Example junk fragmentsused are ?HOLD AT AT <TIME> TRY?
and?ANY CHOICE EXCEPT <AIRLINE> OR?, thefirst one having a repetition of the word ?AT?while the second one should include one moretime the concept ?<AIRLINE>?
in the end to bemeaningful.Junk fragments help better model a real-worldscenario, where the candidate fragments will in-clude irrelevant examples too.
For example, ifweb corpora are used to extract the candidate frag-ments grammatical mistakes and out-of-domainsentences might appear.
Similarly, if the transcrip-tions from a deployed SDS system are used forgrammar induction, transcription errors might in-troduce noise (Bechet et al., 2014).Junk fragments account for roughly 5% of thetrain test and 15% of the test set.
The discrep-ancy between train and test set ratios is due to aconscious effort to model realistic train/test condi-tions, where train data is manually processed anddoes not include errors, while candidate fragmentsare typically more noisy.3.2 DatasetsWe have provided four datasets, travel English,travel Greek, tourism English and finance English.The travel domain grammar covers flight, car andhotel reservation utterances.
The tourism domaincovers touristic information including accommo-dation, restaurants and movies.
The finance do-main covers utterances of a bank client askingquestions about his bank account as well as re-porting problems.
In Table 1 are presented typicalexamples of fragments for every subtask.All grammars have been manually constructedby a grammar developer.
For the three Englishgrammars, a small corpus (between 500 and 2000sentences) was initially available.
The grammardeveloper first identified terminal concepts, whichcorrespond to low-level rules.
Typical examplesinclude city names for the travel domain, restau-rant names for the tourism domain and credit cardnames in the finance domain.
After covering alllow-level rules the grammar developer proceededto identify high-level rules present in the corpus,like the departure city in the travel domain, or theuser request type for a credit card.
The gram-mar developer was instructed to identify all rulespresent in the corpus, but also spend some effortto include rules not appearing in the corpus so thatthe resulting grammar better covers the domain athand.
For the case of Greek travel grammar nocorpus was initially available.
The Greek gram-mar was instead produced by manually translat-ing the English one, accounting for the differencesin syntax between the two languages.
The gram-mars have been developed as part of the PortDialFP7 project and are explained in detail in (Portdial,2014b).For the first three datasets that have been avail-able from the beginning of the campaign we havesplit the release into train, development and testset.
For the finance domain which was announcedwhen the test sets were released we only providedthe train and test set, to simulate a resource poorscenario.
The statistics of the datasets for all lan-guage/domain pairs are given in Table 2.In addition to the high-level rules we madeavailable the low-level rules for each grammar,which although not used in the evaluation, can beuseful for expanding the high-level rules to coverall lexicalizations expressed by the grammar.3.3 EvaluationFor the evaluation of the task we have used preci-sion, recall and f-measure, both weighted and un-weighted.If Rjdenotes the set of fragments for one ruleand Cjthe set of fragments classified to this ruleby a system then per-rule precision is computed bythe equation:Prj=|Rj?
Cj||Cj|and per-rule recall by:Rcj=|Rj?
Cj||Rj|F-measure is then computed by:12Grammar Rule FragmentTravel English <FLIGHTFROM> FLIGHT FROM <CITY>Travel Greek <FLIGHTFROM> ?TH?H A?O <CITY>Tourism English <TRANSFERQ> TRANSFERS FROM <airportname> TO <cityname>Finance English <CARDNAME> <BANKNAME> CARDTable 1: Example grammar fragments for each application domain.Grammar Rules FragmentsTrain set Dev set Test setTravel English 32 623 331 284Travel Greek 35 616 340 324Tourism English 24 694 334 285Finance English 9 136 - 37Table 2: Number of rules in the training, development and test sets for each application domain.Fj=2PrjRcjPrj+ Rcj.Precision for all the J rules Rj, 1 ?
j ?
J iscomputed by the following equation:Pr =?jPrjwjIn the unweighted case the weight wjhas a fixedvalue for all rules, so wj=1J.
Taking into accountthe fact that the rules are not balanced in terms offragments, a better way to compute for the weightis wj=|Rj|?j|Rj|.
In the latter, weighted, case thetotal precision will better describe the results.Recall is similarly computed using the sameweighting scheme as:Rc =?jRcjwj3.4 BaselineFor comparison purposes we have developed anaive baseline system.
To classify a test fragment,first its similarity with all the train fragments iscomputed, and it is classified to the rule wherethe most similar train fragment belongs.
Fragmentsimilarity is computed as the ratio of their LongestCommon Substring (LCS) divided by the sum oftheir lengths:Sim(s, t) =|LCS(s, t)||s|+ |t|where s and t are two strings, |s| and |t| theirlength in characters and |LCS(s, t)| the length oftheir LCS.
This is a very simple baseline, comput-ing similarity without taking into account contextor semantics.4 Participating SystemsThree teams have participated in the task with fivesystems.
All teams participated in all subtaskswith the exception of travel Greek, where onlytwo teams participated.
An overview of coresystem features is presented in Table 3.
Theremainder of this section briefly describes eachof the submissions and then compares them.
Abrief description for each system is provided inthe following paragraphs.tucSage.
The core of the tucSage system isa combination of two components.
The firstcomponent is used for the selection of candidaterule fragments from a corpus.
Specifically, theposterior probability of a candidate fragmentbelonging to a rule is computed using a variety offeatures.
The feature set includes various lexicalfeatures (e.g., the number of tokens), the fragmentperplexity computed using n-gram languagemodeling, and features based on lexical similarity.The second component is used for computingthe similarity between a candidate fragment anda grammar rule.
In total, two different types ofsimilarity metrics are used relying on the overlapof character bigrams and contextual features.These similarities are fused with the posteriorprobabilities produced by the fragment selectionmodel.
The contribution of the two components isadjusted using an exponential weight.SAIL-GRS.
The SAIL-GRS system is basedon the well-established term frequency?inversedocument frequency (TF ?IDF ) measurement.This metric is adapted to the present task byconsidering each grammar rule as a ?document?.For each rule, all its fragments are aggregated13System Use of Features Similarity External Language-acronym machine learn.
used metrics corpora specificBaseline no lexical Longest Common no noSubstringtucSage yes: lexical, perplexity, character overlap, web norandom forests similarity-based , heuristic cosine similarity documentsSAIL-GRS no lexical cosine similarity no noBiel no lexical, expansion of cosine Wikipedia yeslow-level rules similarity articlesTable 3: Overview of the characteristics of the participating systems.and the frequency of the respective n-grams(constituents) is computed.
The inverse documentfrequency is casted as inverse rule frequencyand it is computed for the extracted n-grams.The process is performed for both unigrams andbigrams.Biel.
The fundamental idea behind the Bielsystem is the encoding of domain semantics viatopic modeling.
For this purpose a backgrounddocument space is constructed using thousandsof Wikipedia articles.
Particular focus is givento the transformation of the initial documentspace according to the paradigm of explicitsemantic analysis.
For each domain, a topicspace is defined and a language-specific functionis employed for the mapping of documents.
Inessence, the mapping function is an associationmeasurement that is based on TF?IDF scores.An approximation regarding the construction ofthe topic space is investigated in order to reducedata sparsity, while a number of normalizationschemes are also presented.Overall, only the tucSage system employs a ma-chine learning-based approach (random forests),while an unsupervised approach is followed by theSAIL-GRS and Biel systems.
All systems exploitlexical information extracted from rule fragments.This information is realized as the lexical surfaceform of the constituents of fragments.
For ex-ample, consider the ?depart for <CITY>?
frag-ment that corresponds to the high-level rule refer-ring to the notion of departure city.
The follow-ing set of lexical features can be extracted fromthe aforementioned fragment: (?depart?, ?from?,?<CITY>?).
Unlike the other systems, the Bielsystem utilizes low-level rules to expand high-level rules with terminal concept instances.
Forexample, the ?<CITY>?
rule is not processed asis, but it is represented as a list of city names(?New York?, ?Boston?, .
.
.
).
The most rich fea-ture set is used by the tucSage system which com-bines lexical, perplexity and similarity featureswith a set of heuristic rules.
All three systemsemploy the widely-used cosine similarity metric.Both SAIL-GRS and Biel systems rely solely onthis metric during the assignment of an unknownfragment to a high-level rule.
A more sophis-ticated approach is presented by tucSage, wherefirst a classifier is built for every grammar rule,computing the probability of a fragment belong-ing to this rule and then the similarity between thefragment and the rule is computed.
Classificationis then performed by combining the two scores.Also, another difference regarding the employ-ment of the cosine similarity deals with the com-putation of the vectorial feature values.
A simplebinary scheme is used in the tucSage system, whilevariations of the term frequency-inverse documentfrequency scheme are used in SAIL-GRS and Biel.Besides cosine similarity, a similarity metric basedon the overlap of character bigrams is used by thetucSage system.
External corpora (i.e., corporathat were not provided as part of the official taskdata) were used by the tucSage and Biel systems.Such corpora were meant as an additional sourceof information with respect to the domains underinvestigation.
Regarding tucSage, the training datawere exploited in order to construct web searchqueries for harvesting a collection of web docu-ments from which a number of sentences were se-lected for corpus creation.
In the case of the Bielsystem, a set of Wikipedia articles was exploited.Language specific resources where used for theBiel system, while the other two teams used lan-guage agnostic methods.5 ResultsThe results for all participating teams and thebaseline system are given in Table 4.
The tucSageteam submitted three runs, the first one being theprimary, indicated with an asterisk in the results.14Focusing on the weighted F-measure we seethat in all domains but the tourism English, atleast one submission manages to outperform thebaseline provided by the organizers.
In travel En-glish the baseline system achieves 0.51 weightedf-measure, with two out of the three systemsachieving 0.68 and 0.58.
The improvement overthe baseline is greater for the travel Greek sub-task, where the baseline score of 0.26 is muchlower than the achieved 0.52 from tucSage.
In thetourism English subtask the best submitted sys-tems managed to match the performance of thebaseline system, but not to exceed it.
This canbe attributed to the good performance of the base-line system, due to the fact that the tourism gram-mar is composed of longer fragments than the rest,helping the naive baseline system achieve top per-formance exploiting lexical similarity only.
Wecan however assume that more complex systemswould beat the baseline if the test set fragmentswere built using different lexicalizations, as wouldbe the case in unannotated data coming from de-ployed SDS.In the finance domain, even though the amountof training data is quite smaller than in all othersubtasks the submitted systems still manage tooutperform the baseline system.
This means thatthe submitted systems display robust performanceboth in resource-rich and resource-poor condi-tions.6 ConclusionThe tucSage and SAIL-GRS systems are shown tobe portable across domains and languages, achiev-ing performance that exceeds the baseline for threeout of four datasets.
The highest performance ofthe tucSage system compared to the SAIL-GRSsystem may be attributed to the use of a model forfragment selection.
Interestingly, the simple vari-ation of the TF?IDF scheme used by the SAILsystem achieved very good results being a closesecond performer.
The UNIBI system proposeda very interesting new application of the frame-work of topic modeling to the task of grammar in-duction, however, the respective performance doesnot exceed the state-of-the-art.
The combinationof the tucSage and SAIL-GRS systems could givebetter results.team Weighted UnweightedPr.
Rec.
F-m. Pr.
Rec.
F-m.Travel EnglishBaseline 0.40 0.69 0.51 0.38 0.67 0.48tucSage1?0.60 0.73 0.66 0.59 0.74 0.66tucSage2 0.59 0.72 0.65 0.59 0.74 0.65tucSage3 0.69 0.67 0.68 0.66 0.69 0.67SAIL-GRS 0.54 0.62 0.58 0.57 0.66 0.61Biel 0.13 0.39 0.20 0.09 0.34 0.14Travel GreekBaseline 0.17 0.65 0.26 0.16 0.73 0.26tucSage1?0.47 0.58 0.52 0.55 0.72 0.62tucSage2 0.46 0.53 0.49 0.50 0.59 0.54tucSage3 0.51 0.48 0.49 0.52 0.56 0.54SAIL-GRS 0.46 0.51 0.49 0.49 0.62 0.55Biel - - - - - -Tourism EnglishBaseline 0.80 0.94 0.87 0.82 0.94 0.87tucSage1?0.79 0.94 0.86 0.76 0.91 0.83tucSage2 0.78 0.93 0.85 0.73 0.90 0.80tucSage3 0.80 0.93 0.86 0.77 0.90 0.83SAIL-GRS 0.75 0.90 0.82 0.75 0.90 0.82Biel 0.04 0.14 0.06 0.02 0.08 0.04Finance EnglishBaseline 0.48 0.78 0.60 0.40 0.63 0.49tucSage1?0.61 0.81 0.70 0.43 0.54 0.48tucSage2 0.55 0.74 0.63 0.40 0.51 0.45tucSage3 0.52 0.67 0.58 0.39 0.43 0.41SAIL-GRS 0.78 0.78 0.78 0.67 0.62 0.65Biel 0.22 0.30 0.25 0.06 0.18 0.09Average over all four tasksBaseline 0.46 0.73 0.56 0.44 0.74 0.53tucSage1?0.62 0.77 0.69 0.58 0.73 0.65tucSage2 0.60 0.73 0.66 0.56 0.69 0.61tucSage3 0.63 0.69 0.65 0.59 0.65 0.61SAIL-GRS 0.63 0.70 0.67 0.62 0.70 0.66Biel 0.13 0.28 0.17 0.06 0.20 0.09Table 4: Weighted and unweighted precision, re-call and f-measure for all systems.
Best perfor-mance per metric and dataset shown in bold.AcknowledgementsThe task organizers wish to thank Maria Gian-noudaki and Maria Vomva for the editing of thehand-crafted grammars used in this evaluationtask.
The authors would like to thank the anony-mous reviewer for the valuable comments and sug-gestions to improve the quality of the paper.
Thiswork has been partially funded by the SpeDial andPortDial projects, supported by the EU SeventhFramework Programme (FP7), with grant number611396 and 296170 respectively.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: Apilot on semantic textual similarity.
In Proceedings15of the First Joint Conference on Lexical and Compu-tational Semantics, pages 385?393.Frederic Bechet, Benoit Favre, Alexis Nasr, and Math-ieu Morey.
2014.
Retrieving the syntactic structureof erroneous ASR transcriptions for open-domainspoken language understanding.
In Proceedings ofthe International Conference on Acoustics, Speech,and Signal Processing (ICASSP), pages 4125?4129.Yonatan Bisk and Julia Hockenmaier.
2012.
Simplerobust grammar induction with combinatory catego-rial grammars.
In Proceedings of the 26th Confer-ence on Artificial Intelligence, pages 1643?1649.Bart Cramer.
2007.
Limitations of current grammarinduction algorithms.
In Proceedings of the 45thannual meeting of the ACL: Student Research Work-shop, pages 43?48.Katerina T. Frantzi and Sophia Ananiadou.
1997.
Au-tomatic term recognition using contextual cues.
InProceedings of the International Joint Conferenceon Artificial Intelligence, pages 41?46.Spiros Georgiladakis, Christina Unger, Elias Iosif,Sebastian Walter, Philipp Cimiano, Euripides Pe-trakis, and Alexandros Potamianos.
2014.
Fusionof knowledge-based and data-driven approaches togrammar induction.
In Proceedings of Interspeech(accepted).Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Elias Iosif and Alexandros Potamianos.
2007.
A soft-clustering algorithm for automatic induction of se-mantic classes.
In Proceedings of Interspeech, pages1609?1612.Elias Iosif, Athanasios Tegos, Apostolos Pangos, EricFosler-Lussier, and Alexandros Potamianos.
2006.Unsupervised combination of metrics for semanticclass induction.
In Proceedings of the InternationalWorkshop on Spoken Language Technology (SLT),pages 86?89.Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,Spiros Georgiladakis, and Gianluca Mameli.
2013.Web data harvesting for speech understanding gram-mar induction.
In Proceedings of Interspeech, pages2733?2737.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proceedings of the 42nd an-nual meeting of the ACL, pages 470?477.Helen M. Meng and Kai-chung Siu.
2002.
Semi-automatic acquisition of semantic structures forunderstanding domain-specific natural languagequeries.
IEEE Transactions on Knowledge and DataEngineering, 14(1):172?181.Dmitrijs Milajevs and Matthew Purver.
2014.
Inves-tigating the contribution of distributional semanticinformation for dialogue act classification.
In Pro-ceedings of the 2nd Workshop on Continuous Vec-tor Space Models and their Compositionality, pages40?47.Teruhisa Misu and Tatsuya Kawahara.
2006.
A boot-strapping approach for developing language modelof new spoken dialogue systems by selecting webtexts.
In Proceedings of Interspeech, pages 9?12.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting of the ACL, pages 311?318.Andrew N. Pargellis, Eric Fosler-Lussier, Chin-HuiLee, Alexandros Potamianos, and Augustine Tsai.2004.
Auto-induced semantic classes.
Speech Com-munication, 43(3):183?203.Elias Ponvert, Jason Baldridge, and Katrin Erk.
2011.Simple unsupervised grammar induction from rawtext with cascaded finite state models.
In Proceed-ings of the 49th annual meeting of the ACL, pages1077?1086.Portdial.
2014a.
PortDial project, finalreport on automatic grammar inductionand evaluation D3.3.
Technical report,https://sites.google.com/site/portdial2/deliverables-publications.Portdial.
2014b.
PortDial project, freedata deliverable D3.2.
Technical report,https://sites.google.com/site/portdial2/deliverables-publications.Aarne Ranta.
2004.
Grammatical framework: A type-theoretical grammar formalism.
Journal of Func-tional Programming, 14(2):145?189.Ruhi Sarikaya.
2008.
Rapid bootstrapping of statisti-cal spoken dialogue systems.
Speech Communica-tion, 50(7):580?593.Abhinav Sethy, Shrikanth S. Narayanan, and BhuvanaRamabhadran.
2007.
Data driven approach for lan-guage model adaptation using stepwise relative en-tropy minimization.
In Proceedings of the Interna-tional Conference on Acoustics, Speech, and SignalProcessing (ICASSP), pages 177?180.Ye-Yi Wang and Alex Acero.
2006.
Rapid develop-ment of spoken language understanding grammars.Speech Communication, 48(3-4):390?416.16
