Unsupervised learning of word sense disambiguation rules by estimating anoptimum iteration number in the EM algorithmHiroyuki ShinnouDepartment of Systems Engineering,Ibaraki University4-12-1 Nakanarusawa, Hitachi, Ibaraki316-8511 JAPANshinnou@dse.ibaraki.ac.jpMinoru SasakiDepartment of Computer andInformation Sciences,Ibaraki University4-12-1 Nakanarusawa, Hitachi, Ibaraki316-8511 JAPANsasaki@cis.ibaraki.ac.jpAbstractIn this paper, we improve an unsuper-vised learning method using the Expectation-Maximization (EM) algorithm proposed byNigam et al for text classification problemsin order to apply it to word sense disambigua-tion (WSD) problems.
The improved methodstops the EM algorithm at the optimum itera-tion number.
To estimate that number, we pro-pose two methods.
In experiments, we solved50 noun WSD problems in the Japanese Dic-tionary Task in SENSEVAL2.
The score of ourmethod is a match for the best public score ofthis task.
Furthermore, our methods were con-firmed to be effective also for verb WSD prob-lems.1 IntroductionIn this paper, we improve an unsupervised learningmethod using the Expectation-Maximization (EM) algo-rithm proposed by (Nigam et al, 2000) for text classifi-cation problems in order to apply it to word sense disam-biguation (WSD) problems.
The original method workswell, but often causes worse classification for WSD.
Toavoid this, we propose two methods to estimate the opti-mum iteration number in the EM algorithm.Many problems in natural language processing can beconverted into classification problems, and be solved byan inductive learning method.
This strategy has been verysuccessful, but it has a serious problem in that an in-ductive learning method requires labeled data, which isexpensive because it must be made manually.
To over-come this problem, unsupervised learning methods usinghuge unlabeled data to boost the performance of ruleslearned by small labeled data have been proposed re-cently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Parket al, 2000)(Li and Li, 2002).
Among these methods,the method using the EM algorithm proposed by the pa-per(Nigam et al, 2000), which is referred to as the EMmethod in this paper, is the state of the art.
However, thetarget of the EM method is text classification.
It is hopedthat this method can be applied to WSD, because WSD isthe most important problem in natural language process-ing.The EM method works well in text classification,but often causes worse classification in WSD.
The EMmethod is expected to improve the accuracy of learnedrules step by step in proportion to the iteration number inthe EM algorithm.
However, this rarely happens in prac-tice, and in many cases, the accuracy falls after a certainiteration number in the EM algorithm.
In the worst case,the accuracy of the rule learned through only labeled datais degraded by using unlabeled data.
To overcome thisproblem, we estimate an optimum iteration number in theEM algorithm, and in actual learning, we stop the itera-tion of the EM algorithm at the estimated number.
If theestimated number is 0, it means that the EM method isnot used.
To estimate the optimum iteration number, wepropose two methods: one uses cross validation and theother uses two heuristics besides cross validation.
In thispaper, we refer to the former method as CV-EM and thelatter method as CV-EM2.In experiments, we solved 50 noun WSD problems inthe Japanese Dictionary Task in SENSEVAL2(Kurohashiand Shirai, 2001).
The original EM method failed toboost the precision (76.78%) of the rule learned throughonly labeled data.
On the other hand, CV-EM and CV-EM2 boosted the precision to 77.88% and 78.56%.
Thescore of CV-EM2 is a match for the best public score ofthis task.
Furthermore, these methods were confirmed tobe effective also for verb WSD problems.2 WSD by Naive BayesIn a classification problem, let C = {c1, c2, ?
?
?
, cm} bea set of classes.
An instance x is represented as a featurelistx = (f1, f2, ?
?
?
, fn).We can solve the classification problem by estimating theprobability P (c|x).
Actually, the class cx of x, is givenbycx = arg maxc?CP (c|x).Bayes theorem shows thatP (c|x) =P (c)P (x|c)P (x).As a result, we getcx = arg maxc?CP (c)P (x|c).In the above equation, P (c) is estimated easily; the ques-tion is how to estimate P (x|c).
Naive Bayes models as-sume the following:P (x|c) =n?i=1P (fi|c).
(1)The estimation of P (fi|c) is easy, so we can estimateP (x|c)(Mitchell, 1997).
In order to use Naive Bayes ef-fectively, we must select features that satisfy the equation1 as much as possible.
In text classification tasks, theappearance of each word corresponds to each feature.In this paper, we use following six attributes (e1 to e6)for WSD.
Suppose that the target word is wi which is thei-th word in the sentence.e1: the word wi?1e2: the word wi+1e3: two content words in front of wie4: two content words behind wie5: thesaurus ID number of e3e6: thesaurus ID number of e4For example, we make features from the following sen-tence 1 in which the target word is ?kiroku?2.kako/saikou/wo/kiroku/suru/ta/.Because the word to the left of the word ?kiroku?
is ?wo?,we get ?e1=wo?.
In the same way, we get ?e2=suru?.Content words to the left of the word ?kiroku?
are theword ?kako?
and the word ?saikou?.
We select two wordsfrom them in the order of proximity to the target word.Thus, we get ?e3=kako?
and ?e3=saikou?.
In thesame way, we get ?e4=suru?
and ?e4=.?.
Note1A sentence is segmented into words, and each word istransformed to its original form by morphological analysis.2?kiroku?
has at least two meanings: ?memo?
and ?record?.that the comma and the period are defined as a kind ofcontent words in this paper.
Next we look up the the-saurus ID of the word ?saikou?, and find 3.1920_4 3.In our thesaurus, as shown in Figure 1, a higher numbercorresponds to a higher level meaning.3313193192031920_4`saikou'Figure 1: Japanese thesaurus: Bunrui-goi-hyouIn this paper, we use a four-digit number anda five-digit number of a thesaurus ID.
As a re-sult, for ?e3=saikou?, we get ?e5=3192?
and?e5=31920?.
In the same way, for ?e3=kako?, weget ?e5=1164?
and ?e5=11642?.
Following this pro-cedure, we should look up the thesaurus ID of the word?suru?.
However, we do not look up the thesaurus ID fora word that consists of hiragana characters, because suchwords are too ambiguous, that is, they have too many the-saurus IDs.
When a word has multiple thesaurus IDs, wecreate a feature for each ID.As a result, we get following ten features from theabove example sentence:e1=wo, e2=suru, e3=saikou, e3=kako,e4=suru, e4=., e5=3192, e5=31920,e5=1164, e5=11642.3 Unsupervised learning using EMalgorithmWe can use the EM method if we use Naive Bayes forclassification problems.
In this paper, we show only keyequations and the key algorithm of this method(Nigam etal., 2000).Basically the method computes P (fi|cj) where fi is afeature and cj is a class.
This probability is given by4P (fi|cj) =1 +?|D|k=1 N(fi, dk)P (cj|dk)|F | +?|F |m=1?|D|k=1N(fm, dk)P (cj |dk).
(2)3In this paper we use the bunrui-goi-hyou as a Japanese the-saurus.4This equation is smoothed by taking into account the fre-quency 0.D: all data consisting of labeled data and unla-beled datadk: an element in DF : the set of all featuresfm: an element in FN(fi, dk): the number of fi in the instance dk.In our problem, N(fi, dk) is 0 or 1, and almost all ofthem are 0.
If dk is labeled, P (cj|dk) is 0 or 1.
If dk isunlabeled, P (cj|dk) is initially 0, and is updated to an ap-propriate value step by step in proportion to the iterationof the EM algorithm.By using equation 2, the following classifier is con-structed:P (cj|di) =P (cj)?fn?KdiP (fn|cj)?|C|r=1P (cr)?fn?KdiP (fn|cr).
(3)In this equation, Kdi is the set of features in the instancedi.P (cj) is computed byP (cj) =1 +?|D|k=1P (cj|dk)|C | + |D|.
(4)The EM algorithm computes P (cj|di) by using equa-tion 3 (E-step).
Next, by using equation 2, P (fi|cj)is computed (M-step).
By iterating E-step and M-step,P (fi|cj) and P (cj|di) converge.
In our experiment,when the difference between the current P (fi|cj) and theupdated P (fi|cj) comes to less than 8 ?
10?6 or the itera-tion number reaches 10 times, we judge that the algorithmhas converged.4 Estimation of the optimum iterationnumberIn this paper, we propose two methods (CV-EM and CV-EM2) to estimate the optimum iteration number in theEM algorithm.The CV-EM method is cross validation.
First of all, wedivide labeled data into three parts, one of which is usedas test data and the others are used as new labeled data.By using this new labeled data and huge unlabeled data,we conduct the EM method.
After each iteration in theEM algorithm, the learned rules at the time are evaluatedby using test data.
This experiment is conducted threetimes by changing the labeled data and test data.
Theprecision of each iteration number is given by the meanof three experiments.
The optimum iteration number isestimated to be the iteration number at which the highestprecision is achieved.The CV-EM2 method also uses cross validation, butestimates the optimum iteration number by ad-hoc mech-anism.First, we judge whether we can use the EM methodwithout modification or not.
To do this, we compare theprecision at convergence with the precision of the itera-tion number 1.
If the former is higher than the latter, wejudge that we can use the EM method without modifica-tion.
In this case, the optimum iteration number is esti-mated to be the converged number.
On the other hand, ifthe former is not higher than the latter, we go to the sec-ond judgment, namely whether the EM method should beused or not.
To judge this, we compare the two precisionsof the iteration number 0 and 1.
The iteration number 0means that the EM method is not used.
If the precision ofthe iteration number 0 is higher than the precision of theiteration number 1, we judge that the EM method shouldnot be used.
In this case, the optimum iteration numberis estimated to be 0.
Conversely, if the precision of theiteration number 1 is higher than the precision of the it-eration number 0, we judge that the EM method shouldbe used.
In this case, the optimum iteration number isestimated to be the number obtained by CV-EM.In the many cases, the CV-EM2 outputs the same num-ber as the CV-EM.
However, the basic idea is different.Roughly speaking, the CV-EM2 relies on two heuris-tics: (1) Basically we only have to judge whether the EMmethod can be used or not, because the EM algorithmimproves or degrades the precision monotonically.
(2)Whether the EM algorithm succeeds correlates closelywith whether the precision is improved by the first iter-ation of the EM algorithm.
Therefore, we estimate theoptimum iteration number by comparing three precisions,the precision of the iteration number 0, 1 and at conver-gence.The figure 2 shows a typical case that the CV-EM2 dif-fers from the CV-EM.
In the cross validation, the preci-sion is degraded by the first iteration of the EM algorithm,and then it is improved by iteration, and the maximumprecision is achieved at the k-th iteration, but the preci-sion converges to the lower point than the precision of theiteration number 1.
In this case, the CV-EM gives k as theestimation, but the CV-EM2 gives 0.0 1 kiterationprecisionCV-EM   =>  kCV-EM2   =>  0Figure 2: Typical difference between CV-EM and CV-EM25 ExperimentsTo confirm the effectiveness of our methods, we testedwith 50 nouns of the Japanese Dictionary Task in SEN-SEVAL2(Kurohashi and Shirai, 2001).The Japanese Dictionary Task is a standard WSD prob-lem.
As the evaluation words, 50 noun words and 50 verbwords are provided.
These words are selected so as tobalance the difficulty of WSD.
The number of labeled in-stances for nouns is 177.4 on average, and for verbs is172.7 on average.
The number of test instances for eachevaluation word is 100, so the number of test instancesof noun and verb evaluation words is 5,000 respectively.However, unlabeled data are not provided.
Note that wecannot use simple raw texts including the target word, be-cause we must use the same dictionary and part of speechset as labeled data.
Therefore, we use Mainichi newspa-per articles for 1995 with word segmentations providedby RWC.
This data is the origin of labeled data.
As a re-sult, we gathered 7585.5 and 6571.9 unlabeled instancesfor per noun and per verb evaluation word on average,respectively.Table 1 shows the results of experiments for noun eval-uation words.
In this table, NB means Naive Bayes,EM the EM method, and ideal the EM methodstopping at the ideal iteration number.
Note that the pre-cision is computed by mixed-gained scoring(Kurohashiand Shirai, 2001) which gives partial points in somecases.The precision of Naive Bayes which learns throughonly labeled data was 76.58%.
The EM method failed toboost it, and degraded it to 73.56%.
On the other hand, byusing CV-EM the precision was boosted to 77.88%.
Fur-thermore, CV-EM2 boosted it to 78.56%.
This score is amatch for the best public score of this task.
As success-ful results in this task, two researches are reported.
Oneused Naive Bayes with various attributes, and achieved78.22% precision(Murata et al, 2001).
Another usedAdaboost of decision trees, and achieved 78.47% preci-sion(Nakano and Hirai, 2002).
Our score is higher thanthese scores 5.
Furthermore, their methods used syntacticanalysis, but our methods do not need it.In the same way, we performed experiments for verbevaluation words.
Table 2 shows the results.
In the ex-periment, Naive Bayes achieved 78.16% precision.
TheEM method boosted it to 78.74%.
Furthermore, CV-EMand CV-EM2 boosted it to 79.22% and 79.26% respec-tively.
CV-EM2 is marginally higher than CV-EM.5The best score for the total of noun words and verb wordsis reported to be 79.33% in (Murata et al, 2001).Table 1: Results of experiments (Noun)Word NB EM CV-EM CV-EM2 ideal(%) (%) (%) (%) (%)aida 81.0 80.0 82.0 82.0 82.0atama 60.0 64.0 60.0 64.0 66.0ippan 88.0 86.0 89.0 89.0 89.0ippou 82.0 88.0 88.0 88.0 89.0ima 90.0 90.0 90.0 90.0 90.0imi 45.0 53.0 53.0 53.0 53.0utagai 100.0 95.0 98.0 98.0 100.0otoko 92.0 89.0 92.0 92.0 92.0kaihatsu 62.0 63.0 62.0 62.0 63.0kaku n 71.0 77.0 71.0 77.0 81.0kankei 85.0 90.0 90.0 90.0 90.0kimochi 65.0 65.0 65.0 65.0 66.0kiroku 74.0 71.0 73.0 73.0 77.0gijutsu 96.0 92.0 96.0 96.0 96.0genzai 97.0 09.0 98.0 98.0 98.0koushou 100.0 88.0 100.0 100.0 100.0kokunai 46.0 58.0 46.0 46.0 58.0kotoba 45.0 40.0 40.0 40.0 45.0kodomo 67.0 73.0 72.0 72.0 73.0gogo 77.0 65.0 86.0 86.0 86.0shijo 77.0 55.0 77.0 77.0 77.0shimin 67.0 63.0 67.0 67.0 67.0shakai 82.0 83.0 83.0 83.0 83.0shonen 92.0 90.0 90.0 90.0 92.0jikan 54.0 15.0 54.0 54.0 54.0jigyou 69.0 70.0 69.0 69.0 71.0jidai 72.0 77.0 77.0 77.0 78.0jibun 100.0 100.0 100.0 100.0 100.0joho 77.0 64.0 77.0 77.0 77.0sugata 55.0 63.0 61.0 61.0 63.0seishin 65.0 66.0 66.0 66.0 66.0taishou 98.0 98.0 98.0 98.0 98.0daihyou 85.0 95.0 96.0 96.0 98.0chikaku 74.0 87.0 87.0 87.0 87.0chihou 70.0 72.0 70.0 70.0 72.0chushin 98.0 98.0 98.0 98.0 98.0te 47.0 48.0 47.0 48.0 48.0teido 100.0 100.0 100.0 100.0 100.0denwa 84.0 65.0 83.0 83.0 85.0doujitsu 81.0 51.0 57.0 81.0 81.0hana 99.0 97.0 99.0 99.0 99.0hantai 97.0 97.0 97.0 97.0 97.0baai 82.0 91.0 91.0 91.0 92.0mae 86.0 91.0 92.0 91.0 92.0minkan 100.0 100.0 100.0 100.0 100.0musume 88.0 88.0 88.0 88.0 88.0mune 71.0 77.0 77.0 77.0 79.0me 18.0 17.0 18.0 18.0 18.0mono 31.0 27.0 27.0 27.0 31.0mondai 97.0 97.0 97.0 97.0 97.0average 76.78 73.56 77.88 78.56 79.64Table 2: Results of experiments (Verb)Word NB EM CV-EM CV-EM2 ideal(%) (%) (%) (%) (%)ataeru 71.0 78.0 78.0 78.0 78.0iu 94.0 94.0 94.0 94.0 94.0ukeru 59.0 64.0 59.0 64.0 64.0uttaeru 84.0 87.0 87.0 87.0 88.0umareru 69.0 83.0 82.0 83.0 83.0egaku 58.0 56.0 56.0 56.0 58.0omou 90.0 89.0 89.0 89.0 90.0kau 83.0 83.0 83.0 83.0 83.0kakaru 58.0 57.0 58.0 58.0 58.0kaku v 72.0 66.0 72.0 72.0 72.0kawaru 92.0 92.0 92.0 92.0 92.0kangaeru 99.0 99.0 99.0 99.0 99.0kiku 56.0 55.0 55.0 55.0 56.0kimaru 96.0 96.0 96.0 96.0 96.0kimeru 93.0 93.0 93.0 93.0 93.0kuru 84.0 85.0 86.0 85.0 86.0kuwaeru 89.0 89.0 89.0 89.0 89.0koeru 78.0 82.0 85.0 82.0 88.0shiru 97.0 97.0 97.0 97.0 97.0susumu 49.0 50.0 50.0 50.0 50.0susumeru 97.0 95.0 97.0 97.0 97.0dasu 35.0 29.0 35.0 35.0 36.0chigau 100.0 100.0 100.0 100.0 100.0tsukau 97.0 97.0 97.0 97.0 97.0tsukuru 69.0 75.0 78.0 75.0 78.0tsutaeru 75.0 76.0 76.0 76.0 76.0dekiru 81.0 81.0 81.0 81.0 81.0deru 59.0 64.0 64.0 64.0 64.0tou 69.0 79.0 79.0 79.0 79.0toru 32.0 34.0 32.0 34.0 37.0nerau 99.0 99.0 99.0 99.0 99.0nokosu 79.0 79.0 79.0 79.0 79.0noru 54.0 54.0 54.0 54.0 54.0hairu 36.0 36.0 36.0 36.0 36.0hakaru 92.0 92.0 92.0 92.0 92.0hanasu 100.0 87.0 100.0 100.0 100.0hiraku 86.0 94.0 94.0 94.0 94.0fukumu 99.0 99.0 99.0 99.0 99.0matsu 52.0 50.0 51.0 51.0 52.0matomeru 79.0 80.0 80.0 80.0 80.0mamoru 79.0 71.0 70.0 71.0 79.0miseru 98.0 98.0 98.0 98.0 98.0mitomeru 89.0 89.0 89.0 89.0 89.0miru 73.0 71.0 73.0 73.0 73.0mukaeru 89.0 89.0 89.0 89.0 89.0motsu 57.0 62.0 57.0 57.0 62.0motomeru 87.0 87.0 87.0 87.0 87.0yomu 88.0 88.0 88.0 88.0 88.0yoru 97.0 97.0 97.0 97.0 97.0wakaru 90.0 90.0 90.0 90.0 90.0average 78.16 78.74 79.22 79.26 79.926 Discussion6.1 Cause of failure of the EM methodWhy does the EM method often fail to boost the perfor-mance?
One reason may be the difference among classdistributions of labeled data L, unlabeled data U and testdata T .
Practically L, U and T are the same because theyconsist of random samples from all data.
However, thereare differences among them.Intuitively, learning by combining labeled data and un-labeled data is regarded as learning from the distributionof L + U .
It is expected that the EM method is effectiveif d = d(L, T )?d(L+U, T ) > 0, and is counterproduc-tive if d < 0, in which d(?, ?)
means the distance of twodistributions.To confirm the above expectation, we conduct an ex-periment by using Kullback-Leibler divergence as d(?, ?
).The distribution of L + U can be obtained from Equation4 when the EM algorithm converges.
The result of theexperiment is shown in Table 3.Table 3: Effects of the distribution of meaningsd > 0 d < 0improvement 6 7deterioration 2 8The columns of the table are divided into positive(d > 0) and negative (d < 0).
Positive means that L + Ugets close to T and negative means that L+U goes awayfrom T .
The rows of the table are divided into improve-ment of precision and deterioration of precision.
In thispaper, improvement of precision is when the precision isimproved by over 5%, and deterioration of precision iswhen the precision is degraded by over 5%.This result indicates that there is a weak correlation be-tween whether L + U gets close to T or goes away fromT and whether the EM method is effective or not, but wecannot conclude they are completely dependent.
How-ever, the evaluation word ?genzai?
whose precision fallsmost by the EM method is precisely the above case.
Thed for this word is the smallest, ?0.30, among all evalua-tion words.
Further investigation of the causes of failureof the EM method is our future work.6.2 Effectiveness of estimation of CV-EM2CV-EM2 achieved ideal estimation for 29 of 50 evalua-tion words, that is 58%.
Furthermore, for 15 of the other21 evaluation words, the difference between the preci-sion through our method and that through ideal estima-tion did not exceed 2%.
Therefore, estimation of CV-EM2 is mostly effective.The words ?kokunai?
and ?kotoba?
are typical caseswhere estimation fails.
The difference between the pre-cision of CV-EM2 and that through ideal estimation ex-ceeded 5%.
The failure of estimation for these two wordsreduced the whole precision.Figure 3 compares the precision for cross validationand that for actual evaluation for the word ?kokunai?.
Inthe same way, Figure 3 shows the case of the word ?ko-toba?.
In these figures, the x-axis shows the iterationnumber of the EM algorithm.
To clarify the change ofprecision, the initial precision is set to 0, and the y-axisshows the difference (%) between the actual and initialprecision.In the case of ?kokunai?, the precision got worse incross validation, but the precision got better in the ac-tual evaluation.
This means that cross validation is use-less, so it is difficult to estimate an optimum iterationnumber in the EM algorithm.
However, such cases arerare.
In the experiment, this case arises for only this word?kokunai?.
Consider next the case of ?kotoba?.
In crossvalidation, the precision improved in the first iteration ofthe EM algorithm, but got worse step by step thereafter.On the other hand, in the actual evaluation, the precisiongot worse even in the first iteration of the EM algorithm.The difference of these results in the first iteration of theEM algorithm causes our estimation to fail.
In future,we must improve our method by further investigation ofthese words.-505100 2 4 6 8 10differencefrombaseprecision(%)iterationREALCROSS-VALFigure 3: Comparison between cross validation and ac-tual evaluation (?kokunai?
)-6-5-4-3-2-1010 2 4 6 8 10differencefrombaseprecision(%)iterationREALCROSS-VALFigure 4: Comparison between cross validation and ac-tual evaluation (?kotoba?
)6.3 Comparison of CV-EM and CV-EM2CV-EM2 is slightly superior to CV-EM.
In the evaluationword ?doujitu?, there is a remarkable difference betweenthe two methods.Figure 5 shows the change of the precision for ?dou-jitsu?
in cross validation, and Figure 6 shows that in ac-tual evaluation.The precision goes up in cross validation, but goesdown largely in actual evaluation.
In CV-EM, the bestpoint is selected in cross validation, that is 3.
On the otherhand, CV-EM2 estimates 0 by using the relation of threeprecisions: the initial precision, the precision for the iter-ation 1 and the precision at convergence.Let?s count the number of words for which CV-EM2is better or worse than CV-EM.
For one word ?mae?
innouns and three words ?kuru?, ?koeru?
and ?tukuru?
inverbs, CV-EM was superior to CV-EM2.
On the otherhand, for four words ?atama?, ?kaku n?, ?te?
and ?dou-jitsu?
in nouns and four words ?ukeru?, ?umareru?, ?toru?and ?mamortu?
in verbs, CV-EM2 was better to CV-EM.These numbers show that our method is somewhat supe-rior to CV-EM.-6-5-4-3-2-10120 2 4 6 8 10differencefrombaseprecision(%)iterationCROSS-VALFigure 5: Cross validation in ?doujitsu?-30-25-20-15-10-500 2 4 6 8 10differencefrombaseprecision(%)iterationCROSS-VALFigure 6: Actual evaluation in ?doujitsu?6.4 Unsupervised learning for verb WSDIn the experiments, CV-EM and CV-EM2 improved theEM method for both noun words and verb words.
Theeffectiveness of these methods was large for noun words,but was small for verb words.
We believe that the causeof this difference is the difficulty of unsupervised learningfor verb WSD.
In ideal estimation, the precision for nounwords was boosted from 76.78% to 79.64% by the EMmethod, that is 1.037 times.
On the other hand, the preci-sion for verb words was boosted from 78.16% to 79.92%by the EM method, that is 1.022 times.
This shows thatthe EM method does not work so well for verb words.We consider that feature independence plays a key rolein unsupervised learning.
Suppose the instance x con-sists of two features f1and f2.
When class cx of x isjudged from feature f1, the probability P (cx|f2) is tunedto be larger.
The question is whether it is actually rightor not to increase P (cx|f2).
If it is right, unsupervisedlearning works well, but if it is not, unsupervised learn-ing fails.
Intuitively, feature independence warrants in-creasing P (cx|f2).
In noun WSD, the left context of thetarget word corresponds to the words modifying the tar-get word, and the right context of the target word cor-responds to the verb word whose case slot can have thetarget word.
Both the left context and right context canjudge the meaning of the target word by itself, and are in-dependent.
Left context and right context act as indepen-dent features.
On the other hand, we cannot find such anopportune interpretation for the features of verbs (Shin-nou, 2002).
Therefore, the EM method is not so effectivefor verb words.Naive Bayes assumes the independence of features,too.
However, this assumption is not so rigid in practice.We believe that the improvement by the EM method forverb words depends on the robustness of Naive Bayes.
Inour experiments, the EM method for noun words failedto boost the precision.
We think that the cause is the im-balance of labeled data, unlabeled data and test data.
Weshould investigate this in a future study.6.5 Related worksCo-training(Blum and Mitchell, 1998) is a powerful un-supervised learning method.
In Co-training, if we canfind two independent feature sets for the target problem,any supervised learning method can be used.
Further-more, it is reported that Co-training is superior to theEM method if complete independent feature sets can beused(Nigam and Ghani, 2000).
However, Co-trainingrequires consistency besides independence for two fea-ture sets.
This condition makes it difficult to apply Co-training to multiclass classification problems.
On theother hand, the EM method requires Naive Bayes to beused as the supervised learning method, but can be ap-plied to multiclass classification problems without anymodification.
Therefore, the EM method is more prac-tical than Co-training.Yarowsky proposed the unsupervised learning methodfor WSD(Yarowsky, 1995).
His method is reported to bea special case of Co-training(Blum and Mitchell, 1998).As two independent feature sets, one is the context sur-rounding the target word and the other is the heuristic of?one sense per discourse?.
However, it is unknown howvalid this heuristic is for granularity of meanings of ourevaluation words.
Furthermore, this method needs doc-uments in which the target word appears multiple times,as unlabeled data.
Therefore, it is not so easy to gatherunlabeled data.
On the other hand, the EM method doesnot have such problem because it uses sentences includ-ing the target word as unlabeled data.6.6 Future worksWe have three future works.
First, we must raise the pre-cision for verb words, which may be impossible unlesswe use other features, so we need to investigate other fea-tures.
Second, we must improve the estimation method ofthe optimum iteration number in the EM algorithm.
Thedifference between the precision through our estimationand that through the ideal estimation is large.
We can im-prove the accuracy by improving the estimation method.Finally, we will investigate the reason for the failure ofthe EM method, which may be the key to unsupervisedlearning.7 ConclusionsIn this paper, we improved the EM method proposed byNigam et al for text classification problems in order toapply it to WSD problems.
To avoid some failures in theoriginal EM method, we proposed two methods to esti-mate the optimum iteration number in the EM algorithm.In experiments, we tested with 50 noun WSD problems inthe Japanese Dictionary Task in SENSEVAL2.
Our twomethods greatly improved the original EM method.
Es-pecially, the score of noun evaluation words was equiva-lent to the best public score of this task.
Furthermore, ourmethods were also effective for verb WSD problems.
Infuture, we will tackle three works: (1) To find other effec-tive features for unsupervised learning of verb WSD, (2)To improve the estimation method of the optimum itera-tion number in the EM algorithm, and (3) To investigatethe reason for the failure of the EM method.ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In 11thAnnual Conference on Computational Learning The-ory (COLT-98), pages 92?100.Sadao Kurohashi and Kiyoaki Shirai.
2001.SENSEVAL-2 Japanese Tasks (in Japansese).
InTechnical Report of IEICE, NLC-36?48, pages 1?8.Cong Li and Hang Li.
2002.
Word Translation Dis-ambiguation Using Bilingual Bootstrapping.
In 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL-02), pages 343?351.Tom Mitchell.
1997.
Machine Learning.
McGraw-HillCompanies.Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto,Qing Ma, and Hitoshi Isahara.
2001.
CRL at Japanesedictionary-based task of SENSEVAL-2 (in Japanese).In Technical Report of IEICE, NLC-36?48, pages 31?38.Keigo Nakano and Yuuzou Hirai.
2002.
AdaBoost womotiita gogi no aimaisei kaisyou (in Japanese).
In 8thAnnual Meeting of the Association for Natural Lan-guage Processing, pages 659?662.Kamal Nigam and Rayid Ghani.
2000.
Analyzing theeffectiveness and applicability of co-training.
In 9thInternational Conference on Information and Knowl-edge Management, pages 86?93.Kamal Nigam, Andrew McCallum, Sebastian Thrun, andTom Mitchell.
2000.
Text Classification from Labeledand Unlabeled Documents using EM.
Machine Learn-ing, 39(2/3):103?134.Seong-Bae Park, Byoung-Tak Zhang, and Yung TaekKim.
2000.
Word sense disambiguation by learningfrom unlabeled data.
In 38th Annual Meeting of theAssociation for Computational Linguistics (ACL-00),pages 547?554.Hiroyuki Shinnou.
2002.
Learning of word sensedisambiguation rules by Co-training, checking co-occurrence of features.
In 3rd international conferenceon Language resources and evaluation (LREC-2002),pages 1380?1384.David Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In 33thAnnual Meeting of the Association for ComputationalLinguistics (ACL-95), pages 189?196.
