Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750?758,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Topic Similarity Modelfor Hierarchical Phrase-based TranslationXinyan Xiao?
Deyi Xiong?
Min Zhang??
Qun Liu?
Shouxun Lin?
?Key Lab.
of Intelligent Info.
Processing ?Human Language TechnologyInstitute of Computing Technology Institute for Infocomm ResearchChinese Academy of Sciences{xiaoxinyan, liuqun, sxlin}@ict.ac.cn {dyxiong, mzhang?
}@i2r.a-star.edu.sgAbstractPrevious work using topic model for statis-tical machine translation (SMT) explore top-ic information at the word level.
Howev-er, SMT has been advanced from word-basedparadigm to phrase/rule-based paradigm.
Wetherefore propose a topic similarity model toexploit topic information at the synchronousrule level for hierarchical phrase-based trans-lation.
We associate each synchronous rulewith a topic distribution, and select desirablerules according to the similarity of their top-ic distributions with given documents.
Weshow that our model significantly improvesthe translation performance over the baselineon NIST Chinese-to-English translation ex-periments.
Our model also achieves a betterperformance and a faster speed than previousapproaches that work at the word level.1 IntroductionTopic model (Hofmann, 1999; Blei et al, 2003) isa popular technique for discovering the underlyingtopic structure of documents.
To exploit topic infor-mation for statistical machine translation (SMT), re-searchers have proposed various topic-specific lexi-con translation models (Zhao and Xing, 2006; Zhaoand Xing, 2007; Tam et al, 2007) to improve trans-lation quality.Topic-specific lexicon translation models focuson word-level translations.
Such models first esti-mate word translation probabilities conditioned ontopics, and then adapt lexical weights of phrases?Corresponding authorby these probabilities.
However, the state-of-the-art SMT systems translate sentences by using se-quences of synchronous rules or phrases, instead oftranslating word by word.
Since a synchronous ruleis rarely factorized into individual words, we believethat it is more reasonable to incorporate the topicmodel directly at the rule level rather than the wordlevel.Consequently, we propose a topic similari-ty model for hierarchical phrase-based translation(Chiang, 2007), where each synchronous rule is as-sociated with a topic distribution.
In particular,?
Given a document to be translated, we cal-culate the topic similarity between a rule andthe document based on their topic distributions.We augment the hierarchical phrase-based sys-tem by integrating the proposed topic similaritymodel as a new feature (Section 3.1).?
As we will discuss in Section 3.2, the similaritybetween a generic rule and a given source docu-ment computed by our topic similarity model isoften very low.
We don?t want to penalize thesegeneric rules.
Therefore we further propose atopic sensitivity model which rewards genericrules so as to complement the topic similaritymodel.?
We estimate the topic distribution for a rulebased on both the source and target side topicmodels (Section 4.1).
In order to calculate sim-ilarities between target-side topic distributionsof rules and source-side topic distributions ofgiven documents during decoding, we project75000.20.40.61 5 10 15 20 25 30(a) ??
??
?
opera-tional capability00.20.40.61 5 10 15 20 25 30(b) ?
?X1 ?
grandsX100.20.40.61 5 10 15 20 25 30(c) ?
?X1 ?
giveX100.20.40.61 5 10 15 20 25 30(d) X1 ??
??
X2 ?held talksX1 X2Figure 1: Four synchronous rules with topic distributions.
Each sub-graph shows a rule with its topic distribution,where the X-axis means topic index and the Y-axis means the topic probability.
Notably, the rule (b) and rule (c) sharesthe same source Chinese string, but they have different topic distributions due to the different English translations.the target-side topic distributions of rules intothe space of source-side topic model by one-to-many projection (Section 4.2).Experiments on Chinese-English translation tasks(Section 6) show that, our method outperforms thebaseline hierarchial phrase-based system by +0.9BLEU points.
This result is also +0.5 points high-er and 3 times faster than the previous topic-specificlexicon translation method.
We further show thatboth the source-side and target-side topic distribu-tions improve translation quality and their improve-ments are complementary to each other.2 Background: Topic ModelA topic model is used for discovering the topicsthat occur in a collection of documents.
Both La-tent Dirichlet Allocation (LDA) (Blei et al, 2003)and Probabilistic Latent Semantic Analysis (PLSA)(Hofmann, 1999) are types of topic models.
LDAis the most common topic model currently in use,therefore we exploit it for mining topics in this pa-per.
Here, we first give a brief description of LDA.LDA views each document as a mixture pro-portion of various topics, and generates each wordby multinomial distribution conditioned on a topic.More specifically, as a generative process, LDA firstsamples a document-topic distribution for each doc-ument.
Then, for each word in the document, it sam-ples a topic index from the document-topic distribu-tion and samples the word conditioned on the topicindex according the topic-word distribution.Generally speaking, LDA contains two types ofparameters.
The first one relates to the document-topic distribution, which records the topic distribu-tion of each document.
The second one is used fortopic-word distribution, which represents each topicas a distribution over words.
Based on these param-eters (and some hyper-parameters), LDA can infer atopic assignment for each word in the documents.
Inthe following sections, we will use these parametersand the topic assignments of words to estimate theparameters in our method.3 Topic Similarity ModelSentences should be translated in consistence withtheir topics (Zhao and Xing, 2006; Zhao and Xing,2007; Tam et al, 2007).
In the hierarchical phrasebased system, a synchronous rule may be related tosome topics and unrelated to others.
In terms ofprobability, a rule often has an uneven probabilitydistribution over topics.
The probability over a topicis high if the rule is highly related to the topic, other-wise the probability will be low.
Therefore, we usetopic distribution to describe the relatedness of rulesto topics.Figure 1 shows four synchronous rules (Chiang,2007) with topic distributions, some of which con-tain nonterminals.
We can see that, although thesource part of rule (b) and (c) are identical, their top-ic distributions are quite different.
Rule (b) containsa highest probability on the topic about ?China-U.S.relationship?, which means rule (b) is much morerelated to this topic.
In contrast, rule (c) containsan even distribution over various topics.
Thus, giv-en a document about ?China-U.S.
relationship?, wehope to encourage the system to apply rule (b) butpenalize the application of rule (c).
We achieve thisby calculating similarity between the topic distribu-tions of a rule and a document to be translated.More formally, we associate each rule with a rule-topic distribution P (z|r), where r is a rule, and z isa topic.
Suppose there are K topics, this distribution751can be represented by a K-dimension vector.
Thek-th component P (z = k|r) means the probabilityof topic k given the rule r. The estimation of suchdistribution will be described in Section 4.Analogously, we represent the topic informationof a document d to be translated by a document-topic distribution P (z|d), which is also a K-dimension vector.
The k-th dimension P (z = k|d)means the probability of topic k given document d.Different from rule-topic distribution, the document-topic distribution can be directly inferred by an off-the-shelf LDA tool.Consequently, based on these two distribution-s, we select a rule for a document to be translat-ed according to their topic similarity (Section 3.1),which measures the relatedness of the rule to thedocument.
In order to encourage the applicationof generic rules which are often penalized by oursimilarity model, we also propose a topic sensitivitymodel (Section 3.2).3.1 Topic SimilarityBy comparing the similarity of their topic distribu-tions, we are able to decide whether a rule is suitablefor a given source document.
The topic similaritycomputes the distance of two topic distributions.
Wecalculate the topic similarity by Hellinger function:Similarity(P (z|d), P (z|r))=K?k=1(?P (z = k|d) ?
?P (z = k|r))2(1)Hellinger function is used to calculate distributiondistance and is popular in topic model (Blei and Laf-ferty, 2007).1 By topic similarity, we aim to encour-age or penalize the application of a rule for a giv-en document according to their topic distributions,which then helps the SMT system make better trans-lation decisions.3.2 Topic SensitivityDomain adaptation (Wu et al, 2008; Bertoldi andFederico, 2009) often distinguishes general-domaindata from in-domain data.
Similarly, we divide therules into topic-insensitive rules and topic-sensitive1We also try other distance functions, including Euclideandistance, Kullback-Leibler divergence and cosine function.They produce similar results in our preliminary experiments.rules according to their topic distributions.
Let?srevisit Figure 1.
We can easily find that the topicdistribution of rule (c) distribute evenly.
This in-dicates that it is insensitive to topics, and can beapplied in any topics.
We call such a rule a topic-insensitive rule.
In contrast, the distributions of therest rules peak on a few topics.
Such rules are calledtopic-sensitive rules.
Generally speaking, a topic-insensitive rule has a fairly flat distribution, while atopic-sensitive rule has a sharp distribution.A document typically focuses on a few topics, andhas a sharp topic distribution.
In contrast, the distri-bution of topic-insensitive rule is fairly flat.
Hence,a topic-insensitive rule is always less similar to doc-uments and is punished by the similarity function.However, topic-insensitive rules may be morepreferable than topic-sensitive rules if neither ofthem are similar to given documents.
For a doc-ument about the ?military?
topic, the rule (b) and(c) in Figure 1 are both dissimilar to the document,because rule (b) relates to the ?China-U.S. relation-ship?
topic and rule (c) is topic-insensitive.
Never-theless, since rule (c) occurs more frequently acrossvarious topics, it may be better to apply rule (c).To address such issue of the topic similarity mod-el, we further introduce a topic sensitivity model todescribe the topic sensitivity of a rule using entropyas a metric:Sensitivity(P (z|r))= ?K?k=1P (z = k|r) ?
log (P (z = k|r)) (2)According to the Eq.
(2), a topic-insensitive rule hasa large entropy, while a topic-sensitive rule has a s-maller entropy.
By incorporating the topic sensitivi-ty model with the topic similarity model, we enableour SMT system to balance the selection of these t-wo types of rules.
Given rules with approximatelyequal values of Eq.
(1), we prefer topic-insensitiverules.4 EstimationUnlike document-topic distribution that can be di-rectly learned by LDA tools, we need to estimate therule-topic distribution according to our requirement.In this paper, we try to exploit the topic information752of both source and target language.
To achieve thisgoal, we use both source-side and target-side mono-lingual topic models, and learn the correspondencebetween the two topic models from word-alignedbilingual corpus.Specifically, we use two types of rule-topic dis-tributions: one is source-side rule-topic distributionand the other is target-side rule-topic distribution.These two rule-topic distributions are estimated bycorresponding topic models in the same way (Sec-tion 4.1).
Notably, only source language documentsare available during decoding.
In order to computethe similarity between the target-side topic distribu-tion of a rule and the source-side topic distributionof a given document?we need to project the target-side topic distribution of a synchronous rule into thespace of the source-side topic model (Section 4.2).A more principle way is to learn a bilingual topicmodel from bilingual corpus (Mimno et al, 2009).However, we may face difficulty during decoding,where only source language documents are avail-able.
It requires a marginalization to infer the mono-lingual topic distribution using the bilingual topicmodel.
The high complexity of marginalization pro-hibits such a summation in practice.
Previous workon bilingual topic model avoid this problem by somemonolingual assumptions.
Zhao and Xing (2007)assume that the topic model is generated in a mono-lingual manner, while Tam et al, (2007) constructtheir bilingual topic model by enforcing a one-to-one correspondence between two monolingual topicmodels.
We also estimate our rule-topic distributionby two monolingual topic models, but use a differ-ent way to project target-side topics onto source-sidetopics.4.1 Monolingual Topic Distribution EstimationWe estimate rule-topic distribution from word-aligned bilingual training corpus with documen-t boundaries explicitly given.
The source and tar-get side distributions are estimated in the same way.For simplicity, we only describe the estimation ofsource-side distribution in this section.The process of rule-topic distribution estimationis analogous to the traditional estimation of ruletranslation probability (Chiang, 2007).
In additionto the word-aligned corpus, the input for estimationalso contains the source-side topic-document distri-bution of every documents inferred by LDA tool.We first extract synchronous rules from trainingdata in a traditional way.
When a rule r is extractedfrom a document d with topic distribution P (z|d),we collect an instance (r, P (z|d), c), where c is thefraction count of an instance as described in Chiang,(2007).
After extraction, we get a set of instancesI = {(r, P (z|d), c)} with different document-topicdistributions for each rule.
Using these instances,we calculate the topic probability P (z = k|r) asfollows:P (z = k|r) =?I?I c?
P (z = k|d)?Kk?=1?I?I c?
P (z = k?|d)(3)By using both source-side and target-sidedocument-topic distribution, we obtain two rule-topic distributions for each rule in total.4.2 Target-side Topic Distribution ProjectionAs described in the previous section, we also esti-mate the target-side rule-topic distribution.
How-ever, only source document-topic distributions areavailable during decoding.
In order to calculatethe similarity between the target-side rule-topic dis-tribution of a rule and the source-side document-topic distribution of a source document, we need toproject target-side topics into the source-side topicspace.
The projection contains two steps:?
In the first step, we learn the topic-to-topic cor-respondence probability p(zf |ze) from target-side topic ze to source-side topic zf .?
In the second step, we project the target-sidetopic distribution of a rule into source-side top-ic space using the correspondence probability.In the first step, we estimate the correspondenceprobability by the co-occurrence of the source-sideand the target-side topic assignment of the word-aligned corpus.
The topic assignments are outputby LDA tool.
Thus, we denotes each sentence pairby (zf , ze,a), where zf and ze are the topic as-signments of source-side and target-side sentencesrespectively, and a is a set of links {(i, j)}.
Alink (i, j) means a source-side position i aligns toa target-side position j.
Thus, the co-occurrence ofa source-side topic with index kf and a target-side753e-topic f-topic 1 f-topic 2 f-topic 3enterprises ??
(agricultural) ??
(enterprise) ??
(develop)rural ??
(rural) ??
(market) ??
(economic)state ??
(peasant) ??
(state) ??
(technology )agricultural ??
(reform) ??
(company) ??
(China)market ??
(finance) ??
(finance) ??
(technique)reform ??
(social) ??
(bank) ??
(industry)production ??
(safety) ??
(investment) ??
(structure)peasants ??
(adjust) ??
(manage) ??
(innovation)owned ??
(policy) ??
(reform) ??
(accelerate)enterprise ??
(income) ??
(operation) ??
(reform)p(zf |ze) 0.38 0.28 0.16Table 1: Example of topic-to-topic correspondence.
Thelast line shows the correspondence probability.
Each col-umnmeans a topic represented by its top-10 topical word-s.
The first column is a target-side topic, while the restthree columns are source-side topics.topic ke is calculated by:?
(zf ,ze,a)?(i,j)?a?
(zfi , kf ) ?
?
(zej , ke) (4)where ?
(x, y) is the Kronecker function, which is 1if x = y and 0 otherwise.
We then compute theprobability of P (z = kf |z = ke) by normalizingthe co-occurrence count.
Overall, after the first step,we obtain an correspondence matrix MKe?Kf fromtarget-side topic to source-side topic, where the itemMi,j represents the probability P (zf = i|ze = j).In the second step, given the correspondence ma-trix MKe?Kf , we project the target-side rule-topicdistribution P (ze|r) to the source-side topic spaceby multiplication as follows:T (P (ze|r)) = P (ze|r) ?MKe?Kf (5)In this way, we get a second distribution for a rulein the source-side topic space, which we called pro-jected target-side topic distribution T (P (ze|r)).Obviously, our projection method allows onetarget-side topic to align to multiple source-side top-ics.
This is different from the one-to-one correspon-dence used by Tam et al, (2007).
From the trainingresult of the correspondence matrix MKe?Kf , wefind that the topic correspondence between sourceand target language is not necessarily one-to-one.Typically, the probability P (z = kf |z = ke) of atarget-side topic mainly distributes on two or threesource-side topics.
Table 1 shows an example ofa target-side topic with its three mainly alignedsource-side topics.5 DecodingWe incorporate our topic similarity model as anew feature into a traditional hiero system (Chi-ang, 2007) under discriminative framework (Ochand Ney, 2002).
Considering there are a source-side rule-topic distribution and a projected target-side rule-topic distribution, we add four features intotal:?
Similarity (P (zf |d), P (zf |r))?
Similarity(P (zf |d), T (P (ze|r)))?
Sensitivity(P (zf |r))?
Sensitivity(T (P (ze|r))To calculate the total score of a derivation on eachfeature listed above during decoding, we sum up thecorrespondent feature score of each applied rule.2The source-side and projected target-side rule-topic distribution are calculated before decoding.During decoding, we first infer the topic distributionP (zf |d) for a given document on source language.When applying a rule, it is straightforward to calcu-late these topic features.
Obviously, the computa-tional cost of these features is rather small.In the topic-specific lexicon translation model,given a source document, it first calculates the topic-specific translation probability by normalizing theentire lexicon translation table, and then adapts thelexical weights of rules correspondingly.
This makesthe decoding slower.
Therefore, comparing with theprevious topic-specific lexicon translation method,our method provides a more efficient way for incor-porating topic model into SMT.6 ExperimentsWe try to answer the following questions by experi-ments:1.
Is our topic similarity model able to improvetranslation quality in terms of BLEU?
Further-more, are source-side and target-side rule-topicdistributions complementary to each other?2Since glue rule and rules of unknown words are not extract-ed from training data, here, we just ignore the calculation of thefour features for them.754System MT06 MT08 Avg SpeedBaseline 30.20 21.93 26.07 12.6TopicLex 30.65 22.29 26.47 3.3SimSrc 30.41 22.69 26.55 11.5SimTgt 30.51 22.39 26.45 11.7SimSrc+SimTgt 30.73 22.69 26.71 11.2Sim+Sen 30.95 22.92 26.94 10.2Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with thetraditional hierarchical system (?Baseline?)
and the topic-specific lexicon translation method (?TopicLex?).
?SimSrc?and ?SimTgt?
denote similarity by source-side and target-side rule-distribution respectively, while ?Sim+Sen?
acti-vates the two similarity and two sensitivity features.
?Avg?
is the average BLEU score on the two test sets.
Scoresmarked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01).2.
Is it helpful to introduce the topic sensitivi-ty model to distinguish topic-insensitive andtopic-sensitive rules?3.
Is it necessary to project topics by one-to-manycorrespondence instead of one-to-one corre-spondence?4.
What is the effect of our method on varioustypes of rules, such as phrase rules and ruleswith non-terminals?6.1 DataWe present our experiments on the NIST Chinese-English translation tasks.
The bilingual training da-ta contains 239K sentence pairs with 6.9M Chinesewords and 9.14M English words, which comes fromthe FBIS portion of LDC data.
There are 10,947documents in the FBIS corpus.
The monolingual da-ta for training English language model includes theXinhua portion of the GIGAWORD corpus, whichcontains 238M English words.
We used the NISTevaluation set of 2005 (MT05) as our developmentset, and sets of MT06/MT08 as test sets.
The num-bers of documents in MT05, MT06, MT08 are 100,79, and 109 respectively.We obtained symmetric word alignments of train-ing data by first running GIZA++ (Och and Ney,2003) in both directions and then applying re-finement rule ?grow-diag-final-and?
(Koehn et al,2003).
The SCFG rules are extracted from thisword-aligned training data.
A 4-gram languagemodel was trained on the monolingual data by theSRILM toolkit (Stolcke, 2002).
Case-insensitiveNIST BLEU (Papineni et al, 2002) was used to mea-sure translation performance.
We used minimum er-ror rate training (Och, 2003) for optimizing the fea-ture weights.For the topic model, we used the open source L-DA tool GibbsLDA++ for estimation and inference.3GibssLDA++ is an implementation of LDA usinggibbs sampling for parameter estimation and infer-ence.
The source-side and target-side topic modelsare estimated from the Chinese part and English partof FBIS corpus respectively.
We set the number oftopic K = 30 for both source-side and target-side,and use the default setting of the tool for training andinference.4 During decoding, we first infer the top-ic distribution of given documents before translationaccording to the topic model trained on Chinese partof FBIS corpus.6.2 Effect of Topic Similarity ModelWe compare our method with two baselines.
In addi-tion to the traditional hiero system, we also comparewith the topic-specific lexicon translation method inZhao and Xing (2007).
The lexicon translation prob-ability is adapted by:p(f |e,DF ) ?
p(e|f,DF )P (f |DF ) (6)=?kp(e|f, z = k)p(f |z = k)p(z = k|DF ) (7)However, we simplify the estimation of p(e|f, z =k) by directly using the word alignment corpus with3http://gibbslda.sourceforge.net/4We determine K by testing {15, 30, 50, 100, 200} in ourpreliminary experiments.
We find that K = 30 produces a s-lightly better performance than other values.755Type Count Src% Tgt%Phrase-rule 3.9M 83.4 84.4Monotone-rule 19.2M 85.3 86.1Reordering-rule 5.7M 85.9 86.8All-rule 28.8M 85.1 86.0Table 3: Percentage of topic-sensitive rules of varioustypes of rule according to source-side (?Src?)
and target-side (?Tgt?)
topic distributions.
Phrase rules are fullylexicalized, while monotone and reordering rules containnonterminals (Section 6.5).topic assignment that is inferred by the GibbsL-DA++.
Despite the simplification of estimation, theimprovement of our implementation is comparablewith the improvement in Zhao et al,(2007).
Given anew document, we need to adapt the lexical transla-tion weights of the rules based on topic model.
Theadapted lexicon translation model is added as a newfeature under the discriminative framework.Table 2 shows the result of our method compar-ing with the traditional system and the topic-lexiconspecific translation method described as above.
Byusing all the features (last line in the table), we im-prove the translation performance over the baselinesystem by 0.87 BLEU point on average.
Our methodalso outperforms the topic-lexicon specific transla-tion method by 0.47 points.
This verifies that topicsimilarity model can improve the translation qualitysignificantly.In order to gain insights into why our model ishelpful, we further investigate how many rules aretopic-sensitive.
As described in Section 3.2, we useentropy to measure the topic sensitivity.
If the en-tropy of a rule is smaller than a certain threshold,then the rule is topic sensitive.
Since documents of-ten focus on some topics, we use the average entropyof document-topic distribution of all training docu-ments as the threshold.
We compare both source-side and target-side distribution shown in Table 3.We find that more than 80 percents of the rules aretopic-sensitive, thus provides us a large space to im-prove the translation by exploiting topics.We also compare these methods in terms of thedecoding speed (words/second).
The baseline trans-lates 12.6 words per second, while the topic-specificlexicon translation method only translates 3.3 word-s in one second.
The overhead of the topic-specificSystem MT06 MT08 AvgBaseline 30.20 21.93 26.07One-to-One 30.27 22.12 26.20One-to-Many 30.51 22.39 26.45Table 4: Effects of one-to-one and one-to-many topic pro-jection.lexicon translation method mainly comes from theadaptation of lexical weights.
It takes 72.8% ofthe time to do the adaptation, despite only lexicalweights of the used rules are adapted.
In contrast,our method has a speed of 10.2 words per second foreach sentence on average, which is three times fasterthan the topic-specific lexicon translation method.Meanwhile, we try to separate the effects ofsource-side topic distribution from the target-sidetopic distribution.
From lines 4-6 of Table 2.
Weclearly find that the two rule-topic distributions im-prove the performance by 0.48 and 0.38 BLEUpoints over the baseline respectively.
It seems thatthe source-side topic model is more helpful.
Fur-thermore, when combine these two distributions, theimprovement is increased to 0.64 points.
This indi-cates that the effects of source-side and target-sidedistributions are complementary.6.3 Effect of Topic Sensitivity ModelAs described in Section 3.2, because the similari-ty features always punish topic-insensitive rules, weintroduce topic sensitivity features as a complemen-t.
In the last line of Table 2, we obtain a fur-ther improvement of 0.23 points, when incorporat-ing topic sensitivity features with topic similarityfeatures.
This suggests that it is necessary to dis-tinguish topic-insensitive and topic-sensitive rules.6.4 One-to-One Vs. One-to-Many TopicProjectionIn Section 4.2, we find that source-side topic andtarget-side topics may not exactly match, hence weuse one-to-many topic correspondence.
Yet anoth-er method is to enforce one-to-one topic projection(Tam et al, 2007).
We achieve one-to-one projectionby aligning a target topic to the source topic with thelargest correspondence probability as calculated inSection 4.2.Table 4 compares the effects of these two method-756System MT06 MT08 AvgBaseline 30.20 21.93 26.07Phrase-rule 30.53 22.29 26.41Monotone-rule 30.72 22.62 26.67Reordering-rule 30.31 22.40 26.36All-rule 30.95 22.92 26.94Table 5: Effect of our topic model on three types of rules.Phrase rules are fully lexicalized, while monotone andreordering rules contain nonterminals.s.
We find that the enforced one-to-one topic methodobtains a slight improvement over the baseline sys-tem, while one-to-many projection achieves a largerimprovement.
This confirms our observation of thenon-one-to-one mapping between source-side andtarget-side topics.6.5 Effect on Various Types of RulesTo get a more detailed analysis of the result, wefurther compare the effect of our method on differ-ent types of rules.
We divide the rules into threetypes: phrase rules, which only contain terminal-s and are the same as the phrase pairs in phrase-based system; monotone rules, which contain non-terminals and produce monotone translations; re-ordering rules, which also contain non-terminals butchange the order of translations.
We define themonotone and reordering rules according to Chianget al, (2008).Table 5 show the results.
We can see that ourmethod achieves improvements on all the three type-s of rules.
Our topic similarity method on mono-tone rule achieves the most improvement which is0.6 BLEU points, while the improvement on reorder-ing rules is the smallest among the three types.
Thisshows that topic information also helps the selec-tions of rules with non-terminals.7 Related WorkIn addition to the topic-specific lexicon transla-tion method mentioned in the previous sections,researchers also explore topic model for machinetranslation in other ways.Foster and Kunh (2007) describe a mixture-modelapproach for SMT adaptation.
They first split atraining corpus into different domains.
Then, theytrain separate models on each domain.
Finally, theycombine a specific domain translation model with ageneral domain translation model depending on var-ious text distances.
One way to calculate the dis-tance is using topic model.Gong et al (2010) introduce topic model for fil-tering topic-mismatched phrase pairs.
They first as-sign a specific topic for the document to be translat-ed.
Similarly, each phrase pair is also assigned withone specific topic.
A phrase pair will be discarded ifits topic mismatches the document topic.Researchers also introduce topic model for cross-lingual language model adaptation (Tam et al, 2007;Ruiz and Federico, 2011).
They use bilingual topicmodel to project latent topic distribution across lan-guages.
Based on the bilingual topic model, they ap-ply the source-side topic weights into the target-sidetopic model, and adapt the n-gram language modelof target side.Our topic similarity model uses the document top-ic information.
From this point, our work is relatedto context-dependent translation (Carpuat and Wu,2007; He et al, 2008; Shen et al, 2009).
Previouswork typically use neighboring words and sentencelevel information, while our work extents the con-text into the document level.8 Conclusion and Future WorkWe have presented a topic similarity model whichincorporates the rule-topic distributions on both thesource and target side into traditional hierarchicalphrase-based system.
Our experimental results showthat our model achieves a better performance withfaster decoding speed than previous work on topic-specific lexicon translation.
This verifies the advan-tage of exploiting topic model at the rule level overthe word level.
Further improvement is achieved bydistinguishing topic-sensitive and topic-insensitiverules using the topic sensitivity model.In the future, we are interesting to find ways toexploit topic model on bilingual data without docu-ment boundaries, thus to enlarge the size of trainingdata.
Furthermore, our training corpus mainly focuson news, it is also interesting to apply our method oncorpus with more diverse topics.
Finally, we hope toapply our method to other translation models, espe-cially syntax-based models.757AcknowledgementThe authors were supported by High-TechnologyR&D Program (863) Project No 2011AA01A207and 2012BAH39B03.
This work was done dur-ing Xinyan Xiao?s internship at I2R.
We would liketo thank Yun Huang, Zhengxian Gong, WenliangChen, Jun lang, Xiangyu Duan, Jun Sun, JinsongSu and the anonymous reviewers for their insightfulcomments.ReferencesNicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In Proc of WMT 2009.David M. Blei and John D. Lafferty.
2007.
A correlatedtopic model of science.
AAS, 1(1):17?35.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
JMLR, 3:993?1022.Marine Carpuat and Dekai Wu.
2007.
Context-dependent phrasal translation lexicons for statisticalmachine translation.
In Proceedings of the MT Sum-mit XI.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proc.
EMNLP 2008.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proc.
of the Second Work-shop on Statistical Machine Translation, pages 128?135, Prague, Czech Republic, June.Zhengxian Gong, Yu Zhang, and Guodong Zhou.
2010.Statistical machine translation based on lda.
In Proc.IUCS 2010, page 286?290, Oct.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexical-ized rule selection.
In Proc.
EMNLP 2008.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proc.
of UAI 1999, pages 289?296.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL 2003.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP2004.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proc.
of EMNLP 2009.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proc.
ACL 2002.Franz Josef Och and Hermann Ney.
2003.
A systemat-ic comparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL 2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
ACL 2002.Nick Ruiz and Marcello Federico.
2011.
Topic adapta-tion for lecture translation through bilingual latent se-mantic models.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, July.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proc.
EMNLP 2009.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Proc.
ICSLP 2002.Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz.
2007.Bilingual lsa-based adaptation for statistical machinetranslation.
Machine Translation, 21(4):187?207.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
InProc.
Coling 2008.Bing Zhao and Eric P. Xing.
2006.
BiTAM: Bilingualtopic admixture models for word alignment.
In Proc.ACL 2006.Bin Zhao and Eric P. Xing.
2007.
HM-BiTAM: Bilingualtopic exploration, word alignment, and translation.
InProc.
NIPS 2007.758
