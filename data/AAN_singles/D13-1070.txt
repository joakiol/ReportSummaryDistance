Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747?757,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA discourse-driven content model for summarising scientific articlesevaluated in a complex question answering taskMaria LiakataUniversity of Warwick/EMBL-EBI, UKM.Liakata@warwick.ac.ukSimon DobnikUniversity of Gothenburg, Swedensimon.dobnik@gu.seShyamasree SahaEMBL-EBI, UKsaha@ebi.ac.ukColin BatchelorRoyal Society of Chemistry, UKbatchelorc@rsc.orgDietrich Rebholz-SchuhmannUniversity of Zurich, Switzerland/EMBL-EBI, UKrebholz@ebi.ac.ukAbstractWe present a method which exploits auto-matically generated scientific discourse an-notations to create a content model for thesummarisation of scientific articles.
Full pa-pers are first automatically annotated using theCoreSC scheme, which captures 11 content-based concepts such as Hypothesis, Result,Conclusion etc at the sentence level.
A contentmodel which follows the sequence of CoreSCcategories observed in abstracts is used to pro-vide the skeleton of the summary, making adistinction between dependent and indepen-dent categories.
Summary creation is alsoguided by the distribution of CoreSC cate-gories found in the full articles, in order toadequately represent the article content.
Fi-nally, we demonstrate the usefulness of thesummaries by evaluating them in a complexquestion answering task.
Results are very en-couraging as summaries of papers from auto-matically obtained CoreSCs enable experts toanswer 66% of complex content-related ques-tions designed on the basis of paper abstracts.The questions were answered with a precisionof 75%, where the upper bound for humansummaries (abstracts) was 95%.1 IntroductionThe publication boom of the last few years, espe-cially in the life sciences, has highlighted the needto facilitate automatic access to the information con-tent of articles.
Researchers, curators, reviewers allneed to process a continuously expanding flow ofarticles whether the purpose is to follow the state ofthe art, curate large knowledge bases or have a goodworking knowledge of their own and related disci-plines to assess progress in research.
While a lotof effort has concentrated on information extractionof particular types of entities and relations from thescientific literature (Cohen and Hersh, 2005; Kimet al 2009; Ananiadou et al 2010; Kim et al2011), with a view to support scientists in obtain-ing relevant information from scientific articles andabstracts, less work has focussed on automaticallycombining such information in the form of a co-hesive summary which preserves the context.
Re-searchers rely to a great extent on author-written ab-stracts, but the latter suffer from a number of prob-lems; they are less structured, vary significantly interms of length, are often not self-contained andhave been written independently of the main doc-ument (Teufel, 2010, p.83).Teufel (2001; 2010), (Teufel and Moens, 2002)identify argumentative zones within scientific arti-cles and use them to create use-targeted extractivesummaries.
Argumentative zones are annotationswhich designate the type of knowledge claim andrhetorical status for a sentence and how these relateto the communicative function of the entire paper.A selection of various combinations of argumenta-tive zones are chosen for the use-targeted extractivesummaries (rhetorical extracts), each of which ful-fills a different role.
For instance, purpose-orientedextracts less than 10 sentences long are generatedcontaining a predetermined number of AIM, SOLU-TION and BACKGROUND zones.
As the emphasisof this approach was the identification of the argu-mentative zones, less attention was given to the sen-tence selection criteria for the extractive summaries.747The sentences chosen for the rhetorical extracts wereeither all sentences of a particular category (in thecase of rare categories) (Teufel and Moens, 2002),selected according to a classifier trained on a rele-vance gold standard (Teufel and Moens, 2002), man-ually or randomly selected (Teufel, 2010, p.60).More recently Contractor et al(2012) have usedautomatically annotated argumentative zones (Guoet al 2011) to guide the creation of extractive sum-maries of scientific articles.
Here argumentativezones are used as features for the summariser, alongwith verbs, tf-idf values and sentence location.
Theyuse a standard approach to summarisation, with a bi-nary classification recognising candidate sentenceswhich are then fed into a clustering mechanism.
Ex-tracts can be created to summarise the entire paper orfocus on specific user-specified aspects.
The num-ber of sentences to include in the summary is pre-specified (either directly or using a compression ra-tio).Our approach also makes use of the scientific dis-course for summarisation purposes.
We use the sci-entific discourse to create a content model for ex-tractive summarisation, with a focus on represent-ing the content of the full paper, while keeping thecohesion of the narrative.
We first automaticallyannotate the articles with a scheme which capturesfine-grained aspects of the content and conceptualstructure of the papers, namely the Core ScientificConcepts (CoreSC) scheme (Liakata et al 2010; Li-akata et al 2012).
The CoreSC scheme is ?uniquelysuited to recovering common types of scientific ar-guments about hypotheses, explanations, and evi-dence?
(White et al 2011), which are not read-ily identifiable by other annotation schemes.
Also,when compared to argumentative zoning and morespecifically its extension for chemistry papers, AZ-II (Teufel et al 2009), it was shown to provide agreater level of detail in terms of categories denot-ing objectives, methods and outcomes whereas AZ-II focusses on the attribution of knowledge claimsand the relation with previous work (Liakata et al2010).We then use the distribution of CoreSC categoriesobserved in abstracts to create a content modelwhich provides a skeleton for extractive summaries.The reasoning behind this is to try to preserve co-hesion within the summaries and we hypothesisethat the sequence of CoreSC categories is a goodproxy for cohesion (see section 3.1).
In creatingthe summary, instantiating the content model, weidentify independent categories and dependent cate-gories, and we argue that in order to preserve the co-hesion of the text the independent categories shouldbe determined first (see section 3.2).
We also pre-serve in the summary the distribution of CoreSC cat-egories found in the corresponding full paper.Finally, we evaluate the extractive summaries ina complex real world question-answering task, inwhich we assess the usefulness of the summaries aswell as to what extent the generated CoreSC sum-maries represent the content of the original arti-cle.
Experts are presented with different types ofsummaries and are asked to answer article-specificquestions on the basis of the summaries (see sec-tion 4.1).
Our results show that automatically gen-erated CoreSC summaries can answer 66% of com-plex questions with 75% precision, outperforminga baseline of microsoft autosummarise summaries(See section 4.2).We have also peformed an intrinsic evaluation ofthe summaries using ROUGE and automatic mea-sures for summary informativeness, such as theJensen-Shannon divergence, yielding positive re-sults (See section 4.2).
However, as such measureshave not yet reached maturity and are harder to in-terpret, we consider the user-based evaluation to bea more reliable measure of summary quality.Code for generating the summaries can be ob-tained by contacting the first author and/or visitinghttp://www.sapientaproject.com/software.2 Related workThe Core Scientific Concepts (CoreSC) Scheme:The CoreSC scheme consists of three layers; the firstlayer corresponds to eleven concepts (Background(BAC), Hypothesis (HYP), Motivation (MOT), Goal(GOA), Object (OBJ), Method (MET), Model(MOD), Experiment (EXP), Observation (OBS),Result (RES) and Conclusion (CON)); the secondlayer corresponds to properties of the concepts (e.g.New/Old) and the third layer provides identifierswhich link instances of the same category.
Liakataet al(2010) created a corpus of 265 full scientificarticles from chemistry and biochemistry annotated748with this scheme and trained classifiers using SVMsand CRFs in (Liakata et al 2012), with an accu-racy of >51% across the 11 concepts.
Their dataand CoreSC classification system are available on-line and can provide a good benchmark for com-parison.
Louis & Nenkova (2012) have successfullyused the CoreSC corpus for evaluating syntax-basedcoherence models, which indicates the strong con-nection between coherence and discourse structure.Summarisation for scientific articles: A lot ofthe work on summarising scientific articles has fo-cussed on citation-based summaries.
Qazvinian &Radev (2008) use sentences from papers citing thearticle to be summarised.
Sentences are clustered to-gether creating a topic, with the combination of clus-ters forming a citation summary network.
Qazvinian& Radev (2010), (Qazvinian et al 2010) also makeuse of citation sentences in other scientific papers tosummarize the contributions of a paper.
The draw-back of citation summaries is that a paper must bealready cited, so this type of summary will not beuseful to a paper reviewer.
Also, citations of articleswill have been influenced by other citations ratherthan the paper itself.Document models for summarisation: Our con-tent model has some similarities with content mod-elling using global sentence ordering (Barzilay andLee, 2004; Chen et al 2009).
In (Barzilay andLee, 2004) unsupervised methods are used to cre-ate HMM topic sequence models for newswire textarticles.
Topics are assigned to texts according tothe content model and extracts of fixed length arecreated by selecting the topics most likely to occurin summaries.
While we use supervised methods toannotate papers with a fixed set of topics (CoreSCs)in scientific papers, our summary content model forextracts shares similar principles such as global or-dering of sentences and non-recurrence.
However,their evaluation involved newspaper articles and ex-tracts which are a lot shorter (15 and 6 sentences,respectively).It is not clear whether unsupervised topic mod-elling such as (Chen et al 2009) can be applied toscientific articles (over 100 sentences long), whichby nature include repetition of topics.
It would beinteresting to make comparisons with summaries us-ing content models learnt from our data automati-cally, following a similar approach to (Sauper et al2010) which learns a content model jointly with aparticular supervised task in web-based documents.3 Extractive Summarisation usingCoreSCsIn this section we describe how we use CoreSC dis-course categories annotated at the sentence level tocreate extractive summaries of full papers, which wesubsequently evaluate in a question answering taskin section 4.To generate summaries we follow classic text ex-traction techniques while making use of a documentcontent model based on CoreSCs.
Our aim is forthe content model to reflect both the distribution ofCoreSCs in the paper as well as the discourse modelof human summaries, as the latter is indicated by thegeneric ordering of CoreSC categories in abstractsencountered in a corpus of 265 annotated full pa-pers (Liakata and Soldatova, 2009; Liakata et al2012).
While we do not consider abstracts to be ade-quate summaries, we at least consider them to be co-herent summaries, which is why the content modelreflects the distribution of CoreSCs in the abstracts.To create our summaries, we employed automat-ically generated CoreSC annotations, which are theoutput of the classifiers described in (Liakata et al2012).
These classifiers assign CoreSC categoriesto sentences on the basis of features local to a sen-tence, such as significant n-grams, verbs and wordtriples, as well as global features such as the posi-tion of the sentence within the document and withina paragraph and section headers.
The following sub-sections give details about the creation of extractivesummaries from CoreSC categories.3.1 A content model for CoreSC extractivesummariesBuilding an extractive summary using a computa-tional model of document structure is an idea sharedby many previous approaches, whether the model ishand-crafted, based on rhetorical elements (McKe-own, 1985; Teufel and Moens, 2002) or rhetoricalrelations (Marcu, 1998b; Marcu, 1998a) or whetherit is a content model, learnt automatically from textas in (Barzilay and Lee, 2004), focussing on the lo-cal content or a combination of the local content andglobal structure (Sauper et al 2010).749Our document content model is primarily basedon the global discourse of the article as provided bythe type and number of CoreSC categories.
How-ever, unlike (Teufel and Moens, 2002), who take afixed number of AZ categories of specific type tocreate rhetorical extracts, the number of categoriesused from each CoreSC category depends on theirdistribution in the original article.
Any and all typesof CoreSC category could potentially appear in asummary, as our summaries are meant to be repre-sentative of the entire content of the paper.
Also, theordering of the categories in the summary is learntto reflect the ordering of categories observed in ab-stracts of papers from the same domain.Our model also caters for local discourse depen-dencies.
For example, the selection of a particu-lar ?Method?
sentence for inclusion in the summaryshould influence the choice of ?Experiment?
sen-tences, which refers to particular experimental pro-cedures performed.
This is not an issue of concernto (Teufel and Moens, 2002), but relates to the no-tion of NUCLEUS and SATELLITE clauses, whichform the foundation of Rhetorical Structure The-ory (Mann and Thompson, 1998), and guides thesummarisation paradigm of (Marcu, 1998a; Marcu,1998b).
However, the difference here is that wedefine a-priori certain categories to be independent(have the property of playing the role of nucleus inthe discourse) and specify their relation with partic-ular types of dependent categories.
Thus, nuclearitybecomes a property of the CoreSC category, whichis indirectly inherited by the sentence.Therefore, when creating the CoreSC contentmodel for summaries we addressed the following is-sues: (i) summary length; (ii) number of sentencesfrom each CoreSC, (iii) the ordering in which sen-tences from each CoreSC category should appearand (iv) the extraction of sentences according to in-dependent and dependent categories.?
Summary length: While the literature (Teufel,2010, p.45) suggests that 20?30% of the originaldocument is required for an adequately informa-tive summary, (Teufel, 2010, p.55) assumes thisis too long for scientific papers.
For this reasonand to allow better comparison between papersof varying lengths, we fixed our summary lengthto 20 sentences.
This is reasonable consideringwe have 11 CoreSCs, any and all of which canappear in both abstracts and full papers.?
Number of sentences from each category: Toreflect the content of the paper, the distributionof the CoreSC categories in the extract followsthe distribution of CoreSCs in the full paper.For each CoreSC we determine the number ofsentences to be selected (n(selected(C))) bymultiplying the ratio of that category in the paperby 20.
A difficulty arises if the ratio of a partic-ular concept in the paper is very low (?
0.05)in which case we prefer to include one sentence.If a particular concept is not at all present in thepaper, the number of selected sentences for thatcategory will be 0.?
Ordering of CoreSC categories in the sum-mary: According to a study of empirical sum-maries (Liddy, 1991), sentences of a particulartextual type appear in a particular order.
Sincepaper abstracts were the closest approximationof human summaries available to us, CoreSCcategory transitions found in abstracts have beenadopted in our content model for extracts.
Thetransitions were derived semi-empirically.
First,we extracted initial, medium and final bi-gramsof categories from paper abstracts together withtransition probabilities.Using this information we manually constructedtransitions of the CoreSC categories that best fitthe observed frequencies and our own intuitions.This gave us the following sequence: MOT >(HYP) > OBJ > GOA > BAC > MOD > MET> EXP > OBS > (HYP) > RES > CON.
HYPappears twice in the sequence as annotators haddistinguished two types of hypotheses, globalhypotheses (stated together with other objec-tives) and hypotheses about particular observa-tions.
The model provides an amalgamated rep-resentation of CoreSC concepts in abstracts.
In-terestingly, our semi-empirically derived modelclosely follows the content model for abstractsdescribed in (Liddy, 1991).
It would be interest-ing to see how this compares to a Markov modelof CoreSC categories learnt from the annotatedabstracts.7503.2 Sentence extraction based on independentand dependent categoriesSentence extraction involves selecting the most rel-evant sentences to include in a summary.
Typically,this entails ranking the sentences according to somemeasure of salience and selecting the top n-bestsentences.
For example, a sentence will be repre-sented by a number of features associated with it,such as whether it contains certain high frequencywords or cue phrases, its location in the document,location in a paragraph (Brandow et al 1995; Ku-piec et al 1995).
Other methods include clusteringbased on sentence similarity and choosing the cen-troids (Erkan and Radev, 2004) or choosing the bestconnected sentences (Mihalcea and Tarau, 2004).When sentences are classified according toCoreSC categories features such as the ones de-scribed above for text extraction are taken intoaccount.
Liakata et al(2012) report that themost salient features for classifying CoreSC cat-egories are overall n-grams, verbs and direct ob-jects whereas other features such as the location ofthe sentence, the neighbouring section headings andwhether a sentence contains citations play an impor-tant role for some of the categories.
Thus, classi-fication into CoreSC categories already provides aselection bias for sentence extraction.As explained in section 3.1, the number ofCoreSC categories in the summaries is determinedaccording to their distribution in the paper and theorder of the categories is specified in the contentmodel.
Salience for sentence extraction in this caseis determined by the need to select the most repre-sentative sentences for a category.
There isn?t muchpoint, for example, in identifying that we need to in-clude a Method sentence (MET) and that this shouldbe followed by an Experiment sentence (EXP), if weare not sure that those are indeed the categories ofthe sentences we are about to select.We therefore rank sentences according to the clas-sifier confidence score (probability) with which theywere assigned a CoreSC category in (Liakata et al2012).
The intuition behind this is that sentenceswith high classifier confidence will be less noisy,high precision cases and more representative of aparticular category.
Indeed, (Liakata et al 2012)report statistical significance for the correlation be-tween high classifier confidence and agreement be-tween manual and automatic classificationHowever, as mentioned in section 3.1, there isinter-dependence between sentences in the text,which is in turn inherited by the categories assignedto them.
For example, the highest ranking METsentence will be related to an Experiment (EXP) orBackground (BAC) sentence, which may not be theones with the highest confidence score in their cate-gory.In order to preserve discourse cohesion it is im-portant to select related sentences from differentcategories.
We resolve this by distinguising theCoreSCs into independent categories, which by def-inition are expected to show nucleus behaviour, anddependent categories.
We also specify the rela-tion between independent and dependent categories.The independent categories include the categorieswith the lowest percentage of sentences in scien-tific articles as reported in (Liakata et al 2012),namely: Motivation (MOT) (1%), Goal (GOA)(1%),Hypothesis (HYP)(2%), Object (OBJ)(3%), Model(MOD)(9%), Conclusion (CON)(9%) and Method(MET)(11%).
Categories whose sentence selec-tion semantically depends on the former are Exper-iment (EXP)(10%), Background (BAC)(19%), Re-sult (RES)(21%) and Observation (OBS)(14%).
Theindependent categories also have higher precisionthan recall, in contrast to the dependent categories.While MET and EXP are almost equally representedin the CoreSC corpus, EXP by definition providesthe detailed steps of an experimental method andthus it is semantically dependent on some MET cat-egory.
More specifically, the dependencies are con-sidered to be as follows: EXP, BAC depend on MET,RES depends on CON and OBS depends on RES(OBS is double-dependent).Sentence extraction is driven by first identifyingthe independent categories based on classifier con-fidence scores and then choosing the correspondingdependent categories on the basis of both related-ness to the independent categories and classifier con-fidence.
We use sentence proximity (defined below)as a measure for relatedness and combine it withclassifier confidence during sentence extraction.The mechanism to select sentences for inclusionin the summary, which considers category depen-dencies, proceeds as follows:751?
For an independent category CatI, order sentences bydecreasing order of confidence score.
The confidencescore is the average confidence score of the SVM andCRF classifiers reported in (Liakata et al 2012) fora sentence.?
For a dependent category Cat, for which we need nsentences, given the selected sentences m from thecorresponding independent category CatI we do thefollowing:?
If m = 0, then treat Cat as independent categoryfor this case.?
Otherwise, for each selected sentence ti in CatI,calculate its proximity score to every sentence cjof the dependent category Cat.
Proximity is de-fined as 1?Distance where Distance is an ab-solute difference in sentence ids between cj andti normalised by the maximum absolute distancefound between all cj and ti pairs.?
The classifier prediction score for each cj is mul-tiplied by the Proximity(cj , ti) score and thesentences are re-ranked according to the newscores, where only the n highest ranking cjs arekept.
The last two steps result in an m?n matrix.?
If m = 1, then the choice for the n sentences forCat is straightforward.?
Otherwise, we pick the n highest ranking cjs,proceeding row-wise.
Thus, the highest rankingcjs for the highest ranking independent sentencesti are given priority and any cj is chosen at mostonce.Once the sentence ids are selected for each inde-pendent and each dependent category we plug theminto the content model.
Sentence order is preservedwithin each CoreSC category.
For example, if twoResult sentences are selected, the order in whichthey appear in the paper will be preserved in thesummary.4 Summary evaluation via questionanswering4.1 Task Description and experimental setupWe evaluate the extractive CoreSC summaries interms of how well they enable 12 chemistry ex-perts/evaluators (with at least a Masters degree inchemistry) to answer complex questions about thepapers.
Our test corpus consists of 28 papers heldout from the ART/CoreSC corpus, roughly 1/9,which were annotated automatically with the SVMand CRF classifiers described in (Liakata et al2012) trained on the remaining 8/9 of the corpus.For each of the 28 papers in the test corpus, we gen-erated CoreSC summaries automatically using themethod described in section 3.
We compare theperformance of the experts on a question answer-ing (Q-A) task when given the CoreSC summariesand two other types of summary, amounting to atotal of three experimental conditions (A,B,C).
Theother two types of summary are the original paperabstracts (summaries A), in the absence of humansummaries, and summaries generated by MicrosoftOffice Word 2007 AutoSummarize (summaries B).Microsoft Office Word 2007 AutoSummarize(MA) is a widely available commercial system withreportedly good results (Garcia-Hernandez et al2009) and performance equivalent to TextRank (Mi-halcea and Tarau, 2004).
MA works by assigning ascore to each word in a sentence depending on itsfrequency in the document and sentences are rankedand extracted according to the combination of scoresof the words they contain.
MA therefore followsclassic lexicalised text extraction techniques, is do-main independent and is completely agnostic of thediscourse.
For the latter reason, we considered MAto be a suitable baseline the comparison with whichwould illustrate the effect of using CoreSC cate-gories on the summary and the merits of having adiscourse based model for summarisation.Neither the paper title nor section headings wereavailable to any of the summarising systems as ourextractive system does not make direct use of themand we were not sure how they would influence MA.To ensure that each evaluator considered only onetype of summary per paper, so as to avoid bias fromprevious stimuli, and to make sure all experts wereexposed to all papers and all types of summary, the12 experts were assigned to four groups (G1-G4)and were allocated 28 summaries each according tothe Latin Square design in Table 1.1.The experimental setup follows the paradigmof (Teufel, 2001).
However, while (Teufel, 2001) de-veloped a Q-A task to evaluate summaries showingthe contribution of a scientific article in relation toprevious work, the purpose of the Q-A task at hand1Initially we had four experimental conditions but one wasdropped, so is not presented in this context752is to show the usefulness of the extracted summariesin answering questions on the paper, and how theycompare to a discourse-agnostic baseline.
In thecase of (Teufel, 2001) the task consists of a fixed setof five questions, the same for all articles tuned par-ticularly to the relation of current and previous work.By contrast, the current Q-A task aims to show howwell the summaries represent the content of the en-tire paper, which means that questions are individ-ual to each paper and required domain knowledge tocreate.Each of the 12 experts answered three content-based questions per summary, where the questionswere individual to each paper.
An example of thequestions and the corresponding answers for a givenpaper can be found below.Example 4.1.1?
Q:What do DNJ imino sugars inhibit the action of?A: They inhibit glycosidases and ceramide glucosyl-transferases.?
Q:What methods do the authors use to study the confor-mation of N-benzyl-DNJ?A: They use resonant two-photon ionization (R2PI),ultraviolet?ultraviolet (UV?UV) hole burning, and in-frared (IR) ion-dip spectroscopies in conjunction withelectronic structure theory calculations.?
Q:What is the conformation of the exocyclic hydrox-ymethyl group?A: The exocyclic hydroxymethyl group is axial to thepiperidine ring (gauche- to the ring nitrogen).As one can see, the questions are complex wh-questions and correspond to answers with multiplecomponents.
Questions were complex, to minimisethe likelihood of correct random answers.
Theywere designed by a senior chemistry expert withknowledge of linguistics, so that they could be an-swered based on the abstracts (A).
For this purpose,the senior expert chose abstracts that were at leastthree sentences long.
Ideally, the questions and an-swers should have been set on the basis of the en-tire paper, but this was not possible given our time-frame for the experiment.The underlying assump-tion is that a good summary should cover most of themain points of the paper.
One of the merits of set-ting the questions on the basis of the abstracts wasthat the answers to be identified were deemed suf-ficiently important to be expressed in the humanlycreated abstract.
However, automatic summariescreated in the way proposed here could potentiallyanswer questions beyond the scope of the abstractand in cases of very short abstracts be much moreinformative.Experts were told that summaries were automati-cally generated with no details about different typesof summary; it is assumed that none of them is com-pletely familiar with the work mentioned in the 28papers.On average, it took experts less than 10 minutes toread a summary and answer the three content-basedquestions.Papers (28)Evaluator groups 1?7 8?14 15?21 22-28G1 A B - CG2 C A B -G3 - C A BG4 B - C ATable 1: Distribution of summaries to evaluators4.2 Results and DiscussionWe compared each evaluator?s answers obtained af-ter reading a summary against the model answersset by the senior expert, the author of the questions,based on the abstract (A) of the corresponding pa-per.
If an evaluator?s answer is identical to a modelanswer, then this counts as ?matched?.For instance in example 4.1.1 above, ?axial tothe piperidine ring?, ?gauche- to the ring nitrogen?and ?The OH6 group is axial (Gauche) to the ringnitrogen?
were all considered correct, fully matchedanswers to the question ?What is the conformationof the exocyclic hydroxymethyl group??.
In thecase of the second question in the same example allof the following were considered correct and fullymatched: ?Resonant two-photon ionization (R2PI),UV/UV hole-burn, and IR ion-dip spectroscopies inconjunction with electronic structure theory calcula-tions?, ?R2PI UV/UV hole-burn IR ion-dip e- struc-ture theory calculations?
and ?a combination of res-onant two-photon ionization (R2PI), UV/UV hole-burn, and IR ion-dip spectroscopies in conjunctionwith electronic structure theory calculations?.If the answer requires listing more than one item(as is the case with questions one and two of ex-ample 4.1.1), all of the items have to be matched.Partially matched answers are counted as ?partiallymatched?.
Non-matching answers can be of two753types.
If an un-matched answer coincided with theanswer the senior expert would have given afterreading that particular summary, then it was markedas ?un-matched:justified?
: Such answers were cor-rect given the particular summary, but are not nec-essarily correct with respect to the paper and donot count as alternative answers.
If the answerwas un-matched and also unjustified given the con-tent of the summary, then it was marked as ?un-matched:unjustified?
.
These are cases of evalua-tor error.
Similarly, cases where the evaluator gave?N/A?
as an answer were marked as ?justified?
or?unjustified?
according to whether the senior expertcould find the answer in the summary or not.
Theresults from marking answers are shown in Table 2.Number of A B CMatched 240 126 135Partially matched 0 4 3Un-matched:justified 0 25 15N/A:justified 0 71 71Un-matched:unjustified 5 11 17N/A:unjustified 7 15 11All answers 252 252 252Table 2: Matches between summary-based answers andmodel answersMicro-AVG Macro-AVGS.
types R P F R P FA 1 0.95 0.98 1 0.95 0.97B 0.64 0.70 0.67 0.64 0.64 0.60C 0.66 0.75 0.70 0.64 0.70 0.65Table 3: Precision, Recall and F-score for answeringquestions using the four types of summary.
A: abstracts,B: autosummarize, C:automatic CoreSC summaries.We report Precision, Recall and F-score (P-R-F)for answering questions given each type of sum-mary (Table 3).
To calculate these we define TP asmatched answers, FN as N/A:justified and FP every-thing else (partially matched + un-matched:justified+ un-matched:unjustified + N/A:unjustified).
Here,the standard definition of recall (TP/(TP+FN))demonstrates how many questions can be answeredusing the summary (summary coverage) and Preci-sion (TP/(TP+FP)) how well the questions are an-swered (summary clarity).We consider the F-measure to be an overall indi-cator of the summary usefulness.
Micro-averagingis obtained by adding all answers from all papers tocalculate TP, FN and FP whereas macro-averagingcalculates P-R-F first per paper and then averagesover all papers.The rankings remain consistent regardless of theaveraging method.
Condition A (abstracts) showsperfect Recall (the evaluators are able to answer allthe questions) whereas Precision is affected by un-justified failed matches (Table 2).
The perfect recallis hardly surprising as the questions are designedon the basis of the abstract but provides a sanitycheck for the experiment.
The precision sets an up-per bound for precision with automatic summaries.Summaries of condition C provide answers to morequestions (Recall) and with greater accuracy (Pre-cision) than summaries B.
When macro-averaging,the Recall score of summaries C is tied with that forsummaries B but Precision is 6% higher.To verify the statistical significance for the dif-ference in precision and recall for summaries B andC respectively, we performed Monte Carlo sampling10000 times, for the populations of answers for sum-maries B and C. During each iteration of sampling,precision and recall were calculated, creating popu-lations of 10000 recalls and 10000 precisions propa-gated to be representative of the original populationof answers.
A t-test performed on the populationof precision and the population of recalls showedstatistical significance at 95% in both cases, withsummaries C having a precision of 5% higher anda recall of 1.4-1.6% higher than summaries B (seeTable 4).
Therefore, we can say that CoreSC sum-maries C are overall better for answering questionsthan summaries B.Comparison between B and C (B-C)precision recallt = -105.90 t = -32.52df = 19959.79 df = 19994.40p-value < 2.2e-16 p-value < 2.2e-16alternative hypothesis: true difference in means 6= 095% confidence interval: 95% confidence interval:-0.051 -0.049 -0.016 -0.014sample estimates: sample estimates:mean of x mean of y mean of x mean of y0.696 0.746 0.639 0.655Table 4: Test for statistical significance betwen sum-maries B (microsoft) and C (CoreSC)The difference in precision between summariesB and C shows the advantage of having a con-754tent model: summaries C are significantly clearer.We had also expected CoreSC summaries to have amuch higher coverage than summaries B, and there-fore significantly higher recall.
However, this dif-ference was less pronounced perhaps because au-tosummarize favours shorter sentences, which aremore likely to be found in the abstracts.
We expectthat a refinement in the sentence selection criterion,which would also take sentence length into account,will help to showcase further the benefits of using aCoreSC-based content model.Analysis using ROUGE showed that while sum-maries C had a slightly higher ROUGE-1 measurethan summaries B (0.75 vs 0.73), with respect to ab-stracts, ROUGE-L was the same for the two (0.70).In table 5 we also report measurements on sum-mary informativeness based on divergence (Kull-back Leibler (KL) divergence and Jensen Shannon(JS) divergence), as in (Louis and Nenkova, 2013).KL divergence is asymmetric and reflects the aver-age number of bits wasted by coding samples of adistribution P using another distribution Q. JS diver-gence is an information-theoretic measure, reflect-ing the average distance of the KL divergence be-tween summary and input (the full paper in our case)from the mean vocabulary distributions.
Comparedto other measures, JS divergence has been foundto produce the best predictions of summary qual-ity (Louis and Nenkova, 2013).
In practice, what JSdivergence tells us is how ?different?/divergent thesummary is from the original paper.
Low divergencescores are indicative of greater overlap between thesummaries and the original paper and are consideredpositive in terms of the summary information con-tent.type KLI-S KLS-I UnJSD SJSDB 1.66 0.70 0.21 0.19C 1.40 0.62 0.18 0.17random 1.61 0.79 0.21 0.19Table 5: Macro-averaged divergence scores for the 28test summaries.
B: Autosummarize, C: CoreSC, random:random summaries each 20 sentences long for each paper.KLI-S: Average Kullback Leibler divergence between in-put and summary.
KLS-I: Kullback Leibler divergencebetween summary and input, since KL divergence is notsymmetric.
UnJSD: Jensen Shannon divergence betweeninput and summary.
No smoothing.
SJSD:A version withsmoothing.One can see the that CoreSC summaries have con-sistently lower divergence (both KL and JS) than mi-crosoft autosummarise summaries and random sum-maries of the same length.
This is a positive out-come but since such automatic measures of sum-mary quality have not yet reached maturity and areharder to interpret, we consider the manual evalua-tion a more reliable indicator of summary informa-tiveness and usefulness.
Note that it is not appropri-ate to use divergence to assess the abstracts as thismeasure is influenced by the length of a text, whichvaries dramatically in the case of abstracts.5 Conclusions and future workWe have shown how a content model based onthe scientific discourse as annotated by the CoreSCscheme can be used to produce extractive sum-maries.
These summaries can be generated as al-ternatives to abstracts.
Since they preserve the dis-tribution of CoreSCs in the paper and are not pro-duced independently of it, as is the case with manyabstracts, they are potentially more representative ofabstracts than the full article.
We have tested the use-fulness CoreSC based summaries in answering com-plex questions relating to the content of scientificpapers.
Extracts from automated CoreSCs are infor-mative, outperform microsoft autosummarise sum-maries, in both intrinsic and extrinsic evaluation, andenable experts to answer 66% of complex questionswith a precision of 75%.In the future we would like to experiment furtherwith refining the sentence selection method so asto consider criteria for local cohesion, such as lex-ical chains.
We would also like to perform com-parisons with automatically induced content mod-els and check their viability for scientific articles.We also would like to perform a human based eval-uation of coherence and explore the full potentialof these summaries as alternatives to author-writtenabstracts.
This work constitutes a very importantstep in producing automatic summaries of scientificpapers and enabling experts to extract informationfrom the papers, a major requirement for resourcecuration, which is dependent on constant reviewingof the literature.755AcknowledgementsThis work has been funded by an Early CareerLeverhulme Trust Fellowship to Dr Liakata and byEMBL-EBI, UK.
The authors would like to thankAnnie Louis, Yufan Guo, Simone Teufel, StephenClark and the anonymous reviewers for their valu-able comments.
We would also like to thank MoAbrahams for the python version of the summarisa-tion code and the cafe summary toolkit.ReferencesS.
Ananiadou, Pyysalo S., and J. Tsujii.
2010.
Eventextraction for systems biology by text mining the liter-ature.
Trends in Biotechnology, 28(7):381?390.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Proceedings of the Main Conference, pages113?120.
Best paper award.R.
Brandow, K. Mitze, and L. Rau.
1995.
Automaticcondensation of electronic publications by sentenceselection.
Information Processing and Management,31:675?685.Harr Chen, S. R. K. Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Content modeling using latentpermutations.
J. Artif.
Int.
Res., 36:129?163, Septem-ber.A.
M. Cohen and W. R. Hersh.
2005.
A survey of currentwork in biomedical text a survey of current work inbiomedical text mining.
Briefings in Bioinformatics,6:57?71.Danish Contractor, Yufan Guo, and Anna Korhonen.2012.
Using argumentative zones for extractive sum-marization of scientific articles.
In COLING, pages663?678.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in text sum-marization.
Journal of Artificial Intelligence Re-search, 22:2004.R.A.
Garcia-Hernandez, Y. Ledeneva, G.M.
Mendoza,A.H.
Dominguez, J. Chavez, A. Gelbukh, and J.L.T.Fabela.
2009.
Comparing commercial tools and state-of-the-art methods for generating text summaries.In Artificial Intelligence, 2009.
MICAI 2009.
EighthMexican International Conference on, pages 92?96,November.Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011.A weakly-supervised approach to argumentative zon-ing of scientific documents.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 273?283.
Association forComputational Linguistics.T.
Kim, J.and Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.2009.
Overview of bionlp?09 shared task on event ex-traction.
In Proceedings of the Workshop on BioNLP:Shared Task, pages 1?9, Boulder, Colorado.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Jun?ichi Tsujii.
2011.Overview of bionlp shared task 2011.
In Proceedingsof BioNLP Shared Task 2011 Workshop, pages 1?6,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.756Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedings ofthe 18th annual international ACM SIGIR conferenceon Research and development in information retrieval,SIGIR ?95, pages 68?73, New York, NY, USA.
ACM.M.
Liakata and L.N.
Soldatova.
2009.
The ART Corpus.Technical report, Aberystwyth University.M.
Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.2010.
Corpora for the conceptualisation and zoning ofscientific papers.
In Proceedings of the 7th Interna-tional Conference on Language Resources and Evalu-ation, Valetta,Malta.M.
Liakata, S. Saha, S. Dobnik, C. Batchelor, andRebholz-Schuhmann D. 2012.
Automatic recognitionof conceptualisation zones in scientific articles andtwo life science applications.
Bioinformatics, 28:991?1000.Elizabeth DuRoss Liddy.
1991.
The discourse-levelstructure of empirical abstracts: an exploratory study.Inf.
Process.
Manage., 27:55?81, February.Annie Louis and Ani Nenkova.
2012.
A coherencemodel based on syntactic patterns.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1157?1168.
Asso-ciation for Computational Linguistics.Annie Louis and Ani Nenkova.
2013.
Automatically as-sessing machine summary content without a gold stan-dard.
Computational Linguistics, 39(2):267?300.W.
C. Mann and S. A. Thompson.
1998.
Rhetoricalstructure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Daniel Marcu.
1998a.
Improving summarizationthrough rhetorical parsing tuning.
In Proceedings ofThe Sixth Workshop on Very Large Corpora, pages206?215, Montreal,Canada.Daniel C. Marcu.
1998b.
The rhetorical parsing,summarization, and generation of natural languagetexts.
Ph.D. thesis, Toronto, Ont., Canada, Canada.AAINQ35238.Kathleen R. McKeown.
1985.
Text generation: usingdiscourse strategies and focus constraints to generatenatural language text.
Cambridge University Press,New York, NY, USA.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing Order into Texts.
In Conference on EmpiricalMethods in Natural Language Processing, Barcelona,Spain.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In Proceedings of the 22nd InternationalConference on Computational Linguistics - Volume 1,COLING ?08, pages 689?696, Morristown, NJ, USA.Association for Computational Linguistics.Vahed Qazvinian and Dragomir R Radev.
2010.
Identi-fying non-explicit citing sentences for citation-basedsummarization.
In Proceedings of the 48th annualmeeting of the association for computational linguis-tics, pages 555?564.
Association for ComputationalLinguistics.Vahed Qazvinian, Dragomir R Radev, and ArzucanO?zgu?r.
2010.
Citation summarization throughkeyphrase extraction.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics,pages 895?903.
Association for Computational Lin-guistics.Christina Sauper, Aria Haghighi, and Regina Barzilay.2010.
Incorporating content structure into text anal-ysis applications.
In EMNLP?10, pages 377?387.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: experiments with relevance andrhetorical status.
Comput.
Linguist., 28:409?445, De-cember.Simone Teufel, Advaith Siddharthan, and Colin Batche-lor.
2009.
Towards discipline-independent argumen-tative zoning: Evidence from chemistry and computa-tional linguistics.
In Proceedings of EMNLP-09, Sin-gapore.Simone Teufel.
2001.
Task-based evaluation of sum-mary quality: Describing relationships between sci-entific papersworkshop ?automatic summarization?,naacl-2001.
In NAACL-01 Workshop ?Automatic TextSummarisation?, Pittsburgh, PA.Simone Teufel.
2010.
The Structure of Scientific Arti-cles: Applications to Citation Indexing and Summa-rization.
CSLI Studies in Computational Linguistics.Center for the Study of Language and Information,Stanford, California.Elizabeth White, K. Bretonnel Cohen, and Larry Hunter.2011.
Hypothesis and evidence extraction from full-text scientific journal articles.
In Proceedings ofBioNLP 2011 Workshop, pages 134?135, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.757
