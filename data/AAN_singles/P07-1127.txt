Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008?1015,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsUser Requirements Analysis for Meeting Information RetrievalBased on Query ElicitationVincenzo PallottaDepartment of Computer ScienceUniversity of FribourgSwitzerlandVincenzo.Pallotta@unifr.chVioleta SeretanLanguage Technology LaboratoryUniversity of GenevaSwitzerlandseretan@lettres.unige.chMarita AilomaaArtificial Intelligence LaboratoryEcole Polytechnique F?d?ralede Lausanne (EPFL), SwitzerlandMarita.Ailomaa@epfl.chAbstractWe present a user requirements study forQuestion Answering on meeting recordsthat assesses the difficulty of users ques-tions in terms of what type of knowledge isrequired in order to provide the correct an-swer.
We grounded our work on the em-pirical analysis of elicited user queries.
Wefound that the majority of elicited queries(around 60%) pertain to argumentativeprocesses and outcomes.
Our analysis alsosuggests that standard keyword-based In-formation Retrieval can only deal success-fully with less than 20% of the queries, andthat it must be complemented with othertypes of metadata and inference.1 IntroductionMeeting records constitute a particularly importantand rich source of information.
Meetings are afrequent and sustained activity, in which multi-party dialogues take place that are goal-orientedand where participants perform a series of actions,usually aimed at reaching a common goal: theyexchange information, raise issues, expressopinions, make suggestions, propose solutions,provide arguments (pro or con), negotiatealternatives, and make decisions.
As outcomes ofthe meeting, agreements on future action items arereached, tasks are assigned, conflicts are solved,etc.
Meeting outcomes have a direct impact on theefficiency of organization and team performance,and the stored and indexed meeting records serveas reference for further processing (Post et al,2004).
They can also be used in future meetings inorder to facilitate the decision-making process byaccessing relevant information from previousmeetings (Cremers et al, 2005), or in order tomake the discussion more focused (Conklin, 2006).Meetings constitute a substantial and importantsource of information that improves corporate or-ganization and performance (Corrall, 1998; Ro-mano and Nunamaker, 2001).
Novel multimediatechniques have been dedicated to meeting record-ing, structuring and content analysis according tothe metadata schema, and finally, to accessing theanalyzed content via browsing, querying or filter-ing (Cremers et al, 2005; Tucker and Whittaker,2004).This paper focuses on debate meetings (Cuginiet al, 1997) because of their particular richness ininformation concerning the decision-making proc-ess.
We consider that the meeting content can beorganized on three levels: (i) factual level (whathappens: events, timeline, actions, dynamics); (ii)thematic level (what is said: topics discussed anddetails); (iii) argumentative level (which/how com-mon goals are reached).The information on the first two levels is ex-plicit information that can be usually retrieved di-rectly by searching the meeting records with ap-propriate IR techniques (i.e., TF-IDF).
The thirdlevel, on the contrary, contains more abstract andtacit information pertaining to how the explicit in-formation contributes to the rationale of the meet-ing, and it is not present as such in raw meetingdata: whether or not the meeting goal was reached,what issues were debated, what proposals weremade, what alternatives were discussed, what ar-guments were brought, what decisions were made,what task were assigned, etc.The motivating scenario is the following: A user1008needs information about a past meeting, either inquality of a participant who wants to recollect adiscussion (since the memories of co-participantsare often inconsistent, cf.
Banerjee et al, 2005), oras a non-participant who missed that meeting.Instead of consulting the entire meeting-relatedinformation, which is usually heterogeneous andscaterred (audio-video recordings, notes, minutes,e-mails, handouts, etc.
), the user asks naturallanguage questions to a query engine whichretrieves relevant information from the meetingrecords.In this paper we assess the users' interest inretrieving argumentative information frommeetings and what kind of knowledge is requiredfor answering users' queries.
Section 2 reviewsprevious user requirements studies for the meetingdomain.
Section 3 describes our user requirementsstudy based on the analysis of elicited user queries,presents its main findings, and discusses theimplications of these findings for the design ofmeeting retrieval systems.
Section 4 concludes thepaper and outlines some directions for future work.2  Argumentative Information in MeetingInformation RetrievalDepending on the meeting browser type1, differentlevels of meeting content become accessible forinformation retrieval.
Audio and video browsersdeal with factual and thematic information, whileartifact browsers might also touch on deliberativeinformation, as long as it is present, for instance, inthe meeting minutes.
In contrast, derived-databrowsers aim to account for the argumentative in-formation which is not explicitly present in themeeting content, but can be inferred from it.
Ifminutes are likely to contain only the most salientdeliberative facts, the derived-data browsers aremuch more useful, in that they offer access to thefull meeting record, and thus to relevant detailsabout the deliberative information sought.2 .1  Importance of Argumentative StructureAs shown by Rosemberg and Silince (1999), track-ing argumentative information from meeting dis-1 (Tucker and Whittaker, 2004) identifies 4 types of meetingbrowsers: audio browsers, video browsers, artifacts browsers(that exploit meeting minutes or other meeting-related docu-ments), and browsers that work with derived data (such asdiscourse and temporal structure information).cussions is of central importance for building pro-ject memories since, in addition to the "strictly fac-tual, technical information", these memories mustalso store relevant information about deci-sion-making processes.
In a business context, theinformation derived from meetings is useful forfuture business processes, as it can explain phe-nomena and past decisions and can support futureactions by mining and assessment (Pallotta et al,2004).
The argumentative structure of meeting dis-cussions, possibly visualized in form of argumen-tation diagrams or maps, can be helpful in meetingbrowsing.
To our knowledge, there are at leastthree meeting browsers that have adopted argu-mentative structure: ARCHIVUS (Lisowska et al,2004b), ViCoDe (Marchand-Maillet and Bruno,2005), and the Twente-AMI JFerret browser(Rienks and Verbree, 2006).2 .2  Query Elicitation StudiesThe users' interest in argumentation dimension ofmeetings has been highlighted by a series of recentstudies that attempted to elicit the potential userquestions about meetings (Lisowska et al, 2004a;Benerjee at al., 2005; Cremers et al, 2005).The study of Lisowska et al (2004a), part of theIM2 research project2, was performed in a simu-lated environment in which users were asked toimagine themselves in a particular role from a se-ries of scenarios.
The participants were both IM2members and non-IM2 members and producedabout 300 retrospective queries on recorded meet-ings.
Although this study has been criticized byPost et al (2004), Cremers et al (2005), and Ban-erjee et al (2005) for being biased, artificial, ob-trusive, and not conforming to strong HCI method-ologies for survey research, it shed light on poten-tial queries and classified them in two broad cate-gories, that seem to correspond to our argumenta-tive/non-argumentative distinction (Lisowska etal., 2004a: 994):?
?elements related to the interaction among par-ticipants: acceptance/rejection, agree-ment/disagreement; proposal, argumentation(for and against); assertions, statements; deci-sions; discussions, debates; reactions; ques-tions; solutions?
;2 http://www.im2.ch1009?
?concepts from the meeting domains: dates,times; documents; meeting index: current, pre-vious, sets; participants; presentations, talks;projects; tasks, responsibilities; topics?.Unfortunately, the study does not provide preciseinformation on the relative proportions of queriesfor the classification proposed, but simply suggeststhat overall more queries belong to the secondcategory, while queries requiring understanding ofthe dialogue structure still comprise a sizeableproportion.The survey conducted by Banerjee et al (2005)concerned instead real, non-simulated interviewsof busy professionals about actual situations, re-lated either to meetings in which they previouslyparticipated, or to meetings they missed.
More thanhalf of the information sought by intervieweesconcerned, in both cases, the argumentative dimen-sion of meetings.For non-missed meetings, 15 out of the 26 in-stances (i.e., 57.7%) concerned argumentative as-pects: what the decision was regarding a topic (7);what task someone was assigned (4); who made aparticular decision (2); what was the participants'reaction to a particular topic (1); what the futureplan is (1).
The other instances (42.3%) relate tothe thematic dimension, i.e., specifics of the dis-cussion on a topic (11).As for missed meetings, the argumentative in-stances were equally represented (18/36): decisionson a topic (7); what task was assigned to inter-viewee (4); whether a particular decision was made(3); what decisions were made (2); reasons for adecision (1); reactions to a topic (1).
The thematicquestions concern topics discussed, announce-ments made, and background of participants.The study also showed that the recovery of in-formation from meeting recordings is significantlyfaster when discourse annotations are available,such as the distinction between discussion, presen-tation, and briefing.Another unobtrusive user requirements studywas performed by Cremers et al (2005) in a "semi-natural setting" related to the design of a meetingbrowser.
The top 5 search interests highlighted bythe 60 survey participants were: decisions made,participants/speakers, topics, agenda items, andarguments for decision.
Of these, the ones shownin italics are argumentative.
In fact, the authorsacknowledge the necessity to include some "func-tional" categories as innovative search options.Interestingly, from the user interface evaluationpresented in their paper, one can indirectly inferhow salient the argumentative information is per-ceived by users: the icons that the authors intendedfor emotions, i.e., for a emotion-based search facil-ity, were actually interpreted by users as referringto people?s opinion: What is person X's opinion?
?positive, negative, neutral.3  User Requirements AnalysisThe existing query elicitation experiments reportedin Section 2 highlighted a series of question typesthat users typically would like to ask about meet-ings.
It also revealed that the information soughtcan be classified into two broad categories: argu-mentative information (about the argumentativeprocess and the outcome of debate meetings), andnon-argumentative information (factual, i.e., aboutthe meeting as a physical event, or thematic, i.e.,about what has been said in terms of topics).The study we present in this section is aimed atassessing how difficult it is to answer the questionsthat users typically ask about a meeting.
Our goalis to provide insights into:?
how many queries can be answered using stan-dard IR techniques on meeting artefacts only(e.g., minutes, written agenda, invitations);?
how many queries can be answered with IR onmeeting recordings;?
what kind of additional information and infer-ence is needed when IR does not apply or it isinsufficient (e.g., information about the par-ticipants and the meeting dynamics, externalinformation about the meeting?s context suchas the relation to a project, semantic interpreta-tion of question terms and references, compu-tation of durations, aggregation of results, etc).Assessing the level of difficulty of a query basedon the two above-mentioned categories might notprovide insightful results, because these would betoo general, thus less interpretable.
Also, the com-plex queries requiring mixed information wouldescape observation because assigned to a too gen-eral class.
We therefore considered it necessary toperform a separate analysis of each query instance,as this provides not only detailed, but also trace-able information.10103 .1  Data: Collecting User QueriesOur analysis is based on a heterogeneous collec-tion of queries for meeting data.
In general, an un-biased queries dataset is difficult to obtain, and thequality of a dataset can vary if the sample is madeof too homogenous subjects (e.g., people belong-ing to the same group as members of the same pro-ject).
In order to cope with this problem, our strat-egy was to use three different datasets collected indifferent settings:?
First, we considered the I M2 dataset  collectedby Lisowska et al (2004a), the only set of userqueries on meetings available to date.
It com-prises 270 questions (shortly described in Sec-tion 2) annotated with a label showing whetheror not the query was produced by an IM2-member.
These queries are introspective andnot related to any particular recorded meeting.?
Second, we cross-validated this dataset with alarge corpus of 294 natural language state-ments about existing meetings records.
Thisdataset, called the B ET observations  (Wellneret al, 2005), was collected by subjects whowere asked to watch several meeting record-ings and to report what the meeting partici-pants appeared to consider interesting.
We useit as a ?validation?
set for the IM2 queries: anIM2 query is considered as ?realistic?
or ?em-pirically grounded?
if there is a BET observa-tion that represents a possible answer to thequery.
For instance, the query Why was theproposal made by X not accepted?
matches theBET observation Denis eliminated Silence ofthe Lambs as it was too violent .?
Finally, we collected a new set of ?real?
queriesby conducting a survey of user requirementson meeting querying in a natural business set-ting.
The survey involved 3 top managers froma company and produced 35 queries.
We calledthis dataset Manager Survey Set  (MS-Set).The queries from the IM2-set (270 queries) and theMS-Set (35 queries) were analyzed by two differ-ent teams of two judges.
Each team discussed eachquery, and classified it along the two main dimen-sions we are interested in:?
query type : the type of meeting content towhich the query pertains;?
query difficulty : the type of information re-quired to provide the answer.3 .2  Query Type AnalysisEach query was assigned exactly one of the follow-ing four possible categories (the one perceived asthe most salient):1. factual: the query pertains to the factual meet-ing content;2. thematic: the query pertains to the thematicmeeting content;3.  process : the query pertains to the argumenta-tive meeting content, more precisely to the ar-gumentative process;4. outcome: the query pertains to the argumenta-tive meeting content, more precisely to theoutcome of the argumentative process.IM 2- s et(s iz e:2 70)MS-S et(s iz e: 3 5) Cate go ryTea m 1  Tea m 2  Tea m 1  Tea m 2Fac tu al 24.
8 %  20.
0 %  20.
0 %The m atic 18.
5 %  45.
6 %  20.
0 %  11.
4 %Proc es s 30.
0 %  32.
6 %  22.
9 %  28.
6 %Outc o me 26.
7 %  21.
8 %  37.
1 %  40.
0 %Proc es s + O utc o me 56.
7 %  54.
4 %  60.
0 %  68.
6 %Table 1.
Query classification according to themeeting content type.Results from this classification task for both querysets are reported in Table 1.
In both sets, theinformation most sought was argumentative: about55% of the IM2-set queries are argumentative(process or outcome).
This invalidates the initialestimation of Lisowska et al (2004a:994) that thenon-argumentative queries prevail, and confirmsthe figures obtained in (Banerjee et al, 2005), ac-cording to which 57.7% of the queries are argu-mentative.
In our real managers survey, we ob-tained even higher percentages for the argumenta-tive queries (60% or 68.6%, depending on the an-notation team).
The argumentative queries are fol-lowed by factual and thematic ones in both querysets, with a slight advantage for factual queries.The inter-annotator agreement for this first clas-sification is reported in Table 2.
The proportion ofqueries on which annotators agree in classifyingthem as argumentative is significantly high.
Weonly report here the agreement results for the indi-vidual argumentative categories (Process, Out-come) and both (Process & Outcome).
There were213 queries (in IM2-set) and 30 queries (in MS-1011set) that were consistently annotated by the twoteams on both categories.
Within this set, a highpercentage of queries were argumentative, that is,they were annotated as either Process or Outcome(label AA in the table).IM 2- s et (s iz e: 27 0) MS-s e t (s iz e: 3 5)  C ate go ry rati o k app a  rati o k app aProc es s 84.
8 %  82.
9 %  88.
6 %  87.
8 %Outc o me 90.
7 %  89.
6 %  91.
4 %  90.
9 %Proc es s &Outc o me 78.
9 %  76.
2 %  85.
7 %  84.
8 %AA 11 7/2 13 =  54.
9 %19/ 30 =63.
3 %Table 2.
Inter-annotator agreement for query-typeclassification.Furthermore, we provided a re-assessment of theproportion of argumentative queries with respect toquery origin for the IM2-set (IM2 members vs.non-IM2 members): non-IM2 members issued30.8% of agreed argumentative queries, a propor-tion that, while smaller compared to that of IM2members (69.2%), is still non-negligible.
This con-trasts with the opinion expressed in (Lisowska etal., 2004a) that argumentative queries are almostexclusively produced by IM2 members.Among the 90 agreed IM2 queries that werecross-validated with the BET-observation set,28.9% were argumentative.
We also noted that theratio of BET statements that contain argumentativeinformation is quite high (66.9%).3 .3  Query Difficulty AnalysisIn order to assess the difficulty in answering aquery, we used the following categories that theannotators could assign to each query, according tothe type of information and techniques they judgednecessary for answering it:1.
Role of I R : states the role of standard3  Informa-tion Retrieval (in combination with Topic Ex-traction4) techniques in answering the query.Possible values:a.
Irrelevant (IR techniques are not appli-cable).
Example: What decisions havebeen made?3  By standard IR we mean techniques based on bag-of-wordsearch and TF-IDF indexing.4 Topic extraction techniques are based on topic shift detec-tion (Galley et al, 2003) and keyword extraction (van der Plaset al, 2004).b.
successful (IR techniques are sufficient).Example: Was the budget approved?c.
insufficient (IR techniques are necessary,but not sufficient alone since they re-quire additional inference and informa-tion, such as argumentative, cross-meeting, external corporate/projectknowledge).
Example: Who rejected theproposal made by X on issue Y?2.
Artefacts : information such as agenda, min-utes of previous meetings, e-mails, invita-tions and other documents related and avail-able before the meeting.
Example: Who wasinvited to the meeting?3.
Recordings : the meeting recordings (audio,visual, transcription).
This is almost alwaystrue, except for queries where Artefacts orMetadata are sufficient, such as What wasthe agenda?,  Who was invited to the meet-ing?
).4 .
Metadata : context knowledge kept in staticmetadata (e.g., speakers, place, time).
Ex-ample: Who were the participants at themeeting?5.
Dialogue Acts & Adjacency Pairs : Example:What was John?s response to my commenton the last meeting?6.
Argumentation : metadata (annotations)about the argumentative structure of themeeting content.
Example: Did  everybodyagree on the decisions, or were there differ-ences of opinion?7.
Semantics : semantic interpretation of termsin the query and reference resolution, in-cluding deictics (e.g., for how long, usually,systematically, criticisms; this, about me, I ).Example: What decisions got made easily ?The term requiring semantic interpretation isunderlined.8.
Inference : inference (deriving informationthat is implicit), calculation, and aggregation(e.g., for ?command?
queries asking for listsof things ?
participants, issues, proposals).Example: What would be required from me?10129.
Multiple meetings : availability of multiplemeeting records.
Example: Who usually at-tends the project meetings?10.
External : related knowledge, not explicitlypresent in the meeting records (e.g., infor-mation about the corporation or the projectsrelated to the meeting).
Example: Did some-body talk about me or about my work?Results of annotation reported on the two querysets are synthesized in Table 3: IR is sufficient foranswering 14.4% of the IM2 queries, and 20% ofthe MS-set queries.
In 50% and 25.7% of the cases,respectively, it simply cannot be applied (irrele-vant).
Finally, IR alone is not enough in 35.6% ofthe queries from the IM2-set, and in 54.3% of theMS-set; it has to be complemented with othertechniques.IM 2- s et MS-s e tIR is : allqu eri es AAallqu eri es AASuff ic ie nt 39/ 27 0 =  14.
4 %1/1 17 =0.8 %7/3 5 =20.
0 %1/1 9 =5.3 %Irrel ev a nt 13 5/2 70 =  50.
0 %55/ 11 7 =47.
0 %9/3 5 =25.
7 %3/1 9 =15.
8 %Ins uf fic i ent  96/ 27 0 =  35.
6 %61/ 11 7 =52.
1 %19/ 35 =54.
3 %15/ 19 =78.
9 %Table 3.
The role of IR (and topic extraction) inanswering users?
queries.If we consider agreed argumentative queries(Section 3.2), IR is effective in an extremely lowpercentage of cases (0.8% for IM2-set and 5.3%for MS-Set).
IR is insufficient in most of the cases(52.1% and 78.9%) and inapplicable in the rest ofthe cases (47% and 15.8%).
Only one argumenta-tive query from each set was judged as being an-swerable with IR alone: What were the decisions tobe made (open questions) regarding the topic t1?When is the NEX T M E E TIN G planned?
(e.g.
tofollow up on action items) .Table 4 shows the number of queries in each setthat require argumentative information in order tobe answered, distributed according to the querytypes.
As expected, no argumentation informationis necessary for answering factual queries, butsome thematic queries do need it, such as Whatwas decided about topic T?
(24% in the IM2-setand 42.9% in the M.S.-set).Overall, the majority of queries in both sets re-quire argumentation information in order to be an-swered (56.3% from IM2 queries, and 65.7% fromMS queries).IM 2- s et, An no ta tio n 1  MS-s e t, A nn ot ati on 1Cate go ry  tot al  Req.
arg.
Rati o Tot alReq.arg.
Rati oFac tu al 67  0  0%  7  0  0%The m atic  50  12  24.
0 %  7  3  42.
9 %Proc es s 81  73  90.
1 %  8  7  87.
5 %Outc o me  72  67  93.
1 %  13  13  10 0%All 27 0  15 2  56.
3 %  35  23  65.
7 %Table 4.
Queries requiring argumentative informa-tion.We finally looked at what kind of information isneeded in those cases where IR is perceived as in-sufficient or irrelevant.
Table 5 lists the most fre-quent combinations of information types requiredfor the IM2-set and the MS-set.3 .4  Summary of FindingsThe analysis of the annotations obtained for the305 queries (35 from the Manager Survey set, and270 from the IM2-set) revealed that:?
The information most sought by users frommeetings is argumentative (i.e., pertains to theargumentative process and its outcome).
Itconstitutes more than half of the total queries,while factual and thematic information aresimilar in proportions (Table 1);?
There was no significant difference in this re-spect between the IM2-set and the MS-set(Table 1);?
The decision as to whether a query is argumen-tative or not is easy to draw, as suggested bythe high inter-annotator agreement shown inTable 2;?
Standard IR and topic extraction techniquesare perceived as insufficient in answering mostof the queries.
Only less than 20% of thewhole query set can be answered with IR, andalmost no argumentative question (Table 3).?
Argumentative information is needed in an-swering the majority of the queries (Table 4);?
When IR alone fails, the information types thatare needed most are (in addition to recordings):Argumentation, Semantics, Inference, andMetadata (Table 5); see Section 3.3 for theirdescription.1013IR a lo ne fa ils IM 2-s etInf orm at io n ty p es IR i ns uff ic ie nt             96 c as es   3 5.6 % IR irr el ev an t         13 5 c as es    50 %Artef ac ts         xRec ord in gs x x x x x x x x x x xMe ta- da ta   x  x   x  x  x xDlg ac ts & A dj .
p airsArgu m en tat io n x x x x x x x x x  xSe ma ntic s x x x x x   x x x x xInf ere nc e x  x x   x x x x x xMu lti pl e me et in gs    x        xEx tern alCas es 15 11 9 8 7 5 4 14 9 8 8 7 5Ra tio (% )  15.
6  11.
5  9.4  8.3  7.3  5.2  4.2  10.
4  6.7  5.9  5.9  5.2  3.7IR a lo ne fa ils MS-s e tInf orm at io n ty p es IR i ns uff ic ie nt     19 c as es   5 4.3 %  IR irr el ev an t   9 c as es   54.
3 %Artef ac ts     x xRec ord in gs x x x xMe ta- da ta     x xDlg ac ts & A dj .
p airsArgu m en tat io n x x x xSe ma ntic s x  x x xInf ere nc e x x  x xMu lti pl e me et in gsEx tern al    xCas es 6 4 2 2 2 2Ra tio (% )  31.
6 21 10.
5 10.
5 22.
2 22.
2Table 5.
Some of the most frequent combinations of information required for answering the queries in theIM2-Set and in the MS-set when IR alone fails.3 .5  DiscussionSearching relevant information through the re-corded meeting dialogues poses important prob-lems when using standard IR indexing techniques(Baeza-Yates and Ribeiro-Nieto, 2000), becauseusers ask different types of queries for which asingle retrieval strategy (e.g., keywords-based) isinsufficient.
This is the case when looking at an-swers that require some sort of entailment, such asinferring that a proposal has been rejected when ameeting participant says Are you kidding?
.Spoken-language information retrieval (Vinci-arelli, 2004) and automatic dialogue-act extractiontechniques (Stolke et al, 2000; Clark and Popescu-Belis, 2004; Ang et al, 2005) have been applied tomeeting recordings and produced good results un-der the assumption that the user is interested inretrieving either topic-based or dialog act-basedinformation.
But this assumption is partially in-validated by our user query elicitation analysis,which showed that such information is only soughtin a relatively small fraction of the users?
queries.A particular problem for these approaches is thatthe topic looked for is usually not a query itself( Was topic T mentioned?)
, but just a parameter inmore structured questions ( What was decidedabout T?
).
Moreover, the relevant participants?contributions (dialog acts) need to be retrieved incombination, not in isolation (The reactions  to theproposal made by X ).4  Conclusion and Future WorkWhile most of the research community has ne-glected the importance of argumentative queries inmeeting information retrieval, we provided evi-dence that this type of queries is actually verycommon.
We quantified the proportion of queriesinvolving the argumentative dimension of themeeting content by performing an in-depth analy-sis of queries collected in two different elicitationsurveys.
The analysis of the annotations obtainedfor the 305 queries (270 from the IM2-set, 35 fromMS-set) was aimed at providing insights into dif-ferent matters: what type of information is typi-cally sought by users from meetings; how difficultit is, and what kind of information and techniquesare needed in order to answer user queries.This work represents an initial step towards abetter understanding of user queries on the meetingdomain.
It could provide useful intuitions about1014how to perform the automatic classification of an-swer types and, more importantly, the automaticextraction of argumentative features and their rela-tions with other components of the query (e.g.,topic, named entities, events).In the future, we intend to better ground our firstempirical findings by i) running the queries againsta real IR system with indexed meeting transcriptsand evaluate the quality of the obtained answers;ii) ask judges to manually rank the difficulty ofeach query, and iii) compare the two rankings.
Wewould also like to see how frequent argumentativequeries are in other domains (such as TV talkshows or political debates) in order to generalizeour results.AcknowledgementsWe wish to thank Martin Rajman and HatemGhorbel for their constant and valuable feedback.This work has been partially supported by theSwiss National Science Foundation NCCR IM2and by the SNSF grant no.
200021-116235.ReferencesJeremy Ang, Yang Liu and Elizabeth Shriberg.
2005.Automatic Dialog Act Segmentation and Classification inMultiparty Meetings.
In Proceedings of IE E E IC A S S P2 0 0 5 , Philadelphia, PA, USA.Ricardo Baeza-Yates and Berthier Ribeiro-Nieto.
2000.Modern Information Retrieval .
Addison Wesley.Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky.2005.
The Necessity of a Meeting Recording and PlaybackSystem, and the Benefit of Topic-Level Annotations toMeeting Browsing.
In Proceedings of INT E R A C T 2 0 0 5 ,Rome, Italy.Alexander Clark and Andrei Popescu-Belis.
2004.
Multi-levelDialogue Act Tags.
In Proceedings of SIG D IA L ' 0 4 , pages163?170.
Cambridge, MA, USA.Jeff Conklin.
2006.
Dialogue Mapping: Building SharedUnderstanding of Wicked Problems .
John Wiley & Sons.Sheila Corrall.
1998.
Knowledge management.
Are we in theknowledge management business?
A R IA D N E : the Webversion,  18.Anita H.M Cremers, Bart Hilhorst and Arnold P.O.SVermeeren.
2005.
?What was discussed by whom, how,when and where?"
personalized browsing of annotatedmultimedia meeting recordings.
In Proceedings of HC I2 0 0 5 , pages 1?10, Edinburgh, UK.John Cugini, Laurie Damianos, Lynette Hirschman, RobynKozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz.1997.
Methodology for evaluation of collaborativesystems.
Technical Report Rev.
3.0, The EvaluationWorking Group of the DARPA Intelligent Collaborationand Visualization Program.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier andHongyan Jing.
2003.
Discourse Segmentation of Multi-Party Conversation.
In Proceedings of AC L 2 0 0 3 , pages562?569, Sapporo, Japan.Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong.2004a.
User Query Analysis for the Specification andEvaluation of a Dialogue Processing and Retrieval System.In Proceedings LR E C 2 0 0 4 , pages 993?996, Lisbon,Portugal.Agnes Lisowska, Martin Rajman and Trung H. Bui.
2004b.ARCHIVUS: A System for Accesssing the Content ofRecorded Multimodal Meetings.
In Proceedings of ML M I2 0 0 4 , Martigny, Switzerland.St?phane Marchand-Maillet and Eric Bruno.
2005.
CollectionGuiding: A new framework for handling large multimediacollections.
In Proceeding of AV IV DiLib05 , Cortona, Italy.Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, AgnesLisowska and St?phane Marchand-Maillet.
2004.
Towardsmeeting information systems: Meeting knowledgemanagement.
In Proceedings of ICE IS 2 0 0 5 , pages 464?469, Porto, Portugal.Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman andHatem Ghorbel.
2004.
Automatic keyword extraction fromspoken text: A comparison between two lexical resources:the EDR and WordNet.
In Proceedings of the LR E C 2 0 0 4 ,pages 2205?2208, Lisbon, Portugal.Wilfried M. Post, Anita H.M. Cremers and Olivier BlansonHenkemans.
2004.
A Research Environment for MeetingBehavior.
In Proceedings of the 3rd Workshop on SocialIntelligence Design,  pages 159?165, University of Twente,Enschede, The Netherlands.Rutger Rienks and Daan Verbree.
2006.
About the Usefulnessand Learnability of Argument?Diagrams from RealDiscussions.
In Proceedings of ML MI 2 0 0 6 , WashingtonDC, USA.Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001.Meeting Analysis: Findings from Research and Practice.
InProceedings of HIC S S-3 4 , Maui, HI, IEEE ComputerSociety.Duska Rosemberg and John A.A. Silince.
1999.
Commonground in computer-supported collaborative argumentation.In Proceedings of the CL S C L 9 9 , Stanford, CA, USA.Andreas Stolcke, Klaus Ries, Noah Coccaro, ElizabethShriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,Rachel Martin, Carol Van Ess-Dykema and Marie Meteer.2000.
Dialog Act Modeling for Automatic Tagging andRecognition of Conversational Speech.
ComputationalLinguistics,  26(3):339?373.Simon Tucker and Steve Whittaker.
2004.
Accessingmultimodal meeting data: systems, problems andpossibilities.
In Proceedings of ML M I 2 0 0 4 , Martigny,Switzerland.Alessandro Vinciarelli.
2004.
Noisy text categorization.
InProceedings of ICP R 2 0 0 4 , Cambridge, UK.Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker.2005.
A Meeting Browser Evaluation Test.
In Proceedingsof CHI 2 0 0 5 , Portand, Oregon, USA.1015
