Assigning Domains to Speech Recognition HypothesesKlaus Ru?ggenmann and Iryna GurevychEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germany{rueggenmann,gurevych}@eml-r.villa-bosch.deAbstractWe present the results of experimentsaimed at assigning domains to speechrecognition hypotheses (SRH).
The meth-ods rely on high-level linguistic repre-sentations of SRHs as sets of ontolog-ical concepts.
We experimented withtwo domain models and evaluated theirperformance against a statistical, word-based model.
Our hand-annotated andtf*idf-based models yielded a precisionof 88,39% and 82,59% respectively, com-pared to 93,14% for the word-based base-line model.
These results are explained interms of our experimental setup.1 MotivationHigh-level linguistic knowledge has been shown tohave the potential of improving the state of the artin automatic speech recognition (ASR).
Such know-ledge can be integrated in the ASR component (Gao,2003; Gao et al, 2003; Stolcke et al, 2000; Sarikayaet al, 2003; Taylor et al, 2000).
Alternatively,it may be included in the processing pipeline at alater stage, namely at the interface between the au-tomatic speech recognizer and the spoken languageunderstanding component (Gurevych et al, 2003a;Gurevych and Porzel, 2003).In any of these cases, it is necessary to provide asystematic account of domain and world knowledge.These types of knowledge have largely been ignoredso far in ASR research.
The reason for this state ofaffairs lies in the fact that the manual construction ofappropriate knowledge sources for broad domains isextremely costly.
Also, easy domain portability isan important requirement for any ASR system.
Theemergence of wide coverage linguistic knowledgebases for multiple languages, such as WordNet (Fell-baum, 1998), FrameNet (Baker et al, 1998; Baker etal., 2003), PropBank (Palmer et al, 2003; Xue et al,2004) is likely to change this situation.Domain recognition, which is the central topic ofthis paper, can be thought of as high-level seman-tic tagging of utterances.
We expect significant im-provements in the performance of the ASR compo-nent of the system if information about the currentdomain of discourse is available.
An obvious intu-ition behind this expectation is that knowing the cur-rent domain of discourse narrows down the searchspace of the speech recognizer.
It also allows torule out incoherent speech recognition hypotheses aswell as those which do not fit in a given domain.Apart from that, there are additional importantreasons for the inclusion of information about thecurrent domain in any spoken language process-ing (SLP) system.
Current SLP systems dealnot only with a single, but with multiple do-mains, e.g., Levin et al (2000), Itou et al (2001),Wahlster et al (2001).
In fact, the development ofmulti-domain systems is one of the new research di-rections in SLP, which makes the issue of automati-cally assigning domains to utterances especially im-portant.
This type of knowledge can be effectivelyutilized at different stages of the spoken languageand multi-domain input processing in the followingways:?
optimizing the performance of the speech rec-ognizer;?
improving the performance of the dialoguemanager, e.g., if a domain change occurred inthe discourse;?
dynamic loading of resources, e.g.
speech rec-ognizer lexicons or dialogue plans, especiallyin mobile environments.Here, we present the results of research directedat automatic assigning of domains to speech recog-nition hypotheses.
In Section 2, we briefly introducethe knowledge sources in our experiments, such asthe ontology, the lexicon and domain models.
Thedata and annotation experiments will be presented inSection 3, followed by the detailed description of thedomain classification algorithms in Section 4.
Sec-tion 5 will give the evaluation results for the linguis-tically motivated conceptual as well as purely statis-tical models.
Conclusions and some future researchdirections can be found in Section 6.2 High-Level Knowledge Sources2.1 Ontology and lexiconCurrent SLP systems often employ multi-domain ontologies representing the relevantworld and discourse knowledge.
The know-ledge encoded in such an ontology can beapplied to a variety of natural language pro-cessing tasks, e.g.
Mahesh and Nirenburg (1995),Flycht-Eriksson (2003).Our ontology models the domains Electronic Pro-gram Guide, Interaction Management, Cinema In-formation, Personal Assistance, Route Planning,Sights, Home Appliances Control and Off Talk.The hierarchically structured ontology consists ofca.
720 concepts and 230 properties specifying rela-tions between concepts.
For example every instanceof the concept Process features the relationshasBeginTime, hasEndTime and hasState.A detailed description of the ontology employed inour experiments is given in Gurevych et al (2003b).Ontological concepts are high-level units.
Theyallow to reduce the amount of information needed torepresent relations existing between individual lex-emes and to effectively incorporate this knowledgeinto automatic language processing.
E.g., there mayexist a large number of movies in a cinema reser-vation system.
All of them will be represented bythe concept Movie, thus allowing to map a varietyof lexical items (instances) to a single unit (concept)describing their meaning and the relations to otherconcepts in a generic way.We did not use the structure of the ontology inan explicit way in the reported experiments.
Theknowledge was used implicitly to come up with aset of ontological concepts needed to represent theuser?s utterance.The high-level domain knowledge represented inthe ontology is linked with the language-specificknowledge through a lexicon.
The lexicon con-tains ca.
3600 entries of lexical items and theirsenses (0 or more), encoded as concepts in theontology.
E.g., the word am is mapped to the onto-logical concepts StaticSpatialProcessas in the utterance I am in New York,SelfIdentificationProcess as in theutterance I am Peter Smith, and NONE, if the lexemehas a grammatical function only, e.g., I am going toread a book.2.2 Domain modelsFor scoring high-level linguistic representations ofutterances we use a domain model.
A domain modelis a two-dimensional matrix DM with the dimen-sions (#d ?
#c), where #d and #c denote theoverall number of domain categories and ontologi-cal concepts, respectively.
This can be formalizedas: DM = (Sdc)d=1,...,#d,c=1,...,#c, where the ma-trix elements Sdc are domain specificity scores ofindividual concepts.We experimented with two different domain mod-els.
The first model DManno was obtained throughdirect annotation of concepts with respect to do-mains as reported in Section 3.2.
The second domainmodel DMtf?idf resulted from statistical analysis ofDataset 1 (described in Section 3.1).
In this case,we computed the term frequency - inverse documentfrequency (tf*idf) score (Salton and Buckley, 1988)of each concept for individual domains.
In the caseof human annotations, we deal with binary values,whereas tf*idf scores range over the interval [0,1].3 Data and Annotation ExperimentsWe performed a number of annotation experiments.The purpose of these experiments was to:?
investigate the reliability of the annotations;?
create a domain model based on human anno-tations;?
produce a training dataset for statistical classi-fiers;?
set a Gold Standard as a test dataset for theevaluation.All annotation experiments were conducted ondata collected in hidden-operator tests followingthe paradigm described in Rapp and Strube (2002).Subjects were asked to verbalize a predefined inten-tion in each of their turns, the system?s reaction wassimulated by a human operator.
We collected ut-terances from 29 subjects in 8 dialogues with thesystem each.
All user turns were recorded in sep-arate audio files.
These audio files were processedby two versions of our dialogue system with differ-ent speech recognition modules.
Data describing ourcorpora is given in Table 1.
The first and the sec-ond system?s runs are referred to as Dataset 1 andDataset 2 respectively.Dataset 1 Dataset 2Number of dialogues 232 95Number of utterances 1479 552Number of SRHs 2.239 1.375Number of coherent SRHs 1511 867Number of incoherent SRHs 728 508Table 1: Descriptive corpus statistics.The corpora obtained from these experimentswere further transformed into a set of annotationfiles, which can be read into GUI-based annotationtools, e.g., MMAX (Mu?ller and Strube, 2003).
Thistool can be adopted for annotating different levels ofinformation, e.g., semantic coherence and domainsof utterances, the best speech recognition hypothe-sis in the N-best list, as well as domains of individualconcepts.
The two annotators were trained with thehelp of an annotation manual.
A reconciled versionof both annotations resulted in the Gold Standard.In the following, we present the results of our anno-tation experiments.3.1 Coherence, domains of SRHs in Dataset 1The first experiment was aimed at annotating thespeech recognition hypotheses (SRH) from Dataset1 w.r.t.
their domains.
This process was two-staged.In the first stage, the annotators labeled randomlymixed SRHs, i.e.
SRHs without discourse context,for their semantic coherence as coherent or incoher-ent.
In the second stage, coherent SRHs were la-beled for their domains, resulting in a corpus of 1511hypotheses labeled for at least one domain category.The numbers for ambiguous domain attributions canbe found in Table 2.
The class distribution is givenin Table 3.Number of domains Annotator 1 Annotator 21 90.06% 87.11%2 6.94% 11.27%3 3.01% 1.28%4 0% 0.35%Table 2: Multiple domain assignments in Dataset 1.Annotator 1 Annotator 2Electr.
Program Guide 14.43% 14.86%Interaction Management 15.56% 15.17%Cinema Information 5.32% 8.7%Personal Assistance 0.31% 0.3%Route Planning 37.05% 36%Sights 12.49% 12.74%Home Appliances Control 14.12% 11.22%Off Talk 0.72% 1.01%Table 3: Class distribution for domain assignments.P(A) P(E) KappaElectr.
Program Guide 0.9743 0.7246 0.9066Interaction Management 0.9836 0.7107 0.9434Cinema Information 0.9661 0.8506 0.7229Personal Assistance 0.9953 0.9930 0.3310Route Planning 0.9777 0.5119 0.9544Sights 0.9731 0.7629 0.8865Home Appliances Control 0.9626 0.7504 0.8501Off Talk 0.9871 0.9780 0.4145Table 4: Kappa coefficient for separate domains.Table 4 presents the Kappa coefficient valuescomputed for individual categories.
P(A) is the per-centage of agreement between annotators.
P(E) isthe percentage we expect them to agree by chance.The annotations are generally considered to be reli-able if K > 0.8.
This is true for all classes exceptthose which occur very rarely on our data.3.2 Domains of ontological conceptsIn the second experiment, ontological concepts wereannotated with zero or more domain categories.1 We1Top-level concepts like Event are typically not domain-specific.
Therefore, they will not be assigned any domains.extracted 231 concepts from the lexicon, which is asubset of ontological concepts relevant for our cor-pus of SRHs.
The annotators were given the tex-tual descriptions of all concepts.
These definitionsare supplied with the ontology.
We computed twokinds of inter-annotator agreement.
In the first case,we calculated the percentage of concepts, for whichthe annotators agreed on all domain categories, re-sulting in ca.
47.62% (CONCabs, see Figure 1).
Inthe second case, the agreement on individual domaindecisions (1848 overall) was computed, ca.
86.85%(CONCindiv, see Figure 1).3.3 Best conceptual representation anddomains of SRHs in Dataset 2As will be evident from Section 4.1, each SRH canbe mapped to a set of possible interpretations, whichare called conceptual representations (CR).
In thisexperiment, the best conceptual representation andthe domains of coherent SRHs from Dataset 2 wereannotated.
As our system operates on the basis ofCR, it is necessary to disambiguate them in a pre-processing step.867 SRHs used in this experiment are mapped to2853 CR, i.e.
on average each SRH is mapped to3.29 CR.
The annotators?
agreement on the task ofdetermining the best CR reached ca.
88.93%.For the task of domain annotation, again, we com-puted the absolute agreement, when the annotatorsagreed on all domains for a given SRH.
This resultedin ca.
92.5% (SRHabs, see Figure 1).
The agree-ment on individual domain decisions (6936 over-all) yielded ca.
98.92% (SRHindiv, see Figure 1).As the Figure 1 suggests, annotating utterances withdomains is an easier task for humans than annotat-ing ontological concepts with the same information.One possible reason for this is that even for an iso-lated SRH of an utterance there is at least some lo-cal context available, which clarifies its high-levelmeaning to some extent.
An isolated concept has nodefining context whatsoever.4 Domain ClassificationIn this section, we present the algorithms employedfor assigning domains to speech recognition hy-potheses.
The system called DOMSCORE performsseveral processing steps, each of which will be de-Figure 1: Agreement in % on domain annotationsfor concepts and SRHs.
Absolute agreement (CON-Cabs, SRHabs) means that annotators agreed onall domains.
Individual agreement (CONCindiv,SRHindiv) refers to identical individual domain de-cisions.scribed separately in the respective subsections.4.1 From SRHs to conceptual representationsSRH is a set of words W = {w1, ..., wn}.
DOM-SCORE operates on high-level representations ofSRHs as conceptual representations (CR).
CR isa set of ontological concepts CR = {c1, ..., cn}.Conceptual representations are obtained from Wthrough the process called word-to-concept map-ping.
In this process, all possible ontological sensescorresponding to individual words in the lexicon arepermutated resulting in a set I of possible interpre-tations I = {CR1, ..., CRn} for each speech recog-nition hypothesis.For example, in our data a user formulated thequery concerning the TV program, as:2(1) UndAndwas fu?rwhichSpielfilmemovieskommencomeheute abendtonightThis utterance resulted in the following SRHs:2All examples are displayed with the German original and aglossed translation.SRH1 Was fu?rWhichSpielfilmemovieskommencomeheute abendtonightSRH2 Was fu?rWhichkommencomeheute abendtonightThe two hypotheses have two conceptual represen-tations each.
This is due to the lexical ambiguityof the word come as either MotionProcess orWatchProcess in German.
Movie in SRH1 ismapped to Broadcast.
As a consequence, thepermutation yields CR1a,1b for SRH1 and CR2a,2bfor SRH2:CR1a: {Broadcast, MotionProcess}CR1b: {Broadcast, WatchProcess}CR2a: {MotionProcess}CR2b: {WatchProcess}In Tables 5 and 6, the domain specificity scoresSdc for all concepts of Example 1 are given.Broadcast Motion WatchElectr.
Program Guide 1 0 1Interaction Management 0 0 0Cinema Information 0 0 1Personal Assistance 0 0 0Route Planning 0 1 1Sights 0 0 1Home Appliances Control 1 0 0Off Talk 0 0 0Table 5: Matrix DManno derived from human anno-tations.Broadcast Motion WatchElectr.
Program Guide 1 0.496 0.744Interaction Management 0 0 0Cinema Information 0.283 0.178 0.043Personal Assistance 0 0 0Route Planning 0 0.689 0.044Sights 0 0.020 0.079Home Appliances Control 0.494 0.027 0.147Off Talk 0 0.238 0.374Table 6: Matrix DMtf?idf derived from the anno-tated corpus.4.2 Domain classification of CRThe domain specificity score S of the conceptualrepresentation CR for the domain d is, then, definedas the average score of all concepts in CR for thisdomain.
For a given domain model DM , this for-mally means:SCR(d) =1nn?i=1Sd,iwhere n is the number of concepts in the respectiveCR.
As each CR is scored for all domains d, theoutput of DOMSCORE is a set of domain scores:SCR = {Sd1 , ..., S#d}where #d is the number of domain categories.Tables 7 and 8 display the results of the domainscoring algorithm for the conceptual representationsof Example 1.SRH1 SRH2CR1a CR1b CR2a CR2bElectr.
Program Guide 0.5 1.0 0 1.0Interaction Management 0 0 0 0Cinema Information 0 0.5 0 1.0Personal Assistance 0 0 0 0Route Planning 0.5 0.5 1.0 1.0Sights 0 0.5 0 1.0Home Appliances Control 0.5 0.5 0 0Off Talk 0 0 0 0Table 7: Domain scores on the basis of DManno.SRH1 SRH2CR1a CR1b CR2a CR2bElectr.
Program Guide 0.748 0.872 0.496 0.744Interaction Management 0 0 0 0Cinema Information 0.231 0.163 0.178 0.043Personal Asssitance 0 0 0 0Route Planning 0.344 0.022 0.689 0.044Sights 0.01 0.04 0.02 0.079Home Appliances Control 0.26 0.32 0.027 0.147Off Talk 0.119 0.187 0.238 0.374Table 8: Domain scores on the basis of DMtf?idf .In the Gold Standard evaluation data, SRH1 wasannotated as the best SRH and attributed the do-main Electronic Program Guide, CR1b was selectedas its best conceptual representation.
As can be seenin the above tables, this CR1b gets the highest do-main score for Electronic Program Guide on the ba-sis of both DManno and DMtf?idf .
Consequently,both domain models attribute this domain to SRH1.SRH2 was not labeled with any domains in theGold Standard, as this hypothesis is an incoherentone and hence cannot be considered to belong toany domain at all.
According to DManno, its rep-resentation CR2a gets a single score 1 for the do-main Route Planning and CR2b gets multiple equalscores.
DOMSCORE interprets a single score as amore reliable indicator for a specific domain thanmultiple equal scores and assigns the domain RoutePlanning to SRH2.
On the basis of DMtf?idf thehighest overall score for CR2a,2b is the one for do-main Electronic Program Guide.
Therefore, themodel will assign this domain to SRH2.4.3 Word2Concept ratioIn previous experiments (Gurevych et al, 2003a),we found that when operating on sets of conceptsas representations of speech recognition hypotheses,the ratio of the number of ontological concepts n ina given CR and the total number of words w in therespective SRH must be accounted for.
This relationis defined by the ratio R = n/w.The idea is to prevent an incoherent SRH contain-ing many function words with zero concept map-pings, represented by a single concept in the ex-treme, from being classified as coherent.
Exper-imental results indicate that the optimal thresholdR should be set to 0.33.
This means that if thereare more than three words corresponding to a singleconcept on average, the SRH is likely to be incoher-ent and should be excluded from processing.DOMSCORE implements this as a post-processingtechnique.
For both conceptual representations ofSRH1 the ratio is R = 1/3, whereas for those ofSRH2, we find R = 1/5.
This value is under thethreshold, which means that SRH2 is considered in-coherent and its domain scores are dropped.
Finally,this results in both models assigning the single do-main Electronic Program Guide as the best one tothe utterance in Example 1.5 Evaluation5.1 Evaluation metricsThe evaluation of the algorithms and domain mod-els presented herein poses a methodological prob-lem.
As stated in Section 3.3, the annotators wereallowed to assign 1 or more domains to an SRH, sothe number of domain categories varies in the GoldStandard data.
The output of DOMSCORE, however,is a set with confidence values for all domains rang-ing from 0 to 1.
To the best of our knowledge, thereexists no evaluation method that allows the straight-forward evaluation of these confidence sets againstthe varying number of binary domain decisions.As a consequence, we restricted the evaluation tothe subset of 758 SRHs unambiguously annotatedfor a single domain in Dataset 2.
For each SRHwe compared the recognized domain of its best CRwith the annotated domain.
This recognized domainis the one that was scored the highest confidence byDOMSCORE.
In this way we measured the precisionon recognizing the best domain of an SRH.
The bestconceptual representation of an SRH had been previ-ously disambiguated by humans as reported in Sec-tion 3.3.
Alternatively, this kind of disambiguationcan be performed automatically, e.g., with the helpof the system presented in Gurevych et al (2003a).The system scores semantic coherence of SRHs,where the best CR is the one with the highest se-mantic coherence.5.2 ResultsWe included two baselines in this evaluation.
As as-signing domains to speech recognition hypothesesis a classification task, the majority class frequencycan serve as a first baseline.
For a second base-line, we trained a statistical classifier employing thek-nearest neighbour method using Dataset 1.
Thisdataset had also been employed to create the tf*idfmodel.
The statistical classifier treated each SRH asa bag of words or bag of concepts labeled with do-main categories.Figure 2: Precision on domain assignments.The results of DOMSCORE employing the hand-annotated and tf*idf domain models as well asthe baseline systems?
performances are displayedin Figure 2.
The diagram shows that all sys-tems clearly outperform the majority class base-line.
The hand-annotated domain model (precision88.39%) outperforms the tf*idf domain model (pre-cision 82.59%).
The model created by humans turnsout to be of higher quality than the automaticallycomputed one.
However, the k-nearest neighbourbaseline with words as features performs better (pre-cision 93.14%) than the other methods employingontological concepts as representations.5.3 DiscussionWe believe that this finding can be explained interms of our experimental setup which favours thestatistical model.
Table 9 gives the absolute fre-quency for all domain categories in the evaluationdata.
As the data implies, three of the possible cate-gories are missing in the data.Number of instancesElectr.
Program Guide 74Interaction Management 85Cinema Information 0Personal Assistance 0Route Planning 385Sights 150Home Appliances Control 64Off Talk 0Table 9: Class distribution in the evaluation dataset.The main reason for our results, however, lies inthe controlled experimental setup of the data col-lection.
Subjects had to verbalize pre-defined in-tentions in 8 scenarios, e.g.
record a specific pro-gram on TV or ask for information regarding a givenhistorical sight.
Naturally, this leads to restrictedman-machine interactions using controlled vocabu-lary.
As a result, there is rather limited lexical vari-ation in the data.
This is unfortunate for illustrat-ing the strengths of high-level ontological represen-tations.In our opinion, the power of ontological represen-tations is just their ability to reduce multiple lexi-cal surface realizations of the same concept to a sin-gle unit, thus representing the meaning of multiplewords in a compact way.
This effect could not beexploited in a due way given the test corpora in theseexperiments.
We expect a better performance ofconcept-based methods as compared to word-basedones in broader domains.An additional important point to consider is theportability of the domain recognition approach.
Sta-tistical models, e.g., tf*idf and k-nearest neighbourrely on substantial amounts of annotated data whenmoving to new domains.
Such data is difficult toobtain and requires expensive human efforts for an-notation.
When the manually created domain modelis employed for the domain classification task, theextension of knowledge sources to a new domainboils down to extending the list of concepts withsome additional ones and annotating them for do-mains.
These new concepts are part of the extensionof the system?s general ontology, which is not cre-ated specifically for domain classification, but em-ployed for many purposes in the system.6 ConclusionsIn this paper, we presented a system which de-termines domains of speech recognition hypothe-ses.
Our approach incorporates high-level semanticknowledge encoded in a domain model of ontologi-cal concepts.
We believe that this type of semanticinformation has the potential to improve the perfor-mance of the automatic speech recognizer, as wellas other components of spoken language processingsystems.Basically, information about the current domainof discourse is a type of contextual knowledge.
Oneof the future challenges will be to find ways ofincluding this high-level semantic knowledge intoSLP systems in the most beneficial way.
It remainsto be studied how to integrate semantic processinginto the architecture, including speech recognitionand discourse processing.An important aspect of the scalability of ourmethods is their dependence on concept-based do-main models.
A natural extension would be to re-place hand-crafted ontological concepts with, e.g.,WordNet concepts.
The structure of WordNet canthen be used to determine high-level domain con-cepts that can replace human domain annotations.One of the evident problems with this approach is,however, the high level of lexical ambiguity of theWordNet concepts.
Apparently, the problem of am-biguity scales up together with the coverage of therespective knowledge source.Another remaining challenge is to define themethodology for the evaluation of methods such asproposed herein.
We have to think about appropri-ate evaluation metrics as well as reference corpora.Following the practices in other NLP fields, such assemantic text analysis (SENSEVAL), message anddocument understanding conferences (MUC/DUC),it is desirable to conduct rigourous large-scale eval-uations.
This should facilitate the progress in study-ing the effects of individual methods and cross-system comparisons.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of COLING-ACL, Montreal, Canada.Collin F. Baker, Charles J. Fillmore, and Beau Cronin.2003.
The structure of the FrameNet database.
Inter-national Journal of Lexicography, 16.3:281?296.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Annika Flycht-Eriksson.
2003.
Representing knowledgeof dialogue, domain, task and user in dialogue systems- how and why?
Electronic Transactions on ArtificialIntelligence, 3:5?32.Yuqing Gao, Bowen Zhou, Zijian Diao, Jeffrey Sorensen,and Michael Picheny.
2003.
MARS: A statisticalsemantic parsing and generation-based multilingualautomatic translation system.
Machine Translation,17(3):185 ?
212.Yuqing Gao.
2003.
Coupling vs. unifying: Modelingtechniques for speech-to-speech translation.
In Pro-ceedings of Eurospeech, pages 365 ?
368, Geneva,Switzerland, 1-4 September.Iryna Gurevych and Robert Porzel.
2003.
Usingknowledge-based scores for identifying best speechrecognition hypotheses.
In Proceedings of ISCA Tu-torial and Research Workshop on Error Handling inSpoken Dialogue Systems, pages 77 ?
81, Chateau-d?Oex-Vaud, Switzerland, 28-31 August.Iryna Gurevych, Rainer Malaka, Robert Porzel, andHans-Peter Zorn.
2003a.
Semantic coherence scoringusing an ontology.
In Proceedings of the HLT-NAACLConference, pages 88?95, 27 May - 1 June.Iryna Gurevych, Robert Porzel, Elena Slinko, Nor-bert Pfleger, Jan Alexandersson, and Stefan Merten.2003b.
Less is more: Using a single knowledge rep-resentation in dialogue systems.
In Proceedings ofthe HLT-NAACL?03 Workshop on Text Meaning, pages14?21, Edmonton, Canada, 31 May.Katunobu Itou, Atsushi Fujii, and Tetsuya Ishikawa.2001.
Language modeling for multi-domain speech-driven text retrieval.
In Proceedings of IEEE Auto-matic Speech Recognition and Understanding Work-shop, December.Lori Levin, Alon Lavie, Monika Woszczyna, DonnaGates, Marsal Gavalda, Detlef Koll, and Alex Waibel.2000.
The JANUS-III translation system: Speech-to-speech translation in multiple domains.
MachineTranslation, 15(1-2):3 ?
25.K.
Mahesh and S. Nirenburg.
1995.
A Situated Ontol-ogy for Practical NLP.
In Workshop on Basic On-tological Issues in Knowledge Sharing, InternationalJoint Conference on Artificial Intelligence (IJCAI-95),Montreal, Canada, 19-20 August.Christoph Mu?ller and Michael Strube.
2003.
Multi-levelannotation in MMAX.
In Proceedings of the 4th SIG-dial Workshop on Discourse and Dialogue, pages 198?207, Sapporo, Japan, 4-5 July.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2003.
The Proposition Bank: An annotated corpus ofsemantic roles.
Submitted to Computational Linguis-tics, December.Stefan Rapp and Michael Strube.
2002.
An iterative datacollection approach for multimodal dialogue systems.In Proceedings of the 3rd International Conference onLanguage Resources and Evaluation, pages 661?665,Las Palmas, Canary Island, Spain, 29-31 May.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation Processing and Management, 24(5):513?523.Ruhi Sarikaya, Yuqing Gao, and Michael Picheny.
2003.Word level confidence measurement using semanticfeatures.
In Proceedings of ICASSP, Hong Kong,April.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.Paul Taylor, Simon King, Steve Isard, and Helen Wright.2000.
Intonation and dialogue context as constraintsfor speech recognition.
Language and Speech, 41(3-4):493?512.Wolfgang Wahlster, Norbert Reithinger, and AnselmBlocher.
2001.
SmartKom: Multimodal communi-cation with a life-like character.
In Proceedings of the7th European Conference on Speech Communicationand Technology, pages 1547?1550.Nianwen Xue, Fei Xia, Fu-dong Chiou, and MarthaPalmer.
2004.
The Penn Chinese Treebank: PhraseStructure Annotation of a Large Corpus.
Natural Lan-guage Engineering, 10(4):1?30, June.
