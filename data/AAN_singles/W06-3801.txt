Workshop on TextGraphs, at HLT-NAACL 2006, pages 1?8,New York City, June 2006. c?2006 Association for Computational LinguisticsA Graphical Framework for Contextual Search and Name Disambiguationin EmailEinat MinkovLanguage Technologies Inst.Carnegie Mellon UniversityPittsburgh, PA 15213einatm@cs.cmu.eduWilliam W. CohenMachine Learning Dept.Carnegie Mellon UniversityPittsburgh, PA 15213wcohen@cs.cmu.eduAndrew Y. NgComputer Science Dept.Stanford UniversityStanford, CA 94305ang@cs.stanford.eduAbstractSimilarity measures for text have histor-ically been an important tool for solvinginformation retrieval problems.
In this pa-per we consider extended similarity met-rics for documents and other objects em-bedded in graphs, facilitated via a lazygraph walk.
We provide a detailed in-stantiation of this framework for emaildata, where content, social networks anda timeline are integrated in a structuralgraph.
The suggested framework is evalu-ated for the task of disambiguating namesin email documents.
We show that rerank-ing schemes based on the graph-walk sim-ilarity measures often outperform base-line methods, and that further improve-ments can be obtained by use of appropri-ate learning methods.1 IntroductionMany tasks in information retrieval can be per-formed by clever application of textual similaritymetrics.
In particular, The canonical IR problem ofad hoc retrieval is often formulated as the task offinding documents ?similar to?
a query.
In modernIR settings, however, documents are usually not iso-lated objects: instead, they are frequently connectedto other objects, via hyperlinks or meta-data.
(Anemail message, for instance, is connected via headerinformation to other emails in the same thread andalso to the recipient?s social network.)
Thus it isimportant to understand how text-based documentsimilarity measures can be extended to documentsembedded in complex structural settings.Our similarity metric is based on a lazy graphwalk, and is closely related to the well-knownPageRank algorithm (Page et al, 1998).
PageRankand its variants are based on a graph walk of infi-nite length with random resets.
In a lazy graph walk,there is a fixed probability of halting the walk at eachstep.
In previous work (Toutanova et al, 2004), lazywalks over graphs were used for estimating worddependency distributions: in this case, the graphwas one constructed especially for this task, and theedges in the graph represented different flavors ofword-to-word similarity.
Other recent papers havealso used walks over graphs for query expansion (Xiet al, 2005; Collins-Thompson and Callan, 2005).In these tasks, the walk propagates similarity to astart node through edges in the graph?incidentallyaccumulating evidence of similarity over multipleconnecting paths.In contrast to this previous work, we considerschemes for propogating similarity across a graphthat naturally models a structured dataset like anemail corpus: entities correspond to objects includ-ing email addresses and dates, (as well as the usualtypes of documents and terms), and edges corre-spond to relations like sent-by.
We view the simi-larity metric as a tool for performing search acrossthis structured dataset, in which related entities thatare not directly similar to a query can be reached viamulti-step graph walk.In this paper, we formulate and evaluate this ex-tended similarity metric.
The principal problem we1consider is disambiguating personal names in email,which we formulate as the task of retrieving the per-son most related to a particular name mention.
Weshow that for this task, the graph-based approach im-proves substantially over plausible baselines.
Afterretrieval, learning can be used to adjust the rankingof retrieved names based on the edges in the pathstraversed to find these names, which leads to an ad-ditional performance improvement.
Name disam-biguation is a particular application of the suggestedgeneral framework, which is also applicable to anyreal-world setting in which structural data is avail-able as well as text.This paper proceeds as follows.
Sections 2 and3 formalize the general framework and its instanti-ation for email.
Section 4 gives a short summaryof the learning approach.
Section 5 includes experi-mental evaluation, describing the corpora and resultsfor the person name disambiguation task.
The paperconcludes with a review of related work, summaryand future directions.2 Email as a GraphA graph G consists of a set of nodes, and a set of la-beled directed edges.
Nodes will be denoted by let-ters like x, y, or z, and we will denote an edge fromx to y with label ` as x `??
y.
Every node x hasa type, denoted T (x), and we will assume that thereare a fixed set of possible types.
We will assume forconvenience that there are no edges from a node toitself (this assumption can be easily relaxed.
)We will use these graphs to represent real-worlddata.
Each node represents some real-world entity,and each edge x `??
y asserts that some binaryrelation `(x, y) holds.
The entity types used hereto represent an email corpus are shown in the left-most column of Table 1.
They include the tradi-tional types in information retrieval systems, namelyfile and term.
In addition, however, they include thetypes person, email-address and date.
These enti-ties are constructed from a collection of email mes-sages in the obvious way?for example, a recipient of?Einat Minkov <einat@cs.cmu.edu>?
indicates theexistence of a person node ?Einat Minkov?
and anemail-address node ?einat@cs.cmu.edu?.
(We as-sume here that person names are unique identifiers.
)The graph edges are directed.
We will assumethat edge labels determine the source and targetnode types: i.e., if x `??
z and w `??
y thenT (w) = T (x) and T (y) = T (z).
However, mul-tiple relations can hold between any particular pairof nodes types: for instance, it could be that x `??
yor x `???
y, where ` 6= `?.
(For instance, an emailmessage x could be sent-from y, or sent-to y.)
Notealso that edges need not denote functional relations:for a given x and `, there may be many distinct nodesy such that x `??
y.
For instance, for a file x, thereare many distinct terms y such that x has-term??
y holds.In representing email, we also create an inverselabel `?1 for each edge label (relation) `.
Note thatthis means that the graph will definitely be cyclic.Table 1 gives the full set of relations used in ouremail represention scheme.3 Graph Similarity3.1 Edge weightsSimilarity between two nodes is defined by a lazywalk process, and a walk on the graph is controlledby a small set of parameters ?.
To walk away froma node x, one first picks an edge label `; then, given`, one picks a node y such that x `??
y.
We assumethat the probability of picking the label ` dependsonly on the type T (x) of the node x, i.e., that theoutgoing probability from node x of following anedge type ` is:Pr(` | x) = Pr(` | Ti) ?
?`,TiLet STi be the set of possible labels for an edge leav-ing a node of type Ti.
We require that the weightsover all outgoing edge types given the source nodetype form a probability distribution, i.e., that?`?STi?`,Ti = 1In this paper, we will assume that once ` is picked,y is chosen uniformly from the set of all y such thatx `??
y.
That is, the weight of an edge of type lconnecting source node x to node y is:Pr(x `??
y | `) = ?`,Ti| y : x `??
y |This assumption could easily be generalized, how-ever: for instance, for the type T (x) = file and2source type edge type target typefile sent-from personsent-from-email email-addresssent-to personsent-to-email email-addressdate-of datehas-subject-term termhas-term termperson sent-from inv.
filesent-to?1 filealias email-addresshas-term termemail-address sent-to-email?1 filesent-from-email?1 filealias-inverse personis-email?1 termterm has-term?1 filehas subject-term?1 fileis-email email-addresshas-term?1 persondate date-of?1 fileTable 1: Graph structure: Node and relation types` = has-term, weights for terms y such that x `??
ymight be distributed according to an appropriate lan-guage model (Croft and Lafferty, 2003).3.2 Graph walksConceptually, the edge weights above define theprobability of moving from a node x to some othernode y.
At each step in a lazy graph walk, thereis also some probability ?
of staying at x. Puttingthese together, and denoting byMxy the probabilityof being at node y at time t + 1 given that one is atx at time t in the walk, we defineMxy ={(1 ?
?
)?` Pr(x`??
y|`) ?
Pr(`|T (x)) x 6= y?
x = yIf we associate nodes with integers, and makeMa matrix indexed by nodes, then a walk of k stepscan then be defined by matrix multiplication: specif-ically, if V0 is some initial probability distributionover nodes, then the distribution after a k-step walkis proportional to Vk = V0Mk.
Larger values of ?increase the weight given to shorter paths betweenx and y.
In the experiments reported here, we con-sider small values of k, and this computation is car-ried out directly using sparse-matrix multiplicationmethods.1 If V0 gives probability 1 to some node x01We have also explored an alternative approach based onsampling; this method scales better but introduces some addi-tional variance into the procedure, which is undesirable for ex-perimentation.and probability 0 to all other nodes, then the valuegiven to y in Vk can be interpreted as a similaritymeasure between x and y.In our framework, a query is an initial distribu-tion Vq over nodes, plus a desired output type Tout ,and the answer is a list of nodes y of type Tout ,ranked by their score in the distribution Vk.
For in-stance, for an ordinary ad hoc document retrievalquery (like ?economic impact of recycling tires?
)would be an appropriate distribution Vq over queryterms, with Tout = file .
Replacing Tout with personwould find the person most related to the query?e.g., an email contact heavily associated with theretread economics.
Replacing Vq with a point dis-tribution over a particular document would find thepeople most closely associated with the given docu-ment.3.3 Relation to TF-IDFIt is interesting to view this framework in compar-ison to more traditional IR methods.
Suppose werestrict ourselves to two types, terms and files, andallow only in-file edges.
Now consider an initialquery distribution Vq which is uniform over the twoterms ?the aardvark?.
A one-step matrix multiplica-tion will result in a distribution V1, which includesfile nodes.
The common term ?the?
will spreadits probability mass into small fractions over manyfile nodes, while the unusual term ?aardvark?
willspread its weight over only a few files: hence theeffect will be similar to use of an IDF weightingscheme.4 LearningAs suggested by the comments above, this graphframework could be used for many types of tasks,and it is unlikely that a single set of parameter val-ues will be best for all tasks.
It is thus important toconsider the problem of learning how to better rankgraph nodes.Previous researchers have described schemes foradjusting the parameters ?
using gradient descent-like methods (Diligenti et al, 2005; Nie et al, 2005).In this paper, we suggest an alternative approach oflearning to re-order an initial ranking.
This rerank-ing approach has been used in the past for meta-search (Cohen et al, 1999) and also several natural-3language related tasks (Collins and Koo, 2005).
Theadvantage of reranking over parameter tuning is thatthe learned classifier can take advantage of ?global?features that are not easily used in walk.Note that node reranking, while can be used asan alternative to weight manipulation, it is betterviewed as a complementary approach, as the tech-niques can be naturally combined by first tuning theparameters ?, and then reranking the result using aclassifier which exploits non-local features.
This hy-brid approach has been used successfully in the paston tasks like parsing (Collins and Koo, 2005).We here give a short overview of the reranking ap-proach, that is described in detail elsewhere (Collinsand Koo, 2005).
The reranking algorithm is pro-vided with a training set containing n examples.
Ex-ample i (for 1 ?
i ?
n) includes a ranked list ofli nodes.
Let wij be the jth node for example i,and let p(wij) be the probability assigned to wij bythe graph walk.
A candidate node wij is representedthrough m features, which are computed by m fea-ture functions f1, .
.
.
, fm.
We will require that thefeatures be binary; this restriction allows a closedform parameter update.
The ranking function fornode x is defined as:F (x, ??)
= ?0L(x) +m?k=1?kfk(x)where L(x) = log(p(x)) and ??
is a vector of real-value parameters.
Given a new test example, the out-put of the model is the given node list re-ranked byF (x, ??
).To learn the parameter weights ?
?, we use a boost-ing method (Collins and Koo, 2005), which min-imizes the following loss function on the trainingdata:ExpLoss(??)
=?ili?j=2e?
(F (xi,1,??
)?F (xi,j ,??
))where xi,1 is, without loss of generality, a correcttarget node.
The weights for the function are learnedwith a boosting-like method, where in each itera-tion the feature fk that has the most impact on theloss function is chosen, and ?k is modified.
Closedform formulas exist for calculating the optimal ad-ditive updates and the impact per feature (Schapireand Singer, 1999).5 EvaluationWe experiment with three separate corpora.The Cspace corpus contains email messages col-lected from a management course conducted atCarnegie Mellon University in 1997 (Minkov etal., 2005).
In this course, MBA students, orga-nized in teams of four to six members, ran simu-lated companies in different market scenarios.
Thecorpus we used here includes the emails of allteams over a period of four days.
The Enron cor-pus is a collection of mail from the Enron cor-pus that has been made available for the researchcommunity (Klimt and Yang, 2004).
Here, weused the saved email of two different users.2 Toeliminate spam and news postings we removedemail files sent from email addresses with suf-fix ?.com?
that are not Enron?s; widely distributedemail files (sent from ?enron.announcement?, to?all.employees@enron.com?
etc.).
Text from for-warded messages, or replied-to messages were alsoremoved from the corpus.Table 2 gives the size of each processed corpus,and the number of nodes in the graph representationof it.
In deriving terms for the graph, terms werePorter-stemmed and stop words were removed.
Theprocessed Enron corpora are available from the firstauthor?s home page.corpus Person setfiles nodes train testCspace 821 6248 26 80Sager-E 1632 9753 11 51Shapiro-R 978 13174 11 49Table 2: Corpora Details5.1 Person Name Disambiguation5.1.1 Task definitionConsider an email message containing a commonname like ?Andrew?.
Ideally an intelligent mailerwould, like the user, understand which person ?An-drew?
refers to, and would rapidly perform tasks likeretrieving Andrew?s prefered email address or homepage.
Resolving the referent of a person name is alsoan important complement to the ability to performnamed entity extraction for tasks like social networkanalysis or studies of social interaction in email.2Specifially, we used the ?all documents?
folder, includingboth incoming and outgoing files.4However, although the referent of the name isunambiguous to the recipient of the email, it canbe non-trivial for an automated system to find outwhich ?Andrew?
is indicated.
Automatically de-termining that ?Andrew?
refers to ?Andrew Y. Ng?and not ?Andrew McCallum?
(for instance) is espe-cially difficult when an informal nickname is used,or when the mentioned person does not appear in theemail header.
As noted above, we model this prob-lem as a search task: based on a name-mention in anemail message m, we formulate query distributionVq, and then retrieve a ranked list of person nodes.5.1.2 Data preparationUnfortunately, building a corpus for evaluatingthis task is non-trivial, because (if trivial cases areeliminated) determining a name?s referent is oftennon-trivial for a human other than the intended re-cipient.
We evaluated this task using three labeleddatasets, as detailed in Table 2.The Cspace corpus has been manually annotatedwith personal names (Minkov et al, 2005).
Addi-tionally, with the corpus, there is a great deal ofinformation available about the composition of theindividual teams, the way the teams interact, andthe full names of the team members.
Using thisextra information it is possible to manually resolvename mentions.
We collected 106 cases in whichsingle-token names were mentioned in the the bodyof a message but did not match any name from theheader.
Instances for which there was not suffi-cient information to determine a unique person en-tity were excluded from the example set.
In additionto names that refer to people that are simply not inthe header, the names in this corpus include peoplethat are in the email header, but cannot be matchedbecause they are referred to using: initials?this iscommonly done in the sign-off to an email; nick-names, including common nicknames (e.g., ?Dave?for ?David?
), unusual nicknames (e.g., ?Kai?
for?Keiko?
); or American names adopted in place ofa foreign name (e.g., ?Jenny?
for ?Qing?
).For Enron, two datasets were generated automat-ically.
We collected name mentions which corre-spond uniquely a names that is in the email ?Cc?header line; then, to simulate a non-trivial matchingtask, we eliminate the collected person name fromthe email header.
We also used a small dictionary of16 common American nicknames to identify nick-names that mapped uniquely to full person nameson the ?Cc?
header line.For each dataset, some examples were picked ran-domly and set aside for learning and evaluation pur-poses.initials nicknames otherCspace 11.3% 54.7% 34.0%Sager-E - 10.2% 89.8%Shapiro-R - 15.0% 85.0%Table 3: Person Name Disambiguation Datasets5.2 Results for person name disambiguation5.2.1 Evaluation detailsAll of the methods applied generate a ranked listof person nodes, and there is exactly one correct an-swer per example.3 Figure 1 gives results4 for twoof the datasets as a function of recall at rank k, upto rank 10.
Table 4 shows the mean average preci-sion (MAP) of the ranked lists as well as accuracy,which we define as the percentage of correct answersat rank 1 (i.e., precision at rank 1.
)5.2.2 Baseline methodTo our knowledge, there are no previously re-ported experiments for this task on email data.
As abaseline, we apply a reasonably sophisticated stringmatching method (Cohen et al, 2003).
Each namemention in question is matched against all of the per-son names in the corpus.
The similarity score be-tween the name term and a person name is calculatedas the maximal Jaro similarity score (Cohen et al,2003) between the term and any single token of thepersonal name (ranging between 0 to 1).
In addition,we incorporate a nickname dictionary5, such that ifthe name term is a known nickname of a name, thesimilarity score of that pair is set to 1.The results are shown in Figure 1 and Table 4.
Ascan be seen, the baseline approach is substantiallyless effective for the more informal Cspace dataset.Recall that the Cspace corpus includes many casessuch as initials, and also nicknames that have noliteral resemblance to the person?s name (section3If a ranking contains a block of items with the same score,a node?s rank is counted as the average rank of the ?block?.4Results refer to test examples only.5The same dictionary that was used for dataset generation.55.1.2), which are not handled well by the string sim-ilarity approach.
For the Enron datasets, the base-line approach perfoms generally better (Table 4).
Inall the corpora there are many ambiguous instances,e.g., common names like ?Dave?
or ?Andy?
thatmatch many people with equal strength.5.2.3 Graph walk methodsWe perform two variants of graph walk, corre-sponding to different methods of forming the querydistribution Vq.
Unless otherwise stated, we will usea uniform weighting of labels?i.e., ?`,T = 1/ST ;?
= 1/2; and a walk of length 2.In the first variant, we concentrate all the prob-ability in the query distribution on the name term.The column labeled term gives the results of thegraph walk from this probability vector.
Intuitively,using this variant, the name term propagates itsweight to the files in which it appears.
Then, weightis propagated to person nodes which co-occur fre-quently with these files.
Note that in our graphscheme there is a direct path between terms to per-son names, so that they recieve weight as well.As can be seen in the results, this leads to veryeffective performance: e.g., it leads to 61.3% vs.41.3% accuracy for the baseline approach on theCSpace dataset.
However, it does not handle am-biguous terms as well as one would like, as the querydoes not include any information of the context inwhich the name occurred: the top-ranked answer forambiguous name terms (e.g., ?Dave?)
will alwaysbe the same person.
To solve this problem, we alsoused a file+term walk, in which the query Vq givesequal weight to the name term node and the file inwhich it appears.We found that adding the file node to Vq providesuseful context for ambiguous instances?e.g., thecorrect ?David?
would in general be ranked higherthan other persons with this same name.
On theother hand, though, adding the file node reducesthe the contribution of the term node.
Although theMAP and accuracy are decreased, file+term has bet-ter performance than term at higher recall levels, ascan be seen in Figure 1.5.2.4 Reranking the output of a walkWe now examine reranking as a technique for im-proving the results.
After some preliminary exper-imentation, we adopted the following types of fea-tures f for a node x.
The set of features are fairlygeneric.
Edge unigram features indicate, for eachedge label `, whether ` was used in reaching x fromVq.
Edge bigram features indicate, for each pair ofedge labels `1, `2, whether `1 and `2 were used (inthat order) in reaching x from Vq.
Top edge bigramfeatures are similar but indicate if `1, `2 were usedin one of the two highest-scoring paths between Vqand x (where the ?score?
of a path is the product ofPr(y `??
z) for all edges in the path.
)We believe that these features could all be com-puted using dynamic programming methods.
Cur-rently, however, we compute features by using amethod we call path unfolding, which is simi-lar to the back-propagation through time algorithm(Haykin, 1994; Diligenti et al, 2005) used in train-ing recurrent neural networks.
Graph unfolding isbased on a backward breadth-first visit of the graph,starting at the target node at time step k, and expand-ing the unfolded paths by one layer per each timestep.
This procedure is more expensive, but offersmore flexibility in choosing alternative features, andwas useful in determining an optimal feature set.In addition, we used for this task some addi-tional problem-specific features.
One feature indi-cates whether the set of paths leading to a node orig-inate from one or two nodes in Vq.
(We conjecturethat in the file+term walk, nodes are connected toboth the source term and file nodes are more rele-vant comparing to nodes that are reached from thefile node or term node only.)
We also form featuresthat indicate whether the given term is a nickname ofthe person name, per the nicknames dictionary; andwhether the Jaro similarity score between the termand the person name is above 0.8.
This informationis similar to that used by the baseline method.The results (for the test set, after training on thetrain set) are shown in Table 4 and (for two represen-tative cases) Figure 1.
In each case the top 10 nodeswere reranked.
Reranking substantially improvesperformance, especially for the file+term walk.
Theaccuracy rate is higher than 75% across all datasets.The features that were assigned the highest weightsby the re-ranker were the literal similarity featuresand the source count feature.600.10.20.30.40.50.60.70.80.910  5  10  15  20CumulativeRateCSPACE00.10.20.30.40.50.60.70.80.910  5  10  15  20  25CumulativeRateRankSHAPIRO-Rbaselinetermterm rerankedfile+termfile+term re-rankedFigure 1: Person name disambiguation results: Re-call at rank k6 Related WorkAs noted above, the similarity measure we use isbased on graph-walk techniques which have beenadopted by many other researchers for several dif-ferent tasks.
In the information retrieval commu-nity, infinite graph walks are prevalent for deter-mining document centrality (e.g., (Page et al, 1998;Diligenti et al, 2005; Kurland and Lee, 2005)).
Arelated venue of research is of spreading activa-tion over semantic or association networks, wherethe underlying idea is to propagate activation fromsource nodes via weighted links through the network(Berger et al, 2004; Salton and Buckley, 1988).The idea of representing structured data as agraph is widespread in the data mining community,which is mostly concerned with relational or semi-structured data.
Recently, the idea of PageRankMAP AccuracyCspaceBaseline 49.0 41.3Graph - term 72.6 61.3Graph - file+term 66.3 48.8Reranking - term 85.6 72.5Reranking - file+term 89.0 83.8Sager-EBaseline 67.5 39.2Graph - term 82.8 66.7Graph - file+term 61.7 41.2Reranking - term 83.2 68.6Reranking - file+term 88.9 80.4Shapiro-RBaseline 60.8 38.8Graph - term 84.1 63.3Graph - file+term 56.5 38.8Reranking - term 87.9 65.3Reranking - file+term 85.5 77.6Table 4: Person Name Disambiguation Resultshas been applied to keyword search in structureddatabases (Balmin et al, 2004).
Analysis of inter-object relationships has been suggested for entitydisambiguation for entities in a graph (Kalashnikovet al, 2005), where edges are unlabelled.
It has beensuggested to model similarity between objects in re-lational data in terms of structural-context similarity(Jeh and Widom, 2002).We propose the use of learned re-ranking schemesto improve performance of a lazy graph walk.Earlier authors have considered instead using hill-climbing approaches to adjust the parameters of agraph-walk (Diligenti et al, 2005).
We have notcompared directly with such approaches; prelimi-nary experiments suggest that the performance gainof such methods is limited, due to their inability toexploit the global features we used here6.
Relatedresearch explores random walks for semi supervisedlearning (Zhu et al, 2003; Zhou et al, 2005).The task of person disambiguation has been stud-ied in the field of social networks (e.g., (Malin etal., 2005)).
In particular, it has been suggested toperform name disambiguation in email using traf-fic information, as derived from the email headers(Diehl et al, 2006).
Our approach differs in that itallows integration of email content and a timeline inaddition to social network information in a unified6For instance, re-ranking using a set of simple locally-computable features only modestly improved performance ofthe ?random?
weight set for the CSpace threading task.7framework.
In addition, we incorporate learning totune the system parameters automatically.7 ConclusionWe have presented a scheme for representing a cor-pus of email messages with a graph of typed entities,and an extension of the traditional notions of docu-ment similarity to documents embedded in a graph.Using a boosting-based learning scheme to rerankoutputs based on graph-walk related, as well as otherdomain-specific, features provides an additional per-formance improvement.
The final results are quitestrong: for the explored name disambiguation task,the method yields MAP scores in the mid-to-upper80?s.
The person name identification task illustratesa key advantage of our approach?that context canbe easily incorporated in entity disambiguation.In future work, we plan to further explore thescalability of the approach, and also ways of inte-grating this approach with language-modeling ap-proaches for document representation and retrieval.An open question with regard to contextual (multi-source) graph walk in this framework is whether it ispossible to further focus probability mass on nodesthat are reached from multiple source nodes.
Thismay prove beneficial for complex queries.ReferencesAndrey Balmin, Vagelis Hristidis, and Yannis Papakonstanti-nou.
2004.
ObjectRank: Authority-based keyword search indatabases.
In VLDB.Helmut Berger, Michael Dittenbach, and Dieter Merkl.
2004.An adaptive information retrieval system.
based on associa-tive networks.
In APCCM.William W. Cohen, Robert E. Schapire, and Yoram Singer.1999.
Learning to order things.
Journal of Artificial Intelli-gence Research (JAIR), 10:243?270.William W. Cohen, Pradeep Ravikumar, and Stephen Fienberg.2003.
A comparison of string distance metrics for name-matching tasks.
In IIWEB.Michael Collins and Terry Koo.
2005.
Discriminative rerank-ing for natural language parsing.
Computational Linguistics,31(1):25?69.Kevyn Collins-Thompson and Jamie Callan.
2005.
Query ex-pansion using random walk models.
In CIKM.W.
Bruce Croft and John Lafferty.
2003.
Language Modelingfor Information Retrieval.
Springer.Christopher P. Diehl, Lise Getoor, and Galileo Namata.
2006.Name reference resolution in organizational email archives.In SIAM.Michelangelo Diligenti, Marco Gori, and Marco Maggini.2005.
Learning web page scores by error back-propagation.In IJCAI.Simon Haykin.
1994.
Neural Networks.
Macmillan CollegePublishing Company.Glen Jeh and Jennifer Widom.
2002.
Simrank: A measure ofstructural-context similarity.
In SIGKDD.Dmitri Kalashnikov, Sharad Mehrotra, and Zhaoqi Chen.
2005.Exploiting relationship for domain independent data clean-ing.
In SIAM.Brown Klimt and Yiming Yang.
2004.
The enron corpus: Anew dataset for email classification research.
In ECML.Oren Kurland and Lillian Lee.
2005.
Pagerank without hyper-links: Structural re-ranking using links induced by languagemodels.
In SIGIR.Bradely Malin, Edoardo M. Airoldi, and Kathleen M. Carley.2005.
A social network analysis model for name disam-biguation in lists.
Journal of Computational and Mathemat-ical Organization Theory, 11(2).Einat Minkov, Richard Wang, and William Cohen.
2005.
Ex-tracting personal names from emails: Applying named entityrecognition to informal text.
In HLT-EMNLP.Zaiqing Nie, Yuanzhi Zhang, Ji-Rong Wen, and Wei-Ying Ma.2005.
Object-level ranking: Bringing order to web objects.In WWW.Larry Page, Sergey Brin, R. Motwani, and T. Winograd.
1998.The pagerank citation ranking: Bringing order to the web.
InTechnical Report, Computer Science department, StanfordUniversity.Gerard Salton and Chris Buckley.
1988.
On the use of spread-ing activation methods in automatic information retrieval.
InSIGIR.Robert E. Schapire and Yoram Singer.
1999.
Improved boost-ing algorithms using confidence-rated predictions.
MachineLearning, 37(3):297?336.Kristina Toutanova, Christopher D. Manning, and Andrew Y.Ng.
2004.
Learning random walk models for inducing worddependency distributions.
In ICML.Wensi Xi, Edward Allan Fox, Weiguo Patrick Fan, BenyuZhang, Zheng Chen, Jun Yan, and Dong Zhuang.
2005.Simfusion: Measuring similarity using unified relationshipmatrix.
In SIGIR.Dengyong Zhou, Bernhard Scholkopf, and Thomas Hofmann.2005.
Semi-supervised learning on directed graphs.
InNIPS.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003.Semi-supervised learning using gaussian fields and harmonicfunctions.
In ICML.8
