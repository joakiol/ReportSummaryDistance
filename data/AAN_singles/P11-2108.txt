Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 614?619,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsContrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback forRapportSiwei WangDepartment of PsychologyUniversity of ChicagoChicago, IL 60637 USAsiweiw@cs.uchicago.eduGina-Anne LevowDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195 USAlevow@uw.eduAbstractVerbal feedback is an important informationsource in establishing interactional rapport.However, predicting verbal feedback acrosslanguages is challenging due to language-specific differences, inter-speaker variation,and the relative sparseness and optionality ofverbal feedback.
In this paper, we employ anapproach combining classifier weighting andSMOTE algorithm oversampling to improveverbal feedback prediction in Arabic, English,and Spanish dyadic conversations.
This ap-proach improves the prediction of verbal feed-back, up to 6-fold, while maintaining a highoverall accuracy.
Analyzing highly weightedfeatures highlights widespread use of pitch,with more varied use of intensity and duration.1 IntroductionCulture-specific aspects of speech and nonverbal be-havior enable creation and maintenance of a sense ofrapport.
Rapport is important because it is known toenhance goal-directed interactions and also to pro-mote learning.
Previous work has identified cross-cultural differences in a variety of behaviors, forexample, nodding (Maynard, 1990), facial expres-sion (Matsumoto et al, 2005), gaze (Watson, 1970),cues to vocal back-channel (Ward and Tsukuhara,2000; Ward and Al Bayyari, 2007; Rivera andWard, 2007), nonverbal back-channel (Bertrand etal., 2007)), and coverbal gesturing (Kendon, 2004).Here we focus on the automatic prediction of lis-tener verbal feedback in dyadic unrehearsed story-telling to elucidate the similarities and differencesin three language/cultural groups: Iraqi Arabic-,Mexican Spanish-, and American English-speakingcultures.
(Tickle-Degnen and Rosenthal, 1990)identified coordination, along with positive emo-tion and mutual attention, as a key element of in-teractional rapport.
In the verbal channel, this co-ordination manifests in the timing of contributionsfrom the conversational participants, through turn-taking and back-channels.
(Duncan, 1972) pro-posed an analysis of turn-taking as rule-governed,supported by a range of prosodic and non-verbalcues.
Several computational approaches have inves-tigated prosodic and verbal cues to these phenom-ena.
(Shriberg et al, 2001) found that prosodic cuescould aid in the identification of jump-in points inmulti-party meetings.
(Cathcart et al, 2003) em-ployed features such as pause duration and part-of-speech (POS) tag sequences for back-channel pre-diction.
(Gravano and Hirschberg, 2009) investi-gated back-channel-inviting cues in task-oriented di-alog, identifying increases in pitch and intensity aswell as certain POS patterns as key contributors.
Inmulti-lingual comparisons, (Ward and Tsukuhara,2000; Ward and Al Bayyari, 2007; Rivera and Ward,2007) found pitch patterns, including periods of lowpitch or drops in pitch, to be associated with elic-iting back-channels across Japanese, English, Ara-bic, and Spanish.
(Herrera et al, 2010) collected acorpus of multi-party interactions among AmericanEnglish, Mexican Spanish, and Arabic speakers toinvestigate cross-cultural differences in proxemics,gaze, and turn-taking.
(Levow et al, 2010) identi-fied contrasts in narrative length and rate of verbalfeedback in recordings of American English-, Mexi-614can Spanish-, and Iraqi Arabic-speaking dyads.
Thiswork also identified reductions in pitch and intensityassociated with instances of verbal feedback as com-mon, but not uniform, across these groups.2 Multi-modal Rapport CorpusTo enable a more controlled comparison of listenerbehavior, we collected a multi-modal dyadic corpusof unrehearsed story-telling.
We audio- and video-recorded pairs of individuals who were close ac-quaintances or family members with, we assumed,well-established rapport.
One participant viewed asix minute film, the ?Pear Film?
(Chafe, 1975), de-veloped for language-independent elicitation.
In therole of Speaker, this participant then related the storyto the active and engaged Listener, who understoodthat they would need to retell the story themselveslater.
We have collected 114 elicitations: 45 Arabic,32 Mexican Spanish, and 37 American English.All recordings have been fully transcribed andtime-aligned to the audio using a semi-automatedprocedure.
We convert an initial manual coarse tran-scription at the phrase level to a full word and phonealignment using CUSonic (Pellom et al, 2001), ap-plying its language porting functionality to Spanishand Arabic.
In addition, word and phrase level En-glish glosses were manually created for the Span-ish and Arabic data.
Manual annotation of a broadrange of nonverbal cues, including gaze, blink, headnod and tilt, fidget, and coverbal gestures, is under-way.
For the experiments presented in the remainderof this paper, we employ a set of 45 vetted dyads, 15in each language.Analysis of cross-cultural differences in narrativelength, rate of listener verbal contributions, and theuse of pitch and intensity in eliciting listener vocal-izations appears in (Levow et al, 2010).
That workfound that the American English-speaking dyadsproduced significantly longer narratives than theother language/cultural groups, while Arabic listen-ers provided a significantly higher rate of verbal con-tributions than those in the other groups.
Finally, allthree groups exhibited significantly lower speakerpitch preceding listener verbal feedback than inother contexts, while only English and Spanish ex-hibited significant reductions in intensity.
The cur-rent paper aims to extend and enhance these find-ings by exploring automatic recognition of speakerprosodic contexts associated with listener verbalfeedback.3 Challenges in Predicting VerbalFeedbackPredicting verbal feedback in dyadic rapport in di-verse language/cultural groups presents a number ofchallenges.
In addition to the cross-linguistic, cross-cultural differences which are the focus of our study,it is also clear that there are substantial inter-speakerdifferences in verbal feedback, both in frequencyand, we expect, in signalling.
Furthermore, whilethe rate of verbal feedback differs across languageand speaker, it is, overall, a relatively infrequentphenomenon, occurring in as little as zero percentof pausal intervals for some dyads and only at an av-erage of 13-30% of pausal intervals across the threelanguages.
As a result, the substantial class imbal-ance and relative sparsity of listener verbal feedbackpresent challenges for data-driven machine learn-ing methods.
Finally, as prior researchers have ob-served, provision of verbal feedback can be viewedas optional.
The presence of feedback, we assume,indicates the presence of a suitable context; the ab-sence of feedback, however, does not guarantee thatfeedback would have been inappropriate, only thatthe conversant did not provide it.We address each of these issues in our experi-mental process.
We employ a leave-one-dyad-outcross-validation framework that allows us to deter-mine overall accuracy while highlighting the differ-ent characteristics of the dyads.
We employ andevaluate both an oversampling technique (Chawlaet al, 2002) and class weighting to compensate forclass imbalance.
Finally, we tune our classificationfor the recognition of the feedback class.4 Experimental SettingWe define a Speaker pausal region as an interval inthe Speaker?s channel annotated with a contiguousspan of silence and/or non-speech sounds.
TheseSpeaker pausal regions are tagged as ?Feedback(FB)?
if the participant in the Listener role initi-ates verbal feedback during that interval and as ?NoFeedback (NoFB)?
if the Listener does not.
We aimto characterize and automatically classify each such615Arabic English Spanish0.30 (0.21) 0.152 (0.10) 0.136 (0.12)Table 1: Mean and standard deviation of proportion ofpausal regions associated with listener verbal feedbackregion.
We group the dyads by language/culturalgroup to contrast the prosodic characteristics of thespeech that elicit listener feedback and to assess theeffectiveness of these prosodic cues for classifica-tion.
The proportion of regions with listener feed-back for each language appears in Table 1.4.1 Feature ExtractionFor each Speaker pausal region, we extract fea-tures from the Speaker?s words immediately preced-ing and following the non-speech interval, as wellas computing differences between some of thesemeasures.
We extract a set of 39 prosodic fea-tures motivated by (Shriberg et al, 2001), usingPraat?s (Boersma, 2001) ?To Pitch...?
and ?To In-tensity...?.
All durational measures and word posi-tions are based on the semi-automatic alignment de-scribed above.
All measures are log-scaled and z-score normalized per speaker.
The full feature setappears in Table 2.4.2 Classification and AnalysisFor classification, we employ Support Vector Ma-chines (SVM), using the LibSVM implementation(C-C.Cheng and Lin, 2001) with an RBF kernel.
Foreach language/cultural group, we perform ?leave-one-dyad-out?
cross-validation based on F-measureas implemented in that toolkit.
For each fold, train-ing on 14 dyads and testing on the last, we determinenot only accuracy but also the weight-based rankingof each feature described above.Managing Class Imbalance Since listener verbalfeedback occurs in only 14-30% of candidate posi-tions, classification often predicts only the majority?NoFB?
class.
To compensate for this imbalance, weapply two strategies: reweighting and oversampling.We explore increasing the weight on the minorityclass in the classifier by a factor of two or four.
Wealso apply SMOTE (Chawla et al, 2002) oversam-pling to double or quadruple the number of minorityclass training instances.
SMOTE oversampling cre-ates new synthetic minority class instances by iden-tifying k = 3 nearest neighbors and inducing a newinstance by taking the difference between a sampleand its neighbor, multiplying by a factor between 0and 1, and adding that value to the original instance.5 ResultsTable 4 presents the classification accuracy for dis-tinguishing FB and NoFB contexts.
We present theoverall class distribution for each language.
We thencontrast the minority FB class and overall accuracyunder each of three weighting and oversampling set-tings.
The second row has no weighting or over-sampling; the third has no weighting with quadru-ple oversampling on all folds, a setting in which thelargest number of Arabic dyads achieves their bestperformance.
The last row indicates the oracle per-formance when the best weighting and oversamplingsetting is chosen for each fold.We find that the use of reweighting and over-sampling dramatically improves the recognition ofthe minority class, with only small reductions inoverall accuracy of 3-7%.
Under a uniform set-ting of quadruple oversampling and no reweight-ing, the number of correctly recognized Arabic andEnglish FB samples nearly triples, while the num-ber of Spanish FB samples doubles.
We furthersee that if we can dynamically select the optimaltraining settings, we can achieve even greater im-provements.
Here the number of correctly recog-nized FB examples increases between 3- (Spanish)and 6-fold (Arabic) with only a reduction of 1-4%in overall accuracy.
These accuracy levels corre-spond to recognizing between 38% (English, Span-ish) and 73% (Arabic) of the FB instances.
Even un-der these tuned conditions, the sparseness and vari-ability of the English and Spanish data continue topresent challenges.Finally, Table 3 illustrates the impact of the fullrange of reweighting and oversampling conditions.Each cell indicates the number of folds in each ofArabic, English, and Spanish respectively, for whichthat training condition yields the highest accuracy.We can see that the different dyads achieve optimalresults under a wide range of training conditions.616Feature Type Description Feature IDsPitch 5 uniform points across word pre 0,pre 0.25,pre 0.5,pre 0.75,pre 1post 0,post 0.25,post 0.5,post 0.75,post 1Maximum, minimum, mean pre pmax, pre pmin, pre pmeanpost pmax, post pmin, post pmeanDifferences in max, min, mean diff pmax, diff pmin, diff pmeanDifference b/t boundaries diff pitch endbegStart and end slope pre bslope, pre eslope, post bslope, post eslopeDifference b/t slopes diff slope endbegIntensity Maximum, minimum, mean pre imax, pre imin, pre imeanpost imax,post imin, post imeanDifference in maxima diff imaxDuration Last rhyme, last vowel, pause pre rdur, pre vdur, post rdur, post vdur, pause durVoice Quality Doubling & halving pre doub, pre half,post doub,post halfTable 2: Prosodic features for classification and analysis.
Features tagged ?pre?
are extracted from the word immedi-ately preceding the Speaker pausal region; those tagged ?post?
are extracted from the word immediatey following.weight 1 2 4no SMOTE 1,2,3 2,2,2 1,0,3SMOTE Double 1,0,2 1,2,0 2,2,1SMOTE Quad 3,0,0 1,2,2 3,6,2Table 3: Varying SVM weight and SMOTE ratio.
Eachcell shows # dyads in each language (Arabic, English,Spanish) with their best performance with this setting.Arabic English SpanishOverall 478 (1405) 395 (2659) 173 (1226)Baseline 53 (950) 23 (2167) 23 (1066)S=2, W=1 145 (878) 67 (2120) 47 (1023)Oracle 347 (918) 152 (2033) 68 (1059)Table 4: Row 1: Class distribution: # FB instances (#total instances).
Rows 2-4: Recognition under differentsettings: # FB correctly recognized (total # correct)6 Discussion: Feature AnalysisTo investigate the cross-language variation inspeaker cues eliciting listener verbal feedback, weconduct a feature analysis.
Table 5 presents thefeatures with highest average weight for each lan-guage assigned by the classifier across folds, as wellas those distinctive features highly ranked for onlyone language.We find that the Arabic dyads make extensiveand distinctive use of pitch in cuing verbal feed-back, from both preceding and following words,while placing little weight on other feature types.In contrast, both English and Spanish dyads exploitboth pitch and intensity features from surroundingwords.
Spanish alone makes significant use of bothvocalic and pause duration.
We also observe that, al-though there is substantial variation in feature rank-ing across speakers, the highly ranked features arerobustly employed across almost all folds.7 ConclusionBecause of the size of our data set, it may be pre-mature to draw firm conclusion about differencesbetween these three language groups based on thisanalysis.
The SVM weighting and SMOTE over-sampling strategy discussed here is promising forimproving recognition on imbalanced class data.This strategy substantially improves the prediction617Most Important FeaturesArabic English Spanishpre pmax pre pmean pre minpre pmean post pmean post 0.5pre 0.25 post 0.5 post 0.75pre 0.5 post 0.75 post 1pre 0.75 post 1 pre imaxpre 1 diff pmin pre imeanpost pmin pre imax post imaxpost bslope pre imean pause durdiff pmin post imean pre vdurMost Distinctive FeaturesArabic English Spanishpost pmin post pmean post 0post bslope post 0.25 post eslopepre 0.25 pre eslopepre 0.5 post vdurpre 1 pre imeanTable 5: Highest ranked and distinctive features for eachlanguage/cultural groupof verbal feedback.
The resulting feature rankingalso provides insight into the contrasts in the use ofprosodic cues among these language cultural groups,while highlighting the widespread, robust use ofpitch features.In future research, we would like to extend ourwork to exploit sequential learning frameworks topredict verbal feedback.
We also plan to explore thefusion of multi-modal features to enhance recogni-tion and increase our understanding of multi-modalrapport behavior.
We will also work to analyze howquickly people can establish rapport, as the short du-ration of our Spanish dyads poses substantial chal-lenges.8 AcknowledgmentsWe would like to thank our team of annota-tor/analysts for their efforts in creating this corpus,and Danial Parvaz for the development of the Arabictransliteration tool.
We are grateful for the insightsof Susan Duncan, David McNeill, and Dan Loehr.This work was supported by NSF BCS#: 0729515.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the National Science Foundation.ReferencesR.
Bertrand, G. Ferre, P. Blache, R. Espesser, andS.
Rauzy.
2007.
Backchannels revisited from a mul-timodal perspective.
In Auditory-visual Speech Pro-cessing, The Netherlands.
Hilvarenbeek.P.
Boersma.
2001.
Praat, a system for doing phoneticsby computer.
Glot International, 5(9?10):341?345.C-C.Cheng and C-J.
Lin.
2001.
LIBSVM:a libraryfor support vector machines.
Software available at:http://www.csie.ntu.edu.tw/ cjlin/libsvm.N.
Cathcart, J. Carletta, and E. Klein.
2003.
A shallowmodel of backchannel continuers in spoken dialogue.In Proceedings of the tenth conference on Europeanchapter of the Association for Computational Linguis-tics - Volume 1, pages 51?58.W.
Chafe.
1975.
The Pear Film.Nitesh Chawla, Kevin Bowyer, Lawrence O.
Hall, andW.
Philip Legelmeyer.
2002.
SMOTE: Synthetic mi-nority over-sampling technique.
Journal of ArtificialIntelligence Research, 16:321?357.S.
Duncan.
1972.
Some signals and rules for takingspeaking turns in conversations.
Journal of Person-ality and Social Psychology, 23(2):283?292.A.
Gravano and J. Hirschberg.
2009.
Backchannel-inviting cues in task-oriented dialogue.
In Proceedingsof Interspeech 2009, pages 1019?1022.David Herrera, David Novick, Dusan Jan, and DavidTraum.
2010.
The UTEP-ICT cross-cultural mul-tiparty multimodal dialog corpus.
In Proceedings ofthe Multimodal Corpora Workshop: Advances in Cap-turing, Coding and Analyzing Multimodality (MMC2010).A.
Kendon.
2004.
Gesture: Visible Action as Utterance.Cambridge University Press.G.-A.
Levow, S. Duncan, and E. King.
2010.
Cross-cultural investigation of prosody in verbal feedback ininteractional rapport.
In Proceedings of Interspeech2010.D.
Matsumoto, S. H. Yoo, S. Hirayama, and G. Petrova.2005.
Validation of an individual-level measure ofdisplay rules: The display rule assessment inventory(DRAI).
Emotion, 5:23?40.S.
Maynard.
1990.
Conversation management in con-trast: listener response in Japanese and American En-glish.
Journal of Pragmatics, 14:397?412.B.
Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang,X.
Yu, and S. Pradhan.
2001.
University of Coloradodialog systems for travel and navigation.618A.
Rivera and N. Ward.
2007.
Three prosodic featuresthat cue back-channel in Northern Mexican Span-ish.
Technical Report UTEP-CS-07-12, University ofTexas, El Paso.E.
Shriberg, A. Stolcke, and D. Baron.
2001.
Canprosody aid the automatic processing of multi-partymeetings?
evidence from predicting punctuation, dis-fluencies, and overlapping speech.
In Proc.
of ISCATutorial and Research Workshop on Prosody in SpeechRecognition and Understanding.Linda Tickle-Degnen and Robert Rosenthal.
1990.
Thenature of rapport and its nonverbal correlates.
Psycho-logical Inquiry, 1(4):285?293.N.
Ward and Y. Al Bayyari.
2007.
A prosodic featurethat invites back-channels in Egyptian Arabic.
Per-spectives in Arabic Linguistics XX.N.
Ward and W. Tsukuhara.
2000.
Prosodic fea-tures which cue back-channel responses in English andJapanese.
Journal of Pragmatics, 32(8):1177?1207.O.
M. Watson.
1970.
Proxemic Behavior: A Cross-cultural Study.
Mouton, The Hague.619
