Empirically Estimating Order Constraints forContent Planning in GenerationPablo A. Duboue and Kathleen R. McKeownComputer Science DepartmentColumbia University10027, New York, NY, USA{pablo,kathy}@cs.columbia.eduAbstractIn a language generation system, acontent planner embodies one or more?plans?
that are usually hand?crafted,sometimes through manual analysis oftarget text.
In this paper, we present asystem that we developed to automati-cally learn elements of a plan and theordering constraints among them.
Astraining data, we use semantically an-notated transcripts of domain expertsperforming the task our system is de-signed to mimic.
Given the large degreeof variation in the spoken language ofthe transcripts, we developed a novel al-gorithm to find parallels between tran-scripts based on techniques used incomputational genomics.
Our proposedmethodology was evaluated two?fold:the learning and generalization capabil-ities were quantitatively evaluated us-ing cross validation obtaining a level ofaccuracy of 89%.
A qualitative evalua-tion is also provided.1 IntroductionIn a language generation system, a content plan-ner typically uses one or more ?plans?
to rep-resent the content to be included in the out-put and the ordering between content elements.Some researchers rely on generic planners (e.g.,(Dale, 1988)) for this task, while others use plansbased on Rhetorical Structure Theory (RST) (e.g.,(Bouayad-Aga et al, 2000; Moore and Paris,1993; Hovy, 1993)) or schemas (e.g., (McKe-own, 1985; McKeown et al, 1997)).
In all cases,constraints on application of rules (e.g., plan op-erators), which determine content and order, areusually hand-crafted, sometimes through manualanalysis of target text.In this paper, we present a method for learn-ing the basic patterns contained within a plan andthe ordering among them.
As training data, weuse semantically tagged transcripts of domain ex-perts performing the task our system is designedto mimic, an oral briefing of patient status af-ter undergoing coronary bypass surgery.
Giventhat our target output is spoken language, there issome level of variability between individual tran-scripts.
It is difficult for a human to see patternsin the data and thus supervised learning based onhand-tagged training sets can not be applied.
Weneed a learning algorithm that can discover order-ing patterns in apparently unordered input.We based our unsupervised learning algorithmon techniques used in computational genomics(Durbin et al, 1998), where from large amountsof seemingly unorganized genetic sequences, pat-terns representing meaningful biological featuresare discovered.
In our application, a transcript isthe equivalent of a sequence and we are searchingfor patterns that occur repeatedly across multiplesequences.
We can think of these patterns as thebasic elements of a plan, representing small clus-ters of semantic units that are similar in size, forexample, to the nucleus-satellite pairs of RST.1By learning ordering constraints over these ele-1Note, however, that we do not learn or represent inten-tion.age, gender, pmh, pmh, pmh, pmh, med-preop,med-preop, med-preop, drip-preop, med-preop,ekg-preop, echo-preop, hct-preop, procedure,.
.
.Figure 2: The semantic sequence obtained fromthe transcript shown in Figure 1.ments, we produce a plan that can be expressedas a constraint-satisfaction problem.
In this pa-per, we focus on learning the plan elements andthe ordering constraints between them.
Our sys-tem uses combinatorial pattern matching (Rigout-sos and Floratos, 1998) combined with clusteringto learn plan elements.
Subsequently, it appliescounting procedures to learn ordering constraintsamong these elements.Our system produced a set of 24 schemataunits, that we call ?plan elements?2 , and 29 order-ing constraints between these basic plan elements,which we compared to the elements contained inthe orginal hand-crafted plan that was constructedbased on hand-analysis of transcripts, input fromdomain experts, and experimental evaluation ofthe system (McKeown et al, 2000).The remainder of this article is organized asfollows: first the data used in our experimentsis presented and its overall structure and acqui-sition methodology are analyzed.
In Section 3our techniques are described, together with theirgrounding in computational genomics.
The quan-titative and qualitative evaluation are discussedin Section 4.
Related work is presented in Sec-tion 5.
Conclusions and future work are discussedin Section 6.2 Our dataOur research is part of MAGIC (Dalal et al, 1996;McKeown et al, 2000), a system that is designedto produce a briefing of patient status after un-dergoing a coronary bypass operation.
Currently,when a patient is brought to the intensive careunit (ICU) after surgery, one of the residents whowas present in the operating room gives a brief-ing to the ICU nurses and residents.
Several ofthese briefings were collected and annotated forthe aforementioned evaluation.
The resident was2These units can be loosely related to the concept of mes-sages in (Reiter and Dale, 2000).equipped with a wearable tape recorder to tapethe briefings, which were transcribed to providethe base of our empirical data.
The text was sub-sequently annotated with semantic tags as shownin Figure 1.
The figure shows that each sentenceis split into several semantically tagged chunks.The tag-set was developed with the assistance ofa domain expert in order to capture the differentinformation types that are important for commu-nication and the tagging process was done by twonon-experts, after measuring acceptable agree-ment levels with the domain expert (see (McK-eown et al, 2000)).
The tag-set totalled over 200tags.
These 200 tags were then mapped to 29 cat-egories, which was also done by a domain expert.These categories are the ones used for our currentresearch.From these transcripts, we derive the sequencesof semantic tags for each transcript.
These se-quences constitute the input and working materialof our analysis, they are an average length of 33tags per transcript (min = 13, max = 66, ?
=11.6).
A tag-set distribution analysis showed thatsome of the categories dominate the tag counts.Furthermore, some tags occur fairly regularly to-wards either the beginning (e.g., date-of-birth) orthe end (e.g., urine-output) of the transcript, whileothers (e.g., intraop-problems) are spread more orless evenly throughout.Getting these transcripts is a highly expensivetask involving the cooperation and time of nursesand physicians in the busy ICU.
Our corpus con-tains a total number of 24 transcripts.
Therefore,it is important that we develop techniques that candetect patterns without requiring large amounts ofdata.3 MethodsDuring the preliminary analysis for this research,we looked for techniques to deal with analysis ofregularities in sequences of finite items (semantictags, in this case).
We were interested in devel-oping techniques that could scale as well as workwith small amounts of highly varied sequences.Computational biology is another branch ofcomputer science that has this problem as onetopic of study.
We focused on motif detectiontechniques as a way to reduce the complexity ofthe overall setting of the problem.
In biologicalHe is 58-year-oldagemalegender.
History is significant for Hodgkin?s diseasepmh, treatedwith .
.
.
to his neck, back and chest.
Hyperspadiaspmh, BPHpmh, hiatal herniapmhandproliferative lymph edema in his right armpmh.
No IV?s or blood pressure down in the leftarm.
Medications ?
Inderalmed-preop, Lopidmed-preop, Pepcidmed-preop, nitroglycerinedrip-preopand heparinmed-preop.
EKG has PAC?sekg-preop.His Echo showed AI, MR of 47 cine amps with hypokinetic basal and anterior apical region.echo-preopHematocrit 1.2hct-preop, otherwise his labs are unremarkable.
Went to OR for what was felt to be2 vessel CABG off pump both mammariesprocedure.
.
.
.
.
.Figure 1: An annotated transcription of an ICU briefing (after anonymising).terms, a motif is a small subsequence, highly con-served through evolution.
From the computer sci-ence standpoint, a motif is a fixed-order pattern,simply because it is a subsequence.
The problemof detecting such motifs in large databases hasattracted considerable interest in the last decade(see (Hudak and McClure, 1999) for a recent sur-vey).
Combinatorial pattern discovery, one tech-nique developed for this problem, promised tobe a good fit for our task because it can be pa-rameterized to operate successfully without largeamounts of data and it will be able to iden-tify domain swapped motifs: for example, givena?b?c in one sequence and c?b?a in another.This difference is central to our current research,given that order constraints are our main focus.TEIRESIAS (Rigoutsos and Floratos, 1998) andSPLASH (Califano, 1999) are good representa-tives of this kind of algorithm.
We used an adap-tation of TEIRESIAS.The algorithm can be sketched as follows: weapply combinatorial pattern discovery (see Sec-tion 3.1) to the semantic sequences.
The obtainedpatterns are refined through clustering (Section3.2).
Counting procedures are then used to es-timate order constraints between those clusters(Section 3.3).3.1 Pattern detectionIn this section, we provide a brief explanation ofour pattern discovery methodology.
The explana-tion builds on the definitions below:?L,W ?
pattern.
Given that ?
represents the se-mantic tags alphabet, a pattern is a string ofthe form ?(?|?)?
?, where ?
represents adon?t care (wildcard) position.
The ?L,W ?parameters are used to further control theamount and placement of the don?t cares:every subsequence of length W, at least Lpositions must be filled (i.e., they are non-wildcards characters).
This definition entailsthat L ?
W and also that a ?L,W ?
patternis also a ?L,W + 1?
pattern, etc.Support.
The support of pattern p given a set ofsequences S is the number of sequences thatcontain at least one match of p. It indicateshow useful a pattern is in a certain environ-ment.Offset list.
The offset list records the matchinglocations of a pattern p in a list of sequences.They are sets of ordered pairs, where the firstposition records the sequence number andthe second position records the offset in thatsequence where p matches (see Figure 3).Specificity.
We define a partial order relation onthe pattern space as follows: a pattern p issaid to be more specific than a pattern qif: (1) p is equal to q in the defined posi-tions of q but has fewer undefined (i.e., wild-cards) positions; or (2) q is a substring of p.Specificity provides a notion of complexityof a pattern (more specific patterns are morecomplex).
See Figure 4 for an example.Using the previous definitions, the algorithm re-duces to the problem of, given a set of sequences,L, W , a minimum windowsize, and a supportpattern: AB?D0 1 2 3 4 5 6 7 8 .
.
.
?
offsetseq?
: A B C D F A A B F D .
.
.seq?
: F C A B D D F F .
.
.
.
.
....offset list: {(?, 0); (?, 6); (?, 2); .
.
.
}Figure 3: A pattern, a set of sequences and anoffset list.ABC?
?DFABCA?DF ABC?
?DFGHHHjless specific thanFigure 4: The specificity relation among patterns.threshold, finding maximal ?L,W ?-patterns withat least a support of support threshold.
Our im-plementation can be sketched as follows:Scanning.
For a given window size n, all the pos-sible subsequences (i.e., n-grams) occurringin the training set are identified.
This processis repeated for different window sizes.Generalizing.
For each of the identified subse-quences, patterns are created by replacingvalid positions (i.e., any place but the firstand last positions) with wildcards.
Only?L,W ?
patterns with support greater thansupport threshold are kept.
Figure 5 showsan example.Filtering.
The above process is repeated increas-ing the window size until no patterns withenough support are found.
The list of iden-tified patterns is then filtered according tospecificity: given two patterns in the list, oneof them more specific than the other, if bothhave offset lists of equal size, the less spe-cific one is pruned3 .
This gives us the listof maximal motifs (i.e.
patterns) which aresupported by the training data.3Since they match in exactly the same positions, weprune the less specific one, as it adds no new information.A B C D E F ?
subsequenceAB?DEF ABCD?F ?
patterns.
.
.HHHjFigure 5: The process of generalizing an existingsubsequence.3.2 ClusteringAfter the detection of patterns is finished, thenumber of patterns is relatively large.
Moreover,as they have fixed length, they tend to be prettysimilar.
In fact, many tend to have their supportfrom the same subsequences in the corpus.
We areinterested in syntactic similarity as well as simi-larity in context.A convenient solution was to further cluster thepatterns, according to an approximate matchingdistance measure between patterns, defined in anappendix at the end of the paper.We use agglomerative clustering with the dis-tance between clusters defined as the maximumpairwise distance between elements of the twoclusters.
Clustering stops when no inter-clusterdistance falls below a user-defined threshold.Each of the resulting clusters has a single pat-tern represented by the centroid of the cluster.This concept is useful for visualization of thecluster in qualitative evaluation.3.3 Constraints inferenceThe last step of our algorithm measures the fre-quencies of all possible order constraints amongpairs of clusters, retaining those that occur of-ten enough to be considered important, accord-ing to some relevancy measure.
We also discardany constraint that it is violated in any trainingsequence.
We do this in order to obtain clear-cutconstraints.
Using the number of times a givenconstraint is violated as a quality measure is astraight-forward extension of our framework.
Thealgorithm proceeds as follows: we build a tableof counts that is updated every time a pair of pat-terns belonging to particular clusters are matched.To obtain clear-cut constraints, we do not countoverlapping occurrences of patterns.From the table of counts we need some rele-vancy measure, as the distribution of the tags isskewed.
We use a simple heuristic to estimatea relevancy measure over the constraints that arenever contradicted.
We are trying to obtain an es-timate ofPr (A ?precedes B)from the counts ofc = A ?
?preceded BWe normalize with these counts (where x rangesover all the patterns that match before/after A orB):c1 = A ?
?preceded xandc2 = x ?
?preceded BThe obtained estimates, e1 = c/c1 and e2 = c/c2,will in general yield different numbers.
We usethe arithmetic mean between both, e = (e1+e2)2 ,as the final estimate for each constraint.
It turnsout to be a good estimate, that predicts accuracyof the generated constraints (see Section 4).4 ResultsWe use cross validation to quantitatively evaluateour results and a comparison against the plan ofour existing system for qualitative evaluation.4.1 Quantitative evaluationWe evaluated two items: how effective the pat-terns and constraints learned were in an unseentest set and how accurate the predicted constraintswere.
More precisely:Pattern Confidence.
This figure measures thepercentage of identified patterns that wereable to match a sequence in the test set.Constraint Confidence.
An ordering constraintbetween two clusters can only be checkableon a given sequence if at least one patternfrom each cluster is present.
We measurethe percentage of the learned constraints thatare indeed checkable over the set of test se-quences.Constraint Accuracy.
This is, from our perspec-tive, the most important judgement.
It mea-sures the percentage of checkable orderingTable 1: Evaluation results.Test Resultpattern confidence 84.62%constraint confidence 66.70%constraint accuracy 89.45%constraints that are correct, i.e., the orderconstraint was maintained in any pair ofmatching patterns from both clusters in allthe test-set sequences.Using 3-fold cross-validation for computing thesemetrics, we obtained the results shown in Ta-ble 1 (averaged over 100 executions of the exper-iment).
The different parameter settings were de-fined as follows: for the motif detection algorithm?L,W ?
= ?2, 3?
and support threshold of 3.
Thealgorithm will normally find around 100 maximalmotifs.
The clustering algorithm used a relativedistance threshold of 3.5 that translates to an ac-tual treshold of 120 for an average inter-clusterdistance of 174.
The number of produced clusterswas in the order of the 25 clusters or so.
Finally, athreshold in relevancy of 0.1 was used in the con-straint learning procedure.
Given the amount ofdata available for these experiments all these pa-rameters were hand-tunned.4.2 Qualitative evaluationThe system was executed using all the availableinformation, with the same parametric settingsused in the quantitative evaluation, yielding a setof 29 constraints, out of 23 generated clusters.These constraints were analyzed by hand andcompared to the existing content-planner.
Wefound that most rules that were learned were val-idated by our existing plan.
Moreover, we gainedplacement constraints for two pieces of semanticinformation that are currently not represented inthe system?s plan.
In addition, we found minororder variation in relative placement of two differ-ent pairs of semantic tags.
This leads us to believethat the fixed order on these particular tags canbe relaxed to attain greater degrees of variabilityin the generated plans.
The process of creationof the existing content-planner was thorough, in-formed by multiple domain experts over a threeyear period.
The fact that the obtained constraintsmostly occur in the existing plan is very encour-aging.5 Related workAs explained in (Hudak and McClure, 1999), mo-tif detection is usually targeted with alignmenttechniques (as in (Durbin et al, 1998)) or withcombinatorial pattern discovery techniques suchas the ones we used here.
Combinatorial patterndiscovery is more appropriate for our task becauseit allows for matching across patterns with permu-tations, for representation of wild cards and foruse on smaller data sets.Similar techniques are used in NLP.
Align-ments are widely used in MT, for example(Melamed, 1997), but the crossing problem is aphenomenon that occurs repeatedly and at manylevels in our task and thus, this is not a suitableapproach for us.Pattern discovery techniques are often used forinformation extraction (e.g., (Riloff, 1993; Fisheret al, 1995)), but most work uses data that con-tains patterns labelled with the semantic slot thepattern fills.
Given the difficulty for humans infinding patterns systematically in our data, weneeded unsupervised techniques such as those de-veloped in computational genomics.Other stochastic approaches to NLG normallyfocus on the problem of sentence generation,including syntactic and lexical realization (e.g.,(Langkilde and Knight, 1998; Bangalore andRambow, 2000; Knight and Hatzivassiloglou,1995)).
Concurrent work analyzing constraints onordering of sentences in summarization found thata coherence constraint that ensures that blocks ofsentences on the same topic tend to occur together(Barzilay et al, 2001).
This results in a bottom-up approach for ordering that opportunisticallygroups sentences together based on content fea-tures.
In contrast, our work attempts to automati-cally learn plans for generation based on semantictypes of the input clause, resulting in a top-downplanner for selecting and ordering content.6 ConclusionsIn this paper we presented a technique for extract-ing order constraints among plan elements thatperforms satisfactorily without the need of largecorpora.
Using a conservative set of parameters,we were able to reconstruct a good portion of acarefully hand-crafted planner.
Moreover, as dis-cussed in the evaluation, there are several piecesof information in the transcripts which are notpresent in the current system.
From our learnedresults, we have inferred placement constraints ofthe new information in relation to the previousplan elements without further interviews with ex-perts.Furthermore, it seems we have captured order-sensitive information in the patterns and free-order information is kept in the don?t care model.The patterns, and ordering constraints amongthem, provide a backbone of relatively fixed struc-ture, while don?t cares are interspersed amongthem.
This model, being probabilistic in nature,means a great deal of variation, but our gener-ated plans should have variability in the right po-sitions.
This is similar to findings of floating posi-tioning of information, together with oportunisticrendering of the data as used in STREAK (Robinand McKeown, 1996).6.1 Future workWe are planning to use these techniques to reviseour current content-planner and incorporate infor-mation that is learned from the transcripts to in-crease the possible variation in system output.The final step in producing a full-fledgedcontent-planner is to add semantic constraints onthe selection of possible orderings.
This can begenerated through clustering of semantic input tothe generator.We also are interested in further evaluating thetechnique in an unrestricted domain such as theWall Street Journal (WSJ) with shallow seman-tics such as the WordNet top-category for eachNP-head.
This kind of experiment may showstrengths and limitations of the algorithm in largecorpora.7 AcknowledgmentsThis research is supported in part by NLM Con-tract R01 LM06593-01 and the Columbia Uni-versity Center for Advanced Technology in In-formation Management (funded by the New YorkState Science and Technology Foundation).
Theauthors would like to thank Regina Barzilay,intraop-problems intraop-problems??
?operation 11.11%drip 33.33%intraop-problems 33.33%total-meds-anesthetics 22.22%???dripintraop-problems??
?operation 14.29%drip 14.29%intraop-problems 42.86%total-meds-anesthetics 28.58%??
?drip dripintraop-problems intraop-problems??
?operation 20.00%drip 20.00%intraop-problems 20.00%total-meds-anesthetics 40.00%??
?drip dripFigure 6: Cluster and patterns example.
Each line corresponds to a different pattern.
The elementsbetween braces are don?t care positions (three patterns conform this cluster: intraop-problems intraop-problems ?
drip,intraop-problems ?
drip drip and intraop-problems intraop-problems drip drip the don?t care model shown in each brace must sum up to1 but there is a strong overlap between patterns ?the main reason for clustering)Noemie Elhadad and Smaranda Muresan for help-ful suggestions and comments.
The aid of twoanonymous reviewers was also highly appreci-ated.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gen-eration.
In COLING, 2000, Saarbrcken, Germany.Regina Barzilay, Noemie Elhadad, and Kathleen R.McKeown.
2001.
Sentence ordering in multidoc-ument summarization.
In HLT, 2001, San Diego,CA.Nadjet Bouayad-Aga, Richard Power, and DoniaScott.
2000.
Can text structure be incompatiblewith rhetorical structure?
In Proceedings of the1st International Conference on Natural LanguageGeneration (INLG-2000), pages 194?200, MitzpeRamon, Israel.Andrea Califano.
1999.
Splash: Structural pattern lo-calization analysis by sequential histograms.
Bioin-formatics, 12, February.Mukesh Dalal, Steven Feiner, , Kathleen McKeown,ShiMei Pan, Michelle Zhou, Tobias Hollerer, JamesShaw, Yong Feng, and Jeanne Fromer.
1996.
Nego-tiation for automated generation of temporal multi-media presentations.
In Proceedings of ACM Mul-timedia ?96, Philadelphia.Robert Dale.
1988.
Generating referring expressionsin a domain of objects and processes.
Ph.D. thesis,University of Edinburgh.Richard Durbin, S. Eddy, A. Krogh, and G. Mitchi-son.
1998.
Biological sequence analysis.
Cam-bridge Univeristy Press.David Fisher, Stephen Soderland, Joseph McCarthy,Fangfang Feng, and Wendy Lehnert.
1995.
De-scription of the umass system as used for muc-6.
In Morgan Kaufman, editor, Proceedings of theSixth Message Understanding Conference (MUC-6), pages 127?140, San Francisco.Eduard H. Hovy.
1993.
Automated discourse gener-ation using discourse structure relations.
ArtificialIntelligence.
(Special Issue on Natural LanguageProcessing).J.
Hudak and Marcela McClure.
1999.
A comparativeanalysis of computational motif?detection methods.In R.B.
Altman, A. K. Dunker, L. Hunter, T. E.Klein, and K. Lauderdale, editors, Pacific Sympo-sium on Biocomputing, ?99, pages 138?149, NewJersey.
World Scientific.Kevin Knight and Vasileios Hatzivassiloglou.
1995.Two-level, many-paths generation.
In Proceedingsof the Conference of the Association for Computa-tional Linguistics (ACL?95).Irene Langkilde and Kevin Knight.
1998.
The practi-cal value of n-grams in generation.
In Proceedingsof the Ninth International Natural Language Gen-eration Workshop (INLG?98).Kathleen McKeown, ShiMei Pan, James Shaw, JordanDesmand, and Barry Allen.
1997.
Language gen-eration for multimedia healthcare briefings.
In Pro-ceedings of the 5th Conference on Applied NaturalLanguage Processing (ANLP?97), Washington, DC,April.Kathleen R. McKeown, Desmond Jordan, StevenFeiner, James Shaw, Elizabeth Chen, Shabina Ah-mad, Andre Kushniruk, and Vimla Patel.
2000.
Astudy of communication in the cardiac surgery in-tensive care unit and its implications for automatedbriefing.
In AMIA ?2000.Kathleen R. McKeown.
1985.
Text Generation: Us-ing Discourse Strategies and Focus Constraints toGenerate Natural Language Text.
Cambridge Uni-versity Press.I.
Dan Melamed.
1997.
A portable algorithm formapping bitext correspondence.
In 35th Confer-ence of the Association for Computational Linguis-tics (ACL?97), Madrid, Spain.Johanna D. Moore and Ce?cile L. Paris.
1993.
Plan-ning text for advisory dialogues: Capturing inten-tional and rhetorical information.
ComputationalLinguistics, 19(4):651?695.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press.Isidore Rigoutsos and Aris Floratos.
1998.
Combina-torial pattern discovery in biological sequences: theteiresias algorithm.
Bioinformatics, 14(1):55?67.Ellen Riloff.
1993.
Automatically constructing a dic-tionary for information extraction.
In AAAI Press/ MIT Press, editor, Proceedings of the Eleventh Na-tional Conference on Artificial Intelligence, pages811?816.Jacques Robin and Kathleen McKeown.
1996.
Em-pirically designing and evaluating a new revision?based model for summary generation.
Artificial In-telligence, 85(1?2):135?179.Appendix - Definition of the distance mea-sure used for clustering.An approximate matching measure is de-fined for a given extended pattern.
The ex-tended pattern is represented as a sequence ofsets; defined positions have a singleton set,while wildcard positions contain the non-zeroprobability elements in their don?t care model(e.g.
given intraop-problems, intraop-problems, {drip 10%,intubation90%}, drip we model this as [{intraop-problems}; {intraop-problems}; {drip, intubation}; {drip}}]).Consider p to be such a pattern, o an offset andS a sequence, the approximate matching is de-fined bym?
(p, o, S) =?length(p)i=0 match(p[i], S[i + o])length(p)where the match(P, e) function is defined as 0 ife ?
P , 1 otherwise, and where P is the set atposition i in the extended pattern p and e is theelement of the sequence S at position i + o.Our measure is normalized to [0, 1].
Usingthis function, we define the approximate match-ing distance measure (one way) between a patternp1 and a pattern p2 as the sum (averaged over thelength of the offset list of p1) of all the approxi-mate matching measures of p2 over the offset listof p1.
This is, again, a real number in [0, 1].
Toensure symmetry, we define the distance betweenp1 and p2 as the average between the one way dis-tance between p1 and p2 and between p2 and p1.
