Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1037?1046, Dublin, Ireland, August 23-29 2014.Automatic Discovery of Adposition TypologyRishiraj Saha RoyIIT KharagpurKharagpur, India ?
721302.rishiraj@cse.iitkgp.ernet.inRahul Katare?IIT KharagpurKharagpur, India ?
721302.rah.ykg@gmail.comNiloy GangulyIIT KharagpurKharagpur, India ?
721302.niloy@cse.iitkgp.ernet.inMonojit ChoudhuryMicrosoft Research IndiaBangalore, India ?
560001.monojitc@microsoft.comAbstractNatural languages (NL) can be classified as prepositional or postpositional based on the order ofthe noun phrase and the adposition.
Categorizing a language by its adposition typology helps inaddressing several challenges in linguistics and natural language processing (NLP).
Understand-ing the adposition typologies for less-studied languages by manual analysis of large text corporacan be quite expensive, yet automatic discovery of the same has received very little attention tilldate.
This research presents a simple unsupervised technique to automatically predict the adpo-sition typology for a language.
Most of the function words of a language are adpositions, and weshow that function words can be effectively separated from content words by leveraging differ-ences in their distributional properties in a corpus.
Using this principle, we show that languagescan be classified as prepositional or postpositional based on the rank correlations derived fromentropies of word co-occurrence distributions.
Our claims are substantiated through experimentson 23 languages from ten diverse families, 19 of which are correctly classified by our technique.1 IntroductionAdpositions form a subcategory of function words that combine with noun phrases to denote their se-mantic or grammatical relationships with verbs, and sometimes other noun phrases.
NLs can be neatlydivided into a few basic typologies based on the order of the noun phrase and its adposition.
If the ad-position is placed before the noun phrase, it is called a preposition.
Postpositions and inpositions, on theother hand, are adpositions that are placed after and inside noun phrases respectively.
If prepositions arepredominantly used in the language, for example in English, Bulgarian and Russian, then the languageis said to be prepositional.
Similarly, Japanese, Hindi and Turkish are some examples of postpositionallanguages, which predominantly use postpositions.
These two are the most commonly found adpositiontypologies across the globe.
Out of 1185 languages analyzed on the World Atlas of Language Structures(WALS)1(Dryer and Haspelmath, 2011), there are 577 postpositional, 512 prepositional and only 8 in-positional languages.
There are a few (30 and 58 respectively) languages which use no or both kinds ofadpositions.
The order of adpositions is strongly correlated with many other word order typologies.
Forinstance, postpositional languages usually have Object-Verb ordering, whereas prepositional languageshave Verb-Object ordering (Greenberg, 1963).
Daum?e and Campbell (2007) present a statistical modelfor automatically discovering such implications from a large typological database and discuss many othertypological implications involving adpositions.Motivation.
Knowledge of the typological characteristics of languages is not only of interest to lin-guists, but also very useful in NLP for two main reasons.
First, typological information, if appropriatelyexploited while designing computational methods, can lead to very promising results in tasks like co-reference resolution and machine translation (Haghighi and Klein, 2007; Moore and Quirk, 2007).
Sec-ond, as Bender and Langendoen (2010) have pointed out, in order to claim that a computational technique?This work was done during the author?s internship at Microsoft Research India.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://wals.info/1037is truly language independent, one must show its usefulness for languages having diverse typological fea-tures.
However, there is very little work on the automatic discovery of typological characteristics, primar-ily because it is assumed that such information is readily available.
However, Hammarstr?om et al.
(2008)argue that documenting a language and its typological features is a time consuming process for the lin-guists and therefore, automatic methods for bootstrapping language description is a worthwhile efforttowards language preservation.
Lewis and Xia (2008) mine inter-linearized data from the Web and infertypological features for ?low-density?
languages, i.e.
languages represented in scarce quantities on theWeb.
We argue that apart from documenting and understanding the typology of ?low-density?
languages,unsupervised discovery of adposition typology is also useful for analyzing undeciphered languages andscripts, such as the Indus valley script (Rao et al., 2009) and the Cypro-Minoan syllabary (Palaima,1989), as well as newly emerging languages, such as the language of Web search queries (Saha Roy etal., 2012) or the Nicaraguan sign language (Meir et al., 2010).
While the former cases are interestingfrom historical and language change perspectives, the latter cases are useful for more practical reasons(for example, improvement in query understanding leading to better Web search, and development ofinteractive systems for deaf children).Approach.
In this work, we show that some simple word co-occurrence statistics, that can easily becomputed from any medium-sized text corpus, can be used as reliable predictors of adposition typologyof a language.
These statistics have been arrived at based on two fundamental assumptions: (a) adposi-tions constitute a large fraction of function words; and (b) the strict ordering between the adposition andthe noun phrase leads to differential co-occurrence characteristics on the left and right sides of the adpo-sition.
Therefore, if the function words of a language are automatically detected and the co-occurrencestatistics on the left and right of those words are appropriately analyzed, then it should be possible totell the prepositional languages apart from the postpositional ones.
Specifically, we measure counts andentropies of left, right and total (either side) co-occurrence distributions for each word.
We show that leftco-occurrence statistics are better indicators of function words for prepositional languages, while rightco-occurrence statistics perform better for postpositional languages.
Interestingly, the performance oftotal co-occurrence statistics lie in between the two for both types of languages.
Thus, the nature of thedifference in performances of left (or right) and total co-occurrences is likely to be indicative of the ad-position typology of the language.
We formalize this intuition to devise our test for adposition typology.We demonstrate our technique on 23 languages from ten language families, of which 14 are prepositionaland 9 are postpositional.
Our technique is able to consistently predict the correct adposition typology for19 of these languages.
The remaining four languages are highly inflectional and agglutinating in nature,and hence not amenable to the present technique.Organization.
The rest of this paper is organized as follows.
In Sec.
2, we present our method forfunction word detection using word co-occurrence statistics, along with results showing the effectivenessof such an approach.
In Sec.
3, we propose our test for discovering the adposition typology of a languagebased on correlations inferred from different co-occurrence statistics.
Sec.
4 discusses experiments con-ducted on diverse languages and inferences drawn from the observations.
Finally, Sec.
5 summarizes ourcontribution and indicates possible directions for future work.2 Function Word DetectionOur method for the prediction of the adposition typology of a language relies on the facts that mostadpositions are function words, and the distributional properties of function words are very differentfrom those of content words.
We exploit this difference to first formulate a method for extracting thefunction words of a language from a corpus.
We then proceed to use the same underlying principleto automatically discover the adposition typology for languages, where we do not assume that the truefunction word lists are available.By function words, we refer to all the closed-class lexical items in a language, e.g., pronouns, de-terminers, prepositions, conjunctions, interjections and other particles (as opposed to open-class items,e.g., nouns, verbs, adjectives and adverbs).
For the function word detection experiments, we shall lookat four languages from different families: English, Italian, Hindi and Bangla.
English is a Germanic1038Language Corpus source S N V Function word list source FEnglish Leipzig Corporaa1M 19.8M 342157 Sequence Publishingb229Italian -do- 1M 20M 434680 -do- 257Hindi -do- 0.3M 5.5M 127428 Manually constructed by linguists and 481augmented by extracting pronouns,determiners, prepositions, conjunctionsand interjections from POS-taggedcorpora available at LDCcBangla Crawl of Anandabazar Patrikad0.05M 16.2M 411878 -do- 510ahttp://corpora.informatik.uni-leipzig.de/download.htmlbhttp://www.sequencepublishing.com/academic.html\#function-wordschttp://www.ldc.upenn.edu (Catalog Nos.
LDC2010T24 and LDC2010T16 for Hindi and Bangla respectively)dhttp://www.anandabazar.com/Table 1: Details of NL corpora for function word detection experiments.language, Italian is a Romanic language, and Hindi and Bangla belong to the Indo-Aryan family.
Englishand Italian are prepositional languages with subject-verb-object word order, while Hindi and Bangla arepostpositional, relatively free word order with preference for subject-object-verb.
Therefore, any func-tion word characterization strategy that works across these languages is expected to work for a largevariety of languages.The details of the corpora used for these four languages are summarized in Table 1.
M in the valuecolumns denotes million.
S, N , V and F denote the numbers of all sentences, all words, unique words(vocabulary size) and function words, respectively.
We note that the Indian languages have almost twiceas many function words as compared to the European ones.
This is due to morphological richness andthe existence of large numbers of modal and vector verbs.Frequency is often used as an indicator for detecting function words, but the following factors affectits robustness.
If the corpus size is not large, many function words will not occur a sufficient num-ber of times.
For example, even though the and in will be very frequent in most English corpora,meanwhile and off may not be so.
As a result, if frequency is used as a function word detector withsmall datasets, we will have a problem of low recall.
In our experiments, we measure corpus size, N ,as the total number of words present.
If our language corpus is restricted, or sampled only from specificdomains, words specific to those domains will have high frequencies and will get detected as functionwords.
For example, the word government will be much more frequent in political news corpora thanalthough.
The number of unique words in a corpus, or the vocabulary size, V , is a good indicator ofits diversity.
For restricted domain corpora, V grows much more slowly withN than in a general domaincorpus.We now introduce other properties of function words that may help in more robust detection.
Weobserve the following interesting characteristics about the syntactic distributions of function and contentwords in NL, which can be summarized by the following two postulates.Postulate I.
Function words, in general, tend to co-occur with a larger number of distinct words thancontent words.
What can occur to the immediate left or right of a content word is much more restrictedthan that in the case of function words.
We hypothesize that even if a content word, e.g., government,might have high frequency owing to the nature of the domain, there will only be a relatively fewer numberof words that can co-occur immediately after or before it.
Therefore, the co-occurrence count may be amore robust indicator of function words.Postulate II.
The co-occurrence patterns of function words are less likely to show bias towards spe-cific words than those for content words.
For example, and will occur beside several other words likeschool, elephant and pipe with more or less equally distributed co-occurrence counts with all ofthese words.
In contrast, the co-occurrence distribution of school will be skewed, with more bias to-wards to, high and bus than over, through and coast, with the list of words occurring besideschool also being much smaller than that for and.In order to test Postulate I, we measure the number of distinct words that occur to the immediate left,1039right and either side of each unique word in the sub-sampled corpora.
We shall refer to these statisticsas left, right and total co-occurrence counts (LCC, RCC and TCC) respectively.
To test Postulate II, wecompute the entropy of the co-occurrence distributions of the words occurring to the left, right and eitherside (i.e., total) contexts of a word w:Entropy(w) = ??ti?
context(w)pti|wlog2(pti|w) (1)where, context(w) is the set of all words co-occurring with w either in the left, the right or the totalcontexts, and p(ti|w) is the probability of observing word tiin that specific context.Context.
In this paper, the left, right and total contexts of a word w respectively denote the imme-diately preceding (one) word, immediately succeeding (one) word and both the immediately precedingand the immediately succeeding words for w respectively, in sentences of the corpus.
The definition ofcontext (i.e., whether it includes the preceding or the succeeding one or two or three words) will changethe absolute values of our results, but all the trends are expected to remain the same.We shall refer to the co-occurrence entropies as left, right and total Co-occurrence Entropies (LCE,RCE and TCE respectively).
Due to their pivotal role in syntactically connecting the different words orparts of a sentence to each other, we would expect LCC, RCC or TCC of function words to be higherthan that of content words due to Postulate I; similarly, due to Postulate II we can expect the LCE, RCEor TCE to be higher for function words than for content words.
If the LCE or LCC of a word w is high,it means that a large number of distinct words can precede w in the language (additionally, almost withequal probabilities for high LCE).
Thus, predicting the previous word of w is difficult.
Similarly, if RCEor RCC of w is high, it means that a large number of words can follow w in the language (additionally,almost with equal probabilities for high RCE).
Thus, predicting the next word of w is difficult.
A highTCE for a word implies that the word can be preceded and followed by a large number of words, makingthe prediction of either the next or the previous word (or both) for w difficult.2.1 Experiments and ResultsIn our approach, the output is a ranked list of words sorted in descending order of the correspondingproperty.
Here we adopt a popular metric, Average Precision (AP), used in Information Retrieval (IR)for the evaluation of ranked lists.
More specifically, let w1, w2, .
.
.
, wnbe a ranked list of words sortedaccording to some corpus statistic, say, frequency.
Thus, if i < j, then frequency of wiis greater thanthe frequency of wj.
Precision at rank k, denoted by P@k, is defined asP@k =1kk?i=1f(wi) (2)where, f(wi) is one if wiis a function word, and is zero otherwise.
This function can be computedbased on the gold standard lists of function words.
Subsequently, average precision at rank n, denotedby AP@n, is defined asAP@n =1nn?k=1P@k (3)AP@n is a better metric than P@k because P@k is insensitive to the rank at which function wordsoccur in the list.
In our experiments, we compute AP@n averaged overN corpus sub-samples, which isgiven by1N?Nr=1(AP@n)rwhere (AP@n)ris the AP@n for the rthsub-sample.
We note that there areother metrics popularly used in IR, e.g.
the Normalized Discounted Cumulative Gain (nDCG).
However,these are more sensitive to the correctness of the top few items in the list and hence, are not suitable forus.
Knowing that the number of function words in a popular NL is at least 200 (Table 1), we computeAP@200 with respect to the gold standard lists of function words for all our experiments.1040Language Typology Fr LCC LCE TCC TCE RCC RCEEnglish Prepositional 0.663 0.702?0.729?0.684?0.679?0.637 0.527Italian Prepositional 0.611 0.639?0.645?0.636?0.620 0.606 0.601Hindi Postpositional 0.682 0.614 0.510 0.698?0.694?0.716?0.713?Bangla Postpositional 0.648 0.684?0.691?0.730?0.763?0.741?0.757?The four highest values in a row are marked in boldface.
Statistically significant improvement over frequency is marked by?.The paired t-test was performed and the null hypothesis was rejected if p-value < 0.05.Table 2: AP@200 for all indicators, averaged over 200 (N , V ) pairs for each language.
(a) (b)Figure 1: (Colour online) Performance of co-occurrence statistics for (a) English, and (b) Hindi, withrespect to frequency for AP@200 with variation in N.We now sort the list of all words in descending order of each of the seven indicators.
We then computeAP@200 for these seven lists.
To bring out the performance difference of each of the six co-occurrencefeatures with respect to frequency, we plot (in Figs.
1 and 2) the following measure against N :Value plotted =Metric for indicator ?
Metric for FrMetric for Fr(4)The x-axis can now be thought of as representing the performance of frequency.
In Fig.
1, for aparticular N , the data points were averaged over all (N , V ) pairs (we had 20 (N , V ) pairs for each N ).For Fig.
2, V was binned into five zones, and for each zone, the AP was averaged over all corresponding(N , V ) pairs.
The observations (both N and V variation) for French and Italian were similar to that ofEnglish, while those for Hindi and Bangla were similar to each other.
Table 2 reports AP values for allstatistics for the four languages.
From Table 2, we see that for all the languages, AP for some of theco-occurrence statistics are higher than AP obtained using frequency.Regular improvements over frequency.
From the plots and Table 2, it is evident that some of theco-occurrence statistics consistently beat frequency as indicators.
In fact, as evident from Figs.
1 and2, use of co-occurrence statistics results in systematic improvement over frequency with variations in Nand V , and hence, are very robust indicators.
Among the co-occurrence statistics, both entropies andcounts are observed to have comparable performance.3 Detection of Adposition TypologyFrom the results presented above, we observe that the best function word indicator depends upon lan-guage typology.
Interestingly, while LCE and LCC are the best indicators of function words for the twoprepositional languages of English and Italian, RCE and RCC perform better for Hindi and Bangla, thepostpositional languages.
This observation can be explained as follows.
For a prepositional language,the function words, which are often the adpositions, precede the content word it is linked to.
Therefore,the words following an adposition (or a function word) mark the beginnings of syntactic units such as1041(a) (b)Figure 2: (Colour online) Performance of co-occurrence statistics for (a) Italian, and (b) Bangla, withrespect to frequency for AP@200 with variation in V.noun phrases and are typically restricted to certain syntactic categories.
However, the words that precedethe adpositions have no or much weaker syntactic restrictions.
Hence, the LCE and LCC are higher andconsequently better and more robust indicators of function words for prepositional languages.
For verysimilar reasons, the RCE and RCC are better indicators of function words for postpositional languages.Importantly, we observe that TCE and TCC seem to be reasonably good predictors of function wordsirrespective of the typology, with performances lying in between the poorest indicators (RCE and RCCfor prepositional languages and LCE and LCC for postpositional languages) and the best indicators (LCEand LCC for prepositional languages and RCE and RCC for postpositional languages) for all the fourlanguages.
This makes them safe indicators to rely on when not much is known about the languagesyntax.
In fact, the philosophy of this research is to be of assistance in these less-known cases.
Thus,co-occurrence statistics have potential in predicting the adposition typology of a new language, whichwe leverage in this research.We now describe our intuition and method behind our tests for automatically detecting the adpositiontypology of a language.
In this context, we do not know the actual function words or adpositions of thelanguage under consideration.
Let us take the three lists of the top 200 words from a language corpus,sorted according to the statistics TCE, LCE and RCE.
For a prepositional language, we can expect to seethe highest number of function words towards the top of the list when sorted according to LCE, followedby the number of function words towards the top of the TCE list.
The RCE list would be expected to bethe poorest in this regard.
Thus, we expect a higher overlap between the top 200 word lists for TCE andLCE, than for TCE and RCE.
The reverse is expected to be true for postpositional languages.
Similararguments can be presented for LCC, RCC and TCC as well.
We quantify this correlation betweenthe lists using two different statistics ?
the Pearson?s correlation coefficient (r) and Spearman?s RankCorrelation Coefficient (?
).For computing Pearson?s coefficients, we use the actual values of the distributional statistics, while forSpearman?s rank coeffcients, we use the ranks of the words.
Let r(TL) and ?
(TL) respectively denotethe Pearson?s and Spearman?s Rank correlation coefficients of the lists sorted by TCE and LCE (or TCCand LCC), and similarly, let r(TR) and ?
(TR) denote the respective coefficients for the lists sorted byTCE and RCE (or TCC and RCC).Postulate.
For a prepositional language, the top-200 words by LCE will have a higher correlation withthe top-200 words by TCE than the corresponding correlation of RCE with TCE.
For a postpositionallanguage, the top-200 words by RCE will have a higher correlation with the top-200 words by TCE.Formally, for prepositional languages, r(TL) > r(TR), and ?
(TL) > ?
(TR), while for postpositionallanguages r(TL) < r(TR) and ?
(TL) < ?
(TR).1042Language Family ?
(TL) ?
(TR) ?
(Diff .)
Predicted TrueBulgarian Slavic (Indo-European) 0.726 0.518 0.208 Pre- Pre- (Scatton, 1984)Danish Germanic (Indo-European) 0.621 0.495 0.126 Pre- Pre- (Allan et al., 1995)Dutch Germanic (Indo-European) 0.662 0.204 0.458 Pre- Pre- (Shetter, 1958)English Germanic (Indo-European) 0.461 0.436 0.025 Pre- Pre- (Selkirk, 1996)German Germanic (Indo-European) 0.563 0.517 0.046 Pre- Pre- (Lederer, 1969)Italian Romance (Indo-European) 0.730 0.456 0.274 Pre- Pre- (Sauer, 1891)Macedonian Slavic (Indo-European) 0.692 0.488 0.205 Pre- Pre- (Friedman, 1993)Norwegian Germanic (Indo-European) 0.619 0.600 0.019 Pre- Pre- (Olson, 1901)Polish Slavic (Indo-European) 0.798 0.554 0.243 Pre- Pre- (Bielec, 1998)Russian Slavic (Indo-European) 0.743 0.652 0.091 Pre- Pre- (Borras and Christian, 1959)Slovenian Slavic (Indo-European) 0.701 0.668 0.032 Pre- Pre- (Priestly, 1993)Swedish Germanic (Indo-European) 0.663 0.525 0.138 Pre- Pre- (Holmes and Hinchliffe, 1994)Ukrainian Slavic (Indo-European) 0.785 0.714 0.070 Pre- Pre- (Stechishin, 1958)Gujarati Indic (Indo-European) 0.540 0.581 ?0.041 Post- Post- (Cardona, 1965)Hindi Indic (Indo-European) 0.529 0.731 ?0.202 Post- Post- (McGregor, 1977)Japanese Japanese (Japanese) 0.429 0.626 ?0.197 Post- Post- (Hinds, 1986)Nepali Indic (Indo-European) 0.495 0.719 ?0.224 Post- Post- (Bandhu, 1973)Tamil Southern Dravidian (Dravidian) 0.748 0.805 ?0.057 Post- Post- (Asher, 1982)Turkish Turkic (Altaic) 0.531 0.769 ?0.238 Post- Post- (Underhill, 1976)Estonian Finnic(Uralic) 0.790 0.733 0.057 Pre- Post- (Tauli, 1983)Finnish Finnic (Uralic) 0.671 0.656 0.015 Pre- Post- (Sulkala and Karjalainen, 1992)Hungarian Ugric (Uralic) 0.457 0.329 0.128 Pre- Post- (Kenesei et al., 1998)Lithuanian Baltic (Indo-European) 0.715 0.724 ?0.009 Post- Pre- (Dambriunas et al., 1966)Misclassified languages are marked in gray.Table 3: Detecting adposition typology using Spearman?s rank correlation coefficients on entropy lists.4 Experimental Results and ObservationsIn this section, we first present our datasets, followed by detailed experiments on adposition typologydetection and inferences drawn from the observations.4.1 DatasetsFor all our typology detection experiments, we use datasets from the publicly available Leipzig Corpora2.We selected 23 languages from various families that are typologically diverse.
A (300, 000)-sentencecorpora was used for all the languages so as to ensure similar-sized corpora for all the languages (manylanguages do not have a larger corpus).
All languages examined have been listed in Table 3, along withtheir families and true adposition typologies (accompanied by appropriate references).4.2 Experiments and ResultsWe extracted the top-200 words by TCE, LCE and RCE, and TCC, LCC and RCC from the 300k-sentence corpora.
We then computed r(TL), ?
(TL), r(TR) and ?
(TR), for both entropies and counts.As per our postulate, if ?(TL)?
?
(TR) (= ?
(Difference)) is positive, the language is prepositional; if itis negative, the language is postpositional.
The same can be expected for r(Difference).The performance of ?
as a predictor was found to be better than r. Results when the entropy lists areused are presented in Table 3.
For only 4 out of 23 languages, the typology predictions are incorrect.
Weobserve that three of these misclassified languages are from the Uralic family that are synthetic in naturecharacterized by extensive regular agglutination of modifiers to verbs, nouns, adjectives and numerals.2http://corpora.informatik.uni-leipzig.de/download.html1043Corpus Size Entropy lists (r) Entropy lists (?)
Count lists (r) Count lists (?
)10k 17/23 21/23 17/23 13/23100k 17/23 19/23 18/23 13/23300k 16/23 19/23 16/23 13/23The highest value in a row is marked in boldface.Table 4: Correct predictions by strategy with varying factors.The average number of characters in words of these languages were found to be in the relatively higherrange of nine to eleven.
Thus, function words, especially the adpositions, seldom occur as free words inthese languages and hence our method cannot capture the distributional characteristics of the adpositions.It is worthwhile to note that the method can predict the correct typology for other languages that employagglutination to a lesser degree (Bulgarian, Dutch, German, Tamil and Turkish).
Lithuanian, though notsynthetic, is a highly inflectional language and therefore, instead of adpositions it makes extensive useof case-markers.
With ?
(Difference) very close to zero, our prediction for Lithuanian is inconclusive.A note on synthetic languages: For synthetic languages, the difference between the two rank corre-lation coefficients are close to zero, which provides us with a direct way to identify them.
One couldalso employ unsupervised morphological analysis to automatically identify and segment affixes, whichwill provide deeper insight into the morpho-syntactic properties of the language.
Nevertheless, affixes(like infixes in Arabic or case-marking suffixes in Bangla) are technically not considered as adposi-tions, and therefore, they do not really determine the adposition typology.
Languages are divided intofour classes according to their adposition usage: prepositional, postpositional, ambi-positional (use bothtypes) and adposition-less (use none).
Thus, as far as adposition typology is concerned, it suffices toidentify whether a language is primarily adposition-less, which our technique is potentially capable ofdoing (we demonstrate it for four languages, but we believe more experimentation is needed to establishthis claim).
Note that a language may use case-marking affixes along with adpositions.
In such cases ourmethod is able to correctly determine the typology, as demonstrated for Bangla.4.3 Experimental variationsWe repeated the above experiments with lists of TCC, LCC and RCC instead of the co-occurrence en-tropies.
The performance was found to be poorer than the entropy lists, with nine classification errorsinstead of the earlier four.
Performance of these lists by co-occurrence counts was found to be poorer inother cases as well (Table 4).
We systematically experimented with r instead of ?.
To test the perfor-mance of our method with even smaller corpora, we sub-sampled 3 and 30 corpora containing 100k and10k sentences respectively from the 300k corpus.
We computed the correlation between the original top200 words obtained using TCE (or TCC) from the 300k corpus and the corresponding LCE and RCE (orLCC and RCC) lists obtained from the smaller corpora.
For a given language, the mean of ?
(Difference)and r(Difference) were used to predict the typology (observed standard deviations were very low, of theorder of 10?3).
The results of these experiments are summarized in Table 4.
Out of 23 languages, 21 and19 were correctly classified by ?
for corpora of 10k and 100k sentences.
The corresponding number forr are 18 for both 10k and 100k, and 17 for 300k corpora.
Thus, the sensitivity of the method improveswith slightly smaller corpora, provided that the TCE list, which is being used as a proxy for the goldstandard function word list, is computed from a slightly larger corpus.
Finally, we note that using Spear-man?s rank correlation coefficient with lists constructed by co-occurrence entropy consistently producesthe best results.5 Conclusions and Future WorkKnowing the adposition typology of a natural language can be useful in several NLP tasks, and can beespecially useful in understanding new or undeciphered languages.
In this research, we have taken one1044of the first steps towards automatic discovery of adposition typology.
First, we have shown, through ex-periments on two prepositional and two postpositional languages, that function words can be effectivelyextracted from medium-sized corpora using word co-occurrence statistics, and such measures usuallyoutperform simple frequency when used for the same task.
Next, difference in behavior of various co-occurrence statistics for prepositional and postpositional languages has been exploited to devise a simplestrategy for predicting the adposition typology of a language.
Simple differences of rank correlationcoefficients among total, left and right word co-occurrence entropies have been shown to be potent sig-nals towards automatic discovery of adposition and noun phrase typology in a language.
Results showsufficient promise through an extensive evaluation over 23 languages.We ventured into this study while solving a very practical and important problem: query understandingthrough analysis of the structure of Web search queries.
While queries seem to have an emergent syntax,it is unclear whether they have function words, and if so what role they play in determining the querygrammar.
To this end, we conducted the current study.
Thus, we envisage that this technique will beapplicable for any such emergent linguistic system, such as pidgins, creoles, and computer mediatedcommunications (CMCs) like SMS and chats, where there is a large amount of text data available butthe grammar is emerging or yet to be analyzed.
Other examples are that of undeciphered languages,e.g., Indus valley language or script.
In fact, our method can be applied to any system of symbols, be itlinguistic or non-linguistic, such as musical note sequences.As future work, it is important to improve our prediction accuracy further, while including morelanguages in the experimental setup.
Combining clues from other sources to resolve uncertain casesand devising better ways of choosing corpus size and significance thresholds are some of the avenues inwhich effort may be channelized.
Extending our approach to a morpheme-level analysis would also bebeneficial in dealing with highly agglutinative and inflectional languages.AcknowledgementsThe first author was supported by Microsoft Corporation and Microsoft Research India under the Mi-crosoft Research India PhD Fellowship Award.
We would like to thank Amritayan Nayak, WalmarteCommerce (who was then a student of IIT Kharagpur working as an intern at Microsoft Research India)for contributing to some of the early experiments related to this study.ReferencesRobin Allan, Philip Holmes, and Tom Lundsk?r-Nielsen.
1995.
Danish: A Comprehensive Grammar.
Routledge,London.R.
E. Asher.
1982.
Tamil, volume 7 of Lingua Descriptive Studies.
North-Holland, Amsterdam.Churamani Bandhu.
1973.
Clause patterns in nepali.
In Austin Hale, editor, Clause, sentence, and discoursepatterns in selected languages of Nepal 2, volume 40.2 of Summer Institute of Linguistics Publications inLinguistics and Related Fields, pages 1?79.
Summer Institute of Linguistics of the University of Oklahoma,Norman.Emily M. Bender and D Terence Langendoen.
2010.
Computational linguistics in support of linguistic theory.Linguistic Issues in Language Technology, 3(1).Dana Bielec.
1998.
Polish: An Essential Grammar.
Routledge, London.F.
M. Borras and R. F. Christian.
1959.
Russian Syntax: Aspects of Modern Russian Syntax and Vocabulary.Clarendon Press, Oxford.George Cardona.
1965.
A Gujarati Reference Grammar.
The University of Pennsylvania Press, Philadelphia.Leonardas Dambriunas, Antanas Klimas, and William R. Schmalstieg.
1966.
Introduction to Modern Lithuanian.Franciscan Fathers Press, Brooklyn.Hal Daum?e and Lyle Campbell.
2007.
A bayesian model for discovering typological implications.
In AnnualMeeting of the Association for Computational Linguistics, pages 65?72.1045Matthew S. Dryer and Martin Haspelmath, editors.
2011.
The World Atlas of Language Structures Online.
MaxPlanck Digital Library, Munich, 2011 edition.Victor A. Friedman.
1993.
Macedonian.
In Bernard Comrie and Greville G. Corbett, editors, The SlavonicLanguages, pages 249?305.
Routledge, London / New York.Joseph H. Greenberg.
1963.
Some universals of grammar with particular reference to the order of meaningfulelements.
Universals of language, 2:73?113.Aria Haghighi and Dan Klein.
2007.
Unsupervised coreference resolution in a nonparametric bayesian model.
InAnnual meeting-Association for Computational Linguistics, pages 848?855.Harald Hammarstr?om, Christina Thornell, Malin Petzell, and Torbj?orn Westerlund.
2008.
Bootstrapping languagedescription: The case of mpiemo (bantu a, central african republic).
In Proceedings of the Sixth internationalconference on Language Resources and Evaluation, LREC ?08.John Hinds.
1986.
Japanese, volume 4 of Croom Helm Descriptive Grammars.
Croom Helm, Routledge, London.Philip Holmes and Ian Hinchliffe.
1994.
Swedish: A Comprehensive Grammar.
Routledge, London.Istvn Kenesei, Robert M. Vago, and Anna Fenyvesi.
1998.
Hungarian.
Descriptive Grammars.
Routledge, London/ New York.Herbert Lederer.
1969.
Reference Grammar of the German Language.
Charles Scribner?s Sons, New York.
Basedon Grammatik der Deutschen Sprache, by Doras Schulz and Heinz Griesbach.W.
Lewis and F. Xia.
2008.
Automatically identifying computationally relevant typological features.
In Proc.
ofthe Third International Joint Conference on Natural Language Processing (IJCNLP-2008).R.
S. McGregor.
1977.
Outline of Hindi Grammar.
Oxford University Press, Delhi.
2nd edition.Irit Meir, Wendy Sandler, Carol Padden, and Mark Aronoff.
2010.
Emerging sign languages.
Oxford handbook ofdeaf studies, language, and education, 2:267?280.R.
C. Moore and C. Quirk.
2007.
An iteratively-trained segmentation-free phrase translation model for statisticalmachine translation.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 112?119.
Association for Computational Linguistics.Julius E. Olson.
1901.
Norwegian Grammar and Reader.
Scott, Foresman and Co, Chicago.Thomas G. Palaima.
1989.
Cypro-minoan scripts: Problems of historical context in problems in decipherment.Biblioth`eque des Cahiers de l?Institut de Linguistique de Louvain, 49:121?187.T.
M. S. Priestly.
1993.
Slovene.
In Bernard Comrie and Greville G. Corbett, editors, The Slavonic Languages,pages 388?451.
Routledge, London.R.P.N.
Rao, N. Yadav, M.N.
Vahia, H. Joglekar, R. Adhikari, and I. Mahadevan.
2009.
Entropic evidence forlinguistic structure in the Indus script.
Science, 324(5931):1165?1165.Rishiraj Saha Roy, Monojit Choudhury, and Kalika Bali.
2012.
Are web search queries an evolving protolan-guage?
In Proceedings of the 9th International Conference on the Evolution of Language, Evolang 9, pages304?311, Singapore.
World Scientific Publishing Co.Charles Marquard Sauer.
1891.
Italian Conversational Grammar.
Julius Gross, Heidelberg.Ernest A. Scatton.
1984.
A Reference Grammar of Modern Bulgarian.
Slavica Publishers, Columbus, Ohio.Elizabeth Selkirk.
1996.
The Prosodic Structure of Function Words.
In James L. Morgan and Katherine Demuth,editors, Signal to Syntax: Bootstrapping from Speech to Grammar in Early Acquisition.
Routledge.William Z. Shetter.
1958.
Introduction to Dutch.
Martinus Nijhoff, The Hague.J.
W. Stechishin.
1958.
Ukrainian Grammar.
Trident Press, Winnipeg.Helena Sulkala and Merja Karjalainen.
1992.
Finnish.
Descriptive Grammar Series.
Routledge, London.Valter Tauli.
1983.
Standard Estonian Grammar.
Volume 2: Syntax, volume 14 of Studia Uralica et AltaicaUpsaliensia.
Almqvist and Wiksell, Uppsala.Robert Underhill.
1976.
Turkish Grammar.
Massachusetts Institute of Technology (MIT) Press, Cambridge.1046
