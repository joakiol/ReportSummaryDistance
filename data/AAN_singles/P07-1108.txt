Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 856?863,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsPivot Language Approach for Phrase-Based Statistical MachineTranslationHua Wu and Haifeng WangToshiba (China) Research and Development Center5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng DistrictBeijing, 100738, China{wuhua,wanghaifeng}@rdc.toshiba.com.cnAbstractThis paper proposes a novel method forphrase-based statistical machine translationby using pivot language.
To conduct trans-lation between languages Lf and Le with asmall bilingual corpus, we bring in a thirdlanguage Lp, which is named the pivot lan-guage.
For Lf-Lp and Lp-Le, there existlarge bilingual corpora.
Using only Lf-Lpand Lp-Le bilingual corpora, we can build atranslation model for Lf-Le.
The advantageof this method lies in that we can performtranslation between Lf and Le even if thereis no bilingual corpus available for thislanguage pair.
Using BLEU as a metric,our pivot language method achieves an ab-solute improvement of 0.06 (22.13% rela-tive) as compared with the model directlytrained with 5,000 Lf-Le sentence pairs forFrench-Spanish translation.
Moreover, witha small Lf-Le bilingual corpus available,our method can further improve the transla-tion quality by using the additional Lf-Lpand Lp-Le bilingual corpora.1 IntroductionFor statistical machine translation (SMT), phrase-based methods (Koehn et al, 2003; Och and Ney,2004) and syntax-based methods (Wu, 1997; Al-shawi et al 2000; Yamada and Knignt, 2001;Melamed, 2004; Chiang, 2005; Quick et al, 2005;Mellebeek et al, 2006) outperform word-basedmethods (Brown et al, 1993).
These methods needlarge bilingual corpora.
However, for some lan-guages pairs, only a small bilingual corpus isavailable, which will degrade the performance ofstatistical translation systems.To solve this problem, this paper proposes anovel method for phrase-based SMT by using apivot language.
To perform translation betweenlanguages Lf and Le, we bring in a pivot languageLp, for which there exist large bilingual corpora forlanguage pairs Lf-Lp and Lp-Le.
With the Lf-Lp andLp-Le bilingual corpora, we can build a translationmodel for Lf-Le by using Lp as the pivot language.We name the translation model pivot model.
Theadvantage of this method lies in that we can con-duct translation between Lf and Le even if there isno bilingual corpus available for this language pair.Moreover, if a small corpus is available for Lf-Le,we build another translation model, which isnamed standard model.
Then, we build an interpo-lated model by performing linear interpolation onthe standard model and the pivot model.
Thus, theinterpolated model can employ both the small Lf-Le corpus and the large Lf-Lp and Lp-Le corpora.We perform experiments on the Europarl corpus(Koehn, 2005).
Using BLEU (Papineni et al, 2002)as a metric, our method achieves an absolute im-provement of 0.06 (22.13% relative) as comparedwith the standard model trained with 5,000 Lf-Lesentence pairs for French-Spanish translation.
Thetranslation quality is comparable with that of themodel trained with a bilingual corpus of 30,000 Lf-Le sentence pairs.
Moreover, translation quality isfurther boosted by using both the small Lf-Le bilin-gual corpus and the large Lf-Lp and Lp-Le corpora.Experimental results on Chinese-Japanese trans-lation also indicate that our method achieves satis-factory results using English as the pivot language.856The remainder of this paper is organized as fol-lows.
In section 2, we describe the related work.Section 3 briefly introduces phrase-based SMT.Section 4 and Section 5 describes our method forphrase-based SMT using pivot language.
We de-scribe the experimental results in sections 6 and 7.Lastly, we conclude in section 8.2 Related WorkOur method is mainly related to two kinds ofmethods: those using pivot language and thoseusing a small bilingual corpus or scarce resources.For the first kind, pivot languages are employedto translate queries in cross-language informationretrieval (CLIR) (Gollins and Sanderson, 2001;Kishida and Kando, 2003).
These methods onlyused the available dictionaries to perform word byword translation.
In addition, NTCIR 4 workshoporganized a shared task for CLIR using pivot lan-guage.
Machine translation systems are used totranslate queries into pivot language sentences, andthen into target sentences (Sakai et al, 2004).Callison-Burch et al (2006) used pivot lan-guages for paraphrase extraction to handle the un-seen phrases for phrase-based SMT.
Borin (2000)and Wang et al (2006) used pivot languages toimprove word alignment.
Borin (2000) used multi-lingual corpora to increase alignment coverage.Wang et al (2006) induced alignment models byusing two additional bilingual corpora to improveword alignment quality.
Pivot Language methodswere also used for translation dictionary induction(Schafer and Yarowsky, 2002), word sense disam-biguation (Diab and Resnik, 2002), and so on.For the second kind, Niessen and Ney (2004)used morpho-syntactic information for translationbetween language pairs with scarce resources.Vandeghinste et al (2006) used translation dic-tionaries and shallow analysis tools for translationbetween the language pair with low resources.
Ashared task on word alignment was organized aspart of the ACL 2005 Workshop on Building andUsing Parallel Texts (Martin et al, 2005).
Thistask focused on languages with scarce resources.For the subtask of unlimited resources, some re-searchers (Aswani and Gaizauskas, 2005; Lopezand Resnik, 2005; Tufis et al, 2005) used lan-guage-dependent resources such as dictionary, the-saurus, and dependency parser to improve wordalignment results.In this paper, we address the translation problemfor language pairs with scarce resources by bring-ing in a pivot language, via which we can makeuse of large bilingual corpora.
Our method doesnot need language-dependent resources or deeplinguistic processing.
Thus, the method is easy tobe adapted to any language pair where a pivot lan-guage and corresponding large bilingual corporaare available.3 Phrase-Based SMTAccording to the translation model presented in(Koehn et al, 2003), given a source sentence f ,the best target translation beste  can be obtainedaccording to the following model)()()|(maxarg)|(maxargeeeeeffeelengthLMbest?ppp==(1)Where the translation model )|( efp can bedecomposed into?=??=IiiiiiiiIIaefpbadefefp1111),|()()|()|(??
w(2)Where )|( ii ef?
and )( 1??
ii bad  denote phrasetranslation probability and distortion probability,respectively.
),|( aefp iiw  is the lexical weight,and ?
is the strength of the lexical weight.4 Phrase-Based SMT Via Pivot LanguageThis section will introduce the method that per-forms phrase-based SMT for the language pair Lf-Le by using the two bilingual corpora of Lf-Lp andLp-Le.
With the two additional bilingual corpora,we train two translation models for Lf-Lp and Lp-Le,respectively.
Based on these two models, we builda pivot translation model for Lf-Le, with Lp as apivot language.According to equation (2), the phrase translationprobability and the lexical weight are languagedependent.
We will introduce them in sections 4.1and 4.2, respectively.4.1 Phrase Translation ProbabilityUsing the Lf-Lp and Lp-Le bilingual corpora, wetrain two phrase translation probabilities857)|( ii pf?
and )|( ii ep?
, where ip  is the phrasein the pivot language Lp.
Given the phrasetranslation probabilities )|( ii pf?
and )|( ii ep?
,we obtain the phrase translation probability)|( ii ef?
according to the following model.
?=ipiiiiiii epepfef )|(),|()|( ???
(3)The phrase translation probability ),|( iii epf?does not depend on the phase ie  in the language Le,since it is estimated from the Lf-Lp bilingual corpus.Thus, equation (3) can be rewritten as?=ipiiiiii eppfef )|()|()|( ???
(4)4.2 Lexical WeightGiven a phrase pair ),( ef  and a word alignmenta  between the source word positions ni ,...,1=and the target word positions mj ,...,1= , thelexical weight can be estimated according to thefollowing method (Koehn et al, 2003).?
?= ??
?=ni ajiji efwajijaefp1 ),()|(),(|1),|(w(5)In order to estimate the lexical weight, we firstneed to obtain the alignment information a  be-tween the two phrases f  and e , and then estimatethe lexical translation probability )|( efw  accord-ing to the alignment information.
The alignmentinformation of the phrase pair ),( ef  can be in-duced from the two phrase pairs ),( pf  and ),( ep .Figure 1.
Alignment Information InductionLet 1a  and 2a  represent the word alignment in-formation inside the phrase pairs ),( pf  and ),( eprespectively, then the alignment information ainside ),( ef  can be obtained as shown in (6).
Anexample is shown in Figure 1.
}),(&),(:|),{( 21 aepapfpefa ??
?=  (6)With the induced alignment information, thispaper proposes a method to estimate the probabil-ity directly from the induced phrase pairs.
Wename this method phrase method.
If we use K todenote the number of the induced phrase pairs, weestimate the co-occurring frequency of the wordpair ),( ef  according to the following model.?
?===niaiKkk ieeffefefcount11),(),()|(),(???
(7)Where )|( efk?
is the phrase translation probabil-ity for phrase pair k .
1),( =yx?
if yx = ; other-wise, 0),( =yx?
.
Thus, lexical translation prob-ability can be estimated as in (8).
?='),'(),()|(fefcountefcountefw  (8)We also estimate the lexical translation prob-ability )|( efw  using the method described in(Wang et al, 2006), which is shown in (9).
Wenamed it word method in this paper.
);,()|()|()|( pefsimepwpfwefwp?= (9)Where )|( pfw  and )|( epw  are two lexicalprobabilities, and );,( pefsim  is the cross-language word similarity.5 Interpolated ModelIf we have a small Lf-Le bilingual corpus, we canemploy this corpus to estimate a translation modelas described in section 3.
However, this model mayperform poorly due to the sparseness of the data.
Inorder to improve its performance, we can employthe additional Lf-Lp and Lp-Le bilingual corpora.Moreover, we can use more than one pivot lan-guage to improve the translation performance if thecorresponding bilingual corpora exist.
Differentpivot languages may catch different linguistic phe-858nomena, and improve translation quality for thedesired language pair Lf-Le in different ways.If we include n  pivot languages, n  pivot mod-els can be estimated using the method as describedin section 4.
In order to combine these n  pivotmodels with the standard model trained with theLf-Le corpus, we use the linear interpolationmethod.
The phrase translation probability and thelexical weight are estimated as shown in (10) and(11), respectively.
?==niii efef0)|()|( ???
(10)?==niii aefpaefp0),|(),|( w,w ?
(11)Where )|(0 ef?
and ),|( aefpw,0  denote thephrase translation probability and lexical weighttrained with the Lf-Le bilingual corpus, respec-tively.
)|( efi?
and ),|( aefp iw,  ( ni ,...,1= ) arethe phrase translation probability and lexicalweight estimated by using the pivot languages.
i?and i?
are the interpolation coefficients.6 Experiments on the Europarl Corpus6.1 DataA shared task to evaluate machine translation per-formance was organized as part of theNAACL/HLT 2006 Workshop on Statistical Ma-chine Translation (Koehn and Monz, 2006).
Theshared task used the Europarl corpus (Koehn,2005), in which four languages are involved: Eng-lish, French, Spanish, and German.
The shared taskperformed translation between English and theother three languages.
In our work, we performtranslation from French to the other three lan-guages.
We select French to Spanish and French toGerman translation that are not in the shared taskbecause we want to use English as the pivot lan-guage.
In general, for most of the languages, thereexist bilingual corpora between these languagesand English since English is an internationallyused language.Table 1 shows the information about the bilin-gual training data.
In the table, "Fr", "En", "Es",and "De" denotes "French", "English", "Spanish",and "German", respectively.
For the language pairsLf-Le not including English, the bilingual corpus isLanguagePairsSentencePairsSourceWordsTargetWordsFr-En 688,031 15,323,737 13,808,104Fr-Es 640,661 14,148,926 13,134,411Fr-De 639,693 14,215,058 12,155,876Es-En 730,740 15,676,710 15,222,105De-En 751,088 15,256,793 16,052,269De-Es 672,813 13,246,255 14,362,615Table 1.
Training Corpus for European Languagesextracted from Lf-English and English-Le sinceEuroparl corpus is a multilingual corpus.For the language models, we use the same dataprovided in the shared task.
We also use the samedevelopment set and test set provided by the sharedtask.
The in-domain test set includes 2,000 sen-tences and the out-of-domain test set includes1,064 sentences for each language.6.2 Translation System and EvaluationMethodTo perform phrase-based SMT, we use Koehn'straining scripts1 and the Pharaoh decoder (Koehn,2004).
We run the decoder with its default settingsand then use Koehn's implementation of minimumerror rate training (Och, 2003) to tune the featureweights on the development set.The translation quality was evaluated using awell-established automatic measure: BLEU score(Papineni et al, 2002).
And we also use the toolprovided in the NAACL/HLT 2006 shared task onSMT to calculate the BLEU scores.6.3 Comparison of Different Lexical WeightsAs described in section 4, we employ two methodsto estimate the lexical weight in the translationmodel.
In order to compare the two methods, wetranslate from French to Spanish, using English asthe pivot language.
We use the French-English andEnglish-Spanish corpora described in Table 1 astraining data.
During training, before estimatingthe Spanish to French phrase translation probabil-ity, we filter those French-English and English-Spanish phrase pairs whose translation probabili-ties are below a fixed threshold 0.001.2 The trans-lation results are shown in Table 2.1  It is located at http://www.statmt.org/wmt06/shared-task/baseline.htm2 In the following experiments using pivot languages, we usethe same filtering threshold for all of the language pairs.859The phrase method proposed in this paper per-forms better than the word method proposed in(Wang et al, 2006).
This is because our methoduses phrase translation probability as a confidenceweight to estimate the lexical translation probabil-ity.
It strengthens the frequently aligned pairs andweakens the infrequently aligned pairs.
Thus, thefollowing sections will use the phrase method toestimate the lexical weight.Method In-Domain Out-of-DomainPhrase  0.3212 0.2098Word 0.2583 0.1672Table 2.
Results with Different Lexical Weights6.4 Results of Using One Pivot LanguageThis section describes the translation results byusing only one pivot language.
For the languagepair French and Spanish, we use English as thepivot language.
The entire French-English andEnglish-Spanish corpora as described in section 4are used to train a pivot model for French-Spanish.As described in section 5, if we have a small Lf-Le bilingual corpus and large Lf-Lp and Lp-Le bilin-gual corpora, we can obtain interpolated models.In order to conduct the experiments, we ran-domly select 5K, 10K, 20K, 30K, 40K, 50K, and100K sentence pairs from the French-Spanish cor-pus.
Using each of these corpora, we train a stan-dard translation model.For each standard model, we interpolate it withthe pivot model to get an interpolated model.
Theinterpolation weights are tuned using the develop-ment set.
For all the interpolated models, we set9.00 =?
, 1.01 =?
, 9.00 =?
, and 1.01 =?
.
Wetest the three kinds of models on both the in-domain and out-of-domain test sets.
The results areshown in Figures 2 and 3.The pivot model achieves BLEU scores of0.3212 and 0.2098 on the in-domain and out-of-domain test set, respectively.
It achieves an abso-lute improvement of 0.05 on both test sets (16.92%and 35.35% relative) over the standard modeltrained with 5,000 French-Spanish sentence pairs.And the performance of the pivot models are com-parable with that of the standard models trainedwith 20,000 and 30,000 sentence pairs on the in-domain and out-of-domain test set, respectively.When the French-Spanish training corpus is in-creased, the standard models quickly outperformthe pivot model.252729313335375 10 20 30 40 50 100Fr-Es Data (k pairs)BLEU(%)InterpolatedStandardPivotFigure 2.
In-Domain French-Spanish Results141618202224265 10 20 30 40 50 100Fr-Es Data (K pairs)BLEU(%)InterpolatedStandardPivotFigure 3.
Out-of-Domain French-Spanish Results182022242628305 10 20 30 40 50 100Fr-En Data (k Pairs)BLEU(%)InterpolatedStandardPivotFigure 4.
In-Domain French-English Results910111213141516175 10 20 30 40 50 100Fr-De Data (k Pairs)BLEU(%)InterpolatedStandardPivotFigure 5.
In-Domain French-German ResultsWhen only a very small French-Spanish bilin-gual corpus is available, the interpolated methodcan greatly improve the translation quality.
Forexample, when only 5,000 French-Spanish sen-tence pairs are available, the interpolated modeloutperforms the standard model by achieving arelative improvement of 17.55%, with the BLEUscore improved from 0.2747 to 0.3229.
With50,000 French-Spanish sentence pairs available,the interpolated model significantly3 improves thetranslation quality by achieving an absolute im-3 We conduct the significance test using the same method asdescribed in (Koehn and Monz, 2006).860provement of 0.01 BLEU.
When the French-Spanish training corpus increases to 100,000 sen-tence pairs, the interpolated model achieves almostthe same result as the standard model.
This indi-cates that our pivot language method is suitable forthe language pairs with small quantities of trainingdata available.Besides experiments on French-Spanish transla-tion, we also conduct translation from French toEnglish and French to German, using German andEnglish as the pivot language, respectively.
Theresults on the in-domain test set4 are shown in Fig-ures 4 and 5.
The tendency of the results is similarto that in Figure 2.6.5 Results of Using More Than One PivotLanguageFor French to Spanish translation, we also intro-duce German as a pivot language besides English.Using these two pivot languages, we build two dif-ferent pivot models, and then perform linear inter-polation on them.
The interpolation weights for theEnglish pivot model and the German pivot modelare set to 0.6 and 0.4 respectively5.
The translationresults on the in-domain test set are 0.3212, 0.3077,and 0.3355 for the pivot models using English,German, and both German and English as pivotlanguages, respectively.With the pivot model using both English andGerman as pivot languages, we interpolate it withthe standard models trained with French-Spanishcorpora of different sizes as described in the abovesection.
The comparison of the translation resultsamong the interpolated models, standard models,and the pivot model are shown in Figure 6.It can be seen that the translation results can befurther improved by using more than one pivotlanguage.
The pivot model "Pivot-En+De" usingtwo pivot languages achieves an absolute im-provement of 0.06 (22.13% relative) as comparedwith the standard model trained with 5,000 sen-tence pairs.
And it achieves comparable translationresult as compared with the standard model trainedwith 30,000 French-Spanish sentence pairs.The results in Figure 6 also indicate the interpo-lated models using two pivot languages achieve the4 The results on the out-of-domain test set are similar to that inFigure 3.
We only show the in-domain translation results in allof the following experiments because of space limit.5 The weights are tuned on the development set.best results of all.
Significance test shows that theinterpolated models using two pivot languages sig-nificantly outperform those using one pivot lan-guage when less than 50,000 French-Spanish sen-tence pairs are available.27282930313233343536375 10 20 30 40 50 100Fr-Es Data (k Pairs)BLEU(%)Interpolated-En+DeInterpolated-EnInterpolated-DeStandardPivot-En+DeFigure 6.
In-Domain French-Spanish TranslationResults by Using Two Pivot Languages6.6 Results by Using Pivot Language RelatedCorpora of Different SizesIn all of the above results, the corpora used to trainthe pivot models are not changed.
In order to ex-amine the effect of the size of the pivot corpora,we decrease the French-English and English-French corpora.
We randomly select 200,000 and400,000 sentence pairs from both of them to traintwo pivot models, respectively.
The translationresults on the in-domain test set are 0.2376, 0.2954,and 0.3212 for the pivot models trained with200,000, 400,000, and the entire French-Englishand English-Spanish corpora, respectively.
Theresults of the interpolated models and the standardmodels are shown in Figure 7.
The results indicatethat the larger the training corpora used to train thepivot model are, the better the translation quality is.27282930313233343536375 10 20 30 40 50 100Fr-Es Data (k pairs)BLEU(%)Interpolated-Allinterpolated-400kInterpolated-200kStandardFigure 7.
In-Domain French-Spanish Results byUsing Lf-Lp and Lp-Le Corpora of Different Sizes8617 Experiments on Chinese to JapaneseTranslationIn section 6, translation results on the Europarlmultilingual corpus indicate the effectiveness ofour method.
To investigate the effectiveness of ourmethod by using independently sourced parallelcorpora, we conduct Chinese-Japanese translationusing English as a pivot language in this section,where the training data are not limited to a specificdomain.The data used for this experiment is the same asthose used in (Wang et al, 2006).
There are 21,977,329,350, and 160,535 sentence pairs for the lan-guage pairs Chinese-Japanese, Chinese-English,and English-Japanese, respectively.
The develop-ment data and testing data include 500 and 1,000Chinese sentences respectively, with one referencefor each sentence.
For Japanese language modeltraining, we use about 100M bytes Japanese corpus.The translation result is shown in Figure 8.
Thepivot model only outperforms the standard modeltrained with 2,500 sentence pairs.
This is because(1) the corpora used to train the pivot model aresmaller as compared with the Europarl corpus; (2)the training data and the testing data are not limitedto a specific domain; (3) The languages are notclosely related.6810121416182.5 5 10 21.9Chinese-Japanese Data (k pairs)BLEU(%)InterpolatedStandardPivotFigure 8.
Chinese-Japanese Translation ResultsThe interpolated models significantly outper-form the other models.
When only 5,000 sentencepairs are available, the BLEU score increases rela-tively by 20.53%.
With the entire (21,977 pairs)Chinese-Japanese available, the interpolated modelrelatively increases the BLEU score by 5.62%,from 0.1708 to 0.1804.8 ConclusionThis paper proposed a novel method for phrase-based SMT on language pairs with a small bilin-gual corpus by bringing in pivot languages.
To per-form translation between Lf and Le, we bring in apivot language Lp, via which the large corpora ofLf-Lp and Lp-Le can be used to induce a translationmodel for Lf-Le.
The advantage of this method isthat it can perform translation between the lan-guage pair Lf-Le even if no bilingual corpus for thispair is available.
Using BLEU as a metric, ourmethod achieves an absolute improvement of 0.06(22.13% relative) as compared with the model di-rectly trained with 5,000 sentence pairs for French-Spanish translation.
And the translation quality iscomparable with that of the model directly trainedwith 30,000 French-Spanish sentence pairs.
Theresults also indicate that using more pivot lan-guages leads to better translation quality.With a small bilingual corpus available for Lf-Le,we built a translation model, and interpolated itwith the pivot model trained with the large Lf-Lpand Lp-Le bilingual corpora.
The results on boththe Europarl corpus and Chinese-Japanese transla-tion indicate that the interpolated models achievethe best results.
Results also indicate that our pivotlanguage approach is suitable for translation onlanguage pairs with a small bilingual corpus.
Theless the Lf-Le bilingual corpus is, the bigger theimprovement is.We also performed experiments using Lf-Lp andLp-Le corpora of different sizes.
The results indi-cate that using larger training corpora to train thepivot model leads to better translation quality.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning Dependency Translation Models asCollections of Finite-State Head Transducers.
Com-putational Linguistics, 26(1):45-60.Niraj Aswani and Robert Gaizauskas.
2005.
AligningWords in English-Hindi Parallel Corpora.
In Proc.
ofthe ACL 2005 Workshop on Building and Using Par-allel Texts: Data-driven Machine Translation andBeyond, pages 115-118.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics, 19(2):263-311.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Transla-862tion Using Paraphrases.
In Proc.
of NAACL-2006,pages 17-24.Lars Borin.
2000.
You'll Take the High Road and I'llTake the Low Road: Using a Third Language to Im-prove Bilingual Word Alignment.
In Proc.
of COL-ING-2000, pages 97-103.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.of ACL-2005, pages 263-270.Mona Diab  and  Philip Resnik.
2002.
An UnsupervisedMethod for Word Sense Tagging using Parallel Cor-pora.
In Proc.
of ACL-2002, pages 255-262.Tim Gollins and Mark Sanderson.
2001.
ImprovingCross Language Information Retrieval with Triangu-lated Translation.
In Proc.
of ACM SIGIR-2001,pages 90-95.Kazuaki Kishida and Noriko Kando.
2003.
Two-StageRefinement of Query Translation in a Pivot Lan-guage Approach to Cross-Lingual Information Re-trieval: An Experiment at CLEF 2003.
In Proc.
ofCLEF-2003.
pages 253-262.Philipp Koehn.
2004.
Pharaoh: A Beam Search Decoderfor Phrase-Based Statistical Machine TranslationModels.
In Proc.
of AMTA-2004, pages 115-124.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proc.
of MTSummit X, pages 79-86.Philipp Koehn and Christof Monz.
2006.
Manual andAutomatic Evaluation of Machine Translation be-tween European Languages.
In Proc.
of the 2006HLT-NAACL Workshop on Statistical MachineTranslation, pages 102-121.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of HLT-NAAC- 2003, pages 127-133.Adam Lopez and Philip Resnik.
2005.
Improved HMMAlignment Models for Languages with Scarce Re-sources.
In Proc.
of the ACL-2005 Work-shop onBuilding and Using Parallel Texts: Data-driven Ma-chine Translation and Beyond, pages 83-86.Joel Martin, Rada Mihalcea, and Ted Pedersen.
2005.Word Alignment for Languages with Scarce Re-sources.
In Proc.
of the ACL-2005 Workshop onBuilding and Using Parallel Texts: Data-driven Ma-chine Translation and Beyond, pages 65-74.Dan Melamed.
2004.
Statistical Machine Translation byParsing.
In Proc.
of ACL-2004, pages 653-660.Bart Mellebeek, Karolina Owczarzak, Declan Groves,Josef Van Genabith, and Andy Way.
2006.
A Syntac-tic Skeleton for Statistical Machine Translation.
InProc.
of EAMT-2006, pages 195-202.Sonja Niessen and Hermann Ney.
2004.
StatisticalMachine Translation with Scarce Resources UsingMorpho-Syntactic Information.
Computationallinguistics, 30(2): 181-204.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of ACL-2003, pages 160-167.Franz Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical MachineTranslation.
Computational Linguistics, 30(4):417-449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proc.
of ACL-2002, pages 311-318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency Treelet Translation: Syntactically In-formed Phrasal SMT.
In Proc.
of ACL-2005, pages271-279.Tetsuya Sakai, Makoto Koyama, Akira Kumano, andToshihiko Manabe.
2004.
Toshiba BRIDJE atNTCIR-4 CLIR: Monolingual/Bilingual IR andFlexible Feedback.
In Proc.
of NTCIR 4.Charles Schafer and David Yarowsky.
2002.
InducingTranslation Lexicons via Diverse Similarity Meas-ures and Bridge Languages.
In Proc.
of CoNLL-2002,pages 1-7.Haifeng Wang, Hua Wu, and Zhanyi Liu.
2006.
WordAlignment for Languages with Scarce Resources Us-ing Bilingual Corpora of Other Language Pairs.
InProc.
of COLING/ACL-2006 Main ConferencePoster Sessions, pages 874-881.Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Ste-fanescu.
2005.
Combined Word Alignments.
In Proc.of the ACL-2005 Workshop on Building and UsingParallel Texts: Data-driven Machine Translation andBeyond, pages 107-110.Vincent Vandeghinste, Ineka Schuurman, Michael Carl,Stella Markantonatou, and Toni Badia.
2006.METIS-II: Machine Translation for Low-ResourceLanguages.
In Proc.
of LREC-2006, pages 1284-1289.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377-403.Kenji Yamada and Kevin Knight.
2001.
A Syntax BasedStatistical Translation Model.
In Proc.
of ACL-2001,pages 523-530.863
