Mining Linguistically Interpreted TextsCassiana Fagundes da Silva, Renata Vieira,Fernando Santos Os?rioPIPCA - UnisinosAv.
Unisinos, 950  - S?o Leopoldo, RSBrasil ?
93.022-000{cassiana, renata, osorio}@exatas.unisinos.brPaulo QuaresmaDepartamento de Inform?tica,Universidade de ?vora, 7000?vora - Portugal{pq}@di.uevora.ptAbstractThis paper proposes and evaluates the use oflinguistic information in the pre-processingphase of text mining tasks.
We present severalexperiments comparing our proposal forselection of terms based on linguisticknowledge with usual techniques applied inthe field.
The results show that part of speechinformation is useful for the pre-processingphase of text categorization and clustering, asan alternative for stop words and stemming.1 IntroductionNatural language texts can be viewed asresources containing uniform data in such a waythat methods similar to those used in Data BaseKnowledge Extraction can be applied to them.
Theadaptation of these methods to texts is known asText Mining (Tan, 1999).
Machine learningtechniques are applied to document collectionsaiming at extracting patterns that may be useful toorganize or recover information from thecollections.
Tasks related to this area are textcategorization, clustering, summarization, andinformation extraction.
One of the first steps intext mining tasks is the pre-processing of thedocuments, as they need to be represented in amore structured way.Our work proposes a new technique to the pre-processing phase of documents and we compare itwith usual pre-processing methods.
We focus ontwo text mining tasks, namely text categorizationand clustering.
In the categorization task weassociate each document to a class from a pre-defined set, in the clustering task the challenge isto identify groups of similar documents withoutbeing aware of pre-defined classes.
Usually, thepre-processing phase in these tasks are based onthe approach called bag-of-words, in which justsimple techniques are used to eliminateuninteresting words and to reduce varioussemantically related terms to the same root (stop-words and stemming, respectively).
As analternative, we propose the use of linguisticinformation in the pre-processing phase, byselecting words according to their category (nouns,adjectives, proper names, verbs) and using itscanonical form.
We ran a series of experiments toevaluate this proposal over Brazilian Portuguesetexts.This paper is organized as follows.
Section 2presents an overview of text mining.
Section 3presents the methods used for collecting thelinguistic knowledge used in the experiments.
Theexperiments themselves are described in Section 4.Section 5 presents an analysis of the results and thepaper is concluded in Section 6.2 Text MiningText mining processes are usually divided in fivemajor phases: A) Document collection: consists ofthe definition of the set of the documents fromwhich knowledge must be extracted.
B) Pre-processing: consists of a set of actions thattransform the set of documents in natural languageinto a list of useful terms.
C) Preparation andselection of the data: consists in the identificationand selection of relevant terms form the pre-processed ones.
D) Knowledge Extraction: consistsof the application of machine learning techniquesto identify patterns that can classify or cluster thedocuments in the collection.
E) Evaluation andinterpretation of the results: consists of theanalysis of the results.The pre-processing phase in text mining isessential and usually very expensive and timeconsuming.
As texts are originally non-structured aseries of steps are required to represent them in aformat compatible with knowledge extractionmethods and tools.
The usual techniques employedin phase B are the use of a list of stop-words,which are discarded from the original documentsand the use of stemming which reduces the wordsto their root.Having the proper tools to process Portuguesetexts, we investigate whether linguistic informationcan have an impact on the results of the wholeprocess.
In the next section we describe the toolswe used for acquiring the linguistic knowledge inwhich we base our experiments.3 Tools for acquiring linguistic knowledgeThe linguistic knowledge we use in theexperiments is based on the syntactic analysisperformed by the PALAVRAS parser (Bick,2000).
This Portuguese parser is robust enough toalways give an output even for incomplete orincorrect sentences (which might be the case forthe type of documents used in text mining tasks).
Ithas a comparatively low percentage of errors (lessthan 1% for word class and 3-4% for surfacesyntax) (Bick, 2003).
We also used another toolthat makes easier the extraction of features fromthe analyzed texts: the Palavras Xtractor (Gasperinet.
al.
2003).
This tool converts the parser outputinto three XML files, containing: a) the list of allwords from the text and their identifier; b) morpho-syntactic information for each word; c) thesentence?s syntactic structures.
Using XSL(eXtensible Stylesheet Language)1 we can extractspecified terms from the texts, according to theirlinguistic value.
The resulting lists of termsaccording to each combination are then passed tophases C, D and E. The experiments are describedin detail in the next section.4 Experiments4.1 CorpusThe corpus used in the experiments is composedby a subset of the NILC corpus (N?cleoInterdisciplinar de Ling?
?stica Computacional2)containing 855 documents corresponding tonewspaper articles of Folha de S?o Paulo from1994.
These documents are related to fivenewspaper sections: informatics, property, sports,politics and tourism.4.2 Pre-processing techniquesWe prepared three different versions of thecorpus (V1, V2 and V3) for 3-fold cross validation.Each version is partitioned in different training andtesting parts, containing 2/3 and 1/3 of thedocuments respectively.For the experiments with the usual methods,irrelevant terms (stop-words) were eliminated fromthe documents, on the basis of a list of stop-words,containing 476 terms (mainly articles, prepositions,auxiliary verbs, pronouns, etc).
The remainingterms were stemmed according to Martin Porter?salgorithm (Porter, 1980).
Based on these1Available in http://www.w3.org/Style/XSL/2Available in http://nilc.icmc.sc.usp.br/nilc/techniques we generated a collection of pre-processed documents called PD1.To test our proposal we then pre-processed the855 documents in a different way: we parsed alltexts of our corpus, generating the correspondingXML files and extracted terms according to theirgrammatical categories, using XSL.
Based on thesetechniques we generated a collection of pre-processed documents called PD2.4.2.1 Other mining phasesAll other text mining phases were equallyapplied to both PD1 and PD2.
We used relativefrequency for the selection of relevant terms.
Therepresentation of the documents was according tothe vector space model.
For the categorization task,vectors corresponding to each class were built,where the more frequent terms were selected.
Afterthat, a global vector was composed.
We also testedwith different numbers of terms in the globalvector (30, 60, 90, 120, 150).
For the clusteringtask we measured the similarity of the documentsusing cosine.
After calculating similarity of thedocuments, the data was codified according toformat required by the machine learning tool Weka(Witten, 2000).
Weka is a collection of machinelearning algorithms for data mining tasks thatcontains tools for data pre-processing,classification, regression, clustering, associationrules, and visualization.In this work the adopted machine learningtechniques are Decision Tree for the categorizationprocess and K-means for text clustering.Decision Tree is a supervised learning algorithmbased on the recursive division of the trainingexamples in representative subsets, using themetric of information gain.
After the induction of aclassifying tree, it can be applied to new examples,described with the same attributes of the trainingexamples.K-means divides a group of objects in k groupsin a way that the resulting intracluster similarity ishigh, but the intercluster similarity is low.
Thesimilarity of groups is measured in respect to themedium value of the objects in a group, which canbe seen as the center of gravity (centroid) of thegroup.
The parameters used to run k-means are thedefault ones as suggested by the tool, seed 10 and5 groups.The evaluation of the results for thecategorization task is based on the classificationerror, which was used to compare the results forPD1 and PD2.
For the clustering task theevaluation of the results is given by recall andprecision, based on the generated confusionmatrices.5 Results5.1 Text CategorizationTable 1 shows the results for text categorizationof PD1, given by the average error ratesconsidering the three versions the corpus (V1, V2and V3).
We had around 20% of error for thecategorization task.
We can see minor variations inthe results according to the size of the vectors.
Bestresults were obtained for 150 terms.Terms 30 60 90 120 150Errors 21,64 21,99 20,47 20,35 19,77Table 1: Average Classification Error for PD1%Table 2 shows the results for differentgrammatical combinations in PD2, while Figure 1summarizes the lowest error rates found for PD1and all groups of PD2.
The group nouns andadjectives presents the lower error rates of allexperiments (18,01).
However, due to the smallsize of the corpus, the improvement reportedbetween usual methods (18,01) and nouns-adjectives (20,47), when considering the samenumber of terms (90), are at 75-80% confidencelevel only (t-test).In general, the results show that the presence ofnouns is crucial, the worst classification errors arebased on groups that do not contain the categorynouns, and here the confidence level for thedifferences reported reaches 95%.
The groupscontaining nouns present results comparable tothose found in the experiments based on usualmethods of pre-processing.
The use of verbs, eitheralone or with other grammatical groups is not aninteresting option.Terms 30 60 90 120 150Nouns 24,91 21,75 23,98 23,51 22,69Nouns-adjec.
23,15 20,35 18,01 19,18 18,71Nouns-adjec.-proper names 20,82 22,92 20,94 21,05 21,17Nouns-propernames24,09 24,56 22,80 22,45 22,80Adjec.-propernames47,01 46,34 32,51 33,21 32,86Verbs 63,73 62,33 57,75 58,45 55,64Nouns-verbs 40 27,72 25,61 24,21 26,32Nouns-verbs-adjectives 35,09 27,02 27,72 24,21 23,51Table 2: Average Classification Error for PD2It can be observed that usually the best resultsare obtained when the documents are representedby a larger number of terms (90, 120 and 150), forthe group nouns, however, the best results wereobtained for vectors containing just 60 terms.Figure 1: Lower error rates for PD1 and PD2We looked at the terms resulting from differentselection methods and categories to check theoverlap among the groups.
From PD1 to PD2based on nouns and adjectives (the one with thebest results) we could see that we had around 50%of different terms.
That means that 50% of terms inPD1 are terms included in the categories nouns andadjectives and 50% of the terms selected on thebasis of stop-words and stemming are from othergrammatical categories.
As adjectives added tonouns improved the results, we checked adjectivesto figure out their significance.
We found termssuch as Brazilian, electoral, multimedia, political.Intuitively, these terms seem to be relevant for theclasses we had.
Analysing the groups containingverbs, we observed that the verbs are usually verycommon or auxiliary verbs (such as to be, to have,to say), therefore not relevant for classification.5.2 Text ClusteringWe tested our hypothesis through clusteringexperiments for PD1 and variations of PD2.
Forthe experiments on clustering we used vectorscontaining 150 features from V2 and we set k to 5groups.
The resulting confusion matrix for PD1 ispresented in Table 3.Cl.0 Cl.1 Cl.2 Cl.3 Cl.4Sp.
1  31 2 0 23Prop.
2 0 4 0 51Inf.
0 0 1 0 55Pol.
0 0 2 39 16Tour.
5 0 17 0 33Table 3: Confusion Matrix PD1 (150 terms)Considering the larger group in each row andcolumn (highlighted in the table) as the intendedcluster for each class, the   corresponding precisionis of 50,52%.We repeated the same set of experiments forPD2.
We tested several grammatical groups, thebest result was related to nouns and proper names.The results are shown in Tables 4.
Thecorresponding precision is 63,15%.Cl.0 Cl.1 Cl.2 Cl.3  Cl.4Sp.
0 38 19 0 0Prop.
11 0 44 1 1Inf.
0 0 19 0 38Pol.
0 1 20 36 0Tour.
0 0 57 0 0Table 4:  Confusion Matrix PD2 (nouns + propernames, 150 terms)6 ConclusionsThis paper presented a series of experimentsaiming at comparing our proposal of pre-processing techniques based on linguisticinformation with usual methods adopted for pre-processing in text mining.We find in the literature other alternativeproposals for the pre-processing phase of textmining.
(Gon?alves and Quaresma, 2003) use thecanonical form of the word instead stemming, forEuropean Portuguese.
(Feldman et al 1998)proposes the use of compound terms as opposed tosingle terms for text mining.
Similarly, (Aizawa,2001) uses morphological analysis to aid theextraction of compound terms.
Our approachdiffers from those since we propose single termsselection based on different part of speechinformation.The results show that a selection made solely onthe basis of category information produces resultsat least as good as those produced by usualmethods (when the selection considers nouns andadjectives or nouns and proper nouns) both incategorization and clustering tasks.
In thecategorization experiments we obtained the lowesterror rate for PD2 when the pre-processing phasewas based on the selection of nouns and adjectives,18,01%.
However, the second best score in thecase of categorization was achieved by thetraditional methods, 19,77%.
Due to the smallcorpus, further experiments are needed to verifythe statistical significance of  the reported gains.The results of the clustering experiments show adifference in precision from  50,52% to 63,15%.As we are planning to test our techniques with alarger number of documents and consequently alarger number of terms, we are consideringapplying other machine-learning techniques suchas Support Vector Machines that are robust enoughto deal with a large number of terms.
We are alsoplanning to apply more sophisticated linguisticknowledge than just grammatical categories, as, forinstance, the use of noun phrases for termsselection, since this information is provided by theparser PALAVRAS.
Other front for future work isfurther tests for other languages.ReferencesAizawa A., 2001.
Linguistic Techniques toImprove the Performance of Automatic TextCategorization.
Proc.
of the Sixth NaturalLanguage Processing Pacific Rim Symposium,pages 307-314.Bick, E. 2000.
The Parsing System PALAVRAS:Automatic Gramatical Analysis of Porutugese ina Constraint Grammar Framework.
?rhusUniversity.
?rhus: ?rhus University Press.Bick, E. 2003.
A Constraint Grammar BasedQuestion Answering System for Portuguese.Proceedings of the 11?
Portuguese Conferenceon Artificial Intelligence, pages 414-418.
LNAISpringer Verlag.Feldman R., et al 1998.
Text Mining at the TermLevel.
Proc.
of the Second European Symposiumon Principles of Data Mining and KnowledgeDiscovery, pages 65-73.
LNCS Springer.Gasperin, C.; Vieira, R.; Goulart, R. andQuaresma, P. 2003.
Extracting XML SyntacticChunks from Portuguese Corpora.
Proc.
of theTALN Workshop on Natural LanguageProcessing of Minority Languages and SmallLanguages, pages 223-232.
Batz-sur-MerFrance.Gon?alves, T. and Quaresma, P. 2003.
A prelimaryapproach classification problem of Portuguesejuridical documents.
Proceedings of the 11?Portuguese Conference on Artificial Intelligence,pages 435-444.
LNAI Springer Verlag.Porter, M. F. 1980.
An Algorithm for SuffixStripping.
Program, 14 no.
3, pages 130-137.Tan, Ah-Hwee.
1999.
Text mining: the state of theart and the challenges.
Proc.
of the Pacific-AsiaWorkshop on Knowledge Discovery fromAdvanced Databases, pages 65-70, Beijing.Witten, I. H. 2000.
Data mining: Pratical MachineLearning tools and techniques with Javaimplementations.
Academic Press.
