Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726?731,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEntity Set Expansion using Topic informationKugatsu Sadamitsu, Kuniko Saito, Kenji Imamura and Genichiro Kikui?NTT Cyber Space Laboratories, NTT Corporation1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan{sadamitsu.kugatsu, saito.kuniko, imamura.kenji}@lab.ntt.co.jpkikui@cse.oka-pu.ac.jpAbstractThis paper proposes three modules based onlatent topics of documents for alleviating ?se-mantic drift?
in bootstrapping entity set ex-pansion.
These new modules are added to adiscriminative bootstrapping algorithm to re-alize topic feature generation, negative exam-ple selection and entity candidate pruning.
Inthis study, we model latent topics with LDA(Latent Dirichlet Allocation) in an unsuper-vised way.
Experiments show that the accu-racy of the extracted entities is improved by6.7 to 28.2% depending on the domain.1 IntroductionThe task of this paper is entity set expansion inwhich the lexicons are expanded from just a fewseed entities (Pantel et al, 2009).
For example,the user inputs a few words ?Apple?, ?Google?
and?IBM?
, and the system outputs ?Microsoft?, ?Face-book?
and ?Intel?.Many set expansion algorithms are based on boot-strapping algorithms, which iteratively acquire newentities.
These algorithms suffer from the generalproblem of ?semantic drift?.
Semantic drift movesthe extraction criteria away from the initial criteriademanded by the user and so reduces the accuracyof extraction.
Pantel and Pennacchiotti (2006) pro-posed Espresso, a relation extraction method basedon the co-training bootstrapping algorithm with en-tities and attributes.
Espresso alleviates semantic-drift by a sophisticated scoring system based on?
Presently with Okayama Prefectural Universitypointwise mutual information (PMI).
Thelen andRiloff (2002), Ghahramani and Heller (2005) andSarmento et al (2007) also proposed original scorefunctions with the goal of reducing semantic-drift.Our purpose is also to reduce semantic drift.
Forachieving this goal, we use a discriminative methodinstead of a scoring function and incorporate topicinformation into it.
Topic information means thegenre of each document as estimated by statisti-cal topic models.
In this paper, we effectively uti-lize topic information in three modules: the firstgenerates the features of the discriminative mod-els; the second selects negative examples; the thirdprunes incorrect examples from candidate examplesfor new entities.
Our experiments show that the pro-posal improves the accuracy of the extracted entities.The remainder of this paper is organized as fol-lows.
In Section 2, we illustrate discriminative boot-strapping algorithms and describe their problems.Our proposal is described in Section 3 and experi-mental results are shown in Section 4.
Related worksare described in Section 5.
Finally, Section 6 pro-vides our conclusion and describes future works.2 Problems of the previous DiscriminativeBootstrapping methodSome previous works introduced discriminativemethods based on the logistic sigmoid classifier,which can utilize arbitrary features for the relationextraction task instead of a scoring function such asEspresso (Bellare et al, 2006; Mintz et al, 2009).Bellare et al reported that the discriminative ap-proach achieves better accuracy than Espresso whenthe number of extracted pairs is increased because726multiple features are used to support the evidence.However, three problems exist in their methods.First, they use only local context features.
The dis-criminative approach is useful for using arbitraryfeatures, however, they did not identify which fea-ture or features are effective for the methods.
Al-though the context features and attributes partly re-duce entity word sense ambiguity, some ambiguousentities remain.
For example, consider the domainbroadcast program (PRG) and assume that PRG?sattribute is advertisement.
A false example is shownhere: ?Android ?s advertisement employs Japanesepopular actors.
The attractive smartphone begins totarget new users who are ordinary people.?
The en-tity Android belongs to the cell-phone domain, notPRG, but appears with positive attributes or contextsbecause many cell-phones are introduced in adver-tisements as same as broadcast program.
By us-ing topic, i.e.
the genre of the document, we candistinguish ?Android?
from PRG and remove suchfalse examples even if the false entity appeared withpositive context strings or attributes.
Second, theydid not solve the problem of negative example se-lection.
Because negative examples are necessaryfor discriminative training, they used all remainingexamples, other than positive examples, as negativeexamples.
Although this is the simplest technique,it is impossible to use all of the examples providedby a large-scale corpus for discriminative training.Third, their methods discriminate all candidates fornew entities.
This principle increases the risk of gen-erating many false-positive examples and is ineffi-cient.
We solve these three problems by using topicinformation.3 Set expansion using Topic information3.1 Basic bootstrapping methodsIn this section, we describe the basic methodadopted from Bellare (Bellare et al, 2006).
Oursystem?s configuration diagram is shown in Figure1.
In Figure 1, arrows with solid lines indicate thebasic process described in this section.
The otherparts are described in the following sections.
AfterNs positive seed entities are manually given, everynoun co-occurring with the seed entities is rankedby PMI scores and then selected manually as Napositive attributes.
Ns and Na are predefined ba-Figure 1: The structure of our system.sic adjustment numbers.
The entity-attribute pairsare obtained by taking the cross product of seed en-tity lists and attribute lists.
The pairs are used asqueries for retrieving the positive documents, whichinclude positive pairs.
The document set De,a in-cluding same entity-attribute pair {e, a} is regardedas one example Ee,a to alleviate over-fitting for con-text features.
These are called positive examples inFigure 1.
Once positive examples are constructed,discriminative models can be trained by randomlyselecting negative examples.Candidate entities are restricted to only theNamed Entities that lie in the close proximity to thepositive attributes.
These candidates of documents,including Named Entity and positive attribute pairs,are regarded as one example the same as the train-ing data.
The discriminative models are used to cal-culate the discriminative positive score, s(e, a), ofeach candidate pair, {e, a}.
Our system extracts Nntypes of new entities with high scores at each iter-ation as defined by the summation of s(e, a) of allpositive attributes (AP );?a?AP s(e, a).
Note thatwe do not iteratively extract new attributes becauseour purpose is entity set expansion.3.2 Topic features and Topic modelsIn previous studies, context information is only usedas the features of discriminative models as we de-scribed in Section 2.
Our method utilizes not onlycontext features but also topic features.
By utiliz-ing topic information, our method can disambiguatethe entity word sense and alleviate semantic drift.In order to derive the topic information, we utilizestatistical topic models, which represent the relation727between documents and words through hidden top-ics.
The topic models can calculate the posteriorprobability p(z|d) of topic z in document d. Forexample, the topic models give high probability totopic z =?cell-phone?
in the above example sen-tences 1.
This posterior probability is useful as aglobal feature for discrimination.
The topic featurevalue ?t(z, e, a) is calculated as follows.
?t(z, e, a) =?d?De,a p(z|d)?z?
?d?De,a p(z?|d).In this paper, we use Latent Dirichlet Allocation(LDA) as the topic models (Blei et al, 2003).
LDArepresents the latent topics of the documents and theco-occurrence between each topic.In Figure 1, shaded part and the arrows with bro-ken lines indicate our proposed method with its useof topic information including the following sec-tions.3.3 Negative example selectionIf we choose negative examples randomly, such ex-amples are harmful for discrimination because someexamples include the same contexts or topics as thepositive examples.
By contrast, negative examplesbelonging to broad genres are needed to alleviate se-mantic drift.
We use topic information to efficientlyselect such negative examples.In our method, the negative examples are cho-sen far from the positive examples according to themeasure of topic similarity.
For calculating topicsimilarity, we use a ranking score called ?positivetopic score?, PT (z), defined as follows, PT (z) =?d?DP p(z|d), where DP indicates the set of pos-itive documents and p(z|d) is topic posterior prob-ability for a given positive document.
The bottom50% of the topics sorted in decreasing order of pos-itive topic score are used as the negative topics.Our system picks up as many negative documentsas there are positive documents with each selectednegative topic being equally represented.3.4 Candidate PruningPrevious works discriminate all candidates for ex-tracting new entities.
Our basic system can constrain1z is a random variable whose sample space is representedas a discrete variable, not explicit words.the candidate set by positive attributes, however, thisis not enough as described in Section 2.
Our candi-date pruning module, described below, uses the mea-sure of topic similarity to remove obviously incor-rect documents.This pruning module is similar to negative exam-ple selection described in the previous section.
Thepositive topic score, PT , is used as a candidate con-straint.
Taking all positive examples, we select thepositive topics, PZ, which including all topics z sat-isfying the condition PT (z) > th.
At least onetopic with the largest score is chosen as a positivetopic when PT (z) ?
th about all topics.
After se-lecting this positive topic, the documents includingentity candidates are removed if the posterior prob-ability satisfy p(z|d) ?
th for all topics z.
In thispaper, we set the threshold to th = 0.2.
This con-straint means that the topic of the document matchesthat of the positive entities and can be regarded as ahard constraint for topic features.4 Experiments4.1 Experimental SettingsWe use 30M Japanese blog articles crawled in May2008.
The documents were tokenized by JTAG(Fuchi and Takagi, 1998), chunked, and labeled withIREX 8 Named Entity types by CRFs using Mini-mum Classification Error rate (Suzuki et al, 2006),and transformed into features.
The context featureswere defined using the template ?
(head) entity (mid.
)attribute (tail)?.
The words included in each partwere used as surface, part-of-speech and Named En-tity label features added position information.
Max-imum word number of each part was set at 2 words.The features have to appear in both the positive andnegative training data at least 5 times.In the experiments, we used three domains, car(?CAR?
), broadcast program (?PRG?)
and sports or-ganization (?SPT?).
The adjustment numbers for ba-sic settings are Ns = 10, Na = 10, Nn = 100.
Af-ter running 10 iterations, we obtained 1000 entitiesin total.
SVM light (Joachims, 1999) with secondorder polynomial kernel was used as the discrimina-tive model.
Parallel LDA, which is LDA with MPI(Liu et al, 2011), was used for training 100 mix-ture topic models and inference.
Training corpus fortopic models consisted of the content gathered from728CAR PRG SPT1.
Baseline 0.249 0.717 0.7812.
Topic features + 1.
0.483 0.727 0.8443.
Negative selection + 2.
0.509 0.762 0.8464.
Candidate pruning + 3.
0.531 0.824 0.848Table 1: The experimental results for the three domains.Bold font indicates that the difference between accuracyof the methods in the row and the previous row is signifi-cant (P < 0.05 by binomial test) and italic font indicates(P < 0.1).14 days of blog articles.
In the Markov-chain MonteCarlo (MCMC) method, sampling was iterated 200times for training with a burn-in taking 50 iterations.These parameters were selected based on the resultsof a preliminary experiment.Four experimental settings were examined.
Firstis Baseline; it is described in Section 3.1.
Second isthe first method with the addition of topic features.Third is the second method with the addition of anegative example selection module.
Fourth is thethird method with the addition of a candidate prun-ing module (equals the entire shaded part in Fig-ure 1).
Each extracted entity is labeled with cor-rect or incorrect by two evaluators based on the re-sults of a commercial search engine.
The ?
score foragreement between evaluators was 0.895.
Becausethe third evaluator checked the two evaluations andconfirmed that the examples which were judged ascorrect by either one of the evaluators were correct,those examples were counted as correct.4.2 Experimental ResultsTable 1 shows the accuracy and significance for eachdomain.
Using topic features significantly improvesaccuracy in the CAR and SPT domains.
The nega-tive example selection module improves accuracy inthe CAR and PRG domains.
This means the methodcould reduce the risk of selecting false-negative ex-amples.
Also, the candidate pruning method is ef-fective for the CAR and PRG domains.
The CARdomain has lower accuracy than the others.
Thisis because similar entities such as motorcycles areextracted; they have not only the same context butalso the same topic as the CAR domain.
In the SPTdomain, the method with topic features offer signif-icant improvements in accuracy and no further im-provement was achieved by the other two modules.To confirm whether our modules work properly,we show some characteristic words belonging toeach topic that is similar and not similar to target do-main in Table 2.
Table 2 shows characteristic wordsfor one positive topic zh and two negative topics zland ze, defined as follow.?
zh (the second row) is the topic that maximizesPT (z), which is used as a positive topic.?
zl (the fourth row) is the topic that minimizesPT (z), which is used as a negative topic.?
ze (the fifth row) is a topic that, we consider, ef-fectively eliminates ?drifted entities?
extractedby the baseline method.
ze is eventually in-cluded in the lower half of topic list sorted byPT (z).For a given topic, z, we chose topmost three wordsin terms of topic-word score.
The topic-word scoreof a word, v, is defined as p(v|z)/p(v), where p(v)is the unigram probability of v, which was estimatedby maximum likelihood estimation.
For utilizingcandidate pruning, near topics including zh must besimilar to the domain.
By contrast, for utilizing neg-ative example selection, the lower half of topics, zl,ze and other negative topics, must be far from thedomain.
Our system succeeded in achieving this.As shown in ?CAR?
in Table 2, the nearest topicincludes ?shaken?
(automobile inspection) and thefarthest topic includes ?naika?
(internal medicine)which satisfies our expectation.
Furthermore, the ef-fective negative topic is similar to the topic of driftedentity sets (digital device).
This indicates that ourmethod successfully eliminated drifted entities.
Wecan confirm that the other domains trend in the samedirection as ?CAR?
domain.5 Related WorksSome prior studies use every word in a docu-ment/sentence as the features, such as the distribu-tional approaches (Pantel et al, 2009).
These meth-ods are regarded as using global information, how-ever, the space of word features are sparse, even ifthe amount of data available is large.
Our approachcan avoid this problem by using topic models which729domain CAR PRG SPTwords of thenearest topic zh(highest PT score)shaken(automobile inspection),nosha (delivering a car),daisha (loaner car)Mari YAMADA,Tohru KUSANO,Reiko TOKITA(Japanese stars)toshu (pitcher),senpatsu(starting member),shiai (game)drifted entities(using baseline)iPod, mac(digital device)PS2, XBOX360(video game)B?z, CHAGE&ASKA(music)words of effectivenegative topic ze(Lower half ofPT score)gaso (pixel),kido (brightness),mazabodo (mother board)Lv.
(level),kariba (hunting area),girumen (guild member)sinpu (new release),X JAPAN ,Kazuyoshi Saito(Japanese musicians)words ofthe farthest topic zl(Lowest PT score)naika (internal medicine),hairan (ovulation),shujii (attending doctor)tsure (hook a fish),choka (result of hooking),choko (diary of hooking)toritomento (treatment),keana (pore),hoshitsu (moisture retention)Table 2: The characteristic words belonging to three topics, zh, zl and ze.
zh is the nearest topic and zl is the farthesttopic for positive entity-attribute seed pairs.
ze is an effective negative topic for eliminating ?drifted entities?
extractedby the baseline system.are clustering methods based on probabilistic mea-sures.
By contrast, Pas?ca and Durme (2008) pro-posed clustering methods that are effective in termsof extraction, even though their clustering target isonly the surrounding context.
Ritter and Etzioni(2010) proposed a generative approach to use ex-tended LDA to model selectional preferences.
Al-though their approach is similar to ours, our ap-proach is discriminative and so can treat arbitraryfeatures; it is applicable to bootstrapping methods.The accurate selection of negative examples is amajor problem for positive and unlabeled learningmethods or general bootstrapping methods and someprevious works have attempted to reach a solution(Liu et al, 2002; Li et al, 2010).
However, theirmethods are hard to apply to the Bootstrapping al-gorithms because the positive seed set is too smallto accurately select negative examples.
Our methoduses topic information to efficiently solve both theproblem of extracting global information and theproblem of selecting negative examples.6 ConclusionWe proposed an approach to set expansion that usestopic information in three modules and showed thatit can improve expansion accuracy.
The remainingproblem is that the grain size of topic models is notalways the same as the target domain.
To resolvethis problem, we will incorporate the active learningor the distributional approaches.
Also, comparisonswith the previous works are remaining work.
Fromanother perspective, we are considering the use ofgraph-based approaches (Komachi et al, 2008) in-corporated with the topic information using PHITS(Cohn and Chang, 2000), to further enhance entityextraction accuracy.ReferencesKedar Bellare, Partha P. Talukdar, Giridhar Kumaran,Fernando Pereira, Mark Liberman, Andrew McCal-lum, and Mark Dredze.
2006.
Lightly-supervised at-tribute extraction.
In Proceedings of the Advances inNeural Information Processing Systems Workshop onMachine Learning for Web Search.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
The Journal of Ma-chine Learning Research, 3:993?1022.David Cohn and Huau Chang.
2000.
Learning to prob-abilistically identify authoritative documents.
In Pro-ceedings of the 17th International Conference on Ma-chine Learning, pages 167?174.Takeshi Fuchi and Shinichiro Takagi.
1998.
JapaneseMorphological Analyzer using Word Co-occurrence-JTAG.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics and17th International Conference on Computational Lin-guistics, pages 409?413.Zoubin Ghahramani and Katherine A. Heller.
2005.Bayesian sets.
In Proceedings of the Advances in Neu-ral Information Processing Systems.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods -Support Vector Learning.
Software available athttp://svmlight.joachims.org/.730Mamoru Komachi, Taku Kudo, Masashi Shimbo, andYuji Matsumoto.
2008.
Graph-based analysis of se-mantic drift in Espresso-like bootstrapping algorithms.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages1011?1020.Xiao-Li Li, Bing Liu, and See-Kiong Ng.
2010.
Neg-ative Training Data can be Harmful to Text Classi-fication.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 218?228.Bing Liu, Wee S. Lee, Philip S. Yu, and Xiaoli Li.
2002.Partially supervised classification of text documents.In Proceedings of the 19th International Conferenceon Machine Learning, pages 387?394.Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, andMaosong Sun.
2011.
PLDA+: Parallel latent dirich-let alocation with data placement and pipeline pro-cessing.
ACM Transactions on Intelligent Systemsand Technology, special issue on Large Scale MachineLearning.
Software available at http://code.google.com/p/plda.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 1003?1011.Marius Pas?ca and Benjamin Van Durme.
2008.
Weakly-supervised acquisition of open-domain classes andclass attributes from web documents and query logs.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics, pages 19?27.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 113?120.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 938?947.Alan Ritter and Oren Etzioni.
2010.
A Latent Dirich-let Allocation method for Selectional Preferences.
InProceedings of the 48th ACL Conference, pages 424?434.Luis Sarmento, Valentin Jijkuon, Maarten de Rijke, andEugenio Oliveira.
2007.
More like these: grow-ing entity classes from seeds.
In Proceedings of the16th ACM Conference on Information and KnowledgeManagement, pages 959?962.Jun Suzuki, Erik McDermott, and Hideki Isozaki.
2006.Training Conditional Random Fields with Multivari-ate Evaluation Measures.
In Proceedings of the 21stCOLING and 44th ACL Conference, pages 217?224.Michael Thelen and Ellen Riloff.
2002.
A bootstrap-ping method for learning semantic lexicons using ex-traction pattern contexts.
In Proceedings of the 2002conference on Empirical methods in natural languageprocessing, pages 214?221.731
