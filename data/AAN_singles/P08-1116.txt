Proceedings of ACL-08: HLT, pages 1021?1029,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCombining Multiple Resources to Improve SMT-based Paraphrasing Model?Shiqi Zhao1, Cheng Niu2, Ming Zhou2, Ting Liu1, Sheng Li11Harbin Institute of Technology, Harbin, China{zhaosq,tliu,lisheng}@ir.hit.edu.cn2Microsoft Research Asia, Beijing, China{chengniu,mingzhou}@microsoft.comAbstractThis paper proposes a novel method that ex-ploits multiple resources to improve statisti-cal machine translation (SMT) based para-phrasing.
In detail, a phrasal paraphrase ta-ble and a feature function are derived fromeach resource, which are then combined in alog-linear SMT model for sentence-level para-phrase generation.
Experimental results showthat the SMT-based paraphrasing model canbe enhanced using multiple resources.
Thephrase-level and sentence-level precision ofthe generated paraphrases are above 60% and55%, respectively.
In addition, the contribu-tion of each resource is evaluated, which indi-cates that all the exploited resources are usefulfor generating paraphrases of high quality.1 IntroductionParaphrases are alternative ways of conveying thesame meaning.
Paraphrases are important in manynatural language processing (NLP) applications,such as machine translation (MT), question an-swering (QA), information extraction (IE), multi-document summarization (MDS), and natural lan-guage generation (NLG).This paper addresses the problem of sentence-level paraphrase generation, which aims at generat-ing paraphrases for input sentences.
An example ofsentence-level paraphrases can be seen below:S1: The table was set up in the carriage shed.S2: The table was laid under the cart-shed.
?This research was finished while the first author worked asan intern in Microsoft Research Asia.Paraphrase generation can be viewed as monolin-gual machine translation (Quirk et al, 2004), whichtypically includes a translation model and a lan-guage model.
The translation model can be trainedusing monolingual parallel corpora.
However, ac-quiring such corpora is not easy.
Hence, data sparse-ness is a key problem for the SMT-based paraphras-ing.
On the other hand, various methods have beenpresented to extract phrasal paraphrases from dif-ferent resources, which include thesauri, monolin-gual corpora, bilingual corpora, and the web.
How-ever, little work has been focused on using the ex-tracted phrasal paraphrases in sentence-level para-phrase generation.In this paper, we exploit multiple resources toimprove the SMT-based paraphrase generation.
Indetail, six kinds of resources are utilized, includ-ing: (1) an automatically constructed thesaurus, (2)a monolingual parallel corpus from novels, (3) amonolingual comparable corpus from news articles,(4) a bilingual phrase table, (5) word definitionsfrom Encarta dictionary, and (6) a corpus of simi-lar user queries.
Among the resources, (1), (2), (3),and (4) have been investigated by other researchers,while (5) and (6) are first used in this paper.
Fromthose resources, six phrasal paraphrase tables are ex-tracted, which are then used in a log-linear SMT-based paraphrasing model.Both phrase-level and sentence-level evaluationswere carried out in the experiments.
In the formerone, phrase substitutes occurring in the paraphrasesentences were evaluated.
While in the latter one,the acceptability of the paraphrase sentences wasevaluated.
Experimental results show that: (1) The1021SMT-based paraphrasing is enhanced using multipleresources.
The phrase-level and sentence-level pre-cision of the generated paraphrases exceed 60% and55%, respectively.
(2) Although the contributions ofthe resources differ a lot, all the resources are useful.
(3) The performance of the method varies greatly ondifferent test sets and it performs best on the test setof news sentences, which are from the same sourceas most of the training data.The rest of the paper is organized as follows: Sec-tion 2 reviews related work.
Section 3 introduces thelog-linear model for paraphrase generation.
Section4 describes the phrasal paraphrase extraction fromdifferent resources.
Section 5 presents the parameterestimation method.
Section 6 shows the experimentsand results.
Section 7 draws the conclusion.2 Related WorkParaphrases have been used in many NLP applica-tions.
In MT, Callison-Burch et al (2006) utilizedparaphrases of unseen source phrases to alleviatedata sparseness.
Kauchak and Barzilay (2006) usedparaphrases of the reference translations to improveautomatic MT evaluation.
In QA, Lin and Pantel(2001) and Ravichandran and Hovy (2002) para-phrased the answer patterns to enhance the recall ofanswer extraction.
In IE, Shinyama et al (2002)automatically learned paraphrases of IE patterns toreduce the cost of creating IE patterns by hand.
InMDS, McKeown et al (2002) identified paraphrasesentences across documents before generating sum-marizations.
In NLG, Iordanskaja et al (1991) usedparaphrases to generate more varied and fluent texts.Previous work has examined various resources foracquiring paraphrases, including thesauri, monolin-gual corpora, bilingual corpora, and the web.
The-sauri, such as WordNet, have been widely usedfor extracting paraphrases.
Some researchers ex-tract synonyms as paraphrases (Kauchak and Barzi-lay, 2006), while some others use looser defini-tions, such as hypernyms and holonyms (Barzilayand Elhadad, 1997).
Besides, the automaticallyconstructed thesauri can also be used.
Lin (1998)constructed a thesaurus by automatically clusteringwords based on context similarity.Barzilay andMcKeown (2001) used monolingualparallel corpora for identifying paraphrases.
Theyexploited a corpus of multiple English translationsof the same source text written in a foreign language,from which phrases in aligned sentences that appearin similar contexts were extracted as paraphrases.
Inaddition, Finch et al (2005) applied MT evalua-tion methods (BLEU, NIST,WER and PER) to buildclassifiers for paraphrase identification.Monolingual parallel corpora are difficult to find,especially in non-literature domains.
Alternatively,some researchers utilized monolingual compara-ble corpora for paraphrase extraction.
Differentnews articles reporting on the same event are com-monly used as monolingual comparable corpora,from which both paraphrase patterns and phrasalparaphrases can be derived (Shinyama et al, 2002;Barzilay and Lee, 2003; Quirk et al, 2004).Lin and Pantel (2001) learned paraphrases froma parsed monolingual corpus based on an extendeddistributional hypothesis, where if two paths in de-pendency trees tend to occur in similar contexts it ishypothesized that the meanings of the paths are simi-lar.
The monolingual corpus used in their work is notnecessarily parallel or comparable.
Thus it is easyto obtain.
However, since this resource is used toextract paraphrase patterns other than phrasal para-phrases, we do not use it in this paper.Bannard and Callison-Burch (2005) learnedphrasal paraphrases using bilingual parallel cor-pora.
The basic idea is that if two phrases arealigned to the same translation in a foreign language,they may be paraphrases.
This method has beendemonstrated effective in extracting large volume ofphrasal paraphrases.
Besides, Wu and Zhou (2003)exploited bilingual corpora and translation informa-tion in learning synonymous collocations.In addition, some researchers extracted para-phrases from the web.
For example, Ravichandranand Hovy (2002) retrieved paraphrase patterns fromthe web using hand-crafted queries.
Pasca and Di-enes (2005) extracted sentence fragments occurringin identical contexts as paraphrases from one bil-lion web documents.
Since web mining is rathertime consuming, we do not exploit the web to ex-tract paraphrases in this paper.So far, two kinds of methods have been pro-posed for sentence-level paraphrase generation, i.e.,the pattern-based and SMT-based methods.
Auto-matically learned patterns have been used in para-1022phrase generation.
For example, Barzilay and Lee(2003) applied multiple-sequence alignment (MSA)to parallel news sentences and induced paraphras-ing patterns for generating new sentences.
Pang etal.
(2003) built finite state automata (FSA) from se-mantically equivalent translation sets based on syn-tactic alignment and used the FSAs in paraphrasegeneration.
The pattern-based methods can generatecomplex paraphrases that usually involve syntacticvariation.
However, the methods were demonstratedto be of limited generality (Quirk et al, 2004).Quirk et al (2004) first recast paraphrase gener-ation as monolingual SMT.
They generated para-phrases using a SMT system trained on parallel sen-tences extracted from clustered news articles.
Inaddition, Madnani et al (2007) also generatedsentence-level paraphrases based on a SMT model.The advantage of the SMT-based method is thatit achieves better coverage than the pattern-basedmethod.
The main difference between their methodsand ours is that they only used bilingual parallel cor-pora as paraphrase resource, while we exploit andcombine multiple resources.3 SMT-based Paraphrasing ModelThe SMT-based paraphrasing model used by Quirket al (2004) was the noisy channel model of Brownet al (1993), which identified the optimal paraphraseT ?
of a sentence S by finding:T ?
= argmaxT{P (T |S)}= argmaxT{P (S|T )P (T )} (1)In contrast, we adopt a log-linear model (Ochand Ney, 2002) in this work, since multiple para-phrase tables can be easily combined in the log-linear model.
Specifically, feature functions are de-rived from each paraphrase resource and then com-bined with the language model feature1:T ?
= argmaxT{N?i=1?TM ihTM i(T, S)+?LMhLM (T, S)} (2)where N is the number of paraphrase tables.hTM i(T, S) is the feature function based on the i-th paraphrase table PTi.
hLM (T, S) is the language1The reordering model is not considered in our model.model feature.
?TM i and ?LM are the weights ofthe feature functions.
hTM i(T, S) is defined as:hTM i(T, S) = logKi?k=1Scorei(Tk, Sk) (3)where Ki is the number of phrase substitutes fromS to T based on PTi.
Tk in T and Sk in S arephrasal paraphrases in PTi.
Scorei(Tk, Sk) is theparaphrase likelihood according to PTi2.
A 5-gramlanguage model is used, therefore:hLM (T, S) = logJ?j=1p(tj |tj?4, ..., tj?1) (4)where J is the length of T , tj is the j-th word of T .4 Exploiting Multiple ResourcesThis section describes the extraction of phrasalparaphrases using various resources.
Similar toPharaoh (Koehn, 2004), our decoder3 uses top 20paraphrase options for each input phrase in the de-fault setting.
Therefore, we keep at most 20 para-phrases for a phrase when extracting phrasal para-phrases using each resource.1 - Thesaurus: The thesaurus4 used in this workwas automatically constructed by Lin (1998).
Thesimilarity of two words e1 and e2 was calculatedthrough the surrounding context words that have de-pendency relations with the investigated words:Sim(e1, e2)=P(r,e)?Tr(e1)?Tr(e2)(I(e1, r, e) + I(e2, r, e))P(r,e)?Tr(e1)I(e1, r, e) +P(r,e)?Tr(e2)I(e2, r, e)(5)where Tr(ei) denotes the set of words that have de-pendency relation r with word ei.
I(ei, r, e) is themutual information between ei, r and e.For each word, we keep 20 most similar words asparaphrases.
In this way, we extract 502,305 pairs ofparaphrases.
The paraphrasing score Score1(p1, p2)used in Equation (3) is defined as the similaritybased on Equation (5).2If none of the phrase substitutes from S to T is from PTi(i.e., Ki = 0), we cannot compute hTM i(T, S) as in Equation(3).
In this case, we assign hTM i(T, S) a minimum value.3The decoder used here is a re-implementation of Pharaoh.4http://www.cs.ualberta.ca/ lindek/downloads.htm.10232 - Monolingual parallel corpus: Following Barzi-lay and McKeown (2001), we exploit a corpusof multiple English translations of foreign nov-els, which contains 25,804 parallel sentence pairs.We find that most paraphrases extracted using themethod of Barzilay and McKeown (2001) are quiteshort.
Thus we employ a new approach for para-phrase extraction.
Specifically, we parse the sen-tences with CollinsParser5 and extract the chunksfrom the parsing results.
Let S1 and S2 be a pairof parallel sentences, p1 and p2 two chunks from S1and S2, we compute the similarity of p1 and p2 as:Sim(p1, p2) = ?Simcontent(p1, p2)+(1 ?
?
)Simcontext(p1, p2) (6)where, Simcontent(p1, p2) is the content similarity,which is the word overlapping rate of p1 and p2.Simcontext(p1, p2) is the context similarity, which isthe word overlapping rate of the contexts of p1 andp26.
If the similarity of p1 and p2 exceeds a thresh-old Th1, they are identified as paraphrases.
We ex-tract 18,698 pairs of phrasal paraphrases from thisresource.
The paraphrasing score Score2(p1, p2) isdefined as the similarity in Equation (6).
For theparaphrases occurring more than once, we use theirmaximum similarity as the paraphrasing score.3 - Monolingual comparable corpus: Similar tothe methods in (Shinyama et al, 2002; Barzilay andLee, 2003), we construct a corpus of comparabledocuments from a large corpus D of news articles.The corpusD contains 612,549 news articles.
Givenarticles d1 and d2 from D, if their publication dateinterval is less than 2 days and their similarity7 ex-ceeds a threshold Th2, they are recognized as com-parable documents.
In this way, a corpus containing5,672,864 pairs of comparable documents is con-structed.
From the comparable corpus, parallel sen-tences are extracted.
Let s1 and s2 be two sentencesfrom comparable documents d1 and d2, if their sim-ilarity based on word overlapping rate is above athreshold Th3, s1 and s2 are identified as parallelsentences.
In this way, 872,330 parallel sentencepairs are extracted.5http://people.csail.mit.edu/mcollins/code.html6The context of a chunk is made up of 6 words around thechunk, 3 to the left and 3 to the right.7The similarity of two documents is computed using the vec-tor space model and the word weights are based on tf?idf.We run Giza++ (Och and Ney, 2000) on the paral-lel sentences and then extract aligned phrases as de-scribed in (Koehn, 2004).
The generated paraphrasetable is pruned by keeping the top 20 paraphrases foreach phrase.
After pruning, 100,621 pairs of para-phrases are extracted.
Given phrase p1 and its para-phrase p2, we compute Score3(p1, p2) by relativefrequency (Koehn et al, 2003):Score3(p1, p2) = p(p2|p1) =count(p2, p1)Pp?
count(p?, p1)(7)People may wonder why we do not use the samemethod on the monolingual parallel and comparablecorpora.
This is mainly because the volumes of thetwo corpora differ a lot.
In detail, the monolingualparallel corpus is fairly small, thus automatical wordalignment tool like Giza++ may not work well onit.
In contrast, the monolingual comparable corpusis quite large, hence we cannot conduct the time-consuming syntactic parsing on it as we do on themonolingual parallel corpus.4 - Bilingual phrase table: We first constructa bilingual phrase table that contains 15,352,469phrase pairs from an English-Chinese parallel cor-pus.
We extract paraphrases from the bilingualphrase table and compute the paraphrasing scoreof phrases p1 and p2 as in (Bannard and Callison-Burch, 2005):Score4(p1, p2) =?fp(f |p1)p(p2|f) (8)where f denotes a Chinese translation of both p1 andp2.
p(f |p1) and p(p2|f) are the translation probabil-ities provided by the bilingual phrase table.
For eachphrase, the top 20 paraphrases are kept accordingto the score in Equation (8).
As a result, 3,177,600pairs of phrasal paraphrases are extracted.5 - Encarta dictionary definitions: Words and theirdefinitions can be regarded as paraphrases.
Hereare some examples from Encarta dictionary: ?hur-ricane: severe storm?, ?clever: intelligent?, ?travel:go on journey?.
In this work, we extract words?
def-initions from Encarta dictionary web pages8.
If aword has more than one definition, all of them areextracted.
Note that the words and definitions in the8http://encarta.msn.com/encnet/features/dictionary/diction-aryhome.aspx1024dictionary are lemmatized, but words in sentencesare usually inflected.
Hence, we expand the word- definition pairs by providing the inflected forms.Here we use an inflection list and some rules for in-flection.
After expanding, 159,456 pairs of phrasalparaphrases are extracted.
Let < p1, p2 > be a word- definition pair, the paraphrasing score is definedaccording to the rank of p2 in all of p1?s definitions:Score5(p1, p2) = ?i?1 (9)where ?
is a constant (we empirically set ?
= 0.9)and i is the rank of p2 in p1?s definitions.6 - Similar user queries: Clusters of similar userqueries have been used for query expansion and sug-gestion (Gao et al, 2007).
Since most queries are atthe phrase level, we exploit similar user queries asphrasal paraphrases.
In our experiment, we use thecorpus of clustered similar MSN queries constructedby Gao et al (2007).
The similarity of two queriesp1 and p2 is computed as:Sim(p1, p2) = ?Simcontent(p1, p2)+(1 ?
?
)Simclick?through(p1, p2) (10)where Simcontent(p1, p2) is the content similarity,which is computed as the word overlapping rate ofp1 and p2.
Simclick?through(p1, p2) is the clickthrough similarity, which is the overlapping rate ofthe user clicked documents for p1 and p2.
For eachquery q, we keep the top 20 similar queries, whosesimilarity with q exceeds a threshold Th4.
As a re-sult, 395,284 pairs of paraphrases are extracted.
Thescore Score6(p1, p2) is defined as the similarity inEquation (10).7 - Self-paraphrase: In addition to the six resourcesintroduced above, a special paraphrase table is used,which is made up of pairs of identical words.
Thereason why this paraphrase table is necessary is thata word should be allowed to keep unchanged in para-phrasing.
This is a difference between paraphras-ing and MT, since all words should be translated inMT.
In our experiments, all the words that occur inthe six paraphrase table extracted above are gath-ered to form the self-paraphrase table, which con-tains 110,403 word pairs.
The score Score7(p1, p2)is set 1 for each identical word pair.5 Parameter EstimationThe weights of the feature functions, namely ?TM i(i = 1, 2, ..., 7) and ?LM , need estimation9.
In MT,the max-BLEU algorithm is widely used to estimateparameters.
However, it may not work in our case,since it is more difficult to create a reference set ofparaphrases.We propose a new technique to estimate parame-ters in paraphrasing.
The assumption is that, since aSMT-based paraphrase is generated through phrasesubstitution, we can measure the quality of a gener-ated paraphrase by measuring its phrase substitutes.Generally, the paraphrases containing more correctphrase substitutes are judged as better paraphrases10.We therefore present the phrase substitution errorrate (PSER) to score a generated paraphrase T :PSER(T ) = ?PS0(T )?/?PS(T )?
(11)where PS(T ) is the set of phrase substitutes in Tand PS0(T ) is the set of incorrect substitutes.In practice, we keep top n paraphrases for eachsentence S. Thus we calculate the PSER for eachsource sentence S as:PSER(S) = ?n[i=1PS0(Ti)?/?n[i=1PS(Ti)?
(12)where Ti is the i-th generated paraphrase of S.Suppose there are N sentences in the develop-ment set, the overall PSER is computed as:PSER =NXj=1PSER(Sj) (13)where Sj is the j-th sentence in the development set.Our development set contains 75 sentences (de-scribed in detail in Section 6).
For each sentence,all possible phrase substitutes are extracted from thesix paraphrase tables above.
The extracted phrasesubstitutes are then manually labeled as ?correct?
or?incorrect?.
A phrase substitute is considered as cor-rect only if the two phrases have the same meaningin the given sentence and the sentence generated by9Note that, we also use some other parameters when extract-ing phrasal paraphrases from different resources, such as thethresholds Th1, Th2, Th3, Th4, as well as ?
and ?
in Equa-tion (6) and (10).
These parameters are estimated using differ-ent development sets from the investigated resources.
We donot describe the estimation of them due to space limitation.10Paraphrasing a word to itself (based on the 7-th paraphrasetable above) is not regarded as a substitute.1025substituting the source phrase with the target phraseremains grammatical.
In decoding, the phrase sub-stitutes are printed out and then the PSER is com-puted based on the labeled data.Using each set of parameters, we generate para-phrases for the sentences in the development setbased on Equation (2).
PSER is then computed asin Equation (13).
We use the gradient descent algo-rithm (Press et al, 1992) to minimize PSER on thedevelopment set and get the optimal parameters.6 ExperimentsTo evaluate the performance of the method on dif-ferent types of test data, we used three kinds of sen-tences for testing, which were randomly extractedfrom Google news, free online novels, and forums,respectively.
For each type, 50 sentences were ex-tracted as test data and another 25 were extracted asdevelopment data.
For each test sentence, top 10 ofthe generated paraphrases were kept for evaluation.6.1 Phrase-level EvaluationThe phrase-level evaluation was carried out to in-vestigate the contributions of the paraphrase tables.For each test sentence, all possible phrase substituteswere first extracted from the paraphrase tables andmanually labeled as ?correct?
or ?incorrect?.
Here,the criterion for identifying paraphrases is the sameas that described in Section 5.
Then, in the stageof decoding, the phrase substitutes were printed outand evaluated using the labeled data.Two metrics were used here.
The first is thenumber of distinct correct substitutes (#DCS).
Ob-viously, the more distinct correct phrase substitutesa paraphrase table can provide, the more valuable itis.
The second is the accuracy of the phrase substi-tutes, which is computed as:Accuracy =#correct phrase substitutes#all phrase substitutes(14)To evaluate the PTs learned from different re-sources, we first used each PT (from 1 to 6) alongwith PT-7 in decoding.
The results are shown in Ta-ble 1.
It can be seen that PT-4 is the most useful, asit provides the most correct substitutes and the ac-curacy is the highest.
We believe that it is becausePT-4 is much larger than the other PTs.
Comparedwith PT-4, the accuracies of the other PTs are fairlyPT combination #DCS Accuracy1+7 178 14.61%2+7 94 25.06%3+7 202 18.35%4+7 553 56.93%5+7 231 20.48%6+7 21 14.42%Table 1: Contributions of the paraphrase tables.PT-1: from the thesaurus; PT-2: from the monolingualparallel corpus; PT-3: from the monolingual comparablecorpus; PT-4: from the bilingual parallel corpus; PT-5:from the Encarta dictionary definitions; PT-6: from thesimilar MSN user queries; PT-7: self-paraphrases.low.
This is because those PTs are smaller, thus theycan provide fewer correct phrase substitutes.
As aresult, plenty of incorrect substitutes were includedin the top 10 generated paraphrases.PT-6 provides the least correct phrase substitutesand the accuracy is the lowest.
There are severalreasons.
First, many phrases in PT-6 are not realphrases but only sets of keywords (e.g., ?lottery re-sults ny?
), which may not appear in sentences.
Sec-ond, many words in this table have spelling mis-takes (e.g., ?widows vista?).
Third, some phrasepairs in PT-6 are not paraphrases but only ?relatedqueries?
(e.g., ?back tattoo?
vs. ?butterfly tattoo?
).Fourth, many phrases of PT-6 contain proper namesor out-of-vocabulary words, which are difficult to bematched.
The accuracy based on PT-1 is also quitelow.
We found that it is mainly because the phrasepairs in PT-1 are automatically clustered, many ofwhich are merely ?similar?
words rather than syn-onyms (e.g., ?borrow?
vs.
?buy?
).Next, we try to find out whether it is necessary tocombine all PTs.
Thus we conducted several runs,each of which added the most useful PT from theleft ones.
The results are shown in Table 2.
We cansee that all the PTs are useful, as each PT providessome new correct phrase substitutes and the accu-racy increases when adding each PT except PT-1.Since the PTs are extracted from different re-sources, they have different contributions.
Here weonly discuss the contributions of PT-5 and PT-6,which are first used in paraphrasing in this paper.PT-5 is useful for paraphrasing uncommon conceptssince it can ?explain?
concepts with their definitions.1026PT combination #DCS Accuracy4+7 553 56.93%4+5+7 581 58.97%4+5+3+7 638 59.42%4+5+3+2+7 649 60.15%4+5+3+2+1+7 699 60.14%4+5+3+2+1+6+7 711 60.16%Table 2: Performances of different combinations of para-phrase tables.For instance, in the following test sentence S1, theword ?amnesia?
is a relatively uncommon word, es-pecially for the people using English as the secondlanguage.
Based on PT-5, S1 can be paraphrasedinto T1, which is much easier to understand.S1: I was suffering from amnesia.T1: I was suffering from memory loss.The disadvantage of PT-5 is that substitutingwords with the definitions sometimes leads to gram-matical errors.
For instance, substituting ?heatshield?
in the sentence S2 with ?protective barrieragainst heat?
keeps the meaning unchanged.
How-ever, the paraphrased sentence T2 is ungrammatical.S2: The U.S. space agency has been cautiousabout heat shield damage.T2: The U.S. space administration has beencautious about protective barrier against heatdamage.As previously mentioned, PT-6 is less effectivecompared with the other PTs.
However, it is use-ful for paraphrasing some special phrases, such asdigital products, computer software, etc, since thesephrases often appear in user queries.
For example,S3 below can be paraphrased into T3 using PT-6.S3: I have a canon powershot S230 that usesCF memory cards.T3: I have a canon digital camera S230 thatuses CF memory cards.The phrase ?canon powershot?
can hardly beparaphrased using the other PTs.
It suggests that PT-6 is useful for paraphrasing new emerging conceptsand expressions.Test sentences Top-1 Top-5 Top-10All 150 55.33% 45.20% 39.28%50 from news 70.00% 62.00% 57.03%50 from novel 56.00% 46.00% 37.42%50 from forum 40.00% 27.60% 23.34%Table 3: Top-n accuracy on different test sentences.6.2 Sentence-level EvaluationIn this section, we evaluated the sentence-level qual-ity of the generated paraphrases11.
In detail, eachgenerated paraphrase was manually labeled as ?ac-ceptable?
or ?unacceptable?.
Here, the criterion forcounting a sentence T as an acceptable paraphrase ofsentence S is that T is understandable and its mean-ing is not evidently changed compared with S. Forexample, for the sentence S4, T4 is an acceptableparaphrase generated using our method.S4: The strain on US forces of fighting in Iraqand Afghanistan was exposed yesterday whenthe Pentagon published a report showing thatthe number of suicides among US troops is atits highest level since the 1991 Gulf war.T4: The pressure on US troops of fighting inIraq and Afghanistan was revealed yesterdaywhen the Pentagon released a report showingthat the amount of suicides among US forcesis at its top since the 1991 Gulf conflict.We carried out sentence-level evaluation using thetop-1, top-5, and top-10 results of each test sentence.The accuracy of the top-n results was computed as:Accuracytop?n =?Ni=1 niN ?
n(15)where N is the number of test sentences.
ni is thenumber of acceptable paraphrases in the top-n para-phrases of the i-th test sentence.We computed the accuracy on the whole test set(150 sentences) as well as on the three subsets, i.e.,the 50 news sentences, 50 novel sentences, and 50forum sentences.
The results are shown in table 3.It can be seen that the accuracy varies greatly ondifferent test sets.
The accuracy on the news sen-tences is the highest, while that on the forum sen-tences is the lowest.
There are several reasons.
First,11The evaluation was based on the paraphrasing results usingthe combination of all seven PTs.1027the largest PT used in the experiments is extractedusing the bilingual parallel data, which are mostlyfrom news documents.
Thus, the test set of newssentences is more similar to the training data.Second, the news sentences are formal while thenovel and forum sentences are less formal.
Espe-cially, some of the forum sentences contain spellingmistakes and grammar mistakes.Third, we find in the results that, most phrasesparaphrased in the novel and forum sentences arecommonly used phrases or words, such as ?food?,?good?, ?find?, etc.
These phrases are more dif-ficult to paraphrase than the less common phrases,since they usually have much more paraphrases inthe PTs.
Therefore, it is more difficult to choose theright paraphrase from all the candidates when con-ducting sentence-level paraphrase generation.Fourth, the forum sentences contain plenty ofwords such as ?board (means computer board)?,?site (means web site)?, ?mouse (means computermouse)?, etc.
These words are polysemous and haveparticular meanings in the domains of computer sci-ence and internet.
Our method performs poor whenparaphrasing these words since the domain of a con-text sentence is hard to identify.After observing the results, we find that there arethree types of errors: (1) syntactic errors: the gener-ated sentences are ungrammatical.
About 32% of theunacceptable results are due to syntactic errors.
(2)semantic errors: the generated sentences are incom-prehensible.
Nearly 60% of the unacceptable para-phrases have semantic errors.
(3) non-paraphrase:the generated sentences are well formed and com-prehensible but are not paraphrases of the input sen-tences.
8% of the unacceptable results are of thistype.
We believe that many of the errors above canbe avoided by applying syntactic constraints and bymaking better use of context information in decod-ing, which is left as our future work.7 ConclusionThis paper proposes a method that improves theSMT-based sentence-level paraphrase generationusing phrasal paraphrases automatically extractedfrom different resources.
Our contribution is thatwe combine multiple resources in the framework ofSMT for paraphrase generation, in which the dic-tionary definitions and similar user queries are firstused as phrasal paraphrases.
In addition, we analyzeand compare the contributions of different resources.Experimental results indicate that although thecontributions of the exploited resources differ a lot,they are all useful to sentence-level paraphrase gen-eration.
Especially, the dictionary definitions andsimilar user queries are effective for paraphrasingsome certain types of phrases.In the future work, we will try to use syntacticand context constraints in paraphrase generation toenhance the acceptability of the paraphrases.
In ad-dition, we will extract paraphrase patterns that con-tain more structural variation and try to combine theSMT-based and pattern-based systems for sentence-level paraphrase generation.AcknowledgmentsWe would like to thank Mu Li for providing us withthe SMT decoder.
We are also grateful to DongdongZhang for his help in the experiments.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Proceed-ings of ACL, pages 597-604.Regina Barzilay and Michael Elhadad.
1997.
Using Lex-ical Chains for Text Summarization.
In Proceedings ofthe ACL Workshop on Intelligent Scalable Text Sum-marization, pages 10-17.Regina Barzilay and Lillian Lee.
2003.
Learning to Para-phrase: An Unsupervised Approach Using Multiple-Sequence Alignment.
In Proceedings of HLT-NAACL,pages 16-23.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting Paraphrases from a Parallel Corpus.
In Pro-ceedings of ACL, pages 50-57.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
In Computational Linguistics 19(2): 263-311.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Trans-lation Using Paraphrases.
In Proceedings of HLT-NAACL, pages 17-24.Andrew Finch, Young-Sook Hwang, and EiichiroSumita.
2005.
Using Machine Translation Evalua-tion Techniques to Determine Sentence-level SemanticEquivalence.
In Proceedings of IWP, pages 17-24.1028Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,Kam-Fai Wong, and Hsiao-Wuen Hon.
2007.
Cross-Lingual Query Suggestion Using Query Logs of Dif-ferent Languages.
In Proceedings of SIGIR, pages463-470.Lidija Iordanskaja, Richard Kittredge, and AlainPolgue`re.
1991.
Lexical Selection and Paraphrase ina Meaning-Text Generation Model.
In Natural Lan-guage Generation in Artificial Intelligence and Com-putational Linguistics, pages 293-312.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for Automatic Evaluation.
In Proceedings of HLT-NAACL, pages 455-462.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models: User Manual and Description for Version1.2.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT-NAACL, pages 127-133.De-Kang Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of COLING/ACL,pages 768-774.De-Kang Lin and Patrick Pantel.
2001.
Discovery ofInference Rules for Question Answering.
In NaturalLanguage Engineering 7(4): 343-360.Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie J. Dorr.
2007.
Using Paraphrases for Parame-ter Tuning in Statistical Machine Translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 120-127.Kathleen R. Mckeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and Summarizing News ona Daily Basis with Columbia?s Newsblaster.
In Pro-ceedings of HLT, pages 280-285.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings of ACL,pages 440-447.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive Training and Maximum Entropy Models for Sta-tistical Machine Translation.
In Proceedings of ACL,pages 295-302.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based Alignment of Multiple Translations: Ex-tracting Paraphrases and Generating New Sentences.In Proceedings of HLT-NAACL, pages 102-109.Marius Pasca and Pe?ter Dienes.
2005.
Aligning Nee-dles in a Haystack: Paraphrase Acquisition Across theWeb.
In Proceedings of IJCNLP, pages 119-130.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
1992.
Numerical Recipesin C: The Art of Scientific Computing.
CambridgeUniversity Press, Cambridge, U.K., 1992, 412-420.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual Machine Translation for ParaphraseGeneration.
In Proceedings of EMNLP, pages 142-149.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing Surface Text Patterns for a Question AnsweringSystem.
In Proceedings of ACL, pages 41-47.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic Paraphrase Acquisition from NewsArticles.
In Proceedings of HLT, pages 40-46.Hua Wu and Ming Zhou.
2003.
Synonymous Collo-cation Extraction Using Translation Information.
InProceedings of ACL, pages 120-127.1029
