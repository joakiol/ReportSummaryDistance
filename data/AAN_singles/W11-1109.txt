Proceedings of the TextGraphs-6 Workshop, pages 60?68,Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational LinguisticsFrom ranked words to dependency trees: two-stage unsupervisednon-projective dependency parsingAnders S?gaardCenter for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkAbstractUsually unsupervised dependency parsingtries to optimize the probability of a corpusby modifying the dependency model that waspresumably used to generate the corpus.
Inthis article we explore a different view inwhich a dependency structure is among otherthings a partial order on the nodes in terms ofcentrality or saliency.
Under this assumptionwe model the partial order directly and derivedependency trees from this order.
The result isan approach to unsupervised dependency pars-ing that is very different from standard ones inthat it requires no training data.
Each sentenceinduces a model from which the parse is readoff.
Our approach is evaluated on data from 12different languages.
Two scenarios are consid-ered: a scenario in which information aboutpart-of-speech is available, and a scenario inwhich parsing relies only on word forms anddistributional clusters.
Our approach is com-petitive to state-of-the-art in both scenarios.1 IntroductionUnsupervised dependency parsers do not achievethe same quality as supervised or semi-supervisedparsers, but in some situations precision may be lessimportant compared to the cost of producing manu-ally annotated data.
Moreover, unsupervised depen-dency parsing is attractive from a theoretical pointof view as it does not rely on a particular style of an-notation and may potentially provide insights aboutthe difficulties of human language learning.Unsupervised dependency parsing has seen rapidprogress recently, with error reductions on English(Marcus et al, 1993) of about 15% in six years(Klein and Manning, 2004; Spitkovsky et al, 2010),and better and better results for other languages(Gillenwater et al, 2010; Naseem et al, 2010),but results are still far from what can be achievedwith small seeds, language-specific rules (Druck etal., 2009) or using cross-language adaptation (Smithand Eisner, 2009; Spreyer et al, 2010).The standard method in unsupervised dependencyparsing is to optimize the overall probability of thecorpus by assigning trees to its sentences that cap-ture general patterns in the distribution of part-of-speech (POS).
This happens in several iterationsover the corpus.
This method requires clever initial-ization, which can be seen as a kind of minimal su-pervision.
State-of-the-art unsupervised dependencyparsers, except Seginer (2007), also rely on manu-ally annotated text or text processed by supervisedPOS taggers.
Since there is an intimate relationshipbetween POS tagging and dependency parsing, thePOS tags can also be seen as a seed or as partial an-notation.
Inducing a model from the corpus is typi-cally a very slow process.This paper presents a new and very different ap-proach to unsupervised dependency parsing.
Theparser does not induce a model from a big corpus,but with a few exceptions only considers the sen-tence in question.
It does use a larger corpus toinduce distributional clusters and a ranking of keywords in terms of frequency and centrality, but thisis computationally efficient and is only indirectly re-lated to the subsequent assignment of dependencystructures to sentences.
The obvious advantage ofnot relying on training data is that we do not have to60worry about whether the test data reflects the samedistribution as the target data (domain adaptation),and since our models are much smaller, parsing willbe very fast.The parser assigns a dependency structure to a se-quence of words in two stages.
It first decoratesthe n nodes of what will become our dependencystructure with word forms and distributional clus-ters, constructs a directed acyclic graph from thenodes in O(n2), and ranks the nodes using iterativegraph-based ranking (Page and Brin, 1998).
Subse-quently, it constructs a tree from the ranked list ofwords using a simple O(n log n) parsing algorithm.Our parser is evaluated on the selection of 12dependency treebanks also used in Gillenwater etal.
(2010).
We consider two cases: parsing raw textand parsing text with information about POS.Strictly unsupervised dependency parsing is ofcourse a more difficult problem than unsuperviseddependency parsing of manually annotated POS se-quences.
Nevertheless our strictly unsupervisedparser, which only sees word forms, performs signif-icantly better than structural baselines, and it outper-forms the standard POS-informed DMV-EM model(Klein and Manning, 2004) on 3/12 languages.
Thefull parser, which sees manually annotated text, iscompetitive to state-of-the-art models such as E-DMV PR AS 140 (Gillenwater et al, 2010).11.1 PreliminariesThe observed variables in unsupervised dependencyparsing are a corpus of sentences s = s1, .
.
.
, snwhere each word wj in si is associated with a POStag pj .
The hidden variables are dependency struc-tures t = t1, .
.
.
, tn where si labels the vertices ofti.
Each vertex has a single incoming edge, possiblyexcept one called the root of the tree.
In this workand in most other work in dependency parsing, weintroduce an artificial root node so that all verticesdecorated by word forms have an incoming edge.A dependency structure such as the one in Fig-ure 1 is thus a tree decorated with labels and aug-mented with a linear order on the nodes.
Each edge(i, j) is referred to as a dependency between a headword wi and a dependent word wj and sometimes1Naseem et al (2010) obtain slightly better results, but onlyevaluate on six languages.
They made their code public, though:http://groups.csail.mit.edu/rbg/code/dependency/written wi ?
wj .
Let w0 be the artificial root ofthe dependency structure.
We use?+ to denote thetransitive closure on the set of edges.
Both nodesand edges are typically labeled.
Since a dependencystructure is a tree, it satisfies the following threeconstraints: A dependency structure over a sentences : w1, .
.
.
, wn is connected, i.e.
:?wi ?
s.w0 ?+ wiA dependency structure is also acyclic, i.e.:?
?wi ?
s.wi ?+ wiFinally, a dependency structure is single-headed,i.e.:?wi.?wj.
(w0 ?
wi ?
w0 ?
wj)?
wi = wjIf we also require that each vertex other than theartificial root node has an incoming edge we have acomplete characterization of dependency structures.In sum, a dependency structure is a tree with a lin-ear order on the leaves where the root of the treefor practical reasons is attached to an artificial rootnode.
The artificial root node makes it easier to im-plement parsing algorithms.Finally, we define projectivity, i.e.
whether thelinear order is projective wrt.
the dependency tree,as the property of dependency trees that if wi ?
wjit also holds that all words in between wi and wjare dominated by wi, i.e.
wi ?+ wk.
Intuitively,a projective dependency structure contains no cross-ing edges.
Projectivity is not a necessary propertyof dependency structures.
Some dependency struc-tures are projective, others are not.
Most if notall previous work in unsupervised dependency pars-ing has focused on projective dependency parsing,building on work in context-free parsing, but ourparser is guaranteed to produce well-formed non-projective dependency trees.
Non-projective pars-ing algorithms for supervised dependency parsinghave, for example, been presented in McDonald etal.
(2005) and Nivre (2009).1.2 Related workDependency Model with Valence (DMV) by Kleinand Manning (2004) was the first unsupervised de-pendency parser to achieve an accuracy for manually61POS-tagged English above a right-branching base-line.DMV is a generative model in which the sentenceroot is generated and then each head recursively gen-erates its left and right dependents.
For each si ?
s,ti is assumed to have been built the following way:The arguments of a head h in direction d are gen-erated one after another with the probability that nomore arguments of h should be generated in direc-tion d conditioned on h, d and whether this would bethe first argument of h in direction d. The POS tag ofthe argument of h is generated given h and d. Kleinand Manning (2004) use expectation maximization(EM) to estimate probabilities with manually tunedlinguistically-biased priors.Smith and Eisner (2005) use contrastive es-timation instead of EM, while Smith and Eis-ner (2006) use structural annealing which penal-izes long-distance dependencies initially, graduallyweakening the penalty during training.
Cohen etal.
(2008) use Bayesian priors (Dirichlet and Logis-tic Normal) with DMV.
All of the above approachesto unsupervised dependency parsing build on thelinguistically-biased priors introduced by Klein andManning (2004).In a similar way Gillenwater et al (2010) tryto penalize models with a large number of dis-tinct dependency types by using sparse posteriors.They evaluate their system on 11 treebanks from theCoNLL 2006 Shared Task and the Penn-III treebankand achieve state-of-the-art performance.An exception to using linguistically-biased priorsis Spitkovsky et al (2009) who use predictions onsentences of length n to initialize search on sen-tences of length n+ 1.
In other words, their methodrequires no manual tuning and bootstraps itself onincreasingly longer sentences.A very different, but interesting, approach is takenin Brody (2010) who use methods from unsuper-vised word alignment for unsupervised dependencyparsing.
In particular, he sees dependency parsingas directional alignment from a sentence (possibledependents) to itself (possible heads) with the mod-ification that words cannot align to themselves; fol-lowing Klein and Manning (2004) and the subse-quent papers mentioned above, Brody (2010) con-siders sequences of POS tags rather than raw text.Results are below state-of-the-art, but in some casesbetter than the DMV model.2 Ranking dependency tree nodesThe main intuition behind our approach to unsuper-vised dependency parsing is that the nodes near theroot in a dependency structure are in some sense themost important ones.
Semantically, the nodes nearthe root typically express the main predicate andits arguments.
Iterative graph-based ranking (Pageand Brin, 1998) was first used to rank webpagesaccording to their centrality, but the technique hasfound wide application in natural language process-ing.
Variations of the algorithm presented in Pageand Brin (1998) have been used in keyword extrac-tion and extractive summarization (Mihalcea and Ta-rau, 2004), word sense disambiguation (Agirre andSoroa, 2009), and abstractive summarization (Gane-san et al, 2010).
In this paper, we use it as the firststep in a two-step unsupervised dependency parsingprocedure.The parser assigns a dependency structure to a se-quence of words in two stages.
It first decoratesthe n nodes of what will become our dependencystructure with word forms and distributional clus-ters, constructs a directed acyclic graph from thenodes in O(n2), and ranks the nodes using iterativegraph-based ranking.
Subsequently, it constructsa tree from the ranked list of words using a sim-ple O(n log n) parsing algorithm.
This section de-scribes the graph construction step in some detailand briefly describes the iterative graph-based rank-ing algorithm used.The first step, however, is assigning distributionalclusters to the words in the sentence.
We use a hi-erarchical clustering algorithm to induce 500 clus-ters from the treebanks using publicly available soft-ware.2 This procedure is quadratic in the number ofclusters, but linear in the size of the corpus.
Thecluster names are bitvectors (see Figure 1).2.1 EdgesThe text graph is now constructed by adding dif-ferent kinds of directed edges between nodes.
Theedges are not weighted, but multiple edges betweennodes will make transitions between these nodes in2http://www.cs.berkeley.edu/?pliang/software/brown-cluster-1.2.zip62iterative graph-based ranking more likely.
The dif-ferent kinds of edges play the same role in our modelas the rule templates in the DMV model, and theyare motivated below.Some of the edge assignments discussed belowmay seem rather heuristic.
The edge template wasdeveloped on development data from the EnglishPenn-III treebank (Marcus et al, 1993).
Our edgeselection was incremental considering first an ex-tended set of candidate edges with arbitrary param-eters and then considering each edge type at a time.If the edge type was helpful, we optimized any pos-sible parameters (say context windows) and went onto the next edge type: otherwise we disregarded it.3Following data set et al (2010), we apply the bestsetting for English to all other languages.Vine edges.
Eisner and Smith (2005) motivate a vineparsing approach to supervised dependency parsingarguing that language users have a strong prefer-ence for short dependencies.
Reflecting preferencefor short dependencies, we first add links betweenall words and their neighbors and neighbors?
neigh-bors.
This also guarantees that the final graph is con-nected.Keywords and closed class words.
We use a key-word extraction algorithm without stop word lists toextract non-content words and the most importantcontent words, typically nouns.
The algorithm is acrude simplification of TextRank (Mihalcea and Ta-rau, 2004) that does not rely on linguistic resources,so that we can easily apply it to low-resource lan-guages.
Since we do not use stop word lists, highlyranked words will typically be non-content words,followed by what is more commonly thought of askeywords.
Immediate neighbors to top-100 wordsare linked to these words.
The idea is that non-content words may take neighboring words as ar-guments, but dependencies are typically very local.The genuine keywords, ranked 100?1000, may beheads of dependents further away, and we thereforeadd edges between these words wi and their neigh-boring words wj if |i?
j| ?
4.Head-initial/head-final.
It is standard in unsuper-vised dependency parsing to compare against a3The search was simplified considerably.
For example, weonly considered symmetric context windows, where left contextlength equals length of right context, and we binned this lengthconsidering only values 1, 2, 4, 8 and all.structural baseline; either left-attach, i.e.
all wordsattach to their left neighbor, or right-attach.
Whichstructural baseline is used depends on the languagein question.
It is thus assumed that we know enoughabout the language to know what structural baselineperforms best.
It is therefore safe to incorporate thisknowledge in our unsupervised parsers; our parsersare still as ?unsupervised?
as our baselines.
If a lan-guage has a strong left-attach baseline, like Bulgar-ian, the first word in the sentence is likely to be verycentral for reasons of economy of processing.
Thelanguage is likely to be head-initial.
On the otherhand, if a language has a strong right-attach base-line, like Turkish, the last word is likely to be cen-tral.
The language is likely to be head-final.
Somelanguages like Slovene have strong (< 20%) left-attach and right-attach baselines, however.
We in-corporate the knowledge that a language has a strongleft-attach or right-attach baseline if more than onethird of the dependencies are attachments to a im-mediate left, resp.
right, neighbor.
Specifically, weadd edges from all nodes to the first element in thesentence if a language has a strong left-attach base-line; and from all edges to the last (non-punctuation)element in the sentence if a language has a strongright-attach baseline.Word inequality.
An edge is added between twowords if they have different word forms.
It is notvery likely that a dependent and a head have thesame word form.Cluster equality.
An edge is added between twowords if they are neighbors or neighbors?
neighborsand belong to the same clusters.
If so, the two wordsmay be conjoined.Morphological inequality.
If two words wi, wj inthe same context (|i ?
j| ?
4) share prefix or suf-fix, i.e.
the first or last three letters, we add an edgebetween them.2.2 Edges using POSVerb edges.
All words are attached to all words witha POS tag beginning with ?V.
.
.
?.Finally, when we have access to POS information,we do not rely on vine edges besides left-attach, andwe do not rely on keyword edges or suffix edges ei-ther.632.3 RankingGiven the constructed graph we rank the nodes usingthe algorithm in Page and Brin (1998), also knownas PageRank.
The input to this algorithm is any di-rected graph G = ?E,V ?
and the output is an as-signment PR : V ?
R of a score, also referred toas PageRank, to each vertex in the graph such that allscores sum to 1.
A simplified version of PageRankcan be defined recursively as:PR(v) = ?w?BvPR(w)L(w)where Bv is the set of vertices such that (w, v) ?E, and L(w) is the number of outgoing links fromw, i.e.
|{(u, u?
)|(u, u?)
?
E, u = w}|.
In addi-tion to this, Page and Brin (1998) introduces a so-called damping factor to reflect that fact that Internetusers do not continue crawling web sites forever, butrestart, returning to random web sites.
This influ-ences centrality judgments and therefore should bereflected in the probability assignment.
Since thereis no obvious analogue of this in our case, we sim-plify the PageRank algorithm and do not incorpo-rate damping (or equivalent, set the damping factorto 1.0).Note, by the way, that although our graphs arenon-weighted and directed, like a graph of webpages and hyperlinks (and unlike the text graphs inMihalcea and Tarau (2004), for example), severalpairs of nodes may be connected by multiple edges,making a transition between them more probable.Multiple edges provide a coarse weighting of the un-derlying minimal graph.2.4 ExampleIn Figure 1, we see an example graph of word nodes,represented as a matrix, and a derived dependencystructure.4 We see that there are four edges fromThe to market and six from The to crumbles, forexample.
We then compute the PageRank of eachnode using the algorithm described in Page andBrin (1998); see also Figure 1.
The PageRank val-ues rank the nodes or the words.
In Sect.
3, we de-scribe a method for building a dependency tree from4The dependency structure in Figure 1 contains dependencylabels such as ?SBJ?
and ?ROOT?.
These are just included forreadability.
We follow the literature on unsupervised depen-dency parsing and focus only on unlabeled dependency parsing.from/to The market crumbled .The 0 4 6 3market 4 0 5 3crumbled 4 4 0 4.
3 4 6 0PR(%) 22.8 24.1 30.3 22.7Figure 1: Graph, pagerank (PR) and predicted depen-dency structure for sentence 5, PTB-III Sect.
23.a ranking of the nodes.
This method will producethe correct analysis of this sentence; see Figure 1.This is because the PageRank scores reflect syntac-tic superiority; the root of the sentence typically hasthe highest rank, and the least important nodes areranked lowly.3 From ranking of nodes to dependencytreesConsider the example in Figure 1 again.
Once wehave ranked the nodes in our dependency structure,we build a dependency structure from it using theparsing algorithm in Figure 2.
The input of thegraph is a list of ranked words pi = ?n1, .
.
.
, nm?,where each node ni corresponds to a sentence posi-tion npr2ind(i) decorated by a word form wpr2ind(i),where pr2ind : {1, .
.
.
,m} ?
{1, .
.
.
,m} is amapping from rank to sentence position.The interesting step in the algorithm is the headselection step.
Each word is assigned a head takenfrom all the previously used heads and the word towhich a head was just assigned.
Of these words,we simply select the closest head.
If two possibleheads are equally close, we select the one with high-est PageRank.Our parsing algorithm runs in O(n log n), sinceit runs over the ranked words in a single pass con-sidering only previously stored words as possibleheads, and guarantees connectivity, acyclicity andsingle-headedness, and thus produces well-formednon-projective dependency trees.
To see this, re-member that wellformed dependency trees are suchthat all nodes but the artificial root nodes have a sin-gle incoming edge.
This follows immediately from641: pi = ?n1, .
.
.
, nm?
# the ranking of nodes2: H = ?n0?
# possible heads3: D = ?
# dependency structure4: pr2ind : {1, .
.
.
,m} ?
{1, .
.
.
,m} # a mapping from rank to sentence position5: for ni ?
pi do6: if |H|=1 then7: c = 0 # used to ensure single-headedness8: else9: c = 110: end if11: nj?
= argminnj?H[c:] |pr2ind(i)?
pr2ind(j)| # select head of wj12: H = ni ?H # make ni a possible head13: D = {(wpr2ind (i) ?
wpr2ind(j?))}
?D # add new edge to D14: end for15: return DFigure 2: Parsing algorithm.the fact that each node is assigned a head (line 11).Furthermore, the dependency tree must be acyclic.This follows immediately from the fact that a wordcan only attach to a word with higher rank than it-self.
Connectivity follows from the fact that thereis an artificial root node and that all words attach tothis node or to nodes dominated by the root node.Finally, we ensure single-headedness by explicitlydisregarding the root node once we have attached thenode with highest rank to it (line 6?7).
Our parsingalgorithm does not guarantee projectivity, since theiterative graph-based ranking of nodes can permutethe nodes in any order.4 ExperimentsWe use exactly the same experimental set-up asGillenwater et al (2010).
The edge model wasdeveloped on development data from the EnglishPenn-III treebank (Marcus et al, 1993), and we eval-uate on Sect.
23 of the English treebanks and the testsections of the remaining 11 treebanks, which wereall used in the CoNLL-X Shared Task (Buchholz andMarsi, 2006).
Gillenwater et al (2010) for somereason did not evaluate on the Arabic and Chinesetreebanks also used in the shared task.
We also fol-low Gillenwater et al (2010) in only evaluating ourparser on sentences of at most 10 non-punctuationwords and in reporting unlabeled attachment scoresexcluding punctuation.4.1 Strictly unsupervised dependency parsingWe first evaluate the strictly unsupervised parsingmodel that has no access to POS information.
Sincewe are not aware of other work in strictly unsuper-vised multi-lingual dependency parsing, so we com-pare against the best structural baseline (left-attachor right-attach) and the standard DMV-EM modelof Klein and Manning (2004).
The latter, however,has access to POS information and should not bethought of as a baseline.
Results are presented inFigure 3.It is interesting that we actually outperform DMV-EM on some languages.
On average our scores aresignificantly better (p < 0.01) than the best struc-tural baselines (3.8%), but DMV-EM with POS tagsis still 3.0% better than our strictly unsupervisedmodel.
For English, our system performs a lot worsethan Seginer (2007).4.2 Unsupervised dependency parsing(standard)We then evaluate our unsupervised dependencyparser in the more standard scenario of parsing sen-tences annotated with POS.
We now compare our-selves to two state-of-the-art models, namely DMVPR-AS 140 and E-DMV PR-AS 140 (Gillenwater etal., 2010).
Finally, we also report results of the IBMmodel 3 proposed by Brody (2010) for unsuperviseddependency parsing, since this is the only recent pro-65baseline EM oursBulgarian 37.7 37.8 41.9Czech 32.5 29.6 28.7Danish 43.7 47.2 43.7Dutch 38.7 37.1 33.1English 33.9 45.8 36.1German 27.2 35.7 36.9Japanese 44.7 52.8 56.5Portuguese 35.5 35.7 35.2Slovene 25.5 42.3 30.0Spanish 27.0 45.8 38.4Swedish 30.6 39.4 34.5Turkish 36.6 46.8 45.9AV 34.5 41.3 38.3Figure 3: Unlabeled attachment scores (in %) on raw text.
(EM baseline has access to POS.
)posal we are aware of that departs significantly fromthe DMV model.
The results are presented in Fig-ure 4.Our results are on average significantly better thanDMV PR-AS 140 (2.5%), and better than DMV PR-AS 140 on 8/12 languages.
E-DMV PR-AS 140 isslightly better than our model on average (1.3%),but we still obtain better results on 6/12 languages.Our results are a lot better than IBM-M3.
Naseemet al (2010) report better results than ours on Por-tuguese, Slovene, Spanish and Swedish, but worseon Danish.5 Error analysisIn our error analysis, we focus on the results forGerman and Turkish.
We first compare the resultsof the strictly unsupervised model on German withthe results on German text annotated with POS.
Themain difference between the two models is that morelinks to verbs are added to the sentence graph priorto ranking nodes when parsing text annotated withPOS.
For this reason, the latter model improves con-siderably in attaching verbs compared to the strictlyunsupervised model:acc strict-unsup unsupNN 43% 48%NE 41% 39%VVFIN 31% 100%VAFIN 9% 86%VVPP 13% 53%While the strictly unsupervised model is about asFigure 5: Predicted dependency structures for sentence 4in the German test section; strictly unsupervised (above)and standard (below) approach.
Red arcs show wrongdecisions.good at attaching nouns as the model with POS, itis much worse attaching verbs.
Since more linksto verbs are added, verbs receive higher rank, andthis improves f-scores for attachments to the artifi-cial root node:f-score strict-unsup unsupto root 39.5% 74.0%1 62.3% 69.6%2 7.4% 24.4%3?6 0 22.4%7 0 0This is also what helps the model with POS whenparsing the example sentence in Figure 5.
The POS-informed parser also predicts longer dependencies.The same pattern is observed in the Turkish data,but perhaps less dramatically so:acc strict-unsup unsupNoun 43% 42%Verb 41% 51%The increase in accuracy is again higher withverbs than with nouns, but the error reduction washigher for German.f-score strict-unsup unsupto root 57.4% 90.4%1 65.7% 69.6%2 32.1% 26.5%3?6 11.6% 24.7%7 0 12.5%The parsers predict more long dependencies forTurkish than for German; precision is generallygood, but recall is very low.6 ConclusionWe have presented a new approach to unsuperviseddependency parsing.
The key idea is that a depen-66DMV PR-AS 140 E-DMV PR-AS 140 ours IBM-M3Bulgarian 54.0 59.8 52.5Czech 32.0 54.6 42.8Danish 42.4 47.2 55.2 41.9Dutch 37.9 46.6 49.4 35.3English 61.9 64.4 50.2 39.3German 39.6 35.7 50.4Japanese 60.2 59.4 58.3Portuguese 47.8 49.5 52.8Slovene 50.3 51.2 44.1Spanish 62.4 57.9 52.1Swedish 38.7 41.4 45.5Turkish 53.4 56.9 57.9AV 48.4 52.2 50.9Figure 4: Unlabeled attachment scores (in %) on text annotated with POS.dency structure also expresses centrality or saliency,so by modeling centrality directly, we obtain infor-mation that we can use to build dependency struc-tures.
Our unsupervised dependency parser thusworks in two stages; it first uses iterative graph-based ranking to rank words in terms of central-ity and then constructs a dependency tree from theranking.
Our parser was shown to be competitive tostate-of-the-art unsupervised dependency parsers.ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In EACL.Samuel Brody.
2010.
It depends on the translation: un-supervised dependency parsing via word alignment.
InEMNLP.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InCoNLL.Shay Cohen, Kevin Gimpel, and Noah Smith.
2008.
Un-supervised bayesian parameter estimation for depen-dency parsing.
In NIPS.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2009.
Semi-supervised learning of dependencyparsers using generalized expectation criteria.
In ACL-IJCNLP.Jason Eisner and Noah A. Smith.
2005.
Parsing with softand hard constraints on dependency length.
In IWPT.K Ganesan, C Zhai, and J Han.
2010.
Opinosis: a graph-based approach to abstractive summarization of highlyredudant opinions.
In COLING.Jennifer Gillenwater, Kuzman Ganchev, Joao Graca, Fer-nando Pereira, and Ben Taskar.
2010.
Sparsity in de-pendency grammar induction.
In ACL.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In ACL.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Lin-guistics, 19(2):313?330.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In HLT-EMNLP.Rada Mihalcea and Paul Tarau.
2004.
Textrank: bringingorder into texts.
In EMNLP.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowledgeto guide grammar induction.
In EMNLP.Joakim Nivre.
2009.
Non-projective dependency parsingin expected linear time.
In ACL-IJCNLP.Larry Page and Sergey Brin.
1998.
The anatomy of alarge-scale hypertextual web search engine.
In Inter-national Web Conference.Yoav Seginer.
2007.
Fast unsupervised incremental pars-ing.
In ACL.Noah Smith and Jason Eisner.
2005.
Contrastive estima-tion: training log-linear models on unlabeled data.
InACL.Noah Smith and Jason Eisner.
2006.
Annealing struc-tural bias in multilingual weighted grammar induction.In COLING-ACL.David Smith and Jason Eisner.
2009.
Parser adaptationand projection with quasi-synchronous grammar fea-tures.
In EMNLP.Valentin Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2009.
Baby steps: how ?less is more?
in unsu-pervised dependency parsing.
In NIPS Workshop onGrammar Induction, Representation of Language andLanguage Learning.67Valentin Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,and Christopher Manning.
2010.
Viterbi training im-proves unsupervised dependency parsing.
In CoNNL.Kathrin Spreyer, Lilja ?vrelid, and Jonas Kuhn.
2010.Training parsers on partial trees: a cross-languagecomparison.
In LREC.68
