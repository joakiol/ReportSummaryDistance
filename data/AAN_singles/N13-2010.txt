Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 69?76,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsDomain-Independent Captioning of Domain-Specific ImagesRebecca MasonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912rebecca@cs.brown.eduAbstractAutomatically describing visual content is anextremely difficult task, with hard AI prob-lems in Computer Vision (CV) and NaturalLanguage Processing (NLP) at its core.
Pre-vious work relies on supervised visual recog-nition systems to determine the content of im-ages.
These systems require massive amountsof hand-labeled data for training, so the num-ber of visual classes that can be recognized istypically very small.
We argue that these ap-proaches place unrealistic limits on the kindsof images that can be captioned, and are un-likely to produce captions which reflect hu-man interpretations.We present a framework for image captiongeneration that does not rely on visual recog-nition systems, which we have implementedon a dataset of online shopping images andproduct descriptions.
We propose future workto improve this method, and extensions forother domains of images and natural text.1 IntroductionAs the number of images on the web continues to in-crease, the task of automatically describing imagesbecomes especially important.
Image captions canprovide background information about what is seenin the image, can improve accessibility of websitesfor visually-impaired users, and can improve im-age retrieval by providing text to search user queriesagainst.
Typically, online search engines rely on col-located textual information to resolve queries, ratherthan analyzing visual content directly.
Likewise,earlier image captioning research from the NaturalLanguage Processing (NLP) community use collo-cated information such as news articles or GPS co-ordinates, to decide what information to include inthe generated caption (Deschacht and Moens, 2007;Aker and Gaizauskas, 2010; Fan et al 2010; Fengand Lapata, 2010a).However, in some instances visual recognition isnecessary because collocated information is miss-ing, irrelevant, or unreliable.
Recognition is a clas-sic Computer Vision (CV) problem including taskssuch as recognizing instances of object classes inimages (such as car, cat, or sofa); classifyingimages by scene (such as beach or forest); ordetecting attributes in an image (such as woodenor feathered).
Recent works in image captiongeneration represent visual content via the outputof trained recognition systems for a pre-defined setof visual classes.
They then use linguistic modelsto correct noisy initial detections (Kulkarni et al2011; Yang et al 2011), and generate more natural-sounding text (Li et al 2011; Mitchell et al 2012;Kuznetsova et al 2012).A key problem with this approach is that it as-sumes that image captioning is a grounding prob-lem, with language acting only as labels for visualmeaning.
One good reason to challenge this assump-tion is that it imposes unrealistic constraints on thekinds of images that can be automatically described.Previous work only recognizes a limited number ofvisual classes ?
typically no more than a few dozenin total ?
because training CV systems requires ahuge amount of hand-annotated data.
For example,the PASCAL VOC dataset1 has 11,530 training im-1http://pascallin.ecs.soton.ac.uk/69ages with 27,450 labeled objects, in order to learnonly 20 object classes.
Since training visual recog-nition systems is such a burden, ?general-domain?image captioning datasets are limited by the currenttechnology.
For example, the SBU-Flickr dataset(Ordonez et al 2011), which contains 1 million im-ages and captions, is built by first querying Flickrusing a pre-defined set of queries, then further filter-ing to remove instances where the caption does notcontain at least two words belonging to their termlist.
Furthermore, detections are too noisy to gener-ate a good caption for the majority of images.
Forexample, Kuznetsova et al(2012) select their testset according to which images receive the most con-fident visual object detection scores.We instead direct our attention to the domain-specific image captioning task, assuming that weknow a general object or scene category for thequery image, and that we have access to a dataset ofimages and captions from the same domain.
Whilesome techniques may be unrealistic in assuming thathigh-quality collocated text is always available, as-suming that there is no collocated information atall is equally unrealistic.
Data sources such asfile names, website text, Facebook likes, and websearches all provide clues to the content of an im-age.
Even an image file by itself carries metadataon where and when it was taken, and the camerasettings used to take it.
Since visual recognition ismuch easier for domain-specific tasks, there is morepotential for natural language researchers to do re-search that will impact the greater community.Finally, labeling visual content is often notenough to provide an adequate caption.
The mean-ing of an image to a user is more than just listing theobjects in the image, and can even change for dif-ferent users.
This problem is commonly known as?bridging the semantic gap?
:?The semantic gap is the lack of coinci-dence between the information that onecan extract from the visual data and theinterpretation that the same data have fora user in a given situation.
A linguis-tic description is almost always contex-tual, whereas an image may live by itself.?
(Smeulders et al 2000)challenges/VOC/General-domain models of caption generation fail tocapture context because they assume that all the rel-evant information has been provided in the image.However, training models on data from the same do-main gives implicit context about what informationshould be provided in the generated text.This thesis proposes a framework for image cap-tioning that does not require supervision in the formof hand-labeled examples.
We train a topic model ona corpus of images and captions in the same domain,in order to jointly learn image features and naturallanguage descriptions.
The trained topic model isused to estimate the likelihood of words appearingin a caption, given an unseen query image.
We thenuse these likelihoods to rewrite an extracted human-written caption to accurately describe the query im-age.
We have implemented our framework using adataset of online shopping images and captions, andpropose to extend this model to other domains, in-cluding natural images.2 FrameworkIn this section, we provide an overview of our im-age captioning framework, as it is currently imple-mented.
As shown in Figure 1, the data that we useare a set of images and captions in a specific do-main, and a query image that is from the same do-main, but is not included in the training data.
Thetraining data is used in two ways: for sentence ex-traction from the captions of training images thatare visually similar to the query image overall; andfor training a topic model of individual words andlocal image features, in order to capture fine-graineddetails.
Finally, a sentence compression algorithmis used to remove details from the extracted captionsthat do not fit the query image.The work that we have done so far has been imple-mented using the Attribute Discovery Dataset (Berget al 2010), a publicly available dataset of shop-ping images and product descriptions.2 Here, werun our framework on the women?s shoes section,which has over 14000 images and captions, rep-resenting a wide variety of attributes for texture,shapes, materials, colors, and other visual quali-ties.
The women?s shoes section is formally split2http://tamaraberg.com/attributesDataset/index.html70Figure 1: Overview of our framework for image caption generation.into ten subcategories, such as wedding shoes,sneakers, and rainboots.
However, many ofthe subcategories contain multiple visually distinctkinds of shoes.
We do not make use of the sub-categories, instead we group all of the categories ofshoe images together.
The shoes in the images aremostly posed against solid color backgrounds, whilethe captions have much more variability in lengthand linguistic quality.For our thesis work, we intend to extend our cur-rent framework to different domains of data, includ-ing natural images.
However, it is important to pointout that no part of the framework as it is currentlyimplemented is specific to describing shoes or shop-ping images.
This will be described in Section 4.2.1 Sentence ExtractionGIST (Oliva and Torralba, 2001) is a global imagedescriptor which describes how gradients are ori-ented in different regions of an image.
It is com-monly used for classifying background scenes inimages, however images in the Attribute DiscoveryDataset do not have ?backgrounds?
per se.
Instead,we treat the overall shape of the object as the ?scene?and extract a caption sentence using GIST nearestneighbors between the query image and the imagesin the training set.
Because similar objects and at-tributes tend to appear in similar scenes, we expectthat at least some of the extracted caption will de-scribe local attributes that are also in the query im-age.
The rest of our framework finds and removesthe parts of the extracted caption that are not accu-rate to the query image.2.2 Topic ModelImage captions often act as more than labels of vi-sual content.
Some visual ideas can be describedusing several different words, while others are typ-ically not described at all.
Likewise, some wordsdescribe background information that is not shownvisually, or contextual information that is interpretedby the user.
Rather than modeling images and textsuch that one generates the other, we a topic modelbased on LDA (Blei et al 2003) where both an im-age and its caption are generated by a shared latentdistribution of topics.Previous work by (Feng and Lapata, 2010b)shows that topic models where image features or re-gions generate text features (such as Blei and Jor-dan (2003)) are not appropriate for modeling imageswith captions or other collocated text.
We use a topicmodel designed for multi-lingual data, specificallythe Polylingual Topic Model (Mimno et al 2009).This model was developed for correlated documentsin different languages that are topically similar, butare not direct translations, such as Wikipedia ornews articles in different languages.
We train thetopic model with images and text as two languages.For query images, we estimate the topic distribu-tion that generated just the image, and then In themodel, images and their captions are represented us-ing bag-of-words, a commonly-used technique fordocument representation in both CV and NLP re-search.
The textual features are non-function wordsin the model, including words that describe specificobjects or attributes (such as boot, snake-skin,buckle, and metallic) in addition to words thatdescribe more abstract attributes and affordances(such as professional, flirty, support,71Original: Go all-out glam in the shimmer-ing Dyeables Roxie sandals.
Metallic fauxleather upper in a dress thong sandal stylewith a round open toe.
...Original: Find the softness of shearling combined with sup-port in this clog slipper.
The cork footbed mimics the foot?snatural shape, offering arch support, while a flexible outsoleflexes with your steps and resists slips.
...Original: Perforated leather with cap toe andbow detail.Extracted: Shimmering snake-embossed leather upper in a slingbackevening dress sandal style with a roundopen toe .Extracted: This sporty sneaker clogkeeps foot cool and comfortable andfully supported.Extracted: Italian patent leather peep-toe ballet flat with a signature tailoredgrosgrain bow .System: Shimmering upper in a sling-back evening dress sandal style with around open toe .System: This clog keeps foot comfort-able and supported.System: leather ballet flat with a signa-ture tailored grosgrain bow .Table 1: Some examples of shoes images from the Attribute Discovery Dataset and performance with our imagecaptioning model.
Left: Correctly removes explicitly visual feature ?snake-embossed leather?
from extraction; leavesin correct visual attributes ?shimmering?, ?slingback?, and ?round open toe?.
Center: Extracted sentence with somecontextually visual attributes; the model correctly infers that ?sporty?
and ?cool?
are not likely given an image of awool bedroom slipper, but ?comfortable?
and ?supported?
are likely because of the visible cork soles.
Right: Extractedsentence with some non-visual attributes; model removes ?Italian?
but keeps ?signature tailored?.and waterproof).
For ?image words?, we com-pute features at several points in the image such asthe color values of pixels, the angles of edges orcorners, and response to various filters, and clusterthem into discrete image words.
However, the in-formation that an image word conveys is very dif-ferent than the information conveyed in a text word,so models which require direct correspondence be-tween features in the two modalities would not beappropriate here.We train the topic model with images and text astwo languages.
We estimate the probabilities of tex-tual words given a query image by first estimatingthe topic distribution that generated the image, andthen using the same distribution to find the probabil-ities of textual words given the query image.
How-ever, we also perform an annotation task similarlyto Feng and Lapata (2010b), in order to evaluatethe topic model on its own.
Our method has a 30-35% improvement in finding words from the held-out image caption, compared to previous methodsand baselines.2.3 Sentence Compression via CaptionGenerationWe describe an ILP for caption generation, draw-ing inspiration from sentence compression work byClarke and Lapata (2008).
The ILP has three in-puts: the extracted caption; the prior probabilitieswords appearing in captions, p(w); and their pos-terior probabilities of words appearing in captionsgiven the query image, p(w|query).
The latter isestimated using the topic model we have just de-scribed.
The output of the ILP is a compressed im-age caption where the inaccurate words have beendeleted.Objective: The formal ILP objective3 is to max-imize a weighted linear combination of two mea-sures.
The first we define as?ni=1 ?i ?
I(wi), wherewi, ..., wn are words in the extracted caption, ?i is abinary decision variable which is true if we includewi in the compressed output, and I(wi) is a score forthe accuracy of each word.
For non-function words,3To formulate this problem as a linear program, the proba-bilities are actually log probabilities, but we omit the logs in thispaper to save space.72I(wi) = p(w|query)?p(w), which can have a pos-itive or negative value.
We do not use p(wi|query)directly in order to distinguish between cases wherep(wi|query) is low because wi is inaccurate, andcases where p(wi|query) is low because p(wi) islow generally.
Function words do not affect the ac-curacy of the generated caption, so I(wi) = 0.The second measure in the objective is a tri-gram language model, described in detail in Clarke(2008).
In the original sentence compression task,the language model is a component as it naturallyprefers shorter output sentences.
However, our ob-jective is not to generate a shorter caption, but togenerate a more accurate caption.
However, we stillinclude the language model in the objective, with aweighting factor , as it helps remove unnecessaryfunction words and help reduce the search space ofpossible sentence compressions.Constraints: The ILP constraints include sequen-tial constraints to ensure the mathematical validityof the model, and syntactic constraints that ensurethe grammatical correctness of the compressed sen-tence.
We do not have space here to describe allof the constraints, but basically, using the ?semantichead?
version of the headfinder from Collins (1999),we constrain that the head word of the sentence andthe head word of the sentence?s object cannot bedeleted, and for any word that we include in the out-put sentence, we must include its head word as well.We also have constraints that define valid use of co-ordinating conjunctions and punctuation.We evaluate generated captions using automaticmetrics such as BLEU (Papineni et al 2002) andROUGE (Lin, 2004).
These metrics are commonlyused in summarization and translation research andhave been previously used in image captioning re-search to compare automatically generated captionsto human-written captions for each image (Ordonezet al 2011; Yang et al 2011; Kuznetsova et al2012).
Although human-written captions may usesynonyms to describe a visual object or attribute, oreven describe entirely different attributes than whatis described in the generated captions, computingthe automatic metrics over a large test set finds sta-tistically significant improvements in the accuracyof the extracted and compressed captions over ex-traction alone.For our proposed work (Section 4), we also planto perform manual evaluations of our captions basedon their content and language quality.
However,cross-system comparisons would be more difficultbecause our method uses an entirely different kindof data.
In order to compare our work to relatedmethods (Section 3), we would have to train for vi-sual recognition systems for hundreds of visual at-tributes, which would mean having to hand-label theentire dataset.3 Related Work in Image CaptioningIn addition to visual recognition, caption genera-tion is a very challenging problem.
In some ap-proaches, sentences are constructed using templatesor grammar rules, where content words are selectedaccording to the output of visual recognition systems(Kulkarni et al 2011; Yang et al 2011; Mitchell etal., 2012).
Function words, as well as words likeverbs and prepositions which are difficult to rec-ognize visually, may be selected using a languagemodel trained on non-visual text.
There is also simi-lar work that uses large-scale ngram models to makethe generated output sound more natural (Li et al2011).In other approaches, captions are extracted inwhole or in part from similar images in a database.For example, Farhadi et al(2010) and Ordonez etal.
(2011) build semantic representations for visualcontent of query images, and extract captions fromdatabase images with similar content.
Kuznetsova etal.
(2012) extract phrases corresponding to classes ofobjects and scenes detected in the query image, andcombine extracted phrases into a single sentence.Our work is different than these approaches, becausewe directly measure how visually relevant individualwords are, rather than only using visual similarity toextract sentences or phrases.Our method is most similar to that of Feng andLapata (2010a), who generate captions for news im-ages.
Like them, we train an LDA-like model onboth images and text to find latent topics that gener-ate both.
However, their model requires both an im-age and collocated text (a news article) to estimatethe topic distribution for an unseen image, while ourtopic model only needs related text for the trainingdata.
They also use the news article to help gen-erate captions, which means that optimizing their73generated output for content and grammaticality isa much easier problem.
Although their model com-bines phrases and n-grams from different sentencesto form an image caption, they only consider the textfrom a single news article for extraction, and theycan assume that the text is mostly accurate and rele-vant to the content of the image.In this sense, our method is more like Kuznetsovaet al(2012), which also uses an Integer Linear Pro-gram (ILP) to rapidly optimize how well their gen-erated caption fits the content of the image model.However, it is easier to get coherent image captionsfrom our model since we are not combining partsof sentences from multiple sources.
Since we buildour output from extracted sentences, not phrases, ourILP requires fewer grammaticality and coherenceconstraints than it would for building new sentencesfrom scratch.
We also model how relevant each in-dividual word is to the query image, while they ex-tract phrases based on visual similarity of detectedobjects in the images.4 Proposed WorkOne clear direction for future work is to extend ourimage captioning framework to natural images.
By?natural images?
we refer to images of everydayscenes seen by people, unlike the shopping images,where objects tend to be posed in similar positionsagainst plain backgrounds.
Instead of domains suchas handbags and shoes, we propose to cluster thetraining data based on visual scene domains such asmountains, beaches, and living rooms.
We are par-ticularly interested in the scene attributes and clas-sifiers by Patterson and Hays (2012) which buildsan attribute-based taxonomy of scene types usingcrowd-sourcing, rather than categorical scene typeswhich are typically used.Visual recognition is generally much more diffi-cult in natural scenes than in posed images, sincelighting and viewpoints are not consistent, and ob-jects may be occluded by other objects or truncatedby the edge of the image.
However, we are opti-mistic because we do not need to solve the generalvisual recognition task, since our model only learnshow visual objects and attributes appear in specificdomains of scenes, a much easier problem.
Addi-tionally, the space of likely objects and attributes todetect is limited by what typically appears in thattype of scene.
Finally, we can use the fact that ourimage captioning method is not grounded in our fa-vor, and assume that if an object is partially occludedor truncated in an image, than it is less likely thatthe photographer considered that object to be inter-esting, so it is not as important whether that objectis described in the caption or not.Finally, there is also much that could be done toimprove the text generation component on its own.Our framework currently extracts only a single cap-tion sentence to compress, while recent work insummarization has focused on the problem of learn-ing how to jointly extract and compress (Martins andSmith, 2009; Berg-Kirkpatrick et al 2011).
Sincea poor extraction choice can make finding an accu-rate compression impossible, we should also studydifferent methods of extraction to learn about whatkinds of features are most likely to help us find goodsentences.
As mentioned in Section 2.1, we havealready found that global feature descriptors are bet-ter than bag of image word descriptors for extract-ing sentences to use in image caption compressionsin the shopping dataset.
As we extend our frame-work to other domains of images, we are interestedin finding whether scene-based descriptors and clas-sifiers in general are better at finding good sentencesthan local descriptors, and whether there is a con-nection between region and phrase-based detectorscorrelating better with sentence and phrase-lengthtext, while local image descriptors are more relatedto single words.
Finding patterns like this in visualtext in general would be helpful for many other tasksbesides image captioning.ReferencesAhmet Aker and Robert Gaizauskas.
2010.
Generatingimage descriptions using dependency relational pat-terns.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, ACL?10, pages 1250?1258, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.2010.
Automatic attribute discovery and character-ization from noisy web data.
In Proceedings ofthe 11th European conference on Computer vision:Part I, ECCV?10, pages 663?676, Berlin, Heidelberg.Springer-Verlag.74Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 481?490, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.David M. Blei and Michael I. Jordan.
2003.
Modelingannotated data.
In Proceedings of the 26th annual in-ternational ACM SIGIR conference on Research anddevelopment in informaion retrieval, SIGIR ?03, pages127?134, New York, NY, USA.
ACM.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression an integer linear pro-gramming approach.
J. Artif.
Int.
Res., 31(1):399?429,March.James Clarke.
2008.
Global Inference for Sentence Com-pression: An Integer Linear Programming Approach.Dissertation, University of Edinburgh.Michael John Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. thesis,Philadelphia, PA, USA.
AAI9926110.Koen Deschacht and Marie-Francine Moens.
2007.
Textanalysis for automatic image annotation.
In ACL, vol-ume 45, page 1000.Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart, MarkSanderson, and Robert Gaizauskas.
2010.
Automaticimage captioning from the web for gps photographs.In Proceedings of the international conference on Mul-timedia information retrieval, MIR ?10, pages 445?448, New York, NY, USA.
ACM.Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,Peter Young, Cyrus Rashtchian, Julia Hockenmaier,and David Forsyth.
2010.
Every picture tells a story:generating sentences from images.
In Proceedings ofthe 11th European conference on Computer vision:Part IV, ECCV?10, pages 15?29, Berlin, Heidelberg.Springer-Verlag.Yansong Feng and Mirella Lapata.
2010a.
How manywords is a picture worth?
automatic caption gener-ation for news images.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1239?1249, Stroudsburg,PA, USA.
Association for Computational Linguistics.Yansong Feng and Mirella Lapata.
2010b.
Topic mod-els for image annotation and text illustration.
In HLT-NAACL, pages 831?839.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and generat-ing simple image descriptions.
In CVPR, pages 1601?1608.Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,Tamara L. Berg, and Yejin Choi.
2012.
Collectivegeneration of natural image descriptions.
In ACL.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-der C. Berg, and Yejin Choi.
2011.
Composingsimple image descriptions using web-scale n-grams.In Proceedings of the Fifteenth Conference on Com-putational Natural Language Learning, CoNLL ?11,pages 220?228, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Andre?
F. T. Martins and Noah A. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop on In-teger Linear Programming for Natural Langauge Pro-cessing, ILP ?09, pages 1?9, Stroudsburg, PA, USA.Association for Computational Linguistics.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 2 - Volume 2, EMNLP ?09,pages 880?889, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,Alexander C. Berg, Tamara L. Berg, and Hal Daume?III.
2012.
Midge: Generating image descriptionsfrom computer vision detections.
In European Chap-ter of the Association for Computational Linguistics(EACL).Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: A holistic representation of thespatial envelope.
International Journal of ComputerVision, 42:145?175.V.
Ordonez, G. Kulkarni, and T.L.
Berg.
2011.
Im2text:Describing images using 1 million captioned pho-tographs.
In NIPS.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.75G.
Patterson and J. Hays.
2012.
Sun attribute database:Discovering, annotating, and recognizing scene at-tributes.
2012 IEEE Conference on Computer Visionand Pattern Recognition, 0:2751?2758.Arnold W. M. Smeulders, Marcel Worring, Simone San-tini, Amarnath Gupta, and Ramesh Jain.
2000.Content-based image retrieval at the end of the earlyyears.
IEEE Trans.
Pattern Anal.
Mach.
Intell.,22(12):1349?1380, December.Yezhou Yang, Ching Lik Teo, Hal Daume?
III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gen-eration of natural images.
In Empirical Methods inNatural Language Processing (EMNLP), Edinburgh,Scotland.76
