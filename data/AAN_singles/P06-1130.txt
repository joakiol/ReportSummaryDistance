Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1033?1040,Sydney, July 2006. c?2006 Association for Computational LinguisticsRobust PCFG-Based Generation using Automatically Acquired LFGApproximationsAoife Cahill1 and Josef van Genabith1,21 National Centre for Language Technology (NCLT)School of Computing, Dublin City University, Dublin 9, Ireland2 Center for Advanced Studies, IBM Dublin, Ireland{acahill,josef}@computing.dcu.ieAbstractWe present a novel PCFG-based archi-tecture for robust probabilistic generationbased on wide-coverage LFG approxima-tions (Cahill et al, 2004) automaticallyextracted from treebanks, maximising theprobability of a tree given an f-structure.We evaluate our approach using string-based evaluation.
We currently achievecoverage of 95.26%, a BLEU score of0.7227 and string accuracy of 0.7476 onthe Penn-II WSJ Section 23 sentences oflength ?20.1 IntroductionWide coverage grammars automatically extractedfrom treebanks are a corner-stone technologyin state-of-the-art probabilistic parsing.
Theyachieve robustness and coverage at a fraction ofthe development cost of hand-crafted grammars.
Itis surprising to note that to date, such grammars donot usually figure in the complementary operationto parsing ?
natural language surface realisation.Research on statistical natural language surfacerealisation has taken three broad forms, differ-ing in where statistical information is applied inthe generation process.
Langkilde (2000), for ex-ample, uses n-gram word statistics to rank alter-native output strings from symbolic hand-craftedgenerators to select paths in parse forest repre-sentations.
Bangalore and Rambow (2000) usen-gram word sequence statistics in a TAG-basedgeneration model to rank output strings and ad-ditional statistical and symbolic resources at in-termediate generation stages.
Ratnaparkhi (2000)uses maximum entropy models to drive generationwith word bigram or dependency representationstaking into account (unrealised) semantic features.Valldal and Oepen (2005) present a discriminativedisambiguation model using a hand-crafted HPSGgrammar for generation.
Belz (2005) describesa method for building statistical generation mod-els using an automatically created generation tree-bank for weather forecasts.
None of these prob-abilistic approaches to NLG uses a full treebankgrammar to drive generation.Bangalore et al (2001) investigate the ef-fect of training size on performance while usinggrammars automatically extracted from the Penn-II Treebank (Marcus et al, 1994) for generation.Using an automatically extracted XTAG grammar,they achieve a string accuracy of 0.749 on theirtest set.
Nakanishi et al (2005) present proba-bilistic models for a chart generator using a HPSGgrammar acquired from the Penn-II Treebank (theEnju HPSG).
They investigate discriminative dis-ambiguation models following Valldal and Oepen(2005) and their best model achieves coverage of90.56% and a BLEU score of 0.7723 on Penn-IIWSJ Section 23 sentences of length ?20.In this paper we present a novel PCFG-basedarchitecture for probabilistic generation based onwide-coverage, robust Lexical Functional Gram-mar (LFG) approximations automatically ex-tracted from treebanks (Cahill et al, 2004).
InSection 2 we briefly describe LFG (Kaplan andBresnan, 1982).
Section 3 presents our genera-tion architecture.
Section 4 presents evaluation re-sults on the Penn-II WSJ Section 23 test set us-ing string-based metrics.
Section 5 compares ourapproach with alternative approaches in the litera-ture.
Section 6 concludes and outlines further re-search.2 Lexical Functional GrammarLexical Functional Grammar (LFG) (Kaplan andBresnan, 1982) is a constraint-based theory ofgrammar.
It (minimally) posits two levels of repre-sentation, c(onstituent)-structure and f(unctional)-structure.
C-structure is represented by context-free phrase-structure trees, and captures surface1033S?=?NP VP(?
SUBJ)= ?
?=?NNP V SBAR?=?
?=?
(?
COMP)= ?They believe S(?
PRED) = ?pro?
(?
PRED) = ?believe?
?=?(?
NUM) = PL (?
TENSE) = present(?
PERS) = 3 NP VP(?
SUBJ)= ?
?=?NNP V?=?
?=?John resigned(?
PRED) = ?John?
(?
PRED) = ?resign?(?
NUM) = SG (?
TENSE) = PAST(?
PERS) = 3f1:????????
?PRED ?BELIEVE?(?SUBJ)(?COMP)?
?SUBJ f2:[PRED ?PRO?NUM PLPERS 3]COMP f3:??
?SUBJ f4:[PRED ?JOHN?NUM SGPERS 3]PRED RESIGN?(?SUBJ)?
?TENSE PAST??
?TENSE PRESENT????????
?Figure 1: C- and f-structures for the sentence They believe John resigned.grammatical configurations such as word order.The nodes in the trees are annotated with func-tional equations (attribute-value structure con-straints) which are resolved to produce an f-structure.
F-structures are recursive attribute-value matrices, representing abstract syntacticfunctions.
F-structures approximate to basicpredicate-argument-adjunct structures or depen-dency relations.
Figure 1 shows the c- and f-structures for the sentence ?They believe John re-signed?.3 PCFG-Based Generation forTreebank-Based LFG ResourcesCahill et al (2004) present a method to au-tomatically acquire wide-coverage robust proba-bilistic LFG approximations1 from treebanks.
Themethod is based on an automatic f-structure an-notation algorithm that associates nodes in tree-bank trees with f-structure equations.
For eachtree, the equations are collected and passed on toa constraint solver which produces an f-structurefor the tree.
Cahill et al (2004) present twoparsing architectures: the pipeline and the inte-grated parsing architecture.
In the pipeline ar-chitecture, a PCFG (or a history-based lexicalisedgenerative parser) is extracted from the treebankand used to parse unseen text into trees, the result-ing trees are annotated with f-structure equationsby the f-structure annotation algorithm and a con-straint solver produces an f-structure.
In the in-1The resources are approximations in that (i) they do notenforce LFG completeness and coherence constraints and (ii)PCFG-based models can only approximate LFG and similarconstraint-based formalisms (Abney, 1997).tegrated architecture, first the treebank trees areautomatically annotated with f-structure informa-tion, f-structure annotated PCFGs with rules ofthe form NP(?OBJ=?)?DT(?=?)
NN(?=?)
areextracted, syntactic categories followed by equa-tions are treated as monadic CFG categories dur-ing grammar extraction and parsing, unseen text isparsed into trees with f-structure annotations, theannotations are collected and a constraint solverproduces an f-structure.The generation architecture presented herebuilds on the integrated parsing architecture re-sources of Cahill et al (2004).
The generationprocess takes an f-structure (such as the f-structureon the right in Figure 1) as input and outputs themost likely f-structure annotated tree (such as thetree on the left in Figure 1) given the input f-structureargmaxTreeP (Tree|F-Str)where the probability of a tree given an f-structure is decomposed as the product of theprobabilities of all f-structure annotated produc-tions contributing to the tree but where in addi-tion to conditioning on the LHS of the produc-tion (as in the integrated parsing architecture ofCahill et al (2004)) each production X ?
Y isnow also conditioned on the set of f-structure fea-tures Feats ?-linked2 to the LHS of the rule.
Foran f-structure annotated tree Tree and f-structureF-Str with ?(Tree)=F-Str:32?
links LFG?s c-structure to f-structure in terms of many-to-one functions from tree nodes into f-structure.3?
resolves the equations in Tree into F-Str (if satisfiable)in terms of the piece-wise function ?.1034Conditioning F-Structure Features Grammar Rules Probability{PRED, SUBJ, COMP, TENSE} VP(?=?)
?
VBD(?=?)
SBAR(?COMP=?)
0.4998{PRED, SUBJ, COMP, TENSE} VP(?=?)
?
VBP(?=?)
SBAR(?COMP=?)
0.0366{PRED, SUBJ, COMP, TENSE} VP(?=?)
?
VBD(?=?)
, S(?COMP=?)
6.48e-6{PRED, SUBJ, COMP, TENSE} VP(?=?)
?
VBD(?=?)
S(?COMP=?)
3.88e-6{PRED, SUBJ, COMP, TENSE} VP(?=?)
?
VBP(?=?)
, SBARQ(?COMP=?)
7.86e-7{PRED, SUBJ, COMP, TENSE} VP(?=?)
?
VBD(?=?)
SBARQ(?COMP=?)
1.59e-7Table 1: Example VP Generation rules automatically extracted from Sections 02?21 of the Penn-IITreebankP (Tree|F-Str) :=?X ?
Y in Tree?
(X) = FeatsP (X ?
Y |X,Feats) (1)P (X ?
Y |X,Feats) = P (X ?
Y,X, Feats)P (X,Feats) = (2)P (X ?
Y, Feats)P (X,Feats) ?#(X ?
Y, Feats)#(X ?
.
.
.
, F eats) (3)and where probabilities are estimated using asimple MLE and rule counts (#) from the auto-matically f-structure annotated treebank resourceof Cahill et al (2004).
Lexical rules (rules ex-panding preterminals) are conditioned on the fullset of (atomic) feature-value pairs ?-linked to theRHS.
The intuition for conditioning rules in thisway is that local f-structure components of the in-put f-structure drive the generation process.
Thisconditioning effectively turns the f-structure an-notated PCFGs of Cahill et al (2004) into prob-abilistic generation grammars.
For example, inFigure 1 (where ?-links are represented as ar-rows), we automatically extract the rule S(?=?)
?NP(?SUBJ=?)
VP(?=?)
conditioned on the featureset {PRED,SUBJ,COMP,TENSE}.
The probabilityof the rule is then calculated by counting the num-ber of occurrences of that rule (and the associatedset of features), divided by the number of occur-rences of rules with the same LHS and set of fea-tures.
Table 1 gives example VP rule expansionswith their probabilities when we train a grammarfrom Sections 02?21 of the Penn Treebank.3.1 Chart Generation AlgorithmThe generation algorithm is based on chart gen-eration as first introduced by Kay (1996) withViterbi-pruning.
The generation grammar is firstconverted into Chomsky Normal Form (CNF).
Werecursively build a chart-like data structure in abottom-up fashion.
In contrast to packing of lo-cally equivalent edges (Carroll and Oepen, 2005),in our approach if two chart items have equiva-lent rule left-hand sides and lexical coverage, onlythe most probable one is kept.
Each grammaticalfunction-labelled (sub-)f-structure in the overall f-structure indexes a (sub-)chart.
The chart for eachf-structure generates the most probable tree forthat f-structure, given the internal set of condition-ing f-structure features and its grammatical func-tion label.
At each level, grammatical function in-dexed charts are initially unordered.
Charts arelinearised by generation grammar rules once thecharts themselves have produced the most prob-able tree for the chart.
Our example in Figure 1generates the following grammatical function in-dexed, embedded and (at each level of embedding)unordered (sub-)chart configuration:SUBJ f :2COMP f :3SUBJ f :4TOP f :1For each local subchart, the following algorithmis applied:Add lexical rulesWhile subchart is ChangingApply unary productionsApply binary productionsPropagate compatible rules3.2 A Worked ExampleAs an example, we step through the construc-tion of the COMP-indexed chart at level f3 ofthe f-structure in Figure 1.
For lexical rules,we check the feature set at the sub-f-structurelevel and the values of the features.
Only fea-tures associated with lexical material are consid-ered.
The SUBJ-indexed sub-chart f4 is con-structed by first adding the rule NNP(?=?)
?John(?PRED=?John?,?NUM=pl,?PERS=3).
If morethan one lexical rule corresponds to a particular setof features and values in the f-structure, we add allrules with different LHS categories.
If two or more1035rules with equal LHS categories match the featureset, we only add the most probable one.Unary productions are applied if the RHS of theunary production matches the LHS of an item al-ready in the chart and the feature set of the unaryproduction matches the conditioning feature set ofthe local sub-f-structure.
In our example, this re-sults in the rule NP(?SUBJ=?)
?
NNP(?=?
), con-ditioned on {NUM, PERS, PRED}, being added tothe sub-chart at level f4 (the probability associatedwith this item is the probability of the rule multi-plied by the probability of the previous chart itemwhich combines with the new rule).
When a ruleis added to the chart, it is automatically associatedwith the yield of the rule, allowing us to propa-gate chunks of generated material upwards in thechart.
If two items in the chart have the same LHS(and the same yield independent of word order),only the item with the highest probability is kept.This Viterbi-style pruning ensures that processingis efficient.At sub-chart f4 there are no binary rules thatcan be applied.
At this stage, it is not possibleto add any more items to the sub-chart, thereforewe propagate items in the chart that are compat-ible with the sub-chart index SUBJ.
In our ex-ample, only the rule NP(?SUBJ=?)
?
NNP(?=?
)(which yields the string John) is propagated to thenext level up in the overall chart for considerationin the next iteration.
If the yield of an item be-ing propagated upwards in the chart is subsumedby an element already at that level, the subsumeditem is removed.
This results in efficiently treat-ing the well known problem originally describedin Kay (1996), where one unnecessarily retainssub-optimal strings.
For example, generating thestring ?The very tall strong athletic man?, onedoes not want to keep variations such as ?The verytall man?, or ?The athletic man?, if one can gener-ate the entire string.
Our method ensures that onlythe most probable tree with the longest yield willbe propagated upwards.The COMP-indexed chart at level f3 of the f-structure is constructed in a similar fashion.
Firstthe lexical rule V(?=?)
?
resigned is added.Next, conditioning on {PRED, SUBJ, TENSE}, theunary rule VP(?=?)
?
V(?=?)
(with yield re-signed) is added.
We combine the new VP(?=?
)rule with the NP(?SUBJ=?)
already present fromthe previous iteration to enable us to add the ruleS(?=?)
?
NP(?SUBJ=?)
VP(?=?
), conditionedon {PRED, SUBJ, TENSE}.
The yield of this ruleis John resigned.
Next, conditioning on the samefeature set, we add the rule SBAR(?comp=?)
?S(?=?)
with yield John resigned to the chart.
It isnot possible to add any more new rules, so at thisstage, only the SBAR(?COMP=?)
rule with yieldJohn resigned is propagated up to the next level.The process continues until at the outermostlevel of the f-structure, there are no more rules tobe added to the chart.
At this stage, we search forthe most probable rule with TOP as its LHS cate-gory and return the yield of this rule as the outputof the generation process.
Generation fails if thereis no rule with LHS TOP at this level in the chart.3.3 Lexical SmoothingCurrently, the only smoothing in the system ap-plies at the lexical level.
Our backoff usesthe built-in lexical macros4 of the automatic f-structure annotation algorithm of Cahill et al(2004) to identify potential part-of-speech cate-gories corresponding to a particular set of features.Following Baayen and Sproat (1996) we assumethat unknown words have a probability distribu-tion similar to hapax legomena.
We add a lexicalrule for each POS tag that corresponds to the f-structure features at that level to the chart with aprobability computed from the original POS tagprobability distribution multiplied by a very smallconstant.
This means that lexical rules seen duringtraining have a much higher probability than lexi-cal rules added during the smoothing phase.
Lexi-cal smoothing has the advantage of boosting cov-erage (as shown in Tables 3, 4, 5 and 6 below) butslightly degrades the quality of the strings gener-ated.
We believe that the tradeoff in terms of qual-ity is worth the increase in coverage.Smoothing is not carried out when there is nosuitable phrasal grammar rule that applies duringthe process of generation.
This can lead to the gen-eration of partial strings, since some f-structurecomponents may fail to generate a correspondingstring.
In such cases, generation outputs the con-catenation of the strings generated by the remain-ing components.4 ExperimentsWe train our system on WSJ Sections 02?21 ofthe Penn-II Treebank and evaluate against the raw4The lexical macros associate POS tags with sets of fea-tures, for example the tag NNS (plural noun) is associatedwith the features ?PRED=$LEMMA and ?NUM=pl.1036S.
length ?
20 ?
25 ?
30 ?
40 allTraining 16667 23597 29647 36765 39832Test 1034 1464 1812 2245 2416Table 2: Number of training and test sentences persentence lengthstrings from Section 23.
We use Section 22 as ourdevelopment set.
As part of our evaluation, we ex-periment with sentences of varying length (20, 25,30, 40, all), both in training and testing.
Table 2gives the number of training and test sentences foreach sentence length.
In each case, we use the au-tomatically generated f-structures from Cahill etal.
(2004) from the original Section 23 treebanktrees as f-structure input to our generation experi-ments.
We automatically mark adjunct and coor-dination scope in the input f-structure.
Notice thatthese automatically generated f-structures are not?perfect?, i.e.
they are not guaranteed to be com-plete and coherent (Kaplan and Bresnan, 1982): alocal f-structure may contain material that is notsupposed to be there (incoherence) and/or may bemissing material that is supposed to be there (in-completeness).
The results presented below showthat our method is robust with respect to the qual-ity of the f-structure input and will always attemptto generate partial output rather than fail.
We con-sider this an important property as pristine gen-eration input cannot always be guaranteed in re-alistic application scenarios, such as probabilistictransfer-based machine translation where genera-tion input may contain a certain amount of noise.4.1 Pre-Training Treebank TransformationsDuring the development of the generation system,we carried out error analysis on our developmentset WSJ Section 22 of the Penn-II Treebank.
Weidentified some initial pre-training transformationsto the treebank that help generation.Punctuation: Punctuation is not usually en-coded in f-structure representations.
Because ourarchitecture is completely driven by rules con-ditioned by f-structure information automaticallyextracted from an f-structure annotated treebank,its placement of punctuation is not principled.This led to anomalies such as full stops appear-ing mid sentence and quotation marks appearingin undesired locations.
One partial solution to thiswas to reduce the amount of punctuation that thesystem trained on.
We removed all punctuationapart from commas and full stops from the train-ing data.
We did not remove any punctuation fromthe evaluation test set (Section 23), but our systemwill ever only produce commas and full stops.
Inthe evaluation (Tables 3, 4, 5 and 6) we are pe-nalised for the missing punctuation.
To solve theproblem of full stops appearing mid sentence, wecarry out a punctuation post-processing step on allgenerated strings.
This removes mid-sentence fullstops and adds missing full stops at the end of gen-erated sentences prior to evaluation.
We are work-ing on a more appropriate solution allowing thesystem to generate all punctuation.Case: English does not have much case mark-ing, and for parsing no special treatment was en-coded.
However, when generating, it is veryimportant that the first person singular pronounis I in the nominative case and me in the ac-cusative.
Given the original grammar used in pars-ing, our generation system was not able to distin-guish nominative from accusative contexts.
Thesolution we implemented was to carry out a gram-mar transformation in a pre-processing step, to au-tomatically annotate personal pronouns with theircase information.
This resulted in phrasal and lex-ical rules such as NP(?SUBJ) ?
PRP?nom(?=?
)and PRP?nom(?=?)
?
I and greatly improved theaccuracy of the pronouns generated.4.2 String-Based EvaluationWe evaluate the output of our generation systemagainst the raw strings of Section 23 using theSimple String Accuracy and BLEU (Papineni etal., 2002) evaluation metrics.
Simple String Accu-racy is based on the string edit distance betweenthe output of the generation system and the goldstandard sentence.
BLEU is the weighted averageof n-gram precision against the gold standard sen-tences.
We also measure coverage as the percent-age of input f-structures that generate a string.
Forevaluation, we automatically expand all contractedwords.
We only evaluate strings produced by thesystem (similar to Nakanishi et al (2005)).We conduct a total of four experiments.
Theparameters we investigate are lexical smoothing(Section 3.3) and partial output.
Partial outputis a robustness feature for cases where a sub-f-structure component fails to generate a string andthe system outputs a concatenation of the stringsgenerated by the remaining components, ratherthan fail completely.1037Sentence length of Evaluation Section 23 Sentences of length:Training Data Metric ?
20 ?
25 ?
30 ?
40 all?
20 BLEU 0.6812 0.6601 0.6373 0.6013 0.5793String Accuracy 0.7274 0.7052 0.6875 0.6572 0.6431Coverage 96.52 95.83 94.59 93.76 93.92?
25 BLEU 0.6915 0.6800 0.6696 0.6396 0.6233String Accuracy 0.7262 0.7095 0.6983 0.6731 0.6618Coverage 96.52 95.83 94.59 93.76 93.92?
30 BLEU 0.6979 0.6881 0.6792 0.6576 0.6445String Accuracy 0.7317 0.7169 0.7075 0.6853 0.6749Coverage 97.97 97.95 97.41 97.15 97.31?
40 BLEU 0.7045 0.6951 0.6852 0.6715 0.6605String Accuracy 0.7349 0.7212 0.7074 0.6881 0.6788Coverage 98.45 98.36 98.01 97.82 97.93all BLEU 0.7077 0.6974 0.6859 0.6734 0.6651String Accuracy 0.7373 0.7221 0.7087 0.6894 0.6808Coverage 98.65 98.5 98.12 97.95 98.05Table 3: Generation +partial output +lexical smoothingSentence length of Evaluation Section 23 Sentences of length:Training Data Metric ?
20 ?
25 ?
30 ?
40 allall BLEU 0.6253 0.6097 0.5887 0.5730 0.5590String Accuracy 0.6886 0.6688 0.6513 0.6317 0.6207Coverage 91.20 91.19 90.84 90.33 90.11Table 4: Generation +partial output -lexical smoothingVarying the length of the sentences included inthe training data (Tables 3 and 5) shows that re-sults improve (both in terms of coverage and stringquality) as the length of sentence included in thetraining data increases.Tables 3 and 5 give the results for the exper-iments including lexical smoothing and varyingpartial output.
Table 3 (+partial, +smoothing)shows that training on sentences of all lengths andevaluating all strings (including partial outputs),our system achieves coverage of 98.05%, a BLEUscore of 0.6651 and string accuracy of 0.6808.
Ta-ble 5 (-partial, +smoothing) shows that coveragedrops to 89.49%, BLEU score increases to 0.6979and string accuracy to 0.7012, when the systemis trained on sentences of all lengths.
Similarly,for strings ?20, coverage drops from 98.65% to95.26%, BLEU increases from 0.7077 to 0.7227and String Accuracy from 0.7373 to 0.7476.
In-cluding partial output increases coverage (by morethan 8.5 percentage points for all sentences) andhence robustness while slightly decreasing quality.Tables 3 (+partial, +smoothing) and 4 (+partial,-smoothing) give results for the experiments in-cluding partial output but varying lexical smooth-ing.
With no lexical smoothing (Table 4), thesystem (trained on all sentence lengths) producesstrings for 90.11% of the input f-structures andachieves a BLEU score of 0.5590 and string ac-curacy of 0.6207.
Switching off lexical smooth-ing has a negative effect on all evaluation met-rics (coverage and quality), because many morestrings produced are now partial (since for PREDvalues unseen during training, no lexical entriesare added to the chart).Comparing Tables 5 (-partial, +smoothing)and 6 (-partial, -smoothing), where the systemdoes not produce any partial outputs and lexi-cal smoothing is varied, shows that training onall sentence lengths, BLEU score increases from0.6979 to 0.7147 and string accuracy increasesfrom 0.7012 to 0.7192.
At the same time, cover-age drops dramatically from 89.49% (Table 5) to47.60% (Table 6).Comparing Tables 4 and 6 shows that while par-tial output almost doubles coverage, this comesat a price of a severe drop in quality (BLEUscore drops from 0.7147 to 0.5590).
On the otherhand, comparing Tables 5 and 6 shows that lexicalsmoothing achieves a similar increase in coveragewith only a very slight drop in quality.5 DiscussionNakanishi et al (2005) achieve 90.56% cover-age and a BLEU score of 0.7723 on Section 231038Sentence length of Evaluation Section 23 Sentences of length:Training Data Metric ?
20 ?
25 ?
30 ?
40 all?
20 BLEU 0.7326 0.7185 0.7165 0.7082 0.7052String Accuracy 0.76 0.7428 0.7363 0.722 0.7175Coverage 85.49 81.56 77.26 71.94 69.08?
25 BLEU 0.7300 0.7235 0.7218 0.7118 0.7077String Accuracy 0.7517 0.7382 0.7315 0.7172 0.7116Coverage 89.65 87.77 84.38 80.31 78.56?
30 BLEU 0.7207 0.7125 0.7107 0.6991 0.6946String Accuracy 0.747 0.7336 0.7275 0.711 0.7045Coverage 93.23 92.14 89.74 86.59 85.18?
40 BLEU 0.7221 0.7140 0.7106 0.7016 0.6976String Accuracy 0.746 0.7331 0.7236 0.7072 0.7001Coverage 94.58 93.85 91.89 89.62 88.33all BLEU 0.7227 0.7145 0.7095 0.7011 0.6979String Accuracy 0.7476 0.7331 0.7239 0.7077 0.7012Coverage 95.26 94.40 92.55 90.69 89.49Table 5: Generation -partial output +lexical smoothingSentence length of Evaluation Section 23 Sentences of length:Training Data Metric ?
20 ?
25 ?
30 ?
40 allall BLEU 0.7272 0.7237 0.7201 0.7160 0.7147String Accuracy 0.7547 0.7436 0.7361 0.7237 0.7192Coverage 61.99 57.38 53.64 47.60 47.60Table 6: Generation -partial output -lexical smoothingsentences, restricted to length ?20 for efficiencyreasons.
Langkilde-Geary?s (2002) best systemachieves 82.8% coverage, a BLEU score of 0.924and string accuracy of 0.945 against Section 23sentences of all lengths.
Callaway (2003) achieves98.7% coverage and a string accuracy of 0.6607on sentences of all lengths.
Our best results forsentences of length ?
20 are coverage of 95.26%,BLEU score of 0.7227 and string accuracy of0.7476.
For all sentence lengths, our best resultsare coverage of 89.49%, a BLEU score of 0.6979and string accuracy of 0.7012.Using hand-crafted grammar-based genera-tion systems (Langkilde-Geary, 2002; Callaway,2003), it is possible to achieve very high results.However, hand-crafted systems are expensive toconstruct and not easily ported to new domains orother languages.
Our methodology, on the otherhand, is based on resources automatically acquiredfrom treebanks and easily ported to new domainsand languages, simply by retraining on suitabledata.
Recent work on the automatic acquisitionof multilingual LFG resources from treebanks forChinese, German and Spanish (Burke et al, 2004;Cahill et al, 2005; O?Donovan et al, 2005) hasshown that given a suitable treebank, it is possi-ble to automatically acquire high quality LFG re-sources in a very short space of time.
The genera-tion architecture presented here is easily ported tothose different languages and treebanks.6 Conclusion and Further WorkWe present a new architecture for stochastic LFGsurface realisation using the automatically anno-tated treebanks and extracted PCFG-based LFGapproximations of Cahill et al (2004).
Our modelmaximises the probability of a tree given an f-structure, supporting a simple and efficient imple-mentation that scales to wide-coverage treebank-based resources.
An improved model wouldmaximise the probability of a string given an f-structure by summing over trees with the sameyield.
More research is required to implementsuch a model efficiently using packed representa-tions (Carroll and Oepen, 2005).
Simple PCFG-based models, while effective and computationallyefficient, can only provide approximations to LFGand similar constraint-based formalisms (Abney,1997).
Research on discriminative disambigua-tion methods (Valldal and Oepen, 2005; Nakanishiet al, 2005) is important.
Kaplan and Wedekind(2000) show that for certain linguistically interest-ing classes of LFG (and PATR etc.)
grammars,generation from f-structures yields a context freelanguage.
Their proof involves the notion of a1039?refinement?
grammar where f-structure informa-tion is compiled into CFG rules.
Our probabilis-tic generation grammars bear a conceptual similar-ity to Kaplan and Wedekind?s ?refinement?
gram-mars.
It would be interesting to explore possibleconnections between the treebank-based empiricalwork presented here and the theoretical constructsin Kaplan and Wedekind?s proofs.We presented a full set of generation experi-ments on varying sentence lengths training on Sec-tions 02?21 of the Penn Treebank and evaluat-ing on Section 23 strings.
Sentences of length?20 achieve coverage of 95.26%, BLEU scoreof 0.7227 and string accuracy of 0.7476 againstthe raw Section 23 text.
Sentences of all lengthsachieve coverage of 89.49%, BLEU score of0.6979 and string accuracy of 0.7012.
Our methodis robust and can cope with noise in the f-structureinput to generation and will attempt to producepartial output rather than fail.AcknowledgementsWe gratefully acknowledge support from ScienceFoundation Ireland grant 04/BR/CS0370 for theresearch reported in this paper.ReferencesStephen Abney.
1997.
Stochastic Attribute-Value Gram-mars.
Computational Linguistics, 23(4):597?618.Harald Baayen and Richard Sproat.
1996.
Estimating lexi-cal priors for low-frequency morphologically ambiguousforms.
Computational Linguistics, 22(2):155?166.Srinivas Bangalore and Owen Rambow.
2000.
Exploit-ing a probabilistic hierarchical model for generation.
InProceedings of COLING 2000, pages 42?48, Saarbrcken,Germany.Srinivas Bangalore, John Chen, and Owen Rambow.
2001.Impact of quality and quantity of corpora on stochasticgeneration.
In Proceedings of EMNLP 2001, pages 159?166.Anja Belz.
2005.
Statistical generation: Three methods com-pared and evaluated.
In Proceedings of the 10th EuropeanWorkshop on Natural Language Generation (ENLG?
05),pages 15?23, Aberdeen, Scotland.Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill,Ruth O?Donovan, Adams Bodomo, Josef van Genabith,and Andy Way.
2004.
Treebank-Based Acquisition of aChinese Lexical-Functional Grammar.
In Proceedings ofthe 18th Pacific Asia Conference on Language, Informa-tion and Computation, pages 161?172, Tokyo, Japan.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef vanGenabith, and Andy Way.
2004.
Long-Distance De-pendency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations.
In Pro-ceedings of ACL-04, pages 320?327, Barcelona, Spain.Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-Carthy, Ruth O?Donovan, Christian Rohrer, Josef vanGenabith, and Andy Way.
2005.
Treebank-based acquisi-tion of multilingual unification grammar resources.
Jour-nal of Research on Language and Computation; SpecialIssue on ?Shared Representations in Multilingual Gram-mar Engineering?, pages 247?279.Charles B. Callaway.
2003.
Evaluating coverage for largesymbolic NLG grammars.
In Proceedings of the Eigh-teenth International Joint Conference on Artificial Intelli-gence, pages 811?817, Acapulco, Mexico.John Carroll and Stephan Oepen.
2005.
High efficiency real-ization for a wide-coverage unification grammar.
In Pro-ceedings of IJCNLP05, pages 165?176, Jeju Island, Ko-rea.Ron Kaplan and Joan Bresnan.
1982.
Lexical FunctionalGrammar, a Formal System for Grammatical Representa-tion.
In Joan Bresnan, editor, The Mental Representationof Grammatical Relations, pages 173?281.
MIT Press,Cambridge, MA.Ron Kaplan and Juergen Wedekind.
2000.
LFG Generationproduces Context-free languages.
In Proceedings of COL-ING 2000, pages 141?148, Saarbruecken, Germany.Martin Kay.
1996.
Chart Generation.
In Proceedings of the34th Annual Meeting of the Association for ComputationalLinguistics, pages 200?204, Santa Cruz, CA.Irene Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sentencegenerator.
In Second International Natural LanguageGeneration Conference, pages 17?24, Harriman, NY.Irene Langkilde.
2000.
Forest-based statistical sentence gen-eration.
In Proceedings of NAACL 2000, pages 170?177,Seattle, WA.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
The Penn Treebank: An-notating Predicate Argument Structure.
In Proceedingsof the ARPA Workshop on Human Language Technology,pages 110?115, Princton, NJ.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005.Probabilistic models for disambiguation of an HPSG-based chart generator.
In Proceedings of the InternationalWorkshop on Parsing Technology, Vancouver, Canada.Ruth O?Donovan, Aoife Cahill, Josef van Genabith, andAndy Way.
2005.
Automatic Acquisition of Spanish LFGResources from the CAST3LB Treebank.
In Proceedingsof LFG 05, pages 334?352, Bergen, Norway.Kishore Papineni, Salim Roukos, Todd Ward, and WeiJingZhu.
2002.
BLEU: a Method for Automatic Evaluation ofMachine Translation.
In Proceedings of ACL 2002, pages311?318, Philadelphia, PA.Adwait Ratnaparkhi.
2000.
Trainable methods for natu-ral language generation.
In Proceedings of NAACL 2000,pages 194?201, Seattle, WA.Erik Valldal and Stephan Oepen.
2005.
Maximum En-tropy Models for Realization Reranking.
In Proceedingsof the 10th Machine Translation Summit, pages 109?116,Phuket, Thailand.1040
