Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1713?1722,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCrowd Prefers the Middle Path: A New IAA Metric for CrowdsourcingReveals Turker Biases in Query SegmentationRohan Ramanath?R.
V. College of EngineeringBangalore, Indiaronramanath@gmail.comMonojit ChoudhuryMicrosoft Research Lab IndiaBangalore, Indiamonojitc@microsoft.comKalika BaliMicrosoft Research Lab IndiaBangalore, Indiakalikab@microsoft.comRishiraj Saha Roy?Indian Institute of Technology KharagpurKharagpur, Indiarishiraj@cse.iitkgp.ernet.inAbstractQuery segmentation, like text chunking,is the first step towards query understand-ing.
In this study, we explore the effec-tiveness of crowdsourcing for this task.Through carefully designed control ex-periments and Inter Annotator Agreementmetrics for analysis of experimental data,we show that crowdsourcing may not be asuitable approach for query segmentationbecause the crowd seems to have a verystrong bias towards dividing the query intoroughly equal (often only two) parts.
Sim-ilarly, in the case of hierarchical or nestedsegmentation, turkers have a strong prefer-ence towards balanced binary trees.1 IntroductionText chunking of Natural Language (NL) sentencesis a well studied problem that is an essential pre-processing step for many NLP applications (Ab-ney, 1991; Abney, 1995).
In the context of Websearch queries, query segmentation is similarly thefirst step towards analysis and understanding ofqueries (Hagen et al, 2011).
The task in both thecases is to divide the sentence or the query intocontiguous segments or chunks of words such thatthe words from a segment are related to each othermore strongly than words from different segments(Bendersky et al, 2009).
It is typically assumedthat the segments are structurally and semanticallycoherent and, therefore, the information containedin them can be processed holistically.
?The work was done during author?s internship at Mi-crosoft Research Lab India.?
This author was supported by Microsoft Corporationand Microsoft Research India under the Microsoft ResearchIndia PhD Fellowship Award.f Pipe representation Boundary var.4 apply | first aid course | on line 1 0 0 1 03 apply first aid course | on line 0 0 0 1 02 apply first aid | course on line 0 0 1 0 01 apply | first aid | course | on line 1 0 1 1 0Table 1: Example of flat segmentation by Turkers.f is the frequency of annotations; segment bound-aries are represented by |.f Bracket representation Boundary var.4 ((apply first) ((aid course) (on line))) 0 2 0 1 02 (((apply (first aid)) course) (on line)) 1 0 2 3 02 ((apply ((first aid) course)) (on line)) 2 0 1 3 01 (apply (((first aid) course) (on line))) 3 0 1 2 01 ((apply (first aid)) (course (on line))) 1 0 2 1 0Table 2: Example of nested segmentation by Turk-ers.
f is the frequency of annotations.A majority of work on query segmentation re-lies on manually segmented queries by human ex-perts for training and evaluation of segmentationalgorithms.
These are typically small datasets andeven with detailed annotation guidelines and/orclose supervision, low Inter Annotator Agreement(IAA) remains an issue.
For instance, Table 1 il-lustrates the variation in flat segmentation by 10annotators.
This confusion is mainly because thedefinition of a segment in a query is ambiguousand of an unspecified granularity.
This is fur-ther compounded by the fact that other than eas-ily recognizable and agreed upon segments such asNamed Entities or Multi-Word Expressions, thereis no established notion of linguistic grouping suchas phrases and clauses in a query.Although there is little work on the use ofcrowdsourcing for query segmentation (Hagen etal., 2011; Hagen et al, 2012), the idea that the1713crowd could be a potential (and cheaper) sourcefor reliable segmentation seems a reasonable as-sumption.
The need for larger datasets makes thisan attractive proposition.
Also, a larger numberof annotations could be appropriately distilled toobtain better quality segmentations.In this paper we explore crowdsourcing as anoption for query segmentation through experi-ments designed using Amazon Mechanical Turk(AMT)1.
We compare the results against golddatasets created by trained annotators.
We ad-dress the issues pertaining to disagreements due toboth ambiguity and granularity and attempt to ob-jectively quantify their role in IAA.
To this end,we also conduct similar annotation experimentsfor NL sentences and randomly generated queries.While queries are not as structured as NL sen-tences they are not simply a set of random words.Thus, it is necessary to compare query segmenta-tion to the u?ber-structure of NL sentences as wellas the unter-structure of random n-grams.
This hasimportant implications for understanding any in-herent biases annotators may have as a result ofthe apparent lack of structure of the queries.To quantify the effect of granularity on segmen-tation, we also ask annotators to provide hierar-chical or nested segmentations for real and ran-dom queries, as well as sentences.
FollowingAbney?s (1992) proposal for hierarchical chunk-ing of NL, we ask the annotators to group ex-actly two words or segments at a time to recur-sively form bigger segments.
The concept is illus-trated in Fig.
1.
Table 2 shows annotations from10 Turkers.
It is important to constrain the join-ing of exactly two segments or words at a timeto avoid the issue of fuzziness in granularity.
Weshall refer to this style of annotation as Nestedsegmentation, whereas the non-hierarchical non-constrained chunking will be referred to as Flatsegmentation.Through statistical analysis of the experimen-tal data we show that crowdsourcing may not bethe best practice for query segmentation, not onlybecause of ambiguity and granularity issues, butbecause there exist very strong biases amongst an-notators to divide a query into two roughly equalparts that result in misleadingly high agreements.As a part of our analysis framework, we introducea new IAA metric for comparison across flat andnested segmentations.
This versatile metric can be1https://www.mturk.com/mturk/welcome321apply 0first aidcourse0on lineFigure 1: Nested Segmentation: Illustration.readily adapted for measuring IAA for other lin-guistic annotation tasks, especially when done us-ing crowdsourcing.The rest of the paper is organized as follows.Sec 2 provides a brief overview of related work.Sec 3 describes the experiment design and proce-dure.
In Sec 4, we introduce a new metric for IAA,that could be uniformly applied across flat andnested segmentations.
Results of the annotationexperiments are reported in Sec 5.
In Sec 6, we an-alyze the possible statistical and linguistic biasesin annotation.
Sec 7 concludes the paper by sum-marizing the work and discussing future researchdirections.
All the annotated datasets used in thisresearch are freely available for non-commercialresearch purposes2.2 Related WorkQuery segmentation was introduced by Risvik et.al.
(2003) as a possible means to improve Informa-tion Retrieval.
Since then there has been a signif-icant amount of research exploring various algo-rithms for this task and its use in IR (see Hagen et.al.
(2011) for a survey).
Most of the research andevaluation considers query segmentation as a pro-cess analogous to identification of phrases withina query which when put within double-quotes (im-plying exact matching of the quoted phrase in thedocument) leads to better IR performance.
How-ever, this is a very restricted view of the processand does not take into account the full potential ofquery segmentation.A more generic notion of segments leads to di-verse and ambiguous definitions, making its eval-uation a hard problem (see Saha Roy et.
al.
(2012)for a discussion on issues with evaluation).
Mostautomatic segmentation techniques (Bergsma andWang, 2007; Tan and Peng, 2008; Zhang et al,2Related datasets and supplementary material can be ac-cessed from http://bit.ly/161Gkk9 or can be ob-tained by directly emailing the authors.17142009; Brenes et al, 2010; Hagen et al, 2011; Li etal., 2011) have so far been evaluated only againsta small set of human-annotated queries (Bergsmaand Wang, 2007).
The reported low IAA for suchdatasets casts serious doubts on the reliability ofannotation and the performance of the algorithmsevaluated on them (Hagen et al, 2011; Saha Royet al, 2012).To address the problem of data scarcity, Ha-gen et.
al.
(2011) have created larger annotateddatasets through crowdsourcing3.
However, intheir approach the crowd is provided with a few(four) possible segmentations of a query to choosefrom (known through a personal communicationwith a authors).
Thus, it presupposes an automaticprocess that can generate the correct segmentationof a query within top few options.
It is far fromobvious how to generate these initial segmenta-tions in a reliable manner.
This may also resultin an over-optimistic IAA.
An ideal segmentationshould be based on the annotators?
own interpreta-tion of the query.
Nevertheless, if large scale datahas to be procured, crowdsourcing seems to be theonly efficient and effective model for this task, andhas been proven to be so for other IR and linguisticannotations; see Carvalho et al (2011) for exam-ples of crowdsourcing for IR resources and (Snowet al, 2008; Callison-Burch, 2009) for languageresources.In the context of NL text, segmentation hasbeen traditionally referred to as chunking and isa well-studied problem.
Abney (1991; 1992;1995) defines a chunk as a sub-tree within asyntactic phrase structure tree corresponding toNoun, Prepositional, Adjectival, Adverbial andVerb Phrases.
Similarly, Bharati et al(1995) de-fines it as Noun Group and Verb Group based onlyon local surface information.
However, cognitiveand annotation experiments for chunking of En-glish (Abney, 1992) and other language text (Baliet al, 2009) have shown that native speakers agreeon major clause and phrase boundaries, but maynot do so on more fine-grained chunks.
One im-portant implication of this is that annotators areexpected to agree more on the higher level bound-aries for nested segmentation than the lower ones.We note that hierarchical query segmentation wasproposed for the first time by Huang et al (2010),where the authors recursively split a query (or itsfragment) into exactly two parts and evaluate the3http://www.webis.de/research/corporafinal output against human annotations.3 ExperimentsThe annotation experiments have been designed tosystematically study the various aspects of querysegmentation.
In order to verify the effective-ness and reliability of crowdsourcing, we designedan AMT experiment for flat segmentation of Websearch queries.
As a baseline, we would like tocompare these annotations with those from hu-man experts trained for the task.
We shall referto this baseline as the Gold annotation set.
Sincewe believe that the issue of granularity could bethe prime reason for previously reported low IAAfor segmentation, we also designed AMT-basednested segmentation experiments for the same setof queries, and obtained the corresponding goldannotations.Finally, to estimate the role of ambiguity inher-ent in the structure of Web search queries on IAA,we conducted two more control experiments, boththrough crowdsourcing.
First, flat and nested seg-mentation of well-formed English, i.e., NL sen-tences of similar length distribution; and second,flat and nested segmentation of randomly gener-ated queries.
Higher IAA for NL sentences wouldlead us to conclude that ambiguity and lack ofstructure in queries is the main reason for lowagreements.
On the other hand high or comparableIAA for random queries would mean that annota-tions have strong biases.Thus, we have the following four pairs of anno-tation experiments: flat and nested segmentationof queries from crowdsourcing, corresponding flatand nested gold annotations, flat and nested seg-mentation of English sentences from crowdsourc-ing, and flat and nested segmentations for ran-domly generated queries through crowdsourcing.3.1 DatasetFor our experiments, we need a set of Web searchqueries and well-formed English sentences.
Fur-thermore, for generating the random queries, wewill use search query logs to learn n-gram mod-els.
In particular, we use the following datasets:Q500, QG500: Saha Roy et al (2012) re-leased a dataset of 500 queries, 5 to 8 words long,for evaluation of various segmentation algorithms.This dataset has flat segmentations from three an-notators obtained under controlled experimentalsettings, and can be considered as Gold annota-1715Figure 2: Length distribution of datasets.tions.
Hence, we select this set for our experimentsas well.
We procured the corresponding nestedsegmentation for these queries from two humanexperts, who are regular search engine users, be-tween 20 and 30 years old, and familiar with var-ious linguistic annotation tasks.
They annotatedthe data under supervision.
They were trained andpaid for the task.
We shall refer to the set of flatand nested gold annotations as QG500, whereasQ500 will be reserved for AMT experiments.Q700: Since 500 queries may not be enoughfor reliable conclusion and since the queries maynot have been chosen specifically for the purposeof annotation experiments, we expanded the setwith another 700 queries sampled from a slice ofthe query logs of Bing Australia4 containing 16.7million queries issued over a period of one month(May 2010).
We picked, uniformly at random,queries that are 4 to 8 words long, have only En-glish letters and numerals, and a high click entropybecause ?a query with a larger click entropy valueis more likely to be an informational or ambiguousquery?
(Dou et al, 2008).
Q500 consists of tail-ish queries with frequency between 5 and 15 thathave at least one multiword named entity; but un-like the case of Q700, click-entropy was not con-sidered during sampling.
As we shall see, this dif-ference is clearly reflected in the results.S300: We randomly selected 300 English sen-tences from a collection of full texts of public do-main books5 that were 5 to 15 words long, andchecked them for well-formedness.
This set willbe referred to as S300.QRand: Instead of generating search queriesby throwing in words randomly, we thought itwill be more interesting to explore annotation of4http://www.bing.com/?cc=au5http://www.gutenberg.orgParameter Flat Details Nested DetailsTime needed: actual (allotted) 49 sec (10 min) 1 min 52 sec (15 min)Reward per HIT $0.02 $0.06Instruction video duration 26 sec 1 min 40 secTurker qualification Completion rate >100 tasksTurker approval rate Acceptance rate >60 %Turker location United States of AmericaTable 3: Specifics of the HITs for AMT.queries generated using n-gram models for n =1, 2, 3.
We estimated the models from the BingAustralia log of 16.7 million queries.
We gener-ated 250 queries each of desired length distribu-tion using the 1, 2 and 3-gram models.
We shallrefer to these as U250, B250, T250 (for Uni, Biand Trigram) respectively, and the whole datasetas QRand.
Fig.
2 shows the query and sentencelength distribution for the various sets.3.2 Crowdsourcing ExperimentsWe used AMT to get our annotations throughcrowdsourcing.
Pilot experiments were carried outto test the instruction set and examples presented.Based on the feedback, the precise instructions forthe final experiments were designed.Two separate AMT Human Intelligence Tasks(HITs) were designed for flat and nested querysegmentation.
Also, the experiments for queries(Q500+Q700) were conducted separately fromS300 and QRand.
Thus, we had six HITs inall.
The concept of flat and nested segmentationwas introduced to the Turkers with the help of ex-amples presented in two short videos6.
When indoubt regarding the meaning of a query, the Turk-ers were advised to issue the query on a searchengine of their choice and find out its possibleinterpretation(s).
Note that we intentionally keptdefinitions of flat and nested segmentation fuzzybecause (a) it would require very long instructionmanuals to cover all possible cases and (b) Turkersdo not tend to read verbose and complex instruc-tions.
Table 3 summarizes other specifics of HITs.Honey pots or trap questions whose answers areknown a priori are often included in a HIT to iden-tify turkers who are unable to solve the task ap-propriately leading to incorrect annotations.
How-ever, this trick cannot be employed in our case be-cause there is no notion of an absolutely correctsegmentation.
We observe that even with unam-biguous queries, even expert annotators may dis-6Flat: http://youtu.be/eMeLjJIvIh0, Nested:http://youtu.be/xE3rwANbFvU1716agree on some of the segment boundaries.
Hence,we decided to include annotations from all theturkers, except for those that were syntactically ill-formed (e.g., non-binary nested segmentation).4 Inter Annotator AgreementInter Annotator Agreement is the only way tojudge the reliability of annotated data in absenceof an end application.
Therefore, before we canventure into analysis of the experimental data, weneed to formalize the notion of IAA for flat andnested queries.
The task is non-trivial for tworeasons.
First, traditional IAA measures are de-fined for a fixed set of annotators.
However, forcrowdsourcing based annotations, different anno-tators might have annotated different parts of thedataset.
For instance, we observed that a totalof 128 turkers have provided the flat annotationsfor Q700, when we had only asked for 10 anno-tations per query.
Thus, on average, a turker hasannotated only 7.81% of the 700 queries.
In fact,we found that 31 turkers had annotated less than5 queries.
Hence, measures such as Cohen?s ?
(1960) cannot be directly applied in this contextbecause for crowdsourced annotations, we cannotmeaningfully compute annotator-specific distribu-tion of the labels and biases.Second, most of the standard annotation metricsdo not generalize for flat segmentation and trees.Artstein and Poesio (2008) provides a comprehen-sive survey of the IAA metrics and their usage inNLP.
They note that all the metrics assume thata fixed set of labels are used for items.
There-fore, it is far from obvious how to compare chunk-ing or segmentation that covers the whole text orthat might have overlapping units as in the case ofnested segmentation.
Furthermore, we would liketo compare the reliability of flat and nested seg-mentation, and therefore, ideally we would like tohave an IAA metric that can be meaningfully ap-plied to both of these cases.After considering various measures, we decidedto appropriately generalize one of the most versa-tile and effective IAA metrics proposed till date,the Kripendorff?s ?
(2004).
To be consistent withprior work, we will stick to the notation usedin Artstein and Poesio (2008) and redefine the?
in the context of flat and nested segmentation.Note that though the notations introduced here willbe from the perspective of queries, it is equallyapplicable to sentences and the generalization isstraightforward.4.1 Notations and DefinitionsLet Q be the set of all queries with cardinality q.A query q ?
Q can be represented as a sequence of|q| words: w1w2 .
.
.
w|q|.
We introduce |q?1| ran-dom variables, b1, b2, .
.
.
b|q|?1, such that bi rep-resents the boundary between the words wi andwi+1.
A flat or nested segmentation of q, repre-sented by qj , j varying from 1 to total number ofannotations c, is a particular instantiation of theseboundary variables as described below.Definition.
A flat segmentation, qj can beuniquely defined by a binary assignment of theboundary variables bj,i, where bj,i = 1 iff wi andwi+1 belong to two different flat segments.
Oth-erwise, bj,i = 0.
Thus, q has 2|q|?1 possible flatsegmentations.Definition.
A nested segmentation qj can alsobe uniquely defined by assigning non-negative in-tegers to the boundary variables such that bj,i = 0iff words wi and wi+1 form an atomic segment(i.e., they are grouped together), else bj,i = 1 +max(lefti, righti), where lefti and righti arethe heights of the largest subtrees ending at wi andbeginning at wi+1 respectively.This numbering scheme for nested segmenta-tion can be understood through Fig.
1.
Every in-ternal node of the binary tree corresponding to thenested segmentation is numbered according to itsheight.
The lowest internal nodes, both of whosechildren are query words, are assigned a value of0.
Other internal nodes get a value of one greaterthan the height of its higher child.
Since every in-ternal node corresponds to a boundary, we assignthe height of the node to the corresponding bound-aries.
The number of unique nested segmentationsof a query of length |q| is its corresponding Cata-lan number7.Boundary variables for flat and nested segmen-tation are illustrated with an example of each kindin Tables 1 and 2 (last column).4.2 Krippendorff ?s ?
for SegmentationKrippendorff ?s ?
(Krippendorff, 2004) is an ex-tremely versatile agreement coefficient, which isbased on the assumption that the expected agree-ment is calculated by looking at the overall distri-bution of judgments without regard to which anno-tator produced them (Artstein and Poesio, 2008).7http://goo.gl/vKQvK1717Hence, it is appropriate for crowdsourced annota-tion, where the judgments come from a large num-ber of unrelated annotators.
Moreover, it allowsfor different magnitudes of disagreement, whichis a useful feature as we might want to differen-tially penalize disagreements at various levels ofthe tree for nested segmentation.?
is defined as?
= 1?
DoDe= 1?
s2withins2total(1)where Do and De are, respectively, the observedand expected disagreements that are measured bys2within ?
the variance within the annotation of anitem and s2total ?
variance across annotations ofall items.
We adapt the equations presented inpp.565-566 of Artstein and Poesio (2008) for mea-suring these quantities for queries:s2within =12qc(c?
1)?q?Qc?m=1c?n=1d(qm, qn)(2)s2total =12qc(qc?
1)?q?Qc?m=1?q?
?Qc?n=1d(qm, q?n)(3)where, d(qm, q?n) is a distance metric for the agree-ment between annotations qm and q?n.We define two different distance metrics d1 andd2 that are applicable to flat and nested segmenta-tion.
We shall first define these metrics for com-paring queries with equal length (i.e., |q| = |q?|):d1(qm, q?n) =1|q| ?
1|q|?1?i=1|bm,i ?
b?n,i| (4)d2(qm, q?n) =1|q| ?
1|q|?1?i=1|b2m,i ?
(b?n,i)2| (5)While d1 penalizes all disagreements equally, d2penalizes disagreements higher up the tree more.d2 might be a desirable metric for nested seg-mentation, because research on sentence chunk-ing shows that annotators agree more on clause ormajor phrase boundaries, even though they maynot always agree on intra-clausal or intra-phrasalboundaries (Bali et al, 2009).
Note that for flatsegmentation, d1 and d2 are identical, and hencewe will denote them as d.We propose the following extension to thesemetrics for queries of unequal lengths.
Withoutloss of generality, let us assume that |q| < |q?|.
kis 1 or 2; r = |q?| ?
|q|+ 1.dk(qm, q?n) =1r(|q| ?
1)r?1?a=0|q|?1?i=1|bkm,i ?
(b?n,i+a)k| (6)4.3 IAA under Random Bias AssumptionKrippendorff?s ?
uses the cross-item variance asan estimate of chance agreement, which is reli-able in general.
However, this might result in mis-leadingly low values of IAA, especially when theitems in the set are indeed expected to have sim-ilar annotations.
To resolve this, we also com-pute the chance agreement under a random biasmodel.
The random model assumes that all thestructural annotations of q are equiprobable.
Forflat segmentation, it boils down to the fact thatall the 2|q|?1 annotations are equally likely, whichis equivalent to the assumption that any boundaryvariable bi has 0.5 probability of being 0 and 0.5for 1.Analytical computation of the expected proba-bility distributions of d1(qm, qn) and d2(qm, qn)is harder for nested segmentation.
Therefore, weprogrammatically generate all possible trees for q,which is again dependent only on |q| and com-pute d1 and d2 between all pairs of trees, fromwhich the expected distributions can be readilyestimated.
Let us denote this expected cumula-tive probability distribution for flat segmentationas Pd(x; |q|) = the probability that for a pairof randomly chosen flat segmentations of q, qmand qn, d(qm, qn) ?
x.
Likewise, let Pd1(x; |q|)and Pd2(x; |q|) be the respective probabilities thatfor any two nested segmentations qm and qn ofq, the following holds: d1(qm, qn) ?
x andd2(qm, qn) ?
x.We define the IAA under random bias model as(k is 1, 2 or null):S = 1qc2?q?Qc?m=1c?n=1Pdk(dk(qm, qn); |q|) (7)Thus, S is the expected probability of observing asimilar or worse agreement by random chance, av-eraged over all pairs of annotations for all queries,and not a chance corrected IAA metric such as?.
Thus, S = 1 implies that the observed agree-ment is almost always better than that by randomchance and S = 0.5 and 0 respectively imply thatthe observed agreement is as good as and almostalways worse than that by random chance.
We1718Dataset Flat Nestedd1 d1 d2Q700 0.21(0.59) 0.21(0.89) 0.16(0.68)Q500 0.22(0.62) 0.15(0.70) 0.15(0.44)QG500 0.61(0.88) 0.66(0.88) 0.67(0.80)S300 0.27(0.74) 0.18(0.94) 0.14(0.75)U250 0.23(0.89) 0.42(0.90) 0.30(0.78)B250 0.22(0.86) 0.34(0.88) 0.22(0.71)T250 0.20(0.86) 0.44(0.89) 0.34(0.76)Table 4: Agreement Statistics: ?
(S).also note that a high value of S and low valueof ?
indicate that though the annotators agree onthe judgment of individual items, they also tend toagree on judgments of two different items, whichin turn, could be due to strong annotator biases ordue to lack of variability of the dataset.In the supplementary material, computations of?
and S have been explained in further detailsthrough worked out examples.
Tables for the ex-pected distributions of d, d1 and d2 under the ran-dom annotation assumption are also available.5 ResultsTable 4 reports the values of ?
and S for flatand nested segmentation on the various datasets.For nested segmentation, the values were com-puted for two different distance metrics d1 andd2.
As expected, the highest value of ?
for bothflat and nested segmentation is observed for goldannotations.
An ?
> 0.6 indicates quite goodIAA, and thus, reliable annotations.
Higher ?
fornested segmentation QG500 than flat further vali-dates our initial postulate that nested segmentationmay reduce disagreement from granularity issuesinherent in the definition of flat segmentation.Opposite trends are observed for Q700, Q500and S300, where ?
for flat is the highest, followedby that for nested using d1, and then d2.
More-over, except for flat segmentation of sentences, ?lies between 0.14 and 0.22, which is quite low.This clearly shows that segmentation, either flator nested, cannot be reliably procured throughcrowdsourcing.
Lower ?
for d2 than d1 furtherindicates that annotators disagree more for higherlevels of the trees, contrary to what we had ex-pected.
However, nearly equal IAA for sentencesand queries implies that low agreement may not bean outcome of inherent ambiguity in the structureof queries.
Slightly higher ?
for flat segmentationand a much higher ?
for nested segmentation ofQRand reinforce the fact that low IAA is not dueto a lack of structure in queries.It is interesting to note that ?
for nested segmen-tation of S300 and all segmentations of QRandare low or medium despite the fact that S is veryhigh in all these cases.
Thus, it is clear that an-notators have a strong bias towards certain struc-tures across queries.
In the next section, we willanalyze some of these biases.
We also computedthe IAA between QG500 and Q500, and found?
= 0.27.
This is much lower than ?
for QG500,though slightly higher than that for Q500.
We didnot observe any significant variation in agreementwith respect to the length of the queries.6 Biases in AnnotationThe IAA statistics clearly show that there are cer-tain strong biases in both flat and nested querysegmentation, especially those obtained throughcrowdsourcing.
To identify these biases, we wentthrough the annotations and came up with possi-ble hypotheses, which we tried to verify throughstatistical analysis of the data.
Here, we report themost prominent biases that were thus discovered.Bias 1: During flat segmentation, annotators pre-fer dividing the query into two segments of roughlyequal length.As discussed earlier, one of the major problemsof flat segmentation is the fuzziness in granularity.In our experiments, we intentionally left the de-cision of whether to go for fine or coarse-grainedsegmentation to the annotator.
However, it is sur-prising to observe that annotators typically dividethe query into two segments (see Fig.
3, plots A1and A2), and at times three, but hardly ever morethan three.
This bias is observed across queries,sentences and random queries, where the percent-age of annotations with 2 or 3 segments are greaterthan 83%, 91% and 96% respectively.
This biasis most strongly visible for QRand because thelack of syntactic or semantic cohesion between thewords provides no clue for segmentation.Furthermore, we observe that typically seg-ments tend to be of equal length.
For this, we com-puted standard deviations (sd) of segment lengthsfor all annotations having 2 or 3 segments; the dis-tribution of sd is shown in Fig.
3, plots B1 and B2.We observe that for all datasets, sd lies mainly be-tween 0.5 and 1 (for perspective, consider a query1719Figure 3: Analysis of annotation biases: A1, A2 ?
number of segments per flat segmentation vs. length;B1, B2 ?
standard deviation of segment length for flat segmentation; C1, C2 ?
distribution of the treeheights in nested segmentation.Length Expected Q500 QG500 Q700 S300 QRand5 2.57 2.00 2.02 2.08 2.02 2.016 3.24 2.26 2.23 2.23 2.24 2.027 3.88 2.70 2.71 2.67 2.55 2.628 4.47 2.89 2.68 2.72 2.72 2.35Table 5: Average height for nested segmentation.with 7 words; with two segments of length 3 and4 the sd is 0.5, and for 2 and 5, the sd is 1.5), im-plying that segments are roughly of equal length.It is likely that due to this bias, the S or observedagreement is moderately high for queries and veryhigh for sentences, but then it also leads to highagreement across different queries and sentences(i.e., high s2total) especially when they are of equallength, which in turn brings down the value of ?
?the true agreement after bias correction.Bias 2: During nested segmentation, annotatorsprefer balanced binary trees.Quite analogous to bias 1, for nested segmen-tation we observe that annotators tend to prefermore balanced binary trees.
Fig.
3 plots C1 and C2show the distribution of the tree heights for variouscases and Table 5 reports the corresponding aver-age height of the trees for queries and sentencesof various lengths and the the expected value ofthe height if all trees were equally likely.
The ob-served heights are much lower than the expectedvalues clearly implying the preference of the an-notators for more balanced trees.Thus, the crowd seems to choose the middlepath, avoiding extremes and hence may not be areliable source of annotation for query segmen-tation.
It can be argued that similar biases arealso observed for gold annotations, and therefore,probably it is the inherent structure of the queriesand sentences that lead to such biased distributionof segmentation patterns.
However, note that ?
forQG500 is much higher than all other cases, whichshows that the true agreement between gold anno-tators is immune to such biases or skewed distri-butions in the datasets.
Furthermore, high valuesof ?
for QRand despite the very strong biases inannotation shows that there perhaps is very littlechoice that the annotators have while segmentingrandomly generated queries.
On the other hand,the textual coherence of the real queries and sen-tences provide many different choices for segmen-tation and the Turker typically gets carried awayby these biases, leading to low ?.Bias 3: Phrase structure drives segmentation onlywhen reconcilable with Bias 1.
Whenever the sen-tence or query has a verb phrase (VP) spanningroughly half of it, annotators seem to chunk be-fore the VP as one would expect, quite as of-ten as just after the verb, which is quite unex-pected.
For instance, the sentence A gentlesarcasm ruffled her anger.
gathers asmany as eight flat annotations with a boundary be-tween sarcasm and ruffled, and four witha boundary between ruffled and her.
How-ever, if the VP is very short consisting of a single1720Position Q500 QG500 Q700 S300 QRandBoth 2.24 0.37 2.78 2.08 0.63None 50.34 56.85 35.74 35.84 39.81Right 23.86 21.50 19.02 12.52 15.23Left 18.08 15.97 40.59 45.96 21.21Table 6: Percentages of positions of segmentboundaries with respect to prepositions.
Prepo-sitions occurring in the beginning or end of aquery/sentence have been excluded from the anal-ysis; hence, numbers in a column do not total 100.verb, as in A fleeting and furtive airof triumph erupted., annotators seem toattempt for a balanced annotation due to Bias 1.As a clear middle boundary is not present in suchsentences, the annotations show a lot more varia-tion and disagreement.
For instance, only 1 out of10 annotations had a boundary before eruptedin the above example.
In fact, at least one anno-tation had a boundary after each word in the sen-tence, with no clear majority.Bias 4: Prepositions influence segment bound-aries differently for queries and sentences.
Weautomatically labeled all the prepositions in theflat annotations and classified them according tothe criterion of whether a boundary was placedimmediately before or after it, or on both sidesor neither side.
The statistics, reported in Ta-ble 6, show that for NL sentences a majorityof the boundaries are present before the prepo-sition, marking the beginning of a prepositionalphrase.
However, for queries, a much richer pat-tern emerges depending on the specific preposi-tion.
For instance, to, of and for are oftenchunked with the previous word (e.g., how to |choose a bike size, birthday partyideas for | one year old).
We believethat this difference is because in sentences dueto the presence of a verb, the PP has a well-defined head, lack of which leads to prepositionin queries getting chunked with words that formmore commonly seen patterns (e.g., flightsto and tickets for).Bias 3 and 4 present the complex interpretationof the structure of queries by the annotators whichcould be due to some emerging cognitive model ofqueries among the search engine users.
This is afascinating and unexplored aspect of query struc-tures that demands deeper investigation throughcognitive and psycholinguistic experiments.7 ConclusionWe have studied various aspects of query segmen-tation through crowdsourcing by designing andconducting suitable experiments.
Analysis of ex-perimental data leads us to conclude the follow-ing: (a) crowdsoucing may not be a very effectiveway to collect judgments for query segmentation;(b) addressing fuzziness of granularity for flat seg-mentation by introducing strict binary nested seg-ments does not lead to better agreement in crowd-sourced annotations, though it definitely improvesthe IAA for gold standard segmentations, imply-ing that low IAA in flat segmentation among ex-perts is primarily an effect of unspecified granular-ity of segments; (c) low IAA is not due to the in-herent structural ambiguity in queries as this holdstrue for sentences as well; (d) there are strong bi-ases in crowdsourced annotations, mostly becauseturkers prefer more balanced segment structures;and (e) while annotators are by and large guidedby linguistic principles, application of these prin-ciples differ between query and NL sentences andalso closely interact with other biases.One of the important contributions of this workis the formulation of a new IAA metric for com-paring across flat and nested segmentations, espe-cially for crowdsourcing based annotations.
Sincetrees are commonly used across various linguisticannotations, this metric can have wide applicabil-ity.
The metric, moreover, can be easily adaptedto other annotation schemes as well by defining anappropriate distance metric between annotations.Since large scale data for query segmentation isvery useful, it would be interesting to see if theproblem can be rephrased to the Turkers in a wayso as to obtain more reliable judgments.
Yet adeeper question is regarding the theoretical statusof query structure, which though in an emergentstate is definitely an operating model for the anno-tators.
Our future work in this area would specifi-cally target understanding and formalization of thetheoretical model underpinning a query.AcknowledgmentsWe thank Ed Cutrell and Andrew Cross, MicrosoftResearch Lab India, for their help in setting up theAMT experiments.
We would also like to thankAnusha Suresh, IIT Kharagpur, India, for helpingus with data preparation.1721ReferencesSteven P. Abney.
1991.
Parsing By Chunks.
KluwerAcademic Publishers.Steven P. Abney.
1992.
Prosodic Structure, Perfor-mance Structure And Phrase Structure.
In Proceed-ings 5th DARPA Workshop on Speech and NaturalLanguage, pages 425?428.
Morgan Kaufmann.Steven P. Abney.
1995.
Chunks and dependencies:Bringing processing evidence to bear on syntax.Computational Linguistics and the Foundations ofLinguistic Theory, pages 145?164.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,Sankalan Prasad, and Arpit Maheswari.
2009.
Cor-relates between Performance, Prosodic and PhraseStructures in Bangla and Hindi: Insights from a Psy-cholinguistic Experiment.
In Proceedings of Inter-national Conference on Natural Language Process-ing, pages 101 ?
110.Michael Bendersky, W. B. Croft, and David A. Smith.2009.
Two-stage query segmentation for informa-tion retrieval.
In Proceedings of the 32nd interna-tional ACM Special Interest Group on InformationRetrieval (SIGIR) Conference on Research and De-velopment in Information Retrieval, pages 810?811.ACM.Shane Bergsma and Qin Iris Wang.
2007.
LearningNoun Phrase Query Segmentation.
In Proceedingsof Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 819?826.Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, andKV Ramakrishnamacharyulu.
1995.
Natural lan-guage processing: a Paninian perspective.
Prentice-Hall of India New Delhi.David J. Brenes, Daniel Gayo-Avello, and RodrigoGarcia.
2010.
On the fly query segmentation usingsnippets.
In CERI ?10, pages 259?266.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using amazon?smechanical turk.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?09, pages 286?295.
Associa-tion for Computational Linguistics.Vitor R Carvalho, Matthew Lease, and Emine Yilmaz.2011.
Crowdsourcing for search evaluation.
ACMSigir forum, 44(2):17?22.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-Rong Wen.
2008.
Are Click-through Data Adequatefor Learning Web Search Rankings?
In Proceed-ings of the 17th ACM Conference on Informationand Knowledge Management, pages 73?82.
ACM.Matthias Hagen, Martin Potthast, Benno Stein, andChristof Bra?utigam.
2011.
Query SegmentationRevisited.
In Proceedings of the 20th Interna-tional Conference on World Wide Web, pages 97?106.
ACM.Matthias Hagen, Martin Potthast, Anna Beyer, andBenno Stein.
2012.
Towards Optimum Query Seg-mentation: In Doubt Without.
In Proceedings of theConference on Information and Knowledge Man-agement, pages 1015?1024.Jian Huang, Jianfeng Gao, Jiangbo Miao, XiaolongLi, Kuansan Wang, Fritz Behr, and C. Lee Giles.2010.
Exploring web scale language models forsearch query processing.
In Proceedings of the 19thinternational conference on World wide web, WWW?10, pages 451?460, New York, NY, USA.
ACM.Klaus Krippendorff.
2004.
Content Analysis: AnIntroduction to its Methodology.
Sage,ThousandOaks, CA.Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, andKuansan Wang.
2011.
Unsupervised query segmen-tation using clickthrough for information retrieval.In SIGIR ?11, pages 285?294.
ACM.Knut Magne Risvik, Tomasz Mikolajewski, and PeterBoros.
2003.
Query segmentation for web search.In WWW (Posters).Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-hury, and Srivatsan Laxman.
2012.
An IR-basedEvaluation Framework for Web Search Query Seg-mentation.
In Proceedings of the International ACMSpecial Interest Group on Information Retrieval (SI-GIR) Conference on Research and Development inInformation Retrieval, pages 881?890.
ACM.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?08, pages 254?263, Stroudsburg, PA,USA.
Association for Computational Linguistics.Bin Tan and Fuchun Peng.
2008.
Unsupervised QuerySegmentation Using Generative Language Modelsand Wikipedia.
In Proceedings of the 17th Inter-national Conference on World Wide Web (WWW),pages 347?356.
ACM.Chao Zhang, Nan Sun, Xia Hu, Tingzhu Huang, andTat-Seng Chua.
2009.
Query segmentation based oneigenspace similarity.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort?09, pages 185?188, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.1722
