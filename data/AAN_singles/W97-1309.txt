Towards Reliable Partial Anaphora Reso lut ionSab ine  Berg le r  *Computer Science Department, Concordia University1455 de Maisonneuve Blvd.
W., Montrgal, Qugbec, H3G 1M8bergler@cs, concordia, caAbst ractThis paper assumes that currently, anaphora resolutionat a desired level of reliability has to remain partial.
Itpresents the thesis that multiple small ("expert") proce-dures of known reliability that are conceived for partialanalysis have to be developed and combined in orderto increase coverage.
These resolution experts will bespecific to style, domain, task, etc.
The paper describescorpus analysis that suggests uch experts and their po-tential ordering.
A quick and partial implementation fthe ideas is evaluated on Wall Street Journal articles.In t roduct ionTotally correct anaphora resolution requires full nat-ural language understanding, since anaphoric relationscould be hidden in the context.
At present, only partialnatural anguage understanding is possible.
This paperclaims that one way to increase the reliability (or atleast in assessing the reliability) of anaphora resolutionlies in acknowledging and making use of this limitation.Strategies of anaphora resolution depend on the genreand style of text under consideration, as the differentstyle manuals for major newspapers show.
Since manypractical applications are limited to a certain genre, it islegitimate to optimize results by studying peculiaritiesof the genre.We focus our attention on the Wall Street Journalcorpus available on CD-ROM from the Association forComputational Linguistics.
Our main interest at theoutset was in assessing the lexical complexity of NPcoreference to guide us in our development of a lexi-con.
We reported initial corpus analysis results thatshow the relative frequency of semantic relations thathold between elements in eoreference chains \[Berglerand Knoll, 1996\].
Analyzing the reference chains 1 of*This work is funded in part by the Natural Sciences andEngineering Research Council of Canada and Fonds pour laformation de chercheurs et l'aide k la recherche.1 Reference chains in this study contain all (partial) nounphrases that corefer in a text.
They are thus different from\[Morris and Hirst, 1991\], who do not limit their referencechains to NPs.
6279 articles (28,798 words) from the Wall Street Jour-nal we found that 35% of the subsequent references areactually equal to the first reference of that entity, 23%are close variations of the first reference (i.e.
retain atleast the same headword).
Pronouns and appositionsaccount for 22% and systematic lexical relation (syn-onymy and hyponymy) for 7%.
We consider the re-maining 13% to be tough cases that might require fullsyntactic, lexical, and semantic processing.
The other87% we expect can be addressed with a subset of thesetools.This paper presents further results of this corpusanalysis which lead to some resolution strategies andpresents an experiment in implementing some of theseaspects in a knowledge poor system.Corpus  Ana lys i s  Resu l tsThe corpus study of 79 articles from the Wall StreetJournal was performed manually by a single analyst,thus the results are as consistent as possible for a man-ual analysis.
The analyst separated all NPs from thetext, when appropriate separating referring sub-NPsfrom larger NPs, and for each NP placed it in a chainthat contained a coreferent when possible or started anew chain.
Chains were additionally annotated for afew features of interest, such as length of the article(short or long), the textual designator (see below), andwhether the chain was part of the topic of the article(see next Section.)
These annotations were ultimatelyleft to the judgement of the analyst within a strict setof rules.Particularly encouraging are the first two lines in Fig-ure 1.
It turns out that over a third of the coreferringNPs are identical and can therefore be recovered reli-ably and correctly without linguistic tools.
Almost aquarter of the coreferring NPs are very close to the firstNP in the reference chain, that is they share at least theheadword, if not a larger substring with the first refer-ence.
Thus in theory almost 60% of coreferring NPsshould be identifiable with very simple techniques oncethe NPs have been identified.To put things into perspective, let us reconsider thenumbers in Figure 1 with some additional information.Semantic relation TotalEqual to first reference 1424Close to first reference 955Appositions 128Pronouns 756Acronyms 46Synonyms 73Hypernyms 174Neither of the above 520Total NPs 4076Percent35%23~o3%19%1%2%4%13%100%Figure 1: Semantic relations between elements in coref-erence chainsThe 4076 NPs analyzed there constitute roughly halfof the NPs counted in total in that collection of texts,namely 8,027.
The 3,951 NPs that are not analyzed inFigure 1 are NPs that do not corefer with any otherNP in the text.
These singular occurrences account for49% of all NPs.
One obvious question is: can singularoccurrences of entities be singled out?
This is an openquestion.
We address the easier problem of: how canwe determine NPs which are likely to corefer and whichare most important o the text overall?Top ica l i tyThe same study showed that NP chains considered to bein the topic of an article usually require anaphora reso-lution and are lexically more complex than non-topicalreference chains.
For this study we defined a topic tobe one of the NPs that occur in the headline or the firstsentence 2 (see \[Lundquist, 1989\] for a motivation of thisheuristic.)
A text can have no more than 4 topics (thisnumber was chosen intuitively.)
The analyst decidedhow many topics there were in each article accordingto her understanding of it.
The text in Figure 3 wasassigned a single topical chain containing NPs 1, 2.1,10, and 11.
There are 185 topical chains in 79 articles,averaging 2.4 topics per article.
17% of the topical ref-erence chains are singular occurrences, i.e.
NPs thatdo not corefer.
3 That  establishes that the topic of anarticle is usually referred to more than once.
Intuitivelywe assume that the topic of an artice is more importantto resolve than non-topical NPs.
One partial strategy,consequently, is to establish potentially topical NPs inthe first n sentences of a newspaper article and to re-solve coreference only to these NPs.
This strategy hasthe advantage of reducing the search space considerablyand of focusing on important (topical) chains.2The heuristic to find potential topics of an article is easyto implement: consider all NPs in the headline and the firstsentence.3There are two possible explanations why this numberis relatively high: the Wall Street Journal contains everalvery short segments of very few sentences, thus a topic thatis mentioned only once is a possibility.
Also, headlines oftenuse a summarizing term that never corefers with anotherNP, but rather corefers with the article as a whole.63Textua l  Des ignatorsWe feel that NP resolution in newspaper articles isstraightforward (by design) for human readers becauseof recurrent terms that designate an entity.
To as-sess this intuition quantitatively, \[Bergler and Knoll,1996\] report the sum of distinct words within a chainover all non-singular chains.
This count eliminates therecurrent words, whether empty (the) or descriptive(company).
4 The results show the significance of thisphenomenon which we call textual designator.
A tex-tual designator in this study is the first non-pronominalreference to an entity.
Consider the text in Figure 3.One chain consists of two identical NPs, its Houstonwork force (NPs 3 and 15.)
Other textual designatorsof that text are NP1, NP6, and NP16.
Counting thenumber of different words in each chain allows us to as-sess the lexical diversity within a chain and the contri-bution of the textual designator to that diversity.
Theresults are summarized in Figure 2.Chain Totaltype chainsTopic 1,632Not topic 19,449Non-singularchains1,5445,672Excludingdesignator9812,808Figure 2: Number of different words per reference chain.We find 1,632 different words in topical chains and19,449 different words in non-topical chains.
When weconsider only the chains that actually involve coref-erence, the sums reduce to 1,544 different words fortopical chains and 5,672 different words for non-topicalchains.
Removing the words that are part of the textualdesignator, we observe a drastic reduction to 981 differ-ent words for topical chains and 2,808 different wordsfor non-topical chains.The 185 topical chains average about 9 differentwords per chain while non-topical chains average 4.Removing the words of the textual designator reducesthe different word count by 40% for topical and 86%for non-topical chains when counting all chains.
If wecount only chains that involve coreference the reduc-tions are 36% for topical chains and 50% for non-topicalchains.
The number of different words excluding thewords of the textual designator on average are 5 fortopical chains and .6 for non-topical chains.
5These numbers suggest that the textual designatordefined as the first reference to an entity leads to astrong resolution heuristic, one that is in fact strongerfor non-topical chains.
The numbers also show howsurprisingly small the lexical diversity of words outside4Note that the same word could potentially be countedin every single chain, as the recurrence is only eliminatedwithin a chain.5This average includes singular chains.a textual designator are.
The sum for both types ofchains is 3,789.
The textual designators are made upof 563 different words for topical non-singular chainsand 2864 different words for non-topical, non-singularchains.
The total number of different words for singularchains is 13,865.
Thus the sum total of different wordsfor first references i 17,292, a surprisingly high num-ber considering that the overall corpus has only 28,798words, which includes all duplicates.Resolution StrategiesWhile we are still mining the results of our study formore data, some strategies for resolution in general arealready emerging.We believe that there is strong evidence for an ap-proach to anaphora resolution in multiple passes, whereearlier passes implement more reliable 6, less knowledge-intensive, and computationally less complex strategieswith faster tools than later ones.
For each pass, anexpected reliability should ideally be known.The expectation is that early, knowledge-poor, andhighly reliable passes can be used for almost any task.Matching equal NPs, for instance, can be done withbasic, fast tools independently of further linguistic pro-cessing.
Anaphora resolution could then be tailored toparticular needs by determining which levels of reliabil-ity are acceptable to the task and using the passes 7 upto that threshold.
For the remaining resolution task, adomain- and genre-specific set of procedures has to bedeveloped.We argue that such a multi-pass approach as advan-tages to a monolithic approach, be it statistical or sym-bolic.
While most symbolic anaphora resolution sys-tems probably correctly identify identical NPs as core-letting, making this a first step and using very fast, lowlevel tools can pre-process a text faster.
The modularapproach allows for use of the tools of choice at eachlevel.
Moreover, a text can be left partly resolved, po-tentially allowing anaphora resolution to be interleavedwith other linguistic processing as required.Another interesting result of our study is the factthat many NPs correctly resolve to more than onecoreferring NP, and often resolve to the first reference.This provides upport for the viability of partial pars-ing methods, because a missing link does not mean therest of the chain is unresolvable.
As mentioned above,this also allows for a focused partial resolution strategythat attempts to resolve subsequent NPs only to a setof NPs determined at the outset (e.g., topical NPs, pre-determined subjects or persons, .
.
.
).
This provides thebasis for a series of principled heuristics.
These partialresolution strategies are of great importance where theamount of text to be processed is large but the depth6 Refiability here is with respect o errors of commission.7An appropriate subset of the following heuristics can ofcourse be combined into a single pass, which is the case inour experimental system presented below.64of processing is shallow, as for certain text annotationtasks.The study presented above suggests the followingstrategies:1.
Ident ica l  NPsPrerequisites: NP boundariesProcedure: string matching2.
Focused  par t ia l  reso lu t ionPrerequisites: NP boundaries, identification proce-dures of the NP chains of interestProcedure: according to desired resolution strategies3.
Common headPrerequisites: parsed NPsProcedure: matching head positions4.
Appos i t ionsPrerequisites: parsed NPsProcedure: matching syntactic pattern of apposition5.
Extended head match ingPrerequisites: parsed NPs, lexicon or thesaurusProcedure: compare heads for synonymy, hypernomy6.
P ronoun reso lu t ionPrerequisites: parsed textProcedure: as described in the literatureThese resolution strategies are not exhaustive, norcan an optimal ordering be assigned in general; the de-sired level of reliability, the genre and style features, andany possible additional linguistic processing will deter-mine different combinations, extensions, and orderings.Knowledge Poor  Reso lu t ionWe implemented an experimental resolution system(ERS) based on these ideas.
The system uses as inputthe parse trees provided by the Penn Treebank on theACL CD-ROM.
These parse trees have been correctedmanually for inconsistencies.The system includes a (partial) implementation f allof the six strategies except for Focused Partial Resolu-tion.
The most carefully worked out strategy is pronounresolution.
Pronoun resolution makes use of the parsetrees and follows the ideas in \[Lappin and Leass, 1994,Hobbs, 1978\].
The other strategies have only beenpartially or crudely implemented.
The Common Headstrategy, for instance, uses a crude heuristic to deter-mine the head of a complex noun phrase that fails incertain cases.
Extended Head Matching is limited tovery few lexical items such as company, which receivespecial treatment.
No lexicon is used, the required lex-ical knowledge has been provided in a list of gendereditems.
An additional limitation is the fact that the sys-tem considers coreference only within a sentence andbetween adjacent sentences.A lgor i thmFor every NP in the text:(1 Telxon Corp. a) said (2 (2.1<<ref=l its 2.a) vice president formanufacturing 2) resigned and (3 (3.,<re/=1 its 3.1) Houstonwork force 3) has been trimmed by (4 40 people 4), or about(5<tel=4 15% 5) ?
(6<rey=a The maker of hand-held computers and computersystems 6) said (7 the personnel changes r) were needed toimprove (s the efficiency s) of (9 (9.a<rey=s its 9.1) manufac-turing operation 9).
(lO<<ref=l The company 10) said (ll<<re/=lo it aa) hasn'tnamed (12 a successor 12) to (a3>re$=a4 Ronald Burton 13),04<re\]=2 the vice president a4) who resigned.
(15<re/=3(15.1<ref=l Its 15.1) Houston work force as) now totals (as230 as).Figure 3: Manually determined coreference in a shorttext from the Wall Street Journal1.
Determine candidate referents within the sentence.If none are found (i.e.
lack of agreement), determinecandidate referents in previous entence.2.
Test each candidate referent for actual coreferenceusing:(a) Common Head (with slight modifications)(b) Extended Head Matching (limited to few cases)(c) Appositions(d) Copula3.
If there is more than one possible coreference, selectbest.4.
Merge the new coreference pair with existing refer-ence chains or start a new chain.Sample  OutputThis algorithms is clearly too constrained to everachieve full resolution, but except for the pronoun res-olution, it was quickly implemented and performs ur-prisingly well.Both strengths and limitations are best illustrated ona short example.
Consider the text in Figure 3, whichhas been annotated with manually determined corefer-ence links following the Lancaster notation \[Fligelstone,1992\] with slight modificationsThe annotation (2.1<re/=1 means that NP2.1 (a sub-NP of NP2) starts at this point and that it refers back-wards to NP1.
The << sign indicates that this referencehas also been detected by ERS.ERS determined 4 reference chains in this article.The first chain consists of NPs 1, 2.1, 10, and 11.
Thesecond chain contains NPs 6 and 9.1, the third containsNPs 3 and 15, and chain number four contains NPs16 and 15.1.
The coreference link stipulated for chainfour is wrong (an artifact of a strong bias towards in-trasentential resolution.)
All other stipulated corefer-ence links are correct.
There are six coreferring NPswhose coreference link has not been identified.
Two ofthe stipulated chains could be merged (chains one andtwo.
)65Pre l iminary  Resu l tsWe evaluated ERS on a set of twelve short articles fromthe Wall Street Journal which had also been part of thecorpus study described above.
The chains of the studywere compared to the chains stipulated by ERS andboth were examined for correctness by a person notinvolved in the development of either.Text Sent.
NPsIIIIIIIVVVIVIIVIIIIXXXIXII11 614 2014 858 534 143 153 172 133 167 307 3810 49Wrong Omit ted4 7% 13 21%1 5% 6 30%2 2% 20 24%1 2% 17 32%0 0% 2 14%0 0% 1 7%1 6% 4 24%0 0% 3 23%1 6% 3 19%0 0% 5 17%16 42% 13 34%20 41% 18 37%76 411 \[ 46 \[ 11% 105 \[ 26%Figure 4: Evaluation of NP coreferenceFigure 4 shows the length of the articles in numberof sentences, the number of NPs per article, the num-ber of NPs that have been placed in the wrong chain(wrong)  and the number of coreference links that havenot been identified (omi t ted . )
ERS showed an errorrate of 11% (wrongly dereferenced NPs) and an omis-sion rate of 26%.
Interestingly, the error rates for in-dividual articles fluctuates between 0% and 7% for allbut two articles.
The two articles with over 40% errorrate are both very typical Wall Street Journal articlesthat cover earnings of a particular company.
The arti-cles are very similar and the high error rate is due tothe same shortcoming of ERS: it has no special treat-ment for amount terms and currency terms and thuscreates chains that contain all NPs with headword yenor million (in these articles about Japanese companies18.32 billion yen are also expressed as $128.9 million.
)Since none of these NPs should corefer, the error rate isextremely high but could be fixed with shallow lexicalknowledge.Before analyzing the performance of the heuristics inthe evaluation, consider the results for reference chains.The twelve articles have 81 (non-singular) referencechains, ERS stipulates 50.
35 of the 81 chains have notbeen established by ERS (43%), while 17 (21%) containNPs that do not corefer with any member of the chain.14 chains (17%) were correct (agreement between study,ERS, and verification), another 19 chains were correctbut incomplete (24%).
Thus 33 chains were establishedcorrectly and contained no errors, this amounts to a41% accuracy rate for chains.
This figure is extremelylow compared to the NP coreference resolution resultbecause in the case of an incorrect coreference link inTextIIIIIIivVVIVIIVIIIIXXXIXIIE104181011424410137 3 1 34 2 i"' 110 4 4 26 1 4 10 0 0 01 0 1 01 0 0 11 0 1 03 1 1 14 1 3 06 0 2' 47 2 481 50 14 19 17Om1tted32851031114635Figure 5: Evaluation of reference chainsa chain the entire chain is discounted.
Correct chainsare those were ERS picked up all the NPs that the an-alysts determined as belonging to that chain.
Partlycorrect chains are those were every coreference link iscorrect but some are missing.
Wrong chains are thosethat include an incorrect coreference link and omittedchains are those that the analysts determined to existbut which have no counterpart in ERS's output, sCauses  o f  E r rorsAn important first observation is that the human an-alysts did not agree in all cases, which reminds us totake figures and percentages with a grain of salt.
Dis-agreements between the analysts are of course few.
Themajority of ERS's errors stem from three sources: lackof a lexicon, lack of syntactic finesse, and simplistic headmatching for the Common Head heuristic.The lack of any lexicon tricks ERS into matching allamounts measured in yen as possessing the same headand thus coreferring.
A lexicon could also avoid pro-noun agreement mistakes to a larger extent.ERS lacks syntactic finesse even though it uses therather sophisticated Penn Treebank parse trees, be-cause its extraction of the head is based on heuristicsrather than syntactic knowledge.Most mistakes, by far, are due to matching identi-cal heads where the modifying information in the NPmakes it clear that no coreference exists, such as an is-s Note that the columns for correct, partly correct, wrong,and omitted chains do not add up to the number of realchains.
This is due to several factors.
Take for instanceText II, given in Figure 3, where two partly correct chainsshould have been merged and ERS's incorrect chain doesnot correspond to any real chain, thus there is an overcountof two chains.
For the entire test set there is an overcountof 4 chains from three articles.sue price of $849 .
.
.
and a conversion price of $25 .
.
.
.These, as well as omissions, would be re~ced by con-sidering the compositional semantics of the NPs.
Omis-sions would also be reduced by implementing the res-olution strategies fully (acronyms, hyponymic relationsof head nouns, etc.
)ConclusionThis paper reviews results from previous corpus analy-sis and presents new data that show that simple resolu-tion procedures based on lexical similarity can achievepartial anaphora resolution.
We advocate a multi-pass approach, sequencing appropriate simple resolu-tion strategies for any given application.
These passescan be interleaved with other linguistic processing andthus afford more flexibility than a monolithic anaphoraresolution strategy.We have tested these ideas with a crude experimentalsystem that had an error rate of 11% and an omissionrate of 26%.
This result confirms the estimate from ourcorpus analysis and error analysis indicates refinementsrequired for a robust system.AcknowledgementsThe manual corpus analysis was done by Sonja Knoll.ERS has been implemented by Dr. Xiaobin Li.
JenniferScott did the evaluation.
Thanks also to the anonymousreviewers, whose comments helped to improve the pa-per.References\[Bergler and Knoll, 1996\] S. Bergler and S. Knoll.
Coref-erence patterns in the wall street journal.
In C. Percy,C.F.
Meyer, and I. Lancashire, editors, Synchronic orpuslinguistics.
Papers from the sixteenth International Con-ference on English Language Research on ComputerizedCorpora (1CAME 16).
Rodopi, Amsterdam, 1996.\[Fligelstone, 1992\] S. Fligelstone.
Developing a scheme forannotating text to show anaphoric relations.
In G. Leit-net, editor, New Directions in English Language Corpora.Mouton deGruyter, Berlin, 1992.\[Hobbs, 1978\] .I.R.
Hobbs.
Resolving pronoun references.Lingua, 44:311-338, 1978.\[Lappin and Leass, 1994\] S. Lappin and H.J.
Leass.
An al-gorithm for pronominal anaphora resolution.
Computa-tional Linguistics, 20(4):535-561, 994.\[Lundquist, 1989\] L. Lundquist.
Modality and text consti-tution.
In M-E. Conte, J.S.
PetSfi, and E. SSzer, editors,Text and Discourse Connectedness.
Proceedings of theConference on Connexity and Coherence, Urbino, July1984.
John Benjamins Publishing Co., Amsterdam, 1989.\[Morris and Hirst, 1991\] J. Morris and G. Hirst.
Lexicalcohesion computed by thesaural relations as an indica-tor of the structure of text.
Computational Linguistics,17(1):21-48, 1991.66
