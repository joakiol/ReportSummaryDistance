CLAWS4: THE TAGGING OF THE BRITISH NATIONAL CORPUSGeoffrey Leech, Roger Garside and Michael BryantUCREL, Lancaster University, UK1 INTRODUCTIONThe main purpose of this paper is to describethe CLAWS4 general-purpose grammatical tagger,used for the tagging of the 100-million-word BritishNational Corpus, of which c.70 million words havebeen tagged at the time of writing (April 1994))We will emphasise the goals of (a) gener~d-purposeadaptability, (b) incorporation of linguistic knowl-edge to improve quality ,and consistency, and (c)accuracy, measured consistently and in a linguisti-cally informed way.The British National Corpus (BNC) consists ofc.100 million words of English written texts andspoken transcriptions, sampled from a comprehen-sive range of text types.
The BNC includes 10million words of spoken h'mguage, c.45% of whichis impromptu conversation (see Crowdy, forthcom-ing).
It also includes ,an immense variety of writtentexts, including unpublished materials.
The gr,'un-matical tagging of the corpus has therefore requiredthe 'super-robustness' of a tagger which can adaptwell to virtually all kinds of text.
The tagger also hashad to be versatile in dealing with different agsets(sets of grammatical category labels-- see 3 below)and accepting text in varied input formats.
For thepurposes of the BNC, l, he tagger has been requircdboth to accept and to output ext in a corpus-orientedTEl-confonnant mark-up definition known as CDIF(Corpus Document Interchange Format), but withinthis format many variant fornaats (affecting, forexample, segmentation into words and sentences)can be readily accepted.
In addition, CLAWS al-XThe BNC is the result of a collaboration, supportedby the Science mid Engineering Research Council (SERCGrant No.
GR/F99847) ,and the UK Dep,'u'tment of Tradeand Industry, between Oxford University Press (lead p~u't-ner), Longman Group Ltd., ChambersHarrap, OxfordUniversity Computer Services, the British Library andLancaster University.
We thank Elizabeth Eyes, NickSmith, mid Andrew Wilson for their v,'duable help in thepreparation fthis paper.lows variable output formats: for the current ag-ger, these include (a) a vertically-presented formatsuitable for manual editing, and (b) a more com-pact horizontally-presented format often more suit-able for end-users.
Alternative output formats arealso glowed with (c) so-called 'portmanteau tags',i.e.
combinations of two alternative tags, where thetagger calculates there is insufficient evidence forsafe dis,'unbiguation, and (d) with simplified 'plaintext' malk-up for the human reader.
(See Tables Iand 2 for examples of output formats.
)CLAWS4, the BNC tagger, 2 incorporates manyfeatures of adaptability such as the above.
It "alsoincorporates many refinements oflinguistic analysiswhich have built up over 14 years: particularly inthe construction and content of the idiom-taggingcomponent (see 2 below).
At the same time, thereare still many improvements to be made: the claimthat 'you can put together a tagger from scratch ina couple of months' (recently heard at a researchconference) is, in our view, absurdly optimistic.2 THE DESIGN OF THE GRAM-MATICAL TAGGER (CLAWS4)The CLAWS4 tagger is a successor f the CLAWS 1tagger described in outline in Marshall (1983), andmore fully in Garside t al (1987), and has the samebasic architecture.
The system (if we include input,and output procedures) has five major sections:(a) segnnentation of text into word and sentenceunits(b) initial (non-contextual) part-of-speech assign-ment \[using a lexicon, word-ending list, andvarious sets of rules for tagging unknownitems\]2CLAWS4 has been written by Roger Garside, withCLAWS adjunct software written by Michael Bryant.622(c) rule-driven contextual part-of-speech assign-ment(d) probabilistie tag disambiguation \[Markov pro-cess\]\[(c') second pass of (c)\](e) output in intermediate form.The intermediate form of text output is the formsuitable for post-editing (see 1 above; also Table 1),which can then be converted into other formats ac-cording to particular output needs, as already noted.The pre-processing section (a) is not trivial, since,in any large and varied corpus, there is a needto handle unusual text structures (such as thoseof many popular and technical magazines), lessusual graphic features (e.g.
non-roman alphabeticcharacters, mathematical symbols), and features ofconversation transcriptions: e.g.
false starts, incom-plete words and utterances, unusual expletives, un-planned repetitions, and (sometimes multiple) over-lapping speech.Sections (b) and (d) apply essentially a HiddenMarkov Model (HMM) to the assignmeut and dis-ambiguation of tags.
But file intervening section (c)has become increasingly important as CLAWS4 tuLsdeveloped the need for versatility across a range oftext types.
This task of rule-driven contextualpart-of-speech assignment began in 1981 as an 'idiom-tagging' program for dealing, in the main, withparts of speech extending over more than one or-thographic word (e.g.
complex prepositions such asaccording to and complex conjunctions such as sothat).
In the more fully developed form it now has,this section utilises everal different idiom lexiconsdealing, for example, with (i) general idioms suchas as much as (which is on one analysis a singlecoordinator, and on another analysis, a word se-quence), (it) complex munes such as Dodge Cityand Mrs Charlotte Green (where the capital etteralone would not be enough to show that Dodge madGreen are proper nouns), (iii) foreign expressionssuch as annus horribilis.These idiom lexicons (with over 3000 entries inall) can match on both tags and word-tokens, em-ploying a regular expression formalisnl at the levelboth of the individual item and of the sequenceof items.
Recognition of unspecified words withinitial capitals is also incorporated.
Conceptually,each entry has two parts: (a) a regular-expression-based 'template' specifying a set of conditions onsequences of word-tag pairs, and (b) a set of tag as-sigmnents or substitutions to be performed on anysequence matching the set of conditions in (a).
Ex-amples of entries from each of tile above kinds ofidiom lexicon entry are:(i) did/do\]does, (\[XXO/AVO/ORD\])2, \[VVB\] VVI(it) Monte/Mount/Mt NP0, (\[WIC\])2 NP0, \[WIC\]NP0(iii) ad AV021 AJ021, hoe AV022 AJ022Exphmatory note:(a) Symbolic formalism:Let "IT be any tag, and let ww be any word.Let n,m be arbitrary integers.
Then:ww TT represents a word and its associated tag, separates a word from its predecessor\[TT\] represents :m already assigned tag\[WICI represents an unspecified word with a Word Ini-tial Capilal"I"I'/'I'T means 'either '71" or TT'; ww/ww means 'eitherWW or WW'ww'13'TT represenls an unresolved ambiguity be-tween "lq' and TI"TT* represents a tag wilh * marking the location ofunspecified ch:u'acters(\[TTI)n represents he number of words (up to n) whichmay optionally intervene at a giveq point in thetemplateTTnm represents the 'ditto tag' attached to ~m ortho-graphic word to indicate it is part of a complexsequence (e.g.so that is tagged so CJS21 , thatCJS22).
The variable n indicates the number of or-thographic words in the sequence, and m indicatesthat the current word is in tile ttzth position in thatsequence.
(b) Ex~unples ofword lags (in the C5 'basic' tagset):A J0 adjective (tmm:uked)AV0 adverb (unmarked)CJS subordinating conjunctionNP0 proper nounORD ordinal numberVVB finite base form of lexical verbVVI infinitive of lexical verbXX0 negative mmker: not or n't623(c) Explanation of the three rules above:Rule (i) ensures that following a finite form of do and(optionally) up to two adverbs, negators or ordinals,a base form of the verb is tagged as an infinitive.Rule (ii) ensures that in complex names uch as MonteAlegre, Mount Pleasant, Mount Palomar Obser-vatory, Mt Rushmore National Memorial, all thewords with word-initi,'d caps are tagged as propernouns.Rule (iil) ensures that the Latin expression ad hoc istagged as a single word, either an adjective or anadverb.We have also now moved to a more complex,two-pass application of these idiomlist entries.
It ispossible, onthe first pass, to specify ambiguous out-put of an idiom assignment (as is necessary, e.g., foras much as, mentioned earlier), so that this can thenbe input to the probabilistic disambiguation process(d).
On the second pass, however, after probabilis-tic disambiguation, the idiom entry is deterministicin both its input and output conditions, replacingone or more tags by others.
In effect, this last kindof idiom application is used to correct a tagging er-ror arising from earlier procedures.
For exanlple, anot uncommon result from Sections (a)-(d) is thatthe base form of the verb (e.g.
carry) is wronglytagged as a finite present ense form, rather thanan infinitive.
This can be retrospectively correctedby replacing VVB (= finite base form) by VVI (=infinitive) in appropriate circumstances.While the HMM-type process employed in Sec-tions (b) and (d) affirms our faith in probabilisticmethods, the growing importance of the contextualpart-of-speech assigxwaent in (c) and (c') demon-strates the extent o which it is important to tran-scend the limitations of the orthographic word, asthe basic unit of grammatical tagging, and also toselectively adopt non-probabilistic solutions.
Theterm 'idiom-tagging' originally used was never par-ticularly appropriate for these sections, which nowhandle more generally the interdependence betweengrammatical nd lexical processing which NLP sys-tems ultimately have to cope with, and are also ableto incorporate parsing information beyond the rangeof the one-step Markov process (based on tag bi-gram frequences) employed in (d).
3 Perhaps theterm 'phraseological component' would be moreappropriate here.
The need to combine probabilistic3We have experimented with a two-step Markov pro-cess model (using tag trigrams), and found little benefitover the one-step model (using tag bigrams).and non-probabilistic methods in tagging has beenwidely noted (see, e.g., Voutilainen et al 1992:14).3 EXTENDING ADAPTABILITY:SPOKEN DATA AND TAGSETSThe tagging of 10 million words of spoken data(including c.4.6 million words of conversation)presents particular challenges to the versatility ofthe system: renderings of spoken pronunciationssuch ,as 'avin' (for having) cause difficulties, as dounplanned repetitions uch as I et, mean, I mean,1 mean to go.
Our solution to the latter pn~blemhas been to recognize such repetitions by a specialprocedure, and to disregard, in most cases, the re-peated occurrences of tile same word or phrase forthe purposes of tagging.
It has become clear thatthe CLAWS4 resources (lexicon, idiomllsts, and tagtransition matrix), developed for written English,need to be adapted if certain frequent and ratherconsistent errors in the tagging of spoken data areto be avoided (words such as I, well, and right areoften wrongly tagged, because their distribution inconversation differs mazkedly from that in writtentexts).
We have moved in this direction by allowingCLAWS4 to 'slot in' different resources accordingto the text type being processed, by e.g.
providinga separate supplementary lexicon and idlomlist forthe spoken material.
Eventually, probabilistic anal-ysis of the tagged BNC will provide the necessaryinformation for adapting datastructures at run timeto the special demands of particular types of data,but there is much work to be done before this po-tential benefit of having tagged a large corpus isrealised.The BNC tagging takes place within the contextof a larger project, in which a major task (undertakenby OUCS at Oxford) is to encode the texts in a TEI-conformant mark-up (CDIF).
Two tagsets have beenemployed: one, more detailed than the other, isused for tagging a2-million-word Core Corpus (anepitome of the whole BNC), which is being post-edited for maximum accuracy.
Thus tagsets, liketext formats and resources, are among the featureswhich are task-definable in CLAWS4.
In general,the system has been revised to allow many adaptivedecisions to be made at run time, and to render itsuitable for non-specialist researchers touse.6244 ERROR RATES AND WHATTHEY MEANCun~ntly, judged in terms of major categories, 4 thesystem has an error-rate of approximately 1.5%, andleaves c.3.3% ,'unbiguitics unresolved (as portman-teau tags) in the output.
However, it is all too easyto quote error rates, without giving enough infor-mation to enable them to be properly assessed.
~ Webelieve that any evaluation of the accuracy of auto-matte grammatical tagging should take account ofa number of factors, some of which arc extremelydifficult o measure:4.1 ConsistencyIt is necessary to measure tagging practice againstsome standard of what is an appropriate tag for agiven word in a given context.
For example, ishor-rifying in a horrifying adventure, or washing in awashing machine an adjective, a norm, or a verbparticiple?
Only if this is specified independently,by an annotation scheme, c:m we feel confident injudging where the tagger is 'correct' or 'incorrect'.For the tagging of the LOB Corpus by the earli-est version of CLAWS, the :umotation scheme waspublished in some detail (Johzmssou et al1986).
Weare working on a similar annotation scheme docu-ment (at present agrowing in-house document) forthe tagging of the BNC.4"lqqe rror rate and ambiguity rate are less favourableif we take account of errors and ambiguities which occtu"within major categories.
E.g.
the porlmanteau tag NP0-NN1 records contidently that a word is a noun, but notwhether itis a proper or common oun.
If such cases areadded to the count, then the estimated error rate rises lo1.78%, and the estimated ambiguity ale to 4.60%.5One reasonable attempt to evaluate competing ac-curacy of different aggcrs is that in Voutilaincn ct al(1992:11-13), where it is argued, on the basis of tltetagging of sample written texls, that the performance ofthe llclsinki constraint grammar p:u'ser ENGCG is supe-rior to that of CLAWS 1 (the e:u-liest version of CLAWS,completed in 1983), which is in turn is somewhat supe-rior to Church's Parts tagger.
While recognizing that heaccuracy of the tlelsinki system is impressive, we notealso that he method of evaluation (in terms of 'precision'and 'recall') employed by Voutilainen ct al in not easyto comp~u'e with the method employed here.
Further,a strict attempt at measuring compmability would haveto take fuller account of the 'consistency' and 'qu'dily'criteria we mention, and of the need it) compare acrossa broader rtmge of texts, spoke~ and written.
This issuec,-umot be taken further in this paper, but will hopefullybe tile bztsis of future research.4.2 Size of TagsetIt might be supposed that tagging with a finer-grained tagset which contains more tags is morelikely to produce rror than tagging with a smallerand cruder tagset.
Ill tile BNC project, we have useda tagset of 58 tags (the C5 tagsct) for the whole cor~pus, ,'rod in addition we have used a larger tagsetof 138 tags (the C6 tagset) 6 for the Core Corpus of2 million words.
The evidence so far is that thismakes little difference to the error rate.
llut sizeof tagset must, in the absence of more conclusiveevidence, remain a factor to be considered.4.3 Discr iminat ive Value of TagsTile difficulty of grammatical tagging is directly re-lated to the nuinber of words for which a given tagdistinction is made.
This measure may be called'discriminative alue'.
For example, in the C5tagset, one tag (VDI) is used for the inlinitive ofjust one verb - -  to do - -  where:ts ~mother tag (VVI)is used for the infinitive of all lexical verbs.
On lheother hand, VDB is used for linite base forms ofto do (including tile present tense, imperative, andsubjunctive), whereas VVB is used of finite t)aseforms of all lexic:d verbs.
It is clc,'u" the tags VDIand VDB have a low discriminative alue, whereasVVI and VVB have a high one - -  since there arethousands of lexieal verbs in Englisb.
It is ~dsoclear that a tagset of the lowest possible discrimi-native value - -  one which assigned a single tag toeach word and a single word to each tag - -  wouldbc utterly valueless.4.4 L inguis l ic  Qual i lyThis is a very elusive, but crucial concept, tIowfar are tile tags in a particular tagset valuable, bycriteria either of linguistic thet~ry/description, rof usefulness in NLP?
For example, tile tag VDI,mentioned ill C. above, appears trivial, but it can beargued that this is nevertheless a useful categorytbr English grammar, where the verb do (unlike itsequivalent inmost other Europe,'m languages) has avet-y special ftmction, e.g.
in forming questions andnegatives.
On the other band, if we had decided toassign a special tag to the ved~ become, this wouldhave been more questionable.
Linguistic quality is,6The tagset figures exclude punctuation tags and port-manteau Rigs.62.5on the face of it, determined only in a judgementalmanner.
Arguably, in the long term, it can be deter-mined only by the contribution a particular tag dis-tinction makes to Success in particular applications,such as speech recognition or machine-aided trans-lation.
At present, this issue of linguistic quality isthe Achilles' heel of grammatical tagging evalua-tion, and we must note that without judgement onlinguistic quality, evaluation in terms of b. and c. isinsecurely anchored,It seems reasonable, therefore, to lump criteriab.-d. together as 'quality criteria', and to say thatevaluation of tagging accuracy must be undertakenin conjunction with (i) consistency \[How far has thearmotation scheme been consistently applied?\], and(ii) quality of tagging \[How good is the annotationscheme?\].
7 Error rates are useful interim indica-tions of success, but they have to be corroboratedby checking, if only impressionistically, in termsof qualitative criteria.
Our work, since 1980, hasbeen based on the assumption that qualitative crite-ria count, and that it is worth building 'consensual'linguistic knowledge into the datastructures used bythe tagger, to make sure that the tagger's decisionsare fully informed by qualitative considerations.REFERENCESCrowdy, S. (forthcoming).
The BNC Spoken Cor-pus.
In G. Leech, G. Myers and J. Thomas (Ed.
).Spoken English on Computer.
London: Long-mal l .Garside, R., G. Leech ,and G. Szunpson (Eds).(1987).
The ComputationalAnalysis of English:A Corpus-basedApproach.
London: Longman.Johansson, S., E. Atwell, R. Garside ~md G.
Leech.(1986).
The Tagged LOB Corpus: User's Man-ual.
Bergen: Norwegian Computing Centre forthe Humanities.Marshall, I.
(1983).
Choice of grammatical word-class without global syntactic analysis: taggingwords in the LOB Corpus.
Computers and theHumanities, 17, 139-50.VAn example of a consistency issue is: How consis-tently is Time \[the name of a mag~ine\] tagged in thecorpus?
Is it tagged always NP0 (,as a proper noun), oralways NN1 (as a common oun), or sometimes NP0 andsometimes NNI?
An example of a quality issue is: Is itworth distinguishing between proper norms and commonnouns, anyway?Voutilainen, A., J. Heikkil~i and A.
Anttila.(1992).
Constraint Grammar of English: AP etf ormance-O riented lntroduction.
Universityof Helsinki: Department of General Linguistics.626Table 1: Taggcd Spoken D~mt fiom the BNC-  Verticttl Format0000203 070 I 03 PNP0000203 080 can 03 \[VM0/100\] NNI%/00000203 090 just 03 \[AV0/100\] AJ0%/00000203 i00 take 98 WI0000204 010 note 03 \[NNI/99\] VVB/I0000204 020 of 03 PRF0000204 030 any 03 \[DT0/100\] AV0%/00000204 040 other 03 \[AJ0/99\] NNI@/I0000204 050 er 03 UNC0000204 060 persona\].
03 AJ00000204 070 pension 03 \[NNI/100\] VVB@/00000204 071 , 03 ,0000204 080 not 03 XX00000204 090 personal  03 AJ00000204 i00 pens ion 03 \[NNI/100\] WB@/00000204 i01 , 03 ,0000204 ii0 any 03 \[DT0/97\] AV0%/30000204 120 erm 03 UNC0000205 010 other 03 \[AJ0/98\] NNI@/20000205 020 insurance 03 NNI0000205 030 you > 03 PNP0000205 031 're < 03 VIIB0000205 040 got 98 VVN0000205 041 , 03 ,0000205 050 just 03 \[AV0/100\] AJ0%/00000205 060 put 03 \ [WB/66\ ]  VVD@/22 VVN@/130000205 070 it 03 PNP0000205 080 on 03 \[AVP@/62\] PRP/380000205 090 there 03 \[AV0/100\] EX0/00000205 i00 and 97 CJC0000205 i01 , 96 ,0000205 ii0 and 96 CJC0000205 120 that > 97 DT00000205 121 's < 97 \[VBZ/100\] VilZ@/00000206 001 <unclear> 01 NULL0000206 002 </u> 01 NULL0000207 001 * '22;2679;u 01 NULL0000207 002 .........................................................627Table 2: Tagged Spoken Data fi'om tile BNC - -  Horizontal Format<s c:"0000201 002" n:00061><ptr t:Pl3> That&DT0;'s&VBZ; what&DTQ; <ptr t=Pl4> I&PNP; was&VBD;told&VVN; to&T00; bring&VVI; and&CJC; that&DT0;'s&VBZ; what&DTQ;I&PNP; have&VHB; brought&VVN;.&PUN;</U><U id=D0027 who=W0000><S C='0000203 002" n=00062>Yeah&ITJ;,&PUN; I&PNP;'m&VBB;,&PUN; I&PNP;'ve&VHB; got&VVN;another&DT0; form&NNI;,&PUN; I&PNP; can&VM0; just&AV0; take&WI;note&NNl; of&PRF; any&DT0; other&AJ0; er&UNC; personal&AJ0;pension&NNl;,&PUN; not&XX0; personal&AJ0; pension&NNl;,&PUN; any&DT0;e~m&UNC; other&AJ0; insurance&NNl; you&PNP;'ve&VHB; got&VVN;,&PUN;just&AV0; put&VVB; it&PNP; on&AVP-PRP; there&AV0; and&CJC;,&PUN;and&CJC; that&DT0;'s&VBZ; <unclear></u></u>628
