Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115?1126,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Regularized Competition Model for Question Difficulty Estimation inCommunity Question Answering ServicesQuan Wang?Jing Liu?Bin Wang?Li Guo?
?Institute of Information Engineering, Chinese Academy of Sciences, Beijing, P. R. China{wangquan,wangbin,guoli}@iie.ac.cn?Harbin Institute of Technology, Harbin, P. R. Chinajliu@ir.hit.edu.cnAbstractEstimating questions?
difficulty levels isan important task in community questionanswering (CQA) services.
Previous stud-ies propose to solve this problem basedon the question-user comparisons extract-ed from the question answering threads.However, they suffer from data sparsenessproblem as each question only gets a lim-ited number of comparisons.
Moreover,they cannot handle newly posted question-s which get no comparisons.
In this pa-per, we propose a novel question difficul-ty estimation approach called RegularizedCompetition Model (RCM), which natu-rally combines question-user comparisonsand questions?
textual descriptions into aunified framework.
By incorporating tex-tual information, RCM can effectively dealwith data sparseness problem.
We furtheremploy a K-Nearest Neighbor approach toestimate difficulty levels of newly post-ed questions, again by leveraging textu-al similarities.
Experiments on two pub-licly available data sets show that for bothwell-resolved and newly-posted question-s, RCM performs the estimation task sig-nificantly better than existing methods,demonstrating the advantage of incorpo-rating textual information.
More interest-ingly, we observe that RCM might providean automatic way to quantitatively mea-sure the knowledge levels of words.1 IntroductionRecent years have seen rapid growth in communi-ty question answering (CQA) services.
They havebeen widely used in various scenarios, includinggeneral information seeking on the web1, knowl-1http://answers.yahoo.com/edge exchange in professional communities2, andquestion answering in massive open online cours-es (MOOCs)3, to name a few.An important research problem in CQA ishow to automatically estimate the difficulty lev-els of questions, i.e., question difficulty estima-tion (QDE).
QDE can benefit many applications.Examples include 1) Question routing.
Routingquestions to appropriate answerers can help ob-tain quick and high-quality answers (Li and K-ing, 2010; Zhou et al., 2009).
Ackerman andMcDonald (1996) have demonstrated that rout-ing questions by matching question difficulty lev-el with answerer expertise level will make betteruse of answerers?
time and expertise.
This is evenmore important for enterprise question answeringand MOOCs question answering, where humanresources are expensive.
2) Incentive mechanismdesign.
Nam et al.
(2009) have found that win-ning point awards offered by reputation system-s is a driving factor for user participation in C-QA services.
Assigning higher point awards tomore difficult questions will significantly improveuser participation and satisfaction.
3) Linguisticsanalysis.
Researchers in computational linguisticsare always interested in investigating the correla-tion between language and knowledge, to see howthe language reflects one?s knowledge (Church,2011).
As we will show in Section 5.4, QDE pro-vides an automatic way to quantitatively measurethe knowledge levels of words.Liu et al.
(2013) have done the pioneer workon QDE, by leveraging question-user comparison-s extracted from the question answering threads.Specifically, they assumed that the difficulty lev-el of a question is higher than the expertise levelof the asker (i.e.
the user who asked the question),but lower than that of the best answerer (i.e.
the us-er who provided the best answer).
A TrueSkill al-2http://stackoverflow.com/3http://coursera.org/1115gorithm (Herbrich et al., 2006) was further adopt-ed to estimate question difficulty levels as well asuser expertise levels from the pairwise compar-isons among them.
To our knowledge, it is the on-ly existing work on QDE.
Yang et al.
(2008) haveproposed a similar idea, but their work focuses ona different task, i.e., estimating difficulty levels oftasks in crowdsourcing contest services.There are two major drawbacks of previousmethods: 1) data sparseness problem and 2) cold-start problem.
By the former, we mean that un-der the framework of previous work, each questionis compared only twice with the users (once withthe asker and the other with the best answerer),which might not provide enough information andcontaminate the estimation accuracy.
By the latter,we mean that previous work only deals with well-resolved questions which have received the bestanswers, but cannot handle newly posted question-s with no answers received.
In many real-worldapplications such as question routing and incentivemechanism design, however, it is usually requiredthat the difficulty level of a question is known in-stantly after it is posted.To address the drawbacks, we propose furtherexploiting questions?
textual descriptions (e.g., ti-tle, body, and tags) to perform QDE.
Preliminaryobservations have shown that a question?s difficul-ty level can be indicated by its textual descrip-tion (Liu et al., 2013).
We take advantage of theobservations, and assume that if two questions areclose in their textual descriptions, they will alsobe close in their difficulty levels, i.e., the smooth-ness assumption.
We employ manifold regular-ization (Belkin et al., 2006) to characterize theassumption.
Manifold regularization is a well-known technique to preserve local invariance inmanifold learning algorithms, i.e., nearby pointsare likely to have similar embeddings (Belkin andNiyogi, 2001).
Then, we propose a novel Reg-ularized Competition Model (RCM), which for-malizes QDE as minimizing a loss on question-user comparisons with manifold regularization onquestions?
textual descriptions.
As the smoothnessassumption offers extra information for inferringquestion difficulty levels, incorporating it will ef-fectively deal with data sparsity.
Finally, we adopta K-Nearest Neighbor approach (Cover and Hart,1967) to perform cold-start estimation, again byleveraging the smoothness assumption.Experiments on two publicly available data setscollected from Stack Overflow show that 1) RCMperforms significantly better than existing meth-ods in the QDE task for both well-resolved andcold-start questions.
2) The performance of RCMis insensitive to the particular choice of the termweighting schema (determines how a question?stextual description is represented) and the similar-ity measure (determines how the textual similaritybetween two questions is measured).
The resultsdemonstrate the advantage of incorporating textu-al information for QDE.
Qualitative analysis fur-ther reveals that RCM might provide an automaticway to quantitatively measure the knowledge lev-els of words.The main contributions of this paper include: 1)We take fully advantage of questions?
textual de-scriptions to address data sparseness problem andcold-start problem which previous QDE methodssuffer from.
To our knowledge, it is the first timethat textual information is introduced in QDE.
2)We propose a novel QDE method that natural-ly combines question-user comparisons and ques-tions?
textual descriptions into a unified frame-work.
The proposed method performs QDE sig-nificantly better than existing methods.
3) Wedemonstrate the practicability of estimating diffi-culty levels of cold-start questions purely based ontheir textual descriptions, making various applica-tions feasible in practice.
As far as we know, it isthe first work that considers cold-start estimation.4) We explore how a word?s knowledge level canbe automatically measured by RCM.The rest of the paper is structured as follows.Section 2 describes the problem formulation andthe motivation of RCM.
Section 3 presents the de-tails of RCM.
Section 4 discusses cold-start esti-mation.
Section 5 reports experiments and results.Section 6 reviews related work.
Section 7 con-cludes the paper and discusses future work.2 Preliminaries2.1 Problem FormulationA CQA service provides a platform where peoplecan ask questions and seek answers from others.Given a CQA portal, consider a specific catego-ry where questions on the same topic are askedand answered, e.g., the ?C++ programming?
cat-egory of Stack Overflow.
When an asker uapostsa question q in the category, there will be sever-al answerers to answer the question.
Among allthe received answers, a best one will be chosen1116by the asker or voted by the community.
The an-swerer who provides the best answer is called thebest answerer ub.
The other answerers are denotedby O ={uo1, uo2, ?
?
?
, uoM}.
A question answeringthread (QA thread) is represented as a quadruplet(q, ua, ub,O).
Collecting all such QA threads in thecategory, we get M users and N questions, denotedbyU = {u1, u2, ?
?
?
, uM} and Q = {q1, q2, ?
?
?
, qN}respectively.
Each user umis associated with anexpertise score ?m, representing his/her expertiselevel.
A larger ?mindicates a higher expertise lev-el of the user.
Each question qnis associated witha difficulty score ?n, representing its difficulty lev-el.
A larger ?nindicates a higher difficulty levelof the question.
Difficulty scores (as well as ex-pertise scores) are assumed to be comparable witheach other in the specified category.
Besides, eachquestion qnhas a textual description, and is repre-sented as a V-dimensional term vector dn, whereV is the vocabulary size.The question difficulty estimation (QDE) taskaims to automatically learn the question difficul-ty scores (?n?s) by utilizing the QA threads T ={(q, ua, ub,O): q ?
Q} as well as the question de-scriptions D = {d1, d2, ?
?
?
, dN} in the specifiedcategory.
Note that in Section 2 and Section 3, weconsider estimating difficulty scores of resolvedquestions, i.e., questions with the best answers se-lected or voted.
Estimating difficulty scores of un-resolved questions, e.g., newly posted ones, willbe discussed in Section 4.2.2 Competition-based MethodsLiu et al.
(2013) have proposed a competition-based method for QDE.
The key idea is to 1) ex-tract pairwise competitions from the QA threadsand 2) estimate question difficulty scores based onextracted competitions.To extract pairwise competitions, it is assumedthat question difficulty scores and user expertisescores are expressed on the same scale.
Given aQA thread(q, ua, ub,O), it is further assumed that:Assumption 1 (pairwise comparison assumption)The difficulty score of question q is higher than theexpertise score of the asker ua, but lower than thatof the best answerer ub.
Moreover, the expertisescore of the best answerer ubis higher than thatof the asker ua, as well as any answerer in O.44The difficulty score of question q is not assumed to belower than the expertise score of any answerer in O, sincesuch a user may just happen to see the question and respondto it, rather than knowing the answer well.Given the assumption, there are(|O| + 3)pairwisecompetitions extracted from the QA thread, in-cluding 1) one competition between the questionq and the asker ua, 2) one competition betweenthe question q and the best answerer ub, 3) onecompetition between the best answerer uband theasker ua, and 4) |O| competitions between the bestanswerer uband each of the answerers in O. Thequestion q is the winner of the first competition,and the best answerer ubis the winner of the re-maining(|O| + 2)competitions.
These pairwisecompetitions are denoted byCq={ua?q, q?ub, ua?ub, uo1?ub, ?
?
?
, uoM?ub},where i ?
j means that competitor j beats com-petitor i in a competition.
LetC =?q?QCq(1)be the set containing all the pairwise competitionsextracted from T .Given the competition set C, Liu et al.
(2013)further adopted a TrueSkill algorithm (Herbrichet al., 2006) to learn the competitors?
skill level-s (i.e.
the question difficulty scores and the us-er expertise scores).
TrueSkill assumes that thepractical skill level of each competitor follows anormal distribution N(?, ?2), where ?
is the aver-age skill level and ?
is the estimation uncertain-ty.
Then it updates the estimations in an onlinemode: for a newly observed competition with itswin-loss result, 1) increase the average skill levelof the winner, 2) decrease the average skill levelof the loser, and 3) shrink the uncertainties of bothcompetitors as more data has been observed.
Yanget al.
(2008) have proposed a similar competition-based method to estimate tasks?
difficulty levelsin crowdsourcing contest services, by leveragingPageRank (Page et al., 1999) algorithm.2.3 Motivating DiscussionsThe methods introduced above estimate competi-tors?
skill levels based solely on the pairwise com-petitions among them.
The more competitions acompetitor participates in, the more accurate theestimation will be.
However, according to thepairwise comparison assumption (Assumption 1),each question participates in only two competi-tions, one with the asker and the other with thebest answerer.
Hence, there might be no enoughinformation to accurately infer its difficulty score.We call this the data sparseness problem.1117(a) Low difficulty.
(b) Medium difficulty.
(c) High difficulty.Figure 1: Tag clouds of SO/Math questions with different difficulty levels.Taking advantage of additional metadata hasbeen demonstrated to be an effective way of deal-ing with data sparsity in various applications suchas collaborative filtering (Claypool et al., 1999;Schein et al., 2002) and personalized search (Douet al., 2007; Sugiyama et al., 2004).
The ratio-nale behind is to bridge the gap among users/itemsby leveraging their similarities based on the meta-data.
As for QDE, preliminary observations haveshown that a question?s difficulty level can be in-dicated by its textual description (Liu et al., 2013).As an example, consider the QA threads in the?mathematics?
category of Stack Overflow.
Di-vide the questions into three groups: 1) low dif-ficulty, 2) medium difficulty, and 3) high difficul-ty, according to their difficulty scores estimated byTrueSkill.
Figure 1 visualizes the frequency dis-tribution of tags in each group, where the size ofeach tag is in proportion to its frequency in thegroup.
The results indicate that the tags associ-ated with the questions do have the ability to re-flect the questions?
difficulty levels, e.g., low dif-ficulty questions usually have tags such as ?home-work?
and ?calculus?, while high difficulty onesusually have tags such as ?general topology?
and?number theory?.
We further calculate the Pearsoncorrelation coefficient (Rodgers and Nicewander,1988) between 1) the gap between the averageddifficulty scores in each two groups and 2) theEuclidean distance between the aggregated textu-al descriptions in each two groups .
The result isr = 0.6424, implying that the difficulty gap is posi-tively correlated with the textual distance.
In otherwords, the more similar two questions?
textual de-scriptions are, the more close their difficulty levelsare.
Therefore, we take the textual information tobridge the difficulty gap among questions, by as-suming thatAssumption 2 (smoothness assumption) If twoquestions qiand qjare close in their textual de-scriptions diand dj, they will also be close in theirdifficulty scores ?iand ?j.The smoothness assumption brings us additionalinformation about question difficulty scores by in-ferring textual similarities.
It serves as a supple-ment to the pairwise competitions, and might helpaddress the data sparseness problem which previ-ous methods suffer from.3 Modeling Text Similarities for QDEThis section presents a novel Regularized Compe-tition Model (RCM) for QDE, which combines thepairwise competitions and the textual descriptionsinto a unified framework.
RCM can alleviate thedata sparseness problem and perform more accu-rate estimation.3.1 Regularized Competition ModelWe start with several notations.
As question dif-ficulty scores can be directly compared with userexpertise scores, we take questions as pseudo user-s.
Let??
?
RM+Ndenote the skill levels (i.e.
theexpertise scores and the difficulty scores) of all the(pseudo) users:?
?i={?i, 1 ?
i ?
M,?i?M, M < i ?
M + N,where?
?iis the i-th entry of??.
The first M entriesare the user expertise scores, denoted by??u?
RM.The last N entries are the question difficulty s-cores, denoted by??q?
RN.
Let??(u)iand??
(q)idenotethe i-th entries of??uand?
?qrespectively.Exploiting Pairwise Competitions.
We definea loss on each pairwise competition i ?
j:?(??i,?
?j)= max(0, ?
?(??j??
?i))p, (2)where p is either 1 or 2.
The loss is defined on theskill gap between the two competitors, i.e.,??j??
?i,1118measuring the inconsistency between the expect-ed outcome and the actual outcome.
If the gap islarger than a predefined threshold ?, competitor jwould probably beat competitor i in the compe-tition, which coincides with the actual outcome.Then the loss will be zero.
Otherwise, there is ahigher chance that competitor j loses the competi-tion, which goes against the actual outcome.
Thenthe loss will be greater than zero.
The smaller thegap is, the higher the chance of inconsistency be-comes, and the greater the loss will be.
Note thatthe threshold ?
can take any positive value sincewe do not pose a norm constraint on?
?.5Withoutloss of generality we take ?
= 1 throughout thispaper.
As we will show in Section 3.2, the loss de-fined in Eq.
(2) has some similarity with the SVMloss (Chapelle, 2007).
We name it hinge loss whenp = 1, and quadratic loss when p = 2.Given the competition set C, estimating skil-l levels of (pseudo) users then amounts to solvingthe following optimization problem:min???(i?
j)?C?(??i,??j)+?12??T?
?, (3)where the first term is the empirical loss measur-ing the total inconsistency; the second term is aregularizer to prevent overfitting; and ?1?
0 is atrade-off coefficient.
It is also a competition-basedQDE method, called Competition Model (CM).Exploiting Question Descriptions.
Manifoldregularization is a well-known technique used inmanifold learning algorithms to preserve local in-variance, i.e., nearby points are likely to have sim-ilar embeddings (Belkin and Niyogi, 2001).
InQDE, the smoothness assumption expresses sim-ilar ?invariance?, i.e., nearby questions (in termsof textual similarities) are likely to have similardifficulty scores.
Hence, we characterize the as-sumption with the following manifold regularizer:R =12N?i=1N?j=1(??(q)i???
(q)j)2wi j=??TqD??q???TqW??q=??TqL?
?q, (4)where wi jis the textual similarity between ques-tion i and question j; W ?
RN?Nis the similaritymatrix with the (i, j)-th entry being wi j; D ?
RN?Nis a diagonal matrix with the i-th entry on the diag-onal being dii=?Nj=1wi j; and L = D?W ?
RN?N5Given any??i,?
?j, and ?, there always exists a linear trans-formation which keeps the sign of(?
?(??j??
?i))unchanged.is the graph Laplacian (Chung, 1997).
MinimizingR results in the smoothness assumption: for anyquestions i and j, if their textual similarity wi jishigh, the difficulty gap(??(q)i???
(q)j)2will be small.A Hybrid Method.
Combining Eq.
(3) andEq.
(4), we obtain RCM, which amounts to thefollowing optimization problem:min???(i?
j)?C?(??i,??j)+?12??T??
+?22??TqL??q.
(5)Here ?2?
0 is also a trade-off coefficient.
Theadvantages of RCM include 1) It naturally formal-izes QDE as minimizing a manifold regularizedloss function, which seamlessly integrates both thepairwise competitions and the textual description-s. 2) By incorporating textual information, it canaddress the data sparseness problem which previ-ous methods suffer from, and perform significantlybetter in the QDE task.3.2 Learning AlgorithmRedefine the k-th pairwise competition (assumedto be carried out between competitors i and j) as(xk, yk).
xk?
RM+Nindicates the competitors:x(k)i= 1, x(k)j= ?1, and x(k)l= 0 for any l , i, j,where x(k)lis the l-th entry of xk.
yk?
{1,?1} isthe outcome: if competitor i beats competitor j,yk= 1; otherwise, yk= ?1.
The objective in Eq.
(5) can then be rewritten asL(??
)=|C|?k=1max(0, 1 ?
yk(??Txk))p+12??TZ?
?,where Z =(?1IM00 ?1IN+ ?2L)is a block matrix; IM?RM?Mand IN?
RN?Nare identity matrices; p =1 corresponds to the hinge loss, and p = 2 thequadratic loss.
It is clear that the loss defined inEq.
(2) has the same format as the SVM loss.The objectiveL is differentiable for the quadrat-ic loss but non-differentiable for the hinge loss.We employ a subgradient method (Boyd et al.,2003) to solve the optimization problem.
The al-gorithm starts at a point?
?0and, as many iterationsas needed, moves from??tto?
?t+1in the directionof the negative subgradient:??t+1=??t?
?t?L(?
?t),1119Algorithm 1 Regularized Competition ModelRequire: competition set C and description setD1:??0?
12: for t = 0 : T ?
1 do3: Kt?
{k : 1 ?
yk(?
?Ttxk)> 0}4: ?L(??t)?
calculated by Eq.
(6)5:??t+1???t?
?t?L(?
?t)6: ?t+1?{??0,?
?1, ?
?
?
,??t+1}7:??t+1?
argmin????t+1L(??
)8: end for9: return?
?Twhere ?t> 0 is the learning rate.
The subgradientis calculated as?L(??t)=?????????Z??t?
?k?Ktykxk, p=1,Z?
?t+ 2?k?KtxkxTk??t?
2?k?Ktykxk, p=2,(6)where Kt={k : 1 ?
yk(?
?Ttxk)> 0}.
As it is notalways a descent method, we keep track of the bestpoint found so far (Boyd et al., 2003):?
?t+1= arg min????t+1L(??),where?t+1={??0,?
?1, ?
?
?
,??t+1}.
The whole proce-dure is summarized in Algorithm 1.Convergence.
For constant learning rate (i.e.,?t= ?
), Algorithm 1 is guaranteed to converge towithin some range of the optimal value, i.e.,limt??L(??t)?
L?< ?,where L?denotes the minimum of L(?
), and ?
is aconstant defined by the learning rate ?.
For moredetails, please refer to (Boyd et al., 2003).
Duringour experiments, we set the iteration number asT = 1000 and the learning rate as ?t= 0.001, andconvergence was observed.Complexity.
For both the hinge loss and thequadratic loss, the time complexity (per itera-tion) and the space complexity of RCM are bothO(|C| + ?N2).
Here, |C| is the total number ofcompetitions, M and N are the numbers of user-s and questions respectively, and ?
is the ratio ofnon-zero entries in the graph Laplacian L.6In theanalysis, we have assumed that M ?
?N2andN ?
?N2.6Owing to the sparse nature of questions?
textual descrip-tions, the graph Laplacian L is usually sparse, with about70% entries being zero according to our experiments.4 Cold-Start EstimationPrevious sections discussed estimating difficulty s-cores of resolved questions, from which pairwisecompetitions could be extracted.
However, fornewly posted questions without any answers re-ceived, no competitions could be extracted andnone of the above methods work.
We call it thecold-start problem.We heuristically apply a K-Nearest Neighbor(KNN) approach (Cover and Hart, 1967) to cold-start estimation, again by leveraging the smooth-ness assumption.
The key idea is to propagatedifficulty scores from well-resolved questions tocold-start ones according to their textual simi-larities.
Specifically, suppose that there existsa set of well-resolved questions whose difficul-ty scores have already been estimated by a QDEmethod.
Given a cold-start question q?, we firstpick K well-resolved questions that are closest toq?in textual descriptions, referred to as the near-est neighbors.
The difficulty score of question q?is then predicted as the averaged difficulty scoresof its nearest neighbors.
The KNN method bridgesthe gap between cold-start and well-resolved ques-tions by inferring their textual similarities, andmight effectively deal with the cold-start problem.5 ExperimentsWe have conducted experiments to test the effec-tiveness of RCM in estimating difficulty scores ofboth well-resolved and cold-start questions.
More-over, we have explored how a word?s difficulty lev-el can be quantitatively measured by RCM.5.1 Experimental SettingsData Sets.
We obtained a publicly available da-ta set of Stack Overflow between July 31, 2008and August 1, 20127, containing QA threads invarious categories.
We considered the categoriesof ?C++ programming?
and ?mathematics?, andrandomly sampled about 10,000 QA threads fromeach category, denoted by SO/CPP and SO/Mathrespectively.
For each question, we took the titleand body fields as its textual description.
For bothdata sets, stop words in a standard list8and wordswhose total frequencies are less than 10 were re-moved.
Table 1 gives the statistics of the data sets.7http://blog.stackoverflow.com/category/cc-wiki-dump/8http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop1120# users # questions # competitions # wordsSO/CPP 14,884 10,164 50,043 2,208SO/Math 6,564 10,528 40,396 2,009Table 1: Statistics of the data sets.For evaluation, we randomly sampled 600 ques-tion pairs from each data set, and asked annotatorsto compare the difficulty levels of the questionsin each pair.
We had two graduate students ma-joring in computer science annotate the SO/CPPquestions, and two majoring in mathematics an-notate the SO/Math questions.
For each question,only the title, body, and tags were exposed to theannotators.
Given a question pair(q1, q2), the an-notators were asked to give one of the three labels:q1?
q2, q2?
q1, or q1= q2, which respective-ly means that question q1has a higher, lower, orequal difficulty level compared with question q2.We used Cohen?s kappa coefficient (Cohen, 1960)to measure the inter-annotator agreement.
The re-sult is ?
= 0.7533 on SO/CPP and ?
= 0.8017on SO/Math, indicating that the inter-annotator a-greement is quite substantial on both data sets.
Af-ter removing the question pairs with inconsisten-t labels, we got 521 annotated SO/CPP questionpairs and 539 annotated SO/Math question pairs.We further randomly split the annotated ques-tion pairs into development/test/cold-start sets,with the ratio of 2:2:1.
The first two sets were usedto evaluate the methods in estimating difficulty s-cores of resolved questions.
Specifically, the de-velopment set was used for parameter tuning andthe test set was used for evaluation.
The last setwas used to evaluate the methods in cold-start esti-mation, and the questions in this set were excludedfrom the learning process of RCM as well as anybaseline method.Baseline Methods.
We considered three base-line methods: PageRank (PR), TrueSkill (TS), andCM, which are based solely on the pairwise com-petitions.?
PR first constructs a competitor graph, bycreating an edge from competitor i to com-petitor j if j beats i in a competition.
APageRank algorithm (Page et al., 1999) isthen utilized to estimate the relative impor-tance of the nodes, i.e., question difficulty s-cores and user expertise scores.
The dampingfactor was set from 0.1 to 0.9 in steps of 0.1.?
TS has been applied to QDE by Liu et al.(2013).
We set the model parameters in thesame way as they suggested.?
CM performs QDE by solving Eq.
(3).
Weset ?1in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.We compared RCM with the above baseline meth-ods.
In RCM, both parameters ?1and ?2were setin {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.Evaluation Metric.
We employed accuracy(ACC) as the evaluation metric:ACC =# correctly judged question pairs# all question pairs.A question pair is regarded as correctly judged ifthe relative difficulty ranking given by an estima-tion method is consistent with that given by theannotators.
The higher the accuracy is, the bettera method performs.5.2 Estimation for Resolved QuestionsThe first experiment tested the methods in estimat-ing difficulty scores of resolved questions.Estimation Accuracies.
We first compared theestimation accuracies of PR, TS, CM, and RCMon the test sets of SO/CPP and SO/Math, obtainedwith the best parameter settings determined by thedevelopment sets.
Table 2 gives the results, where?H?
denotes the hinge loss and ?Q?
the quadraticloss.
In RCM, to calculate the graph Laplacian L,we adopted Boolean term weighting schema andtook Jaccard coefficient as the similarity measure.From the results, we can see that 1) RCM perform-s significantly better than the baseline methods onboth data sets (t-test, p-value < 0.05), demonstrat-ing the advantage of exploiting questions?
textu-al descriptions for QDE.
2) The improvements ofRCM over the baseline methods on SO/Math aregreater than those on SO/CPP, indicating that thetextual descriptions of the SO/Math questions aremore powerful in reflecting their difficulty level-s.
The reason is that the SO/Math questions aremuch more heterogeneous, belonging to varioussubfields of mathematics.
The difficulty gaps a-mong different subfields are sometimes obvious(e.g., a question in topology in general has a high-er difficulty level than a question in linear algebra),making the textual descriptions more powerful indistinguishing the difficulty levels.Graph Laplacian Variants.
We further inves-tigated the performances of different term weight-ing schemas and similarity measures in the graph1121PR TSCM RCMH Q H QSO/CPP 0.5876 0.6134 0.6340 0.6753 0.7371 0.7268SO/Math 0.6067 0.6109 0.6527 0.6820 0.7699 0.7699Table 2: ACC of different methods for well-resolved questions.Notation DefinitionBoolean v(w, q) =??????
?1, if word w occurs in question q0, otherwiseTF-1 v(w, q) = f (w, q), the number of occurrencesTF-2 v(w, q) = log ( f (w, q) + 1)TF-3 v(w, q) = 0.5 +0.5 ?
f (w, q)max { f (w, q) : w ?
q}TFIDF-1 v(w, q) = TF-1 ?
log|Q||{q?Q:w?q}|TFIDF-2 v(w, q) = TF-2 ?
log|Q||{q?Q:w?q}|TFIDF-3 v(w, q) = TF-3 ?
log|Q||{q?Q:w?q}|Cosine Sim (d1, d2) =dT1d2?d1???d2??
[0, 1]Jaccard Sim(d1, d2)=dT1d2?d1?2+?d2?2??d1???d2??
[0, 1]Table 3: Different term weighting schemas andsimilarity measures.Laplacian.
The term weighting schema deter-mines how a question?s textual description is rep-resented.
We explored a Boolean schema, threeTF schemas, and three TFIDF schemas (Saltonand Buckley, 1988).
The similarity measure de-termines how the textual similarity between twoquestions is calculated.
We explored the Co-sine similarity and the Jaccard coefficient (Huang,2008).
Detailed descriptions are given in Table 3.Figure 2 and Figure 3 show the estimation ac-curacies of the RCM variants on the test sets ofSO/CPP and SO/Math respectively, again obtainedwith the best parameter settings determined bythe development sets.
The performance of CMis also given (the straight lines in the figures).9From the results, we can see that 1) All the RCMvariants can improve over CM on both data sets,and most of the improvements are significant (t-test, p-value < 0.05).
This further demonstratesthat the effectiveness of incorporating textual de-scriptions is not affected by the particular choiceof the term weighting schema or similarity mea-sure.
2) Boolean term weighting schema performsthe best, considering different similarity measures,loss types, and data sets collectively.
3) Jaccard9CM performs better than PR and TS on both data sets.0.550.60.650.70.75 Cosine Jaccard CM(H)(a) Hinge loss.0.550.60.650.70.75 Cosine Jaccard CM(Q)(b) Quadratic loss.Figure 2: ACC of RCM variants for well-resolvedquestions on SO/CPP.0.60.650.70.750.8 Cosine Jaccard CM(H)(a) Hinge loss.0.60.650.70.750.8 Cosine Jaccard CM(Q)(b) Quadratic loss.Figure 3: ACC of RCM variants for well-resolvedquestions on SO/Math.coefficient performs as well as Cosine similari-ty on SO/Math, but almost consistently better onSO/CPP.
Throughout the experiments, we adoptedBoolean term weighting schema and Jaccard coef-ficient to calculate the graph Laplacian.5.3 Estimation for Cold-Start QuestionsThe second experiment tested the methods in es-timating difficulty scores of cold-start questions.We employed Boolean term weighting schema torepresent a cold-start question, and utilized Jac-card Coefficient to select its nearest neighbors.Figure 4 and Figure 5 list the cold-start estima-tion accuracies of different methods on SO/CPPand SO/Math respectively, with different K val-ues (the number of nearest neighbors).
As theaccuracy oscillates drastically with a K value s-maller than 11 on SO/CPP and smaller than 6 onSO/Math, we report the results with K ?
[11, 20]on SO/CPP and K ?
[6, 15] on SO/Math.
The av-eraged (over different K values) cold-start estima-tion accuracies are further given in Table 4.
All theresults are reported on the cold-start sets, with theoptimal parameter settings adopted in Section 5.2.From the results, we can see that 1) Cold-start es-timation is possible, and can achieve a consider-ably high accuracy by choosing a proper method(e.g.
RCM), making applications such as betterquestion routing and better incentive mechanism11220.450.550.650.7511 12 13 14 15 16 17 18 19 20AccuracyKPR TS CM(H)CM(Q) RCM(H) RCM(Q)Figure 4: ACC of different methods for cold-startquestions on SO/CPP.0.50.60.70.86 7 8 9 10 11 12 13 14 15AccuracyKPR TS CM(H)CM(Q) RCM(H) RCM(Q)Figure 5: ACC of different methods for cold-startquestions on SO/Math.design feasible in practice.
2) As the value of Kvaries, RCM (the red/blue solid line) performs al-most consistently better than CM with the sameloss type (the red/blue dotted line), as well as PRand TS (the gray dotted lines), showing the advan-tages of RCM in the cold-start estimation.
3) Thecold-start estimation accuracies on SO/Math arehigher than those on SO/CPP, again demonstratingthat the textual descriptions of the SO/Math ques-tions are more powerful in reflecting their difficul-ty levels.
This is consistent with the phenomenonobserved in Section 5.2.5.4 Difficulty Levels of WordsThe third experiment explored how a word?s diffi-culty level can be measured by RCM automatical-ly and quantitatively.On both SO/CPP and SO/Math, we evenly splitthe range of question difficulty scores (estimatedby RCM) into 10 buckets, and assigned questionsto the buckets according to their difficulty scores.A larger bucket ID indicates a higher difficulty lev-el.
Then, given a word w, we calculated its fre-quency in each bucket as follows:fi(w) =# questions in bucket i where w occurs# all questions in bucket i.To make the frequency meaningful, buckets withless than 50 questions were discarded.
We pickedPR TSCM RCMH Q H QSO/CPP 0.5870 0.5413 0.6120 0.6304 0.6380 0.6609SO/Math 0.6411 0.6305 0.6653 0.7263 0.6958 0.7442Table 4: Averaged ACC of different methods forcold-start questions.00.40.81.23 3.5 4 4.5 5 5.5 6 6.5 7OccurrencefrequencyQuestion bucketsarray string virtual multithreadFigure 6: Frequencies of different words in thebuckets on SO/CPP.four words from each data set as examples.
Theirnormalized frequencies in different buckets areshown in Figure 6 and Figure 7.
On SO/CPP,we can observe that ?array?
and ?string?
occurmost frequently in questions with lower difficul-ty levels, ?virtual?
higher, and ?multithread?
thehighest.
It coincides with the intuition: ?array?and ?string?
are usually related to some basic con-cepts in programming language, while ?virtual?and ?multithread?
usually discuss more advancedtopics.
Similar phenomena can be observed onSO/Math.
The results indicate that RCM mightprovide an automatic way to measure the difficul-ty levels of words.6 Related WorkQDE is relevant to the problem of estimating taskdifficulty levels and user expertise levels in crowd-sourcing services (Yang et al., 2008; Whitehill etal., 2009).
Studies on this problem fall into twocategories: 1) binary response based and 2) par-tially ordered response based.
In the first cate-gory, binary responses (i.e.
whether the solutionprovided by a user is correct or not) are observed,and techniques based on item response theory arefurther employed (Whitehill et al., 2009; Welin-der et al., 2010; Zhou et al., 2012).
In the secondcategory, partially ordered responses (i.e.
whichof the two given solutions is better) are observed,and pairwise comparison based methods are fur-ther adopted (Yang et al., 2008; Liu et al., 2013).QDE belongs to the latter.112300.40.81.24 5 6 7 8 9OccurrencefrequencyQuestion bucketshomework calculus ring topologyFigure 7: Frequencies of different words in thebuckets on SO/Math.The most relevant work to ours is a pairwisecomparison based approach proposed by Liu et al.
(2013) to estimate question difficulty levels in C-QA services.
They have also demonstrated thata similar approach can be utilized to estimate us-er expertise levels (Liu et al., 2011).
Yang et al.
(2008) and Chen et al.
(2013) have also proposedpairwise comparison based methods, for task dif-ficulty estimation and rank aggregation in crowd-sourcing settings.
Our work differs from previouspairwise comparison based methods in that it fur-ther utilizes textual information, formalized as amanifold regularizer.Manifold regularization is a geometrically mo-tivated framework for machine learning, enforcingthe learning model to be smooth w.r.t.
the geomet-rical structure of data (Belkin et al., 2006).
Withinthe framework, dimensionality reduction (Belkinand Niyogi, 2001; Cai et al., 2008) and semi-supervised learning (Zhou et al., 2004; Zhu andLafferty, 2005) algorithms have been constructed.In dimensionality reduction, manifold regulariza-tion is utilized to guarantee that nearby points willhave similar low-dimensional representations (Caiet al., 2008), while in semi-supervised learning itis utilized to ensure that nearby points will havesimilar labels (Zhou et al., 2004).
In our work, weassume that nearby questions (in terms of textualsimilarities) will have similar difficulty levels.Predicting reading difficulty levels of text isalso a relevant problem (Collins-Thompson andCallan, 2004; Schwarm and Ostendorf, 2005).
Itis a key to automatically finding materials at ap-preciate reading levels for students, and also helpsin personalized web search (Collins-Thompson etal., 2011).
In the task of predicting reading dif-ficulty levels, documents targeting different gradelevels are taken as ground truth, which can be eas-ily obtained from the web.
However, there is nonaturally annotated data for our QDE task on theweb.
Other related problems include query dif-ficulty estimation for search engines (Carmel etal., 2006; Yom-Tov et al., 2005) and question dif-ficulty estimation for automatic question answer-ing systems (Lange et al., 2004).
In these tasks,query/question difficulty is system-oriented and ir-relevant with human knowledge, which is a differ-ent setting from ours.7 Conclusion and Future WorkIn this paper, we have proposed a novel method forestimating question difficulty levels in CQA ser-vices, called Regularized Competition Model (R-CM).
It takes fully advantage of questions?
textu-al descriptions besides question-user comparisons,and thus can effectively deal with data sparsity andperform more accurate estimation.
A K-NearestNeighbor approach is further adopted to estimatedifficulty levels of cold-start questions.
Experi-ments on two publicly available data sets showthat RCM performs significantly better than exist-ing methods in the estimation task, for both well-resolved and cold-start questions, demonstratingthe advantage of incorporating textual informa-tion.
It is also observed that RCM might automat-ically measure the knowledge levels of words.As future work, we plan to 1) Enhance the ef-ficiency and scalability of RCM.
The complexityanalysis in Section 3.2 indicates that storing andprocessing the graph Laplacian is a bottleneck ofRCM.
We would like to investigate how to dealwith the bottleneck, e.g., via parallel or distribut-ed computing.
2) Apply RCM to non-technicaldomains.
For non-technical domains such as the?news and events?
category of Yahoo!
Answer-s, there might be no strongly distinct notions of?experts?
and ?non-experts?, and it might be moredifficult to distinguish between ?hard questions?and ?easy questions?.
It is worthy investigatingwhether RCM still works on such domains.AcknowledgmentsWe would like to thank the anonymous review-ers for their helpful comments.
This work issupported by the Strategic Priority Research Pro-gram of the Chinese Academy of Sciences (grantNo.
XDA06030200), the National Key Technolo-gy R&D Program (grant No.
2012BAH46B03),and the National Natural Science Foundation ofChina (grant No.
61272427).1124ReferencesMark S. Ackerman and David W. McDonald.
1996.Answer garden 2: merging organizational memorywith collaborative help.
In Proceedings of the 1996ACM Conference on Computer Supported Coopera-tive Work, pages 97?105.Mikhail Belkin and Partha Niyogi.
2001.
Laplacianeigenmaps and spectral techniques for embeddingand clustering.
In Advances in Neural InformationProcessing Systems, pages 585?591.Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: a geometric frame-work for learning from labeled and unlabeled ex-amples.
Journal of Machine Learning Research,7:2399?2434.Stephen Boyd, Lin Xiao, and Almir Mutapcic.
2003.Subgradient methods.
Lecture Notes of EE392o, S-tanford University.Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han.2008.
Non-negative matrix factorization on mani-fold.
In Proceedings of the 8th IEEE InternationalConference on Data Mining, pages 63?72.David Carmel, Elad Yom-Tov, Adam Darlow, and DanPelleg.
2006.
What makes a query difficult?
InProceedings of the 29th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 390?397.Olivier Chapelle.
2007.
Training a support vec-tor machine in the primal.
Neural Computation,19(5):1155?1178.Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson,and Eric Horvitz.
2013.
Pairwise ranking aggrega-tion in a crowdsourced setting.
In Proceedings of the6th ACM International Conference on Web Searchand Data Mining, pages 193?202.Fan RK.
Chung.
1997.
Spectral Graph Theory, vol-ume 92.Kenneth Church.
2011.
How many multiword expres-sions do people know.
In Proceedings of the ACL-HLT Workshop on Multiword Expressions: fromParsing and Generation to the Real World, pages137?144.Mark Claypool, Anuja Gokhale, Tim Miranda, PavelMurnikov, Dmitry Netes, andMatthew Sartin.
1999.Combining content-based and collaborative filters inan online newspaper.
In Proceedings of the ACMSIGIR workshop on Recommender Systems.Jacob Cohen.
1960.
A coefficient of agreemen-t for nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Kevyn Collins-Thompson and James P. Callan.
2004.A language modeling approach to predicting readingdifficulty.
In Proceedings of the 2004 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 193?200.Kevyn Collins-Thompson, Paul N Bennett, Ryen WWhite, Sebastian de la Chica, and David Sontag.2011.
Personalizing web search results by readinglevel.
In Proceedings of the 20th ACM Internation-al Conference on Information and Knowledge Man-agement, pages 403?412.Thomas Cover and Peter Hart.
1967.
Nearest neighborpattern classification.
IEEE Transactions on Infor-mation Theory, 13(1):21?27.Zhicheng Dou, Ruihua Song, and Ji Rong Wen.
2007.A large-scale evaluation and analysis of personal-ized search strategies.
In Proceedings of the 16thInternational Conference on World Wide Web, pages581?590.Ralf Herbrich, Tom Minka, and Thore Graepel.
2006.Trueskill: a bayesian skill rating system.
In Ad-vances in Neural Information Processing Systems,pages 569?576.Anna Huang.
2008.
Similarity measures for text doc-ument clustering.
In Proceedings of the 6th NewZealand Computer Science Research Student Con-ference, pages 49?56.Rense Lange, Juan Moran, Warren R. Greiff, and LisaFerro.
2004.
A probabilistic rasch analysis of ques-tion answering evaluations.
In Proceedings of the2004 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 65?72.Baichuan Li and Irwin King.
2010.
Routing ques-tions to appropriate answerers in community ques-tion answering services.
In Proceedings of the 19thACM International Conference on Information andKnowledge Management, pages 1585?1588.Jing Liu, Young-In Song, and Chin-Yew Lin.
2011.Competition-based user expertise score estimation.In Proceedings of the 34th International ACM SI-GIR Conference on Research and Development inInformation Retrieval, pages 425?434.Jing Liu, Quan Wang, Chin-Yew Lin, and Hsiao-WuenHon.
2013.
Question difficulty estimation in com-munity question answering services.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 85?90.Kevin Kyung Nam, Mark S. Ackerman, and Lada A.Adamic.
2009.
Questions in, knowledge in?
: a s-tudy of naver?s question answering community.
InProceedings of the SIGCHI Conference on HumanFactors in Computing Systems, pages 779?788.Larry Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The pagerank citation ranking:bringing order to the web.
Technical Report, Stan-ford University.1125Joseph Lee Rodgers and W. Alan Nicewander.
1988.Thirteen ways to look at the correlation coefficient.The American Statistician, 42(1):59?66.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation Processing & Management, 24(5):513?523.Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar,and David M. Pennock.
2002.
Methods and met-rics for cold-start recommendations.
In Proceed-ings of the 25th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 253?260.Sarah E. Schwarm and Mari Ostendorf.
2005.
Readinglevel assessment using support vector machines andstatistical language models.
In Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 523?530.Kazunari Sugiyama, Kenji Hatano, and MasatoshiYoshikawa.
2004.
Adaptive web search based onuser profile constructed without any effort from user-s.
In Proceedings of the 13th International Confer-ence on World Wide Web, pages 675?684.Peter Welinder, Steve Branson, Serge Belongie, andPietro Perona.
2010.
The multidimensional wis-dom of crowds.
In Advances in Neural InformationProcessing Systems, pages 2424?2432.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier R Movellan.
2009.
Whosevote should count more: optimal integration of la-bels from labelers of unknown expertise.
In Ad-vances in Neural Information Processing Systems,pages 2035?2043.Jiang Yang, Lada Adamic, and Mark Ackerman.
2008.Competing to share expertise: the taskcn knowledgesharing community.
In Proceedings of the 2nd In-ternational AAAI Conference on Weblogs and SocialMedia.Elad Yom-Tov, Shai Fine, David Carmel, and AdamDarlow.
2005.
Learning to estimate query difficulty:including applications to missing content detectionand distributed information retrieval.
In Proceed-ings of the 28th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 512?519.Dengyong Zhou, Olivier Bousquet, Thomas NavinLal, Jason Weston, and Bernhard Sch?olkopf.
2004.Learning with local and global consistency.
In Ad-vances in Neural Information Processing Systems,pages 321?328.Yanhong Zhou, Gao Cong, Bin Cui, Christian S.Jensen, and Junjie Yao.
2009.
Routing questions tothe right users in online communities.
In Proceed-ings of the 25th IEEE International Conference onData Engineering, pages 700?711.Dengyong Zhou, John C Platt, Sumit Basu, and Y-i Mao.
2012.
Learning from the wisdom of crowdsby minimax entropy.
In Advances in Neural Infor-mation Processing Systems, pages 2204?2212.Xiaojin Zhu and John Lafferty.
2005.
Harmonic mix-tures: combining mixture models and graph-basedmethods for inductive and scalable semi-supervisedlearning.
In Proceedings of the 22nd Internation-al Conference on Machine Learning, pages 1052?1059.1126
