Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 230?233,Paris, October 2009. c?2009 Association for Computational LinguisticsHeuristic search in a cognitive model of human parsingJohn T. HaleCornell University217 Morrill HallIthaca, New York 14853jthale@cornell.eduAbstractWe present a cognitive process modelof human sentence comprehension basedon generalized left-corner parsing.
Asearch heuristic based upon previously-parsed corpora derives garden path effects,garden path paradoxes, and the local co-herence effect.1 IntroductionOne of the most interesting applications of pars-ing technology has, for some researchers, beenpsycholinguistic models (Kay, 2005).
Algorith-mic models of language use have led in the pastto a variety of cognitive insights (Kaplan, 1972;Marcus, 1980; Thibadeau et al, 1982; Pereira,1985; Pulman, 1986; Johnson, 1989; Stabler,1994).
However they are challenged by a veritabletidal wave of new data collected during the 1990sand 2000s.
Work during this later period revealsphenomena, such as the local coherence effect dis-cussed in section 5, that have yet to be truly inte-grated into any particular theoretical framework.This short paper presents a parsing system in-tended to serve as a model of the syntactic partof human sentence comprehension.
Such a modelhelps make sense of sentence-difficulty data fromself-paced reading, eye-tracking and other behav-ioral studies.
It also sketches a relationship be-tween calculations carried out in the course ofautomated syntactic analysis and the inferencesabout linguistic structure taking place in our mindsduring ordinary sentence-understanding.Section 2 defines the model itself, highlight-ing its relationship to generalized left-corner pars-ing.
Sections 3?5 apply this model to three contro-versial phenomena that are well-established in thepsycholinguistics literature.
Section 6 concludes.2 Architecture of the model2.1 Problem states and OperatorsWe model the human sentence comprehen-sion mechanism as search within a prob-lem space (Newell and Simon, 1972).
We as-sume that all (incremental) parser states havea (partial) grammatical interpretation (Chomsky,1965, 9).
In this paper, the grammatical inter-pretation employs context-free grammar.
An in-ventory of operators carries the model from onepoint in the problem space to another.
In the in-terest of simplicity, we place no bound on thenumber of problem states the model can explore.However, we do acknowledge with Johnson-Laird(1983) and Resnik (1992) a pressure to minimizememory consumption internal to a problem state.The model?s within-problem state memory usageshould reflect human acceptability judgments withembedded sentences.
These considerations moti-vate a generalized left-corner (GLC) parsing strat-egy (Demers, 1977) whose stack consumption ismaximal on just the center-embedded examplesthat are so difficult for people to understand.
Toreflect the argument/adjunct distinction (Tutun-jian and Boland, 2008) we adopt a mixed strat-egy that is bottom-up for optional postmodifiersbut left-corner everywhere else.
Leaving the arc-eager/arc-standard decision (Abney and Johnson,1991) to the control policy allows four possibleoperators, schematized in Table 1.2.2 Informed SearchInformed search differs from uninformed searchprocedures such as depth-first and breadth-firstby making use of heuristic knowledge about thesearch domain.
The strategy is to choose for ex-pansion the node whose cost is lowest (Barr andFeigenbaum, 1981, 61).
In A?
search (Hart et al,1968) this cost is divided up into a sum consistingof the known cost to reach a search node and an230shift a word W project a rule LHS ?
Trigger?announcepointRestscan the sought word Wproject and match the sought parent LHS usingthe rule LHS ?
Trigger?announcepointRestTable 1: Four schema define the operatorsstack n E[steps] standard error[VP] S [TOP] 55790 44.936 0.1572S [TOP] 53991 10.542 0.0986[NP] S [TOP] 43635 33.092 0.1633NP [TOP] 38844 55.791 0.2126NP [S] S [TOP] 34415 47.132 0.2122[S] S [TOP] 33578 52.800 0.2195[PP] S [TOP] 30693 34.454 0.1915IN [PP] S [TOP] 27272 32.379 0.2031DT [NP] S [TOP] 22375 34.478 0.2306[AUX] [VP] S [TOP] 16447 46.536 0.2863VBD [VP] S [TOP] 16224 43.057 0.2826VB [VP] S [TOP] 13548 40.404 0.3074the [NP] S [TOP] 12507 34.120 0.3046NP [NP] S [TOP] 12092 43.821 0.3269DT [TOP] 10440 66.452 0.3907Table 2: Popular left-corner parser states.
Stacksgrow to the left.
The categories are as described inTable 3 of Marcus et al (1993).estimate of the costs involved in finishing searchfrom that node.
In this work, rather than relyingon the guarantee provided by the A?
theorem, weexamine the exploration pattern that results froman inadmissable completion cost estimator.
Thechoice of estimator is Hypothesis 1.Hypothesis 1 Search in parsing is informed by anestimate of the expected number of steps to com-pletion, given previous experience.Table 2 writes out the expected number ofsteps to completion (E[steps]) for a selection ofproblem states binned together according to theirgrammatical interpretation.
Categories enclosedin square brackets are predicted top-down whereasunbracketed have been found bottom-up.
Thesestates are some of the most popular states vis-ited during a simulation of parsing the Brown cor-pus (Kuc?era and Francis, 1967; Marcus et al,1993) according to the mixed strategy introducedabove in subsection 2.1.
The quantity E[steps]serves in what follows as the completion cost esti-mate in A?
search.3 Garden pathingAny model of human sentence comprehensionshould address the garden path effect.
The con-trast between 1a and 1b is an example of this phe-nomenon.
(1) a. while Mary was mending a sock fell on the floorb.
while Mary was mending, a sock fell on the floorThe control condition 1b includes a comma which,in spoken language, would be expressed as aprosodic break (Carroll and Slowiaczek, 1991;Speer et al, 1996).Figure 1 shows the search space explored inthe experimental condition 1a.
In this picture,ovals represent problem states.
The number in-side the oval encodes the vistation order.
Arcs be-tween ovals represent operator applications.
Thepath (14, 22, 23, 24, 25, 29, 27) is the garden pathwhich builds a grammatical interpretation where asock is attached as a direct object of the verb mend.The grey line highlights the order in which A?search considers this path.
At state 21 after shift-ing sock, experience with the Brown corpus sug-gests reconsidering the garden path.Whereas the model examines 45 search nodesduring the analysis of the temporarily ambiguousitem 1a, it dispatches the unambiguous item 1b af-ter only 40 nodes despite that sentence having anadditional token (the comma).
Garden paths, onthis view, are sequences of parser states exploredonly in a temporarily ambiguous item.4 Garden pathing counterexamplesPurely structural attachment preferences likeRight Association (Kimball, 1973) and Mini-mal Attachment (Frazier and Fodor, 1978; Pereira,1985) are threatened by paradoxical counterexam-ples such as 2 from Gibson (1991, 22) where nofixed principle yields correct predictions acrossboth examples.
(2) a. I gave her earrings on her birthday .b.
I gave her earrings to another girl .A parser guided by Hypothesis 1 interleaves thegarden path attachment and the globally-correctattachment in both cases, resulting in a search that2311210434241323637453541383940313329201615272624171891928341482313112644223525213070Figure 1: Heavy line is the globally-correct pathis strictly committed to neither analysis.
In 2a,32% of discovered states represent the globally-incorrect attachment of her.
In 2b, 27% of statesrepresent the globally-incorrect attachment of herto give as a one-word direct object.
The para-dox for purely structural attachment heuristics isdissolved by the observation that neither pathwayfully achieves priority over the other.5 Local CoherenceTabor et al (2004) discovered1 a processing dif-ficulty phenomenon called ?local coherence.
?Among the stimuli they considered, the locally-coherent condition is 3a where the substring theplayer tossed a frisbee could be analyzed as a sen-tence, if considered in isolation.
(3) a.
The coach smiled at the player tossed a frisbee bythe opposing team.b.
The coach smiled at the player thrown a frisbeeby the opposing teamc.
The coach smiled at the player who was tossed afrisbee by the opposing team.d.
The coach smiled at the player who was thrown afrisbee by the opposing team.Tabor and colleagues observe an interaction be-tween the degree of morphological ambiguity ofthe embedded verb (tossed or thrown) and thepresence or absence of the relative-clause initialwords who was.
These two factors are known as?ambiguity and ?reduction, respectively.
If thehuman parser were making full use of the gram-mar, its operation would reflect the impossibilityof continuing the coach smiled at... with a sen-tence.
The ungrammaticality of a sentence in thisleft context would preclude any analysis of theplayer as a subject of active voice toss.
But greaterreading times observed on the ambiguous tossedas compared to the unambiguous thrown suggestcontrariwise that this grammatical deduction is notmade uniformly based on the left context.Table 3 shows how an informed parser?s stepcounts, when guided by Hypothesis 1, deriveTabor et al?s observed pattern.
The cell pre-dicted to be hardest is the local coherence,shaded gray.
The degree of worsening due to rel-ative clause reduction is greater in +ambiguousthan in ?ambiguous.
This derives the observedinteraction.1Konieczny and Mu?ller (2006) documents a closely re-lated form of local coherence in German.232+ambiguous ?ambiguous+reduced 119 84?reduced 67 53Table 3: Count of states examined6 ConclusionWhen informed by experience with the Browncorpus, the parsing system described in this pa-per exhibits a pattern of ?time-sharing?
perfor-mance that corresponds to human behavioral diffi-culty in three controversial cases.
The built-in el-ements ?
context-free grammar, generalized left-corner parsing and the A?-type cost function ?are together adequate to address a range of com-prehension difficulty phenomena without impos-ing an a priori memory limit.
The contribution isan algorithmic-level account of the cognitive pro-cesses involved in perceiving syntactic structure.ReferencesSteven Abney and Mark Johnson.
1991.
Memory require-ments and local ambiguities of parsing strategies.
Journalof Psycholinguistic Research, 20(3):233?249.Avron Barr and Edward A. Feigenbaum, editors.
1981.
TheHandbook of Artificial Intelligence.
William Kaufmann.Patrick J. Carroll and Maria L. Slowiaczek.
1991.
Modes andmodules: multiple pathways to the language processor.In Jay L. Garfield, editor, Modularity in Knowledge Rep-resentation and Natural Language Understanding, pages221?247.
MIT Press.Noam Chomsky.
1965.
Aspects of the Theory of Syntax.MIT Press.Alan J. Demers.
1977.
Generalized left corner parsing.
InConference Report of the 4th annual association for com-puting machinery symposium on Principles of Program-ming Languages, pages 170?181.Lyn Frazier and Janet Dean Fodor.
1978.
The sausage ma-chine: a new two-stage parsing model.
Cognition, 6:291?325.Edward Gibson.
1991.
A Computational Theory of HumanLinguistic Processing: Memory Limitations and Process-ing Breakdown.
Ph.D. thesis, Carnegie Mellon University.Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968.
Aformal basis for the heuristic determination of minimumcost paths.
IEEE Transactions of systems science and cy-bernetics, ssc-4(2):100?107.Philip N. Johnson-Laird.
1983.
Mental Models.
CambridgeUniversity Press.Mark Johnson.
1989.
Parsing as deduction: the use of knowl-edge of language.
Journal of Psycholinguistic Research,18(1):105?128.Ronald M. Kaplan.
1972.
Augmented transition networks aspsychological models of sentence comprehension.
Artifi-cial Intelligence, 3:77?100.Martin Kay.
2005.
A life of language.
Computational Lin-guistics, 31(4):425?438.John P. Kimball.
1973.
Seven principles of surface structureparsing in natural language.
Cognition, 2:15?48.Lars Konieczny and Daniel Mu?ller.
2006.
Local coherencesin sentence processing.
CUNY Conference on HumanSentence Processing.Henry Kuc?era and W. Nelson Francis.
1967.
ComputationalAnalysis of Present-day American English.
Brown Uni-versity Press.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Linguis-tics, 19.Mitchell P. Marcus.
1980.
A theory of syntactic recognitionfor natural language.
MIT Press.Allen Newell and Herbert A. Simon.
1972.
Human ProblemSolving.
Prentice-Hall, Englewood Cliffs, New Jersey.Fernando Pereira.
1985.
A new characterization of attach-ment preference.
In David Dowty, Lauri Karttunen, andArnold Zwicky, editors, Natural Language Parsing: Psy-chological, Computational and Theoretical Perspectives,ACL Studies in Natural Language Processing, pages 307?319.
Cambridge University Press.Steven G. Pulman.
1986.
Grammars, parsers, and mem-ory limitations.
Language and Cognitive Processes,1(3):197?2256.Philip Resnik.
1992.
Left-corner parsing and psychologi-cal plausibility.
In Proceedings of the Fourteenth Interna-tional Conference on Computational Linguistics, Nantes,France.Shari R. Speer, Margaret M. Kjelgaard, and Kathryn M. Do-broth.
1996.
The influence of prosodic structure onthe resolution of temporary syntactic closure ambiguities.Journal of Psycholinguistic Research, 25(2):249?271.Edward Stabler.
1994.
The finite connectivity of lin-guistic structure.
In Charles Clifton, Lyn Frazier, andKeith Rayner, editors, Perspectives on Sentence Process-ing, pages 303?336.
Lawrence Erlbaum.Whitney Tabor, Bruno Galantuccia, and Daniel Richardson.2004.
Effects of merely local syntactic coherence onsentence processing.
Journal of Memory and Language,50(4):355?370.Robert Thibadeau, Marcel A.
Just, and Patricia Carpenter.1982.
A model of the time course and content of read-ing.
Cognitive Science, 6:157?203.D.
Tutunjian and J.E.
Boland.
2008.
Do We Need a Distinc-tion between Arguments and Adjuncts?
Evidence fromPsycholinguistic Studies of Comprehension.
Languageand Linguistics Compass, 2(4):631?646.233
