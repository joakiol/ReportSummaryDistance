Towards Automatic Grammar Acquisition from a BracketedCorpusThanaruk TheeramunkongJapan Advanced Institute ofScience and TechnologyGraduate School of Information Science15 Asahidai TatsunokuchiNomi Ishikawa 923-12 Japanping~j a i s t .
ac.
jpManabu OkumuraJapan Advanced Institute ofScience and TechnologyGraduate School of Information Science15 Asahidai TatsunokuchiNomi Ishikawa 923-12 Japan0ku@j a i s t .
ac.
jpAbst rac tIn this paper, we propose a method to group brackets in a bracketed corpus (with lexical tags),according to their local contextual information, as a first step towards the automatic acquisitionof a context-free grammar.
Using a bracketed corpus, the learning task is reduced to the problemof how to determine the nonterminal label of each bracket in the corpus.
In a grouping process, asingle nonterminai label is assigned to each group of brackets which are similar.
Two techniques,distributional analysis and hierarchical Bayesian clustering, are applied to exploit local contextualinformation for computing similarity between two brackets.
We also show a technique developedfor determining the appropriate number of bracket groups based on the concept of entropy analysis.Finally, we present a set of experimental results and evaluate the obtained results with a modelsolution given by humans.1 Introduct ionDesigning and refining a natural language grammar is a diiBcult and time-consuming task and re-quires a large amount of skilled effort.
A hand-crafted grammar is usually not completely satisfactoryand frequently fails to cover many unseen sentences.
Automatic acquisition of grammars i a solu-tion to this problem.
Recently, with the increasing availability of large, machine-readable, parsedcorpora, there have been numerous attempts to automatically acquire a CFG grammar through theapplication of enormous existing corporaILar90\]\[Mi194\]\[Per92\]\[Shi95 \].Lari and Young\[Lar90\] proposed so-called inside-outside algorithm, which constructs a grammarfrom an unbracketed corpus based on probability theory.
The grammar acquired by this methodis assumed to be in Chomsky normal form and a large amount of computation is required.
Later,Pereira\[Per92\] applied this algorithm to a partially bracketed corpus to improve the computationtime.
Kiyono\[Kiy94b\]\[Kiy94a\] combined symbolic and statistical approaches to extract useful gram-mar rules from a partially bracketed corpus.
To avoid generating a large number of grammar rules,some basic grammatical constraints, local boundaries constraints and X bar-theory were applied.Kiyono's approach performed a refinement of an original grammar by adding some additional ruleswhile the inside-outside algorithm tries to construct a whole grammar from a corpus based on Max-imum Likelihood.
However, it is costly to obtain a suitable grammar from an unbracketed corpusand hard to evaluate results of these approaches.
As the increase of the construction of brack-eted corpora, an attempt o use a bracketed (tagged) corpus for grammar inference was made byShiral\[Shi95\].
Shirai constructed a Japanese grammar based on some simple rules to give a name (alabel) to each bracket in the corpus.
To reduce the grammar size and ambiguity, some hand-encodedknowledge is applied in this approach.In our work, like Shirai's approach, we make use of a bracketed corpus with lexical tags, butinstead of using a set of human-encoded predefined rules to give a name (a label) to each bracket, weintroduce some statistical techniques to acquire such label automatically.
Using a bracketed corpus,the grammar learning task is reduced to the problem of how to determine the nonterminal label ofeach bracket in the corpus.
More precisely, this task is concerned with the way to classify bracketsto some certain groups and give each group a label.
We propose a method to group brackets in168a bracketed corpus (with lexical tags), according to their local contextual information, as a firststep towards the automatic acquisition of a context-free grammar.
In the grouping process, a singlenontermina\] label is assigned to each group of brackets which are similar.
To do this, we applyand compare two types of techniques called distributional analysis\[HarSl\] and hierarchical Bayesianclustering\[Iwa95\] for setting a measure representing similarity among the bracket groups.
We alsopropose a method to determine the appropriate number of bracket groups based on the concept ofentropy analysis.
Finally, we present a set of experimental results and evaluate our methods with amodel solution given by humans.2 Grammar  Acquis i t ion wi th  a Bracketed CorpusIn this section, we give a brief explanation of grammar acquisition using a bracketed corpus.
In thiswork, the grammar acquisition utilizes a lexical-tagged corpus with bracketings.
An example of theparse structures of two sentences in the corpus is shown graphically in Figure 1.Sentence  (1)Parse  Tree (1)Sentence  (2)Parse Tree (2): A big man slipped on the ice.
: (((ART,"a") ((ADJ," big')  (NOUN," man")))((VI," sl ipped")((PREP,"on") ((ART,"the") (NOUN," ice'))))): The boy dropped his wallet somewhere.
: (((ART,"the") (NOUN,"boy'))(((VT,"dropped") ((PROg,"his") (NOUN,"wallet')))(AVV,"somewhere")))m m ?
|| t i |ART ADJ NOUN Vl PREP ART NOUN ART NOUN VT PRON NOUN ADVA big man sl ipped on the ice The boy dropped his wallet somewhereF igure 1: The graphical  representat ion of the parse structures of a big man slipped on the ice and theboy dropped his wallet somewhereIn the parse structures, each terminal category (leaf node) is given a name (tag) while there is nolabel for each nonterminal category (intermediate node).
With this corpus, the grammar learningtask corresponds to a process to determine the nonterminal label of each bracket in the corpus.More precisely, this task is concerned with the way to classify the brackets into some certain groupsand give each group a label.
For instance, in Figure 1, it is reasonable to classify the brackets 1(c2),(c4) and (c5) into a same group and give them a same label (e.g., NP(noun phrase)).
Asthe result, we obtain three grammar ules: NP ~ (ART) (NOUN) ,  NP  ~ (PRON)(NOUN)  andNP  ~ (ART)(el) .
To perform this task, our grammar acquisition algorithm operates in five stagesas follows.1.
Assign a unique label to each node of which lower nodes are assigned labels.
At the initialstep, such node is one whose lower nodes are lexical categories 2.
This process is performedthroughout all parse trees in the corpus.1A bracket corresponds to a node in Figure 1.~In Figure 1, there are three unique labels derived: el---~(ADJ)(NOUN), cc2-*(ART)(NOUN) andcs ~ (PRON)(NOUJV).1692.
Calculate the similarity of every pair of the derived labels.3.
Merge the most similar pair to a single new label(i.e., a label group) and recalculate thesimilarity of this new label with other labels.4.
Repeat (3) until a termination condition is detected.
As the result of this step, a certain setof label groups is derived.5.
Replace labels in each label group with a new label in the corpus.
For example, if (ART)(NOUN)and (PRON)(NOUN) are in the same label group, we replace them with a new label (such asNP) in the whole corpus.6.
Repeat (1)-(5) until all brackets(nodes) in the corpus are assigned labels.In this paper, as a first step of our grammar acquisition, we focus on step (1)-(4), that is how togroup nodes of which lower nodes are lexical categories.
Figure 2 depicts an example of the groupingprocess.G3I(ADJ){NOUN) C1  - - -~-  g l  (rip without an article)(NOUN)(NOUN) C7 t(ART)(NOUN) C2 t(PRON)INOUN) C5 g2 (rip with an article)(INDEF)(NOUN) C6I|INDEF = { both, some, any .... }ART ={a,  the .... }PRON = { my, his, her, their .... }NOUN = {trip, newspaper .... }ADJ = {high, available .... }Figure 2: A part of the bracket grouping processTo compute the similarity of a pair of labels(in step 2), we propose two types of techniques calleddistributional nalysis and hierarchical Bayesian cbtstering as shown in section 3.
In section 4, weintroduce the concept of differential entropy as the termination condition used in step (4).3 Loca l  Contextua l  In fo rmat ion  as S imi la r i ty  MeasureIn this section, we describe two techniques which utilize "local context information" to calculatesimilarity between two labels.
The term "local contextual information" considered here is repre-sented by a pair of words immediately before and after a label.
In the rest of this section, we firstdescribe distributional nalysis in subsection 3.1.
Next, we give the concept of Bayesian clusteringin subsection 3.2.3 .1  D is t r ibut iona l  Ana lys i sDistributional nalysis is a statistical method originally proposed by Harris\[Harbl\] to uncover eg-ularities in the distributional relations among the features of speech.
Applications of this techniqueare varied\[Bri92\]\[Per93\].
In this paper, we apply this technique to group similar brackets in abracketed corpus.
The detail of this technique is illustrated below.Let P1 and P2 be two probability distributions over environments.
The relative ntropy betweenP1 and P2 is:Pl(e) D(PIlIP2) = ~ Pz(e) x log P~(e)?
E Env~i ro~m?~' tsRe la t ive  ent ropy  D(PIlIP2 ) is a measure of  the amount  o f  ext ra  in fo rmat ion  beyond P2  neededto describe P l .
The divergence between Pz and P= is defined as D(PIlIP2 ) + D(P2IIP1), and isa measure of how difficult it is to distinguish between the two distributions.
The environment is170a pair of words immediately before and after a label(bracket).
A pair of labels is considered to beidentical when they are distributionaliy similar, i.e., the divergence of their probability distributionsover environments i  low.The probability distribution can be simply calculated by counting the occurrence of (c~) and(word1 c~ words).
For the example in Figure 1, the numbers of appearances of (c1), (c2), (c5),(ART cz VI), (PREP c2 NULL) and (VT es ADV) are collected from the whole corpus.
NULLstands for a blank tag representing the beginning or ending mark of a sentence.Sparse  Data  Cons iderat ionsUtilizing divergence as a similarity measure, there is a serious problem caused by the sparsenessof existing data or the characteristic of language itself.
In the formula of relative entropy, thereis a possibility that P2(e) becomes zero.
In this condition, we cannot calculate the divergence oftwo probability distributions.
To cope with this problem, we extend the original probability to oneshown in the following formula.P(ac~ b) = A N(ac~b) ~.
(l_A)Nt~g N(~) s 2where, N(a) is the occurrence frequency of o~, Ntag8 is the number of terminal categories and A isa interpolation coefficient.
The first term in the right part of the formula is the original estimatedprobability.
The second term is generally called a uniform distribution, where the probability of anunseen event is estimated to a uniform fixed number.
A is applied as a balancing weight betweenthe observed istribution and the uniform distribution.
Intuitively, when the size of data is large,the small number should be used as A.
In the experimental results in this paper, we assigned A witha value of 0.6.3.2 Hierarchical Bayesian Clustering MethodAs a probabilistic method, hierarchical Bayesian clustering was proposed by Iwaya~na\[Iwa95\] toautomatically classify given texts.
It was applied to improve the efficiency and the effectiveness oftext retrieval/categorization.
Referring to this method,we try to make use of Baycsiar~ posteriorprobability as another similarity measure for grouping the similar brackets.
In this section, weconclude the concept of this measure as follows.Let's denote a posterior probability with P(GIC), where C is a collection of data (i.e., in Figure2, C = {c1,e2, ..., CN}) and G is a set of groups(clusters) (i.e., G = {gz,g2, ...}).
Each group(cluster)gj is a set of data and the groups are mutually exclusive.
In the initial stage, each group is a singletonset; g~ = {~} for all i.
The method tries to select and merge the group pair that brings about themaximum value of the posterior probability ~ P(GIC).
That is, in each step of merging, this methodsearches for the most plausible situation that the data in C are partitioned in the certain groups G.For instance, at a merge step h + 1 (0 < b < N - 1), a data collection C has been partitioned intoa set of groups G~.
That is each datum e belongs to a group g E Gk.
The posterior probability atthe merging step/?
+ 2 can be calculated using the posterior probability at the merging step/?
+ 1as shown below (for more detail, see\[Iwa95\]).PC(Gh) SC(g=)SC(g,)Here PC(G~) corresponds to the prior probability that N random data are classified in to a set ofgroups O~.
As for the factor of ~ a well known estimate\[Ris89\] is applied and it is reduced PC(G~)  'to a constant value A -1 regardless of the merged pair.
For a certain merging step, P(G~IC ) isidentical independently of which groups are merged together.
Therefore we can use the followingmeasure to select he best group pair to merge.
The similarity between two bracket groups(labels),g= and gv, can be defined by SIM(g=,gv).
Here, the larger SIM(g=,g~) is, the more similar twobrackets are.SO(g, U g,)S IM(g=,g , )  = SC(g= )SC(g, )SC(g) = I~  P(elg)cEgSMaximizing P(GIC ) is a generalization of Mac/mUSh L//cel/hood estimation.171= ~ P(clg, e)P(elg) P(clg)eEEnv i~onwtc~ta,~, E P(cle)p(elg)eP(elc) P(elg) = P(c) P(e)where SC(g) expresses the probability that all the labels in a group g are produced from the group,an elementai probability P(c\[g) means the probability that a group g produces its member c andP(elc ) denotes a relative frequency of an environment e of a label e, P(elg ) means a relative frequencyof an environment e of a group g and P(e) is a relative frequency of an environment e of the entirelabel set.
In the calculation of SIM(g=,gv), we can ignore the value of P(c) because it occursIg= U gvl times in both denominator and numerator.
Normally, SIM(g=,gy) is ranged between 0and 1 due to the fact that P(c\[g= Ugy) _< P(clg= ) when c E g=.4 Differential Entropy as Terminat ion Condit ionDuring iteratively merging the most similar labels, all labels will finally be gathered to a single group.Due to this, it is necessary to provide a criterion for determining whether this merging process houldbe continued or terminated.
In this section, we describe a criterion amed differential entropy whichis a measure of entropy (perplexity) fluctuation before and after merging a pair of labels, Let cl andc2 be the most similar pair of labels based on divergence or Bayesia~u posterior probability.
Also letc3 be the result label.
P~i (e), Pc= (e) and Pc3(e) are probability distributions over environment e ofcl, e2 and c3, respectively.
Pc1, Pc= and P~3 are estimated probabilities of cl, c2 and c3, respectively.The differentiaJ entropy (Z~E) is defined as follows.~E = Consequence Entropy - Previous Entropy= - P0~ x ~Po~(e) logPo~(e)e+ Po, x  Po,(e)logPo,(e) + Po= x  P0,(e)logPo,(e)?
ewhere ~,  Pc~ (e) log Pc, (e) is the total entropy over various environments of label c~.
The larger ~Eis, the larger the information fluctuation before aad after merging becomes.
Generally, we prefera small fluctuation to a larger one.
When ZXE is large, the current merging process introduces alarge amount of information fluctuation and its reliability should be low.
From this viewpoint, weapply this measure as a criterion for determining the termination of the merging process which willbe given in the next section.5 Prel iminary Exper imental  ResultsIn this section, we show some results of our preliminary experiments o confirm effectiveness ofthe proposed techniques.
The corpus we used is constructed by EDR and includes nearly 48,000bracketed, tagged sentences\[EDR94\].
As mentioned in the previous sections, we focus on onlythe rules with lexical categories as their right hand side 4.
For instance, cx --~ (ADJ)(NOUN),e2 --* (ART)(NOUN) and cs --* (PRON)(NOUN) in Figure 1.
To evaluate our method, we usethe rule tokens which appear more than 500 times in the corpus.
Table I gives some characteristicsof the corpus.From the 35 initial rules, we calculate the similarity between any two rules (i.e., any rule pair)based on divergence and Bayesian posterior probability (BPP).
For the divergence measure, thesmaller the vaiue is, the more similar the rule pair is.
Inversely, for BPP, the larger the value is, themore similar the pair looks.
After calculating all pairs' similarities, we merge the most similar pair(the minimum divergence or the maximum BPP) to a new label and recalculate the similarity ofthe new label with other remaining labels.
The merging process is carried out in an iterative way.Figure 3 shows the minimum divergence (left) and the maximum Bayesian posterior probability(right) of each merge step.In each iterative step of the merging process, we calculate differential entropy for both cases.
Thedifferential entropy of each step equals to the entropy difference between the entropy of two rulesbefore merging and the entropy of a new rule after merging as described in the previous ection.4Other types of rules can be acquired in almost he same way and are left now as our further work.172No.of sentences 48259No.of initial rules (.f > 500) 35(from total 761 rules)Total number of rule tokens 136087(from total 152925)Table 1: Some features of the corpus8 ==2.5i \[\]2 .
.
.
.
.
.
.
.
.
.
.
.
.  '
.
.
.
.
.
.
.
.
.
.
.
.
.
4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
:  .
.
.
.
.1 5  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I0.5 , ~ - "  ~BE\ ]~i |0 5 10 15 20 25 30 350.750.50.25%i ~aGa "0 5 10 15 20 25 30 35Merge Step " Merge StepFigure 3: The minimum divergence (left) and the maximum Bayesian posterior probability (right) ofeach merge stepTwo graphs in Figure 4 indicate the results of differential entropy (LXE calculated by the formulain section 4) when the merging process advanced with divergence and BPP as its similarity measures.There are some sharp peaks indicating the rapid fluctuation of entropy in the graphs.
In this work,we use these peaks as a clue to find the timing we should to terminate the merging process.
As theresult, we halt up the process at the 22nd step and the 27th step for the cases of divergence andBPP, respectively.
Table 2 shows the obtained grouping results.
In these tables, there axe 13 groupsfor divergence and 8 groups for Bayesian posterior probability.
To clarify the result in the tables,Some sample words of each label axe given in the appendix.0.50.4O.3o.e0.10I0 5 10 15 20 25 30 350.2 ii0.15 .
.
.
.
.
.
.
.
.
.
.
.
.
"; .................................................................... ~'"'"J ..i/ /0 5 10 15 20 25 30 35Merge Step Merge StepFigure 4: Differential entropy during the merging processes using divergence (left) and BPP (right)We also made an experiment to evaluate these results with the solution given by three humanevaluators (later called A, B and C) who axe non-natlve but high-educated with more than 20 yearsof English education.
The evaluators were told to construct 7-15 groups from the 35 initial rules,based on the grammatical similarity as they thought.
As the result, the evaluators A, B and Cclassified the rules into 14, 13 and 14 groups, respectively.173Members(INDEF)(NOUN), (ART)(NOUN),(PRON)(NOUN), (DEMO)(NOUN),(NUM)(NOUN), (NUM)(UNIT),(NOUN)(NUM)(ADJ)(NOUN), (NOUN)(NOUN),(NOUN) (CONJ)(NOUN)3 (AUX)(VT)(PREP)(NOUN), (PREP)(NUM),(PREP)(PRON), (ADV)(ADV),(PTCL)(VI)m I\[ gDI(VT)(NOUN), (VI)(ADV),(VT)(PRON), (AUX)(VI),(BE)(VI), (BE)(VT),(BE)(ADJ), (ADV)(VI),(VI)(PTCL)7 (ADV)(ADJ)8 (AUX)(ADV)9 (ADV)(VT), (VT)(PTGL),(VI)(PREP)10 (AUX)(BE)11 (BE)(ADV)12 (ADV)(BE)13 (PRON)(VT)Members1 (INDEF)(NOUN), (ART)(NOUN),(PRON)(NOUN), (DEMO)(NOUN),(NOUN)(CONJ)(NOUN)2 (ADJ)(NOUN), (NOUN)(NOUN)3 (AUX)(VT), (AUX)(BE),(BE)(ADV),(PTCL)(VT), (AUX)(ADV)4 (PREP)(NOUN), (ADV)(ADV),(PREP)(PRON), (PTCL)(VI),(PREP)(NUM)5 (VT)(NOUN), (Vl)(ADV),(VT)(PRON), (AUX)(VI),(BE)(Vl), (BE)(ADJ),(BE)(VT), (ADV)(VI),(VI)(PTCL)6 (ADV)(AD3), (NUM)(NOUN),(NUM)(UNIT)7 (ADV)(VT), (VT)(PTCL),(VI)(PREP)8 (NOUN)(NUM), (ADV)(BE),(PRON)(VT)Table 2: The grouping result  using divergence (left) and BPP  (f ight)The system says YesThe system says NoThe Evaluator's AnswerYes ' Noa be dTable 3: The number of entry pairs for evaluating accuracyTo evaluate the system with the model solutions, we applied a contingency table model as oneshown in Table 3.
This table model was introduced in \[Swe69\] and widely used in InformationRetrieval and Psychology.
In the table, a is the number of the label pairs which an evaiuatorassigned in the same group and so did the system, b is the number of the pairs which an evaluatordid not assign in the same group but the system did, e is the number of the pairs which an evaluatorassigned but the system did not, and d is the number of the pairs which both an evaluator andthe system did not assign in the same group.
From this table, we define seven measures, as shownbelow, for evaluating performance of the proposed methods.
This evaluation technique was alsoapplied partly for computing "closeness" between a system's answer and an evaluator's answer in\[Hat93\]\[Aga95\]\[Iwa95\]?
Pos i t i ve  Recal l  (PR)  :?
Pos i t ive  Prec is ion  (PP)  :d ?
Negat ive  Recal l  (NR)  : ~+---~?
Negat ive  Prec is ion  (NP)  : 7~PR~NR ?
Averaged Reca l l  (AR)  : 2PP.~.NP ?
Averaged Prec is ion  (AP)  : 2(fl3-1-1)?PPxPR ?
F -measure  (FM)  : fla?PP+PRThe F-measure is used as a combined measure of recall and precision, where fl is the weight ofrecall relative to precision.
Here, we use fl = 1.0, which corresponds to equal weighting of the twomeasures.
The results compared with three human evaluators are shown in Table 4.174Evaluator AEvaluator BEvaluator CAveragedSimilaxity MeasuresPR I PP NR NP AR AP  FMDivergence 0.91 i 0.70 0.96 0.99 0.93 0.84 0.79BPP 0.63 i 0.46 0.92 0.96 0.77 0.71 0.53Divergence 0.73 i 0.66 0.95 0.97 0.84 0.81 0.69BPP 0.68 i 0.59 0.94 0.96 0.81 0.78 0.63Divergence 0.89 i 0.66 0.95 0.99 0.92 0.82 0.76BPP 0 .80  0.57 0.94 0.98 0.87 0.77 0.66Divergence 0.84 0.67 0.95 0.98 0.90 0.82 0.75BPP 0 .70  0.54 0 .93  0.971 0.82 0.75 0.61Table 4: Eva~luation results using three human eva~uators' solutionsFrom these results, we observe some features as follows.
The divergence gives a better solutionthan Bayesian posterior probability does.
Normally, the positive measures (PR and PP) have smallervalues than the negative ones (NR and NP) do.
This means that it is difficult to judge two labelsto be in a same group rather than to judge them to be in a separate group.
Using divergenceas a similarity measure, we get, on average, 84 % positive recall and 67 ~ positive precision andup to 90 ~ and 82 % when considering both positive and negative measures.
Even for the worstresult(Evaluator B), we can get up to 84 % and 81% for averaged recall and precision.
In orderto confirm the performance of the system, the evaluators' results axe compared with each other.This comparison is useful for investigating the difficulty of the grouping problem.
The comparisonresult is shown in Table 5.
At this point, we can observe that the label grouping process is a hardproblem that may make an evaluator's solution inconsistent with the others' solutions.
However,our proposed method seem to give a reconciliation solution between those solutions.
Especially, themethod which applies divergence as the similarity measure, has a good performance in groupingbrackets in the bracketed corpus.A+BB+AB+CC+BC+AA+Cl Measures lPR \[ PP \[ NR \[ NP \ [AR  AP \[ FM0.55 0.47 0~94 0.95 0.74 0.71 0.510.68 0.83 0.98 0.96 0.83 0.90 0.750.57 0,55 0.95 0.96 0.76 0.76 0.56I, Averaged10"6110"61\[ 0"96 \[ 0"96 \[ 0"78 0"78\[0"61lTable 5: Comparing the grouping results obtained by the evaluators(A,B,C)We also make an experiment to evaluate whether divergence is a better measure than BPP,and whether the application of differential entropy to cut off the merging process is appropriate.This examination can be held by plotting values of recall, precision and F-measure during each stepof merging process.
Figure 5 shows the fluctuation of positive recall(PR), positive preclsion(AP),averaged recall(AR), averaged precision and F-measure (FM).From the graphs, we found out that the maximum value of F-measure is 0.75 in the case ofdivergence while it is only 0.65 in the case of BPP.
That is, divergence provides a better solutionthan BPP.
Moreover, the 22nd an 25th merge steps were the most suitable points to terminate themerging process for divergence and BPP, respectively.
This result is consistent with the groupingresult of our system (13 groups) in the case of divergence.
Although differential entropy leads usto terminate the merging process at the 27th merge step in the case of BPP, we observe that thereis just a little difference between the F-measure value of the 25th merge step and that of the 27thmerge step.
From this result, we conclude that differential entropy can be used a good measure topredict the cut-off timing of the merging process.1750.8EtL 0.6"~ 0.4~ 0.200r - -v ' - " i  .
.
.
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
7 .
.
.
.
.
.
.~- ...,...~........\ ~... ~ ~/  !.
.
.
.
.
.
.
.
.
.
.
.
~....,~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.. .
.
.p .
.
.
.
.
.
: .
, , .
.
.w .
.
.
.
.~  .
.
.
.
.
.
.
.
.
.
.
.
~ .~ .
.
.
.
.
.
.
.
.
.i " "  ~ ', .
- "  ~ ~ i ~, P..:..,.~.............. ~ / i '~  ............. P'~: ....... ~ ~ i i ~  { .
........... .. ~ ..............
!--..v---.~.-?
........ ~---', / : : ? "
_ _ -L  ~ :~ : , .
L .
.
/ '~ ,~ % ,~i," ~ : A : :  ....
~:~,.},--4.........
: ~ : .
: : : F "  i,, " ' i.~ ' i l l  ~-measbre - - i -  i .... 4~f i  i i i i5 10 15 20  25  30 35i 0.80.6~, 0.4" 0.2Q:00I I I I Ii !
i i ir~-~ ............. ?
............
I..............
I .............. !
............ t ...... b, ' ,  _ ,?
, .
.
,  ~ ~ ~ !
~ /........... \[......... );!~ ........ J .
.
.
.
.
.
i ............ ~:,,:::t ~ ,~ .
.
.
.
.
.
.
"~ " .F ,.
.
.
.
.
.
.
.
.
.
.
.
.
~.
.
.
.
.
.
.
.
.
.
.
.
.
$ .
,~ ,* , : : t .
.~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
~~: :~.
.~\ [ i i :~ .
,~ .
.~ ' .~- .~_ , .
:  ..t~.,, ...~....:~.,i ~ ~ .
/  : !
X !
\............. ~ ............. ~- .......... ~.~::::-- /- .~-.-  -----i-.- .-----.
:~.-\-.~ .,.
~.
, , J  Reca l l i  i \ \i ~ .
- -~"  jPI, P r~ is ion~ .
.
.
.
i \......... 7 " ~ .
: :  ............. .
.
.
.5 10 15 20 25 30Merge Step Merge Step35Figure 5: The transition of PR, PP, AR, AP, FM during the merging process using divergence(left) andBPP(r ight)  as the similarity measures6 Conclus ionThere has been an increase of the construction of many types of corpora, including bracketed cor-pora.
In this work, we attempt o use a bracketed (tagged) corpus for grammar inference.
Towardsthe automatic acquisition of a context-free grammar, we proposed some statistical techniques togroup brackets in a bracketed corpus (with lexical tags), according to their local contextual infor-mation.
Two measures, divergence and Bayesian posterior probability, were introduced to expressthe similarity between two brackets.
Merging the most similar bracket pair iteratively, a set oflabel groups was constructed.
To terminate a merging process at appropriate timing, we proposeddifferential entropy as a measure to represent the entropy difference before and after merging twobrackets and stopl~ed the merging process at a large fluctuation.
From the experimental resultscompared with the model solutions given by three human evaluators, we observed that divergencegave a better solution than Bayesian posterior probability.
For divergence, we obtained 84 % recalland 67 % precision, and up to 90 ~ and 82 % when considering both positive and negative measures.We also investigated the fitness of using differential entropy for terminating the merging process byway of experiment and confirmed it.In this paper, we focus on only rules with lexical categories as their right hand side.
As a furtherwork, we are on the way to introduce the techniques introduced here to acquire the other rules(ruleswith nonterminal categories as their right hand side).
At that time, it is also necessary for us todevelop some suitable evaluation techniques for assessing the obtained grammar.References\[Aga95\] Agarwal, R.: Evaluation of Semantic Clusters, in Proceeding of 33th Annual Meeting ofthe AGL, pp.
284-286, 1995.\[Bri92\] BrlU, E.: Automatically Acquiring Phrase Structure using Distributional Analysis, in Pro?.of Speech and Natural Language Workshop, pp.
155-159, 1992.\[EDR94\] EDR: Japan Electronic Dictionary Research Institute: EDR Electric Dictionary User'sManual (in Japanese), 2.1 edition, 1994.\[Har51\] Harris, Z.: Structural Linguistics, Chicago: University of Chicago Press, 1951.\[Hat93\] Hatziwassiloglou, V. and K. It.
McKeown: Towards the Automatic Identification of Ad-jectival Scales: Clustering Adjectives according to Meaning, in Proceeding of 31st AnnualMeeting of the ACL, pp.
172-182, 1993.\[Iwa95\] Iwayama, M. and T. Tokunaga: Hierarchical Bayesian Clustering for Automatic TextClassification, in IJCAI, pp.
1322-1327, 1995.\[Kiy94a\] Kiyono, M. and J. Tsujii: Combination of Symbolic and Statistical Approaches for Gram-matical Knowledge Acquisition, in Proc.
of 4th Conference on Applied Natural LangnageProcessing(ANLP'9,~), pp.
72-77, 1994.176\[Kiy94b\]\[Larg0\]\[Mi194\]\[Per92\]\[Per93\]\[Ris89\]\[Shi95\]\[Swe69\]Kiyono, M. and J. Tsujii: Hypothesis Selection in Grammar Acquisition, in COLING.g~,pp.
837-841, 1994.Lari, K. and S. Young: "The Estimation of Stochastic Context-free Grammars Using theInside-Outside Algorithm", Computer speech and languages, Vol.
4, pp.
35-56, 1990.Miller, S. and H. J.
Fox: Automatic Grammar Acquisition, in Proc.
of the Human LanguageTechnology Workshop, pp.
268-271, 1994.Pereira, F. and Y. Schabes: Inside-Outside r estimatlon from partially bracketed corpora,in Proceeding of 30th Annual Meeting of the ACL, pp.
128-135, 1992.Pereira, F., N. Tishby, and L. Lee: Distributional Clustering of English Words, in Pro.ceeding of 31st Annual Meeting of the ACL, pp.
183-190, 1993.Rissanen, J.: Stochastic Complexity in Statistical Inquiry, World Scientific Publishing,1989.Shirai, K., T. Tokunaga, and H. Tanaka: Automatic Extraction of Japanese Grammar f oma Bracketed Corpus, in Natural Language Processing Pacific Rim Symposium(NLPRS'gs),pp.
211-216, 1995.Swets, J.: Effectiveness of Information Retrieval Methods, American Documentation,Vol.
20, pp.
72-89, 1969.Appendixlabels(ADJ)(ADV)(ART)(AUX)(BE)(CON J)(DEMO)(INDEF)(NOUN)(NUM)(PREP)(PRON)(PTCL)(UNIT)(VI)(VT)some instances'specific' 'commercial' 'adequate' 'structural' 'old''explicitly' 'enormously' 'quite' not''the' 'a' 'an''may' should' 'did' 'could' will' 'have''be' 'is' 'are''and' 'when' or''this' 'that' these' 'such''few' 'one' any' 'some''member' 'Japan' merchant' 'tour' area''2' '0.5' '60 billion''with' 'in' 'to' 'of'' I ' 'my' 'me' your' 'us''up' 'to (to V)' 'down' out''centimeter' 'percent' '%' 'mm' 'dollar''grow' delay' 'feed' go' 'went' gone',give' gave' 'given'177
