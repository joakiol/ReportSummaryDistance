Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 30?34,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsIs it Worth Submitting this Run?Assess your RTE System with a Good Sparring PartnerMilen KouylekovCELI s.r.l.Turin, Italykouylekov@celi.itYashar MehdadFBK-irst and University of TrentoTrento, Italymehdad@fbk.euMatteo NegriFBK-irstTrento, Italynegri@fbk.euAbstractWe address two issues related to the devel-opment of systems for Recognizing TextualEntailment.
The first is the impossibility tocapitalize on lessons learned over the differentdatasets available, due to the changing natureof traditional RTE evaluation settings.
Thesecond is the lack of simple ways to assessthe results achieved by our system on a giventraining corpus, and figure out its real potentialon unseen test data.
Our contribution is the ex-tension of an open-source RTE package withan automatic way to explore the large searchspace of possible configurations, in order toselect the most promising one over a givendataset.
From the developers?
point of view,the efficiency and ease of use of the system,together with the good results achieved on allprevious RTE datasets, represent a useful sup-port, providing an immediate term of compar-ison to position the results of their approach.1 IntroductionResearch on textual entailment (TE) has received astrong boost by the Recognizing Textual Entailment(RTE) Challenges, organized yearly to gather thecommunity around a shared evaluation framework.Within such framework, besides the intrinsic diffi-culties of the task (i.e.
deciding, given a set of Text-Hypothesis pairs, if the hypotheses can be inferredfrom the meaning of the texts), the development ofRTE systems has to confront with a number of ad-ditional problems and uncertainty factors.
First ofall, since RTE systems are usually based on com-plex architectures that integrate a variety of tools andresources, it is per se very difficult to tune them anddefine the optimal configuration given a new dataset.In general, when participating to the evaluation chal-lenges there?s no warranty that the submitted runsare those obtained with the best possible configura-tion allowed by the system.
Second, the evaluationsettings change along the years.
Variations in thelength of the texts, the origin of the pairs, the bal-ance between positive and negative examples, andthe type of entailment decisions allowed, reflect theneed to move from easier and more artificial settingsto more complex and natural ones.
However, in con-trast with other more stable tasks in terms of eval-uation settings and metrics (e.g.
machine transla-tion), such changes make it difficult to capitalize onthe experience obtained by participants throughoutthe years.
Third, looking at RTE-related literatureand the outcomes of the six campaigns organised sofar, the conclusions that can be drawn are often con-troversial.
For instance, it is not clear whether theavailability of larger amounts of training data corre-lates with better performance (Hickl et al, 2006) ornot (Zanzotto et al, 2007; Hickl and Bensley, 2007),even within the same evaluation setting.
In addi-tion, ablation tests carried out in recent editions ofthe challenge do not allow for definite conclusionsabout the actual usefulness of tools and resources,even the most popular ones (Bentivogli et al, 2009).Finally, the best performing systems often have dif-ferent natures from one year to another, showing al-ternations of deep (Hickl and Bensley, 2007; Tatuand Moldovan, 2007) and shallow approaches (Jiaet al, 2010) ranked at the top positions.
In lightof these considerations, it would be useful for sys-30tems developers to have: i) automatic ways to sup-port systems?
tuning at a training stage, and ii) reli-able terms of comparison to validate their hypothe-ses, and position the results of their work before sub-mitting runs for evaluation.
In this paper we addressthese needs by extending an open-source RTE pack-age (EDITS1) with a mechanism that automatizesthe selection of the most promising configurationover a training dataset.
We prove the effectivenessof such extension showing that it allows not only toachieve good performance on all the available RTEChallenge datasets, but also to improve the officialresults, achieved with the same system, through adhoc configurations manually defined by the devel-opers team.
Our contribution is twofold.
On oneside, in the spirit of the collaborative nature of opensource projects, we extend an existing tool with auseful functionality that was still missing.
On theother side, we provide a good ?sparring partner?
forsystem developers, to be used as a fast and free termof comparison to position the results of their work.2 ?Coping?
with configurabilityEDITS (Kouylekov and Negri, 2010) is an opensource RTE package, which offers a modular, flex-ible, and adaptable working environment to experi-ment with the RTE task over different datasets.
Thepackage allows to: i) create an entailment engineby defining its basic components (i.e.
algorithms,cost schemes, rules, and optimizers); ii) train suchentailment engine over an annotated RTE corpus tolearn a model; and iii) use the entailment engine andthe model to assign an entailment judgement and aconfidence score to each pair of an un-annotated testcorpus.
A key feature of EDITS is represented by itshigh configurability, allowed by the availability ofdifferent algorithms, the possibility to integrate dif-ferent sets of lexical entailment/contradiction rules,and the variety of parameters for performance opti-mization (see also Mehdad, 2009).
Although config-urability is per se an important aspect (especially foran open-source and general purpose system), thereis another side of the coin.
In principle, in order toselect the most promising configuration over a givendevelopment set, one should exhaustively run a hugenumber of training/evaluation routines.
Such num-1http://edits.fbk.eu/ber corresponds to the total number of configura-tions allowed by the system, which result from thepossible combinations of parameter settings.
Whendealing with enlarging dataset sizes, and the tighttime constraints usually posed by the evaluationcampaigns, this problem becomes particularly chal-lenging, as developers are hardly able to run exhaus-tive training/evaluation routines.
As recently shownby the EDITS developers team, such situation re-sults in running a limited number of experimentswith the most ?reasonable?
configurations, whichconsequently might not lead to the optimal solution(Kouylekov et al, 2010).The need of a mechanism to automatically ob-tain the most promising solution on one side, andthe constraints posed by the evaluation campaignson the other side, arise the necessity to optimizethis procedure.
Along this direction, the objectiveis good a trade-off between exhaustive experimen-tation with all possible configurations (unfeasible),and educated guessing (unreliable).
The remainderof this section tackles this issue introducing an op-timization strategy based on genetic algorithms, anddescribing its adaptation to extend EDITS with thenew functionality.2.1 Genetic algorithmGenetic algorithms (GA) are well suited to effi-ciently deal with large search spaces, and have beenrecently applied with success to a variety of opti-mization problems and specific NLP tasks (Figueroaand Neumann, 2008; Otto and Riff, 2004; Aycinenaet al, 2003).
GA are a direct stochastic method forglobal search and optimization, which mimics natu-ral evolution.
To this aim, they work with a popu-lation of individuals, representing possible solutionsto the given task.
Traditionally, solutions are rep-resented in binary as strings of 0s and 1s, but otherencodings (e.g.
sequences of real values) are possi-ble.
The evolution usually starts from a populationof randomly generated individuals, and at each gen-eration selects the best-suited individuals based ona fitness function (which measures the optimality ofthe solution obtained by the individual).
Such selec-tion is then followed by modifications of the selectedindividuals obtained by recombining (crossover) andperforming random changes (mutation) to form anew population, which will be used in the next iter-31ation.
Finally, the algorithm is terminated when themaximum number of generations, or a satisfactoryfitness level has been reached for the population.2.2 EDITS-GAOur extension to the EDITS package, EDITS-GA,consists in an iterative process that starts with aninitial population of randomly generated configura-tions.
After a training phase with the generated con-figurations, the process is evaluated by means of thefitness function, which is manually defined by theuser2.
This measure is used by the genetic algo-rithm to iteratively build new populations of config-urations, which are trained and evaluated.
This pro-cess can be seen as the combination of: i) a microtraining/evaluation routine for each generated con-figuration of the entailment engine; and ii) a macroevolutionary cycle, as illustrated in Figure 1.
Thefitness function is an important factor for the evalu-ation and the evolution of the generated configura-tions, as it drives the evolutionary process by deter-mining the best-suited individuals used to generatenew populations.
The procedure to estimate and op-timize the best configuration applying the GA, canbe summarized as follows.
(1) Initialization: generate a random initial popula-tion (i.e.
a set of configurations).
(2) Selection:2a.
The fitness function (accuracy, or F-measure)is evaluated for each individual in the population.2b.
The individuals are selected according to theirfitness function value.
(3) Reproduction: generate a new population ofconfigurations from the selected one, through ge-netic operators (cross-over and mutation).
(4) Iteration: repeat the Selection and Reproductionuntil Termination.
(5) Termination: end if the maximum number ofiterations has been reached, or the population hasconverged towards a particular solution.In order to extend EDITS with genetic algo-rithms, we used a GA implementation available inthe JGAP tool3.
In our settings, each individual con-tains a sequence of boolean parameters correspond-2For instance, working on the RTE Challenge ?Main?
taskdata, the fitness function would be the accuracy for RTE1 toRTE5, and the F-measure for RTE6.3http://jgap.sourceforge.net/Figure 1: EDITS-GA framework.ing to the activation/de-activation of the system?sbasic components (algorithms, cost schemes, rules,and optimizers).
The configurations correspondingto such individuals constitute the populations itera-tively evaluated by EDITS-GA on a given dataset.3 ExperimentsOur experiments were carried out over the datasetsused in the six editions of the RTE Challenge(?Main?
task data from RTE1 to RTE6).
For eachdataset we obtained the best model by trainingEDITS-GA over the development set, and evaluat-ing the resulting model on the test pairs.
To thisaim, the optimization process is iterated over allthe available algorithms in order to select the bestcombination of parameters.
As termination crite-rion, we set to 20 the maximum number of itera-tions.
To increase efficiency, we extended EDITSto pre-process each dataset using the tokenizer andstemmer available in Lucene4.
This pre-processingphase is automatically activated when the EDITS-GA has to process non-annotated datasets.
How-ever, we also annotated the RTE corpora with theStanford parser plugin (downloadable from the ED-ITS websitein order to run the syntax-based algo-rithms available (e.g.
tree edit distance).
The num-ber of boolean parameters used to generate the con-figurations is 18.
In light of this figure, it becomesevident that the number of possible configurationsis too large (218=262,144) for an exhaustive train-ing/evaluation routine over each dataset5.
However,4http://lucene.apache.org/5In an exploratory experiment we measured in around 4days the time required to train EDITS, with all possible con-32# Systems Best Lowest Average EDITS (rank) EDITS-GA (rank) % Impr.
Comp.
TimeRTE1 15 0.586 0.495 0.544 0.559 (8) 0.5787 (3) +3.52% 8m 24sRTE2 23 0.7538 0.5288 0.5977 0.605 (6) 0.6225 (5) +2.89% 9m 8sRTE3 26 0.8 0.4963 0.6237 - 0.6875 (4) - 9mRTE4 26 0.746 0.516 0.5935 0.57 (17) 0.595 (10) +4.38% 30m 54sRTE5 20 0.735 0.5 0.6141 0.6017 (14) 0.6233 (9) +3.58% 8m 23sRTE6 18 0.4801 0.116 0.323 0.4471 (4) 0.4673 (3) +4.51% 1h 54m 20sTable 1: RTE results (acc.
for RTE1-RTE5, F-meas.
for RTE6).
For each participant, only the best run is considered.with an average of 5 reproductions on each iteration,EDITS-GA makes an average of 100 configurationsfor each algorithm.
Thanks to EDITS-GA, the aver-age number of evaluated configurations for a singledataset is reduced to around 4006.Our results are summarized in Table 1, showingthe total number of participating systems in eachRTE Challenge, together with the highest, lowest,and average scores they achieved.
Moreover, the of-ficial results obtained by EDITS are compared withthe performance achieved with EDITS-GA on thesame data.
We can observe that, for all datasets,the results achieved by EDITS-GA significantly im-prove (up to 4.51%) the official EDITS results.
It?salso worth mentioning that such scores are alwayshigher than the average ones obtained by partici-pants.
This confirms that EDITS-GA can be poten-tially used by RTE systems developers as a strongterm of comparison to assess the capabilities oftheir own system.
Since time is a crucial factor forRTE systems, it is important to remark that EDITS-GA allows to converge on a promising configura-tion quite efficiently.
As can be seen in Table 1,the whole process takes around 9 minutes7 for thesmaller datasets (RTE1 to RTE5), and less than 2hours for a very large dataset (RTE6).
Such timeanalysis further proves the effectiveness of the ex-tended EDITS-GA framework.
For the sake of com-pleteness we gave a look at the differences betweenthe ?educated guessing?
done by the EDITS de-velopers for the official RTE submissions, and the?optimal?
configuration automatically selected byEDITS-GA.
Surprisingly, in some cases, even a mi-nor difference in the selected parameters leads tofigurations, over small datasets (RTE1 to RTE5).6With these settings, training EDITS-GA over small datasets(RTE1 to RTE5) takes about 9 minutes each.7All time figures are calculated on an Intel(R) Xeon(R),CPU X3440 @ 2.53GHz, 8 cores with 8 GB RAM.significant gaps in the results.
For instance, in RTE6dataset, the ?guessed?
configuration (Kouylekov etal., 2010) was based on the lexical overlap algo-rithm, setting the cost of replacing H terms with-out an equivalent in T to the minimal Levenshteindistance between such words and any word in T.EDITS-GA estimated, as a more promising solution,a combination of lexical overlap with a different costscheme (based on the IDF of the terms in T).
In ad-dition, in contrast with the ?guessed?
configuration,stop-words filtering was selected as an option, even-tually leading to a 4.51% improvement over the of-ficial RTE6 result.4 Conclusion?Is it worth submitting this run?
?,?How good is mysystem??.
These are the typical concerns of systemdevelopers approaching the submission deadline ofan RTE evaluation campaign.
We addressed these is-sues by extending an open-source RTE system witha functionality that allows to select the most promis-ing configuration over an annotated training set.
Ourcontribution provides developers with a good ?spar-ring partner?
(a free and immediate term of compar-ison) to position the results of their approach.
Ex-perimental results prove the effectiveness of the pro-posed extension, showing that it allows to: i) achievegood performance on all the available RTE datasets,and ii) improve the official results, achieved with thesame system, through ad hoc configurations manu-ally defined by the developers team.AcknowledgmentsThis work has been partially supported by the EC-funded projects CoSyne (FP7-ICT-4-24853), andGalateas (CIP-ICT PSP-2009-3-250430).33ReferencesMargaret Aycinena, Mykel J. Kochenderfer, and DavidCarl Mulford.
2003.
An Evolutionary Approach toNatural Language Grammar Induction.
Stanford CS224N Natural Language Processing.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, Bernardo Magnini.
2009.
The FifthPASCAL Recognizing Textual Entailment Challenge.Proceedings of the TAC 2009 Workshop.Ido Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
Proceedings of the PASCAL Workshop ofLearning Methods for Text Understanding and Min-ing.Alejandro G. Figueroa and Gu?nter Neumann.
2008.
Ge-netic Algorithms for Data-driven Web Question An-swering.
Evolutionary Computation 16(1) (2008) pp.89-125.Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recog-nizing Textual Entailment with LCCs Groundhog Sys-tem.
Proceedings of the Second PASCAL ChallengesWorkshop.Andrew Hickl and Jeremy Bensley.
2007.
A discoursecommitment-based framework for recognizing textualentailment.
Proceedings of the ACL-PASCAL Work-shop on Textual Entailment and Paraphrasing.Houping Jia, Xiaojiang Huang, Tengfei Ma, XiaojunWan, and Jianguo Xiao.
2010.
PKUTM Participationat TAC 2010 RTE and Summarization Track.
Proceed-ings of the Sixth Recognizing Textual Entailment Chal-lenge.Milen Kouylekov and Matteo Negri.
2010.
An Open-source Package for Recognizing Textual Entailment.Proceedings of ACL 2010 Demo session.Milen Kouylekov, Yashar Mehdad, Matteo Negri, andElena Cabrio.
2010.
FBK Participation in RTE6:Main and KBP Validation Task.
Proceedings of theSixth Recognizing Textual Entailment Challenge.Yashar Mehdad 2009.
Automatic Cost Estimation forTree Edit Distance Using Particle Swarm Optimiza-tion.
Proceedings of ACL-IJCNLP 2009.Eridan Otto and Mar?
?a Cristina Riff 2004.
Towards anefficient evolutionary decoding algorithm for statisti-cal machine translation.
LNAI, 2972:438447..Marta Tatu and Dan Moldovan.
2007.
COGEX at RTE3.Proceedings of the ACL-PASCAL Workshop on TextualEntailment and Paraphrasing.Fabio Massimo Zanzotto, Marco Pennacchiotti andAlessandro Moschitti.
2007.
Shallow Semantics inFast Textual Entailment Rule Learners.
Proceedingsof the Third Recognizing Textual Entailment Chal-lenge.34
