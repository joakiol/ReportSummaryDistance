A Chinese Word Segmentation System Based on StructuredSupport Vector Machine Utilization of Unlabeled Text CorpusChongyang ZhangAnhui ProvinceEngineering Laboratoryof Speech and Language,University of Science andTechnology of Chinacyzhang9@mail.ustc.edu.cnZhigang ChenAnhui ProvinceEngineering Laboratoryof Speech and Language,University of Science andTechnology of ChinaChenzhigang@ustc.eduGuoping HuAnhui ProvinceEngineering Laboratoryof Speech and Language,University of Science andTechnology of ChinaApplecore@ustc.edu?
?AbstractCharacter-based tagging method hasachieved great success in Chinese WordSegmentation (CWS).
This paperproposes a new approach to improve theCWS tagging accuracy by structuredsupport vector machine (SVM)utilization of unlabeled text corpus.
First,character N-grams in unlabeled textcorpus are mapped into low-dimensionalspace by adopting SOM algorithm.
Thennew features extracted from these mapsand another kind of feature based onentropy for each N-gram are integratedinto the structured SVM methods forCWS.
We took part in two tracks of theWord Segmentation for SimplifiedChinese Text in bakeoff-2010: Closedtrack and Open track.
The test corporacover four domains: Literature,Computer Science, Medicine andFinance.
Our system achieved goodperformance, especially in the opentrack on the domain of medicine, oursystem got the highest score among 18systems.1 IntroductionIn the last decade, many statistics-basedmethods for automatic Chinese wordsegmentation (CWS) have been proposed withdevelopment of machine learning and statisticalmethod (Huang and Zhao, 2007).
Especially,character-based tagging method which wasproposed by Nianwen Xue (2003) achievesgreat success in the second InternationalChinese word segmentation Bakeoff in 2005(Low et al, 2005).
The character-based taggingmethod formulates the CWS problem as a taskof predicting a tag for each character in thesentence, i.e.
every character is considered asone of four different types in 4-tag set: B (beginof word), M (middle of word), E (end of word),and S (single-character word).Most of these works train tagging modelsonly on limited labeled training sets, withoutusing any unsupervised learning outcomes fromunlabeled text.
But in recent years, researchersbegin to exploit the value of enormousunlabeled corpus for CWS, such as somestatistics information on co-occurrence of sub-sequences in the whole text has been extractedfrom unlabeled data and been employed as inputfeatures for tagging model training (Zhao andKit , 2007).Word clustering is a common method toutilize unlabeled corpus in language processingresearch to enhance the generalization ability,such as part-of-speech clustering and semanticclustering (Lee et al, 1999 and B Wang and HWang 2006).
Character-based tagging methodusually employs N-gram features, where an N-gram is an N-character segment of a string.
Webelieve that there are also semantic orgrammatical relationships between most of N-grams and these relationships will be useful inCWS.
Intuitively, assuming the training datacontains the bigram ??
/?
?
(The last twocharacters of the word ?Israel?
in Chinese), notcontain the bigram ??
/?
?
(The last twocharacters of the word ?Turkey?
in Chinese), ifwe could cluster the two bigrams togetheraccording to unlabeled corpus and employ it asa feature for supervised training of taggingmodel, then maybe we will know that thereshould be a word boundary after ??/??
thoughwe only find the existence of word boundaryafter ??
/?
?
in the training data.
So weinvestigate how to apply clustering method ontounlabeled data for the purpose of improvingCWS accuracy in this paper.This paper proposes a novel method ofusing unlabeled data for CWS, which employsSelf-Organizing Map (SOM) (Kohonen 1982)to organize Chinese character N-grams on atwo-dimensional array, named as ?N-gramcluster map?
(NGCM), in which the characterN-grams similar in grammatical structure andsemantic meaning are organized in the same oradjacent position.
Two different arrays are builtbased the N-gram?s preceding context andsucceeding context respectively becausenormally N-gram is just part of Chinese wordand doesn?t share similar preceding andsucceeding context in the same time.
ThenNGCM-based features are extracted and appliedto tagging model of CWS.
Another kind offeature based on entropy for each N-gram isalso introduced for improving the performanceof CWS.The rest of this paper is organized asfollows: Section 2 describes our system; Section3 describes structured SVM and the featureswhich are obtained from labeled corpus and alsounlabeled corpus; Section 4 shows experimentalresults on Bakeoff-2010 and Section 5 gives ourconclusion.2 System description2.1 Open track:The architecture of our system for open track isshown in Figure 1.
For improving the cross-domain performance, we train and test withdictionary-based word segmentation outputs.On large-scale unlabeled corpus we use Self-Organizing Map (SOM) (Kohonen 1982) toorganize Chinese character N-grams on a two-dimensional array, named as ?N-gram clustermap?
(NGCM), in which the character N-gramssimilar in grammatical structure and semanticmeaning are organized in the same or adjacentposition.
Then new features are extracted fromthese maps and integrated into the structuredSVM methods for CWS.The large-scaleunlabeled corpustest textSomDictionary BasedCWSNGCMStructured SVM ModelResultsTraining textLabeleddataDictionary BasedCWSLabeleddata?Figure 1: Open track system?2.2 Closed track:Training text test textSom StatisticNGCM EntropyStructured SVM ModelResultsFigure 2: closed track systemBecause the large-scale unlabeled corpus isforbidden to be used on closed track.
We trainedthe SOM only on the data provided byorganizers.
To make up for the deficiency of thesparse data on SOM, we add entropy-basedfeatures (ETF) for every N-gram to structuredSVM model.
The architecture of our system forclose track is shown in Figure 2.3 Learning algorithm3.1 Structured support vector machineThe structured support vector machine can learnto predict structured y , such as trees sequencesor sets, from x  based on large-margin approach.We employ a structured SVM that can predict asequence of labels 1( ,..., )Ty y y=  for a givenobservation sequences?
1( ,..., )Tx x x= , wherety ??
,?
is the label set for y.There are two types of features in thestructured SVM: transition features (interactionsbetween neighboring labels along the chain),emission features (interactions betweenattributes of the observation vectors and aspecific label).we can represent the input-outputpairs via joint feature map (JFM)1111( ) ( )( , )( ) ( )Tt c ttTc t c ttx yx yy y???=?
+=?
????
??
?= ?
??
???
??
??
?where{1 21 2,1,0,( ) ( ( , ), ( , ),..., ( , )) '{0,1} ,  { , ,..., }Kronecker delta  ,cKKKi ji ji jy y y y y y yy y y y?
?
??
?
=???
??
?
==( )x?
denotes an arbitrary feature representationof the inputs.
The sign " "?
expresses tensorproduct defined as ?
: d kR R?
dkR?
,[ ] ( 1)i j da b + ??
[ ] [ ]i ja b= .
T  is the length of anobservation sequence.
0?
?
is a scaling factorwhich balances the two types of contributions.
?Note that both transition features andemission features can be extended by includinghigher-order interdependencies of labels (e.g.1 2( ) ( ) ( )c t c t c ty y y+ +?
??
??
),by includinginput features from a window centered at thecurrent position (e.g.
replacing ( )tx?
with( ,..., ,... )t r t t rx x x?
?
+ )or by combining higher-order output features with input features (e.g.1( ) ( ) ( )t c t c ttx y y?
+??
???
)The w-parametrized discriminant function:F X Y R?
?
interpreted as measuring thecompatibility of x and y is defined as:( , ; ) , ( , )F x y w w x y?=So we can maximize this function over theresponse variable to make a prediction( ) arg max ( , , )y Yf x F x y w?=Training the parameters can be formulatedas the following optimization problem.,11min ,2. .
, :, ( , ) ( , ) ( , )niwii i i i i i iCw wns t i y Yw x y x y y y?
??
?
?=+?
?
??
?
?
?
?where n  is the number of the training set, i?
isa slack variable , 0C ?
is a constantcontrolling the tradeoff between training errorminimization and margin maximization,1( , )y y?
is the loss function ,usually thenumber of misclassified tags in the sentence.3.2 Features set for tagging modelFor a training sample denoted as1( ,..., )Tx x x=  and 1( ,..., )Ty y y= .
We chosefirst-order interdependencies of labels to betransition features, and dependencies betweenlabels and N-grams (n=1, 2, 3, 4) at currentposition in observed input sequence to beemission features.So our JFM is the concatenation of thefollow vectors111( ) ( )Tc t c tty y?
+=?
??
?1( ) ( ), { 1,0,1}Tt m c ttx y m?
+=??
?
?
?11( ) ( ), { 2, 1,0,1}Tt m t m c ttx x y m?
+ + +=??
?
?
?
?1 11( ) ( ),{ 2, 1,0,1,2}Tt m t m t m c ttx x x ym?
+ ?
+ + +=???
?
??
?1 1 21( ) ( ),{ 3, 2, 1,0,1,2}Tt m t m t m t m c ttx x x x ym?
+ ?
+ + + + +=???
?
?
??
?Figure 3 shows the transition features andthe emission features of N-grams (n=1, 2) at 3y .The emission features of 3-grams and 4-gramsare not shown here because of the large numberof the dependencies.1y 2y 3y 4y1x 2x 3x 4x5y5xFigure 3: the transition features and theemission features at 3y  for structured SVM?3.3 SOM-based N-gram cluster mapsand the NGCM mapping featureThe Self-Organizing Map (SOM) (Kohonen1982), sometimes called Kohonen map, wasdeveloped by Teuvo Kohonen in the early1980s.Self-organizing semantic maps (Ritter andKohonen 1989, 1990) are SOMs that have beenorganized according to word similarities,measured by the similarity of the short contextsof the words.
Our algorithm of building N-gramcluster maps is similar to self-organizingsemantic maps.
Because normally N-gram isjust part of Chinese word and do not sharesimilar preceding and succeeding context in thesame time, so we build two different mapsaccording to the preceding context and thesucceeding context of N-gram individually.
Inthe end we build two NGCMs: NGCMP(NGCM according to preceding context) andNGCMS (NGCM according to succeedingcontext).Due to the limitation of our computer andtime we only get two 15 15?
size 2GCMs foropen track system from large-scale unlabeledcorpus which was obtained easily from websiteslike Sohu, Netease, Sina and People Daily.The 2GCMP and 2GCMS we got for theopen track task are shown in Figure 4 andFigure 5 respectively.0,00,1 1,1 2,10,2 1,2 2,21,0 2,014,1414,214,00,14 1,14 2,1414,1?/??/??/??/???/??/??/??/??/???/??/??/??/???/??/??/??/?
?Figure 4: 2GCMP0,00,1 1,1 2,10,2 1,2 2,21,0 2,014,1414,214,00,14 1,14 2,1414,1?/??/??/??/??/???/??/??/??/???/??/??/??/?...?/??/??/?
?/?...Figure 5: 2GCMSAfter checking the results, we find that the2GCMS have following characters:1) most ofthe meaningless bigrams that contain charactersfrom more than one word, such as the bigram "??"
in "...????..."
, are organized into thesame neurons in the map, 2) most of the first orlast bigrams of the country names are organizedinto a few adjacent neurons, such as ??/??,??/?
?, ??/??
and ??/?
?in 2GCMS , ??/?
?, ??/?
?, ??/?
?, ??/??
, and ??/??
in2GCMP.Two 20 1?
size 2GCMs are trained for theclosed track system only on the data providedby organizers.
The results are not as good as theresults of the 15 15?
size 2GCMs because ofthe less training data.
The second characterdescribed above is no longer apparent as well asthe 15 15?
size 2GCMs, but it still kept the firstcharacter.Then we adopt the position of the neuronswhich current N-gram mapped in the NGCM asa new feature.
So every feature has Ddimensions (D equals to the dimension of theNGCM, every dimension is corresponding tothe coordinate value in the NGCM).
In this way,N-gram which is originally represented as ahigh dimensional vector based on its context ismapped into a very low-dimensional space.
Wecall it NGCM mapping feature.
So our previousJFM in section 3.2 is concatenated with thefollowing features:2GCMS 11( ) ( ), { 2, 1}Tt m t m c ttx x y m?
+ + +=??
?
?
?
?2GCMP 11( ) ( ), {0,1}Tt m t m c ttx x y m?
+ + +=??
?
?2GCMS 11( ) ( ), { 2, 1}Tt m t m c ttx x y m?
+ + +=??
?
?
?
?2GCMP 11( ) ( ), {0,1}Tt m t m c ttx x y m?
+ + +=??
?
?where 2GCMS( )x?
and 2GCMP ( )x?2{0,1,...,14}?
denote the NGCM mappingfeature from 2GCMS and 2GCMP respectively.NGCM ( )x?
denotes the quantization error  ofcurrent N-gram x on its NGCM.As an example, the process of import featuresfrom NGCMs at 3y  is presented in Figure 6.1y 2y 3y 4y1x 2x 3x 4x5y5x2GCMS 2GCMPFigure 6: Using 2GCMS and 2GCMP as inputto structured SVM3.4 Entropy-based featuresOn closed track, the entropy of the precedingand succeeding characters conditional on the N-gram and also the self-information of the N-gram are used as features for the structuredSVM methods.
Then our previous JFM insection 3.2 is concatenated with the followingfeatures:11 2 1 2 3 1 2 3 4( | ) ( ),{ , , }TNgram c ttNgram t t t t t t t t tH P N x yx x x x x x x x x x=+ + + + + + + + += ????
?12 1 3 2 1 4 3 2 11( | ) ( ),{ , , }( ) ( )TNgram c ttNgram t t t t t t t t tTNgram c ttH S N x yx x x x x x x x x xI N x y=?
?
?
?
?
?
?
?
?== ??
?= ???
?all the ngrams used in section 3.2Ngramx ?Where P and S  denote the set of the precedingand succeeding characters respectively.
Theentropy: ( | )NgramH X N x= =( | ) log ( | )tt Ngram t NgramX xp x x p x x??
?The self-information of the N-gram NgramN x= :( ) log ( )Ngram NgramI x p x= ?4 Applications and Experiments4.1 Text PreprocessingText is usually mixed up with numerical oralphabetic characters in Chinese naturallanguage, such as ???
office ?????
9??.
These numerical or alphabetic charactersare barely segmented in CWS.
Hence, we treatthese symbols as a whole ?character?
accordingto the following two preprocessing steps.
Firstreplace one alphabetic character to fourcontinuous alphabetic characters with E1 to E4respectively, five or more alphabetic characterswith E5.
Then replace one numerical number tofour numerical numbers with N1 to N4 and fiveor more numerical numbers with N5.
After textpreprocessing, the above examples will be ???
E5?????
N1?
?.4.2 Character-based tagging methodfor CWSPrevious works show that 6-tag set achieveda better CWS performance (Zhao et al,2006).
Thus, we opt for this tag set.
This 6-tag set adds ?B2?
and ?B3?
to 4-tag setwhich stand for the type of the second andthe third character in a Chinese wordrespectively.
For example, the tag sequencefor the sentence ??????/?/??/??
(Shanghai World Expo / will / last / sixmonths)?
will be ?B B2 B3 M E S B E B E?.4.3 Results in the bakeoff-2010We use hmmsvm  version 3.1 to build ourstructured SVM models.
The cut-off threshold isset to 2.
The precision parameter is set to 0.1.The tradeoff between training errorminimization and margin maximization is set to1000.We took part in two tracks of the WordSegmentation for Simplified Chinese Text inbakeoff-2010: c (Closed track), o (Open track).The test corpora cover four domains: A(Literature), B (Computer Science), C(Medicine), D (Finance).Precision(P),Recall(R),F-measure(F),Out-Of-Vocabulary Word Recall(OOV RR) and In-Vocabulary Word Recall(IV RR) are adopted tomeasure the performance of word segmentationsystem.Table 1 shows the results of our system onthe word segmentation task for simplifiedChinese text in bakeoff-2010.
Table 2 shows thecomparision between our system results andbest results in bakeoff-2010.Table 1: The results of our systemsTabel 2: The comparision between our systemresults and best results in bakeoff-2010It is obvious that our systems are stable andreliable even in the domain of medicine whenthe F-measure of the best results was decreased.Our open track system performs better thanclosed track system, demonstrating the benefitof the dictionary-based word segmentationoutputs and the NGCMs which are training onlarge-scale unlabeled corpus.5 ConclusionThis paper proposes a new approach to improvethe CWS tagging accuracy by structured supportvector machine (SVM) utilization of unlabeledtext corpus.
We use SOM to organize Chinesecharacter N-grams on a two-dimensional array,so that the N-grams similar in grammaticalstructure and semantic meaning are organized inthe same or adjacent position.
Then newfeatures extracted from these maps and anotherkind of feature based on entropy for each N-gram are integrated into the structured SVMmethods for CWS.
Our system achieved goodperformance, especially in the open track on thedomain of medicine, our system got thehighest score among 18 systems.In future work, we will try to organizing allthe N-grams on a much larger array, so thatevery neuron will be labeled by a single N-gram.The ultimate objective is to reduce thedimension of input features for supervised CWSlearning by replacing N-gram features with two-dimensional NGCM mapping features.ReferencesB.Wang, H.Wang 2006.A Comparative Study onChinese Word Clustering.
Computer Processingof Oriental Languages.
Beyond the Orient: TheResearch Challenges Ahead, pages 157-164Chang-Ning Huang and Hai Zhao.
2007.
Chineseword segmentation: A decade review.
Journal ofChinese Information Processing, 21(3):8?20.Chung-Hong Lee & Hsin-Chang Yang.1999, A WebText Mining Approach Based on Self-OrganizingMap, ACM-libraryG.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B.Taskar, and S. V. N. Vishwanathan, editors.
2007Predicting Structured Data.
MIT Press,Cambridge, Massachusetts.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu.
2006.
Effective tag set selectionR P F1 OOV RR IV RRA c 0.932 0.935 0.933 0.654 0.953o 0.942 0.943 0.942 0.702 0.959B c 0.935 0.934 0.935 0.792 0.961o 0.948 0.946 0.947 0.812 0.973C c 0.937 0.934 0.936 0.761 0.959o 0.941 0.935 0.938 0.787 0.96D c 0.955 0.956 0.955 0.848 0.965o 0.948 0.955 0.951 0.853 0.957F1(Bakeoff-2010) F1(Our system)A c 0.946 0.933o 0.955 0.942B c 0.951 0.935o 0.95 0.947C c 0.939 0.936o 0.938 0.938D c 0.959 0.955o 0.96 0.951inChinese word segmentation via conditionalrandom field modeling.
In Proceedings ofPACLIC-20, pages 87?94.
Wuhan, China.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.Animproved Chinese word segmentation system withconditional random field.
In SIGHAN-5, pages162?165, Sydney, Australia, July 22-23.Hai Zhao and Chunyu Kit.
2007.
Incorporatingglobal information into supervised learning forChinese word segmentation.
In PACLING-2007,pages 66?74, Melbourne,Australia, September 19-21.H.Ritter, and T.Kohonen, 1989.
Self-organizingsemantic maps.
Biological Cybernetics, vol.
61,no.
4, pp.
241-254.I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun.2005.
Large Margin Methods for Structured andInterdependent Output Variables, Journal ofMachine Learning Research (JMLR),6(Sep):1453-1484.Jin Kiat Low, Hwee Tou Ng, and WenyuanGuo.2005.
A maximum entropy approach toChinese word segmentation.
In Proceedings of theFourth SIGHAN Workshop on Chinese LanguageProcessing, pages 161?164.
Jeju Island,Korea.J.Lafferty,A.McCallum, F.Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.
InProceedings of the International Conference onMachine Learning (ICML).
San Francisco:Morgan Kaufmann Publishers, 282?289.Nianwen Xue and Susan P.
Converse., 2002,Combining Classifiers for Chinese WordSegmentation, In Proceedings of First SIGHANWorkshop on Chinese Language Processing.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.R.Sproat and T.Emerson.
2003.The firstinternational Chinese word segmentation bakeoff.In The Second SIGHAN Workshop on ChineseLanguage Processing, pages 133?143.Sapporo,Japan.S.Haykin, 1994.
Neural Networks: A ComprehensiveFoundation.
NewYork: MacMillan.T.Joachims, T.Finley, Chun-Nam Yu.
2009, Cutting-Plane Training of Structural SVMs, MachineLearning Journal,77(1):27-59.T.Joachims.
2008 .hmmsvm  Sequence Tagging withStructural Support Vector Machines,http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.htmlT.Honkela, 1997.
Self-Organizing Maps in NaturalLanguage Processing.
PhD thesis, HelsinkiUniversity of Technology, Department ofComputer Science and Engineering, Laboratory ofComputer and Information Science.T.Kohonen.
1982.Self-organized formation oftopologically correct feature maps.
BiologicalCybernetics, 43, pp.
59-69.T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen,1996 ,SOM_PAK: The Self-Organizing MapProgram Package,Technical Report A31,Helsinki University of Technology ,http://www.cis.hut.fi/nnrc/nnrc-programs.htmlY.Altun, I.Tsochantaridis, T.Hofmann.
2003.
HiddenMarkov Support Vector Machines.
In Proceedingsof International Conference on Machine Learning(ICML).
