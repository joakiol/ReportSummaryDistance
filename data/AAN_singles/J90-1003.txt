WORD ASSOCIATION NORMS, \] /IUTUAL INFORMATION,AND LEXICOGRAPHYKenneth  Ward  ChurchBell Laboratories Murray Hill, N.J.Patrick HanksCollins Publishers Glasgow, ScotlandThe term word association is used in a very particular sense in the psycholinguistic literature.
(Generallyspeaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word suchas doctor.)
We will extend the term to provide the basis for a statistical description of a variety of interestinglinguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word)to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).This paper will propose an objective measure based on the information theoretic notion of mutualinformation, for estimating word association orms from computer eadable corpora.
(The standard methodof obtaining word association orms, testing a few thousand :mbjects on a few hundred words, is both costlyand unreliable.)
The proposed measure, the association ratio, estimates word association norms directlyfrom computer eadable corpora, making it possible to estimate norms for tens of thousands of words.1 MEANING AND ASSOCIATIONIt is common practice in linguistics to classify words notonly on the basis of their meanings but also on the basis oftheir co-occurrence with other words.
Running through thewhole Firthian tradition, for example, is the theme that"You shall know a word by the company it keeps" (Firth,1957).On the one hand, bank co-occurs with words and expres-sion such as money, notes, loan, account, investment,clerk, official, manager, robbery, vaults, working in a,its actions, First National, of England, and so forth.
Onthe other hand, we find bank co-occurring with river,swim, boat, east (and of course West and South, whichhave acquired special meanings of their own), on top ofthe, and of the Rhine.
(Hanks 1987, p. 127)The search for increasingly delicate word classes is not new.In lexicography, for example, it goes back at least to the"verb patterns" described in Hornby's Advanced Learner'sDictionary (first edition 1948).
What is new is that facili-ties for the computational storage and analysis of largebodies of natural anguage have developed significantly inrecent years, so that it is now becoming possible to test andapply informal assertions of this kind in a more rigorousway, and to see what company our words do keep.2 PRACTICAL APPLICATIONSThe proposed statistical description has a large number ofpotentially important applications, including: (a) constrain-ing the language model both for speech recognition andoptical character recognition (OCR), (b) providing disam-biguation cues for parsing highly ambiguous syntactic struc-tures such as noun compounds, conjunctions, and preposi-tional phrases, (c) retrieving texts from large databases(e.g.
newspapers, patents), (d) enhancing the productivityof computational linguists in compiling lexicons of lexico-synWctic facts, and (e) enhancing the productivity of lexi-cographers in identifying normal and conventional usage.Consider the optical character recognizer (OCR) appli-cation.
Suppose that we have an OCR device as in Kahan etal.
(1987), and it has assigned about equal probability tohaving recognized farm and form, where the context iseither: (1) federal c red i t  or (2) some o f .farm?
federal ~form \] credit/farm22 Computational Linguistics Volume 16, Number 1, March 1990Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and LexicographyThe proposed association measure can make use of the factthat farm is much more likely in the first context and formis much more likely in the second to resolve the ambiguity.Note that alternative disambiguation methods based onsyntactic onstraints such as part of speech are unlikely tohelp in this case since both form and farm are commonlyused as nouns.3 WORD ASSOCIATION ANDPSYCHOLINGUISTICSWord association orms are well known to be an importantfactor in psycholinguistic research, especially in the area oflexical retrieval.
Generally speaking, subjects respondquicker than normal to the word nurse if it follows a highlyassociated word such as doctor.Some results and implications are summarized fromreaction-time experiments in which subjects either (a)classified successive strings of letters as words and non-words, or (b) pronounced the strings.
Both types ofresponse to words (e.g.
BUTTER) were consistentlyfaster when preceded by associated words (e.g.
BREAD)rather than unassociated words (e.g.
NURSE) (Meyeret al 1975, p. 98)Much of this psycholinguistic research is based on empiri-cal estimates of word association orms as in Palermo andJenkins (1964), perhaps the most influential study of itskind, though extremely small and somewhat dated.
Thisstudy measured 200 words by asking a few thousand sub-jects to write down a word after each of the 200 words to bemeasured.
Results are reported in tabular form, indicatingwhich words were written down, and by how many subjects,factored by grade level and sex.
The word doctor, forexample, is reported on pp.
98-100 to be most often associ-ated with nurse, followed by sick, health, medicine, hospi-tal, man, sickness, lawyer, and about 70 more words.4 AN INFORMATION THEORETIC MEASUREWe propose an alternative measure, the association ratio,for measuring word association orms, based on the infor-mation theoretic concept of mutual information.
1 Theproposed measure is more objective and less costly than thesubjective method employed in Palermo and Jenkins (1964).The association ratio can be scaled up to provide robustestimates of word association orms for a large portion ofthe language.
Using the association ratio measure, the fivemost associated words are, in order: dentists, nurses, treat-ing, treat, and hospitals.What is "mutual information?"
According to Fano(1961), if two points (words), x and y, have probabilitiesP(x) and P(y), then their mutual information, I(x,y), isdefined to beP(x, y)I(x, y) =- log2 P(x)P(y)Informally, mutual information compares the probabilityof observing x and y together (the joint probability) withthe probabilities of observing x and y independently(chance).
If there is a genuine association between x and y,then the joint probability P(x,y) will be much larger thanchance P(x) P(y), and consequently I(x,y) >> 0.
If  there isno interesting relationship between x and y, then P(x,y)P(x) P(y), and thus, I(x,y) ~ O.
If x and y are in comple-mentary distribution, then P(x,y) will be much less thanP(x) P(y), forcing I(x,y) << 0.In our application, word probabilities P(x) and P(y) areestimated by counting the number of observations of x andy in a corpus, f (x) andf (y ) ,  and normalizing by N, thesize of the corpus.
(Our examples use a number of differentcorpora with different sizes: 15 million words for the 1987AP corpus, 36 million words for the 1988 AP corpus, and8.6 million tokens for the tagged corpus.)
Joint probabili-ties, P(x,y), are estimated by counting the number of timesthat x is followed by y in a window of w words, fw (x,y), andnormalizing by N.The window size parameter allows us to look at differentscales.
Smaller window sizes will identify fixed expressions(idioms such as bread and butter) and other relations thathold over short ranges; larger window sizes will highlightsemantic oncepts and other relationships that hold overlarger scales.Table 1 may help show the contrast.
2 In fixed expres-sions, such as bread and butter and drink and drive, thewords of interest are separated by a fixed number of wordsand there is very little variance.
In the 1988 AP, it wasfound that the two words are always exactly two wordsapart whenever they are found near each other (within fivewords).
That is, the mean separation is two, and thevariance is zero.Compounds also have very fixed word order (little vari-ance), but the average separation is closer to one wordrather than two.
In contrast, relations uch as man/womanare less fixed, as indicated by a larger variance in theirseparation.
(The nearly zero value for the mean separationfor man/women i dicates the words appear about equallyTable 1.
Mean and Variance of the Separation BetweenX and YSeparationRelation Word x Word y Mean VarianceFixed break butter 2.00 0.00drink drive 2.00 0.00Compound computer scientist 1.12 O. I 0United States 0.98 0.14Semantic man woman 1.46 8.07man women - 0.12 13.08Lexical refraining from 1.11 0.20coming from 0.83 2.89keeping from 2.14 5.53Computational Linguistics Volume 16, Number 1, March 1990 23Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicographyoften in either order.)
Lexical relations come in severalvarieties.
There are some like refraining from that arefairly fixed, others such as coming from that may beseparated by an argument, and still others like keepingfrom that are almost certain to be separated by an argu-ment.The ideal window size is different in each case.
For theremainder of this paper, the window size, w, will be set tofive words as a compromise; this setting is large enough toshow some of the constraints between verbs and arguments,but not so large that it would wash out constraints thatmake use of strict adjacency)Since the association ratio becomes unstable when thecounts are very small, we will not discuss word pairs withf (x ,y )  _< 5.
An improvement would make use of t-scores,and throw out pairs that were not significant.
Unfortu-nately, this requires an estimate of the variance of f (x ,y ) ,which goes beyond the scope of this paper.
For the remain-der of this paper, we will adopt the simple but arbitrarythreshold, and ignore pairs with small counts.Technically, the association ratio is different from mu-tual information in two respects.
First, joint probabilitiesare supposed to be symmetric: P(x ,y )  = P(y,  x), andthus, mutual information is also symmetric: I (x ,y )  =I (y ,  x).
However, the association ratio is not symmetric,sincef(x, y) encodes linear precedence.
(Recall thatf(x, y)denotes the number of times that word x appears before yin the window of w words, not the number of times the twowords appear in either order.)
Although we could fix thisproblem by redefiningf(x, y) to be symmetric (by averag-ing the matrix with its transpose), we have decided not todo so, since order information appears to be very interest-ing.
Notice the asymmetry in the pairs in Table 2 (com-puted from 44 million words of 1988 AP text), illustrating awide variety of biases ranging from sexism to syntax.Second, one might expect f (x ,  y) <_ f (x )  and f (x ,  y) <_f (y ) ,  but the way we have been counting, this needn't bethe case if x and y happen to appear several times in thewindow.
For example, given the sentence, "Library work-ers were prohibited from saving books from this heap ofruins," which appeared in an AP story on April 1, 1988,f(prohibited) = 1 and f(prohibited, from) = 2.
Thisproblem can be fixed by dividingf(x, y) by w - 1 (whichhas the consequence of subtracting log2 (w - 1) = 2 fromour association ratio scores).
This adjustment has the addi-Table 2.
Asymmetry in 1988 AP Corpus (N = 44 million)x y f(x, y) f(y, x)doctors nurses 99 10man woman 256 56doctors lawyers 29 19bread butter 15 1save life 129 11save money 187 11save from 176 18supposed to 1188 25tional beneft of assuring that Z f (x ,y )  = ~ f (x )  =Z f (y )  = N.When I (x,  y) is large, the association ratio produces verycredible results not unlike those reported in Palermo andJenkins (1964), as illustrated in Table 3.
In contrast, whenI(x, y) ---: 0, the pairs are less interesting.
(As a very roughrule; of thumb, we have observed that pairs with I(x, y) > 3tend to be interesting, and pairs with smaller I(x, y) aregenerally not.
One can make this statement precise bycalibrating the measure with subjective measures.
Alterna-tively, one could make estimates of the variance and thenmake statements about confidence levels, e.g.
with 95%confidence, P(x, y) > e (x )  P(y).
)If I(x, y) << 0, we would predict that x and y are incomplementary distribution.
However, we are rarely ableto observe I(x, y) << 0 because our corpora are too small(and our measurement techniques are too crude).
Suppose,for example, that both x and y appear about 10 times permillion words of text.
Then, P(x) = P (y )  = 10 -5 andchance is P(x) P(x) = 10 -I?.
Thus, to say that I(x, y) ismuch less than 0, we need to say that P(x, y) is much lessthan 10 -t?, a statement that is hard to make with muchconfidence given the size of presently available corpora.
Infact, we cannot (easily) observe a probability less than1/N ~ 10 -7, and therefore it is hard to know if I(x, y) ismuch less than chance or not, unless chance is very large.
(In fact, the pair a. .
.
doctors in Table 3, appears ignifi-cantly less often than chance.
But to justify this statement,we need to compensate for the window size (which shiftsthe score downward by 2.0, e.g.
from 0.96 down to - 1.04),and we need to estimate the standard eviation, using amethod such as Good (1953).
45 LEXICO-SYNTACTIC REGULARITIESAlthough the psycholinguistic literature documents thesignificance of noun/noun word associations such as doctor/nurse in considerable detail, relatively little is said aboutTable 3.
Some interesting Associations with "Doctor" in the1987 AP Corpus (N = 15 million)I(x, y) f(x, y) f(x) x f(y) y11.3 12 111 honorary 621 doctor11.3 8 1105 doctors 44 dentists10.7 30 1105 doctors 241 nurses9.4 8 1105 doctors 154 treating9.0 6 275 examined 621 doctor8.9 11 1105 doctors 317 treat8.7 25 621 doctor 1407 bills8.7 6 621 doctor 350 visits8.6 19 1105 doctors 676 hospitals8,4 6 241 nurses 1105 doctorsSome Uninteresting Associations with "Doctor"0.96 6 621 doctor 73785 with0.95 41 284690 a 1105 doctors0.93 12 84716 is 1105 doctors24 Computational Linguistics Volume 16, Number 1, March 1990Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicographyassociations among verbs, function words, adjectives, andother non-nouns.
In addition to identifying semantic rela-tions of the doctor/nurse variety, we believe the associationratio can also be used to search for interesting lexico-syntactic relationships between verbs and typical argu-ments/adjuncts.
The proposed association ratio can beviewed as a formalization of Sinclair's argument:How common are the phrasal verbs with set?
Set isparticularly rich in making combinations with wordslike about, in, up, out, on, off, and these words arethemselves very common.
How likely is set offto occur?Both are frequent words \[set occurs approximately 250times in a million words and off  occurs approximately556 times in a million words .
.
.
\[T\]he question we areasking can be roughly rephrased as follows: how likely isoff to occur immediately after set?
.
.
.
This is 0.00025 x0.00055 \[P(x) P(y) \ ] ,  which gives us the tiny figure of0.0000001375 .
.
.
The assumption behind this calcula-tion is that the words are distributed at random in a text\[at chance, in our terminology\].
It is obvious to a linguistthat this is not so, and a rough measure of how much setand offattract each other is to compare the probabilitywith what actually happens .
.
.
Set off  occurs nearly70 times in the 7.3 million word corpus \[P(x, y) =70/(7.3 x 106) >> P(x)  P(y) \ ] .
That is enough to showits main patterning and it suggests that in currently-heldcorpora there will be found sufficient evidence for thedescription of a substantial collection of phrases .
.
.
(Sinclair 1987c, pp.
151-152).Using Sinclair's estimates P(set) ~ 250 x 10 -6, P(of f )  ~-556 x 10 -6, and P(set, off)  ~ 70/(7.3 x 106), we wouldestimate the mutual information to be I(set; of f )  =log2P(set, o f f ) / (P (set )  P(of f ) )  ~ 6.1.
In the 1988 APcorpus (N = 44,344,077), we estimate P(set) ~ 13,046/N,P(of f )  ~ 20,693/N, and P(set, off)  ~ 463/N.
Given theseestimates, we would compute the mutual information to bel(set; off)  ~ 6.2.In this example, at least, the values seem to be fairlycomparable across corpora.
In other examples, we will seesome differences due to sampling.
Sinclair's corpus is afairly balanced sample of (mainly British) text; the APcorpus is an unbalanced sample of American journalese.This association between set and off is relatively strong;the joint probability is more than 26 = 64 times larger thanchance.
The other particles that Sinclair mentions haveassociation ratios that can be seen in Table 4.The first three, set up, set off, and set out, are clearlyTable 4.
Some Phrasal Verbs in 1988 AP Corpus(N = 44 million)x y f(x) f(y) f(x, y) I(x; y)set up 13,046 64,601 2713 7.3set off 13,046 20,693 463 6.2set out 13,046 47,956 301 4.4set on 13,046 258,170 162 1.1set in 13,046 739,932 795 1.8set about 13,046 82,319 16 - 0.6associated; the last three are not so clear.
As Sinclairsuggests, the approach is well suited for identifying thephrasal verbs, at least in certain cases.6 PREPROCESSING WITH A PARTOF SPEECH TAGGERPhrasal verbs involving the preposition to raise an interest-ing problem because of the possible confusion with theinfinitive marker to.
We have found that if we first tagevery word in the corpus with a part of speech using amethod such as Church (1988), and then measure associa-tions between tagged words, we can identify interestingcontrasts between verbs associated with a following prepo-sition to~in and verbs associated with a following infinitivemarker to~to.
(Part of speech notation is borrowed fromFrancis and Kucera (1982); in = preposition; to = infini-tive marker; vb = bare verb; vbg = verb + ing; vbd =verb + ed; vbz = verb + s; vbn = verb + en.)
Theassociation ratio identifies quite a number of verbs associ-ated in an interesting way with to; restricting our attentionto pairs with a score of 3.0 or more, there are 768 verbsassociated with the preposition to~in and 551 verbs withthe infinitive marker to/to.
The ten verbs found to be mostassociated before to/in are:?
to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/vbg, amounting/vbg, revert/vb, reverted/vbn, resorting/vbg, relegated/vbn?
to~to: obligated/vbn, trying/vbg, compelled/vbn, en-ables/vbz, supposed/vbn, intends/vbz, vowing/vbg,tried/vbd, enabling/vbg, tends/vbz, tend/vb, intend/vb,tries/vbzThus, we see there is considerable verage to be gained bypreprocessing the corpus and manipulating the inventory oftokens.7 PREPROCESSING WITH A PARSERHindle (Church et al 1989) has found it helpful to prepro-cess the input with the Fidditch parser (Hindle 1983a,1983b) to identify associations between verbs and argu-ments, and postulate semantic lasses for nouns on thisbasis.
Hindle's method is able to find some very interestingassociations, as Tables 5 and 6 demonstrate.After running his parser over the 1988 AP corpus (44million words), Hindle found N = 4,112,943 subject/verb/object (SVO) triples.
The mutual information between averb and its object was computed from these 4 milliontriples by counting how often the verb and its object werefound in the same triple and dividing by chance.
Thus, forexample, disconnect/V and te lephone/0 have a joint prob-ability of 7/N.
In this case, chance is 84/N x 481/Nbecause there are 84 SVO triples with the verb disconnect,and 481 SVO triples with the object elephone.
The mutualinformation is log z 7N/(84 ?
481) = 9.48.
Similarly, themutual information for dr ink /Vbeer /O  is 9.9 = log 2 29N/(660 ?
195).
(d r ink /V  and beer/O are found in 660 andComputational Linguistics Volume 16, Number 1, March 1990 25Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and LexicographyTable 5.
What Can You Drink?Verb Object Mutual Info Joint Freqdrink/V martinis/O 12.6 3drink/V cup_water/O 11.6 3drink/V champagne/O 10.9 3drink/V beverage/O 10.8 8drink/V cup_coffee/O 10.6 2drink/V cognac/ O 10.6 2drink/V beer/O 9.9 29drink/V eup/O 9.7 6drink/V coffee/O 9.7 12drink/V toast/O 9.6 4drink/V alcohol/O 9.4 20drink/V wine/ O 9.3 10drink/V fluid/O 9.0 5drink/V liquor/O 8.9 4drink/V tea\]O 8.9 5drink/V milk/O 8.7 8drink/V juice/O 8.3 4drink/V water/O 7.2 43drink/V quantity\]O 7.1 4195 SVO triples, respectively; they are found together in 29of these triples).This application of Hindle's parser illustrates a secondexample of preprocessing the input to highlight certainconstraints of interest.
For measuring syntactic onstraints,it may be useful to include some part of speech informationand to exclude much of the internal structure of nounphrases.
For other purposes, it may be helpful to tag itemsand/or phrases with semantic labels such as *person*,*place*, *time*, *body part*, *bad*, and so on.8 APPLICATIONS IN LEXICOGRAPHYLarge machine-readable corpora re only just now becom-ing available to lexicographers.
Up to now, lexicographershave been reliant either on citations collected by humanTable 6.
What Can You Do to a Telephone?Verb Object Mutual Info Joint Freqsit_by/V telephone/O 11.78 7disconnect/V telephone/O 9.48 7answer/V telephone/O 8.80 98hang_up\]V telephone/O 7.87 3tap/V telephone/O 7.69 15pick_up/V telephone/O 5.63 11return/V telephone/O 5.01 19be_by/V telephone/O 4.93 2spot/V telephone/O 4.43 2repeat/V telephone/O 4.39 3place/V telephone/O 4.23 7receive/V telephone/O 4.22 28install/V telephone/O 4.20 2be_on/V telephone/O 4.05 15come_to/V telephone/O 3.63 6use/V telephone/O 3.59 29operate/V telephone/O 3.16 4readers, which introduced an element of selectivity and soinevitably distortion (rare words and uses were collectedbut common uses of common words were not), or on smallcorpora of only a million words or so, which are reliablyinformative for only the most common uses of the few mostfrequent words of English.
(A million-word corpus such asthe Brown Corpus is reliable, roughly, for only some uses ofonly some of the forms of around 4000 dictionary entries.But standard ictionaries typically contain twenty timesthis number of entries.
)The computational tools available for studying machine-readable corpora re at present still rather primitive.
Theseare concordancing programs (see Figure 1), which arebasically KWIC (key word in context; Aho et al 1988)indexes with additional features such as the ability toextend the context, sort leftward as well as rightward, andso on.
There is very little interactive software.
In a typicalsituation in the lexicography of the 1980s, a lexicographeris giwen the concordances for a word, marks up the printoutwith colored pens to identify the salient senses, and thenwrites syntactic descriptions and definitions.Although this technology is a great improvement onusing human readers to collect boxes of citation index cards(tlhe method Murray used in constructing The OxfordEnglish Dictionary a century ago), it works well if there areno more than a few dozen concordance lines for a word, andonly two or three main sense divisions.
In analyzing acomplex word such as take, save, or from, the lexicogra-pher is trying to pick out significant patterns and subtledistinctions that are buried in literally thousands of concor-dance lines: pages and pages of computer printout.
Theunaided human mind simply cannot discover all the signifi-Is Su~Say, call ing for ~x~ater conomic reforms tommi~:ion asseaed that " the Postal Se~wice couldThen.
sl0e said, the family hopes toe out-of-work steelworker, " because that doesn't.
.
.
.
We suspend reality when we say we'l lsclent~ts has won the first round in an effort toabout three children in a mining town who plot toGM executives say the s lmtdow~ wi l lrtr~ent as receiver, lilstracted officials to U3, toThe package, which is tonewly enhanced image as the moderate who moved tomffiina offer from chairman Victor Posner to helpafter telling a delivery-room doctor not to try toh bliffiday Tmr~day, cheered by those who fought toat be ~s l  formed an all iance with Moslem rebels to? '
Basically we couldWe worked for a year totheir expet~ive mirrors, just l ike in wartime, toald of many who risked their Own lives in order toWe must increase tile amount Americanssave Oatha ~ poveay.save enormous ums of money in conwacling out individual esave enough for a down payment on a boule.save jobs, that costs jobs.
"save money by spending $10,000 in wage~ for a public work~save one of Egypt's great m:Lsxtre.s, the decaying tomb of Rsave the " pit ponies " doomed to be slaughtered.save the automaker $500 mil l ion a year in operating e~ts  asave the ?
?m3pany rather than liquidate it and then declaredsave the counW/nearly $2 billion, also includes a programsave the counw/.save the financially troubled company, but said Pc~er  stilsave the infant by imsertlnli a tube in its throat o belp isave the majestic Beaux Arts arcl~tecmral mE~-telpiece.save ate nation from commumsm.save the operating costs of the Pershing, s and ground-launchsave the ~te at enormous expense to us, " said Levei l \]ee.save them fi~m diamken yankee brawlel~, " Ta~ said.save those who were p~=aengers.
"save.
"Figure 1 Short Sample of the Concordance to"save" from the AP 1987 Corpus.26 Computational Linguistics Volume 16, Number 1, March 1990Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicographycant patterns, let alne group them and rank them in orderof importance.The AP 1987 concordance to save is many pages long;there are 666 lines for the base form alone, and many morefor the inflected forms saved, saves, saving, and savings.
Inthe discussion that follows, we shall, for the sake of simplic-ity, not analyze the inflected forms and we shall only look atthe patterns to the right of save (see Table 7).It is hard to know what is important in such a concor-dance and what is not.
For example, although it is easy tosee from the concordance selection in Figure 1 that theword "to" often comes before "save" and the word "the"often comes after "save," it is hard to say from examinationof a concordance alone whether either or both of theseco-occurrences have any significance.Two examples will illustrate how the association ratiomeasure helps make the analysis both quicker and moreaccurate.8.1 EXAMPLE 1: "SAVE .
.
.
FROM"The association ratios in Table 7 show that associationnorms apply to function words as well as content words.
Forexample, one of the words significantly associated with saveis f rom.
Many dictionaries, for example Webster's N inthNew Collegiate Dict ionary (Merriam Webster), make noexplicit mention of f rom in the entry for save, althoughTable 7.
Words Often Co-Occurring to the Right of"Save"I(x, y) f(x, y) f(x) x f(y) y9.5 6 724 save 170 forests9.4 6 724 save 180 $1.28.8 37 724 save 1697 lives8.7 6 724 save 301 enormous8.3 7 724 save 447 annually7.7 20 724 save 2001 jobs7.6 64 724 save 6776 money7.2 36 724 save 4875 life6.6 8 724 save 1668 dollars6.4 7 724 save 1719 costs6.4 6 724 save 1481 thousands6.2 9 724 save 2590 face5.7 6 724 save 2311 son5.7 6 724 save 2387 estimated5.5 7 724 save 3141 your5.5 24 724 save 10880 billion5.3 39 724 save 20846 million5.2 8 724 save 4398 us5.1 6 724 save 3513 less5.0 7 724 save 4590 own4.6 7 724 save 5798 world4.6 7 724 save 6028 my4.6 15 724 save 13010 them4.5 8 724 save 7434 country4.4 15 724 save 14296 time4.4 64 724 save 61262 f rom4.3 23 724 save 23258 more4.2 25 724 save 27367 their4.1 8 724 save 9249 company4.1 6 724 save 7114 monthBritish learners' dictionaries do make specific mention off rom in connection with save.
These learners' dictionariespay more attention to language structure and collocationthan do American collegiate dictionaries, and lexicogra-phers trained in the British tradition are often fairly skilledat spotting these generalizations.
However, teasing outsuch facts and distinguishing true intuitions from falseintuitions takes a lot of time and hard work, and there is ahigh probability of inconsistencies and omissions.Which other verbs typically associate with f rom,  andwhere does save rank in such a list?
The association ratioidentified 1530 words that are associated with f rom;  911 ofthem were tagged as verbs.
The first 100 verbs are:refrain/vb, gleaned/vbn, stems/vbz, stemmed/vbd,stemming/vbg, ranging/vbg, stemmed/vbn, ranged/vbn, derived/vbn, ranged/vbd, extort/vb, graduated/vbd, barred/vbn, benefiting/vbg, benefitted/vbn, bene-fited/vbn, excused/vbd, arising/vbg, range/vb, exempts/vbz, suffers/vbz, exempting/vbg, benefited/vbd,prevented/vbd (7.0), seeping/vbg, barred/vbd, prevents/vbz, suffering/vbg, excluded/vbn, marks/vbz, profiting/vbg, recovering/vbg, discharged/vbn, rebounding/vbg,vary/vb, exempted/vbn, separate/vb, banished/vbn,withdrawing/vbg, ferry/vb, prevented/vbn, profit/vb,bar/vb, excused/vbn, bars/vbz, benefit/vb, emerges/vbz, emerge/vb, varies/vbz, differ/vb, removed/vbn,exempt/vb, expelled/vbn, withdraw/vb, stem/vb, sepa-rated/vbn, judging/vbg, adapted/vbn, escaping/vbg, in-herited/vbn, differed/vbd, emerged/vbd, withheld/vbd,leaked/vbn, strip/vb, resulting/vbg, discourage/vb, pre-vent/vb, withdrew/vbd, prohibits/vbz, borrowing/vbg,preventing/vbg, prohibit/vb, resulted/vbd (6.0), pre-clude/vb, divert/vb, distinguish/vb, pulled/vbn, fell/vbn, varied/vbn, emerging/vbg, suffer/vb, prohibiting/vbg, extract/vb, subtract/vb, recover/vb, paralyzed/vbn, stole/vbd, departing/vbg, escaped/vbn, prohibited/vbn, forbid/vb, evacuated/vbn, reap/vb, barring/vbg,removing/vbg, stolen/vbn, receives/vbz.Save .
.
.
f rom is a good example for illustrating the advan-tages of the association ratio.
Save is ranked 319th in thislist, indicating that the association is modest, strong enoughto be important (21 times more likely than chance), but notso strong that it would pop out at us in a concordance, orthat it would be one of the first things to come to mind.If the dictionary is going to list save .
.
,  f rom,  then, forconsistency's sake, it ought to consider listing all of themore important associations as well.
Of the 27 bare verbs(tagged 'vb') in the list above, all but seven are listed inColl ins Cobui ld Engl ish Language Dict ionary as occurringwith f rom.
However, this dictionary does not note thatvary, ferry,  strip, divert, forbid,  and reap occur with f rom.If the Cobuild lexicographers had had access to the pro-posed measure, they could possibly have obtained bettercoverage at less cost.8.2 EXAMPLE 2: IDENTIFYING SEMANTIC CLASSESHaving established the relative importance of save .
.
.f rom,  and having noted that the two words are rarelyComputational Linguistics Volume 16, Number 1, March 1990 27Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicographyadjacent, we would now like to speed up the labor-intensivetask of categorizing the concordance lines.
Ideally, wewould like to develop a set of semi-automatic tools thatwould help a lexicographer produce something like Figure2, which provides an annotated summary of the 65 concor-dance lines for save .
.
.
f rom.
5 The save .
.
.
f rom patternoccurs in about 10% of the 666 concordance lines for save.Traditionally, semantic ategories have been only vaguelyrecognized, and to date little effort has been devoted to asystematic classification of a large corpus.
Lexicographershave tended to use concordances impressionistically; seman-tic theorists, AI-ers, and others have concentrated on a fewinteresting examples, e.g.
bachelor,  and have not givenmuch thought o how the results might be scaled up.With this concern in mind, it seems reasonable to askhow well these 65 lines for save .
.
.
f rom fit in with all otheruses of save A laborious concordance analysis was under-taken to answer this question.
When it was nearing comple-tion, we noticed that the tags that we were inventing tocapture the generalizations could in most cases have beensuggested by looking at the lexical items listed in theassociation ratio table for save.
For example, we had failedto notice the significance of time adverbials in our analysisof save, and no dictionary records this.
Yet it should besave X f rom Y (65 concordance lines)1 save PERSON f rom Y (23 concordance lines)1.1 save PERSON f rom BAD (19 concordance lines)( Robert DeNiro ) to save Indian tribes(PERSON\] from genocide\[DESTRUCT\[BAD\]\] at the hands of" We wanted to  save  him(PERSON\] ~orn undue ~ouble\[BAD\] and loss(BAD\] of money , "Murphy was sacrificed to save more powerful Democrats(PERSON\] from harm(BAD\] ."
God sent this man to save my five children(PERSON\] from being burned to death(DESTRUCT(BAD\]\] andPope John Paul I\] to " save us(PERSON\] fl~m sin(BAD\] .
"1.2 save PERSON f rom (BAD) LOC(AT1ON) (4 concordance lines)rescuers who helped save the toddler(PERSON\] from an abandoned weU\[LOC\] will be feted with a paradewhile attempting to save two drowning hoys\[PERSON\] from a turbulent(BAD\] creeklLOC\] in Otdo\[LOC\]2. save INST( ITUT ION)  f rom (ECON) BAD (27 concordance lines)member states to help save the EEC\[INSTI from possible bankaxlptcy\[BCON\]\[BAD\] this year.should be sought " to save the compeny\[CORP\[1NST\]\] from bankmptfy\[BCON\]\[BAD\].l aw was  necessary  to  save the counffy\[NATIOlq\[lNST\]\] floradisaster(BAD\].operation " to  save  the nation(NATION(INS'r\]\] from COmmUnL~n\[BAD\]\[POL1TICAL\] .were not needed to save the system from benkauptcy\[ECON\]\[BAD\].his efforts to  save  the wodd\[INST\] from the like~ of Lothax and the Spider Woman3.
save ANIMAL f rom DESTRUCT( ION)  (5 concordance lines)give them the money to save the dogs(ANIMAL\] from being destroyed(DESTRUCT\] ,program intended to save the giant birds(ANIMAL\] ~om extinction\[DESTRUCTI,UNCLASSIF IED (10 concordance lines)walnut and ash tx~es to save them from the axes and saws of a logging company.after the a~aek to  save the ship from a temble\[BAD\] fire, Navy reports concluded Thursday.cemficates that would save shopper~\[pERSON\] anywhere f~m $50\[MONEY\] [NUMBER\] to $500\[MONEY\] (/fluFigure 2 Some AP 1987 Concordance Lines to"save .
.
.
f rom,  " Roughly Sorted into Categories.clear fi'om the association ratio table above that annua l lyand month  6 are commonly found with save.
More detailedinspection shows that the time adverbials correlate interest-ingly with just one group of save objects, namely thosetagged \[MONEY\].
The AP wire is full of discussions ofsav ing $1.2 bi l l ion per  month ;  computational lexicographyshould measure and record such patterns if they are gen-eral, even when traditional dictionaries do not.A,; another example illustrating how the association ratiotables would have helped us analyze the save concordancelines, we found ourselves contemplating the semantic tagENV( IRONMENT)  to analyze lines such as:the trend to save the forests\[ENV\]it's our turn to save the lake\[ENV\],joined a fight to save their forests\[ENV\],can we get busy to save the planet\[ENV\] ?If we had looked at the association ratio tables beforelabC.ing the 65 lines for save .
.
.
f rom,  we might havenoticed the very large value for save .
.
,  fo rests ,  suggestingthat there may be an important pattern here.
In fact, thispattern probably subsumes most of the occurrences of the"save \ [ANIMAL\]"  pattern noticed in Figure 2.
Thus,these tables do not provide semantic tags, but they providea powerful set of suggestions to the lexicographer for whatneeds to be accounted for in choosing a set of semantic tags.It may be that everything said here about save and otherwords is true only of 1987 American journalese.
Intuitively,however, many of the patterns discovered seem to be goodcandidates for conventions of general English.
A futurestep would be to examine other more balanced corpora andtest how well the patterns hold up.9 CONCLUSIONSWe began this paper with the psycholinguistic notion ofword association orm, and extended that concept owardthe information theoretic definition of mutual information.This provided a precise statistical calculation that could beapplied to a very large corpus of text to produce a table ofassociations for tens of thousands of words.
We were thenable to show that the table encoded a number of veryinteresting patterns ranging from doctor .
.
,  nurse to save.
.
.
.
f rom.
We finally concluded by showing how the pat-terns in the association ratio table might help a lexicogra-pher organize a concordance.In point of fact, we actually developed these results inbasically the reverse order.
Concordance analysis is stillextremely labor-intensive and prone to errors of omission.The ways that concordances are sorted don't adequatelysupport current lexicographic practice.
Despite the factthat a concordance is indexed by a single word, oftenlexicographers actually use a second word such as f rom oran equally common semantic oncept such as a time adver-bial to decide how to categorize concordance lines.
In otherwords, they use two words to t r iangu late  in on a word sense.This triangulation approach clusters concordance lines to-gether into word senses based primarily on usage (distribu-28 Computational Linguistics Volume 16, Number 1, March 1990Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicographytional evidence), as opposed to intuit ive notions of meaning.Thus, the question of  what is a word sense can be addressedwith syntactic methods (symbol pushing), and need notaddress semantics ( interpretat ion),  even though the inven-tory of tags may appear to have semantic  values.The tr iangulat ion approach requires "ar t . "
How does thelex icographer  dec ide which potent ia l  cut  points are" interest ing"  and which are merely due to chance?
Theproposed association ratio score provides a practical  andobjective measure that is often a fair ly good approximat ionto the "ar t . "
Since the proposed measure is objective, it canbe applied in a systematic way over a large body of mate-rial, steadily improving consistency and productivity.But on the other hand, the objective score can be mislead-ing.
The score takes only distr ibutional evidence into ac-count.
For example, the measure favors set .
.
.
fo r  overset .
.
.
down; it doesn't  know that the former is lessinteresting because its semantics are composit ional.
Inaddition, the measure is extremely superficial; it cannotcluster words into appropr iate syntactic classes without anexplicit preprocess such as Church 's  parts program orHindle's  parser.
Ne i ther  of these preprocesses, though, canhelp highl ight the "natura l "  s imilar ity between nouns suchas picture and photograph.
Al though one might  imagine apreprocess that would help in this part icular  case, there willprobably always be a class of general izat ions that areobvious to an intel l igent lexicographer,  but lie hopelesslybeyond the objectivity of a computer.Despite these problems, the association ratio could be animportant  ool to aid the lexicographer,  ather like an indexto the concordances.
It can help us decide what to look for;it provides a quick summary  of what company our words dokeep.REFERENCESChurch, K. 1988 "A Stochastic Parts Program and Noun Phrase Parserfor Unrestricted Text," Second Conference on Applied Natural Lan-guage Processing, Austin, TX.Church, K.; Gale, W.; Hanks, P.; and Hindle, D. 1989 "Parsing, WordAssociations and Typical Predicate-Argument Relations," Interna-tional Workshop on Parsing Technologies, CMU.Fano, R. 1961 Transmission of Information: A Statistical Theory ofCommunications.
MIT Press, Cambridge, MA.Firth, J.
1957 "A Synopsis of Linguistic Theory 1930-1955," in Studiesin Linguistic Analysis, Philological Society, Oxford; reprinted in Palmer,F.
(ed.)
1968 Selected Papers of J. R. Firth, Longman, Harlow.Francis, W. and Ku~era, H. 1982 Frequency Analysis of English Usage.Houghton Mifflin Company, Boston, MA.Good, I. J.
1953 The Population Frequencies of Species and the Estima-tion of Population Parameters.
Biometrika, Vol.
40, 237-264.Hanks, P. 1987 "Definitions and Explanations," in J. Sinclair (ed.
),Looking Up: An Account of the COBUILD Project in Lexical Comput-ing.
Collins, London and Glasgow.Hindle, D. 1983a "Deterministic Parsing of Syntactic Non-fluencies."
InProceedings of the 23rd Annual Meeting of the Association for Compu-tational Linguistics.Hindle, D. 1983b "User Manual for Fidditch, a Deterministic Parser.
"Naval Research Laboratory Technical Memorandum #7590-142.Hornby, A.
1948 The Advanced Learner's Dictionary, Oxford UniversityPress, Oxford, U.K.Jelinek, F. 1982.
(personal communication)Kahan, S.; Pavlidis, T.; and Baird, H. 1987 "On the Recognition ofPrinted Characters of any Font or Size," IEEE Transactions PAMI,274-287.Meyer, D.; Schvaneveldt, R.; and Ruddy, M. 1975 "Loci of ContextualEffects on Visual Word-Recognition," in P. Rabbitt and S.
Dornic(eds.
), Attention and Performance V, Academic Press, New York.Palermo, D. and Jenkins, J.
1964 "Word AssociationNorms."
Universityof Minnesota Press, Minneapolis, MN.Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; and Stock, P.
(eds.)
1987aCollins Cobuild English Language Dictionary.
Collins, London andGlasgow.Sinclair, J.
1987b "The Nature of the Evidence," in J. Sinclair (ed.
),Looking Up: An Account of the COBUILD Project in Lexical Comput-ing.
Collins, London and Glasgow.Smadja, F. In press.
"Microcoding the Lexicon with Co-OccurrenceKnowledge," in Zernik (ed.
), Lexical Acquisition: Using On-Line Re-sources to Build a Lexicon, MIT Press, Cambridge, MA.NOTES1.
This statistic has also been used by the IBM speech group (Jelinek1982) for constructing language models for applications in speechrecognition.2.
Smadja (in press) discusses the separation between collocates in avery similar way.3.
This definition fw(x,y) uses a rectangular window.
It might beinteresting to consider alternatives (e.g.
a triangular window or adecaying exponential) that would weight words less and less as theyare separated by more and more words.
Other windows are alsopossible.
For example, Hindle (Church et al 1989) has used asyntactic parser to select words in certain constructions of interest.4.
Although the Good-Turing Method (Good 1953) is more than 35years old, it is still heavily cited.
For example, Katz (1987) uses themethod in order to estimate trigram probabilities in the IBM speechrecognizer.
The Good-Turing Method is helpful for trigrams thathave not been seen very often in the training corpus.5.
The last unclassified line .
.
.
.
save shoppers anywhere from $50.. .raises interesting problems.
Syntactic "chunking" shows that, in spiteof its co-occurrence offrom with save, this line does not belong here.An intriguing exercise, given the lookup table we are trying toconstruct, is how to guard against false inferences uch as that sinceshoppers is tagged \[PERSON\], $50 to $500 must here count as eitherBAD or a LOCATION.
Accidental coincidences of this kind do nothave a significant effect on the measure, however, although they doserve as a reminder of the probabilistic nature of the findings.6.
The word time itself also occurs ignificantly in the table, but on closerexamination it is clear that this use of time (e.g.
to save time) countsas something like a commodity or resource, not as part of a timeadjunct.
Such are the pitfalls of lexicography (obvious when they arepointed out).Computational Linguistics Volume 16, Number 1, March 1990 29
