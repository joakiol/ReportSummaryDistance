Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 372?381,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAutomatic Interpretation of the English PossessiveStephen Tratz ?Army Research LaboratoryAdelphi Laboratory Center2800 Powder Mill RoadAdelphi, MD 20783stephen.c.tratz.civ@mail.milEduard Hovy ?Carnegie Mellon UniversityLanguage Technologies Institute5000 Forbes AvenuePittsburgh, PA 15213hovy@cmu.eduAbstractThe English ?s possessive construction oc-curs frequently in text and can encodeseveral different semantic relations; how-ever, it has received limited attention fromthe computational linguistics community.This paper describes the creation of a se-mantic relation inventory covering the useof ?s, an inter-annotator agreement studyto calculate how well humans can agreeon the relations, a large collection of pos-sessives annotated according to the rela-tions, and an accurate automatic annota-tion system for labeling new examples.Our 21,938 example dataset is by far thelargest annotated possessives dataset weare aware of, and both our automatic clas-sification system, which achieves 87.4%accuracy in our classification experiment,and our annotation data are publicly avail-able.1 IntroductionThe English ?s possessive construction occurs fre-quently in text?approximately 1.8 times for every100 hundred words in the Penn Treebank1(Marcuset al, 1993)?and can encode a number ofdifferent semantic relations including ownership(John?s car), part-of-whole (John?s arm), extent (6hours?
drive), and location (America?s rivers).
Ac-curate automatic possessive interpretation couldaid many natural language processing (NLP) ap-plications, especially those that build semanticrepresentations for text understanding, text gener-ation, question answering, or information extrac-tion.
These interpretations could be valuable formachine translation to or from languages that al-low different semantic relations to be encoded by?The authors were affiliated with the USC InformationSciences Institute at the time this work was performed.the possessive/genitive.This paper presents an inventory of 17 semanticrelations expressed by the English ?s-construction,a large dataset annotated according to the this in-ventory, and an accurate automatic classificationsystem.
The final inter-annotator agreement studyachieved a strong level of agreement, 0.78 Fleiss?Kappa (Fleiss, 1971) and the dataset is easily thelargest manually annotated dataset of possessiveconstructions created to date.
We show that ourautomatic classication system is highly accurate,achieving 87.4% accuracy on a held-out test set.2 BackgroundAlthough the linguistics field has devoted signif-icant effort to the English possessive (?6.1), thecomputational linguistics community has given itlimited attention.
By far the most notable excep-tion to this is the line of work by Moldovan andBadulescu (Moldovan and Badulescu, 2005; Bad-ulescu and Moldovan, 2009), who define a tax-onomy of relations, annotate data, calculate inter-annotator agreement, and perform automatic clas-sification experiments.
Badulescu and Moldovan(2009) investigate both ?s-constructions and ofconstructions in the same context using a list of 36semantic relations (including OTHER).
They taketheir examples from a collection of 20,000 ran-domly selected sentences from Los Angeles Timesnews articles used in TREC-9.
For the 960 ex-tracted ?s-possessive examples, only 20 of their se-mantic relations are observed, including OTHER,with 8 of the observed relations occurring fewerthan 10 times.
They report a 0.82 Kappa agree-ment (Siegel and Castellan, 1988) for the twocomputational semantics graduates who annotatethe data, stating that this strong result ?can be ex-plained by the instructions the annotators received1Possessive pronouns such as his and their are treated as?s constructions in this work.372prior to annotation and by their expertise in Lexi-cal Semantics.
?Moldovan and Badulescu experiment with sev-eral different classification techniques.
They findthat their semantic scattering technique signifi-cantly outperforms their comparison systems withits F-measure score of 78.75.
Their SVM systemperforms the worst with only 23.25% accuracy?suprisingly low, especially considering that 220 ofthe 960 ?s examples have the same label.Unfortunately, Badulescu and Moldovan (2009)have not publicly released their data2.
Also, it issometimes difficult to understand the meaning ofthe semantic relations, partly because most rela-tions are only described by a single example and,to a lesser extent, because the bulk of the givenexamples are of -constructions.
For example, whyPresident of Bolivia warrants a SOURCE/FROM re-lation but University of Texas is assigned to LOCA-TION/SPACE is unclear.
Their relations and pro-vided examples are presented below in Table 1.Relation ExamplesPOSSESSION Mary?s bookKINSHIP Mary?s brotherPROPERTY John?s coldnessAGENT investigation of the crewTEMPORAL last year?s exhibitionDEPICTION-DEPICTED a picture of my niecePART-WHOLE the girl?s mouthCAUSE death of cancerMAKE/PRODUCE maker of computerLOCATION/SPACE Univerity of TexasSOURCE/FROM President of BoliviaTOPIC museum of artACCOMPANIMENT solution of the problemEXPERIENCER victim of lung diseaseRECIPIENT Josephine?s rewardASSOCIATED WITH contractors of shipyardMEASURE hundred (sp?)
of dollarsTHEME acquisition of the holdingRESULT result of the reviewOTHER state of emergencyTable 1: The 20 (out of an original 36) seman-tic relations observed by Badulescu and Moldovan(2009) along with their examples.3 Dataset CreationWe created the dataset used in this work fromthree different sources, each representing a distinctgenre?newswire, non-fiction, and fiction.
Of the2Email requests asking for relation definitions and thedata were not answered, and, thus, we are unable to providean informative comparison with their work.21,938 total examples, 15,330 come from sections2?21 of the Penn Treebank (Marcus et al, 1993).Another 5,266 examples are from The History ofthe Decline and Fall of the Roman Empire (Gib-bon, 1776), a non-fiction work, and 1,342 are fromThe Jungle Book (Kipling, 1894), a collection offictional short stories.
For the Penn Treebank, weextracted the examples using the provided goldstandard parse trees, whereas, for the latter cases,we used the output of an open source parser (Tratzand Hovy, 2011).4 Semantic Relation InventoryThe initial semantic relation inventory for pos-sessives was created by first examining some ofthe relevant literature on possessives, includingwork by Badulescu and Moldovan (2009), Barker(1995), Quirk et al (1985), Rosenbach (2002), andTaylor (1996), and then manually annotating thelarge dataset of examples.
Similar examples weregrouped together to form initial categories, andgroups that were considered more difficult werelater reexamined in greater detail.
Once all theexamples were assigned to initial categories, theprocess of refining the definitions and annotationsbegan.In total, 17 relations were created, not includingOTHER.
They are shown in Table 3 along with ap-proximate (best guess) mappings to relations de-fined by others, specifically those of Quirk et al(1985), whose relations are presented in Table 2,as well as Badulescu and Moldovan?s (2009) rela-tions.Relation ExamplesPOSSESSIVE my wife?s fatherSUBJECTIVE boy?s applicationOBJECTIVE the family?s supportORIGIN the general?s letterDESCRIPTIVE a women?s collegeMEASURE ten days?
absenseATTRIBUTE the victim?s couragePARTITIVE the baby?s eyeAPPOSITION (marginal) Dublin?s fair cityTable 2: The semantic relations proposed by Quirket al (1985) for ?s along with some of their exam-ples.4.1 Refinement and Inter-annotatorAgreementThe semantic relation inventory was refined us-ing an iterative process, with each iteration involv-373Relation Example HDFRE JB PTB MappingsSUBJECTIVE Dora?s travels 1083 89 3169 Q:SUBJECTIVE, B:AGENTPRODUCER?S PRODUCT Ford?s Taurus 47 44 1183 Q:ORIGIN, B:MAKE/PRODUCEB:RESULTOBJECTIVE Mowgli?s capture 380 7 624 Q:OBJECTIVE, B:THEMECONTROLLER/OWNER/USER my apartment 882 157 3940 QB:POSSESSIVEMENTAL EXPERIENCER Sam?s fury 277 22 232 Q:POSSESSIVE, B:EXPERIENCERRECIPIENT their bonuses 12 6 382 Q:POSSESSIVE, B:RECIPIENTMEMBER?S COLLECTION John?s family 144 31 230 QB:POSSESSIVEPARTITIVE John?s arm 253 582 451 Q:PARTITIVE, B:PART-WHOLELOCATION Libya?s people 24 0 955 Q:POSSESSIVE, B:SOURCE/FROMB:LOCATION/SPACETEMPORAL today?s rates 0 1 623 Q:POSSESSIVE, B:TEMPORALEXTENT 6 hours?
drive 8 10 5 QB:MEASUREKINSHIP Mary?s kid 324 156 264 Q:POSSESSIVE, B:KINSHIPATTRIBUTE picture?s vividness 1013 34 1017 Q:ATTRIBUTE, B:PROPERTYTIME IN STATE his years in Ohio 145 32 237 QB:POSSESSIVEPOSSESSIVE COMPOUND the [men?s room] 0 0 67 Q:DESCRIPTIVEADJECTIVE DETERMINED his fellow Brit 12 0 33OTHER RELATIONAL NOUN his friend 629 112 1772 QB:POSSESSIVEOTHER your Lordship 33 59 146 B:OTHERTable 3: Possessive semantic relations along with examples, counts, and approximate mappings to otherinventories.
Q and B represent Quirk et al (1985) and Badulescu and Moldovan (2009), respectively.HDFRE, JB, PTB: The History of the Decline and Fall of the Roman Empire, The Jungle Book, and thePenn Treebank, respectively.ing the annotation of a random set of 50 exam-ples.
Each set of examples was extracted suchthat no two examples had an identical possesseeword.
For a given example, annotators were in-structed to select the most appropriate option butcould also record a second-best choice to provideadditional feedback.
Figure 1 presents a screen-shot of the HTML-based annotation interface.
Af-ter the annotation was complete for a given round,agreement and entropy figures were calculated andchanges were made to the relation definitions anddataset.
The number of refinement rounds was ar-bitrarily limited to five.
To measure agreement,in addition to calculating simple percentage agree-ment, we computed Fleiss?
Kappa (Fleiss, 1971),a measure of agreement that incorporates a cor-rection for agreement due to chance, similar toCohen?s Kappa (Cohen, 1960), but which can beused to measure agreement involving more thantwo annotations per item.
The agreement and en-tropy figures for these five intermediate annotationrounds are given in Table 4.
In all the possessiveannotation tables, Annotator A refers to the pri-mary author and the labels B and C refer to twoadditional annotators.To calculate a final measure of inter-annotatoragreement, we randomly drew 150 examples fromthe dataset not used in the previous refinement it-erations, with 50 examples coming from each ofFigure 1: Screenshot of the HTML template pageused for annotation.the three original data sources.
All three annota-tors initially agreed on 82 of the 150 examples,leaving 68 examples with at least some disagree-ment, including 17 for which all three annotatorsdisagreed.Annotators then engaged in a new task in whichthey re-annotated these 68 examples, in each casebeing able to select only from the definitions pre-viously chosen for each example by at least oneannotator.
No indication of who or how manypeople had previously selected the definitions was374Figure 2: Semantic relation distribution for the dataset presented in this work.
HDFRE: History of theDecline and Fall of the Roman Empire; JB: Jungle Book; PTB: Sections 2?21 of the Wall Street Journalportion of the Penn Treebank.given3.
Annotators were instructed not to choosea definition simply because they thought they hadchosen it before or because they thought some-one else had chosen it.
After the revision pro-cess, all three annotators agreed in 109 cases andall three disagreed in only 6 cases.
During the re-vision process, Annotator A made 8 changes, Bmade 20 changes, and C made 33 changes.
Anno-tator A likely made the fewest changes because he,as the primary author, spent a significant amountof time thinking about, writing, and re-writing thedefinitions used for the various iterations.
Anno-tator C?s annotation work tended to be less consis-tent in general than Annotator B?s throughout thiswork as well as in a different task not discussedwithin this paper, which probably why AnnotatorC made more changes than Annotator B.
Prior tothis revision process, the three-way Fleiss?
Kappascore was 0.60 but, afterwards, it was at 0.78.
Theinter-annotator agreement and entropy figures forbefore and after this revision process, includingpairwise scores between individual annotators, arepresented in Tables 5 and 6.4.2 Distribution of RelationsThe distribution of semantic relations varies some-what by the data source.
The Jungle Book?s dis-tribution is significantly different from the oth-3Of course, if three definitions were present, it could beinferred that all three annotators had initially disagreed.ers, with a much larger percentage of PARTITIVEand KINSHIP relations.
The Penn Treebank andThe History of the Decline and Fall of the Ro-man Empire were substantially more similar, al-though there are notable differences.
For instance,the LOCATION and TEMPORAL relations almostnever occur in The History of the Decline and Fallof the Roman Empire.
Whether these differencesare due to variations in genre, time period, and/orother factors would be an interesting topic for fu-ture study.
The distribution of relations for eachdata source is presented in Figure 2.Though it is harder to compare across datasetsusing different annotation schemes, there areat least a couple notable differences betweenthe distribution of relations for Badulescu andMoldovan?s (2009) dataset and the distribution ofrelations used in this work.
One such difference isthe much higher percentage of examples labeled asTEMPORAL?11.35% vs only 2.84% in our data.Another difference is a higher incidence of theKINSHIP relation (6.31% vs 3.39%), although itis far less frequent than it is in The Jungle Book(11.62%).4.3 Encountered AmbiguitiesOne of the problems with creating a list of rela-tions expressed by ?s-constructions is that someexamples can potentially fit into multiple cate-gories.
For example, Joe?s resentment encodes375both SUBJECTIVE relation and MENTAL EXPE-RIENCER relations and UK?s cities encodes bothPARTITIVE and LOCATION relations.
A represen-tative list of these types of issues along with ex-amples designed to illustrate them is presented inTable 7.5 ExperimentsFor the automatic classification experiments, weset aside 10% of the data for test purposes, andused the the remaining 90% for training.
We used5-fold cross-validation performed using the train-ing data to tweak the included feature templatesand optimize training parameters.5.1 Learning ApproachThe LIBLINEAR (Fan et al, 2008) package wasused to train linear Support Vector Machine(SVMs) for all the experiments in the one-against-the-rest style.
All training parameters took theirdefault values with the exception of the C pa-rameter, which controls the tradeoff between mar-gin width and training error and which was set to0.02, the point of highest performance in the cross-validation tuning.5.2 Feature GenerationFor feature generation, we conflated the pos-sessive pronouns ?his?, ?her?, ?my?, and ?your?to ?person.?
Similarly, every term match-ing the case-insensitive regular expression(corp|co|plc|inc|ag|ltd|llc)\\.?)
was replaced withthe word ?corporation.
?All the features used are functions of the follow-ing five words.?
The possessor word?
The possessee word?
The syntactic governor of the possessee word?
The set of words between the possessor andpossessee word (e.g., first in John?s first kiss)?
The word to the right of the possesseeThe following feature templates are used to gener-ate features from the above words.
Many of thesetemplates utilize information from WordNet (Fell-baum, 1998).?
WordNet link types (link type list) (e.g., at-tribute, hypernym, entailment)?
Lexicographer filenames (lexnames)?toplevel categories used in WordNet (e.g.,noun.body, verb.cognition)?
Set of words from the WordNet definitions(gloss terms)?
The list of words connected via WordNetpart-of links (part words)?
The word?s text (the word itself)?
A collection of affix features (e.g., -ion, -er,-ity, -ness, -ism)?
The last {2,3} letters of the word?
List of all possible parts-of-speech in Word-Net for the word?
The part-of-speech assigned by the part-of-speech tagger?
WordNet hypernyms?
WordNet synonyms?
Dependent words (all words linked as chil-dren in the parse tree)?
Dependency relation to the word?s syntacticgovernor5.3 ResultsThe system predicted correct labels for 1,962 ofthe 2,247 test examples, or 87.4%.
The accuracyfigures for the test instances from the Penn Tree-bank, The Jungle Book, and The History of the De-cline and Fall of the Roman Empire were 88.8%,84.7%, and 80.6%, respectively.
The fact that thescore for The Jungle Book was the lowest is some-what surprising considering it contains a high per-centage of body part and kinship terms, which tendto be straightforward, but this may be because theother sources comprise approximately 94% of thetraining examples.Given that human agreement typically repre-sents an upper bound on machine performancein classification tasks, the 87.4% accuracy figuremay be somewhat surprising.
One explanation isthat the examples pulled out for the inter-annotatoragreement study each had a unique possesseeword.
For example, ?expectations?, as in ?ana-lyst?s expectations?, occurs 26 times as the pos-sessee in the dataset, but, for the inter-annotatoragreement study, at most one of these examplescould be included.
More importantly, when theinitial relations were being defined, the data werefirst sorted based upon the possessee and then thepossessor in order to create blocks of similar ex-amples.
Doing this allowed multiple examples tobe assigned to a category more quickly becauseone can decide upon a category for the whole lotat once and then just extract the few, if any, that be-long to other categories.
This is likely to be bothfaster and more consistent than examining each376Agreement (%) Fleiss?
?
EntropyIteration A vs B A vs C B vs C A vs B A vs C B vs C All A B C1 0.60 0.68 0.54 0.53 0.62 0.46 0.54 3.02 2.98 3.242 0.64 0.44 0.50 0.59 0.37 0.45 0.47 3.13 3.40 3.633 0.66 0.66 0.72 0.57 0.58 0.66 0.60 2.44 2.47 2.704 0.64 0.30 0.38 0.57 0.16 0.28 0.34 2.80 3.29 2.875 0.72 0.66 0.60 0.67 0.61 0.54 0.61 3.21 3.12 3.36Table 4: Intermediate results for the possessives refinement work.Agreement (%) Fleiss?
?
EntropyPortion A vs B A vs C B vs C A vs B A vs C B vs C All A B CPTB 0.62 0.62 0.54 0.56 0.56 0.46 0.53 3.22 3.17 3.13HDFRE 0.82 0.78 0.72 0.77 0.71 0.64 0.71 2.73 2.75 2.73JB 0.74 0.56 0.54 0.70 0.50 0.48 0.56 3.17 3.11 3.17All 0.73 0.65 0.60 0.69 0.61 0.55 0.62 3.43 3.35 3.51Table 5: Final possessives annotation agreement figures before revisions.Agreement (%) Fleiss?
?
EntropySource A vs B A vs C B vs C A vs B A vs C B vs C All A B CPTB 0.78 0.74 0.74 0.75 0.70 0.70 0.72 3.30 3.11 3.35HDFRE 0.78 0.76 0.76 0.74 0.72 0.72 0.73 3.03 2.98 3.17JB 0.92 0.90 0.86 0.90 0.87 0.82 0.86 2.73 2.71 2.65All 0.83 0.80 0.79 0.80 0.77 0.76 0.78 3.37 3.30 3.48Table 6: Final possessives annotation agreement figures after revisions.First Relation Second Relation ExamplePARTITIVE CONTROLLER/... BoA?s Mr. DavisPARTITIVE LOCATION UK?s citiesPARTITIVE OBJECTIVE BoA?s adviserPARTITIVE OTHER RELATIONAL NOUN BoA?s chairmanPARTITIVE PRODUCER?S PRODUCT the lamb?s woolCONTROLLER/... PRODUCER?S PRODUCT the bird?s nestCONTROLLER/...
OBJECTIVE his assistantCONTROLLER/...
LOCATION Libya?s oil companyCONTROLLER/...
ATTRIBUTE Joe?s strengthCONTROLLER/...
MEMBER?S COLLECTION the colonel?s unitCONTROLLER/...
RECIPIENT Joe?s trophyRECIPIENT OBJECTIVE Joe?s rewardSUBJECTIVE PRODUCER?S PRODUCT Joe?s announcementSUBJECTIVE OBJECTIVE its changeSUBJECTIVE CONTROLLER/... Joe?s employeeSUBJECTIVE LOCATION Libya?s devolutionSUBJECTIVE MENTAL EXPERIENCER Joe?s resentmentOBJECTIVE MENTAL EXPERIENCER Joe?s concernOBJECTIVE LOCATION the town?s inhabitantsKINSHIP OTHER RELATIONAL NOUN his fianceeTable 7: Ambiguous/multiclass possessive examples.example in isolation.
This advantage did not ex-ist in the inter-annotator agreement study.5.4 Feature Ablation ExperimentsTo evaluate the importance of the different typesof features, the same experiment was re-run multi-ple times, each time including or excluding exactlyone feature template.
Before each variation, the Cparameter was retuned using 5-fold cross valida-tion on the training data.
The results for these runsare shown in Table 8.Based upon the leave-one-out and only-one fea-ture evaluation experiment results, it appears thatthe possessee word is more important to classifica-tion than the possessor word.
The possessor wordis still valuable though, with it likely being more377valuable for certain categories (e.g., TEMPORALand LOCATION) than others (e.g., KINSHIP).
Hy-pernym and gloss term features proved to be aboutequally valuable.
Curiously, although hypernymsare commonly used as features in NLP classifica-tion tasks, gloss terms, which are rarely used forthese tasks, are approximately as useful, at least inthis particular context.
This would be an interest-ing result to examine in greater detail.6 Related Work6.1 LinguisticsSemantic relation inventories for the English ?s-construction have been around for some time; Tay-lor (1996) mentions a set of 6 relations enumeratedby Poutsma (1914?1916).
Curiously, there is nota single dominant semantic relation inventory forpossessives.
A representative example of semanticrelation inventories for ?s-constructions is the onegiven by Quirk et al (1985) (presented earlier inSection 2).Interestingly, the set of relations expressed bypossessives varies by language.
For example,Classical Greek permits a standard of comparisonrelation (e.g., ?better than Plato?)
(Nikiforidou,1991), and, in Japanese, some relations are ex-pressed in the opposite direction (e.g., ?blue eye?sdoll?)
while others are not (e.g., ?Tanaka?s face?
)(Nishiguchi, 2009).To explain how and why such seemingly dif-ferent relations as whole+part and cause+effectare expressed by the same linguistic phenomenon,Nikiforidou (1991) pursues an approach ofmetaphorical structuring in line with the workof Lakoff and Johnson (1980) and Lakoff (1987).She thus proposes a variety of such metaphors asTHINGS THAT HAPPEN (TO US) ARE (OUR) POS-SESSIONS and CAUSES ARE ORIGINS to explainhow the different relations expressed by posses-sives extend from one another.Certainly, not all, or even most, of the linguis-tics literature on English possessives focuses oncreating lists of semantic relations.
Much of thework covering the semantics of the ?s construc-tion in English, such as Barker?s (1995) work,dwells on the split between cases of relationalnouns, such as sister, that, by their very definition,hold a specific relation to other real or conceptualthings, and non-relational, or sortal nouns (L?b-ner, 1985), such as car.Vikner and Jensen?s (2002) approach for han-dling these disparate cases is based upon Puste-jovsky?s (1995) generative lexicon framework.They coerce sortal nouns (e.g., car) into beingrelational, purporting to create a uniform analy-sis.
They split lexical possession into four types:inherent, part-whole, agentive, and control, withagentive and control encompassing many, if notmost, of the cases involving sortal nouns.A variety of other issues related to possessivesconsidered by the linguistics literature include ad-jectival modifiers that significantly alter interpre-tation (e.g., favorite and former), double geni-tives (e.g., book of John?s), bare possessives (i.e.,cases where the possessee is omitted, as in ?Eatat Joe?s?
), possessive compounds (e.g., driver?slicense), the syntactic structure of possessives,definitiveness, changes over the course of his-tory, and differences between languages in termsof which relations may be expressed by the geni-tive.
Representative work includes that by Barker(1995), Taylor (1996), Heine (1997), Partee andBorschev (1998), Rosenbach (2002), and Viknerand Jensen (2002).6.2 Computational LinguisticsThough the relation between nominals in theEnglish possessive construction has received lit-tle attention from the NLP community, there isa large body of work that focuses on similarproblems involving noun-noun relation interpreta-tion/paraphrasing, including interpreting the rela-tions between the components of noun compounds(Butnariu et al, 2010), disambiguating prepositionsenses (Litkowski and Hargraves, 2007), or anno-tating the relation between nominals in more arbi-trary constructions within the same sentence (Hen-drickx et al, 2009).Whereas some of these lines of work use fixedinventories of semantic relations (Lauer, 1995;Nastase and Szpakowicz, 2003; Kim and Bald-win, 2005; Girju, 2009; ?
S?aghdha and Copes-take, 2009; Tratz and Hovy, 2010), other work al-lows for a nearly infinite number of interpretations(Butnariu and Veale, 2008; Nakov, 2008).
RecentSemEval tasks (Butnariu et al, 2009; Hendrickx etal., 2013) pursue this more open-ended strategy.
Inthese tasks, participating systems recover the im-plicit predicate between the nouns in noun com-pounds by creating potentially unique paraphrasesfor each example.
For instance, a system mightgenerate the paraphrase made of for the noun com-378Feature Type Word(s) ResultsL R C G B N LOO OOGloss Terms  0.867 (0.04) 0.762 (0.08)Hypernyms  0.870 (0.04) 0.760 (0.16)Synonyms  0.873 (0.04) 0.757 (0.32)Word Itself  0.871 (0.04) 0.745 (0.08)Lexnames  0.871 (0.04) 0.514 (0.32)Last Letters  0.870 (0.04) 0.495 (0.64)Lexnames  0.872 (0.04) 0.424 (0.08)Link types  0.874 (0.02) 0.398 (0.64)Link types  0.870 (0.04) 0.338 (0.32)Word Itself  0.870 (0.04) 0.316 (0.16)Last Letters  0.872 (0.02) 0.303 (0.16)Gloss Terms  0.872 (0.02) 0.271 (0.04)Hypernyms  0.875 (0.02) 0.269 (0.08)Word Itself  0.874 (0.02) 0.261 (0.08)Synonyms  0.874 (0.02) 0.260 (0.04)Lexnames  0.874 (0.02) 0.247 (0.04)Part-of-speech List  0.873 (0.02) 0.245 (0.16)Part-of-speech List  0.874 (0.02) 0.243 (0.16)Dependency  0.872 (0.02) 0.241 (0.16)Part-of-speech List  0.874 (0.02) 0.236 (0.32)Link Types  0.874 (0.02) 0.236 (0.64)Word Itself  0.870 (0.02) 0.234 (0.32)Assigned Part-of-Speech  0.874 (0.02) 0.228 (0.08)Affixes  0.873 (0.02) 0.227 (0.16)Assigned Part-of-Speech  0.873 (0.02) 0.194 (0.16)Hypernyms  0.873 (0.02) 0.186 (0.04)Lexnames  0.870 (0.04) 0.170 (0.64)Text of Dependents  0.874 (0.02) 0.156 (0.08)Parts List  0.873 (0.02) 0.141 (0.16)Affixes  0.870 (0.04) 0.114 (0.32)Affixes  0.873 (0.02) 0.105 (0.04)Parts List  0.874 (0.02) 0.103 (0.16)Table 8: Results for leave-one-out and only-one feature template ablation experiment results for allfeature templates sorted by the only-one case.
L, R, C, G, B, and N stand for left word (possessor), rightword (possessee), pairwise combination of outputs for possessor and possessee, syntactic governor ofpossessee, all tokens between possessor and possessee, and the word next to the possessee (on the right),respectively.
The C parameter value used to train the SVMs is shown in parentheses.pound pepperoni pizza.
Computer-generated re-sults are scored against a list of human-generatedoptions in order to rank the participating systems.This approach could be applied to possessives in-terpretation as well.Concurrent with the lack of NLP research onthe subject is the absence of available annotateddatasets for training, evaluation, and analysis.
TheNomBank project (Meyers et al, 2004) providescoarse annotations for some of the possessive con-structions in the Penn Treebank, but only thosethat meet their criteria.7 ConclusionIn this paper, we present a semantic relation in-ventory for ?s possessives consisting of 17 rela-tions expressed by the English ?s construction, thelargest available manually-annotated collection ofpossessives, and an effective method for automat-ically assigning the relations to unseen examples.We explain our methodology for building this in-ventory and dataset and report a strong level ofinter-annotator agreement, reaching 0.78 Kappaoverall.
The resulting dataset is quite large, at21,938 instances, and crosses multiple domains,including news, fiction, and historical non-fiction.It is the only large fully-annotated publicly-available collection of possessive examples thatwe are aware of.
The straightforward SVM-based automatic classification system achieves87.4% accuracy?the highest automatic posses-sive interpretation accuracy figured reported todate.
These high results suggest that SVMs area good choice for automatic possessive interpre-379tation systems, in contrast to Moldovan and Bad-ulescu (2005) findings.
The data and softwarepresented in this paper are available for down-load at http://www.isi.edu/publications/licensed-sw/fanseparser/index.html.8 Future WorkGoing forward, we would like to examine the var-ious ambiguities of possessives described in Sec-tion 4.3.
Instead of trying to find the one-bestinterpretation for a given possessive example, wewould like to produce a list of all appropriate in-tepretations.Another avenue for future research is to studyvariation in possessive use across genres, includ-ing scientific and technical genres.
Similarly, onecould automatically process large volumes of textfrom various time periods to investigate changesin the use of the possessive over time.AcknowledgmentsWe would like to thank Charles Zheng and SarahBenzel for all their annotation work and valuablefeedback.ReferencesAdriana Badulescu and Dan Moldovan.
2009.
A Se-mantic Scattering Model for the Automatic Interpre-tation of English Genitives.
Natural Language En-gineering, 15:215?239.Chris Barker.
1995.
Possessive Descriptions.
CSLIPublications, Stanford, CA, USA.Cristina Butnariu and Tony Veale.
2008.
A Concept-Centered Approach to Noun-Compound Interpreta-tion.
In Proceedings of the 22nd International Con-ference on Computational Linguistics, pages 81?88.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-armuid ?
S?aghdha, Stan Szpakowicz, and TonyVeale.
2009.
SemEval-2010 Task 9: The Inter-pretation of Noun Compounds Using ParaphrasingVerbs and Prepositions.
In DEW ?09: Proceedingsof the Workshop on Semantic Evaluations: RecentAchievements and Future Directions, pages 100?105.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-armuid.
?
S?aghdha, Stan Szpakowicz, and TonyVeale.
2010.
SemEval-2010 Task 9: The Interpreta-tion of Noun Compounds Using Paraphrasing Verbsand Prepositions.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, pages39?44.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
Journalof Machine Learning Research, 9:1871?1874.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press, Cambridge, MA.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.Edward Gibbon.
1776.
The History of the Declineand Fall of the Roman Empire, volume I of The His-tory of the Decline and Fall of the Roman Empire.Printed for W. Strahan and T. Cadell.Roxanna Girju.
2009.
The Syntax and Seman-tics of Prepositions in the Task of Automatic In-terpretation of Nominal Phrases and Compounds:A Cross-linguistic Study.
Computational Linguis-tics - Special Issue on Prepositions in Application,35(2):185?228.Bernd Heine.
1997.
Possession: Cognitive Sources,Forces, and Grammaticalization.
Cambridge Uni-versity Press, United Kingdom.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid ?
S?aghdha, SebastianPad?, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz.
2009.
Semeval-2010 task8: Multi-Way Classification of Semantic Relationsbetween Pairs of Nominals.
In Proceedings ofthe Workshop on Semantic Evaluations: RecentAchievements and Future Directions (SEW-2009),pages 94?99.Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov,Diarmuid ?
S?aghdha, Stan Szpakowicz, andTony Veale.
2013.
Task Description: SemEval-2013 Task 4: Free Paraphrases of Noun Com-pounds.
http://www.cs.york.ac.uk/semeval-2013/task4/.
[Online; accessed1-May-2013].Su Nam Kim and Timothy Baldwin.
2005.
AutomaticInterpretation of Noun Compounds using Word-Net::Similarity.
Natural Language Processing?IJCNLP 2005, pages 945?956.Rudyard Kipling.
1894.
The Jungle Book.
Macmillan,London, UK.George Lakoff and Mark Johnson.
1980.
MetaphorsWe Live by.
The University of Chicago Press,Chicago, USA.George Lakoff.
1987.
Women, Fire, and DangerousThings: What Categories Reveal about the Mind.The University of Chicago Press, Chicago, USA.Mark Lauer.
1995.
Corpus Statistics Meet the NounCompound: Some Empirical Results.
In Proceed-ings of the 33rd Annual Meeting on Association forComputational Linguistics, pages 47?54.Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word-Sense Disambiguation ofPrepositions.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages24?29.380Sebastian L?bner.
1985.
Definites.
Journal of Seman-tics, 4(4):279.Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):330.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The NomBankProject: An Interim Report.
In Proceedings of theNAACL/HLT Workshop on Frontiers in Corpus An-notation.Dan Moldovan and Adriana Badulescu.
2005.
A Se-mantic Scattering Model for the Automatic Interpre-tation of Genitives.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Process-ing, pages 891?898.Preslav Nakov.
2008.
Noun Compound Interpreta-tion Using Paraphrasing Verbs: Feasibility Study.
InProceedings of the 13th International Conference onArtificial Intelligence: Methodology, Systems, andApplications, pages 103?117.Vivi Nastase and Stan Szpakowicz.
2003.
Explor-ing Noun-Modifier Semantic Relations.
In Fifth In-ternational Workshop on Computational Semantics(IWCS-5), pages 285?301.Kiki Nikiforidou.
1991.
The Meanings of the Gen-itive: A Case Study in the Semantic Structure andSemantic Change.
Cognitive Linguistics, 2(2):149?206.Sumiyo Nishiguchi.
2009.
Qualia-Based LexicalKnowledge for the Disambiguation of the JapanesePostposition No.
In Proceedings of the Eighth Inter-national Conference on Computational Semantics.Diarmuid ?
S?aghdha and Ann Copestake.
2009.
Us-ing Lexical and Relational Similarity to Classify Se-mantic Relations.
In Proceedings of the 12th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 621?629.Barbara H. Partee and Vladimir Borschev.
1998.
In-tegrating Lexical and Formal Semantics: Genitives,Relational Nouns, and Type-Shifting.
In Proceed-ings of the Second Tbilisi Symposium on Language,Logic, and Computation, pages 229?241.James Pustejovsky.
1995.
The Generative Lexicon.MIT Press, Cambridge, MA, USA.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
Longman Inc., NewYork.Anette Rosenbach.
2002.
Genitive Variation in En-glish: Conceptual Factors in Synchronic and Di-achronic Studies.
Topics in English linguistics.Mouton de Gruyter.Sidney Siegel and N. John Castellan.
1988.
Non-parametric statistics for the behavioral sciences.McGraw-Hill.John R. Taylor.
1996.
Possessives in English.
OxfordUniversity Press, New York.Stephen Tratz and Eduard Hovy.
2010.
A Taxonomy,Dataset, and Classifier for Automatic Noun Com-pound Interpretation.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 678?687.Stephen Tratz and Eduard Hovy.
2011.
A Fast, Accu-rate, Non-Projective, Semantically-Enriched Parser.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1257?1268.Carl Vikner and Per Anker Jensen.
2002.
A Seman-tic Analysis of the English Genitive.
Interation ofLexical and Formal Semantics.
Studia Linguistica,56(2):191?226.381
