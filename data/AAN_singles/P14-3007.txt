Proceedings of the ACL 2014 Student Research Workshop, pages 48?55,Baltimore, Maryland USA, June 22-27 2014.c?2014 Association for Computational LinguisticsLearning Grammar with Explicit Annotations for SubordinatingConjunctionsDongchen Li, Xiantao Zhang and Xihong WuKey Laboratory of Machine Perception and IntelligenceSpeech and Hearing Research CenterPeking University, Beijing, China{lidc,zhangxt,wxh}@cis.pku.edu.cnAbstractData-driven approach for parsing may suf-fer from data sparsity when entirely un-supervised.
External knowledge has beenshown to be an effective way to alleviatethis problem.
Subordinating conjunctionsimpose important constraints on Chinesesyntactic structures.
This paper proposes amethod to develop a grammar with hierar-chical category knowledge of subordinat-ing conjunctions as explicit annotations.Firstly, each part-of-speech tag of the sub-ordinating conjunctions is annotated withthe most general category in the hierar-chical knowledge.
Those categories arehuman-defined to represent distinct syn-tactic constraints, and provide an appropri-ate starting point for splitting.
Secondly,based on the data-driven state-split ap-proach, we establish a mapping from eachautomatic refined subcategory to the onein the hierarchical knowledge.
Then thedata-driven splitting of these categories isrestricted by the knowledge to avoid overrefinement.
Experiments demonstrate thatconstraining the grammar learning by thehierarchical knowledge improves parsingperformance significantly over the base-line.1 IntroductionProbabilistic context-free grammars (PCFGs) un-derlie most of the high-performance parsers(Collins, 1999; Charniak, 2000; Charniak andJohnson, 2005; Zhang and Clark, 2009; Chen andKit, 2012; Zhang et al, 2013).
However, a naivePCFG which simply takes the empirical rules andprobabilities off of a Treebank does not performwell (Klein and Manning, 2003; Levy and Man-ning, 2003; Bansal and Klein, 2012), becauseits context-freedom assumptions are too strong insome cases (e.g.
it assumes that subject and ob-ject NPs share the same distribution).
Therefore,a variety of techniques have been developed to en-rich PCFG (Klein and Manning, 2005; Matsuzakiet al, 2005; Zhang and Clark, 2011; Shindo et al,2012).Hierarchical state-split approach (Petrov et al,2006; Petrov and Klein, 2007; Petrov and Klein,2008a; Petrov and Klein, 2008b; Petrov, 2009)refines and generalizes the original grammars ina data-driven manner, and achieves state-of-the-art performance.
Starting from a completelymarkovized X-Bar grammar, each category is splitinto two subcategories.
EM is initialized with thisstarting point and used to climb the highly non-convex objective function of computing the jointlikelihood of the observed parse trees.
Then amerging step applies a likelihood ratio test to re-verse the least useful half part of the splits.
Learn-ing proceeds by iterating between those two stepsfor six rounds.
Spectral learning of latent-variablePCFGs (Cohen et al, 2012; Bailly et al, ; Co-hen et al, 2013b; Cohen et al, 2013a) is an-other effective manner of state-split approach thatprovides accurate and consistent parameter esti-mates.
However, this two complete data-drivenapproaches are likely to be hindered by the over-fitting issue.Incorporating knowledge (Zhang et al, 2013;Wu et al, 2011) to refine the categories in train-ing a parser has been proved to remedy theweaknesses of probabilistic context-free grammar(PCFG).
The knowledge contains content wordssemantic resources base (Fujita et al, 2010; Agirreet al, 2008; Lin et al, 2009), named entity cues(Li et al, 2013) and so on.
However, they arelimited in that they do not take into account theknowledge about subordinating conjunctions.Subordinating conjunctions are important in-dications for different syntactic structure, espe-48cially for Chinese.
For example, the subordinatingconjunction ????
(no matter what) is typicallyahead of a sentence with pros and cons of the sit-uation; on the contrary, a sufficient condition of-ten occurs after the subordinating conjunction ?XJ?
(if).
Those two cases are of distinct syntac-tic structure.
Figure 1 demonstrates that althoughthe sequences of the part-of-speech of the inputwords are similar, these two subordinating con-junctions exert quite different syntactic constraintsto the following clauses.IPVPVPVA??succeedADVPAD?notCC??orVPVA??succeedADVPCS?
?Nomatter(a) ????
(no matter what) is typically ahead of a sentencewith pros and cons of the situation.IPIPVPVPVPVA??succeedADVPAD?don?tADVPAD?
?stillNPPN\youADVPCSXJif(b) ?XJ?
(if) often precedes a sufficient condition.Figure 1: Different types of subordinating con-junctions indicate distinct syntactic structure.Based on the hierarchical state-split approach,this paper proposes a data-oriented model super-vised by our hierarchical subcategories of subordi-nating conjunctions.
In order to constrain the auto-matic subcategory refinement, we firstly establishthe mapping between the automatic clustered sub-categories and the predefined subcategories.
Thenwe employ a knowledge criterion to supervise thehierarchical splitting of these subordinating con-junction subcategories by the automatic state-splitapproach, which can alleviate over-fitting.
The ex-periments are carried out on Penn Chinese Tree-bank and Tsinghua Treebank, which verify thatthe refined grammars with refined subordinatingconjunction categories can improve parsing per-formance significantly.The rest of this paper is organized as follows.We first describe our hierarchical subcategories ofsubordinating conjunction.
Section 3 illustratesthe constrained grammar learning process in de-tails.
Section 4 presents the experimental evalua-tion and the comparison with other approaches.2 Hierarchical Subcategories ofSubordinating ConjunctionThe only tag ?CS?
for all the various subordinat-ing conjunctions is too coarse to indicate the in-tricate subordinating relationship.
The words in-dicating different grammatical features share thesame tag ?CS?, such as transition relationship,progression relationship, preference relationship,purpose relationship and condition relationship.
Ineach case, the context is different, and the subor-dinating conjunction is an obvious indication forthe parse disambiguation for the context.
The ex-isting resources for computational linguistic, likeHowNet (Dong and Dong, 2003) and Cilin (Meiet al, 1983), have classified all subordinating con-junctions as one category, which is too coarse tocapture the syntactic implication.To make use of the indication, we subdivide thesubordinating conjunctions according to its gram-matical features in our scheme.
Subordinatingconjunctions indicating each relationship is furthersubdivided into two subcategories: one is used be-fore the principal clause, the other is before thesubordinate clause.
For example, the conjunc-tions representing cause and effect contains ?be-cause?
and ?so?, where ?because?
should mod-ify the cause, and ?so?
should modify the effect.In addition, we found that there are several casesin the conditional clause.
Accordingly, we sub-divide the conditional subordinating conjunctionsinto seven types: assumption, universalization,49SubordinatingConjunction???????????????????????????????????????????????????????????????????????????????????Transition???????????????????LatterOf?Transition??
%Formerof?Transition??
?,Progression{FormerOf?Progression??
?LatterOf?Progression??
$?Preference{LatterOf?Preference??
?XFormerOf?Preference??
??LogicCoordination???LatterOfTheCoordinationqLogic?And??
??Condition????????????????????
?Assumption XJUniversalization??UnnecessaryConditionQ,InsufficientCondition=?SufficientCondition??NecessaryCondition?kEquality??Purpose{LatterOf?Purpose??
?BFormerOf?Purpose??
?
?CauseAndEffect{CauseduEffectJFigure 2: Hierarchical subcategories of subordinating conjunctions with examples.equality, sufficient condition, necessary condition,sufficient but unnecessary condition and necessarybut insufficient condition (concession).
The de-tailed hierarchical subcategories of subordinatingconjunctions are displayed in Figure 2.3 Parsing with Hierarchical CategoriesThe automatic state-split approach is designed torefine all symbols together through a data-drivenmanner, which takes the over-fitting risk.
Insteadof splitting and merging all symbols together auto-matically, we employ a knowledge-based criterionwith hierarchical refinement knowledge to con-straint the splitting of these new refined tags forsubordinating conjunctions.At the beginning, we produce a good startingannotation with the top subcategories in the hi-erarchical subcategories, which is of great use toconstraining the automatic splitting process.
Asdemonstrated in Figure 4, our parser is trained onthe good initialization with the automatic hierar-chical state-split process, and gets improvementscompared with the original training data.
For ex-ample, as shown in Figure 2, the category for%(but) and ?Cause?
for du(because) is anno-tated as the top category ?Transition?
and ?CauseAnd Effect?
respectively.However, during this process, only the mostgeneral hypernyms are used as the semantic rep-resentation of words, and the lower subcategoryknowledge in the hierarchy is not explored.
Thus,we further constraint the split of the subordinatingconjunctions subcategories to be consistent withthe hierarchical subcategories to alleviate the over-fitting issue.
The top class is only used as the start-ing annotations of POS tags to reduce the searchspace for EM in our method.
It is followed by thehierarchical state-split process to further refine thestarting annotations based on the hierarchical sub-categories.3.1 Mapping from Automatic Subcategoriesto Predefined SubcategoriesWith the initialization proposed above, the auto-matically split-merge approach produces a seriesof refined categories for each tag.
We restrict eachautomatically refined subcategory of subordinat-ing conjunctions to correspond to a special node50Figure 3: A schematic figure for the hierarchical state-split process of the tag ?CS?.
Each subcategoryof this tag has its own word set, and corresponds to one layer at the appropriate level in the hierarchicalsubcategories.in the hierarchical subcategories, as a hyponymof ?CS?.
The hierarchical subcategories are em-ployed in the hierarchical state-split process to im-pose restrictions on the subcategory refinement.First of all, it is necessary to establish the map-ping from each subcategory in the data-driven hi-erarchical subcategories to the subcategory in thepredefined hierarchical subcategories.
We trans-fer the method for semantic-related labels (Lin etal., 2009) to our case here.
The mapping is imple-mented with the word set related to each automati-cally refined granularity of clustered subordinatingconjunctions and the node at the special level inthe subcategory knowledge.
The schematic in Fig-ure 3 demonstrates this supervised splitting pro-cess for CS.
The left part of this figure is the wordsets of automatic clustered subcategories of theCS, which is split hierarchically.
As expressedby the lines, each subcategory corresponds to onenode in the right part of this figure, which is our hi-erarchical subcategory knowledge of subordinat-ing conjunctions.As it is shown in Figure 3, the original tag ?CS?treats all the words it produces as its word set.Upon splitting each coarse category into two morespecific subcategories, its word set is also cut intotwo subsets accordingly, through forcedly divid-ing each word in the word set into one subcategorywhich is most probable for this word in the lex-ical grammar.
And each automatic refinement ismapped to the most specific subcategory (that is tosay, the lowest node) that contains the entirely cor-responding word set in the human-defined knowl-edge.
On this basis, the new knowledge-based cri-terion is introduced to enrich and generalize thesesubcategories, with the purpose of fitting the re-finement to the subcategory knowledge rather thanthe training data.3.2 Knowledge-based Criterion forSubordinating Conjunctions RefinementWith the mapping between the automatic refinedsubcategories and the human-defined hierarchicalsubcategory knowledge, we could supervise theautomatic state refinement by the knowledge.Instead of being merged by likelihood, aknowledge-based criterion is employed, to decidewhether or not to go back to the upper layer inthe hierarchical subcategories and thus remove thenew subcategories of these tags.
The criterion isthat, we assume that the bottom layer in the hi-erarchical subcategories is special enough to ex-press the distinction of the subordinating conjunc-tions.
If the subcategories of the subordinatingconjunctions has gone beyond the bottom layer,then the new split subcategories are deemed to beunnecessary and should be merged back.
That isto say, once the parent layer of this new subcate-gory is mapped onto the most special subcategory,it should be removed immediately.
As illustrated51Treebank Train Dataset Develop Dataset Test DatasetCTB5 Articles 1-270 Articles 400-1151, 301-325 Articles 271-300TCT 16000 sentences 800 sentences 758 sentencesTable 1: Data allocation of our experiment.in Figure 3, if the node has no hyponym, this sub-category has been specialized enough according tothe knowledge, and thus the corresponding subcat-egory will stop splitting.By introducing a knowledge-based criterion,the issue is settled whether or not to further splitsubcategories from the perspective of predefinedknowledge.
To investigate the effectiveness of thepresented approach, several experiments are con-ducted on both Penn Chinese Treebank and Ts-inghua Treebank.
They reveal that the subcategoryknowledge of subordinating conjunctions is effec-tive for parsing.4 Experiments4.1 Experimental SetupWe present experimental results on both ChineseTreebank (CTB) 5.0 (Xue et al, 2002) (All tracesand functional tags were stripped.)
and TsinghuaTreebank (TCT) (Zhou, 2004).
All the experi-ments were carried out after six cycles of split-merge.The data set alocation is described in Table 1.We use the EVALB parseval reference imple-mentation (Sekine, 1997) for scoring.
Statisticalsignificance was checked by Bikel?s randomizedparsing evaluation comparator (Bikel, 2000).4.2 Parsing Performance with HierarchicalSubcategoriesWe presented a flexible approach which refinesthe subordinating conjunctions in a hierarchy fash-ion where the hierarchical layers provide differentgranularity of specificity.
To facilitate the compar-isons, we set up 6 experiments on CTB5.0 withdifferent strategies of choosing the subcategorylayers in the hierarchical subcategory knowledge:?
baseline: Training without hierarchical sub-category knowledge?
top: Choosing the top layer in hierarchi-cal subcategories (using ?Transition?, ?Con-dition?
, ?Purpose?
and so on)?
bottom: Choosing the bottom layer in hierar-chical subcategories (the most specified sub-categories)?
word: Substituting POS tag with the word it-self?
knowledge criterion: Automatically choos-ing the appropriate layer through the knowl-edge criterionFigure 4: Comparison of parsing performance foreach model in the split-merge cycles.Figure 4 shows the F1scores of the last 4 cy-cles in the 6 split-merge cycles.
The results arejust as expectation, through which we can tell thatthe ?top?
model performs slightly better than thebaseline owing to a better start point of the state-splitting.
This result confirms the value of ourinitial explicit annotations.
While the ?bottom?model doesn?t improve the performance due toexcessive refinement and causes over-fitting, the?word?
model behaves even worse for the samereason.
In the 5th split-merge cycle, the ?knowl-edge criterion?
model picks the appropriate layer52in hierarchical subcategories and achieves the bestresult.We also test our method on TCT.
Table 2 com-pares the accuracies of the baseline, initializationwith top subcategories and the ?knowledge cri-terion?
model, and confirms that the subcategoryknowledge helps parse disambiguation.Parser P R F1baseline 74.40 74.28 74.34top 75.12 75.17 75.14knowledge criterion 76.18 76.27 76.22Table 2: Our parsing performance with differentcriterions on TCT.4.3 Final ResultsOur final results are achieved using the ?knowl-edge criterion?
model.
As we can see from thetable 3, our final parsing performance is higherthan the unlexicalized parser (Levy and Manning,2003; Petrov, 2009) and the parsing system inQian and Liu (2012), but falls short of the systemsusing semantic knowledge of Lin et al (2009) andexhaustive word formation knowledge of Zhang etal.
(2013).Parser P R F1Levy(2003) 78.40 79.20 78.80Petrov(2009) 84.82 81.93 83.33Qian(2012) 84.57 83.68 84.13Zhang(2013) 84.42 84.43 84.43Lin(2009) 86.00 83.10 84.50This paper 85.93 82.87 84.32Table 3: Our final parsing performance comparedwith the best previous works on CTB5.0.The improvement on the hierarchical state-splitapproach verifies the effectiveness of the subcat-egory knowledge of subordinating conjunctionsfor alleviating over-fitting.
And the subcategoryknowledge could be integrated with the knowl-edge base employed in Lin et al (2009) and Zhanget al (2013) to contribute more on parsing accu-racy improvement.5 ConclusionIn this paper, we present an approach to constrainthe data-driven state-split method by hierarchi-cal subcategories of subordinating conjunctions,which appear as explicit annotations in the gram-mar.
The parsing accuracy is improved by thismethod owing to two reasons.
Firstly, the mostgeneral hypernym of subordinating conjunctionsexerts an initial restrict to the following splittingstep.
Secondly, the splitting process is confinedby a knowledge-based criterion with the human-defined hierarchical subcategories to avoid overrefinement.AcknowledgmentsWe thank Baidu for travel and conference sup-port for this paper.
We thank Meng Zhang andDingsheng Luo for their valuable advice.
Thiswork was supported in part by the National Ba-sic Research Program of China (973 Program) un-der grant 2013CB329304, the Research SpecialFund for Public Welfare Industry of Health undergrant 201202001, the Key National Social ScienceFoundation of China under grant 12&ZD119, theNational Natural Science Foundation of China un-der grant 91120001.ReferencesEneko Agirre, Timothy Baldwin, and David Martinez.2008.
Improving parsing and pp attachment perfor-mance with sense information.
Proceedings of ACL-08: HLT, pages 317?325.Rapha?el Bailly, Xavier Carreras P?erez, Franco MLuque, and Ariadna Julieta Quattoni.
Unsupervisedspectral learning of wcfg as low-rank matrix com-pletion.
Association for Computational Linguistics.Mohit Bansal and Daniel Klein.
2012.
An all-fragments grammar for simple and accurate parsing.Technical report, DTIC Document.Bikel.
2000.
Dan bikel?s random-ized parsing evaluation comparator.
Inhttp://www.cis.upenn.edu/dbikel/software.html.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd annual meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st North53American chapter of the association for computa-tional Linguistics conference, pages 132?139.
Asso-ciation for Computational Linguistics.Xiao Chen and Chunyu Kit.
2012.
Higher-order con-stituent parsing and parser combination.
In Pro-ceedings of the 50th annual meeting of the Associ-ation for Computational Linguistics: Short papers-Volume 2, pages 1?5.
Association for ComputationalLinguistics.Shay B Cohen, Karl Stratos, Michael Collins, Dean PFoster, and Lyle Ungar.
2012.
Spectral learning oflatent-variable pcfgs.
In Proceedings of the 50th an-nual meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 223?231.Association for Computational Linguistics.Shay B Cohen, Giorgio Satta, and Michael Collins.2013a.
Approximate pcfg parsing using tensor de-composition.
In Proceedings of NAACL-HLT, pages487?496.Shay B Cohen, Karl Stratos, Michael Collins, Dean PFoster, and Lyle Ungar.
2013b.
Experiments withspectral learning of latent-variable pcfgs.
In Pro-ceedings of NAACL-HLT, pages 148?157.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Zhendong Dong and Qiang Dong.
2003.
Hownet-a hy-brid language and knowledge resource.
In Proceed-ings of the international conference on natural lan-guage processing and knowledge engineering, pages820?824.
IEEE.Sanae Fujita, Francis Bond, Stephan Oepen, andTakaaki Tanaka.
2010.
Exploiting semantic infor-mation for hpsg parse selection.
Research on lan-guage and computation, 8(1):1?22.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st annual meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Dan Klein and Christopher D Manning.
2005.
Parsingand hypergraphs.
In New developments in parsingtechnology, pages 351?372.
Springer.Roger Levy and Christopher D Manning.
2003.
Isit harder to parse chinese, or the chinese treebank?In Proceedings of the 41st annual meeting on As-sociation for Computational Linguistics-Volume 1,pages 439?446.
Association for Computational Lin-guistics.Dongchen Li, Xiantao Zhang, and Xihong Wu.
2013.Improved chinese parsing using named entity cue.In Proceeding of the 13th international conferenceon parsing technology, pages 45?53.Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu,and Huisheng Chi.
2009.
Refining grammars forparsing with hierarchical semantic knowledge.
InProceedings of the 2009 conference on empiricalmethods in natural language processing: Volume 3-Volume 3, pages 1298?1307.
Association for Com-putational Linguistics.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProceedings of the 43rd annual meeting on Associ-ation for Computational Linguistics, pages 75?82.Association for Computational Linguistics.Jia-Ju Mei, YM Li, YQ Gao, et al 1983.
Chinesethesaurus (tong-yi-ci-ci-lin).Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human language tech-nologies 2007: the conference of the North Amer-ican chapter of the Association for ComputationalLinguistics, pages 404?411.Slav Petrov and Dan Klein.
2008a.
Discriminativelog-linear grammars with latent variables.
Advancesin neural information processing systems, 20:1153?1160.Slav Petrov and Dan Klein.
2008b.
Sparse multi-scalegrammars for discriminative latent variable parsing.In Proceedings of the conference on empirical meth-ods in natural language processing, pages 867?876.Association for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stinternational conference on computational linguis-tics and the 44th annual meeting of the Associationfor Computational Linguistics, pages 433?440.
As-sociation for Computational Linguistics.Slav Orlinov Petrov.
2009.
Coarse-to-Fine naturallanguage processing.
Ph.D. thesis, University ofCalifornia.Xian Qian and Yang Liu.
2012.
Joint chinese wordsegmentation, pos tagging and parsing.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 501?511.
Association for Computational Linguistics.Collins Sekine.
1997.
Evalb bracket scoring program.In http://nlp.cs.nyu.edu/evalb/.Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, andMasaaki Nagata.
2012.
Bayesian symbol-refinedtree substitution grammars for syntactic parsing.In Proceedings of the 50th annual meeting of theAssociation for Computational Linguistics: LongPapers-Volume 1, pages 440?448.
Association forComputational Linguistics.54Xihong Wu, Meng Zhang, and Xiaojun Lin.
2011.Parsing-based chinese word segmentation integrat-ing morphological and syntactic information.
InProceedings of 7th international conference on nat-ural language processing and knowledge engineer-ing (NLP-KE), pages 114?121.
IEEE.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese cor-pus.
In Proceedings of the 19th international confer-ence on computational linguistics-Volume 1, pages1?8.
Association for Computational Linguistics.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the chinese treebank using a global dis-criminative model.
In Proceedings of the 11th Inter-national Conference on Parsing Technologies, pages162?171.
Association for Computational Linguis-tics.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational linguistics, 37(1):105?151.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2013.
Chinese parsing exploiting characters.51st annual meeting of the Association for Compu-tational Linguistics.Qiang Zhou.
2004.
Annotation scheme for chinesetreebank.
Journal of Chinese information process-ing, 18(4):1?8.55
