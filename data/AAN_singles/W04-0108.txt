A Comparison of Two Different Approachesto Morphological Analysis of DutchGuy De Pauw1, Tom Laureys2, Walter Daelemans1, Hugo Van hamme21 University of Antwerp 2 K.U.LeuvenCNTS - Language Technology Group ESATUniversiteitsplein 1 Kasteelpark Arenberg 102610 Antwerpen (Belgium) 3001 Leuven (Belgium)firstname.lastname@ua.ac.be firstname.lastname@esat.kuleuven.ac.beAbstractThis paper compares two systems for computa-tional morphological analysis of Dutch.
Bothsystems have been independently designed asseparate modules in the context of the FLa-VoR project, which aims to develop a modulararchitecture for automatic speech recognition.The systems are trained and tested on the sameDutch morphological database (CELEX), andcan thus be objectively compared as morpho-logical analyzers in their own right.1 IntroductionFor many NLP and speech processing tasks, anextensive and rich lexical database is essential.Even a simple word list can often constitutean invaluable information source.
One of themost challenging problems with lexicons is theissue of out-of-vocabulary words.
Especially forlanguages that have a richer morphology suchas German and Dutch, it is often unfeasible tobuild a lexicon that covers a sufficient numberof items.
We can however go a long way intoresolving this issue by accounting for novel pro-ductions through the use of a limited lexiconand a morphological system.This paper describes two systems for morpho-logical analysis of Dutch.
They are conceived aspart of a morpho-syntactic language model forinclusion in a modular speech recognition enginebeing developed in the context of the FLaVoRproject (Demuynck et al, 2003).
The FLaVoRproject investigates the feasibility of using pow-erful linguistic information in the recognitionprocess.
It is generally acknowledged that moreaccurate linguistic knowledge sources improveon speech recognition accuracy, but are onlyrarely incorporated into the recognition process(Rosenfeld, 2000).
This is due to the fact thatthe architecture of most current speech recog-nition systems requires all knowledge sources tobe compiled into the recognition process at runtime, making it virtually impossible to includeextensive language models into the process.The FLaVoR project tries to overcome thisrestriction by using a more flexible architec-ture in which the search engine is split intotwo layers: an acoustic-phonemic decoding layerand a word decoding layer.
The reduction indata flow performed by the first layer allows formore complex linguistic information in the worddecoding layer.
Both morpho-phonologicaland morpho-syntactic modules function in theword decoding process.
Here we focus on themorpho-syntactic model which, apart from as-signing a probability to word strings, provides(scored) morphological analyses of word can-didates.
This morphological analysis can helpovercome the previously mentioned problem ofout-of-vocabulary words, as well as enhance thegranularity of the speech recognizer?s languagemodel.Successful experiments on introducing mor-phology into a speech recognition system haverecently been reported for the morphologicallyrich languages of Finnish (Siivola et al, 2003)and Hungarian (Szarvas and Furui, 2003), sothat significant advances can be expected forFLaVoR?s target language Dutch as well.
Butas the modular nature of the FLaVoR architec-ture requires the modules to function as stand-alone systems, we are also able to evaluate andcompare the modules more generally as mor-phological analyzers in their own right, whichcan be used in a wide range of natural lan-guage applications such as information retrievalor spell checking.In this paper, we describe and evaluate thesetwo independently developed systems for mor-phological analysis: one system uses a machinelearning approach for morphological analysis,while the other system employs finite state tech-niques.
After looking at some of the issues whendealing with Dutch morphology in section 2, wediscuss the architecture of the machine learn-Barcelona, July 2004Association for Computations LinguisticsACL Special Interest Group on Computational Phonology (SIGPHON)Proceedings of the Workshop of theing approach in section 3, followed by the finitestate method in section 4.
We discuss and com-pare the results in section 5, after which we drawconclusions.2 Dutch Morphology: Issues andResourcesDutch can be situated between English and Ger-man if we define a scale of morphological rich-ness in Germanic languages.
It lacks certainaspects of the rich inflectional system of Ger-man, but features a more extensive inflection,conjugation and derivation system than En-glish.
Contrary to English, Dutch for instanceincludes a set of diminutive suffixes: e.g.
ap-pel+tje (little apple) and has a larger set of suf-fixes to handle conjugation.Compounding in Dutch can occur in dif-ferent ways: words can simply be concate-nated (e.g.
plaats+bewijs (seat ticket)), theycan be conjoined using the ?s?
infix (e.g.
toe-gang+s+bewijs (entrance ticket)) or the ?e(n)?infix (e.g.
fles+en+mand (bottle basket)).
InDutch affixes are used to produce derivations:e.g.
aanvaard+ing (accept-ance).Morphological processes in Dutch account fora wide range of spelling alternations.
For in-stance: a syllable containing a long vowel iswritten with two vowels in a closed syllable (e.g.poot (paw)) or with one vowel in an open syl-lable (e.g.
poten (paws)).
Consonants in thecoda of a syllable can become voiced (e.g.
huis -huizen (house(s)) or doubled (e.g.
kip - kippen(chicken(s))).
These and other types of spellingalternations make morphological segmentationof Dutch word forms a challenging task.
Itis therefore not surprising to find that only ahandful of research efforts have been attempted.
(Heemskerk, 1993; Dehaspe et al, 1995; Vanden Bosch et al, 1996; Van den Bosch andDaelemans, 1999; Laureys et al, 2002).
Thislimited number may to some extent also be dueto the limited amount of Dutch morphologicalresources available.The Morphological Database of CELEXCurrently, CELEX is the only extensive andpublicly available morphological database forDutch (Baayen et al, 1995).
Unfortunately,this database is not readily applicable as an in-formation source in a practical system due toboth a considerable amount of annotation er-rors and a number of practical considerations.Since both of the systems described in this pa-per are data-driven in nature, we decided tosemi-automatically make some adjustments toallow for more streamlined processing.
A fulloverview of these adjustments can be found in(Laureys et al, 2004) but we point out some ofthe major problems that were rectified:?
Annotation of diminutive suffix and unan-alyzed plurals and participles was added(e.g.
appel+tje).?
Inconsistent treatment of several suffixeswas resolved (e.g.
acrobaat+isch (acro-bat+ic) vs. agnostisch (agnostic)).?
Truncation operations were removed(e.g.
filosoof+(isch+)ie (philosophy)).The Task: Morphological SegmentationThe morphological database of CELEX con-tains hierarchically structured and fully taggedmorphological analyses, such as the followinganalysis for the word ?arbeidsfilosofie?
(laborphilosophy):NHHHHHHHNarbeidN?N.NsNHHNfilosoofN?N.ieThe systems described in this paper deal withthe most important subtask of morphologicalanalysis: segmentation, i.e.
breaking up a wordinto its respective morphemes.
This type ofanalysis typically also requires the modeling ofthe previously mentioned spelling changes, ex-emplified in the above example (arbeidsfilosofie?
arbeid+s+filosoof+ie).
In the next 2 sec-tions, we will describe two different approachesto the segmentation/alternation task: one us-ing a machine-learning method, the other usingfinite state techniques.
Both systems howeverwere trained and tested on the same data, i.e.the Dutch morphological database of CELEX.3 A Machine Learning ApproachOne of the most notable research efforts model-ing Dutch morphology can be found in Van denBosch and Daelemans (1999).
Van den Boschand Daelemans (1999) define morphologicalanalysis as a classification task that can belearned from labeled data.
This is accomplishedat the level of the grapheme by recording a localcontext of five letters before and after the focusletter and associating this context with a mor-phological classification which not only predictsa segmentation decision, but also a graphemic(alternation) and hierarchical mapping.The system described in Van den Boschand Daelemans (1999) employs the ib1-igmemory-based learning algorithm, which usesinformation-gain to attribute weighting to thefeatures.
Using this method, the system is ableto achieve an accuracy of 64.6% of correctly an-alyzed word forms.
On the segmentation taskalone, the system achieves a 90.7% accuracy ofcorrectly segmented words.
On the morphemelevel, a 94.5% F-score is observed.Towards a Cascaded AlternativeThe machine learning approach to morpholog-ical analysis described in this paper is inspiredby the method outlined in Van den Bosch andDaelemans (1999), but with some notable differ-ences.
The first difference is the data set used:rather than using the extended morphologicaldatabase, we concentrated on the database ex-cluding inflections and conjugated forms.
Thesemorphological processes are to a great extentregular in Dutch.
As derivation and compound-ing pose the most challenging task when mod-eling Dutch morphology, we have opted to con-centrate on those processes first.
This allows usto evaluate the systems with a clearer insightinto the quality of the morphological analyzerswith respect to the hardest tasks.Further, the systems described in this paperuse the adjusted version of CELEX described insection 2, instead of the original dataset.
Themain reason for this can be situated in the con-text of the FLaVoR project: since our mor-phological analyzer needs to operate within aspeech recognition engine, it is paramount thatour analyzers do not have to deal with truncatedforms, as it would require us to hypothesizeunrealized forms in the acoustic input stream.Even though using the modified dataset doesnot affect the general applicability of the mor-phological analyzer itself, it does entail that adirect comparison with the results in Van denBosch and Daelemans (1999) is not possible.The overall design of our memory-based sys-tem for morphological analysis differs from theone described in Van den Bosch and Daelemans(1999) as our approach takes a more traditionalstance with respect to classification.
Ratherthan encoding different types of classificationin conglomerate classes, we have set up a cas-caded approach in which each classification task(spelling alternation, segmentation) is handledseparately.
This allows us to identify problemsat each point in the task and enables us to op-timize each classification step accordingly.
Toavoid percolation of bad classification decisionsat one point throughout the entire classifica-tion cascade, we ensure that all solutions are re-tained throughout the entire process, effectivelyturning later classification steps into re-rankingmechanisms.AlternationThe input of the morphological analyzer is aword form such as ?arbeidsfilosofie?.
As a firststep to arrive at the desired segmented output?arbeid+s+filosoof+ie?, we need to account forthe spelling alternation.
This is done as a pre-cursor to the segmentation task, since prelimi-nary experiments showed that segmentation ona word form like ?arbeidsfilosoofie?
is easier tomodel accurately than segmentation on the ini-tial word form.First, we record all possible alternations onthe training set.
These range from general al-ternations like doubling the vowel of the lastsyllable (e.g.
arbeidsfilosoof) to very detailed,almost word-specific alternations (e.g.
Europa?
euro).
Next, these alternations in the train-ing set are annotated and an instance base is ex-tracted.
Table 1 shows an example of instancesfor the word ?aanbidder?
(admirer).
In this ex-ample we see that alternation number 3 is asso-ciated with the double ?d?, denoting a deletionof that particular letter.Precision Recall F-scoreMBL 80.37% 88.12% 84.07%Table 2: Results for alternation experimentsThese instances were used to train thememory-based learner TiMBL (Daelemans etal., 2003).
Table 2 displays the results for thealternation task on the test set.
Even thoughthese appear quite modest, the only restrictionwe face with respect to consecutive processingsteps lies in the recall value.
The results showthat 255 out of 2,146 alternations in the test setwere not retrieved.
This means that we will notbe able to correctly analyze 2.27% of the testset (which contains 11,256 items).Left Context Focus Right Context Combined Class- - - - - a a n b i d ?a -aa aan 0- - - - a a n b i d d -aa aan anb 0- - - a a n b i d d e aan anb nbi 0- - a a n b i d d e r anb nbi bid 0- a a n b i d d e r - nbi bid idd 0a a n b i d d e r - - bid idd dde 0a n b i d d e r - - - idd dde der 3n b i d d e r - - - - dde der er- 0b i d d e r - - - - - der er- r?
0Table 1: Alternation instances for ?aanbidder?SegmentationA memory-based learner trained on an instancebase extracted from the training set constitutesthe segmentation system.
An initial feature setwas extracted using a windowing approach sim-ilar to the one described in Van den Bosch andDaelemans (1999).
Preliminary experimentswere however unable to replicate the high seg-mentation accuracy of said method, so that ex-tra features needed to be added.
Table 3 showsan example of instances extracted for the word?rijksontvanger?
(state collector).
Experimentson a held-out validation set confirmed both leftand right context sizes determined in Van denBosch and Daelemans (1999)1 .
The last twofeatures are combined features from the left andright context and were shown to be beneficialon the validation set.
They denote a group con-taining the focus letter and the two consecutiveletters and a group containing the focus letterand the three previous letters respectively.A numerical feature (?Dist?
in Table 3) wasadded that expresses the distance to the previ-ous morpheme boundary.
This numerical fea-ture avoids overeager segmentation, i.e.
a smallvalue for the distance feature has to be compen-sated by other strong indicators for a morphemeboundary.
We also consider the morpheme thatwas compiled since the last morpheme boundary(features in the column ?Current Morpheme?
).A binary feature indicates whether or not thismorpheme can be found in the lexicon extractedfrom the training set.
The next two featuresconsider the morpheme formed by adding thenext letter in line.Note however that the introduction of thesefeatures makes it impossible to precompile theinstance base for the test set, since for instance1Context size was restricted to four graphemes forreasons of space in Table 3.the distance to the previous morpheme bound-ary can obviously not be known before actualsegmentation takes place.
We therefore set upa server application and generated instances onthe fly.1,141,588 instances were extracted from thetraining set and were used to power a TiMBLserver.
The optimal algorithmic parameterswere determined with cross-validation on thetraining set2.
A client application extractedinstances from the test set and sent them tothe server on the fly, using the previous out-put of the server to determine the value of theabove-mentioned features.
We also adjusted theverbosity level of the output so that confidencescores were added to the classifier?s decision.A post-processing step generated all possiblesegmentations for all possible alternations.
Thepossible segmentations for the word ?apotheker?
(pharmacist) for example constituted the fol-lowing set: {(apotheek)(er), (apotheker),(apotheeker), (apothek)(er)}.
Next, the confi-dence scores of the classifier?s output were mul-tiplied for each possible segmentation to ex-press the overall confidence score for the mor-pheme sequence.
Also, a lexicon extracted fromthe training set with associated probabilitieswas used to compute the overall probability ofthe morpheme sequence (using a Laplace-typesmoothing process to account for unseen mor-phemes).
Finally, a bigram model computed theprobability of the possible morpheme sequencesas well.Table 4 describes the results at differentstages of processing and expresses the number ofwords that were correctly segmented.
Only us-ing the confidence scores output by the memory-based learner (equivalent to using a non-ranking2ib1-ig was used with Jeffrey divergence as distancemetric, no weighting, considering 11 nearest neighborsusing inverse linear weighting.Left Right Current NextContext Focus Context Dist Morpheme Morpheme Combined Class- - - - r i j k s 0 r 1 ri 0 rij ?r 0- - - r i j k s o 1 ri 0 rij 1 ijk ?ri 0- - r i j k s o n 2 rij 1 rijk 1 jks -rij 0- r i j k s o n t 3 rijk 1 rijks 0 kso rijk 1r i j k s o n t v 0 s 1 so 0 son ijks 1i j k s o n t v a 0 o 0 on 1 ont jkso 0j k s o n t v a n 1 on 1 ont 1 ntv kson 0k s o n t v a n g 2 ont 1 ontv 0 tva sont 0s o n t v a n g e 3 ontv 0 ontva 0 van ontv 0o n t v a n g e r 4 ontva 0 ontvan 0 ang ntva 0n t v a n g e r - 5 ontvan 0 ontvang 1 nge tvan 0t v a n g e r - - 6 ontvang 1 ontvange 0 ger vang 1v a n g e r - - - 0 e 1 er 1 er- ange 0a n g e r - - - - 1 er 1 er- 0 r?
nger 1Table 3: Instances for Segmentation Task for the word ?rijksontvanger?.Ranking Method Full Word ScoreMBL 81.36%Lexical 84.56%Bigram 82.44%MBL+Lexical 86.37%MBL+Bigram 85.79%MBL+Lexical+Bigram 87.57%Table 4: Results at different stages of post-processing for segmentation taskapproach) achieves a low score of 81.36%.
Us-ing only the lexical probabilities yields a betterscore, but the combination of the two achievesa satisfying 86.37% accuracy.
Adding bigramprobabilities to the product further improves ac-curacy to 87.57%.
In Section 5 we will look atthe results of the memory-based morphologicalanalyzer in more detail.4 A Finite State ApproachSince the invention of the two-level formalism byKimmo Koskenniemi (Koskenniemi, 1983) finitestate technology has been the dominant frame-work for computational morphological analysis.In the FLaVoR project a finite state morpholog-ical analyzer for Dutch is being developed.
Wehave several motivations for this.
First, untilnow no finite state implementation for Dutchmorphology was freely available.
In addition,finite state morphological analysis can be con-sidered a kind of reference for the evaluationof other analysis techniques.
In the currentproject, however, most important is the inher-ent bidirectionality of finite state morphologi-cal processing.
This bidirectionality should al-low for a flexible integration of the morphologi-cal model in the speech recognition engine as itleaves open a double option: either the morpho-logical system acts in analysis mode on word hy-potheses offered by the recognizer?s search algo-rithm, or the system works in generation modeon morpheme hypotheses.
Only future practi-cal implementation of the complete recognitionsystem will reveal which option is preferable.After evaluation of several finite state imple-mentations it was decided to implement the cur-rent system in the framework of the Xerox finitestate tools, which are well described and allowfor time and space efficient processing (Beesleyand Karttunen, 2003).
The output of the fi-nite state morphological analyzer is further pro-cessed by a filter and a probabilistic score func-tion, as will be detailed later.Morphotactics and OrthographicAlternationThe morphological system design is a composi-tion of finite state machines modeling morpho-tactics and orthographic alternations.
For mor-photactics a lexicon of 29,890 items was gen-erated from the training set (118 prefixes, 189suffixes, 3 infixes and 29,581 roots).
The itemswere divided in 23 lexicon classes, each of whichcould function as an item?s continuation class.The resulting finite state network has 24,858states and 61,275 arcs.The Xerox finite state tools allow for a speci-fication of orthographical alternation by meansof (conditional) replace rules.
Each replacerule compiles into a single finite state trans-ducer.
These transducers can be put in cas-cade or in parallel.
In the case at hand, alltransducers were put in cascade.
The result-ing finite state transducer contains 3,360 statesand 81,523 arcs.
The final transducer (a com-position of the lexical network and the ortho-graphical transducer) contains 29,234 states and106,105 arcs.Dealing with OvergenerationAs the finite state machine has no memorystack3, the use of continuation classes onlyallows for rather crude morphotactic model-ing.
For example, in ?on-ont-vlam-baar?
(un-in-flame-able) the noun ?vlam?
first combines withthe prefix ?ont?
to form a verb.
Next, the suffix?baar?
is attached and an adjective is built.
Fi-nally, the prefix ?on?
negates the adjective.
Thisexample shows that continuation classes cannotbe strictly defined: the suffix ?baar?
combineswith a verb but immediately follows a nounroot, while the prefix ?on?
requires an adjectivebut is immediately followed by another prefix.Obviously, such a model leads to overgenera-tion.
In practice, the average number of anal-yses per test set item is 7.65.
The maximumnumber of analyses is 1,890 for the word ?be-lastingadministratie?
(tax administration).In section 3 the numerical feature ?Dist?
wasused to avoid overeager segmentation.
We applya similar kind of filter to the segmentations gen-erated by the morphological finite state trans-ducer.
A penalty function for short morphemesis defined: 1- and 2-character morphemes re-ceive penalty 3, 3-character morphemes penalty1.
Both an absolute and relative4 penaltythreshold are set.
Optimal threshold values (11and 2.5 respectively) were determined on thebasis of the training set.
Analyses exceedingone of both thresholds are removed.
This filterproves quite effective as it reduces the averagenumber of analyses per item with 36.6% to 4.85.Finally, all remaining segmentation hypothe-ses are scored and ranked using an N-gram mor-pheme model.
We applied a bigram and trigrammodel, both using Katz back-off and modifiedKneser-Ney smoothing.
The bigram slightly3Actually, the Xerox finite state tools do allow for alimited amount of ?memory?
by a restricted set of uni-fication operations termed flag diacritics.
Yet, they areinsufficient for modeling long distance dependencies withhierarchical structure.4Relative to the number of morphemes.outperformed the trigram model, showing thatthe training data is rather sparse.
Tables 5, 6and 7 all show results obtained with the bigrammodel.Monomorphemic ItemsThe biggest remaining problem at this stage ofdevelopment is the scoring of monomorphemictest items which are not included as wordroots in the lexical finite state network.
Some-times these items do not receive any analysisat all, in which case we correctly consider themmonomorphemic.
Mostly however, monomor-phemes are wrongly analyzed as morphologi-cally complex.
Scoring all test items as poten-tially monomorphemic does not offer any solu-tion, as the items at hand were not in the train-ing data and thus receive just the score for un-known items.
This problem of spurious analysesaccounts for 57.23% of all segmentation errorsmade by the finite state system.5 Comparing the Two SystemsSystem 1-best 2-best 3-bestBaseline 18.64MBM 87.57 91.20 91.68FSM 89.08 90.87 91.01Table 5: Full Word Scores (%) on the segmen-tation taskTo evaluate both morphological systems, wedefined a training and test set.
Of the 124,136word forms in CELEX, 110,627 constitute thetraining set.
The test set is further split up intowords with only one possible analysis (11,256word forms) and words with multiple analyses(2,253).
Since the latter set requires morpho-logical processes beyond segmentation, we focusour evaluation on the former set in this paper.For the machine learning experiments, we alsodefined a held-out validation set of 5,000 wordforms, which is used to perform parameter op-timization and feature selection.Tables 5, 6 and 7 show a comparison of theresults5.
Table 5 describes the percentage ofwords in the test set that have been segmentedcorrectly.
We defined a baseline system whichconsiders all words in the test set as monomor-phemic.
Obviously this results in a very low5MBM: the memory-based morphological analyzer,FSM: the finite state morphological analyzerfull word score (which shows us that 18.64% ofthe words in the test set are actually monomor-phemic).
The finite state system seems to havea slight edge over the memory-based analyzerwhen we looking at the single best solution.
Yet,when we consider 2-best and 3-best scores, thememory-based analyzer in turn beats the finitestate analyzer.System Precision Recall F?=1Baseline 18.64 07.94 11.14MBM 91.63 90.52 91.07FSM 89.60 94.00 91.75Table 7: Precision and Recall Scores (%) (mor-phemes) on the segmentation taskWe also calculated Precision and Recall onmorpheme boundaries.
The results are dis-played in Table 6.
This score expresses howmany of the morpheme boundaries have beenretrieved.
We make a distinction between word-internal morpheme boundaries and all mor-pheme boundaries.
The former does not in-clude the morpheme boundaries at the end ofa word, while the latter does.
We provide thelatter in reference to Van den Bosch and Daele-mans (1999), but the focus lies on the resultsfor word-internal boundaries as these are non-trivial.
We notice that the memory-based sys-tem outperforms the finite state system, but thedifference is once again small.
However, whenwe look at Table 7 in which we calculate theamount of full morphemes that have been cor-rectly retrieved (meaning that both the startand end-boundary have been correctly placed),we see that the finite state method has the ad-vantage.Slight differences in accuracy put aside, wefind that both systems achieve similar scores onthis dataset.
When we look at the output, we donotice that these systems are indeed performingquite well.
There are not many instances wherethe morphological analyzer cannot be said tohave found a decent alternative analysis to thegold standard one.
In many cases, both systemseven come up with a more advanced morpholog-ical analysis: e.g.
?gekwetst?
(hurt) is featuredin the database as a monomorphemic artefact.Both systems described in this paper correctlysegment the word form as ?ge+kwets+t?, eventhough they have not yet specifically been de-signed to handle this type of inflection.When performing an error analysis of the out-put, one does notice a difference in the waythe systems have tried to solve the erroneousanalyses.
The finite state method often seemsto generate more morpheme boundaries thannecessary, while the reverse is the case for thememory-based system, which seems too eagerto revert to monomorphemic analyses when indoubt.
This behavior might also explain thereversed situation when comparing Table 6 toTable 7.
Also noteworthy is the fact that al-most 60% of the errors is made on wordformsthat both systems were not able to analyze cor-rectly.
Work is currently also underway to im-prove the performance by combining the rank-ings of both systems, as there is a large degreeof complementarity between the two systems.Each system is able to uniquely find the correctsegmentation for about 5% of the words in thetest set, yielding an upperbound performance of98.75% on the full word score for an optimallycombined system.6 ConclusionCurrent work in the project focuses on furtherdeveloping the morphological analyzer by try-ing to provide part-of-speech tags and hierar-chical bracketing properties to the segmentedmorpheme sequences in order to comply withthe type of analysis found in the morphologi-cal database of CELEX.
We will further try toincorporate other machine learning algorithmslike maximum entropy and support vector ma-chines to see if it is at all possible to overcomethe current accuracy threshold.
Algorithmicparameter ?degradation?
will be attempted toentice more greedy morpheme boundary place-ment in the raw output, in the hope that thepost-processing mechanism will be able to prop-erly rank the extra alternative segmentations.Finally, we will experiment on the full CELEXdata set (including inflection) as featured inVan den Bosch and Daelemans (1999).In this paper we described two data-drivensystems for morphological analysis.
Trainedand tested on the same data set, these systemsachieve a similar accuracy, but do exhibit quitedifferent processing properties.
Even thoughthese systems were originally designed to func-tion as language models in the context of a mod-ular architecture for speech recognition, theyconstitute accurate and elegant morphologicalanalyzers in their own right, which can be incor-porated in other natural language applicationsas well.System Precision Recall F?=1All Intern All Intern All InternBaseline 100 0 42.59 0 59.74 0MBM 94.15 89.71 93.00 87.81 93.57 88.75FSM 90.25 83.58 94.68 90.73 92.41 87.01Table 6: Precision and Recall Scores (%) (morpheme boundaries) on the segmentation taskAcknowledgementsThe research described in this paper was funded byIWT in the GBOU programme, project FLaVoR:Flexible Large Vocabulary Recognition: Incorporat-ing Linguistic Knowledge Sources Through a Modu-lar Recogniser Architecture.
(Project number 020192).http://www.esat.kuleuven.ac.be/spch/projects/FLaVoR.ReferencesR.H.
Baayen, R. Piepenbrock, and L. Gulik-ers.
1995.
The Celex Lexical Database (Re-lease2) [CD-ROM].
Linguistic Data Consor-tium, University of Pennsylvania, Philadel-phia, U.S.A.K.
R. Beesley and L. Karttunen, editors.
2003.Finite State Morphology.
CSLI Publications,Stanford.W.
Daelemans, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
2003.TiMBL: Tilburg memory based learner, ver-sion 5.0, reference guide.
ILK Technical Re-port 01-04, Tilburg University.
Availablefrom http://ilk.kub.nl.L.
Dehaspe, H. Blockeel, and L. De Raedt.1995.
Induction, logic and natural languageprocessing.
In Proceedings of the jointELSNET/COMPULOG-NET/EAGLESWorkshop on Computational Logic forNatural Language Processing.K.
Demuynck, T. Laureys, D. Van Compernolle,and H. Van hamme.
2003.
Flavor: a flexi-ble architecture for LVCSR.
In Proceedings ofthe 8th European Conference on Speech Com-munication and Technology, pages 1973?1976,Geneva, Switzerland, September.J.
Heemskerk.
1993.
A probabilistic context-free grammar for disambiguation in morpho-logical parsing.
Technical Report 44, itk,Tilburg University.K.
Koskenniemi.
1983.
Two-level morphology:A general computational model for word-formrecognition and production.
Ph.D. thesis, De-partment of General Linguistics, Universityof Helsinki.T.
Laureys, V. Vandeghinste, and J. Duchateau.2002.
A hybrid approach to compounds inLVCSR.
In Proceedings of the 7th Interna-tional Conference on Spoken Language Pro-cessing, volume I, pages 697?700, Denver,U.S.A., September.T.
Laureys, G. De Pauw, H. Van hamme,W.
Daelemans, and D. Van Compernolle.2004.
Evaluation and adaptation of theCELEX Dutch morphological database.
InProceedings of the 4th International Confer-ence on Language Resources and Evaluation,Lisbon, Portugal, May.R.
Rosenfeld.
2000.
Two decades of statisti-cal language modeling: Where do we go fromhere?
Proceedings of the IEEE, 88(8):1270?1278.V.
Siivola, T. Hirismaki, M. Creutz, and M. Ku-rimo.
2003.
Unlimited vocabulary speechrecognition based on morphs discovered inan unsupervised manner.
In Proceedings ofthe 8th European Conference on Speech Com-munication and Technology, pages 2293?2296,Geneva, Switzerland, September.M.
Szarvas and S. Furui.
2003.
Finite-statetransducer based modeling of morphosyntaxwith applications to Hungarian LVCSR.
InProceedings of the International Conferenceon Acoustics, Speech and Signal Processing,pages 368?371, Hong Kong, China, May.A.
Van den Bosch and W. Daelemans.
1999.Memory-based morphological analysis.
InProceedings of the 37th Annual Meeting ofthe Association for Computational Linguis-tics, pages 285?292, New Brunswick, U.S.A.,September.A.
Van den Bosch, W. Daelemans, and A. Wei-jters.
1996.
Morphological analysis as classi-fication: an inductive-learning approach.
InK.
Oflazer and H. Somers, editors, Proceed-ings of the Second International Conferenceon New Methods in Natural Language Pro-cessing, NeMLaP-2, Ankara, Turkey, pages79?89.
