Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 17?24,Sydney, July 2006. c?2006 Association for Computational LinguisticsNumbat: Abolishing Privileges when Licensing New Constituents inConstraint-oriented ParsingJean-Philippe ProstCentre for Language TechnologyMacquarie University, Sydney, Australiaand Laboratoire Parole et LangageUniversite?
de Provence, Aix-en-Provence, Francejpprost@ics.mq.edu.auAbstractThe constraint-oriented approaches to lan-guage processing step back from the gen-erative theory and make it possible, in the-ory, to deal with all types of linguistic re-lationships (e.g.
dependency, linear prece-dence or immediate dominance) with thesame importance when parsing an inpututterance.
Yet in practice, all implementedconstraint-oriented parsing strategies stillneed to discriminate between ?important?and ?not-so-important?
types of relationsduring the parsing process.In this paper we introduce a newconstraint-oriented parsing strategy basedon Property Grammars, which overcomesthis drawback and grants the same impor-tance to all types of relations.1 IntroductionIn linguistics, the term gradience is often used torefer to the notion of acceptability as a gradient,as opposed to a more classical all-or-none notion.The research goal of this project is to build an ex-perimental platform for computing gradience, i.e.for quantifying the degree of acceptability of aninput utterance.
We called this platform Numbat.In order to be able to quantify such a gradi-ent of acceptability with no a priori opinion onthe influence played by different types of linguis-tic relationships, we want to adopt a frameworkwhere no one type of (syntactic) relation (e.g.
de-pendency, immediate dominance, or linear prece-dence) is preferred over the other ones.
Althougha constraint-oriented (CO) paradigm such as Prop-erty Grammars (Blache, 2001) theoretically doesnot rely on any preferred relations, we observe thatthe parsing strategies implemented so far (Moraw-ietz and Blache, 2002; Balfourier et al, 2002;Dahl and Blache, 2004; VanRullen, 2005) do notaccount for such a feature of the formalism.
Thestrategy we have designed overcomes that prob-lem and allows for constituents to be licensed byany type of relation.
Not only does our approachmaintain a close connection between implementa-tion and underpinning theory, but it also allows forthe decisions made with respect to gradience to bebetter informed.
The purpose of the present pa-per is to present this new parsing strategy, and toemphasise how it ?abolishes the privilege?
usuallyonly granted to a subset of syntactic relationships.Section 2 presents some background informa-tion about the CO approaches and briefly intro-duces the Property Grammars formalism.
Section3 exposes and discusses the parsing strategy im-plemented in Numbat.
Section 4 then draws theconclusion.2 Constraint-oriented ApproachesThe main feature common to all Constraint-oriented approaches is that parsing is mod-elled as a Constraint Satisfaction Problem (CSP).Maruyama?s Constraint Dependency Grammar(CDG) (Maruyama, 1990) is the first formalismto introduce the parsing process as a CSP solver.Several extensions of CDG have then been pro-posed (Heinecke et al, 1998; Duchier, 1999; Fothet al, 2004).Menzel and colleagues (Heinecke et al, 1998;Foth et al, 2004) developed a weighted (or?graded?)
version of CDG.
Their parsing strate-gies are explored in the context of robust parsing.These strategies are based on an over-generationof candidate solutions.
In this approach the CSP isturned into an optimisation problem, where sub-optimal solutions are filtered out according to afunction of the weights associated to the violatedconstraints, and the notion of well-formedness isreplaced by one of optimality.
Indeed, the over-generation introduces inconsistencies in the con-straint system, which prevents the use of the con-17straint system as a set of well-formedness condi-tions, since even a well-formed utterance violatesa subset of constraints.
Consequently it is not pos-sible to distinguish an optimal structure of an ill-formed utterance from an optimal structure of awell-formed utterance.Duchier (1999) relies on set constraints and se-lection constraints1 to axiomatise syntactic well-formedness and provides a concurrent constraintprogramming account of the parsing process.
Withthe eXtended Dependency Grammar (XDG) (De-busmann et al, 2004) the notion of dependencytree is further extended to ?multi-dimensional?
de-pendency graph, where each dimension (e.g.
Im-mediate Dominance and Linear Precedence) is as-sociated with its own set of well-formedness con-ditions (called principles).
Duchier (2000) seesdependency parsing as a configuration problem,where given a finite set of components (nodes ina graph) and a set of constraints specifying howthese components may be connected, the task con-sists of finding a solution tree.It seems, to the best of our knowledge, that nei-ther of these works around XDG attempts to ac-count for ill-formedness.The Property Grammars (PG), introduced byBlache (Blache, 2001; Blache, 2005)2, step backfrom Dependency Grammar.
Solving the con-straint system no longer results in a dependencystructure but in a phrase structure, whose granular-ity may be tailored from a shallow one (i.e.
a col-lection of disconnected components) to a deep one(i.e.
a single hierarchical structure of constituents)according to application requirements3.
This fea-ture makes the formalism well suited for account-ing for both ill-formedness and well-formedness,which is a key requirement for our experimentalplatform.Introducing degrees of acceptability for an ut-terance does not mean indeed that it should bedone at the expense of well-formedness: we wantour model to account for ill-formedness and yetto also be able to recognise and acknowledgewhen an utterance is well-formed.
This require-1Although they are referred to with the same name bytheir respective authors, Duchier?s notion of selection con-straint is not to be confused with Dahl?s selection constraints(Dahl and Blache, 2004).
The two notions are significantlydifferent.2The Property Grammars were defined on the basis of the5P formalism (Be`s and Blache, 1999).3For a discussion regarding PG and parsing with variablegranularity see (VanRullen, 2005).ment rules out Optimality-theoretic frameworksas well as the ones based on Maruyama?s CDG.Note that this is not to say that the task couldnot be achieved in a CDG-based framework; sim-ply at this stage there is no work based on CDG,which would combine both an account of well-formedness and of optimality.
A CO frameworkbased on PG seems therefore best-suited for ourpurpose.
Meanwhile, though different parsingstrategies have been proposed for PG (Moraw-ietz and Blache, 2002; Balfourier et al, 2002;Dahl and Blache, 2004; VanRullen, 2005), noneof these strategies implements the possibility af-forded by the theory to rely on any type of con-straint in order to license a (possibly ill-formed)constituent.We will see in this paper how the parsing strat-egy implemented in Numbat overcomes this prob-lem.2.1 The Property Grammars Formalism2.1.1 TerminologyConstruction.
In PG a construction can be alexical item?s Part-of-Speech, a phrase, or top-level constructions such as, for example, theCaused-motion or the Subject-auxiliary Inversionconstructions.
The notion of construction is sim-ilar to the one in Construction Grammar (CxG)4,as in (Goldberg, 1995), where:Cx is a construction iff Cx is a form-meaning pair ?Fi, Si?
such that some as-pect of Fi or some aspect of Si is notstrictly predictable from Cx?s compo-nent parts or from other previously es-tablished constructions.In this paper we only focus on syntax.
For us, atthe syntactic level, a construction is defined by aform, where a form is specified as a list of proper-ties.
When building a traditional phrase structure(i.e.
a hierarchical structure of constituents) a con-struction can be simply seen as a non-terminal.Property.
A property is a constraint, whichmodels a relationship among constructions.
PGpre-defines several types of properties, which arespecified according to their semantics.
Moreover,the framework allows for new types to be defined.4Blache (2004) discussed how PG can be used as a formalframework for CxG.18In Numbat, a property type is also called a rela-tion.
Section 2.1.2 briefly presents some of thepre-defined property types and their semantics.Assignment.
In PG an assignment is a list ofconstituents.
Let?s consider, for example, the threeconstituents DET, ADJ and N, the following listsare possible assignments: [DET], [ADJ], [DET,ADJ], [ADJ, N], [DET, N], [DET, ADJ, N], etc..2.1.2 Some Pre-defined Property TypesHere are some property types pre-defined in PG.See (Blache, 2005) for more types and more de-tailed definitions.Notation.
We note:?
K a set of constructions, with {C, C1, C2} ?K;?
C a set of constituents, with {c, c1, c2} ?
C;?
A an assignment;?
ind a function such that ind(c,A) is the in-dex of c in A;?
cx a function such that cx(c) is the construc-tion of c;?
P(C1, C2)[c1, c2,A] or (C1 P C2)[c1, c2,A]the constraint such that the relation P param-etered with (C1, C2), applies to [c1, c2,A].Linear Precedence (?
).By definition, (C1 ?
C2)[c1, c2,A] holds iff??????
?cx(c1) = C1, andcx(c2) = C2, and{c1, c2} ?
A, andind(c1,A) < ind(c2,A)Exclusion (<).By definition, (C1 < C2)[c1, c2,A] holds iff??
?cx(c1) = C1, andcx(c2) = C2, and{c1, c2} ?
A 6= {c1, c2}Uniqueness (Uniq).By definition, Uniq(C)[c,A] holds iff??
?cx(c) = C, andc ?
A, and?c?
?
A\{c}, cx(c?)
6= C2.2 Related ProblemsCO parsing with PG is an intersection of differ-ent classes of constraint-related problems, each ofwhich is listed below.Configuration problem.
Given a set of com-ponents and a set of constraints specifying howthese components can be connected, a configu-ration problem consists of finding a solution treewhich connects the components together.
Deepparsing with PG is a configuration problem wherethe components are constituents, and the resultingstructure is a phrase structure.
By extension, a so-lution to such a problem is called a configuration.A configuration problem can be modelled with a(static) CSP.Dynamic CSP.
In our case the problem is actu-ally dynamic, in that the set of constraints to besolved evolves by the addition of new constraints.As we will see it later new constituents are inferredduring the parsing process, and subsequently newconstraints are dynamically added to the system.When dealing with deep parsing, i.e.
with well-formedness only, the problem can be tackled asa Dynamic CSP, and solving techniques such asLocal Search (Verfaillie and Schiex, 1994) can beapplied.Optimisation problem.
In order to account forill-formedness as well as well-formedness, weneed to allow constraint relaxation, which turnsthe problem into an optimisation one.
The ex-pected outcome is thus an optimal configurationwith respect to some valuation function.
Shouldthe input be well-formed, no constraints are re-laxed and the expected outcome is a full parse.Should the input be ill-formed, constraints are re-laxed and the expected outcome is either an opti-mal full parse or a set of (optimal) partial parses.3 Numbat Architecture3.1 The Parsing Strategy in NumbatRelying on a design pattern used in various optimi-sation techniques, such as dynamic programming,the top-level strategy adopted in Numbat consistsin three main steps:1. splitting the problem into overlapping sub-problems;2. solving the sub-problems?or building opti-mal sub-solutions;193. building an optimal global solution, using thesub-solutions.More specifically, the strategy adopted pro-ceeds by successive generate-and-test: the possi-ble models to local systems are generated, thentheir satisfiability is tested against the grammar.The partial solutions are re-injected in the pro-cess dynamically, and the basic process is iteratedagain.
Note that the generate-and-test method isnot compulsory and is only chosen here becauseit allows us to conveniently control and then filterthe assignments.Given an input utterance, the parsing process ismade up of a re-iteration of the basic followingsteps:1.
Building Site.
Build a set of constituents;2.
Assignation.
Build all the possible assign-ments, i.e.
all the possible combinations ofone or more constituents;3.
Checkpoint Alpha.
Filter out illegal assign-ments;4.
Appropriation.
For every assignment, iden-tify and build all the relevant propertiesamong its elements, which leaves us with aproperty store, i.e.
a constraint system;5.
Checkpoint Bravo.
Filter out illegal assign-ments and irrelevant properties;6.
Satisfaction.
Solve the constraint system;7.
Formation.
Identify forms of construction,i.e.
subsets of properties from the propertystore and nominate the corresponding candi-date constructions;8.
Polling booth.
Decide which of the candi-date constructions are licensed and carriedover to the next iteration;The process stops when no new constituent can bebuilt.Each of these steps is defined in the followingsection.3.1.1 Building SiteDuring the first iteration, this phase builds oneconstituent for each Part-of-Speech (POS) associ-ated with an input word.
From the second itera-tion onwards, new constituents are built providedthe candidate assignments output by the previousround.3.1.2 AssignationFrom one iteration to the next new assignmentsare built, involving at least one of the new con-stituents.
These constituents result from the pre-vious iteration.
Notice that the amount of new as-signments created by each iteration grows expo-nentially with the amount of constituents (the ?old?ones and the new ones).
Fortunately, the next stepwill filter out a large proportion of them.This phase of assignation is essential to the pro-cess, and makes Numbat different from any otherparsing strategy for PG.
The difference will bemade clear in the Satisfaction phase.3.1.3 Checkpoint AlphaIn Numbat we use a filtering profile to specifywhich combination of heuristics applies during theparsing process.
This feature proves to be veryuseful when performing experiments, as it allowsan incremental approach, in order to determine therelative importance of each of the criteria on gra-dience by turning on and off one or other heuristic.The heuristics play different roles.
They are pri-marily used to prune the search space as early aspossible in the process.
Meanwhile, most of themcapture language specific aspects (e.g.
Contigu-ity, see below).
These language specific heuris-tics are already present in previous works on PG inone form or another.
We are working in the sameframework and accept these restrictions, whichmight be relaxed by future work on the formalside.During Checkpoint Alpha the following heuris-tics may apply.Heuristic 1 (Distinct Constituents) An as-signment may contain no pairwise intersectingconstituents.That is, any two constituents may not have anyconstituent in common.
For example, the con-stituents {DET1, ADJ2} and {ADJ2, NOUN3} maynot belong to the same assignment, since they haveone constituent in common.Heuristic 2 (Contiguity) An assignment is a setof contiguous elements.This heuristic rules out crossing-over elements.Although this heuristic has little consequencewhen dealing with languages such as French orEnglish, it may have to be turned off for languageswith cross-serial dependencies such as Dutch.
Butif turned off, an additional problem then occurs20that the semantics of pre-defined property typesmust be re-defined.
The linear precedence, for in-stance, would need to account for the order be-tween two crossing-over phrases, which is not thecase in the current definition.
On the other hand,notice that long distance dependencies are notruled out by heuristic 2, since nested constituentsare still legal.3.1.4 AppropriationThis step has to do with the gathering of all theproperties relevant to every assignment from thegrammar.
This operation is made easier by pre-processing the grammar, which is done at an ini-tialisation step.
During this preliminary phase, alookup table is created for the grammar, where allthe properties are indexed by their operands.
Ev-ery property is also linked directly to the construc-tions for which it participates in the definition?i.e.
the constructions for which the property isa member of the form.
This table is actually ahash table, where the keys are the constructionson which the properties hold.
For example, theproperty (Det ?
Noun) is indexed by the coupleof constructions (Det, Noun).
And the property({Pronoun, Adv} < V) is indexed by the tripletsof constructions (Pronoun, Adv, V).
Thus, givenan assignment, i.e.
a set of constituents, all wehave to do here is to retrieve all the relevant prop-erties from the lookup table, using all the (rele-vant) combinations of constituents as keys.3.1.5 Checkpoint BravoFilters apply here, which aim to prune again thesearch space.
The following heuristics may apply.Heuristic 3 (Full Coverage) Every element of anassignment must be involved in at least one con-straint.
That is, for each element in an assignmentthere must be at least one constraint defined overthis element.Example 1 Consider the assignment A =?Det,N, V ?, and the grammar made up of the fol-lowing properties:VP ::= {V ?
NP} (1)NP ::= {Uniq(N), Det ?
N, N ?
Adj} (2)S ::= {NP ?
VP} (3)According to heuristic 3 A is ruled out, since the Velement is not covered by any constraints, whetherwe build an NP or a VP.Notice that this heuristic is semantically equiv-alent to the Constituency property present in earlyversions of PG5.
The Constituency property usedto specify which types of constituent (i.e.
con-structions) were legal ones (for a construction).Such a constraint is unnecessary since the infor-mation can be retrieved by simply listing all thetypes of constituents used in the definitions ofproperties.
In example 1 for instance, the setof legal constituents for the NP construction is[Det,N,Adj].A main reason for dealing with constituency asa filter rather than as a constraint is to improve ef-ficiency by reducing the amount of constraints inthe system.
Indeed, a filter aims to rule out con-straints, which are subsequently removed from theconstraint system.
If dealt with as a constraint it-self, Constituency would only make the constraintsystem more complex.Heuristic 3 raises the issue of ruling out assign-ments with ?free?
constituents, i.e.
constituentswhich are not connected to the rest of the assign-ment.
Such a situation may occur, for example,in the case of an unknown word, either becauseit is absent from the lexicon, or misspelled.
Wechoose to leave it up to the grammar writer to de-sign their own ad hoc solutions regarding how tohandle such cases.
It may be done, for instance,through the definition of a ?wildcard construc-tion?, and perhaps also a ?wildcard property type?,which will be used appropriately in the grammar.3.1.6 SatisfactionAt this stage, only legal assignments and rele-vant properties are kept in the system.
All the re-quired information for evaluating the properties isthus available and all we have to do now is to solvethe constraint system.The solver we use is implemented in ConstraintHandling Rules (CHR) (Fru?hwirth, 1994).
Un-like other CHR implementations of PG (Moraw-ietz and Blache, 2002; Dahl and Blache, 2004)where the semantics of the property types are en-coded in the handlers6?and therefore each typeof property requires a different handler?, the ap-proach we have adopted allows us to externalisethe semantics and to generalise the properties eval-uation with one single handler.
The algorithm un-5The Constituency property is discarded in the version ofPG underpinning Numbat.6A CHR handler is a rule of the general form (A => B| C), which can be read ?if A then (if B then C)?21derlying this handler can be expressed as follows:for each (list of n constituents, assignment, property)if (the list of n constituents and the assignment match theproperty?s ones)thenif (property is satisfied)then (tick property as being SATISFIED)else (tick property as being VIOLATED)The CHR handler takes the following form:listOfConstituents(Ccs) &&assignment(Asg) &&property(Pp) ==>Pp.isConsistentWith(Asg,Ccs) |(Pp.isSatisfied() ->sat(Pp) ; unSat(Pp)).3.1.7 FormationThis phase is concerned with identifying theconstructions in the grammar which can be trig-gered (i.e.
licensed) by the properties present inthe property store.
A construction is triggered byany of the properties which are used to define thisconstruction.
This task can be performed easilyby accessing them directly in the lookup table (seesection 3.1.4), using a property?s operands as thekey.
The constructions which are triggered arecalled target constructions.
We then build a con-stituent for each of these target construction.
Sucha constituent is called a candidate constituent.This phase basically builds constituent struc-tures.
During the next iteration these candidatesmay be used in turn as constituents.
The processthus accounts for recursive structures as well asnon-recursive ones.
Meanwhile, it is interesting toemphasise that building such a constituent struc-ture is not necessary when parsing with PG.
Wecould, for instance, deal with the whole sentenceat once as a sequence of word order constraints.This way no constituent structure would be neededto license infinite sets of strings.
In this case, theefficiency of such a process is something that hasbeen worked on extensively within the CSP field.What we are contributing is merely a representa-tion and translation to CSP, which allows us totake advantage of these efficiencies that decadesof other work have produced.Monotonic and Non-monotonic Constraints.The notions of Selection Constraint in (Dahl andBlache, 2004) and of non-Lacunar Constraintin (VanRullen, 2005) are equivalent and denotea class of constraint types, whose semantics ismonotonic, in that their satisfiability does notchange when new elements are added to the as-signment.
Constraint types such as Linear Prece-dence or Obligation, for example, are monotonic.On the other hand the constraint Uniq(C)[c,A](see 2.1.2), for example, is non-monotonic: if thecontextual assignment A grows?i.e.
if new con-stituents are added to it?the constraint needs tobe re-evaluated.
In parsing strategies where the as-signments are built dynamically by successive ad-ditions of new constituents, the evaluation of therelevant constraints is performed on the fly, whichmeans that the non-monotonic constraints need tobe re-evaluated every time the assignment grows.This problem is tackled in different ways, accord-ing to implementation.
But we observe that in allcases, the decision to trigger new candidate con-stituents relies only on the evaluation of the mono-tonic constraints.
The decision process usuallysimply ignores the non-monotonic ones.
Numbat,by fixing the assignments prior to evaluating thelocal constraint systems, includes both the mono-tonic and the non-monotonic constraints in the li-censing process (i.e.
in the Formation phase).3.1.8 Polling BoothThis phase is concerned with the election pro-cess, which leads to choosing the candidates whowill make it to the next iteration.The following heuristics may apply.Heuristic 4 (Minimum Satisfaction) An assign-ment is valid only if at least one constraint holdson any of its constituents.Notice that in all other implementations of PG thisheuristic is much more restrictive and requires thata monotonic constraint must hold.Heuristic 5 (Full Input Span) A valid (partial orfinal) solution to the parsing problem is either asingle constituent which spans exactly the inpututterance, or a combination of constituents (i.e.a combination of partial parses) which spans ex-actly the input utterance.In theory, we want the Polling Booth to build allthe candidate constituents we have identified, andre-inject them in the system for new iterations.
Inpractice, different strategies may apply in order toprune the search space, such as strategies based onthe use of a ranking function.
In our case, every it-eration of the parsing process only propagates one22valid combination of constituents to the next iter-ation (e.g.
the best one according to a valuationfunction).
Somehow such a strategy correspondsto always providing the main process with a ?dis-ambiguated?
set of input constituents from one it-eration to another.
This heuristic may also be usedas a termination rule.A question then arises regarding the relaxationpolicy: Do all the constraint types carry same im-portance with respect to relaxation?
This ques-tion addresses the relative importance of differ-ent constraint types with respect to acceptability.Does, for instance, the violation of a constraintof Linear Precedence between a Determiner anda Noun in a Noun Phrase have the same impacton the overall acceptability of the Noun Phrasethan the violation of Uniqueness of the Noun (stillwithin a Noun Phrase)?
From a linguistic point ofview, the answer to that question is not straight-forward and requires number of empirical studies.Some works have been carried out (Gibson, 2000;Keller, 2000), which aim to provide elements ofanswer in very targeted syntactic contexts.The impact that the relaxation of different con-straint types has on acceptability should not be bi-ased by a particular parsing strategy.
Thus, theframework provides the linguist (and the grammarwriter) with maximum flexibility when it comes todecide the cost of relaxing different types of con-straint on acceptability, since any type may be re-laxed.
Intuitively, one can clearly relax (in French)a constraint of Agreement in gender between de-terminer and noun; on the other hand one couldnot as easily relax constraints of type Obligation,which are often used to specify heads.
A com-plete breakdown of constraints into relaxable andnon-relaxable is future work.
But at the end, theparser just produces sets of satisfied and violatedconstraints, regardless of how important they are.There will then be a separate process for predict-ing gradience, where the relative importance ofparticular constraints in determining acceptabilitywill be decided experimentally.4 ConclusionIn this paper we have presented the constraint-oriented parsing strategy based on Property Gram-mars, that we have developed as part of the Num-bat platform.
We have also demonstrated that,unlike other existing parsers for PG, this strategydoes not privilege any particular type of propertywhen licensing a new constituent.
By doing so,this parser contributes to maintain a close connec-tion with the underpinning theory.
In the contextof robust parsing, where decisions must be madeon the basis of a balance between satisfied and vi-olated properties, it also allows the decision pro-cess to be better informed by providing it withmore grounding linguistic material concerning theinput.For the same reason, this contribution is alsofairly valuable in the context of our prime researchgoal, which is concerned with quantifying accept-ability.In further works we plan to evaluate the perfor-mance of the parser.
We also plan to use Numbatto run series of experiments on gradience, in orderto design and test a suitable valuation function tobe used to assess the degree of acceptability of aninput utterance.ReferencesJean-Marie Balfourier, Philippe Blache, and Tris-tan Van Rullen.
2002.
From Shallow to Deep Pars-ing Using Constraint Satisfaction.
In Proc.
of the6th Int?l Conference on Computational Linguistics(COLING 2002).Gabriel Be`s and Philippe Blache.
1999.
Proprie?te?s etanalyse d?un langage.
In TALN.Philippe Blache.
2001.
Les Grammaires de Proprie?te?s: des contraintes pour le traitement automatique deslangues naturelles.
Herme`s Sciences.Philippe Blache.
2004.
Constraints: an operationalframework for constructions grammars.
In ICCG-04, pages 25?26.Philippe Blache.
2005.
Property Grammars: A fullyconstraint-based theory.
In Henning Christiansen,Peter Rossen Skadhauge, and Jorgen Villadsen, ed-itors, Constraint Solving and Language Processing,volume 3438 of LNAI.
Springer.Veronica Dahl and Philippe Blache.
2004.
Directlyexecutable constraint based grammars.
In JourneesFrancophones de Programmation en Logique avecContraintes, pages 149?166, Angers, France.Ralph Debusmann, Denys Duchier, and Geert-Jan M.Kruijff.
2004.
Extensible Dependency Grammar: ANew Methodology.
In Proceedings of the 7th Inter-national Conference on Computational Linguistics(COLING 2004).Denys Duchier.
1999.
Axiomatizing DependencyParsing Using Set Constraints.
In Proceedings 6thMeeting on the Mathematics of Language, Orlando,FL.23Denys Duchier.
2000.
Configuration Of Labeled TreesUnder Lexicalized Constraints And Principles.
Toappear in the Journal of Language and Computation,December.Kilian Foth, Wolfgang Menzel, and Ingo Schr?der.2004.
Robust Parsing with Weighted Constraints.Natural Language Engineerings.Thom Fru?hwirth.
1994.
Theory and Practice of Con-straint Handling Rules.
The Journal of Logic Pro-gramming, 37((1-3)), October.
Special Issue onConstraint Logic Programming.Edward Gibson.
2000.
The Dependency LocalityTheory: A Distance-Based Theory of LinguisticComplexity.
In Alec Marantz, Yasushi Miyashita,and Wayne ONeil, editors, Image, Language, Brain,pages 95?126.
Cambridge, Mass., MIT Press.Adele Goldberg.
1995.
Constructions: A Con-struction Grammar Approach to Argument Struc-ture.
Chicago University Press.Johannes Heinecke, Ju?rgen Kunze, Wolfgang Menzel,and Ingo Shro?der.
1998.
Eliminative Parsing withGraded Constraints.
In Proc.
7th CoLing conf., 36thAnnual Meeting of the ACL, volume Coling?ACL?98, pages pp.
526?530, Montreal, Canada.Frank Keller.
2000.
Gradience in Grammar - Exper-imental and Computational Aspects of Degrees ofGrammaticality.
Ph.D. thesis, University of Edin-burgh.Hiroshi Maruyama.
1990.
Structural Disambiguationwith Constraint Propagation.
In Proceedings 28thAnnual Meeting of the ACL, pages pp.
31?38, Pit-tburgh, PA.Frank Morawietz and Philippe Blache.
2002.
Pars-ing natural languages with chr.
Under considerationfor publication in Theory and Practice of Logic Pro-gramming.Tristan VanRullen.
2005.
Vers une analyse syntaxiquea` granularite?
variable.
Ph.D. thesis, Universite?
deProvence, Informatique.Ge?rard Verfaillie and Thomas Schiex.
1994.
Solutionreuse in dynamic CSPs.
In AAAI ?94: Proc.
of thetwelfth national conf.
on AI (vol.
1), pages 307?312,Menlo Park, CA, USA.
American Ass.
for AI.24
