Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 89?98,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsSyllable and language model based features for detecting non-scorabletests in spoken language proficiency assessment applicationsAngeliki Metallinou, Jian ChengKnowledge Technologies, Pearson4040 Campbell Ave., Menlo Park, California 94025, USAangeliki.metallinou@pearson.com jian.cheng@pearson.comAbstractThis work introduces new methods for de-tecting non-scorable tests, i.e., tests thatcannot be accurately scored automatically,in educational applications of spoken lan-guage proficiency assessment.
Those in-clude cases of unreliable automatic speechrecognition (ASR), often because of noisy,off-topic, foreign or unintelligible speech.We examine features that estimate signal-derived syllable information and compareit with ASR results in order to detectresponses with problematic recognition.Further, we explore the usefulness of lan-guage model based features, both for lan-guage models that are highly constrainedto the spoken task, and for task inde-pendent phoneme language models.
Wevalidate our methods on a challengingdataset of young English language learn-ers (ELLs) interacting with an automaticspoken assessment system.
Our proposedmethods achieve comparable performancecompared to existing non-scorable detec-tion approaches, and lead to a 21% rela-tive performance increase when combinedwith existing approaches.1 IntroductionAutomatic language assessment systems are be-coming a valuable tool in education, and provideefficient and consistent student assessment thatcan complement teacher assessment.
Recently,there has been a great increase of English Lan-guage Learners (ELLs) in US education (Pear-son, 2006).
ELLs are students coming from non-English speaking backgrounds, and often requireadditional teacher attention.
Thus, assessing ELLstudent language proficiency is a key issue.Pearson has developed an automatic spoken as-sessment system for K-12 students and collecteda large dataset of ELL students interacting withthe system.
This is a challenging dataset, con-taining accented speech and speech from youngstudents.
Thus, for a small percentage of tests, itis technically challenging to compute an accurateautomatic score, often because of background/linenoise, off-topic or non-English responses or un-intelligible speech.
Such tests as referred to asnon-scorable.
Here, our goal is to propose newmethods for better classifying non-scorable testsand describe a system for non-scorable detection.We propose two new sets of features: sylla-ble based and language model (LM) based.
Theintuition is to contrast information from differ-ent sources when processing a test, in order todetect inconsistencies in automatic speech recog-nition (ASR), that often appear in non-scorabletests.
Syllable features measure similarity be-tween different estimates of syllable locations, oneextracted from ASR and the second from the rawsignal.
LM features measure similarity betweentwo ASR results, one using a standard item spe-cific word LM, and the second using a item inde-pendent phoneme LM.
Finally, an additional setof ASR confidence scores and log-likelihoods iscomputed using the proposed phoneme LM.Compared to existing work, our new methodsachieve comparable performance, although theyapproach the problem from a different perspective.Furthermore, our proposed features carry comple-mentary information to existing ones, and lead toa 21% relative performance increase when com-bined with existing work.2 Related WorkA review of spoken language technologies for ed-ucation can be found in Eskanazi (2009).
Thereis a considerable amount of previous work onautomatic speech assessment.
Pearson?s auto-mated speech scoring technologies that measurethe candidates?
speaking skill (pronunciation, flu-89ency, content) have been used in the Versant seriestests: English, Aviation English, Junior English,Spanish, Arabic, French, Dutch, Chinese (Bern-stein et al., 2000; Bernstein and Cheng, 2007;Cheng et al., 2009; Bernstein et al., 2010; Xu etal., 2012), and Pearson Test of English Academic(Pearson, 2011).
A non-scorable detection com-ponent (Cheng and Shen, 2011) is usually requiredfor such systems.
Educational Testing Service de-scribed a three-stage system on spoken languageproficiency scoring, that rates open-ended speechand includes a non-scorable detection component(Higgins et al., 2011).The system described here evaluates spoken En-glish skills of ELL students in manner and con-tent.
Past work on children?s automatic assess-ment of oral reading fluency includes systems thatscore performance at the passage-level (Cheng andShen, 2010; Downey et al., 2011) or word-level(Tepperman et al., 2007).Regarding detecting problematic responses inspeech assessment applications, related work in-cludes off-topic and non-scorable detection.
Non-scorable detection is a more general problemwhich includes not only off-topic responses, butalso noisy, poor quality, foreign or unintelligibleresponses, etc.
Higgins et al.
(2011) describe asystem that uses linear regression and four infor-mative features (number of distinct words, averageASR confidence, average and standard deviationof speech energy) for filtering out non-scorable re-sponses.
Yoon et al.
(2011) use a set of 42 signal-derived and ASR features along with a decisiontree classifier for non-scorable response detection.Many of their features are also extracted here forcomparison purposes (see Section 7).Chen and Mostow (2011) focus on off-topicdetection for a reading tutor application.
Theyuse signal features (energy, spectrum, cepstrumand voice quality features) and ASR features (per-centage of off-topic words) with a Support VectorMachine (SVM) classifier.
In our previous work(Cheng and Shen, 2011), we described an off-topicdetection system, where we computed three vari-ations for ASR confidence scores, along with fea-tures derived from acoustic likelihood, languagemodel likelihood, and garbage modeling.
Linearregression was used for classification.Here, we focus on non-scorable test detection,using aggregate information from multiple test re-sponses.
We propose new similarity features thatare derived from syllable location estimation andthe use of a item independent phoneme LM.3 The ELL student dataset3.1 The asessment systemPearson has developed an English proficiency as-sessment test, which has been administered in alarge number of K-12 ELL students in a U.S. state.The speaking component of the test is deliveredvia speakerphone, and the student performance isautomatically scored.
Each tests consists of a se-ries of spoken tasks which are developed by pro-fessional educators to elicit various displays ofspeaking ability.
There are repeat tasks, wherestudents repeat a short sentence, and open endedtasks, where students are required to answer ques-tions about an image or a topic, give instructions,ask a question about an image, etc.
Each testcontains multiple test prompts (also referred to asitems), some of which may belong to the sametask.
For example, for the ?question about image?task, there may be items refering to different im-ages.
Each test contains student responses to theitems.
Responses which are typically two or threesentences long.Figure 1 summarizes the components of Pear-son?s automatic proficiency assessment system.Assessment is done through combination of ASR,speech and text processing, and machine learn-ing to capture the linguistic content, pronuncia-tion and fluency of the student?s responses.
In thiswork, we focus on the lower block of Figure 1 thatillustrates the non-scorable detection component,whose purpose is to detect the tests that cannotbe reliably scored.
It exploits signal related andASR information to extract features that are laterused by a binary classifier to decide whether a testis scorable or not.
Our goal is to filter out non-scorable tests, to be graded by humans.
The pro-ficiency assessment system (upper part of Figure1) is described elsewhere (Cheng and Shen, 2010;Downey et al., 2011).
The word error rate (WER)over the test set using the final acoustic models isaround 35%.3.2 The non-scorable testsThis research focuses on data obtained from fourstages; elementary, primary, middle school andhigh school.
Those consist of 6000 spoken tests(1500 per stage), of which 4800 were used fortraining (1200 per stage) and the remaining 120090Figure 1: Outline of the assessment system.
Thelower block is the non-scorable test detectionmodule, that is the focus of this work.were used for testing.
Professional human graderswere recruited to provide a grade for each testresponse, following pre-defined rubrics per item.The grades per test are then summed up to com-pute an overall human grade in the range 0-14.Each test was double graded and the final humangrade was computed by averaging.
Our automaticscoring system was also used to estimate an over-all machine grade in the range 0-14 for each test,after considering all student responses.We define a test as non-scorable when the over-all machine and human grades differ by more than3 points.
For our dataset of 6000 tests, only 308(or approx.
5.1%) are non-scorable, according tothis definition.
Inspecting a subset of those tests,revealed various reasons that may cause a test tobe non-scorable.
Those include poor audio qual-ity (recording or background noise, volume tooloud or too soft), excessive mouth noises and vo-calizations, foreign language, off-topic responsesand unintelligible speech (extremely disfluent andmispronounced).
As expected, the above issuesare more common among younger test takers.
Al-though the cases above can be very different, acommonality is that their ASR results are unreli-able, therefore making subsequent automatic scor-ing inaccurate.
In the following sections, we pro-pose new methods for detecting problematic ASRoutputs and filtering out non-scorable tests.4 Syllable based featuresThe intuition behind the syllable based featuresis to compare information coming from the ASRcomponent with information that is derived di-rectly from the speech signal.
If these two sourcesare inconsistent, this may indicate problems in therecognition output, which often results in non-scorable tests.
Here, we focus on syllable loca-tions as the type of information to compare.
Sylla-ble locations can be approximated as the vowel lo-Figure 2: Two examples of mapping betweenASR-derived and signal-derived syllable loca-tions.cations of the speech recognition output.
Alterna-tively, they can be approximated using the speechpitch and intensity signals.
By examining inten-sity, we may find intensity peaks that are precededby intensity dips, and by examining pitch, we mayselect voiced intensity peaks as estimates of sylla-ble locations.
This method for identifying sylla-bles was described by Jong and Wempe (2009),and the number of syllables has been used as afeature for non-scorable detection in Yoon et al.(2011).
In this work, we propose to use the sylla-ble information in order to compute features thatmeasure similarities between signal-derived andASR-derived syllable locations.Assume that we have a sequence of n ASR-derived syllable locations: X = {x1, x2, .
.
.
, xn}and a sequence of m signal-derived locations:Y = {y1, y2, .
.
.
, ym}.
The first step in com-puting similarity features is finding a mapping be-tween the two sequences.
Specifically, we wantto find an appropriate mapping that pairs points(xi, yj), xi?
X, yj?
Y such that the smallestpossible distances d(xi, yj) are preferred.
Poten-tially inconsistent points can be discarded.
Twoexamples are presented in Figure 2.
In the up-per example n > m, therefore some syllable lo-cations of the longer sequence will be discarded(here location x3).
In the lower example, althoughn = m, the mapping that produces location pairswith the smallest distances is (x1, y2) and (x2, y3),while locations y1, x3will be discarded.
A map-ping (x3, y1) would be invalid as it violates timeconstraints, given the existing mappings.
We use agreedy algorithm for finding the mapping, whichiteratively searches all available valid paired loca-tions and finds the pair (xi, yj) with the smallest91distance.
A mapping (xi, yj) is valid if no timeconstraints are violated, e.g., there is no previouslyselected mapping (xk, yl), where k < i, l > j ork > i, l < j.The algorithm is described in Algorithm 1.
Ourimplementation is recursive: after finding the loca-tions that define the best available mapping at eachstep, the algorithm is recursively called to searchfor mappings between points that are both either atthe right subsequences, or at the left subsequences,with respect to the recent mapping.
The right sub-sequences contain points on the right of the se-lected mapping (similarly for left subsequences).That way we avoid searching for mappings thatwould violate the time constraints.Data: Syllable locations X = {x1, x2, .
.
.
, xn} andY = {y1, y2, .
.
.
, ym}Result: Mapping between X and Y.
Some locations in X or Ymay be discardedCompute pairwise distances: d(xi, yj), xi?
X, yj?
Y ;Set of pairs: E = mapping(1, n, 1,m);function mapping(i, j, k, l) returns set of pairs ;if i > j or k > l thenreturn empty setendFind min(d(xu, yv)), u ?
[i, j], v ?
[k, l];Enow= (u, v);//check left subsequencesEleft= mapping(i, u?
1, k, v ?
1);//check right subsequencesEright= mapping(u+ 1, j, v + 1, l);return union(Eleft, Enow, Eright);Algorithm 1: Compute mapping between ASR-based and signal-based syllable locationsBased on the mapping of Algorithm 1, we es-timate a set similarity features including numberof pairs found, number of syllables that were notpaired, the absolute length difference between thetwo sequences, as well as normalized versions ofthese features (we normalize the features by divid-ing with the maximum sequence length).
For ex-ample, in the lower part of Figure 2, there are twopairs and the longest sequence has length three,so the normalized number of pairs found is 2/3.Other features include average, min, max and stan-dard deviation of the distances of the pairs found,as well as the lengths of the two sequences.
Thesefeatures are a set of descriptions of the quality ofthe mapping or, in other words, of the similaritybetween the two syllable sequences.Algorithm 1 follows a greedy approach, how-ever, one could derive a similar mapping using dy-namic programming (DP) to minimize the averagedistance over all selected pairs.
In practice, we do(a) Number of syllable pairs found.
(b) Number of pairs over length of largest sequence.Figure 3: Visualization of feature values acrosstasks during a test, for sampled tests.
Scorabletests are in black, non-scorable in dashed red lines.For tasks that contain multiple responses, we aver-age the feature values of the responses of a task.not expect the choice of greedy or DP approachto greatly affect the final computed similarity fea-tures, and we chose the greedy approach for sim-plicity (although DP implementations could be ex-plored in the future).To visualize the feature information, we plot thefeature values across tasks of a test, for randomlysampled tests.
For tasks that contain multiple re-sponses (multiple items), we average the featurevalues of the responses of a task.
Figure 3(a) visu-alizes the number of pairs found.
Each test is rep-resented by a set of feature values (one per task)connected by lines between tasks.
Values of sometasks may be missing if they are undefined, e.g.,the student did not reply.
Scorable tests are repre-sented in black, and non-scorable tests in dashedred lines.
We notice that the number of pairs foundfor non-scorable tests is consistently low through-out the tasks of the test.
This agrees with ourintuition that for non-scorable tests there will beless similarity between the ASR-based and signal-based syllable locations, thus there will be fewerpairs between these two location sequences, com-pared to scorable tests.
Similarly, Figure 3(b) vi-92sualizes the normalized pairs found, and again thispercentage is lower for non-scorable tests, indicat-ing that fewer pairs were found for those tests.In our implementation, we computed the ASR-based syllable sequences by performing phoneme-level forced alignment of the ASR, and approxi-mating the syllable location as the center of eachvowel segment of the force aligned result.
Wecomputed the signal-based syllable sequence byaugmenting the open source Praat script developedby Jong and Wempe (2009) to output syllable lo-cations.
The syllable locations are approximate:computing the syllable detection accuracy wouldrequire human annotation of syllables in our cor-pus, which is out of the scope of this work.
Ourfocus is to estimate syllables well enough, so asto compute useful features.
Based on Figures 3(a)and (b) and the results of Section 9, our syllabledetection works sufficiently well for our purpose.5 Language model based featuresLanguage models (LMs) are used to model wordtransition probabilities in ASR systems, and arelearnt using large text corpora.
For cases wherethe input speech belongs to a specific topic, it iscommon to use constrained LMs, e.g., learn theword transitions from corpora related to the topicin question.
Here we explore the idea of using dif-ferent LMs for our ASR system, either highly con-strained or unconstrained ones, and comparing thecorresponding recognition results.
If the ASR re-sults of the two LMs are very different, then it islikely that the ASR result is problematic, whichmay be indicative of a non-scorable test.
To de-tect those cases, we introduce a set of features thatmeasure the similarity between ASR results ob-tained using different language models.In our system, each item requires the user to talkabout a specific known topic.
The default LM usedby our ASR component is item dependent and isconstrained on the topic of the item.
In general,this is beneficial to our system as it allows the ASRto focus on words that have a high enough likeli-hood of appearing given the item topic.
However,for some non-scorable tests, we noticed that thisapproach may result in misrecognizing phrasesthat are off-topic or non-English as valid on-topicphrases.
Therefore, we introduce an unconstrainedLM to detect cases where the constrained LMcauses our system to misrecognize topic specificwords that were not actually spoken.
We create the(a) Edit distance over longest sequence length.
(b) Length difference over longest sequence length.Figure 4: Visualization of feature values acrosstasks during a test, for sampled tests.
Scorabletests are in black, non-scorable in dashed red lines.unconstrained LM independent of the vocabularyused, by training a phoneme bigram LM that mod-els phoneme transition probabilities.
Hence, ourLM can handle out of vocabulary or non-Englishwords that often appear in non-scorable tests.We use item specific training data to build astandard bigram word LM for each item.
Forthe unconstrained LM, we perform phoneme-levelforced alignment of all training data, and builda item independent bigram phoneme LM.
Weperform recognition using both LMs and com-pare the resulting phoneme-level recognition re-sults.
Comparison is performed by computingthe edit distance between the two phoneme se-quences, obtained from the two LMs.
Edit dis-tance is a common metric for measuring similar-ity between sequences and estimates the minimumnumber of insertions, deletions or substitutions re-quired to change one sequence to the other.
Wecompute a number of similarity features includ-ing edit distance, length difference between the se-quences, number of insertions, deletions and sub-stitutions, as well as normalized versions of thosefeatures (by dividing with the maximum sequencelength).
We also include the two phoneme se-93quence lengths as features.Similarly to Section 4, we visualize feature in-formation by plotting feature values across tasks,for randomly sampled tests.
The resulting plotsfor edit distance and length difference between se-quences, both normalized, are presented in Figures4 (a) and (b) respectively.
Scorable tests are inblack and non-scorable in red dashed lines.
Intu-itively, the more dissimilar the sequences from thetwo LMs are, the larger the features values will befor these two features.
Looking at the plots, wenotice that, as expected, non-scorable tests tendto have larger feature values compared to scorableones.
This indicates that the proposed phonemeLM can help detect cases of non-scorable tests.6 Confidence featuresThe ASR component of the Pearson assessmentsystem assigns confidence scores to the recog-nized words.
Three variants of confidence scoresare computed: mconf (based on normalized acous-tic scores), aconf (based on force alignment andphoneme recognition) and lconf (lattice-based).They are described in our previous work (Chengand Shen, 2011), where they were used for off-topic detection.
Here, we use them for non-scorable detection, and compute them separatelyusing the ASR result obtained from either theitem specific word LM or the item independentphoneme LM.
For each confidence score, our fea-ture set includes the average score value overwords of a response, and the maximum, mini-mum and standard deviation.
We also compute theword-level recognition log-likelihood using eachof the two LMs, and include as features the aver-age, minimum, maximum and standard deviationof these log-likelihoods over words of a response.Although the confidence scores are described inCheng and Shen (2011), here we compute themusing the proposed phoneme LM (in addition tothe standard word LM), thus they are significantlydifferent from prior work.
Indeed, scores com-puted by the proposed phoneme LM prove to behighly informative (see Section 9, Table 3).7 Signal derived and ASR featuresA variety of signal-derived and ASR-based fea-tures have been used in the literature for non-scorable detection (Cheng and Shen, 2011; Yoonet al., 2011; Chen and Mostow, 2011), as well asrelated work on pronunciation and fluency assess-ment (Bernstein et al., 2010; Higgins et al., 2011).In this study, we extract and include a set of com-mon features.Signal-derived features typically describe prop-erties of the pitch and energy of the speech sig-nal.
Our feature set includes maximum and mini-mum energy, number of nonzero pitch frames andaverage pitch.
We also extract features that esti-mate noise level, specifically Signal to Noise Ra-tio (SNR).
For SNR estimation we used the NISTSpeech Quality Assurance package (NIST, 2009)Furthermore, we use features extracted from theASR result, including utterance duration, numberof words spoken, number of interword pauses, av-erage interword pause duration, average pause du-ration before the first spoken word (response la-tency), and number of hesitations.
Pauses, hesi-tations and response latency have been found in-formative of speaking fluency (Bernstein et al.,2010), and could be indicative of problematic,non-scorable tests.
We also compute two varia-tions of speech rate: words over total responseduration and words over duration of speech (ex-cluding pauses).
Other ASR features we use in-clude recognition log-likelihood, average LM like-lihood, number of phonemes pruned during recog-nition, and average word lattice confidence.
Weinclude some additional confidence-related fea-tures, like percentage of low confidence words orphonemes in the response (low confidence is de-fined based on an experimental threshold).We compute ASR features that are specific tothe task: either repeat or non-repeat.
For the re-peat tasks, where the student is asked to repeat aprompt sentence, we compute the number of inser-tions, deletions and substitutions of the recognizedresponse compared to the prompt, as well as thenumber and percentage of the recognized promptwords.
For the open question (non-repeat) tasks,where the student gives an open ended responseon a topic, we estimate the number of key wordsrecognized in the response, from a set of prede-fined, topic key words.Finally, we also include some features that arenot used in previous work, and were devised toenhance earlier versions of our non-scorable de-tection system.
Specifically, we compute the num-ber of clipped energy frames, where clipping hap-pens when energy exceeds a max value (often be-cause the student is speaking too close to the mi-crophone).
Also, we include an indicator feature94that indicates when the number of non zero pitchframes exceeds a certain threshold but the ASRrecognizes only silence.
This is a rough way todetect inconsistencies between the ASR and thepitch signal, where pitch indicates the presenceof voiced speech, but the ASR recognizes silence.Although these features are new, for simplicity, wemerge them in our baseline feature set.Overall, we have extracted a diverse and pow-erful set of representative features, which will bereferred as ?base?
feature set, and is summarizedin Table 1.Table 1: Summary of features included in the?Base?
feature setdescriptionsignal max and min energy, nonzero pitch frames, avg.
pitch, num-ber of clipped frames, SNRASRnumber of words spoken, pauses and hesitations, utteranceduration, speech rate (2 variations), avg.
interword pause du-ration, leading pause duration.ASR log-likelihood, average LM likelihood, number ofphonemes pruned, average word lattice confidence, percent-age of low confidence words and phonemesRepeat types: number of insertions, deletions, substitutions,number of recognized prompt words, percentage of recog-nized prompt words.Non repeat types: number of recognized key wordsindicator indicator when number of zero pitch frames exceeds a thresh-old while ASR recognizes silence8 Random forest classificationWe use a binary random forest classifier to decideif a test is scorable or not.
A random forest isan ensemble of decision trees where each tree de-cides using a subset of the features and the finaldecision is computed by combining the tree deci-sions (Breiman, 2001).
Random forests can takeadvantage of feature combinations to construct acomplex, non-linear decision region in the featurespace.
In addition, they can be trained fast, havegood generalization properties and do not requiremuch parameter tuning, which makes them popu-lar classifiers in the machine learning literature.
Inour work, a variety of diverse reasons may causea test to be non-scorable, including background orline/static noise, off-topic responses, non-Englishor unintelligible speech.
Random forests combinea number of decision trees that could correspondto the different sub-cases of our problem, there-fore they seem well suited for non-scorable testdetection.
According to our experiments, randomforests outperform decision trees and maximumentropy classifiers.
Therefore, all results of Sec-tion 9 are based on random forest classification.Up to now, we have described feature extractionfor each test response.
The non-scorable detectionsystem needs to aggregate multiple response in-formation to make an overall decision at the testlevel.
We can combine response-level features ina straightforward manner by taking their averageover a test.
However, responses may belong todifferent types of tasks, either repeat or non repeatones, and some of the features are task specific.Also, repeat task responses often resemble recitedspeech, while non-repeat ones tend to be morespontaneous.
To preserve this information, weseparately average features that belong to repeatresponses and non-repeat responses of a test (twoaveraged features are extracted per test and perfeature).
There are cases where a feature cannot beextracted for a response, because it is undefined,i.e., for a response that is recognized as silencethe average interword pause duration is undefined.Therefore, we also include the percentage of re-peat or non-repeat responses used to compute theaverage, i.e., two percentage features (for repeatand non-repeat cases) are extracted per test and perresponse.
More statistics could be extracted whencombining response features, e.g., variance, maxand min values, and others.
However, our pre-liminary experiments indicated that including justaverages and corresponding percentages is suffi-cient, and adding more statistics greatly increasesthe feature vector size without significant perfor-mance gains.
Therefore, our final feature set in-cludes only averages and percentages.9 Experiments and results9.1 Experimental setupOur experiments are organized in 5-fold cross val-idation: we randomly split the 6000 tests into fivesets, and each time we use three sets for trainingthe random forest classifier, one set as a develop-ment for optimizing the number of trees, and oneset for testing non-scorable classification perfor-mance.
Performance is computed after mergingall test set results.
Because the percentage of non-scorable tests in our dataset is small (approx.
5%)and random forests are trained with a degree ofrandomness, different runs of an experiment cancause small variations in performance.
To mini-mize this effect, we repeat each 5-fold cross vali-dation experiment 10 times, and report the averageand standard deviation over the 10 runs.Performance is estimated using the ROC curveof false acceptance rate (FAR) versus false rejec-95tion rate (FRR) for the binary (scorable vs non-scorable) classification task.
Our goal is to mini-mize the area under the curve (AUC), e.g., achievelow values for both FAR and FRR.
Our exper-iments were performed using the Python Scikit-Learn toolbox (Scikit-Learn, 2014).9.2 ResultsTable 2 presents the average AUC performanceof non-scorable test detection over 10 experi-ment runs, using different feature sets and ran-dom forests.
?Base?
denotes the set of standardASR-based and signal-based features described inSection 7.
Syllable based and LM based denotethe similarity features introduced in Sections 4and 5 respectively.
Finally, ?confidence?
denotesthe confidence and log-likelihood features derivedfrom the standard and the proposed phoneme LM,as described in Section 6.
According to our results,?base?
features are the best performing.
However,it is encouraging that our proposed comparison-based syllable and LM approaches, that approachthe problem from a different perspective and onlyuse similarity features, still achieve comparableperformance.Table 2: Average and standard deviation of AUCover 10 experiment runs for the different featuresets, and combinations of feature sets.features AUC (Avg ?
Std.dev)Base 0.102 ?
0.007Syllable based 0.122 ?
0.011LM based 0.123 ?
0.008Confidence 0.106 ?
0.011Feature CombinationBase+Syllable 0.091 ?
0.007Base+LM 0.091 ?
0.011Base+Confidence 0.094 ?
0.011All 0.097 ?
0.011Feature Combination (select top 300 features)Base+Syllable 0.092 ?
0.008Base+LM 0.088 ?
0.012Base+Confidence 0.097 ?
0.010All 0.092 ?
0.008Classifier Decision CombinationBase+Syllable 0.087 ?
0.008Base+LM 0.085 ?
0.007Base+Confidence 0.084 ?
0.007All 0.081 ?
0.006Table 2 also presents the AUC performance af-ter concatenating the feature vectors of differentfeature sets, under ?Feature Combination?.
We no-tice that adding separately each of our proposedsyllable based, LM based and confidence featuresto the base features improves performance by de-creasing AUC.
This further indicates that the pro-Figure 5: Test set ROC curves for different featuresets, and their combination using decision fusion(averaging), for one run of the experiment.posed features carry useful information, which iscomplementary to the ?base?
feature set.
Combin-ing all features together leads to a relatively smallperformance increase, possibly because the largenumber of features may cause overfitting.We also perform feature selection by selectingthe top 300 features from each feature set.
Fea-tures are ranked based on their positions in thetrees of the random forest: features closer to theroot of a tree contribute to the decision of a largernumber of input samples, thus, the expected frac-tion of the samples that each feature contributesto, can be used as an estimate of feature impor-tance.
We use Scikit-Learn to compute the fea-ture importance for each feature, and rank featuresbased on their average importance over the 10 ex-periment runs.
The results, presented in Table 2,show that feature selection helps for cases of largefeature sets, i.e., when combining all features to-gether.
However, for cases when fewer featuresare used, the performance does not change muchcompared to no feature selection.Finally, instead of concatenating features, weperform decision combination by averaging thedecisions of classifiers trained on different featuresets.
For simplicity, we perform simple averag-ing (in future when a larger train set will be avail-able, we can explore learning appropriate classifierweights, and performing weighted average).
Fromthe results of Table 2, we notice that this approachis advantageous and leads to a significant perfor-mance increase, especially when we combine allfour classifiers: one using existing ?base?
features,and the rest using our new features.
Overall, we96Table 3: Top-10 ranked features from each featureset.
?Av?
and ?prc?
denote that the feature is an av-erage or percentage respectively, while ?r?
and ?nr?denote that the feature is computed over repeat ornon-repeat responses, respectively.
For the confi-dence features, ?wLM?
denotes the feature is com-puted using regular bigram word LM and ?pLM?denotes proposed bigram phoneme LM.feature set descriptionsignal and ASRn hesitations (av, r) indicator pitch asr (av,r)min energy (av,r) n pitch frames (av, nr)n pitch frames (av,r) asr loglik (av, nr)asr loglik (av,r) min energy (av, nr)avg pitch( av,nr) snr (av, nr)syllable baseddiff lengths norm (av,r) diff lengths norm (av,nr)min pair distances(av,nr) diff lengths (av,r)n pairs norm (av,nr) diff lengths(av,nr)avg pair distances (av,r) min pair distances (av,r)n pairs norm (av,r) max pair distances (av,nr)LM basededit dist norm (av,r) diff lengths norm (av,r)n insert norm (av,r) edit dist norm (av,nr)diff lengths norm (av, nr) n insert norm (av,nr)n substitute norm (av,nr) min length (av,nr)min length (av,r) n substitute (av, nr)Confidenceavg aconf pLM (av,nr) min loglik pLM (av,r)min loglik pLM (av,nr) max lconf pLM (av,r)min aconf pLM (av,nr) stddev loglik pLM (av,nr)min loglik wLM (av,r) min aconf pLM (av,r)std loglik pLM (av,r) avg loglik pLM (av,r)achieved a decrease in AUC from 0.102 to 0.081,a 21% relative performance improvement.Figure 5 presents the ROC curves for one runof the experiment, for the four feature sets, andtheir combination using averaging of the classifierdecisions.
Combining all feature sets leads to alower AUC (thick black line).
We notice improve-ment especially in reducing false positives, e.g.,misclassifying scorable test as non-scorable.In Table 3, we present the top 10 selected fea-tures from each feature set, based on their aver-aged feature importance.
Overall, we notice thatboth repeat and non-repeat features are among thetop ranked, indicating that both types are infor-mative.
Only average features are among the topranked, which suggests that averages carry moreinformation than percentage features.
For the syl-lable and LM features, we can see many intuitivesimilarity features being at the top, such as differ-ence of sequence lengths, edit distance and num-ber of insertions (LM based feature set), and aver-age, min and max of the distances of paired sylla-bles (syllable based feature set).
For confidence,we note that many log-likelihood features are atthe top (here log-likelihood statistics are computedover words of a response).
Also, note that thegreat majority of top-ranked confidence featuresare computed using our proposed item indepen-dent phoneme LM, instead of the regular item de-pendent word LM, indicating the usefulness of thisapproach.10 Conclusion and future workIn this work, we have proposed new methods fordetecting non-scorable tests in spoken languageproficiency assessment applications.
Our meth-ods compare information extracted from differ-ent sources when processing a test, and computesimilarity features.
Inconsistencies suggest prob-lematic ASR, which is often indicative of non-scorable tests.
We extract two sets of features:syllable based, which compare syllable locationinformation, and LM based, which compare ASRobtained using item specific and item independentLMs.
Our proposed item independent LM is abigram phoneme LM, which can handle out-of-vocabulary or non-English words, that often ap-pear in non-scorable tests.
By visualizing the pro-posed similarity features, we verify that they canhighlight inconsistencies that are common in non-scorable tests.
We experimentally validate ourmethods in a large, challenging dataset of youngELLs interacting with the Pearson spoken assess-ment system.
Our features carry complementaryinformation to existing features, and when com-bined with existing work, they achieve a 21% rel-ative performance improvement.
Our final, non-scorable detection system combines the decisionsof four random forest classifiers: one using base-line features, and the rest using proposed features.We are currently collecting human annotationsfor non-scorable tests in our dataset, which containadditional annotation of the different non-scorablesubcases in these tests, e.g., noise, off-topic, non-English, unintelligible speech etc.
In the future,we plan to use these annotations to further validateour methods, as well as perform detailed evalua-tion of the usefulness of our proposed feature setsfor each of the non-scorable test subcases.ReferencesJ.
Bernstein and J. Cheng.
2007.
Logic and valida-tion of a fully automatic spoken English test.
InV.
M. Holland and F. P. Fisher, editors, The Pathof Speech Technologies in Computer Assisted Lan-guage Learning, pages 174?194.
Routledge, NewYork.J.
Bernstein, J.
De Jong, D. Pisoni, and B. Townshend.2000.
Two experiments on automatic scoring ofspoken language proficiency.
In Proc.
of STIL (Inte-grating Speech Technology in Learning).97J.
Bernstein, A.
Van Moere, and J. Cheng.
2010.
Vali-dating automated speaking tests.
Language Testing,27.L.
Breiman.
2001.
Random forests.
Machine Learn-ing, 45.W.
Chen and J. Mostow.
2011.
A tale of two tasks: De-tecting children?s off-task speech in a reading tutor.In Proc.
of Interspeech.J.
Cheng and J. Shen.
2010.
Towards accurate recog-nition for children?s oral reading fluency.
In Proc.
ofIEEE-SLT, pages 91?96.J.
Cheng and J. Shen.
2011.
Off-topic detection inautomated speech assessment applications.
In Proc.of Interspeech.J.
Cheng, J. Bernstein, U. Pado, and M. Suzuki.
2009.Automated assessment of spoken modern standardarabic.
In Proc.
of the Fourth Workshop on Innova-tive Use of NLP for Building Educational Applica-tions.R.
Downey, D. Rubin, J. Cheng, and J. Bernstein.2011.
Performance of automated scoring for chil-dren?s oral reading.
In Proc.
of the 6th Workshopon Innovative Use of NLP for Building EducationalApplications.M.
Eskanazi.
2009.
An overview of spoken languagetechnology for education.
Speech Communication,51.D.
Higgins, X. Xi, K. Zechner, and D. Williamson.2011.
A three-stage approach to the automated scor-ing of spontaneous spoken responses.
ComputerSpeech and Language, 25.N.
H. De Jong and T. Wempe.
2009.
Praat script todetect syllable nuclei and measure speech rate auto-matically.
Behavior research methods, 41:385?390.NIST.
2009.
The NIST SPeech Quality Assurance(SPQA) Package.
http://www.nist.gov/speech/tools/index.htm.G.
Pearson.
2006.
Ask NCELA No.1: How manyschool-aged English-language learners (ELLs) arethere in the U.S.?
Washington, D.C: Na-tional Clearing House for English-Language Ac-quisition and Language Instruction EducationalPrograms 2006, Retrieved Online February 2007at http://www.ncela.gwu.edu/expert/faq/01leps.htm.Pearson.
2011.
Skills and scoring in PTE Aca-demic.
http://www.pearsonpte.com/SiteCollectionDocuments/US_Skills_Scoring_PTEA_V3.pdf.Scikit-Learn.
2014.
The Scikit-Learn Machine Learn-ing Python Toolbox.
http://scikit-learn.org/.J.
Tepperman, M. Black, P. Price, S. Lee,A.
Kazemzadeh, M. Gerosa, M. Heritage, A. Al-wan, and S. Narayanan.
2007.
A Bayesian networkclassifier for word-level reading assessment.
InProc.
of Interspeech.X.
Xu, M. Suzuki, and J. Cheng.
2012.
An automatedassessment of spoken Chinese: Technical definitionof hanyu standards for content and scoring develop-ment.
In Proc.
of the Seventh International Confer-ence & Workshops on Technology & Chinese Lan-guage Teaching.S.-Y.
Yoon, K. Evanini, and K. Zechner.
2011.
Non-scorable response detection for automated speakingproficiency assessment.
In Proc.
of the Sixth Work-shop on Innovative Use of NLP for Building Educa-tional Applications.98
