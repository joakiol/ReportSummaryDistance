Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 177?180,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLearning Semantic Links from a Corpus ofParallel Temporal and Causal RelationsSteven BethardInstitute for Cognitive ScienceDepartment of Computer ScienceUniversity of ColoradoBoulder, CO 80309, USAsteven.bethard@colorado.eduJames H. MartinInstitute for Cognitive ScienceDepartment of Computer ScienceUniversity of ColoradoBoulder, CO 80309, USAjames.martin@colorado.eduAbstractFinding temporal and causal relations is cru-cial to understanding the semantic structureof a text.
Since existing corpora provide noparallel temporal and causal annotations, weannotated 1000 conjoined event pairs, achiev-ing inter-annotator agreement of 81.2% ontemporal relations and 77.8% on causal re-lations.
We trained machine learning mod-els using features derived from WordNet andthe Google N-gram corpus, and they out-performed a variety of baselines, achievingan F-measure of 49.0 for temporals and 52.4for causals.
Analysis of these models sug-gests that additional data will improve perfor-mance, and that temporal information is cru-cial to causal relation identification.1 IntroductionWorking out how events are tied together temporallyand causally is a crucial component for successfulnatural language understanding.
Consider the text:(1) I ate a bad tuna sandwich, got food poisoningand had to have a shot in my shoulder.
wsj 0409To understand the semantic structure here, a systemmust order events along a timeline, recognizing thatgetting food poisoning occurred BEFORE having ashot.
The system must also identify when an eventis not independent of the surrounding events, e.g.got food poisoning was CAUSED by eating a badsandwich.
Recognizing these temporal and causalrelations is crucial for applications like question an-swering which must face queries like How did he getfood poisoning?
or What was the treatment?Currently, no existing resource has all the neces-sary pieces for investigating parallel temporal andcausal phenomena.
The TimeBank (Pustejovsky etal., 2003) links events with BEFORE and AFTERrelations, but includes no causal links.
PropBank(Kingsbury and Palmer, 2002) identifies ARGM-TMPand ARGM-CAU relations, but arguments may onlybe temporal or causal, never both.
Thus existingcorpora are missing some crucial pieces for study-ing temporal-causal interactions.
Our research aimsto fill these gaps by building a corpus of paralleltemporal and causal relations and exploring machinelearning approaches to extracting these relations.2 Related WorkMuch recent work on temporal relations revolvedaround the TimeBank and TempEval (Verhagen etal., 2007).
These works annotated temporal relationsbetween events and times, but low inter-annotatoragreement made many TimeBank and TempEvaltasks difficult (Boguraev and Ando, 2005; Verha-gen et al, 2007).
Still, TempEval showed that on aconstrained tense identification task, systems couldachieve accuracies in the 80s, and Bethard and col-leagues (Bethard et al, 2007) showed that temporalrelations between a verb and a complement clausecould be identified with accuracies of nearly 90%.Recent work on causal relations has also foundthat arbitrary relations in text are difficult to annotateand give poor system performance (Reitter, 2003).Girju and colleagues have made progress by select-ing constrained pairs of events using web search pat-terns.
Both manually generated Cause-Effect pat-terns (Girju et al, 2007) and patterns based on nouns177Full Train TestDocuments 556 344 212Event pairs 1000 697 303BEFORE relations 313 232 81AFTER relations 16 11 5CAUSAL relations 271 207 64Table 1: Contents of the corpus and its train/test sectionsTask Agreement Kappa FTemporals 81.2 0.715 71.9Causals 77.8 0.556 66.5Table 2: Inter-annotator agreement by task.linked causally in WordNet (Girju, 2003) were usedto collect examples for annotation, with the result-ing corpora allowing machine learning models toachieve performance in the 70s and 80s.3 Conjoined Events CorpusPrior work showed that finding temporal and causalrelations is more tractable in carefully selected cor-pora.
Thus we chose a simple construction thatfrequently expressed both temporal and causal rela-tions, and accounted for 10% of all adjacent verbalevents: events conjoined by the word and.Our temporal annotation guidelines were basedon the guidelines for TimeBank and TempEval, aug-mented with the guidelines of (Bethard et al, 2008).Annotators used the labels:BEFORE The first event fully precedes the secondAFTER The second event fully precedes the firstNO-REL Neither event clearly precedes the otherOur causal annotation guidelines were based onparaphrasing rather than the intuitive notions ofcause used in prior work (Girju, 2003; Girju et al,2007).
Annotators selected the best paraphrase of?and?
from the following options:CAUSAL and as a result, and as a consequence,and enabled by thatNO-REL and independently, and for similar reasonsTo build the corpus, we first identified verbsthat represented events by running the system of(Bethard and Martin, 2006) on the TreeBank.
Wethen used a set of tree-walking rules to identify con-joined event pairs.
1000 pairs were annotated bytwo annotators and adjudicated by a third.
Table 1SADVPRBThenNPPRPtheyVPVP CC VPVBDtookNPDTtheNNartPPTOtoNPNNPAcapulcoandbeganSVBDVPTOtoVPVBtradeNPsome of itPPfor cocaineFigure 1: Syntactic tree from wsj 0450 with events tookand began highlighted.and Table 2 give statistics for the resulting corpus1.The annotators had substantial agreement on tem-porals (81.2%) and moderate agreement on causals(77.8%).
We also report F-measure agreement, sinceBEFORE, AFTER and CAUSAL relations are more in-teresting than NO-REL.
Annotators had F-measureagreement of 71.9 on temporals and 66.5 causals.4 Machine Learning MethodsWe used our corpus for machine learning experi-ments where relation identification was viewed aspair-wise classification.
Consider the sentence:(2) The man who had brought it in for an esti-mate had [EVENT returned] to collect it and was[EVENT waiting] in the hall.
wsj 0450A temporal classifier should label returned-waitingwith BEFORE since returned occurred first, and acausal classifier should label it CAUSAL since thisand can be paraphrased as and as a result.We identified both syntactic and semantic featuresfor our task.
These will be described using the ex-ample event pair in Figure 1.
Our syntactic featurescharacterized surrounding surface structures:?
The event words, lemmas and part-of-speech tags,e.g.
took, take, VBD and began, begin, VBD.?
All words, lemmas and part-of-speech tags in theverb phrases of each event, e.g.
took, take, VBDand began, to, trade, begin, trade, VBD,TO,VB.?
The syntactic paths from the first event tothe common ancestor to the second event, e.g.VBD>VP, VP and VP<VBD.1Train: wsj 0416-wsj 0759.
Test: wsj 0760-wsj 0971.verbs.colorado.edu/?bethard/treebank-verb-conj-anns.xml178?
All words before, between and after the event pair,e.g.
Then, they plus the, art, to, Acapulco, andplus to, trade, some, of, it, for, cocaine.Our semantic features encoded surrounding wordmeanings.
We used WordNet (Fellbaum, 1998) rootsynsets (roots) and lexicographer file names (lex-names) to derive the following features:?
All event roots and lexnames, e.g.
take#33,move#1 .
.
.
body, change .
.
.
for took and be#0,begin#1 .
.
.
change, communication .
.
.
for began.?
All lexnames before, between and after the eventpair, e.g.
all plus artifact, location, etc.
plus pos-session, artifact, etc.?
All roots and lexnames shared by both events, e.g.took and began were both act#0, be#0 and change,communication, etc.?
The least common ancestor (LCA) senses sharedby both events, e.g.
took and began meet only attheir roots, so the LCA senses are act#0 and be#0.We also extracted temporal and causal word associ-ations from the Google N-gram corpus (Brants andFranz, 2006), using <keyword> <pronoun><word> patterns, where before and after were thekeywords for temporals, and because was the key-word for causals.
Word scores were assigned as:score(w) = log(Nkeyword(w)N(w))where Nkeyword(w) is the number of times the wordappeared in the keyword?s pattern, and N(w) is thenumber of times the word was in the corpus.
Thefollowing features were derived from these scores:?
Whether the event score was in at least the N thpercentile, e.g.
took?s ?6.1 because score placedit above 84% of the scores, so the feature was truefor N = 70 and N = 80, but false for N = 90.?
Whether the first event score was greater than thesecond by at least N , e.g.
took and began haveafter scores of ?6.3 and ?6.2 so the feature wastrue for N = ?1, but false for N = 0 and N = 1.5 ResultsWe trained SVMperf classifiers (Joachims, 2005) forthe temporal and causal relation tasks2 using the2We built multi-class SVMs using the one-vs-rest approachand used 5-fold cross-validation on the training data to set pa-rameters.
For temporals, C=0.1 (for syntactic-only models),Temporals CausalsModel P R F1 P R F1BEFORE 26.7 94.2 41.6 - - -CAUSAL - - - 21.1 100.0 34.81st Event 35.0 24.4 28.8 31.0 20.3 24.52nd Event 36.1 30.2 32.9 22.4 17.2 19.5POS Pair 46.7 8.1 13.9 30.0 4.7 8.1Syntactic 36.5 53.5 43.4 24.4 79.7 37.4Semantic 35.8 55.8 43.6 27.2 64.1 38.1All 43.6 55.8 49.0 27.0 59.4 37.1All+Tmp - - - 46.9 59.4 52.4Table 3: Performance of the temporal relation identifica-tion models: (A)ccuracy, (P)recision, (R)ecall and (F1)-measure.
The null label is NO-REL.train/test split from Table 1 and the feature sets:Syntactic The syntactic features from Section 4.Semantic The semantic features from Section 4.All Both syntactic and semantic features.All+Tmp (Causals Only) Syntactic and semanticfeatures, plus the gold-standard temporal label.We compared our models against several baselines,using precision, recall and F-measure since the NO-REL labels were uninteresting.
Two simple baselineshad 0% recall: a lookup table of event word pairs3,and the majority class (NO-REL) label for causals.We therefore considered the following baselines:BEFORE Classify all instances as BEFORE, the ma-jority class label for temporals.CAUSAL Classify all instances as CAUSAL.1st Event Use a lookup table of 1st words and thelabels they were assigned in the training data.2nd Event As 1st Event, but using 2nd words.POS Pair As 1st Event, but using part of speech tagpairs.
POS tags encode tense, so this suggests theperformance of a tense-based classifier.The results on our test data are shown in Table 3.
Fortemporal relations, the F-measures of all SVM mod-els exceeded all baselines, with the combination ofsyntactic and semantic features performing 5 pointsbetter (43.6% precision and 55.8% recall) than eitherfeature set individually.
This suggests that our syn-tactic and semantic features encoded complemen-tary information for the temporal relation task.
ForC=1.0 (for all other models), and loss-function=F1 (for allmodels).
For causals, C=0.1 and loss-function=precision/recallbreak even point (for all models).3Only 3 word pairs from training were seen during testing.179Figure 2: Model precisions (dotted lines) and percent ofevents in the test data seen during training (solid lines),given increasing fractions of the training data.causal relations, all SVM models again exceeded allbaselines, but combining syntactic features with se-mantic ones gained little.
However, knowing aboutunderlying temporal relations boosted performanceto 46.9% precision and 59.4% recall.
This showsthat progress in causal relation identification will re-quire knowledge of temporal relations.We examined the effect of corpus size on ourmodels by training them on increasing fractions ofthe training data and evaluating them on the testdata.
The precisions of the resulting models areshown as dotted lines in Figure 2.
The models im-prove steadily, and the causals precision can be seento follow the solid curves which show how eventcoverage increases with increased training data.
Alogarithmic trendline fit to these seen-event curvessuggests that annotating all 5,013 event pairs in thePenn TreeBank could move event coverage up fromthe mid 50s to the mid 80s.
Thus annotating addi-tional data should provide a substantial benefit to ourtemporal and causal relation identification systems.6 ConclusionsOur research fills a gap in existing corpora and NLPsystems, examining parallel temporal and causal re-lations.
We annotated 1000 event pairs conjoinedby the word and, assigning each pair both a tempo-ral and causal relation.
Annotators achieved 81.2%agreement on temporal relations and 77.8% agree-ment on causal relations.
Using features based onWordNet and the Google N-gram corpus, we trainedsupport vector machine models that achieved 49.0F on temporal relations, and 37.1 F on causal rela-tions.
Providing temporal information to the causalrelations classifier boosted its results to 52.4 F. Fu-ture work will investigate increasing the size of thecorpus and developing more statistical approacheslike the Google N-gram scores to take advantage oflarge-scale resources to characterize word meaning.AcknowledgmentsThis research was performed in part under an ap-pointment to the U.S. Department of Homeland Se-curity (DHS) Scholarship and Fellowship Program.ReferencesS.
Bethard and J. H. Martin.
2006.
Identification of eventmentions and their semantic class.
In EMNLP-2006.S.
Bethard, J. H. Martin, and S. Klingenstein.
2007.Timelines from text: Identification of syntactic tem-poral relations.
In ICSC-2007.S.
Bethard, W. Corvey, S. Klingenstein, and J. H. Martin.2008.
Building a corpus of temporal-causal structure.In LREC-2008.B.
Boguraev and R. K. Ando.
2005.
Timebank-driven timeml analysis.
In Annotating, Extractingand Reasoning about Time and Events.
IBFI, SchlossDagstuhl, Germany.T.
Brants and A. Franz.
2006.
Web 1t 5-gram version 1.Linguistic Data Consortium, Philadelphia.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicDatabase.
MIT Press.R.
Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney,and D. Yuret.
2007.
Semeval-2007 task 04: Classi-fication of semantic relations between nominals.
InSemEval-2007.R.
Girju.
2003.
Automatic detection of causal relationsfor question answering.
In ACL Workshop on Multi-lingual Summarization and Question Answering.T.
Joachims.
2005.
A support vector method for multi-variate performance measures.
In ICML-2005.P.
Kingsbury and M. Palmer.
2002.
From Treebank toPropBank.
In LREC-2002.J.
Pustejovsky, P. Hanks, R.
Saur?
?, A.
See, R. Gaizauskas,A.
Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,and M. Lazo.
2003.
The timebank corpus.
In CorpusLinguistics, pages 647?656.D.
Reitter.
2003.
Simple signals for complexrhetorics: On rhetorical analysis with rich-feature sup-port vector models.
LDV-Forum, GLDV-Journal forComputational Linguistics and Language Technology,18(1/2):38?52.M.
Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,G.
Katz, and J. Pustejovsky.
2007.
Semeval-2007task 15: Tempeval temporal relation identification.
InSemEval-2007.180
