Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1?7,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTransition-based dependency parsing with topological fieldsDani?el de Kok and Erhard HinrichsSeminar f?ur SprachwissenschaftWilhemstra?e 1972072, T?ubingen, Germany{daniel.de-kok,erhard.hinrichs}@uni-tuebingen.deAbstractThe topological field model is commonlyused to describe the regularities in Germanword order.
In this work, we show thattopological fields can be predicted reliablyusing sequence labeling and that the pre-dicted field labels can inform a transition-based dependency parser.1 IntroductionThe topological field model (Herling, 1821; Erd-mann, 1886; Drach, 1937; H?ohle, 1986) has tra-ditionally been used to account for regularities inword order across different clause types of Ger-man.
This model assumes that each clause typecontains a left bracket (LK) and a right bracket(RK), which appear to the left and the right of themiddle field (MF).
Additionally, in a verb-seconddeclarative clause, the LK is preceded by the ini-tial field (VF) with the RK optionally followed bythe final field (NF).1Table 1 gives examples oftopological fields in verb-second declarative (MC)and verb-final relative (RC) clauses.Certain syntactic restrictions can be describedin terms of topological fields.
For instance, onlya single constituent is typically allowed in the VF,while multiple constituents are allowed in the MFand the NF.
Many ordering preferences can alsobe stated using the model.
For example, in a mainclause, placing the subject in the VF and the directobject in the MF is preferred over the opposite or-der.In parsing, topological field analysis is oftenseen as a task that is embedded in parsing itself.For instance, K?ubler (2005), Maier (2006), andCheung and Penn (2009) train PCFG parsers on1The abbreviations are derived from the German termslinke Klammer, rechte Klammer, Mittelfeld, Vorfeld, andNachfeld.treebanks that annotate topological fields as inte-rior nodes.
It is perhaps not surprising that this ap-proach works effectively for phrase structure pars-ing, because topological fields favor annotationsthat do not rely on crossing or discontinuous de-pendencies (Telljohann et al, 2006).However, the possible role of topological fieldsin statistical dependency parsing (K?ubler et al,2009) has not been explored much.
We will showthat statistical dependency parsing of German canbenefit from knowledge of clause structure as pro-vided by the topological field model.2 Motivation and corpus analysisTransition-based dependency parsers (Nivre,2003; K?ubler et al, 2009) typically use two tran-sitions (LEFT ARC and RIGHT ARC) to introducea dependency relation between the token thatis on top of the processing stack and the nexttoken on the buffer of unprocessed tokens.
Thedecision to make an attachment, the directionof attachment, and the label of the attachmentis made by a classifier.
Consequently, a goodclassifier is tasked to learn syntactic constraints,ordering preferences, and selectional preferences.Since transition-based dependency parsers pro-cess sentences in one deterministic linear-timeleft-to-right sweep, the classifier typically has lit-tle global information.
One popular approachfor reducing the effect of early attachment er-rors is to retain some competition between alter-native parses using a globally optimized modelwith beam search (Zhang and Clark, 2008).
Beamsearch presents a trade-off between speed (smallerbeam) and higher accuracy (larger beam).
Morerecently, Dyer et al (2015) have proposed touse Long short-term memory networks (LSTMs)to maintain (unbounded) representations of thebuffer of unprocessed words, previous parsing ac-1VF LK MF RK NFMC: In Tansania ist das Rad mehr verbreitet al in UgandaIn Tansania is the bike more common than in UgandaRC: der f?unfmal mehr nach Bremerhaven liefert als Daewoowho five-times more to Bremerhaven delivers than DaewooTable 1: Topological fields of a verb-second clause and a verb-final clause.tions, and constructed tree fragments.We believe that in the case of German, the topo-logical field model can provide a linguistically-motivated approach for providing the parser withmore global knowledge of the sentence structure.More concretely, if we give the transition classi-fier access to topological field annotations, it canlearn regularities with respect to the fields whereinthe head and dependent of a particular dependencyrelations lie.In the remainder of this section, we provide ashort (data-driven) exploration of such regulari-ties.
Since there is a myriad of possible triples2consisting of relation, head field, and dependentfield, we will focus on dependency relations thatvirtually never cross a field and relations thatnearly always cross a field.Table 2 lists the five dependency relation thatcross fields the least often in the T?uBa-D/Z tree-bank (Telljohann et al, 2006; Versley, 2005) ofGerman newspaper text.
Using these statistics, aclassifier could learn hard constraints with regardto these dependency relations ?
they should neverbe used to attach heads and dependents that are indifferent fields.Dependency label Cross-field (%)Particles 0.00Determiner 0.03Adjective or attr.
pronoun 0.04Prepositional complement 0.04Genetive attribute 0.07Table 2: The five dependency relations that mostrarely cross fields in the T?uBa-D/Z.Table 3 lists the five dependency relations thatcross fields most frequently.3These relations (vir-tually) always cross fields because they are verbalattachments and verbs typically form the LK andRK.
This information is somewhat informative,2335 in the T?uBa-D/Z treebank.3Dependency relations that connect two clauses are ex-cluded.since a classifier should clearly avoid to attach to-kens within the same field using one of these re-lations.
However, we can gain more interestinginsights by looking at the dependents?
fields.Dependency label Cross-field (%)Expletive es 100.00Separated verb prefix 100.00Subject 100.00Prepositional object 99.80Direct object 99.51Table 3: The five dependency relations that mostfrequently cross fields in the T?uBa-D/Z.Table 4 enumerates the three (where applicable)most frequent head and dependent field combina-tions of the five relations that always cross fields.As expected, the head is always in the LK or RK.Moreover, the dependents are in VF or MF in thefar majority of cases.
The actual distributions pro-vides some insights with respect to these depen-dency relations.
We will discuss the direct object,prepositional object, and separated verb prefix re-lations in some more detail.Direct objects In German, direct objects canbe put in the VF.
However, we can see that di-rect object fronting only happens very rarely inthe T?uBa-D/Z.
This is in line with earlier obser-vations in corpus-based studies (c.f.
Weber andM?uller (2004)).
Since the probability of having asubject in the VF is much higher, the parser shouldattach the head of a noun phrase in the VF as a sub-ject, unless there is overwhelming evidence to thecontrary, such as case markers, verb agreement, orother cues (Uszkoreit, 1984; M?uller, 1999).Prepositional objects The dependency annota-tion scheme used by the T?uBa-D/Z makes a dis-tinction between prepositional phrases that are arequired complement of a verb (prepositional ob-jects) and other prepositional phrases.
Since a sta-tistical dependency parser does not typically haveaccess to a valency dictionary, it has difficulty de-2Dependency label Head Dep %Expletive es RK MF 44.23RK VF 32.99LK VF 13.43Separated verb prefix LK RK 99.95RK RK 00.05Subject LK VF 36.40LK MF 35.10RK MF 20.11Prepositional object RK MF 51.04LK MF 39.81LK VF 04.11Direct object RK MF 54.84LK MF 35.64RK LK 03.38Table 4: The three most frequent head-dependentfield combinations of the five relations that alwayscross fields.ciding whether a prepositional phrase is a preposi-tional object or not.
Topological field informationcan complement verb-preposition co-occurrencestatistics in deciding between these two differentrelations.
The prepositional object mainly occursin MF, while a prepositional phrase headed by theLK is almost as likely to be in the VF as in the MF(42.12% and 55.70% respectively).Separated verb prefixes Some verbs in Germanhave separable prefixes.
A complicating factor inparsing is that such prefixes are often words thatcan also be used by themselves.
For example, in(1-a) fest is a separated prefix of bindet (presenttense third person of festbinden), while in (1-b)fest is an optional adverbial modifier of gebunden(the past participle of binden).
(1) a. SieShebindettiesdasthePferdhorsefesttight..b.
DasTheBuchbookistisfesttightlygebundenbound..Similarly to prepositional objects, a statisticalparser is handicapped by not having an extensivelexicon.
Again, topological fields can complementco-occurence statistics.
In (1-a), fest is in the RK.As we can see in Table 4, the separated verb pre-fix is always in the RK.
In contrast, an adverbialmodifier as in (1-b) is rarely in the RK (0.35% ofthe adverbs cases in the T?uBa-D/Z).3 Predicting fieldsAs mentioned in Section 1, topological field an-notation has often been performed as a part ofphrase structure parsing.
In order to test our hy-pothesis that topological field annotation could in-form dependency parsing, it would be more ap-propriate to use a syntax-less approach.
Severalshallow approaches have been tried in the past.For instance, Veenstra et al, (2002) compare threedifferent chunkers (finite state, PCFG, and clas-sification using memory-based learning).
Beckerand Frank (2002) predict topological fields usinga PCFG specifically tailored towards topologicalfields.
Finally, Liepert (2003) proposes a chunkerthat uses support vector machines.In the present work, we will treat the topolog-ical field annotation as a sequence labeling task.This is more useful in the context of dependencyparsing because it allows us to treat the topologicalfield as any other property of a token.Topological field projection In order to obtaindata for training, validation, and evaluation, weuse the T?uBa-D/Z treebank.
Topological fieldsare only annotated in the constituency version ofthe T?uBa-D/Z, where the fields are represented asspecial constituent nodes.
To obtain token-levelfield annotations for the dependency version of thetreebank, we project the topological fields of theconstituency trees on the tokens.
The recursiveprojection function for projection is provided inAppendix B.
The function is initially called withthe root of the tree and a special unknown fieldmarker, so that tokens that are not dominated by atopological field node (typically punctuation) alsoreceive the topological field feature.We should point out that our current projectionmethod results in a loss of information when asentence contains multiple clauses.
For instance,an embedded clause is in a topological field ofthe main clause, but also has its own topologicalstructure.
In our projection method, the topologi-cal field features of tokens in the embedded clausereflect the topological structure of the embeddedclause.Model Our topological field labeler uses a recur-rent neural network.
The inputs consist of con-catenated word and part-of-speech embeddings.The embeddings are fed to a bidirectional LSTM(Graves and Schmidhuber, 2005), on which westack a regular LSTM (Hochreiter and Schmidhu-3ber, 1997), and finally an output layer with thesoftmax activation function.
The use of a recur-rent model is motivated by the necessity to havelong-distance memory.
For example, (2-a) con-sists of a main clause with the LK wird and RKbegr?unt and an embedded clause wie geplant withits own clausal structure.
When the labeler en-counters jetzt, it needs to ?remember?
that it wasin the MF field of the main clause.
(2) a.
DieTheneuenewStreckestretchwirdis,,wieasgeplantplanned,,jetztnowbegr?untbeing-greened..Moreover, the use of a bidirectional LSTM is mo-tivated by the need for backwards-flowing infor-mation to make some labeling decisions.
For in-stance, die Siegerin is in the VF of the verb-secondclause (3-a), while it is in the MF of the verb-final clause (3-b).
The labeller can only make suchchoices by knowing the position of the finite verb.
(3) a. diedieSiegerinwinnerwurdewasdisqualifiziertdisqualifiedb.
dietheSiegerinwinnerzutodisqualifizierendisqualify4 Parsing with topological fieldsTo evaluate the effectiveness of adding topo-logical fields to the input, we use the publiclyavailable neural network parser described by DeKok (2015).
This parser uses an architecture thatis similar to that of Chen and Manning (2014).However, it learns morphological analysis as anembedded task of parsing.
Since most inflectionalinformation that can be relevant for parsing Ger-man is available in the prefix or suffix, this parserlearns morphological representations over charac-ter embeddings of prefixes and suffixes.We use the same parser configuration as that ofDe Kok (2015), with the addition of topologicalfield annotations.
We encode the topological fieldsas one-hot vectors in the input of the parser.
Thisinformation is included for the four tokens on topof the stack and the next three tokens on the buffer.5 Evaluation and resultsTo evaluate the proposed topological field model,we use the same partitioning of T?uBa-D/Z and theword and tag embeddings as De Kok (2015).
Fortraining, validation, and evaluation of the parser,we use these splits as-is.
Since we want to test theparser with non-gold topological field annotationsas well, we swapped the training and validationdata for training our topological field predictor.The parser was trained using the same hyper-parameters and embeddings as in De Kok (2015).Our topological field predictor is trained usingKeras (Chollet, 2015).4The hyperparameters thatwe use are summarized in Appendix A.
The topo-logical field predictor uses the same word and tagembeddings as the parser.In Table 5, we show the accuracy of the topo-logical field labeler.
The use of a bi-directionalLSTM is clearly justified, since it outperforms thestacked unidirectional LSTM by a wide margin.Parser Accuracy (%)LSTM + LSTM 93.33Bidirectional LSTM + LSTM 97.24Table 5: Topological field labeling accuracies.The addition of backward flowing information im-proves accuracy considerably.Table 6 shows the labeled attachment scores(LAS) for parsing with topological fields.
Aswe can see, adding gold topological field annota-tions provides a marked improvement over pars-ing without topological fields.
Although the parserdoes not achieve quite the same performance withthe output of the LSTM-based sequence labeler,it is still a relatively large improvement over theparser of De Kok (2015).
All differences are sig-nificant at p < 0.0001.5Parser LAS UASDe Kok (2015) 89.49 91.88Neural net + TFs 90.00 92.36Neural net + gold TFs 90.42 92.76Table 6: Parse results with topological fields andgold topological fields.
Parsers that use topolog-ical field information outperform parsers withoutaccess to such information.6 Result analysisOur motivation for introducing topological fieldsin dependency parsing is to provide the parser with4The software is available from: https://github.com/danieldk/toponn5Using paired approximate randomization tests (Noreen,1989).4a more global view of sentence structure (Sec-tion 2).
If this is indeed the case, we expect theparser to improve especially for longer-distancerelations.
Figure 1 shows the improvement inLAS as a result of adding gold-standard topolog-ical fields.
We see a strong relation between therelation length and the improvement in accuracy.The introduction of topological fields clearly ben-efits the attachment of longer-distance dependents.5 10 15 2012345Head?dependent distance?
AccuracyFigure 1: The improvement in labeled attachmentscore as a result of adding gold topological fieldsto the parser by dependency length.Since the introduction of topological fields hasvery little impact on short-distance relations, thedifferences in the attachment of relations that vir-tually never cross fields (Table 2) turn out to benegligable.
However, for the relations that crossfields frequently, we see a marked improvements(Table 7) for every relation except the preposi-tional object.
In hindsight, this difference shouldnot be surprising ?
the relations that never crossfields are usually very local, while those that al-most always cross fields tend to have longer dis-tances and/or are subject to relatively free order-ing.Dependency label LAS ?Expletive es 2.71Separated verb prefix 1.64Subject 1.22Prepositional object -0.29Direct object 1.59Table 7: The LAS ?
of the parser with access togold standard topological fields compared to theDe Kok (2015) parser for the relations of Table 4.Dependency label LAS ?Coordinating conjunction (clausal) 11.48Parenthesis 8.31Dependent clause 3.49Conjunct 3.38Sentence root72.92Expletive es 2.71Sentence 2.64Comparative 1.87Separated verb prefix 1.64Direct object 1.59Table 8: The ten dependency relations with thehighest LAS ?
of the parser with access to goldtopological fields compared to the (de Kok, 2015)parser.The ten dependency relations with the highestoverall improvement in LAS are shown in Table 8.Many of these relations are special when it comesto topological field structure and were not dis-cussed in Section 2.
The relations parenthesis, de-pendent clause, and sentence link two clauses; thesentence root marks the root of the dependencytree; and the coordinating conjunction (clausal)relation attaches a token that is always in its ownfield.6This confirms that the addition of topologi-cal fields also improves the analysis of the overallclausal structure.7 Conclusion and outlookIn this paper, we have argued and shown thataccess to topological field information can im-prove the accuracy of transition-based dependencyparsers.
In future, we plan to see how com-petitive the bidirectional LSTM-based sequencelabeling approach is compared to existing ap-proaches.
Moreover, we plan to evaluate the useof topological fields in the architecture proposedby Dyer et al, (2015) to see how many of theseregularities that approach captures.AcknowledgmentsThe authors gratefully acknowledge the financialsupport of their research by the German Ministryfor Education and Research (BMBF) as part ofthe CLARIN-D research infrastructure grant givento the University of T?ubingen.
Furthermore, wewould like to thank Jianqiang Ma for his extensivecomments on an early draft of this paper.6The KOORD field, see Telljohan et al (2006).5ReferencesMarkus Becker and Anette Frank.
2002.
A stochas-tic topological parser for German.
In Proceedingsof the 19th international conference on Computa-tional linguistics-Volume 1, pages 1?7.
Associationfor Computational Linguistics.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), volume 1, pages 740?750.Jackie Chi Kit Cheung and Gerald Penn.
2009.
Topo-logical field parsing of German.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP: Vol-ume 1-Volume 1, pages 64?72.
Association for Com-putational Linguistics.Franc?ois Chollet.
2015.
Keras.
https://github.com/fchollet/keras.Dani?el de Kok.
2015.
A poor man?s morphology forGerman transition-based dependency parsing.
In In-ternational Workshop on Treebanks and LinguisticTheories (TLT14).Erich Drach.
1937.
Grundgedanken der DeutschenSatzlehre.
Frankfurt/Main.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 334?343, Beijing, China, July.
Asso-ciation for Computational Linguistics.Oskar Erdmann.
1886.
Grundz?uge der deutschenSyntax nach ihrer geschichtlichen Entwicklungdargestellt.
Stuttgart: Cotta.
Erste Abteilung.Alex Graves and J?urgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.Simon Herling.
1821.?Uber die Topik der deutschenSprache.
In Abhandlungen des frankfurterischenGelehrtenvereins f?ur deutsche Sprache, pages 296?362, 394.
Frankfurt/Main.
Drittes St?uck.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Tilman H?ohle.
1986.
Der Begriff ?Mittelfeld?.Anmerkungen ?uber die Theorie der topologischenFelder.
In A. Sch?one, editor, Kontroversen alteund neue.
Akten des 7.
Internationalen Germanis-tenkongresses G?ottingen, pages 329?340.
T?ubingen:Niemeyer.Sandra K?ubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
Synthesis Lectures onHuman Language Technologies, 1(1):1?127.Sandra K?ubler.
2005.
How do treebank annota-tion schemes influence parsing results?
or how notto compare apples and oranges.
Proceedings ofRANLP 2005.Martina Liepert.
2003.
Topological fields chunking forGerman with SVM?s: Optimizing SVM-parameterswith GA?s.
In Proceedings of the International Con-ference on Recent Advances in Natural LanguageProcessing.Wolfgang Maier.
2006.
Annotation schemes andtheir influence on parsing results.
In Proceedingsof the 21st International Conference on computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics: StudentResearch Workshop, pages 19?24.
Association forComputational Linguistics.Gereon M?uller.
1999.
Optimality, markedness, andword order in German.
Linguistics, 37(5):777?818.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT), pages 149?160.Eric W Noreen.
1989.
Computer intensive methodsfor hypothesis testing: An introduction.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Heike Telljohann, Erhard W Hinrichs, Sandra K?ubler,Heike Zinsmeister, and Kathrin Beck.
2006.
Style-book for the t?ubingen treebank of written German(T?uBa-D/Z).
In Seminar fur Sprachwissenschaft,Universit?at Tubingen, T?ubingen, Germany.Hans Uszkoreit.
1984.
Word order and constituentstructure in German.
CSLI Publications.Jorn Veenstra, Frank Henrik M?uller, and Tylman Ule.2002.
Topological field chunking for German.
InProceedings of the 6th Conference on Natural Lan-guage Learning - Volume 20, COLING-02, pages 1?7, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Yannick Versley.
2005.
Parser evaluation across texttypes.
In Proceedings of the Fourth Workshop onTreebanks and Linguistic Theories (TLT 2005).Andrea Weber and Karin M?uller.
2004.
Word ordervariation in German main clauses: A corpus analy-sis.
In Proceedings of the 20th International confer-ence on Computational Linguistics, pages 71?77.6Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.
Association for Computa-tional Linguistics.A HyperparametersThe topological field labeler was trained usingKeras (Chollet, 2015).
Here, we provide a shortoverview the hyperparameters that we used:?
Solver: rmsprop, this solver is recommendedby the Keras documentation for recurrentneural networks.
The solver is used with itsdefault parameters.?
Learning rate: the learning rate was deter-mined by the function 0.01(1 + 0.02i)?2,where i is the epoch.
The intuition was tostart with some epochs with a high learningrate, dropping the learning rate quickly.
Theresults were not drastically different when us-ing a constant learning rate of 0.001.?
Epochs: The models was trained for 200epochs, then we picked the model of theepoch with the highest performance on thevalidation data (27 epochs for the unidirec-tional LSTM, 124 epochs for the bidirec-tional LSTM).?
LSTM layers: all LSTM layers were trainedwith 50 output dimensions.
Increasing thenumber of output dimensions did not providean improvement.?
Regularization: 10% dropout (Srivastava etal., 2014) was used after each LSTM layerfor regularization.
A stronger dropout did notprovide better performance.B Topological field projection algorithmAlgorithm 1 Topological field projection.function PROJECT(node,field)if IS TERMINAL NODE(node) thennode.field?
fieldelseif IS TOPO NODE(node) thenfield?
node.fieldend iffor child ?
node doPROJECT(child,field)end forend ifend function7
