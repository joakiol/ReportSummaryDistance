Proceedings of ACL-08: HLT, pages 63?71,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsContradictions and Justifications: Extensions to the Textual Entailment TaskEllen M. VoorheesNational Institute of Standards and TechnologyGaithersburg, MD 20899-8940, USAellen.voorhees@nist.govAbstractThe third PASCAL Recognizing Textual En-tailment Challenge (RTE-3) contained an op-tional task that extended the main entailmenttask by requiring a system to make three-wayentailment decisions (entails, contradicts, nei-ther) and to justify its response.
Contradic-tion was rare in the RTE-3 test set, occurringin only about 10% of the cases, and systemsfound accurately detecting it difficult.
Subse-quent analysis of the results shows a test setmust contain many more entailment pairs forthe three-way decision task than the traditionaltwo-way task to have equal confidence in sys-tem comparisons.
Each of six human judgesrepresenting eventual end users rated the qual-ity of a justification by assigning ?understand-ability?
and ?correctness?
scores.
Ratings ofthe same justification across judges differedsignificantly, signaling the need for a bettercharacterization of the justification task.1 IntroductionThe PASCAL Recognizing Textual Entailment (RTE)workshop series (see www.pascal-network.org/Challenges/RTE3/) has been a catalystfor recent research in developing systems that areable to detect when the content of one piece oftext necessarily follows from the content of anotherpiece of text (Dagan et al, 2006; Giampiccolo et al,2007).
This ability is seen as a fundamental com-ponent in the solutions for a variety of natural lan-guage problems such as question answering, sum-marization, and information extraction.
In additionto the main entailment task, the most recent Chal-lenge, RTE-3, contained a second optional task thatextended the main task in two ways.
The first exten-sion was to require systems to make three-way en-tailment decisions; the second extension was for sys-tems to return a justification or explanation of howits decision was reached.In the main RTE entailment task, systems reportwhether the hypothesis is entailed by the text.
Thesystem responds with YES if the hypothesis is en-tailed and NO otherwise.
But this binary decisionconflates the case when the hypothesis actually con-tradicts the text?the two could not both be true?with simple lack of entailment.
The three-way en-tailment decision task requires systems to decidewhether the hypothesis is entailed by the text (YES),contradicts the text (NO), or is neither entailed bynor contradicts the text (UNKNOWN).The second extension required a system to explainwhy it reached its conclusion in terms suitable for aneventual end user (i.e., not system developer).
Ex-planations are one way to build a user?s trust in asystem, but it is not known what kinds of informa-tion must be conveyed nor how best to present thatinformation.
RTE-3 provided an opportunity to col-lect a diverse sample of explanations to begin to ex-plore these questions.This paper analyzes the extended task results,with the next section describing the three-way deci-sion subtask and Section 3 the justification subtask.Contradiction was rare in the RTE-3 test set, occur-ring in only about 10% of the cases, and systemsfound accurately detecting it difficult.
While thelevel of agreement among human annotators as to63the correct answer for an entailment pair was withinexpected bounds, the test set was found to be toosmall to reliably distinguish among systems?
three-way accuracy scores.
Human judgments of the qual-ity of a justification varied widely, signaling the needfor a better characterization of the justification task.Comments from the judges did include some com-mon themes.
Judges prized conciseness, though theywere uncomfortable with mathematical notation un-less they had a mathematical background.
Judgesstrongly disliked being shown system internals suchas scores reported by various components.2 The Three-way Decision TaskThe extended task used the RTE-3 main task test setof entailment pairs as its test set.
This test set con-tains 800 text and hypothesis pairs, roughly evenlysplit between pairs for which the text entails the hy-pothesis (410 pairs) and pairs for which it does not(390 pairs), as defined by the reference answer keyreleased by RTE organizers.RTE uses an ?ordinary understanding?
principlefor deciding entailment.
The hypothesis is consid-ered entailed by the text if a human reading the textwould most likely conclude that the hypothesis weretrue, even if there could exist unusual circumstancesthat would invalidate the hypothesis.
It is explicitlyacknowledged that ordinary understanding dependson a common human understanding of language aswell as common background knowledge.
The ex-tended task also used the ordinary understandingprinciple for deciding contradictions.
The hypoth-esis and text were deemed to contradict if a humanwould most likely conclude that the text and hypoth-esis could not both be true.The answer key for the three-way decision taskwas developed at the National Institute of Standardsand Technology (NIST) using annotators who hadexperience as TREC and DUC assessors.
NIST as-sessors annotated all 800 entailment pairs in the testset, with each pair independently annotated by twodifferent assessors.
The three-way answer key wasformed by keeping exactly the same set of YES an-swers as in the two-way key (regardless of the NISTannotations) and having NIST staff adjudicate as-sessor differences on the remainder.
This resultedin a three-way answer key containing 410 (51%)Reference Systems?
ResponsesAnswer YES UNKN NO TotalsYES 2449 2172 299 4920UNKN 929 2345 542 3816NO 348 415 101 864Totals 3726 4932 942 9600Table 1: Contingency table of responses over all 800 en-tailment pairs and all 12 runs.YES answers, 319 (40%) UNKNOWN answers, and72 (9%) NO answers.2.1 System resultsEight different organizations participated in thethree-way decision subtask submitting a total of 12runs.
A run consists of exactly one response of YES,NO, or UNKNOWN for each of the 800 test pairs.Runs were evaluated using accuracy, the percentageof system responses that match the reference answer.Figure 1 shows both the overall accuracy of eachof the runs (numbers running along the top of thegraph) and the accuracy as conditioned on the ref-erence answer (bars).
The conditioned accuracy forYES answers, for example, is accuracy computed us-ing just those test pairs for which YES is the ref-erence answer.
The runs are sorted by decreasingoverall accuracy.Systems were much more accurate in recognizingentailment than contradiction (black bars are greaterthan white bars).
Since conditioned accuracy doesnot penalize for overgeneration of a response, theconditioned accuracy for UNKNOWN is excellent forthose systems that used UNKNOWN as their defaultresponse.
Run H never concluded that a pair was acontradiction, for example.Table 1 gives another view of the relative diffi-culty of detecting contradiction.
The table is a con-tingency table of the systems?
responses versus thereference answer summed over all test pairs and allruns.
A reference answer is represented as a row inthe table and a system?s response as a column.
Sincethere are 800 pairs in the test set and 12 runs, thereis a total of 9600 responses.As a group the systems returned NO as a response942 times, approximately 10% of the time.
While10% is a close match to the 9% of the test set forwhich NO is the reference answer, the systems de-tected contradictions for the wrong pairs: the table?s64A B C D E F G H I J K L0.00.20.40.60.81.0ConditionedAccuracyYESUNKNOWNNO0.731 0.713 0.591 0.569 0.494 0.471 0.454 0.451 0.436 0.425 0.419 0.365Figure 1: Overall accuracy (top number) and accuracy conditioned by reference answer for three-way runs.diagonal entry for NO is the smallest entry in both itsrow and its column.
The smallest row entry meansthat systems were more likely to respond that the hy-pothesis was entailed than that it contradicted whenit in fact contradicted.
The smallest column entrymeans than when the systems did respond that thehypothesis contradicted, it was more often the casethat the hypothesis was actually entailed than that itcontradicted.
The 101 correct NO responses repre-sent 12% of the 864 possible correct NO responses.In contrast, the systems responded correctly for 50%(2449/4920) of the cases when YES was the refer-ence answer and for 61% (2345/3816) of the caseswhen UNKNOWN was the reference answer.2.2 Human agreementTextual entailment is evaluated assuming that thereis a single correct answer for each test pair.
This is asimplifying assumption used to make the evaluationtractable, but as with most NLP phenomena it is notactually true.
It is quite possible for two humans tohave legitimate differences of opinions (i.e., to dif-fer when neither is mistaken) about whether a hy-pothesis is entailed or contradicts, especially givenannotations are based on ordinary understanding.Since systems are given credit only when they re-spond with the reference answer, differences in an-notators?
opinions can clearly affect systems?
accu-racy scores.
The RTE main task addressed this issueby including a candidate entailment pair in the testset only if multiple annotators agreed on its dispo-sition (Giampiccolo et al, 2007).
The test set aloMain Task NIST Judge 1YES UNKN NOYES 378 27 5NO 48 242 100conflated agreement = .90Main Task NIST Judge 2YES UNKN NOYES 383 23 4NO 46 267 77conflated agreement = .91Table 2: Agreement between NIST judges (columns) andmain task reference answers (rows).contains 800 pairs so an individual test case con-tributes only 1/800 = 0.00125 to the overall accu-racy score.
To allow the results from the two- andthree-way decision tasks to be comparable (and toleverage the cost of creating the main task test set),the extended task used the same test set as the maintask and used simple accuracy as the evaluation mea-sure.
The expectation was that this would be as ef-fective an evaluation design for the three-way task asit is for the two-way task.
Unfortunately, subsequentanalysis demonstrates that this is not so.Recall that NIST judges annotated all 800 entail-ment pairs in the test set, with each pair indepen-dently annotated twice.
For each entailment pair,one of the NIST judges was arbitrarily assigned asthe first judge for that pair and the other as the sec-ond judge.
The agreement between NIST and RTEannotators is shown in Table 2.
The top half of65the table shows the agreement between the two-wayanswer key and the annotations of the set of firstjudges; the bottom half is the same except using theannotations of the set of second judges.
The NISTjudges?
answers are given in the columns and thetwo-way reference answers in the rows.
Each cell inthe table gives the raw count before adjudication ofthe number of test cases that were assigned that com-bination of annotations.
Agreement is then com-puted as the percentage of matches when a NISTjudge?s NO or UNKNOWN annotation matched a NOtwo-way reference answer.
Agreement is essentiallyidentical for both sets of judges at 0.90 and 0.91 re-spectively.Because the agreement numbers reflect the rawcounts before adjudication, at least some of the dif-ferences may be attributable to annotator errors thatwere corrected during adjudication.
But there do ex-ist legitimate differences of opinion, even for the ex-treme cases of entails versus contradicts.
Typicaldisagreements involve granularity of place namesand amount of background knowledge assumed.Example disagreements concerned whether Holly-wood was equivalent to Los Angeles, whether EastJerusalem was equivalent to Jerusalem, and whethermembers of the same political party who were atodds with one another were ?opponents?.RTE organizers reported an agreement rate ofabout 88% among their annotators for the two-waytask (Giampiccolo et al, 2007).
The 90% agree-ment rate between the NIST judges and the two-way answer key probably reflects a somewhat largeramount of disagreement since the test set aleadyhad RTE annotators?
disagreements removed.
Butit is similar enough to support the claim that theNIST annotators agree with other annotators as of-ten as can be expected.
Table 3 shows the three-way agreement between the two NIST annotators.As above, the table gives the raw counts before ad-judication and agreement is computed as percentageof matching annotations.
Three-way agreement is0.83?smaller than two-way agreement simply be-cause there are more ways to disagree.Just as annotator agreement declines as the setof possible answers grows, the inherent stability ofthe accuracy measure also declines: accuracy andagreement are both defined as the percentage of ex-act matches on answers.
The increased uncertaintyYES UNKN NOYES 381UNKN 82 217NO 11 43 66three-way agreement = .83Table 3: Agreement between NIST judges.when moving from two-way to three-way decisionssignificantly reduces the power of the evaluation.With the given level of annotator agreement and 800pairs in the test set, in theory accuracy scores couldchange by as much as 136 (the number of test casesfor which annotators disagreed) ?0.00125 = .17 byusing a different choice of annotator.
The maximumdifference in accuracy scores actually observed inthe submitted runs was 0.063.Previous analyses of other evaluation tasks suchas document retrieval and question answeringdemonstrated that system rankings are stable de-spite differences of opinion in the underlying anno-tations (Voorhees, 2000; Voorhees and Tice, 2000).The differences in accuracy observed for the three-way task are large enough to affect system rank-ings, however.
Compared to the system ranking ofABCDEFGHIJKL induced by the official three-wayanswer key, the ranking induced by the first set ofjudges?
raw annotations is BADCFEGKHLIJ.
Theranking induced by the second set of judges?
raw an-notations is much more similar to the official results,ABCDEFGHKIJL.How then to proceed?
Since the three-way de-cision task was motivated by the belief that distin-guishing contradiction from simple non-entailmentis important, reverting back to a binary decision taskis not an attractive option.
Increasing the size of thetest set beyond 800 test cases will result in a morestable evaluation, though it is not known how big thetest set needs to be.
Defining new annotation rulesin hopes of increasing annotator agreement is a satis-factory option only if those rules capture a character-istic of entailment that systems should actually em-body.
Reasonable people do disagree about entail-ment and it is unwise to enforce some arbitrary defi-nition in the name of consistency.
Using UNKNOWNas the reference answer for all entailment pairs onwhich annotators disagree may be a reasonable strat-egy: the disagreement itself is strong evidence that66neither of the other options holds.
Creating balancedtest sets using this rule could be difficult, however.Following this rule, the RTE-3 test set would have360 (45%) YES answers, 64 (8%) NO answers, and376 (47%) UNKNOWN answers, and would inducethe ranking ABCDEHIJGKFL.
(Runs such as H, I,and J that return UNKNOWN as a default responseare rewarded using this annotation rule.
)3 JustificationsThe second part of the extended task was for systemsto provide explanations of how they reached theirconclusions.
The specification of a justification forthe purposes of the task was deliberately vague?a collection of ASCII strings with no minimum ormaximum size?so as to not preclude good ideas byarbitrary rules.
A justification run contained all ofthe information from a three-way decision run plusthe rationale explaining the response for each of the800 test pairs in the RTE-3 test set.
Six of the runsshown in Figure 1 (A, B, C, D, F, and H) are jus-tification runs.
Run A is a manual justification run,meaning there was some human tweaking of the jus-tifications (but not the entailment decisions).After the runs were submitted, NIST selected asubset of 100 test pairs to be used in the justificationevaluation.
The pairs were selected by NIST staffafter looking at the justifications so as to maximizethe informativeness of the evaluation set.
All runswere evaluated on the same set of 100 pairs.Figure 2 shows the justification produced by eachrun for pair 75 (runs D and F were submitted bythe same organization and contained identical jus-tifications for many pairs including pair 75).
Thetext of pair 75 is Muybridge had earlier developedan invention he called the Zoopraxiscope., and thehypothesis is The Zoopraxiscope was invented byMuybridge.
The hypothesis is entailed by the text,and each of the systems correctly replied that it isentailed.
Explanations for why the hypothesis is en-tailed differ widely, however, with some rationalesof dubious validity.Each of the six different NIST judges rated all 100justifications.
For a given justification, a judge firstassigned an integer score between 1?5 on how un-derstandable the justification was (with 1 as unintel-ligible and 5 as completely understandable).
If theunderstandability score assigned was 3 or greater,the judge then assigned a correctness score, also aninteger between 1?5 with 5 the high score.
This sec-ond score was interpreted as how compelling the ar-gument contained in the justification was rather thansimple correctness because justifications could bestrictly correct but immaterial.3.1 System resultsThe motivation for the justification subtask was togather data on how systems might best explain them-selves to eventual end users.
Given this goal and theexploratory nature of the exercise, judges were givenminimal guidance on how to assign scores other thanthat it should be from a user?s, not a system devel-oper?s, point of view.
Judges used a system that dis-played the text, hypothesis, and reference answer,and then displayed each submission?s justification inturn.
The order in which the runs?
justifications weredisplayed was randomly selected for each pair; for agiven pair, each judge saw the same order.Figure 2 includes the scores assigned to each ofthe justifications of entailment pair 75.
Each pairof numbers in brackets is a score pair assigned byone judge.
The first number in the pair is the un-derstandability score and the second the correctnessscore.
The correctness score is omitted (???)
whenthe understandability score is 1 or 2 because no cor-rectness score was assigned in that case.
The scoresfrom the different judges are given in the same orderfor each justification.With 100 entailment pairs evaluated by each of6 judges assigning 2 separate scores, each run hada total of 1200 numbers assigned to it.
Figure 3shows two views of these numbers: a histogram ofthe number of justifications in the run that were as-signed a given score value summed over all judgesand all test pairs, and the overall mean score for therun1.
A correctness score that was not assigned be-cause understandability was too poor is displayed asa score of 0 in the histogram and treated as a 0 in thecomputation of the mean.
Understandability scoresare shown toward the left in the figure and correct-ness scores toward the right.The mean scores for correctness are fairly low1Using the mean treats a score as an interval variable ratherthan an ordinal variable, a questionable treatment given a 5-point scale.
A mean is a convenient summary, however.67Run A:There is a relationship between Zoopraxiscope and Muybridge in both the text andhypothesis.
The term "invention" is morphologically similar to "invented".Scores: [4,4] [4,4] [5,4] [4,1] [5,4], [3,2]Run B:The text mentions ?Muybridge?.
We can infer that Muybridge is inventor.
Fromthe fact that Muybridge is inventor, we can infer that Muybridge invented.
We canconclude that the Zoopraxiscope was invented by Muybridge.Scores:[4,3] [3,3] [5,4] [5,1] [5,3] [3,2]Run C:1: The Hypothesis could be precisely matched with content in the Text, withallowance for polarity and embedded contexts.2: Hypothesis words match well with words in the Text.3: text adjunct "called" of "invention" dropped on aligned hyp word "invented"Scores: [3,3] [4,4] [4,4] [2,?]
[1,?]
[2,?
]Runs D and F:The words in the hypothesis are all found, with the approximately all the exact samesyntactic dependencies, also in the text.
Therefore, I concluded that the givenhypothesis:The Zoopraxiscope was invented by Muybridge.is entailed by the given textMuybridge had earlier developed an invention he called the Zoopraxiscope.Scores: [3,3] [4,3] [4,3] [5,1] [4,3] [2,?
]Run H:Yes!I have general knowledge that:IF Y is developed by X THEN Y is manufactured by XHere: X = Muybridge, Y = the inventionThus, here:We are told in T: the invention is developed by MuybridgeThus it follows that: the invention is manufactured by MuybridgeIn addition, I know:"manufacture" and "invent" mean roughly the same thingHence: The Zoopraxiscope was invented by Muybridge.Scores: [2,?]
[4,1] [3,3] [3,1] [2,?]
[1,?
]Figure 2: Justification for entailment pair 75 from each justification run.
Brackets contain the pair of scores assignedto the justification by one of the six human judges; the first number in the pair is the understandability score and thesecond is the correctness score.for all runs.
Recall, however, that the ?correctness?score was actually interpreted as compellingness.There were many justifications that were strictly cor-rect but not very informative, and they received lowcorrectness scores.
For example, the low correctnessscores for the justification from run A in Figure 2were given because those judges did not feel thatthe fact that ?invention and inventor are morpholog-ically similar?
was enough of an explanation.
Meancorrectness scores were also affected by understand-ability.
Since an unassigned correctness score wastreated as a zero when computing the mean, systemswith low understandability scores must have lowercorrectness scores.
Nonetheless, it is also true thatsystems reached the correct entailment decision byfaulty reasoning uncomfortably often, as illustratedby the justification from run H in Figure 2.680100200300400 Run A* [4.27 2.75]011223 34455Understandability Correctness0100200300400 Run B [4.11 2.00]01122334455Understandability Correctness0100200300400 Run C [2.66 1.23]01122334455Understandability Correctness0100200300400 Run D [3.15 1.54]01122334455Understandability Correctness0100200300400 Run F [3.11 1.47]01122334455Understandability Correctness0100200300400 Run H [4.09 1.49]01122334455Understandability CorrectnessFigure 3: Number of justifications in a run that were assigned a particular score value summed over all judges and alltest pairs.
Brackets contain the overall mean understandability and correctness scores for the run.
The starred run (A)is the manual run.3.2 Human agreementThe most striking feature of the system results inFigure 3 is the variance in the scores.
Not explicitin that figure, though illustrated in the example inFigure 2, is that different judges often gave widelydifferent scores to the same justification.
One sys-tematic difference was immediately detected.
TheNIST judges have varying backgrounds with respectto mathematical training.
Those with more train-ing were more comfortable with, and often pre-ferred, justifications expressed in mathematical no-tation; those with little training strongly disliked anymathematical notation in an explanation.
This pref-erence affected both the understandability and thecorrectness scores.
Despite being asked to assigntwo separate scores, judges found it difficult to sep-arate understandability and correctness.
As a result,correctness scores were affected by presentation.The scores assigned by different judges were suf-ficiently different to affect how runs compared toone another.
This effect was quantified in the follow-ing way.
For each entailment pair in the test set, theset of six runs was ranked by the scores assigned byone assessor, with rank one assigned to the best runand rank six the worst run.
If several systems had thesame score, they were each assigned the mean rankfor the tied set.
(For example, if two systems had thesame score that would rank them second and third,they were each assigned rank 2.5.)
A run was thenassigned its mean rank over the 100 justifications.Figure 4 shows how the mean rank of the runs variesby assessor.
The x-axis in the figure shows the judgeassigning the score and the y-axis the mean rank (re-member that rank one is best).
A run is plotted us-ing its letter name consistent with previous figures,and lines connect the same system across differentjudges.
Lines intersect demonstrating that differentjudges prefer different justifications.After rating the 100 justifications, judges wereasked to write a short summary of their impressionof the task and what they looked for in a justification.These summaries did have some common themes.Judges prized conciseness and specificity, and ex-pected (or at least hoped for) explanations in fluentEnglish.
Judges found ?chatty?
templates such asthe one used in run H more annoying than engaging.Verbatim repetition of the text and hypothesis within69Judge1 Judge2 Judge3 Judge4 Judge5 Judge612345MeanRankUnderstandabiltyBBBBBBA A AAA ACCCC C CDD DDDDFFFFFFHHHHH HJudge1 Judge2 Judge3 Judge4 Judge5 Judge612345MeanRankCorrectnessBBB BBBAAAAAAC C CC CCDDDD DDFFFF FFHHHHHHFigure 4: Relative effectiveness of runs as measured by mean rank.the justification (as in runs D and F) was criticizedas redundant.
Generic phrases such as ?there is a re-lation between?
and ?there is a match?
were worsethan useless: judges assigned no expository value tosuch assertions and penalized them as clutter.Judges were also adverse to the use of system in-ternals and jargon in the explanations.
Some sys-tems reported scores computed from WordNet (Fell-baum, 1998) or DIRT (Lin and Pantel, 2001).
Suchreports were penalized since the judges did not carewhat WordNet or DIRT are, and if they had cared,had no way to calibrate such a score.
Similarly, lin-guistic jargon such as ?polarity?
and ?adjunct?
and?hyponym?
had little meaning for the judges.Such qualitative feedback from the judges pro-vides useful guidance to system builders on ways toexplain system behavior.
A broader conclusion fromthe justifications subtask is that it is premature for aquantitative evaluation of system-constructed expla-nations.
The community needs a better understand-ing of the overall goal of justifications to developa workable evaluation task.
The relationships cap-tured by many RTE entailment pairs are so obviousto humans (e.g., an inventor creates, a niece is a rel-ative) that it is very unlikely end users would wantexplanations that include this level of detail.
Havinga true user task as a target would also provide neededdirection as to the characteristics of those users, andthus allow judges to be more effective surrogates.4 ConclusionThe RTE-3 extended task provided an opportunityto examine systems?
abilities to detect contradic-tion and to provide explanations of their reasoningwhen making entailment decisions.
True contradic-tion was rare in the test set, accounting for approx-imately 10% of the test cases, though it is not pos-sible to say whether this is a representative fractionfor the text sources from which the test was drawnor simply a chance occurrence.
Systems found de-tecting contradiction difficult, both missing it whenit was present and finding it when it was not.
Levelsof human (dis)agreement regarding entailment andcontradiction are such that test sets for a three-waydecision task need to be substantially larger than forbinary decisions for the evaluation to be both reli-able and sensitive.The justification task as implemented in RTE-3is too abstract to make an effective evaluation task.Textual entailment decisions are at such a basic levelof understanding for humans that human users don?twant explanations at this level of detail.
User back-grounds have a profound effect on what presentationstyles are acceptable in an explanation.
The justifi-cation task needs to be more firmly situated in thecontext of a real user task so the requirements of theuser task can inform the evaluation task.AcknowledgementsThe extended task of RTE-3 was supported by theDisruptive Technology Office (DTO) AQUAINTprogram.
Thanks to fellow coordinators of the task,Chris Manning and Dan Moldovan, and to the par-ticipants for making the task possible.70ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Lecture Notes in Computer Science, vol-ume 3944, pages 177?190.
Springer-Verlag.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, andBill Dolan.
2007.
The third PASCAL recognizing tex-tual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 1?9.
Association for ComputationalLinguistics.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
Discov-ery of inference rules from text.
In Proceedings of theACM Conference on Knowledge Discovery and DataMining (KDD-01), pages 323?328.Ellen M. Voorhees and Dawn M. Tice.
2000.
Buildinga question answering test collection.
In Proceedingsof the Twenty-Third Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 200?207, July.Ellen M. Voorhees.
2000.
Variations in relevance judg-ments and the measurement of retrieval effectiveness.Information Processing and Management, 36:697?716.71
