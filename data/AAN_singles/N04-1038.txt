Unsupervised Learning of Contextual Role Knowledge for CoreferenceResolutionDavid BeanAttensity Corporation, Suite 600Gateway One 90 South 400 WestSalt Lake City, UT 84101dbean@attensity.comEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.eduAbstractWe present a coreference resolver calledBABAR that uses contextual role knowledge toevaluate possible antecedents for an anaphor.BABAR uses information extraction patternsto identify contextual roles and creates fourcontextual role knowledge sources using unsu-pervised learning.
These knowledge sourcesdetermine whether the contexts surroundingan anaphor and antecedent are compatible.BABAR applies a Dempster-Shafer probabilis-tic model to make resolutions based on ev-idence from the contextual role knowledgesources as well as general knowledge sources.Experiments in two domains showed that thecontextual role knowledge improved corefer-ence performance, especially on pronouns.1 IntroductionThe problem of coreference resolution has received con-siderable attention, including theoretical discourse mod-els (e.g., (Grosz et al, 1995; Grosz and Sidner, 1998)),syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le-ass, 1994)), and supervised machine learning systems(Aone and Bennett, 1995; McCarthy and Lehnert, 1995;Ng and Cardie, 2002; Soon et al, 2001).
Most compu-tational models for coreference resolution rely on prop-erties of the anaphor and candidate antecedent, such aslexical matching, grammatical and syntactic features, se-mantic agreement, and positional information.The focus of our work is on the use of contextual roleknowledge for coreference resolution.
A contextual rolerepresents the role that a noun phrase plays in an eventor relationship.
Our work is motivated by the observa-tion that contextual roles can be critically important indetermining the referent of a noun phrase.
Consider thefollowing sentences:(a) Jose Maria Martinez, Roberto Lisandy, and DinoRossy, who were staying at a Tecun Uman hotel,were kidnapped by armed men who took them to anunknown place.
(b) After they were released...(c) After they blindfolded the men...In (b) ?they?
refers to the kidnapping victims, but in (c)?they?
refers to the armed men.
The role that each nounphrase plays in the kidnapping event is key to distinguish-ing these cases.
The correct resolution in sentence (b)comes from knowledge that people who are kidnappedare often subsequently released.
The correct resolution insentence (c) depends on knowledge that kidnappers fre-quently blindfold their victims.We have developed a coreference resolver calledBABAR that uses contextual role knowledge to makecoreference decisions.
BABAR employs information ex-traction techniques to represent and learn role relation-ships.
Each pattern represents the role that a noun phraseplays in the surrounding context.
BABAR uses unsu-pervised learning to acquire this knowledge from plaintext without the need for annotated training data.
Train-ing examples are generated automatically by identifyingnoun phrases that can be easily resolved with their an-tecedents using lexical and syntactic heuristics.
BABARthen computes statistics over the training examples mea-suring the frequency with which extraction patterns andnoun phrases co-occur in coreference resolutions.In this paper, Section 2 begins by explaining howcontextual role knowledge is represented and learned.Section 3 describes the complete coreference resolutionmodel, which uses the contextual role knowledge as wellas more traditional coreference features.
Our corefer-ence resolver also incorporates an existential noun phraserecognizer and a Dempster-Shafer probabilistic model tomake resolution decisions.
Section 4 presents experimen-tal results on two corpora: the MUC-4 terrorism cor-pus, and Reuters texts about natural disasters.
Our re-sults show that BABAR achieves good performance inboth domains, and that the contextual role knowledgeimproves performance, especially on pronouns.
Finally,Section 5 explains how BABAR relates to previous work,and Section 6 summarizes our conclusions.2 Learning Contextual Role KnowledgeIn this section, we describe how contextual role knowl-edge is represented and learned.
Section 2.1 describeshow BABAR generates training examples to use in thelearning process.
We refer to this process as Reli-able Case Resolution because it involves finding casesof anaphora that can be easily resolved with their an-tecedents.
Section 2.2 then describes our representationfor contextual roles and four types of contextual roleknowledge that are learned from the training examples.2.1 Reliable Case ResolutionsThe first step in the learning process is to generate train-ing examples consisting of anaphor/antecedent resolu-tions.
BABAR uses two methods to identify anaphorsthat can be easily and reliably resolved with their an-tecedent: lexical seeding and syntactic seeding.2.1.1 Lexical SeedingIt is generally not safe to assume that multiple occur-rences of a noun phrase refer to the same entity.
Forexample, the company may refer to Company X in oneparagraph and Company Y in another.
However, lex-ically similar NPs usually refer to the same entity intwo cases: proper names and existential noun phrases.BABAR uses a named entity recognizer to identify propernames that refer to people and companies.
Proper namesare assumed to be coreferent if they match exactly, or ifthey closely match based on a few heuristics.
For exam-ple, a person?s full name will match with just their lastname (e.g., ?George Bush?
and ?Bush?
), and a companyname will match with and without a corporate suffix (e.g.,?IBM Corp.?
and ?IBM?).
Proper names that match areresolved with each other.The second case involves existential noun phrases(Allen, 1995), which are noun phrases that uniquely spec-ify an object or concept and therefore do not need aprior referent in the discourse.
In previous work (Beanand Riloff, 1999), we developed an unsupervised learn-ing algorithm that automatically recognizes definite NPsthat are existential without syntactic modification be-cause their meaning is universally understood.
For exam-ple, a story can mention ?the FBI?, ?the White House?,or ?the weather?
without any prior referent in the story.Although these existential NPs do not need a prior ref-erent, they may occur multiple times in a document.
Bydefinition, each existential NP uniquely specifies an ob-ject or concept, so we can infer that all instances of thesame existential NP are coreferent (e.g., ?the FBI?
alwaysrefers to the same entity).
Using this heuristic, BABARidentifies existential definite NPs in the training corpususing our previous learning algorithm (Bean and Riloff,1999) and resolves all occurrences of the same existentialNP with each another.12.1.2 Syntactic SeedingBABAR also uses syntactic heuristics to identifyanaphors and antecedents that can be easily resolved.
Ta-ble 1 briefly describes the seven syntactic heuristics usedby BABAR to resolve noun phrases.
Words and punctua-tion that appear in brackets are considered optional.
Theanaphor and antecedent appear in boldface.1.
Reflexive pronouns with only 1 NP in scope.Ex: The regime gives itself the right...2.
Relative pronouns with only 1 NP in scope.Ex: The brigade, which attacked ...3.
Some cases of the pattern ?NP to-be NP?.Ex: Mr. Cristiani is the president ...4.
Some cases of ?NP said [that] it/they?Ex: The government said it ...5.
Some cases of ?
[Locative-prep] NP [,] where?Ex: He was found in San Jose, where ...6.
Simple appositives of the form ?NP, NP?Ex: Mr. Cristiani, president of the country ...7.
PPs containing ?by?
and a gerund followed by ?it?Ex: Mr. Bush disclosed the policy by reading it...Table 1: Syntactic Seeding HeuristicsBABAR?s reliable case resolution heuristics produceda substantial set of anaphor/antecedent resolutions thatwill be the training data used to learn contextual roleknowledge.
For terrorism, BABAR generated 5,078 res-olutions: 2,386 from lexical seeding and 2,692 fromsyntactic seeding.
For natural disasters, BABAR gener-ated 20,479 resolutions: 11,652 from lexical seeding and8,827 from syntactic seeding.2.2 Contextual Role KnowledgeOur representation of contextual roles is based on infor-mation extraction patterns that are converted into simplecaseframes.
First, we describe how the caseframes arerepresented and learned.
Next, we describe four con-textual role knowledge sources that are created from thetraining examples and the caseframes.2.2.1 The Caseframe RepresentationInformation extraction (IE) systems use extraction pat-terns to identify noun phrases that play a specific role in1Our implementation only resolves NPs that occur in thesame document, but in retrospect, one could probably resolveinstances of the same existential NP in different documents too.an event.
For IE, the system must be able to distinguishbetween semantically similar noun phrases that play dif-ferent roles in an event.
For example, management suc-cession systems must distinguish between a person whois fired and a person who is hired.
Terrorism systemsmust distinguish between people who perpetrate a crimeand people who are victims of a crime.We applied the AutoSlog system (Riloff, 1996) to ourunannotated training texts to generate a set of extractionpatterns for each domain.
Each extraction pattern repre-sents a linguistic expression and a syntactic position in-dicating where a role filler can be found.
For example,kidnapping victims should be extracted from the subjectof the verb ?kidnapped?
when it occurs in the passivevoice (the short-hand representation of this pattern wouldbe ?<subject> were kidnapped?).
The types of patternsproduced by AutoSlog are outlined in (Riloff, 1996).Ideally we?d like to know the thematic role of each ex-tracted noun phrase, but AutoSlog does not generate the-matic roles.
As a (crude) approximation, we normalizethe extraction patterns with respect to active and passivevoice and label those extractions as agents or patients.For example, the passive voice pattern ?<subject> werekidnapped?
and the active voice pattern ?kidnapped<direct object>?
are merged into a single normalizedpattern ?kidnapped <patient>?.2 For the sake of sim-plicity, we will refer to these normalized extraction pat-terns as caseframes.3 These caseframes can capture twotypes of contextual role information: (1) thematic rolescorresponding to events (e.g, ?<agent> kidnapped?
or?kidnapped <patient>?
), and (2) predicate-argument re-lations associated with both verbs and nouns (e.g., ?kid-napped for <np>?
or ?vehicle with <np>?
).We generate these caseframes automatically by run-ning AutoSlog over the training corpus exhaustively sothat it literally generates a pattern to extract every nounphrase in the corpus.
The learned patterns are then nor-malized and applied to the corpus.
This process producesa large set of caseframes coupled with a list of the nounphrases that they extracted.
The contextual role knowl-edge that BABAR uses for coreference resolution is de-rived from this caseframe data.2.2.2 The Caseframe NetworkThe first type of contextual role knowledgethat BABAR learns is the Caseframe Network(CFNet), which identifies caseframes that co-occur inanaphor/antecedent resolutions.
Our assumption is thatcaseframes that co-occur in resolutions often have a2This normalization is performed syntactically without se-mantics, so the agent and patient roles are not guaranteed tohold, but they usually do in practice.3These are not full case frames in the traditional sense, butthey approximate a simple case frame with a single slot.conceptual relationship in the discourse.
For example,co-occurring caseframes may reflect synonymy (e.g.,?<patient> kidnapped?
and ?<patient> abducted?
)or related events (e.g., ?<patient> kidnapped?
and?<patient> released?).
We do not attempt to identifythe types of relationships that are found.
BABARmerely identifies caseframes that frequently co-occur incoreference resolutions.Terrorism Natural Disastersmurder of <NP> <agent> damagedkilled <patient> was injured in <NP><agent> reported <agent> occurred<agent> added cause of <NP><agent> stated <agent> wreaked<agent> added <agent> crossedperpetrated <patient> driver of <NP>condemned <patient> <agent> carryingFigure 1: Caseframe Network ExamplesFigure 1 shows examples of caseframes that co-occurin resolutions, both in the terrorism and natural disasterdomains.
The terrorism examples reflect fairly obviousrelationships: people who are murdered are killed; agentsthat ?report?
things also ?add?
and ?state?
things; crimesthat are ?perpetrated?
are often later ?condemned?.
In thenatural disasters domain, agents are often forces of na-ture, such as hurricanes or wildfires.
Figure 1 reveals thatan event that ?damaged?
objects may also cause injuries;a disaster that ?occurred?
may be investigated to find its?cause?
; a disaster may ?wreak?
havoc as it ?crosses?
ge-ographic regions; and vehicles that have a ?driver?
mayalso ?carry?
items.During coreference resolution, the caseframe networkprovides evidence that an anaphor and prior noun phrasemight be coreferent.
Given an anaphor, BABAR iden-tifies the caseframe that would extract it from its sen-tence.
For each candidate antecedent, BABAR identifiesthe caseframe that would extract the candidate, pairs itwith the anaphor?s caseframe, and consults the CF Net-work to see if this pair of caseframes has co-occurred inprevious resolutions.
If so, the CF Network reports thatthe anaphor and candidate may be coreferent.2.2.3 Lexical Caseframe ExpectationsThe second type of contextual role knowledge learnedby BABAR is Lexical Caseframe Expectations, which areused by the CFLex knowledge source.
For each case-frame, BABAR collects the head nouns of noun phrasesthat were extracted by the caseframe in the training cor-pus.
For each resolution in the training data, BABAR alsoassociates the co-referring expression of an NP with theNP?s caseframe.
For example, if X and Y are coreferent,then both X and Y are considered to co-occur with thecaseframe that extracts X as well as the caseframe thatextracts Y.
We will refer to the set of nouns that co-occurwith a caseframe as the lexical expectations of the case-frame.
Figure 2 shows examples of lexical expectationsthat were learned for both domains.TerrorismCaseframe: engaged in <NP>NPs: activity, battle, clash, dialogue, effort, fight, group,shoot-out, struggle, village, violenceCaseframe: ambushed <patient>NPs: company, convoy, helicopter, member, motorcade,move, Ormeno, patrol, position, response, soldier,they, troops, truck, vehicle, whichNatural DisastersCaseframe: battled through <NP>NPs: flame, night, smoke, wallCaseframe: braced for <NP>NPs: arrival, battering, catastrophe, crest, Dolly, epidemics,evacuate, evacuation, flood, flooding, front, Hortense,hurricane, misery, rains, river, storm, surge, test, typhoon.Figure 2: Lexical Caseframe ExpectationsTo illustrate how lexical expectations are used, supposewe want to determine whether noun phrase X is the an-tecedent for noun phrase Y.
If they are coreferent, thenX and Y should be substitutable for one another in thestory.4 Consider these sentences:(S1) Fred was killed by a masked man with a revolver.
(S2) The burglar fired the gun three times and fled.
?The gun?
will be extracted by the caseframe ?fired<patient>?.
Its correct antecedent is ?a revolver?, whichis extracted by the caseframe ?killed with <NP>?.
If?gun?
and ?revolver?
refer to the same object, then itshould also be acceptable to say that Fred was ?killedwith a gun?
and that the burglar ?fired a revolver?.During coreference resolution, BABAR checks (1)whether the anaphor is among the lexical expectations forthe caseframe that extracts the candidate antecedent, and(2) whether the candidate is among the lexical expecta-tions for the caseframe that extracts the anaphor.
If eithercase is true, then CFLex reports that the anaphor and can-didate might be coreferent.2.2.4 Semantic Caseframe ExpectationsThe third type of contextual role knowledge learnedby BABAR is Semantic Caseframe Expectations.
Se-mantic expectations are analogous to lexical expectationsexcept that they represent semantic classes rather thannouns.
For each caseframe, BABAR collects the seman-tic classes associated with the head nouns of NPs thatwere extracted by the caseframe.
As with lexical expec-tions, the semantic classes of co-referring expressions are4They may not be perfectly substitutable, for example oneNP may be more specific (e.g., ?he?
vs. ?John F.
Kennedy?
).But in most cases they can be used interchangably.collected too.
We will refer to the semantic classes thatco-occur with a caseframe as the semantic expectationsof the caseframe.
Figure 3 shows examples of semanticexpectations that were learned.
For example, BABARlearned that agents that ?assassinate?
or ?investigate acause?
are usually humans or groups (i.e., organizations).TerrorismCaseframe Semantic Classes<agent> assassinated group, humaninvestigation into <NP> eventexploded outside <NP> buildingNatural DisastersCaseframe Semantic Classes<agent> investigating cause group, humansurvivor of <NP> event, natphenomhit with <NP> attribute, natphenomFigure 3: Semantic Caseframe ExpectationsFor each domain, we created a semantic dictionary bydoing two things.
First, we parsed the training corpus,collected all the noun phrases, and looked up each headnoun in WordNet (Miller, 1990).
We tagged each nounwith the top-level semantic classes assigned to it in Word-Net.
Second, we identified the 100 most frequent nounsin the training corpus and manually labeled them withsemantic tags.
This step ensures that the most frequentterms for each domain are labeled (in case some of themare not in WordNet) and labeled with the sense most ap-propriate for the domain.Initially, we planned to compare the semantic classesof an anaphor and a candidate and infer that they might becoreferent if their semantic classes intersected.
However,using the top-level semantic classes of WordNet provedto be problematic because the class distinctions are toocoarse.
For example, both a chair and a truck would be la-beled as artifacts, but this does not at all suggest that theyare coreferent.
So we decided to use semantic class in-formation only to rule out candidates.
If two nouns havemutually exclusive semantic classes, then they cannot becoreferent.
This solution also obviates the need to per-form word sense disambiguation.
Each word is simplytagged with the semantic classes corresponding to all ofits senses.
If these sets do not overlap, then the wordscannot be coreferent.The semantic caseframe expectations are used in twoways.
One knowledge source, called WordSem-CFSem,is analogous to CFLex: it checks whether the anaphor andcandidate antecedent are substitutable for one another,but based on their semantic classes instead of the wordsthemselves.
Given an anaphor and candidate, BABARchecks (1) whether the semantic classes of the anaphorintersect with the semantic expectations of the caseframethat extracts the candidate, and (2) whether the semanticclasses of the candidate intersect with the semantic ex-pectations of the caseframe that extracts the anaphor.
Ifone of these checks fails then this knowledge source re-ports that the candidate is not a viable antecedent for theanaphor.A different knowledge source, called CFSem-CFSem,compares the semantic expectations of the caseframe thatextracts the anaphor with the semantic expectations of thecaseframe that extracts the candidate.
If the semantic ex-pectations do not intersect, then we know that the case-frames extract mutually exclusive types of noun phrases.In this case, this knowledge source reports that the candi-date is not a viable antecedent for the anaphor.2.3 Assigning Evidence ValuesContextual role knowledge provides evidence as towhether a candidate is a plausible antecedent for ananaphor.
The two knowledge sources that use semanticexpectations, WordSem-CFSem and CFSem-CFSem, al-ways return values of -1 or 0.
-1 means that an NP shouldbe ruled out as a possible antecedent, and 0 means that theknowledge source remains neutral (i.e., it has no reasonto believe that they cannot be coreferent).The CFLex and CFNet knowledge sources providepositive evidence that a candidate NP and anaphor mightbe coreferent.
They return a value in the range [0,1],where 0 indicates neutrality and 1 indicates the strongestbelief that the candidate and anaphor are coreferent.BABAR uses the log-likelihood statistic (Dunning, 1993)to evaluate the strength of a co-occurrence relationship.For each co-occurrence relation (noun/caseframe forCFLex, and caseframe/caseframe for CFNet), BABARcomputes its log-likelihood value and looks it up in the2 table to obtain a confidence level.
The confidencelevel is then used as the belief value for the knowledgesource.
For example, if CFLex determines that the log-likelihood statistic for the co-occurrence of a particularnoun and caseframe corresponds to the 90% confidencelevel, then CFLex returns .90 as its belief that the anaphorand candidate are coreferent.3 The Coreference Resolution ModelGiven a document to process, BABAR uses four modulesto perform coreference resolution.
First, a non-anaphoricNP classifier identifies definite noun phrases that are exis-tential, using both syntactic rules and our learned existen-tial NP recognizer (Bean and Riloff, 1999), and removesthem from the resolution process.
Second, BABAR per-forms reliable case resolution to identify anaphora thatcan be easily resolved using the lexical and syntacticheuristics described in Section 2.1.
Third, all remain-ing anaphora are evaluated by 11 different knowledgesources: the four contextual role knowledge sources justdescribed and seven general knowledge sources.
Finally,a Dempster-Shafer probabilistic model evaluates the ev-idence provided by the knowledge sources for all can-didate antecedents and makes the final resolution de-cision.
In this section, we describe the seven generalknowledge sources and explain how the Dempster-Shafermodel makes resolutions.3.1 General Knowledge SourcesFigure 4 shows the seven general knowledge sources(KSs) that represent features commonly used for corefer-ence resolution.
The gender, number, and scoping KSseliminate candidates from consideration.
The scopingheuristics are based on the anaphor type: for reflexivepronouns the scope is the current clause, for relative pro-nouns it is the prior clause following its VP, for personalpronouns it is the anaphor?s sentence and two preced-ing sentences, and for definite NPs it is the anaphor?ssentence and eight preceding sentences.
The semanticagreement KS eliminates some candidates, but also pro-vides positive evidence in one case: if the candidate andanaphor both have semantic tags human, company, date,or location that were assigned via NER or the manuallylabeled dictionary entries.
The rationale for treating thesesemantic labels differently is that they are specific andreliable (as opposed to the WordNet classes, which aremore coarse and more noisy due to polysemy).KS FunctionGender filters candidate if gender doesn?t agree.Number filters candidate if number doesn?t agree.Scoping filters candidate if outside the anaphor?s scope.Semantic (a) filters candidate if its semantic tagsdon?t intersect with those of the anaphor.
(b) supports candidate if selected semantictags match those of the anaphor.Lexical computes degree of lexical overlapbetween the candidate and the anaphor.Recency computes the relative distance between thecandidate and the anaphor.SynRole computes relative frequency with which thecandidate?s syntactic role occurs in resolutions.Figure 4: General Knowledge SourcesThe Lexical KS returns 1 if the candidate and anaphorare identical, 0.5 if their head nouns match, and 0 other-wise.
The Recency KS computes the distance betweenthe candidate and the anaphor relative to its scope.
TheSynRole KS computes the relative frequency with whichthe candidates?
syntactic role (subject, direct object, PPobject) appeared in resolutions in the training set.
Dur-ing development, we sensed that the Recency and Syn-role KSs did not deserve to be on equal footing with theother KSs because their knowledge was so general.
Con-sequently, we cut their evidence values in half to lessentheir influence.3.2 The Dempster-Shafer Decision ModelBABAR uses a Dempster-Shafer decision model (Stefik,1995) to combine the evidence provided by the knowl-edge sources.
Our motivation for using Dempster-Shaferis that it provides a well-principled framework for com-bining evidence from multiple sources with respect tocompeting hypotheses.
In our situation, the competinghypotheses are the possible antecedents for an anaphor.An important aspect of the Dempster-Shafer model isthat it operates on sets of hypotheses.
If evidence indi-cates that hypotheses C and D are less likely than hy-potheses A and B, then probabilities are redistributed toreflect the fact that fA, Bg is more likely to contain theanswer than fC, Dg.
The ability to redistribute belief val-ues across sets rather than individual hypotheses is key.The evidence may not say anything about whether A ismore likely than B, only that C and D are not likely.Each set is assigned two values: belief and plausibil-ity.
Initially, the Dempster-Shafer model assumes that allhypotheses are equally likely, so it creates a set called that includes all hypotheses.
 has a belief value of 1.0,indicating complete certainty that the correct hypothesisis included in the set, and a plausibility value of 1.0, in-dicating that there is no evidence for competing hypothe-ses.5 As evidence is collected and the likely hypothesesare whittled down, belief is redistributed to subsets of .Formally, the Dempster-Shafer theory defines a proba-bility density function m(S), where S is a set of hypothe-ses.
m(S) represents the belief that the correct hypothe-sis is included in S. The model assumes that evidence alsoarrives as a probability density function (pdf) over sets ofhypotheses.6 Integrating new evidence into the existingmodel is therefore simply a matter of defining a functionto merge pdfs, one representing the current belief systemand one representing the beliefs of the new evidence.
TheDempster-Shafer rule for combining pdfs is:m3(S) =XX\Y =Sm1(X)  m2(Y )1 ?XX\Y =;m1(X)  m2(Y )(1)All sets of hypotheses (and their corresponding beliefvalues) in the current model are crossed with the sets ofhypotheses (and belief values) provided by the new evi-dence.
Sometimes, however, these beliefs can be contra-dictory.
For example, suppose the current model assignsa belief value of .60 to fA, Bg, meaning that it is 60%sure that the correct hypothesis is either A or B. Thennew evidence arrives with a belief value of .70 assigned5Initially there are no competing hypotheses because all hy-potheses are included in  by definition.6Our knowledge sources return some sort of probability es-timate, although in some cases this estimate is not especiallywell-principled (e.g., the Recency KS).to fCg, meaning that it is 70% sure the correct hypothe-sis is C. The intersection of these sets is the null set be-cause these beliefs are contradictory.
The belief valuethat would have been assigned to the intersection of thesesets is .60*.70=.42, but this belief has nowhere to go be-cause the null set is not permissible in the model.7 So thisprobability mass (.42) has to be redistributed.
Dempster-Shafer handles this by re-normalizing all the belief valueswith respect to only the non-null sets (this is the purposeof the denominator in Equation 1).In our coreference resolver, we define  to be the setof all candidate antecedents for an anaphor.
Each knowl-edge source then assigns a probability estimate to eachcandidate, which represents its belief that the candidate isthe antecedent for the anaphor.
The probabilities are in-corporated into the Dempster-Shafer model using Equa-tion 1.
To resolve the anaphor, we survey the final be-lief values assigned to each candidate?s singleton set.
Ifa candidate has a belief value  .50, then we select thatcandidate as the antecedent for the anaphor.
If no candi-date satisfies this condition (which is often the case), thenthe anaphor is left unresolved.
One of the strengths of theDempster-Shafer model is its natural ability to recognizewhen several credible hypotheses are still in play.
In thissituation, BABAR takes the conservative approach anddeclines to make a resolution.4 Evaluation Results4.1 CorporaWe evaluated BABAR on two domains: terrorism andnatural disasters.
We used the MUC-4 terrorism cor-pus (MUC-4 Proceedings, 1992) and news articles fromthe Reuter?s text collection8 that had a subject code cor-responding to natural disasters.
For each domain, wecreated a blind test set by manually annotating 40 doc-uments with anaphoric chains, which represent sets ofnoun phrases that are coreferent (as done for MUC-6(MUC-6 Proceedings, 1995)).
In the terrorism domain,1600 texts were used for training and the 40 test docu-ments contained 322 anaphoric links.
For the disastersdomain, 8245 texts were used for training and the 40 testdocuments contained 447 anaphoric links.In recent years, coreference resolvers have been evalu-ated as part of MUC-6 and MUC-7 (MUC-7 Proceedings,1998).
We considered using the MUC-6 and MUC-7 datasets, but their training sets were far too small to learn reli-able co-occurrence statistics for a large set of contextualrole relationships.
Therefore we opted to use the much7The Dempster-Shafer theory assumes that one of the hy-potheses in  is correct, so eliminating all of the hypothesesviolates this assumption.8Volume 1, English language, 1996-1997, Format version 1,correction level 0Terrorism DisastersAnaphor Rec Pr F Rec Pr FDef.
NPs .43 .79 .55 .42 .91 .58Pronouns .50 .72 .59 .42 .82 .56Total .46 .76 .57 .42 .87 .57Table 2: General Knowledge SourcesTerrorism DisastersAnaphor Rec Pr F Rec Pr FDef.
NPs .45 .71 .55 .46 .84 .59Pronouns .63 .73 .68 .57 .79 .66Total .53 .73 .61 .51 .82 .63Table 3: General + Contextual Role Knowledge Sourceslarger MUC-4 and Reuters corpora.94.2 ExperimentsWe adopted the MUC-6 guidelines for evaluating coref-erence relationships based on transitivity in anaphoricchains.
For example, if fNP1, NP2, NP3g are all coref-erent, then each NP must be linked to one of the other twoNPs.
First, we evaluated BABAR using only the sevengeneral knowledge sources.
Table 2 shows BABAR?sperformance.
We measured recall (Rec), precision (Pr),and the F-measure (F) with recall and precision equallyweighted.
BABAR achieved recall in the 42-50% rangefor both domains, with 76% precision overall for terror-ism and 87% precision for natural disasters.
We suspectthat the higher precision in the disasters domain may bedue to its substantially larger training corpus.Table 3 shows BABAR?s performance when the fourcontextual role knowledge sources are added.
The F-measure score increased for both domains, reflecting asubstantial increase in recall with a small decrease in pre-cision.
The contextual role knowledge had the greatestimpact on pronouns: +13% recall for terrorism and +15%recall for disasters, with a +1% precision gain in terror-ism and a small precision drop of -3% in disasters.The difference in performance between pronouns anddefinite noun phrases surprised us.
Analysis of the datarevealed that the contextual role knowledge is especiallyhelpful for resolving pronouns because, in general, theyare semantically weaker than definite NPs.
Since pro-nouns carry little semantics of their own, resolving themdepends almost entirely on context.
In contrast, eventhough context can be helpful for resolving definite NPs,context can be trumped by the semantics of the nounsthemselves.
For example, even if the contexts surround-ing an anaphor and candidate match exactly, they are notcoreferent if they have substantially different meanings9We would be happy to make our manually annotated testdata available to others who also want to evaluate their corefer-ence resolver on the MUC-4 or Reuters collections.Pronouns Definite NPsRec Pr F Rec Pr FNo CF KSs .50 .72 .59 .43 .79 .55CFLex .56 .74 .64 .42 .73 .53CFNet .56 .74 .64 .43 .74 .54CFSem-CFSem .58 .76 .66 .44 .76 .56WordSem-CFSem .61 .74 .67 .45 .76 .56All CF KSs .63 .73 .68 .45 .71 .55Table 4: Individual Performance of KSs for TerrorismPronouns Definite NPsRec Pr F Rec Pr FNo CF KSs .42 .82 .56 .42 .91 .58CFLex .48 .83 .61 .44 .88 .59CFNet .45 .82 .58 .43 .88 .57CFSem-CFSem .51 .81 .62 .44 .87 .58WordSem-CFSem .52 .79 .63 .43 .86 .57All CF KSs .57 .79 .66 .46 .84 .59Table 5: Individual Performance of KSs for Disasters(e.g., ?the mayor?
vs. ?the journalist?
).We also performed experiments to evaluate the impactof each type of contextual role knowledge separately.
Ta-bles 4 and 5 show BABAR?s performance when just onecontextual role knowledge source is used at a time.
Fordefinite NPs, the results are a mixed bag: some knowl-edge sources increased recall a little, but at the expenseof some precision.
For pronouns, however, all of theknowledge sources increased recall, often substantially,and with little if any decrease in precision.
This resultsuggests that all of contextual role KSs can provide use-ful information for resolving anaphora.
Tables 4 and 5also show that putting all of the contextual role KSs inplay at the same time produces the greatest performancegain.
There are two possible reasons: (1) the knowl-edge sources are resolving different cases of anaphora,and (2) the knowledge sources provide multiple pieces ofevidence in support of (or against) a candidate, therebyacting synergistically to push the Dempster-Shafer modelover the belief threshold in favor of a single candidate.5 Related WorkMany researchers have developed coreference resolvers,so we will only discuss the methods that are most closelyrelated to BABAR.
Dagan and Itai (Dagan and Itai, 1990)experimented with co-occurrence statistics that are sim-ilar to our lexical caseframe expectations.
Their workused subject-verb, verb-object, and adjective-noun rela-tions to compare the contexts surrounding an anaphor andcandidate.
However their work did not consider othertypes of lexical expectations (e.g., PP arguments), seman-tic expectations, or context comparisons like our case-frame network.
(Niyu et al, 1998) used unsupervised learning to ac-quire gender, number, and animacy information from res-olutions produced by a statistical pronoun resolver.
Thelearned information was recycled back into the resolverto improve its performance.
This approach is similar toBABAR in that they both acquire knowledge from ear-lier resolutions.
(Kehler, 1997) also used a Dempster-Shafer model to merge evidence from different sourcesfor template-level coreference.Several coreference resolvers have used supervisedlearning techniques, such as decision trees and rule learn-ers (Aone and Bennett, 1995; McCarthy and Lehnert,1995; Ng and Cardie, 2002; Soon et al, 2001).
Thesesystems rely on a training corpus that has been manuallyannotated with coreference links.6 ConclusionsThe goal of our research was to explore the use of contex-tual role knowledge for coreference resolution.
We iden-tified three ways that contextual roles can be exploited:(1) by identifying caseframes that co-occur in resolu-tions, (2) by identifying nouns that co-occur with case-frames and using them to cross-check anaphor/candidatecompatibility, (3) by identifying semantic classes that co-occur with caseframes and using them to cross-checkanaphor/candidate compatability.
We combined evidencefrom four contextual role knowledge sources with ev-idence from seven general knowledge sources using aDempster-Shafer probabilistic model.Our coreference resolver performed well in two do-mains, and experiments showed that each contextual roleknowledge source contributed valuable information.
Wefound that contextual role knowledge was more beneficialfor pronouns than for definite noun phrases.
This sug-gests that different types of anaphora may warrant differ-ent treatment: definite NP resolution may depend moreon lexical semantics, while pronoun resolution may de-pend more on contextual semantics.
In future work, weplan to follow-up on this approach and investigate otherways that contextual role knowledge can be used.7 AcknowledgementsThis work was supported in part by the National Sci-ence Foundation under grant IRI-9704240.
The inven-tions disclosed herein are the subject of a patent applica-tion owned by the University of Utah and licensed on anexclusive basis to Attensity Corporation.ReferencesJ.
Allen.
1995.
Natural Language Understanding.
Ben-jamin/Cummings Press, Redwood City, CA.C.
Aone and S. Bennett.
1995.
Applying Machine Learningto Anaphora Resolution.
In IJCAI-95 Workshop on New Ap-proaches to Learning for NLP.D.
Bean and E. Riloff.
1999.
Corpus-Based Identification ofNon-Anaphoric Noun Phrases.
In Proc.
of the 37th AnnualMeeting of the Association for Computational Linguistics.I.
Dagan and A. Itai.
1990.
Automatic Processing of LargeCorpora for the Resolution of Anaphora References.
In Pro-ceedings of the Thirteenth International Conference on Com-putational Linguistics (COLING-90), pages 330?332.T.
Dunning.
1993.
Accurate methods for the statistics of sur-prise and coincidence.
Computational Linguistics, 19(1):61?74.B.
Grosz and C. Sidner.
1998.
Lost Intuitions and Forgotten In-tentions.
In M. Walker, A. Joshi, and E. Prince, editors, Cen-tering Theory in Discourse, pages 89?112.
Clarendon Press.B.
Grosz, A. Joshi, and S. Weinstein.
1995.
Centering: AFramework for Modeling the Local Coherence of Discourse.Computational Linguistics, 21(2):203?226.J.
Hobbs.
1978.
Resolving Pronoun References.
Lingua,44(4):311?338.A.
Kehler.
1997.
Probabilistic Coreference in Information Ex-traction.
In Proceedings of the Second Conference on Em-pirical Methods in Natural Language Processing.S.
Lappin and H. Leass.
1994.
An algorithm for pronominalanaphora resolution.
Computational Linguistics, 20(4):535?561.J.
McCarthy and W. Lehnert.
1995.
Using Decision Trees forCoreference Resolution.
In Proc.
of the Fourteenth Interna-tional Joint Conference on Artificial Intelligence.G.
Miller.
1990.
Wordnet: An On-line Lexical Database.
In-ternational Journal of Lexicography, 3(4).MUC-4 Proceedings.
1992.
Proceedings of the Fourth Mes-sage Understanding Conference (MUC-4).MUC-6 Proceedings.
1995.
Proceedings of the Sixth MessageUnderstanding Conference (MUC-6).MUC-7 Proceedings.
1998.
Proceedings of the Seventh Mes-sage Understanding Conference (MUC-7).V.
Ng and C. Cardie.
2002.
Improving Machine Learning Ap-proaches to Coreference Resolution.
In Proceedings of the40th Annual Meeting of the Association for ComputationalLinguistics.G.
Niyu, J. Hale, and E. Charniak.
1998.
A statistical approachto anaphora resolution.
In Proceedings of the Sixth Workshopon Very Large Corpora.E.
Riloff.
1996.
An Empirical Study of Automated DictionaryConstruction for Information Extraction in Three Domains.Artificial Intelligence, 85:101?134.W.
Soon, H. Ng, and D. Lim.
2001.
A Machine Learning Ap-proach to Coreference of Noun Phrases.
Computational Lin-guistics, 27(4):521?541.M.
Stefik.
1995.
Introduction to Knowledge Systems.
MorganKaufmann, San Francisco, CA.
