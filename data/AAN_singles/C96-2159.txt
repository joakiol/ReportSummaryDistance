Decis ion Tree Learning Algor i thm with StructuredAttr ibutes:Appl icat ion to Verbal Case Frame Acquis i t ionHidek i  TanakaNHK Science and  Techn ica l  Research  Laborator iesAbst rac tThe Decision Tree Learning Algorithms(DTLAs) are getting keen attentionfrom the natural anguage processing re-search comlnunity, and there have beena series of attempts to apply them toverbal case frame acquisition.
However,a DTLA cannot handle structured at-tributes like nouns, which are classifiedunder a thesaurus.
In this paper, wepresent a new DTLA that can ratio-nally handle the structured attributes.In the process of tree generation, thealgorithm generalizes each attribute op-timally using a given thesaurus.
We ap-ply this algorithm to a bilingual corpusand show that it successfiflly learned ageneralized ecision tree for classifyingthe verb "take" and that the tree wassmaller with more prediction power onthe open data than the tree learned bythe conventional DTLA.1 In t roduct ionThe group of Decision Tree Learning Algorithms(DTLAs) like CART (Breiman et al, 1984), ID3(Quinlan, 1986) and C4.5 (Quinlan, 1993) aresome of the most widely used algorithms for learn-ing the rules for expert systems and has been sue-eessfully applied to several areas so far.These algorithms are now getting keen atten-tion from the natural anguage processing (NLP)research community since the huge text corpusis becoming widely available.
The most populartouchstone for the DTLA in this community is theverbal case frame or the translation rules.
Therehave already been some attempts, like (Tanaka,1994) and (Almuallim et al, 1994).The group of DTLAs, however, was origi-nally designed to handle "plain" data, whereasnouns are "structured" under a thesaurus.
Al-though handling such "structured attributes" inthe DTLA was described as a "desirable xten-sion" in the book of Quinlan (Quinlan, 1993), the1-10-11 K inuta ,  Setagaya-kuTokyo,  157, Japantanakah@strl, nhk.
or.
jpvalue attribute (case)(semantic restriction) object noun ,.t\ [?
I I Q l \[ ITaro Hanako cat dog elephant TV cameratsurete-iku \[escort\] hakobu\[carry\]class (translation)Figure 1: Case Prame Tree Learned by DTLAproblem has received rather limited attention sofar (Ahnuallim et el., 1995).There have been several attempts to solve tileproblem in the NLP community, such as (Tanaka,1995b), (Almuallim et el., 1995).
These attempts,however, are not always satisfactory in that thehandling of the thesaurus is not flexible enough.In this paper, we introduce an extended DTLA,LASA-1 (inductive arning Algorithm with Struc-tured Attributes), which can handle structuredattributes in an optimmn way.
We first presentan algorithm called T*, which can solve thesub-problem for structured attributes and thenpresent he whole algorithm of LASA-1.
Finally,we report an application of our new algorithm toverbal case frame acquisition and show its effec-tiveness.2 The  St ructured  At t r ibuteP rob lemFigure 1 shows an example decision tree represent-ing acase frame for the verb "take."
This decisiontree was called the case frame tree (Tanaka, 1994)and we follow that convention in this paper, too.One may recognize that the restrictions in figure 1are not semantic ategories but are words: thistree was learned from table I which contains wordforms for the values.
Although the tree has someattractive features mentioned in (Tanaka, 1994),it suffers from two problems.?
weak prediction powerA case frame tree with word forms does nothave high prediction power on the open data943Table 1: Single Attribute Table.,for:.
"take"Object Noun Japanese TranslationTaroHanakocatdogelephantelephantTVcameratsurete-iku (escort)tsurete-iku (escort)tsurete-iku (escort)tsurete-iku (escort)hakobu (carry)hakobu (carry)hakobu (carry)hakobu (carry)(the data not used for learning).
The nounsare the most problematic.
There will bemany unknown nouns in the open data.?
low legibilityIf we include many different nouns in thetraining data (the data used for learning),the obtained tree will have as many branchesas the number of nouns.
The ramified tree ishard for humans to understand.Introducing a thesaurus or a semantic hierar-chy in a case frame tree seems a sound way toameliorate these two problems.
We can replacethe similar nouns in a case fl'ame tree by a propersemantic lass, which will reduce the size of thetree while increasing the prediction power on theopen data.
But how can we introduce a thesaurusinto the conventional DTLA framework?
This isexactly the "structured attributes" problem thatwe mentioned in section 1.3 The  Prob lem Set t ing3.1 Par t ia l  ThesaurusThe DTLA takes an attribute, value and class ta-ble for an input 1 Although the table usuallyincludes multiple attributes, the algorithm evalu-ates an attribute's goodness as a classifier inde-pendently of the rest of the attributes.
In otherwords a "single attribute table" as shown in ta-ble 1 is the flmdamental unit for the DTLA.
Thistable shows an imaginary relationship between anobject noun of the verb "take" and the Japanesetranslation.
We used this table to learn the caseframe tree in figure 1 and it suffered from the twoproblems.Here, we can assume that the word forms of theON are in a thesaurus (We call this thesaurus theoriginal thesaurus) and we can extract the rele-vant part as in figure 2.
We call this tree a partialthesaurus T 2.
If we replace "Taro" and "Hanako"lWe are going to mainly use the terms attribute,value, and class for generality.
They actually refer tothe case, restrictions for the case, and the translationof the verb respectively in our application.
In thispaper, we use these terms interehangeably.2The scores at each node will be explained in sec-tion 3.3.Taro  Hanako  cat  dog  e lephant  TV  camera0 .197 0 .197 0 .197 0 .197 0 .197 0 .197 0 .197tsure tsure tsure tsure hakobu hakobu hakobute- iku te- iku te- iku te- iku hakobuFigure 2: Partial Thesaurus TTable 2: Notations 1TI"PPNL(v)partial thesaurusroot node of Tany node in T, take subscripts i, jany node set in Tset of all nodes in Tset of leaf under pin table 1 by "*human" in T, for example, and as-sign the translation "tsurete-iku" to "*human,"the learned case frame tree will reduce the sizeby one (two leaves in figure 1 are replaced byone leaf).
If we replace "Taro," "Hanako," "cat,""dog" and "elephant" by "*mammal," and assignthe translation "tsurete- iku" to "*mammal" (Themajority translation under the node "*mammal"in T. We are going to use this "majority rule"for the class assignment.
), then the learned casefl'ame tree will reduce the size by four.
But thecase frame tree will produce two translation er-rors ("hakobu" for "elephant") when we classifythe original table 1.
In both cases, the learnedcase frame trees are expected to have reinforcedprediction power on the open data thanks to thesemantic lasses: the replacement in the table gen-eralizes the case frame tree.
We want high-levelgeneralization but low-level translation errors; buthow do we achieve this in an optimum way?3.2 Un ique  and  Complete  CoverGeneral izat ionOne factor we have to consider is the possible com-binations of the node set in T which we use forthe generalization of the single attribute table.
Inthis paper, we allow to use the node sets whichcover the word forms in the table uniquely andcompletely.
These two requirements are formallydefined below using the notations in table 2.Def in i t ion 1: For a given node set P C N, P iscalled the unique cover node set if L(p{)n L(p\]) =?
for Vpi, pj c P and i # j.Def in i t ion 2: For a given node set P CN, P is called the complete cover node set ifU ,cv L(p ) = L(,').944Tab le  3: Notations 2Mp'D(p')O(p)Cif(cOAIAIIt(A)total word count in thesaurusthesaurus node corresponding to pword count under p'set of class under pclassfrequency of (:iset of classf(c )entropy of class distri/mtion in A'rile node set that satisfy the two definitions iscalled the unique and complete cow'~r (UCC) nodeset and each such node set is denoted by P~ ....The set of all UCC node set is denoted by "P. Itshould be noted that if we use only the leaves in Tfor generMization, there will be no actual changein the table and this node set is included in 7 ).The total nund)er of UCC node sets in a tree isgenerally high.
For example, the number of UCCnode set in a 10 ary tree with the depth of 3 isabout 1.28 ?
l0 ~?.
We will consider this prol)lemin section 4.3.3 Goodness  of  Genera l i za t ionAnother factor to consider is the measurement ofthe goodness of a generalization.
To evaluate thisquantitatively, we assign a t)enalty score S(p) toeach node p in ?1' a.~S(p) = a"  C;,~.,~(p) + r':(p), (1)where a is a coefficient, Gw~(p ) is the penalty forgenerality ~ , and E(p) is a I)enalty for the inducederrors by using p.The node that ha,s small S(p) is pro, ferabh;.
AndGv~,n(p ) and E(p) are generally mutually conflict-ing: high generality node p (with low Gv,,n(p)) willinduce many errors resulting in high E(p) and viceversa.
We measure a generalization's goodness bytile total sum of the penalty scores of the nodesused for the generalization.
There are several pos~sible candidates for the penalty score function andwe (:hose the formula (2) for this research.D(v') IO(V)I H, O, , ,  s(p)  = log + t vv J (2)New notations are listed in table 3 in additionto table 2.
The second term in formula (2) is the"weighted entropy" of the class distribution undernode p, which coincides Quinlan's criterion (Quin-lan, 1993).We calculated Gp~n(p) (tile first term of for-mula (2)) based on the word numt)er coverage ofp' in the original thesaurus rather than in the par-tim thesaurus, since the original thesaurus usu-ally contains many more words than tile partialalf p has low generality, it will have high Gp~,~(p).Original ThesaurusPartial Tesaurus T, = - - Z_T  .
.
-To o  o '-" MFigure 3: Generality Calculationthesaurus, and is thus expected to yield a betterestimate on the generMity of node p. TILe ideais shown in figure 3.
The coefficient a is ratherditlicult to handle and wc will touch oil this is-sue ill section 4.3.
The figures attached to eachnode in figure 2 are the example penalty scoresgiven by formula (2) under the assmnption thatthe T and the original thesaurus are tile same anda = 0.0074.With these preparations, we now formally ad-dress the problem of tlm optimum generMizationof the singh' attribute tattle.The Opt imum At t r ibute  Genera l i za t ionGiven a tree whose nodes each have a score:Find 1~ ...... that has the minimal total sum ofscores :arg rain ~ S(pi) (3)I ~,, ,~ ,: Q 7 )Pl G P~ (:,:4 The Algorithms4.1 The  A lgor i thm T*As was mentioned in section 3, the number ofUCC node set in a tree tends to be gigantic, andwe should obviously avoid an exllaustive search tofind the optimum generalization.
To do this searchefficiently, we propose a new algorithm, T*.
Theessence of T* lies in the conversion of the partialthesaurus: from a tree T into a directed acyclicgraph (DAG) T. This makes the problem into"the shortest path problem in a graph," to whichwe can apply several efficient algorithms.
We usethe new notations in table 4 in addition to thosein table 2.The A lgor i thm T*Tstar( value, class){extract partial thesaurus T withvalue and class;/* conversion of T into a DAG T */assign index numbers (1 , .
.
.
,  m)to leaves in T from the left;add start node s to T withindex number 0and c with index number re+l;ror~ach( n ~ N U {s}){extend an arc from n to each4This coefficient was fixed cxperimentMly.945*mammal *anything *instrument0.723 1.00 0.127beast/ I ,,'*human "',,~,,~ 0.586 / / " , // : /  o.1.
--.
, , .%<-- / : '.. /,' , : , ", : ; : ,I1 Taro 2Hanako 3cat 4dog 5elephant 6TV 7camera0.197 0.197 0.197 0.197 0.197 0.197 0.197Figure 4: Traversal Graph 7"Table 4: Notations 3Lmi,~(p) leaf with smallest index in L(p)Lm~,(p) leaf with biggest index in L(p)element in the set H,~defined by (4);}delete original edges appeared in T;/* search for shortest path in 7" */opt=node_set = find_short(7");re turn  opt_node_set;H, ,  : {xlxeNU{e},Lining(x) - 1 : L~(n)}  (4)This algorithm first converts T in figure 2 intoa DAG 7-, as in figure 4.
We call this graph atraversal 9raph and each path from s to e in thetraversal graph a traverse.
The set of nodes oneach traverse is called a traversal node set.Here we have two propositions related to thetraversal graph.P ropos i t ion  1: A traversal graph is a DAG.P ropos i t ion  2: For any P C N, P is a UCCnode set if and only if P is a traversal node set.Since proposition 2 holds, we can solve the opti-mum attribute generalization problem by findingthe shortest raverse 5 in the traversal graph.
Byapplying a shortest path algorithm (Gondran andMinoux, 1984) to figure 4, we find the shortest tra-verse as (s ~ *human --+ *beast --+ *instrument---+ e) arm get the optimally generalized table asin table 5 and the generalized ecision tree as infigure 5.4.2 Cor rectness  and T ime Complex i tyWe will not give a full proof for propositions 1 and2 (correctness of T*) because of the limited space,but give an intuitive explanation of why the twopropositions hold.5The sum of the scores in the traversM node set isminimal.Table 5: Optimally Generalized Single AttributeTable for "take"ON Translation I Freq\[  Error*human tsurete-iku 2 0*beast tsurete-iku 4 2-~instrument hakobu 2 0*human *beast *instrumenttsurete-iku hakobuFigure 5: Optimally Generalized Decision Tree for"take"Let's suppose that we select "*human" in fig-ure 2 for a UCC node set P~cc; then we cannotinclude "*mammal" in the P~c~: there will beleaf overlap between the two nodes, which vio-lates the unique cover.
Meanwhile, we have toinclude nodes that govern Lm~(*human)+ 1, i.e.
"cat," to satisfy the complete cover.
In conclu-sion, we have to include "cat" or "*beast" in theP~,  which satisfies formula (4).
The T* links allsuch possible nodes with arcs, and the traversalnode sets can exhaust T'.One may easily understand that the traversalgraph will be a DAG, since formula (4) allowsan arc between two nodes to be spanned onlyin the direction that increases the index num-ber of the leaf.
Since proposition 1 holds, thetime complexity of the T* can be estimated bythe number of arcs in a traversal graph: there isan algorithm for the shortest path problem in anacyclic graph which runs with time complexity ofO(M), where M is the number of arcs (Gondranand Minoux, 1984).
Then we want to clarify therelationship between the number of leaves (dataamount, denoted by D) and the number of arcsin the traversal graph.
Unfortunately, the rela-tionship between the two quantities varies depend-ing on the shape of the tree (partial thesaurus),then we consider a practical case: k-ary tree withdepth d (Tanaka, 1995a).
In this case, the numberof arcs in the traversal graph is given byk(k + 1), d d 2 _ k (k+l )(k - 1) 2.
(5)Since the number of leaves D in the present he-saurus is k ~ , the first term in formula (5) be-~ D ,  showing that T* has O(D) time comescomplexity in this case.Theoretically speaking, when the partial the-saurus becomes deep and has few leaves, the timecomplexity will become worse, but this is hardlythe situation.
We can say that T* has approxi-mately linear order time complexity in practice.9464.3 The  LASA-1The essence of DTLAs lies in the recursive "searchand division."
It searches for the best classifierattribute in a given table.
It then divides the tablewith values of the attribute.The goodness of an attribute is usually mea-sm'ed by the following quantities (Quinlan, 1993)(The notations are in table 3.).
Now let's a~s-sume that a table contains a set of class A ={Cl, .
.
.
,  c,~}.
The DTLA then evaluates the "pu-rity" of A in terms of the entropy of the classdistribution, H(A).If an attribute has m different values whicil di-vide A into m subsets as A = {BI , .
.
.
,J~m}, theDTLA evahmtes the "purity after division" by the"weighed sum of entropy," WSH(attribute, A).WSH(attribute, A) : B~A ~H(B/ )  (6)The DTLA then measures the goodness of the at-tribute bygain : H(A) - WSH(attribute, A).
(7)With these processes in mind, we can naturallyextend the DTLA to handle the structured at-tributes while integrating T*.
The algorithm islisted below.
Here we have two functions namedmake'lYee 0 and Wsh 0.
The function make~lh'ee0executes the recursive "search and division" andthe Wsh() calculates the weighted sum of entropy.T* is integrated in Wsh 0 at the first "if clause."
aIn short, we use T* to optimally generalize the val-ues of an attribute at each tree generation step,which makes the extension quite natural.The  LASA-1place all classes in input table underroot;makeTree( root, table);makeTree(node, table){A: class set in table;find attribute which maximizesH(A) - Wsh( attribute, table);/* table division part follows*/}Wsh( attribute, table){if(attribute is structured){node_set = Tstar( value, class);replace value with node=set;}re turn  WSH(attributc, A) (6)}We have implemcnted this algorithm as a pack-age that we called LASA- 1(inductive LearningAlgorithm with Structured Attributes).
Thispackage has many parameter setting options.
The6Without this clause, the algorithm is just a con-ventional DTLA.most important one is for parameter a in for-mula (2).
Since it is not easy to find the bestvalue before a trial, we used a heuristic method.The one used in the next section was set by thefollowing method.We put equal emphasis on the two terms informula (2) and fixed a so that the traverse viathe root node of Tand the traverse via leavesonly would have equal scores.
At the beginning,LASA-1 calculated the value for each attribute inthe original table.Although this heuristics does not guarantee tooutput the a that has the minimum errors on ()pendata, the value was not too far off in our experi-ence.5 Empi r i ca l  Eva luat ion5.1 Exper imentWe conducted a case frame tree acquisition exper-iment on LASA-1 and the DTLA 7 using part ofour bilingual corpus for the verb "take."
We used100 English-Japanese sentence pairs.
The pairscontained 15 translations (classes) for "take,"whose occm'rences ranged from 5 to 9.
We firstconverted the sentence pairs into an input tableconsisting of the case (attribute), English wordform (value), and Japanese translation for "take"(class).
We used 6 cases for attributes  and someof these appear in figure 6.We used the Japanese "Ruigo-Kokugo-J iten"(Ono, 1985) for the thesaurus.
It is a 10-ary treewith the depth of 3 or 4.
The semantic lass ateach node of the tree was represented by 1 (toplevel) to 4 (lowest level) digits.
To link the Englishword forms in the input table to the thesaurus inorder to extract a partial thesaurus, we used theJapanese translations for the English word forms.When there was more than one possible semanticclass for a word form, we gave all of them 9 andexpanded the input table using all the semanticclasses.We evaluated both algorithms with using the10-fold cross validation method(Quinlan, 1993).The purity threshold for halting the tree gen-eration was experimentally set at 7570 10 for bothalgorithms.A part of a case frame tree obtained by LASA-1 is shown in figure 6.
We can observe that bothsemantic odes and word forms are mixed at the7Part of LASA-1 was used as the DTLA.Sadverb (DDhl), adverbial particle (Dhl), objectnoun (ONhl), preposition (PNfl), the head of theprepositional phrase (PNhl), and subject (SNhl).9We basically disambiguated the word senses man-ually, and there were not a disastrously large numberof such cases.1?If the total frequency of the majority translationexceeds 75% of the total translation frequency, subtreegeneration halts.947<Root> generalized semantic class<Dhl> 0:I <ONhl> \[04\] : -~ < (2/0) \[ highway(l) path(l) \]<ONhl> \[44\] : ~-~ (9/1) \[ command(l)control(6) power(2) \]<ONhl> \[45\] ?<SNhl> \[5\] ~ -~ ~ (1/0) \[ she(l) \]<SNhl> \[7\] : ~-~l~-j\[:~ 7~ (1/0) \[ West Germany(I) \]I <ONhl> \[9\] : original word forms (occurrence)I <SNhl> \[5\] : ~:~j~o "Ck, a < (1/0) \[ rebel(l) \]I <SNhl> Delta: -:~b:_ 7~'L7~ (1/0)I <SNhl>Shaw: ~Tj~o~ < (1/0)"~g word(A/B) A: data count, B: classification errorFigure 6: Case Frame Tree Learned by LASA-1Table 6: Classification Results on Open Data(%)completeincompletetotalleaf sizeLASA(120)correct err.59.2 20.06.7 14.265.8 34.250.9DTLA(100)correct err.47.0 7.04.0 42.051.0 49.057.9same depth of the tree.
We can also observe thatsemantically close words are generalized by theircommon semantic ode.Table 6 shows the percentage ofeach evaluationitem.
We have 120 open data, not 100, for LASA-1, because the data is expanded ue to the seman-tic ambiguity.
The term "incomplete" in the tabledenotes the cases where the tree retrieval stoppedmid-way because of an "unknown word" in theclassification.
Such cases, however, could some-times hit the correct translation since the algo-rithm output the most frequent translation underthe stopped node as the default answer.In table 6, we can recognize the sharp decreasein incomplete matching rate from 46.0 % (DTLA)to 20.8 % (LASA-1).
The error rate also de-creased from 49.0 % (DTLA) to 34.2 % (LASA-1).The average tree size (measured by the numberof leaves) for DTLA was 57.9, which dropped to50.9 for LASA-1.These results show that LASA-1 was able tosatisfy our primary objectives: to solve the twoproblems mentioned in section 3, "weak predictionpower" and "low legibility.
"5.2 Discuss ionThe shape of the decision tree learned by LASA-1is sensitive to parameter a and the purity thresh-old.
There is no guarantee that our method is thebest, so it would be better to explore for a bettercriterion to decide these values.The penalty score !n this research was designedso that we get the maximum generalization if theerror term in formula (2) stays constant.
As aresult, the subtrees in the deep part are highlygeneralized.
In those parts, the data is sparse andthe high-level generalization is questionable froma linguistic viewpoint.
Some elaboration in thepenalty function might be required.6 Conc lus ionWe have proposed a decision tree learning al-gorithm (inductive Learning Algorithm with theStructured Attributes: LASA-1) that optimallyhandles the structured attributes.
We appliedLASA-1 to bilingual (English and Japanese) dataand showed that it successfully leaned the gener-alized decision tree to classify the Japanese trans-lation for "take."
The LASA-1 package still hassome unmentioned features like the handling ofthe words unknown to the thesaurus and differ-ent a parameter setting.
We would like to reportthose features at another opportunity after furtherexperiments.ReferencesHussein Almuallim, Yasuhiro Akiba, and Take-fumi Yamazaki.
1994.
Two methods for learn-ing alt-j/e translation rules from examples anda semantic hierarchy.
In Proc.
of COLING9~,volume 1, pages 57-63.Hussein Almuallim, Yasuhiro Akiba, and ShigeoKaneda.
1995.
On handling tree-structuredattributes in decision tree learning.
In Proe.of 12th International Conference on MachineLearning, pages 12-20.Leo Breiman, Jerome H. Friedman, Richard A.Olshen, and Charles J.
Stone.
1984.
Classifica-tion and Regression Trees.
Chapman & Hall.Michel Gondran and Michel Minoux.
1984.Graphs and Algorithms.
John Wiley ~z Sons.Susumu Ono.
1985.
Ruigo-kokugo-jiten.Kadokawa Shoten.John Ross Quinlan.
1986.
Induction of decisiontrees.
Machine Learning, 1:81-106.John Ross Quinlan.
1993.
C~.5: Programs forMachine Learning.
Morgan Kaufmann.Hideki Tanaka.
1994.
Verbal case frame acqui-sition from a bilingual corpus: Gradual knowl-edge acquisition.
In Proc.
of COLINGg~, vol-ume 2, pages 727-731.Hideki Tanaka.
1995a.
A linear-time algorithmfor optimal generalization of language data.Technical Report NLC-95-07, IECE.Hideki Tanaka.
1995b.
Statistical earning of"case frame tree" for translating english verbs.Natural Language Processing, 2(3):49-72.948
