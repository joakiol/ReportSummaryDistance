Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 53?59,Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational LinguisticsCoNLL 2014 Shared Task: Grammatical Error Correction witha Syntactic N-gram Language Model from a Big CorporaS.
David HernandezCentro de Investigaci?n enComputaci?n - IPN / M?xicoshernandez_b12@sagitario.cic.ipn.mxHiram CalvoCentro de Investigaci?n enComputaci?n - IPN / M?xicohcalvo@cic.ipn.mxAbstractWe describe our approach to grammatical er-ror correction presented in the CoNLL SharedTask 2014.
Our work is focused on error detec-tion in sentences with a language model basedon syntactic tri-grams and bi-grams extractedfrom dependency trees generated from 90% ofthe English Wikipedia.
Also, we add a na?vemodule to error correction that outputs a setof possible answers, those sentences are scoredusing a syntactic n-gram language model.
Thesentence with the best score is the final sug-gestion of the system.The system was ranked 11th, evidently thisis a very simple approach, but since the begin-ning our main goal was to test the syntacticn-gram language model with a big corpus tofuture comparison.1 IntroductionGrammatical error correction is a difficult taskto solve even for humans, because there are alot of phenomena that can occur in a sentence.One example of the difficulty of the task is thatthe annotators of the training and test data inthe NUCLE (Dahlmeier et al., 2013) differsin the corrections that they made to the sen-tences, those differences in the annotations aremostly because depend on uncontrolled con-ditions, such knowledge, emotional state andthe environment of the annotator at the mo-ment that the task is performed.
This timethe shared task is more difficult than the lastyear (Ng and Wu et al., 2013) that consideredonly five types of errors, and this time the taskconsist into correct all the grammatical errorsin the NUCLE (Ng and Wu et al., 2014).We are interested into test the behaviour ofdifferent methods used in different NLP taskwith the syntactic n-grams as a resource, in or-der to set a baseline to future work.
There iswork that probes that there is an improvementusing syntactic n-grams in (Sidorov and Ve-lasquez et al., 2014) where the author uses syn-tactic n-grams as machine learning features,another example of the use of syntactic n-grams occurred in the CoNLL 2013 SharedTask in (Sidorov and Gupta et al., CoNLL2013), but they used a different approach fromus.Until the moment we do not have a com-parison with the same method that we used inthis task using normal n-grams, still our hy-pothesis is that syntactic n-grams allow us torelate words that in a common n-gram modelwouldn?t be related and that can outperformthe results.For example, in the sentence:"Genetic risk refers more to your chance ofinheriting a disorder or disease .
"Some common tri-grams are "to yourchance", "your chance of", "chance of inher-iting".
The word chance can not be relatedto the words "disorder" or "disease", unless weuse 5-grams or 7-grams, unlike with the syn-tactic tri-grams that as can be appreciated inthe Table 3 the relation between this words arenormally included.Another hypothesis is that a low probabilityin a syntactic n-gram is an indicator that exista wrong token in the portion of a dependencytree.
A simple example of this intuition can beseen in the Table 1 for the sentence "This will, if not already , caused problems as there arevery limited spaces for us ."
from the trainingdata in the NUCLE.
The bold words are wrongtokens annotated in the training data and thenumbers are the token number in the sentence.As can be observed the low probability syn-tactic tri-grams include the wrong tokens.
Theproblem is to establish a threshold in the prob-53qi Syntactic tri-grams0.0 ?are-12 spaces-15 us-17 True?0.0 ?spaces-15 limited-14 us-17 False?0.00004 ?caused-8 will-2 are-12 False?0.00004 ?caused-8 will-2 not-5 False?0.00004 ?caused-8 will-2 This-1 True?0.00004 ?caused-8 will-2 problems-9 False?0.00029 ?caused-8 not-5 are-12 False?0.00047 ?caused-8 are-12 as-10 True?0.00054 ?are-12 spaces-15 limited-14 True?0.00054 ?caused-8 are-12 spaces-15 True?0.00057 ?caused-8 are-12 there-11 True?0.00065 ?caused-8 problems-9 are-12 False?0.00109 ?spaces-15 limited-14 very-13 True?0.00194 ?caused-8 not-5 already-6 True?0.00314 ?caused-8 not-5 problems-9 False?0.00522 ?caused-8 not-5 if-4 True?0.22841 ?are-12 as-10 there-11 False?0.375 ?are-12 as-10 spaces-15 False?0.75510 ?are-12 there-11 spaces-15 False?1.0 ?ROOT-0 caused-8 are-12 True?1.0 ?ROOT-0 caused-8 not-5 True?1.0 ?ROOT-0 caused-8 problems-9 True?1.0 ?ROOT-0 caused-8 will-2 True?1.0 ?not-5 if-4 already-6 False?Table 1: Ordered probabilities of syntactic tri-grams.
The wrong tokens are "caused", "are"and "spaces".abilities to consider as wrong a syntactic tri-gram and separate the wrong tokens from thecorrect ones.2 ResourcesFor the language model we used a Wikipediadump as training data (Wikipedia, 2013)and extracted the text with the MultithreadWikipedia Extractor (Souza, 2012) then wastokenized with the Stanford Tokenizer (Man-ning et al., ).
There are about 87 millions ofsentences and 1,480 millions of tokens.To generate the dependency trees we usedthe Stanford Parser 3.2 (Socher et al., 2013),but for the syntactic n-gram language modelwe only took 90% of the sentences randomlychosen.
The parsing task took a lot of time tobe made with our computing resources and wehad to use threads with the Stanford Parser,unfortunately this increases the amount ofmemory required by the software, so we hadto exclude the sentences with more than onehundred token.
At the end we parsed about75 millions of sentences.The dependency trees were generated asStanford typed dependencies (Marneffe et al.,2006), in specific in the collapsed with prop-agation version as described in (Marneffe etal., 2008).
One example of this kind of de-pendencies can be seen in the Figure 2.
Ascan be observed the collapsed with propaga-tion typed dependencies can break the tree, sostrictly this is a directed graph with the gram-matical relations in the edges and the words ofthe sentence in the nodes, though as conven-tion we will continue referring it as a tree.
Intotal there are about 1,166 million grammati-cal relations.In the error detection phase we usedthe information provided with the NUCLE(Dahlmeier et al., 2013), specifically the to-kens, POS and the grammatical relations fromthe test data in CoNLL style.
From the train-ing data we only made some calculations aboutthe kinds of errors that occur with higher fre-quency and we used this information to in-clude some rules in the correction phase.3 System description3.1 Syntactic n-gram language modelWe used the dependency trees from Wikipediacorpus to generate the syntactic n-grams in thenon-continuous form as described in (Sidorov,2013) and in the book (Sidorov, Book 2013),but there is an significant difference, the cur-rent work with syntactic n-grams was madewith the basic dependencies, and as we saidbefore, we are using the dependencies thatcollapses the prepositions and propagates theconjunctions.
The tree in Figure 1 is in the Ba-sic representation and the differences with thecollapsed and propagated dependencies can beappreciated in the Figure 2.This change allow us to increase the scope ofthe relations between content words, but alsoit makes difficult to find preposition errors, soour system do not consider preposition correc-tion.The tables 2 and 3 show the syntactic tri-grams generated whit each one of the depen-dency representations, but without the rela-tions for lack of space.
As can be observed the54Genetic risk refers more to your chance of in-heriting a disorder or diseaseROOT-0refers-3risk-2more-4 to-5chance-7Genetic-1your-6of-8inheriting-9disorder-11a-10 or-12disease-13rootnsubjdobjprepamodpobjposspreppcompdobjdetccconjFigure 1: Basic dependencies.Genetic risk refers more to your chance of in-heriting a disorder or diseaseROOT-0refers-3risk-2more-4chance-7Genetic-1your-6inheriting-9disorder-11disease-13a-10rootnsubjdobjprep_toamodpossprepc_ofdobjdobjdetconj_orFigure 2: Collapsed dependencies with propa-gation.word "chance" in the basic dependencies is notdirectly related with the words "disorder" and"disease", on the contrary with the collapsedand propagated dependencies.w1w2w3Continuousto-5 chance-7 of-8 Trueto-5 chance-7 your-6 Truerefers-3 to-5 chance-7 Truerefers-3 risk-2 Genetic-1 Trueof-8 inheriting-9 disorder-11 Trueinheriting-9 disorder-11 a-10 Trueinheriting-9 disorder-11 disease-13 Trueinheriting-9 disorder-11 or-12 Truechance-7 of-8 inheriting-9 TrueROOT-0 refers-3 to-5 TrueROOT-0 refers-3 risk-2 TrueROOT-0 refers-3 more-4 Truerefers-3 risk-2 to-5 Falserefers-3 risk-2 more-4 Falserefers-3 more-4 to-5 Falsedisorder-11 a-10 disease-13 Falsedisorder-11 a-10 or-12 Falsedisorder-11 or-12 disease-13 Falsechance-7 your-6 of-8 FalseTable 2: Syntactic tri-grams from the basicdependencies.w1w2w3Continuousrefers-3 chance-7 inheriting-9 Truerefers-3 chance-7 your-6 Truerefers-3 risk-2 Genetic-1 Trueinheriting-9 disorder-11 a-10 Trueinheriting-9 disorder-11 disease-13 Truechance-7 inheriting-9 disorder-11 Truechance-7 inheriting-9 disease-13 TrueROOT-0 refers-3 chance-7 TrueROOT-0 refers-3 risk-2 TrueROOT-0 refers-3 more-4 Truerefers-3 risk-2 chance-7 Falserefers-3 risk-2 more-4 Falserefers-3 more-4 chance-7 Falseinheriting-9 disorder-11 disease-13 Falsedisorder-11 a-10 disease-13 Falsechance-7 your-6 inheriting-9 FalseTable 3: Syntactic tri-grams from the col-lapsed with propagation dependencies.Next we show the maximum likelihood es-timations that we calculated for this languagemodel.
Where w1, w2, w3?
W and W is theset of words of the sentence, r1, r2?
R with Ras the set of grammatical relations between thewords and c ?
{True, False}, with True rep-resenting a continuous syntactic n-gram andFalse a non-continuous syntactic n-gram.In equation (1) we take the maximum valuebetween the probability estimation of the tri-gram with and without grammatical relationsin order to favour the complete tri-gram.Even with a big corpus as Wikipedia andwith the non-continuous syntactic tri-gramsthese estimations can produce zeros in theprobabilities, then we have to draw upon aback-off, so, we add other estimations.55q1= max(q(w1|w2, w3; r1, r2; c),q(w1|w2, w3; c))(1)q2= max(q(w3|w1, w2; r1, r2; c),q(w3|w1, w2; c))(2)Notice that equation (2) is similar to (1),both evaluate the same syntactic tri-gram, butwith a different word of interest.q3={min(q(w2|w1; r1), q(w3|w2; r2)) if c = Truemin(q(w2|w1; r1), q(w3|w1; r2)) if c = False(3)q4={min(q(w2|w1), q(w3|w2)) if c = Truemin(q(w2|w1), q(w3|w1)) if c = False(4)q5= max(q3, q4) (5)q6={min(q(w1|w2; r1), q(w2|w3; r2)) if c = Truemin(q(w1|w2; r1), q(w1|w3; r2)) if c = False(6)q7={min(q(w1|w2), q(w2|w3)) if c = Truemin(q(w1|w2), q(w1|w3)) if c = False(7)q8= max(q6, q7) (8)When the probabilities in equations (1) and(2) are equal to zero, we add a back-off es-timation based in syntactic bi-grams, since asyntactic tri-gram is formed of two syntacticbi-grams or grammatical relations with differ-ent probabilities, but both or one of them cancontain wrong tokens, so we decided to penal-ize the complete probability estimation of thesyntactic tri-gram by choosing the min proba-bility between the two relations.
In the equa-tions (3), (4), (6) and (7) a min operation isincluded to penalize the low probability in asyntactic bi-gram that corresponds to a syn-tactic tri-gram.
In the equations (5) and (8)the max operation plays the same role as inequations (1) and (2).The final expression of the model is shownin equation (9).qstri =????????????????
?q1if q1> 0q2if q1= 0 and q2> 0q5if q2= 0 and q5> 0q8if q5= 0 and q8> 00 Otherwise(9)Where qstri = q(w1, w2, w3; r1, r2; c) andrepresents the probability of the syntactic tri-gram.The syntactic tri-grams continuous and non-continuous produced a vast amount of data,for that reason we only took about 1,660 mil-lions of syntactic tri-grams to made the lan-guage model.
This data can be downloadedfrom (Syntactic N-grams, 2014).3.2 Detection and correctionIn order to detect errors in the test data of NU-CLE (Dahlmeier et al., 2013), we extract theStanford typed dependencies from the conll-style file and to be congruent with the data ofour language model excluded the punct gram-matical relations.
Then we obtain the syn-tactic tri-grams and probabilities of each sen-tence.
The assumption is that low probabilityin a syntactic tri-gram makes it a candidateto be wrong, since grammatical errors couldproduce trees with portions where grammati-cal relations are unseen in the training data orwith a low probability.qi Syntactic tri-grams0.0 refers-3 more-4 chance-7 False0.0 refers-3 risk-2 chance-7 False0.0 refers-3 chance-7 your-6 True0.0 refers-3 chance-7 inheriting-9 True0.00015 refers-3 risk-2 Genetic-1 True0.00023 refers-3 risk-2 more-4 False0.00355 chance-7 your-6 inheriting-9 False0.00355 chance-7 inheriting-9 disorder-11 True0.00609 inheriting-9 disorder-11 disease-13 True0.00609 inheriting-9 disorder-11 a-10 True0.00609 inheriting-9 disorder-11 disease-13 False0.02128 disorder-11 a-10 disease-13 False1.0 ROOT-0 refers-3 more-4 True1.0 ROOT-0 refers-3 risk-2 True1.0 ROOT-0 refers-3 chance-7 True1.0 chance-7 inheriting-9 disease-13 TrueTable 4: Ordered probabilities of the syntactictri-grams.To add the wrong syntactic tri-grams to aset E we include two parameters, ?
= 0.000156which is a threshold and ?
= 0.5 that is apercentage.
To decide whose syntactic tri-grams must be in the set E, we sort them up-wardly as in the table 4, if satisfy the condition(qi < ?)
and (qi >= ?qi+1) for i ?
{1, 2, ..., un-til the first exception } the syntactic tri-gramis added to the set E. The fixed values of ?and ?
were selected by experimentation.w1w2w3Continuousrefers-3 more-4 chance-7 Falserefers-3 risk-2 chance-7 Falserefers-3 chance-7 your-6 Truerefers-3 chance-7 inheriting-9 TrueTable 5: Set of possible wrong syntactic tri-grams.The syntactic tri-grams in the table 5 arethe selected as suspicious to be wrong withthe above considerations.
All the tokens canbe part of a grammatical error, but to get re-placement candidates of all of them can in-crease the complexity of the task and with thewindow of time that we had to accomplish thetask, so we decided to select words in the set Eto be considered as wrong tokens.
We countedthe total amount of occurrences of each tokenin the set E and took the two with higher val-ues.Count Tokens4 refers-34 chance-71 more-41 risk-21 your-61 inheriting-9Table 6: Possible wrong tokens.We chose the best candidates that can re-place each word in the sentence and gener-ate new sentences with each one of the can-didates in his different combinations.
Is easyto see that can be a lot of sentences, consid-ering that each word can have more than onecandidate and that each sentence could havemore than one wrong token to be replaced.
Toobtain the candidates to each suspicious to-ken we search in our training data, words thatstart with the stemmed form of the selected to-ken and that depends of the same word withthe same relation, also we add the lemmatizedword.
The lemmatization was made with theWordNetLemmatizer and the stemming withLancasterStemmer, both from NLTK.
Also weapplied as we said some na?ve rules based onthe most frequent errors in the training cor-pus from NUCLE, for example, when the sus-picious token is a pronoun or a common verbas "have" or "do" we replace them with theirconjugations.For the example in table 6, we have therespective candidates in table 7.
Visibly theword "chants" has nothing to do with the origi-nal token to be replaced, it shows the main rea-son of why we have low score, the rules used inthe correction phase are very simple.
For thisexample, the word "chance" stemmed with theLancasterStemmer is "chant", then the searchof words in the grammatical relations that de-pends on the word "refers" and with the samerelation, outputs the word "chants".Tokens Candidatesrefers referschance-7 chance, chantsTable 7: Candidates.The possible sentences generated for this ex-ample are "Genetic risk refers more to yourchance of inheriting a disorder or disease .
"and "Genetic risk refers more to your chantsof inheriting a disorder or disease .
".In this example the first sentence is the se-lected as the answer by the system.
As canbe appreciated the word chants just worsenthe second sentence.
This capacity to discrim-inate the wrong sentence is what draws ourattention to continue with future work.With this conditions our system produced3613 new sentences from the original 1312.
Tochoose the final answer from the set of pro-posed sentences for each sentence, we only sumall the probabilities of the syntactic tri-gramsof each sentence, naturally the sentence with ahigher mass of probability is the final proposedanswer.4 EvaluationOur official results in the CoNLL 2014 SharedTask on grammatical error correction of theNUCLE and evaluated with the official scorer57(Dahlmeier and Ng, 2012) are shown in the ta-ble 8.
The organizers provide all the resources.Without alternative annotationRecall 2.85Precision 11.28F_0.5 7.09With alternative annotationRecall 3.17Precision 11.66F_0.5 7.59Table 8: Results in the CoNLL 2014 SharedTask .The scoring without alternative answers wasmade with gold edits of the annotators and thescoring with alternative annotation includesanswers proposed by 3 teams that participatedon the Shared Task and were judged by theannotators.5 ConclusionsThe result of the system was not good or as weexpected, first because our approach is simpleand was motivated to test the use of a syntac-tic n-grams language model, second becausethe poor election of candidates to correct theerrors.
However, this task gave us the oppor-tunity to test the behaviour in different condi-tions and now we have a reference to improveour system.6 Future workWe have a lot of work to do, in order to sup-port the use of this kind of resources.
Firstwe have to compare the same method thatwe used, but with a common n-gram languagemodel.
Second is necessary to make a moregeneral language model that can be used withsyntactic 4-grams or more, and analyse howthis increase can affect the recall.
Third find away to made more efficient the consult of theresources.Also we need to add a more wise method tocorrect the detected errors, including prepo-sitions.
The fact that we did not take intoaccount this type of error does not mean thatis not possible to do it with this resources, sowe have to propose an alternative that takesinto account this and other types of errors.AcknowledgementsWork done under partial support of Mexi-can Government (CONACYT, SNI) and Insti-tuto Polit?cnico Nacional, M?xico (SIP-IPN,COFAA-IPN, PIFI-IPN).ReferencesChristopher Manning, Tim Grow, Teg Grenager,Jenny Finkel, and John Bauer.
PTBTokenizerhttp://nlp.stanford.edu/software/tokenizer.shtmlDaniel Dahlmeier, Hwee Tou Ng, and Siew MeiWu 2013.
Building a Large Annotated Corpusof Learner English: The NUS Corpus of LearnerEnglish.
Proceedings of the Eighth Workshop onInnovative Use of NLP for Building EducationalApplications (BEA 2013) .
(pp.
22 ?
31).
At-lanta, Georgia, USA.Daniel Dahlmeier and Hwee Tou Ng 2012.
BetterEvaluation for Grammatical Error Correction.Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics: Human Language Tech-nologies (NAACL 2012) .
(pp.
568 ?
572).
Mon-treal, Canada.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-tian Hadiwinoto and Joel Tetreault.
2013.
TheCoNLL-2013 Shared Task on Grammatical Er-ror Correction.
Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task (CoNLL-2013 SharedTask) .Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Chris-tian Hadiwinoto, Raymond Hendy Susanto, andChristopher Bryant 2014.
The CoNLL-2014Shared Task on Grammatical Error Correc-tion.
Proceedings of the Eighteenth Conferenceon Computational Natural Language Learning:Shared Task (CoNLL-2014 Shared Task) .
Bal-timore, Maryland, USA.Leonardo Souza (leonardossz@gmail.com).2012.
Multithread-Wikipedia-Extractorfor SMP based architectures, Ver-sion: 1.0 (October 15, 2012).https://bitbucket.org/leonardossz/multithreaded-wikipedia-extractor/wiki/HomeMarie Catherine de Marneffe and Christopher D.Manning.
2008.
Stanford Dependencies manual.Marie Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
GeneratingTyped Dependency Parses from Phrase Struc-ture Parses.
In LREC 2006.Grigori Sidorov Book 2013.
Non-linear construc-tion of n-grams in computational linguistics:syntactic, filtered, and generalized n-grams.
G.Sidorov.
2013, 166 p.58Grigori Sidorov, Francisco Velasquez, EfstathiosStamatatos, Alexander Gelbukh, LilianaChanona-Hern?ndez.
2014.
Syntactic N-gramsas Machine Learning Features for NaturalLanguage Processing I Expert Syst.
Appl.. vol.41, no.
3, pp.
853-860, 2014.Grigori Sidorov, 2013.
Syntactic DependencyBased N-grams in Rule Based Automatic En-glish as Second Language Grammar Correc-tion.
International Journal of ComputationalLinguistics and Applications.
vol.
4, no.
2, pp.169-188, 2013.Grigori Sidorov, Anubhav Gupta, Martin Tozer,Dolors Catala, Angels Catena and SandrineFuentes.
2013.
Rule-based System for Auto-matic Grammar Correction Using Syntactic N-grams for English Language Learning (L2).
Pro-ceedings of the Seventeenth Conference on Com-putational Natural Language Learning: SharedTask.
pp.
96-101, 2013.Non-continuous Syntactic N-gramsfrom Wikipedia for the CoNLL2014 Shared Task.
2014.http://iarp.cic.ipn.mx/~dhernandez/conll2014/http://sdavidhernandez.com/conll2014/Richard Socher, John Bauer, Christopher D. Man-ning, and Andrew Y. Ng.
2013.
Parsing WithCompositional Vector Grammars.
Proceedingsof ACL 2013Wikipedia English dump.
2013.enwiki-20130904-pages-articles.xml.bz2http://en.wikipedia.org/wiki/Wikipedia:Database_download59
