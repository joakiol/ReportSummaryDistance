Us ing  Dec is ion  Trees to Const ruct  a P ract i ca l  ParserMasah iko  Haruno*  Satosh i  Sh i ra i  t Yosh i fumi  Ooyama tmharuno  ~hlp.atr .co.
jp shirai,~cslab.kecl.ntt.co.
jp oovama~csla l ) .kecl .nt  t.co.j p*ATR Human In format ion Processing Research Laborator ies2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto  619-02, Japan.tNTT  Communicat ion  Science Laborator ies2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto  619-02, Japan.Abst rac tThis paper describes novel and practical Japaneseparsers that uses decision trees.
First, we con-struct a single decision tree to estimate modifica-tion probabilities; how one phrase tends to modifyanother.
Next, we introduce a boosting algorithmin which several decision trees are constructed andthen combined for probability estimation.
The twoconstructed parsers are evaluated by using the EDRJapanese annotated corpus.
The single-tree methodoutperforms the conventional .Japanese stochasticmethods by 4%.
Moreover, the boosting version isshown to have significant advantages; 1) better pars-ing accuracy than its single-tree counterpart for anyamount of training data and 2) no over-fitting todata for various iterations.1 In t roduct ionConventional parsers with practical levels of perfor-mance require a number of sophisticated rules thathave to be hand-crafted by human linguists.
It istime-consunaing and cumbersome to naaintain theserules for two reasons.?
The rules are specific to the application domain.?
Specific rules handling collocational expressionscreate side effects.
Such rules often deterioratet, he overall performance of the parser.The stochastic approach, on the other hand, hasthe potential to overcome these difficulties.
Becauseit.
induces tochastic rules to maximize overall per-formance against raining data, it not only adaptsto any application domain but.
also may avoid over-fitting to the data.
In the late 80s and early 90s, theinduction and parameter estimation of probabilis-tic context free grammars (PCFGs) from corporawere intensively studied.
Because these grammarscomprise only nonterminal and part-of-speech tagsymbols, their performances were not enough to beused in practical applications (Charniak, 1993).
Abroader ange of information, in particular lexical in-formation, was found to be essential in disambiguat-ing the syntactic structures of real-world sentences.SPATTER (Magerman, 1995) augmented the purePCFG by introducing a number of lexical attributes.The parser controlled applications ofeach rule by us-ing the lexical constraints induced by decision treealgorithm (Quinlan, 1993).
The SPATTER parserattained 87% accuracy and first made stochasticparsers a practical choice.
The other type of high-precision parser, which is based on dependency anal-ysis was introduced by Collins (Collins, 1996).
De-pendency analysis first segments a sentence into syn-tactically meaningful sequences of words and thenconsiders the modification of each segment.
Collins'parser computes the likelihood that each segmentmodifies the other (2 term relation) by using largecorpora.
These modification probabilities are con-ditioned by head words of two segments, distancebetween the two segments and other syntactic fea-tures.
Although these two parsers have shown simi-lar performance, the keys of their success are slightlydifferent.
SPATTER parser performance greatly de-pends on the feature selection ability of the decisiontree algorithm rather than its linguistic representa-tion.
On the other hand, dependency analysis playsan essential role in Collins' parser for efficiently ex-tracting information from corpora.In this paper, we describe practical Japanese de-pendency parsers that uses decision trees.
In theJapanese language, dependency analysis has beenshown to be powerful because segment (bunsetsu)order in a sentence is relatively free compared toEuropean languages..Japanese dependency parsersgenerally proceed in three steps.1.
Segment a sentence into a sequence of bunsetsu.2.
Prepare a modification matrix, each value ofwhich represents how one bunsetsu is likely tomodify another.3.
Find optimal modifications in a sentence by adynamic programming technique.The most difficult part is the second; how to con-struct a sophisticated modification matrix.
Withconventional Japanese parsers, the linguist nmstclassify the bunsetsu and select appropriate f aturesto compute modification values.
The parsers thussuffer from application domain diversity and the sideeffects of specific rules.505Stochastic dependency parsers like Collins', on theother hand, define a set of attributes for condition-ing the modification probabilities.
The parsers con-sider all of the attributes regardless of bunsetsu type.These methods can encompass only a small numberof features if the probabilities are to be preciselyevaluated from finite number of data.
Our decisiontree method constructs a more sophisticated modi-fication matrix.
It automatically selects a sufficientnumber of significant attributes according to bun-setsu type.
We can use arbitrary numbers of theattributes which potentially increase parsing accu-racy.Natural anguages are full of exceptional nd collo-cational expressions.
It is difficult for machine learn-ing algorithms, as well as human linguists, to judgewhether a specific rule is relevant in terms of over-all performance.
To tackle this problem, we testthe mixture of sequentially generated ecision trees.Specifically, we use the Ada-Boost algorithm (Fre-und and Schapire, 1996) which iteratively performstwo procedures: 1. construct a decision tree basedon the current data distribution and 2. updatingthe distribution by focusing on data that are notwell predicted by the constructed tree.
The finalmodification probabilities are computed by mixingall the decision trees according to their performance.The sequential decision trees gradually change frombroad coverage to specific exceptional trees that.
can-not be captured by a single general tree.
In otherwords, the method incorporates not only general ex-pressions but also infrequent specific ones.The rest of the paper is constructed as follows.Section 2 summarizes dependency analysis for theJapanese language.
Section 3 explains our decisiontree models that compute modification probabili-ties.
Section 4 then presents experimental resultsobtained by using EDR Japanese annotated corpora.Finally, section 5 concludes the paper.2 Dependency Analysis in JapaneseLanguageThis section overviews dependency analysis in theJapanese language.
The parser generally performsthe following three steps.1.
Segment a sentence into a sequence ofbunsetsu.2.
Prepare modification matrix each value of whichrepresents how one bunsetsu is likely to modifythe other.3.
Find optimal modifications in a sentence by adynamic programming technique.Because there are no explicit delimiters betweenwords in Japanese, input sentences are first wordsegmented, part-of-speech tagged, and then chunkedinto a sequence of bunsetsus.
The first step yields,for the following example, the sequence of bunsetsudisplayed below.
The parenthesis in the Japaneseexpressions represent the internal structures of thebunsetsu (word segmentations).Example: a~lq e)~7~12~.~:C)-~U ~o75~r7 -1' Y -~ ~r,A.
t~((~l~)(e~)) ( (Y~)( I : ) )  ( (~)i)(e)))kinou-no yuugata-ni kinjo-noyesterday-NO evenin~Nl neighbor-No( (~?
~)(~)) ((v -?
: -)(?))
((~2,z,)(t:)kodomo-ga wain-wo nornuTtachildren-GA wine-WO drink+PASTThe second step of parsing is to construct a modifi-cation matrix whose values represent the likelihoodthat one bunsetsu modifies another in a sentence.In the Japanese language, we usually make two as-sumptions:1.
Every bunsetsu except the last one modifiesonly one posterior bunsetsu.2.
No modification crosses to other modificationsin a sentence.Table 1 illustrates a modification matrix for theexample sentence.
In the matrix, columns and rowsrepresent anterior and posterior bunsetsus, respec-tively.
For example, the first bunsetsu "kinou- no"modifics the second 'yuugala-ni'with score 0.T0 andthe third 'kinjo-no' with score 0.07.
The aim of thispaper is to generate a modification matrix by usingdecision trees.kfnou-no~tul#ata.ni 0.70  yvugata-ni**njo-no 0.07  0 .10  kfnjo.nokodorna-#a 0,10  0 .10  0 .70  kadomo*~a~ain-~o 0,10  0 .10  0 .20  0 .05nomu.
ta  0.03  0 .70  0 .10  0 .95i ,  a ln .
mlo1.00Table 1: Modification Matrix for Sample SentenceThe final step of parsing optimizes the entire de-pendency structure by using the values in the mod-ification matrix.Before going into our model, we introduce the no-tations that will be used in the model.
Let S bethe input sentence.
S comprises a bunsetsu set B oflength m ({< bl,f~ >,- .
- ,<  bm,f,, >}) in whichbi and f i  represent the ith bunsetsu and its features,respectively.
We define D to be a modification set; D= {rood(l),..., mod(m - 1)} in which rood(i) indi-cates the number of busetsu modified by the ith bun-setsu.
Because of the first assumption, the length ofD is always m-  1.
Using these notations, the resultof the third step for the example can be given as D= {2, 6, 4, 6, 6} as displayed in Figure 1.3 Decision Trees for DependencyAnalysis3.1 Stochast ic  Mode l  and  Decis ion TreesThe stochastic dependency parser assigns the mostplausible modification set Dbe,t to a sentence S in5061kmou-no uugat3 4jc-no kodomo-ga,ll5 6t'ain- '0 n0mu.tatFigure 1: Modification Set for Sample Sentenceterms of the training data distribution.Dbest = argmax D P( D\[S) = arg,nax D P( D\[B)By assuming the independence of modifica-tions, P(D\[B)  can be transformed as follows.P(yeslbi, bj, f l  , " ' ,  fro) means the probability thata pair of bunsetsu bi and bj have a modification rela-tion.
Note that each modification is constrained byall features{f, , - - .
,  fro} in a sentence despite of theassumption of independence.We use decision treesto dynamically select appropriate features for eachcombination of bunsetsus from {f , , - - - ,  fm }.mi-~P(yes\[bi, "" ,fro) P (D IB)  = 1-I - bj, f , , .Let us first consider the single tree case.
Thetraining data for the decision tree comprise any un-ordered combination of two bunsetsu in a sentence.Features used for learning are the linguistic informa-tion associated with the two bunsetsu.
The next sec-tion will explain these features in detail.
The classset for learning has binary values yes and no whichdelineate whether the data (the two bunstsu) hasa modification relation or not.
In this setting, thedecision tree algorithm automatically and consecu-tively selects the significant, features for discriminat-ing modify/non-modify relations.We slightly changed C4.5 (Quinlan, 1993) pro-grams to be able to extract class frequen-cies at every node in the decision tree be-cause our task is regression rather than classi-fication.
By using the class distribution, wecompute the probability PDT(yeslbi, bj, f ~, .
.
.
,  fro)which is the Laplace estimate of empirical likeli-hood that bi modifies bj in the constructed eci-sion tree DT.
Note that it.
is necessary to nor-realize PDT(yes\[bi, bj, f , , .
.
.
,  fro) to approximateP(yes \ [b i ,b j , fx , " ' , fm) .
By considering all can-didates posterior to bi, P (yes lb i ,b .
i , f l , ' " , fm)  iscomputed using a heulistic rule (1).
It is of coursereasonable to normalize class frequencies instead ofthe probability PoT(yeslbi,  bj,, f , , .
.
.
,  fro).
Equa-tion (1) tends to emphasize long distance dependen-cies more than is true for frequency-based normal-ization.P(yeslbi, bj, f ,  , .
.
.
,  f.~) ~_PDT(yeslbi, bj, f l , ' " ,  fro) (1)~ >i m P DT(yeslbl, by, f ~ , .
.
.
, f ,, )Let us extend the above to use a set of decisiontrees.
As briefly mentioned in Section 1, a numberof infrequent and exceptional expressions appear inany natural language phenomena; they deterioratethe overall performance of application systems.
Itis also difficult for automated learning systems todetect and handle these expressions because xcep-tional expressions are placed ill the same class asfrequent ones.
To tackle this difficulty, we gener-ate a set of decision trees by adaboost (Freund andSchapire, 1996) algorithm illustrated in Table 2.
Thealgorithm first sets the weights to 1 for all exana-pies (2 in Table 2) and repeats the following twoprocedures T times (3 in Table 2).1.
A decision tree is constructed by using the cur-rent weight vector ((a) in Table 2)2.
Example data are then parsed by using the treeand the weights of correctly handled examplesare reduced ((b),(c) in Table 2)1.
'2..3.Input: sequence of N examples < eL, u,~ > .
.
.
.
, <eN, .wN > in which el and wi represent an exampleand its weight, respectively.Initialize the weight vector wi =1 for i = 1,. .
.
,  NDo for t = l ,2 , .
.
.
, T(a) Call C4.5 providing it with the weight vectorw,s and Construct  a modification probabilityset ht(b) Let Error be a set of examples that are not.identified by litCompute the pseudo error rate of ht:e' = E iCE .
.
.
.
wi/ ~ ,=INw,if et > 5' then abort loopl - -e  t(c) For examples correctly predicted by ht, updatethe weights vector to be wi = wiflt4.
Output  a final probability set:h l=Zt=,T( log~)ht /Zt=,T( Iog~)Table 2: Combining Decision Trees by Ada-boostAlgorithmThe final probability set h I is then computedby mixing T trees according to their perfor-mance (4 in Table 2).
Using h: instead ofPoT(yeslbi  , bj, f l , ' " ,  f,,~), in equation (1) gener-ates a boosting version of the dependency parser.3.2 L inguist ic  Feature  Types  Used forLearn ingThis section explains the concrete feature setting weused for learning.
The feature set mainly focuses on5071 lexical information of head word 6 distance between two bunsetsu2 part-of-speech of head word 7 particle 'wa' between two bunsetsu3 type of bunsetsu 8 punctuation between two bunsetsu4 punctuation5 parenthesesTable 3: Linguistic Feature Types Used for LearningFeature  Type Va |net4,5$ ' ) ,  <6~' ,  ~tE,  t~'~t ~', l~'tt~"6, .
:~ ,  - '~ ' ,  5 ,  a~.
,  L ,  L?~', E ' . '
,  "tr.,'t~L, "1-6, "t',"~, "~,  "~ st ' ~-.
\ ] .
'~,  %*~t.t,- " , "~, \]_'0'), t.?l~ * , ~**?9"C, \ ] ' .gt~,gl~,9\] '*~,9"C,  99 ~,~?~,, & ~,  __%, ~,  ~a~, @t , ,  @t ,L ,  @t,Ll2, @~6,  ~'~", t?6 ,  @6Ul : ,  to0 ,~k~' ,  ~k 'C ,  : : ,  ~ ,  0~, d )h ,  t l ,  I~./J':), ~,  I |E ,  I t : ,  tt::~., t-C, ~b, ~ L<I / ,l.t~.
~, ~-, ~I.~R~I~'~, .~1~., ~,.~l~;l~\]f'tit, l g '~ ,  $1"tf~,t~l, .V,?IL ~\[\]glllql~\].
e~ i~\ ] ,n o n ,  k~.,.X, ~J.
?~non ,  " ,  ~, ~ .
\ [ ,  \ [ .
\ [ ,  ~, l ,  " , ' ,~ , , , I , .
I , \ ] , JA(0) ,  B ( ;~4) ,  C (>5)7 0, 18 0 ,  1Table 4: Values for Each Feature Type?3.Sie3a2sa2"graph.dirt-sooo *occo ~Sooo 2oo00 2scoo 3o00o asooo 4ooco 45ooo sooooN~bet  of Ttammg DataFigure 2: Learning Curve of Single-Tree Parserthe two bunsetsu constituting each data.. Tile classset consists of binary values which delineate whethera sample (the two bunsetsu) have a modification re-lation or not.
We use 13 features for the task, 10 di-rectly from the 2 bunsetsu under consideration and3 for other bunsetu information as summarized inTable 3.Each bunsetsu (anterior and posterior) has the 5features: No.1 to No.5 in Table 3.
Features No.6to No.8 are related to bunsetsu pairs.
Both No.1and No.2 concern the head word of the bunsetsu.No.1 takes values of frequent words or thesaurus cat-egories (NLRI, 1964).
No.2, on the other hand, takesvalues of part-of-speech tags.
No.3 deals with bull-setsu types which consist of functional word chunksor tile part-of-speech tags that dominate tile bull-setsu's syntactic haracteristics.
No.4 and No.5 arebinary features and correspond to punctuation andparentheses, respectively.
No.6 represents how manybunsetsus exist, between the two bunsetsus.
Possiblevalues are A(0), B(0--4) and C(>5).
No.7 deals withthe post-positional particle 'wa' which greatly influ-ences the long distance dependency of subject-verbmodifications.
Finally, No.8 addresses tile punctua-tion between the two bunsetsu.
Tile detailed valuesof each feature type are summarized ill Table 4.4 Exper imenta l  Resu l tsWe evaluated the proposed parser using the EDRJapanese annotated corpus (EDR, 199.5).
The ex-periment consisted of two parts.
One evaluated thesingle-tree parser and the other tile boosting coun-terpart.
In tile rest of this section, parsing accuracyrefers only to precision; how many of tile system'soutput are correct in terms of the annotated corpus.We do not show recall because we assume very bun-setsu modifies only one posterior bunsetsu.
The fea-tures used for learning were non head-word features,( i .e.
,  type 2 to 8 in Table 3).
Section 4.1.4 investi-gates lexical information of head words such as fre-quent, words and thesaurus categories.
Before goinginto details of tile experimental results, we sunnna-rize here how training and test data were selected.1.
After all sentences in the EDR corpuswere word-segmented and part-of-speechtagged (Matsumoto and others, 1996), theywere then chunked into a sequence of bunsetsu.2.
All bunsetsu pairs were compared with EDRbracketing annotation (correct segmentations508I Conf idence  Level  \]1 25% ~50%(,  75(~,  95% IPars ing  Accuracy  82.01% ~3.43~, 83.52% 83.35%Table 5: Number of Training Sentences v.s.
Parsing AccuracyI Number  of  T ra in ing  Sentences  H 3000 6000 10000 20000 30000 50000I \ [ \ [ P a r s i n g  Accuracy  ' 82.07% 82.70% 83.52% 84.07% 84.27% 84.33%Table 6: Pruning Confidence Level v.s.Parsing Accuracyand modifications).
If a sentence contained apair inconsistent with the EDR annotation, thesentence was removed from the data.3.
All data examined (total number of sen-tences:207802, total number of bun-set.su:1790920) were divided into 20 files,The training data were same number of firstsentences of the 20 files according to thetraining data size.
Test data (10000 sentences)were the 2501th to 3000th sentences of eachfile.4.1 Single T ree  Exper imentsIn the single tree experiments, we evaluated the fol-lowing 4 properties of the new dependency parser.?
Tree pruning and parsing accuracy?
Number of training data and parsing accuracy?
Significance of features other than Head-wordLexical Information?
Significance of Head-word Lexical Information4.1.1 P run ing  and Pars ing  AccuracyTable 5 summarizes the parsing accuracy with var-ious confidence levels of pruning.
The number oftraining sentences was 10000.In C4.5 programs, a larger value of confidencemeans weaker pruning and 25% is connnonly used invarious domains (Quinlan, 1993).
Our experimentalresults show that 75% pruning attains the best per-formance, i.e.
weaker pruning than usual.
In theremaining single tree experiments, we used the 75%confidence level.
Although strong pruning treats in-frequent data as noise, parsing involves many ex-ceptional and infrequent modifications as mentionedbefore.
Our result means that only information in-cluded in small numbers of samples are useful fordisambiguating the syntactic structure of sentences.4.1.2 The  amount  of  T ra in ing  Data  andParsing AccuracyTable 6 and Figure 2 show how the number of train-ing sentences influences parsing accuracy for thesame 10000 test.
sentences.
They illustrate tile fol-lowing two characteristics of the learning curve.1.
The parsing accuracy rapidly rises up to 30000sentences and converges at around 50000 sen-tences.2.
The maximum parsing accuracy is 84.33% at50000 training sentences.We will discuss the maximum accuracy of 84.33%.Compared to recent stochastic English parsers thatyield 86 to 87% accuracy (Collins, 1996; Mager-man, 1995), 84.33% seems unsatisfactory at the firstglance.
The main reason behind this lies in the dif-ference between the two corpora used: Penn Tree-bank (Marcus et al, 1993) and EDR corpus (EDR,1995).
Penn Treebank(Marcus et al, 1993) was alsoused to induce part-of-speech (POS) taggers becausethe corpus contains very precise and detailed POSmarkers as well as bracket, annotations.
In addition,English parsers incorporate the syntactic tags thatare contained in the corpus.
The EDR corpus, on theother hand, contains only coarse POS tags.
We usedanother Japanese POS tagger (Matsumoto and oth-ers, 1996) to make use of well-grained informationfor disambiguating syntactic structures.
Only thebracket information in the EDR corpus was consid-ered.
We conjecture that the difference between theparsing accuracies i due to the difference of the cor-pus information.
(Fujio and Matsumoto, 1997) con-structed an EDR-based dependency parser by usinga similar method to Collins' (Collins, 1996).
Theparser attained 80.48% accuracy.
Although thiertraining and test.
sentences are not exactly same asours, the result seems to support our conjecture onthe data difference between EDR and Penn Tree-bank.4.1.3 Significance of Non Head-WordFeaturesWe will now summarize tile significance of each nonhead-word feature introduced in Section 3.
The in-fluence of the lexical information of head words willbe discussed in the next section.
Table 7 illustrateshow the parsing accuracy is reduced when each fea-ture is removed.
The number of training sentenceswas 10000.
In the table, ant and post.
represent, heanterior and the posterior bunsetsu, respectively.Table 7 clearly demonstrates that the most signifi-509Feature Accuracy Decrease Feature Accuracy Decreaseant POS of head -0.07% post punctuation +1.62(7(,ant bunsetsu typeant punctuationant parenthesespost POS of headpost bunsetsu type+9.34%+1.15%+0.00%+2.13%+0.52%post parentheses -e0.00%distance between two bunsetsus +5.21%punctuation between two bunsetsus +0.01%'wa' between two bunsetsus +1.79%Table 7: Decrease of Parsing Accuracy When Each Attribute RemovedHead Word  In format ionParsing Accuracyl\] 100words 200words Level l  Level2 I83.34% 82.68%82.51%81.67%Table 8: Head Word Information v.s.
Parsing Accuracycant features are anterior bunsetsu type and distancebetween the two bunsetsu.
This result may partiallysupport an often used heuristic; bunsetsu modifica-tion should be as short range as possible, providedthe modification is syntactically possible.
In partic-ular, we need to concentrate on the types of bunsetsuto attain a higher level of accuracy.
Most featurescontribute, to some extent, to the parsing perfor-mance.
In our experiment, information on paren-theses has no effect on the performance.
The reasonmay be that EDR contains only a small number ofparentheses.
One exception in our features is an-terior POS of head.
We currently hypothesize thatthis drop of accuracy arises from two reasons.?
In many cases, the POS of head word can bedetermined from bunsetsu type.?
Our POS tagger sometimes assigns verbs forverb-derived nouns.4.1.4 Signif icance of  Head-words  LexicalIn fo rmat ionWe focused on the head-word feature by testing thefollowing 4 lexical sources.
The first and the secondare the 100 and 200 most frequent words, respec-tively.
The third and the fourth are derived from abroadly used Japanese thesaurus, Word List by Se-mantic Principles (NLRI, 1964).
Level 1 and Level 2classify words into 15 and 67 categories, respectively.1.
100 most Frequent words2.
200 most Frequent words3.
Word List Level 14.
Word List Level 2Table 8 displays the parsing accuracy when eachhead word information was used in addition to theprevious features.
The number of training sentenceswas 10000.
In all cases, the performance was worsethan 83.52% which was attained without head wordlexical information.
More surprisingly, more headword information yielded worse performance.
Fromthis result, it.
may be safely said, at least, for theJapanese language,' that we cannot expect, lexica\] in-formation to always improve the performance.
Fur-ther investigation of other thesaurus and cluster-ing (Charniak, 1997) techniques i necessary to fullyunderstand the influence of lexical information.4.2 Boost ing  Exper imentsThis section reports experimental results on theboosting version of our parser.
In all experiments,pruning confidence levels were set.
to 55%.
Table 9and Figure 3 show the parsing accuracy when thenumber of training examples was increased.
Becausethe number of iterations in each data set changed be-tween 5 and 8, we will show the accuracy by combin-ing the first 5 decision trees.
In Figure 3, the dottedline plots the learning of the single tree case (identi-cal to Figure 2) for reader's convenience.
The char-acteristics of the boosting version can be summa-rized as follows compared to the single tree version.?
The learning curve rises more rapidly with asmall number of examples.
It is surprising thatthe boosting version with 10000 sentences per-forms better than the single tree version with50000 sentences.?
The boosting version significantly outperformsthe single tree counterpart for any number ofsentences although they use the same featuresfor learning.Next, we discuss how the number of iterations in-fluences the parsing accuracy.
Table 10 shows theparsing accuracy for various iteration numbers when50000 sentences were used as training data.
The re-suits have two characteristics.?
Parsing accuracy rose up rapidly at the seconditeration.
* No over-fitting to data was seen although theperformance of each generated tree fell around30% at the final stage of iteration.510I Nombe.
o T. i,,i,,gSe,l*e,,co.
I 3OO0 6OOO I'0000 2OOOO 3OO0O 5O0OO IPars ing  Accuracy 83.10% 84.03% 84.44% 84.74% 84.91% 85.03%Table 9: Number of Training Sentences v.s.
Parsing AccuracyPars ing Accuracy \[\[ 84.32% 84.93% 84.89% 84.86% 85.03% 85.01% ITable 10: Number of Iteration v.s.
Parsing Accuracy5 Conc lus ionWe have described a new Japanese dependencyparser that uses decision trees.
First, we introducedthe single tree parser to clarify the basic character-istics of our method.
The experimental results howthat it outperforms conventional stochastic parsersby 4%.
Next, the boosting version of our parser wasintroduced.
The promising results of the boostingparser can be summarized as follows.?
The boosting version outperforms the single-tree counterpart regardless of training dataamount.?
No data over-fitting was seen when the numberof iterations changed.We now plan to continue our research in two direc-tions.
One is to make our parser available to a broadrange of researchers and to use their feedback to re-vise the features for learning.
Second, we will applyour method to other languages, say English.
Al-though we have focused on the Japanese language,it is straightforward to modi~" our parser to workwith other languages.05.5858,358382,5B2"laoostJng.O=r"// /'/JN~ber  Ot Tra~mg OataProc.
15th National Conference on Artificial 172-telligence, pages 598-603.Michael Collins.
1996.
A New Statistical Parserbased on bigram lexical dependencies.
In Proc.34th Annual Meeting of Association for Compu-tational Linguistics, pages 184-191.Japan Electronic Dictionary Reseaech Institute Ltd.EDR, 1995. the EDR Electronic Dictionary Tech-nical Guide.Yoav Freund and Robert Schapire.
1996.
Adecision-theoretic generalization of on-line learn-ing and an application to boosting.M.
Fujio and Y. Matsumoto.
1997.
Japanese de-pendency structure analysis based on statistics.In SIGNL NL117-12, pages 83-90.
(in Japanese).David M. Magerman.
1995.
Statistical Decision-Tree Models for Parsing.
In Proc.33rd AnnualMeeting of Association for Computational Lin-guistics, pages 276-283.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Compu-tational Linguistics, 19(2):313-330, June.Y.
Matsumoto et al 1996.
Japanese MorphologicalAnalyzer Chasen2.0 User's Manual.NLRI.
1964.
Word List by Semantic Principles.Syuei Syuppan.
(in Japanese).J.Ross Quinlan.
1993.
C4.5 Programs for MachineLearning.
Morgan Kaufinann Publishers.Figure 3: Learning Curve of Boosting ParserReferencesEugene Charniak.
1993.
Statistical Language Learn-ing.
The MIT Press.Eugene Charniak.
1997.
Statistical Parsing with aContext-free Grammar and Word Statistics.
In511
