Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217?220,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsCooperative User Models in Statistical Dialog SimulatorsMeritxell Gonza?lez1,2, Silvia Quarteroni1, Giuseppe Riccardi1, Sebastian Varges11 DISI - University of Trento, Povo (Trento), Italy2 TALP Center - Technical University of Catalonia, Barcelona, Spainmgonzalez@lsi.upc.edu, name.lastname@disi.unitn.itAbstractStatistical user simulation is a promis-ing methodology to train and evaluate theperformance of (spoken) dialog systems.We work with a modular architecture fordata-driven simulation where the ?inten-tional?
component of user simulation in-cludes a User Model representing user-specific features.
We train a dialog sim-ulator that combines traits of human be-havior such as cooperativeness and con-text with domain-related aspects via theExpectation-Maximization algorithm.
Weshow that cooperativeness provides a finerrepresentation of the dialog context whichdirectly affects task completion rate.1 IntroductionData-driven techniques are a promising approachto the development of robust (spoken) dialog sys-tems, particularly when training statistical dialogmanagers (Varges et al, 2009).
User simulatorshave been introduced to cope with the scarcity ofreal user conversations and optimize a number ofSDS components (Schatzmann et al, 2006).In this work, we investigate the combination ofaspects of human behavior with contextual aspectsof conversation in a joint yet modular data-drivensimulation model.
For this, we integrate conversa-tional context representation, centered on a DialogAct and a Concept Model, with a User Model rep-resenting persistent individual features.
Our aimis to evaluate different simulation regimes againstreal dialogs to identify any impact of user-specificfeatures on dialog performance.In this paper, Section 2 presents our simulatorarchitecture and Section 3 focuses on our model ofcooperativeness.
Our experiments are illustratedWork partly funded by EU project ADAMACH (022593)and Spanish project OPENMT-2 (TIN2009-14675-C03).in Section 4 and conclusions are summarized inSection 5.2 Simulator ArchitectureData-driven simulation takes place within the rule-based version of the ADASearch system (Vargeset al, 2009), which uses a taxonomy of 16 dialogacts and a dozen concepts to deal with three tasksrelated to tourism in Trentino (Italy): Lodging En-quiry, Lodging Reservation and Event Enquiry.Simulation in our framework occurs at the in-tention level, where the simulator and the Dia-log Manager (DM) exchange actions, i.e.
or-dered sequences of dialog acts and a number ofconcept-value pairs.
In other words, we repre-sent the DM action as as = {da0, .., dan}, (sis for ?System?)
where daj is short for a dialogact defined over zero or more concept-value pairs,daj(c0(v0), .., cm(vm)).In response to the DM action as, the differentmodules that compose the User Simulator gener-ate an N -best list of simulated actions Au(as) ={a0u, .., aNu }.
The probability of each possible ac-tion being generated after the DM action as is es-timated based on the conversation context.
Such acontext is represented by a User Model, a DialogAct Model, a Concept Model and an Error Model(Quarteroni et al, 2010).
The User Model simu-lates the behavior of an individual user in terms ofgoals and other behavioral features such as coop-erativeness and tendency to hang up.
The DialogAct Model generates a distribution of M actionsAu = {a0u, .., aMu }.
Then, one action a?u is chosenout of Au.
In order to vary the simulation behav-ior, the choice of the user action a?u is a randomsampling according to the distribution of proba-bilities therein; making the simulation more realis-tic.
Finally, the Concept Model generates conceptvalues for a?u; and the Error Model simulates thenoisy ASR-SLU channel by ?distorting?
a?u.These models are derived from the ADASearch217dataset, containing 74 spoken human-computerconversations.2.1 User ModelThe User Model represents user-specific fea-tures, both transient and persistent.
Thetransient feature we focus on in this work isthe user?s goal in the dialog (UG), representedas a task name and the list of concepts andvalues required to fulfill it: an example ofUG is {Activity(EventEnquiry), Time day(2),Time month(may), Event type(fair), Loca-tion name(Povo)}.Persistent features included in our model so farare: patience, silence (no input) and cooperative-ness.
Patience pat is defined as the tendencyto abandon the conversation (hang up event), i.e.pat = P (HangUp|as).
Similarly, NoInput prob-ability noi is used to account for user behavior innoisy environments: noi = P (NoInput|as).
Fi-nally, cooperativeness coop is a real value repre-senting the ratio of concepts mentioned in as thatalso appear in a?u (see Section 3).2.2 Dialog Act ModelWe define three Dialog Act (DA) Models: Obedi-ent (OB), Bigram (BI) and Task-based (TB).In the Obedient model, total patience and coop-erativeness are assumed of the user, who will al-ways respond to each query requiring values for aset of concepts with an answer concerning exactlysuch concepts.
Formally, the model responds to aDM action as with a single user action a?u obtainedby consulting a rule table, having probability 1.
Incase a request for clarification is issued by the DM,this model returns a clarifying answer.
Any offerfrom the DM to continue the conversation will beeither readily met with a new task request or de-nied at a fixed probability: Au(as) = {(a?u, 1)}.In the Bigram model, first defined in (Eckert etal., 1997), a transition matrix records the frequen-cies of transition from DM actions to user actions,including hang up and no input/no match.
Givena DM action as, the model responds with a list ofM user actions and their probabilities estimatedaccording to action distribution in the real data:Au(as) = {(a0u, P (a0u|as)), .., (aMu , P (aMu |as))}.The Task-based model, similarly to the ?goal?model in (Pietquin, 2004), produces an action dis-tribution containing only the actions observed inthe dataset of dialogs in the context of a spe-cific task Tk.
The TB model divides the datasetinto one partition for each Tk, then creates atask-specific bigram model, by computing ?
k:Au(as) = {(a0u, P (a0u|as, Tk)), .., (aMu , P (aMu |as, Tk))}.As the partition of the dataset reduces the numberof observations, the TB model includes a mech-anism to back off to the simpler bigram and uni-gram models.2.3 Concept & Error ModelThe Concept Model takes the action a?u selectedby the DA Model and attaches values and sam-pled interpretation confidences to its concepts.
Inthis work, we adopt a Concept Model which as-signs the corresponding User Goal values for therequired concepts, which makes the user simulatedresponses consistent with the user goal.The Error Model is responsible of simulatingthe noisy communication channel between userand system; as we simulate the error at SLU level,errors consist of incorrect concept values.
We ex-periment with a data-driven model where the pre-cision Prc obtained by a concept c in the refer-ence dataset is used to estimate the frequency withwhich an error in the true value v?
of c will be in-troduced: P (c(v)|c(v?))
= 1?
Prc (Quarteroni etal., 2010).3 Modelling CooperativenessAs in e.g.
(Jung et al, 2009), we define coop-erativeness at the turn level (coopt) as a functionof the number of dialog acts in the DM action assharing concepts with the dialog acts in the useraction au; at the dialog level, coop is the averageof turn-level cooperativeness.We discretize coop into a binary variable reflect-ing high vs low cooperativeness based on whetheror not the dialog cooperativeness exceeds the me-dian value of coop found in a reference corpus; inour ADASearch dataset, the median value foundfor coop is 0.28; hence, we annotate dialogs as co-operative if they exceed such a threshold, and asuncooperative otherwise.
Using a corpus thresh-old allows domain- and population-driven tuningof cooperativeness rather than a ?hard?
definition(as in (Jung et al, 2009)).We then model cooperativeness as two bigrammodels, reflecting the high vs low value of coop.In practice, given a DM action as and the coopvalue (?
= high/low) we obtain a list of user ac-tions and their probabilities:Au(as, ?)
= {(a0u, P (a0u|as, ?
)), .., (aMu , P (aMu |as, ?
))}.2183.1 Combining cooperativeness and contextAt this point, the distribution Au(as, ?)
is lin-early interpolated with the distribution of actionsAu(as, ?)
obtained using the DA model ?
(in theTask-based DA model; ?
can have three values,one for each task as explained in Section 2.2):Au(as) = ??
?Au(as, ?)
+ ??
?Au(as, ?
),where ??
and ??
are the weights of each fea-ture/model and ??
+ ??
= 1.For each user action aiu, ??
and ??
areestimated using the Baum-Welch Expectation-Maximization algorithm as proposed by (Jelinekand Mercer, 1980).
We use the distributions of ac-tions obtained from our dataset and we align theset of actions of the two models.
Since we onlyhave two models, we only need to calculate ex-pectation for one of the distributions:P (?|as, aiu) =P (aiu|as, ?
)P (aiu|as, ?)
+ P (aiu|as, ?
)?Mi=0aiuwhere M is the number of actions.
Then, theweights ??
and ??
that maximize the data like-lihood are calculated as follows:??
=?Mj=0 P (?|as, aju)M;??
= 1?
?
?.The resulting combined distribution Au(as) isobtained by factoring the probabilities of each ac-tion with the weight estimated for the particulardistribution:Au(as) = {(a0u, ??
?P (a0u|as, ?
)), .., (aMu , ??
?P (aMu |as, ?
)),(a0u, ??
?
P (a0u|as, ?
)), .., (aMu , ??
?
P (aMu |as, ?
))}3.2 Effects of cooperativenessTo assess the effect of the cooperativeness featurein the final distribution of actions, we set a 5-foldcross-validation experiment with the ADASearchdataset where we average the ??
estimated at eachturn of the dialog.
We investigated in which con-text cooperativeness provides more contributionby comparing the ??
weights attributed by highvs.
low coop models to user action distributions inresponse to Dialog Manager actions.Figure 1 shows the values achieved by ??
forseveral DM actions for high vs low coop regimes.We can see that ??
achieves high values in caseof uncooperative users in response to DM dialogacts as [ClarificationRequest] and [Info-request].In contrast, forward-looking actions, such as theones including [Offer], seem to discard the con-tribution of the low coop model, but to favor thecontribution provided by high coop.!
"!#$"!#%"!#&"'()*+,-*./012345" '-*./012345" '67189/34:3;<5" '=/33<,>?3/5" '>?3/5" '6718/@,>?3/5"A0+A" *8B"Figure 1: Estimated ??
weights in response to se-lected DM actions in case of high/low coop4 ExperimentsWe evaluate our simulator models using two meth-ods: first, ?offline?
statistics are used to assesshow realistic the action estimations by DA Modelsare with respect to a dataset of real conversations(Sec.
4.1); then, ?online?
statistics (Sec.
4.2) eval-uate end-to-end simulator performance in terms ofdialog act distributions, error robustness and taskduration and completion rates by comparing realdialogs with fresh simulated dialogs using actionsampling in the different simulation models.4.1 ?Offline?
statisticsIn order to compare simulated and real user ac-tions, we evaluate dialog act Precision (PDA)and Recall (RDA) following the methodology in(Schatzmann et al, 2005).For each DM action as the simulator picks auser action a?u from Au(as) and we compare itwith the real user choice a?u.
A simulated dialogact is correct when it appears in the real actiona?u.
The measurements were obtained using 5-foldcross-validation on the ADASearch dataset.Table 1: Dialog Act Precision and RecallSimulation (a?u) Most frequent (a?u)DA Model PDA RDA PDA RDAOB 33.8 33.4 33.9 33.5BI (+coop) 35.6 (35.7) 35.5 (35.8) 49.3 (47.9) 48.8 (47.4)TB (+coop) 38.2 (39.7) 38.1 (39.4) 51.1 (50.6) 50.6 (50.2)Table 1 shows PDA/RDA obtained for the OB,BI and TB models alone and with cooperative-ness models (+coop).
First, we see that TB ismuch better than BI and OB at reproducing realaction selection.
This is also visible in both PDAand RDA obtained by selecting a?u, the most fre-quent user action from the As generated by eachmodel.
By definition, a?u maximizes the expectedPDA and RDA, providing an upper bound for ourmodels; however, to reproduce any possible userbehavior, we need to sample au rather than choos-ing it by frequency.
By now inspecting (+coop)219values in Table 1, we see that explicit cooperative-ness models match real dialogs more closely.
Itpoints out that partitioning the reference dataset inhigh vs low coop sets allows better data represen-tation.
There is however no improvement in thea?u case: we explain this by the fact that by ?slic-ing?
the reference dataset, the cooperative modelaugments data sparsity, affecting robustness.4.2 ?Online?
statisticsWe now discuss online deployment of our sim-ulation models with different user behaviors and?fresh?
user goals and data.
To align with theADASearch dataset, we ran 60 simulated dialogsbetween the ADASearch DM and each combina-tion of the Task-based and Bigram models andhigh and low values of coop.
For each set of simu-lated dialogs, we measured task duration, definedas the average number of turns needed to completeeach task, and task completion rate, defined as:TCR = number of times a task has been completedtotal number of task requests .Table 2 reports such figures in comparisonto the ones obtained for real dialogs from theADASearch dataset.
In general, we see that taskduration is closer to real dialogs in the Bigram andTask-based models when compared to the Obedi-ent model.
Moreover, it can easily be observedin both BI and TB models that under high-coopregime (in boldface), the number of turns takento complete tasks is lower than under low-coop.Furthermore, in both TB and BI models, TCRis higher when cooperativeness is higher, indicat-ing that cooperative users make dialogs not onlyshorter but also more efficient.Table 2: Task duration and TCR in simulated di-alogs with different regimes vs real dialogs.Lodging Enquiry Lodging Reserv Event Enquiry AllModel #turns TCR #turns TCR #turns TCR TCROB 9.2?0.0 78.1 9.7?1.4 82.4 8.1?2.9 66.7 76.6BI+low 15.1?4.1 71.4 14.2?3.9 69.4 9.3?1.8 52.2 66.7BI+high 12.1?2.5 74.6 12.9?3.1 82.9 7.8?1.8 75.0 77.4TB+low 13.6?4.1 75.8 13.4?3.7 83.3 8.4?3.3 64.7 77.2TB+high 11.6?2.8 80.0 12.6?3.6 83.7 6.5?1.9 57.1 78.4Real dialogs 11.1?3.0 71.4 12.7?4.7 69.6 9.3?4.0 85.0 73.45 ConclusionIn this work, we address data-driven dialog sim-ulation for the training of statistical dialog man-agers.
Our simulator supports a modular combina-tion of user-specific features with different modelsof dialog act and concept-value estimation, in ad-dition to ASR/SLU error simulation.We investigate the effect of joining a model ofuser intentions (Dialog Act Model) with a modelof individual user traits (User Model).
In partic-ular, we represent the user?s cooperativeness asa real-valued feature of the User Model and cre-ate two separate simulator behaviors, reproducinghigh and low cooperativeness.
We explore the im-pact of combining our cooperativeness model withthe Dialog Act model in terms of dialog act accu-racy and task success.We find that 1) an explicit modelling of usercooperativeness contributes to an improved accu-racy of dialog act estimation when compared toreal conversations; 2) simulated dialogs with highcooperativeness result in higher task completionrates than low-cooperativeness dialogs.
In futurework, we will study yet more fine-grained and re-alistic User Model features.ReferencesW.
Eckert, E. Levin, and R. Pieraccini.
1997.
Usermodeling for spoken dialogue system evaluation.
InProc.
IEEE ASRU.F.
Jelinek and R. L. Mercer.
1980.
Interpolated estima-tion of Markov source parameters from sparse data.In Workshop on Pattern Recognition in Practice.S.
Jung, C. Lee, K. Kim, and G. G. Lee.
2009.
Hy-brid approach to user intention modeling for dialogsimulation.
In Proc.
ACL-IJCNLP.O.
Pietquin.
2004.
A Framework for UnsupervisedLearning of Dialogue Strategies.
Ph.D. thesis, Fac-ulte?
Polytechnique de Mons, TCTS Lab (Belgique).S.
Quarteroni, M. Gonza?lez, G. Riccardi, andS.
Varges.
2010.
Combining user intention and errormodeling for statistical dialog simulators.
In Proc.INTERSPEECH.J.
Schatzmann, K. Georgila, and S. Young.
2005.Quantitative evaluation of user simulation tech-niques for spoken dialogue systems.
In Proc.
SIG-DIAL.J.
Schatzmann, K. Weilhammer, M. Stuttle, andS.
Young.
2006.
A survey of statistical user sim-ulation techniques for reinforcement-learning of di-alogue management strategies.
Knowl.
Eng.
Rev.,21(2):97?126.S.
Varges, S. Quarteroni, G. Riccardi, A. V. Ivanov, andP.
Roberti.
2009.
Leveraging POMDPs trained withuser simulations and rule-based dialogue manage-ment in a spoken dialogue system.
In Proc.
SIG-DIAL.220
