Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 78?83,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDomain-Independent Shallow Sentence OrderingThade NahnsenSchool of InformaticsUniversity of EdinburghT.Nahnsen@sms.ed.ac.ukAbstractWe present a shallow approach to the sentenceordering problem.
The employed features arebased on discourse entities, shallow syntac-tic analysis, and temporal precedence relationsretrieved from VerbOcean.
We show that theserelatively simple features perform well in amachine learning algorithm on datasets con-taining sequences of events, and that the re-sulting models achieve optimal performancewith small amounts of training data.
Themodel does not yet perform well on datasetsdescribing the consequences of events, such asthe destructions after an earthquake.1 IntroductionSentence ordering is a problem in many natural lan-guage processing tasks.
While it has, historically,mainly been considered a challenging problem in(concept-to-text) language generation tasks, morerecently, the issue has also generated interest withinsummarization research (Barzilay, 2003; Ji and Pul-man, 2006).
In the spirit of the latter, this paperinvestigates the following questions: (1) Does thetopic of the text influence the factors that are im-portant to sentence ordering?
(2) Which factors aremost important for determining coherent sentenceorderings?
(3) How much performance is gainedwhen using deeper knowledge resources?Past research has investigated a wide range of as-pects pertaining to the ordering of sentences in text.The most prominent approaches include: (1) tem-poral ordering in terms of publication date (Barzi-lay, 2003), (2) temporal ordering in terms of textualcues in sentences (Bollegala et al, 2006), (3) thetopic of the sentences (Barzilay, 2003), (4) coher-ence theories (Barzilay and Lapata, 2008), e.g., Cen-tering Theory, (5) content models (Barzilay and Lee,2004), and (6) ordering(s) in the underlying docu-ments in the case of summarisation (Bollegala et al,2006; Barzilay, 2003).2 The ModelWe view coherence assessment, which we recast asa sentence ordering problem, as a machine learningproblem using the feature representation discussedin Section 2.1.
It can be viewed as a ranking task be-cause a text can only be more or less coherent thansome other text.
The sentence ordering task usedin this paper can easily be transformed into a rank-ing problem.
Hence, paralleling Barzilay and Lapata(2008), our model has the following structure.The data consists of alternative orderings(xij , xik) of the sentences of the same document di.In the training data, the preference ranking of thealternative orderings is known.
As a result, trainingconsists of determining a parameter vector w thatminimizes the number of violations of pairwiserankings in the training set, a problem whichcan be solved using SVM constraint optimization(Joachims, 2002).
The following section exploresthe features available for this optimization.2.1 FeaturesApproaches to sentence ordering can generally becategorized as knowledge-rich or knowledge-lean.Knowledge-rich approaches rely on manually cre-ated representations of sentence orderings using do-78main communication knowledge.Barzilay and Lee (2004)?s knowledge-lean ap-proach attempts to automate the inference ofknowledge-rich information using a distributionalview of content.
In essence, they infer a number oftopics using clustering.
The clusters are representedby corresponding states in a hidden Markov model,which is used to model the transitions between top-ics.Lapata (2003), in contrast, does not attempt tomodel topics explicitly.
Instead, she reduces sen-tence ordering to the task of predicting the next sen-tence given the previous sentence, which representsa coarse attempt at capturing local coherence con-straints.
The features she uses are derived from threecategories - verbs, nouns, and dependencies - all ofwhich are lexicalised.
Her system thereby, to someextent, learns a precedence between the words in thesentences, which in turn represent topics.Ji and Pulman (2006) base their ordering strategynot only on the directly preceding sentence, but onall preceding sentences.
In this way, they are able toavoid a possible topic bias when summarizing mul-tiple documents.
This is specific to their approach asboth Lapata (2003)?s and Barzilay and Lee (2004)?sapproaches are not tailored to summarization andtherefore do not experience the topic bias problem.The present paper deviates from Lapata (2003)insofar as we do not attempt to learn the orderingpreferences between pairs of sentences.
Instead, welearn the ranking of documents.
The advantage ofthis approach is that it allows us to straightforwardlydiscern the individual value of various features (cf.Barzilay and Lapata (2008)).The methods used in this paper are mostly shallowwith the exception of two aspects.
First, some of themeasures make use of WordNet relations (Fellbaum,1998), and second, some use the temporal orderingprovided by the ?happens-before?
relation in VerbO-cean (Chklovski and Pantel, 2004).
While the use ofWordNet is self-explanatory, its effect on sentenceordering algorithms does not seem to have been ex-plored in any depth.
The use of VerbOcean is meantto reveal the degree to which common sense order-ings of events affect the ordering of sentences, orwhether the order is reversed.With this background, the sentence ordering fea-tures used in this paper can be grouped into threecategories:2.1.1 Group SimilarityThe features in this category are inspired by dis-course entity-based accounts of local coherence.Yet, in contrast to Barzilay and Lapata (2008), whoemploy the syntactic properties of the respective oc-currences, we reduce the accounts to whether or notthe entities occur in subsequent sentences (similarto Karamanis (2004)?s NOCB metric).
We also in-vestigate whether using only the information fromthe head of the noun group (cf.
Barzilay and Lapata(2008)) suffices, or whether performance is gainedwhen allowing the whole noun group in order to de-termine similarity.
Moreover, as indicated above,some of the noun group measures make use of Word-Net synonym, hypernym, hyponym, antonym rela-tionships.
For completeness, we also consider theeffects of using verb groups and whole sentences assyntactic units of choice.2.1.2 Temporal OrderingThis set of features uses information on the tem-poral ordering of sentences, although it currentlyonly includes the ?happens-before?
relations in Ver-bOcean.2.1.3 Longer Range RelationsThe group similarity features only capture the re-lation between a sentence and its immediate suc-cessor.
However, the coherence of a text is clearlynot only defined by direct relations, but also re-quires longer range relations between sentences(e.g., Barzilay and Lapata (2008)).
The features inthis section explore the impact of such relations onthe coherence of the overall document as well as theappropriate way of modeling them.3 ExperimentsThis section introduces the datasets used for the ex-periments, describes the experiments, and discussesour main findings.3.1 Evaluation DatasetsThe three datasets used for the automatic evaluationin this paper are based on human-generated texts(Table 1).
The first two are the earthquake and acci-dent datasets used by Barzilay and Lapata (2008).79Each of these sets consists of 100 datasets in thetraining and test sets, respectively, as well as 20 ran-dom permutations for each text.The third dataset is similar to the first two in thatit contains original texts and random permutations.In contrast to the other two sources, however, thisdataset is based on the human summaries from DUC2005 (Dang, 2005).
It comprises 300 human sum-maries on 50 document sets, resulting in a total of6,000 pairwise rankings split into training and testsets.
The source furthermore differs from Barzilayand Lapata (2008)?s datasets in that the content ofeach text is not based on one individual event (anearthquake or accident), but on more complex top-ics followed over a period of time (e.g., the espi-onage case between GM and VW along with thevarious actions taken to resolve it).
Since the differ-ent document sets cover completely different topicsthe third dataset will mainly be used to evaluate thetopic-independent properties of our model.Dataset Training TestingEarthquakes 1,896 2,056Accidents 2,095 2,087DUC2005 up to 3,300 2,700Table 1: Number of pairwise rankings in the training andtest sets for the three datasets3.2 Experiment 1In the first part of this experiment, we consider theproblem of the granularity of the syntactic units to beused.
That is, does it make a difference whether weuse the words in the sentence, the words in the noungroups, the words in the verb groups, or the wordsin the respective heads of the groups to determinecoherence?
(The units are obtained by processingthe documents using the LT-TTT2 tools (Grover andTobin, 2006); the lemmatizer used by LT-TTT2 ismorpha (Minnen and Pearce, 2000).)
We also con-sider whether lemmatization is beneficial in each ofthe granularities.The results - presented in Table 2 - indicate thatconsidering only the heads of the verb and noungroups separately provides the best performance.
Inparticular, the heads outperform the whole groups,and the heads separately also outperform noun andverb group heads together.
As for the questionof whether lemmatization provides better results,one needs to distinguish the case of noun and verbgroups.
For noun groups, lemmatization improvesperformance, which can mostly be attributed to sin-gular and plural forms.
In the case of verb groups,however, the lemmatized version yields worse re-sults than the surface forms, a fact mainly explainedby the tense and modality properties of verbs.Syntactic Unit Processing AccuracyAcc Earthsentence surface form 52.27 14.21lemma 52.27 12.04heads sentence surface form 77.35 60.30lemma 73.18 61.67noun group surface form 80.14 59.84lemma 81.58 59.54head NG surface form 80.49 59.75lemma 81.65 59.12verb group surface form 71.57 68.14lemma 53.40 68.01head VG surface form 71.15 68.39lemma 53.76 67.85Table 2: Performance with respect to the syntactic unitof processing of the training datasets.
Accuracy is thefraction of correctly ranked pairs of documents over thetotal number of pairs.
(?Heads sentence?
is the heads ofNGs and VGs.
)Given the appropriate unit of granularity, we canconsider the impact of semantic relations betweensurface realizations on coherence.
For these exper-iments we use the synonym, hypernym, hyponym,and antonym relations in WordNet.
The rationalefor the consideration of semantic relations lies in thefact that the frequent use of the same words is usu-ally deemed bad writing style.
One therefore tendsto observe the use of semantically similar terms inneighboring sentences.
The results of using seman-tic relations for coherence rating are provided in Ta-ble 3.
Synonym detection improves performance,while the other units provide poorer performance.This suggests that the hypernym and hyponym rela-tions tend to over-generalize in the semantics.The third category of features investigated is thetemporal ordering of sentences; we use VerbO-cean to obtain the temporal precedence between twoevents.
One would expect events to be described ei-80Syntactic Unit Processing AccuracyAcc Earthhead NGsynonyms 82.37 59.40hypernyms 76.98 61.02hyponyms 81.59 59.14antonyms 74.20 48.07combines 70.84 56.51head VGsynonyms 54.19 70.80hypernyms 53.36 60.54hyponyms 55.27 68.32antonyms 47.45 63.91combines 49.73 66.77Table 3: The impact of WordNet on sentence orderingaccuracyTemporal Ordering AccuracyAcc EarthPrecedence Ordering 60.41 47.09Reverse Ordering 39.59 52.61Precedence w/ matching NG 62.65 57.52Reverse w/ matching NG 37.35 42.48Table 4: The impact of the VerbOcean ?happens-before?temporal precedence relation on accuracy on the trainingdatasetsther in chronological order or in its reverse.
Whilethe former ordering represents a factual account ofsome sequence of events, the latter corresponds tonewswire-style texts, which present the most impor-tant event(s) first, even though they may derive fromprevious events.Table 4 provides the results of the experimentswith temporal orderings.
The first two rows vali-date the ordering of the events, while the latter tworequire the corresponding sentences to have a noungroup in common in order to increase the likeli-hood that two events are related.
The results clearlyshow that there is potential in the direct orderingof events.
This suggests that sentence ordering canto some degree be achieved using simple temporalprecedence orderings in a domain-independent way.This holds despite the results indicating that the fea-tures work better for sequences of events (as in theaccident dataset) as opposed to accounts of the re-sults of some event(s) (as in the earthquake dataset).Range AccuracyAcc Earth2 occ.
in 2 sent.
80.57 50.112 occ.
in 3 sent.
73.17 45.433 occ.
in 3 sent.
71.35 52.812 occ.
in 4 sent.
66.95 50.413 occ.
in 4 sent.
69.38 41.614 occ.
in 4 sent.
71.93 58.972 occ.
in 5 sent.
61.48 66.253 occ.
in 5 sent.
68.59 42.334 occ.
in 5 sent.
65.77 40.755 occ.
in 5 sent.
81.39 62.40sim.
w/ sent.
1 sent.
away 83.39 71.94sim.
w/ sent.
2 sent.
away 60.44 67.52sim.
w/ sent.
3 sent.
away 52.28 54.65sim.
w/ sent.
4 sent.
away 49.65 44.50sim.
w/ sent.
5 sent.
away 43.68 52.11Table 5: Effect of longer range relations on coherenceaccuracyThe final category of features investigates the de-gree to which relations between sentences other thandirectly subsequent sentences are relevant.
To thisend, we explore two different approaches.
The firstset of features considers the distribution of entitieswithin a fixed set of sentences, and captures in howmany different sentences the entities occur.
The re-sulting score is the number of times the entities oc-cur in N out of M sentences.
The second set onlyconsiders the similarity score from the current sen-tence and the other sentences within a certain rangefrom the current sentence.
The score of this fea-ture is the sum of the individual similarities.
Table 5clearly confirms that longer range relations are rele-vant to the assessment of the coherence of text.
Aninteresting difference between the two approaches isthat sentence similarity only provides good resultsfor neighboring sentences or sentences only one sen-tence apart, while the occurrence-counting methodalso works well over longer ranges.Having evaluated the potential contributions ofthe individual features and their modeling, we nowuse SVMs to combine the features into one com-prehensive measure.
Given the indications from theforegoing experiments, the results in Table 6 are dis-appointing.
In particular, the performance on the81Combination AccuracyAcc EarthChunk+Temp+WN+LongRange+ 83.11 54.88Chunk+Temp+WN+LongRange- 77.67 62.76Chunk+Temp+WN-LongRange+ 74.17 59.28Chunk+Temp+WN-LongRange- 68.15 63.55Chunk+Temp-WN+LongRange+ 86.88 63.83Chunk+Temp-WN+LongRange- 80.19 59.43Chunk+Temp-WN-LongRange+ 76.63 60.86Chunk+Temp-WN-LongRange- 64.43 60.94NG Similarity w/ Synonyms 85.90 63.55Coreference+Syntax+Salience+ 90.4 87.2Coreference-Syntax+Salience+ 89.9 83.0HMM-based Content Models 75.8 88.0Latent Semantic Analysis 87.3 81.0Table 6: Comparison of the developed model with otherstate-of-the-art systems.
Coreference+Syntax+Salience+and Coreference?Syntax+Salience+ are the Barzilay andLapata (2008) model, HMM-based Content Models is theBarzilay and Lee (2004) paper and Latent Semantic Anal-ysis is the Barzilay and Lapata (2008) implementation ofPeter W. Foltz and Landauer (1998).
The results of thesesystems are reproduced from Barzilay and Lapata (2008).
(Temp = Temporal; WN = WordNet)earthquake dataset is below standard.
However, itseems that sentence ordering in that set is primarilydefined by topics, as only content models performwell.
(Barzilay and Lapata (2008) only perform wellwhen using their coreference module, which de-termines antecedents based on the identified coref-erences in the original sentence ordering, therebybiasing their orderings towards the correct order-ing.)
Longer range and WordNet relations together(Chunk+Temp-WN+LongRange+) achieve the bestperformance.
The corresponding configuration isalso the only one that achieves reasonable perfor-mance when compared with other systems.4 Experiment 2As stated, the ultimate goal of the models presentedin this paper is the application of sentence orderingto automatically generated summaries.
It is, in thisregard, important to distinguish coherence as studiedin Experiment 1 and coherence in the context of au-tomatic summarization.
Namely, for newswire sum-marization systems, the topics of the documents areCoreference+Syntax+Salience+Test Earthquakes AccidentsTrainEarthquakes 87.3 67.0Accidents 69.7 90.4HMM-based Content ModelsTest Earthquakes AccidentsTrainEarthquakes 88.0 31.7Accidents 60.3 75.8Chunk+Temporal-WordNet+LongRange+Test Earthquakes AccidentsTrainEarthquakes 63.83 86.63Accidents 64.19 86.88Table 7: Cross-Training between Accident andEarthquake datasets.
The results for Corefer-ence+Syntax+Salience+ and HMM-Based ContentModels are reproduced from Barzilay and Lapata (2008).unknown at the time of training.
As a result, modelperformance on out-of-domain texts is important forsummarization.
Experiment 2 seeks to evaluate howwell our model performs in such cases.
To thisend, we carry out two sets of tests.
First, we cross-train the models between the accident and earth-quake datasets to determine system performance inunseen domains.
Second, we use the dataset basedon the DUC 2005 model summaries to investigatewhether our model?s performance on unseen topicsreaches a plateau after training on a particular num-ber of different topics.Surprisingly, the results are rather good, whencompared to the poor results in part of the previ-ous experiment (Table 7).
In fact, model perfor-mance is nearly independent of the training topic.Nevertheless, the results on the earthquake test setindicate that our model is missing essential compo-nents for the correct prediction of sentence order-ings on this set.
When compared to the results ob-tained by Barzilay and Lapata (2008) and Barzilayand Lee (2004), it would appear that direct sentence-to-sentence similarity (as suggested by the Barzilayand Lapata baseline score) or capturing topic se-quences are essential for acquiring the correct se-quence of sentences in the earthquake dataset.The final experimental setup applies the best82Different Topics Training Pairs Accuracy2 160 55.174 420 63.546 680 65.208 840 65.5710 1,100 64.8015 1,500 64.9320 2,100 64.8725 2,700 64.9430 3,300 65.61Table 8: Accuracy on 20 test topics (2,700 pairs) withrespect to the number of topics used for training usingthe model Chunk+Temporal-WordNet+LongRange+model (Chunk+Temporal-WordNet+LongRange+)to the summarization dataset and evaluates how wellthe model generalises as the number of topics in thetraining dataset increases.
The results - provided inTable 8 - indicate that very little training data (bothregarding the number of pairs and the number of dif-ferent topics) is needed.
Unfortunately, they alsosuggest that the DUC summaries are more similarto the earthquake than to the accident dataset.5 ConclusionsThis paper investigated the effect of different fea-tures on sentence ordering.
While a set of featureshas been identified that works well individually aswell as in combination on the accident dataset, theresults on the earthquake and DUC 2005 datasets aredisappointing.
Taking into account the performanceof content models and the baseline of the Barzilayand Lapata (2008) model, the most convincing ex-planation is that the sentence ordering in the earth-quake datasets is based on some sort of topic notion,providing a variety of possible antecedents betweenwhich our model is thus far unable to distinguishwithout resorting to the original (correct) ordering.Future work will have to concentrate on this aspectof sentence ordering, as it appears to coincide withthe structure of the summaries for the DUC 2005dataset.ReferencesBarzilay, R. (2003).
Information fusion for multi-document summarization: paraphrasing and gen-eration.
Ph.
D. thesis, Columbia University.Barzilay, R. and M. Lapata (2008).
Modeling localcoherence: An entity-based approach.
Comput.Linguist.
34, 1?34.Barzilay, R. and L. Lee (2004).
Catching the drift:probabilistic content models, with applications togeneration and summarization.
In Proceedings ofHLT-NAACL 2004.Bollegala, D., N. Okazaki, and M. Ishizuka (2006).A bottom-up approach to sentence ordering formulti-document summarization.
In Proceedingsof ACL-44.Chklovski, T. and P. Pantel (2004).
Verbocean: Min-ing the web for fine-grained semantic verb rela-tions.
In Proceedings of EMNLP 2004.Dang, H. (2005).
Overview of duc 2005.Fellbaum, C.
(Ed.)
(1998).
WordNet An ElectronicLexical Database.
The MIT Press.Grover, C. and R. Tobin (2006).
Rule-based chunk-ing and reusability.
In Proceedings of LREC 2006.Ji, P. D. and S. Pulman (2006).
Sentence order-ing with manifold-based classification in multi-document summarization.
In Proceedings ofEMNLP 2006.Joachims, T. (2002).
Evaluating retrieval perfor-mance using clickthrough data.
In Proceedingsof the SIGIR Workshop on Mathematical/FormalMethods in Information Retrieval.Karamanis, N. (2004).
Evaluating centering for sen-tence ordering in two new domains.
In Proceed-ings of the NAACL 2004.Lapata, M. (2003).
Probabilistic text structuring:Experiments with sentence ordering.
In Proc.
ofACL 2003.Minnen, G., C. J. and D. Pearce (2000).
Robust, ap-plied morphological generation.
In Proceedingsof the 1st International Natural Language Gener-ation Conference.Peter W. Foltz, W. K. and T. K. Landauer (1998).Textual coherence using latent semantic analysis.Discourse Processes 25, 285?307.83
