Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33?40,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Phrase-based Statistical Model for SMS Text NormalizationAiTi Aw, Min Zhang, Juan Xiao, Jian SuInstitute of Infocomm Research21 Heng Mui Keng TerraceSingapore 119613{aaiti,mzhang,stuxj,sujian}@i2r.a-star.edu.sgAbstractShort Messaging Service (SMS) texts be-have quite differently from normal writtentexts and have some very special phenom-ena.
To translate SMS texts, traditionalapproaches model such irregularities di-rectly in Machine Translation (MT).
How-ever, such approaches suffer fromcustomization problem as tremendous ef-fort is required to adapt the languagemodel of the existing translation system tohandle SMS text style.
We offer an alter-native approach to resolve such irregulari-ties by normalizing SMS texts before MT.In this paper, we view the task of SMSnormalization as a translation problemfrom the SMS language to the Englishlanguage 1  and we propose to adapt aphrase-based statistical MT model for thetask.
Evaluation by 5-fold cross validationon a parallel SMS normalized corpus of5000 sentences shows that our method canachieve 0.80702 in BLEU score againstthe baseline BLEU score 0.6958.
Anotherexperiment of translating SMS texts fromEnglish to Chinese on a separate SMS textcorpus shows that, using SMS normaliza-tion as MT preprocessing can largelyboost SMS translation performance from0.1926 to 0.3770 in BLEU score.1 MotivationSMS translation is a mobile Machine Translation(MT) application that translates a message fromone language to another.
Though there existsmany commercial MT systems, direct use ofsuch systems fails to work well due to the specialphenomena in SMS texts, e.g.
the unique relaxedand creative writing style and the frequent use ofunconventional and not yet standardized short-forms.
Direct modeling of these special phenom-ena in MT requires tremendous effort.
Alterna-tively, we can normalize SMS texts into1 This paper only discusses English SMS text normalization.grammatical texts before MT.
In this way, thetraditional MT is treated as a ?black-box?
withlittle or minimal adaptation.
One advantage ofthis pre-translation normalization is that the di-versity in different user groups and domains canbe modeled separately without accessing andadapting the language model of the MT systemfor each SMS application.
Another advantage isthat the normalization module can be easily util-ized by other applications, such as SMS tovoicemail and SMS-based information query.In this paper, we present a phrase-based statis-tical model for SMS text normalization.
Thenormalization is visualized as a translation prob-lem where messages in the SMS language are tobe translated to normal English using a similarphrase-based statistical MT method (Koehn et al,2003).
We use IBM?s BLEU score (Papineni etal., 2002) to measure the performance of SMStext normalization.
BLEU score computes thesimilarity between two sentences using n-gramstatistics, which is widely-used in MT evalua-tion.
A set of parallel SMS messages, consistingof 5000 raw (un-normalized) SMS messages andtheir manually normalized references, is con-structed for training and testing.
Evaluation by 5-fold cross validation on this corpus shows thatour method can achieve accuracy of 0.80702 inBLEU score compared to the baseline system of0.6985.
We also study the impact of our SMStext normalization on the task of SMS transla-tion.
The experiment of translating SMS textsfrom English to Chinese on a corpus comprising402 SMS texts shows that, SMS normalization asa preprocessing step of MT can boost the transla-tion performance from 0.1926 to 0.3770 inBLEU score.The rest of the paper is organized as follows.Section 2 reviews the related work.
Section 3summarizes the characteristics of English SMStexts.
Section 4 discusses our method and Sec-tion 5 reports our experiments.
Section 6 con-cludes the paper.2 Related WorkThere is little work reported on SMS normaliza-tion and translation.
Bangalore et al (2002) used33a consensus translation technique to bootstrapparallel data using off-the-shelf translation sys-tems for training a hierarchical statistical transla-tion model for general domain instant messagingused in Internet chat rooms.
Their method dealswith the special phenomena of the instant mes-saging language (rather than the SMS language)in each individual MT system.
Clark (2003)proposed to unify the process of tokenization,segmentation and spelling correction for nor-malization of general noisy text (rather than SMSor instant messaging texts) based on a noisychannel model at the character level.
However,results of the normalization are not reported.
Awet al (2005) gave a brief description on their in-put pre-processing work for an English-to-Chinese SMS translation system using a word-group model.
In addition, in most of the com-mercial SMS translation applications 2 , SMSlingo (i.e., SMS short form) dictionary is pro-vided to replace SMS short-forms with normalEnglish words.
Most of the systems do not han-dle OOV (out-of-vocabulary) items and ambigu-ous inputs.
Following compares SMS textnormalization with other similar or related appli-cations.2.1 SMS Normalization versus GeneralText NormalizationGeneral text normalization deals with Non-Standard Words (NSWs) and has been well-studied in text-to-speech (Sproat et al, 2001)while SMS normalization deals with Non-Words(NSs) or lingoes and has seldom been studiedbefore.
NSWs, such as digit sequences, acronyms,mixed case words (WinNT, SunOS), abbrevia-tions and so on, are grammatically correct in lin-guistics.
However lingoes, such as ?b4?
(before)and ?bf?
(boyfriend), which are usually self-created and only accepted by young SMS users,are not yet formalized in linguistics.
Therefore,the special phenomena in SMS texts impose abig challenge to SMS normalization.2.2 SMS Normalization versus SpellingCorrection ProblemIntuitively, many would regard SMS normaliza-tion as a spelling correction problem where thelingoes are erroneous words or non-words to bereplaced by English words.
Researches on spell-ing correction centralize on typographic andcognitive/orthographic errors (Kukich, 1992) anduse approaches (M.D.
Kernighan, Church and2 http://www.etranslator.ro and http://www.transl8bit.comGale, 1991) that mostly model the edit operationsusing distance measures (Damerau 1964; Leven-shtein 1966), specific word set confusions (Gold-ing and Roth, 1999) and pronunciation modeling(Brill and Moore, 2000; Toutanova and Moore,2002).
These models are mostly character-basedor string-based without considering the context.In addition, the author might not be aware of theerrors in the word introduced during the edit op-erations, as most errors are due to mistype ofcharacters near to each other on the keyboard orhomophones, such as ?poor?
or ?pour?.In SMS, errors are not isolated within wordand are usually not surrounded by clean context.Words are altered deliberately to reflect sender?sdistinct creation and idiosyncrasies.
A charactercan be deleted on purpose, such as ?wat?
(what)and ?hv?
(have).
It also consists of short-formssuch as ?b4?
(before), ?bf?
(boyfriend).
In addi-tion, normalizing SMS text might require thecontext to be spanned over more than one lexicalunit such as ?lemme?
(let me), ?ur?
(you are) etc.Therefore, the models used in spelling correctionare inadequate for providing a complete solutionfor SMS normalization.2.3 SMS Normalization versus Text Para-phrasing ProblemOthers may regard SMS normalization as a para-phrasing problem.
Broadly speaking, paraphrasescapture core aspects of variability in language,by representing equivalencies between differentexpressions that correspond to the same meaning.In most of the recent works (Barzilay andMcKeown, 2001; Shimohata, 2002), they areacquired (semi-) automatically from large com-parable or parallel corpora using lexical andmorpho-syntactic information.Text paraphrasing works on clean texts inwhich contextual and lexical-syntactic featurescan be extracted and used to find ?approximateconceptual equivalence?.
In SMS normalization,we are dealing with non-words and ?ungram-matically?
sentences with the purpose to normal-ize or standardize these words and form bettersentences.
The SMS normalization problem isthus different from text paraphrasing.
On theother hand, it bears some similarities with MT aswe are trying to ?convert?
text from one lan-guage to another.
However, it is a simpler prob-lem as most of the time; we can find the sameword in both the source and target text, makingalignment easier.343 Characteristics of English SMSOur corpus consists of 55,000 messages collectedfrom two sources, a SMS chat room and corre-spondences between university students.
Thecontent is mostly related to football matches,making friends and casual conversations on?how, what and where about?.
We summarizethe text behaviors into two categories as below.3.1 Orthographic VariationThe most significant orthographic variant inSMS texts is in the use of non-standard, self-created short-forms.
Usually, sender takes advan-tage of phonetic spellings, initial letters or num-ber homophones to mimic spoken conversationor shorten words or phrases (hw vs. homework orhow, b4 vs. before, cu vs. see you, 2u vs. to you,oic vs. oh I see, etc.)
in the attempt to minimizekey strokes.
In addition, senders create a newform of written representation to express theiroral utterances.
Emotions, such as ?:(?
symboliz-ing  sad, ?:)?
symbolizing smiling, ?:()?
symbol-izing shocked, are representations of bodylanguage.
Verbal effects such as ?hehe?
forlaughter and emphatic discourse particles such as?lor?, ?lah?, ?meh?
for colloquial English areprevalent in the text collection.The loss of ?alpha-case?
information posts an-other challenge in lexical disambiguation andintroduces difficulty in identifying sentenceboundaries, proper nouns, and acronyms.
Withthe flexible use of punctuation or not using punc-tuation at all, translation of SMS messages with-out prior processing is even more difficult.3.2 Grammar VariationSMS messages are short, concise and conveymuch information within the limited space quota(160 letters for English), thus they tend to be im-plicit and influenced by pragmatic and situationreasons.
These inadequacies of language expres-sion such as deletion of articles and subject pro-noun, as well as problems in number agreementsor tenses make SMS normalization more chal-lenging.
Table 1 illustrates some orthographicand grammar variations of SMS texts.3.3 Corpus StatisticsWe investigate the corpus to assess the feasibilityof replacing the lingoes with normal Englishwords and performing limited adjustment to thetext structure.
Similarly to Aw et al (2005), wefocus on the three major cases of transformationas shown in the corpus: (1) replacement of OOVwords and non-standard SMS lingoes; (2) re-moval of slang and (3) insertion of auxiliary orcopula verb and subject pronoun.Phenomena Messages1.
Dropping ???
atthe end ofquestionbtw, wat is ur view(By the way, what is yourview?)2.
Not using anypunctuation atallEh speak english mi malaynot tt good(Eh, speak English!
My Ma-lay is not that good.)3.
Using spell-ing/punctuationfor emphasisgoooooood Sunday morning!!!!!!
(Good Sunday morning!)4.
Using phoneticspellingdat iz enuf(That is enough)5.
Droppingvoweli hv cm to c my luv.
(I have come to see my love.)6.
Introducinglocal flavoryar lor where u go juz now(yes, where did you go justnow?)7.
Dropping verbI hv 2 go.
Dinner w parents.
(I have to go.
Have dinnerwith parents.
)Table 1.
Examples of SMS MessagesTransformation Percentage (%)Insertion 8.09Deletion 5.48Substitution 86.43Table 2.
Distribution of Insertion, Deletion andSubstitution Transformation.Substitution  Deletion Insertionu -> you m are2 ?
to lah amn ?
and t isr ?
are ah youur ?your leh todun ?
don?t 1 doman ?
manches-terhuh ano ?
number one inintro ?
introduce lor yourselfwat ?
what ahh willTable 3.
Top 10 Most Common Substitu-tion, Deletion and InsertionTable 2 shows the statistics of these transfor-mations based on 700 messages randomly se-lected, where 621 (88.71%) messages required35If we include the word ?null?
in the Englishvocabulary, the above model can fully addressthe deletion and substitution transformations, butinadequate to address the insertion transforma-tion.
For example, the lingoes ?duno?, ?ysnite?have to be normalized using an insertion trans-formation to become ?don?t know?
and ?yester-day night?.
Moreover, we also want thenormalization to have better lexical affinity andlinguistic equivalent, thus we extend the modelto allow many words to many words alignment,allowing a sequence of SMS words to be normal-ized to a sequence of contiguous English words.We call this updated model a phrase-based nor-malization model.normalization with a total of 2300 transforma-tions.
Substitution accounts for almost 86% of alltransformations.
Deletion and substitution makeup the rest.
Table 3 shows the top 10 most com-mon transformations.4 SMS NormalizationWe view the SMS language as a variant of Eng-lish language with some derivations in vocabu-lary and grammar.
Therefore, we can treat SMSnormalization as a MT problem where the SMSlanguage is to be translated to normal English.We thus propose to adapt the statistical machinetranslation model (Brown et al, 1993; Zens andNey, 2004) for SMS text normalization.
In thissection, we discuss the three components of ourmethod: modeling, training and decoding forSMS text normalization.4.2 Phrase-based ModelGiven an English sentence e  and SMS sentences , if we assume that e  can be decomposed intophrases with a segmentation T , such thateach phrase e  in  can be corresponded withone phrase s  inKkkes , we have e eand1 1Nk Ke e  ?
?=1 1Mk Ks s s  s= ?
?
.
The channel model can berewritten in equation (3).4.1 Basic Word-based ModelThe SMS normalization model is based on thesource channel model (Shannon, 1948).
Assum-ing that an English sentence e, of length N is?corrupted?
by a noisy channel to produce aSMS message s, of length M, the English sen-tence e, could be recovered through a posterioridistribution for a channel target text given thesource text P s , and a prior distribution forthe channel source text .
( | )e( )P e{ }{ }111 1 11 1 1?
arg max ( | )arg max ( | ) ( )NNN N MeM N Nee P e sP s e P e== i          (1){ }1 1 1 11 1 11 1 11 1 1( | ) ( , | )( | ) ( | , )( | ) ( | )max ( | ) ( | )M N M NTN M NTN K KTN K KTP s e P s T eP T e P s T eP T e P s eP T e P s e===???
?i i i(3)This is the basic function of the channel modelfor the phrase-based SMS normalization model,where we used the maximum approximation forthe sum over all segmentations.
Then we furtherdecompose the probability 1 1( | )K KP s e  using aphrase alignment  as done in the previousword-based model.AAssuming that one SMS word is mapped ex-actly to one English word in the channel modelunder an alignment , we need to con-sider only two types of probabilities: the align-ment probabilities denoted by P m  and thelexicon mapping probabilities denoted by(Brown et al 1993).
The channelmodel can be written as in the following equationwhere m is the position of a word in( | )P s e( |m aP s eA( | )ma)ms and  itsalignment in .mae{ }1 1 1 11 1 11( | ) ( , | )( | ) ( | , )( | ) ( | )mM N M NAN M NAMm m aA mP s e P s A eP A e P s A eP m a P s e===?
??
?????
?ii ??
(2){ }{ }{ }11 1 1 11 1 11111( | ) ( , | )( | ) ( | , )( | ) ( | , )( | ) ( | )kkK K K KAK K KAKakk k akAKk k akAP s e P s A eP A e P s A eP k a P s s eP k a P s e?====?
?= ?
??
??
??
?
??
????
??
?      i   i  i(4)We are now able to model the three transfor-mations through the normalization pair ( , )kk as e    ,36with the mapping probability .
The fol-lowings show the scenarios in which the threetransformations occur.
( | )kk aP s e kk as e<  kk as e=  | ) (k ka P s??
 i( |kP s e ))k ke }11111 1| ))) ( |) (MKkN Kn kP s eP sP s?=?= =????????
?i ii1 )NInsertionDeletionkae  = nullSubstitutionThe statistics in our training corpus shows thatby selecting appropriate phrase segmentation, theposition re-ordering at the phrase level occursrarely.
It is not surprising since most of the Eng-lish words or phrases in normal English text arereplaced with lingoes in SMS messages withoutposition change to make SMS text short and con-cise and to retain the meaning.
Thus we need toconsider only monotone alignment at phraselevel, i.e., k , as in equation (4).
In addition,the word-level reordering within phrase islearned during training.
Now we can further de-rive equation (4) as follows:ka= { }1 111( | ) ( | )( | )kKK KakAKk kkP s e P k eP s e==?
?
???
?
?   (5)?
?The mapping probability is esti-mated via relative frequencies as follows:)k''( ,( | )( ,kk kk ksN s eP s eN s= ?                            (6)Here, denotes the frequency of thenormalization pair .
( , )k kN s e ( , )k ks e Using a bigram language model and assumingBayes decision rule, we finally obtain the follow-ing search criterion for equation (1).
{1111 1 111,?
arg max ( ) (arg max ( |max ( | )arg max ( | | )NNNN N NeNn ne nNk kTn n k ke Te P eP e eP T e eP e e e==??
?????
?i  (7)???
?The alignment process given in equation (8) isdifferent from that of normalization given inequation (7) in that, here we have an aligned in-put sentence pair, s and .
The alignmentprocess is just to find the alignment segmentation,?
,k ks e ks e?
< > =<   ( , )k kP s e between the two sen-tences that maximizes the joint probability.Therefore, in step (2) of the EM algorithm givenat Figure 1, only the joint probabilitiesare involved and updated.??
?For the above equation, we assume the seg-mentation probability ( |P T e to be constant.Finally, the SMS normalization model consists oftwo sub-models: a word-based language model(LM), characterized by 1( | )n nP e e ?
)kand a phrase-based lexical mapping model (channel model),characterized by ( |kP s e)ke )ke   .,?arg ms ek k 1ax ( ,KkkP s=?
 ?
< > 1M1Ne1,k k K=>4.3 Training IssuesFor the phrase-based model training, the sen-tence-aligned SMS corpus needs to be alignedfirst at the phrase level.
The maximum likelihoodapproach, through EM algorithm and Viterbisearch (Dempster et al, 1977) is employed toinfer such an alignment.
Here, we make a rea-sonable assumption on the alignment unit that asingle SMS word can be mapped to a sequenceof contiguous English words, but not vice verse.The EM algorithm for phrase alignment is illus-trated in Figure 1 and is formulated by equation(8).The Expectation-Maximization Algorithm(1) Bootstrap initial alignment using ortho-graphic similarities(2)  Expectation: Update the joint probabili-ties  ( ,kP s(3)  Maximization: Apply the joint probabili-ties to get new alignment usingViterbi search algorithm( ,kP s(4)  Repeat (2) to (3) until alignment con-verges(5) Derive normalization pairs from finalalignmentFigure 1.
Phrase Alignment Using EM Algorithm, 1?
| , )k kM Ns e ke s e?
< > =  (8) 1Since EM may fall into local optimization, inorder to speed up convergence and find a nearlyglobal optimization, a string matching techniqueis exploited at the initialization step to identifythe most probable normalization pairs.
The or-37thographic similarities captured by edit distanceand a SMS lingo dictionary3  which contains thecommonly used short-forms are first used to es-tablish phrase mapping boundary candidates.Heuristics are then exploited to match tokenswithin the pairs of boundary candidates by tryingto combine consecutive tokens within the bound-ary candidates if the numbers of tokens do notagree.Finally, a filtering process is carried out tomanually remove the low-frequency noisyalignment pairs.
Table 4 shows some of the ex-tracted normalization pairs.
As can be seen fromthe table, our algorithm discovers ambiguousmappings automatically that are otherwise miss-ing from most of the lingo dictionary.
( , )s e   log ( | )P s e (2, 2) 0(2, to) -0.579466(2, too) -0.897016(2, null) -2.97058(4, 4) 0(4, for) -0.431364(4, null) -3.27161(w, who are) -0.477121(w, with) -0.764065(w, who) -1.83885(dat, that) -0.726999(dat, date) -0.845098(tmr, tomorrow) -0.341514Table 4.
Examples of normalization pairsGiven the phrase-aligned SMS corpus, thelexical mapping model, characterized by( | )k kP s e  , is easily to be trained using equation(6).
Our n-gram LM 1( | )n nP e e ?
is trained onEnglish Gigaword provided by LDC usingSRILM language modeling toolkit (Stolcke,2002).
Backoff smoothing (Jelinek, 1991) is usedto adjust and assign a non-zero probability to theunseen words to address data sparseness.4.4 Monotone SearchGiven an input , the search, characterized inequation (7), is to find a sentence e that maxi-smizes  using the normalizationmodel.
In this paper, the maximization problemin equation (7) is solved using a monotone search,implemented as a Viterbi search through dy-namic programming.
( | ) ( )P s e P ei5 ExperimentsThe aim of our experiment is to verify the effec-tiveness of the proposed statistical model forSMS normalization and the impact of SMS nor-malization on MT.A set of 5000 parallel SMS messages, whichconsists of raw (un-normalized) SMS messagesand reference messages manually prepared bytwo project members with inter-normalizationagreement checked, was prepared for trainingand testing.
For evaluation, we use IBM?s BLEUscore (Papineni et al, 2002) to measure the per-formance of the SMS normalization.
BLEU scoremeasures the similarity between two sentencesusing n-gram statistics with a penalty for tooshort sentences, which is already widely-used inMT evaluation.Setup BLEU score (3-gram)Raw SMS withoutNormalization 0.5784Dictionary Look-upplus Frequency 0.6958Bi-gram LanguageModel Only 0.7086Table 5.
Performance of different set-ups of the baseline experiments on the5000 parallel SMS messages5.1 Baseline Experiments: Simple SMSLingo Dictionary Look-up and UsingLanguage Model OnlyThe baseline experiment is to moderate the textsusing a lingo dictionary comprises 142 normali-zation pairs, which is also used in bootstrappingthe phrase alignment learning process.Table 5 compares the performance of the dif-ferent setups of the baseline experiments.
Wefirst measure the complexity of the SMS nor-malization task by directly computing the simi-larity between the raw SMS text and thenormalized English text.
The 1st row of Table 5reports the similarity as 0.5784 in BLEU score,which implies that there are quite a number ofEnglish word 3-gram that are common in the rawand normalized messages.
The 2nd experiment iscarried out using only simple dictionary look-up.3 The entries are collected from various websites such ashttp://www.handphones.info/sms-dictionary/sms-lingo.php,and http://www.funsms.net/sms_dictionary.htm, etc.38Lexical ambiguity is addressed by selecting thehighest-frequency normalization candidate, i.e.,only unigram LM is used.
The performance ofthe 2nd experiment is 0.6958 in BLEU score.
Itsuggests that the lingo dictionary plus the uni-gram LM is very useful for SMS normalization.Finally we carry out the 3rd experiment usingdictionary look-up plus bi-gram LM.
Only aslight improvement of 0.0128 (0.7086-0.6958) isobtained.
This is largely because the Englishwords in the lingo dictionary are mostly high-frequency and commonly-used.
Thus bi-gramdoes not show much more discriminative abilitythan unigram without the help of the phrase-based lexical mapping model.Experimental result analysis reveals that thestrength of our model is in its ability to disam-biguate mapping as in ?2?
to ?two?
or ?to?
and?w?
to ?with?
or ?who?.
Error analysis showsthat the challenge of the model lies in the properinsertion of subject pronoun and auxiliary orcopula verb, which serves to give further seman-tic information about the main verb, however thisrequires significant context understanding.
Forexample, a message such as ?u smart?
gives littleclues on whether it should be normalized to ?Areyou smart??
or ?You are smart.?
unless the fullconversation is studied.Takako w r u?Takako who are you?Im in ns, lik soccer, clubbin hangin w frenz!Wat bout u mee?I'm in ns, like soccer, clubbing hanging withfriends!
What about you?fancy getting excited w others' boredomFancy getting excited with others' boredomIf u ask me b4 he ask me then i'll go out w u alllor.
N u still can act so real.If you ask me before he asked me then I'll goout with you all.
And you still can act so real.Doing nothing, then u not having dinner w us?Doing nothing, then you do not having dinnerwith us?Aiyar sorry lor forgot 2 tell u... Mtg at 2 pm.Sorry forgot to tell you...  Meeting at two pm.tat's y I said it's bad dat all e gals know u...Wat u doing now?That's why I said it's bad that all the girls knowyou...  What you doing now?5.2 Using Phrase-based ModelWe then conducted the experiment using the pro-posed method (Bi-gram LM plus a phrase-basedlexical mapping model) through a five-fold crossvalidation on the 5000 parallel SMS messages.Table 6 shows the results.
An average score of0.8070 is obtained.
Compared with the baselineperformance in Table 5, the improvement is verysignificant.
It suggests that the phrase-basedlexical mapping model is very useful and ourmethod is effective for SMS text normalization.Figure 2 is the learning curve.
It shows that ouralgorithm converges when training data isincreased to 3000 SMS parallel messages.
Thissuggests that our collected corpus is representa-tive and enough for training our model.
Table 7illustrates some examples of the normalizationresults.5-fold cross validation BLEU score (3-gram)Setup 1 0.8023Setup 2 0.8236Setup 3 0.8071Setup 4 0.8113Setup 5 0.7908Ave.
0.8070Table 7.
Examples of Normalization Results5.3 Effect on English-Chinese MTAn experiment was also conducted to study theeffect of normalization on MT using 402 mes-sages randomly selected from the text corpus.We compare three types of SMS message: rawSMS messages, normalized messages using sim-ple dictionary look-up and normalized messagesusing our method.
The messages are passed totwo different English-to-Chinese translation sys-tems provided by Systran4 and Institute for Info-comm Research5(I2R) separately to produce threesets of translation output.
The translation qualityis measured using 3-gram cumulative BLEUscore against two reference messages.
3-gram isTable 6.
Normalization results for 5-fold cross validation test0.70.720.740.760.780.80.821000 2000 3000 4000 5000BLEUFigure 2.
Learning Curve4 http://www.systranet.com/systran/net5 http://nlp.i2r.a-star.edu.sg/techtransfer.html39used as most of the messages are short with aver-age length of seven words.
Table 8 shows thedetails of the BLEU scores.
We obtain an aver-age of 0.3770 BLEU score for normalized mes-sages against 0.1926 for raw messages.
Thesignificant performance improvement suggeststhat preprocessing of normalizing SMS text us-ing our method before MT is an effective way toadapt a general MT system to SMS domain.I2R Systran Ave.Raw Message 0.2633 0.1219 0.1926Dict Lookup 0.3485 0.1690 0.2588Normalization 0.4423 0.3116 0.3770Table 8.
SMS Translation BLEU score with orwithout SMS normalization6 ConclusionIn this paper, we study the differences amongSMS normalization, general text normalization,spelling check and text paraphrasing, and inves-tigate the different phenomena of SMS messages.We propose a phrase-based statistical method tonormalize SMS messages.
The method producesmessages that collate well with manually normal-ized messages, achieving 0.8070 BLEU scoreagainst 0.6958 baseline score.
It also signifi-cantly improves SMS translation accuracy from0.1926 to 0.3770 in BLEU score without adjust-ing the MT model.This experiment results provide us with a goodindication on the feasibility of using this methodin performing the normalization task.
We plan toextend the model to incorporate mechanism tohandle missing punctuation (which potentiallyaffect MT output and are not being taken care atthe moment),  and making use of pronunciationinformation to handle OOV caused by the use ofphonetic spelling.
A bigger data set will also beused to test the robustness of the system leadingto a more accurate alignment and normalization.ReferencesA.T.
Aw, M. Zhang, Z.Z.
Fan, P.K.
Yeo and J. Su.2005.
Input Normalization for an English-to-Chinese SMS Translation System.
MT Summit-2005S.
Bangalore, V. Murdock and G. Riccardi.
2002.Bootstrapping Bilingual Data using ConsensusTranslation for a Multilingual Instant MessagingSystem.
COLING-2002R.
Barzilay and K. R. McKeown.
2001.
Extractingparaphrases from a parallel corpus.
ACL-2001E.
Brill and R. C. Moore.
2000.
An Improved ErrorModel for Noisy Channel Spelling Correction.ACL-2000P.
F. Brown, S. D. Pietra, V. D. Pietra and R. Mercer.1993.
The Mathematics of Statistical MachineTranslation: Parameter Estimation.
ComputationalLinguistics: 19(2)A. Clark.
2003.
Pre-processing very noisy text.
InProceedings of Workshop on Shallow Processingof Large Corpora, Lancaster, 2003F.
J. Damerau.
1964.
A technique for computer detec-tion and correction of spelling errors.
Communica-tions ACM 7, 171-176A.P.
Dempster, N.M. Laird and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm, Journal of the Royal Statistical So-ciety, Series B, Vol.
39, 1-38A.
Golding and D. Roth.
1999.
A Winnow-Based Ap-proach to Spelling Correction.
Machine Learning34: 107-130F.
Jelinek.
1991.
Self-organized language modelingfor speech recognition.
In A. Waibel and K.F.
Lee,editors, Readings in Speech Recognition, pages450-506.
Morgan Kaufmann, 1991M.
D. Kernighan, K Church and W. Gale.
1990.
Aspelling correction program based on a noisychannel model.
COLING-1990K.
Kukich.
1992.
Techniques for automatically cor-recting words in text.
ACM Computing Surveys,24(4):377-439K.
A. Papineni, S. Roukos, T. Ward and W. J. Zhu.2002.
BLEU : a Method for Automatic Evaluationof Machine Translation.
ACL-2002P.
Koehn, F.J. Och and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
HLT-NAACL-2003C.
Shannon.
1948.
A mathematical theory of commu-nication.
Bell System Technical Journal 27(3):379-423M.
Shimohata and E. Sumita 2002.
Automatic Para-phrasing Based on Parallel Corpus for Normaliza-tion.
LREC-2002R.
Sproat, A.
Black, S. Chen, S. Kumar, M. Ostendorfand C. Richards.
2001.
Normalization of Non-Standard Words.
Computer Speech and Language,15(3):287-333A.
Stolcke.
2002.
SRILM ?
An extensible languagemodeling toolkit.
ICSLP-2002K.
Toutanova and R. C. Moore.
2002.
PronunciationModeling for Improved Spelling Correction.
ACL-2002R.
Zens and H. Ney.
2004.
Improvements in Phrase-Based Statistical MT.
HLT-NAALL-200440
