APPL ICAT ION-SPECIF IC  ISSUES IN NATURALLANGUAGE INTERFACER DEVELOPMENTFOR A D IAGNOSTIC  EXPERT SYSTEMKaren L. Ryan Rebecca RootDuane OlawskyHoneywell Inc.CSDD1000 Boone Avenue N.Minneapolis, Minnesota 55427ABSTRACTThis paper describes a natural languageinterface developed for an expert system,Page-7~ The interface accepts Englishdescriptions of observed symptoms and mapsthose descriptions to hypotheses used as initialinput to the Page-X diagnosis system.
Theinterface describes an application-independent linguistic interface and anapplication-specific hypothesis identificationcomponent.PROBLEM DESCRIPT IONThe natural language interface (NLI) wedescribe here makes up one part of the userinterface to Page-X \[STRA85\], an expertsystem for the diagnosis of problems in non-impact page printers.
To better understandsome of our design decisions, it is useful to seethe context in which this system was developedand deployed.
Page-X has been in develop-ment since 1984 and was first deployed inearly 1986.
The knowledge base continues tobe expanded to handle new printers.
Twocopies of the system reside in a centrallocation.
Field technicians who have theresponsibility of serving customer sites aretrained to diagnose problems in theseprinters; however, they often must rely onmore expert knowledge.
They can accessPage-X via modem over a standard ASCIIterminal.
Page-X performs diagnosis byreasoning from an initial set of symptomhypotheses to a probable cause, typicallyasking the technician to perform various testsalong the way.
The task of the user interfaceis to allow the technician to efficiently statewhat symptoms he/she has observed, bethinitially and as a result of the requested tests.Menus were used as the original mode ofspecifying symptoms, and these form a part ofthe current interface.
However, as theknowledge base grew to thousands ofhypotheses, it became unwieldy to input thehypotheses.
A keyword interface was added toallow technicians to describe the situationmore directly.
This interface functioned bymatching a predetermined set of keywords inthe user's typed free-form English input to thehypotheses' names, which were actually shortdescriptions.
Page-X would then startreasoning from this initial set, butdynamically compose menus to receive inputabout the results of requested tests.
This modeof interaction proved to be very easy toimplement.
However, the limitations ofkeyword matching soon became restrictive,e.g.,: (1) it was not possible to be preciseenough in the match process; (2) it wasdifficult to treat synonymous words properly,and (3) in some subdomains (e.g., printquality), the amount of synonymy and nearsynonymy made exact keyword matchingnearly useless.These factors motivated the developers toreplace the keyword part of the interface with arestricted NLI.
Subsequent use ofdynamically created menus is proceeding asbefore.
This project is currently in itsinfancy, having started in March of 1986.
Atpresent it deals with only a subset of thedomain covered by Page-X, namely printquality, and is not as yet complete with respectto this subdomain.
However, the results areextremely promising based on a comparisonof the possible performance of the keywordinterface and the performance of theimplemented natural language interface.109SYSTEM OVERVIEWThe Page-X interface can be divided into twomajor modules: The linguistic interfacecomponent (LIC) and the symptom identifica-tion component (SIC).
The LIC is basedalmost entirely on an interface developedunder the ATOZ project \[LARS85\].
The SIChad to be specifically developed for the Page-Xapplication and domain.LINGUISTIC INTERFACE COMPONENTThe LIC consists of several subcomponentsthat apply various grammars and lexicons toyield a domain- and application-specificinterpretation of the input.
As in most suchNLIs, the components are: (1) a parser thatmakes use of a grammar and lexicon toproduce a constituent structure representation,(2) a logical interpretation component, whichuses a set of Montague grammar style rules toproduce a "logical form" representation, (3) alexical translation component, which uses adomain model and rules to translate Englishterms to domain-model terms, and (4) alogical form translation component, whichuses a set of logical form transformationrules to produce a representation adhering tothe specific formal language syntax requiredto interface to the application.
A box diagramof the LIC is presented in Figure 1.
Adaptingthe ATOZ LIC to Page-X required writing anew domain model and new lexicons as wellas extending each of the three grammars tohandle linguistic phenomena not encounteredin the original query applications.
No soft-ware modifications were necessary.I n p u t ~  1"Lexlcal to LF ~Translation~~ransfL rFm atlo~SY m pt orn ID)G7363-3457-1 ( Page-X ES)Figure i.
Diagram of the LICThe output of the LIC cannot serve as directinput to the application program.
There willtypically be a need for translation fromdomain predicates to application predicatesand/or a matching of input propositions toapplication propositions.
Page-X symptomhypotheses are stated in terms of domainmodel predicates, so no predicate translationis necessary.
However, for reasons that willbe explained in detail below, it is necessary tohave an explicit component to match therepresentation of the input to the hypothesis.This is the function of the SIC.DETAILED INTERFACE TRACEIn this section, we will discuss a detailed traceof one example, showing the overall operationof the system.
Figure 2 shows the initial input,the output of each stage of semantic interpreta-tion, and the final set of symptoms identifiedas closest to the original input.
For spaceconsiderations, we have suppressed the parseroutput which is an unremarkable feature-value graph of the constituent structure.> (px-atoz (white line going down page) t)Parse completed.Logical Form:((((WHITE Yl159)) ((AGO El173 Yl159)(VERB-SUBJ El173 Y159)))((PAGE Yl169) (DOWN El173 Yl169)))Lexical and Logical Transformation:( (AFFECTED-OBJECT Y1159)(COLOR-OFY1159 !1176)(L IGHT-COLOR !1176)(WHITE!1176)(INTENSITY-OF Yl159 !1177)(LIGHT !1177)(BAND-LINE Y1159)(ORIENTATION-OF Yl159!1178)(VERTICAL !1178)(NIL)(NIL)(ENTIRE-PAGEY1169)(AFFECTED-OBJECT Yl159)(ORIENTATION-OF Yl159!1179)(VERTICAL !1179) LOCATION-OFYl159 Y1169)(ENTIRE-PAGE Yl169))Matcher:(HYPO-1503.P3-PRINT-HAS-LIGHT-VARYING-WI DTH-BANDSHYPO-1151.P3-PRINT-HAS-LIGHT=VERTICAL-BANDS110HYPO-1367.
P3-PRINT-HAS-SYMMETRICAL-BANDSHYPO-251.P3-PRINT-HAS-LIGHT-BANDS-CONSTANT-WIDTHHYPO-1374.P3-TONER-STARVATIONHYPO-1371.P3-PRINT-HAS-UNEVEN-PRINT DENSITY-SIDE-TO-SIDEHYPO-165.P-3-THE-PRINTED-PAGE-IS-WASHED-OUTHYPO-111 .P3-PRINT-HAS-VERTICAL-LINESHYPO-1370.P3-PRINT-HAS-SLAS HESFigure 2.
Trace OutputThe keyword interface operated on matchingthe content words found in the hypothesisnames.
As can be seen from this example,that method would fail to identify asignificant number of possible causes.LINGUISTIC COVERAGESYNTACTIC  ANALYS ISSyntactic analysis is performed by means of aunification-based chart parser applied to acombinatory categorial grammar.
Theparser was developed under the LUCY projectat MCC \[WITT86\].
The grammar employedis the ATOZ grammar, with extensions tohandle some aspects of "telegraphic" speech.The scope of this project did not permit athorough determination f the sublanguageused in this domain and application.However, we were able to study approximately20 user transcripts and complete one fieldengineer interview to help us arrive at aworking grammar and lexicon.
Because theapplication requires that the user givesymptom descriptions, input is almost alwaysin the form of simple declarative clauses andphrases.
Some examples are given here:(1) Fat characters,(2) No format print,(3) Overprinting arbage,(4) Characters meared own leftside of page,(5) Character too dark,(6) Characters repeating downpage.As can be seen, structures omittingdeterminers, copula, whole predicates andwhole subjects are often used.
Since thisappears to be the norm for this application,these are treated as fully grammatical.Logical form rules ensure that they receive thesame interpretation as their fully specifiedcounterparts.
Because this grammar is anoutgrowth of a grammar previously developedfor database query, there is also coverage ofstandard interrogative and imperativestructures, though these forms are not presentin our user input.SEMANTIC  TRANSLAT IONThe translation to initial logical form isdriven by a Montague-style grammar thatpairs constituent structure graphs toexpressions of lambda calculus.
Thetranslation component itself simply appliesthese rules recursively and performs formulareduction.
An example rule is given here.This would be for translating a nounmodified by an adjective:IX: \[cat:Nlf: \[rule: ( x (l(x) 2(x)))argl: <1>arg2: <2>\]\]Y: \[cs: \[head: 2\[cat: N\]mod: l\[cat: Adj\]\]\]\]Graph unification is used to match a graphwith the appropriate rule and to supply thearguments to the lambda calculus expression.Since the purpose of this component is to yielda logical formula for every input phrase, thecoverage is identical to that of the syntacticgrammar.
At this point, semantic translationprimarily serves to reveal the predicate argu-ment structure of the constituent s ructuregraph.
Negation and quantifiers are alsohandled, although the current versions of thedomain model and the SIC do not havemechanisms todeal with these.
Because of thelimitations of the SIC< certain other semanticdistinctions that appear to be important withinthis domain, such as iterative aspect, tense,and degree, are ignored at this point.111ISSUESUNGRAMMATICAL  INPUT AND PART IALINTERPRETAT IONSIn describing the grammatical coverageabove, we mentioned that certain kinds ofnonstandard constructions were treated asfully grammatical.
However, it is notpossible to take this approach everywhere theinput deviates from the standard.
For thesystem to be of use during development, and totake care of idiolectic and random deviance,some treatment of true ungrammaticality isnecessary.
One approach would be to revert atthis point to the simple keyword match.However, even in unparsed sentences, there isfrequently enough information available tobetter identify the intended symptom.
Theapproach we take is to heuristically assemblea partial syntactic analysis made up of threefragments, translate the fragments, and thenagain apply heuristics to get the best domain-dependent connection between the logicalform fragments.Syntactic phrase assembly is accomplished byusing path finding techniques on the chart todiscover the best set of chart edges thatexhaustively cover the input with no overlaps.An A* technique is employed with path scorebased on path length and edge length.
Shortpaths are favored over long ones.
Thisensures that the partial analysis is made up ofa few large constituent structure graphs.These general heuristics could be replaced oraugmented by heuristics based on moregrammar-specific characteristics such asmajor category or presence of a requiredsubcategory.
Translation of logicv!
tbrm andinsertion of domain predicates proceed asnormal, yielding a set of partially connectedformulas with default ranslations wherecontexts could not be met.The SIC, by itself, would not work well at all ifgiven a formula where the variables are notproperly shared.
It would simply ignore all ofthe information that was not tied, throughvariable bindings, to the affected-objectpredicate.
This could be most of the formula.To prevent this, whenever a complete parse isnot found, variable bindings are forced in thehypothesis marcher input so that all predicatesare tied through shared variables to theaffected-object predicate.
These forcedbindings may, of course, be incorrect, butinitial tests indicate that the rather simple-minded approach to the ungrammaticality weemploy here still brings noticeableimprovement over the straight keywordmatch.LEXICAL  TRANSLAT ION ISSUESLexical translation is the process ofsubstituting one or more predicates of adomain model for one or more Englishlexical items.
The lexical translation step inthe Page-X system is a subpart of the step oftranslation from English to an intermediatedomain model.Lexical translation as used here is not exactlythe same as the problem of mapping fromEnglish lexical items to standard databaseconstructs.
The problem of translatingEnglish lexical items to standard databaseconstructs can be broken into at least twosteps: (1) English to intermediate domainmodel, and (2) intermediate domain model todatabase model.
Some work specificallyfocusing on mapping problems from domainmodel expressions to database target modelshas been done by \[STAL84\], \[STAL86\] and\[SCHA82\].The system-specific semantics (i.e., theproblem of matching inputs to the appropriateset of relevant hypotheses) has an effect on thestructure of the domain model and the problemof lexical translation.
The system semanticsdetermine the degree to which synonymous oronly nearly synonymous terms should bedistinguished.
Distinctiveness of terms isdetermined by the relevance of theirsemantics to distinguishing a hypothesis'relevance to a given user input.Synonymous lexical items are of coursetranslated to the same set of predicates.
In thissystem, for example, 'dot' and 'spots' are bothtranslated to a predicate DOTS.
The task ofhypothesis matching requires additionalsemantic distinctions to capture the notion ofnear synonymy.
For example, 'streak' and'slash' are not treated as synonyms at thelowest level in the domain model because for112some hypotheses, the distinction betweenstreak and slash is significant.
However,user input referring to 'streaks' and 'slashes'may be relevant to hypotheses that describeconditions such as 'dark lines down page.
'The near synonymy of 'streak' and 'slash'are defined as subconcepts.Lexical ambiguity in this system is dealt withby defining more than one lexical translationrule for each lexical item and supplyingcontexts that must be satisfied before the rulecan be applied.
In general all input can besufficiently disambiguated by appealing to thecontext supplied by the rest of the user input.In occasional cases this is not sufficient.
Tohandle such cases, all predicates have onecontext-free rule associated with them.
Thisis sometimes necessary for elliptical andungrammatical input (i.e., cases where theappropriate contest is frequently not present).The context-free rules guarantee that sometranslation will always be produced for anyuser input; however, there are aspects of thetranslation that are not adequately handled bythe current mechanisms used to specifycontextually dependent and default rules.The formal mechanisms of rule contexts anddefault rules are the primary means ofaccounting for lexical ambiguity in thissystem.
Other systems propose techniques ofconstraint satisfaction (\[RICH87\]) andmarker passing (\[HIRS84\]) todeal with thesame types of problems.
We have investi-gated the possibility of assuming a lexicaltranslation rule context (by entering it into acontext work area where the partial transla-tion is developed) so that the associated lexicaltranslation rule could apply.
If, under theassumed context, a translation for the entireinput can be completed, the translation isconsidered to be correct.
If a translationcannot be completed, all translated clausesthat depend on the original assumption wouldbe removed and a new assumption made (ifnecessary).
This is similar to the constraintsatisfaction approach.
We make a distinctionnot clearly made in either of the citedapproaches in the type of contextualinformation that may be specified in therules.
Lexical translation rules can containboth linguistic contextual information anddomain model contextual information.
Thisis useful since some contextual informationis more easily specified using one set of termsrather than the other.
Currently we have nofacility for specifying negative contexts.This facility would occasionally beconvenient for the specification of rules.HYPOTHESIS  MATCHING ISSUESThe problem of matching hypotheses to theoutput of the semantic interpretationcomponent introduces its own set of problems.It is rarely, if ever, the case that the user inputwill match exactly to any hypothesis represen-tation.
It is necessary to specify what kindand how much of a match is needed betweenthe input and the stored hypotheses tojustifyidentifying the hypothesis as a start state forthe expert system.READINESSEXTENSION TO OTHER SUBDOMAINPLANSTo fully replace the keyword matchingcomponent of the current Page-X interface, wemust extend our work to include the othersubdomains of the general Page-X problemdomain.
These include areas of mechanicalproblems, electrical problems, printingproblems, power problems and exceptionalcases.
This will require additional linguisticextensions and a generalization of thehypothesis identification routine.
The overallstrategy of the heuristic matching processwould remain the same.LINGUISTIC ROBUSTNESSLinguistic robustness can be enhanced byconducting experiments o determinesublanguage and by analyzing and makinguse of the results.
Further improvement in thetreatment of ungrammatical input isnecessary.
Currently, there is no techniquefor handling words that are not in the lexicon.Also, the heuristics employed in assembling apartial interpretation can be made moredependent on linguistic and domain facts.
Itis possible that the heuristics that are effectivefor grammatical input will not be as effectivefor ungrammatical input.
This is because an113ungrammatical parse will have an effect onthe shared variables between predicates in thetranslated user input.
The alfalpha rules andthe lexical translation rules can produce newpredicates with variables shared acrosspredicates.
If default lexical translationrules or nonstandard alfalpha rules must beapplied because the parse has not completelysucceeded, then there will be cases where two(or more) predicates in the translated useroutput will not share variables that would havebeen shared in an equivalent grammaticalinput translation.GENERAL IZED SYMPTOM DESCRIPT IONINTERFACESIt is an open question as to whether or not thisinterface could be used as the basis for ageneralized symptom description interface.The generalization would have to include boththe strictly linguistic aspects of the interfaceand the application-specific aspects.
Thestrictly linguistic portions of the interface (theparser, the three-stage semantic interpreta-tion routines) are applicable to any naturallanguage symptom description interface.The modifications to support partialinterpretation of ungrammatical input arealso useful in any domain.
The application-specific aspects of the interface may begeneralizable under restricted conditions.
Ifthe new domain is one where 'importantconcepts' can be identified, then there is agood chance that some version of the matchingheuristics could be applied.BIBL IOGRAPHY\[HIRS84\] Hirst, Semantic InterpretationAgainst Ambiguity, Ph.D. dissertation,Brown University, 1984.\[LARS85\] J.A.
Larson, W.F.
Kaemmerer,K.L.
Ryan, J. Slagle, W.T.
Wood, "ATOZ - APrototype Intelligent Interface to MultipleSystems," Foundations for Human ComputerCommunication, K. Hopper and I.A.Newman (eds), Elsevier Science PublishersB.
V. (North Holland) New York, 1986.\[RICH87\] Rich, Barnett, Wittenberg,Wroblewski, "Ambiguity Procrastination,"Proceedings of AAAI87, July 13-17, 1987,Seattle, Washington, pp 571-576.\[SCHA82\] J.H.
Remko Scha, "English Wordsand Databases: How to Bridge the Gap,"Proceedings of 20th Annum ACL, June 16-18,1982, Toronto, Ontario, pp 57-59.\[STAL84\] Stallard, "Data Modelling forNatural Language Access," The FirstConference on Artificial IntelligenceApplications, Denver, Colorado, December 5-7,1984, pp 19-24.\[STAL86\] Stallard, "A TerminologicalSimplification Transformation for NaturalLanguage Question-Answering Systems,"Proceedings of 24th Annual ACL, New York,New York, 1986, pp 241-246.\[STRA85\] Strandberg, Abromovich, Mitchell,Prill, "Page-l: A Troubleshooting Aid forNon-Impact Page Printing Systems,"Proceedings of the Second Conference onArtificial Intelligence Applications, MiamiBeach, Florida, December 11-13, 1985, pp 68-74.\[WITT86\] Kent Wittenberg, "A Parser forPortable NL Interfaces Using Graph-Unification-Based Grammars," Proceedingsof AAAI86, August 11-15.
1986, Philadelphia,Pennsylvania, pp 1053-1058.114
