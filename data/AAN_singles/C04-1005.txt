Improving Statistical Word Alignment with a Rule-Based MachineTranslation SystemWU Hua, WANG HaifengToshiba (China) Research & Development Center5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng DistrictBeijing, China, 100738{wuhua, wanghaifeng}@rdc.toshiba.com.cnAbstractThe main problems of statistical word alignmentlie in the facts that source words can only bealigned to one target word, and that the inappro-priate target word is selected because of datasparseness problem.
This paper proposes an ap-proach to improve statistical word alignmentwith a rule-based translation system.
This ap-proach first uses IBM statistical translationmodel to perform alignment in both directions(source to target and target to source), and thenuses the translation information in the rule-basedmachine translation system to improve the statis-tical word alignment.
The improved alignmentsallow the word(s) in the source language to bealigned to one or more words in the target lan-guage.
Experimental results show a significantimprovement in precision and recall of wordalignment.1 IntroductionBilingual word alignment is first introduced as anintermediate result in statistical machine transla-tion (SMT) (Brown et al 1993).
Besides beingused in SMT, it is also used in translation lexiconbuilding (Melamed 1996), transfer rule learning(Menezes and Richardson 2001), example-basedmachine translation (Somers 1999), etc.
In previ-ous alignment methods, some researches mod-eled the alignments as hidden parameters in astatistical translation model (Brown et al 1993;Och and Ney 2000) or directly modeled themgiven the sentence pairs (Cherry and Lin 2003).Some researchers used similarity and associationmeasures to build alignment links (Ahrenberg etal.
1998; Tufis and Barbu 2002).
In addition, Wu(1997) used a stochastic inversion transductiongrammar to simultaneously parse the sentencepairs to get the word or phrase alignments.Generally speaking, there are four cases inword alignment: word to word alignment, wordto multi-word alignment, multi-word to wordalignment, and multi-word to multi-word align-ment.
One of the most difficult tasks in wordalignment is to find out the alignments that in-clude multi-word units.
For example, the statisti-cal word alignment in IBM translation models(Brown et al 1993) can only handle word toword and multi-word to word alignments.Some studies have been made to tackle thisproblem.
Och and Ney (2000) performed transla-tion in both directions (source to target and targetto source) to extend word alignments.
Their re-sults showed that this method improved precisionwithout loss of recall in English to German align-ments.
However, if the same unit is aligned totwo different target units, this method is unlikelyto make a selection.
Some researchers usedpreprocessing steps to identity multi-word unitsfor word alignment (Ahrenberg et al 1998;Tiedemann 1999; Melamed 2000).
The methodsobtained multi-word candidates based on con-tinuous N-gram statistics.
The main limitation ofthese methods is that they cannot handle sepa-rated phrases and multi-word units in low fre-quencies.In order to handle all of the four cases in wordalignment, our approach uses both the alignmentinformation in statistical translation models andtranslation information in a rule-based machinetranslation system.
It includes three steps.
(1) Astatistical translation model is employed to per-form word alignment in two directions1 (Englishto Chinese, Chinese to English).
(2) A rule-basedEnglish to Chinese translation system is em-ployed to obtain Chinese translations for eachEnglish word or phrase in the source language.
(3)The translation information in step (2) is used toimprove the word alignment results in step (1).A critical reader may pose the question ?why1 We use English-Chinese word alignment as a case study.not use a translation dictionary to improve statis-tical word alignment??
Compared with a transla-tion dictionary, the advantages of a rule-basedmachine translation system lie in two aspects: (1)It can recognize the multi-word units, particularlyseparated phrases, in the source language.
Thus,our method is able to handle the multi-wordalignments with higher accuracy, which will bedescribed in our experiments.
(2) It can performword sense disambiguation and select appropriatetranslations while a translation dictionary canonly list all translations for each word or phrase.Experimental results show that our approach im-proves word alignments in both precision andrecall as compared with the state-of-the-art tech-nologies.2Statistical Word AlignmentStatistical translation models (Brown, et al 1993)only allow word to word and multi-word to wordalignments.
Thus, some multi-word units cannotbe correctly aligned.
In order to tackle this prob-lem, we perform translation in two directions(English to Chinese and Chinese to English) asdescribed in Och and Ney (2000).
The GIZA++toolkit is used to perform statistical alignment.Thus, for each sentence pair, we can get twoalignment results.
We use  and  to representthe alignment sets with English as the source lan-guage and Chinese as the target language or viceversa.
For alignment links in both sets, we use ifor English words and j for Chinese words.1S 2S}0  },{|),{(1 ?== jjjj aaAjAS}0  },{|),{(2 ?== iiii aaAAiSWhere, represents the index posi-tion of the source word aligned to the target wordin position x.
For example, if a Chinese word inposition j is connected to an English word in po-sition i, then .
If a Chinese word in positionj is connected to English words in positions iand , then .
),( jixax =ia j =,{ 21 iiA j =)1( >k12i }2  We call an element inthe alignment set an alignment link.
If the linkincludes a word that has no translation, we call ita null link.
If k words have null links, wetreat them as k different null links, not just onelink.2 In the following of this paper, we will use the positionnumber of a word to refer to the word.Based on  and , we obtain their intersec-tion set, union set and subtraction set.1S 2SIntersection: 21 SSS ?=Union: 21 SSP ?=Subtraction: S?= PFThus, the subtraction set contains two differ-ent alignment links for each English word.3 Rule-Based Translation SystemWe use the translation information in a rule-based English-Chinese translation system3 to im-prove the statistical word alignment result.
Thistranslation system includes three modules: sourcelanguage parser, source to target language trans-fer module, and target language generator.From the transfer phase, we get Chinese trans-lation candidates for each English word.
Thisinformation can be considered as another wordalignment result, which is denoted as)},{(3 kCkS = .
C  the set including the trans-lation candidates for the k-th  English word orphrase.
The difference between S  and thecommon alignment set is that each English wordor phrase in S  has one or more translation can-didates.
A translation example for the Englishsentence ?He is used to pipe smoking.?
is shownin Table 1.k  is33English Words Chinese TranslationsHe ?is used to ?
?pipe ????
?smoking ???
?Table 1.
Translation ExampleFrom Table 1, it can be seen that (1) the trans-lation system can recognize English phrases (e.g.is used to); (2) the system can provide one ormore translations for each source word or phrase;(3) the translation system can perform word se-lection or word sense disambiguation.
For exam-ple, the word ?pipe?
has several meanings suchas ?tube?, ?tube used for smoking?
and ?windinstrument?.
The system selects ?tube used forsmoking?
and translates it into Chinese words????
and ????.
The recognized translation3 This system is developed based on the Toshiba English-Japanese translation system (Amano et al 1989).
It achievesabove-average performance as compared with the English-Chinese translation systems available in the market.candidates will be used to improve statisticalword alignment in the next section.44.1Word Alignment ImprovementAs described in Section 2, we have two align-ment sets for each sentence pair, from which weobtain the intersection set S  and the subtractionset .
We will improve the word alignments in Sand  with the translation candidates producedby the rule-based machine translation system.
Inthe following sections, we will first describe howto calculate monolingual word similarity used inour algorithm.
Then we will describe the algo-rithm used to improve word alignment results.FFWord Similarity CalculationThis section describes the method for monolin-gual word similarity calculation.
This methodcalculates word similarity by using a bilingualdictionary, which is first introduced by Wu andZhou (2003).
The basic assumptions of thismethod are that the translations of a word canexpress its meanings and that two words are simi-lar in meanings if they have mutual translations.Given a Chinese word, we get its translationswith a Chinese-English bilingual dictionary.
Thetranslations of a word are used to construct itsfeature vector.
The similarity of two words isestimated through their feature vectors with thecosine measure as shown in (Wu and Zhou 2003).If there are a Chinese word or phrase w  and aChinese word set Z , the word similarity betweenthem is calculated as shown in Equation (1).
))',((),('wwsimMaxZwsimZw?=  (1)4.2 Alignment Improvement AlgorithmAs the word alignment links in the intersectionset are more reliable than those in the subtractionset, we adopt two different strategies for thealignments in the intersection set S  and the sub-traction set .
For alignments in S, we will mod-ify them when they are inconsistent with thetranslation information in S .
For alignments in, we classify them into two cases and make se-lection between two different alignment links ormodify them into a new link.F3FIn the intersection set S , there are only wordto word alignment links, which include no multi-word units.
The main alignment error type in thisset is that some words should be combined intoone phrase and aligned to the same word(s) in thetarget sentence.
For example, for the sentencepair in Figure 1, ?used?
is aligned to the Chineseword ???
?, and ?is?
and ?to?
have null links in.
But in the translation set , ?is used to" is aphrase.
Thus, we combine the three alignmentlinks into a new link.
The words ?is?, ?used?
and?
to?
are all aligned to the Chinese word ???
?,denoted as (is used to, ??).
Figure 2 describesthe algorithm employed to improve the wordalignment in the intersection set S .S 3S)jphk ,3SFigure 1.
Multi-Word Alignment ExampleInput: Intersection set S , Translation set , 3SFinal word alignment set WAFor each alignment link?
in , do:  ,i S(1) If all of the following three conditions aresatisfied, add the new alignment linkWA phk ???
w,  to WA .a) There is an element?
, andthe English word i is a constituent of thephrase  .3) SCk ?kphb) The other words in the phrase ph  alsohave alignment links in S .kc) For each word s in ph , we get k}),|{ St(stT ?= and combine 4  all wordsin T into a phrase w , and the similar-ity 1),( ?>kCwsim .
(2) Otherwise, add?
to WA .
), jiOutput: Word alignment set WAFigure 2.
Algorithm for the Intersection SetIn the subtraction set, there are two differentlinks for each English word.
Thus, we need toselect one link or to modify the links according tothe translation information in .For each English word i in the subtraction set,there are two cases:4 We define an operation ?combine?
on a set consisting ofposition numbers of words.
We first sort the position num-bers in the set ascendly and then regard them as a phrase.For example, there is a set {{2,3}, 1, 4}, the result afterapplying the combine operation is (1, 2, 3, 4).Case 1: In , there is a word to word alignmentlink?
.
In , there is a word to word orword to multi-word alignment link1S1S ), ji ?
2S2), SAi i ??
5.Case 2: In , there is a multi-word to wordalignment link ( .
In S , thereis a word to word or word to multi-word align-ment link?
.1S, Aijj AiSjA ??
&), 12) S?2iFor Case 1, we first examine the translationset .
If there is an element?
, we cal-culate the Chinese word similarity between j inand  with Equation (1) shown inSection 4.1.
We also combine the words in A) into a phrase and get the word simi-larity between this new phrase and C .
The align-ment link with a higher similarity score isselected and added to WA .3S)j ?
), Ai3), SCi i ?i1, Si?
( Si ?
?iCi2Input: Alignment sets S  and  1 2STranslation unit?
3), SCph kk ?
(1) For each sub-sequence6 s of , get thesets andkph}1)?,(|{ 111 StstT =}) 22 St ?,(|{ 22 stT =(2) Combine words in T  and T  into phrasesand  respectively.1 21w 2w(3) Obtain the word similaritiesand .
),Csim(wws k11 = ),Csim(wws k22 =(4) Add a new alignment link to WAaccording to the following steps.a) If ws and 21 ws> 11 ?>ws , add ?to WA ;), 1wphkb) If ws  and 12 ws> 12 ?>ws , add?to WA ;), 2wphkc) If 121 ?>= wsws), 2wk, add ?
orto WA  randomly.
), 1wphkph?Output: Updated alignment set WAFigure 3.
Multi-Word to Multi-Word Align-ment AlgorithmIf, in S , there is an element?
and iis a constituent of , the English word i of thealignment links in both S  and  should be3  ), kk Cph2Skph1combined with other words to form phrases.
Inthis case, we modify the alignment links into amulti-word to multi-word alignment link.
Thealgorithm is described in Figure 3.5  ?
), iAi represents both the word to word and word tomulti-word alignment links.6  If a phrase consists of three words w , the sub-sequences of this phrase are w .321 ww221 ,, www 3321 ,, wwwFor example, given a sentence pair in Figure 4,in S , the word ?whipped?
is aligned to ???
?and ?out?
is aligned to ????.
In S , the word?whipped?
is aligned to both ????
and ???
?and ?out?
has a null link.
In , ?whipped out?
isa phrase and translated into ?????".
And theword similarity between ??????
and ??????
is larger than the threshold1213S?
.
Thus, wecombine the aligned target words in the Chinesesentence into ??????.
The final alignmentlink should be (whipped out, ??
??
).Figure 4.
Multi-Word to Multi-Word AlignmentExampleFor Case 2, we first examine S  to seewhether there is an element?
.
If true,we combine the words in  (? )
into aword or phrase and calculate the similarity be-tween this new word or phrase and C  in thesame way as in Case 1.
If the similarity is higherthan a threshold33S2S?i), Ci i ?
), Ai iiA1?
, we add the alignment linkinto WA .
), iAi?If there is an element?
and i is aconstituent of ph , we combine the Englishwords in A  ( ) into a phrase.
If it isthe same as the phrase  and3), SCph kk ?1kph (k, jA jj )( S?1), ?>kCjsim ,we add (  into WA .
Otherwise, we use themulti-word to multi-word alignment algorithm inFigure 3 to modify the links.
),A j jAfter applying the above two strategies, thereare still some words not aligned.
For each sen-tence pair, we use E and C to denote the sets ofthe source words and the target words that are notaligned, respectively.
For each source word in E,we construct a link with each target word in C.We use L },|),{( CjEiji ?
?=  to denote thealignment candidates.
For each candidate in L,we look it up in the translation set S .
If there isan element33), SCi i ??
and 2)( , ?>j iCsim , weadd the link into the set WA .|C|| CCSS?SCC =||5 Experiments5.15.2Training and Testing SetWe did experiments on a sentence aligned Eng-lish-Chinese bilingual corpus in general domains.There are about 320,000 bilingual sentence pairsin the corpus, from which, we randomly select1,000 sentence pairs as testing data.
The remain-der is used as training data.The Chinese sentences in both the training setand the testing set are automatically segmentedinto words.
The segmentation errors in the testingset are post-corrected.
The testing set is manuallyannotated.
It has totally 8,651 alignment linksincluding 2,149 null links.
Among them, 866alignment links include multi-word units, whichaccounts for about 10% of the total links.Experimental ResultsThere are several different evaluation methodsfor word alignment (Ahrenberg et al 2000).
Inour evaluation, we use evaluation metrics similarto those in Och and Ney (2000).
However, we donot classify alignment links into sure links andpossible links.
We consider each alignment as asure link.If we use S  to indicate the alignments iden-tified by the proposed methods and S  to denotethe reference alignments, the precision, recall andf-measure are calculated as described in Equation(2), (3) and (4).
According to the definition of thealignment error rate (AER) in Och and Ney(2000), AER can be calculated with Equation (5).GC|S|SS|GG ?=precision       (2)|S||SS|CCG ?=recall    (3)||||*2GGSSfmeasure +=  (4)fmeasureSSSAERGG ?+?
?= 1||||*21 (5)In this paper, we give two different alignmentresults in Table 2 and Table 3.
Table 2 presentsalignment results that include null links.
Table 3presents alignment results that exclude null links.The precision and recall in the tables are obtainedto ensure the smallest AER for each method.Precision Recall AEROurs 0.8531 0.7057 0.2276Dic 0.8265 0.6873 0.2495IBM E-C 0.7121 0.6812 0.3064IBM C-E 0.6759 0.7209 0.3023IBM Inter 0.8756 0.5516 0.3233IBM Refined 0.7046 0.6532 0.3235Table 2.
Alignment Results Including Null LinksPrecision Recall AEROurs 0.8827 0.7583 0.1842Dic 0.8558 0.7317 0.2111IBM E-C 0.7304 0.7136 0.2781IBM C-E 0.6998 0.6725 0.3141IBM Inter 0.9392 0.5513 0.3052IBM refined 0.8152 0.6926 0.2505Table 3.
Alignment Results Excluding Null LinksIn the above tables, the row ?Ours?
presentsthe result of our approach.
The results are ob-tained by setting the word similarity thresholds to1.01??
and 5.02??
.
The Chinese-English dic-tionary used to calculate the word similarity has66,696 entries.
Each entry has two English trans-lations on average.
The row ?Dic?
shows the re-sult of the approach that uses a bilingualdictionary instead of the rule-based machinetranslation system to improve statistical wordalignment.
The dictionary used in this method isthe same translation dictionary used in the rule-based machine translation system.
It includes57,684 English words and each English word hasabout two Chinese translations on average.
Therows ?IBM E-C?
and ?IBM C-E?
show the re-sults obtained by IBM Model-4 when treatingEnglish as the source and Chinese as the target orvice versa.
The row ?IBM Inter?
shows resultsobtained by taking the intersection of the align-ments produced by ?IBM E-C?
and ?IBM C-E?.The row ?IBM Refined?
shows the results byrefining the results of ?IBM Inter?
as described inOch and Ney (2000).Generally, the results excluding null links arebetter than those including null links.
This indi-cates that it is difficult to judge whether a wordhas counterparts in another language.
It is be-cause the translations of some source words canbe omitted.
Both the rule-based translation sys-tem and the bilingual dictionary provide no suchinformation.It can be also seen that our approach performsthe best among others in both cases.
Our ap-proach achieves a relative error rate reduction of26% and 25% when compared with ?IBM E-C?and ?IBM C-E?
respectively7.
Although the pre-cision of our method is lower than that of the?IBM Inter?
method, it achieves much higherrecall, resulting in a 30% relative error rate re-duction.
Compared with the ?IBM refined?method, our method also achieves a relative errorrate reduction of 30%.
In addition, our method isbetter than the ?Dic?
method, achieving a relativeerror rate reduction of 8.8%.In order to provide the detailed word align-ment information, we classify word alignmentresults in Table 3 into two classes.
The first classincludes the alignment links that have no multi-word units.
The second class includes at least onemulti-word unit in each alignment link.
The de-tailed information is shown in Table 4 and Table5.
In Table 5, we do not include the method ?In-ter?
because it has no multi-word alignment links.Precision Recall AEROurs 0.9213 0.8269 0.1284Dic 0.8898 0.8215 0.1457IBM E-C 0.8202 0.7972 0.1916IBM C-E 0.8200 0.7406 0.2217IBM Inter 0.9392 0.6360 0.2416IBM Refined 0.8920 0.7196 0.2034Table 4.
Single Word Alignment ResultsPrecision Recall AEROurs 0.5123 0.3118 0.6124Dic 0.3585 0.1478 0.7907IBM E-C 0.1682 0.1697 0.8311IBM C-E 0.1718 0.2298 0.8034IBM Refined 0.2105 0.2910 0.7557Table 5.
Multi-Word Alignment ResultsAll of the methods perform better on singleword alignment than on multi-word alignment.
InTable 4, the precision of our method is close tothe ?IBM Inter?
approach, and the recall of ourmethod is much higher, achieving a 47% relativeerror rate reduction.
Our method also achieves a37% relative error rate reduction over the ?IBMRefined?
method.
Compared with the ?Dic?method, our approach achieves much higher pre-cision without loss of recall, resulting in a 12%relative error rate reduction.6 Discussion7 The error rate reductions in this paragraph are obtainedfrom Table 2.
The error rate reductions in Table 3 areomitted.Our method also achieves much better resultson multi-word alignment than other methods.However, our method only obtains one third ofthe correct alignment links.
It indicates that it isthe hardest to align the multi-word units.Readers may pose the question ?why the rule-based translation system performs better on wordalignment than the translation dictionary??
Forsingle word alignment, the rule-based translationsystem can perform word sense disambiguation,and select the appropriate Chinese words astranslation.
On the contrary, the dictionary canonly list all translations.
Thus, the alignment pre-cision of our method is higher than that of thedictionary method.
Figure 5 shows alignmentprecision and recall values under different simi-larity values for single word alignment includingnull links.
From the figure, it can be seen that ourmethod consistently achieves higher precisions ascompared with the dictionary method.
The t-score value (t=10.37, p=0.05) shows the im-provement is statistically significant.Figure 5.
Recall-Precision CurvesFor multi-word alignment links, the translationsystem also outperforms the translation diction-ary.
The result is shown in Table 5 in Section 5.2.This is because (1) the translation system canautomatically recognize English phrases withhigher accuracy than the translation dictionary; (2)The translation system can detect separatedphrases while the dictionary cannot.
For example,for the sentence pairs in Figure 6, the solid linklines describe the alignment result of the rule-base translation system while dashed lines indi-cate the alignment result of the translation dic-tionary.
In example (1), the phrase ?be going to?indicates the tense not the phrase ?go to?
as thedictionary shows.
In example (2), our methoddetects the separated phrase ?turn ?
on?
whilethe dictionary does not.
Thus, the dictionarymethod produces the wrong alignment link.Figure 6.
Alignment Comparison Examples7 Conclusion and Future WorkThis paper proposes an approach to improve sta-tistical word alignment results by using a rule-based translation system.
Our contribution is that,given a rule-based translation system that pro-vides appropriate translation candidates for eachsource word or phrase, we select appropriatealignment links among statistical word alignmentresults or modify them into new links.
Especially,with such a translation system, we can identifyboth the continuous and separated phrases in thesource language and improve the multi-wordalignment results.
Experimental results indicatethat our approach can achieve a precision of 85%and a recall of 71% for word alignment includingnull links in general domains.
This result signifi-cantly outperforms those of the methods that usea bilingual dictionary to improve word alignment,and that only use statistical translation models.Our future work mainly includes three tasks.First, we will further improve multi-word align-ment results by using other technologies in natu-ral language processing.
For example, we can usenamed entity recognition and transliteration tech-nologies to improve person name alignment.
Sec-ond, we will extract translation rules from theimproved word alignment results and apply themback to our rule-based machine translation sys-tem.
Third, we will further analyze the effect ofthe translation system on the alignment results.ReferencesLars Ahrenberg, Magnus Merkel, and Mikael Anders-son 1998.
A Simple Hybrid Aligner for GeneratingLexical Correspondences in Parallel Texts.
In Proc.of the 36th Annual Meeting of the Association forComputational Linguistics and the 17th Int.
Conf.on Computational Linguistics, pp.
29-35.Lars Ahrenberg, Magnus Merkel, Anna Sagvall Heinand Jorg Tiedemann 2000.
Evaluation of wordalignment systems.
In Proc.
of the Second Int.
Conf.on Linguistic Resources and Evaluation, pp.
1255-1261.ShinYa Amano, Hideki Hirakawa, Hiroyasu Nogami,and Akira Kumano 1989.
Toshiba Machine Trans-lation System.
Future Computing Systems,2(3):227-246.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra and Robert L. Mercer 1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2):263-311.Colin Cherry and Dekang Lin 2003.
A ProbabilityModel to Improve Word Alignment.
In Proc.
of the41st Annual Meeting of the Association for Com-putational Linguistics, pp.
88-95.I.
Dan Melamed 1996.
Automatic Construction ofClean Broad-Coverage Translation Lexicons.
InProc.
of the 2nd Conf.
of the Association for Ma-chine Translation in the Americas, pp.
125-134.I.
Dan Melamed 2000.
Word-to-Word Models ofTranslational Equivalence among Words.
Compu-tational Linguistics, 26(2): 221-249.Arul Menezes and Stephan D. Richardson 2001.
ABest-first Alignment Algorithm for Automatic Ex-traction of Transfer Mappings from Bilingual Cor-pora.
In Proc.
of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation, pp.
39-46.Franz Josef Och and Hermann Ney 2000.
ImprovedStatistical Alignment Models.
In Proc.of the 38thAnnual Meeting of the Association for Computa-tional Linguistics, pp.
440-447.Harold Somers 1999. Review Article: Example-BasedMachine Translation.
Machine Translation 14:113-157.Jorg Tiedemann 1999.
Word Alignment ?
Step by Step.In Proc.
of the 12th Nordic Conf.
on ComputationalLinguistics, pp.
216-227.Dan Tufis and Ana Maria Barbu.
2002.
Lexical TokenAlignment: Experiments, Results and Application.In Proc.
of the Third Int.
Conf.
on Language Re-sources and Evaluation, pp.
458-465.Dekai Wu 1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377-403.Hua Wu and Ming Zhou 2003.
Optimizing SynonymExtraction Using Monolingual and Bilingual Re-sources.
In Proc.
of the 2nd Int.
Workshop on Para-phrasing, pp.
72-79.
