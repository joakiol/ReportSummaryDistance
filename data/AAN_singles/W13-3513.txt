Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 114?123,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsSeparating Disambiguation from Compositionin Distributional SemanticsDimitri KartsaklisUniversity of OxfordDept of Computer ScienceWolfson Bldg, Parks RoadOxford, OX1 3QD, UKdimitri.kartsaklis@cs.ox.ac.ukMehrnoosh SadrzadehQueen Mary Univ.
of LondonSchool of Electr.
Engineeringand Computer ScienceMile End RoadLondon, E1 4NS, UKmehrs@eecs.qmul.ac.ukStephen PulmanUniversity of OxfordDept of Computer ScienceWolfson Bldg, Parks RoadOxford, OX1 3QD, UKstephen.pulman@cs.ox.ac.ukAbstractMost compositional-distributional modelsof meaning are based on ambiguous vec-tor representations, where all the sensesof a word are fused into the same vec-tor.
This paper provides evidence that theaddition of a vector disambiguation stepprior to the actual composition would bebeneficial to the whole process, produc-ing better composite representations.
Fur-thermore, we relate this issue with thecurrent evaluation practice, showing thatdisambiguation-based tasks cannot reli-ably assess the quality of composition.
Us-ing a word sense disambiguation schemebased on the generic procedure of Sch?tze(1998), we first provide a proof of con-cept for the necessity of separating dis-ambiguation from composition.
Then wedemonstrate the benefits of an ?unambigu-ous?
system on a composition-only task.1 IntroductionCompositional and distributional semantic mod-els seem to provide complementary solutions forsolving the same problem, that of assigning aproper ?meaning?
to a text segment.
Specifically,while compositional models deal with the recur-sive nature of the language, providing a way toaddress its inherent ability to create infinite sen-tences from finite resources (words), they leavewords as unexplained primitives whose meaningshave somehow already been set before the compo-sitional process.
On the other hand, distributionalmodels have been especially successful in provid-ing concrete representations for the meaning ofwords as vectors in a vector space, created by tak-ing into account the context in which each wordappears.
Despite its success for smaller languageunits, the distributional hypothesis does not natu-rally lend itself to compounds of words.
Hencethese models do not canonically scale in tasks re-quiring the creation of vector representations fortext constituents larger than words, i.e.
for phrasesand sentences.Given the complementary nature of those twosemantic models, it is not surprising that consider-able research activity has been dedicated on com-bining them into a single framework that wouldbenefit from the best of both worlds in a uni-fied manner: Mitchell and Lapata (2008) exper-iment with intransitive sentences, applying sim-ple compositional models based on vector ad-dition and point-wise multiplication in a disam-biguation task; Baroni and Zamparelli (2010) andGuevara (2010) use regression models in order tobuild vectors for adjective-noun compounds; Erkand Pad?
(2008) work on transitive sentences us-ing structured vector spaces; Socher et al(2010,2011, 2012) use neural networks to combine vec-tors following the grammatical structure; Grefen-stette and Sadrzadeh (2011a,b) apply the categori-cal framework of Coecke et al(2010) on the dis-ambiguation task of Mitchell and Lapata (2008);and Kartsaklis et al(2012) and Grefenstette et al(2013) build upon previous implementations byadding specific algebraic operations and machinelearning techniques to further improve the con-crete abilities of the abstract categorical models.A common strand in all of the above models isthat they are based on ?ambiguous?
vector rep-resentations, where a polysemous word is repre-sented by a single vector regardless of the numberof its actual senses.
For example, the word ?bank?has at least two meanings (financial institution andland alongside a river), both of which will be fusedinto a single vector representation.
And, althoughit is generally true that compositional models fol-lowing the formal semantics view of Montague donot care about disambiguation (meanings of wordsin such models are represented by logical con-stants explicitly set before the compositional pro-cess), the story changes when one moves to a vec-tor space model with ambiguous vector represen-tations.
The main problem is that, when acting onambiguous vector spaces, compositional models114seem to perform two tasks at the same time, com-position and disambiguation, leaving the resultingvector hard to interpret: it is not clear if this vectoris a proper meaning representation for the com-posed compound or just a disambiguated versionof one of the words therein.
This problem escapesthe evaluation schemes, especially when disam-biguation tasks are used as a criterion for evaluat-ing compositional models?a common practice incurrent research for compositional-distributionalsemantics.
Indeed, Pulman (2013) argues that al-though disambiguation can emerge as a welcomeside-effect of the compositional process, it is notclear if compositionality is either a necessary orsufficient condition for disambiguation to happen.On the contrary, it seems that the form of mostcurrent vector space models and the compositionaloperations used on them (quite often some form ofvector point-wise multiplication) mainly achievedisambiguation, but not composition.The purpose of this paper is to further investi-gate the potential of a compositional-distributionalmodel based on disambiguated vector represen-tations, where each word can have one or moredistinct senses.
More specifically, we aim toshow that (a) compositionality is not a neces-sary condition for disambiguation, so the quitecommon practice of using a disambiguation taskas a criterion for evaluating the performance ofcompositional-distributional models is question-able; and (b) the introduction of a separate disam-biguation step in the compositional process of dis-tributional models can be indeed beneficial for thequality of the resulting composed vectors.We train our models from BNC, a 100-millionwords corpus created from samples of written andspoken English.
We perform word sense induc-tion by following the generic algorithm of Sch?tze(1998), in which the senses of a word are repre-sented by distinct clusters created by taking intoaccount the various contexts in which this specificword occur in the corpus.
For the actual cluster-ing step we use a combination of hierarchical ag-glomerative clustering and the Calin?ski-Harabaszindex (Calin?ski and Harabasz, 1974).
The param-eters of the models are fine-tuned on the noun setof SEMEVAL 2010 Word Sense Induction and Dis-ambiguation task (Manandhar et al 2010).Equipped with a disambiguated vector space,we use it on a verb disambiguation experiment,similar in style to that of Mitchell and Lapata(2008), but applied on a more linguistically mo-tivated dataset, based on the work of Pickeringand Frisson (2001).
We find that the applicationof a simple disambiguation algorithm, without anycompositional steps, is proven more effective thana number of compositional models.
We considerthis as an indication for the necessity of separat-ing disambiguation from composition, since it im-plies that the latter is not necessary for achiev-ing the former.
Next, we demonstrate that a com-positional model based on disambiguated vectorscan indeed produce composite vector representa-tions of better quality, by applying the model on aphrase similarity task (Mitchell and Lapata, 2010).The goal here is to evaluate the similarity of shortverb phrases, based on the distance of their com-posite vectors.2 Composition in distributional modelsThe transition from word meaning to sentencemeaning, a task easily done by human subjectsbased on the rules of grammar, implies the exis-tence of a composition operation applied to prim-itive text units in order to build compound ones.Various solutions have been proposed with differ-ent levels of sophistication for this problem in thecontext of vector space models of meaning.At one end of the spectrum the simple modelsof Mitchell and Lapata (2008) address composi-tion as the point-wise multiplication or additionof the involved word vectors.
This bag-of-wordsapproach has been proven a hard-to-beat baselinefor many of the more sophisticated models.
At theother end, composition in the work of Socher et al(2010, 2011, 2012) is served by the advanced ma-chinery of recurring neural networks, where theoutput of the network is used again as input in arecurring fashion, for composing vectors of largerconstituents.
Following a different path, the cat-egorical framework of Coecke et al(2010) ex-ploits a structural homomorphism between gram-mar and vector spaces in order to treat words withspecial meanings, such as verb and adjectives, asfunctions (tensors of rank-n) that apply to their ar-guments.
This application has the form of innerproduct, generalising the familiar notion of matrixmultiplication to tensors of higher rank.Regardless of their level of sophistication, mostof the models which aim to apply composition-ality on word vector representations fail to ad-dress the problem of handling the polysemous na-ture of words.
Even more importantly, many ofthe models are evaluated on their ability to dis-ambiguate the meaning of specific words, follow-ing an idea first introduced by Kintsch (2001) andlater adopted by Mitchell and Lapata (2008) andothers.
For example, in this latter work the au-115thors test their multiplicative and additive modelsas follows: given an ambiguous intransitive verb,say ?run?
(with the two senses to be those of mov-ing fast and of a liquid dissolving), they examineto what extent the composition of the verb withan appropriate subject (e.g.
?horse?
or ?colour?
)will disambiguate the intended sense of the verbwithin the specific context.
Each row in the datasetconsists of a subject (e.g.
?horse?
), a verb (?run?
),a high-similarity landmark verb (?gallop?
), and alow-similarity landmark verb (?dissolve?).
Thesubject is combined with the main verb to form asimple intransitive sentence, and the vector of thissentence is then compared with the vectors of thelandmark verbs.
The goal is to evaluate the degreeto which the composed sentence vector is closerto the high landmark than to the vector of the lowlandmark, and this is considered an indication ofsuccessful composition.However, although it is generally true that mul-tiplying ??
?run with ???
?horse will filter out most of thecomponents of??
?run that are irrelevant to ?dissolve?
(since the ?dissolve?-related elements of ???
?horseshould have values close to zero) and will pro-duce a disambiguated version of this verb underthe context of ?horse?, it is not at all clear if thisvector will also constitute an appropriate repre-sentation for the meaning of the intransitive sen-tence ?horse runs?.
In other words, here we havetwo tasks taking place at the same time: (a) dis-ambiguation of the ambiguous word given its con-text; and (b) composition that produces a mean-ing vector for the whole sentence.
The extent towhich the latter is a necessary condition for theformer remains unclear, and constitutes a factorthat complicates the evaluation and assessment ofsuch systems.
In this paper we argue that as longas the above distinct tasks are interwoven into asingle step, claims of compositionality in distri-butional systems cannot be reliably assessed.
Wetherefore propose the addition of a disambiguationstep in the generic methodology of compositional-distributional models.3 Related workAlthough in general word sense induction is apopular topic in the natural language processingliterature, little has been done to address poly-semy specifically in the context of compositional-distributional models of meaning.
In fact, the onlyworks relevant to ours we are aware of are that ofErk and Pad?
(2008) and Reddy et al(2011).
Thestructured vector space of Erk and Pad?
(2008) isdesigned to handle ambiguity in an implicit way,showing promising results on the Mitchell andLapata (2008) task.
The work of Reddy et al(2011) is closer to our research: the authors eval-uate two word sense disambiguation approacheson the noun-noun compound similarity task intro-duced by Mitchell and Lapata (2010), using sim-ple multiplicative and additive models for compo-sition.
The reported results are also promising,where at least one of their models performs bet-ter than the current practice of using ambiguousvector representations.Compared to both of the above works, thescope of the current paper is broader: it does notsolely aim to demonstrate the positive effect of a?cleaner?
vector space on the compositional pro-cess, but it also proceeds one step further and re-lates this issue with the current evaluation prac-tice, showing that a number of verb disambigua-tion tasks that have been invariantly used for theassessment of compositional-distributional mod-els might be in fact based on a wrong criterion.4 Disambiguation schemeOur word sense induction method is based onthe effective procedure first presented by Sch?tze(1998).
For the ith occurrence of a target word wtin the corpus with context Ci = {w1, .
.
.
, wn},we calculate the centroid of the context as ?
?ci =1n(?
?w1 + .
.
.
+ ?
?wn), where ?
?w is the lexical (orfirst order) vector of word w as it is created by theusual distributional practice (more details in Sec-tion 5).
Then, we cluster these centroids in orderto form a number of sense clusters.
Each senseof the word is represented by the centroid of thecorresponding cluster.
Following Sch?tze, we willrefer to these sense vectors as second-order vec-tors, in order to distinguish them from the lexical(first-order) vectors.
So, in our model each word isrepresented by a tuple ??
?w ,S?, where?
?w is the 1st-order vector of the word and S the set of 2nd-ordervectors created by the above procedure.We are now able to disambiguate the sense of atarget word wt given a context C by calculating acontext vector ?
?c for C as above, and then com-paring this with every 2nd-order vector of wt; theword is assigned to the sense that corresponds tothe closest 2nd-order vector.
That is,??
?spref = arg min?
?s ?Sd(?
?s ,?
?c ) (1)where S is the set of 2nd-order vectors for wt andd(?
?u ,?
?v ) the vector distance metric we use.For the clustering step, we use an iterativebottom-up approach known as hierarchical ag-glomerative clustering (HAC).
Hierarchical clus-1161 0 1 2 3 4 51.00.50.00.51.01.52.02.53.03.523 27 28 20 29 24 22 25 21 26 14 13 16 15 11 19 17 18 10 12 8 1 3 6 9 4 2 7 0 50.00.20.40.60.81.01.21.4Figure 1: Hierarchical agglomerative clustering.tering has been invariably applied to unsupervisedword sense induction on a variety of languages,generally showing good performance?see, forexample, the comparative study of Broda andMazur (2012) for English and Polish.
Comparedto k-means clustering, this approach has the ma-jor advantage that it does not require us to definein advance a specific number of clusters.
Com-pared to more advanced probabilistic techniques,such as Bayesian mixture models, it is muchmore straightforward and simple to implement,yet powerful enough to demonstrate the necessityof factoring out ambiguity from compositional-distributional models.HAC is a bottom-up method of cluster analy-sis, starting with each data point (context vector inour case) forming its own cluster; then, in each it-eration the two closest clusters are merged into anew cluster, until all points are finally merged un-der the same cluster.
This process produces a den-drogram (i.e.
a tree diagram), which essentiallyembeds every possible clustering of the dataset.As an example, Figure 1 shows a small datasetproduced by three distinct Gaussian distributions,and the dendrogram derived by the above algo-rithm.
Implementation-wise, the clustering part inthis work is served by the efficient FASTCLUSTERlibrary (M?llner, 2013).Choosing a number of senses In HAC, one stillneeds to decide where exactly to cut the tree in or-der to get the best possible partitioning of the data.Although the right answer to this problem mightdepend on many factors, we can safely assume thatthe optimal partitioning is the one that providesthe most compact and maximally separated clus-ters.
One way to measure the quality of a cluster-ing based on this criterion is the Calin?ski/Harabaszindex (Calin?ski and Harabasz, 1974), also knownas variance ratio criterion (VRC).
Given a set ofNdata points and a partitioning of k disjoint clusters,VRC is computed as follows:V RCk =trace(B)trace(W ) ?N ?
kk ?
1 (2)Here, W and B are the intra-cluster and inter-cluster dispersion matrices, respectively:W =k?i=1Ni?l=1(??xi(l)?
x?i)(??xi(l)?
x?i)T (3)B =k?i=1Ni(x?i ?
x?
)(x?i ?
x?
)T (4)where Ni is the number of data points assigned tocluster i,?
?xi(l) is the lth point assigned to this clus-ter, x?i is the centroid of ith cluster (the mean), andx?
is the data centroid of the overall dataset.
Giventhe above formulas, the trace of B is the sum ofinter-cluster variances, while the trace of W is thesum of intra-cluster variances.
A good partitioningshould have high values for B (which is an indi-cation for well-separated clusters) and low valuesfor W (an indication for compact clusters), so thehigher the quality of the partitioning the greaterthe value of this ratio.Compared to other criteria, VRC has beenfound to be one of the most effective approachesfor clustering validity?see the comparative stud-ies of Milligan and Cooper (1985) and Vendraminet al(2009).
Furthermore, it has been previouslyapplied to word sense discrimination successfully,returning the best results among a number of othermeasures (Savova et al 2006).
For this work, wecalculate VRC for a number of different partition-ings (ranged from 2 to 10 clusters), and we keepthe partitioning that results in the highest VRCvalue as the optimal number of senses for the spe-cific word.
Note that since the HAC dendrogramalready embeds all possible clusterings, the cut-ting of the tree in order to get a different partition-ing is performed in constant time.5 Experimental settingThe choice of our 1st-order vector space is basedon empirical tests, where we found out that a basiswith elements of the form ?word, class?
presentsthe right balance for our purposes among sim-pler techniques, such as word-based spaces, andmore complex ones, such as dependency-basedapproaches.
In our vector space, each word has adistinct vector representation for every word classunder which occurs in the corpus (e.g.
?suit?
willhave a noun vector and a verb vector).
As our ba-sis elements we use the 2000 most frequent con-tent words in BNC, with weights being calculatedas the ratio of the probability of the context wordgiven the target word to the probability of the con-text word overall.
The context here is a 5-wordwindow on both sides of the target word.The parameters of the clustering scheme are op-timized on the noun set of SEMEVAL 2010 Word117Sense Induction & Disambiguation Task (Man-andhar et al 2010).
Specifically, when using HACone has to decide how to measure the distancebetween the clusters, which is the merging crite-rion applied in every iteration of the algorithm,as well as the measure between the data points,i.e.
the individual vectors.
Based on empiricaltests we limit our options to two inter-cluster mea-sures: complete-link and Ward?s methods.
In thecomplete-link method the distance between twoclustersX and Y is the distance between their twomost remote elements:D(X,Y ) = maxx?X,y?Yd(x, y) (5)In Ward?s method, two clusters are selected formerging if the new partitioning exhibits the mini-mum increase in the overall intra-cluster variance.The cluster distance is given by:D(X,Y ) = 2|X||Y ||X|+ |Y |??
?cX ??
?cY ?2 (6)where ?
?cX and ?
?cY are the centroids of X and Y .We test these linkage methods in combinationwith three vector distance measures: euclidean,cosine, and Pearson?s correlation (6 models in to-tal).
The metrics were chosen to represent pro-gressively more relaxed forms of vector compar-ison, with the strictest form to be the euclideandistance and correlation as the most relaxed.
Forsense detection we use the disambiguation algo-rithm described in Section 4, considering as con-text the whole sentence in which a target wordappears.
The distance metric used for the dis-ambiguation process in each model is identicalto the metric used for the clustering process, soin the Ward/euclidean model the disambiguationis based on the euclidean distance, in complete-link/cosine model on the cosine distance, and soon.
We evaluate the models using V-measure,an entropy-based metric that addresses the so-Model V-Meas.
Avg clust.Ward/Euclidean 0.05 1.44Ward/Correlation 0.14 3.14Ward/Cosine 0.08 1.94Complete/Euclidean 0.00 1.00Complete/Correlation 0.11 2.66Complete/Cosine 0.06 1.74Most frequent sense 0.00 1.001 cluster/instance 0.36 89.15Gold standard 1.0 4.46Table 1: Results on the noun set of SEMEVAL2010 WSI&D task.keyboard: 1105 contexts, 2 sensesCOMPUTER (665 contexts): program dollar disk powerenter port graphic card option select language drivepen application corp external editor woman pricepage design sun cli amstrad lock interface lcd slotnotebookMUSIC (440 contexts): drummer instrumental singergerman father fantasia english generation wolfgangwayne cello body join ensemble mike chamber garysaxophone sax ricercarus apply form son metal guyclean roll barry orchestraTable 2: Derived senses for word ?keyboard?.called matching problem of F-score (Rosenbergand Hirschberg, 2007).
Table 1 shows the results.Ward?s method in combination with correla-tion distance provided the highest V-measure, fol-lowed by the combination of complete-link with(again) correlation.
Although a direct compari-son of our models with the models participatingin this task would not be quite sound (since thesemodels were trained on a special corpus providedby the organizers, while our model was trainedon the BNC), it is nevertheless enlightening tomention that the 0.14 V-measure places the Ward-correlation model at the 4th rank among 28 sys-tems for the noun set of the task, while at thesame time provides a reasonable average numberof clusters per word (3.14), close to that of thehuman-annotated gold standard (4.46).
Comparethis, for example, with the best-performing sys-tem that achieved a V-measure of 0.21, a scorethat was largely due to the fact that the model as-signed the unrealistic number of 11.54 senses perword on average (since V-measure tends to favourhigher numbers of senses, as the baseline 1 clus-ter/instance shows in Table 1).1Table 2 provides an example of the results,showing the senses for the noun ?keyboard?
learntby the best model of Ward?s method and correla-tion measure.
Each sense is visualized as a list ofthe most dominant words in the cluster, ranked bytheir TF-ICF values.
Furthermore, Figure 2 showsthe dendrograms produced by four linkage meth-ods for the word ?keyboard?, demonstrating the su-periority of Ward?s method.6 Disambiguation vs compositionA number of models that aim to equip distribu-tional semantics with compositionality are evalu-ated on some form of the disambiguation task pre-sented in Section 2.
Versions of this task can befound, for example, in Mitchell and Lapata (2008),1The results of SEMEVAL 2010 can be found online athttp://www.cs.york.ac.uk/semeval2010_WSI/task_14_ranking.html.1180.00.10.20.30.40.50.6keyboard (single/cosine)(a) Single-link0.00.20.40.60.8keyboard (average/cosine)(b) Average-link012345keyboard (ward/cosine)(c) Ward?s method0.00.20.40.60.81.0 keyboard (complete/cosine)(d) Complete-linkFigure 2: Dendrograms produced for word ?key-board?
according to 4 different linkage methods.Erk and Pad?
(2008), Grefenstette and Sadrzadeh(2011a,b), Kartsaklis et al(2012) and Grefenstetteet al(2013).
We briefly remind that the goal is toassess how well a compositional model can disam-biguate the meaning of an ambiguous verb, givena specific context.
This kind of evaluation involvestwo distinct tasks: the composition of sentencevectors, and the disambiguation of the verbs.
And,although the evaluation of a model against humanjudgements provides some indication for the suc-cess of the latter task, it leaves unclear to what ex-tent the former has been achieved.
In this sectionwe perform two experiments in order to addressthis question.
The first of them aims to support thefollowing argument: that although disambiguationcan emerge as a side-effect of a compositional pro-cess, compositionality is not a necessary conditionfor this to happen.
The second experiment is basedon a more appropriate task that requires genuinecompositional abilities, and demonstrates the goodperformance of a compositional model based onthe disambiguated vector space of Section 5.As our compositional method for the follow-ing tasks we use the multiplicative and additivemodels of Mitchell and Lapata (2008).
Despitethe simple nature of these models, there is a num-ber of reasons that make them good candidates fordemonstrating the main ideas of this paper.
First,for better or worse ?simple?
does not necessar-ily mean ?ineffective?.
The comparative study ofBlacoe and Lapata (2012) shows that for certaintasks these ?baselines?
perform equally well oreven better than other more sophisticated models.And second, it is reasonable to expect that bettercompositional models would only work in favourof our arguments, and not the other way around.6.1 Evaluating disambiguationOne potential problem with the datasets used forthe disambiguation task of Section 2, similar tothe one of Grefenstette and Sadrzadeh (2011a), isthat ambiguous verbs are usually collected from acorpus based on some automated method.
And,although they do exhibit variations in their senses(as most verbs do), in many cases these meaningsare actually related?for example, the meanings of?write?
in G&S dataset are spell and publish.
Toovercome this problem, we used the work of Pick-ering and Frisson (2001), which provides a list ofgenuinely ambiguous verbs obtained from carefulmanual selection and ranking from human evalu-ators.
The evaluators assessed the relatedness ofeach verb?s different meanings using a scale of0 (totally unrelated) to 7 (highly related).
Fromthese verbs, we picked 10 with an average mark< 1.
An example is ?file?, which means ?smooth?in ?file nails?
and ?register?
as in ?file an applica-tion?.
For each verb we picked the 10 most oc-curring subjects and objects from the BNC (5 foreach landmark).
In the case of verb ?file?, for ex-ample, among these were ?woman?
and ?nails?
forlandmark ?smooth?, and ?union?
and ?lawsuit?
forlandmark ?register?.
Each subject and object wasmodified by its most occurring adjective in the cor-pus.
This resulted in triples of sentences of thefollowing form:(1) main: young woman filed long nailshigh: young woman smoothed long nailslow: young woman registered long nails(2) main: monetary union filed civil lawsuithigh: mon.
union registered civil lawsuitlow: mon.
union smoothed civil lawsuitThe main sentence was paired with both highand low landmark sentences, creating a dataset2 of200 sentence pairs (10 main verbs ?
10 contexts?
2 landmarks)3.
These were randomly presentedto 43 human annotators, whose duty was to judgethe similarity between the sentences of each pair.The human scores were compared with scores pro-duced by a number of models (Table 3).The most successful model (M1) does not ap-ply any form of composition.
Instead, the com-parison of a sentence with a ?landmark?
sentenceis simply based on disambiguated versions of the2The dataset will be available at http://www.cs.ox.ac.uk/activities/compdistmeaning/.3As a comparison, the Mitchell and Lapata (2008) datasetconsists of 15 main verbs?
4 contexts?
2 landmarks = 120sentence pairs, while the Grefenstette and Sadrzadeh (2011a)dataset has the same configuration and size with ours.119verbs alone.
Specifically, the main verb and thelandmark verb are disambiguated given the con-text (subjects, objects, and adjectives that mod-ify them) according to Equation 1; this producestwo 2nd-order vectors, one for the main verb andone for the landmark.
The degree of similarity be-tween the two sentences is then calculated by mea-suring the similarity between the two sense vec-tors of the verbs, without any compositional step.The score of 0.28 achieved by this model is im-pressive, given that the inter-annotator agreement(which serves as an upper-bound) is 0.38.A number of interesting observations can bemade based on the results of Table 3.
First ofall, the ?verbs-only?
model outperforms the twobaselines (which use composition but not disam-biguation) by a large margin, and indeed also theother compositional models.
This is an indica-tion that this kind of disambiguation task mightnot be the best way to evaluate a compositionalmodel.
The fact that the most important condi-tion for success is the proper disambiguation ofthe verb, means that the good performance of acompositional model demonstrates only this: howwell the model is able to disambiguate an am-biguous verb.
This is different from how well thecomposed representation reflects the meaning ofthe larger constituent; that is, it has very little tosay about the extent to which an operation like?????
?woman??
?file???
?nails ( denotes point-wise mul-tiplication) results in a faithful representation ofthe meaning of sentence ?woman filed nails?.M2 to M5 represent different versions of thecompositional models that use disambiguation ina distinct step.
All these models compose both themain verb and the landmark with a given context,and then perform the comparison at sentence level.In M2 and M3 all words are first disambiguatedprior to composition, while in M4 and M5 the 2nd-Disambig.
Composition ?M1 Only verbs No 0.282 ?M2 All words Multiplicative 0.118M3 All words Additive 0.210M4 Only verbs Multiplicative 0.110M5 Only verbs Additive 0.234 ?B1 No Multiplicative 0.143B2 No Additive 0.042Inter-annotator agreement 0.383?
The difference between M1 and M5 is highlystatistically significant with p < 0.0001Table 3: Spearman?s ?
for the Pickering and Fris-son dataset.order vector of the verb is composed with the 1st-order vectors of the other words.
The most im-pressive observation here is that the separation ofdisambiguation results in a tremendous improve-ment for the additive model, from 0.04 to 0.21.This is not surprising since, when using magni-tude invariant measures between vectors (such ascosine distance), the resulting vector is nothingmore than the average of the involved word vec-tors.
The introduction of the disambiguation stepbefore the composition, therefore, makes a greatdifference since it provides much more accuratestarting points to be averaged.On the other hand, the disambiguated versionof multiplicative model (M2) presents inferior per-formance compared to the ?ambiguous?
version(B1).
We argue that the reason behind this is thatthe two models perform different jobs: the resultof B1 is a ?mixing?
of composition and disam-biguation of the most ambiguous word (i.e.
theverb), since this is the natural effect of the point-wise multiplication operation (see discussion inSection 2); on the other hand, M2 is designed toconstruct an appropriate composite meaning forthe whole sentence.
We will try to support thisargument by the experiment of the next section.6.2 A better test of compositionalityAlthough there might not exists such a thingas the best evaluation method for compositional-distributional semantics, it is safe to assume thata phrase similarity task avoids many of the pitfallsof tasks such as the one of Section 6.1.
Given pairsof short phrases, the goal is to assess the similar-ity of the phrases by constructing composite vec-tors for them and computing their distance.
No as-sumptions about disambiguation abilities regard-ing a specific word (e.g.
the verb) are made here;the only criterion is to what extent the compositevector representing the meaning of a phrase is sim-ilar or dissimilar to the vector of another phrase.From this perspective, this task seems the idealchoice for evaluating a model aiming to provideappropriate phrasal semantics.
The scores givenby the models are compared to those of humanevaluators using Spearman?s ?.For this experiment, we use the ?verb-object?part of the dataset presented in the work ofMitchell and Lapata (2010), which consists of 108pairs of short verb phrases exhibiting three de-grees of similarity.
A high similarity pair for ex-ample, is produce effect/achieve result, a mediumone is pour tea/join party, and a low one is closeeye/achieve end.
The original dataset al con-120Disambig.
Composition ?M1 Only verbs No 0.318M2 All words Multiplicative 0.412 ?M3 All words Additive 0.414 ?M4 Only verbs Multiplicative 0.352M5 Only verbs Additive 0.324B1 No Multiplicative 0.379 ?
?B2 No Additive 0.334Inter-annotator agreement 0.550?
Difference between M2/B1 is stat.
sign.
with p ?
0.07?
Difference between M3/B1 is stat.
sign.
with p ?
0.06Table 4: Phrase similarity results.tains noun-noun and adjective-noun compounds.However, the verb-object part serves the pur-poses of this paper much better, for two reasons.First, since by definition the proposed methodol-ogy suits better circumstances involving at leastsome level of word ambiguity, a dataset based onthe most ambiguous part of speech (verbs) seems areasonable choice.
Second, this part of the datasetallows us to do some meaningful comparisonswith the task of Section 6.1, which is again aroundverb structures.
The results are shown in Table 4.This time, the disambiguation step providessolid benefits for both multiplicative (M2) andadditive (M3) models, with differences that arestatistically significant from the best baseline B1(with p ?
0.07 and p ?
0.06, respectively).Note that the ?verbs-only?
model (M1), which wasby a large margin the most successful for thetask of Section 6.1, now shows the worst perfor-mance.
For comparison, the best result reported byMitchell and Lapata (2010) on a 1st-order spacesimilar to ours (regarding dimensions and weights)was 0.38 (?dilation?
model).7 DiscussionThis paper is based on the observation that anycompositional operation between two vectors isessentially a hybrid process consisting of two?components?
that, depending on the form of theunderlying vector space, can have different ?mag-nitudes?.
One of the components results in a cer-tain amount of disambiguation for the most am-biguous original word, while the other one workstowards a composite representation for the mean-ing of the whole phrase or sentence.
The tasks ofSection 6 are designed so that each one of them as-sesses a different aspect of this hybrid process: thetask of Section 6.1 is focused on the disambigua-tion aspect, while the task of Section 6.2 addressesthe compositionality part.
One of our main argu-ments is the observation that, in order the get bet-ter compositional representations, it is essential tofirst eliminate (or at least reduce as much as pos-sible the magnitude of) the disambiguation ?com-ponent?
that might show up as a by-product of thecompositional process, so that the result is mainlya product of pure composition?this is what the?unambiguous?
models do achieve in the task ofSection 6.2.
Based on the experimental work con-ducted in this paper, our first concluding remark isthat the elimination of the ambiguity factor can beessential for the quality of the composed vectors.But, if Table 4 provides a proof that the sep-aration of disambiguation and composition canindeed produce better compositional representa-tions, what is the meaning of the inferior perfor-mance of all ?unambiguous?
models (M2 to M5)compared to verbs-only version (M1) in the taskof Section 6.1?
Why disambiguation is not alwayseffective (as in the case of multiplicative model)for that task?
These are strong indications that thequality of composition is not crucial for disam-biguation tasks of this sort, whose only achieve-ment is that they measure the disambiguation side-effects generated by the compositional process.
Inother words, the practice of evaluating the qual-ity of composition by using disambiguation tasksis problematic.
As the topic of compositionalityin distributional models of meaning increasinglygains popularity in the recent years, this secondconcluding remark is equally important since itcan contribute towards better evaluation schemesof such models.8 Future workA next step to take in the future is the appli-cation of these ideas on more complex spaces,such as those based on the categorical frameworkof Coecke et al(2010).
The challenge here isthe effective generalization of a disambiguationscheme on tensors of rank greater than 1.
Ad-ditionally, we would expect this method to bene-fit from more robust probabilistic clustering tech-niques.
An appealing option is the use of a non-parametric method, such as a hierarchical Dirich-let process (Yao and Van Durme, 2011).AcknowledgementsWe would like to thank Daniel M?llner for hiscomments on the use of FASTCLUSTER library,as well as the three anonymous reviewers for theirfruitful suggestions.
Support by EPSRC grant EP/F042728/1 is gratefully acknowledged by the firsttwo authors.121ReferencesBaroni, M. and Zamparelli, R. (2010).
Nounsare Vectors, Adjectives are Matrices.
In Pro-ceedings of Conference on Empirical Methodsin Natural Language Processing (EMNLP).Blacoe, W. and Lapata, M. (2012).
A compari-son of vector-based representations for seman-tic composition.
In Proceedings of the 2012Joint Conference on Empirical Methods in Nat-ural Language Processing and ComputationalNatural Language Learning, pages 546?556,Jeju Island, Korea.
Association for Computa-tional Linguistics.Broda, B. and Mazur, W. (2012).
Evaluationof clustering algorithms for word sense disam-biguation.
International Journal of Data Anal-ysis Techniques and Strategies, 4(3):219?236.Calin?ski, T. and Harabasz, J.
(1974).
A DendriteMethod for Cluster Analysis.
Communicationsin Statistics-Theory and Methods, 3(1):1?27.Coecke, B., Sadrzadeh, M., and Clark, S.(2010).
Mathematical Foundations for Dis-tributed Compositional Model of Meaning.Lambek Festschrift.
Linguistic Analysis,36:345?384.Erk, K. and Pad?, S. (2008).
A Structured Vector-Space Model for Word Meaning in Context.
InProceedings of Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 897?906.Grefenstette, E., Dinu, G., Zhang, Y.-Z.,Sadrzadeh, M., and Baroni, M. (2013).
Multi-step regression learning for compositional dis-tributional semantics.Grefenstette, E. and Sadrzadeh, M. (2011a).
Ex-perimental Support for a Categorical Composi-tional Distributional Model of Meaning.
In Pro-ceedings of Conference on Empirical Methodsin Natural Language Processing (EMNLP).Grefenstette, E. and Sadrzadeh, M. (2011b).
Ex-perimenting with Transitive Verbs in a DisCo-Cat.
In Proceedings of Workshop on Geomet-rical Models of Natural Language Semantics(GEMS).Guevara, E. (2010).
A Regression Model ofAdjective-Noun Compositionality in Distribu-tional Semantics.
In Proceedings of the ACLGEMS Workshop.Kartsaklis, D., Sadrzadeh, M., and Pulman, S.(2012).
A unified sentence space for categoricaldistributional-compositional semantics: Theoryand experiments.
In Proceedings of 24th Inter-national Conference on Computational Linguis-tics (COLING 2012): Posters, pages 549?558,Mumbai, India.
The COLING 2012 OrganizingCommittee.Kintsch, W. (2001).
Predication.
Cognitive Sci-ence, 25(2):173?202.Manandhar, S., Klapaftis, I., Dligach, D., andPradhan, S. (2010).
Semeval-2010 task 14:Word sense induction & disambiguation.
InProceedings of the 5th International Workshopon Semantic Evaluation, pages 63?68.
Associa-tion for Computational Linguistics.Milligan, G. and Cooper, M. (1985).
An Exami-nation of Procedures for Determining the Num-ber of Clusters in a Data Set.
Psychometrika,50(2):159?179.Mitchell, J. and Lapata, M. (2008).
Vector-basedModels of Semantic Composition.
In Proceed-ings of the 46th Annual Meeting of the Associa-tion for Computational Linguistics, pages 236?244.Mitchell, J. and Lapata, M. (2010).
Compositionin distributional models of semantics.
CognitiveScience, 34(8):1388?1439.M?llner, D. (2013).
fastcluster: Fast HierarchicalClustering Routines for R and Python.
Journalof Statistical Software, 9(53):1?18.Pickering, M. and Frisson, S. (2001).
Process-ing ambiguous verbs: Evidence from eye move-ments.
Journal of Experimental Psychology:Learning, Memory, and Cognition, 27(2):556.Pulman, S. (2013).
Combining Compositional andDistributional Models of Semantics.
In Heunen,C., Sadrzadeh, M., and Grefenstette, E., editors,Quantum Physics and Linguistics: A Composi-tional, Diagrammatic Discourse.
Oxford Uni-versity Press.Reddy, S., Klapaftis, I., McCarthy, D., and Man-andhar, S. (2011).
Dynamic and static prototypevectors for semantic composition.
In Proceed-ings of 5th International Joint Conference onNatural Language Processing, pages 705?713.Rosenberg, A. and Hirschberg, J.
(2007).
V-measure: A conditional entropy-based externalcluster evaluation measure.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning, pages410?420.122Savova, G., Therneau, T., and Chute, C. (2006).Cluster Stopping Rules for Word Sense Dis-crimination.
In Proceedings of the workshopon Making Sense of Sense: Bringing Psy-cholinguistics and Computational LinguisticsTogether, pages 9?16.Sch?tze, H. (1998).
Automatic Word Sense Dis-crimination.
Computational Linguistics, 24:97?123.Socher, R., Huang, E., Pennington, J., Ng, A., andManning, C. (2011).
Dynamic Pooling and Un-folding Recursive Autoencoders for ParaphraseDetection.
Advances in Neural InformationProcessing Systems, 24.Socher, R., Huval, B., Manning, C., and A., N.(2012).
Semantic compositionality through re-cursive matrix-vector spaces.
In Conference onEmpirical Methods in Natural Language Pro-cessing 2012.Socher, R., Manning, C., and Ng, A.
(2010).Learning Continuous Pphrase Representationsand Syntactic Parsing with recursive neural net-works.
In Proceedings of the NIPS-2010 DeepLearning and Unsupervised Feature LearningWorkshop.Vendramin, L., Campello, R., and Hruschka, E.(2009).
On the Comparison of Relative Clus-tering Validity Criteria.
In Proceedings of theSIAM International Conference on Data Min-ing, SIAM, pages 733?744.Yao, X. and Van Durme, B.
(2011).
Nonparamet-ric bayesian word sense induction.
ACL HLT2011, page 10.123
