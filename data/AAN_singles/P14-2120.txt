Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 739?744,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsRecognizing Implied Predicate-Argument Relationshipsin Textual InferenceAsher SternComputer Science DepartmentBar-Ilan Universityastern7@cs.biu.ac.ilIdo DaganComputer Science DepartmentBar-Ilan Universitydagan@cs.biu.ac.ilAbstractWe investigate recognizing impliedpredicate-argument relationships whichare not explicitly expressed in syntacticstructure.
While prior works addressedsuch relationships as an extension to se-mantic role labeling, our work investigatesthem in the context of textual inferencescenarios.
Such scenarios provide priorinformation, which substantially easesthe task.
We provide a large and freelyavailable evaluation dataset for our tasksetting, and propose methods to cope withit, while obtaining promising results inempirical evaluations.1 Motivation and TaskThis paper addresses a typical sub-task in tex-tual inference scenarios, of recognizing impliedpredicate-argument relationships which are notexpressed explicitly through syntactic structure.Consider the following example:(i)The crucial role Vioxx plays in Merck?s port-folio was apparent last week when Merck?sshares plunged 27 percent to 33 dollars after thewithdrawal announcement.While a human reader understands that thewithdrawal refers to Vioxx, and hence an im-plied predicate-argument relationship holds be-tween them, this relationship is not expressed inthe syntactic structure, and will be missed by syn-tactic parsers or standard semantic role labelers.This paper targets such types of implied rela-tionships in textual inference scenarios.
Partic-ularly, we investigate the setting of RecognizingTextual Entailment (RTE) as a typical scenario oftextual inference.
We suggest, however, that thesame challenge, as well as the solutions proposedin our work, are applicable, with proper adap-tations, to other textual-inference scenarios, likeQuestion Answering, and Information Extraction(see Section 6).An RTE problem instance is composed of twotext fragments, termed Text and Hypothesis, as in-put.
The task is to recognize whether a humanreading the Text would infer that the Hypothesisis most likely true (Dagan et al, 2006).
For ourproblem, consider a positive Text Hypothesis pair,where the Text is example (i) above and the Hy-pothesis is:(ii)Merck withdrew Vioxx.A common approach for recognizing textual en-tailment is to verify that all the textual elementsof the Hypothesis are covered, or aligned, by el-ements of the Text.
These elements typically in-clude lexical terms as well as relationships be-tween them.
In our example, the Hypothesis lexi-cal terms (?Merck?, ?withdrew?
and ?Vioxx?)
areindeed covered by the Text.
Yet, the predicate-argument relationships (e.g., ?withdrawal-Vioxx?
)are not expressed in the text explicitly.
In sucha case, an RTE system has to verify that thepredicate-argument relationships which are ex-plicitly expressed in the Hypothesis, are impliedfrom the Text discourse.
Such cases are quite fre-quent (?17%) in the settings of our dataset, de-scribed in Section 3.Consequently, we define the task of recognizingimplied predicate-argument relationships, with il-lustrating examples in Table 1, as follows.
Theinput includes a Text and a Hypothesis.
Two termsin the Hypothesis, predicate and argument, aremarked, where a predicate-argument relationshipbetween them is explicit in the Hypothesis syntac-tic structure.
Two terms in the Text, candidate-predicate and candidate-argument, aligned to theHypothesis predicate and argument, are markedas well.
However, no predicate-argument rela-tionship between them is expressed syntactically.The task is to recognize whether the predicate-739# Hypothesis Text Y/N1 Merck [withdrew]pred[Vioxx]argfrom the market.The crucial role [Vioxx]cand-argplays in Merck?sportfolio was apparent last week when Merck?sshares plunged 27 percent to 33 dollars after the[withdrawal]cand-predannouncement.Y2 Barbara Cummings heard the taleof a woman who was comingto Crawford to [join]predCindySheehans [protest]arg.Sheehan?s [protest]cand-argis misguided and is hurtingtroop morale.
.
.
.Sheehan never wanted Casey to [join]cand-predthe mil-itary.N3 Casey Sheehan was [killed]predin[Iraq]arg.5 days after he arrived in [Iraq]cand-arglast year, CaseySheehan was [killed]cand-pred.Y4 Hurricane Rita [threatened]pred[New Orleans]arg.Hurricane Rita was upgraded from a tropical storm asit [threatened]cand-predthe southeastern United States,forcing an alert in southern Florida and scuttling plansto repopulate [New Orleans]cand-argafter HurricaneKatrina turned it into a ghost city 3 weeks earlier.Y5 Alberto Gonzales defends[renewal]predof the [PatriotAct]argto Congress.A senior official defended the [Patriot Act]cand-arg.
.
.. .
.
President Bush has urged Congress to[renew]cand-predthe law .
.
.Y6 The [train]arg[crash]predinjurednearly 200 people.At least 10 people were killed .
.
.
in the [crash]cand-pred.
.
.Alvarez is accused of .
.
.
causing the derailment of one[train]cand-arg.
.
.YTable 1: Example task instances from our dataset.
The last column specifies the Yes/No annotation,indicating whether the sought predicate-argument relationship is implied in the Text.
For illustration, adashed line indicates an explicit argument that is related to the candidate argument through some kind ofdiscourse reference.
Pred, arg and cand abbreviate predicate, argument and candidate respectively.argument relationship, as expressed in the Hypoth-esis, holds implicitly also in the Text.To address this task, we provide a large andfreely available annotated dataset, and proposemethods for coping with it.
A related task, de-scribed in the next section, deals with such impliedpredicate-argument relationships as an extensionto Semantic Role Labeling.
While the results re-ported so far on that annotation task were rela-tively low, we suggest that the task itself may bemore complicated than what is actually requiredin textual inference scenarios.
On the other hand,the results obtained for our task, which does fittextual inference scenarios, are promising, and en-courage utilizing algorithms for this task in actualinference systems.2 Prior WorkThe most notable work targeting impliedpredicate-argument relationships is the 2010SemEval task of Linking Events and Their Par-ticipants in Discourse (Ruppenhofer et al, 2009).This task extends Semantic Role Labeling to casesin which a core argument of a predicate is missingin the syntactic structure but a filler for thecorresponding semantic role appears elsewhereand can be inferred from discourse.
For example,in the following sentence the semantic role goal isunfilled:(iii)He arrived (0Goal) at 8pm.Yet, we can expect to find an implied filler forgoal elsewhere in the document.The SemEval task, termed henceforth as Im-plied SRL, involves three major sub-tasks.
First,for each predicate, the unfilled roles, termed NullInstantiations (NI), should be detected.
Second,each NI should be classified as Definite NI (DNI),meaning that the role filler must exist in the dis-course, or Indefinite NI otherwise.
Third, the DNIfillers should be found (DNI linking).Later works that followed the SemEval chal-lenge include (Silberer and Frank, 2012) and(Roth and Frank, 2013), which proposed auto-740matic dataset generation methods and featureswhich capture discourse phenomena.
Their high-est result was 12% F1-score.
Another work is theprobabilistic model of Laparra and Rigau (2012),which is trained by properties captured not onlyfrom implicit arguments but also from explicitones, resulting in 19% F1-score.
Another notablework is (Gerber and Chai, 2012), which was lim-ited to ten carefully selected nominal predicates.2.1 Annotations vs. RecognitionComparing to the implied SRL task, our task maybetter fit the needs of textual inference.
First, somerelatively complex steps of the implied SRL taskare avoided in our setting, while on the other handit covers more relevant cases.More concretely, in textual inference the can-didate predicate and argument are typically iden-tified, as they are aligned by the RTE system toa predicate and an argument of the Hypothesis.Thus, the only remaining challenge is to verifythat the sought relationship is implied in the text.Therefore, the sub-tasks of identifying and classi-fying DNIs can be avoided.On the other hand, in some cases the candi-date argument is not a DNI, but is still requiredin textual inference.
One type of such cases arenon-core arguments, which cannot be Definite NIs.However, textual inference deals with non-core ar-guments as well (see example 3 in Table 1).Another case is when an implied predicate-argument relationship holds even though the cor-responding role is already filled by another argu-ment, hence not an NI.
Consider example 4 of Ta-ble 1.
While the object of ?threatened?
is filled (inthe Text) by ?southeastern United States?, a hu-man reader also infers the ?threatened-New Or-leans?
relationship.
Such cases might follow ameronymy relation between the filler (?southeast-ern United States?)
and the candidate argument(?New Orleans?
), or certain types of discourse (co-)references (e.g., example 5 in Table 1), or someother linguistic phenomena.
Either way, they arecrucial for textual inference, while not being NIs.3 DatasetThis section describes a semi-automatic methodfor extracting candidate instances of impliedpredicate-argument relationship from an RTEdataset.
This extraction process directly followsour task formalization.
Given a Text Hypothe-sis pair, we locate a predicate-argument relation-ship in the Hypothesis, where both the predicateand the argument appear also in the Text, whilethe relationship between them is not expressed inits syntactic structure.
This process is performedautomatically, based on syntactic parsing (see be-low).
Then, a human reader annotates each in-stance as ?Yes?
?
meaning that the implied rela-tionship indeed holds in the Text, or ?No?
other-wise.
Example instances, constructed by this pro-cess, are shown in Table 1.In this work we used lemma-level lexicalmatching, as well as nominalization matching, toalign the Text predicates and arguments to the Hy-pothesis.
We note that more advanced match-ing, e.g., by utilizing knowledge resources (likeWordNet), can be performed as well.
To identifyexplicit predicate-argument relationships we uti-lized dependency parsing by the Easy-First parser(Goldberg and Elhadad, 2010).
Nominalizationmatching (e.g., example 1 of Table 1) was per-formed with Nomlex (Macleod et al, 1998).By applying this method on the RTE-6 dataset(Bentivogli et al, 2010), we constructed adataset of 4022 instances, where 2271 (56%)are annotated as positive instances, and 1751as negative ones.
This dataset is significantlylarger than prior datasets for the implied SRLtask.
To calculate inter-annotator agreement, thefirst author also annotated 185 randomly-selectedinstances.
We have reached high agreement scoreof 0.80 Kappa.
The dataset is freely available atwww.cs.biu.ac.il/?nlp/resources/downloads/implied-relationships.4 Recognition AlgorithmWe defined 15 features, summarized in Table 2,which capture local and discourse phenomena.These features do not depend on manually builtresources, and hence are portable to resource-poorlanguages.
Some features were proposed in priorworks, and are marked by G&C (Gerber and Chai,2012) or S&F (Silberer and Frank, 2012).
Our bestresults were obtained with the Random Forestslearning algorithm (Breiman, 2001).
The first twofeatures are described in the next subsection, whilethe others are explained in the table itself.4.1 Statistical discourse featuresStatistical features in prior works mostly cap-ture general properties of the predicate and the741# Category Feature Prev.
work1 co-occurring predicate (explained in subsection 4.1) New2statisticaldiscourse co-occurring argument (explained in subsection 4.1) New3 co-reference: whether an explicit argument of p co-refers with a. New4 last known location: If the NE of a is ?location?, and it is the lastlocation mentioned before p in the document.New5 argument prominence: The frequency of the lemma of a in a two-sentence windows of p, relative to all entities in that window.S&F6localdiscoursepredicate frequency in document: The frequency of p in the docu-ment, relative to all predicates appear in the document.G&C7 statistical argument frequency: The Unigram-model likelihood of ain English documents, calculated from a large corpus.New8 definite NP: Whether a is a definite NP G&C9 indefinite NP: Whether a is an indefinite NP G&C10 quantified predicate: Whether p is quantified (i.e., by expressionslike ?every .
.
.
?, ?a good deal of .
.
.
?, etc.
)G&C11localcandidatepropertiesNE mismatch: Whether a is a named entity but the correspondingargument in the hypothesis is not, or vice versa.New12 predicate-argument frequency: The likelihood of a to be an argu-ment of p (formally: Pr(a|p)) in a large corpus.similar featurein G&C13 sentence distance: The distance between p and a in sentences.
G&C, S&F14 mention distance: The distance between p and a in entity-mentions.
S&F15predicate-argumentrelatednessshared head-predicate: Whether p and a are themselves argumentsof another predicate.G&CTable 2: Algorithmic features.
p and a denote the candidate predicate and argument respectively.argument, like selectional preferences, lexicalsimilarities, etc.
On the contrary, our statis-tical features follow the intuition that explicitpredicate-argument relationships in the discourseprovide plausible indication that an impliedrelationship holds as well.
In our experimentswe collected the statistics from Reuters corpusRCV1 (trec.nist.gov/data/reuters/reuters.html), which contains more than806,000 documents.We defined two features: Co-occurring predi-cate and Co-occurring argument.
Let p and a bethe candidate predicate and the argument in thetext.
While they are not connected syntactically,each of them often has an explicit relationshipswith other terms in the text, that might support thesought (implied) relationship between a and p.More concretely, a is often an explicit argumentof another predicate p?.
For example, example 6 inTable 1 includes the explicit relationship ?derail-ment of train?, which might indicate the impliedrelationship ?crash of train?.
Hence p=?crash?,a=?train?
and p?=?derailment?.
The Co-occurringpredicate feature estimates the probability that adocument would contain a as an argument of p,given that a appears elsewhere in that documentas an argument of p?, based on explicit predicate-argument relationships in a large corpus.Similarly, the Co-occurring argument featurecaptures cases where p has another explicit argu-ment, a?.
This is exemplified in example 5 ofTable 1, where p=?renew?, a=?Patriot Act?
anda?=?law?.
Accordingly, the feature quantifies theprobability that a document including the relation-ship p-a?would also include the relationship p-a.More details about these features can be foundin the first author?s Ph.D. thesis at www.cs.biu.ac.il/?nlp/publications/theses/5 ResultsWe tested our method in a cross-validation setting,and obtained high result as shown in the first rowof Table 3.
Since our task and dataset are novel,there is no direct baseline with which we can com-pare this result.
As a reference point we mentionthe majority class proportion, and also report aconfiguration in which only features adopted fromprior works (G&C and S&F) are utilized.
This742Configuration Accuracy % ?
%Full algorithm 81.0 ?Union of prior work 78.0 3.0Major category (all true) 56.5 24.5Ablation testsno statistical discourse 79.9 1.1no local discourse 79.3 1.7no local candidate properties 79.2 1.8no predicate-argument relatedness 79.7 1.3Table 3: Accuracy of our method, followed bybaselines and ablation tests.Configuration (input) Recall Precision F1 %Explicit only 44.6 44.3 44.4Human annotations 50.9 43.4 46.8Algorithm recognition 48.5 42.3 45.2Table 4: RTE-6 Experimentcomparison shows that the contribution of our newfeatures (3%) is meaningful, which is also statis-tically significant with p < 0.01 using BootstrapResampling test (Koehn, 2004).
The high resultsshow that this task is feasible, and its solutionscan be adopted as a component in textual infer-ence systems.
The positive contribution of eachfeature category is shown in ablation tests.An additional experiment tests the contributionof recognizing implied predicate-argument rela-tionships for overall RTE, specifically on the RTE-6 dataset.
For the scope of this experiment we de-veloped a simple RTE system, which uses the F1optimized logistic regression classifier of Jansche(2005) with two features: lexical coverage andpredicate-argument relationships coverage.
Weran three configurations for the second feature,where in the first only syntactically expressed re-lationships are used, in the second all the impliedrelationships, as detected by a human annotator,are added, and in the third only the implied rela-tionships detected by our algorithm are added.The results, presented in Table 4, first demon-strate the full potential of the implied relation-ship recognition task to improve textual entail-ment recognition (Human annotation vs. Explicitonly).
One third of this potential improvement isachieved by our algorithm1.
Note that all these re-sults are higher than the median result in the RTE-6 challenge (36.14%).
While the delta in the F1score is small in absolute terms, such magnitudes1Following the relatively modest size of the RTE dataset,the Algorithm vs.
Explicit result is not statistically significant(p ' 0.1).
However, the Human annotation vs. Explicitresult is statistically significant with p < 0.01.are typical in RTE for most resources and tools(see (Bentivogli et al, 2010)).6 Discussion and ConclusionsWe formulated the task of recognizing impliedpredicate-argument relationships within textual in-ference scenarios.
We compared this task to thelabeling task of SemEval 2010, where no prior in-formation about candidate arguments in the text isavailable.
We point out that in textual inferencescenarios the candidate predicate and argumentare given by the Hypothesis, while the challengeis only to verify that a predicate-argument rela-tionship between these candidates is implied fromthe given Text.
Accordingly, some complex stepsnecessitated in the SemEval task can be avoided,while additional relevant cases are covered.Moreover, we have shown that this simpler taskis more feasibly solvable, where our 15 featuresachieved more than 80% accuracy.While our dataset and algorithm were presentedin the context of RTE, the same challenge andmethods are applicable to other textual inferencetasks as well.
Consider, for example, the Ques-tion Answering (QA) task.
Typically QA sys-tems detect a candidate predicate that matches thequestion?s predicate.
Similarly, candidate argu-ments, which match either the expected answertype or other arguments in the question are de-tected too.
Consequently, our methods which ex-ploit the availability of the candidate predicate andargument can be adapted to this scenario as well.Similarly, a typical approach for Event Extrac-tion (a sub task of Information Extraction) is tostart by applying an entity extractor, which identi-fies argument candidates.
Accordingly, candidatepredicate and arguments are detected in this sce-nario too, while the remaining challenge is to as-sess the likelihood that a predicate-argument rela-tionship holds between them.Following this observation, we propose futurework of applying our methods to other tasks.
Anadditional direction for future work is to furtherdevelop new methods for our task, possibly byincorporating SRL resources and/or linguisticallyoriented rules, in order to improve the results weachieved so far.AcknowledgmentsThis work was partially supported by the EC-funded project EXCITEMENT (FP7ICT-287923).743ReferencesLuisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2010.
The sixthpascal recognizing textual entailment challenge.
InProccidings of TAC.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1).Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
Machine Learning Challenges.
Evaluat-ing Predictive Uncertainty, Visual Object Classifi-cation, and Recognising Tectual Entailment, pages177?190.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In proceedings of COLING 2008 Work-shop on Cross-framework and Cross-domain ParserEvaluation.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of ACL.Matthew Gerber and Joyce Y. Chai.
2012.
Seman-tic role labeling of implicit arguments for nominalpredicates.
Computational Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Proceedings of NAACL.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of EMNLP.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11(1).Martin Jansche.
2005.
Maximum expected f-measuretraining of logistic regression models.
In Proceed-ings of EMNLP.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP.Egoitz Laparra and German Rigau.
2012.
Exploitingexplicit annotations and semantic types for implicitargument resolution.
In Proceedings of IEEE-ICSC.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
Nomlex: Alexicon of nominalizations.
In Proceedings of EU-RALEX.Michael Roth and Anette Frank.
2013.
Automaticallyidentifying implicit arguments to improve argumentlinking and coherence modeling.
In Proceedings of*SEM.Josef Ruppenhofer, Caroline Sporleder, RoserMorante, Collin Baker, and Martha Palmer.
2009.Semeval-2010 task 10: Linking events and theirparticipants in discourse.
In The NAACL-HLT2009 Workshop on Semantic Evaluations: RecentAchievements and Future Directions (SEW-09).Josef Ruppenhofer, Caroline Sporleder, RoserMorante, Collin Baker, and Martha Palmer.
2010.Semeval-2010 task 10: Linking events and theirparticipants in discourse.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation.Carina Silberer and Anette Frank.
2012.
Casting im-plicit role linking as an anaphora resolution task.
InProceedings of *SEM.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of NAACL.744
