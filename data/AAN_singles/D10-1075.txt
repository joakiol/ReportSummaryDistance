Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767?777,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsThe Necessity of Combining Adaptation MethodsMing-Wei Chang, Michael Connor and Dan RothUniversity of Illinois at Urbana ChampaignUrbana, IL 61801{mchang21,connor2,danr}@uiuc.eduAbstractProblems stemming from domain adaptationcontinue to plague the statistical natural lan-guage processing community.
There has beencontinuing work trying to find general purposealgorithms to alleviate this problem.
In thispaper we argue that existing general purposeapproaches usually only focus on one of twoissues related to the difficulties faced by adap-tation: 1) difference in base feature statisticsor 2) task differences that can be detected withlabeled data.We argue that it is necessary to combine thesetwo classes of adaptation algorithms, usingevidence collected through theoretical analy-sis and simulated and real-world data exper-iments.
We find that the combined approachoften outperforms the individual adaptationapproaches.
By combining simple approachesfrom each class of adaptation algorithm, weachieve state-of-the-art results for both NamedEntity Recognition adaptation task and thePreposition Sense Disambiguation adaptationtask.
Second, we also show that applying anadaptation algorithm that finds shared repre-sentation between domains often impacts thechoice in adaptation algorithm that makes useof target labeled data.1 IntroductionWhile recent advances in statistical modeling fornatural language processing are exciting, the prob-lem of domain adaptation remains a big challenge.It is widely known that a classifier trained on one do-main (e.g.
news domain) usually performs poorly ona different domain (e.g.
medical domain) (Jiang andZhai, 2007; Daume?
III, 2007).
The inability of cur-rent statistical models to handle multiple domains isone of the key obstacles hindering the progress ofNLP.Several general purpose algorithms have beenproposed to address the domain adaptation prob-lem: (Blitzer et al, 2006; Jiang and Zhai, 2007;Daume?
III, 2007; Finkel and Manning, 2009).
Itis widely believed that the drop in performance ofstatistical models on new domains is due to theshift of the joint distribution of labels and examples,P (Y,X), from domain to domain, where X repre-sents the input space and Y represents the outputspace.
In general, we can separate existing adap-tation algorithms into two categories:Focuses on P (X) This type of adaptation algo-rithm attempts to resolve the difference between thefeature space statistics of two domains.
While manydifferent techniques have been proposed, the com-mon goal of these algorithms is to find a bettershared representation that brings the source domainand the target domain closer.
Often these algorithmsdo not use labeled examples in the target domain.The works (Blitzer et al, 2006; Huang and Yates,2009) all belong to this category.Focuses on P (Y |X) These adaptation algorithmsassume that there exists a small amount of labeleddata for the target domain.
Instead of training twoweight vectors independently (one for source andthe other for the target domain), these algorithms tryto relate the source and target weight vectors.
This isoften achieved by using a special designed regular-ization term.
The works (Chelba and Acero, 2004;Daume?
III, 2007; Finkel and Manning, 2009) belongto this category.767It is important to give the definition of an adapta-tion framework.
An adaptation framework is speci-fied by the data/resources used and a specific learn-ing algorithm.
For example, a framework that usedonly source labeled examples and one that used bothsource and target labeled examples should be con-sidered as two different frameworks, even thoughthey might use exactly the same training algorithm.Note that the goal of a good adaptation framework isto perform well on the target domain and quite oftenwe only need to change the data/resource used to in-crease the performance without changing the train-ing algorithm.
We refer to frameworks that do notuse target labeled data and focus on P (X) as Unla-beled Adaptation Frameworks and refer to frame-works that use algorithms that focus on P (Y |X) asLabeled Adaptation Frameworks.The major difference between unlabeled adapta-tion frameworks and labeled adaptation frameworksis the use of target labeled examples.
Unlabeledadaptation frameworks do not use target labeled ex-amples1, while the labeled adaptation frameworksmake use of target labeled examples.
Under thisdefinition, we consider that a model trained on bothsource and target labeled examples (later referred asS+T) is a labeled adaptation framework.It is important to combine the labeled and unla-beled adaptation frameworks for two reasons:?
Mutual Benefit: We analyze these two typesof frameworks and find that they address dif-ferent adaptation issues.
Therefore, it is oftenbeneficial to apply them together.?
Complex Interaction: Another, probablymore important issue, is that these two typesof frameworks are not independent.
Differentrepresentations will impact howmuch a labeledadaptation algorithm can transfer informationbetween domains.
Therefore, in order to have aclear picture of what is the best labeled adapta-tion framework, it is necessary to analyze thesetwo domain adaptation frameworks together.In this paper, we assume we have both a smallamount of target labeled data and a large amount1Note that we still use labeled data from source domain inan unlabeled adaptation framework.of unlabeled data so that we can perform both unla-beled and labeled adaptation.
The goal of our paperis to point out the necessity of applying these twoadaptation frameworks together.
To the best of ourknowledge, this is the first paper that both theoreti-cally and empirically analyzes the interdependencebetween the impact of labeled and unlabeled adap-tation frameworks.The contribution of this paper is as follows:?
Propose a theoretical analysis of the ?Frustrat-ingly Easy?
(FE) framework (Daume?
III, 2007)(Section 3).The theoretical analysis shows that for FE to beeffective the domains must already be ?close?.At some threshold of ?closeness?
it is better toswitch from FE to just pool all training togetheras one domain.?
Demonstrate the complex interaction betweenunlabeled and labeled approaches (Section 4)We construct artificial experiments that demon-strate how applying unlabeled adaptation mayimpact the behavior of two labeled adaptationapproaches.?
Empirically analyze the interaction on realdatasets (Section 5).We show that in general combining both ap-proaches on the tasks of preposition sensedisambiguation and named entity recognitionworks better than either individual method.Our approach not only achieves state-of-the-art results on these two tasks but it also re-veals something surprising ?
finding a bet-ter shared representation often makes a sim-ple source+target approach the best adaptationframework in practice.2 Two Adaptation Aspects: A ReviewWhy do we need two types of adaptation frame-works?
First, unlabeled adaptation frameworks arenecessary since many features only exist in one do-main.
Therefore, it is important to develop algo-rithms that find features which work across domains.On the other hand, labeled adaptation frameworks768are also required because we would like to take ad-vantages of target labeled data.
Even though differ-ent domains may have different definitions for la-bels (say in named entity recognition, specific defi-nition of PER/LOC/ORG may change), labeled datashould still be useful.
We summarize these distinc-tions in Table 1.While these two aspects of adaptation both sawsignificant progress in the past few years, little anal-ysis has been done on the interaction between thesetwo types of algorithms2.In order to have a deep analysis, it is necessary tochoose specific adaptation algorithms for each as-pect of adaptation framework.
While we mainlyconduct analysis on the algorithms we picked, wewould like to point out that the necessity of com-bining these two types of adaptation algorithms hasbeen largely ignored in the community.As our example adaptation algorithms we se-lected:Labeled adaptation: FE framework One of themost popular adaptation frameworks that requiresthe use of labeled target data is the ?Frustrat-ingly Easy?
(FE) adaptation framework (Daume?
III,2007).
However, why and when this frameworkworks remains unclear in the NLP community.
TheFE framework can be viewed as an framework thatextends the feature space, and it requires source andtarget labeled data to work.
We denote n as thetotal number of features3 and m is the number ofthe ?domains?, where one of the domains is the tar-get domain.
The FE framework creates a globalweight vector in Rn(m+1), an extended space for alldomains.
The representation x of the t-th domainis mapped by ?t(x) ?
Rn(m+1).
In the extendedspace, the first n features consist of the ?shared?block, which is always active across all tasks.
The(t+1)-th block (the (nt+1)-th to the (nt+n)-th fea-tures) is a ?specific?
block, and is only active when2Among the previously mentioned work, (Jiang and Zhai,2007) is a special case given that it discusses both aspects ofadaptation algorithms.
However, little analysis on the interac-tion of the two aspects is discussed in that paper3We assume that the number of features in each domain isequal.extracting examples from the task t. More formally,?t(x) =264 x|{z}shared(t?1) blocksz }| {0 .
.
.0 x|{z}specific(m?t) blocksz }| {0 .
.
.0375 .
(1)A single weight vector w?
is obtained by training onthe modified labeled data {yti ,?t(xti)}mt=1.
Giventhat this framework only extends the feature space,in this paper, we also call it the feature extensionframework (still called FE).
We will see in Section 3that this framework is equivalent to applying a reg-ularization trick that bridges the source and the tar-get domains.
As it will become clear in Section 3,in fact, this framework is only effective when thereis target labeled data and hence belongs to labeledadaptation frameworks.Although FE framework is quite popular in thecommunity, there are other even simpler labeledadaptation frameworks that allow the use of tar-get labeled data.
For example, one of the simplestframeworks is the S+T framework, which simplytrains a single model on the pooled and unextendedsource and target training data.Unlabeled adaptation: Adding cluster-like fea-tures Recall that unlabeled adaptation frameworksfind the features that ?work?
across domain.
In thispaper, we find such features in two steps.
First,we use word clusters generated from unlabeled textand/or third party resources that spans domains.Then, for every feature template that contains aword, we append another feature template that usesthe word?s cluster instead of the word itself.
Thistechnique is used in many recent works includingdependency parsing and NER (Koo et al, 2008;Ratinov and Roth, 2009).
Note that the unlabeledtext need not come from the source or target do-main.
In fact, in this paper, we use clusters gen-erated with the Reuters 1996 dataset, a superset ofthe CoNLL03 NER dataset (Koo et al, 2008; Liang,2005).
We adopt the Brown cluster algorithm to findthe word cluster (Brown et al, 1992; Liang, 2005).We can use other resources to create clusters as well.For example, in the NER domain, we also includegazetteers4 as an unlabeled cluster resource, whichcan bring the domains together quite effectively.4Our gazetteers comes from (Ratinov and Roth, 2009).769Framework Labeled Data Unlabeled Data Common ApproachUnlabeled Adaptation(Focus on P (X))Source Encompasses Source and Target.May use other third party resources(dictionaries, gazetteers, etc.
).Generate features that span domains us-ing unlabeled data and/or third party re-sources.Labeled Adaptation(Focus on P (Y |X))Source and Target None Train classifier(s) using both source andtarget training data, relating the two.Table 1: Comparison between two general adaptation frameworks discussed in this paper.
Each framework is specified by its setting(data required) and its learning algorithm.
Multiple previous adaptation approaches fit in one of either framework.While other more complex algorithms (Ando andZhang, 2005; Blitzer et al, 2006) for finding bet-ter shared representation (without using labeled tar-get data) have been proposed, we find that usingstraightforward clustering features is quite effectivein general.3 Analysis of the FE FrameworkIn this section, we propose a simple yet informativeanalysis of the FE algorithm from the perspective ofmulti-task learning.
Note that we ignore the effectof unlabeled adaptation in this section, and focus onthe analysis of the FE framework as a representativelabeled adaptation framework.3.1 Mistake Bound AnalysisWhile (Daume?
III, 2007) proposed this frameworkfor adaptation, a very similar idea had been proposedin (Evgeniou and Pontil, 2004) as a novel regular-ization term for multitask learning with support vec-tor machines.
Assume that w1,w2, .
.
.
,wm are theweight vector for the first domain to the m-th do-main, respectively.
The baseline approach is to as-sume that each weight vector is independent.
As-sume that we adopt a SVM-like optimization prob-lem that consider all m tasks, the baseline approachis equivalent to using the following regularizationterm in the objective function:?mt=1 ?wt?2.In (Evgeniou and Pontil, 2004; Daume?
III, 2007),they assume that wt = u + vt, for t = 1, .
.
.m,where vt is the specific weight vector for t-th do-main and u is a shared weight vector across all do-mains.
The new regularization term then becomes?u?2 +m?t=1?vt?2.
(2)Note that these two regularization terms are differ-ent, given that the new regularization term makesw1,w2, .
.
.
,wm not independent anymore.
It fol-lows thatwTt x = (u+ vt)Tx = w?T?t(x),wherew?T =[uT vT1 .
.
.
vTm].and ?w?
?2 equals to Eq.
(2).
Therefore, we can thinkfeature extension framework as a learning frame-work that adopts Eq.
(2) as its regularization term.The FE framework was in fact originally designedfor the problem of multitask learning so in the fol-lowing, we propose a simple mistake bound analysisbased on the multitask setting, where we calculatethe mistakes on all domains5.
We focus on multi-task setting for two reasons: 1) the analysis is veryeasy and intuitive, and 2) in Section 4.1, we empiri-cally confirm that the analysis holds for the adapta-tion setting.In the following, we assume that the trainingalgorithm used in the FE framework is the on-line perceptron learning algorithm (Novikoff, 1963).This allows us to analyze the mistake bound of theFE framework with the perceptron algorithm.
Thebound can give us an insight on when and why oneshould adopt the FE framework.
By using the stan-dard mistake bound theorem (Novikoff, 1963), weshow:Theorem 1.
Let Dt be the labeled data of domain t.Assume that there exist w1,w2, .
.
.
,wm such thatywTt x ?
?,?
(x, y) ?
Dt,and assume that max(x,y)?Dt ?x?
?
R2,?t =1 .
.
.m.
Then, the number of mistakes made withonline perceptron training (Novikoff, 1963) and the5In the adaptation setting, one generally only cares about theperformance on the target domain.770FE framework is bounded by2R2?2(m?t=1?wt?2 ??
?mt=1 wt?2m + 1).
(3)Proof.
Define w?
as a vector in Rn(m+1).
We claimthat there exists a set Sw?
such that for all w?
?
Sw?,w?T?t(x) = wTt x for any domain t = 1 .
.
.m.
Notethat?t(x) is defined in Eq.
(1).
We can construct Sw?in the following way:Sw?
= {[s (w1 ?
s) .
.
.
(wm ?
s)]| s ?
Rn},where s is an arbitrary vector with n elements.In order to obtain the best possible bound, wewould like to find the most compressed weight vec-tor in Sw?, w?
= minw??Sw?
?w?
?2.The optimization problem has an analytical solu-tion:?w?
?2 =m?t=1?wt?2 ?
?m?t=1wt?2/(m + 1).The proof is completed by the standard mis-take bound theorem and the following fact:maxx ?
?t(x)?2 = 2maxx ?x?2 ?
2R2.3.2 Mistake Bound ComparisonIn the following, we would like to explore underwhat circumstances the FE framework can work bet-ter than individual models and the S+T frameworkusing Theorem 1.
The analysis is done based on theassumption that all frameworks use the perceptronalgorithm.Before showing the bound analysis, note that theframework proposed by (Evgeniou and Pontil, 2004;Finkel and Manning, 2009) is a generalization overthese three frameworks (FE, S+T, and the base-line)6.
However, our goal in this paper is different:we try to provide a deep discussion onwhen and whyone should use a particular framework.Here, we compare the mistake bounds of the fea-ture sharing framework to that of the baseline ap-proach, which learns each task independently7.
In6The framework proposed by (Evgeniou and Pontil, 2004;Finkel and Manning, 2009) is a generalization of Eq.
(1).
Itallows the user to weight each block of features.
If we put zeroweight on the shared block, it becomes the baseline approach.On the other hand, if we put zero weight on all task-specificblocks, the framework becomes the S+T approach.7Note that mistake bound results can be generalized to gen-eralization bound results.
See (Zhang, 2002).order to make the comparison easier, we make somesimplifying assumptions.
First, we assume that theproblem contains only two tasks, 1 and 2.
We alsoassume that ?w1?
= ?w2?
= a.
These assump-tions greatly reduce the complexity of the analysisand can give us greater insight into the comparisons.Following the assumptions and Theorem 1, themistake bound for the FE frameworks is4(2?
cos(w1,w2))R2a2/(3?2) (4)This line of analysis leads to interesting bound com-parisons for two cases.
In the first case, we assumethat task 1 and task 2 are essentially the same.
In thesecond, more common case, we assume that they aredifferent.First, when we know a priori that task 1 and task2 are essentially the same, we can combine the train-ing data from the two tasks and train them as a sin-gle task.
Therefore, given that we do not need toexpand the feature space, the number of mistakes isnow bounded by R2a2/?2.
Note that this bound isin fact better than (4) with cos(w1,w2) = 1.
There-fore, if we know a priori that these two tasks are thesame, training a single model is better than using thefeature shared approach.In practice, it is often the case that the two tasksare not the same.
In this case, the number of mis-takes of an independent approach on both task 1 and2 will be bounded by the summation of the mistakebounds of task 1 and task 2.
Therefore, using theindependent approach, the number of mistakes forthe perceptron algorithm on both tasks is boundedby 2R2a2/?2.
The following results can be obtainedby directly comparing the two bounds,Corollary 1.
Assume there exists w1 and w2 whichseparate D1 and D2 respectively with functionalmargin ?, and ?w1?
= ?w2?
= a.
In this case:(4) will be smaller than the bound of individual ap-proach, 2R2a2/?2, if and only if cos(w1,w2) =(wT1 w2)/(?w1??w2?)
>12 .If we assume that there is no difference inP (X) between domains and hence we can treatcos(w1,w2) as the similarity between two tasks, theabove argument suggests:?
If the two tasks are very different, the baselineapproach (building two models) is better thanFE and S+T.771?
If the tasks are similar enough, FE is better thanbaseline and S+T.?
If the tasks are almost the same, S+T becomesbetter than FE and baseline.In Section 4.1, we will evaluate whether these claimscan be justified empirically.4 Artificial Data Experiment StudyIn this section we will present artificial experiments.We have two primary goals: 1) verifying the analysisproposed in Section 3, and 2) showing that the repre-sentation shift will impact the behavior of the FE al-gorithm.
The second point will be verified again inthe real world experiments in Section 5.Data Generation In the following artificial ex-periments we experiment with domain adaptationby generating training and test data for two tasks,source and target, where we can control the differ-ence between task definitions.
The general proce-dure can be divided into two steps: 1) generatingweight vectors z1 and z2 (for source and target re-spectively), and 2) randomly generating labeled in-stances for training and testing using z1 and z2.The different experiments start with the same ba-sic z1 and z2, but then may alter these weights tointroduce task dissimilarities or similarities.
The ba-sic z1 and z2 are both generated by a multivariateGaussian distribution with mean z and a diagonalcovariance matrix ?I:z1 ?
N (z, ?I), z2 ?
N (z, ?I),where N is the normal distribution and z is randomvector with zero mean.
Note that z is only used togenerate z1 and z2.
There is one parameter, ?, thatcontrols the variance of the Gaussian distribution.Hence we use ?
to roughly control the ?angle?
of z1and z2.
When ?
is close to zero, z1 and z2 will bevery similar.
On the other hand, when ?
is large, z1and z2 can be very different.
In these experiments,we vary ?
between 0.01 and 5 so that we are exper-imenting only with tasks where the weight the taskdifference is the ?angle?
or cosine between z1 andz2.
Once we obtain the z1 and z2, we normalizethem to the unit length.After selecting z1 and z2, we then generate la-beled instances (x, y) for the source task in the fol-lowing way.
For each example x, we randomly gen-erate n binary features, where each feature has 20%chance to be active.
We then label the example byy = sign(zT1 x),The data for the target task is generated similarlywith z2.
In these experiments, we fix the number offeatures n to be 500 and generate 100 source train-ing examples and 40 target training examples, alongwith 1000 target testing examples.
This matches thereasonable case in NLP where there are more fea-tures than training examples and each feature vectoris sparse.
In all of the experiments, we report theaveraged testing error rate on the target testing data.4.1 Experiment 1, FE algorithmGoal The goal here is to verify our theoreticalanalysis in Section 3.
Note that we do not introducerepresentation shift in this experiment and assumethat both source and target domains use exactly thesame features.Result Figure 1(a) shows the performance of thethree training algorithms as variance decreases andthus cosine between weight vectors (or measure oftask similarity) goes to 1.
Note that FE labeled adap-tation framework beats TGT once the task cosinepasses approximately 0.6.
Initially FE slightly out-performs S+T until the tasks are close enough to-gether that it is better to treat all the data as comingfrom one task.
Note that while the experiments arebased on the adaptation setting, the results match ouranalysis based on the multitask setting in Section 3.4.2 Experiment 2, Unseen FeaturesGoal So far we have not considered the differencein P (X) between domains.
In the previous exper-iment, we used only cosine as our task similaritymeasurement to decide what is the best framework.However, task similarity should consider the differ-ence in both P (X) and P (Y |X), and the cosinemeasurement is not sufficient for this.
Here we con-struct a simple example to show that even a simplerepresentation shift can change the behavior of thelabeled adaptation framework.
This case shows thatS+T can be better than FE even when the tasks arenot similar according to the cosine measurement.7720.30.320.340.360.380.40.420.440.460  0.2  0.4  0.6  0.8  1Target%ErrorCosineTgtS+TFE(a) Basic Similarity0.320.3250.330.3350.340.3450.350.3550.360  0.2  0.4  0.6  0.8  1Target%ErrorOriginal CosineTgtS+TFE(b) Shared FeaturesFigure 1: Artificial Experiment comparing labeled adaptation performance vs. cosine between base weight vectors that definestwo tasks, before and after cross-domain shared features are added.
Figure (a) shows results from experiment 1.
For FE adaptationalgorithm to work the tasks need to be close (cosine > 0.6), and if the tasks are close enough (cosine ?
1, dividing line) thenit is better to just pool source and target training data together (the S+T algorithm).
Figure (b) shows results for experiment 3when shared features are added to the base weight vectors as used in experiment 1.
Here the cosine similarity measure is betweenthe base task weight vectors before the shared features have been added.
Both labeled adaptation algorithms effectively use theshared features to improve over just training on target.
With shared features added the dividing line where S+T improves overFE decreases so even for tasks that are initially further apart, once clusters are added the S+T algorithm does better than FE.
Eachpoint represents the average of 2000 training runs with random initial z1 and z2 generating data.Result The second experiment deals with the casewhere features may appear in only one domain butshould be treated like known features in the otherdomain.
An example of this are out of vocabularywords that may not exist in a small target train-ing task, but have synonyms in the source train-ing data.
In this case if we had features groupingwords (say by word meanings) then we would re-cover this cross-domain information.
In this experi-ment we want to explore which adaptation algorithmperforms best before these features are applied.To simulate this case we start with similar weightvectors z1 and z2 (sampled with variance = 0.00001,cos(z1,z2) ?
1), but then shift some set of dimen-sions so that they represent features that appear onlyin one domain.z1 = (a1,b1) ?
z?1 = (0,b1,a1)z2 = (a2,b2) ?
z?2 = (a2,b2,0)By changing the ratio of the size of the dissimilarsubset a to the similar subset b we can make thetwo weight vectors z?1 and z?2 more or less similar.Using these two new weight vectors we can proceedas above, generating training and testing data.Figure 2 shows the performance of the three algo-0.3150.320.3250.330.3350.340.3450.350.3550  0.2  0.4  0.6  0.8  1Target%ErrorCosineTgtS+TFEFigure 2: Artificial Experiment where unknown features areincluded in source or target domains, but not the other.
Thesimple S+T adaptation framework is best able to exploit theset of shared features so performs best over the whole space ofsimilarity in this setting.rithms on this data as the number of unrelated fea-tures are decreased.
Over the entire range the com-bined algorithm S+T does better since it more ef-ficiently exploits the shared similar b subset of thefeature space.
When the FE algorithm tries to cre-ate the shared features, it considers both the similarsubset b and dissimilar subset a.
However, sincea should not be shared, FE algorithm becomes less773effective than the S+T algorithm.
See the boundcomparison in Section 3.2 for more intuitions.
Withthis experiment we have demonstrated that there isa need to consider label and unlabeled adaptationframeworks together.4.3 Experiment 3, Shared FeaturesGoal A good unlabeled adaptation frameworkshould try to find features that ?work?
across do-mains.
However, it is not clear how these newlyadded features will impact the behavior of the la-beled adaptation frameworks.
In this experiment, weshow that the new shared features will bring the do-mains together, and hence make S+T a very strongadaptation framework.Result For the third experiment we start with thesame setup as in the first experiment, but then aug-ment the initial weight vector with additional sharedweights.
These shared weights correspond to the in-troduction of features that appear in both domainsand have the same meaning relative to the tasks, theideal result of unlabeled adaptation methods.To generate this case we again start with z1 andz2 of varying similarity as in section 4.1, then gen-erate a random weight vector for shared features andappend this to both weight vectors.zs ?
N (0, I), z?
?1 = (z1, ?zs), z?
?2 = (z2, ?zs),where ?
is used to put increased importance on theshared weight vectors by increasing the total weightof that section relative to the base z1 and z2 subsets.In our experiments we use 100 shared features to the500 base features and set ?
to 2.Figure 1(b) shows the performance of the labeledadaptation algorithms once shared features had beenadded.
Here the x-axis is the cosine between theoriginal task weight vectors, demonstrating how theshared features improve performance on potentiallydissimilar tasks.
Whereas in the first experimentFE does not improve over just training on target datauntil the cosine is greater than 0.6, once shared fea-tures have been added then both FE and S+T usethese features to learn with originally dissimilartasks.
Furthermore the shared features tend to pushthe tasks ?closer?
so that S+T improves over FE ear-lier.
Comparing to Figure 1(a), there are regionswhere before shared features are added it is betterto use FE, and after shared features are added it isbetter to use S+T.
This shows that labeled adapta-tion and unlabeled are not independent.
Therefore,it is important to combine these two aspects to seethe real contribution of each adaptation framework.In these three artificial experiments we havedemonstrated cases where both FE or S+T arethe best algorithm before and after representationchanges like those created with unlabeled adaptationare imposed.
This fact points to the perhaps obvi-ous conclusion that there is not a single best adapta-tion algorithm, and the determination of specific bestpractices depends on task similarity (in both P (X)and P (Y |X)), especially after being brought closertogether with other adaptation approaches.
If thereis one common trend it is that often once two taskshave been brought close together using a shared rep-resentation, then the tasks are now close enoughsuch that the simple S+T algorithm does well.5 Real World ExperimentsIn Section 4, we have shown through artificial dataexperiments that labeled and unlabeled adaptationalgorithms are not independent.
In this section, wefocus on experiments with real datasets.For the labeled adaptation algorithms, we have thefollowing options:?
TGT: Only uses target labeled training dataset.?
FE: Uses both labeled datasets.?
FE+: Uses both labeled datasets.
A modifica-tion of the FE algorithm, equivalent to multi-plying the ?shared?
part of the FE feature vec-tor (Eq.
(1)) by 10 (Finkel andManning, 2009).?
S+T: Uses both source and target labeleddatasets to train a single model with all labeleddata directly.Throughout all of our experiments, we use SVMstrained with a modified java implementation8 ofLIBLINEAR as our underlying learning classi-fier (Hsieh et al, 2008).
For the tasks that requirestructures, we model each individual decision using8Our code is modified from the version available on http://www.bwaldvogel.de/liblinear-java/774Algorithm TGT FE FE+ S+TSRC labeled data?
no yesTarget labeled data Token F1(a) MUC7 Dev 58.6 70.5 74.3 73.1(a) + cluster 77.5 82.5 83.3 83.3(b) MUC7 Train 73.0 78.2 80.1 78.7(b) + cluster 85.4 86.4 86.2 86.5Table 2: NER Experiments.
We bold face the best accuracyin a row and underline the runner up.
Both unlabeled adapta-tion algorithms (adding cluster features) and labeled adaptationalgorithm (using source labeled data) help the performance sig-nificantly.
Moreover, adding cluster-like features also changesthe behavior of the labeled adaptation algorithms.
Note thatafter adding cluster features, S+T becomes quite competitivewith (or slightly better than) the FE+ approach.
The size ofMUC7 develop set is roughly 20% of the size of the MUC7training set.a local SVM classifier then make our prediction us-ing a greedy approach from left to right.
While wecould use a more complex model such as Condi-tional Random Field (Lafferty et al, 2001), as wewill see later, our simple model generates state-of-the-art results for many tasks.
Regarding parameterselection, we selected the SVM regularization pa-rameter for the baseline model (TGT) and then fix itfor all algorithms9.Named Entity Recognition Our first task isNamed Entity Recognition (NER).
The source do-main is from the CoNLL03 shared task (TjongKim Sang and De Meulder, 2003) and the target do-main is from the MUC7 dataset.
The goal of thisadaptation system is to maximize the performanceon the test data of MUC7 dataset with CoNLL train-ing data and (some) MUC7 labeled data.
As an unla-beled adaptation method to address feature sparsity,we add cluster-like features based on the gazetteersand word clustering resources used in (Ratinov andRoth, 2009) to bridge the source and target domain.We experiment with both MUC development andtraining set as our target labeled sets.The experimental results are in Table 2.
First, no-tice that addressing the feature sparsity issue helpsthe performance significantly.
Adding cluster-like9We use L2-hinge loss for all of the experiments, withC = 2?4 for NER experiments and C = 2?5 for the PSDexperiments.features improves the Token-F1 by around 10%.
Onthe other hand, adding target labeled data also helpsthe results significantly.
Moreover, using both tar-get labeled data and cluster-like shared representa-tion are mutually beneficial in all cases.Importantly, adding cluster-like features changesthe behavior of the labeled adaptation algorithms.When the cluster-like features are not added, theFE+ algorithm is in general the best labeled adap-tation framework.
This result agrees with the re-sults showed in (Finkel and Manning, 2009), wherethe authors show that FE+ is the best labeled adap-tation framework in their settings.
However, afteradding the cluster-like features, the simple S+T ap-proach becomes very competitive to both FE andFE+.
This matches our analysis in Section 4: re-solving features sparsity will change the behavior oflabeled adaptation frameworks.We compare the simple S+T algorithm withcluster-like features to other published results onadapting from CoNLL dataset to MUC7 dataset intable 3.
Past works on this setting often only fo-cus on one class of adaption approach.
For example,(Ratinov and Roth, 2009) only use the cluster-likefeatures to address the feature sparsity problem, and(Finkel and Manning, 2009) only use target labeleddata without using gazetteers and word-cluster in-formation.
Notice that because of combining twoclasses of adaption algorithms, our approach is sig-nificantly better than these two systems10.Preposition Sense Disambiguation We also testthe combination of unlabeled and labeled adaptionon the task of Preposition Sense Disambiguation.Here the data contains multiple prepositions whereeach preposition has many different senses.
Thegoal is to predict the right sense for a given prepo-sition in the testing data.
The source domain is theSemEval 2007 preposition WSD Task and the targetdomain is from the dataset annotated in (Dahlmeieret al, 2009).
Our feature design mainly comesfrom (Tratz and Hovy, 2009) (who do not evalu-ate their system on our target data).
As our un-10The work (Ratinov and Roth, 2009) also combines theirsystem with several document-level features.
While it is possi-ble to add these features in our system, we do not include anyglobal features for the sake of simplicity.
Note that our sys-tem is competitive to (Ratinov and Roth, 2009) even though oursystem does not use global features.775Systems Cluster?
TGT?
P.F1 T.F1Our NER y y 84.1 86.5FM09 n y 79.98 N/ARR09 y n N/A 83.2RR09 + global y n N/A 86.2Table 3: Comparisons between different NER systems.
P.F1and T.F1 represent the phrase-level and token-level F1 score,respectively.
We use ?Cluster??
to indicate if cluster featuresare used and use ?TGT??
to indicate if target labeled data isused.
Previous systems often only use one class of adaptationalgorithms.
Using both adaptation aspects makes our systemperform significantly better than FM09 and RR09.Algorithm TGT FE FE+ S+TSRC labeled data?
no yesTarget labeled data Accuracy10% Tgt 43.8 48.2 51.3 49.710% Tgt + Cluster 44.9 50.5 51.8 52.0100% Tgt 59.5 60.5 60.3 61.2100% Tgt + Cluster 61.3 62.0 61.2 62.1Table 4: Preposition Sense Disambiguation.
We mark the bestaccuracy in a row using the bold font and underline the runnerup.
Note that both adding cluster features and adding source la-beled data help the performance significantly.
Moreover, addingclusters also changes the behavior of the labeled adaptation al-gorithms.labeled adaptation approach we augment all wordbased features with cluster information from sepa-rately generated hierarchical Brown clusters (Brownet al, 1992).The experimental results are in Table 4.
Note thatwe see phenomena similar to what happened in theNER experiments.
First, both labeled and unlabeledadaptation improves the system.
When only 10% ofthe target labeled data is used, the inclusion of thesource labeled data helps significantly.
When thereis more labeled data, labeled and unlabeled adaptionhave similar impact.
Again, using unlabeled adap-tion changes the behavior of the labeled adaption al-gorithms.In Table 5, we compare our system to (Dahlmeieret al, 2009), who do not use the SemEval data butjointly train their preposition sense disambiguationsystem with a semantic role labeling system.
Withboth labeled and unlabeled adaption, our system issignificantly better.Systems ACCOur PSD (S+T and cluster) 62.1DNS09 56.5DNS09 + SRL 58.8Table 5: Comparison between different PSD systems.
Notethat after adding cluster features and source labeled data withS+T approach, our system outperforms the state-of-the-art sys-tem proposed in (Dahlmeier et al, 2009), even though theyjointly learn a PSD and SRL system together.6 ConclusionIn this paper, we point out the necessities of com-bining labeled and unlabeled adaptation algorithms.We analyzed the FE algorithm both theoreticallyand empirically, demonstrating that it requires botha minimal amount of task similarity to work, andpast a certain level of similarity other, simpler ap-proaches are better.
More importantly, through arti-ficial data experiments we found that applying unla-beled adaptation algorithms may change the behav-ior of labeled adaptation algorithms as representa-tions change, and hence affect the choice of labeledadaptation algorithm.
Experiments with real-worlddatasets confirmed that combinations of both adap-tation methods provide the best results, often allow-ing the use of simple labeled adaptation approaches.In the future, we hope to develop a joint algorithmwhich addresses both labeled and unlabeled adapta-tion at the same time.Acknowledgment We thank Vivek Srikumar for provid-ing the baseline implementation of preposition sense disam-biguation.
We also thank anonymous reviewers for their use-ful comments.
University of Illinois gratefully acknowledgesthe support of Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL, or the USgovernment.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
J. Mach.
Learn.
Res.776John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,and R. L. Mercer.
1992.
Class-Based n-gram Modelsof Natural Language.
Computational Linguistics.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
In Dekang Lin and Dekai Wu, editors, EMNLP.D.
Dahlmeier, H. T. Ng, and T. Schultz.
2009.
Jointlearning of preposition senses and semantic roles ofprepositional phrases.
In EMNLP.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.T.
Evgeniou and M. Pontil.
2004.
Regularized multi?task learning.
In KDD.J.
R. Finkel and C. D. Manning.
2009.
Hierarchicalbayesian domain adaptation.
In NAACL.C.-J.
Hsieh, K.-W. Chang, C.-J.
Lin, S. S. Keerthi, andS.
Sundararajan.
2008.
A dual coordinate descentmethod for large-scale linear svm.
In ICML.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence-labeling.
In ACL.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In ACL.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In ACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.A.
Novikoff.
1963.
On convergence proofs for percep-trons.
In Proceeding of the Symposium on the Mathe-matical Theory of Automata.L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Proc.of the Annual Conference on Computational NaturalLanguage Learning (CoNLL).Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In WalterDaelemans and Miles Osborne, editors, Proceedingsof CoNLL-2003.S.
Tratz and D. Hovy.
2009.
Disambiguation of prepo-sition sense using linguistically motivated features.
InNAACL.Tong Zhang.
2002.
Covering number bounds of certainregularized linear function classes.
J. Mach.
Learn.Res.777
