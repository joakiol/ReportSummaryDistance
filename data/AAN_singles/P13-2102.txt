Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?577,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA System for Summarizing Scientific Topics Starting from KeywordsRahul JhaDepartment of EECSUniversity of MichiganAnn Arbor, MI, USArahuljha@umich.eduAmjad Abu-JbaraDepartment of EECSUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduDragomir RadevDepartment of EECS andSchool of InformationUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractIn this paper, we investigate the problemof automatic generation of scientific sur-veys starting from keywords provided bya user.
We present a system that can takea topic query as input and generate a sur-vey of the topic by first selecting a setof relevant documents, and then selectingrelevant sentences from those documents.We discuss the issues of robust evalua-tion of such systems and describe an eval-uation corpus we generated by manuallyextracting factoids, or information units,from 47 gold standard documents (surveysand tutorials) on seven topics in NaturalLanguage Processing.
We have manuallyannotated 2,625 sentences with these fac-toids (around 375 sentences per topic) tobuild an evaluation corpus for this task.We present evaluation results for the per-formance of our system using this anno-tated data.1 IntroductionThe rise of the number of publications in all sci-entific fields is making it more and more difficultto get quickly acquainted with the new develop-ments in a new area.
One way to wade through thishuge amount of scholarly information is to consulttopical surveys written by experts in an area.
Forexample, for machine translation, one might read(Lopez, 2008)1.
Such surveys can be very help-ful when available, but unfortunately, may not beavailable for all areas.
Additionally, the manualsurveys quickly go out of date within a few yearsof publication as additional papers are publishedin the field.1Adam Lopez.
2008.
Statistical machine translation.ACM Comput.
Surv.
40, 3, Article 8Thus, a system that can generate such surveysautomatically would be a useful tool.
Short sum-maries in the form of abstracts are available forindividual papers, but no such information is avail-able for scientific topics.
In this paper, we ex-plore strategies for generating and evaluating suchsurveys of scientific topics automatically startingfrom a phrase representing a topic area.
We evalu-ate our system on a set of topics in the field of Nat-ural Language Processing.
In earlier work, (Teufeland Moens, 2002) have examined the problemof summarizing scientific articles using rhetoricalanalysis of sentences.
Nanba and Okumura (1999)have also discussed the problem of generating sur-veys of multiple papers.
Mohammad et al (2009)presented experiments on generating surveys ofscientific topics starting from papers to be summa-rized.
More recently, Hoang and Kan (2010) havepresented initial results on automatically generat-ing related work section for a target paper by tak-ing a hierarchical topic tree as an input.In this paper, we tackle the more challengingproblem of summarizing a topic starting from atopic query.
Our system takes as an input a stringdescribing the topic area, selects the relevant pa-pers from a corpus of papers, and then selects sen-tences from the citing sentences to these papers togenerate a survey of the topic.
A sample output ofour system for the topic of ?Word Sense Disam-biguation?
is shown in Figure 1.2 Candidate Document SelectionGiven a query representing the topic to be sum-marized, our first task is to find the set of rele-vant documents from the corpus.
The simplestway to do this for a corpus of scientific publica-tions is to do a query search using exact match ora standard TF*IDF system such Lucene, rank thedocuments using either citation counts or pager-ank in the bibliometric citation network, and se-lect the top n documents.
However, comparing572Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or atagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al , 1998), and semi-supervised sense disambiguation(Yarowsky, 1995).Most researchers working on word sense disambiguation (WSD) use manually sense tagged data such as SemCor (Miller et al , 1993) to train statisticalclassifiers, but also use the information in SemCor on the overall sense distribution for each word as a backoff model.Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation.Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002;Mihalcea and Moldovan, 2001; Yarowsky et al , 2001).For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al , 1991; Dagan, 1991; Dagan and Itai, 1994;Ide, 2000; Resnik and Yarowsky, 1999).Figure 1: A sample output survey of our system on the topic of ?Word Sense Disambiguation?
producedby paper selection using Restricted Expansion and sentence selection using Lexrank.
In our evaluations,this survey achieved a pyramid score of 0.82 and Unnormalized RU score of 0.31.Document selection algorithm CG5 CG10 CG20Title match sorted with citation count 1.82 2.75 3.29Title match sorted with pagerank 1.77 2.55 3.34Citation expansion sorted with citationcount 0.53 1.20 2.29Citation expansion sorted with pagerank 0.20 0.78 1.99TF*IDF ranked 0.14 0.14 0.56TF*IDF sorted with citation count 0.44 2.25 3.18TF*IDF sorted with pagerank 1.54 2.22 2.85Restricted Expansion 2.52 3.91 6.01Table 1: Comparison of different methods fordocument selection by measuring the CumulativeGain (CG) of top 5, 10 and 20 results.the results of these techniques with the papers cov-ered by gold standard surveys on a few topics, wefound that some important papers are missed bythese simple approaches.
One reason for this isthat early papers in a field might use non-standardterms in the absence of a stable, accepted termi-nology.
Some early Word Sense Disambiguationpapers, for example, refer to the problem as Lex-ical Ambiguity Resolution.
Additionally, papersmight use alternative forms or abbreviations oftopics in their titles and abstracts, e.g.
for inputquery ?Semantic Role Labelling?, papers such as(Dahlmeier et al, 2009) titled ?Joint Learning ofPreposition Senses and Semantic Roles of Prepo-sitional Phrases?
and (Che and Liu, 2010) titled?Jointly Modeling WSD and SRL with MarkovLogic?
might be missed.To find these papers, we add a simple heuristiccalled Restricted Expansion.
In this method, wefirst create a base set B, by finding papers with anexact match to the query.
This is a high precisionset since a paper with a title that contains the ex-act query phrase is very likely to be relevant to thetopic.
We then find additional papers by expand-ing in the citation network around B, that is, byfinding all the papers that are cited by or cite thepapers in B, to create an extended set E. Fromthis combined set (B ?E), we create a new set Fby filtering out the set of papers that are not citedby or cite a minimum threshold tinit of papers inB.
If the total number of papers is lower than fminor higher than fmax, we iteratively increase or de-crease t till fmin ?
|F | ?
fmax.
This methodallows us to increase our recall without losing pre-cision.
The values for our current experiments are:tinit = 5, fmin = 150, fmax = 250.Authors Year SizeSurveysACL Wiki 2012 4Roberto Navigli 2009 68Eneko Agirre; Philip Edmonds 2006 28Xiaohua Zhou; Hyoil Han 2005 6Nancy Ide; Jean Vronis 1998 41TutorialsSanda Harabagiu 2011 45Diana McCarthy 2011 120Philipp Koehn 2008 17Rada Mihalcea 2005 186Table 2: The set of surveys and tutorials col-lected for the topic of ?Word Sense Disambigua-tion?.
Sizes for surveys are expressed in numberof pages, sizes for tutorials are expressed in num-ber of slides.To evaluate different methods of candidate doc-ument selection, we use Cumulative Gain (CG),where the weight for each paper is estimated bythe fraction of surveys it appears in.
Table 1shows the average Cumulative Gain of top 5, 10and 20 documents for each of eight methods wetried.
Restricted Expansion outperformed everyother method.
Once we obtain a set of papers tobe summarized, we select the top n most cited pa-pers in the document set as the papers to be sum-marized, and extract the set of citing sentences Sfrom all the papers in the document set to these npapers.
S is the input for our sentence selectionalgorithms, described in Section 4.573Factoid S1 S2 S3 S4 S5 T1 T2 T3 T4 Factoid Weightdefinition of wsd X X X X X X X X X 9wordnet X X X X X X X X 8knowledge based wsd X X X X X X X 7supervised wsd X X X X X X X 7senseval X X X X X X X 7definition of word senses X X X X X X 7knowledge based wsd using machine readable dictionaries X X X X X X 6unsupervised wsd X X X X X X 6bootstrapping algorithms X X X X X X 6supervised wsd using decision lists X X X X X X 6Table 3: Top 10 factoids for the topic of ?Word Sense Disambiguation?
and their distribution acrossvarious data sources.3 Evaluation Data for Survey GenerationWe use the ACL Anthology Network (AAN) as thecorpus for our experiments (Radev et al, 2013).We built a factoid inventory for seven topics inNLP based on manual written surveys in the fol-lowing way.
For each topic, we found at least 3recent tutorials and 3 recent surveys on the topicand extracted the factoids that are covered in eachof them.
Table 2 shows the complete list of ma-terial collected for the topic of ?Word Sense Dis-ambiguation?.
We found around 80 factoids pertopic on an average.
Once the factoids were ex-tracted, each factoid was assigned a weight basedon the number of documents it appears in, and anyfactoids with weight one were removed.
Table 3shows the top ten factoids in the topic of WordSense Disambiguation along with their distribu-tion across the different surveys and tutorials andfinal weight.For each of the topics, we used the method de-scribed in Section 2 to create a candidate docu-ment set and extracted the candidate citing sen-tences to be used as the input for the content se-lection component.
Each sentence in each topicwas then annotated by a human judge against thefactoid list for that topic.
A sentence is allowedto have zero or more than one factoid.
The humanassessors were graduate students in Computer Sci-ence who have taken a basic ?Natural LanguageProcessing?
course or an equivalent course.
On anaverage, 375 citing sentences were annotated foreach topic, with 2,625 sentences being annotatedin total.
We present all our experimental results onthis large annotated corpora which is also availablefor download 2.4 Content ModelsOnce we have the set of input sentences, our sys-tem must select the sentences that should be part2http://clair.si.umich.edu/corpora/survey data/of the survey.
For this task, we experimented withthree content models, described below.4.1 CentroidThe centroid of a set of documents is a set of wordsthat are statistically important to the cluster of doc-uments.
Centroid based summarization of a docu-ment set involves first creating the centroid of thedocuments, and then judging the salience of eachdocument based on its similarity to the centroidof the document set.
In our case, the input citingsentences represent the documents from which weextract the centroid.
We use the centroid imple-mentation from the publicly available summariza-tion toolkit, MEAD (Radev et al, 2004).4.2 LexrankLexRank (Erkan and Radev, 2004) is a networkbased content selection algorithm that works byfirst building a graph of all the documents in acluster.
The edges between corresponding nodesrepresent the cosine similarity between them.Once the network is built, the algorithm computesthe salience of sentences in this graph based ontheir eigenvector centrality in the network.4.3 C-LexrankC-Lexrank is another network based content selec-tion algorithm that focuses on diversity (Qazvinianand Radev, 2008).
Given a set of sentences, it firstcreates a network using these sentences and thenruns a clustering algorithm to partition the net-work into smaller clusters that represent differentaspects of the paper.
The motivation behind theclustering is to include more diverse facts in thesummary.5 Experiments and ResultsTo do an evaluation of our different content selec-tion methods, we first select the documents usingour Restricted Expansion method, and then pick574Topic Rand Cent LR C-LRSummarization 0.68 0.61 0.91 0.82Question Answering 0.52 0.50 0.65 0.56Word Sense Disambiguation 0.78 0.73 0.82 0.76Named Entity Recognition 0.90 0.90 0.94 0.94Sentiment Analysis 0.75 0.78 0.77 0.78Semantic Role Labeling 0.78 0.79 0.88 0.94Dependency Parsing 0.67 0.38 0.71 0.53Average 0.72 0.68 0.81?
0.76Table 4: Results of pyramid evaluation for eachof the three methods and the random baseline oneach topic.the citing sentences to be used as the input to thesummarization module as described in Section 2.Given this input, we generate 500 word summariesfor each of the seven topics using the four meth-ods: Centroid, Lexrank, C-Lexrank and a randombaseline.For each summary, we compute two evaluationmetrics.
The first is the Pyramid score (Nenkovaand Passonneau, 2004) computed by treating thefactoids as Summary Content Units (SCU?s).
ThePyramid scores for each summary is shown in Ta-ble 4.
The second metric is an Unnormalized Rel-ative Utility score (Radev and Tam, 2003), com-puted using the factoid scores of sentences basedon the method presented in (Qazvinian, 2012).
Wecall this Unnormalized RU since we are not able tonormalize the scores with human generated goldsummaries.
The results for Unnormalized RU areshown in Table 5.
The parameter ?
is the RUpenalty for including a redundant sentence sub-sumed by an earlier sentence.
If the summarychooses a sentence si with score worig that is sub-sumed by an earlier summary sentence, the scoreis reduced as wsubsumed = (?
?
worig).
We ap-proximate subsumption by marking a sentence sjas being subsumed by si if Fj ?
Fi, where Fi andFj are sets of factoids covered in each sentence.Topic Rand Cent LR C-LRSummarization 0.16 0.57 0.29 0.17Question Answering 0.32 0.39 0.48 0.30Word Sense Disambiguation 0.28 0.33 0.31 0.30Named Entity Recognition 0.36 0.38 0.34 0.31Sentiment Analysis 0.23 0.34 0.48 0.33Semantic Role Labeling 0.11 0.17 0.16 0.21Dependency Parsing 0.16 0.05 0.30 0.15Average 0.23 0.32 0.34?
0.25Table 5: Results of Unnormalized Relative Utilityevaluation for the three methods and random base-line using ?
= 0.5.The reason for the relatively high scores for therandom baseline is that our process to select theinitial set of sentences eliminates many bad sen-tences.
For example, for a subset of 5 topics,the total input set contains 1508 sentences, out ofwhich 922 of the sentences (60%) have at least onefactoid.
This makes it highly likely to pick goodcontent sentences even when we are picking sen-tences at random.We find that the Lexrank method outperformsother sentence selection methods on both evalua-tion metrics.
The higher performance of Lexrankcompared to Centroid is consistent with earlierpublished results (Erkan and Radev, 2004).
Thereason for the low performance of C-Lexrank ascompared to Lexrank on this data set can be at-tributed to the fact that the input sentence set isderived from a much more diverse set of paperswhich can have a high diversity in lexical choicewhen describing the same factoid.
Thus simplelexical similarity is not enough to find good clus-ters in this sentence set.The lower Unnormalized RU scores comparedto Pyramid scores indicate that we are selectingsentences containing highly weighted factoids, butwe do not select the most informative sentencesthat contain a large number of factoids.
Thisalso shows that we select some redundant factoids,since Unnormalized RU contains a penalty for re-dundancy.
This is again, explained by the factthat the simple lexical diversity based model in C-Lexrank is not able to detect the same factoids be-ing present in two sentences.
Despite these short-comings, our system works quite well in termsof content selection for unseen topics, Figure 2shows the top 5 sentences for the query ?Condi-tional Random Fields?.6 Conclusion and Future WorkIn this paper, we described a pipeline for the gen-eration of scientific surveys starting from a topicquery.
Our system is divided into two components.The first component finds the set of papers fromthe corpus relevant to the query using a simpleheuristic called Restricted Expansion.
The secondcomponent selects sentences from these papers togenerate a survey of the topic.
One of the maincontributions of this work is a manually annotateddata set for evaluating both the tasks.
We collected47 gold standard documents (surveys and tutori-als) on seven topics in Natural Language Process-ing and extracted factoids for each topic.
Eachfactoid is given an importance score based on thenumber of gold standard documents it appears in.575In recent years, conditional random fields (CRFs) (Lafferty et al , 2001)have shown success on a number of natural language processing (NLP)tasks, including shallow parsing (Sha and Pereira, 2003), named entityrecognition (McCallum and Li, 2003) and information extraction fromresearch papers (Peng and McCallum, 2004).In natural language processing, two aspects of CRFs have beeninvestigated sufficiently: one is to apply it to new tasks, such as namedentity recognition (McCallum and Li, 2003; Li and McCallum, 2003;Settles, 2004), part-of-speech tagging (Lafferty et al, 2001), shallowparsing (Sha and Pereira, 2003), and language modeling (Roark et al,2004); the other is to exploit new training methods for CRFs, such asimproved iterative scaling (Lafferty et al, 2001), L-BFGS (McCallum,2003) and gradient tree boosting (Dietterich et al, 2004)NP chunks are very similar to the ones of Ramshaw and Marcus (1995).CRFs have shown empirical successes recently in POS tagging (Laffertyet al , 2001), noun phrase segmentation (Sha and Pereira, 2003) andChinese word segmentation (McCallum and Feng, 2003)CRFs have been successfully applied to a number of real-world tasks,including NP chunking (Sha and Pereira, 2003), Chinese wordsegmentation (Peng et al, 2004), information extraction (Pinto et al,2003; Peng and McCallum, 2004), named entity identification (McCallumand Li, 2003; Settles, 2004), and many others.Figure 2: A sample output survey produced byour system on the topic of ?Conditional RandomFields?
using Restricted Expansion and Lexrank.Additionally, we manually annotated 2,625 inputsentences, about 375 sentences per topic, with thefactoids extracted from the gold standard docu-ments for each topic.
Using this corpus, we pre-sented experimental results for the performance ofour document selection component and three sen-tence selection strategies.Our results indicate three main directions forfuture work.
We plan to look at better modelsof diversity in sentence selection, since methodsbased on simple lexical similarity do not seem towork well.
The low factoid recall shown by lowunnormalized RU scores suggests integrating thefull text of papers with citation based summarieswhich might help us find factoids such as topicdefinitions that are unlikely to be present in citingsentences.
A final goal would be to improve thereadability and coherence of our system output.AcknowledgmentsWe thank Vahed Qazvinian, Wanchen Lu, BenKing, and Shiwali Mohan for extremely usefuldiscussions and help with the data annotation.This research is supported by the IntelligenceAdvanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Cen-ter (DoI/NBC) contract number D11PC20153.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoI/NBC, or the U.S. Govern-ment.ReferencesGu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Cong Duy Vu Hoang and Min-Yen Kan. 2010.
To-wards automated related work summarization.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, COLING ?10,pages 427?435, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, NAACL ?09, pages 584?592, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using referenceinformation.
In Proceedings of the 16th Interna-tional Joint Conference on Artificial Intelligence(IJCAI-99), pages 926?931.Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Proceedings of the North Ameri-can Chapter of the Association for ComputationalLinguistics - Human Language Technologies (HLT-NAACL ?04).Vahed Qazvinian and Dragomir R. Radev.
2008.Scientific paper summarization using citation sum-mary networks.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING-08), Manchester, UK.Vahed Qazvinian.
2012.
Using Collective Discourseto Generate Surveys of Scientific Paradigms.
Ph.D.thesis.Dragomir R. Radev and Daniel Tam.
2003.
Sum-marization evaluation using relative utility.
InCIKM2003, pages 508?511.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda C?elebi, StankoDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,Danyu Liu, Jahna Otterbacher, Hong Qi, HoracioSaggion, Simone Teufel, Michael Topper, Adam576Winkel, and Zhu Zhang.
2004.
MEAD - a platformfor multidocument multilingual text summarization.In LREC 2004, Lisbon, Portugal, May.Dragomir R. Radev, Pradeep Muthukrishnan, VahedQazvinian, and Amjad Abu-Jbara.
2013.
The aclanthology network corpus.
Language Resourcesand Evaluation, pages 1?26.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.577
