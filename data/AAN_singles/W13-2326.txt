Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 214?222,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsInvestigation of annotator?s behaviour using eye-tracking dataRyu Iida Koh Mitsuda Takenobu TokunagaDepartment of Computer Science, Tokyo Institute of Technology{ryu-i,mitsudak,take}@cl.cs.titech.ac.jpAbstractThis paper presents an analysis of an anno-tator?s behaviour during her/his annotationprocess for eliciting useful information fornatural language processing (NLP) tasks.Text annotation is essential for machinelearning-based NLP where annotated textsare used for both training and evaluat-ing supervised systems.
Since an annota-tor?s behaviour during annotation can beseen as reflecting her/his cognitive processduring her/his attempt to understand thetext for annotation, analysing the processof text annotation has potential to revealuseful information for NLP tasks, in par-ticular semantic and discourse processingthat require deeper language understand-ing.
We conducted an experiment for col-lecting annotator actions and eye gaze dur-ing the annotation of predicate-argumentrelations in Japanese texts.
Our analysisof the collected data suggests that obtainedinsight into human annotation behaviouris useful for exploring effective linguis-tic features in machine learning-based ap-proaches.1 IntroductionText annotation is essential for machine learn-ing (ML)-based natural language processing(NLP) where annotated texts are used forboth training and evaluating supervised systems.This annotation-then-learning approach has beenbroadly applied to various NLP tasks, rangingfrom shallow processing tasks, such as POS tag-ging and NP chunking, to tasks requiring deeperlinguistic information, such as coreference resolu-tion and discourse relation classification, and hasbeen largely successful for shallow NLP tasks inparticular.
The key to this success is how use-ful information can be effectively introduced intoML algorithms as features.
With shallow NLPtasks, surface information like words and theirPOS within a window of a certain size can be eas-ily employed as useful features.
In contrast, insemantic and discourse processing, such as coref-erence resolution and discourse structure analy-sis, it is not trivial to employ as features deeperlinguistic knowledge and human linguistic intu-ition that are indispensable for these tasks.
Inorder to improve system performance, past at-tempts have integrated deeper linguistic knowl-edge through manually constructed linguistic re-sources such as WordNet (Miller, 1995) and lin-guistic theories such as Centering Theory (Groszet al 1995).
They partially succeed in improv-ing performance, but there is still room for furtherimprovement (duVerle and Prendinger, 2009; Ng,2010; Lin et al 2010; Pradhan et al 2012).Unlike past attempts relying on heuristic fea-ture engineering, we take a cognitive science ap-proach to improving system performance.
In steadof employing existing resources and theories, welook into human behaviour during annotation andelicit useful information for NLP tasks requir-ing deeper linguistic knowledge.
Particularly wefocus on annotator eye gaze during annotation.Because of recent developments in eye-trackingtechnology, eye gaze data has been widely usedin various research fields, including psycholin-guistics and problem solving (Duchowski, 2002).There have been a number of studies on the rela-tions between eye gaze and language comprehen-sion/production (Griffin and Bock, 2000; Richard-son et al 2007).
Compared to the studies onlanguage and eye gaze, the role of gaze in gen-eral problem solving settings has been less stud-ied (Bednarik and Tukiainen, 2008; Rosengrant,2010; Tomanek et al 2010).
Since our current in-terest, text annotation, can be considered a prob-lem solving as well as language comprehensiontask, we refer to them when defining our prob-214lem setting.
Through analysis of annotators?
eye-tracking data, we aim at finding useful informationwhich can be employed as features in ML algo-rithms.This paper is organised as follows.
Section 2presents the details of the experiment for collect-ing annotator behavioural data during annotationas well as details on the collected data.
Section 3explains the structure of the annotation processfor a single annotation instance.
Section 4 pro-vides a detailed analysis of human annotation pro-cesses, suggesting usages of those results in NLP.Section 5 reviews the related work and Section 6concludes and discusses future research direc-tions.2 Data collection2.1 Materials and procedureWe conducted an experiment for collecting anno-tator actions and eye gaze during the annotationof predicate-argument relations in Japanese texts.Given a text in which candidates of predicatesand arguments were marked as segments (i.e.
textspans) in an annotation tool, the annotators wereinstructed to add links between correct predicate-argument pairs by using the keyboard and mouse.We distinguished three types of links based on thecase marker of arguments, i.e.
ga (nominative),o (accusative) and ni (dative).
For elliptical argu-ments of a predicate, which are quite common inJapanese texts, their antecedents were linked to thepredicate.
Since the candidate predicates and ar-guments were marked based on the automatic out-put of a parser, some candidates might not havetheir counterparts.We employed a multi-purpose annotation toolSlate (Kaplan et al 2012), which enables anno-tators to establish a link between a predicate seg-ment and its argument segment with simple mouseand keyboard operations.
Figure 1 shows a screen-shot of the interface provided by Slate.
Segmentsfor candidate predicates are denoted by light bluerectangles, and segments for candidate argumentsare enclosed with red lines.
The colour of linkscorresponds to the type of relations; red, blue andgreen denote nominative, accusative and dative re-spectively.In order to collect every annotator operation, wemodified Slate so that it could record several im-portant annotation events with their time stamp.The recorded events are summarised in Table 1.Event label Descriptioncreate link start creating a link startscreate link end creating a link endsselect link a link is selecteddelete link a link is deletedselect segment a segment is selectedselect tag a relation type is selectedannotation start annotating a text startsannotation end annotating a text endsTable 1: Recorded annotation eventsFigure 2: Snapshot of annotation using Tobii T60Annotator gaze was captured by the Tobii T60eye tracker at intervals of 1/60 second.
The Tobii?sdisplay size was 1, 280?1, 024 pixels and the dis-tance between the display and the annotator?s eyewas maintained at about 50 cm.
The five-point cal-ibration was run before starting annotation.
In or-der to minimise the head movement, we used achin rest as shown in Figure 2.We recruited three annotators who had experi-ences in annotating predicate-argument relations.Each annotator was assigned 43 texts for annota-tion, which were the same across all annotators.These 43 texts were selected from a Japanese bal-anced corpus, BCCWJ (Maekawa et al 2010).
Toeliminate unneeded complexities for capturing eyegaze, texts were truncated to about 1,000 charac-ters so that they fit into the text area of the annota-tion tool and did not require any scrolling.
It tookabout 20?30 minutes for annotating each text.
Theannotators were allowed to take a break whenevershe/he finished annotating a text.
Before restart-ing annotation, the five-point calibration was runevery time.
The annotators accomplished all as-signed texts after several sessions for three or moredays in total.215SLAPFigure 1: Screenshot of the annotation tool Slate2.2 ResultsThe number of annotated links between predicatesand arguments by three annotators A0, A1 and A2were 3,353 (A0), 3,764 (A1) and 3,462 (A2) re-spectively.
There were several cases where theannotator added multiple links with the same linktype to a predicate, e.g.
in case of conjunctive ar-guments; we exclude these instances for simplicityin the analysis below.
The number of the remain-ing links were 3,054 (A0), 3,251 (A1) and 2,996(A2) respectively.
In addition, because our anal-yses explained in Section 4 require an annotator?sfixation on both a predicate and its argument, thenumber of these instances were reduced to 1,776(A0), 1,430 (A1) and 1,795 (A2) respectively.
Thedetails of the instances for our analysis are sum-marised in Table 2.
These annotation instanceswere used for the analysis in the rest of this paper.3 Anatomy of human annotationFrom a qualitative analysis of the annotator?s be-haviour in the collected data, we found the an-case A0 A1 A2 totalga (nominative) 1,170 904 1,105 3,179o (accusative) 383 298 421 1,102ni (dative) 223 228 269 720total 1,776 1,430 1,795 5,001Table 2: Results of annotation by each annotatornotation process for predicate-argument relationscould be decomposed into the following threestages.1.
An annotator reads a given text and under-stands its contents.2.
Having fixed a target predicate, she/hesearches for its argument in the set of preced-ing candidate arguments considering a typeof relations with the predicate.3.
Once she/he finds a probable argument in atext, she/he looks around its context in orderto confirm the relation.
The confirmation isfinalised by creating a link between the pred-icate and its argument.216The strategy of searching for arguments after fix-ing a predicate would reflect the linguistic knowl-edge that a predicate subcategorises its arguments.In addition, since Japanese is a head-final lan-guage, a predicate basically follows its arguments.Therefore searching for each argument within asentence can begin at the same position, i.e.
thepredicate, toward the beginning of the sentence,when the predicate-first search strategy is adopted.The idea of dividing a cognitive process intodifferent functional stages is common in cogni-tive science.
For instance, Just and Carpenter(1985) divided a problem solving process intothree stages: searching, comparison and confirma-tion.
In their task, given a picture of two cubeswith a letter on each surface, a participant is in-structed to judge whether they can be the same ornot.
Since one of the cubes is relatively rotatedin a certain direction and amount, the participantneeds to mentally rotate the cubes for matching.Russo and Leclerc (1994) divided a visual deci-sion making process into three stages: orienta-tion, evaluation and verification.
In their exper-iment, participants were asked to choose one ofseveral daily food products that were visually pre-sented.
The boundaries of the above three stageswere identified based on the participants?
eye gazeand their verbal protocols.
Malcolm and Hender-son (2009) applied the idea to a visual search pro-cess, dividing it into initiation, scanning and ver-ification.
Gidlo?f et al(2013) discussed the dif-ference between a decision making process and avisual search process in terms of the process divi-sion.
Although the above studies deal with the dif-ferent cognitive processes, it is common that thefirst stage is for capturing an overview of a prob-lem, the second is for searching for a tentative so-lution, and the third is for verifying their solution.Our division of the annotation process conformswith this idea.
Particularly, our task is similar tothe decision making process as defined by Russoand Leclerc (1994).
Unlike these past studies,however, the beginning of an orientation stage1 isnot clear in our case, since we collected the datain a natural annotation setting, i.e.
a single anno-tation session for a text includes creation of mul-tiple links.
In other words, the first stage mightcorrespond to multiple second and third stages.
Inaddition, in past research on decision making, asingle object is chosen, but our annotation task in-1We follow the wording by Russo and Leclerc (1994).??
?link creationfirst dwell on the linked argumentfirst dwell on the target predicate?
??
?orientation?
??
?evaluation?
??
?verification-timeFigure 3: Division of an annotation processvolves two objects to consider, i.e.
a predicate andan argument.Considering these differences and the propos-als of previous studies (Russo and Leclerc, 1994;Gidlo?f et al 2013)?we define the three stages asfollows.
As explained above, we can not identifythe beginning of an orientation stage based on anydecisive clue.
We define the end of an orientationstage as the onset of the first dwell2 on a predi-cate being considered.
The succeeding evaluationstage starts at the onset of the first dwell on thepredicate and ends at the onset of the first dwell onthe argument that is eventually linked to the pred-icate.
The third stage, a verification stage, startsat the onset of the first dwell on the linked argu-ment and ends at the creation of the link betweenthe predicate and argument.
These definitions andthe relations between the stages are illustrated inFigure 3.The time points indicating the stage boundariescan be identified from the recorded eye gaze andtool operation data.
First, gaze fixations were ex-tracted by using the Dispersion-Threshold Identi-fication (I-DT) algorithm (Salvucci and Goldberg,2000).
Based on a rationale that the eye movementvelocity slows near fixations, the I-DT algorithmidentifies fixations as clusters of consecutive gazepoints within a particular dispersion.
It has two pa-rameters, the dispersion threshold that defines themaximum distance between gaze points belongingto the same cluster, and the duration threshold thatconstrains the minimum fixation duration.
Con-sidering the experimental configurations, i.e.
(i)the display size and its resolution, (ii) the distancebetween the display and the annotator?s eyes, and(iii) the eye-tracker resolution, we set the disper-sion threshold to 16 pixels.
Following Richard-son et al(2007), we set the duration thresholdto 100 msec.
Based on fixations, a dwell on asegment was defined as a series of fixations thatconsecutively stayed on the same segment where2A dwell is a collection of one or several fixations withina certain area of interest, a segment in our case.217two consecutive fixations were not separated bymore than 100 msec.
We allowed a horizontal er-ror margin of 16 pixels (one-character width) forboth sides of a segment when identifying a dwell.Time points of link creation were determined bythe ?create link start?
event in Table 1.Among these three stages, the evaluation stagewould be most informative for extracting usefulfeatures for ML algorithms, because an annotatoridentifies a probable argument for a predicate un-der consideration during this stage.
Analysing an-notator eye gaze during this stage could reveal use-ful information for predicate-argument analysis.
Itis, however, insufficient to regard only fixated ar-guments as being under the annotator?s consider-ation during the evaluation stage.
The annotatorcaptures an overview of the current problem dur-ing the previous orientation stage, in which she/hecould remember several candidate arguments inher/his short-term memory, then moves on to theevaluation stage.
Therefore, all attended argu-ments are not necessarily observed through gazedwells.
As we explained earlier, we have no meansto identify a rigid duration of an orientation stage,thus it is difficult to identify a precise set of can-didate arguments under the annotator?s considera-tion in the evaluation stage.
For this purpose, weneed a different experimental design so that everypredicate-argument relation is annotated at a timein the same manner as the above decision makingstudies conducted.
Another possibility is using anannotator?s verbal protocols together with her/hiseye gaze as done in Russo and Leclerc (1994).On the other hand, in the verification stage aprobable argument has been already determinedand its validity confirmed by investigating its com-petitors.
We would expect considered competi-tors are explicitly fixated during this stage.
Sincewe have a rigid definition of the verification stageduration, it is possible to analyse the annotator?sbehaviour during this stage based on her/his eyegaze.
For this reason, we concentrate on the anal-ysis of the verification stage of annotation hence-forth.4 Analysis of the verification stageGiven the set of annotation instances, i.e.
pred-icate, argument and case triplets, we categorisethese instances based on the annotator?s behaviourduring the verification stage.
We focus on two fac-tors for categorising annotation instances: (i) the110010,0000 10 20 30 40 50 60 70 80 90 100# InstancesDistance between predicate and argument0%50%100%0 10 20 30 40 50 60 70 80 90 100DistributionDistance between predicate and argument?
Distracted      ?
ConcentratedFigure 4: Distance of predicate and argumentdistance of a predicate and if its argument is ei-ther near or far, and (ii) whether annotator gazedwelled on other arguments than the eventuallylinked argument before creating the link.
We callthe former factor Near/Far distinction, and the lat-ter Concentrated/Distracted distinction.To decide the Near/Far distinction, we inves-tigated the distribution of distances of predicatesand their argument.
The result is shown in theupper graph of Figure 4, where the x-axis is thecharacter-based distance and the y-axis shows thenumber of instances in each distance bin.
Figure 4demonstrates that the instances concentrate at thebin of distance 1.
This reflects the frequentlyoccurring instances where a one-character casemaker follows an argument, and immediately pre-cedes its predicate.
The lower graph in Figure 4shows the ratio of Distracted instances to Con-centrated at each bin.
The distribution indi-cates that there is no remarkable relation betweenthe distance and Concentrated/Distracted distinc-tion.
The correlation coefficient between the dis-tance and the number of Concentrated instancesis ?0.26.
We can conclude that the distance ofa predicate and its argument does not impact theConcentrated/Distracted distinction.
Consideringthe above tendency, we set the distance thresholdto 22, the average distance of all annotation in-stances; instances with a distance of less than 22are considered Near.These two factors make four combinationsin total, i.e.
Near-Concentrated (NC), Near-Distracted (ND), Far-Concentrated (FC) and Far-Distracted (FD).
We analysed 5,001 instancesshown in Table 2 to find three kinds of tendencies,which are described in the following sections.218case Near Far totalga (nominative) 2,201 (0.44) 978 (0.90) 3,179 (0.64)o (accusative) 1,042 (0.34) 60 (0.05) 1,102 (0.22)ni (dative) 662 (0.22) 58 (0.05) 720 (0.14)Table 3: Distribution of cases over Near/FarNC ND FC FDga 0.40 0.47 0.92 0.90o, ni 0.60 0.53 0.08 0.10Table 4: Distribution of arguments across four cat-egories4.1 Predicate-argument distance andargument caseWe hypothesise that an annotator changes her/hisbehaviour with regard to the case of the argu-ment.
The argument case in Japanese is markedby a case marker which roughly corresponds tothe argument?s semantic role, such as Agent andTheme.
We therefore analysed the relationshipbetween the Near/Far distinction and argumentcase.
The results are shown in Table 3.
The ta-ble shows the distribution of argument cases, il-lustrating that Near instances are dispersed overthree cases, while Far instances are concentratedin the ga (nominative) case.
In other words, ga-arguments tend to appear far from their predi-cate.
This tendency reflects the characteristic ofJapanese where a nominative argument tends to beplaced in the beginning of a sentence; furthermore,ga-arguments are often omitted to make ellipses.In our annotation guideline, a predicate with an el-liptical argument should be linked to the referentof the ellipsis, which would be realised at a fur-ther distant position in the preceding context.
Incontrast, o (accusative) and ni (dative) argumentsless frequently appeared as Far instances becausethey are rarely omitted due to their tighter rela-tion with arguments.
This observation suggeststhat each case requires an individual specific treat-ment in the model of predicate argument analysis;the model searches for o and ni arguments close toits predicate, while it considers all preceding can-didates for a ga argument.Table 4 shows the break down of theNear/Far columns with regards to the Con-centrated/Distracted distinction, demonstratingthat the Concentrated/Distracted distinction doesnot impact the distribution of the argument types.05101520110100100005101520# instances# existing links # dwells on competitorsFigure 5: Relationship between the number ofdwells on competitors and already-existing links4.2 Effect of already-existing linksIn the Concentrated instances, an annotator canverify if an argument is correct without inspect-ing its competitors.
As illustrated in Figure 1, al-ready annotated arguments are marked by explicitlinks to their predicate.
These links make the ar-guments visually as well as cognitively salient inan annotator?s short-term memory because theyhave been frequently annotated in the precedingannotation process.
Thus, we expected that bothtypes of saliency help to confirm the predicate-argument relation under consideration.
For in-stance, when searching for an argument of pred-icate P in Figure 1, argument A that already hassix links (SL) is more salient than other competi-tors.To verify this hypothesis, we examined the re-lation of the number of already-existing links andthe number of dwells on competitors, which isshown in Figure 5.
In this analysis, we used onlyFar instances because the Near arguments tendedto have less already-existing links as they wereunder current interest.
Figure 5 shows a three-dimensional declining slope that peaks around theintersection for instances with the fewest numberof links and dwells on competitors.
It revealsa mostly symmetrical relation between existinglinks and dwells on competitors for instances witha lower number of existing links, but that this sym-metry brakes for instances with a higher numberof existing links, visible by the conspicuous hole219toward the left of the figure.
This suggests thatvisual and cognitive saliency reduces annotators?cognitive load, and thus contributes to efficientlyconfirming the correct argument.This result implies that the number of already-existing links of a candidate argument would re-flect its saliency, thus more linked candidatesshould be preferred in the analysis of predicate-argument relations.
Although we analysed the ver-ification stage, the same effect could be expectedin the evaluation stage as well.
Introducing suchinformation into ML algorithms may contribute toimproving system performance.4.3 Specificity of arguments and dispersal ofeye gazeExisting Japanese corpora annotated withpredicate-argument relations (Iida et al 2007;Kawahara et al 2002) have had syntactic heads(nouns) of their projected NPs related their pred-icates.
Since Japanese is a head-final language,a head noun is always placed in the last positionof an NP.
This scheme has the advantage thatpredicate-argument relations can be annotatedwithout identifying the starting boundary of theargument NP under consideration.
The schemeis also reflected in the structure of automaticallyconstructed Japanese case frames, e.g.
Sasano etal.
(2009), which consist of triplets in the formof ?Noun, Case, Verb?.
Noun is a head nounextracted from its projected NP in the originaltext.
We followed this scheme in our annotationexperiments.However, a head noun of an argument does notalways have enough information.
A nominaliserwhich often appears in the head position in anNP does not have any semantic meaning by it-self.
For instance, in the NP ?benkyo?
suru koto(to study/studying)?, the head noun ?koto?
has nospecific semantic meaning, corresponding to anEnglish morpheme ?to?
or ?-ing?.
In such cases,inspecting a whole NP including its modifiers isnecessary to verify the validity of the NP for anargument in question.
We looked at our data tosee if annotators actually behaved like this.For analysis, the annotation instances were dis-tinguished if an argument had any modifier or not(column ?w/o mod?
and ?w/ mod?
in Table 5).The ?w/ mod?
instances are further divided intotwo classes: ?within NP?
and ?out of NP?, the for-mer if all dwells remain ?within?
the region of thew/o mod w/ mod totalwithin NP out of NPConcentrated 1,562 1190 ?
2,752Distracted 1,168 242 839 2,249Table 5: Relation of argument modifiers and gazedispersalargument NP or the later if they go ?out of?
theregion.
Note that our annotation scheme createsa link between a predicate and the head of its ar-gument as described earlier.
Thus, a Distractedinstance does not always mean an ?out of NP?
in-stance, since a distracted dwell might still remainson a segment within the NP region despite not be-ing its head.
Table 5 shows the distribution of theinstances over this categorisation.We found that the number of instances is almostthe same between Concentrated and Distracted,i.e.
(2752 : 2249 = 0.55 : 0.45).
In this re-spect, both Concentrated and Distracted instancescan be treated in the same way in the analysis ofpredicate-argument relations.
A closer look at thebreak down of the ?w/ mod?
category, however, re-veals that almost 22% of the Distracted argumentswith any modifier attracted gaze dwells within theNP region.
This fact suggests that we need to treatcandidate arguments differently depending on ifthey have modifiers or not.
In addition to argumenthead information, we could introduce informationof modifiers into ML algorithms as features thatcharacterise a candidate argument more precisely.5 Related workRecent developments in the eye-tracking technol-ogy enables various research fields to employ eye-gaze data (Duchowski, 2002).Bednarik and Tukiainen (2008) analysed eye-tracking data collected while programmers debuga program.
They defined areas of interest (AOI)based on the sections of the integrated develop-ment environment (IDE): the source code area,the visualised class relation area and the programoutput area.
They compared the gaze transitionsamong these AOIs between expert and novice pro-grammers to find different transition patterns be-tween them.
Since the granularity of their AOIsis coarse, it could be used for evaluating a pro-grammer?s expertise, but hardly explains why theexpert transition pattern realises a good program-ming skill.
In order to find useful information forlanguage processing, we employed smaller AOIs220at the character level.Rosengrant (2010) proposed an analysis methodnamed gaze scribing where eye-tracking data iscombined with a subject?s thought process derivedby the think-aloud protocol (TAP) (Ericsson andSimon, 1984).
As a case study, he analysed a pro-cess of solving electrical circuit problems on thecomputer display to find differences of problemsolving strategy between novice and expert sub-jects.
The AOIs are defined both at a macro level,i.e.
the circuit, the work space for calculation,and a micro level, i.e.
electrical components ofthe circuit.
Rosengrant underlined the importanceof applying gaze scribing to the solving processof other problems.
Although information obtainedfrom TAP is useful, it increases her/his cognitiveload, and thus might interfere with her/his achiev-ing the original goal.Tomanek et al(2010) utilised eye-tracking datato evaluate the degree of difficulty in annotatingnamed entities.
They are motivated by selectingappropriate training instances for active learningtechniques.
They conducted experiments in vari-ous settings by controlling characteristics of targetnamed entities.
Compared to their named entityannotation task, our annotation task, annotatingpredicate-argument relations, is more complex.
Inaddition, our experimental setting is more natural,meaning that all possible relations in a text wereannotated in a single session, while each sessiontargeted a single named entity (NE) in a limitedcontext in the setting of Tomanek et al(2010).Finally, our fixation target is more precise, i.e.words, rather than a coarse area around the targetNE.We have also discussed evaluating annotationdifficulty for predicate-argument relations by us-ing the same data introduced in this paper (Toku-naga et al 2013).
Through manual analysis ofthe collected data, we suggested that an annotationtime necessary for annotating a single predicate-argument relation was correlated with the agree-ment ratio among multiple human annotators.6 ConclusionThis paper presented an analysis of an annota-tor?s behaviour during her/his annotation processfor eliciting useful information for NLP tasks.We first conducted an experiment for collect-ing three annotators?
actions and eye gaze dur-ing their annotation of predicate-argument rela-tions in Japanese texts.
The collected data wereanalysed from three aspects: (i) the relationshipof predicate-argument distances and argument?scases, (ii) the effect of already-existing links and(iii) specificity of arguments and dispersal of eyegaze.
The analysis on these aspects suggested thatobtained insight into human annotation behaviourcould be useful for exploring effective linguisticfeatures in ML-based approaches.As future work, we need to further investigatethe data from other aspects.
There are advantagesto manual analysis, such as done in this paper.Mining techniques for finding unknown but usefulinformation may also be advantageous.
Therefore,we are planning to employ mining techniques forfinding useful gaze patterns for various NLP tasks.In this paper, we suggested useful informationthat could be incorporated into ML algorithms asfeatures.
It is necessary to implement these fea-tures in a specific ML algorithm and evaluate theireffectiveness empirically.Our analysis was limited to the verificationstage of annotation, in which a probable argumentof a predicate was confirmed by comparing it withother competitors.
The preceding evaluation stageshould be also analysed, since it is the stage whereannotators search for a correct argument of a pred-icate in question, thus probably includes useful in-formation for computational models in identifyingpredicate-argument relations.
For the analysis ofthe evaluation stage, a different design of experi-ments would be necessary, as already mentioned,employing single annotation at a time scheme asTomanek et al(2010) did, or using an annota-tor?s verbal protocol together as Russo and Leclerc(1994), and Rosengrant (2010) did.Last but not least, data collection and analy-sis in different annotation tasks are indispensable.It is our ultimate goal to establish a methodol-ogy for collecting an analysing annotators?
be-havioural data during annotation in order to eliciteffective features for ML-based NLP.ReferencesRoman Bednarik and Markku Tukiainen.
2008.
Tem-poral eye-tracking data: Evolution of debuggingstrategies with multiple representations.
In Proceed-ings of the 2008 symposium on Eye tracking re-search & applications (ETRA ?08), pages 99?102.Andrew T. Duchowski.
2002.
A breadth-first survey ofeye-tracking applications.
Behavior Research Meth-221ods, Instruments, and Computers, 34(4):455?470.David duVerle and Helmut Prendinger.
2009.
A noveldiscourse parser based on support vector machineclassification.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 665?673.K.
Anders Ericsson and Herbert A. Simon.
1984.
Pro-tocol Analysis ?
Verbal Reports as Data ?.
The MITPress.Kerstin Gidlo?f, Annika Wallin, Richard Dewhurst, andKenneth Holmqvist.
2013.
Using eye tracking totrace a cognitive process: Gaze behaviour during de-cision making in a natural environment.
Journal ofEye Movement Research, 6(1):1?14.Zenzi M. Griffin and Kathryn Bock.
2000.
What theeyes say about speaking.
Psychological Science,11(4):274?279.Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-stein.
1995.
Centering: A framework for model-ing the local coherence of discourse.
ComputationalLinguistics, 21(2):203?225.Ryu Iida, Mamoru Komachi, Kentaro Inui, and YujiMatsumoto.
2007.
Annotating a Japanese text cor-pus with predicate-argument and coreference rela-tions.
In Proceeding of the ACL Workshop ?Linguis-tic Annotation Workshop?, pages 132?139.Marcel Adam Just and Patricia A. Carpenter.
1985.Cognitive coordinate systems: Accounts of mentalrotation and individual differences in spatial ability.Psychological Review, 92(2):137?172.Dain Kaplan, Ryu Iida, Kikuko Nishina, and TakenobuTokunaga.
2012.
Slate ?
a tool for creating andmaintaining annotated corpora.
Journal for Lan-guage Technology and Computational Linguistics,26(2):89?101.Daisuke Kawahara, Sadao Kurohashi, and Ko?itiHasida.
2002.
Construction of a Japaneserelevance-tagged corpus (in Japanese).
In Proceed-ings of the 8th Annual Meeting of the Association forNatural Language Processing, pages 495?498.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.A PDTB-styled end-to-end discourse parser.
Tech-nical Report TRB8/10, School of Computing, Na-tional University of Singapore.Kikuo Maekawa, Makoto Yamazaki, TakehikoMaruyama, Masaya Yamaguchi, Hideki Ogura,Wakako Kashino, Toshinobu Ogiso, Hanae Koiso,and Yasuharu Den.
2010.
Design, compilation,and preliminary analyses of balanced corpus ofcontemporary written Japanese.
In Proceedings ofthe Eigth International Conference on LanguageResources and Evaluation (LREC 2010), pages1483?1486.George L. Malcolm and John M. Henderson.
2009.The effects of target template specificity on visualsearch in real-world scenes: Evidence from eyemovements.
Journal of Vision, 9(11):8:1?13.George A. Miller.
1995.
WordNet: A lexical databasefor English.
Communications of the ACM, 38:39?41.Vincent Ng.
2010.
Supervised noun phrase corefer-ence research: The first fifteen years.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL 2010), pages1396?1411.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Joint Confer-ence on EMNLP and CoNLL ?
Shared Task, pages1?40.Daniel C. Richardson, Rick Dale, and Michael J.Spivey.
2007.
Eye movements in language and cog-nition: A brief introduction.
In Monica Gonzalez-Marquez, Irene Mittelberg, Seana Coulson, andMichael J. Spivey, editors, Methods in CognitiveLinguistics, pages 323?344.
John Benjamins.David Rosengrant.
2010.
Gaze scribing in physicsproblem solving.
In Proceedings of the 2010 sym-posium on Eye tracking research & applications(ETRA ?10), pages 45?48.J.
Edward Russo and France Leclerc.
1994.
Aneye-fixation analysis of choice processes for con-sumer nondurables.
Journal of Consumer Research,21(2):274?290.Dario D. Salvucci and Joseph H. Goldberg.
2000.Identifying fixations and saccades in eye-trackingprotocols.
In Proceedings of the 2000 symposium onEye tracking research & applications (ETRA ?00),pages 71?78.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2009.
The effect of corpus size on case frameacquisition for discourse analysis.
In Proceedings ofHuman Language Technologies: The 2009 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics (NAACL-HLT 2009), pages 521?529.Takenobu Tokunaga, Ryu Iida, and Koh Mitsuda.2013.
Annotation for annotation - toward elicit-ing implicit linguistic knowledge through annota-tion -.
In Proceedings of the 9th Joint ISO - ACLSIGSEM Workshop on Interoperable Semantic An-notation (ISA-9), pages 79?83.Katrin Tomanek, Udo Hahn, Steffen Lohmann, andJu?rgen Ziegler.
2010.
A cognitive cost model ofannotations based on eye-tracking data.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL 2010), pages1158?1167.222
