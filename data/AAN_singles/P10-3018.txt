Proceedings of the ACL 2010 Student Research Workshop, pages 103?108,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsImportance of linguistic constraints in statistical dependency parsingBharat Ram AmbatiLanguage Technologies Research Centre, IIIT-Hyderabad,Gachibowli, Hyderabad, India ?
500032.ambati@research.iiit.ac.inAbstractStatistical systems with high accuracy are veryuseful in real-world applications.
If these sys-tems can capture basic linguistic information,then the usefulness of these statistical systemsimprove a lot.
This paper is an attempt at in-corporating linguistic constraints in statisticaldependency parsing.
We consider a simplelinguistic constraint that a verb should nothave multiple subjects/objects as its childrenin the dependency tree.
We first describe theimportance of this constraint considering Ma-chine Translation systems which use depen-dency parser output, as an example applica-tion.
We then show how the current state-of-the-art dependency parsers violate this con-straint.
We present two new methods to handlethis constraint.
We evaluate our methods onthe state-of-the-art dependency parsers forHindi and Czech.1 IntroductionParsing is one of the major tasks which helps inunderstanding the natural language.
It is useful inseveral natural language applications.
Machinetranslation, anaphora resolution, word sense dis-ambiguation, question answering, summarizationare few of them.
This led to the development ofgrammar-driven, data-driven and hybrid parsers.Due to the availability of annotated corpora inrecent years, data driven parsing has achievedconsiderable success.
The availability of phrasestructure treebank for English (Marcus et al,1993) has seen the development of many effi-cient parsers.
Using the dependency analysis, asimilar large scale annotation effort for Czech,has been the Prague Dependency Treebank (Ha-jicova, 1998).
Unlike English, Czech is a free-word-order language and is also morphologicallyvery rich.
It has been suggested that free-word-order languages can be handled better using thedependency based framework than the constitu-ency based one (Hudson, 1984; Shieber, 1985;Mel?
?uk, 1988, Bharati et al, 1995).
The basicdifference between a constituent based represen-tation and a dependency representation is thelack of nonterminal nodes in the latter.
It has alsobeen noted that use of appropriate edge labelsgives a level of semantics.
It is perhaps due tothese reasons that the recent past has seen a surgein the development of dependency based tree-banks.Due to the availability of dependency tree-banks, there are several recent attempts at build-ing dependency parsers.
Two CoNLL sharedtasks (Buchholz and Marsi, 2006; Nivre et al,2007a) were held aiming at building state-of-the-art dependency parsers for different languages.Recently in NLP Tools Contest in ICON-2009(Husain, 2009 and references therein), rule-based, constraint based, statistical and hybridapproaches were explored towards building de-pendency parsers for three Indian languagesnamely, Telugu, Hindi and Bangla.
In all theseefforts, state-of-the-art accuracies are obtainedby two data-driven parsers, namely, Malt (Nivreet al, 2007b) and MST (McDonald et al, 2006).The major limitation of both these parsers is thatthey won't take linguistic constraints into accountexplicitly.
But, in real-world applications of theparsers, some basic linguistic constraints are veryuseful.
If we can make these parsers handle lin-guistic constraints also, then they become veryuseful in real-world applications.This paper is an effort towards incorporatinglinguistic constraints in statistical dependencyparser.
We consider a simple constraint that averb should not have multiple subjects/objects asits children.
In section 2, we take machine trans-lation using dependency parser as an exampleand explain the need of this linguistic constraint.In section 3, we propose two approaches to han-dle this case.
We evaluate our approaches on thestate-of-the-art dependency parsers for Hindi andCzech and analyze the results in section 4.
Gen-eral discussion and future directions of the workare presented in section 5.
We conclude our pa-per in section 6.1032 MotivationIn this section we take Machine Translation(MT) systems that use dependency parser outputas an example and explain the need of linguisticconstraints.
We take a simple constraint that averb should not have multiple subjects/objects asits children in the dependency tree.
Indian Lan-guage to Indian Language Machine TranstionSystem1 is one such MT system which uses de-pendency parser output.
In this system the gener-al framework has three major components.
a)dependency analysis of the source sentence.
b)transfer from source dependency tree to targetdependency tree, and c) sentence generationfrom the target dependency tree.
In the transferpart several rules are framed based on the sourcelanguage dependency tree.
For instance, for Te-lugu to Hindi MT system, based on the depen-dency labels of the Telugu sentence post-positions markers that need to be added to thewords are decided.
Consider the following ex-ample,(1)Telugu:  raamu     oka      pamdu       tinnaadu?Ramu?
?one?
?fruit?
?ate?Hindi:   raamu     ne      eka      phala    khaayaa?Ramu?
?ERG?
?one?
?fruit?
?ate?English:  ?Ramu ate a fruit?.In the above Telugu sentence, ?raamu?
is thesubject of the verb ?tinnaadu?.
While translatingthis sentence to Hindi, the post-position marker?ne?
is added to the subject.
If the dependencyparser marks two subjects, both the words willhave ?ne?
marker.
This affects the comprehensi-bility.
If we can avoid such instances, then theoutput of the MT system will be improved.This problem is not due to morphologicalrichness or free-word-order nature of the targetlanguage.
Consider an example of free-word-order language to fixed-word-order language MTsystem like Hindi to English MT system.
Thedependency labels help in identifying the posi-tion of the word in the target sentence.
Considerthe example sentences given below.
(2a)    raama   seba      khaatha  hai?Ram?
?apple?
?eats?
?is?
?Ram eats an apple?1 http://sampark.iiit.ac.in/(2b)    seba        raama       khaatha  hai?apple?
?Ram?
?eats?
?is?
?Ram eats an apple?Though the source sentence is different, thetarget sentence is same.
Even though the sourcesentences are different, the dependency tree issame for both the sentences.
In both the cases,?raama?
is the subject and ?seba?
is the object ofthe verb ?khaatha?.
This information helps ingetting the correct translation.
If the parser forthe source sentence assigns the label ?subject?
toboth ?raama?
and ?seba?, the MT system can notgive the correct output.There were some attempts at handling thesekind of linguistic constraints using integer pro-gramming approaches (Riedel et al, 2006; Bha-rati et al, 2008).
In these approaches dependencyparsing is formulated as solving an integer pro-gram as McDonald et al (2006) has formulateddependency parsing as MST problem.
All thelinguistic constraints are encoded as constraintswhile solving the integer program.
In otherwords, all the parses that violate these constraintsare removed from the solution list.
The parsewith satisfies all the constraints is considered asthe dependency tree for the sentence.
In the fol-lowing section, we describe two new approachesto avoid multiple subjects/objects for a verb.3 ApproachesIn this section, we describe the two different ap-proaches for avoiding the cases of a verb havingmultiple subjects/objects as its children in thedependency tree.3.1 Naive Approach (NA)In this approach we first run a parser on the inputsentence.
Instead of first best dependency label,we extract the k-best labels for each token in thesentence.
For each verb in the sentence, wecheck if there are multiple children with the de-pendency label ?subject?.
If there are any suchcases, we extract the list of all the children withlabel ?subject?.
we find the node in this list whichappears left most in the sentence with respect toother nodes.
We assign ?subject?
to this node.
Forthe rest of the nodes in this list we assign thesecond best label and remove the first best labelfrom their respective k-best list of labels.
Wecheck recursively, till all such instances are104avoided.
We repeat the same procedure for ?ob-ject?.Main criterion to avoid multiple sub-jects/objects in this approach is position of thenode in the sentence.
Consider the following ex-ample,Eg.
3: raama   seba      khaatha  hai?Ram?
?apple?
?eats?
?is?
?Ram eats an apple?Suppose the parser assigns the label ?subject?to both the nouns, ?raama?
and ?seba?.
Thennaive approach assigns the label subject to ?raa-ma?
and second best label to ?seba?
as ?raama?precedes ?seba?.In this manner we can avoid a verb havingmultiple children with dependency labels sub-ject/object.Limitation to this approach is word-order.
Thealgorithm described here works well for fixedword order languages.
For example, consider alanguage with fixed word order like English.English is a SVO (Subject, Verb, Object) lan-guage.
Subject always occurs before the object.So, if a verb has multiple subjects, based on posi-tion we can say that the node that occurs firstwill be the subject.
But if we consider a free-word order language like Hindi, this approachwouldn't work always.Consider (2a) and (2b).
In both these exam-ples, ?raama?
is the subject of the verb ?khaatha?and ?seba?
is the object of the verb ?khaatha?.The only difference in these two sentences is theorder of the word.
In (2a), subject precedes ob-ject.
Whereas in (2b), object precedes subject.Suppose the parser identifies both ?raama?
and?seba?
as subjects.
NA can correctly identify?raama?
as the subject in case of (2a).
But in caseof (2b), ?seba?
is identified as the subject.
Tohandle these kind of instances, we use a proba-bilistic approach.3.2 Probabilistic Approach (PA)The probabilistic approach is similar to naiveapproach except that the main criterion to avoidmultiple subjects/objects in this approach isprobability of the node having a particular label.Whereas in naive approach, position of the nodeis the main criterion to avoid multiple sub-jects/objects.
In this approach, for each node inthe sentence, we extract the k-best labels alongwith their probabilities.
Similar to NA, we firstcheck for each verb if there are multiple childrenwith the dependency label ?subject?.
If there areany such cases, we extract the list of all thechildren with label ?subject?.
We find the node inthis list which has the highest probability value.We assign ?subject?
to this node.
For the rest ofthe nodes in this list we assign the second bestlabel and remove the first best label from theirrespective k-best list of labels.
We check recur-sively, till all such instances are avoided.
Werepeat the same procedure for ?object?.Consider (2a) and (2b).
Suppose the parseridentifies both ?raama?
and ?seba?
as subjects.Probability of ?raama?
being a subject will bemore than ?seba?
being a subject.
So, the proba-bilistic approach correctly marks ?raama?
as sub-ject in both (2a) and (2b).
But, NA couldn't iden-tify ?raama?
as subject in (2b).4 ExperimentsWe evaluate our approaches on the state-of-the-art parsers for two languages namely, Hindi andCzech.
First we calculate the instances of mul-tiple subjects/objects in the output of the state-of-the-art parsers for these two languages.
Then weapply our approaches and analyze the results.4.1 HindiRecently in NLP Tools Contest in ICON-2009(Husain, 2009 and references herein), rule-based,constraint based, statistical and hybrid approach-es were explored for parsing Hindi.
All theseattempts were at finding the inter-chunk depen-dency relations, given gold-standard POS andchunk tags.
The state-of-the-art accuracy of74.48% LAS (Labeled Attachment Score) isachieved by Ambati et al (2009) for Hindi.They used two well-known data-driven parsers,Malt2 (Nivre et al, 2007b), and MST3 (McDo-nald et al, 2006) for their experiments.
As theaccuracy of the labeler of MST parser is verylow, they used maximum entropy classificationalgorithm, MAXENT4 for labeling.For Hindi, dependency annotation is done us-ing paninian framework (Begum et al, 2008;Bharati et al, 1995).
So, in Hindi, the equivalentlabels for subject and object are ?karta (k1)?
and?karma (k2)?.
?karta?
and ?karma?
are syntactico-semantic labels which have some properties ofboth grammatical roles and thematic roles.
k1behaves similar to subject and agent.
k2 behavessimilar to object and patient (Bharati et al, 1995;Bharati et al, 2009).
Here, by object we mean2 Malt Version 1.3.13 MST Version 0.4b4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html105only direct object.
Thus we consider only k1 andk2 labels which are equivalent of subject and di-rect object.
Annotation scheme is such that therewouldn?t be multiple subjects/objects for a verbin any case (Bharati et al, 2009).
For example,even in case of coordination, coordinating con-junction is the head and conjuncts are children ofthe coordinating conjunction.
The coordinatingconjunction is attached to the verb with k1/k2label and the conjuncts get attached to the coor-dinating conjunction with a dependency label?ccof?.We replicated the experiments of Ambati et al(2009) on test set (150 sentences) of Hindi andanalyzed the outputs of Malt and MST+MaxEnt.We consider this as the baseline.
In the output ofMalt, there are 39 instances of multiple sub-jects/objects.
There are 51 such instances in theoutput of MST+MAXENT.Malt is good at short distance labeling andMST is good at long distance labeling (McDo-nald and Nivre, 2007).
As ?k1?
and ?k2?
are shortdistance labels, Malt could able predict these la-bels more accurately than MST.
Because of thisoutput of MST has higher number of instances ofmultiple subjects/objects than Malt.Total InstancesMalt 39MST + MAXENT 51Table 1: Number of instances of multiple subjects orobjects in the output of the state-of-the-art parsers forHindiBoth the parsers output first best label for eachnode in the sentence.
In case of Malt, we mod-ified the implementation to extract all the possi-ble dependency labels with their scores.
As Maltuses libsvm for learning, we couldn't able to getthe probabilities.
Though interpreting the scoresprovided by libsvm as probabilities is not thecorrect way, that is the only option currentlyavailable with Malt.
In case of MST+MAXENT,labeling is performed by MAXENT.
We used ajava version of MAXENT5  to extract all possibletags with their scores.
We applied both the naiveand probabilistic approaches to avoid multiplesubjects/objects.
We evaluated our experimentsbased on unlabeled attachment score (UAS), la-beled attachment score (LAS) and labeled score5 http://maxent.sourceforge.net/(LS) (Nivre et al, 2007a).
Results are presentedin Table 2.As expected, PA performs better than NA.With PA we got an improvement of 0.26% inLAS over the previous best results for Malt.
Incase of MST+MAXENT we got an improvementof 0.61% in LAS over the previous best results.Note that in case of MST+MAXENT, the slightdifference between state-of-the-art results ofAmbati et al (2009) and our baseline accuracy isdue different MAXENT package used.Malt MST+MAXENTUAS LAS LS UAS LAS LSBaseline 90.14 74.48 76.38 91.26 72.75 75.26NA 90.14 74.57 76.38 91.26 72.84 75.26PA 90.14 74.74 76.56 91.26 73.36 75.87Table 2: Comparison of NA and PA with previousbest results for HindiImprovement in case of MST+MAXENT isgreater than that of Malt.
One reason is becauseof more number of instances of multiple sub-jects/objects in case of MST+MAXENT.
Otherreason is use of probabilities in caseMST+MAXENT.
Whereas in case of Malt, weinterpreted the scores as probabilities which isnot a good way to do.
But, in case of Malt, that isthe only option available.4.2 CzechIn case of Czech, we replicated the experimentsof Hall et al (2007) using latest version of Malt(version 1.3.1) and analyzed the output.
We con-sider this as the baseline.
The minor variation ofthe baseline results from the results of CoNLL-2007 shared task is due to different version Maltparser being used.
Due to practical reasons wecouldn't use the older version.
In the output ofMalt, there are 39 instances of multiple sub-jects/objects out of 286 sentences in the testingdata.
In case of Czech, the equivalent labels forsubject and object are ?agent?
and ?theme?.Czech is a free-word-order language similar toHindi.
So as expected, PA performed better thanNA.
Interestingly, accuracy of PA is lower thanthe baseline.
Main reason for this is scores oflibsvm of Malt.
We explain the reason for thisusing the following example, consider a verb ?V?has two children ?C1?
and ?C2?
with dependencylabel subject.
Assume that the label for ?C1?
issubject and the label of ?C2?
is object in the gold-data.
As the parser marked ?C1?
with subject, this106adds to the accuracy of the parser.
While avoid-ing multiple subjects, if ?C1?
is marked as sub-ject, then the accuracy doesn't drop.
If ?C2?
ismarked as object then the accuracy increases.But, if ?C2?
is marked as subject and ?C1?
ismarked as object then the accuracy drops.
Thiscould happen if probability of ?C1?
having sub-ject as label is lower than ?C1?
having subject asthe label.
This is because of two reasons, (a)parser itself wrongly predicted the probabilities,and (b) parser predicted correctly, but due to thelimitation of libsvm, we couldn't get the scorescorrectly.UAS LAS LSBaseline 82.92 76.32 83.69NA 82.92 75.92 83.35PA 82.92 75.97 83.40Table 3: Comparison of NA and PA with previousbest results for Czech5 Discussion and Future WorkResults show that the probabilistic approach per-forms consistently better than the naive ap-proach.
For Hindi, we could able to achieve animprovement 0.26% and 0.61% in LAS over theprevious best results using Malt and MST re-spectively.
We couldn?t able to achieve any im-provement in case of Czech due to the limitationof libsvm learner used in Malt.We plan to evaluate our approaches on all thedata-sets of CoNLL-X and CoNLL-2007 sharedtasks using Malt.
Settings of MST parser areavailable only for CoNLL-X shared task datasets.
So, we plan to evaluate our approaches onCoNLL-X shared task data using MST also.
Malthas the limitation for extracting probabilities dueto libsvm learner.
Latest version of Malt (version1.3.1) provides option for liblinear learner also.Liblinear provides option for extracting probabil-ities.
So we can also use liblinear learning algo-rithm for Malt and explore the usefulness of ourapproaches.
Currently, we are handling only twolabels, subject and object.
Apart from subject andobject there can be other labels for which mul-tiple instances for a single verb is not valid.
Wecan extend our approaches to handle such labelsalso.
We tried to incorporate one simple linguis-tic constraint in the statistical dependency pars-ers.
We can also explore the ways of incorporat-ing other useful linguistic constraints.6 ConclusionStatistical systems with high accuracy are veryuseful in practical applications.
If these systemscan capture basic linguistic information, then theusefulness of the statistical system improves alot.
In this paper, we presented a new method ofincorporating linguistic constraints into the sta-tistical dependency parsers.
We took a simpleconstraint that a verb should not have multiplesubjects/objects as its children.
We proposed twoapproaches, one based on position and the otherbased on probabilities to handle this.
We eva-luated our approaches on state-of-the-art depen-dency parsers for Hindi and Czech.AcknowledgmentsI would like to express my gratitude to Prof. Joa-kim Nivre and Prof. Rajeev Sangal for theirguidance and support.
I would also like to thankMr.
Samar Husain for his valuable suggestions.ReferencesB.
R. Ambati, P. Gadde and K. Jindal.
2009.
Experi-ments in Indian Language Dependency Parsing.
InProceedings of the ICON09 NLP Tools Contest:Indian Language Dependency Parsing, pp 32-37.R.
Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,and R. Sangal.
2008.
Dependency annotationscheme for Indian languages.
In Proceedings ofIJCNLP-2008.A.
Bharati, V. Chaitanya and R. Sangal.
1995.
Natu-ral Language Processing: A Paninian Perspective,Prentice-Hall of India, New Delhi, pp.
65-106.A.
Bharati, S. Husain, D. M. Sharma, and R. Sangal.2008.
A Two-Stage Constraint Based DependencyParser for Free Word Order Languages.
In Pro-ceedings of the COLIPS International Conferenceon Asian Language Processing 2008 (IALP).Chiang Mai, Thailand.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.of the Tenth Conf.
on Computational Natural Lan-guage Learning (CoNLL).E.
Hajicova.
1998.
Prague Dependency Treebank:From Analytic to Tectogrammatical Annotation.
InProc.
TSD?98.J.
Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,M.
Nilsson and M. Saers.
2007.
Single Malt orBlended?
A Study in Multilingual Parser Optimiza-tion.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL.R.
Hudson.
1984.
Word Grammar, Basil Blackwell,108 Cowley Rd, Oxford, OX4 1JF, England.107S.
Husain.
2009.
Dependency Parsers for Indian Lan-guages.
In Proceedings of ICON09 NLP ToolsContest: Indian Language Dependency Parsing.Hyderabad, India.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Treebank, Computational Linguistics1993.I.
A. Mel'?uk.
1988.
Dependency Syntax: Theory andPractice, State University, Press of New York.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Mul-tilingual dependency analysis with a two-stage dis-criminative parser.
In Proceedings of the TenthConference on Computational Natural LanguageLearning (CoNLL-X), pp.
216?220.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.In Proc.
of EMNLP-CoNLL.J.
Nivre, J.
Hall, S. Kubler, R. McDonald, J. Nilsson,S.
Riedel and D. Yuret.
2007a.
The CoNLL 2007Shared Task on Dependency Parsing.
In Proceed-ings of EMNLP/CoNLL-2007.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit, S.K?bler, S. Marinov and E Marsi.
2007b.
MaltPars-er: A language-independent system for data-drivendependency parsing.
Natural Language Engineer-ing, 13(2), 95-135.S.
Riedel, Ruket ?ak?c?
and Ivan Meza-Ruiz.
2006.Multi-lingual Dependency Parsing with Incremen-tal Integer Linear Programming.
In Proceedings ofthe Tenth Conference on Computational NaturalLanguage Learning (CoNLL-X).S.
M. Shieber.
1985.
Evidence against the context-freeness of natural language.
In Linguistics andPhilosophy, p. 8, 334?343.108
