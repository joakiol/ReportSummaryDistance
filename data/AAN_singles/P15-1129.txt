Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1332?1342,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsBuilding a Semantic Parser OvernightYushi Wang?Stanford Universityyushiw@cs.stanford.eduJonathan Berant?Stanford Universityjoberant@stanford.eduPercy LiangStanford Universitypliang@cs.stanford.eduAbstractHow do we build a semantic parser in anew domain starting with zero training ex-amples?
We introduce a new methodol-ogy for this setting: First, we use a simplegrammar to generate logical forms pairedwith canonical utterances.
The logicalforms are meant to cover the desired setof compositional operators, and the canon-ical utterances are meant to capture themeaning of the logical forms (althoughclumsily).
We then use crowdsourcing toparaphrase these canonical utterances intonatural utterances.
The resulting data isused to train the semantic parser.
We fur-ther study the role of compositionality inthe resulting paraphrases.
Finally, we testour methodology on seven domains andshow that we can build an adequate se-mantic parser in just a few hours.1 IntroductionBy mapping natural language utterances to exe-cutable logical forms, semantic parsers have beenuseful for a variety of applications requiring pre-cise language understanding (Zelle and Mooney,1996; Zettlemoyer and Collins, 2005; Liang etal., 2011; Berant et al, 2013; Kwiatkowski et al,2013; Artzi and Zettlemoyer, 2013; Kushman andBarzilay, 2013).
Previous work has focused onhow to train a semantic parser given input utter-ances, but suppose we wanted to build a seman-tic parser for a new domain?for example, a natu-ral language interface into a publications database.Since no such interface exists, we do not even havea naturally occurring source of input utterancesthat we can annotate.
So where do we start?In this paper, we advocate a functionality-driven process for rapidly building a semantic?Both authors equally contributed to the paper.DomainSeed lexiconarticle ?
TypeNP[article]publication date?
RelNP[publicationDate]cites ?
VP/NP[cites]...Logical forms and canonical utterancesarticle that has the largest publication dateargmax(type.article, publicationDate)person that is author of the most number of articleargmax(type.person,R(?x.count(type.article u author.x)))...Paraphraseswhat is the newest published article?who has published the most articles?...Semantic parser(1) by builder (?30 minutes)(2) via domain-general grammar(3) via crowdsourcing (?5 hours)(4) by training a paraphrasing modelFigure 1: Functionality-driven process for build-ing semantic parsers.
The two red boxes are thedomain-specific parts provided by the builder ofthe semantic parser, and the other two are gener-ated by the framework.parser in a new domain.
At a high-level, weseek to minimize the amount of work neededfor a new domain by factoring out the domain-general aspects (done by our framework) fromthe domain-specific ones (done by the builderof the semantic parser).
We assume that thebuilder already has the desired functionality ofthe semantic parser in mind?e.g., the publica-tions database is set up and the schema is fixed.Figure 1 depicts the functionality-driven process:First, the builder writes a seed lexicon specifyinga canonical phrase (?publication date?)
for1332each predicate (publicationDate).
Second,our framework uses a domain-general grammar,along with the seed lexicon and the database, toautomatically generate a few hundred canonicalutterances paired with their logical forms (e.g.,?article that has the largest publication date?
andargmax(type.article, publicationDate)).These utterances need not be the most elegant,but they should retain the semantics of the logicalforms.
Third, the builder leverages crowdsourcingto paraphrase each canonical utterance into afew natural utterances (e.g., ?what is the newestpublished article??).
Finally, our framework usesthis data to train a semantic parser.Practical advantages.
There are two main ad-vantages of our approach: completeness and easeof supervision.
Traditionally, training data iscollected in a best-effort manner, which can re-sult in an incomplete coverage of functionality.For example, the WebQuestions dataset (Berantet al, 2013) contains no questions with numericanswers, so any semantic parser trained on thatdataset would lack that functionality.
These bi-ases are not codified, which results in an idiosyn-cratic and mysterious user experience, a majordrawback of natural language interfaces (Rangelet al, 2014).
In contrast, our compact grammarprecisely specifies the logical functionality.
Weenforce completeness by generating canonical ut-terances that exercise every grammar rule.In terms of supervision, state-of-the-art seman-tic parsers are trained from question-answer pairs(Kwiatkowski et al, 2013; Berant and Liang,2014).
Although this is a marked improvement incost and scalability compared to annotated logicalforms, it still requires non-trivial effort: the an-notator must (i) understand the question and (ii)figure out the answer, which becomes even harderwith compositional utterances.
In contrast, ourmain source of supervision is paraphrases, whichonly requires (i), not (ii).
Such data is thus cheaperand faster to obtain.Linguistic reflections.
The centerpiece of ourframework is a domain-general grammar that con-nects logical forms with canonical utterances.This connection warrants further scrutiny, as thestructural mismatch between logic and languageis the chief source of difficulty in semantic pars-ing (Liang et al, 2011; Kwiatkowski et al, 2013;Berant and Liang, 2014).There are two important questions here.
First, isit possible to design a simple grammar that simul-taneously generates both logical forms and canon-ical utterances so that the utterances are under-standable by a human?
In Section 3, we show howto choose appropriate canonical utterances to max-imize alignment with the logical forms.Second, our grammar can generate an infinitenumber of canonical utterances.
How many dowe need for adequate coverage?
Certainly, singlerelations is insufficient: just knowing that ?pub-lication date of X?
paraphrases to ?when X waspublished?
would offer insufficient information togeneralize to ?articles that came after X?
mappingto ?article whose publication date is larger thanpublication date of X?.
We call this phenomenasublexical compositionality?when a short lexicalunit (?came after?)
maps onto a multi-predicatelogical form.
Our hypothesis is that the sublexi-cal compositional units are small, so we only needto crowdsource a small number of canonical utter-ances to learn about most of the language variabil-ity in the given domain (Section 4).We applied our functionality-driven process toseven domains, which were chosen to explore par-ticular types of phenomena, such as spatial lan-guage, temporal language, and high-arity rela-tions.
This resulted in seven new semantic parsingdatasets, totaling 12.6K examples.
Our approach,which was not tuned on any one domain, was ableto obtain an average accuracy of 59% over all do-mains.
On the day of this paper submission, wecreated an eighth domain and trained a semanticparser overnight.2 Approach OverviewIn our functionality-driven process (Figure 1),there are two parties: the builder, who providesdomain-specific information, and the framework,which provides domain-general information.
Weassume that the builder has a fixed database w,represented as a set of triples (e1, p, e2), where e1and e2are entities (e.g., article1, 2015) and p isa property (e.g., publicationDate).
The databasew can be queried using lambda DCS logical forms,described further in Section 2.1.The builder supplies a seed lexicon L, whichcontains for each database property p (e.g.,publicationDate) a lexical entry of the form?t?
s[p]?, where t is a natural language phrase(e.g., ?publication date?)
and s is a syntactic cat-1333egory (e.g., RELNP).
In addition, L containstwo typical entities for each semantic type in thedatabase (e.g., ?alice?
NP[alice]?
for the typeperson).
The purpose of L is to simply connecteach predicate with some representation in naturallanguage.The framework supplies a grammar G, whichspecifies the modes of composition, both on log-ical forms and canonical utterances.
Formally, Gis a set of rules of the form ??1.
.
.
?n?
s[z]?,where ?1.
.
.
?nis a sequence of tokens or cate-gories, s is a syntactic category and z is the log-ical form constructed.
For example, one rule inG is ?RELNP[r] of NP[x]?
NP[R(r).x]?, whichconstructs z by reversing the binary predicater and joining it with a the unary predicate x.We use the rules G ?
L to generate a set of(z, c) pairs, where z is a logical form (e.g.,R(publicationDate).article1), and c is thecorresponding canonical utterance (e.g., ?publica-tion date of article 1?).
The set of (z, c) is denotedby GEN(G ?
L).
See Section 3 for details.Next, the builder (backed by crowdsourcing)paraphrases each canonical utterance c outputabove into a set of natural utterances P(c) (e.g.,?when was article 1 published??).
This defines aset of training examples D = {(x, c, z)}, for each(z, c) ?
GEN(G ?
L) and x ?
P(c).
The crowd-sourcing setup is detailed in Section 5.Finally, the framework trains a semantic parseron D. Our semantic parser is a log-linear distribu-tion p?
(z, c | x,w) over logical forms and canon-ical utterances specified by the grammar G. Notethat the grammar G will in general not parse x, sothe semantic parsing model will be based on para-phrasing, in the spirit of Berant and Liang (2014).To summarize, (1) the builder produces a seedlexicon L; (2) the framework produces logicalforms and canonical utterances GEN(G ?
L) ={(z, c)}; (3) the builder (via crowdsourcing) usesP(?)
to produce a dataset D = {(x, c, z)}; and (4)the framework uses D to train a semantic parserp?
(z, c | x,w).2.1 Lambda DCSOur logical forms are represented in lambda DCS,a logical language where composition operates onsets rather than truth values.
Here we give a briefdescription; see Liang (2013) for details.Every logical form z in this paper is either aunary (denoting a set of entities) or a binary (de-noting a set of entity-pairs).
In the base case, eachentity e (e.g., 2015) is a unary denoting the single-ton set: JeKw= {e}; and each property p (e.g.,publicationDate) is a binary denoting all entity-pairs (e1, e2) that satisfy the property p. Unariesand binaries can be composed: Given a binary band unary u, the join b.u denotes all entities e1forwhich there exists an e2?
JuKwwith (e1, e2) ?JbKw.
For example, publicationDate.2015 de-note entities published in 2015.The intersection u1u u2, union u1unionsq u2, com-plement ?u denote the corresponding set op-erations on the denotations.
We let R(b) de-note the reversal of b: (e1, e2) ?
JbKwiff(e2, e1) ?
JR(b)Kw.
This allows us to defineR(publicationDate).article1 as the publica-tion date of article 1.
We also include aggregationoperations (count(u), sum(u) and average(u, b)),and superlatives (argmax(u, b)).Finally, we can construct binaries using lambdaabstraction: ?x.u denotes a set of (e1, e2) wheree1?
Ju[x/e2]Kwand u[x/e2] is the logical formwhere free occurrences of x are replaced with e2.For example, R(?x.count(R(cites).x)) denotesthe set of entities (e1, e2), where e2is the numberof entities that e1cites.3 Generation and canonicalcompositionalityOur functionality-driven process hinges on havinga domain-general grammar that can connect logi-cal forms with canonical utterances composition-ally.
The motivation is that while it is hard to writea grammar that parses all utterances, it is possibleto write one that generates one canonical utterancefor each logical form.
To make this explicit:Assumption 1 (Canonical compositionality)Using a small grammar, all logical forms ex-pressible in natural language can be realizedcompositionally based on the logical form.Grammar.
We target database querying appli-cations, where the parser needs to handle superla-tives, comparatives, negation, and coordination.We define a simple grammar that captures theseforms of compositionality using canonical utter-ances in a domain-general way.
Figure 2 illustratesa derivation produced by the grammar.The seed lexicon specified by the builder con-tains canonical utterances for types, entities, andproperties.
All types (e.g., person) have the syn-tactic category TYPENP, and all entities (e.g.,1334NP[type.article u publicationDate.1950]NP[type.article]TypeNP[article]articlewhose RelNP[publicationDate]publication dateis EntityNP[1950]1950Figure 2: Deriving a logical form z (red) and acanonical utterance c (green) from the grammarG.
Each node contains a syntactic category anda logical form, which is generated by applying arule.
Nodes with only leaves as children are pro-duced using the seed lexicon; all other nodes areproduced by rules in the domain-general grammar.alice) are ENTITYNP?s.
Unary predicates arerealized as verb phrases VP (e.g., ?has a privatebath?).
The builder can choose to represent bina-ries as either relational noun phrases (RELNP) orgeneralized transitive verbs (VP/NP).
RELNP?sare usually used to describe functional proper-ties (e.g., ?publication date?
), especially numer-ical properties.
VP/NP?s include transitive verbs(?cites?)
but also longer phrases with the samesyntactic interface (?is the president of?).
Table1 shows the seed lexicon for the SOCIAL domain.From the seed lexicon, the domain-generalgrammar (Table 2) constructs noun phrases (NP),verbs phrases (VP), and complementizer phrase(CP), all of which denote unary logical forms.Broadly speaking, the rules (R1)?
(R4), (C1)?
(C4)take a binary and a noun phrase, and composethem (optionally via comparatives, counting, andnegation) to produce a complementizer phrase CPrepresenting a unary (e.g., ?that cites article 1?or ?that cites more than three article?).
(G3)combines these CP?s with an NP (e.g., ?article?
).In addition, (S0)?
(S4) handle superlatives (we in-clude argmin in addition to argmax), which takean NP and return the extremum-attaining subset ofits denotation.
Finally, we support transformationssuch as join (T1) and disjunction (T4), as well asaggregation (A1)?
(A2).Rendering utterances for multi-arity predicateswas a major challenge.
The predicate in-stances are typically reified in a graph database,akin to a neo-Davidsonian treatment of events:There is an abstract entity with binary predi-cates relating it to its arguments.
For exam-ple, in the SOCIAL domain, Alice?s educationcan be represented in the database as five triples:birthdate ?
RELNP[birthdate]person|university|field ?
TYPENP[person| ?
?
?
]company|job title ?
TYPENP[company| ?
?
?
]student|university|field of study ?
RELNP[student| ?
?
?
]employee|employer|job title ?
RELNP[employee| ?
?
?
]start date|end date ?
RELNP[startDate| ?
?
?
]is friends with ?
VP/NP[friends| ?
?
?
]Table 1: The seed lexicon for the SOCIAL do-main, which specifies for each predicate (e.g.,birthdate) a phrase (e.g., ?birthdate?)
that real-izes that predicate and its syntactic category (e.g.,RELNP).
(e17, student, alice), (e17, university, ucla),(e17, fieldOfStudy, music),(e17, startDate, 2005), (e17, endDate, 2009).All five properties here are represented asRELNP?s, with the first one designated as the sub-ject (RELNP0).
We support two ways of queryingmulti-arity relations: ?student whose university isucla?
(T2) and ?university of student Alice whosestart date is 2005?
(T3).Generating directly from the grammar in Ta-ble 2 would result in many uninterpretable canon-ical utterances.
Thus, we perform type checkingon the logical forms to rule out ?article that cites2004?, and limit the amount of recursion, whichkeeps the canonical utterances understandable.Still, the utterances generated by our grammarare not perfectly grammatical; we do not use de-terminers and make all nouns singular.
Nonethe-less, AMT workers found most canonical utter-ances understandable (see Table 3 and Section 5for details on crowdsourcing).
One tip for thebuilder is to keep the RELNP?s and VP/NP?s ascontext-independent as possible; e.g., using ?pub-lication date?
instead of ?date?.
In cases wheremore context is required, we use parenthetical re-marks (e.g., ?number of assists (over a season)??
RELNP[...]) to pack more context into theconfines of the part-of-speech.Limitations.
While our domain-general gram-mar covers most of the common logical formsin a database querying application, there are sev-eral phenomena which are out of scope, notablynested quantification (e.g., ?show me each au-thor?s most cited work?)
and anaphora (e.g., ?au-thor who cites herself at least twice?).
Handlingthese would require a more radical change to thegrammar, but is still within scope.1335[glue](G1) ENTITYNP[x] ?
NP[x](G2) TYPENP[x] ?
NP[type.x](G3) NP[x] CP[f ] (and CP[g])* ?
NP[x u f u g][simple](R0) that VP[x] ?
CP[x](R1) whose RELNP[r] CMP[c] NP[y] ?
CP[r.c.y]is|is not|is smaller than|is larger than|is at least|is at most ?
CMP[= | 6= | < | > | ?
| ?
](R2) that (not)?
VP/NP[r] NP[y] ?
CP[(?
)r.y](R3) that is (not)?
RELNP[r] of NP[y] ?
CP[(?
)R(r).y](R4) that NP[y] (not)?
VP/NP[r] ?
CP[(?
)(R(r).y)][counting](C1) that has CNT[c] RELNP[r] ?
CP[R(?x.count(R(r).x)).c](C2) that VP/NP[r] CNT[c] NP[y] ?
CP[R(?x.count(y uR(r).x)).c](C3) that is RELNP[r] of CNT[c] NP[y] ?
CP[R(?x.count(y u r.x)).c](C4) that CNT[c] NP[y] VP/NP[r] ?
CP[R(?x.count(y u r.x)).c](less than|more than) NUM[n] ?
CNT[(< .| > .
)n][superlatives](S0) NP[x] that has the largest RELNP[r] ?
NP[argmax(x, r)](S1) NP[x] that has the most number of RELNP[r] ?
NP[argmax(x,R(?y.count(R(r).y)))](S2) NP[x] that VP/NP[r] the most number of NP[y] ?
NP[argmax(x,R(?y.count(R(r).y)))](S3) NP[x] that is RELNP[r] of the most number of NP[y] ?
NP[argmax(x,R(?z.count(y u r.z)))](S4) NP[x] that the most number of NP[y] VP/NP[r] ?
NP[argmax(x,R(?z.count(y u r.z)))][transformation](T1) RELNP[r] of NP[y] ?
NP[R(r).y](T2) RELNP0[h]CP[f ] (and CP[g])* ?
NP[R(h).
(f u g)](T3) RELNP[r] of RELNP0[h] NP[x] CP[f ] (and CP[g])* ?
NP[R(r).
(h.x u f u g)](T4) NP[x] or NP[y] ?
NP[x unionsq y][aggregation](A1) number of NP[x] ?
NP[count(x)](A2) total|average RELNP[r] of NP[x] ?
NP[sum|average(x, r)]Table 2: The domain-general grammar which is combined with the seed lexicon to generate logical formsand canonical utterances that cover the supported logical functionality.4 Paraphrasing and boundednon-compositionalityWhile the canonical utterance c is generatedcompositionally along with the logical form z,natural paraphrases x ?
P(c) generally devi-ate from this compositional structure.
For ex-ample, the canonical utterance ?NP[number ofNP[article CP[whose publication date is largerthan NP[publication date of article 1]]]]?
mightget paraphrased to ?How many articles were pub-lished after article 1??.
Here, ?published after?non-compositionally straddles the inner NP, intu-itively responsible for both instances of ?publica-tion date?.
But how non-compositional can para-phrases be?
Our framework rests on the assump-tion that the answer is ?not very?
:Assumption 2 (Bounded non-compositionality)Natural utterances for expressing complex logicalforms are compositional with respect to fragmentsof bounded size.In the above example, note that while ?publishedafter?
is non-compositional with respect to thegrammar, the rewriting of ?number of?
to ?howmany?
is compositional.
The upshot of Assump-tion 2 is that we only need to ask for paraphrasesof canonical utterances generated by the grammarup to some small depth to learn about all the non-compositional uses of language, and still be ablegeneralize (compositionally) beyond that.We now explore the nature of the possible para-phrases.
Broadly speaking, most paraphrases in-volve some sort of compression, where the clunkybut faithful canonical utterance is smoothed outinto graceful prose.Alternations of single rules.
The most basicparaphrase happens at the single phrase level withsynonyms (?block?
to ?brick?
), which preservethe part-of-speech.
However, many of our prop-erties are specified using relational noun phrases,which are more naturally realized using preposi-tions (?meeting whose attendee is alice ?
meet-ing with alice?)
or verbs (?author of article 1?who wrote article 1?).
If the RELNP is com-plex, then the argument can become embedded:?player whose number of points is 15 ?
playerwho scored 15 points?.
Superlative and compara-tive constructions reveal other RELNP-dependentwords: ?article that has the largest publicationdate ?
newest article?.
When the value of the1336relation has enough context, then the relation iselided completely: ?housing unit whose housingtype is apartment?
apartment?.Multi-arity predicates are compressed into asingle frame: ?university of student alice whosefield of study is music?
becomes ?At which uni-versity did Alice study music?
?, where the se-mantic roles of the verb ?study?
carry the bur-den of expressing the multiple relations: student,university, and fieldOfStudy.
With a differentcombination of arguments, the natural verb wouldchange: ?Which university did Alice attend?
?Sublexical compositionality.
The most interest-ing paraphrases occur across multiple rules, a phe-nomenon which we called sublexical composition-ality.
The idea is that common, multi-part con-cepts are compressed to single words or simplerconstructions.
The simplest compression is a lex-ical one: ?parent of alice whose gender is female?
mother of alice?.
Compression often occurswhen we have the same predicate chained twicein a join: ?person that is author of paper whoseauthor is X?
co-author of X?
or ?person whosebirthdate is birthdate of X ?
person born on thesame day as X?.
When two CP?s combined viacoordination have some similarity, then the co-ordination can be pushed down (?meeting whosestart time is 3pm and whose end time is 5pm ?meetings between 3pm and 5pm?)
and sometimeseven generalized (?that allows cats and that al-lows dogs?
that allows pets?).
Sometimes, com-pression happens due to metonymy, where peoplestand in for their papers: ?author of article that ar-ticle whose author is X cites?
who does X cite?.5 CrowdsourcingWe tackled seven domains covering various lin-guistic phenomena.
Table 3 lists the domains, theirprincipal phenomena, statistics about their predi-cates and dataset, and an example from the dataset.We use Amazon Mechanical Turk (AMT) toparaphrase the canonical utterances generated bythe domain-general grammar.
In each AMT task,a worker is presented with four canonical utter-ances and is asked to reformulate them in natu-ral language or state that they are incomprehensi-ble.
Each canonical utterance was presented to 10workers.
Over all domains, we collected 18,032responses.
The average time for paraphrasing oneutterance was 28 seconds.
Paraphrases that sharethe same canonical utterance are collapsed, whileidentical paraphrases that have distinct canonicalutterances are deleted.
This produced a total of12,602 examples over all domains.To estimate the level of noise in the data, wemanually judged the correctness of 20 examples ineach domain, and found that 17% of the utteranceswere inaccurate.
There are two main reasons: lex-ical ambiguity on our part (?player that has theleast number of team ?
player with the lowestjersey number?
), and failure on the worker?s part(?restaurant whose star rating is 3 stars ?
hotelwhich has a 3 star rating?
).6 Model and LearningOur semantic parsing model defines a distribu-tion over logical forms given by the domain-general grammar G and additional rules trig-gered by the input utterance x. Specifically,given an utterance x, we detect numbers, dates,and perform string matching with database en-tities to recognize named entities.
This resultsin a set of rules T(x).
For example, if xis ?article published in 2015 that cites article1?, then T(x) contains ?2015?
NP[2015]?
and?article 1?
NP[article1]?.
Let Lxbe the rulesin the seed lexicon L where the entity rules (e.g.,?alice?
NP[alice]?)
are replaced by T(x).Our semantic parsing model defines a log-linear distribution over candidate pairs (z, c) ?GEN(G ?
Lx):p?
(z, c | x,w) ?
exp(?
(c, z, x, w)>?
), (1)where ?
(z, c, x, w) ?
Rdis a feature vector and?
?
Rdis a parameter vector.To generate candidate logical forms, we use asimple beam search: For each search state, whichincludes the syntactic category s (e.g., NP) andthe depth of the logical form, we generate at mostK = 20 candidates by applying the rules in Ta-ble 2.
In practice, the lexical rules T(x) are ap-plied first, and composition is performed, but notconstrained to the utterance.
For example, the ut-terance ?article?
would generate the logical formcount(type.article).
Instead, soft paraphras-ing features are used to guide the search.
Thisrather unorthodox approach to semantic parsingcan be seen as a generalization of Berant andLiang (2014) and is explained in more detail inPasupat and Liang (2015).Training.
We train our model by maximiz-ing the regularized log-likelihood O(?)
=1337Domain # pred.
# ex.
Phenomena ExampleCALENDAR 22 837 temporal language x: ?Show me meetings after the weekly standup day?c: ?meeting whose date is at least date of weekly standup?z: type.meeting u date.
> R(date).weeklyStandupBLOCKS 19 1995 spatial language x: ?Select the brick that is to the furthest left.
?c: ?block that the most number of block is right of?z: argmax(type.block,R(?x.count(R(right).x)))HOUSING 24 941 measurement units x: ?Housing that is 800 square feet or bigger?
?c: ?housing unit whose size is at least 800 square feet?z: type.housingUnit u area.
> .800RESTAURANTS 32 1657 long unary relations x: ?What restaurant can you eat lunch outside at?
?c: ?restaurant that has outdoor seating and that serves lunch?z: type.restaurant u hasOutdoorSeating u serveslunchPUBLICATIONS 15 801 sublexical compositionality x: ?Who has co-authored articles with Efron?
?c: ?person that is author of article whose author is efron?z: type.person uR(author).
(type.article u author.efron)SOCIAL 45 4419 multi-arity relations x: ?When did alice start attending brown university?
?c: ?start date of student alice whose university is brown university?z: R(date).
(student.Alice u university.Brown)BASKETBALL 24 1952 parentheticals x: ?How many fouls were played by Kobe Bryant in 2004?
?c: ?number of fouls (over a season) of player kobe bryant whose season is 2004?z: count(R(fouls).
(player.KobeBryant u season.2004)Table 3: We experimented on seven domains, covering a variety of phenomena.
For each domain, weshow the number of predicates, number of examples, and a (c, z) generated by our framework along witha paraphrased utterance x.Category DescriptionBasic #words and bigram matches in (x, c)#words and bigram PPDB matches in (x, c)#unmatched words in x#unmatched words in csize of denotation of z, (|JzKw|)pos(x0:0) conjoined with type(JzKw)#nodes in tree generating zLexical ?
(i, j) ?
A.
(xi:i, cj:j)?
(i, j) ?
A.
(xi:i, cj:j+1)?
(i, j) ?
A.
(xi:i, cj?1:j)?
(i, j), (i + 1, j + 1) ?
A.
(xi:i+1, cj:j+1)all unaligned words in x and c(xi:j, ci?:j?)
if in phrase tableTable 4: Features for the paraphrasing model.pos(xi:i) is the POS tag; type(JzKw) is a coarse se-mantic type for the denotation (an entity or a num-ber).
A is a maximum weight alignment betweenx and c.?
(x,c,z)?Dlog p?
(z, c | x,w) ?
????1.
To opti-mize, we use AdaGrad (Duchi et al, 2010).Features Table 4 describes the features.
Ourbasic features mainly match words and bigramsin x and c, if they share a lemma or are alignedin the PPDB resource (Ganitkevitch et al, 2013).We count the number of exact matches, PPDBmatches, and unmatched words.To obtain lexical features, we run the BerkeleyAligner (Liang et al, 2006) on the training set andcompute conditional probabilities of aligning oneword type to another.
Based on these probabilitieswe compute a maximum weight alignment A be-tween words in x and c. We define features overA(see Table 4).
We also use the word alignments toconstruct a phrase table by applying the consistentphrase pair heuristic (Och and Ney, 2004).
We de-fine an indicator feature for every phrase pair ofx and c that appear in the phrase table.
Examplesfrom the PUBLICATIONS domain include fewest?least number and by?whose author is.
Note thatwe do not build a hard lexicon but only use Aand the phrase table to define features, allowingthe model to learn useful paraphrases during train-ing.
Finally, we define standard features on logicalforms and denotations (Berant et al, 2013).7 Experimental EvaluationWe evaluated our functionality-driven process onthe seven domains described in Section 5 and onenew domain we describe in Section 7.3.
For eachdomain, we held out a random 20% of the exam-ples as the test set, and performed development onthe remaining 80%, further splitting it to a train-ing and development set (80%/20%).
We created adatabase for each domain by randomly generatingfacts using entities and properties in the domain(with type-checking).
We evaluated using accu-racy, which is the fraction of examples that yieldthe correct denotation.7.1 Domain-specific linguistic variabilityOur functionality-driven process is predicated onthe fact that each domain exhibits domain-specific1338Method CALENDAR BLOCKS HOUSING RESTAURANTS PUBLICATIONS RECIPES SOCIAL BASKETBALL Avg.FULL 74.4 41.9 54.0 75.9 59.0 70.8 48.2 46.3 58.8NOLEX 25.0 35.3 51.9 64.6 50.6 32.3 15.3 19.4 36.8NOPPDB 73.2 41.4 54.5 73.8 56.5 68.1 43.6 44.5 56.9BASELINE 17.3 27.7 45.9 61.3 46.7 26.3 9.7 15.6 31.3Table 5: Test set results on all domains and baselines.phenomena.
To corroborate this, we compare ourfull system to NOLEX, a baseline that omits alllexical features (Table 4), but uses PPDB as adomain-general paraphrasing component.
We per-form the complementary experiment and compareto NOPPDB, a baseline that omits PPDB matchfeatures.
We also run BASELINE, where we omitboth lexical and PPDB features.Table 5 presents the results of this experiment.Overall, our framework obtains an average accu-racy of 59% across all eight domains.
The per-formance of NOLEX is dramatically lower thanFULL, indicating that it is important to learndomain-specific paraphrases using lexical fea-tures.
The accuracy of NOPPDB is only slightlylower than FULL, showing that most of the re-quired paraphrases can be learned during training.As expected, removing both lexical and PPDB fea-tures results in poor performance (BASELINE).Analysis.
We performed error analysis on 10 er-rors in each domain.
Almost 70% of the errorsare due to problems in the paraphrasing model,where the canonical utterance has extra material,is missing some content, or results in an incorrectparaphrase.
For example, ?restaurants that havewaiters and you can sit outside?
is paraphrased to?restaurant that has waiter service and that takesreservations?.
Another 12.5% result from reorder-ing issues, e.g, we paraphrase ?What venue hasfewer than two articles?
to ?article that has lessthan two venue?.
Inaccurate paraphrases providedby AMT workers account for the rest of the errors.7.2 Bounded non-compositionalityWe hypothesized that we need to obtain para-phrases of canonical utterances corresponding tological forms of only small depth.
We ran thefollowing experiment in the CALENDAR domainto test this claim.
First, we define by NP0, NP1,and NP2the set of utterances generated by an NPthat has exactly zero, one, and two NPs embed-ded in it.
We define the training scenario 0?
1,where we train on examples from NP0and teston examples from NP1; 0 ?
1?
1, 0 ?
1?
2,and 0 ?
1 ?
2?
2 are defined analogously.
OurScenario Acc.
Scenario Acc.0?
1 22.9 0 ?
1?
2 28.10 ?
1?
1 85.8 0 ?
1 ?
2?
2 47.5Table 6: Test set results in the CALENDAR domainon bounded non-compositionality.hypothesis is that generalization on 0 ?
1?
2should be better than for 0?
1, since NP1ut-terances have non-compositional paraphrases, buttraining on NP0does not expose them.The results in Table 6 verify this hypothesis.The accuracy of 0?
1 is almost 65% lower than0 ?
1?
1.
On the other hand, the accuracy of0 ?
1?
2 is only 19% lower than 0 ?
1 ?
2?
2.7.3 An overnight experimentTo verify the title of this paper, we attemptedto create a semantic parser for a new domain(RECIPES) exactly 24 hours before the submissiondeadline.
Starting at midnight, we created a seedlexicon in less than 30 minutes.
Then we gener-ated canonical utterances and allowed AMT work-ers to provide paraphrases overnight.
In the morn-ing, we trained our parser and obtained an accu-racy of 70.8% on the test set.7.4 Testing on independent dataGeo880.
To test how our parser generalizes toutterances independent of our framework, we cre-ated a semantic parser for the domain of US ge-ography, and tested on the standard 280 test ex-amples from GEO880 (Zelle and Mooney, 1996).We did not use the standard 600 training examples.Our parser obtained 56.4% accuracy, which is sub-stantially lower than state-of-the-art (?
90%).We performed error analysis on 100 randomsentences from the development set where accu-racy was 60%.
We found that the parser learnsfrom the training data to prefer shorter para-phrases, which accounts for 30% of the errors.
Inmost of these cases, the correct logical form isranked at the top-3 results (accuracy for the top-3 derivations is 73%).
GEO880 contains highlycompositional utterances, and in 25% of the errors1339the correct derivation tree exceeds the maximumdepth used for our parser.
Another 17.5% of theerrors are caused by problems in the paraphrasingmodel.
For example, in the utterance ?what is thesize of california?, the model learns that ?size?corresponds to ?population?
rather than ?area?.Errors related to reordering and the syntactic struc-ture of the input utterance account for 7.5% of theerrors.
For example, the utterance ?what is thearea of the largest state?
is paraphrased to ?statethat has the largest area?.Calendar.
In Section 7.1, we evaluated on ut-terances obtained by paraphrasing canonical utter-ances from the grammar.
To examine the cover-age of our parser on independently-produced ut-terances, we asked AMT workers to freely comeup with queries.
We collected 186 such queries; 5were spam and discarded.
We replaced all entities(people, dates, etc.)
with entities from our seedlexicon to avoid focusing on entity detection.We were able to annotate 52% of the utteranceswith logical forms from our grammar.
We couldnot annotate 20% of the utterances due to relativetime references, such as ?What time is my nextmeeting??.
14% of the utterances were not cov-ered due to binary predicates not in the grammar(?What is the agenda of the meeting??)
or missingentities (?When is Dan?s birthday??).
Another 2%required unsupported calculations (?How muchfree time do I have tomorrow??
), and the rest areout of scope for other reasons (?When does myVerizon data plan start over??
).We evaluated our trained semantic parser on the95 utterances annotated with logical forms.
Ourparser obtained an accuracy of 46.3% and oracleaccuracy of 84.2%, which measures how often thecorrect denotation is on the final beam.
The largegap shows that there is considerable room for im-provement in the paraphrasing model.8 Related work and discussionMuch of current excitement around semantic pars-ing emphasizes large knowledge bases such asFreebase (Cai and Yates, 2013; Kwiatkowski etal., 2013; Berant et al, 2013).
However, despitethe apparent scale, the actual question answeringdatasets (Free917 and WebQuestions) are limitedin compositionality.
Moreover, specialized do-mains with specialized jargon will always exist,e.g., in regular expressions (Kushman and Barzi-lay, 2013) or grounding to perception (Matuszeket al, 2012; Tellex et al, 2011; Krishnamurthyand Kollar, 2013).
Therefore, we believe build-ing a targeted domain-specific semantic parser fora new website or device is a very practical goal.Recent work has made significant strides inreducing supervision from logical forms (Zettle-moyer and Collins, 2005; Wong and Mooney,2007) to denotations (Clarke et al, 2010; Liang etal., 2011) and to weaker forms (Artzi and Zettle-moyer, 2011; Reddy et al, 2014).
All of theseworks presuppose having input utterances, whichdo not exist in a new domain.
Our methodol-ogy overcomes this hurdle by exploiting a verylightweight form of annotation: paraphrasing.Paraphrasing has been applied to single-property question answering (Fader et al, 2013)and semantic parsing (Berant and Liang, 2014).We not only use paraphrasing in the semanticparser, but also for data collection.Table 2 might evoke rule-based systems (Woodset al, 1972; Warren and Pereira, 1982) or con-trolled natural languages (Schwitter, 2010).
How-ever, there is an important distinction: the gram-mar need only connect a logical form to onecanonical utterance; it is not used directly for pars-ing.
This relaxation allows the grammar to bemuch simpler.
Our philosophy is to use the simpledomain-general grammar to carry the torch just tothe point of being understandable by a human, andlet the human perform the remaining correction toproduce a natural utterance.In summary, our contributions are two-fold: anew functionality-driven process and an explo-ration of some of its linguistic implications.
Webelieve that our methodology is a promising wayto build semantic parsers, and in future work, wewould like to extend it to handle anaphora andnested quantification.Acknowledgments.
We gratefully acknowledgethe support of the Google Natural Language Un-derstanding Focused Program and the DARPADeep Exploration and Filtering of Text (DEFT)Program contract no.
FA8750-13-2-0040.Reproducibility.
All code,1data, andexperiments for this paper are avail-able on the CodaLab platform athttps://www.codalab.org/worksheets/0x269ef752f8c344a28383240f7bb2be9c/.1Our system uses the SEMPRE toolkit (http://nlp.stanford.edu/software/sempre).1340ReferencesY.
Artzi and L. Zettlemoyer.
2011.
Bootstrap-ping semantic parsers from conversations.
In Em-pirical Methods in Natural Language Processing(EMNLP), pages 421?432.Y.
Artzi and L. Zettlemoyer.
2013.
Weakly supervisedlearning of semantic parsers for mapping instruc-tions to actions.
Transactions of the Association forComputational Linguistics (TACL), 1:49?62.J.
Berant and P. Liang.
2014.
Semantic parsing viaparaphrasing.
In Association for ComputationalLinguistics (ACL).J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).Q.
Cai and A. Yates.
2013.
Large-scale semantic pars-ing via schema matching and lexicon extension.
InAssociation for Computational Linguistics (ACL).J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Computational Natural Language Learn-ing (CoNLL), pages 18?27.J.
Duchi, E. Hazan, and Y.
Singer.
2010.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
In Conference on Learning Theory(COLT).A.
Fader, L. Zettlemoyer, and O. Etzioni.
2013.Paraphrase-driven learning for open question an-swering.
In Association for Computational Linguis-tics (ACL).J.
Ganitkevitch, B. V. Durme, and C. Callison-Burch.2013.
PPDB: The paraphrase database.
In HumanLanguage Technology and North American Associ-ation for Computational Linguistics (HLT/NAACL),pages 758?764.J.
Krishnamurthy and T. Kollar.
2013.
Jointly learningto parse and perceive: Connecting natural languageto the physical world.
Transactions of the Associa-tion for Computational Linguistics (TACL), 1:193?206.N.
Kushman and R. Barzilay.
2013.
Using semanticunification to generate regular expressions from nat-ural language.
In Human Language Technology andNorth American Association for Computational Lin-guistics (HLT/NAACL), pages 826?836.T.
Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.2013.
Scaling semantic parsers with on-the-fly on-tology matching.
In Empirical Methods in NaturalLanguage Processing (EMNLP).P.
Liang, B. Taskar, and D. Klein.
2006.
Align-ment by agreement.
In North American Associationfor Computational Linguistics (NAACL), pages 104?111.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InAssociation for Computational Linguistics (ACL),pages 590?599.P.
Liang.
2013.
Lambda dependency-based composi-tional semantics.
arXiv.C.
Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo,and D. Fox.
2012.
A joint model of language andperception for grounded attribute learning.
In Inter-national Conference on Machine Learning (ICML),pages 1671?1678.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30:417?449.P.
Pasupat and P. Liang.
2015.
Compositional semanticparsing on semi-structured tables.
In Association forComputational Linguistics (ACL).R.
A. P. Rangel, M. A. Aguirre, J. J. Gonzlez, and J. M.Carpio.
2014.
Features and pitfalls that users shouldseek in natural language interfaces to databases.
InRecent Advances on Hybrid Approaches for Design-ing Intelligent Systems, pages 617?630.S.
Reddy, M. Lapata, and M. Steedman.
2014.
Large-scale semantic parsing without question-answerpairs.
Transactions of the Association for Compu-tational Linguistics (TACL), 2(10):377?392.R.
Schwitter.
2010.
Controlled natural languages forknowledge representation.
In International Con-ference on Computational Linguistics (COLING),pages 1113?1121.S.
Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.Banerjee, S. J. Teller, and N. Roy.
2011.
Un-derstanding natural language commands for roboticnavigation and mobile manipulation.
In Associa-tion for the Advancement of Artificial Intelligence(AAAI).D.
Warren and F. Pereira.
1982.
An efficient easilyadaptable system for interpreting natural languagequeries.
Computational Linguistics, 8:110?122.Y.
W. Wong and R. J. Mooney.
2007.
Learningsynchronous grammars for semantic parsing withlambda calculus.
In Association for ComputationalLinguistics (ACL), pages 960?967.W.
A.
Woods, R. M. Kaplan, and B. N. Webber.
1972.The lunar sciences natural language information sys-tem: Final report.
Technical report, BBN Report2378, Bolt Beranek and Newman Inc.M.
Zelle and R. J. Mooney.
1996.
Learning toparse database queries using inductive logic pro-gramming.
In Association for the Advancement ofArtificial Intelligence (AAAI), pages 1050?1055.1341L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Un-certainty in Artificial Intelligence (UAI), pages 658?666.1342
