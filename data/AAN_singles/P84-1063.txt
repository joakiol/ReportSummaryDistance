A Plan Recognition Modelfor Clarification SubdialoguesDiane J. Litman and James F. AllenDepartment ofComputer ScienceUniversity of Rochester, Rochester, NY 14627AbstractOne of the promising approaches to analyzing task-oriented ialogues has involved modeling the plans of thespeakers in the task domain.
In general, these models workwell as long as the topic follows the task structure closely,but they have difficulty in accounting for clarificationsubdialogues and topic change.
We have developed amodel based on a hierarchy of plans and metaplans thataccounts for the clarification subdialogues whilemaintaining the advantages of the plan-based approach.I.
IntroductionOne of the promising approaches to analyzing task-oriented ialogues has involved modeling the plans of thespeakers in the task domain.
The earliest work in this areainvolved tracking the topic of a dialogue by tracking theprogress of the plan in the task domain \[Grosz, 1977\], aswell as explicitly incorporating speech acts into a planningframework \[Cohen and Perrault, 1979; Allen and Perrault,1980\].
A good example of the current status of theseapproaches can be found in \[Carberry, 1983\].
In general,these models work well as long as the topic follows thetask structure closely, but they have difficulty inaccounting for clarification subdialogues and topic change.Sidner and Israel \[1981\]suggest a olution to a class ofclarification subdialogues that correspond to debuggingthe plan in the task domain.
They allow utterances to talkabout the task plan, rather than always being a step in theplan.
Using their suggestions, as well as our early work\[Allen et al, 1982: Litman, 1983\], we have developed amodel based on a hierarchy of plans and metaplans thatThis work was supported in part by the National ScienceFoundation under Grant IST-8210564, the Office of NavalResearch under Grant N00014-80-C-1097, and theDefense Advanced Research Projects Agency under GrantN00014-82-K-0193.accounts for the debugging subdialogues they discussed, aswell as other forms of clarification and topic shi~.Reichman \[1981\] has a structural model of discoursethat addresses clarification subdialogues and topic switchin unconstrained spontaneous discourse.
Unfortunately,there is a large gap between her abstract model and theactual processing of utterances.
Although not the focus ofthis paper, we claim that our new plan recognition modelprovides the link from the processing of actual input to itsabstract discourse structure.
Even more important, thisallows us to use the linguistic results from such work toguide and be guided by our plan recognition.For example, consider the following two dialoguefragments.
The first was collected at an information boothin a train station in Toronto \[Horrigan, 1977\], while thesecond is a scenario developed from protocols in agraphics command and control system that displaysnetwork structures \[Sidner and Bates, 1983\].1) Passenger:2) Clerk:3) Passenger:4) Clerk:5) Passenger:6) User:7) System:8) User:9) System:10) User:11) System:The eight-fifty to Montreal?Eight-fifty to Montreal.
Gate seven.Where is it?Down this way to the left.
Second one onthe left.OK.
Thank you.Dialogue iShow me the generic concept called"employee."OK.
<system displays network>\[ can't fit a new IC below it.
Can youmove it up?Yes.
<system displays network>OK, now make an individual employeeconcept whose first name is "Sam"and whose last name is "Jones."
TheSocial Security number is 234-56-7899.OK.Dialogue 2302While still "task-oriented," these dialogues illustratephenomena characteristic of spontaneous conversation.That is, subdialogues correspond not only to subtasks(utterances (6)-(7) and (10)-(11)), but also to clarifications((3)-(4)), debugging of task execution ((8)-(9)), and othertypes of topic switch and resumption.
Furthermore, sincethese are extended discourses rather than unrelatedquestion/answer xchanges, participants need to use theinformation provided by previous utterances.
For example,(3) would be difficult to understand without he discoursecontext of (1) and (2).
Finally, these dialogues illustratethe following of conversational conventions uch asterminating dialogues (utterance (5)) and answeringquestions appropriately.
For example, in response to (1),the clerk could have conveyed much the same informationwith "The departure location of train 537 is gate seven,"which would not have been as appropriate.To address these issues, we are developing a plan-based natural language system that incorporatesknowledge of both task and discourse structure.
Inparticular, we develop a new model of plan recognitionthat accounts for the recursive nature of plan suspensionsand resumptions.
Section 2 presents this model, followedin Section 3 by a brief description of the discourse analysisperformed and the task and discourse interactions.
Section4 then traces the processing of Dialogue 1 in detail, andthen this work is compared to previous work in Section 5.2.
Task Analysis2.1 The Plan Structuresin addition to the standard domain-dependentknowledge of task plans, we introduce some knowledgeabout the planning process itself.
These are domain-independent plans that refer to the state of other plans.During a dialogue, we shall build a stack of such plans,each plan on the stack referring to the plan below it, withthe domain-dependent task plan at the bottom.
As anexample, a clarification subdialogue is modeled by a planstructure that refers to the plan that is the topic of theclarification.
As we shall see, the manipulations of thisstack of plans is similar to the manipulation of topichierarchies that arise in discourse models.To allow plans about plans, i.e., metaplans, we need avocabulary for referring to and describing plans.Developing a fully adequate formal model would be alarge research effort in its own right.
Our development sofar is meant to be suggestive of what is needed, and isspecific enough for our preliminary implementation.
Weare also, for the purpose of this paper, ignoring alltemporal qualifications (e.g., the constraints need to betemporally qualified), and all issues involving beliefs ofagents.
All plans constructed in this paper should beconsidered mutually known by the speaker and hearer.We consider plans to be networks of actions and statesconnected by links indicating causality and subpartrelationships.
Every plan has a header', a parameterizedaction description that names the plan.
The parameters ofa plan are the parameters in the header.
Associated witheach plan is a set of constraints, which are assertions aboutthe plan and its terms and parameters.
The use ofconstraints will be made clear with examples.
As usual,plans may also contain prerequisites, effects, and adecomposition.
Decompositions may be sequences ofactions, sequences of subgoals to be achieved, or a mixtureof both.
We will ignore most prerequisites and effectsthoughout this paper, except when needed in examples.For example, the first plan in Figure 1 summarizes asimple plan schema with a header "BOARD (agent,train)," with parameters "agent" and "train," and with theconstraint "depart-station (train) = Toronto."
Thisconstraint captures the knowledge that the informationbooth is in the Toronto station.
The plan consists of theHEADER: BOARD (agent, train)STEPS: do BUY-TICKET (agent, train)do GOTO (agent, depart-location (train),depart-time (train))do GETON (agent,train)CONSTRAINTS: depart-station (train) = Toronto.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.HEADER: GOTO (agent, location, time)EFFECT: AT (agent, location, time).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.HEADER: MEET (agent, train)STEPS: do GOTO (agent, arrive-location (train),arrive-time (train))CONSTRAINTS: arrive-station (train) = TorontoFigure I: Domain Plans303shown.
The second plan indicates a primitive action andits effect.
Other plans needed in this domain wouldinclude plans to meet trains, plans to buy tickets, etc.We must also discuss the way terms are described, forsome descriptions of a term are not informative nough toallow a plan to be executed.
What counts as aninformative description varies from plan to plan.
Wedefine the predicate KNOWREF (agent, term, plan) tomean that the agent has a description of the specified termthat is informative nough to execute the specified plan,all other things being equal.
Throughout this paper weassume a typed logic that will be implicit from the namingof variables.
Thus, in the above formula, agent is restrictedto entities capable of agency, term is a description of someobject, and plan is restricted to objects that are plans.Plans about plans, or metaplans, deal with specifyingparts of plans, debugging plans, abandoning plans, etc.
Totalk about the structure of plans we will assume thepredicate IS-PARAMETER-OF (parameter, plan), whichasserts that the specified parameter is a parameter of thespecified plan.
More formally, parameters are skolemfunctions dependent on the plan.Other than the fact that they refer to other plans,metaplans are identical in structure to domain plans.
Twoexamples of metaplans are given in Figure 2.
The first one,SEEK-ID-PARAMETER, is a plan schema to find out asuitable description of the parameter that would allow theplan to be executed.
It has one step in this version, namelyto achieve KNOWREF (agent, parameter, plan), and ithas two constraints that capture the relationship betweenthe metaplan and the plan it concerns, namely that"parameter" must be a parameter of the specified plan,and that its value must be presently unknown.The second metaplan, ASK, involves achievingKNOWREF (agent, term, plan) by asking a question andreceiving back an answer.
Another way to achieveKNOWREF goals would be to look up the answer in areference source.
At the train station, for example, one canfind departure times and locations from a schedule.We are assuming suitable definitions of the speechacts, as in Allen and Perrault \[1980\].
The only deviationfrom that treatment invol~es adding an extra argumentonto each (nonsurface) speech act, namely a planparameter that provides the context for the speech act.
ForHEADER: SEEK-ID-PARAMETER (agent, parameter,plan)STEPS: achieve KNOWREF (agent, parameter, plan)CONSTRAINTS: IS-PARAMETER-OF (parameter, plan)~KNOWREF (agent, parameter, plan).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.HEADER: ASK (agent, term, plan)STEPS: do REQUEST (agent, agent2,INFORMREF (agent2, agent, term, plan),plan)do INFORMREF (agent2., agent, term, plan)EFFECTS: KNOWREF (agent, term, plan)CONSTRAINTS: ~KNOWREF (agent, term, plan).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 2: Metaplansexample, the action INFORMREF (agent, hearer, term,plan) consists of the agent informing the hearer of adescription of the term with the effect that KNOWREF(hearer, term, plan).
Similarly, the action REQUEST(agent, hearer, act, plan) consists of the agent requestingthe hearer to do the act as a step in the specified plan.This argument allows us to express constraints on theplans suitable for various speech acts.There are obviously many more metaplans concerningplan debugging, plan specification, etc.
Also, as discussedlater, many conventional indirect speech acts can beaccounted for using a metaplan for each form.2.2 Plan RecognitionThe plan recognizer attempts to recognize the plan(s)that led to the production of the input utterance.Typically, an utterance ither extends an existing plan onthe stack or introduces a metaplan to a plan on the stack.If either of these is not possible for some reason, therecognizer attempts to construct a plausible plan using anyplan schemas it knows about.
At the beginning of adialogue, a disjunction of the general expectations fromthe task domain is used to guide the plan recognizer.More specifically, the plan recognizer attempts toincorporate the observed action into a plan according tothe following preferences:l) by a direct match with a step in an existing plan onthe stack;3042) by introducing a plausible subplan for a plan onthe stack;3) by introducing a metaplan to a plan on the stack;4) by constructing a plan, or stack of plans, that isplausible given the domain-specific expectationsabout plausible goals of the speaker.Class (1) above involves situations where the speakersays exactly what was expected given the situation.
Themost common example of this occurs in answering aquestion, where the answer is explicitly expected.The remaining classes all involve limited bottom-upforward chaining from the utterance act- In other words,the system tries to find plans in which the utterance is astep, and then tries to find more abstract plans for whichthe postulated plan is a subplan, and so on.
Throughoutthis process, postulated plans are eliminated by a set ofheuristics based on those in Allen and Perrault \[1980\].
Forexample, plans that are postulated whose effects arealready true are eliminated, as are plans whose constraintscannot be satisfied.
When heuristics cannot eliminate allbut one postulated plan, the chaining stops.Class (3) involves not only recognizing a metaplanbased on the utterance, but in satisfying its constraints,also involves connecting the metaplan to a plan on thestack.
If the plan on the stack is not the top plan, the stackmust be popped down to this plan before the newmetaplan is added to the stack.Class (4) may involve not only recognizing metaplansfrom scratch, but also recursively constructing a plausibleplan for the metaplan to be about.
This occurs mostfrequently at the start of a dialogue.
This will be shown inthe examples.For all of the preference classes, once a plan or set ofplans is recognized, it is expanded by adding thedefinitions of all steps and substeps until there is nounique expansion for any of the remaining substeps.If there are multiple interpretations remaining at theend of this process, multiple versions of the stack arecreated to record each possibility.
There are then severalways in which one might be chosen over the others.
Forexample, if it is the hearer's turn in the dialogue (i.e., noadditional utterance is expected from the speaker), thenthe hearer must initiate a clarification subdialogue.
If it isstill the speaker's turn, the hearer may wait for furtherdialogue to distinguish between the possibilities.3.
Communicative Analysis and Interaction with TaskAnalysisMuch research in recent years has studied largelydomain-independent linguistic issues.
Since our workconcentrates on incorporating the results of such work intoour framework, rather than on a new investigation of theseissues, we will first present he relevant results and thenexplain our work in those terms.
Grosz \[1977\] noted thatin task-oriented dialogues the task structure could be usedto guide the discourse structure.
She developed the notionof global focus of attention to represent the influence ofthe discourse structure; this proved useful for theresolution of definite noun phrases.
Immediate focus\[Grosz, 1977; Sidner, 1983\] represented the influence ofthe linguistic form of the utterance and proved useful forunderstanding ellipsis, definite noun phrases,pronominalization, "this" and "that."
Reichman \[1981\]developed the context space theory, in which the non-linear structure underlying a dialogue was reflected by theuse of surface phenomena such as mode of reference andclue words.
Clue words signaled a boundary shift betweencontext spaces (the discourse units hierarchicallystructured) as well as the kind of shift, e.g., the clue word"now" indicated the start of a new context space whichfurther developed the currently active space.
However,Reichman's model was not limited to task-orienteddialogues; she accounted for a much wider range ofdiscourse popping (e.g., topic switch), but used no taskknowledge.
Sacks et ai.
\[1974\] present he systematics ofthe turn-taking system for conversation and present henotion of adjacency pairs.
That is, one way conversation isinteractively governed is when speakers take turnscompleting such conventional, paired forms asquestion/answer.Our communicative analysis is a step towardincorporating these results, with some modification, into awhole system.
As in Grosz \[1977\], the task structure guidesthe focus mechanism, which marks the currently executingsubtask as focused.
Grosz, however, assumed an initialcomplete model of the task structure, as well as themapping from an utterance to a given subtask in this305structure.
Plan recognizers obviously cannot make suchassumptions.
Carberry \[1983\] provided explicit rules fortracking shifts in the task structure.
From an utterance, sherecognized part of the task plan, which was then used asan expectation structure for future plan recognition.
Forexample, upon completion of a subtask, execution of thenext subtask was the most salient expectation.
Similarly,our focus mechanism updates the current focus byknowing what kind of plan structure traversals correspondto coherent topic continuation.
These in turn provideexpectations for the plan recognizer.As in Grosz \[1977\] and Reichman \[1981\], we also usesurface linguistic phenomena to help determine focusshifts.
For example, clue words often explicitly mark whatwould be an otherwise incoherent or unexpected focusswitch.
Our metaplans and stack mechanism captureReichman's manipulation of the context space hierarchiesfor topic suspension and resumption.
Clue words becomeexplicit markers of meta-acts.
In particular, the stackmanipulations can be viewed as corresponding to thefollowing discourse situations.
If the plan is already on thestack, then the speaker is continuing the current topic, oris resuming a previous (stacked) topic.
If the plan is ametaplan to a stacked plan, then the speaker iscommenting on the current topic, or on a previous topicthat is implicitly resumed.
Finally, in other cases, thespeaker is introducing a new topic.Conceptually, the communicative and task analysiswork in parallel, although the parallelism is constrained bysynchronization requirements.
For example, when the taskstructure is used to guide the discourse structure \[Grosz,1977\], plan recognition (production of the task structure)must be performed first.
However, suppose the usersuddenly changes task plans.
Communicative analysiscould pick up any clue words signalling this unexpectedtopic shift, indicating the expectation changes to the planrecognizer.
What is important is that such a strategy isdynamically chosen depending on the utterance, incontrast o any a priori sequential (or even cascaded \[Bolt,Beranek and Newman, Inc., 1979\]) ordering.
The examplebelow illustrates the necessity of such a model ofinteraction.4.
ExampleThis section illustrates the system's task andcommunicative processing of Dialogue 1.
As above, wewill concentrate on the task analysis; some discourseanalysis will be briefly presented to give a feel for thecomplete system.
We will take the role of the clerk, thusconcentrating on understanding the passenger's utterances.Currently, our system performs the plan recognitionoutlined here and is driven by the output of a parser usinga semantic grammar for the train domain.
Theincorporation of the discourse mechanism is underdevelopment.
The system at present does not generatenatural language responses.The following analysis of "The eight-fifty toMontreal?"
is output from the parser:S-REQUEST (Person1, Clerkl, (R1)INFORMREF (Clerkl, Person1, ?fn (train1), ?plan)with constraints: IS-PARAMETER-OF (?plan, ?fn(trainl))arrive-station (trainl) = Montrealdepart-time (trainl) = eight-fiftyIn other words, Person1 is querying the clerk about some(as yet unspecified) piece of information regarding trainl.In the knowledge representation, objects have a set ofdistinguished roles that capture their properties relevant othe domain.
The notation "?fn (train1)" indicates one ofthese roles of trainl.
Throughout, the "?"
notation is usedto indicate skolem variables that need to be identified.
S-REQUEST is a surface request, as described in Allen andPerrault \[19801.Since the stack is empty, the plan recognizer can onlyconstruct an analysis in class (4), where an entire planstack is constructed based on the domain-specificexpectations that the speaker will try to BOARD or MEETa train.
From the S-REQUEST, via REQUEST, itrecognizes the ASK plan and then postulates the SEEK-ID-PARAMETER plan, i.e., ASK is the only known planfor which the utterance is a step.
Since its effect does nothold and its constraint is satisfied, SEEK-ID-PARAMETER can then be similarly postulated.
In a morecomplex example, at this stage there would be competinginterpretations that would need to be eliminated by theplan recognition heuristics discussed above.306In satisfying the IS-PARAMETER-OF constraint ofSEEK-ID-PARAMETER, a second plan is introduced thatmust contain a property of a train as its parameter.
Thisnew plan will be placed on the stack before the SEEK-ID-PARAMETER plan and should satisfy one of the domain-specific expectations.
An eligible domain plan is theGOTO plan, with the ?fn being either a time or a location.Since there are no plans for which SEEK-ID-PARAMETER is a step, chaining stops.
The state of thestack after this plan recognition process is as follows:PLAN2SEEK-ID-PARAMETER (Personl, ?fn (trainl), PLAN1)IASK (Person1, ?fn (train 1), PLAN1)IREQUEST (Person1, Clerk1,INFORMREF (Clerk1, Person1,I ?fn (trainl), PLAN1))S-REQUEST (Personl, Clerkl,INFORMREF (Clerkl, Person1,?fn (trainl), PLAN1))CONSTRAINT: ?fn is location or time role of trainsPLANI: GOTO (?agent, ?location, ?time).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Since SEEK-ID-PARAMETER is a metaplan, thealgorithm then performs a recursive recognition onPLAN1.
This selects the BOARD plan; the MEET plan iseliminated ue to constraint violation, since the arrive-station is not Toronto.
Recognition of the BOARD planalso constrains ?fn to be depart-time or depart-location.The constraint on the ASK plan indicated that the speakerdoes not know the ?fn property of the train.
Since thedepart-time was known from the utterance, depart-timecan be eliminated as a possibility.
Thus, ?fn has beenconstrained to be the depart-location.
Also, since theexpected agent of the BOARD plan is the speaker, ?agentis set equal to Person1.Once the recursive call is completed, plan recognitionends and all postulated plans are expanded to include therest of their steps.
The state of the stack is now as shownin Figure 3.
As desired, we have constructed an entire planstack based on the original domain-specific expectations toBOARD or MEET a train.Recall that in parallel with the above, communicativeanalysis is also taking place.
Once the task structure isrecognized the global focus (the executing step) in eachplan structure is noted.
These are the S-REQUEST in themetaplan and the GOTO in the task plan.
Furthermore,since R1 has been completed, the focus trackingmechanism updates the foci to the next coherent moves(the next possible steps in the task structures).
These arethe INFORMREF or a metaplan to the SEEK-ID-PARAMETER.PLAN2SEEK-ID-PARAMETER (Person1, depart-loc (train1), PLAN1)!ASK (Person1, depart-loc (trainl) PLAN1)REQUEST (Personl, Clerkl, ~ R E F  (Clerkl, Personl,INFORMREF (Clerk1, Person1, depart-loc (trainl), PLAN1)depart-loc (trainl), PLAN1))PLAN1BOARD (Person l, trainl)BUY-TICKET(Pe o 1, trainl) \] GET-ON (Personl, train1)!GOTO (Person1, depart-loc (trainl), depart-time (trainl))Figure 3: The Plan Stack after the First Utterance307The clerk's response to the passenger is theINFORMREF in PLAN2 as expected, which could berealized by a generation system as "Eight-fifty toMontreal.
Gate seven."
The global focus then correspondsto the executed INFORMREF plan step; moreover, sincethis step was completed the focus can be updated to thenext likely task moves, a metaplan relative to the SEEK-ID-PARAMETER or a pop back to the stacked BOARDplan.
Also note that this updating provides expectationsfor the clerk's upcoming plan recognition task.The passenger then asks "Where is it?
", i.e.,S-REQUEST (Person1, clerk1INFORMREF (clerk1, Person1, loc(Gate7), ?plan)(assuming the appropriate resolution of "it" by theimmediate focus mechanism of the communicativeanalysis).
The plan recognizer now attempts o incorporatethis utterance using the preferences described above.
Thefirst two preferences fail since the S-REQUEST does notmatch directly or by chaining any of the steps on the stackexpected for execution.
The third preference succeeds andthe utterance is recognized as part of a new SEEK-ID-PARAMETER referring to the old one.
This process isbasically analogous to the process discussed in detailabove, with the exception that the plan to which theSEEK-ID-PARAMETER refers is found in the stackrather than constructed.
Also note that recognition of thismetaplan satisfies one of our expectations.
The otherexpectation i volving popping the stack is not possible, forthe utterance cannot be seen as a step of the BOARDplan.
With the exception of the resolution of the pronoun,communicative analysis is also analogous to the above.The final results of the task and communicative analysisare shown in Figure 4.
Note the inclusion of INFORM,the clerk's actual realization of the INFORMREF.PLAN3S-REQUEST (Person1, clerk1,INFORMREF (clerk1, Person1,loc (Gate7), PLAN2)SEEK-ID-PARAMETER (Person1, loc (Gate7), PLAN2)lASK (~rsonl, loc (Gate7~), PLAN2)INFO-~MREF (clerkl, Person1,loc (Gate7), PLAN2)PLAN2REQUEST (Person1, Clerk1,INFORMREF (Clerk1, Person1,depart-loc (train1), PLAN1))SEEK-ID-PARAMETER (Person1, depart-loc (uainl), PLAN1)/A~,~nl ,  depar t - loc~LAN1)INFORMREF (Clerk1, Person1,depart-loc (train1), PLAN1)IS-INFORM (Clerk1, Person1,equal (depart-loc (trainl),loc (Gate7)))PLAN1~ .
~ R D  t Personl, trainl)BUY-TICKET P~Pe~onl, trainl) ~ ~ G E  ON (Personl, trainl)GOTO (Personl, depart-loc (train1), depart-time (trainl))Figure 4: The Plan Stack after the Third Utterance308After the clerk replies with the INFORMREF inPLAN3, corresponding to "Down this way to the left--second one on the left," the focus updates the expectedpossible moves to include a metaplan to the top SEEK-ID-PARAMETER (e.g., "Second wharf") or a pop.
Thepop allows a metaplan to the stacked SEEK-ID-PARAMETER of PLAN2 ("What's a gate?")
or a pop,which allows a metaplan to the original domain plan ("It'sfrom Toronto?").
Since the original domain plan involvedno communication, there are no utterances that can be acontinuation of the domain plan itself.The dialogue concludes with the passenger's "OK.Thank you."
The "OK" is an example of a clue word\[Reichman, 1981\], words correlated with specificmanipulations to the discourse structure.
In particular,"OK" may indicate a pop \[Grosz, 1977\], eliminating thefirst of the possible expectations.
All but the last are theneliminated by "thank you," a discourse conventionindicating termination of the dialogue.
Note that unlikebefore, what is going on with respect o the task plan isdetermined via communicative analysis.5.
Comparisons with Other Work5.1 Recognizing Speech ActsThe major difference between our present approachand previous plan recognition approaches to speech acts(e.g., \[Alien and Perrault, 1980\]) is that we have ahierarchy of plans, whereas all the actions in Allen andPerrault were contained in a single plan.
By doing so, wehave simplified the notion of what a plan is and havesolved a puzzle that arose in the one-plan systems.
In suchsystems, plans were networks of action and statede~riptions linked by causality and subpart relationships,plus a set of knowledge-based relationships.
This latterclass could not be categorized as either a causal or asubpart relationship and so needed a special mechanism.The problem was that these relationships were not part ofany plan itself, but a relationship between plans.
In oursystem, this is explicit_ The "knowref" and "know-pos"and "know-neg" relations are modeled as constraintsbetween a plan and a metaplan, i.e., the plan to performthe task and the plan to obtain the knowledge necessary toperform the task.Besides simplifying what counts as a plan, themultiplan approach provides ome insight into how muchof the user's intentions must be recognized in order torespond appropriately.
We suggest that the top plan on thestack must be connected to a discourse goal.
The lowerplans may be only partially specified, and be filled in bylater utterances.
An example of this appears in consideringDialogue 2 from the first section, but there is no space todiscuss this here (see \[Litman and Allen, forthcoming\]).The knowledge-based relationships were crucial to theanalysis of indirect speech acts (ISA) in Allen and Perrault\[1980\].
Following the argument above, this means that theindirect speech act analysis will always occur in a metaplanto the task plan.
This makes ense since the ISA analysis isa communicative phenomena.
As far as the task isconcerned, whether a request was indirect or direct isirrelevant_In our present system we have a set of metaplans thatcorrespond to the common conventional ISA.
These plansare abstractions of inference paths that can be derivedfrom first principles as in Allen and Perrault- Similar"compilation" of ISA can be found in Sidner and Israel\[1981\] and Carberry \[1983\].
It is not clear in those systems,however, whether the literal interpretation of suchutterances could ever be recognized.
In their systems, theISA analysis is performed before the plan recognitionphase.
In our system, the presence of "compiled"metaplans for ISA allows indirect forms to be consideredeasily, but they are just one more option to the planrecognizer.
The literal interpretation is still available andwill be recognized in appropriate contexts.For example, if we set up a plan to ask aboutsomeone's knowledge (say, by an initial utterance of "Ineed to know where the schedule is incomplete"), then theutterance "Do you know when the Windsor train leaves?
"is interpreted literally as a yes/no question because that isthe interpretation explicitly expected from the analysis ofthe initial utterance.Sidner and Israel \[1981\] outlined an approach thatextended Allen and Perrault in the direction we have doneas well.
They allowed for multiple plans to be recognizedbut did not appear to relate the plans in any systematicway.
Much of what we have done builds on their309suggestions and outlines specific aspects that were leftunexplored in their paper.
In the longer version of thispaper \[Litman and Allen, forthcoming\], our analysis of thedialogue from their paper is shown in detail.Grosz \[1979\], Levy \[1979\], and Appelt \[1981\] extendedthe planning framework to incorporate multipleperspectives, for example both communicative and taskgoal analysis; however, they did not present details forextended ialogues.
ARGOT \[Allen et al, 1982\] was anattempt to fill this gap and led to the development of whathas been presented here.Pollack \[1984\] is extending plan recognition forunderstanding in the domain of dialogues with experts;she abandons the assumption that people always knowwhat they really need to know in order to achieve theirgoals.
In our work we have implicitly assumed appropriatequeries and have not yet addressed this issue.Wilensky's use of meta planning knowledge \[1983\]enables his planner to deal with goal interaction.
Forexample, he has meta-goals such as resolving oal conflictsand eliminating circular goals.
This treatment is similar toours except for a matter of emphasis.
His meta-knowledgeis concerned with his planning mechanism, whereas ourmetaplans are concerned with acquiring knowledge aboutplans and interacting with other agents.
The twoapproaches are also similar in that they use the sameplanning and recognition processes for both plans andmetaplans.5.2 DiscourseAlthough both Sidner and Israel \[1981\] and Carberry\[1983\] have extended the Allen and Perrault paradigm todeal with task plan recognition in extended ialogues,neither system currently performs any explicit discourseanalysis.
As described earlier, Carberry does have a (non-discourse) tracking mechanism similar to that used in\[Grosz, 1977\]; however, the mechanism cannot handletopic switches and resumptions, nor use surface linguisticphenomena to decrease the search space.
Yet Carberry isconcerned with tracking goals in an information-seekingdomain, one in which a user seeks information in order toformulate a plan which will not be executed uring thedialogue.
(This is similar to what happens in our traindomain.)
Thus, her recognition procedure is also not astied to the task structure.
Supplementing our model withmetaplans provided a unifying (and cleaner) frameworkfor understanding in both task-execution a d information-seeking domains.Reichman \[1981\] and Grosz \[1977\] used a dialogue'sdiscourse structure and surface phenomena to mutuallyaccount for and track one another.
Grosz concentrated ontask-oriented dialogues with subdialogues correspondingonly to subtasks.
Reichman was concerned with a modelunderlying all discourse genres.
However, although shedistinguished communicative goals from speaker intent herresearch was not concerned with either speaker intent orany interactions.
Since our system incorporates both typesof analysis, we have not found it necessary to performcomplex communicative goal recognition as advocated byReichman.
Knowledge of plans and metaplans, linguisticsurface phenomena, and simple discourse conventionshave so far sufficed.
This approach appears to be moretractable than the use of rhetorical predicates advocated byReichman and others such as Mann et al \[1977\] andMcKeown \[1982\].Carbonell \[1982\] suggests that any comprehensivetheory of discourse must address issues of recta-languagecommunication, aswell as integrate the results with otherdiscourse and domain knowledge, but does not outline aspecific framework.
We have presented a computationalmodel which addresses many of these issues for animportant class of dialogues.6.
ReferencesAllen, J.F., A.M. Frisch, and D.J.
Litman, "ARGOT: TheRochester Dialogue System," Proc., Nat'l.
Conf.
onArtificial Intelligence, Pittsburgh, PA, August 1982.Allen, J.F.
and C.R.
Perrault, "Analyzing intention inutterances," TR 50, Computer Science Dept., U.Rochester, 1979: Artificial lntell.
15, 3, Dec. 1980.Appelt, D.E., "Planning natural language utterances tosatisfy multiple goals," Ph.D. thesis, Stanford U., 1981.Bolt, Beranek and Newman, Inc., "Research in naturallanguage understanding," Report 4274 (AnnualReport), September 1978 - August 1979.310Carberry, S., "Tracking user goals in an informationseeking environment," Proc., Nat'L Conf.
on ArtificialIntelligence, 1983.Carbonell, J.G., "Meta-language utterances in purposivediscourse," TR 125, Computer Science Dept.,Carnegie-Mellon U., June 1982.Cohen, P.R.
and C.R.
Perrault, "Elements of a plan-basedtheory of speech acts," Cognitive Science 3, 3, 1979.Grosz, B.J., "The representation and use of focus indialogue understanding," TN 151, SRI, July 1977.Grosz, B.J., "Utterance and objective: Issues in naturallanguage communication," Proc., IJCAI, 1979.Horrigan, M.K., "Modelling simple dialogs," Master'sThesis, TR 108, U. Toronto, May 1977.Levy, D., "Communicative goals and strategies: Betweendiscourse and syntax," in T. Givon (ed).
Syntax andSemantics (vol.
12).
New York: Academic Press, 1979.Litman, D.J., "Discourse and problem solving," Report5338, Bolt Beranek and Newman, July 1983; TR 130,Computer Science Dept., U. Rochester, Sept. 1983.Litman, D.J.
and J.F.
Allen, "A plan recognition modelfor clarification subdialogues," forthcoming TR,Computer Science Dept., U. Rochester, expected 1984.Mann, W.C., J.A.
Moore, and J,A.
Levin, "Acomprehension model for human dialogue," Proc., 5thIJCAi, MIT, 1977.McKeown, K.R., "Generating natural language text inresponse to questions about database structure," Ph.D.thesis, U. Pennsylvania, 1982.Pollack, M.E., "Goal inference in expert systems," Ph.D.thesis proposal, U.
Penn., January 1984.Reichman, R., "Plain speaking: A theory and grammar ofspontaneous discourse," Report 4681, Bolt, Beranekand Newman, Inc., 1981.Sacks, H., E.A.
Schegloff.
and G. Jefferson, "A simplestsystematics for the organization of turn-taking forconversation," Language 50, 4, Part 1, December 1974.Sidner, C.L., "Focusing in the comprehension of definiteanaphora," in M. Brady (ed).
Computational Models ofDiscourse.
Cambridge, MA: MIT Press, 1983.Sidner, C.L.
and M. Bates, "Requirements for naturallanguage understanding in a system with grapicdisplays," Report 5242, Bolt Beranek and Newman,Inc., 1983.Sidner, C.L.
and D. Israel, "Recognizing intendedmeaning and speakers" plans," Proc., 7th IJCAI,Vancouver, B.C., August 1981.Wilensky, R. Planning and Understanding.
Addison-Wesley, 1983.311
