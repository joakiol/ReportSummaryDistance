ALIGNING SENTENCES IN BILINGUAL CORPORA USINGLEXICAL INFORMATIONStan ley  F .
Chen*A iken Computat ion  LaboratoryDiv is ion of App l ied  SciencesHarvard  Un ivers i tyCambr idge ,  MA 02138In ternet :  s fc@ca l l iope .harvard .eduAbst ractIn this paper, we describe a fast algorithm foraligning sentences with their translations in abilingual corpus.
Existing efficient algorithms ig-nore word identities and only consider sentencelength (Brown el al., 1991b; Gale and Church,1991).
Our algorithm constructs a simple statisti-cal word-to-word translation model on the fly dur-ing alignment.
We find the alignment hat maxi-mizes the probability of generating the corpus withthis translation model.
We have achieved an errorrate of approximately 0.4% on Canadian Hansarddata, which is a significant improvement over pre-vious results.
The algorithm is language indepen-dent.1 Int roduct ionIn this paper, we describe an algorithm for align-ing sentences with their translations in a bilingualcorpus.
Aligned bilingual corpora have proveduseful in many tasks, including machine transla-tion (Brown e/ al., 1990; Sadler, 1989), sense dis-ambiguation (Brown el al., 1991a; Dagan el at.,1991; Gale el al., 1992), and bilingual lexicogra-phy (Klavans and Tzoukermann, 1990; Warwickand Russell, 1990).The task is difficult because sentences frequentlydo not align one-to-one.
Sometimes entencesalign many-to-one, and often there are deletions in*The author wishes to thank Peter Brown, Stephen Del-laPietra, Vincent DellaPietra, and Robert Mercer for theirsuggestions, support, and relentless taunting.
The authoralso wishes to thank Jan Hajic and Meredith Goldsmithas well as the aforementioned forchecking the aligmnentsproduced by the implementation.one of the supposedly parallel corpora of a bilin-gual corpus.
These deletions can be substantial;in the Canadian Hansard corpus, there are manydeletions of several thousand sentences and onedeletion of over 90,000 sentences.Previous work includes (Brown el al., 1991b)and (Gale and Church, 1991).
In Brown, align-ment is based solely on the number of words ineach sentence; the actual identities of words areignored.
The general idea is that the closer inlength two sentences are, the more likely theyalign.
To perform the search for the best align-ment, dynamic programming (Bellman, 1957) isused.
Because dynamic programming requirestime quadratic in the length of the text aligned,it is not practical to align a large corpus as a sin-gle unit.
The computation required is drasticallyreduced if the bilingual corpus can be subdividedinto smaller chunks.
Brown uses anchors to per-form this subdivision.
An anchor is a piece of textlikely to be present at the same location in bothof the parallel corpora of a bilingual corpus.
Dy-namic programming is used to align anchors, andthen dynamic programming is used again to alignthe text between anchors.The Gale algorithm is similar to the Brown al-gorithm except that instead of basing alignmenton the number of words in sentences, alignment isbased on the number of characters in sentences.Dynamic programming is also used to search forthe best alignment.
Large corpora are assumed tobe already subdivided into smaller chunks.While these algorithms have achieved remark-ably good performance, there is definite room forimprovement.
These algorithms are not robustwith respect o non-literal translations and smalldeletions; they can easily misalign small passages9Mr.
McInnis?
M. McInnis?Yes.
Oui.Mr.
Saunders?
M. Saunders?No.
Non.Mr.
Cossitt?
M. Cossitt?Yes.
Oui.
:Figure 1: A Bilingual Corpus Fragmentbecause they ignore word identities.
For example,the type of passage depicted in Figure 1 occurs inthe Hansard corpus.
With length-based alignmentalgorithms, these passages may well be misalignedby an even number of sentences if one of the cor-pora contains a deletion.
In addition, with length-based algorithms it is difficult to automatically re-cover from large deletions.
In Brown, anchors areused to deal with this issue, but the selection ofanchors requires manual inspection of the corpusto be aligned.
Gale does not discuss this issue.Alignment algorithms that use lexical informa-tion offer a potential for higher accuracy.
Previ-ous work includes (Kay, 1991) and (Catizone elal., 1989).
However, to date lexically-based al-gorithms have not proved efficient enough to besuitable for large corpora.In this paper, we describe a fast algorithmfor sentence alignment hat uses lexical informa-tion.
The algorithm constructs a simple statisticalword-to-word translation model on the fly duringsentence alignment.
We find the alignment hatmaximizes the probability of generating the corpuswith this translation model.
The search strategyused is dynamic programming with thresholding.Because of thresholding, the search is linear in thelength of the corpus so that a corpus need not besubdivided into smaller chunks.
The search strat-egy is robust with respect o large deletions; lex-ical information allows us to confidently identifythe beginning and end of deletions.2 The A l ignment  Model2.1  The  A l ignment  F rameworkWe use an example to introduce our framework foralignment.
Consider the bilingual corpus (E, ~')displayed in Figure 2.
Assume that we have con-structed a model for English-to-French transla-tion,/.e., for all E and Fp we have an estimate forP(Fp\]E), the probability that the English sentenceE translates to the French passage Fp.
Then, wecan assign a probability to the English corpus Etranslating to the French corpus :T with a partic-ular alignment.
For example, consider the align-ment .41 where sentence E1 corresponds to sen-tence F1 and sentence E2 corresponds to sentencesF2 and F3.
We getP(-~',.4~l,f:) = P(FIIE1)P(F~., FsIE2),assuming that successive sentences translate inde-pendently of each other.
This value should be rel-atively large, since F1 is a good translation of E1and (F2, F3) is a good translation of E2.
Anotherpossible alignment .42 is one where E1 maps tonothing and E2 maps to F1, F2, and F3.
We getP(.F',.42\]?)
= P(elE1)P(F~, F2, F3IE2)This value should be fairly low, since the align-ment does not map the English sentences to theirtranslations.
Hence, if our translation model isaccurate we will haveP(~',`41I,~) >> P(.r,.421,f:)In general, the more sentences that are mappedto their translations in an alignment .4, the higherthe value of P(~,.AIE).
We can extend this ideato produce an alignment algorithm given a trans-lation model.
In particular, we take the alignmentof a corpus (~, ~)  to be the alignment ,4 that max-imizes P(~',`41E).
The more accurate the transla-tion model, the more accurate the resulting align-ment will be.However, because the parameters are all of theform P(FplE ) where E is a sentence, the aboveframework is not amenable to the situation wherea French sentence corresponds to no English sen-tences.
Hence, we use a slightly different frame-work.
We view a bilingual corpus as a sequenceof sentence beads (Brown et al, 1991b), where asentence bead corresponds to an irreducible groupof sentences that align with each other.
For exam-ple, the correct alignment of the bilingual corpusin Figure 2 consists of the sentence bead \[El; F1\]followed by the sentence bead \[E2; \];'2, F3\].
Wecan represent an alignment `4 of a corpus as a se-quence of sentence beads (\[Epl; Fpl\], \[Ep2; F~\], .
.
. )
,where the E~ and F~ can be zero, one, or moresentences long.Under this paradigm, instead of expressing thetranslation model as a conditional distribution10English (?
)El That is what the consumersare interested in and thatis what the party isinterested in.E2 Hon.
members opposite scoffat the freeze suggested bythis party; to them it islaughable.French (~)/'i Voil~ ce qui int6resse leconsommateur et rol l& ceque int6resse notre parti.F2 Les d6put6s d'en face semoquent du gel que apropos6 notre parti.F3 Pour eux, c'est une mesurerisible.Figure 2: A Bilingual CorpusP(FpIE ) we express the translation model  as adistribution P(\[Ep; Fp\]) over sentence beads.
Thealignment problem becomes discovering the align-ment A that maximizes the joint distributionP(?,2",.A).
Assuming that successive sentencebeads are generated independently, we getLP(C, Yr, A) = p(L) H P(\[E~;F~\])k=lwhere A t. 1 = ( \ [E~,F ; \ ] , .
.
.
,  \[EL; FL \ ] ) i s  consistentwith g and ~" and where p(L) is the probabilitythat a corpus contains L sentence beads.2 .2  The  Bas ic  T rans la t ion  Mode lFor our translation model, we desire the simplestmodel that incorporates lexical information effec-tively.
We describe our model in terms of a seriesof increasingly complex models.
In this section,we only consider the generation of sentence beadscontaining a single English sentence E = el " "enand single French sentence F = f l " " fm.
As astarting point, consider a model that assumes thatall individual words are independent.
We takenP(\[E; F\]) = p(n)p(m) H p(ei) f i  p(fj)i=l j= lwhere p(n) is the probability that an English sen-tence is n words long, p(m) is the probability thata French sentence is m words long, p(ei) is the fre-quency of the word ei in English, and p(fj) is thefrequency of the word fj in French.To capture the dependence between individualEnglish words and individual French words, wegenerate English and French words in pairs inaddition to singly.
For two words e and f thatare mutual translations, instead of having the twoterms p(e) and p(f) in the above equation wewould like a single term p(e, f) that is substan-tially larger than p(e)p(f).
To this end, we intro-duce the concept of a word bead.
A word bead iseither a single English word, a single French word,or a single English word and a single French word.We refer to these as 1:0, 0:1, and 1:1 word beads,respectively.
Instead of generating a pair of sen-tences word by word, we generate sentences beadby bead, using the h l  word beads to capture thedependence between English and French words.As a first cut, consider the following "model":P* (B) = p(l) H p(bi)i=1where B = {bl, .
.
.
,  bl} is a multiset of word beads,p(l) is the probabil ity that an English sentenceand a French sentence contain l word beads, andp(bi) denotes the frequency of the word bead bi.This simple model captures lexical dependenciesbetween English and French sentences.However, this "model" does not satisfy the con-straint that ~B P*(B) = 1; because beddings Bare unordered multisets, the sum is substantiallyless than one.
To force this model to sum to one,we simply normalize by a constant so that we re-tain the qualitative aspects of the model.
We takel p(t) "b" P(B) = -  I I  p\[  i) N, ZWhile a beading B describes an unordered mul-tiset of English and French words, sentences arein actuality ordered sequences of words.
We needto model word ordering, and ideally the probabil-ity of a sentence bead should depend on the or-dering of its component words.
For example, thesentence John ate Fido should have a higher prob-ability of aligning with the sentence Jean a mang4Fido than with the sentence Fido a mang4 Jean.11However, modeling word order under translationis notoriously difficult (Brown et al, 1993), and itis unclear how much improvement in accuracy agood model of word order would provide.
Hence,we model word order using a uniform distribution;we takeIP( \ [E ;F \ ] ,B ) -  p(l) Hp(bi)Nin!m!
i=1which gives usp(\[E;F\])= E p(l) ,(s) N,n!m!
H p(b,)B i=1where B ranges over beadings consistent with\[E; F\] and l(B) denotes the number of beads inB.
Recall that n is the length of the English sen-tence and m is the length of the French sentence.2.3 The Complete TranslationModelIn this section, we extend the translation modelto other types of sentence beads.
For simplicity,we only consider sentence beads consisting of oneEnglish sentence, one French sentence, one En-glish sentence and one French sentence, two En-glish sentences and one French sentence, and oneEnglish sentence and two French sentences.
Werefer to these as 1:0, 0:1, 1:1, 2:1, and 1:2 sentencebeads, respectively.For 1:1 sentence beads, we taket(B)P(\[E; F\]) = P1:1 E P1:1(/) H p(bi) NLHn!mlB i=1where B ranges over beadings consistent with\[E;F\]  and where Pz:I is the probability of gen-erating a 1:1 sentence bead.To model 1:0 sentence beads, we use a similarequation except that we only use 1:0 word beads,and we do not need to sum over beadings sincethere is only one word beading consistent with a1:0 sentence bead.
We takeIP(\[E\]) = Pl-o Pz:0(/) HP(ei)?
N l , l :0n!
i=1Notice that n = I.
We use an analogous equationfor 0:1 sentence beads.For 2:1 sentence beads, we takez(s)P2: l  ( / )  H p(bi) Pr(\[E1, E2; F\]) = P~:I E Nl 2:lnl !n2!m!
B ' i=1where the sum ranges over beadings B consistentwith the sentence bead.
We use an analogousequation for 1:2 sentence beads.3 Implementat ionDue to space limitations, we cannot describe theimplementation i  full detail.
We present its mostsignificant characteristics in this section; for amore complete discussion please refer to (Chen,1993).3 .1  Parameter i za t ionWe chose to model sentence length using a Poissondistribution, i.e., we tookAt1:0Pl:0(/) - l!
e ~1:0for some Al:0, and analogously for the other typesof sentence beads.
At first, we tried to estimateeach A parameter independently, but we foundthat after training one or two A would be unnat-urally small or large in order to specifically modelvery short or very long sentences.
To prevent hisphenomenon, we tied the A values for the differenttypes of sentence beads together.
We tookA1:1 A2:l  AI:2Al :0=A0: l - - -~- - -  3 - 3 (1)To model the parameters p(L) representing theprobability that the bilingual corpus is L sen-tence beads in length, we assumed a uniformdistribution, z This allows us to ignore this term,since length will not influence the probabil ity ofan alignment.
We felt this was reasonable becattseit is unclear what a priori information we have onthe length of a corpus.In modeling the frequency of word beads, noticethat there are five distinct distributions we needto model: the distribution of 1:0 word beads in 1:0sentence beads, the distribution of 0:1 word beadsin 0:1 sentence beads, and the distribution of allword beads in 1:1, 2:1, and 1:2 sentence beads.
Toreduce the number of independent parameters weneed to estimate, we tied these distributions to-gether.
We assumed that the distribution of wordbeads in 1:1, 2:1, and 1:2 sentence beads are iden-tical.
We took the distribution of word beads in1 To be prec ise,  we assumed a un i fo rm d is t r ibut ion  oversome arb i t ra r i l y  la rge  f inite range ,  as one cannot  have  aun i fo rm d is t r ibut ion  over  a countab ly  in f in i te  set.121:0 and 0:1 sentence beads to be identical as wellexcept restricted to the relevant subset of wordbeads and normalized appropriately, i.e., we tookpb(e) for e E Be pc(e) : pb(e')andPb(f) for f E By P:(f) = ~'~.
:'eB, Pb(f')where Pe refers to the distribution of word beadsin 1:0 sentence beads, pf refers to the distribu-tion of word beads in 0:1 sentence beads, pb refersto the distribution of word beads in 1:1, 2:1, and1:2 sentence beads, and Be and B I refer to thesets of 1:0 and 0:1 word beads in the vocabulary,respectively.3 .2  Eva luat ing  the  Probab i l i ty  o f  aSentence  BeadThe probabil ity of generating a 0:1 or 1:0 sentencebead can be calculated efficiently using the equa-tion given in Section 2.3.
To evaluate the proba-bilities of the other sentence beads requires a sumover an exponential number of word beadings.
Wemake the gross approximation that this sum isroughly equal to the maximum term in the sum.For example, with 1:1 sentence beads we haveZ(B)P ( \ [E ;F \ ] )  = px : lE  Pa:I(/) Hp(bi) Nz,Hn!m!
B i=1,~ p l lmax{ P l : I ( / )  I(B): B N~m!
Hp(bi)}i=lEven with this approximation, the calculationof P(\[E; F\]) is still intractable since it requires asearch for the most probable beading.
We use agreedy heuristic to perform this search; we are notguaranteed to find the most probable beading.
Webegin with every word in its own bead.
We thenfind the 0:1 bead and 1:0 bead that, when replacedwith a 1:1 word bead, results in the greatest in-crease in probability.
We repeat this process untilwe can no longer find a 0:1 and 1:0 bead pair thatwhen replaced would increase the probability ofthe beading.3.3 Parameter  Es t imat ionWe estimate parameters by using a variation of theViterbi version of the expectation-maximization(EM) algorithm (Dempster et al, 1977).
TheViterbi version is used to reduce computationalcomplexity.
We use an incremental variation of thealgorithm to reduce the number of passes throughthe corpus required.In the EM algorithm, an expectation phase,where counts on the corpus are taken using thecurrent estimates of the parameters, is alternatedwith a maximization phase, where parameters arere-estimated based on the counts just taken.
Im-proved parameters lead to improved counts whichlead to even more accurate parameters.
In the in-cremental version of the EM algorithm we use, in-stead of re-estimating parameters after each com-plete pass through the corpus, we re-estimate pa-rameters after each sentence.
By re-estimating pa-rameters continually as we take counts on the cor-pus, we can align later sections of the corpus morereliably based on alignments of earlier sections.We can align a corpus with only a single pass, si-multaneously producing alignments and updatingthe model as we proceed.More specifically, we initialize parameters bytaking counts on a small body of previouslyaligned data.
To estimate word bead frequencies,we maintain a count c(b) for each word bead thatrecords the number of times the word bead b oc-curs in the most probable word beading of a sen-tence bead.
We takec(b)pb(b) - Eb, c(V)We initialize the counts c(b) to 1 for 0:1 and 1:0word beads, so that these beads can occur in bead-ings with nonzero probability.
To enable 1:1 wordbeads to occur in beadings with nonzero probabil-ity, we initialize their counts to a small value when-ever we see the corresponding 0:1 and 1:0 wordbeads occur in the most probable word beading ofa sentence bead.To estimate the sentence length parameters ,~,we divide the number of word beads in the mostprobable beading of the initial training data bythe total number of sentences.
This gives us anestimate for hi:0, and the other ~ parameters canbe calculated using equation (1).We have found that one hundred sentence pairsare sufficient o train the model to a state where itcan align adequately.
At this point, we can processunaligned text and use the alignments we produceto further train the model.
We update parametersbased on the newly aligned text in the same waythat we update parameters based on the initial\ ]3training data.
2To align a corpus in a single pass the modelmust be fairly accurate before starting or else thebeginning of the corpus will be poorly aligned.Hence, after bootstrapping the model on one hun-dred sentence pairs, we train the algorithm on achunk of the unaligned target bilingual corpus,typically 20,000 sentence pairs, before making onepass through the entire corpus to produce the ac-tual alignment.3 .4  SearchIt is natural to use dynamic programming tosearch for the best alignment; one can find themost probable of an exponential number of align-ments using quadratic time and memory.
Align-ment can be viewed as a "shortest distance" prob-lem, where the "distance" associated with a sen-tence bead is the negative logarithm of its proba-bility.
The probability of an alignment is inverselyrelated to the sum of the distances associated withits component sentence beads.Given the size of existing bilingual corpora andthe computation ecessary to evaluate the proba-bility of a sentence bead, a quadratic algorithm isstill too profligate.
However, most alignments areone-to-one, so we can reap great benefits throughintelligent thresholding.
By considering only asubset of all possible alignments, we reduce thecomputation to a linear one.Dynamic programming consists of incrementallyfinding the best alignment of longer and longerprefixes of the bilingual corpus.
We prune allalignment prefixes that have a substantially lowerprobability than the most probable alignment pre-fix of the same length.2 In theory, one cannot decide whether a part icular sen-tence bead belongs to the best al ignment of a corpus un-til the whole corpus has been processed.
In practice, somepart ial  al ignments will have much higher probabil it ies thanall other ahgnments,  and it is desirable to train on thesepartial al ignments to aid in aligning later sections of thecorpus.
To decide when it is reasonably safe to train on apart icular sentence bead, we take advantage of the thresh-olding described in Section 3.4, where improbable partialal ignments are discarded.
At a given point in time in align-ing a corpus, all undiscarded partial al ignments will havesome sentence beads in common.
When a sentence bead iscommon to all active part ial  al ignments, we consider it tohe safe to train on.3.5  De le t ion  Ident i f i ca t ionDeletions are automatically handled within thestandard ynamic programming framework.
How-ever, because of thresholding, we must handlelarge deletions using a separate mechanism.Because lexical information is used, correctalignments receive vastly greater probabilitiesthan incorrect alignments.
Consequently, thresh-olding is generally very aggressive and our searchbeam in the dynamic programming array is nar-row.
However, when there is a large deletion inone of the parallel corpora, consistent lexical cor-respondences disappear so no one alignment hasa much higher probability than the others andour search beam becomes wide.
When the searchbeam reaches a certain width, we take this to in-dicate the beginning of a deletion.To identify the end of a deletion, we search lin-early through both corpora simultaneously.
Alloccurrences of words whose frequency is below acertain value are recorded in a hash table.
When-ever we notice the occurrence of a rare word inone corpus and its translation in the other, wetake this as a candidate location for the end of thedeletion.
For each candidate location, we exam-ine the forty sentences following the occurrence ofthe rare word in each of the two parallel corpora.We use dynamic programming to find the prob-ability of the best alignment of these two blocksof sentences.
If this probability is sufficiently highwe take the candidate location to be the end ofthe deletion.
Because it is extremely unlikely thatthere are two very similar sets of forty sentencesin a corpus, this deletion identification algorithmis robust.
In addition, because we key off of rarewords in considering ending points, deletion iden-tification requires time linear in the length of thedeletion.4 Resu l tsUsing this algorithm, we have aligned three largeEnglish/French corpora.
We have aligned a cor-pus of 3,000,000 sentences (of both English andFrench) of the Canadian Hansards, a corpus of1,000,000 sentences of newer Hansard proceedings,and a corpus of 2,000,000 sentences of proceed-ings from the European Economic Community.
Ineach case, we first bootstrapped the translationmodel by training on 100 previously aligned sen-tence pairs.
We then trained the model further on1420,000 sentences of the target corpus.
Note thatthese 20,000 sentences were not previously aligned.Because of the very low error rates involved, in-stead of direct sampling we decided to estimatethe error of the old Hansard corpus through com-parison with the alignment found by Brown of thesame corpus.
We manually inspected over 500 lo-cations where the two alignments differed to esti-mate our error rate on the alignments disagreedupon.
Taking the error rate of the Brown align-ment to be 0.6%, we estimated the overall errorrate of our alignment to be 0.4%.In addition, in the Brown alignment approxi-mately 10% of the corpus was discarded becauseof indications that it would be difficult to align.Their error rate of 0.6% holds on the remainingsentences.
Our error rate of 0.4% holds on theentire corpus.
Gale reports an approximate errorrate of 2% on a different body of Hansard datawith no discarding, and an error rate of 0.4% if20% of the sentences can be discarded.Hence, with our algorithm we can achieve atleast as high accuracy as the Brown and Gale algo-rithms without discarding any data.
This is espe-cially significant since, presumably, the sentencesdiscarded by the Brown and Gale algorithms arethose sentences most difficult to align.In addition, the errors made by our algorithmare generally of a fairly trivial nature.
We ran-domly sampled 300 alignments from the newerHansard corpus.
The two errors we found aredisplayed in Figures 3 and 4.
In the first error,E1 was aligned with F1 and E2 was aligned with/'2.
The correct alignment maps E1 and E2 to F1and F2 to nothing.
In the second error, E1 wasaligned with F1 and F2 was aligned to nothing.Both of these errors could have been avoided withimproved sentence boundary detection.
Becauselength-based alignment algorithms ignore lexicalinformation, their errors can be of a more spec-tacular nature.The rate of alignment ranged from 2,000 to5,000 sentences of both English and French perhour on an IBM RS/6000 530H workstation.
Thealignment algorithm lends itself well to paralleliza-tion; we can use the deletion identification mecha-nism to automatically identify locations where wecan subdivide a bilingual corpus.
While it requiredon the order of 500 machine-hours to align thenewer Hansard corpus, it took only 1.5 days ofreal time to complete the job on fifteen machines.5 Discuss ionWe have described an accurate, robust, and fastalgorithm for sentence alignment.
The algorithmcan handle large deletions in text, it is languageindependent, and it is parallelizable.
It requiresa minimum of human intervention; for each lan-guage pair 100 sentences need to be aligned byhand to bootstrap the translation model.The use of lexical information requires a greatcomputational cost.
Even with numerous approxi-mations, this algorithm is tens of times slower thanthe Brown and Gale algorithms.
This is acceptablegiven that alignment is a one-time cost and givenavailable computing power.
It is unclear, though,how much further it is worthwhile to proceed.The natural next step in sentence alignment isto account for word ordering in the translationmodel, e.g., the models described in (Brown etal., 1993) could be used.
However, substantiallygreater computing power is required before theseapproaches can become practical, and there is notmuch room for further improvements in accuracy.References(Bellman, 1957) Richard Bellman.
Dynamic Pro-gramming.
Princeton University Press, PrincetonN.J., 1957.
(Brown et al, 1990) Peter F. Brown, John Cocke,Stephen A. DellaPietra, Vincent J. DellaPietra,Frederick Jelinek, John D. Lafferty, Robert L.Mercer, and Paul S. Roossin.
A statistical ap-proach to machine translation.
ComputationalLinguistics, 16(2):79-85, June 1990.
(Brown et al, 1991a) Peter F. Brown, Stephen A.DellaPietra, Vincent J. DellaPietra, and Ro-bert L. Mercer.
Word sense disambiguation usingstatistical methods.
In Proceedings 29th Annu-al Meeting of the ACL, pages 265-270, Berkeley,CA, June 1991.
(Brown et al, 1991b) Peter F. Brown, Jennifer C.Lai, and Robert L. Mercer.
Aligning sentencesin parallel corpora.
In Proceedings 29th AnnualMeeting of the ACL, pages 169-176, Berkeley,CA, June 1991.
(Brown et al, 1993) Peter F. Brown, Stephen A. Del-laPietra, Vincent J. DellaPietra, and Robert L.Mercer.
The mathematics of machine transla-tion: Parameter estimation.
Computational Lin-guistics, 1993.
To appear.
(Catizone t al., 1989) Roberta Catizone, GrahamRussell, and Susan Warwick.
Deriving transla-tion data from bilingual texts.
In Proceedings\ ]5E1 If there is some evidencethat it ... and I will seethat it does.E2 \SCM{} Translation \ECM{}ElF1 Si on peut prouver que elle... je verrais & ce queelle se y conforme.
\SCM{}Language = French \ECM{}F2 \SCM{} Paragraph \ECM{}Figure 3: An Alignment ErrorMotion No.
22 that BillC-84 be amended in ... andsubstituting the followingtherefor : secondanniversary of.F 1 Motion No 22 que on modif iele projet de loi C-84 ...et en la rempla?ant par cequi suit : ' 18.F2 Deux ans apr~s : '.Figure 4: Another Alignment Errorof the First International Acquisition Workshop,Detroit, Michigan, August 1989.
(Chen, 1993) Stanley 17.
Chen.
Aligning sentences inbilingual corpora using lexical information.
Tech-nical Report TR-12-93, Harvard University, 1993.
(Dagan et al, 1991) Ido Dagan, Alon Itai, and U1-rike Schwall.
Two languages are more informa-tive than one.
In Proceedings of the 29th AnnualMeeting of the ACL, pages 130-137, 1991.
(Dempster et al, 1977) A.P.
Dempster, N.M. Laird,and D.B.
Rubin.
Maximum likelihood from in-complete data via the EM algorithm.
Journal ofthe Royal Statistical Society, 39(B):1-38, 1977.
(Gale and Church, 1991) William A. Gale and Ken-neth W. Church.
A program for aligning sen-tences in bilingual corpora.
In Proceedings of the29th Annual Meeting of the ACL, Berkeley, Cali-fornia, June 1991.
(Gale et al, 1992) William A. Gale, Kenneth W.Church, and David Yarowsky.
Using bilingualmaterials to develop word sense disambiguationmethods.
In Proceedings of the Fourth Interna-tional Conference on Theoretical and Methodolog-ical lssues in Machine Translation, pages 101-112, Montr4al, Canada, June 1992.
(Kay, 1991) Martin Kay.
Text-translation alignment.In ACH/ALLC 'gl: "Making Connections" Con-ference Handbook, Tempe, Arizona, March 1991.
(Klavans and Tzoukermann, 1990) Judith Klavansand Evelyne Tzoukermann.
The bicord system.In COLING-gO, pages 174-179, Helsinki, Fin-land, August 1990.
(Sadler, 1989) V. Sadler.
The Bilingual KnowledgeBank - A New Conceptual Basis for MT.BSO/Research, Utrecht, 1989.
(Warwick and Russell, 1990) Susan Warwick andGraham Russell.
Bilingual concordancing andbilingual lexicography.
In EURALEX 4th later-national Congress, M~laga, Spain, 1990.16
