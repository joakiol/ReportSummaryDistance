Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385?390,Dublin, Ireland, August 23-24, 2014.KUL-Eval: A Combinatory Categorial Grammar Approach forImproving Semantic Parsing of Robot Commands using Spatial ContextWillem Mattelaer, Mathias Verbeke and Davide NittiDepartment of Computer Science, KU Leuven, Belgiumwillem.mattelaer@gmail.commathias.verbeke@cs.kuleuven.bedavide.nitti@cs.kuleuven.beAbstractWhen executing commands, a robot hasa certain level of contextual knowledgeabout the environment in which it oper-ates.
Taking this knowledge into accountcan be beneficial to disambiguate com-mands with multiple interpretations.
Wepresent an approach that uses combina-tory categorial grammars for improvingthe semantic parsing of robot commandsthat takes into account the spatial contextof the robot.
The results indicate a clearimprovement over non-contextual seman-tic parsing.
This work was done in thecontext of the SemEval-2014 task on su-pervised semantic parsing of spatial robotcommands.1 IntroductionOne of the long-standing goals of robotics is tobuild autonomous robots that are able to performeveryday tasks.
Two important requirements toachieve this are an efficient way of communicatingwith the robot, and transforming these commandssuch that the robot is able to capture their mean-ing.
Furthermore, this needs to be consistent withthe context in which the robot is operating, i.e., therobot?s belief.Semantic parsing focuses on translating naturallanguage (NL) into a formal representation thatcaptures the meaning of the sentence.
Most ofthe current semantic parsing approaches are non-contextual, i.e., they do not take into account thecontext in which the command sentence should beexecuted.
This can lead to erroneous parses, mostoften due to ambiguity in the original sentence.Consider the following example sentence ?MoveThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/the pyramid on the blue cube on the gray cube?.This sentence has two valid interpretations.
Eitherthe robot needs to move the pyramid that is cur-rently standing on the blue cube and put it on thegray cube, or move the pyramid and place it on theblue cube that is standing on the gray cube.Humans will decide on the correct interpreta-tion by taking into account the context.
For in-stance, by looking at Figure 1, it is clear that thesecond interpretation is not possible, because thereis no blue cube on top of a gray cube.
However,there is a pyramid on top of a blue cube, makingthe first interpretation possible.
The goal of thispaper is to improve on non-contextual semanticparsing by tailoring the context to guide the parser.In this way, part of the ambiguity that causes mul-tiple interpretations can be resolved.Figure 1: Possible situation (taken from (Dukes,2013b)).Our approach consists of two steps.
First, non-contextual semantic parsing using combinatorycategorial grammars (CCG) (Steedman, 1996;Steedman, 2000) is performed on the sentence.This returns multiple possible parses, each with anattached likelihood of correctness.
Subsequently,each parse is checked against the current context.The parse with the highest score that is possiblegiven the current context is returned.385This paper is organized as follows.
In Section 2we discuss related work, followed by a detaileddescription of our approach in Section 3.
In Sec-tion 4, the approach is evaluated and compared tonon-contextual parsing.
Finally, in Section 5 weconclude and outline directions for future work.The software is available from https://github.com/wmattelaer/Thesis.2 Related WorkThere is a significant body of previous work onlearning semantic parsers.
We will first reviewapproaches that translate NL sentences into a for-mal representation without taking context into ac-count, followed by related techniques that use thecontext to improve the parsing.Our approach is inspired by the work ofKwiatkowski et al.
(2010).
The authors presenta supervised CCG approach to parse queries toa geographical information database and a flight-booking system.
This differs from the current set-ting in that the database querying does not requireto take the context of the environment into ac-count, as is the case when executing robot com-mands.
SILT (Kate et al., 2005) uses transfor-mation rules to translate the NL sentence to aquery for the robot.
This approach was extendedto tailor support vector machines with string ker-nels (KRISP) (Kate and Mooney, 2006) and statis-tical machine learning (WASP) (Mooney, 2007).Also unsupervised approaches exist.
Poon (2013)solves this lack of supervision by 1) inferring su-pervision using the target database, which con-strains the search space, and 2) by using aug-mented dependency trees.Artzi and Zettlemoyer (2013) study the use ofgrounded CCG semantic parsing using weak su-pervision for interpreting navigational robot com-mands.
Their approach is similar to ours, but in-stead of postprocessing the results in a verificationstep, the context (or state) is added to the trainingdata.
Krishnamurthy and Kollar (2013) use CCGsas a foundation, but match it to the context usingan evaluation function.
This evaluation functionscores a denotation, i.e., the set of entity referentsfor the entire sentence, given a logical form and aknowledge base, which is considered as the con-text.3 MethodologyOur approach consists of two steps: a parse stepand a verification step.
Before these steps canbe executed, a Combinatory Categorial Grammarneeds to be trained.
The training data for thisgrammar consists of typed ?-expressions (Car-penter, 1997) that are annotated with their cor-responding NL sentences.
As the input data forthe SemEval-2014 task consists of Robot ControlLanguage (RCL) expressions (Dukes, 2013a)1, thedata needs to be preprocessed first.3.1 PreprocessingDuring preprocessing, the RCL expressions aretransformed into equivalent ?-expressions.
In the?-expressions, each entity is represented by alambda term where the variable is a reference tothe object.
The properties of an entity are definedby a conjunction of literals with two arguments.The predicate details the property that is being de-fined.
An example entity, a blue cube, can be rep-resented as ?x.color(x, blue), type(x, cube).
Aspatial relation between two entities is a literalwith three arguments: the variable of the first en-tity, the type of relation and the second entity.
Thelatter is given by its lambda term.
This lambdaterm has to be wrapped in a definite determiner,det, that selects a single element from the set cre-ated by the lambda term (Artzi and Zettlemoyer,2013).
For example: the RCL expression(entity:(type: prism)(spatial-relation:(relation: above)(entity:(color: blue)(type: cube))))is transformed to the ?-calculus expression?x.type(x,prism), relation(x, above,det(?y.color(y, blue), type(y, cube)))Events are contained in one lambda term withone variable per event.
There are three possibleevent predicates.
The action predicate defines theaction by detailing the action type and the objectentity.
The destination predicate will set the des-tination of the object2.
Finally, the sequence pred-icate is necessary to detail the order of the events.1RCL is a linguistically-oriented formal language forcontrolling a robot arm, that represents entities, attributes,anaphora, ellipsis and qualitative spatial relations.2Note that this event is not always necessary, e.g., in thecase of a take action, the robot will not release the object.386An example of this can be seen at the bottom ofFigure 2.Besides transforming the RCL expressions to ?-calculus, also the action types of the events arechecked.
If an event has an action of type moveor drop, it is changed to the combined move &drop action type.
This change was introduced be-cause the actual verbs that are used to instruct therobot to perform one of these two actions are oftenthe same.
To illustrate this, consider the followingtwo sentences taken from the training data: ?placeblue block on top of single red block?
and ?placegreen block on top of blue block?.
In the former,the intended action is a drop action, while in thelatter the action should be a move action.
Duringparsing, the correct action can be selected by look-ing at the context it has to be executed in.
If therobot is currently grasping an object, the intendedaction is a drop action, otherwise it is a move ac-tion.Furthermore, the anaphoric references areresolved in the natural language sentences.Anaphoric references are words that refer to one ormore words mentioned earlier in the sentence.
Thesentences of the dataset are annotated with mark-ers that capture the references in the sentence.
Themarkers that are used are [1], (1) and {1} andare placed right after the word that is used for thereference.
[1] is used to mark a word that is re-ferred to by another word, whereas (1) is usedto detail a word that refers to another word, e.g.,it.
Finally, {1} marks a word that refers to thetype of an earlier entity, e.g., one.
The numbersin these markers can increase if there are differ-ent references in one sentence, but the sentencesof this dataset do not contain different references.For instance, the sentence Pick the blue block andplace it above the gray one is transformed to thesentence Pick the blue block [1] and place it (1)above the gray one {1}.The anaphoric references are found usingthe coreference resolution system of StanfordCoreNLP (Recasens et al., 2013; Lee et al., 2013;Lee et al., 2011; Raghunathan et al., 2010).
How-ever, it is not capable of finding references that useone.
This can be solved by letting the one alwaysrefer to the first entity of the sentence, because ofthe simplicity of the sentences.3.2 ParsingTo parse the robot commands, a Probabilis-tic Combinatory Categorial Grammar (PCCG)(Kwiatkowski et al., 2010) is used.
Regular CCGsconsist out of two sets: a lexicon of lexical itemsand a set of operations.
A lexical entry combinesa word or phrase with its meaning.
This meaningis represented by a category.
A category capturesthe syntactic as well as the the semantic informa-tion of a word.
A number of primitive symbols, asubset of the part-of-speech tags, are used to rep-resent the syntax.
These primitive symbols can becombined using specific operator symbols (/, \).The semantics are represented by a ?-expression.Some example lexical entries are:blue ` ADJ : ?x.color(x, blue)pyramid ` N : ?x.type(x, prism)pick up ` S/NP : ?y?x.action(x, take, y)The operator symbols can now be used to de-termine how the categories can be combined usingoperations.
The operations that are used by theCCG take one or two categories as input and re-turn one category as output.
These operations willsimultaneously address syntax and semantics.
Thetwo most frequently used operations are the appli-cation operations, i.e., forward (>) and backward(<):X/Y : f Y : g ?
X : f(g) (>)Y : g X\Y : f ?
X : f(g) (<)The forward application takes as input a CCGcategory with syntax X/Y and ?-expression ffollowed by a category with syntax Y and ?-expression g and returns a CGG category with syn-tax X and ?-expression f(g).The operations will derive syntactic and seman-tic information, while keeping track of the wordorder that is encoded using the slash direction.Another important operation deals with the def-inite determiner in the ?-expressions:N : f ?
NP : det(f)This operation takes a single noun (N) categoryas input and returns an noun phrase (NP) categorywhere the original ?-expression is wrapped in adeterminer.
A complete parsing example is shownin Figure 2.CCGs will usually have multiple possible parsesfor a sentence given a certain lexicon for which it387TakeS/NP?z?y.action(y, take, z)theN/N?x.xpyramidN?x.type(x, prism)>N?x.type(x, prism)NPdet(?x.type(x, prism))>S?y.action(y, take, det(?x.type(x, prism)))Figure 2: A possible parse for the sentence ?Takethe pyramid?.is not possible to determine which of these is best.To alleviate this problem, PCCGs have been intro-duced (Kwiatkowski et al., 2010).
PCCGs will re-turn the most likely parse using a log-linear modelthat contains a parameter vector ?, estimated us-ing stochastic gradient updates.
The joint proba-bility of a ?-calculus expression z and a parse y isgiven by P (y, z|x; ?,?
), with ?
being the entirelexicon.
The most likely ?-calculus expression zgiven a sentence x can then be found by:f(x) = arg maxzP (z|x; ?,?
)where the probability of z is equal to the sum ofthe probabilities of all parses that produce z:P (z|x; ?,?)
=?yP (y, z|x; ?,?
)For training the PCCGs, the algorithm as de-scribed by Kwiatkowski et al.
(2010) was used.
Itconsists of two steps.
In the first step the lexicon isexpanded with new lexical items.
The second stepwill update the parameters of the grammar usingstochastic gradient updates (LeCun et al., 1998).All parameters are associated with a feature.
Thesystem uses lexical features: for each item in thelexicon a feature is added that fires when the itemis used.3.3 VerificationThe parser will return multiple ?-expressions,each with an attached likelihood score.
In theverification step, these resulting expressions arechecked against the context.
These ?-expressionsare first transformed to RCL expressions3.
Next,the entities are extracted from the RCL expres-sions and for each entity a corresponding objectis searched using a spatial planner, provided bythe task organizer.
This spatial planner will, given3Note that during pre- and postprocessing no informationis lost, as the mapping between ?-calculus and RCL is a one-to-one function.Complete Partial Without contextCorrect 71.29% 78.58% 57.76%Wrong 11.66% 4.37% 27.72%No result 17.05% 17.05% 14.52%Table 1: Results.an entity description in RCL, return the objects inthe context that satisfy that description.
RCL ex-pressions with entities that have no correspondingobject in the context are discarded.
From the re-maining RCL expressions the one with the highestlikelihood is returned.4 EvaluationThe provided dataset for the task was crowd-sourced using Train Robots, an online game inwhich players were given before and after im-ages of a scene and were asked to give the NLcommand that the robot had executed (Dukes,2013a).
Each scene is a formal description of adiscrete 8x8x8 3D game board consisting of col-ored blocks.
The entire dataset consists of 3409annotated examples, and was split in a training andtest set of 2500 and 909 sentences respectively.The results are listed in Table 1.
The first col-umn (?Complete?)
contains the results when theresulting RCL expression is exactly the same asthe ground-truth RCL expression.
Next to the fullmatching scores, we also provide the scores forpartial matching of the RCL expressions (?Par-tial?
), based on the Parseval metric (Black et al.,1991).
Each RCL expression is scored between 0and 1 according to the resemblance with the ex-pected expression.
The tree representations of theRCL expressions are compared and the number ofcorrect nodes in the actual expression are dividedby the number of nodes in the tree of the expectedexpression to calculate the score.
A node is correctif it is present at the same position in both trees andif all children are correct.The last column (?Without context?)
containsthe results when using the parser without the veri-fication step.
This can be considered a baseline.It may be clear that the use of contextual parsingis advantageous when comparing the contextualwith the non-contextual setting, with an increaseof 13% in the number of correct results.Error AnalysisWhen inspecting the wrong parses, it could be ob-served that the wrong results were usually mini-mally wrong.
Either the value of a certain element388Expected Actual Occurrencesedge region 17above within 15right left 8left front 7within above 6Table 2: Wrong values.was wrong, an unnecessary element was addedto the expression or a required element was notpresent in the resulting expression.
This is alsoclear when comparing the complete with the par-tial match results, from which it can be seen that66 sentences were only partially incorrect.
Someof the most commonly wrong values are listed inTable 2.
A final common reason for a wrong parsewas that a sequence of a take and a drop actionis considered as a single move action.
There are6 occurrences of this final case of which 5 wouldresult in the same end state.One of the most common reasons that the parserreturned no result for a sentence, is because onetype of sentences was not present in the trainingset.
Sentences of the form ?pick up red block.
putit on grey block?
were completely absent from thetraining data, but did appear 34 times in the testset.
Their structure is quite simple and should notpresent a problem, but the parser was only trainedon sentences that combined the two actions withan ?and?
connective.
This is a problem becausethe trained grammar is very dependent on the pro-vided training data.
Another difficult type of sen-tences are the ones that contain measures.
Only17 of these were parsed correctly, while 70 had noresult and 3 were wrong.Without considering the context, the combinedmove& drop action is not possible, since the con-text is required to decide afterwards which specificaction has to be executed.
59 sentences (6.5%)were wrong because a wrong action was selected.5 Conclusions and Future WorkIn this paper we have presented an improved se-mantic parsing approach for robot commands byintegrating spatial context.
It consists of two steps.First, the sentence is parsed using a Probabilis-tic Combinatory Categorial Grammar.
Next, theparses are checked against the context.
The re-sulting parse is the one with the highest likeli-hood that is valid given the context.
This ap-proach was evaluated on the SemEval-2014 Task6 dataset.
The results indicate that integratingcontextual knowledge is advantageous for parsingspatial robot commands.In future work, we will perform an in-depthanalysis of our system in comparison with theother participating systems.
Furthermore, we willextend our approach to contexts that also containprobabilistic facts, in order to be able to handlenoisy sensor data.ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the ACL,1:49?62.Ezra W. Black, Steven P. Abney, Daniel P. Flickenger,Claudia Gdaniec, Ralph Grishman, Philip Harri-son, Donald Hindle, Robert J. P. Ingria, Freder-ick Jelinek, Judith L. Klavans, Mark Y. Liberman,Mitchell P. Marcus, Salim Roukos, Beatrice San-torini, and Tomek Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverageof English grammars.
In HLT.
Morgan Kaufmann.Bob Carpenter.
1997.
Type-Logical Semantics.
TheMIT Press.Kais Dukes.
2013a.
Semantic Annotation of RoboticSpatial Commands.
In Language and TechnologyConference.Kais Dukes.
2013b.
Supervised semantic parsing ofrobotic spatial commands.
http://alt.qcri.org/semeval2014/task6/.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th An-nual Meeting of the ACL, ACL-44, pages 913?920,Stroudsburg, PA, USA.
ACL.Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to transform natural toformal languages.
In Proceedings of the 20th Na-tional Conference on Artificial Intelligence - Volume3, AAAI?05, pages 1062?1068.
AAAI Press.Jayant Krishnamurthy and Thomas Kollar.
2013.Jointly Learning to Parse and Perceive : ConnectingNatural Language to the Physical World.
In Trans-actions of ACL, volume 1, pages 193?206.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Lexical general-ization in ccg grammar induction for semantic pars-ing.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 1512?1523, Stroudsburg, PA, USA.
ACL.389Yann LeCun, L?eon Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE,86(11):2278?2324.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the CoNLL-2011 sharedtask.
In Proceedings of the CoNLL-11 Shared Task.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4).Raymond J. Mooney.
2007.
Learning for semanticparsing.
In Alexander Gelbukh, editor, Computa-tional Linguistics and Intelligent Text Processing,volume 4394 of Lecture Notes in Computer Science,pages 311?324.
Springer Berlin Heidelberg.Hoifung Poon.
2013.
Grounded unsupervised seman-tic parsing.
In ACL (1), pages 933?943.
ACL.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
EMNLP-2010, Boston, USA.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
InProceedings of NAACL 2013, pages 627?633.
ACL.Mark Steedman.
1996.
Surface structure and inter-pretation.
Linguistic inquiry monographs.
The MITPress, Cambridge, MA, USA.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, MA, USA.390
