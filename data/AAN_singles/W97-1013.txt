Exp lanat ion -Based  Learn ing of  Data -Or iented  Pars ingK.
Sima'anResearch Institute for Language and Speech,Utrecht University,Trans 10, 3512 JK Utrecht, The Netherlandskhalil.simaan~let.ruu.nlAbstractThis paper presents a new view ofExplanatiomBased Learning (EBL) of nat-ural language parsing.
Rather than em-ploying EBL for specializing parsers by in-ferring new ones, this paper suggests em-ploying EBL for learning how to reduceambiguity only partially.We xemplify thisby presenting a new EBL method thatlearns parsers that avoid spurious over-generation, and we show how the samemethod can be used for reducing the sizesof stochastic: grammars learned from tree-banks, e.g.
(Bod, 1995, Charniak, 1996,Sekine and Grishman, 1995).The present method consists of an EBL al-gorithm for learning partial-parsers, and aparsing algorithm which combines partial-parsers with existing "full-parsers".
Thelearned partial-parsers, implementable asCascades of Finite State Transducers (CF-STs), recognize and combine constituentsefficiently, prohibiting spurious overgener-ation.
The parsing algorithm combinesa learned partial-parser with a given full-parser such that the role of the full-parseris limited tO combining the constituents,recognized by the partial-parser, and torecognizing unrecognized portions of theinput sentence.
Besides the reduction ofthe parse-space prior to disambiguation,the present method provides a way for re-fining existing disambiguation models thatlearn stochastic grammars from tree-bankse.g.
(Bod, 1995, Charniak, 1996, Sekineand Grishman, 1995).We exhibit encouraging empirical resultsusing a pilot implementation: parse-spaceis reduced substantially with minimal ossof coverage.
The speedup gain for disam-biguation models is exemplified by experi-ments with the DOP model (Bod, 1995).1 Introduct ionCurrent work on natural anguage parsing is in largepart directed towards eliminating overgeneration fgrammars by employing stochastic models for dis-ambiguation (e.g.
(Bod, 1995, Sekine and Grish-man, 1995, Charniak, 1996)).
For many applications(e.g.
Speech Understanding), probabilistic evalua-tion of the full parse-space using such models is NP-hard (Sima'an, 1996b), and even when it is deter-ministic polynomial-time, then grammar size is pro-hibitive.
Therefore, it is necessary to develop meth-ods that, on the one hand, reduce the space of anal-yses, as much as possible prior to disambiguation,and on the other hand, reduce the sizes of gram-mars used for disambiguation.
This paper presentsa method aimed at these two forms of reduction oftime and space costs.In recent work on speeding up parsing, effort isdirected towards specializing broad-coverage ram-mar by EBL (e.g.
(Rayner, 1988, Samuelsson,1994, Rayner and Carter, 1996, Srinivas and Joshi,1995)).
Grammar-specialization, in these works,amounts to replacing a given parser by a fresh effi-cient parser learned from the tree-bank.
The learnedparser trades coverage for efficiency.
Inspired bythese works, we present a new method based on EBLfor learning efficient parsers.
Rather than special-izing a given full-parser by inferring a new one, thepresent method learns a partial-parser and combinesit with the full-parser in a way that reduces ambi-guity.
The combination is a serial construction inwhich the partial-parser is employed first for rec-ognizing and combining constituents.
The partial-parser is learned such that it parses only those por-tions of the sentence that are "safe" to parse, i.e.at the points where there is clear bias in the tree-Sima 'an 107 Data- Oriented ParsingKhalil Sima'an (1997) Explanation-Based Learning of Data-Oriented Parsing.CoNLL97: Computational Natural Language Learning, ACL pp 107-116.
(~) 1997 Association for Computational LinguisticsIn T.M.
Ellison (ed.)bank.
These constituents are then passed through,together with unrecognized portions of the input, tothe full-parser, that completes the space only wherenecessary.For disambiguation models such as (Bod, 1995,Sekine and Grishman, 1995, Charniak, 1996), thepresent method refines the cutting criteria whichthese models employ for inferring stochastic gram-mars.
This refinement results in the inference ofsmaller, yet no less powerful, statistical grammars.2 Termino logyA Context-Free Grammar (CFG) derivation is a se-quence of one or more rewriting steps, starting withthe start non-terminal of the grammar, employingthe grammar productions.
A subderivation isa subsequence of a derivation.
A string of sym-bols which results from a CFG-derivation (of zeroor more rewriting steps) is called a sentential-form.A partial-tree (also subtree) of a given tree t is atree-structure which is the result of a subderiva-tion of a derivation represented by t. A partial-tree which has as its root the start non-terminalis called a sentential partial-tree.
The string ob-tained from the ordered sequence of leaf nodes ofa partial-tree is called the frontier of the partial-tree; the leaf nodes are called the frontier nodes ofthe partial-tree.
A Context-Free rule (CF-rule)R=A -4 A1 .
.
.An is said to appear in a tree t ifthere is a node in t, labeled A, and that node has nchildren labeled with (maintaining order from left toright) A1...A,~.
The CFG (VN, VT, S, 7~ ) is calledthe CFG underlying a given tree-bank iff ~ is theset {R I rule R appears in a tree in the tree-bank}(and the start non-terminal S, non-terminal set VN,terminal set VT are exactly those of the tree-bank).A parser based on the CFG-underlying a tree-bankis called the Tree-bank parser (denoted T-parser).3 Exp lanat ion -Based  Learn ingEBL (Mitchel et ai., 1986, DeJong and Mooney,1986, van Harmelen and Bundy, 1988) is the nameof a unifying framework for methods that learn frompreviously explained examples of a certain concept.EBL assumes a domain theory (or background the-ory) which provides explanations to and enablesthe definition of concepts.
In existing literature,the main goal of EBL is much faster recognition ofconcepts than the domain-theory does; EBL learns"shortcuts" in computation (called macro-operatorsor "chunks"), or directives for changing the threadof computation.
EBL stores the learned chunks inthe form of partial-explanations to previously seeninput instances, in order to apply them in the futureto "similar" input instances (in EBL, also "similar-ity" is assumed provided by the domain-theory).~ The specification of EBL consists of four precon-ditions and one postcondition.
The preconditionsare: 1) A domain theory: A description languagefor the domain at hand together with rules and factsabout the domain.
2) A target concept: A formaldescription, over the alfabet off the domain-theory,of the to-be-learned relation.
3) An Operationalitycriterion: A requirement on the form of the targetconcept.
And 4) training examples: A history whichmakes explicit the explanations given by the domain-theory to examples that occurred in the past; the ex-planations consist of instances of the target concept.The postcondition is: Find a generalization of the in-stances of the target concept given in the training-examples that satisfies the operationality criterion.Past experience in Machine Learning cast doubtson the feasibility of improving performance by us-ing EBL (Minton, 1990).
Minton explains that EBLdoes not guarantee better performance, since thecost of applying the learned knowledge might out-weigh the gain.
Minton discusses a formula forcomputing the utility of knowledge during learning.Generally speaking, this formulae is neither part ofEBL nor part of the domain-theory; it is an exten-sion to the EBL scheme by e.g.
statistical inferenceover large sets of training examples.4 Learn ing  par t ia l -parsersWe assume a tree-bank representing a certain do-main of application.
The tree-bank forms thetraining-examples of our EBL-based method, andthe linguistic annotation employed for annotatingthe sentences represents he domain-theory.
For thesake of presentation we delay the discussion of de-tail of the algorithm and concentrate on a simplifiedversion of it.
The simplest instances of the target-concept of our algorithm are called probably alwayssubsentential-forms (PA-SSFs).Subsentential- form A subsentential-form (SSF)is a sequence of grammar-symbols which formsthe frontier of a partial-tree.Probably always SSF An SSF ssf  = N1 ""  Nmis called Probably Always SSF (PA-SSF) withrespect o the tree-bank /f the frequency of oc-currence of N1.
.
.Nm in the tree-bank as SSF(denoted f c( N1.
.
.
Nm ) ) is equal to the total fre-quency of its occurrence in the tree-bank (de-noted f (N1 .
.
.
Nm)) .The concept PA-SSF formalizes the intuitive con-cept "probably always constituent".
In reality,~ma ~an 108 Data-Oriented Parsingas discussed below, this concept is refined to be-come context-sensitive and less rigid; it becomes"probably almost always constituent in some local-context".
Moreover, to avoid sparse-data problemswe exclude the words of the language from the SSFswhich we consider; the SSFs may consist of bothpart-of-speech tags (PoSTags) as well as phrase-symbols.Assoc ia ted  subt ree  A partial-tree which has thesequence ssf as its frontier is called a subtree as-sociated with Ssf.
The set of subtrees associatedwith ssf, with respect o a tree-bank, consists ofall partial-trees of the tree-bank trees, which aresubtrees associated with ssf.The  learn ing  a lgor i thmThe goal of the algorithm is to learn the set ofPA-SSFs that represents the tree-bank trees in thefastest and least ambiguous way possible.
The pred-icate "least ambiguous" is instantiated in two ways:1) the learned (almost) PA-SSFs imply bracketswhich are most probably useful.
And 2) the setof subtrees associated with a learned PA-SSF is as-sumed complete, i,e.
no more structures are nec-essary for future sentences containing that PA-SSF.The second goal "fastest" is implemented by select-ing the PA-SSFs that reduce the tree-bank trees inthe fastest way.
To achieve this we employ an op-erationality criterion which measures the utility of aPA-SSF.
A measure of how much a single PA-SSFcontributes to reducing a sentential-form is the Re-duction Factor, and the "expected utility" of a PA-SSF is estimated as the Global Reduction Factor:Reduct ion  Factor  The Reduction Factor (RF)  ofa given SSFssf is RF(ss f )  = n(ssf)  - 1, whereL(ssf)  is the number of symbols which consti-tute ssf.G loba l  RF  The global reduction factor of a givenPA-SSF ssf with respect to the tree-bankis defined as GRF(ss f )  = fc(ssf)  x RF(ss f ) ,where fc(ssf)  is the frequency of ssf as a con-stituent.
In case ssf is an SSF that is not aPA-SSF then GRF(ssf) = -oo .The specification of the learning algorithm is infigure 2.
The algorithm learns PA-SSFs by an iter-ative procedure which "eats" up the tree-bank treesfrom their leaves upwards.
Beginning with the tree-bank at hand, after each iteration, the procedureoutputs: the set of learned PA-SSFs and a new tree-bank obtained by reducing all subtrees associatedwith a learned PA-SSF in all trees of the tree-bankat hand.
In the next iteration, the same procedureis applied to the tree-bank output by this iteration.The procedure stops when there is nothing to learnanymore i.e.
either there are no PA-SSFs to learn,or all tree-bank trees are fully reduced to their roots.Compet i to r  SSF Let N be a node in tree t andlet ss f  be the partial-tree with N as root.
Thefrontier of a partial-tree with root node which isa descendent or ancestor of N in t is called acompetitor of ssf.Operat iona l i ty  c r i te r ion  At each iteration of thealgorithm, for each sentential partial-tree t inthe tree-bank, for each SSF ssf in t, ssf is learnediff ssf is PA-SSF and GRF(ss f )  > GRF(x) ,for all x which is a competitor of ssf in t.Consider again the specification in figure 2.
Letthe algorithm be at a certain iteration i and leteach node in each partial-tree of the current tree-bank (TB~) have a unique address.
Also define theglobal reduction factor of an address N of a node,GRF(N) ,  to be equal to GRF(ssf), where ssf isthe SSF on the frontier of the partial-tree underthe node with address N. The operationality crite-rion is implemented in the specification at step (2.
),where nodes are marked.
The learned PA-SSFs arethose SSFs which form the frontiers of partial-treesof which the root is a node which was marked atsome iteration.Deta i l  o f  learn ing  a lgor i thmNow we present further detail of the algorithm.The term "PA-SSF" is redefined as follows:ssfis called PA-SSF if it fulfills ~ > 0,f ( ss f )  - -where 0 < ~ < 1 is a threshold.This definition of PA-SSF makes the target-conceptof our EBL method become "with probabil ity morethan/~ a constituent".
The algorithm employs thisdefinition as follows.
A threshold is set on the valuesof 8, where 0 is allowed to change during learning(the default value of this threshold is 0 = 1.0 unlessstated otherwise).
Suppose the threshold on ~ is0.75.
The algorithm starts in the first iteration withlearning PA-SSFs of 0 = 1.0.
Each time there are nomore PA-SSFs to learn, under the current value of0, it reduces 0 by a fixed amount (e.g.
0.05) untilbecomes equal to the threshold (0.75).
Then thealgorithm stops learning.We also employ a threshold (r) on the minimumfrequency of SSFs; an SSF must be frequent enoughin order to qualify for the PA-SSF test.
Currentlythis threshold is set at the maximum of a fixed inte-ger (e.g.
10) and a percentage of the number of treesSima 'an 109 Data-Oriented Parsing*MPM~ MP*P *NP *PI I I Ivan Amsterdam naar Utrechtfrom to*S/ / / *V  ~?P,PER INFPI I *MP-wi l .
, P  - - - - - " - -~NP * INFP ik wane II J vertrekken I uit Ni jmegenfrom to_leaveFigure 1: Two trees marked by the learning algorithm/*  Let  N denote  a un ique  address  of  a node  of  a t ree  t. Also let TBi denote  the/*  t ree -bank  obta ined  a f ter  i i te rat ions ,  where  7-B0 denotes  the  g iven t ree -bank ./ *  F ront ie r_Of (N)  denotes  the  f ront ie r  (i.e.
an SSF) of  the  par t ia l - t ree  under  N./ *  Descendent (Nch ,Np)  denotes  the  pred icate :  the  node  addressed  Nch is a/ *  descendent  of  the  node  addressed  Np.1.
i :=0;Repeat2.
V t 6 TB~, V node address N in t: N is marked iffFrontier_Of(N) is PA-SSF in TBi andvgx  ?
g in t: (Descendent(Nx,N) or Deseendent(N, Nx) )3. i := i+ l ;4.
TBi := (TBi-1 after reducing all partial-trees under marked nodes);.
/*/*/:/(GRF(N) > GRF(Nx));unt i l  ((TB, == 0) or (TB, == TBi-1));Figure 2: The Learning Algorithmin the tree-bank (e.g.
0.3%).
However, a more prin-cipled way to set the threshold is by letting it be afunction of the distribution of SSFs in the tree-bank.The algorithm also employs a definition of PA-SSF conditioned on local context, rather than fullycontext-free:A sequence of symbols is called PA-SSF incontext C iff the ratio between its fre-quency as SSF in context C and its totalfrequency in context C is > 0.The local context that is employed consists of fourfields: two grammar symbols to the left of and twoto the right of an SSF.
Since after the first round ofthe learning algorithm the training material consistsof sentential partial-trees, this kind of local contextmay consist of PoSTags as well as phrasal symbols.The algorithm can use this local context in order toenhance learning and parsing.
In the current im-plementation, however, we employ this local contextonly during learning and in a quite simplistic man-ner.Since currently local-context is not employed ur-ing parsing, the learning algorithm is tuned to pre-fer as general ocal-contexts as possible.
The learn-ing algorithm assumes in the first place that all fourfields of the local-context of an SSF are wild-cards.In case the SSF is not a PA-SSF in that context,then the algorithm retreats and assumes any threeof the four fields to be wild-cards.
In case an SSFis not a PA-SSF under three or more wild-cards oflocal-context then it is not learned, i.e.
two or lesswild-card local-contexts do not contribute to learn-ing.
Future implementations, however, shall have totake this local-context more seriously both in learn-ing and in parsing.Example :  In figure 1, two example trees areshown.
The asterisks in the figure denote the bordersof subtrees associated with PA-SSFs learned from thetree-bank.
The sequences of symbols marked withan asterisk at the frontier of a subtree, which hasa marked root, form the learned PA-SSFs.
In thetree at the left-hand side of figure1, there is only onePA-SSF that reduces the tree: (p np p np~),, whichcorresponds to '~rom Amsterdam to Utrecht".
Inthe right-hand side tree of figure 1, there are twoPA-SSFs, (p up) and (per v mp infp).In the first iteration, the learning algorithm re-duced the left tree totally and reduced the right treeonly at the constituent 'Trom Nijmegen".
In the sec-ond iteration, the leftovers of the right-tree, a sen-tential partial-tree with frontier "ik/I wil/want mpvertrekken/to_leave', is reduced fully.
If there areother partial-trees which are left over in the tree-Sima 'an 110 Data- Oriented Parsingbank, after these two iterations, then the algorithmwill attempt reducing them in subsequent i erations.If there are no more PA-SSFs to learn, the algo-rithm stops (possibly leaving some partial-trees notfully reduced).The pars ing  a lgor i thmA Tree-Substitution Grammar (TSG) is a CFG withrules which are partial-trees called elementary-trees.Let the set of subtrees associated with the PA-SSFs,which the learning algorithm outputs, be the set ofelementary-trees of a TSG; the TSG has the samestart-symbol, terminal and non-terminal symbols asthe CFG underlying the tree-bank.
This TSG isemployed as a partial-parser (other implementationsare discussed below).The new parsing algorithm combines the partial-parser with a given full-parser.
It has two stages:firstly it employs the partial-parser for parsing theinput sentence bottom-up, resulting in a space ofpartial-parses combined from subtrees associatedwith PA-SSFs.
In the  second phase it employs agiven full-parser to complete these partial parses intofull parses.
Crucially, the second phase of the al-gorithm takes advantage of the construction of thelearning algorithm.
It makes two assumptions con-cerning the space of partial parses which the partial-parser constructed:?
If a sequence of symbols is recognized by thepartial-parser then it is highly probable that allits subtrees are present in the chart (as theseare either associated subtrees or combinationsof associated subtrees).
Thus it is not necessaryto attempt reparsing portions of the sentencewhich were recognized as by the partial-parser.?
In the default case, 0 = 1.0, a PA-SSF implies"sure" constituent-borders; therefore, bracketsplaced by the full-parser are not allowed to crossthe borders of a PA-SSF.
In case two PA-SSFscross each other, a highly unlikely case, thenboth PA-SSFs are removed from the partial-parser's output.Thus, the task of the full-parser is limited to pars-ing totally uncovered portions and combining themwith the partial-trees provided by the partial-parserin ways that do not cross recognized PA-SSFs with0 = 1.0.
In this paper we employ the CFG underly-ing the tree-bank (iie.
T-parser) as the full-parser.Imp lementat ion  of  pars ing  a lgor i thmThe current pilo~ implementation of the partial-parser does not take local context of PA-SSFs intoconsideration.
The partial-parser is implemented asa parser for TSGs (Sima'an, 1996a), based on anextension to the CYK algorithm (Younger, 1967)).However, the partial-parser can be implemented as aCascade of Finite State Transducers (CFSTs).
A Fi-nite State Transducer (FST) is learned at each iter-ation of the learning algorithm; the FST's languageis the set of PA-SSFs learned at that iteration, andthe output of the FST on recognition of a PA-SSFis the set of subtrees associated with that PA-SSF.5 Existing related methodsEBL was introduced to NLP by Rayner (Rayner,1988); Rayner employs EBL for specializing broad-coverage grammars to specific domains.
In (Raynerand Samuelsson, 1994, Rayner and Carter, 1996)grammar specialization is conducted by chunkingthe trees of a tree-bank according to "chunking crite-ria" which are manually specified e.g.
chunks corre-spond to trees with roots which correspond to full ut-terances, NPs, PPs or non-recursive NPs.
Samuels-son (Samuelsson, 1994) is the first to depart frommanual specification of chunking criteria in NLP;the chunking of the tree-bank trees employs the in-formation theoretic measure of entropy.
Samuelssonmeasures the entropy of a grammar non-terminalas the measure of how hard it is to decide on thechoice of the next rule application given that non-terminal.
Then he marks the nodes with the largestentropy as cutting nodes using an iterative algo-rithm.
In (Srinivas and Joshi, 1995) the specificstructure of the Lexicalized Tree-Adjoining Gram-mar (LTAG) derivations i  exploited to result in anEBL method specific for LTAG.
This differs fromthe other efforts in that the generalization whichthey employ is not limited only to goal-regressionbut allows generalizing the structure of explanations.Their method learns from the LTAG derivations ofthe training-examples all sequences of PoSTags andreduces those to regular-expressions by generalizingon sequences of adjunctions with a Kleene-star; thegeneralized LTAG-derivations are stored indexed bythe PoSTag sequences.Relation to Samuelsson's EBL: The presentmethod is similar to Samuelsson's in that it learns"cutting criteria" from the data.
Our method differsfrom Samuelsson's in that the cutting criteria arecomputed from an opposite direction.
Samuelsson'smaximum entropy is aimed at maximizing coverage,and his approach is derivational since the entropyis computed on steps of derivations tarting fromthe start non-terminal.
The target concept of ourmethod is a PA-SSF not a non-terminM (i.e.
"prob-ably always constituent" vs .
.
"constituent" resp.
).Our method assumes a reductive approach and re-Sima 'an 111 Data- Oriented Parsingsults in a partial-parser rather than a specializedparser.Relation to LTAG's EBL: The concept of PA-SSF employed by our method is a generalization fthe sentential PoSTag sequences employed in (Srini-vas and Joshi, 1995).
Our method can be easilyextended to accommodate LTAG generalizations ofderivations and of PA-SSFs; to this end it is nec-essary to have a tree-bank annotated with LTAGderivations.
The subtrees associated with learnedPA-SSFs are then generalized partial derivations ofLTAG.6 Application to DOPThis section relates the present EBL method toexisting models of disambiguation that projectstochastic grammars from tree-banks, e.g.
(Bod,1995, Charniak, 1996, Sekine and Grishman, 1995).To this end, we firstly relate these models to EBL,and then show that our new EBL method refinesthese models.We are concerned only with models that projectthe same grammatical description as that employedfor annotation of the tree-bank.
Among these mod-els, the Data Oriented Parsing (DOP) model (Scha,1990( Bod, 1995) takes the most radical point ofview.
DOP projects all partial-trees from a tree-bank and employs them as a stochastic gram-mar called a Stochastic Tree-Substitution Gram-mar (STSG).
Other models in the same categoryare presented in (Charniak, 1996, Sekine and Grish-man, 1995).
Charniak (Charniak, 1996) employs thetree-bank for projecting Stochastic CFGs (SCFGs).And (Sekine and Grishman, 1995) presents a con-strained DOP-like model which projects STSGs;cutting the tree-bank trees takes place only at nodeslabeled either with S or with NP.
In this section weconcentrate on DOP since it constitutes a general-ization of the other two efforts.In (Bod, 1995), the specification of DOP is as fol-lows.
A DOP model has four parameters:1. sentence-analyses, i.e.
syntactically labeledphrase structure trees given in a tree-bank,2.
sub-anMyses, i.e.
partial-trees,3.
combination-operations, i.e.
substitution, and4.
combination-probabilities.The rest of the definition of the DOP model con-cerns how to infer probabilities of partial-trees fromthe tree-bank, and how to compute probabilities ofcombinations of partial-trees.
The instantiation ofDOP as realized in (Bod, 1995) is an STSG, whichhas the set of all partial-trees ofthe tree-bank treesas elementary-trees.
We shall not give further de-tails of DOP since this is out of the scope of thispaper.Let us rewrite Bod's specification using the termi-nology of EBL.
Firstly, the so called domain-theoryconsists of the annotation convention as well as theannotation i tuitions used for the annotation of thetree-bank.
The tree-bank contains entences andtheir tree structures: the trees constitute "explana-tions" (proofs) given by the domain-theory to thefact that the sequences of words on their frontiersare sentences.
The target-concept of DOP is the con-cept of a constituent, represented by non-terminalsof the tree-bank trees.
The sub-analysis used byDOP are simply partial-trees, which form instancesof the target-concept.
These partial-trees are ob-tained by using a simple operationality criterion,which states that any partial-tree obtained froma tree-bank tree is acceptable (in the experimentsmentioned in (Bod, 1995), Bod limits the depth ofpartial-trees, Charniak (Charniak, 1996) limits thepartial-trees to CFG rules, and in (Sekine and Gr-ishman, 1995) only a subset of the non-terminals areallowed to supply partial-trees).
The combination-operation of DOP is inherent to the assumption thatthe theory (phrase structure grammar) employs thatoperation.
The fourth parameter of DOP, i.e.
theinference and the definitions of probabilities of com-binations of partial-trees, extends the EBL scheme.This extension enables DOP, and the other modelsmentioned above, to apply statistical analysis overlarge sets of trees in order to facilitate disambigua-tion.
The interesting part of viewing these modelsin EBL terminology is the fact that these models donot aim at speedup, but rather at the memory-basedbehavior of EBL.The new EBL method can be used in order to de-fine the operationality criterion for DOP as follows.?
Apply the algorithm in figure 2 to the giventree-bank.
The result is the same tree-bank ex-cept that now there are marking on nodes whichdelimit he subtrees associated with the learnedPA-SSFs.?
Mark also all nodes which are not internal toany subtree associated with a learned PA-SSF.And mark all PosTag nodes in all tree-banktrees.If learning was successful, then only some of thenodes of the tree-bank trees are marked now.
Theoperationality criterion for DOP is then:A partial-tree is projected iff its root andSima 'an 112 Data-Oriented Parsingthe nodes on its frontier are marked, i.e.cutting the trees for DOP is not allowed atunmarked nodes.
Crucially, this way of cut-ting allows the projection of partial-treeswhich are combinations of subtrees associ-ated with learned PA-SSFs.The main remaining question on this refinement ofDOP concerns the probabilities of the partial-treesprojected from the tree-bank.
In DOP, the proba-bility of a partial-tree with a root labeled N is de-fined as the ratio between its frequency and the to-tal frequency of all partial-trees that have N as theirroot-label.
Since the space of partial-trees i smallerin the refinement, the probabilities will be differentthan in the original DOP.
We conjecture that dueto reducing the number of parameters of the model,sparse-data effects should be reduced (future workshall address this issue).7 Empirical resultsThe present method was developed within a Dutchnational project on a dialogue system concern-ing public-transportatwn information (called OVIS)(http://grid.let.rug.nh4321/).
Within the project,a vast amount of dialogues were collected, and theuser's utterances were syntactically and semanticallyannotated (Scha et al, 1996).
For experimentationwe employ a tree-bank of the first 5000 syntacticallyannotated utterances.
Here we only report experi-ments on parsing transcribed utterances I .____-.--~.__.
vr*P_ER~ *V  " ' ' ' ' " "~ MI~ INFP .
.
_ .
.
_ .
.
_ ___ .
,~IN  FPi ~ J ,p  - - ' f  NPwil I * NUb l - - - ' - - '~  *N \] om j I vertrekken~r U r want  a t  o ,c~onc k to leaveFigure 3: Another tree from OVISThe annotation of the OVIS tree-bank is exem-plified by the trees in figures 1 and 3.
Due to thefact that OVIS contains answers to questions withina dialogue system, the sentences are often short butsurprisingly variable in structure; many of these sen-tences contain repetitions, corrections and strangeconstructions (usually rendered ungrammatical bylinguistic theories).
Below we report on two setsof experiments.
The first set observes the learningcurves of the present EBL method by combining thelearned partial-parsers with a T-parser (i.e.
CFG).1 The present method was apphed together with DOPfor parsing word-graphs in a speech recognition-task andresulted in, compared to DOP, on average speedup of10 times with virtually no loss of accuracy.
Averagespeedup for word-graphs containing more than 40 statesexceeds 20 times.And the second set studies the refinement of DOPusing the present EBL method.
All t iming experi-ments were conducted on SGI Indigo with 640 MBRAM.Precision1.06 , , ,1.041.021.000.98 -c0.960.940.92Retreating ?No -Ret reat ing  ~-CFG 0 -?
.
o  ?
.?
.
. '
.~ .
"?
?
?
"0  ?
?
."
.
.
.
.
.
.
.
.
.
. "
" ?I I I I1000 2000 3000 4000Training-set SizeFigure 4: Learning curves for parser-precisionRatio space1.000.800.600.400.200.00I I. .
.
.
.
.
.+ .
.
.
.
.
.
-I IRetreating ?No-Ret reat ing  -~?
.+ .
?.
+ .
.
.
.
?~ .
.
.
, ?
"?
.
.
.
.
.@.
.?
.
?
.
.
.
.
.
.@.
?
.
?i I i1000 2000 3000 4000Training-set SizeFigure 5: Learning curves: active-nodes BBL+T_P .
.
.
.
.
T_ParserOVIS exper iments  w i th  T-parserThe experiments concern both coverage as well assize of parse-space.
We employ the T-parser un-derlying the tree-bank (CFG) as a full-parser.
Intable 1 we list the results of ten independent ex-periments, each obtained by a random split of 4500training-set and 500 test-set.
Since the domain con-tains many (easy for parsing) one word utterances(e.g.
"yes" or "no"), we exclude one word utter-ances from the results.
On average, the ten test-setscontained 337.2 (of 500) utterances longer than oneword.
Table 1 shows the results on utterances longerthan one word, with mean length of 5.57 words perutterance.
For training the EBL learning algorithmwe set a threshold on the frequency of SSFs: 0.3%of the size of the training-set (i.e.
14).
To avoidproblems of unknown words, we allowed the wordsof the test-set o be included with all postags withSima 'an 113 Data-Oriented ParsingParserT_parserParWT_parserRight parse in chart97.78% (1.1%)93.23% (1.1%)Any parse in chart Precision Active nodes99.62% (0.3%) 98.15% (1.2%) 135.16 (248.93)99.11% (0.5%) 94.06% (1.5%) 31.17 (81.45)Table 1: Means and STDs of ten experiments (OVIS): Par denotes Partial-Parserwhich they appear in the whole tree-bank (for bothparsers).Table 1 shows the statistical means and (in brack-ets) the standard eviations of the ten experiments(always for sentences longer than 1 word).
R ightparse (also structural consistency) denotes the per-centage of test sentences for which the parser'schart contains the right parse (i.e.
test-set parse).Any  parse (also coverage) denotes the percentageof test sentences for which the parser's chart con-tained a parse.
Prec is ion  denotes the ratio (Rightparse/Any parse), which expresses the precision ofthe parser as a parse-space generator.
And act ivenodes denotes the mean number of active items in aCYK parser implementation; active items are thoseitems that participate in a full parse of the sentence.On average the partial-parser reduces the spaceby 4.33 times on all sentence lengths.
The reduc-tion of space reaches a mean of 7 times on sentenceslonger than 6.
The degradation i  precision (4%)is due to several reasons.
Firstly, the fact that thepartial-parser is currently implemented asa context-free recognizer clearly contributes to this degrada-tion.
Secondly, after analyzing the test-results ofone experiment, we found out that about half of theerrors are due to deeper structures assigned by thePartial-Parser rather than really wrong structures;typically those were compound NPs which receivedshallow annotations in the tree-bank.
Thirdly, partof the errors is due to tree-bank annotation mistakes.And finally, there is a remaining part of errors whichis due to the assumptions of the EBL method; theseare harder to solve than the previous three.In figure 4 and 5 we show the learning curvesof the present method for six sizes of training-sets;five of the six training-sets were obtained randomlyfrom a set of 4500 trees, and the sixth consistedof the whole set.
For these experiments we em-ployed the same set of 500 test-trees randomly cho-sen (all length sentences).
The experiments wererepeated twice: once allowing "retreating" on local-context (as explained earlier), and once not allowingthat, during the learning phase (the two versionsare denoted "Retreating" and "No-Retreating" re-spectively).
The learning curves of the Retreatingpartial-parser, show that from a certain point onthere is some deterioration of precision but furthergain of space-reduction.
The situation is differentNumberof  events450.00375.00300.00225.00150.0075.004?I IDOP ?EBL+DOP +Q.?
.
.
.
.
.
.
.
.
.
.
.
.
.~ ' ' - - I , - , , , .
L  .
.
.
.
.
.
A_ .
.
.
, .
.
.
I .
,  , .
.
.
i10 20 30 40CPU-secs.Figure 6: Number of sentences to CPU-time50with the No-Retreating version.
The explanationfor the loss of precision is that when the training-setis smaller, less PA-SSFs are learned, which implies alarger role for the T-Parser.
This situation is mag-nified by the fact that the coverage of the T-Parseris lower on smaller training-sets.
The deteriorationof precision of the Retreating version compared tothe No-Retreating version is due to the fact that thenumber of learned local-context PA-SSFs becomesmuch larger; this implies reduction of parse-spacebut also some loss of precision (since the partial-parser does not employ the local-context).OVIS exper iments  w i th  DOPTo test the present method together with DOP weemployed the same 10 random splits which we em-ployed in the previous experiments.
This time wedid not include anything about unknown words inthe test-sets (i.e.
a sentence that includes an un-known word is not parsable).
DOP and EBL+DOPwere trained employing the following parameter set-ting for partial-trees (cf.
(Sima'an, 1996a)): for eachprojected partial-tree, a maximum was set on itsdepth (D), number of substitution-sites (N) on itsfrontier, number of words (W) and number of con-secutive words (C) on its frontier.
The setting wasD=4, N=2, W=7 and C=2.
This reduces the num-ber of elementary-trees which DOP projects dras-tically without loss of accuracy.
Furthermore, theEBL algorithm was trained with a threshold on thefrequency of SSFs equal to 14.
The EBL method isused for both specializing the T-parser, which DOP~i~na ~an 114 Data-Oriented ParsingSystemDOPEBL+DOPEBL0.75+DOPSystemDOPEBL+DOPParParEBL0.75+DOPParPar0.75Coverage i Accuracy95.00% (1.4%1 93.50% (0.1%94.61% (1.4%t 91.72% (0.1%94.96% (1.3%t 91.90% (1.4%number  of  t rees27907 (1634)23660 (302)84.4 (4.8)23728 (138)80.6 (3.0)CPU-secs .
tor  sentence  length>2 \[ _>7 \[ _>103.98 (11.29) 13.55 q22.84) 37.46 t41.35)1.28 (2.31) 2.98,4.46) 6.2118.67)1.33 (2.43) 3.18~ 4.69) 6.8518.97)number  of  nodes  in t rees141960 (938)117134 (1627)818.5 (10.26)117750 (1551)812 (lO.4O)Table 2: Means and STDs of ten experiments (OVIS), ParPar denotes Partial-Parseremploys prior to disambiguation (Sima'an, 1996a),and for specifying the cut-nodes for DOP.Another set of experiments on the same 10 ran-dom splits (denoted EBL0.75 in table 2) was con-ducted where the threshold on 0 was set at 0.75,i.e.
a sequence of grammar symbols was allowed tobe learned if it was for at least 75% of the time anSSF.
This was achieved by allowing the learning al-gorithm to change the threshold (0) on the definitionof PA-SSF; each time there are no more PA-SSFs tolearn, 0 was reduced by 0.03 and learning went on.Table 2 lists the means and standard eviation forthe 10 experiments for all sentences of length largeror equal to 2 words.
The average (std of) percent-age of the sentences that included an unknown wordis 2.56% (0.93%).
The measures which the table listsare coverage and accuracy, where coverage is thepercentage of sentences that received aparse, and ac-curacy  is the percentage of parsable sentences thatreceived exactly the same parse as the test-set coun-terpart.
The prec is ion  of a method is equal to themultiplication of the two previous measures.On average, DOP "guesses" in 88.82% (i.e.
pre-cision) of the cases exactly the same test-set parse;with EBL this becomes 86.77%, i.e.
a loss of 2.05%.The speedup is on average 3.1 times but, more im-portantly, the standard-deviation in processing timeis less than a fifth.
On longer sentences, the speedupexceeds 6 times.
Figure 6 shows the accumulativefrequency of sentences to CPU-time: for x secs., thefigure shows the number of sentences that take atleast x secs.
in parsing.
If a deadline of 5 secs.
isset beforehand, DOP misses around the 600 cases(of 3372) while the EBL misses less than 100 cases.At 10 secs.
the figures are 263 to 23, and at 20 secs.it's 116 to 6 cases respectively.The version EBL0.75 shows similar learning capa-bilities to the EBL, (i.e.
EBL1.0) version.
Its preci-sion is slightly better with 87.26% and its coverageis virtually the same as DOP's.
The EBL0.75 doesnot improve speedup though (actually it's slightlyslower).
The explanation to this behavior is simple:EBL0.75 does not seem to learn significantly manymore rules than EBL1.0 and, during parsing, it givesup the assumption that PA-SSF borders are trust-worthy.
This way it takes less risk but then it slightlyloses speed.
Again we conjecture that EBL0.75would provide more speedup if local-context wouldbe used during partial-parsing.
Table 2 shows alsothe sizes of grammars which DOP projects with andwithout EBL.
The number of elementary-trees in thetable for the Partial-Parser does not include the lex-icon.
The sizes of the statistical grammars of DOPwith EBL is about 1.2 times smaller than DOP's.This is not the reduction which we hoped for, butit is quite evident that this is due to constrainingthe EBL mechanism; currently learning takes placeonly where local-context can be assumed of minorimportance.8 Conc lus ions  and  fu ture  workWe described anew view of EBL methods for parsingaiming directly at partial-disambiguation.
Speedupis due to fast parsing that minimizes the parse-space prior to the, often, expensive probabilisticdisambiguation.
This view is exemplified by anEBL method, which 1) specializes parsers by infer-ring partial-parsers, and 2) refines existing stochas-tic models of disambiguation.
From preliminary ex-periments with a pilot implementation we observethat the method has the potential of speeding-upparsing, especially for Speech Understanding wherethe input is a word-graph.
Also we see that it ispossible to minimize coverage loss when using EBLand still gain space-reduction and speed.
However,these experiments have shown that it is hard to gainspeed and space-reduction without employing local-context and without extensive training-sets.Work on extending the parsing algorithm to ac-commodate locM-context is being carried out andshall be ready very soon.
Further exploration willproceed on several fronts.
We intend to test thisSima 'an 115 Data-Oriented Parsingmethod on larger and harder tree-banks.
An im-plementation as CSFTs will be studied and imple-mented.
We shall also study other measures ofutility, as mentioned earlier in this paper.
And fi-nally, we might extend this method as in (Srinivasand Joshi, 1995) or employ existing similarity-basedmeasures for matching PA-SSFs, instead.Acknowledgements:  This work was supportedby project 305 00 903, Priority Programme for Lan-guage and Speech Technology, Dutch Organizationfor Scientific Research (NWO).
I thank ChristerSamuelsson, Remko Scha and Remko Bonnema forcomments on earlier versions.Re ferencesBod, R. (1995).
Enriching Linguistics with Statis-tics: Performance models of Natural Lan-guage.
Phd-thesis, ILLC-dissertation series1995-14, University of Amsterdam.Charniak, E. (1996).
Tree-bank Grammars.
In Pro-ceedings AAAI'96, Portland, Oregon.DeJong, G. and Mooney, R. (1986).
Explanation-Based Generalization: A Alternative View.Machine Learning 1:2, pages 145-176.Minton, S. (1990).
Quantitative Results Concern-ing the Utility Problem of Explanation-BasedLearning.
Artificial Intelligence, 42:363-392.Mitchel, T., Keller, R., and Kedar-Cabelli, S. (1986).Explanation-Based Generalization: A Unify-ing View.
Machine Learning 1:1.Rayner, M. (1988).
Applying Explanation-BasedGeneralization to Natural Language Process-ing.
In Proceedings International Conferenceon Fifth Generation Computer Systems, pages1267-1274, Kyoto, Japan.Rayner, M. and Carter, D. (1996).
Fast Parsingusing Pruning and Grammar Specilization.
InProceedings ACL-96, Santa Cruz, CA.Rayner, M. and Samuelsson, C. (1994).
Corpus-Based Grammar Specilization for Fast Anal-ysis.
In Spoken Language Translator: FirstYear Report, Agnas et.
al.
SRI technical reportCRC-043, http ://www.cam.sri.eom.Samuelsson, C. (1994).
Grammar SpecializationThrough Entropy Thresholds.
In ProceedingsACL'94, Las Cruces, New Mexico.Scha, R. (1990).
Language Theory and LanguageTechnology; Competence and Performance (inDutch).
In de Kort, Q. and Leerdam, G.,editors, Computertoepassingen in de Neerlan-distiek, Almere: LVVN-jaarboek.Scha, R., Bonnema, R., Bod, R., and Sima'an,K.
(1996).
Disambiguation and Interpretationof Wordgraphs using Data Oriented Parsing.Probabilistic Natural Language Processing inthe NWO priority Programme on Languageand Speech Technology, Amsterdam.Sekine, S. and Grishman, R. (1995).
A Corpus-based Probabilistic Grammar with Only TwoNon-terminals.
In Proceedings Fourth Inter-national Workshop on Parsing Technologies,Prague, Czech Republic.Sima'an, K. (1996a).
An optimized algorithm forData Oriented Parsing.
In Mitkov, R. and Ni-colov, N., editors, Recent Advances in NaturalLanguage Processing 1995, volume 136 of Cur-rent Issues in Linguistic Theory.
John Ben-jamins, Amsterdam.Sima'an, K. (1996b).
Computational Complexity ofProbabilistic Disambiuation by means of TreeGrammars.
In Proceedings of COLING'96,volume 2, pages 1175-1180, Copenhagen, Den-mark.Srinivas, B. and Joshi, A.
(1995).
Some NovelApplications of Explanation-Based Learningto Parsing Lexicalized Tree-Adjoining Gram-mars.
In Proceedings ACL-95.van Harmelen, F. and Bundy, A.
(1988).Explanation-Based Generalization = Par-tial Evaluation (research note).
ArtificialIntelligence 36, pages 401-412.Younger, D. (1967).
Recognition and parsing ofcontext-free languages in time n 3. lnf.
Control,10(2):189-208.S/ma 'an 116 Data- Oriented Parsing
