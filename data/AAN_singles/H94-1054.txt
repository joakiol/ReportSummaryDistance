JAPANESE WORD SEGMENTATION BY HIDDEN MARKOV MODELConstantine P. PapageorgiouBBN Systems and Technologies70 Fawcett  St.Cambridge, MA 02138ABSTRACTThe processing of Japanese text is complicated by the fact thatthere are no word delimiters.
To segment Japanese text,systems typically use knowledge-based methods and largelexicons.
This paper presents a novel approach to Japaneseword segmentation which avoids the need for Japanese wordlexicons and explicit rule bases.
The algorithm utilizes ahidden Markov model, a stochastic process, to determine wordboundaries.
This method has achieved 91% accuracy insegmenting words in a test corpus.1.
INTRODUCTIONThe segmentation of Japanese words is one of the mainchallenges in the automatic processing of Japanese text.Unlike English text which has spaces that separateconsecutive words, there are no such word boundary indicatorsin sentences of Japanese (kanji and kana) text.The algorithms used to obtain robust segmentation ofJapanese text generally utilize two techniques, lexicon andrule-based approaches.
Large lexicons are inevitably used inconjunction with or as a part of the text segmentingalgorithms that have been developed.
These lexicons are oftentime consuming to build and are thus not an optimal solution.Knowledge based approaches typically entail a significantamount of human effort in specifying the rules that will be usedto determine word segmentation a d do not provide sufficientcoverage of the language's grammar rules.This paper introduces a hidden Markov model (HMM) whichhas been developed for Japanese word segmentation.
HiddenMarkov models are part of the larger class of probabilisticalgorithms.
These approaches use large sets of data to abstractaway the structure of the domain being learned, asprobabilities.
We will see that with sufficient data such astochastic process can achieve 91% accuracy in wordsegmentation which approaches the state-of-the-art ins6gmentation techniques.2.
JAPANESE WORD SEGMENTATIONTECHNIQUESCurrent Japanese word segmentation techniques consistentlyrely on large lexicons of words in their decision makingprocedure.
A typical word processor will have both aknowledge base which encodes rules of Japanese grammar, aswell as a lexicon of over 50,000 words \[Mori, et.
al.
1990\].Hypothesized segments of incoming text are analyzed todetermine if they have any semantics and therefore are morelikely to be correct segments; the grammar ules are theninvoked to make final segmentation decisions.
Thistechnology achieves 95 percent accuracy in segmentation.An alternative approach to Japanese word processingtechnology is the development of an architecture for Japanesesegmentation a d part of speech labeling shown in Figure 1\[Matsukawa, et.
al.
1993\].Japanese textI JUMANAMEDI POSTword segments with part of speechFigure 1: BBN's JUMANIAMEDIPOST word segmentationand part of speech labeling architecture.The architecture of the system is as follows:JUMAN, a rule-based morphological processorwhich uses a 40,000 word lexicon and aconnectivity matrix to determine wordsegmentation a d part of speech labeling,2.
AMED, a rule-based segmentation a d part ofspeech correction system trained on parallelhypothesized and correct annotations ofidentical text,3.
POST, a hidden Markov model whichdisambiguates segmentation a d part of speechdecisions.283This unified architecture achieved an error rate of 8.3% in wordsegmentation; this level of error can be attributed to JUMAN'srelatively small lexicon and lack of sufficient training data forthe AMED and POST modules.Dragon Systems' LINGSTAT machine translation system\[Yamron, et.
al., 1993\] uses a maximum likelihoodsegmentation algorithm which, in essence, calculates allpossible segmentations of a sentence using a large lexicon andchooses the one with the best score, or likelihood.
Theimplementation uses a dynamic programming algorithm tomake this search efficient.MAJESTY is a recently developed morphological preprocessorfor Japanese text \[Kitani, et.
al., 1993\].
On a test corpus, itachieved better than 98% accuracy in word segmentation a dpart of speech determination; this represents he state-of-the-art in such technology.Teller, et.
al.
\[1994\] present a probabilistic algorithm whichuses character type information and bi-gram frequencies oncharacters in conjunction with a small knowledge base tosegment non-kanji stnngs.
While it is related to our hiddenMarkov model approach in that it is character-based and doesnot rely on any lexicons, it differs in that it reties on a certainamount of a priori knowledge about the morphology of theJapanese language.
This algorithm achieved 94.4% accuracyin segmenting words in a test corpus.3.
HIDDEN MARKOV MODELHidden Markov models are widely used stochastic processeswhich have two components.
The first is an observablestochastic process that produces equences of symbols from agiven alphabet.
This process depends on a separate hiddenstochastic process that yields a sequence of states.
HiddenMarkov models can be viewed as finite state machines whichgenerate sequences of symbols by jumping from one state toanother and "emitting" an observation ateach state.The general recognition problem, as stated in the literature, is:given a hidden Markov model, M, with n symbols and mstates, and a sequence of observations, O = OlO2...o t,determine the most likely sequence of states, S = SlS2...s twhich could yield the observed sequence of symbols.3.1.
Model DevelopmentThe hidden Markov model for Japanese word segmentation wasdesigned with several goals in mind:?
Avoiding an approach which relies on having alarge lexicon of Japanese words.?
Allow the model to be easily extensible (withnew training, of course) to accommodate moredata or a different language.While not of paramount importance, analgorithm which segments rapidly would bepreferred; word segmentation is a pre-processing step in most Japanese text systemsand should be as unobtrusive and transparent aspossible.One possible algorithm for segmentation is to use a hiddenMarkov model to find the most likely sequence of words basedon a brute force computation of every possible sequence ofwords; the POST component of the word segmentationarchitecture described in Section 2 uses a similar model.
This,though, violates the above constraint of no reliance on aJapanese word lexicon.
Given that we would like to avoid theoverhead associated with constructing and using a word-basedlexicon, we are therefore forced to approach the problem in amanner which focuses on discrete characters and theirinterrelationships.The segmentation model we developed avoids the need for botha lexicon of Japanese words and explicit rules.
It takesadvantage of the effectiveness of subsequences of two textcharacters in determining the presence or absence of a wordboundary.
In essence, we will show that the morphology ofthe Japanese language is such that 2-character sequences havesome underlying meaning or significance with respect o wordboundaries.To solidify this idea, let us focus on two unspecified textcharacters, k 1 and k 2.
Suppose that out of 100 places in thetraining data where k 1 is followed by k 2, the vast majority ofthese occur at word boundaries.
From a probabilisticviewpoint, we are justified in coming to the conclusion that"klk 2 denotes a word boundary".
To complicate things,assume that out of 100 places where k 1 is followed by k 2, 50of these are at word boundaries and 50 are inside words.
Itwould seem that no conclusions could be drawn from thissituation.
On the other hand, if we notice that the 50 instancesof klk 2 at word boundaries all had word boundaries between k 1and the character preceding k1, but none of the instances ofklk 2 within a word had word boundaries before the kl, then wecan hypothesize the following relationship, where 'T' denotesa word boundary and k x is the character preceding kl:ifk x I k 1, then k 1 I k 2 otherwise klk 2This is exactly the sort of hidden structure that HMMs aregeared towards uncovering.Proceeding in this manner, a model for Japanese wordsegmentation was developed which capitalizes on this idea ofthe significance of 2-character sequences in word boundarydetermination; the state transition diagram is shown in Figure2.
In the model there are just two possible states, either a wordboundary (B) or a word continuation (C).
The observationsymbols are all possible 2-character sequences The kanjialphabet consists of approximately 50,000 characters; ofthese, 6,877 form a standard character set which suits mosttext processing purposes \[Miyazawa, 1990; Mori, et.
al.1990\].
Factoring in the size of the hiragana nd katakanaalphabets, the number of possible 2-character sequencesgenerated exclusively by this subset approaches 5"107, aclearly unmanageable amount of data.
An implicit assumption284of our model is that there is a small subset of all possible 2-character sequences which in fact accounts for a largepercentage of the 2-character sequences normally used inwritten text.
It is such a subset which the model hopes touncover and use in further classification.B = word boundaryC = word continuationFigure 2: The state transition diagram for the Japanese wordsegmentation HMM.The algorithm proceeds by sliding a 2-character window overan input sentence and calculating how likely it is that each ofthese 2-character sequences i a word boundary or within aword, given the previous 2-character sequence's status as eithera word boundary or continuation.
In this manner, the model isa bi-gram model \[Meteer, et.
al., 1991\] over 2-charactersequences ince it relies only on the previous state.
It isimportant to note that consecutive 2-character windowsoverlap by one character.
Figure 3 portrays the progression ofthe window across part of a line of text emitting the 2-character observation symbols.3.2.
TrainingThe model is trained using supervised training over apreviously annotated corpus.
Specifically, training isaccomplished by taking the corpus of segmented text andsimply counting the number of times each 2-character sequence?
has a word boundary between its constituentcharacters?
has no word boundary between its constituentcharactersand the number of times?
word boundaries follow word continuations?
word boundaries follow word boundaries?
word continuations follow word boundaries?
word continuations follow word continuationsSeeing unknown 2-character sequences in the test data (thosesequences that were absent from the training data) leads toobservation probabilities of 0.
To rectify this, upon comingacross an unknown 2-character sequence in the test data, thealgorithm assigns the observation an a priori probability bypostulating that the sequence was actually seen once in thetraining data.
This probability is a sufficiently low value,balancing the fact that the sequence was never seen when allpossible symbols were being gathered, with the hypothesisthat it might be a valid observation.
This procedure is anadmission that even extensive training might not attaincomplete coverage of the domain.3.4.
Implementation IssuesAlgor i thm -- There are generally two basic algorithms forhidden Markov model recognition: the forward-backwardalgorithm and the Viterbi algorithm \[Viterbi, 1967\].
Theforward-backward algorithm (Baum-Welch) computes thelikelihood that the sequence of observation symbols wasproduced by any possible state sequence.
The Viterbi model,on the other hand, computes the likelihoods based on the bestpossible state sequence and is more efficient o compute andtrain.
The word segmentation HMM implementation uses theViterbi approach.
This difference is transparent and mattersonly at the implementation level.Kanji and Kana -- Due to their vast numbers, two bytes areneeded to represent Japanese text characters rather than theconventional one byte for English characters.
Theimplementation can easily support either one or two bytecharacters with few modifications.~"  i~~"-i~'~ -- ~:~:~i~...,.....: .
....... ~,~..,..~....,,.~:.~.~%~?
.
,.~..~ '~.
~f~:...:...:..':~"%~'"%.
?~"~,~, ~.~ .
:, , ~ ........?
~t~ .
.
.
.
~ ~ T  ~'  ~ "~ .
.
.
.
.
.  "
~ ~'"'""~" :: ?
~ " ~''': ~ "Figure 3: Diagram shows the 2-character window sliding over the sentence and uncovering the firstfour observations.285Sentence by Sentence Input -- The only assumption onthe input to the hidden Markov model is that the text be pre-divided into sentences.
Periods were the sole indicators ofsentence ndings that were used.
This assumption is made toprovide for the incremental processing of a body of text.4.
EXPERIMENTS AND ANALYS ISTo train and test the hidden Markov model, a corpus of 5,529Japanese articles that was annotated by the MAJESTY systemwas used since a manually annotated corpus of sufficient sizewas not available.
From these articles, 59,587 sentences(1,882,23'~t words) were used as training material and 634different sentences (21,430 words) were set aside as test data.When the trained model was run over the test sentences, itsegmented 91.15% of the words correctly while achieving96.48% accuracy on word boundaries.
The correctsegmentation f a single word implies that:?
both its beginning and ending word boundariesare.. determined correctly, and?
no extra word boundaries are generated withinthe word.The results over distinct words are given in Table 1 and theresults for word boundaries are in Table 2.Numbertotal 21,430hypothesized 21,298correct 19,533incorrect 1,897% oftotal91.158.85Table 1: Test results over words.Number % oftotaltotal 20,796hypothesized 20,664cor~ct 20,065over~eneratedunder~eneratedTable 2:boundaries.96.48599 2.88731 3.52Test results over wordThese performance figures compare favorably with thepreviously reported results of the BBN Japanese wordsegmentation and part of speech algorithm.
This system,described in Section 2 and currently in use in the BBN PLUMdata extraction system, achieved 91.7% accuracy in wordsegmentation i  a test.
In addition, the word segmentationHMM was designed and implemented in under one person-week, whereas the aforementioned architecture and all itscomponents ook significantly longer.The performance figures listed above are telling; with a simplebut cleverly constructed model, the system managed tocorrectly segment words at a respectable rate.
Thisperformance was achieved entirely without accessing any ofthe word lexicons that are traditionally employed in solvingthis problem.
Furthermore, no rule bases are referred to; thealgorithm simply relies on the structure of the training data toimplicitly obtain a model of Japanese word segmentation.While the HMM both misses and imagines word boundaries, itis encouraging that the total numbers of hypothesized wordsand word boundaries are close to the true numbers.
This assuresus that the model is generating an appropriate number ofboundaries, even though it is not completely accurate on all ofthem.The fact that the model performs to such a high degree hasinteresting implications regarding the morphology of theJapanese language.
The model relies on the idea thatconsecutive characters are significant with regards to whetheror not they will be separated by a word boundary.
Thissuggests that there is a set of pairs of characters which rarelyoccur next to one another within the same word; these are the2-character boundary sequences u ed in the HMM and include atleast the katakana character set as an edge.
Furthermore, theremust be another set of character pairs which are frequentlyfound in succession in the same word, corresponding to themodel's 2-character continuation sequences.4.1.
Training Set SizeAs with any stochastic model, this HMM relies on an accurateset of probabilities which reflect he true nature of the domain.The limiting factor here, barring any gross problems with themodel, is the amount of data on which the model is trained.Clearly, when the training procedure sees the first fewexamples, the HMM is a very poor representation f Japaneseword boundaries.
As such, a large amount of information iscollected in a relatively short period of time in the initialstages of learning.
The model will eventually become morecomplete as it sees a larger and larger portion of the possible2-character sequences.Determining where the size of the training set no longer seemsto be having a great impact on the performance of thealgorithm is of interest as we can find out if the model is under-trained or over-trained.
To get a sense for this, the model wastrained on successively larger test sets, starting with a verysmall training set of 123 words up to the 1,882,231 word set,and then run over the 21,430 word test set and evaluated.Figure 4 summarizes the results of these experiment.286g.=.0o ?mlr~80 T7 0 .
l~~._  Words/ ~60"1- ?~  \[\] Word Boundaries5 0 -I-40302010 m--m--m--m.mI I I I100 1,000 10,000 100,000 1,000,000I10,000,000Words in Training SetFigure 4: Effect of training set size on performance.Using a logarithmic scale for the axis representing training setsize gives a feeling for the additional performance accrued frommore training, while factoring in the impact of theexponentially increasing advances in computing technology.Based on the graph, we can see that while the wordsegmentation error rate is diminishing more slowly as thetraining set size increases to 1,882,231 words (the final pointplotted), the curve still exhibits a downward trend.
Thisimplies that additional training could improve the accuracy ofthis model..~ 200,000.wkm~ ,~~.~ ~ 150,000?
~ 100,000)  o,ooo?~~ 0 I I0 1 ,000 ,000  2 ,000 ,000Words in Training SetFigure 5: Number of unique 2-character sequences.As expected, the largest increase in performance occurs overthe initial 30,000 words where the word segmentation errorrate goes from 75% to 25%.
At approximately 150,000 words,the rate of change in the error rate decreases significantly, butstill shows a distinct downward trend.
Furthermore, thedifference between the word segmentation error rate and wordboundary determination error rate is continuously shrinking;it is expected that with additional training data the gapbetween the curves will diminish.To portray the amount of new information that is received overtime, Figure 5 shows the number of unique 2-charactersequences in each of the successively increasing training sets.It is interesting to note that the model is continuously seeingnew 2-character sequences at a steady, though slightlydecreasing, rate.
By the time the training set numbers 50,000words, the most common 2-character sequences have been seenand further training data, while improving test performance,provides diminishing returns due to the relative rarity of thesenew sequences.5.
CONCLUSIONWe have implemented and described a hidden Markov model forJapanese word segmentation.
The bi-gram model ischaracterized by an unconventional set of observationsymbols, namely, the set of 2-character sequences.
The modelis also extremely simple in that it consists of only two stateswhich encode the existence or absence of a word boundarybetween any two characters.
This probabilistic model wastrained over a large corpus of annotated data and then testedover a different set of data to measure performance; it achievesword segmentation accuracy of 91.15% and determines 96.48%of all the word boundaries correctly.
When contrasted with thestate-of-the-art, the HMM emerges as a worthy contender torelated algorithms based on several observations:2871.
First and foremost, this HMM approachcompletely circumvents the need for Japaneseword lexicons which other approaches heavilyrely upon; the storage issues and overhead forword look-up are thus avoided.2.
The rules that a knowledge-based system woulduse are, in effect, implicit in the probabilitiesdetermined uring supervised training andexactly reflect the morphology of Japaneseword boundaries.3.
The HMM segments text at a blistering pace,approximately 10,000 words/second notincluding initialization time.4.
The model is designed to be easily extensiblewith additional data or to a different language;no lexicons are needed, simply a sufficientlylarge body of text on which the algorithm canbe trained.Most disappointing about the performance of the model is thelarge discrepancy between the word accuracy and the wordboundary accuracy.
This is surely a side-effect of the bi-grammodel topology; there is no way to relate the beginning andending boundaries of a single word with this model unless theword begins and ends in consecutive states (a one-characterword).Regardless, it is interesting and impressive that a two state bi-gram model can model Japanese word boundaries soeffectively.
With additional training data, we anticipate thatthe algorithm's performance will increase.
The nextgeneration of this model should somehow incorporate andmodel the relationship between boundaries of the same word inan effort to raise the word segmentation accuracy closer to theaccuracy level of word boundary determination.
Anothermodification to the algorithm which might improveperformance is extending it to be a tri-gram model.
The HMMcould also be trained and tested on a different language,Chinese for instance, to see how well it performs.The results of this research are encouraging; the re-trainingand extensions noted above should be pursued to increaseaccuracy and to obtain a sense of how generally applicable tocomparable domains this hidden Markov model is.1.2.3.4.5.6.7.8.9.REFERENCESChurch, K. "A Stochastic Parts Program and NounPhrase Parser for Unrestricted Text".
Proceedings ofthe Second Conference on Applied Natural LanguageProcessing, ACL, 1988, p. 136-143.Kitani, T. and Mitamura, T. "Japanese preprocessor forsyntactic and semantic parsing".
Proceedings of theConference on Artificial Intelligence Applications,1993, p. 86-92.Matsukawa, T., Miller, S., and Weischedel, R."Example-Based Correction of Word Segmentation a dPart of Speech Labeling".
Human LanguageTechnology, March 1993, p. 227-232.Meteer, M., Schwartz, R., and Weischedel, R."Empirical Studies in Part of Speech Labeling".Proceedings of the Fourth DARPA Workshop onSpeech and Natural Language, February 1991, p. 331-336.Miyazawa, A.
"Character Code for Japanese TextProcessing".
Journal of Information Processing 13(1),1990, p. 2-9.Moil, K. and Kawada, T. "From kana to kanji: wordprocessing in Japan".
IEEE Spectrum, August 1990, p.46-48.Teller, V. and Batchelder, E. O.
"A ProbabilisticAlgorithm for Segmenting Non-Kanji JapaneseStrings".
to appear in Proceedings of 12th NationalConference on Artificial Intelligence, 1994.Viterbi, A. J.
"Error Bounds for Convolutional Codesand an Asymptot ica l ly  Opt imum DecodingAlgorithm".
IEEE Transactions on Information TheoryIT 13(2), April 1967, pp.
260-269.Yamron, J., Baker, J., Bamberg, P., Chevalier, H.,Dietzel, T., Elder, J., Kampmann, F., Mandel,' M.,Manganaro, L., Margolis, T., and Steele, E."LINGSTAT: An Interactive, Machine-AidedTranslation System".
Human Language Technology,March 1993, p. 191-195.ACKNOWLEDGEMENTSThe work reported here was supported in part by the AdvancedResearch Projects Agency and was monitored by the Rome AirDevelopment Center under Contract No.
F30602-91-C-0051.The views and conclusions contained in this document arethose of the author and should not be interpreted as necessarilyrepresenting the official policies, either expressed or implied,of the Advanced Research Projects Agency or the United StatesGovernment.288
