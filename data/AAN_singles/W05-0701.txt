Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 1?8,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMemory-based morphological analysis generation andpart-of-speech tagging of ArabicErwin Marsi, Antal van den BoschILK, Tilburg UniversityP.O.
Box 90153NL-5000 LE TilburgThe Netherlands{E.C.Marsi,Antal.vdnBosch}@uvt.nlAbdelhadi SoudiCenter for Computational LinguisticsEcole Nationale de L?Industrie Mine?raleRabat,Morocco,asoudi@gmail.com/asoudi@enim.ac.maAbstractWe explore the application of memory-based learning to morphological analy-sis and part-of-speech tagging of writtenArabic, based on data from the ArabicTreebank.
Morphological analysis ?
theconstruction of all possible analyses ofisolated unvoweled wordforms ?
is per-formed as a letter-by-letter operation pre-diction task, where the operation encodessegmentation, part-of-speech, characterchanges, and vocalization.
Part-of-speechtagging is carried out by a bi-modular tag-ger that has a subtagger for known wordsand one for unknown words.
We report onthe performance of the morphological an-alyzer and part-of-speech tagger.
We ob-serve that the tagger, which has an accu-racy of 91.9% on new data, can be used toselect the appropriate morphological anal-ysis of words in context at a precision of64.0 and a recall of 89.7.1 IntroductionMemory-based learning has been successfully ap-plied to morphological analysis and part-of-speechtagging in Western and Eastern-European languages(van den Bosch and Daelemans, 1999; Daelemans etal., 1996).
With the release of the Arabic Treebankby the Linguistic Data Consortium (current version:3), a large corpus has become available for Ara-bic that can act as training material for machine-learning algorithms.
The data facilitates machine-learned part-of-speech taggers, tokenizers, and shal-low parsing units such as chunkers, as exemplifiedby Diab et al (2004).However, Arabic appears to be a special challengefor data-driven approaches.
It is a Semitic languagewith a non-concatenative morphology.
In addition toprefixation and suffixation, inflectional and deriva-tional processes may cause stems to undergo infixa-tional modification in the presence of different syn-tactic features as well as certain consonants.
AnArabic word may be composed of a stem consist-ing of a consonantal root and a pattern, affixes, andclitics.
The affixes include inflectional markers fortense, gender, and number.
The clitics may be ei-ther attached to the beginning of stems (proclitics)or to the end of stems (enclitics) and include pos-sessive pronouns, pronouns, some prepositions, con-junctions and determiners.Arabic verbs, for example, can be conjugated ac-cording to one of the traditionally recognized pat-terns.
There are 15 triliteral forms, of which at least9 are in common.
They represent very subtle dif-ferences.
Within each conjugation pattern, an entireparadigm is found: two tenses (perfect and imper-fect), two voices (active and passive) and five moods(indicative, subjunctive, jussive, imperative and en-ergetic).
Arabic nouns show a comparably rich andcomplex morphological structure.
The broken plu-ral system, for example, is highly allomorphic: fora given singular pattern, two different plural formsmay be equally frequent, and there may be no wayto predict which of the two a particular singular willtake.
For some singulars as many as three further1statistically minor plural patterns are also possible.Various ways of accounting for Arabic morphol-ogy have been proposed.
The type of account ofArabic morphology that is generally accepted by(computational) linguists is that proposed by (Mc-Carthy, 1981).
In his proposal, stems are formedby a derivational combination of a root morphemeand a vowel melody.
The two are arranged accord-ing to canonical patterns.
Roots are said to inter-digitate with patterns to form stems.
For exam-ple, the Arabic stem katab (?he wrote?)
is com-posed of the morpheme ktb (?the notion of writ-ing?)
and the vowel melody morpheme ?a-a?.
Thetwo are integrated according to the pattern CVCVC(C=consonant, V=vowel).
This means that wordstructure in this morphology is not built linearly asis the case in concatenative morphological systems.The attempts to model Arabic morphology in atwo-level system (Kay?s (1987) Finite State Model,Beesley?s (1990; 1998) Two-Level Model and Ki-raz?s (1994) Multi-tape Two-Level Model) reflectMcCarthy?s separation of levels.
It is beyond thescope of this paper to provide a detailed descriptionof these models, but see (Soudi, 2002).In this paper, we explore the use of memory-based learning for morphological analysis and part-of-speech (PoS) tagging of written Arabic.
The nextsection summarizes the principles of memory-basedlearning.
The following three sections describe ourexploratory work on memory-based morphologicalanalysis and PoS tagging, and integration of the twotasks.
The final two sections contain a short discus-sion of related work and an overall conclusion.2 Memory-based learningMemory-based learning, also known as instance-based, example-based, or lazy learning (Aha et al,1991; Daelemans et al, 1999), extensions of the k-nearest neighbor classifier (Cover and Hart, 1967),is a supervised inductive learning algorithm forlearning classification tasks.
Memory-based learn-ing treats a set of labeled (pre-classified) traininginstances as points in a multi-dimensional featurespace, and stores them as such in an instance basein memory.
Thus, in contrast to most other ma-chine learning algorithms, it performs no abstrac-tion, which allows it to deal with productive but low-frequency exceptions (Daelemans et al, 1999).An instance consists of a fixed-length vector ofn feature-value pairs, and the classification of thatparticular feature-value vector.
After the instancebase is stored, new (test) instances are classified bymatching them to all instances in the instance base,and by calculating with each match the distance,given by a distance kernel function.
Classificationin memory-based learning is performed by the k-NN algorithm that searches for the k ?nearest neigh-bours?
according to the ?
(X,Y ) kernel function1 .The distance function and the classifier can berefined by several kernel plug-ins, such as featureweighting (assigning larger distance to mismatcheson important features), and distance weighting (as-signing a smaller vote in the classification to moredistant nearest neighbors).
Details can be found in(Daelemans et al, 2004).3 Morphological analysisWe focus first on morphological analysis .
Trainingon data extracted from the Arabic Treebank, we in-duce a morphological analysis generator which wecontrol for undergeneralization (recall errors) andovergeneralization (precision errors).3.1 Data3.1.1 Arabic TreebankOur point of departure is the Arabic Treebank 1(ATB1), version 3.0, distributed by LDC in 2005,more specifically the ?after treebank?
PoS-taggeddata.
Unvoweled tokens as they appear in the orig-inal news paper are accompanied in the treebankby vocalized versions; all of their morphologicalanalyses are generated by means of Tim Buckwal-ter?s Arabic Morphological Analyzer (Buckwalter,2002), and the appropriate morphological analysis issingled out.
An example is given in Figure 1.
The in-put token (INPUT STRING) is transliterated (LOOK-UPWORD) according to Buckwalter?s transliteration sys-tem.
All possible vocalizations and their morpho-logical analyzes are listed (SOLUTION).
The analysisis rule-based, and basically consists of three steps.First, all possible segmentations of the input string1All experiments with memory-based learning were per-formed with TiMBL, version 5.1 (Daelemans et al, 2004),available from http://ilk.uvt.nl.2INPUT STRING: \331\203\330\252\330\250LOOK-UP WORD: ktbComment:INDEX: P2W38SOLUTION 1: (kataba) [katab-u_1] katab/PV+a/PVSUFF_SUBJ:3MS(GLOSS): write + he/it [verb]* SOLUTION 2: (kutiba) [katab-u_1] kutib/PV_PASS+a/PVSUFF_SUBJ:3MS(GLOSS): be written/be fated/be destined + he/it [verb]SOLUTION 3: (kutub) [kitAb_1] kutub/NOUN(GLOSS): booksSOLUTION 4: (kutubu) [kitAb_1] kutub/NOUN+u/CASE_DEF_NOM(GLOSS): books + [def.nom.
]SOLUTION 5: (kutuba) [kitAb_1] kutub/NOUN+a/CASE_DEF_ACC(GLOSS): books + [def.acc.
]SOLUTION 6: (kutubi) [kitAb_1] kutub/NOUN+i/CASE_DEF_GEN(GLOSS): books + [def.gen.
]SOLUTION 7: (kutubN) [kitAb_1] kutub/NOUN+N/CASE_INDEF_NOM(GLOSS): books + [indef.nom.
]SOLUTION 8: (kutubK) [kitAb_1] kutub/NOUN+K/CASE_INDEF_GEN(GLOSS): books + [indef.gen.
]SOLUTION 9: (ktb) [DEFAULT] ktb/NOUN_PROP(GLOSS): NOT_IN_LEXICONSOLUTION 10: (katb) [DEFAULT] ka/PREP+tb/NOUN_PROP(GLOSS): like/such as + NOT_IN_LEXICONFigure 1: Example token from ATB1in terms of prefixes (0 to 4 characters long), stems (atleast one character), and suffixes (0 to 6 characterslong) are generated.
Next, dictionary lookup is usedto determine if these segments are existing morpho-logical units.
Finally, the numbers of analyses is fur-ther reduced by checking for the mutual compatibil-ity of prefix+stem, stem+suffix, and prefix+stemin three compatibility tables.
The resulting analy-ses have to a certain extent been manually checked.Most importantly, a star (*) preceding a solution in-dicates that this is the correct analysis in the givencontext.3.1.2 PreprocessingWe grouped the 734 files from the treebank intoeleven parts of approximately equal size.
Ten partswere used for training and testing our morphologicalanalyzer, while the final part was used as held-outmaterial for testing the morphological analyzer incombination with the PoS tagger (described in Sec-tion 4).In the corpus the number of analyses per wordis not entirely constant, either due to the automaticgeneration method or to annotator edits.
As our ini-tial goal is to predict all possible analyses for a givenword, regardless of contextual constraints, we firstcreated a lexicon that maps every word to all anal-yses encountered and their respective frequenciesFrom the 185,061 tokens in the corpus, we extracted16,626 unique word types ?
skipping punctuation to-kens ?
and 129,655 analyses, which amounts to 7.8analyses per type on average.= = = = = k t b = = = ka/PREP+;ka;k;ku= = = = k t b = = = = a/PREP+t;uti;ata;t;utu= = = k t b = = = = = ab/PV+a/PVSUFF_SUBJ:3MS+;b/NOUN_PROP+;ub/NOUN+i/CASE_DEF_GEN+;ub/NOUN+a/CASE_DEF_ACC+;ub/NOUN+K/CASE_INDEF_GEN+;ib/PV_PASS+a/PVSUFF_SUBJ:3MS+;ub/NOUN+N/CASE_INDEF_NOM+;ub/NOUN+;ub/NOUN+u/CASE_DEF_NOM+Figure 2: Instances for the analyses of the word ktbin Figure 1.3.1.3 Creating instancesThese separate lexicons were created for trainingand testing material.
The lexical entries in a lexi-con were converted to instances suitable to memory-based learning of the mapping from words to theiranalyses (van den Bosch and Daelemans, 1999).
In-stances consist of a sequence of feature values and acorresponding class, representing a potentially com-plex morphological operation.The features are created by sliding a window overthe unvoweled look-up word, resulting in one in-stance for each character.
Using a 5-1-5 windowyields 11 features, i.e.
the input character in focus,plus the five preceding and five following characters.The equal sign (=) is used as a filler symbol.The instance classes represent the morphologicalanalyses.
The classes corresponding to a word?scharacters should enable us to derive all associatedanalyses.
This implies that the classes need to en-code several aspects simultaneously: vocalization,morphological segmentation and tagging.
The fol-lowing template describes the format of classes:class = subanalysis; subanalysis; ...subanalysis = preceding vowels & tags +input character +following vowels & tagsFor example, the classes of the instances in Fig-ure 2 encode the ten solutions for the word ktb inFigure 1.
The ratio behind this encoding is thatit allows for a simple derivation of the solution,akin to the way that the pieces of a jigsaw puz-zle can be combined.
We can exhaustively try allcombinations of the subanalyses of the classes, andcheck if the right side of one subanalysis matchesthe left side of a subsequent subanalysis.
This re-construction process is illustrated in Figure 3 (onlytwo reconstructions are depicted, corresponding toSOLUTION 1 and SOLUTION 4).
For exam-ple, the subanalysis ka from the first class in Fig-ure 2 matches the subanalysis ata from the sec-3ka kuata utuab/PV+a/PVSUFF_SUBJ:3MS ub/NOUN+u/CASE_DEF_NOM-------------------------- + ------------------------- +katab/PV+a/PVSUFF_SUBJ:3MS kutub/NOUN+u/CASE_DEF_NOMFigure 3: Illustration of how two morphologicalanalyses are reconstructed from the classes in Fig-ure 2.ond class, which in turn matches the subanaly-sis ab/PV+a/PVSUFF SUBJ:3MS from the thirdclass; together these constitute the complete analysiskatab/PV+a/PVSUFF SUBJ:3MS.3.2 Initial ExperimentsTo test the feasibility of our approach, we first trainand test on the full data set.
Timbl is used with its de-fault settings (overlap distance function, gain-ratiofeature weighting, k = 1).
Rather than evaluatingon the accuracy of predicting the complex classes,we evaluate on the complete correctness of all recon-structed analyses, in terms of precision, recall, andF-score (van Rijsbergen, 1979).
As expected, thisresults in a near perfect recall (97.5).
The precision,however, is much lower (52.5), indicating a substan-tial amount of analysis overgeneration; almost onein two generated analyses is actually not valid.
Withan F-score of only 68.1, we are clearly not able toreproduce the training data perfectly.Next we split the data in 9 parts for training and1 part for testing.
The k-NN classifier is again usedwith its default settings.
Table 1 shows the resultsbroken down into known and unknown words.
Asknown words can be looked up in the lexicon derivedfrom the training material, the first row presentsthe results with lookup and the second row withoutlookup (that is, with prediction).
The fact that evenwith lookup the performance is not perfect showsthat the upper bound for this task is not 100%.
Thereason is that apparantly some words in the test ma-terial have received analyses that never occur in thetraining material and vice versa.
For known wordswithout lookup, the recall is still good, but the preci-sion is low.
This is consistent with the initial resultsmentioned above.
For unknown words, both recalland precison are much worse, indicating rather poorgeneralization.To sum up, there appear to be problems with boththe precision and the recall.
The precision is low forknown words and even worse for unknown words.#Wrds Prec Rec FKnown with lookup 3220 92.6 98.1 95.3Known without lookup 3220 49.9 95.0 65.5Unknown 847 22.8 26.8 24.7Table 1: Results of initial experiments split intoknown and unknown words, and with and withoutlookup of known words.#Wrds Prec Rec FKnown 3220 15.6 99.0 26.9Unknown 847 3.9 66.8 7.5Table 2: Results of experiments for improving therecall, split into known and unknown words.Analysis overgeneration seems to be a side effectof the way we encode and reconstruct the analyses.The recall is low for unknown words only.
Thereappear to be at least two reasons for this undergen-eration problem.
First, if just one of the predictedclasses is incorrect (one of the pieces of the jigsawpuzzle is of the wrong shape) then many, or even allof the reconstructions fail.
Second, some generaliza-tions cannot be made, because infrequent classes areovershadowed by more frequent ones with the samefeatures.
Consider, for example, the instance for thethird character (l) of the word jEl:= = = j E l = = = = =Its real class in the test data is:al/VERB_PERFECT+;ol/NOUN+When the k-NN classifier is looking for its nearestneighbors, it finds three; two with a ?verb imperfect?tag, and one with a ?noun?
tag.
{ al/VERB_IMPERFECT+ 2, ol/NOUN+ 1}Therefore, the class predicted by the classifier isal/VERB IMPERFECT+, because this is the majorityclass in the NN-set.
So, although a part of the cor-rect solution is present in the NN-set, simple major-ity voting prevents it from surfacing in the output.3.3 Improving recallIn an attempt to address the low recall, we revisedour experimental setup to take advantage of the com-plete NN-set.
As before, the k-NN classifier is used,4Prec Rec FKnown 58.6 (0.4) 66.6 (0.5) 62.4 (0.3)Unknown 28.7 (3.7) 37.2 (1.2) 32.2 (2.5)All 53.4 (1.2) 62.2 (0.6) 57.5 (0.8)Table 3: Average results and SD of the 10-fold CVexperiment, split into known and unknown wordsbut rather than relying on the classifier to do the ma-jority voting over the (possibly weighted) classes inthe k-NN set and to output a single class, we performa reconstruction of analyses combining all classes inthe k-NN set.
To allow for more classes in k-NN?soutput, we increase k to 3 while keeping the othersettings as before.
As expected, this approach in-creases the number of analyses.
This, in turn, in-creases the recall dramatically, up to nearly perfectfor known words; see Table 2.
However, this gainin recall is at the expense of the precision, whichdrops dramatically.
So, although our revised ap-proach solves the issues above, it introduces massiveovergeneration.3.4 Improving precisionWe try to tackle the overgeneration problem by fil-tering the analyses in two ways.
First, by rankingthe analyses and limiting output to the n-best.
Theranking mechanism relies on the distribution of theclasses in the NN-set.
Normally, some classes occurmore frequently than others in the NN-set.
Duringthe reconstruction of a particular analysis, we sumthe frequencies of the classes involved.
The result-ing score is then used to rank the analyses in de-creasing order, which we filter by taking the n-best.The second filter employs the fact that only cer-tain sequences of morphological tags are valid.
Tagbigrams are already implicit in the way that theclasses are constructed, because a class containsthe tags preceding and following the input charac-ter.
However, cooccurrence restrictions on tags maystretch over longer distances; tag trigram informa-tion is not available at all.
We therefore derive afrequency list of all tag trigrams occurring in thetraining data.
This information is then used to filteranalyses containing tag trigrams occurring below acertain frequency threshold in the training data.Both filters were optimized on the fold that wasused for testing so far, maximizing the overall F-score.
This yieled an n-best value of 40 and tagfrequency treshold of 250.
Next, we ran a 10-foldcross-validation experiment on all data (except theheld out data) using the method described in the pre-vious section in combination with the filters.
Aver-age scores of the 10 folds are given in Table 3.
Incomparison with the initial results, both precisionand recall on unknown words has improved, indi-cating that overgeneration and undergeneration canbe midly counteracted.3.5 DiscussionAdmittedly, the performance is not very impressive.We have to keep in mind, however, that the task isnot an easy one.
It includes vowel insertion in am-biguous root forms, which ?
in contrast to vowel in-sertion in prefixes and suffixes ?
is probably irreg-ular and unpredictable, unless the appropriate stemwould be known.
As far as the evaluation is con-cerned, we are unsure whether the analyses foundin the treebank for a particular word are exhaus-tive.
If not, some of the predictions that are currentlycounted as precision errors (overgeneration) may infact be correct alternatives.Since instances are generated for each type ratherthan for each token in the data, the effect of to-ken frequency on classification is lost.
For exam-ple, instances from frequent tokens are more likelyto occur in the k-NN set, and therefore their (par-tial) analyses will show up more frequently.
This isan issue to explore in future work.
Depending onthe application, it may also make sense to optimizeon the correct prediction of unkown words, or on in-creasing only the recall.4 Part-of-speech taggingWe employ MBT, a memory-based tagger-generatorand tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1 cor-pus2.
We first describe how we prepared the corpusdata.
We then describe how we generated the tag-ger (a two-module tagger with a module for knownwords and one for unknown words), and subse-quently we report on the accuracies obtained on testmaterial by the generated tagger.
We conclude this2In our experiments we used the MBT software pack-age, version 2 (Daelemans et al, 2003), available fromhttp://ilk.uvt.nl/.5w CONJbdA VERB_PERFECTstyfn NOUN_PROPknt NOUN_PROPnHylA ADJ+NSUFF_MASC_SG_ACC_INDEFjdA ADV, PUNCAlA ADV>n FUNC_WORD...Figure 4: Part of an ATB1 sentence with unvoweledwords (left) and their respective PoS tags (right).section by describing the effect of using the outputof the morphological analyzer as extra input to thetagger.4.1 Data preparationWhile the morphological analyzer attempts to gener-ate all possible analyses for a given unvoweled word,the goal of PoS tagging is to select one of theseanalyses as the appropriate one given the context,as the annotators of the ATB1 corpus did using the* marker.
We developed a PoS tagger that is trainedto predict an unvoweled word in context, a concate-nation of the PoS tags of its morphemes.
Essentiallythis is the task of the morphological analyzer with-out segmentation and vocalization.
Figure 4 showspart of a sentence where for each word the respectivetag is given in the second column.
Concatenation ismarked by the delimiter +.We trained on the full ten folds used in the previ-ous sections, and tested on the eleventh fold.
Thetraining set thus contains 150,966 words in 4,601sentences; the test set contains 15,102 words in 469sentences.
358 unique tags occur in the corpus.
Inthe test set 947 words occur that do not occur in thetraining set.4.2 Memory-based tagger generatorMemory-based tagging is based on the idea thatwords occurring in similar contexts will have thesame PoS tag.
A particular instantiation, MBT, wasproposed in (Daelemans et al, 1996).
MBT has threemodules.
First, it has a lexicon module which storesfor all words occurring in the provided training cor-pus their possible PoS tags (tags which occur belowa certain threshold, default 5%, are ignored).
Sec-ond, it generates two distinct taggers; one for knownwords, and one for unknown words.The known-word tagger can obviously benefitfrom the lexicon, just as a morphological analyzercould.
The input on which the known-word tag-ger bases its prediction for a given focus word con-sists of the following set of features and parametersettings: (1) The word itself, in a local context ofthe two preceding words and one subsequent word.Only the 200 most frequent words are representedas themselves; other words are reduced to a genericstring ?
cf.
(Daelemans et al, 2003) for details.
(2)The possible tags of the focus word, plus the pos-sible tags of the next word, and the disambiguatedtags of two words to the left (which are available be-cause the tagger operates from the beginning to theend of the sentence).
The known-words tagger isbased on a k-NN classifier with k = 15, the modi-fied value difference metric (MVDM) distance func-tion, inverse-linear distance weighting, and GR fea-ture weighting.
These settings were manually opti-mized on a held-out validation set (taken from thetraining data).The unknown-word tagger attempts to derive asmuch information as possible from the surface formof the word, by using its suffix and prefix letters asfeatures.
The following set of features and param-eters are used: (1) The three prefix characters andthe four suffix characters of the focus word (possi-bly encompassing the whole word); (2) The possibletags of the next word, and the disambiguated tagsof two words to the left.
The unknown-words tag-ger is based on a k-NN classifier with k = 19, themodified value difference metric (MVDM) distancefunction, inverse-linear distance weighting, and GRfeature weighting ?
again, manually tuned on vali-dation material.The accuracy of the tagger on the held-out cor-pus is 91.9% correctly assigned tags.
On the 14155known words in the test set the tagger attains an ac-curacy of 93.1%; on the 947 unknown words the ac-curacy is considerably lower: 73.6%.5 Integrating morphological analysis andpart-of-speech taggingWhile morphological analysis and PoS tagging areends in their own right, the usual function of thetwo modules in higher-level natural-language pro-cessing or text mining systems is that they jointlydetermine for each word in a text the appropriatesingle morpho-syntactic analysis.
In our setup, this6All words Known words Unknown wordsPart-of-speech source Precision Recall Precision Recall Precision RecallGold standard 70.1 97.8 75.8 99.5 30.2 73.4Predicted 64.0 89.7 69.8 92.0 23.9 59.0Table 4: Precision and recall of the identification of the contextually appropriate morphological analysis,measured on all test words and split on known words and unknown words.
The top line represents the upper-bound experiment with gold-standard PoS tags; the bottom line represents the experiment with predicted PoStags.amounts to predicting the solution that is precededby ?*?
in the original ATB1 data.
For this purpose,the PoS tag predicted by MBT, as described in theprevious section, serves to select the morphologicalanalysis that is compatible with this tag.
We em-ployed the following two rules to implement this:(1) If the input word occurs in the training data,then look up the morphological analyses of the wordin the training-based lexicon, and return all mor-phological analyses with a PoS content matchingthe tag predicted by the tagger.
(2) Otherwise, letthe memory-based morphological analyzer produceanalyses, and return all analyses with a PoS contentmatching the predicted tag.We first carried out an experiment integrating theoutput of the morphological analyzer and the PoStagger, faking perfect tagger predictions, in order todetermine the upper bound of this approach.
Ratherthan predicting the PoS tag with MBT, we directlyderived the PoS tag from the annotations in the tree-bank.
The upper result line in Table 4 displays theprecision and recall scores on the held-out data ofidentifying the appropriate morphological analysis,i.e.
the solution marked by *.
Unsurprisingly, therecall on known words is 99.5%, since we are us-ing the gold-standard PoS tag which is guaranteedto be among the training-based lexicon, except forsome annotation discrepancies.
More interestingly,about one in four analyses of known words matchingon PoS tags actually mismatches on vowel or conso-nant changes, e.g.
because it represents a differentstem ?
which is unpredictable by our method.About one out of four unknown words has mor-phological analyses that do not match the gold-standard PoS (a recall of 73.4); at the same time,a considerable amount of overgeneration of analy-ses accounts for the low amount of analyses thatmatches (a precision of 30.2).Next, the experiment was repeated with predictedPoS tags and morphological analyses.
The resultsare presented in the bottom result line of Table 4.The precision and recall of identifying correct anal-yses of known words degrades as compared to theupper-bounds results due to incorrect PoS tag pre-dictions.
On unknown words the combination ofheavy overgeneration by the morphological analyzerand the 73.6% accuracy of the tagger leads to a lowprecision of 23.9 and a fair recall of 59.0.
On bothknown and unknown words the integration of themorphological analyzer and the tagger is able to nar-row down the analyses by the analyzer to a subset ofmatching analyses that in about nine out of ten casescontains the ?
* SOLUTION?
word.6 Related workThe application of machine learning methods toArabic morphology and PoS tagging appears tobe somewhat limited and recent, compared to thevast descriptive and rule-based literature particularlyon morphology (Kay, 1987; Beesley, 1990; Kiraz,1994; Beesley, 1998; Cavalli-Sfora et al, 2000;Soudi, 2002).We are not aware of any machine-learning ap-proach to Arabic morphology, but find related is-sues treated in (Daya et al, 2004), who propose amachine-learning method augmented with linguisticconstraints to identifying roots in Hebrew words ?a related but reverse task to ours.
Arabic PoS tag-ging seems to have attracted some more attention.Freeman (2001) describes initial work in developinga PoS tagger based on transformational error-drivenlearning (i.e.
the Brill tagger), but does not provideperformance analyses.
Khoja (2001) reports a 90%accurate morpho-syntactic statistical tagger that uses7the Viterbi algorithm to select a maximally-likelypart-of-speech tag sequence over a sentence.
Diabet al (2004) describe a part-of-speech tagger basedon support vector machines that is trained on tok-enized data (clitics are separate tokens), reporting atagging accuracy of 95.5%.7 ConclusionsWe investigated the application of memory-basedlearning (k-nearest neighbor classification) to mor-phological analysis and PoS tagging of unvoweledwritten Arabic, using the ATB1 corpus as trainingand testing material.
The morphological analyzerwas shown to attain F-scores of 0.32 on unknownwords when predicting all aspects of the analysis,including vocalization (a partly unpredictable task,certainly if no context is available).
The PoS tag-ger attains an accuracy of about 74% on unknownwords, and 92% on all words (including knownwords).
A combination of the two which selectsfrom the set of generated analyses a subset of anal-yses with the PoS predicted by the tagger, yieldeda recall of the contextually appropriate analysis of0.90 on test words, yet a low precision of 0.64largely caused by overgeneration of invalid analy-ses.We make two final remarks.
First, memory-based morphological analysis of Arabic words ap-pears feasible, but its main limitation is its inevitableinability to recognize the appropriate stem of un-known words on the basis of the ambiguous rootform input; our current method simply overgener-ates vocalizations, keeping high recall at the cost oflow precision.
Second, memory-based PoS taggingof written Arabic text also appears to be feasible; theobserved performances are roughly comparable tothose observed for other languages.
The PoS taggingtask as we define it is deliberately separated from theproblem of vocalization, which is in effect the prob-lem of stem identification.
We therefore consider theautomatic identification of stems as a component offull morpho-syntactic analysis of written Arabic animportant issue for future research.ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.
Instance-basedlearning algorithms.
Machine Learning, 6:37?66.K.
Beesley.
1990.
Finite-state description of Arabic morphol-ogy.
In Proceedings of the Second Cambridge Conference:Bilingual Computing in Arabic and English.K.
Beesley.
1998.
Consonant spreading in Arabic stems.
InProceedings of COLING-98.T.
Buckwalter.
2002.
Buckwalter Arabic morpho-logical analyzer version 1.0.
Technical ReportLDC2002L49, Linguistic Data Consortium.
availablefrom http://www.ldc.upenn.edu/.V.
Cavalli-Sfora, A. Soudi, and M. Teruko.
2000.
Arabicmorphology generation using a concatenative strategy.
InProceedings of the First Conference of the North-AmericanChapter of the Association for Computational Linguistics,Seattle, WA, USA.T.
M. Cover and P. E. Hart.
1967.
Nearest neighbor patternclassification.
Institute of Electrical and Electronics Engi-neers Transactions on Information Theory, 13:21?27.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996.
MBT: Amemory-based part of speech tagger generator.
In E. Ejerhedand I. Dagan, editors, Proceedings of Fourth Workshop onVery Large Corpora, pages 14?27.
ACL SIGDAT.W.
Daelemans, A. van den Bosch, and J. Zavrel.
1999.
For-getting exceptions is harmful in language learning.
Ma-chine Learning, Special issue on Natural Language Learn-ing, 34:11?41.W.
Daelemans, J. Zavrel, A. van den Bosch, and K. van derSloot.
2003.
MBT: Memory based tagger, version 2.0, ref-erence guide.
ILK Technical Report 03-13, Tilburg Univer-sity.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2004.
TiMBL: Tilburg memorybased learner, version 5.1, reference guide.
ILK TechnicalReport 04-02, Tilburg University.E.
Daya, D. Roth, and S. Wintner.
2004.
Learning Hebrewroots: Machine learning with linguistic constraints.
In Pro-ceedings of EMNLP?04, Barcelona, Spain.M.
Diab, K. Hacioglu, and D. Jurafsky.
2004.
Automatic tag-ging of arabic text: From raw text to base phrase chunks.
InProceedings of HLT/NAACL-2004.A.
Freeman.
2001.
Brill?s POS tagger and a morphology parserfor Arabic.
In ACL/EACL-2001 Workshop on Arabic Lan-guage Processing: Status and Prospects, Toulouse, France.M.
Kay.
1987.
Non-concatenative finite-state morphology.
InProceedings of the third Conference of the European Chap-ter of the Association for Computational Linguistics, pages2?10, Copenhagen, Denmark.S.
Khoja.
2001.
APT: Arabic part-of-speech tagger.
In Pro-ceedings of the Student Workshop at NAACL-2001.G.
Kiraz.
1994.
Multi-tape two-level morphology: A casestudy in semitic non-linear morphology.
In Proceedings ofCOLING?94, volume 1, pages 180?186.J.
McCarthy.
1981.
A prosodic theory of non-concatenativemorphology.
Linguistic Inquiry, 12:373?418.A.
Soudi.
2002.
A Computational Lexeme-based Treatment ofArabic Morphology.
Ph.D. thesis, Mohamed V University(Morocco) and Carnegie Mellon University (USA).A.
van den Bosch and W. Daelemans.
1999.
Memory-basedmorphological analysis.
In Proceedings of the 37th AnnualMeeting of the ACL, pages 285?292, San Francisco, CA.Morgan Kaufmann.C.J.
van Rijsbergen.
1979.
Information Retrieval.
Butter-sworth, London.8
