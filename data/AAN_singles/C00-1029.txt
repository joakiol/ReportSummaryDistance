A Class-based Probabil ist ic approach to StructuralDisambiguat ionStephen Clark and David WeirSchool of Cognit ive and Comput ing  SciencesUniversity of SussexBrighton, BN1 9IIQ, UK{ st ephec:l_, david~r}Ocogs, usx.
ac.
ukAbstractKnowledge of which words are able to fill p~rtic-ular argum.ent slots of a predicate can be usedtbr structural disambiguation.
This paper de-scribes a proposal :for acquiring such knowledge,and in line with much of the recent work in thisarea, a probabilistic approach is taken.
We de-velop a novel way of using a semantic hierar-chy to estimate the probabilities, and demon-strate the general approach using a preposi-tional phrase atta.chment experiment.1 IntroductionKnowledge of which words are able to fillparticular a.rgument slots of a. l?redlca.te ca,nbe used tbr structural disa.mbiguation.
Inthe following example (Charnial~, 1993), thefact that dog, rather than prize, is oftenthe su.1)ject of r'lm, can t)e used to decideon the attachment site of the relative clause:Fred awarded a prize for the dog that ran the fastestWe describe a proposal for acquiring suchknowledge, and as in other recent work in thisarea (Resnik, 1993; l,i and Abe, t998), a prob-abilistic approach is taken.
Using probabilitiesaccords with the intuition that there are no ab-solute constraints on the arguments of predi-cates, bu.t rather that constraints are satisfiedto a certain degree (Resnik, 1993).
Unfortu-nately, defining probabilities in terms of wordsleads to a model with a vast number of param-eters, resulting in a sparse data problem.
Toovercome this, we propose to define a probabil-ity model in terms of senses from a semantic hi-erarchy, exploiting the fact that senses of nounscan be grouped together into semantically sim-ilar classes.We use the semantic hierarchy of noun sensesin WordNet (Fellbamn, 1.998), which consists ofqexicalised concepts' related by the qs-a-kind-of' relation.
If c' is a kind of c, then c is a hy-pcrnym of c', and c' a hyponym of c. Counts arepassed u.p the hierarchy fl'om the senses of nounsappearing in the data.
Thus if cat chicl~cn a.p-pears in the data, th.e count for this item passesu\]) to (meat}, (good}, and all the other hyper-nyms of that sense of chicken.
1 In.
order to es-timate the probability that a sense of chichcn~tppea.rs as the object of the verb cat, we repre-sent (chicken} using a. suitable hypern3qn , suchas (:eood), and base our probability estimate onthat instead.
The level at which (chicken) isrepresented is cruciah it should be high enoughfor adequate counts to have accumulated, butnot too high so that the hypernym is no longerrepresentat ive  of (chicken}.
An exanlp le  of ahypernym whidl would be too high is (e r t t i ty ) ,as not all entities are semantically similar withrespect o the object position ot7 cat.The problem of choosing an appropria.te l velin the h.ierarchy at which to represent a par-ticular noun sense (given a predicate and argu-ment position) has been investigated by Resnik(1993), Li and Abe (1998) and ll,iba,s (1995).The learning mechanism presented \]lore is anovel approach based on tinding semanticallysimilar sets of concepts in a hierarchy.
Wedemonstrate the effectiveness of our approachusing a PP-attachment experiment.2 The Input Data  and Semant icHierarchyThe data used to estimate the probal)ilities isa multiset of 'co-occurrence triples': a nounIWe use italics when referring to words, ~Lnd angledbra.ckets for concepts.
This notation does not alwa.yspick out a concept uniquely, but the context should makecle~n: the concept being referred to.194\]enltna., verb len, ina, and argunien/, i)osition.
2l,et the li;ilivc.rso of verbs~ argll l i lent posi-tions aud ,lOtlBS tha.t call appear in the in-put data.
be denoted "l) = { "Vl~... ~ 'vkv }~ "\]~., :{ , , .
, , .
.
.
,  ,.,,~ } a,d N = { ' ,~, , .
.
.
,  ",J,H }, ~'esi,e,-tiw:ly.
Sucll data c~ui I)e obtMned fro,,, a. tree-ban,s, or from a. shallow pa:rser.
Note tha.t wedo l ie, distinguish I)etwee, i a.lternative seiises ofve,'\])s~ a,lld assUIfle tha.t each \]llsta.llCe O\[ a. no,i l li,I t i le data, refel?s to exactly olle conc, ept .The sei-i-la.ii{ic hiel:a.rchy used is the tie,Ill hy-\[)efltylIl ta.xo:nonly of \/Vo,'dNet (vel'SiOll \]..6).
:3l,e~ (7,' = { e l , .
.
.
,  Chc } be tile sot; of conceptsin WordNet (lq: ,-~ 66,000).
A concel,t is rel)re-sented in \?ordNet 1,y a synset: a sel o1' syilolly-,nous words which cat, I)<7: used to (lenotc lha.1c.oncei)t. I!kil7 exa.nll)iO ~ the COliC;el)l, ~co('.a.iile~a.s iii the (\[rtlg~ iS represented l)y l.he followingsynset: {cocaine, co(-ai't~, coD(;, .,no'w, C}.
l,etsyn(c) C ;V l,e the syllscl; I'or the (:olicel)t c,a.d  let c,,(,,.)
- { c I"" m sy,,(+:) } I,e the.
set ofconcepts that  (;a, ii be denoted by the llO,lli 17..The \]liera.rc\] U has the stl'llCtlll'e O\[ a directedacyciic gi'a.l,\]i : althougli the nunll)er of nodes inthe gra.ph wil;h lllO,'e 1,ha.l/ olle \])al'elll.
is onlya,rOtllid cite i)er(tent o f  the  tota l .
The  edges illthe graph \['orni what we call the dii'ecl.-isa rela--Lion (dire('.t-is;,.
C C' x {').
l,et isa = dii'ect-isa.
Xbe ,lie tI'a, llSitivc~ re\[lexive (;\]OStlre Of (lirect-isa,so t\]iat (c/, c) ~ \]sa :=> c is a \] iy\])ernynl o;\[' (/; a.ndlet ?~ = { c' \[(c' ,c) misa. }
I)e the set consistingof the concept c and all of its hyponynis.
Thus,the set (*ood) conta.ins all the concel)ts wtiich:q,re kinds of food, inclllditig (food).Note tha.t words in the data can al)pear inSyllSCts a, liy\vh01'0 ill the  hiel;archy.
Even  COll-cel)ts Sllch a.s (entity)~ which a,l)pe.ar 1,ear t\]iel:OOt el" the hierarchy, have synsets containingwords which may a.pl)ear hi the da.ta.
Thesynset for (entity) is {c'ntitg, something}, andthe words cntit;q a.nd something (';/,ll apl)ear inthe a.rgulnent positions of verbs in the data.3 Probab i l i ty  Es t imat ionThe problem being a.ddressed in this section isto est imate p(civ, r), for c C C, v < P, and2Only verbs a.re considered here, but this work appliesto other predicates which take a.rgunmnts that can bcorga.nised into a semantic hierarchy.3When wc refer (.o concepts ill \'VoMNct, wc nlea.nconcepts ill WordNet's nomi ta, xonomy.r C 'R..
The I~roba.bility p(eiv , r )  is the 1)rob-ability tha.t some lie\ill in syn(c)~ when dellOfitlg coneet)t c, appears in position 'r of" verl) 'v(given r a.nd v).
Using the relative clause ex-anlpie fl'Otil tile hi,reduction> the p,'obal)ilitiesp((dog}lru,z,subj ) a, nd p((prize)lrv.
',z,sub.i ) ca.nbe toni_pared to decide on the attachluent sitef l  i,i I red awa'rded a p?
'izc Jot iitc dog t/tat ran~l,; f,.~:~;.,.:.
We expe~t 1'((dog) l""', ~m,.i) to 1,egrea.ter than  p((pr?ze) l'l'zt'~z,subj).
Al thougt ,  the\['OCllS iS O11 7,(c1~,,,), the  tochniq l ies  descr ibedhere can be used to estimat, e other lm)babilities,such a.s p(c, f ly  ).
(in fa.ct, the latter prol)alfii-il, y is used hi the Pl>-a?ta.clinient CXl)erhnentsde.qcril)ed in Section 5.
)Using n, axinlun/ likelihood to cstinia.tc\])(C\['V, '/')iS i iot  via.hie 1)eca.use of the l iugc nui l l -t)er of I)al'al,l or.ors i\]i vo\] red.
~/lal ly COl IIt,i II a.tiollsO\[ C, 'l) a.lld 7' will / lot OCCIII" in the data .
'1'o re-duce  the ntii,il)ei' of i)a.ranletcrs which need to\])e estima.ted~ we utilise tile fa.ct tha.t COllCeptsCall be grouped ini;o cla.sses, a.nd \ ] 'epresent  C IlS-ing a class (/, for some hypernynl c' of c. Ilow-ever, p(c'lv , r)ca.nnot be used as a.n estiniate ofv(( l , , ,  ,'), as V((:I"', "') is give.
l,y the foliowi,,g:E .
(  c'l,,, .,.)
= v( ( : ' l .
,  ,)<:"C~The probal)ility ~)(("l'~") i,,c,'eases as c'moves up the hiera.rchy.
For example,s)((eooa)l,;aZ,oi,.i ) is ,,or a good estiu,a,te of1,((chicken)leat,obj ).
What can be donethough, is to condition on sets of concepts, anduse the probabil ity p(v\[c', r).
If it ca:n be showntha.t p(v\[c', r), for some hypernym c' of c, is a.reasoliable esti lnate o1' v(vlc, v), then we have a.wa.y of estiniati,ig p(clv, r).
To get \])(vie; , r ) f roml ,(dv, ,') i~ayes ,',ie is ,,sed:p(4*,,,') p(vl~, "'v(cl'') = 7 ) ~The prol)abilities p(clr ) and p(v\[r) cm~ be esti-mated using maximum likelihood esti,n~tes, a.sthe conditioning event is likely to occur oftenenough for sp;tl'se data not to be a problem.
(Alternatively ()tie could ha.ok-off to p(c) andply) respectively, or use a. linear combhia.tionof p(d' , )a. ,d \],(c), ,.,d P0,1v)a,d V('0, ,'espoc-tively.)
The formula.e for these est imates willI,e give,, shortly.
This only leaves plY\[c, r).
The195proposM is to estilnate P(eatl(~h?cken>, oh j) us-ing  p(eat\](food), oh j), or something similar.
Thefollowing proposition shows that if p(vlc" , r) isIthe same for each c" in c ' ,  where c' is somehypernym of c, then p(v\]c', r) will be equal top(v\[c, r):- -  I\],(~1c",7.)
:/~ for all c" ~ c' ~ \],(vlc',7.
)=The proo f  is as fo l lows:,\],(~17") \],(vl7,7") = \ ] , (71~, , )~- ~'(L 17") ~ p(c"lv,7,)z,(c'l,') - clinG 1_ z,(~l~') V" \],(~ d', 7.~ \]'(c'17)z,(c~lT,) ~ ' ' p(~lT)_ 2 a, ~ p(c"l,0p(c'lT,) _Cl ing I= /~So in order to estimate p(v\[c,r), we need away of searching for a set c', where c' is a hy-pernym of c, which consists of concepts c" whichhave similar p(v\]c", r).
Of conrse we cannot ex-pect to find a set consisting of concepts whichhave identical p(vlc", r), which the propositionstrictly requires, but if the p(vlc" , 7") are simila.r,then we can expect p(vld , r) to be a. reasonableestimate of p(vlc , 7").
We refer to the set c' asthe %imilarity-class' of c, and the suitable hy-pernym, c l, as top(c, v, r).
The next section ex-plains how we determine similarity classes.
Themaxim.urn likelihood estimates for the relevantprobabilities m:e given in Ta.ble 1.44 F ind ing  S imi la r i ty -c lassesFirst we explain how we determine if a set ofconcepts has similar p(vlc", r) for each conceptc" in the set.
Then we explain how we determinetop(c, v, r).4Since we are a.ssuming the data.
is not sense dis-a.mbiguated, f,:eq(c, v, r) cannot be obtained by sim-ply counting senses.
The standard approach, which isadopted here, is to estimate fl'eq(c, v, r) by distributingthe count tor each noun n in syn(c) evenly among allsenses of the noun.
Yarowsky (1992) and \]{esnik (1993)explain how the noise introduced by this technique tendsto dissipate as counts are passed up the hierarchy.Table 1: Maximum Likelihood Esti:lnatesfreq(c, v, r) is the number of (n, v, r) triples inthe data in which n is being used to denote c.fl 'eq(c,r) Ev'EV freq(c,v',r)P(CI? '
)  : " frcq(r)  - -  Ev 'EVEdcc f rcq(c ' ,v ' , r )freq(v,r) Ec'Ec freq(ct,v,r)/ ) (VlT")-  freq(r) : Zv ,EVZc ,  ccfreq(c',v',r)\]}(vie w, 7") -- freq(c-i"v'r) Z~"c~77freq(c't'v'r)rreq(d,,-) = Ev,evE~,,~Tf,-eq(~",'~,',,-)Tile method used for comparing the p(vlc" , r)for c" in some set c', is based on the techniqueill Clark and Weir (1999) used for tinding homo-geneous ets of concepts in the WordNet nounhierarchy.
Rather than directly compare esti-mates ofp(vlc" , r), which are likely to be unreli-able, we consider the children of c', and use esti-mates based on counts which have accumulatedI , /  , /  at the children.
If c' has children Q,%, .
.
.
,  c,,,,,Iwe compare e'(~l<, ") for each i. Th~s is anIa.pproximation, but if the p(vlc}, r) arc similar,Ithen we assume that the p(vlc" ,r)  for c" in c'are similar too.To deterlnine whether the children of some?
.
, ./is the hyperny,~ c' have simila,' \]'('~'14) where c~ith child, we apply a X 2 test to a contingencytM)le of frequency counts.
Table 2 shows somee?a.mple frequencies for c' equM to (nutriment),in the ol)ject position of cat.
The figures inbrackets are the expected values, based on themarginal totMs in the table.
The null hypoth-esis of the test is that p(vl@ r ) i s  the same foreach i. libr TM)1e 2 the null hypothesis is tlmt,I tbr every child, ci, of (nutr?ment}, the probabil-ity p(catlc~, obj) is the same.The log-likelihood X 2 statistic correspondingto TM)le 2 is 4.8.
The log-likelihood X 2 statisticis used rather than the Pearson's X 2 statisticbecause it is thought to be more appropriatewhen the counts in the contingency table arelow (\])unning, 1993).
This tends to occur whenthe test is being applied to a set of conceptsnear the foot of the hierarchy, s We compared5Fisher,s exa.ct test could be used for tables with lowcounts, but we do not do so because tables dolninatedby low counts are likely to have a. high percentage ofnoise, due to the way counts for a noun are split ~unong196ridable 2: Contingency tal)le for children of (nutriment)cimilk)<meal)(course)(d?s~)(del?cacy)f r \ [N(~,  cat, oh.i)o.o (o.
(~)J.a (l.r)s.a (s.r)o.a (~ .s)\] 5.dr,.~,q(~, oh.i)-l't'(x I(~, cal, oh j)9.0 (s..,~)rs.o (so..o)24.r (24.a)s2.a (s~ ..0)2r.4 (~s.9)221.4r,4q(~, oh.i) =E, ,~v  r,.~,q(W, .
,, oh.i)9.086.526.087.627.7the l)erformance of log-likelihood X 2 and Pear-son's X ~2 using the l>P-~tttaehment experhnentdescribed in Section 5.
It was found that thelog-likelihood ~2 test; did perform slightly bet-t('r.
\]"or a signitic~nce l w;I ot' 0.05 (which is thelevel used in the exl)eriments), with 4 degreesof freedom, the critical wdue is 1,1.86 (llowell,;1!197).
Thus in this ca.se, tlle null hyl~othesiswould not be rejected.In order to determine top(c, v, r), we conlparel,(vl~7, v) re,: the children of the hypernyms ofc.
hlitially top(c, 'v, r) ix assigned to I)e the con-eet)t c itself.
Then, l>y worldng Ull the hierarrclly,top((:, 'V, r) is reassigned to I)(' successive hyl)er-nyms of c until the siblings of tol)(C , ~7+ 7')havesiglfifi(:a.ntly different prol)abilities.
In caseswhere a. concept has more than one I)a.J'ent, theparent is chosen which results in tile lowest :\~2wflue as this indicates the p(v\[U,r) are moresimila.r.
The set top(c ,v , r )  is the sinfi\]a.rity-cla.ss of c t'or verb v and position r.Th(; next section provides evidence that tiletechnique for choosing lOl)(C , v, r), which we callthe 'simihu'ity-class' technique, does select anappropriate level of generalisation.5 Exper iments  us ing  PP -a t tachmentambigu i tyThe l>P-atta.chme:nt problem we address con-siders 4-tuples of the form v,:,t,,pr, n2, andthe l)robleln is to decide wllether tile prel)o-sitional phrase pr n2 attaches to the verl> vor the 71oun nl.
For exatnl)le, in the fol-lowing cas(; tim l)rol)lent is to decide whethera l ternat ive  senses.
YVe rely on the  log- l ikel ihood X ,2 testre turn ing  a, non-s ign i f i cant  result  in these cases.J)'om minister attaches to awaii or approvaha.wait apt)7'owd from ministerWe chose the l~P-attachn~ent l)roblenl beca.useP l>-attaehment is a perw,.sive form of ambiguity,and there exist sta.ndard training and text da.ta~which ma.kes for easy comparisons with othera.pproache~s.
This p7'oblenl has been tackled by anu nlber of resea.rehers, lh'ill and Resnik (1994),Ratnal)arkhi et al (\]994), Collins (1995), Za-w:el and l)aelemans (\] 997) all report results be-tween 81% and 85%, with Stetina.
and Nagao(\] 997) tel)erring a result of 88%, which matcheslhe hunm,t+ l>erf'ornlan(;e on this task rel)orted byRatnal>arkhi (% al.
(199.
'1).Althougll th(' l)l)-attachnwnt l)roblem haschara('teristics that n,a.ke it suita.ble for ('valua.-t;ion, it; I)resents a inuch bigger sparse data.
t)\]:ol)-le, m tlla.n would 1)e exl)ected in other l)roblemssuch as relative (:lausc atSadlment.
The reasonfor this is that we need 1;(7 cot,sider how ~l C()l~ -Cel)t is associated with combi~zations of predi-cates and prel)ositions.
T\]le al)proach described11(;7"(; uses prolml)ilities of the Ibrnl p(c, prlv ),u,d ~,,(c.z,,l,,.,), who,;o ,~ ~ ~,l(,+~).
Th is  .
lea, isthat for many predicate/prel)osition combina-tions which occur infl'equently in the d~ta., thereare few examples of n2 which ca.n be used lotpopulating Wo7'dNet in these cases.
Despitethis, we were still able to carry out an ewl.lu-ation by considering subsets of the test (ta.ta forwhich the relewmt predicate~preposition com-I)inations did occur frequently in tit(; trainingd at a,.We deckle on tile a.tta('hnmnt site by compar-197ing p(c~, pr\[v) and p(c,~,, p,'\],q), where= a rg n ax l,(c,p,'lv)c,z 1 = arg max p(c, prlTq )The sense of n2 is chosen which maximisesthe relevant probability in each potential at-tachment case.
If p(c,,,p,jv)is greater than1)(%, :m'l~l), the attachment is made to v, oth-erwise to nl.
If n2 is not in WordNet we com-pare p(prlv ) and p(prl~t~).
Probabilities of theform p(c, prlv ) and p(c, prl~tl ) are used ratherthan p(clv,pr ) and p(cl~l,p,j, because the as-sociation between the preposition and v and ~qcontains useful information.
In fact, for a lotof cases this intbrmation alone can be used todecide on the correct attachment site_ The orig-inal corpus-based method of \]Jindle and ll.ooth(1993) used exactly this information.
Thus themethod described here can be thought of as Hin-dle and Rooth's method with additional class-based information about n2.In order to estimate p(c,, ,pr lv)(andp(C,~l,ln'l,,,,)) we apply the same procedureas described in Section 3, first rewriting theprobability using Bayes' rule:p,,.
)p(c,,, p,.)
p (c , , , j , , l v ) - -  p(vlcv, v(v)p,.)
!
'(P'q c,, ): p(dc,,, l,(v)The probabilities p(c.~) and p(v) can be es-timated using maximum likelihood estimates,a.nd p(vlcv, p,' ) and j,(p,'lc,) can be esti-m.ated using maximum likelihood estimates ofp(vltop(c~ ,v,p,'),pr) and p(prltop(%,pr)) re-spectively.
6We used the training and test data describedin l/.atn.aparkhi et al (1994:), which, was takenDoln the Penn %:eebank and has now becomethe standard data set for this task.
The dataset consists of tuples of the form (v, ~zl, p~', n2),together with the attachment site for each tu-ple.
There is also a development set to preventimplicit training on the test set during develop-ment.
\~e extracted (v, pr, '~2) and (hi, pr, ,z2)~ln Section 4 we only gave the procedure for deter-mining top(c~, v, pr), but top(c~, pr) can be determinedin an analogous fashion.triples from the training set, and in order to in-crease the number of training triples, we alsoextracted triples Kern unambiguous cases of at-tachlnent in the Penn %'eebank.
We prepro-cessed the training and test data by \]emmatisingthe words, replacing numerical amounts withthe words ~definite_quantity', replacing mone-tary amounts with the words 'sum_olLmoney'etc.
We then ignored those triples in the re-sulting training set (but not test set) for which7z2 was not in WordNet, which left a total of66,881 triples of training data..
The test setcontains 3,097 examples.Table 3 gives seine examples of the ex-tent to which the similarity-class techniqueis generalising, using the training data justdescribed, and a significance level of 0.05.The chosen hypernym is shown in Ul)percase.
Note that the WordNet hierarchy con-sists of nine separate sub-hierarchies, headedby such concepts  as (ent i ty>,  (abst rac t ion) ,(psycho log ica l~eature) ,  bnt  we assume the ex-istence of a single root which dominates eachof the sub-hierarchies, which is referred to as(root>.
In cases where WordNet is very sparselypopulated, it is preferable to go to (root),rather than stay at the root of one of the sub-hierarchies where the data may be noisy or toosparse to be o\[' any use.
The table shows thatwith the amount of data ava.ilable from the Tree-bank, the similarity-class technique is selectinga.
level at or close to (root> in many cases.We compared the similarity-class techniquewith fixing the level of generalisation.
Two tixedlevels were used: the root of the entire hieraJ'-chy ((root>), and the set consisting of the rootsof each of the 9 sul>hierarchies.
The procedurewhich always selects (root} ignores any informa-tion about ~z2, and is equivalent o comparingp(prlv ) and p(prl ,h),  which is the ltindle andRooth approach.
The results on the 3,097 testcases are shown in Table 4.
We used a. signifi-cance level a of 0.05 tbr the X 2 test.
rAs the table shows, the disambiguation ac-curacy is below the state of the art.
However,the results are comparable with those of l,i andrSimilar results were obtained using alternative l velsof signifiea.nce.
Rather than simply selecting a value fora, such as 0.05, a' can be tree,ted as a parameter of themodel, whose optimum value caJl be obtained by runningthe disambiguation method on some held-out superviseddata.198'l'al)le 3: Ilow the simila.rity-cla.ss technique chooses top(c, v, pr)a.lld top(c, nq, pr)(?Zl, \])?
', C) I Iypernyms of c( bid,for,(company) )( ~i~io,,,,i,,,,<c~sh> )(v, l", c)('l,ol, i,J!q, O J; <t tans act ion>)( clo.5",8, (t\[,, <def i n i t  e_quant  ity>)( , ,~ ,  wiU,,,<oeeioia~> )< company> <establ ishment> (or ganisat  ion)<social _group> (GROUP) <root>(risk) <venture> (task)(+york> (activity)<act> (ROOT>( cash>( curt ency)(monetary mystem) (asset)(POSSESS I ON)(root )<transact i on)<group_act ion) <act >(ROOT)<D RF II~ I TE_QUAN= TY> <mea~ur e> <abst tact ion> <root><o~i~ial><adjudicator><perso~><li~e~orm><CAUSA~,~G~,NT><e~tity><root>'l'able <1: ( ,o~) le te  test set :~()97 test cases(~eneralisation technique % co:red.SiJnila.rity-cla.ss 80.3Select root of sub-hiera.rchy 77.9Alwa,ys select (root> 79.0Table 5: (root> 1)eing selected for 1)oth a.ttach-nlent 1)oints \] 713 test cases(hmeralisa+tion techniqueSimila.rity-cla.ssSelect root of sub-hierarchyAhva.ys select (root>% tort'cot90.3811.479.6Abe (119!
)8) who a.dol)t a similar a.l>proa(:h us-i:ng \VorclNet, but with a, differ<rot raining andtest set.
I,i a.nd Abe iml>rOVed on the l\[\]n-die and Rooth techni(lue l)y 1.5%, whh;h is i,line with our results.
As a.n evahla.tion of thesimibu'ity-class tec\]lnique, the result is incon-clusive.
The rca.son for this is tha.t when thetechnique wa,s being used to estima.te \])( vlc,,, \])r )a.Hd P(?~.
:I \[c.,zl, I)?
'), in many cases t i le root  o1" 1liehiera.rchy wa.s being chosen as the apl>rOl)riat;elevel of genera.lisa.tion, due to a. sparsely popu-la.ted WordNet in tha.t insta.nce.
Recall that thisis la.rgely due to tit<', fa.ct that we a.rc a.ttemltt-ing to popula.te WordNet fbr comltina.tions ofpredic~tes ~md prepositions.
In such cases tilesinlil~u'ity-elass technique is not helping becausethere is very little or no informa.tion a.1)otlt ~,2.
saln an effort to obtahl more do.to, we a, pplicd the ex-traction heuristic of lla.tna.parkhi (1998) to \?all StreetJourna.l text, which increased the nuntl)er of trainingtriples by ~L factor of 111.
'\['his only a.chievcd comparableresults, however, presumably boca.use the high volumeof noise in the dat~ outweighs the benefit of the increasein da.ta size.
\]{.atnaparkhi reports only 69% a.ccuracy totTable 6: (root> being select(,(I for at most oneo1' ill(, a.tl.a('hnmnt points 1032 i.cst ('asc.~(l(,,eralisaCioll techniqu(,~ % ('ol:re('tSitnilaril.y-cla.ss 88.
ISelect root of sul)-hierar(:hy 85.5Alwa.ys select ( root )  8,5.6In order to eva.lua.te the similarity-class tech-nique further, we took those test cases for whichtile root wa, s not being selected when estima.tingbet:t, J,(,,I,,~.
J,') .+,,d \])('/,.1 I~,,.
pv).
n:\],is .ppliedto 113 c~ses.
The results ~u;e given in Table 5.We a.lso took those test cases for which the rootwas I)eing selected when estimating +~t most oneof p(v\[c+,,pr) a.nd p(,q \[c,~, pr).
This a.pplie, d to\]032 test ca.sos.
The results a.re shown in %>ble 6.the extraction heuristic when applied to the \]%nn Tree-ba.nk (excluding cases where the ln:eposition is of).1996 Conc lus ionsWe have shown that when instances of Word-Net are well populated with examples ofn2, the method described here for solvingP1)-attachment ambiguities is highly accurate.When WordNet is sparsely populated, themethod automatically resorts to comparing justthe preposition and each of the potential attach-ment sites, as the similarity-class technique willselect {root} as the appropriate l vel of general-\]sat\]on for n2 in such cases.
We have also shownthe similarity-class technique to be superior tousing a fixed level of general\]sat\]on in WordNet.Further work will look at how to integrateprobabilities uch as p(clv, r) into a model ofdependency structure, similar to that of Collins(1996) and Collins (1997), which can be used\['or parse selection.
However, knowledge of se-\]ectional preferences cannot by itself solve theproblem of structural disambiguation, and thisfurther work will also look at using additionalknowledge, such a.s subcategorisation informa-tion.Re ferencesEric Brill a.nd Philip Resnik.
1994.
A rule-basedapproach to prel)ositional phrase a.tta.chmentdisanfl)iguation, in 1)~vcccdi'ngs of the .\[iJ-Icc~th International Co~@rcncc on C'ompu-rational Linguistics.Eugene (~harnia.k.
1993. ,5'tali.slical LanguageLcarni'ng.
The MIT Press.Stephen Clark and l)avid Weir.
1999.
An it-erative approach to estimating frequenciesover a semantic hierarchy.
In P'lvcccdinqs ofthe Joint ,57GDA 2' ConJ~rcncc on EmpiricalMethods in Natural Language Proccssi'ng andVery Large Co17~ora , \])ages 258 265.Michael Collins.
11995.
Prepositional phraseattachment through a backed-off model.
InProceedings of the Third l?orhshop on VeryLarge Cou)ora , pages 27-38, Cambridge,Massachusetts.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
InP~vcecdings of the 3/tth Annual Meeting of theA CL, pages 184-1911.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-cccdings of the 351h A~mual Mccti'ng of theAssociation for Computational Linguistics,pages 16-23.Ted \])unning.
1993.
Accurate luethods Ibr thestatistics of surprise and coincidence.
Com-putational Linguistics, 19(1):61-74.Christiane Fellbaum, editor.
1998.
WordNctAn l'2lcctronic Lcxical Database.
The MITPress.Donald ltindle and Mats Rooth.
1993.
Struc-tural ambiguity and lexical relations.
Com-putational Linguistics, \] 9(1): 103-120.l)avid Howell.
11997.
Statistical Methods forPsychology: ~th cd.
Duxbury Press.Hang Li and Naoki Abe.
11998.
Genera.liz-ing case frames using a thesaurus and theMI)L principle.
Computational Linguislics,24(2): 17-244.Adwait l{a.t:na.parldli, Jeff Reynar, and SalilnRoukos.
1994.
A maximum entropy modelfbr prepositional phrase attachment.
In P,v-cccdings of l, hc A RI)A Human Language "l~ch-nology Workshop, pages 250-255.Adwait \]/.atnaparkhi.
1998.
Unsupervised sta-tistical models tbr prepositional phrase at-tachment.
In P~vcccdings of thc ,5'cvcnlccnthhzicrnalionol ConJE'rencc on ComputationalLinguistics, Montreal, Canada, Aug.Pllilip Rcsnik.
71993. ,5'clcction a~zd hdbrma-lion: A Class-Based Approach to l, czical Re-lationships.
Ph.l).
thesis, University of Penn-sylvania.Francesc l{ibas.
1995.
On learning more appro-priate selectional restrictions.
In Procccdingsof the ,5'cvcnth ConJ~rcncc of the IJuropcanChapter of the Association for ComputationalI,i~tguistics, l) U blin, Irelal, d.;liri Stetina and Makoto Na.gao.
1997.
Corpusbased PP attachment ambiguity resolutionwith a semantic dictionary.
In Proceedings ofthc Fiflh I?orteshop on Very Large Corpora,pages 66-80, Beijing and ltong Kong.David Yarowsky.
11992.
Word-sense disam-biguation using statistical models of Roger'scategories trained on large corpora.
\]n P,v-cccdin.qs of COLING-92, pages 454-460.Jakub Zaw:el and Walter l)aelemans.
1997.Melnory-based learning: Using similarity forsmoothing.
In Proceedings of A CL/EACL-97, Madrid, Spain.200
