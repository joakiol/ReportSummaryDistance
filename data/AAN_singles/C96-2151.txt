Handling Sparse Data by Successive AbstractionChrister SamuelssonUniversit?t des Saarlandes, FR 8.7, ComputcrlinguistikPostfach 1150, D-66041 Saarbrfickcn, GermanyInternet: christer?coli.uni-sb, deAbstractA general, practical method for hand-ling sparse data that avoids held-outdata and iterative reestimation is derivedfrom first principles.
It has been testedon a part-of-speech tagging task and out-performed (deleted) interpolation withcontext-independent weights, even whenthe latter used a globally optimal para-meter setting determined a posteriori.1 IntroductionSparse data is a perennial problem when applyingstatistical techniques to natural anguage proces-sing.
The fundamental problem is that there is of-ten not enough data to estimate the required sta-tistical parameters, i.e., the probabilities, directlyfrom the relative frequencies.
This problem is ac-centuated by the fact that in the search for moreaccurate probabilistic language models, more andmore contextual information is added, resulting inmore and more complex conditionings of the cor-responding conditional probabilities.
This in turnmeans that the number of observations tends tobe quite small for such contexts.
Over the years,a number of techniques have been proposed tohandle this problem.One of two different main ideas behind thesetechniques i  that complex contexts can be gene-ralized, and data from more general contexts canbe used to improve the probability estimates formore specific contexts.
This idea is usually re-ferred to as back-off smoothing, see (Katz 1987).These techniques typically require that a sepa-rate portion of the training data be held out fromthe parameter-estimation phase and saved for de-termining appropriate back-off weights.
Further~more, determining the back-off weights usually re-quires resorting to a time-consuming iterative ree-stimation procedure.
A typical example of such atechnique is "deleted interpolation", which is de-scribed in' Section 5.1 below.The other main idea is concerned with im-proving the estimates of low-frequency, or no-frequency, outcomes apparently without trying togeneralize the conditionings.
Instead, these tech-niques are based on considerations of how popu-lation frequencies in general tend to behave.
Ex-amples of this are expected likelihood estimation(ELE), see Section 5.2 below, and Good-Turingestimation, see (Good 1953).We will here derive from first principles apracti-cal method for handling sparse data that does notneed separate training data for determining theback-off weights and which lends itself to directcalculation, thus avoiding time-consuming reesti-mation procedures.2 Linear Successive Abstract ionAssume that we want to estimate the conditionalprobability P(x I C) of tile outcome x given acontext C from the number of times N~ it occursin N = ICI trials, but that this data is sparse.Assume further that there is abundant data in amore general context C t D C that we want to useto get a better estimate of P(x I C).
The idea is tolet the probability estimate/5(x I C) in context Cbe a flmction g of the relative frequency f(x I C)of the outcome x in context C and the probabilityestimate P(x \[C')  ill context C':IV)  = g(f(  I IV '))Let us generalize this scenario slightly to the si-tuation were wc have a sequence of increasinglymore general contexts Cm C Urn-1 C ... C C1,i.e., where there is a linear order of the variouscontexts Ck.
We can then build the estimate ofP(x I Ck) on the relative frequency f(x I Ck)in context Ck and the previously established esti-mate of P(x I Ck-1).
Wc call this method li-near successive abstraction.
A simple example isestimating the probability P(x I/n-j+l?..., In) ofword class x given l,-j+l,...,ln, tile last j let-ters of a word l l , .
.
.
, l , .
In this case, the esti-mate will be based on the relative frequenciesf(x I l,,_~+,,..., l,,),..., f(x \[ In), f(x).We will here consider the special case when theflmction g is a weighted sum of the relative fre-quency and the previous estimate, appropriately895renormalized:f(x I + 0 P(x I P(x I Ck) = 1+0We want the weight 0 to depend on the contextCk, and in particular be proportional to somemeasure of how spread out the relative frequen-cies of the various outcomes in context Ck are fromthe statistical mean.
The variance is the quadraticmoment w.r.t, the mean, and is thus such a mea-sure.
However, we want the weight to have thesame dimension as the statistical mean, and thedimension of the variance is obviously the squareof the dimension of the mean.
The square rootof the variance, which is the standard deviation,should thus be a suitable quantity.
For this rea-son we will use the standard deviation in Ck asa weight, i.e., 0 = ~r(Ck).
One could of coursemultiply this quantity with any reasonable realconstant, but we will arbitrarily set this constantto one, i.e., use ~r(Ck) itself.In linguistic applications, the outcomes areusually not real numbers, but pieces of lingui-stic structure such as words, part-of-speech tags,grammar ules, bits of semantic tissue, etc.
Thismeans that it is not quite obvious what the stan-dard deviation, or the statistical mean for thatmatter, actually should be.
To put it a bit moreabstractly, we need to calculate the standard de-viation of a non-numerical random variable.2.1 Der iv ing  the  S tandard  Dev ia t ionSo how do we find the standard deviation of anon-numerical random variable?
One way is toconstruct an equivalent numerical random varia-ble and use the standard deviation of the latter.This can be done in several different ways.
Theone we will use is to construct a numerical randomvariable with a uniform distribution that has thesame entropy as the non-numerical one.
Whetherwe use a discrete or continuous random variableis, as we shall see, of no importance.We will first factor out the dependence on thecontext size.
Quite in general, if ~N is the samplemean of N independent observations of any nu-merical random variable ( with variance a0 2, i.e.,-} N = ~-,i=1 (i, then~2 = Var\[~N\] =1N 1 N ~---- Var\[ '~-~(i\] = ~ ~Var \ [ ( i \ ] - - - -i=1  i=1In our case, the number of observations N is sim-ply the size of the context Ck, by which we meanthe number of times Ck occurred in the trainingdata, i.e., the frequency count of Ck, which we willdenote \]Ck\[.
Since the standard deviation is thesquare root of the variance, we haveo-(cn = Vic,,IHere ~r0 does not depend on the number of obser-vations in'cofftext Ck, only on the underlying pro-bability distribution conditional on context Ck.To estimate cr0(Ck), we assume that we have eit-her a discrete uniform distribution on {1, .
.
.
,  M}or a continuous uniform distribution on \[0, M\]that is as hard to predict as the one in Ck in thesense that the entropy is the same.
The entropyH\[~\] of a random variable ~ is the expectation va-lue of the logarithm of P(() .
In the discrete casewe thus haveH\[(\] = E \ [ - lnP( ( ) \ ]  : ~-~-P(xi) lnP(xi)itIere P(xi)  is the probability of the random va-riable ( taking the value xi, which is ~ for allpossible outcomes xi and zero otherwise.
Thus,the entropy is In M:ME-P(x i )  lnP(xi) = E-~- lnM-- - - - -  = lnMi i=1The continuous case is similar.
We thus have thatln M = H\[Ck\] or M = e IIICk\]The variance of these uniform distributions isM 2 1--T in the continuous case and ~ in the dis-crete case.
We thus haveM 1 1cr?
(Ck) = X /~ x / r~M -1 - xff2e -H\[cklUnfortunately, the entropy It\[Ck\] depends on theprobability distribution of context Ck and thuson Cro(Ck).
Since we want to avoid trying to solvehighly nonlinear equations, and since we have ac-cess to an estimate of the probability distributionof context Ck-1, we will make the following ap-proximation:O'0(Ck-1)  1~(Ck) ~It is starting to look sensible to specify ~r- 1 insteadof ~, i.e., instead of ~ we will write l q -o"  ' c~-I q-1 "2.2 The  F ina l  Recur rence  FormulaWe have thus established a recurrence formula forthe estimate of the probability distribution in con-text Ck given the estimate of the probability dis-tribution in context Ck-1 and the relative frequen-cies in context Ck:P (x ICk)  = (1),r(Ck)-ly(x C~) + p(x I C~- l )~(Ck) -~ + 1and(cn =We will start by estimating the probability distri-bution in the most general context C1, if necessary896directly from the relative frequencies.
Since thisis the most general context, this will be the con-text with the most training data.
Thus it standsthe best chances of the relative frequencies beingacceptably accurate stimates.
This will allow usto calculate an estimate of the probability distri-bution in context C2, which in turn will allow nsto calculate an estimate of the probability distri-bution in context Ca, etc.
We can thus calcu-late estimates of the probability distributions inall contexts C1, .
.
.
,  Cm.We will next consider some examples from part-of-speech tagging.3 Examples  f rom PoS  Tagg ingPart-of-speech (PoS) tagging consists in assigningto each word of an input text a (set of) tag(s) froma finite set of possible tags, a tag palette or a tagset.
The reason that this is a research issue is thata word can in general be assigned different tagsdepending on context.
In statistical tagging, therelevant information is extracted from a trainingtext and fitted into a statistical language model,which is then used to assign the most likely tag toeach word in the input text.The statistical language model usually consistsof lexical probabilities, which determine the pro-bability of a particular tag conditional on the par-ticular word, and contextual probabilities, whichdetermine the probability of a particular tag con-ditional on the surrounding tags.
The latter con-ditioning is usually on the tags of the neighbouringwords, and very often on the N - 1 previous tags,so-called (tag) N-gram statistics.
These proba-bilities can bc estimated either from a pretaggedtraining corpus or from untagged text, a lexiconand an initial bias.
We will here consider the for-mer case.Statistical taggers usually work as follows:First, each word in the input word string1471, ?
.., W, is assigned all possible tags accordingto the lexicon, thereby creating a lattice.
A dyna-mic programming technique is then used to findtag the sequence 5/\] , .
.
.
,  ~, that maximizesP(T1,...,Tn I Wl, .
.., Wn) =t t= I IP (Tk  T1,.. .
,Tk-1;Wl,.
.
.
,Wn)k=l1:=1P(Tk  Tk -N+I , .
.
.
,Tk -1 ;  VIZk)?
7' ".P(T~ wk) P(Tk ~k-N+l , .
.
.
,  k - l )  \[k=lf l  P(TkP(T,)Tk-N+~,..., ~-~)" P(Wk I Tk)~:~ P(Wk)Since the maximum does not depend on the fac-tors P(Wk), these can be omitted, yielding thestandard statistical PoS tagging task:max \]-\[ P(Tk IU~-~V+~,...,Tk-J.P(Wk JT~)TI ,...,T~, t~l=This is well-described in for example (DeRose1988).We thus have to estimate the two following setsof probabilities:?
Lexical  probabi l i t ies :The probability of each tag T i conditional onthe word W that is to be tagged, p(r' I I wr!
iOften the converse probabilities P(Ware given instead, but we will for reasons oouto become apparent use the former formula-tion.?
Tag N-grams:The probability of tag T i at position k inthe input string, denoted T~, given that tags7~.-N+1 T , .
.
.
,  k-1 have been assigned to theprevious N-  1 words?
Often N is set to two orthree, and thus bigralns or trigrams are em-ployed.
When using trigram statistics, thisquantity is P(T~ \]7' k-~,Tk-1).3.1 N-gram Back-of f  Smooth ingWe will first consider estimating the N-gram pro-babilities P(T~ \]Tk-N+I,...,Tk-1).
IIere, thereis an obvious sequence of generalizations of thecontext 5/~-N+1,...,  7~-1 with a linear order, na-mely ~/~--N+I ~ C Tk-N+2,  ,Tk -1  C , .
.
.
,  k -1 .
.
.?
cT  ?
.
k-1 C fl, where f~ means "no information",corresponding to the nnigram probabilities.
Tiroswe will repeatedly strip off the tag furthest fromthe current word and use the estimate of the pro-bability distribution in this generalized context oimprove the estimate in the current context.
Thismeans that when estimating the (j + 1)-gram pro-babilities, we back off to the estimate of the j-gram probabilities.7' So when estimating P(T\[ I Tk-j , .
.
.
,  ~-~), wesimply strip off the tag 5~_j and apply Eq.
(1):~-~(~'~ \[Tk - j , .
.
.
, Tk - i )  =--1 ,i r, f(% I :tk-j,..., rk-~) = ++ P(Tand~-1+1~-~+1,..., Tk-O~-1 + 1?
., 7}~_ 1\[ e-ll\[Tk_j+l ..... T~._ 113.2 Hand l ing  Unknown WordsWe will next consider improving the probabilityestimates for unknown words, i.e., words that donot occur in tile training corpus, and for which wetherefore have no lexical probabilities, The sametechnique could actually be used for improving theestimates of the lexical probabilities of words that897do occur in the training corpus.
The basic idea isthaF there is a substantial amount of informationin the word suffixes, especially for languages witha richer morphological structure than English.
Forthis reason, we will estimate the probability distri-bution conditional on an unknown word from thestatistical data available for words that end withthe same sequence of letters.
Assume that theword consists of the letters I1,.
?
?, I,~.
We want toknow the probabilities P(T i I l l , .
.
.
, ln) for thevarious tags Ti.
1 Since the word is unknown, thisdata is not available.
However, if we look at thesequence of generalizations of "ending with samelast j letters", here denoted ln- j+l,  ?
?., In, we rea-lize that sooner or later, there will be observationsavailable, in the worst case looking at the last zeroletters, i.e., at the unigram probabilities.So when estimating P(T i I In-j+l,..., ln), wesimply omit the j th  last letter In-j+l and applyEq.
(1):P(T I == er-lf( Ti I l , - j+l , .
.
.
, ln)  +~-1+1P( Ti I In-i+2,..., 1,,) +a- l+ lande r -1  = ln- j+l , .
.
.
, l ,  l e -tl\[1"-j+= ..... MThis data can be collected from the words in thetraining corpus with frequencies below some thres-hold, e.g., words that occur less than say tentimes, and can be indexed in a tree on reversedsuffixes for quick access.4 Par t ia l  Success ive  Abst ract ionIf there is only a partial order of the various gene-ralizations, the scheme is still viable.
For example,consider generalizing symmetric trigram statistics,i.e., statistics of the form P(T I Tz, Tr).
Here, bothTt, the tag of the word to the left, and Tr, the tagof the word to the right, are one-step generaliza-tions of the context 7}, Tr, and both have in turnthe common generalization ~ ("no information").We modify Eq.
(1) accordingly:D(T I Tt,T~ ) = cr(Tt,T~) -~ f (T  I Tz,T~ )o'(r/,Tr) -1 q- 1 -~I P(TIT0 + P(TIT~ ) +-2 ~(T,,T~)-~ + iandP(T I T~)P(TIT,)a(Ti) -1 f (T  I T~) + P(T)a(Tz) -1 + 1~r(Tr) -1 f (T  ITs) + P(T)~(T~) -I + 11Or really, P(T i I 1o, 11,..., ln) where lo is a specialsymbol indicating the beginning o1 the word.We call this partial successive abstraction.
Sincewe really want to estimate cr in the more specificcontext, and since the standard deviation (withthe dependence on context size factored out) willmost likely not increase when we specialize thecontext, we will use:1~r(Tt, Tr) = ~ min(a0(T~), c~0(Tr))In the general case, where we have M one-stepgeneralizations C~ of C, we arrive at the equationP(x I c )  =-1/(x I c) + EY=I I el)and+ 11min a0(C~) o ' (C)  --  \ [X / /~  16{i ..... M}-1 = v, iT -ntcaBy calculating the estimates of the probabilitydistributions in such an order that whenever esti-mating the probability distribution in some parti-cular context, the probability distributions in allmore general contexts have already been estima-ted, we can guarantee that all quantities necessaryfor the calculations are available.5 Re la t ionsh ip  to  Other  MethodsWe will next compare the proposed method to,in turn, deleted interpolation, expected likelihoodestimation and Katz's back-off scheme.5.1 De le ted  In terpo la t ionInterpolation requires that the training corpus isdivided into one part used to estimate the relativefrequencies, and a separate held-back part usedto cope with sparse data through back-off smoo-thing.
For example, tag trigram probabilities carlbe estimated as follows:P(Tj \[Tk-2, Tk-1) ~ Al f (7~) ++ Auf(Tik ITk_1) + Aaf(Tik I Tk-2,T~_I)Since the probability estimate is a linear combina-tion of the various observed relative frequencies,this is called linear interpolation.
The weights 13.may depend on the conditionings, but are requiredto be nonnegative and to sum to one over j. Anenhancement is to partition the training set inton parts and in turn perform linear interpolationwith each of the n parts held out to determinethe back-off weights and use the remaining n - 1parts for parameter estimation.
The various back-off weights are combined in the process.
This isusually referred to as deleted interpolation.The weights Aj are determined by maximizingthe probability of the held-out part of the trai-ning data, see (Jelinek & Mercer 1980).
A locally898optimal weight setting can be found using Baum-Welch reestimation, see (Baum 1972).
Baum-Welch reestimation is however prohibitively time-consuming for complex contexts if the weights areallowed to depend on the contexts, while succes-sive abstraction is clearly tractable; the latter ef-fectively determines these weights directly fromthe same data as the relative frequcncies.5.2 Expected  L ike l ihood  Es t imat ionExpected likelihood estimation (ELE) consists inassigning an extra half a count to all outcomes.Thus, an outcome that didn't occur in the trai-ning data receives half a count, an outcome thatoccurred once receives three half counts.
This isequivalent o assigning a count of one to the oc-curring, and one third to the non-occurring out-comes.
To give an indication of how successiveabstraction is related to ELE, consider the fol-lowing special case: If we indeed have a uniformdistribution with M outcomes of probability M !
incontext Ck-1 and there is but one observation ofone single outcome in context Ck, then Eq.
(1) willassign to this outcome the probability vqh+l and vqh+Mto the other, non-occurring, outcomes 1 So v~+m'if we had used 2 instead of vq2  in Eq.
(1), thiswould have been equivalent o assigning a countof one to the outcome that occurred, anti a countof one third to the ones that didn't.
As it is, thelatter outcomes are assigned a count of 1 4i5+~"5.3 Katz ' s  Back -Of f  SchemeThe proposed method is identical to Katz's back-off method (Katz 1987) up to the point of sugge-sting a, in the general case non-linear, retreat tomore general contexts:P(= I c) = g(f(= I I t ' ) )Blending the involved distributions f(x \] C) and/5( x I C'),  rather than only backing oft" to C'  iff(x \] C) is zero, and in particular, instantiatingthe flmction g(f, P) to a weighted sum, distinguis-hes the two approaches.6 Exper imentsA standard statistical trigram tagger has been im-plemented that uses linear successive abstractionfor smoothing the trigram and bigram probabili-ties, as described in Section 3.1, and that handlesunknown words using a reversed suffix tree, as de-scribed in Section 3.2, again using linear succes-sive abstraction to improve the probability esti-mates.
This tagger was tested on the SusanneCorpus, (Sampson 1995), using a reduced tag setof 62 tags.
The size of the training corpus A wasalmost 130,000 words.
There were three separatetest corpora B, C and D consisting of approxima-tely 10,000 words each.Test corpusTaggerError rate (%)- tag omissions-unknown wordsUnknown wordsError rate (%)Bbigram trigram HMM4.41 4,36 4.490.671.36 1.20 1.526.1822.1 19.4 24,5Test corpus Cbigram HMM trigram TaggerError rate (%)- tag omissions- unknown wordsUnknown wordsError rate (%)4.26 3.93 4.030.681.43  1 .30  1.347.7818.3 16.8 17.3Test corpus Dbigram HMM TaggerError rate (%)- tag omissions- unknown wordsUnknown wordsError rate (%)trigram5.14 4.81 5.130.941.80 1.63 2.028.0622.3 20.2 25.0Figure 1: Results on the Susanne CorpusTile performance of the tagger was comparedwith that of an tlMM-based trigram tagger thatuses linear interpolation for N-gram smoothing,but where the back-off weights do not depend onthe eonditionings.
An optimal weight, setting wasdetermined for each test corpus individually, andused in the experiments.
Incidentally, this settingvaried considerably from corpus to corpus.
Thus,this represented the best possible setting of back-off weights obtainable by linear interpolation, andin particular by linear deleted interpolation, whenthese are not allowed to depend on the context.In contrast, the successive abstraction schemedetermined the back-off weights automaticallyfrom the training corpus alone, and the sameweight setting was nsed for all test corpora, yiel-ding results that were at least on par with thoseobtained using linear interpolation with a globallyoptimal setting of contcxt-independent back-offweights determined a posteriori.
Both taggershandled unknown words by inspecting the suffi-xes, but the HMM-based tagger did not smooththe probability distributions.The experimental results are shown in Figure 1.Note that the absolute performance of the trigramtagger is around 96 % accuracy in two cases anddistinctly above 95 % accuracy in all cases, whichis clearly state-of-the-art results.
Since each testcorpus consisted of about 10,000 words, and theerror rates are between 4 and 5 %, the 5 percentsignificance level for differences in error rate is bet-ween 0.39 and 0.43 % depending on the error rate,and the 10 percent significance level is between0.32 and 0,36 %.899We see that the trigram tagger is better thanthe bigram tagger in all three cases and signifi-cantly better at significance l vel 10 percent, butnot at 5 percent, in case C. So at this signifi-cance level, we can conclude that smoothed tri-gram statistics improve on bigram statistics alone.The trigram tagger performed better than theHMM-based one in all three cases, but not sig-nificantly better at any significance level below10 percent.
This indicates that the successiveabstraction scheme yields back-off weights thatare at least as good as the best ones obtainablethrough linear deleted interpolation with context-independent back-off weights.7 Summary  and  Fur ther  D i rec t ionsIn this paper, we derived a general, practical me-thod for handling sparse data from first principlesthat avoids held-out data and iterative reestima-tion.
It was tested on a part-of-speech taggingtask and outperformed linear interpolation withcontext-independent weights, even when the lat-ter used a globally optimal parameter setting de-termined a posteriori.Informal experiments indicate that it is possibleto achieve slightly better performance by replacingthe expression for ~ro~(Ck) with a fixed global con-1 stant (while retaining the factor I~k l '  which ismost likely a quite accurate model of the depen-dence on context size).
However, the optimal va-lue for this parameter varied more than an orderof magnitude, and the improvements in perfor-mance were not very large.
Furthermore, subop-timal choices of this parameter tended to degradeperformance, rather than improve it.
This indi-cates that the proposed formula is doing a prettygood job of approximating an optimal parameterchoice.
It would nonetheless be interesting to seeif the formula could be improved on, especiallyseeing that it was theoretically derived, and thendirectly applied to the tagging task, immediatelyyielding the quoted results.AcknowledgementsThe work presented in this article was funded bythe N3 "Bidirektionale Linguistische Deduktion(BiLD)" project in the Sonderforschungsbereich314 Kiinstliche Intelligeuz -- Wissensbasierte Sy-steme.I wish to thank greatly Thorsten Brants, SlavaKatz, Khalil Sima'an, the audiences of seminarsat the University of Pennsylvania and the Uni-versity of Sussex, in particular Mark Liberman,and the anonymous reviewers of Coling and ACLfor pointing out inaccuracies and supplying usefulcomments and suggestions to improvements.ReferencesL.
E. Baum.
1972.
"An inequality and as-sociated maximization technique in statisticalestimation for probabilistic functions of Markovprocesses".
Inequalities I Ih 1-8.Steven J. DeRose.
1988.
"Grammatical Cate-gory Disambiguation by Statistical Optimiza-tion".
Computational Linguistics, 14(1): 31-39.I.
J.
Good.
1953.
"The population frequencies ofspecies and the estimation of population para-meters".
Biometrika, 40: 237-264.Frederick Jelinek and Robert L. Mercer.
1980.
"Interpolated Estimation of Markov Source Pa-ramenters from Sparse Data".
Pattern Recogni-tion in Practice: 381-397.
North Holland.Slava M. Katz.
1987.
"Estimation of Probabi-lities from Sparse Data for the Language Mo-del Component of a Speech Recognizer".
IEEETransactions on Acoustics, Speech, and SignalProcessing, 35(3): 400-401.Geoffrey Sampson.
1995.
English for the Compu-ter.
Oxford University Press.900
