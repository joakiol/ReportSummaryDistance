Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 231?239,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsScaling up WSD with Automatically Generated ExamplesWeiwei Cheng, Judita Preiss and Mark StevensonDepartment of Computer Science,Sheffield University,Regent Court, 211 Portobello,Sheffield, S1 4DPUnited Kingdom{W.Cheng, J.Preiss, M.Stevenson}@dcs.shef.ac.ukAbstractThe most accurate approaches to Word SenseDisambiguation (WSD) for biomedical docu-ments are based on supervised learning.
How-ever, these require manually labeled trainingexamples which are expensive to create andconsequently supervised WSD systems arenormally limited to disambiguating a small setof ambiguous terms.
An alternative approachis to create labeled training examples automat-ically and use them as a substitute for manu-ally labeled ones.
This paper describes a largescale WSD system based on automatically la-beled examples generated using informationfrom the UMLS Metathesaurus.
The labeledexamples are generated without any use of la-beled training data whatsoever and is thereforecompletely unsupervised (unlike some previ-ous approaches).
The system is evaluated ontwo widely used data sets and found to outper-form a state-of-the-art unsupervised approachwhich also uses information from the UMLSMetathesaurus.1 IntroductionThe information contained in the biomedical liter-ature that is available in electronic formats is use-ful for health professionals and researchers (West-brook et al, 2005).
The amount is so vast thatit is difficult for researchers to identify informa-tion of interest without the assistance of automatedtools (Krallinger and Valencia, 2005).
However,processing these documents automatically is madedifficult by the fact that they contain terms thatare ambiguous.
For example, ?culture?
can mean?laboratory procedure?
(e.g.
?In peripheral bloodmononuclear cell culture?)
or ?anthropological cul-ture?
(e.g.
?main accomplishments of introducing aquality management culture?).
These lexical ambi-guities are problematic for language understandingsystems.Word sense disambiguation (WSD) is the processof automatically identifying the meanings of am-biguous terms.
Some WSD systems for the biomed-ical domain are only able to disambiguate a smallnumber of ambiguous terms (see Section 2).
How-ever, for WSD systems to be useful in applicationsthey should be able to disambiguate all ambiguousterms.
One way to create such a WSD system is toautomatically create the labeled data that is used totrain supervised WSD systems.
Several approaches(Liu et al, 2002; Stevenson and Guo, 2010; Jimeno-Yepes and Aronson, 2010) have used informationfrom the UMLS Metathesaurus1 to create labeledtraining data that have successfully been used to cre-ate WSD systems.A key decision for any system that automaticallygenerates labeled examples is the number of exam-ples of each sense to create, known as the bias of thedata set.
It has been shown that the bias of a set of la-beled examples affects the performance of the WSDsystem it is used to train (Mooney, 1996; Agirre andMart?
?nez, 2004b).
Some of the previous approachesto generating labeled data relied on manually anno-tated examples to determine the bias of the data setsand were therefore not completely unsupervised.This paper describes the development of a largescale WSD system that is able to disambiguate all1http://www.nlm.nih.gov/research/umls/231terms that are ambiguous in the UMLS Metathe-saurus.
The system relies on labeled examples thatare created using information from UMLS.
Variousbias options are explored, including ones that do notmake use of information from manually labeled ex-amples, and thus we can create a completely unsu-pervised system.
Evaluation is carried out on twostandard datasets (the NLM-WSD and MSH-WSDcorpora).
We find that WSD systems can be cre-ated without using any information from manuallylabeled examples and that their performance is bet-ter than a state-of-the-art unsupervised approach.The remainder of this paper is organized as fol-lows.
Previous approaches to WSD in biomedicaldocuments are described in the next Section.
Section3 presents the methods used to identify bias in thelabeled examples and WSD system.
Experiments inwhich these approaches are compared are describedin Section 4 and their results in Section 5.2 BackgroundMany WSD systems for the biomedical domain arebased on supervised learning (McInnes et al, 2007;Xu et al, 2007; Stevenson et al, 2008; Yepes andAronson, 2011).
These systems require labeledtraining data, examples of an ambiguous term la-beled with the correct meaning.
Some sets of labeleddata have been developed for the biomedical domain(Weeber et al, 2001; Savova et al, 2008; Jimeno-Yepes et al, 2011).
However, these data sets onlycontain examples for a few hundred terms and canonly be used to develop WSD systems to identifythe meanings of those terms.
The process of creat-ing labeled examples is extremely time-consumingand difficult (Artstein and Poesio, 2008), making itimpractical to create labeled examples of all possibleambiguous terms found in biomedical documents.Two alternative approaches have been explored todevelop systems which are able to disambiguate allambiguous terms in biomedical documents.
The firstmakes use of unsupervised WSD algorithms (seeSection 2.1) and the second creates labeled data au-tomatically and uses it to train a supervised WSDsystem (see Section 2.2).2.1 Unsupervised WSDUnsupervised WSD algorithms make use of infor-mation from some knowledge source, rather than re-lying on training data.Humphrey et al (2006) describe an unsupervisedsystem which uses semantic types in UMLS to dis-tinguish between the possible meanings of ambigu-ous words.
However, it cannot disambiguate be-tween senses with the same semantic type, i.e., itis not possible for the system to recognise all sensedistinctions.The Personalised Page Rank (PPR) system(Agirre et al, 2010; Jimeno-Yepes and Aronson,2010) relies on a a graph-based algorithm similarto the Page Rank algorithm originally developed foruse in search engines (Brin, 1998).
It performsWSD by converting the UMLS Metathesaurus intoa graph in which the possible meanings of ambigu-ous words are nodes and relations between them areedges.
Disambiguation is carried out by providingthe algorithm with a list of senses that appear in thetext that is being disambiguated.
This information isthen combined with the graph and a ranked list of thepossible senses for each ambiguous word generated.Unsupervised systems have the advantage of be-ing able to disambiguate all ambiguous terms.
How-ever, the performance of unsupervised systems thathave been developed for biomedical documents islower than that of supervised ones.2.2 Automatic Generation of Labeled DataAutomatic generation of labeled data for WSD com-bines the accuracy of supervised approaches withthe ability of unsupervised approaches to disam-biguate all ambiguous terms.
It was first suggestedby Leacock et al (1998).
Their approach is basedon the observation that some terms in a lexicon oc-cur only once and, consequently, there is no doubtabout their meaning.
These are referred to as beingmonosemous.
Examples for each possible meaningof an ambiguous term are generated by identifyingthe closest monosemous term (the monosemous rel-ative) in the lexicon and using examples of that term.Variants of the approach have been applied to thebiomedical domain using the UMLS Metathesaurusas the sense inventory.232Liu et al (2002) were the first to apply themonosemous relatives approach to biomedical WSDand use it to disambiguate a set of 35 abbreviations.They reported high precision but low recall, indicat-ing that labeled examples could not be created formany of the abbreviations.
Jimeno-Yepes and Aron-son (2010) applied a similar approach and foundthat it performed better than a number of alternativeapproaches on a standard evaluation resource (theNLM-WSD corpus) but did not perform as well assupervised WSD.
Stevenson and Guo (2010) com-pared two techniques for automatically creating la-beled data, including the monosemous relatives ap-proach.
They found that the examples which weregenerated were as good as manually labeled exam-ples when used to train a supervised WSD system.However, Stevenson and Guo (2010) relied on la-beled data to determine the number of examples ofeach sense to create, and therefore the bias of thedata set.
Consequently their approach is not com-pletely unsupervised since it could not be applied toambiguous terms that do not have labeled trainingdata available.3 Approach3.1 WSD SystemThe WSD system is based on a supervised approachthat has been adapted for the biomedical domain(Stevenson et al, 2008).
The system was tested onthe NLM-WSD corpus (see Section 4.1) and foundto outperform alternative approaches.The system can exploit a wide range of fea-tures, including several types of linguistic informa-tion from the context of an ambiguous term, MeSHcodes and Concept Unique Identifiers (CUIs) fromthe UMLS Metathesaurus.
However, computingthese features for every example is a time consum-ing process and to make the system suitable for largescale WSD it was restricted to using a smaller setof features.
Previous experiments (Stevenson et al,2008) showed that this only leads to a small drop indisambiguation accuracy while significantly reduc-ing the computational cost of generating features.3.1.1 FeaturesTwo types of context words are used as features:the lemmas of all content words in the same sen-tence as the ambiguous word and the lemmas of allcontent words in a?4-word window around the am-biguous term.
A list of corpus-specific stopwordswas created containing terms that appear frequentlyin Medline abstracts but which are not useful for dis-ambiguation (e.g.
?abstract?, ?conclusion?).
Anylemmas found in this list were not used as features.3.1.2 Learning algorithmDisambiguation is carried out using the VectorSpace Model, a memory-based learning algorithmin which each occurrence of an ambiguous word isrepresented as a vector created using the features ex-tracted to represent it (Agirre and Mart?
?nez, 2004a).The Vector Space Model was found to outperformother learning algorithms when evaluated using theNLM-WSD corpus (Stevenson et al, 2008).During the algorithm?s training phase a singlecentroid vector, ~Csj , is generated for each possiblesense, sj .
This is shown in equation 1 where T isthe set of training examples for a particular term andsense(~t) is the sense associated with the vector ~t.~Csj =?~ti  T :sense(~ti)=sj~ti|~ti  T : sense(~ti) = sj |(1)Disambiguation is carried out by comparing thevector representing the ambiguous word, ~a, againstthe centroid of each sense using the cosine metric,shown in equation 2, and choosing the one with thehighest score.score(sj ,~a) = cos( ~Csj ,~a) =~Csj .~a| ~Csj ||~a|(2)Note that the learning algorithm does not ex-plicitly model the prior probability of each possi-ble sense, unlike alternative approaches (e.g.
NaiveBayes), since it was found that including this infor-mation did not improve performance.3.2 Automatically generating trainingexamplesThe approaches used for generating training exam-ples used here are based on the work of Stevensonand Guo (2010), who describe two approaches:1.
Monosemous relatives2.
Co-occurring concepts233Both approaches are provided with a set of ambigu-ous CUIs from the UMLS Metathesaurus, whichrepresent the possible meanings of an ambiguousterm, and a target number of training examples to begenerated for each CUI.
Each CUI is associated withat least one term and each term is labeled with a lex-ical unique identifier (LUI) which represents a rangeof lexical variants for a particular term.
The UMLSMetathesaurus contains a number of data files whichare exploited within these techniques, including:AMBIGLUI: a list of cases where a LUI is linkedto multiple CUIs.MRCON: every string or concept name in theMetathesaurus appears in this file.MRCOC: co-occuring concepts.For the monosemous relatives approach, thestrings of monosemous LUIs of the target CUIand its relatives are used to search Medline to re-trieve training examples.
The monosemous LUIs re-lated to a CUI are defined as any LUIs associatedwith the CUI in the MRCON table and not listed inAMBIGLUI table.The co-occurring concept approach works differ-ently.
Instead of using strings of monosemous LUIsof the target CUI and its relatives, the strings associ-ated with LUIs of a number of co-occurring CUIs ofthe target CUI and its relatives found in MRCOC ta-ble are used.
The process starts by finding the LUIsof the top n co-occurring CUIs of the target CUI.These LUIs are then used to form search queries.The query is quite restrictive in the beginning and re-quires all terms appear in the Medline citations files.Subsequently queries are made less restrictive by re-ducing the number of required terms in the query.These techniques were used to generate labeledexamples for all terms that are ambiguous in the2010 AB version of the UMLS Metathesaurus.2 Theset of all ambiguous terms was created by analysingthe AMBIGLUI table, to identify CUIs that are asso-ciated with multiple LUIs.
The Medline BaselineRepository (MBR)3 was also analysed and it wasfound that some terms were ambiguous in this re-source, in the sense that more than one CUI had been2Stevenson and Guo (2010) applied them to a small set ofexamples from the NLM-WSD data set (see Section 4.1).3http://mbr.nlm.nih.govassigned to an instance of a term, but could not beidentified from the AMBIGLUI table.
The final listof ambiguous CUIs was created by combining thoseidentified from the AMBIGLUI table and those findin the MBR.
This list contained a total of 103,929CUIs.Both techniques require large number of searchesover the Medline database and to carry this out ef-ficiently the MBR was indexed using the LuceneInformation Retrieval system4 and all searches ex-ecuted locally.Examples were generated using both approaches.The monosemous relatives approach generated ex-amples for 98,462 CUIs and the co-occurring con-cepts for 98,540.
(Examples generated using themonosemous relatives approach were preferred forthe experiments reported later.)
However, neithertechnique was able to generate examples for 5,497CUIs, around 5% of the total.
This happened whennone of the terms associated with a CUI returnedany documents when queried against the MBR andthat CUI does not have any monosemous relatives.An example is C1281723 ?Entire nucleus pulpo-sus of intervertebral disc of third lumbar vertebra?.The lengthy terms associated with this CUI do notreturn any documents when used as search termsand, in addition, it is only related to one other CUI(C0223534 ?Structure of nucleus pulposus of inter-vertebral disc of third lumbar vertebra?)
which is it-self only connected to C1281723.
Fortunately thereare relatively few CUIs for which no examples couldbe generated and none of them appear in the MBR,suggesting they refer to UMLS concepts that do nottend to be mentioned in documents.3.3 Generating BiasThree different techniques for deciding the numberof training examples to be generated for each CUI(i.e.
the bias) were explored.Uniform Bias (UB) uses an equal number oftraining examples to generate centroid vectors foreach of the possible senses of the ambiguous term.Gold standard bias (GSB) is similar to the uni-form bias but instead of being the same for all pos-sible CUIs the number of training examples for eachCUI is determined by the number of times it appears4http://lucene.apache.org/234in a manually labeled gold standard corpus.
Assumet is an ambiguous term and Ct is the set of possiblemeanings (CUIs).
The number of training examplesused to generate the centroid for that CUI, Ec, iscomputed according to equation 3 where Gc is thenumber of instances in the gold standard corpus an-notated with CUI c and n is a constant which is setto 100 for these experiments.5Ec =Gc?ci  CtGci,t.n (3)The final technique, Metamap Baseline Repos-itory Bias (MBB), is based on the distribution ofCUIs in the MBR.
The number of training examplesare generated in a similar way to the gold standardbias with MBR being used instead of a manually la-beled corpus and is shown in equation 4 whereMc isthe number of times the CUI c appears in the MBR.Ec =Mc?ci  CtMci.n (4)For example, consider the three possible CUIs as-sociated with term ?adjustment?
in the NLM-WSDcorpus: C0376209, C0456081 and C06832696.The corpus contains 18 examples of C0376209,62 examples of C0456081 and 13 of C0683269.Using equation 3, the number of training exam-ples when GSB is applied for C0376209 is 20,67 for C0456081 and 14 for C0683269.
In theMetamap Baseline Repository files, C0376209 hasa frequency count of 98046, C0456081 a count of292809 and C0683269 a count of 83530.
Thereforethe number of training examples used for the threesenses when applying MBB is: 21 for C0376209, 62for C0456081 and 18 for C0683269.4 Evaluation4.1 Data setsWe evaluate our system on two datasets: the NLM-WSD and MSH-WSD corpora.5Small values for Ec are rounded up to ensure that any rareCUIs have at least one training example.6These CUIs are obtained using the mappings from NLM-WSD senses to CUIs available on the NLM website: http://wsd.nlm.nih.gov/collaboration.shtmlThe NLM-WSD corpus7 (Weeber et al, 2001) hasbeen widely used for experiments on WSD in thebiomedical domain, for example (Joshi et al, 2005;Leroy and Rindflesch, 2005; McInnes et al, 2007;Savova et al, 2008).
It contains 50 ambiguous termsfound in Medline with 100 examples of each.
Theseexamples were manually disambiguated by 11 an-notators.
The guidelines provided to the annotatorsallowed them to label a senses as ?None?
if noneof the concepts in the UMLS Metathesaurus seemedappropriate.
These instances could not be mappedonto UMLS Metathesaurus and were ignored for ourexperiments.The larger MSH-WSD corpus (Jimeno-Yepes etal., 2011) contains 203 strings that are associatedwith more than one possible MeSH code in theUMLS Metathesaurus.
106 of these are ambiguousabbreviations, 88 ambiguous terms and 9 a combi-nation of both.
The corpus contains up to 100 ex-amples for each possible sense and a total of 37,888examples of ambiguous strings taken from Medline.Unlike the NLM-WSD corpus, all of the instancescan be mapped to the UMLS Metathesaurus andnone was removed from the dataset for our exper-iments.The two data sets differ in the way the numberof instances of each sense was determined.
Forthe NLM-WSD corpus manual annotation is used todecide the number of instances that are annotatedwith each sense of an ambiguous term.
However,the NLM-MSH corpus was constructed automati-cally and each ambiguous term has roughly the samenumber of examples of each possible sense.4.2 ExperimentsThe WSD system described in Section 3 was testedusing each of the three techniques for determiningthe bias, i.e.
number of examples generated for eachCUI.
Performance is compared against various alter-native approaches.Two supervised approaches are included.
Thefirst, most frequent sense (MFS) (McCarthy et al,2004), is widely used baseline for supervised WSDsystems.
It consists of assigning each ambiguousterm the meaning that is more frequently observedin the training data.
The second supervised approach7http://wsd.nlm.nih.gov235is to train the WSD system using manually labeledexamples from the NLM-WSD and MSH-WSD cor-pora.
10-fold cross validation is applied to evaluatethis approach.Performance of the Personalised Page Rank ap-proach described in Section 2.1 is also provided toallow comparison with an unsupervised algorithm.Both Personalised Page Rank and the techniqueswe employ to generate labeled data, base disam-biguation decisions on information from the UMLSMetathesaurus.The performance of all approaches is measuredin terms of the percentage of instances which arecorrectly disambiguated for each term with the av-erage across all terms reported.
Confidence inter-vals (95%) computed using bootstrap resampling(Noreen, 1989) are also shown.5 ResultsResults of the experiments are shown in Table 1where the first three rows show performance of theapproach described in Section 3 using the threemethods for computing the bias (UB, MMB andGSB).
MFS and Sup refer to the Most FrequentSense supervised baseline and using manually la-beled examples, respectively, and PPR to the Per-sonalised PageRank approach.When the performance of the approaches us-ing automatically labeled examples (UB, MMB andGSB) is compared it is not surprising that the best re-sults are obtained using the gold standard bias sincethis is obtained from manually labeled data.
Resultsusing this technique for computing bias always out-perform the other two, which are completely unsu-pervised and do not make use of any informationfrom manually labeled data.
However, the improve-ment in performance varies according to the corpus,for the NLM-WSD corpus there is an improvementof over 10% in comparison to UB while the corre-sponding improvement for the MSH-WSD corpus isless than 0.5%.A surprising result is that performance obtainedusing the uniform bias (UB) is consistently betterthan using the bias obtained by analysis of the MBR(MMB).
It would be reasonable to expect that in-formation about the distribution of CUIs in this cor-pus would be helpful for WSD but it turns out thatmaking no assumptions whatsoever about their rel-ative frequency, i.e., assigning a uniform baseline,produces better results.The relative performance of the supervised (MFS,Sup and GSB) and unsupervised approaches (UB,MMB and PPR) varies according to the corpus.
Un-surprisingly using manually labeled data (Sup) out-performs all other approaches on both corpora.
Thesupervised approaches also outperform the unsuper-vised ones on the NLM-WSD corpus.
However, forthe MSH-WSD corpus all of the unsupervised ap-proaches outperform the MFS baseline.A key reason for the differences in these results isthe different distributions of senses in the two cor-pora, as shown by the very different performance ofthe MFS approach on the two corpora.
This is dis-cussed in more detail later (Section 5.2).Comparison of the relative performance of the un-supervised approaches (UB, MMB and PPR) showsthat training a supervised system with the automat-ically labeled examples using a uniform bias (UB)always outperforms PPR.
This demonstrates thatthis approach outperforms a state-of-the-art unsu-pervised algorithm that relies on the same infor-mation used to generate the examples (the UMLSMetathesaurus).5.1 Performance by Ambiguity TypeThe MSH-WSD corpus contains both ambiguousterms and abbreviations (see Section 4.1).
Perfor-mance of the approaches on both types of ambiguityare shown in Table 2.MSH-WSD Ambiguity TypeApproach Abbreviation TermUB 91.40 [91.00, 91.75] 72.68 [72.06, 73.32]MMB 84.43 [83.97, 84.89] 69.45 [68.86, 70.10]GSB 90.82 [90.45, 91.22] 73.96 [73.40, 74.62]MFS 52.43 [51.73, 53.05] 51.76 [51.11, 52.36]Sup.
97.41 [97.19, 97.62] 91.54 [91.18, 91.94]PPR 86.40 [86.00, 86.85] 68.40 [67.80, 69.14]Table 2: WSD evaluation results for abbreviations andterms in the MSH-WSD data set.The relative performance of the different ap-proaches on the terms and abbreviations is similar tothe entire MSH-WSD data set (see Table 1).
In par-236CorpusApproach Type NLM-WSD MSH-WSDUB Unsup.
74.00 [72.80, 75.29] 83.19 [82.87, 83.54]MMB Unsup.
71.18 [69.94, 72.38] 78.09 [77.70, 78.46]GSB Sup.
84.28 [83.12, 85.36] 83.39 [83.08, 83.67]MFS Sup.
84.70 [83.67, 85.81] 52.01 [51.50, 52.45]Sup Sup.
90.69 [89.87, 91.52] 94.83 [94.63, 95.02]PPR Unsup.
68.10 [66.80, 69.23] 78.60 [78.23, 78.90]Table 1: WSD evaluation results on NLM-WSD and MSH-WSD data sets.ticular using automatically generated examples witha uniform bias (UB) outperforms using the bias de-rived from the Medline Baseline Repository (MBR)while using the gold standard baseline (GSB) im-proves results slightly for terms and actually reducesthem for abbreviations.Results for all approaches are higher when disam-biguating abbreviations than terms which is consis-tent with previous studies that have suggested thatin biomedical text abbreviations are easier to disam-biguate than terms.5.2 AnalysisAn explanation of the reason for some of the re-sults can be gained by looking at the distributionsof senses in the various data sets used for the ex-periments.
Kullback-Leibler divergence (or KL di-vergence) (Kullback and Leibler, 1951) is a com-monly used measure for determining the differencebetween two probability distributions.
For each termt, we define S as the set of possible senses of t,the sense probability distributions of t as D and D?.Then the KL divergence between the sense probabil-ity distributions D and D?
can be calculated accord-ing to equation 5.KL(D||D?)
=?s  SD(s).
logD(s)D?
(s)(5)The three techniques for determining the bias de-scribed in Section 3.3 each generate a probabilitydistribution over senses.
Table 2 shows the averageKL divergence when the gold standard distributionobtained from the manually labeled data (GSB) iscompared with the uniform bias (UB) and bias ob-tained by analysing the Medline Baseline Reposi-tory (MMB).CorpusAvg.
KL Divergence NLM-WSD MSH-WSDKL(GSB||MMB) 0.5649 0.4822KL(GSB||UB) 0.4600 0.0406Table 3: Average KL divergence of sense probability dis-tributions in the NLM-WSD and MSH-WSD data sets.The average KL divergence scores in the tableare roughly similar with the exception of the muchlower score obtained for the gold-standard and uni-form bias for the MSH-WSD corpus (0.0406).
Thisis due to the fact that the MSH-WSD corpus wasdesigned to have roughly the same number of ex-amples for each sense, making the sense distribu-tion close to uniform (Jimeno-Yepes et al, 2011).This is evident from the MFS scores for the MSH-WSD corpus which are always close to 50%.
Thisalso provides as explanation of why performance us-ing automatically generated examples on the MSH-WSD corpus only improves by a small amount whenthe gold standard bias is used (see Table 1).
The goldstandard bias simply does not provide much addi-tional information to the WSD system.
The situa-tion is different in the NLM-WSD corpus, where theMFS score is much higher.
In this case the additionalinformation available in the gold standard sense dis-tribution is useful for the WSD system and leads toa large improvement in performance.In addition, this analysis demonstrates why per-formance does not improve when the bias gener-ated from the MBR is used.
The distributions whichare obtained are different from the gold standardand are therefore mislead the WSD system ratherthan providing useful information.
The differencebetween these distributions would be expected for237the MSH-WSD corpus, since it contains roughly thesame number of examples for each possible senseand does not attempt to represent the relative fre-quency of the different senses.
However, it is sur-prising to observe a similar difference for the NLM-WSD corpus, which does not have this constraint.The difference suggests the information about CUIsin the MBR, which is generated automatically, hassome limitations.Table 4 shows a similar analysis for the MSH-WSD corpus when abbreviations and terms are con-sidered separately and supports this analysis.
Thefigures in this table show that the gold standard anduniform distributions are very similar for both ab-breviations and terms, which explains the similar re-sults for UB and GSB in Table 2.
However, the goldstandard distribution is different from the one ob-tained from the MBR.
The drop in performance ofMMB compared with GBS in Table 2 is a conse-quence of this.Ambiguity TypeAvg.
KL Divergence Abbreviation TermKL(GSB||MMB) 0.4554 0.4603KL(GSB||UB) 0.0544 0.0241Table 4: Average KL divergence for abbreviations andterms in the MSH-WSD data set.6 ConclusionThis paper describes the development of a largescale WSD system based on automatically labeledexamples.
We find that these examples can be gener-ated for the majority of CUIs in the UMLS Metathe-saurus.
Evaluation on the NLM-WSD and MSH-WSD data sets demonstrates that the WSD systemoutperforms the PPR approach without making anyuse of labeled data.Three techniques for determining the number ofexamples to use for training are explored.
It isfound that a supervised approach (which makes useof manually labeled data) provides the best results.Surprisingly it was also found that using informationfrom the MBR did not improve performance.
Anal-ysis showed that the sense distributions extractedfrom the MBR were different from those observedin the evaluation data, providing an explanation forthis result.Evaluation showed that accurate informationabout the bias of training examples is useful forWSD systems and future work will explore other un-supervised ways of obtaining this information.
Al-ternative techniques for generating labeled exampleswill also be explored.
In addition, further evaluationof the WSD system will be carried out, such as ap-plying it to an all words task and within applications.AcknowledgementsThis research has been supported by the Engineer-ing and Physical Sciences Research Council and aGoogle Research Award.ReferencesE.
Agirre and D.
Mart??nez.
2004a.
The Basque CountryUniversity system: English and Basque tasks.
In RadaMihalcea and Phil Edmonds, editors, Proceedings ofSenseval-3, pages 44?48, Barcelona, Spain.E.
Agirre and D.
Mart??nez.
2004b.
Unsupervised WSDBased on Automatically Retrieved Examples: TheImportance of Bias.
In Proceedings of EMNLP-04,Barcelona, Spain.E.
Agirre, A. Sora, and M. Stevenson.
2010.
Graph-based word sense disambiguation of biomedical docu-ments.
Bioinformatics, 26(22):2889?2896.R.
Artstein and M. Poesio.
2008.
Inter-Coder Agree-ment for Computational Linguistics.
ComputationalLinguistics, 34(4):555?596.S.
Brin.
1998.
Extracting Patterns and relations from theWord-Wide Web.
In Proceedings of WebDB?98.S.
Humphrey, W. Rogers, H. Kilicoglu, D. Demner-Fushman, and T. Rindflesch.
2006.
Word SenseDisambiguation by Selecting the Best Semantic TypeBased on Journal Descriptor Indexing: Preliminary ex-periment.
Journal of the American Society for Infor-mation Science and Technology, 57(5):96?113.A.
Jimeno-Yepes and A. Aronson.
2010.
Knowledge-based biomedical word sense disambiguation: com-parison of approaches.
BMC Bioinformatics,11(1):569.A.
Jimeno-Yepes, B. McInnes, and A. Aronson.
2011.Exploiting MeSH indexing in MEDLINE to generatea data set for word sense disambiguation.
BMC Bioin-formatics, 12(1):223.M.
Joshi, T. Pedersen, and R. Maclin.
2005.
A Com-parative Study of Support Vector Machines Applied tothe Word Sense Disambiguation Problem for the Med-ical Domain.
In Proceedings of IICAI-05, pages 3449?3468, Pune, India.238M.
Krallinger and A. Valencia.
2005.
Text mining andinformation retrieval services for molecular biology.Genome Biology, 6(7):224.S.
Kullback and R. A. Leibler.
1951.
On Information andSufficiency.
The Annals of Mathematical Statistics,22(1):79?86.C.
Leacock, M. Chodorow, and G. Miller.
1998.
Us-ing Corpus Statistics and WordNet Relations for SenseIdentification.
Computational Linguistics, 24(1):147?165.G.
Leroy and T. Rindflesch.
2005.
Effects of Informationand Machine Learning algorithms on Word Sense Dis-ambiguation with Small Datasets.
International Jour-nal of Medical Informatics, 74(7-8):573?585.H.
Liu, S. Johnson, and C. Friedman.
2002.
Au-tomatic Resolution of Ambiguous Terms Based onMachine Learning and Conceptual Relations in theUMLS.
Journal of the American Medical InformaticsAssociation, 9(6):621?636.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Finding Predominant Word Senses in UntaggedText.
In Proceedings of ACL-2004, pages 280?287,Barcelona, Spain.B.
McInnes, T. Pedersen, and J. Carlis.
2007.
UsingUMLS Concept Unique Identifiers (CUIs) for WordSense Disambiguation in the Biomedical Domain.
InProceedings of the AMIA Symposium, pages 533?537,Chicago, IL.R.
Mooney.
1996.
Comparative Experiments on Disam-biguating Word Senses: An Illustration of the Role ofBias in Machine Learning.
In Proceedings of EMNLP-96, pages 82?91, Philadelphia, PA.E.
W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley & Sons.G.
Savova, A. Coden, I. Sominsky, R. Johnson, P. Ogren,C.
de Groen, and C. Chute.
2008.
Word Sense Disam-biguation across Two Domains: Biomedical Literatureand Clinical Notes.
Journal of Biomedical Informat-ics, 41(6):1088?1100.M.
Stevenson and Y. Guo.
2010.
Disambiguation of Am-biguous Biomedical Terms using Examples Generatedfrom the UMLS Metathesaurus.
Journal of Biomedi-cal Informatics, 43(5):762?773.M.
Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.2008.
Disambiguation of biomedical text using di-verse sources of information.
BMC Bioinformatics,9(Suppl 11):S7.M.
Weeber, J. Mork, and A. Aronson.
2001.
Developinga Test Collection for Biomedical Word Sense Disam-biguation.
In Proceedings of AMIA Symposium, pages746?50, Washington, DC.J.
Westbrook, E. Coiera, and A. Gosling.
2005.
Do On-line Information Retrieval Systems Help ExperiencedClinicians Answer Clinical Questions?
Journal of theAmerican Medical Informatics Association, 12:315?321.H.
Xu, J.
Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,and Friedman C. 2007.
Gene symbol disambigua-tion using knowledge-based profiles.
Bioinformatics,23(8):1015?22.A.
Jimeno Yepes and A. Aronson.
2011.
Self-trainingand co-training in biomedical word sense disambigua-tion.
In Proceedings of BioNLP 2011 Workshop, pages182?183, Portland, Oregon, USA, June.239
